- en: Chapter 9\. Inference Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past few chapters, we learned several techniques for adapting and utilizing
    LLMs to solve specific tasks. In this chapter, we will learn how to efficiently
    perform inference on them for real-world usage. LLMs’ large size make deployment
    and inference particularly challenging, as they exert significant pressure on
    compute, memory, and energy requirements. This proves to be especially challenging
    on edge devices like mobile phones.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of the chapter, we will focus on the field of inference optimization,
    discussing the factors influencing LLM inference time. We will then showcase a
    variety of optimization techniques including caching, knowledge distillation,
    early exiting, quantization, parallel and speculative decoding, and more.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Inference Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What are the bottlenecks affecting LLM inference? As we all know, their gargantuan
    sizes necessitate vast computing and memory resources. Apart from that, two additional
    factors exacerbate the situation:'
  prefs: []
  type: TYPE_NORMAL
- en: As seen in [Chapter 4](ch04.html#chapter_transformer-architecture), contemporary
    LLMs are based largely on decoder-only models that operate autoregressively. This
    means that each token is generated one after the other, thus imposing a sequential
    limitation. Later in this chapter, we will discuss techniques for parallel and
    speculative decoding that aim to speed up the decoding process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the input sequence length increases, the amount of compute needed increases
    quadratically. Later this chapter, we will discuss techniques like K-V caching
    that aim to alleviate this bottleneck.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive into the techniques used to optimize inference.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Optimization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this is a problem that severely impacts the deployment of LLMs in real-world
    use cases, considerable attention has been given to inference optimization research
    in major industry and academic labs. Dozens of optimization techniques have been
    developed in recent years, without which the present ubiquity of LLMs would not
    have been achieved. For a near-comprehensive survey of the various types of techniques
    used for optimizing inference, check out [Zhou et al.’s survey paper](https://oreil.ly/MtzNn).
  prefs: []
  type: TYPE_NORMAL
- en: We will now focus on some of the most promising and effective inference optimization
    techniques used in LLM deployments. While most of you may not be implementing
    these techniques by yourself but instead rely on third-party tools, understanding
    the optimization techniques and the tradeoffs involved provide valuable insights
    that can help you choose among various solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Techniques for efficient inference aim to achieve the following three goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce compute
  prefs: []
  type: TYPE_NORMAL
- en: Techniques like caching, knowledge distillation, and early exit, each of them
    employing distinct strategies to reduce computation.
  prefs: []
  type: TYPE_NORMAL
- en: Speed up decoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Techniques for parallel and speculative decoding aim to improve the throughput
    of the model: the number of tokens generated per second.'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce storage needs
  prefs: []
  type: TYPE_NORMAL
- en: Quantization techniques aim to reduce the amount of storage needed for weights
    and activations of the model, by reducing space required to store numbers from
    32 bits to 16, 8, or even 4 bits.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for Reducing Compute
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can reduce compute required during inference by:'
  prefs: []
  type: TYPE_NORMAL
- en: Trading compute for extra storage, using methods like caching.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foregoing certain operations during inference, using methods like *early exit*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving a smaller model from a larger model while preserving as many characteristics
    and capabilities from the larger model as possible, using techniques like knowledge
    distillation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next sections will explore each of these methods in detail.
  prefs: []
  type: TYPE_NORMAL
- en: K-V Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As seen in [Chapter 1](ch01.html#chapter_llm-introduction), LLMs do not have
    session memory; at every turn in an LLM conversation, the previous conversation
    history is added to the input. This means that every request to an LLM could potentially
    contain a lot of repetitive content in the prompt. For the repetitive parts of
    the prompt, the same computation is performed during the inference step again
    and again. Moreover, in autoregressive decoding, each token is generated as a
    function of the entire input and the previously generated tokens. Thus, there
    is a lot of duplicative computation.
  prefs: []
  type: TYPE_NORMAL
- en: One way to alleviate this duplicative computation is to cache the data and reuse
    them when required. More specifically, we cache the keys (K) and values (V) of
    the self-attention blocks of the Transformer architecture, referred to as the
    K-V cache. Recall our discussion in [Chapter 4](ch04.html#chapter_transformer-architecture)
    about keys and values in the self-attention block of the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some examples. Consider the task of analyzing sentiment of movie
    reviews. You might have a lengthy prompt providing detailed instructions on the
    nuances involved in analyzing sentiment. These instructions are included in the
    prompt for every input review being fed to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of incurring unnecessary overhead by repetitively processing the instruction
    tokens, the cache is consulted to fetch the K-V values for these tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, consider the example of a question-answering assistant that provides
    customer support by answering questions from a product manual. In this case, the
    K-V values representing the product manual tokens can be cached and then reused
    for any requests where the product manual needs to be part of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Caching can also enable adding a lot of few-shot examples in the prompt. This
    can sometimes be a lightweight alternative to fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Major LLM providers like Google’s Gemini and Anthropic’s Claude provide caching
    support for their models through their APIs, calling it context caching. This
    also vastly reduces the cost for end users, as cached tokens are billed only once.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that in the caching strategy, we are trading compute for additional storage.
    K-V caches can get unfeasibly large, especially at longer sequence lengths.
  prefs: []
  type: TYPE_NORMAL
- en: To keep costs under control, LLM providers typically limit the age of the cache
    to a short period or price users by caching duration.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s look at a request to Anthropic’s Claude suite of models
    that utilizes context caching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `cache_control` parameter is used to specify that the system prompt and
    the product manual is to be cached. As of the book’s writing, Claude’s cache is
    live by default for five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Organize your prompt to place the cacheable components at the beginning of the
    prompt, i.e., the prompt prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, caching can be very valuable in reducing inference time, especially
    in settings where instructions are repeated for a large number of calls, or the
    context window contains data like API documentation or RAG output that needs to
    persist across multiple calls.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll explore the early exit method for reducing inference-time compute.
  prefs: []
  type: TYPE_NORMAL
- en: Early Exit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in [Chapter 4](ch04.html#chapter_transformer-architecture), the Transformer
    architecture is made up of repeating blocks called layers. The output of each
    layer is an intermediate representation that gets fed as input to the layer above
    it. One simple way of reducing compute during inference is to exit inference at
    an intermediate layer and interpret it as the final output. This technique is
    called early exit. [Figure 9-1](#early-exit) shows early exit in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '![early-exit](assets/dllm_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Early exit in practice
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Early exit can happen both at the sequence level and at the token level.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-level early exit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this scenario, the forward pass in the Transformer is stopped at a particular
    layer for the entire input sequence, and the intermediate representations from
    that layer are taken as the final output. The layer at which to exit can be determined
    in advance or can be dynamically decided based on the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: To dynamically decide the layer to exit, you can train adapters on top of each
    layer, as shown in [Chapter 7](ch07.html#ch07). These modules can then be used
    to predict whether the exit can happen at the current layer. For example, [FastBERT](https://oreil.ly/GCfpt)
    implements modules at each layer that learn to solve a binary classification problem
    (to exit or not exit).
  prefs: []
  type: TYPE_NORMAL
- en: Not all methods depend on adding trainable modules. For example, the [hash-based
    early exiting approach (HashEE)](https://oreil.ly/_JqqH) by Sun et al. uses an
    annotated set of sequences along with their exit layers as the basis for determining
    the exit layers for new input sequences. This method is based on the hypothesis
    that similar sequences should exit at the same layers.
  prefs: []
  type: TYPE_NORMAL
- en: The second early exit option is token-level early exit.
  prefs: []
  type: TYPE_NORMAL
- en: Token-level early exit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this approach, different tokens from the same sequence can exit at different
    layers. This is more complex to implement than sequence-level early exit.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to sequence-level early exit techniques, you can implement binary classifiers
    to decide whether to exit at a particular layer, but this happens at each token
    at each layer, instead of the entire sequence. For more details on token-level
    early exit, refer to [Schuster et al.](https://oreil.ly/hfdCd), who introduced
    the technique Confident Adaptive Language Modeling (CALM), that implements token-level
    early exit.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in the self-attention subblock of the Transformer, the representation
    of a token is calculated using the representations of all other tokens in the
    sequence in the same layer. But if we are using token-level early exit, it is
    possible that some tokens in a sequence might already have exited before that
    layer. The easiest way to resolve this issue is to copy the representations of
    the exited token to every layer above it.
  prefs: []
  type: TYPE_NORMAL
- en: While token-level early exit can be more fine-grained and effective than sequence-level
    early exit, it is slower than sequence-level early exit.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In early exit, the reduction in compute comes at the cost of performance. However,
    this can be minimized by learning to exit at the optimal layer.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic early exit belongs to a class of techniques called *dynamic inference*,
    where the inference compute is determined dynamically, based on the characteristics
    of the input. One important example is the mixture of experts (MoE) class of models,
    introduced in [Chapter 4](ch04.html#chapter_transformer-architecture). In MoE
    models, a routing function chooses a small subset of expert modules to run inference
    on, thus reducing the amount of compute required.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore how we can reduce inference time by creating a smaller derivative
    model from a larger model while limiting performance degradation, using a technique
    called knowledge distillation.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#chapter_utilizing_llms), we briefly introduced distilled
    versions of models, like [DistilBERT](https://oreil.ly/rgiHZ). These are smaller
    models that approximate the capabilities of the larger models they are distilled
    from, thus enabling speedier inference.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, several techniques have been developed for knowledge distillation.
    For a survey of research advances in this field, refer to [Xu et al.’s](https://oreil.ly/JZQf3)
    survey paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of knowledge distillation can be divided into two steps: distillation
    data preparation and training. The base model is referred to as the teacher model
    and the distilled model is referred to as the student model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-2](#knowledge-distillation) depicts the process of distilling a model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![knowledge-distillation](assets/dllm_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Knowledge distillation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here’s how the distillation data preparation and training steps work.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data for distillation is typically prepared by appropriately querying the teacher
    model and using the teacher’s outputs as the *knowledge* to be distilled. Ways
    to elicit relevant outputs from the teacher include:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised generation
  prefs: []
  type: TYPE_NORMAL
- en: In this technique, the teacher is prompted with instructions and/or examples
    for solving a task. The teacher’s responses comprise the distillation dataset.
    This technique is commonly used to teach capabilities like CoT or instruction-following
    to smaller models. To accomplish that, teacher models are asked to respond to
    queries with the thought process leading up to the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: In this technique, the teacher is shown a set of seed input-output examples.
    Based on the seed examples, the teacher generates similar input-output examples,
    constituting the distillation dataset. Note that both the input and output are
    generated by the teacher model in this setting. The limitation of this technique
    is that the teacher is unable to generate sufficiently diverse examples.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate representations
  prefs: []
  type: TYPE_NORMAL
- en: This class of techniques is known as white-box distillation. Here the distillation
    dataset consists of intermediate representations from a model, which can include
    activations or output logits. This data can be used to align the student model
    with the teacher model. The alignment is learned using methods like KL-divergence,
    discussed in [Chapter 4](ch04.html#chapter_transformer-architecture).
  prefs: []
  type: TYPE_NORMAL
- en: Teacher feedback
  prefs: []
  type: TYPE_NORMAL
- en: In this class of techniques, the outputs from a student model are assessed by
    the teacher model to generate feedback. The teacher model can be used to generate
    preference data, i.e., the quality ranking of outputs from the student. Feedback
    can also be given in the form of detailed instructions on how to improve on a
    given task. A popular technique using teacher feedback is RLAIF, which we introduced
    in [Chapter 5](ch05.html#chapter_utilizing_llms).
  prefs: []
  type: TYPE_NORMAL
- en: Self-teaching
  prefs: []
  type: TYPE_NORMAL
- en: In this class of techniques, the teacher and student model are one and the same.
    The student model progressively refines its own outputs and uses them as the distillation
    set. One way of self-teaching is to generate multiple outputs for each task, along
    with reasoning steps, and choosing the best one to be part of the distillation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: How many distillation examples do you need? Perhaps surprisingly, not a whole
    lot. [Zhou et al.](https://oreil.ly/MuOOj) show that even one thousand very high-quality
    examples are enough to create a strong distillation set.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just like fine-tuning and continued pre-training, knowledge distillation is
    susceptible to the catastrophic forgetting problem (introduced in [Chapter 7](ch07.html#ch07)).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the various ways to create distillation datasets, let’s
    turn to the actual distillation process.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some techniques used to perform the distillation task. For a more
    detailed survey of techniques, refer to [Xu et al.](https://oreil.ly/9mbiN):'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: This is the simplest way to accomplish knowledge distillation. The student model
    is fine-tuned using the distillation set with the objective of aligning its predictions
    with that of the teacher model. This method is typically used in black-box knowledge
    distillation settings, where the distillation set does not comprise any internal
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: K-L divergence of output probabilities
  prefs: []
  type: TYPE_NORMAL
- en: In this method, our objective function is to minimize the K-L divergence between
    the output probability distribution of the teacher model and the student model.
  prefs: []
  type: TYPE_NORMAL
- en: Internal representation similarity
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, instead of minimizing divergence, you can maximize similarity between
    aspects of the teacher and student model. This can be leveraged to perform layerwise
    distillation, where the internal representations of the teacher and the student
    are aligned at each layer. Refer to [Liang et al.](https://oreil.ly/g-C4L) for
    an effective technique for layerwise distillation.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: This involves training a reward model using the distillation data. The student
    model is then trained to maximize the reward as per the reward model. Recall our
    discussion on reinforcement learning in [Chapter 8](ch08.html#ch8).
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the technique you choose to distill your models depends on whether
    you have access to the teacher weights. If you do not have access to the teacher
    weights, then you can perform only supervised fine-tuning. White-box distillation,
    where you are trying to align intermediate representations and not just the output
    tokens, can be challenging to achieve. Note that all knowledge distillation techniques
    carry the risk of capability degradation or catastrophic forgetting, so you will
    have to evaluate the student model very carefully to quantify the difference in
    capabilities from the teacher model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we discussed three distinct techniques for reducing compute
    during inference: caching, early exit, and knowledge distillation. Next, let’s
    discuss techniques that can accelerate the decoding process.'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for Accelerating Decoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, autoregressive models output one token at a time, where the next
    token being generated is a function of the input tokens and all the previously
    generated tokens. This imposes a sequential limitation as you have to wait for
    the current token to be generated before generating the next one. Can we bypass
    this limitation? Several recent techniques like *speculative decoding* and *parallel
    decoding* have been developed. Let’s examine them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Speculative Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept behind speculative decoding is simple. A smaller model, called a
    draft model, is used to generate several subsequent candidate output tokens. Then,
    the main larger model is used to compute the conditional probabilities of the
    candidate output tokens at once, using them to decide which tokens to accept and
    which ones to reject. The more draft tokens accepted, the better the draft model.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-3](#speculative-decoding) depicts the speculative decoding process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![speculative-decoding](assets/dllm_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Speculative decoding in action
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Two important metrics in speculative decoding are:'
  prefs: []
  type: TYPE_NORMAL
- en: Token acceptance rate
  prefs: []
  type: TYPE_NORMAL
- en: This is the percentage of tokens generated by the draft model that are accepted.
    Typically, this does not reach 1, because if it did, there is no need to use the
    main larger model.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding speedup
  prefs: []
  type: TYPE_NORMAL
- en: This refers to the reduction in latency between a model purely using autoregressive
    decoding versus one using speculative decoding.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Can we generate more than one token at the same time? This can be done either
    using the same model (multi-token decoding) or multiple instances of the same
    model.
  prefs: []
  type: TYPE_NORMAL
- en: For the latter, we can control parallel generation through the prompt. For example,
    say you are writing an article about a tourist site, containing sections like
    Food, Stay, Safety Tips, etc. You can prompt the LLM to list the sections, marked
    with special tokens. These sections can then be generated in parallel, assuming
    the sections are fully independent of each other.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-4](#parallel-generation) depicts the workflow of a system that generates
    parts of the output in a parallel fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore how the same model can generate multiple tokens at a time,
    called multi-token decoding. Several techniques have been proposed recently for
    multi-token decoding, one of the most promising being Medusa by [Cai et al.](https://oreil.ly/qT94i)
  prefs: []
  type: TYPE_NORMAL
- en: In Medusa, additional decoding heads are added to the model. These decoding
    heads represent subsequent tokens to be generated. For example, the standard decoding
    head is predicting the next (n + 1st) token in the sequence, and the additional
    decoding heads are predicting the n + 2nd, n + 3rd, and so on, tokens, respectively.
    Refer to the Medusa paper for more details on how this is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned techniques to accelerate the decoding process and to
    reduce compute. Next, let’s dive into quantization, a class of techniques to reduce
    the storage required by the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Parallel Generation](assets/dllm_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Parallel decoding workflow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Techniques for Reducing Storage Needs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Chapters [5](ch05.html#chapter_utilizing_llms) and [6](ch06.html#llm-fine-tuning),
    we briefly touched upon quantization but promised to go into detail later. Let’s
    dive in!
  prefs: []
  type: TYPE_NORMAL
- en: The forward pass of a language model involves numbers representing inputs, weights,
    and activations. How are these numbers represented in memory?
  prefs: []
  type: TYPE_NORMAL
- en: Several types of numerical representation formats are available, like integer,
    floating point, etc. Typically, numbers in language models are represented in
    floating-point32 (FP32), also called single-precision floating point, which refers
    to a floating point number composed of 32 bits, or 4 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A number represented in FP32 is composed of three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: A sign bit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exponent (8 bits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mantissa/significand (23 bits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details on how FP32 works, see [“Demystifying Floating Point Precision”](https://oreil.ly/uCYYl).
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum and minimum value that can be represented using FP32 is 3.4028237
    × 10^(38) and 1.175494 × 10^(38), respectively. This is referred to as the range
    of values that can be represented by this format. Similarly, a number represented
    in float16 (FP16), also referred to as half-precision floating point, is composed
    of these three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: A sign bit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exponent (5 bits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mantissa/significand (10 bits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens when you take a number that is represented using FP32 and represent
    it in FP16? This amounts to a lossy conversion. In this case, both the range and
    the precision are impacted, because in FP16, 65,504 is the largest number you
    can represent, compared to 3.4 × 10^(38) for FP32\. The precision is impacted
    too, as the 32-bit version offers ~7 digits of precision, but the 16-bit version
    only offers ~3 digits of precision.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent the massive loss in precision with FP16, bfloat16 (BF16), also called
    the brain floating point, was invented by Google Brain. In BF16, there are 8 digits
    for the exponent, and 7 bits for the mantissa. This keeps the range of numbers
    represented the same as that of float32 at the cost of reduced precision.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Older GPUs like the NVIDIA T4 do not support BF16.
  prefs: []
  type: TYPE_NORMAL
- en: The process of converting representation of a number from a higher-precision
    format to a lower-precision format is called quantization. We can quantize 32-bit
    values to 8-bit integer formats as well. This reduces memory requirements by a
    factor of 4, at the cost of even more precision. With 8-bit quantization, we can
    represent numbers between –127 and 127, without any decimal point.
  prefs: []
  type: TYPE_NORMAL
- en: Integer quantization can be performed either symmetrically or asymmetrically.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this setting, the *0* value in the original format is mapped to the *0* value
    in the integer representation. This means that when you quantize 0 represented
    in fp32 to int8, the value remains 0.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining values can be mapped using various techniques, the most common
    being absmax quantization. In this method, if we know or can estimate the range
    of numbers that need to be represented, we can take the absolute maximum of the
    range and map it to the largest number in int8 (127), while the negative of the
    absolute maximum is mapped to the smallest number in int8 (–127). The remaining
    numbers are mapped according to scale.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-5](#symmetric-quantization) depicts absmax quantization at work,
    quantizing a number represented in FP32 to int8.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Absmax quantization](assets/dllm_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Absmax quantization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Asymmetric Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this setting, the *0* value in the original format is not guaranteed to be
    mapped to the *0* value in the integer representation.
  prefs: []
  type: TYPE_NORMAL
- en: A common technique is to take the minimum and maximum value that we need represented
    and map it to the minimum (–127) and maximum (127) values that can be represented
    in int8, respectively. For example, if the range of numbers we want represented
    is –23 to 87, then –23 is mapped to –127 and 87 is mapped to 127.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the range of numbers you want represented include outliers, they can play
    spoilsport. You can take care of outliers by clipping them, so that all outliers
    will be represented by the same maximum/minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: How is quantization used in practice? Typically, quantization is applied after
    training. Both the model’s weights and activations can be quantized.
  prefs: []
  type: TYPE_NORMAL
- en: Quantizing weights is much easier than quantizing activations. Since we know
    the weights beforehand, we can calculate the range, outliers, scaling factors,
    etc. that are needed for the quantization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: For activations, depending on how much latency we can tolerate, we can either
    perform dynamic or static scaling. In dynamic scaling, statistics like range,
    outliers, etc. are calculated dynamically during inference at each layer. In static
    scaling, we take a reference calibration dataset to estimate the statistics. While
    this approach speeds up inference, it can result in more quantization errors.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on implementing quantization, see [“A Visual Guide to Quantization”](https://oreil.ly/bpi3b)
    by Maarten Grootendorst.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the causes of bottlenecks in LLM inference. We
    discussed a wide variety of techniques to make LLM inference more efficient, including
    techniques to reduce compute requirements, reduce storage requirements, and accelerate
    the decoding process. We explored techniques like caching, early exit, knowledge
    distillation, speculative and parallel decoding techniques, and quantization.
    In the next and final part of the book, we will explore LLM application paradigms
    and discuss the nuances involved in building full-fledged applications.
  prefs: []
  type: TYPE_NORMAL
