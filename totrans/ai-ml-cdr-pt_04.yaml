- en: 'Chapter 3\. Going Beyond the Basics: Detecting Features in Images'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章. 超越基础：检测图像中的特征
- en: In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    you learned how to get started with computer vision by creating a simple neural
    network that matched the input pixels of the Fashion MNIST dataset to 10 labels,
    each of which represented a type (or class) of clothing. And while you created
    a network that was pretty good at detecting clothing types, there was a clear
    drawback. Your neural network was trained on small monochrome images, each of
    which contained only a single item of clothing, and each item was centered within
    the image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中，你学习了如何通过创建一个简单的神经网络来入门计算机视觉，该神经网络将Fashion
    MNIST数据集的输入像素与10个标签相匹配，每个标签代表一种（或一类）服装。虽然你创建了一个在检测服装类型方面相当不错的网络，但存在一个明显的缺点。你的神经网络是在小单色图像上训练的，每个图像只包含一件服装，而且每件服装都位于图像的中心。
- en: To take the model to the next level, you need it to be able to detect *features*
    in images. So, for example, instead of looking merely at the raw pixels in the
    image, what if we could filter the images down to constituent elements? Matching
    those elements, instead of raw pixels, would help the model detect the contents
    of images more effectively. For example, consider the Fashion MNIST dataset that
    we used in the last chapter. When detecting a shoe, the neural network may have
    been activated by lots of dark pixels clustered at the bottom of the image, which
    it would see as the sole of the shoe. But if the shoe were not centered and filling
    the frame, this logic wouldn’t hold.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型提升到下一个层次，你需要它能够检测图像中的*特征*。例如，如果我们能够将图像过滤到构成元素，而不是仅仅查看图像中的原始像素，那会怎么样？用这些元素匹配，而不是原始像素，将有助于模型更有效地检测图像内容。例如，考虑我们在上一章中使用的Fashion
    MNIST数据集。在检测鞋子时，神经网络可能被图像底部的许多暗色像素簇激活，它将看到这是鞋底。但如果鞋子没有居中且填满画面，这种逻辑就不成立了。
- en: One method of detecting features comes from photography and image processing
    methodologies that you may already be familiar with. If you’ve ever used a tool
    like Photoshop or GIMP to sharpen an image, you’ve used a mathematical filter
    that works on the pixels of the image. Another word for what these filters do
    is *convolution*, and by using such filters in a neural network, you’ll create
    a *convolutional neural network* (CNN).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 检测特征的一种方法来自摄影和图像处理方法，你可能已经熟悉。如果你曾经使用过Photoshop或GIMP之类的工具来锐化图像，你就已经使用了一种作用于图像像素的数学滤波器。这些滤波器所做的工作的另一个词是*卷积*，通过在神经网络中使用这些滤波器，你将创建一个*卷积神经网络*（CNN）。
- en: In this chapter, you’ll start by learning about how to use convolutions to detect
    features in an image. Then, you’ll dig deeper into classifying images based on
    the features within. We’ll also explore augmentation of images to get more features
    and transfer learning to take preexisting features that were learned by others,
    and then we’ll look briefly into optimizing your models by using dropouts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将首先学习如何使用卷积来检测图像中的特征。然后，你将深入了解基于图像内部特征进行图像分类。我们还将探索图像增强以获取更多特征，以及迁移学习，即利用他人学习到的现有特征，然后我们将简要探讨通过使用dropout来优化你的模型。
- en: Convolutions
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: A *convolution* is simply a filter of weights that are used to multiply a pixel
    by its neighbors to get a new value for the pixel. For example, consider the ankle
    boot image from Fashion MNIST and the pixel values for it (see [Figure 3-1](#ch03_figure_1_1748570891059985)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*简单来说是一个权重滤波器，它通过将像素与其邻居相乘来得到像素的新值。例如，考虑Fashion MNIST中的踝靴图像及其像素值（见[图3-1](#ch03_figure_1_1748570891059985)）。'
- en: '![](assets/aiml_0301.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0301.png)'
- en: Figure 3-1\. Ankle boot with convolution
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1\. 女式踝靴与卷积
- en: If we look at the pixel in the middle of the selection, we can see that it has
    the value 192\. (Recall that Fashion MNIST uses monochrome images with pixel values
    from 0 to 255.) The pixel above and to the left has the value 0, the one immediately
    above has the value 64, etc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看选择区域中间的像素，我们可以看到它的值是192。（回想一下，Fashion MNIST使用像素值为0到255的单色图像。）上面的像素值是0，直接上面的像素值是64，等等。
- en: If we then define a filter in the same 3 × 3 grid, as shown below the original
    values, we can transform that pixel by calculating a new value for it. We do this
    by multiplying the current value of each pixel in the grid by the value in the
    same position in the filter grid and then summing up the total amount. This total
    will be the new value for the current pixel, and we then repeat this calculation
    for all pixels in the image.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们然后在下面的原始值中定义一个3 × 3的网格过滤器，就像下面所示，我们可以通过计算它的新值来转换该像素。我们通过将网格中每个像素的当前值乘以过滤器网格中相同位置的值，然后求和来实现这一点。这个总和将是当前像素的新值，然后我们重复这个计算过程，直到图像中的所有像素。
- en: 'So, in this case, while the current value of the pixel in the center of the
    selection is 192, we calculate the new value after applying the filter as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，尽管选择中心像素的当前值为192，我们通过以下方式计算应用过滤器后的新值：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The result equals 577, which will be the new value for the pixel. Repeating
    this process for every pixel in the image will give us a filtered image.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 结果等于577，这将是像素的新值。对图像中的每个像素重复此过程将给我们一个过滤后的图像。
- en: 'Now, let’s consider the impact of applying a filter on a more complicated image:
    specifically, the [ascent image](https://oreil.ly/wP8TE) that’s built into SciPy
    for easy testing. This is a 512 × 512 grayscale image that shows two people climbing
    a staircase.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑在一个更复杂的图像上应用过滤器的影响：具体来说，是SciPy中内置的用于简单测试的[ascent image](https://oreil.ly/wP8TE)。这是一张512
    × 512的灰度图像，显示了两个人在爬楼梯。
- en: Using a filter with negative values on the left, positive values on the right,
    and zeros in the middle will end up removing most of the information from the
    image except for vertical lines (see [Figure 3-2](#ch03_figure_2_1748570891060023)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用左侧带负值、右侧带正值和中间为零的过滤器，最终会从图像中移除大部分信息，除了垂直线（参见[图3-2](#ch03_figure_2_1748570891060023)）。
- en: '![](assets/aiml_0302.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0302.png)'
- en: Figure 3-2\. Using a filter to derive vertical lines
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 使用过滤器提取垂直线
- en: Similarly, a small change to the filter can emphasize the horizontal lines (see
    [Figure 3-3](#ch03_figure_3_1748570891060045)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对过滤器的小幅调整可以强调水平线（参见[图3-3](#ch03_figure_3_1748570891060045)）。
- en: '![](assets/aiml_0303.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0303.png)'
- en: Figure 3-3\. Using a filter to derive horizontal lines
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 使用过滤器提取水平线
- en: These examples also show that the amount of information in the image is reduced.
    Therefore, we can potentially *learn* a set of filters that reduce the image to
    features, and those features can be matched to labels as before. Previously, we
    learned parameters that were used in neurons to match inputs to outputs, and similarly,
    we can learn the best filters to match inputs to outputs over time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子还表明，图像中的信息量减少了。因此，我们可能可以*学习*一组过滤器，将图像减少到特征，而这些特征可以像以前一样与标签匹配。以前，我们学习了用于匹配输入到输出的神经元参数，同样，我们可以随着时间的推移学习最佳的过滤器来匹配输入到输出。
- en: When we combine convolution with pooling, we can reduce the amount of information
    in the image while maintaining the features. We’ll explore that next.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将卷积与池化结合使用时，我们可以在保持特征的同时减少图像中的信息量。我们将在下一节中探讨这一点。
- en: Pooling
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化
- en: '*Pooling* is the process of eliminating pixels in your image while maintaining
    the semantics of the content within the image. It’s best explained visually. [Figure 3-4](#ch03_figure_4_1748570891060063)
    depicts the concept of *max pooling*.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*池化*是在保持图像内容语义的同时消除图像中像素的过程。它最好通过视觉来解释。[图3-4](#ch03_figure_4_1748570891060063)展示了*最大池化*的概念。'
- en: '![](assets/aiml_0304.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0304.png)'
- en: Figure 3-4\. An example of max pooling
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. 最大池化的示例
- en: In this case, consider the box on the left to be the pixels in a monochrome
    image. We group them into 2 × 2 arrays, so in this case, the 16 pixels are grouped
    into four 2 × 2 arrays. These arrays are called *pools*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，将左边的框视为单色图像中的像素。我们将它们分成2 × 2的数组，因此在这种情况下，16个像素被分成四个2 × 2的数组。这些数组被称为*池*。
- en: Then, we select the *maximum* value in each of the groups and reassemble them
    into a new image. Thus, the pixels on the left are reduced by 75% (from 16 to
    4), with the maximum value from each pool making up the new image. [Figure 3-5](#ch03_figure_5_1748570891060080)
    shows the version of ascent from [Figure 3-2](#ch03_figure_2_1748570891060023),
    with the vertical lines enhanced, after max pooling has been applied.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从每个组中选择*最大*值，并将它们重新组装成一个新的图像。因此，左边的像素减少了75%（从16变为4），每个池中的最大值构成了新的图像。[图3-5](#ch03_figure_5_1748570891060080)显示了从[图3-2](#ch03_figure_2_1748570891060023)的上升版本，在应用最大池化后，垂直线得到了增强。
- en: '![](assets/aiml_0305.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0305.png](assets/aiml_0305.png)'
- en: Figure 3-5\. Ascent after applying vertical filter and max pooling
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5\. 应用垂直滤波和最大池化后的上升
- en: Note how the filtered features have not just been maintained but have been further
    enhanced. Also, the image size has changed from 512 × 512 to 256 × 256—making
    it a quarter of the original size.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意过滤后的特征不仅被保留，而且得到了进一步的增强。同时，图像大小从512 × 512变为256 × 256——变成了原始大小的四分之一。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are other approaches to pooling. These include *min pooling*, which takes
    the smallest pixel value from the pool, and *average pooling*, which takes the
    overall average value from the pool.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 存在其他池化方法。这些包括*最小池化*，它从池中取最小的像素值，以及*平均池化*，它取池中的整体平均值。
- en: Implementing Convolutional Neural Networks
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现卷积神经网络
- en: 'In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    you created a neural network that recognized fashion images. For convenience,
    here’s the code to define the model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中，你创建了一个用于识别时尚图像的神经网络。为了方便，这里提供了定义模型的代码：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To convert this to a CNN, you simply use convolutional layers in our model definition
    on top of the current linear ones. You’ll also add pooling layers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此转换为CNN，你只需在当前线性层之上使用我们的模型定义中的卷积层。你还将添加池化层。
- en: To implement a convolutional layer, you’ll use the `nn.Conv2D` type. It accepts
    as parameters the number of convolutions to use in the layer, the size of the
    convolutions, the activation function, etc.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现卷积层，你将使用`nn.Conv2D`类型。它接受作为参数的层中要使用的卷积数量、卷积的大小、激活函数等。
- en: 'For example, here’s a convolutional layer that uses this type:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里有一个使用此类型的卷积层：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this case, we want the layer to learn `64` convolutions. It will randomly
    initialize them, and over time, it will learn the filter values that work best
    to match the input values to their labels. The `kernel_size = 3` indicates the
    size of the filter. Earlier, we showed you 3 × 3 filters, and that’s what we’re
    specifying here. The 3 × 3 filter is the most common size of filter. You can change
    it as you see fit, but you’ll typically see an odd number of axes like 5 × 5 or
    7 × 7 because of how filters remove pixels from the borders of the image, as you’ll
    see later.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们希望层学习`64`个卷积。它将随机初始化它们，并且随着时间的推移，它将学习最佳的过滤器值，以匹配输入值和它们的标签。`kernel_size
    = 3`表示过滤器的大小。之前我们展示了3 × 3的过滤器，这就是我们在这里指定的。3 × 3的过滤器是最常见的过滤器大小。你可以根据需要更改它，但通常你会看到像5
    × 5或7 × 7这样的奇数轴，因为过滤器会从图像的边缘移除像素，就像你稍后将要看到的那样。
- en: 'Here’s how to use a pooling layer in the neural network. You’ll typically do
    this immediately after the convolutional layer:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在神经网络中使用池化层的方法。你通常会在卷积层之后立即这样做：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the example back in [Figure 3-4](#ch03_figure_4_1748570891060063), we split
    the image into 2 × 2 pools and picked the maximum value in each. However, we could
    have used the parameters that you see here to define the pool size. The `kernel_size=2`
    parameter indicates that our pools are 2 × 2, and the `stride=2` parameter means
    that the filter will jump over two pixels to get the next pool.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的[图3-4](#ch03_figure_4_1748570891060063)示例中，我们将图像分割成2 × 2的池，并从每个池中选取最大值。然而，我们可以使用这里看到的参数来定义池的大小。`kernel_size=2`参数表示我们的池是2
    × 2的，而`stride=2`参数意味着过滤器将跳过两个像素以获取下一个池。
- en: 'Now, let’s explore the full code to define a model for Fashion MNIST with a
    CNN:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索定义Fashion MNIST模型的完整代码：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we see that the class has two functions, one for initialization and one
    that will be called during the forward pass in each epoch during training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到该类有两个函数，一个用于初始化，另一个将在每个训练周期的正向传递期间被调用。
- en: The `init` simply defines what each of the layers in our neural network will
    look like. The first layer (`self.layer1`) will take in the one-dimensional input,
    have `64` convolutions, a `kernel_size` of `3`, and `padding` of `1`. It will
    then ReLU the output before max pooling it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`init`简单地定义了我们的神经网络中每一层的样子。第一层（`self.layer1`）将接受一维输入，有`64`个卷积，`kernel_size`为`3`，`padding`为`1`。然后它将在最大池化之前对输出进行ReLU操作。'
- en: The next layer (`self.layer2`) will take the 64 convolutions of output from
    the previous layer and then output `64` of its own before ReLUing them and max
    pooling them. Its output will now be `64 × 6 × 6` because the `MaxPool` halves
    the size of the image.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下一层（`self.layer2`）将接受前一层输出的64个卷积，然后输出自己的`64`个卷积，在ReLU和最大池化之前。由于`MaxPool`将图像大小减半，其输出现在将是`64
    × 6 × 6`。
- en: The data is then fed to the next layer (`self.fc1`, where `fc` stands for *fully
    connected*), with the input being the shape of the output of the previous layer.
    The output is 128, which is the same number of neurons we used in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    for the deep neural network (DNN).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，数据被送入下一层（`self.fc1`，其中`fc`代表*全连接*），输入是前一层输出的形状。输出是128，这与我们在[第2章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中用于深度神经网络（DNN）的神经元数量相同。
- en: Finally, these 128 are fed into the final layer (`self.fc1`) with 10 outputs—that
    represent the 10 classes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这128个神经元被送入最终的层（`self.fc1`），有10个输出——代表10个类别。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the DNN, we ran the input through a `Flatten` layer prior to feeding it into
    the first `Dense` layer. We’ve lost that in the input layer here—instead, we’ve
    just specified the 1-D input shape. Note that prior to the first `Linear` layer,
    after convolutions and pooling, the data will be flattened.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在DNN中，我们在将其送入第一个`Dense`层之前，先通过一个`Flatten`层对输入进行处理。在这里，我们失去了输入层——相反，我们只是指定了1-D输入形状。注意，在第一个`Linear`层之前，经过卷积和池化后，数据将被展平。
- en: Then, we stack these layers in the `forward` function. We can see that we get
    the data `x` and pass it through `layer1` to get `out`, which is passed to `layer2`
    to get a new `out`. At this point, we have the convolutions that we’ve learned,
    but we need to flatten them before loading them into the `Linear` layers `fc1`
    and `fc2`. The `out = out.view(out.size(0), -1)` achieves this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在`forward`函数中堆叠这些层。我们可以看到我们得到数据`x`，通过`layer1`得到`out`，然后将其传递给`layer2`以得到新的`out`。在这个阶段，我们有了我们学习到的卷积，但在将它们加载到`Linear`层`fc1`和`fc2`之前，我们需要将它们展平。`out
    = out.view(out.size(0), -1)`实现了这一点。
- en: 'If we train this network on the same data for the same 50 epochs as we used
    when training the network shown in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    we will see that it works nicely. We can get to 91% accuracy on the test set quite
    easily:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用与我们在[第2章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中训练网络时相同的数据和相同的50个epoch来训练这个网络，我们将看到它运行得很好。我们可以在测试集上轻松达到91%的准确率：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, we can see that adding convolutions to the neural network definitely increases
    its ability to classify images. Next, let’s take a look at the journey an image
    takes through the network so we can get a little bit more of an understanding
    of why this process works.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到向神经网络添加卷积确实增加了其分类图像的能力。接下来，让我们看看图像在网络中的旅程，这样我们可以更深入地理解这个过程为什么有效。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you are using the accompanying code from my GitHub, you’ll notice that I’m
    using model.to(device) a lot. In PyTorch, if an accelerator is available, you
    can request that the model and/or its data use the accelerator with this command.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用我GitHub上的配套代码，您会注意到我大量使用了model.to(device)。在PyTorch中，如果可用加速器，您可以使用此命令请求模型及其数据使用加速器。
- en: Exploring the Convolutional Network
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索卷积网络
- en: 'With the torchsummary library, you can inspect your model. When you run it
    on the Fashion MNIST convolutional network we’ve been working on, you’ll see something
    like this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用torchsummary库，您可以检查您的模型。当您在我们一直在工作的Fashion MNIST卷积网络上运行它时，您会看到类似这样的内容：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s first take a look at the `Output Shape` column to get an understanding
    of what’s going on here. Our first layer will have 28 × 28 images and apply 64
    filters to them. But because our filter is 3 × 3, a one-pixel border around the
    image would typically be lost, reducing our overall information to 26 × 26 pixels.
    However, because we used the `padding=1` parameter, the image was artificially
    inflated to 30 × 30, meaning that its output would be the correct 28 × 28 and
    no information would be lost.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看“输出形状”列，以了解这里发生了什么。我们的第一层将有 28 × 28 的图像，并对其应用 64 个滤波器。但由于我们的滤波器是 3 × 3，图像周围通常会有一个像素的边界丢失，这会减少我们的整体信息到
    26 × 26 像素。然而，因为我们使用了 `padding=1` 参数，图像被人工膨胀到 30 × 30，这意味着其输出将是正确的 28 × 28，并且不会丢失任何信息。
- en: If you don’t pad the image, you’ll end up with a result like the one in [Figure 3-6](#ch03_figure_6_1748570891060097).
    If we take each of the boxes as a pixel in the image, the first possible filter
    we can use starts in the second row and the second column. The same would happen
    on the right side and at the bottom of the diagram.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有对图像进行填充，你最终会得到 [图 3-6](#ch03_figure_6_1748570891060097) 中的结果。如果我们把每个方框看作图像中的一个像素，第一个可能的滤波器从第二行第二列开始。同样的事情会在图的右侧和底部发生。
- en: '![](assets/aiml_0306.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0306.png)'
- en: Figure 3-6\. Losing pixels when running a filter
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 运行滤波器时丢失像素
- en: Thus, an image that is *a* × *b* pixels in shape when run through a 3 × 3 filter
    will become (*a* – 2) × (*b* – 2) pixels in shape. Similarly, a 5 × 5 filter would
    make it (*a* – 4) × (*b* – 4), and so on. As we’re using a 28 × 28 image and a
    3 × 3 filter, our output would now be 26 × 26\. But because we padded the image
    up to 30 × 30 (again, to prevent loss of information), the output is now 28 ×
    28.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个形状为 *a* × *b* 像素的图像经过 3 × 3 滤波器处理后，其形状将变为 (*a* – 2) × (*b* – 2) 像素。同样，5
    × 5 滤波器会使它变为 (*a* – 4) × (*b* – 4)，依此类推。由于我们使用的是 28 × 28 的图像和 3 × 3 滤波器，我们的输出现在将是
    26 × 26。但是，因为我们已经将图像填充到 30 × 30（再次，为了防止信息丢失），所以输出现在是 28 × 28。
- en: After that, the pooling layer will be 2 × 2, so the size of the image will halve
    on each axis, and it will then become 14 × 14\. The next convolutional layer does
    *not* use padding, so it will reduce this further to 12 × 12, and the next pooling
    will output 6 x 6.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，池化层将是 2 × 2，因此图像在每个轴上的大小将减半，然后变为 14 × 14。下一个卷积层不使用填充，因此它将进一步减少到 12 × 12，下一个池化将输出
    6 × 6。
- en: 'So, by the time the image has gone through two convolutional layers, the result
    will be many 6 × 6 images. How many? We can see that in the `Param #` (number
    of parameters) column.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当图像通过两个卷积层后，结果将是许多 6 × 6 的图像。有多少个？我们可以在“参数数量”列中看到。
- en: Each convolution is a 3 × 3 filter, plus a bias. Remember earlier, with our
    dense layers, when each layer was *y* = *wx* + *b*, where *w* was our parameter
    (aka weight) and *b* was our bias? This case is very similar, except that because
    the filter is 3 × 3, there are 9 parameters to learn. Given that we have 64 convolutions
    defined, we’ll have 640 overall parameters. (Each convolution has 9 parameters
    plus a bias, for a total of 10, and there are 64 of them.)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积都是一个 3 × 3 滤波器，加上一个偏差。记得之前，在我们的密集层中，每个层都是 *y* = *wx* + *b*，其中 *w* 是我们的参数（即权重）和
    *b* 是我们的偏差？这个情况非常相似，只是因为滤波器是 3 × 3，所以有 9 个参数要学习。鉴于我们定义了 64 个卷积，我们将有 640 个总参数。（每个卷积有
    9 个参数加上一个偏差，总共 10 个，共有 64 个。）
- en: The `ReLU and MaxPooling` layers don’t learn anything; they just reduce the
    image, so there are no learned parameters there—hence, 0 are reported.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: “ReLU 和 MaxPooling”层不会学习任何东西；它们只是减少图像的大小，因此那里没有学习的参数——因此，报告为 0。
- en: The next convolutional layer has 64 filters, but each of them is multiplied
    across the *previous* 64 filters, each of which has 9 parameters. We have a bias
    on each of the new 64 filters, so our number of parameters should be (64 × (64
    × 9)) + 64, which gives us 36,928 parameters the network needs to learn.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个卷积层有 64 个滤波器，但每个滤波器都是乘以前面的 64 个滤波器，每个滤波器有 9 个参数。我们在每个新的 64 个滤波器上都有一个偏差，所以我们的参数数量应该是
    (64 × (64 × 9)) + 64，这给我们带来了网络需要学习的 36,928 个参数。
- en: If this is confusing, try changing the number of convolutions in the first layer
    to something else—for example, 10\. You’ll see that the number of parameters in
    the second layer becomes 5,824, which is (64 × (10 × 9)) + 64).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这让你感到困惑，试着将第一层的卷积数量改为其他数值——例如，10。你会发现第二层的参数数量变为 5,824，这是 (64 × (10 × 9)) +
    64)。
- en: By the time we get through the second convolution, our images are 6 × 6, and
    we have 64 of them. If we multiply this out, we’ll have 1,600 values, which we’ll
    feed into a dense layer of 128 neurons. Each neuron has a weight and a bias, and
    we’ll have 128 of them, so the number of parameters the network will learn is
    ((6 × 6 × 64) × 128) + 128, giving us 295,040 parameters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到我们完成第二次卷积时，我们的图像是6 × 6，我们有64个这样的图像。如果我们乘以这个数字，我们将有1,600个值，我们将把这些值输入到一个有128个神经元的密集层中。每个神经元都有一个权重和偏差，我们将有128个这样的神经元，所以网络将学习的参数数量是((6
    × 6 × 64) × 128) + 128，这给我们295,040个参数。
- en: Then, our final dense layer of 10 neurons will take in the output of the previous
    128, so the number of parameters learned will be (128 × 10) + 10, which is 1,290.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的最终密集层将有10个神经元，它将接收之前128个神经元的输出，所以学习的参数数量将是(128 × 10) + 10，即1,290。
- en: 'The total number of parameters will be the sum of all of these: 333,898.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 参数总数将是所有这些的总和：333,898。
- en: Training this network requires us to learn the best set of these 333,898 parameters
    to match the input images to their labels. It’s a slower process because there
    are more parameters, but as we can see from the results, it also builds a more
    accurate model!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个网络需要我们学习最佳的333,898个参数集合，以匹配输入图像和它们的标签。由于参数更多，这是一个更慢的过程，但正如我们从结果中可以看到的，它也构建了一个更精确的模型！
- en: Of course, with this dataset, we still have the limitation that the images are
    28 × 28, monochrome, and centered. So next we’ll take a look at using convolutions
    to explore a more complex dataset comprising color pictures of horses and humans,
    and we’ll try to make the model determine whether an image contains one or the
    other. In this case, the subject won’t always be centered in the image like with
    Fashion MNIST, so we’ll have to rely on convolutions to spot distinguishing features.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用这个数据集，我们仍然存在限制，即图像是28 × 28，单色，并且居中。所以接下来我们将看看如何使用卷积来探索一个更复杂的数据集，该数据集包含马和人类的彩色图片，并且我们将尝试让模型确定图像中包含的是马还是人。在这种情况下，主题不总是像Fashion
    MNIST那样居中在图像中，因此我们不得不依赖卷积来识别区分特征。
- en: Building a CNN to Distinguish Between Horses and Humans
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建CNN以区分马和人
- en: In this section, we’ll explore a more complex scenario than the Fashion MNIST
    classifier. We’ll extend what we’ve learned about convolutions and CNNs to try
    to classify the contents of images in which the location of a feature isn’t always
    in the same place. I’ve created the “Horses or Humans” dataset for this purpose.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索比Fashion MNIST分类器更复杂的场景。我们将扩展我们对卷积和CNN的了解，尝试对图像内容进行分类，其中特征的位置不总是相同。我为此目的创建了“马或人”数据集。
- en: The “Horses or Humans” Dataset
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “马或人”数据集
- en: '[The dataset for this section](https://oreil.ly/8VXwy) contains over a thousand
    300 × 300–pixel images. Approximately half the images are of horses, and the other
    half are of humans—and all are rendered in different poses. You can see some examples
    in [Figure 3-7](#ch03_figure_7_1748570891060112).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[本节的数据集](https://oreil.ly/8VXwy)包含超过一千张300 × 300像素的图像。大约一半的图像是马，另一半是人——并且它们都以不同的姿势呈现。您可以在[图3-7](#ch03_figure_7_1748570891060112)中看到一些示例。'
- en: '![](assets/aiml_0307.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0307.png)'
- en: Figure 3-7\. Horses and humans
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7\. 马和人
- en: As you can see, the subjects have different orientations and poses, and the
    image composition varies. Consider the two horses, for example—their heads are
    oriented differently, and one image is zoomed out (showing the complete animal),
    while the other is zoomed in (showing just the head and part of the body). Similarly,
    the humans are lit differently, have different skin tones, and are posed differently.
    The man has his hands on his hips, while the woman has hers outstretched. The
    images also contain backgrounds such as trees and beaches, so a classifier will
    have to determine which parts of the image are the important features that determine
    what makes a horse a horse and a human a human, without being affected by the
    background.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，主题有不同的方向和姿势，图像构图也各不相同。以两匹马为例，它们的头部方向不同，其中一张图像被放大（显示整个动物），而另一张图像被缩小（只显示头部和身体的一部分）。同样，人类被不同的光线照亮，有不同的肤色，姿势也不同。男人把手放在臀部，而女人把手伸展开。图像还包含背景，如树木和海滩，因此分类器必须确定图像中哪些部分是重要的特征，这些特征决定了马是马，人是人，而不受背景的影响。
- en: While the previous examples of predicting *y* = 2*x* – 1 or classifying small
    monochrome images of clothing *might* have been possible with traditional coding,
    it’s clear that this example is far more difficult and that you are crossing the
    line into where ML is essential to solve a problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然之前的预测*y* = 2*x* – 1或对小型单色服装图像的分类可能通过传统编码实现，但很明显，这个例子要困难得多，而且你已经跨过了机器学习成为解决问题必需品的界限。
- en: An interesting side note is that these images are all computer generated. The
    theory is that features spotted in a CGI image of a horse should apply to a real
    image, and you’ll see how well this works later in this chapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的旁注是，这些图像都是计算机生成的。理论是，在CGI马图像中发现的特征应该适用于真实图像，你将在本章后面看到这是如何工作的。
- en: Handling the Data
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据
- en: The Fashion MNIST dataset that you’ve been using up to this point comes with
    labels, and every image file has an associated file with the label details. Many
    image-based datasets do not have this, and “Horses or Humans” is no exception.
    Instead of labels, the images are sorted into subdirectories of each type, and
    with the DataLoader in PyTorch, you can use this structure to *automatically*
    assign labels to images.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止你一直在使用的Fashion MNIST数据集包含标签，并且每个图像文件都有一个与之关联的标签详细信息文件。许多基于图像的数据集没有这个，"马或人"也不例外。没有标签，图像被分类到每个类型的子目录中，而在PyTorch的DataLoader中，你可以使用这种结构来*自动*为图像分配标签。
- en: First, you simply need to ensure that your directory structure has a set of
    named subdirectories, with each subdirectory being a label. For example, the “Horses
    or Humans” dataset is available as a set of ZIP files, one of which contains the
    training data (1,000+ images) and another of which contains the validation data
    (256 images). When you download and unpack them into a local directory for training
    and validation, you need to ensure that they are in a file structure like the
    one in [Figure 3-8](#ch03_figure_8_1748570891060128).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要确保你的目录结构有一组命名子目录，每个子目录都是一个标签。例如，“马或人”数据集作为一组ZIP文件提供，其中一个包含训练数据（1,000+图像），另一个包含验证数据（256图像）。当你下载并解压缩到本地目录用于训练和验证时，你需要确保它们具有如图[图3-8](#ch03_figure_8_1748570891060128)所示的文件结构。
- en: 'Here’s the code to get the training data and extract it into the appropriately
    named subdirectories, as shown in [Figure 3-8](#ch03_figure_8_1748570891060128):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是获取训练数据并将其提取到适当命名的子目录中的代码，如图[图3-8](#ch03_figure_8_1748570891060128)所示：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](assets/aiml_0308.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0308.png)'
- en: Figure 3-8\. Ensuring that images are in named subdirectories
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8\. 确保图像在命名子目录中
- en: This code simply downloads the ZIP of the training data and unzips it into a
    directory at *horse-or-human/training*. (We’ll deal with downloading the validation
    data shortly.) This is the parent directory that will contain subdirectories for
    the image types.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码仅下载训练数据的ZIP文件，并将其解压缩到*horse-or-human/training*目录中。（我们很快就会处理验证数据的下载。）这是包含图像类型子目录的父目录。
- en: 'Now, to use the `DataLoader`, we simply use the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要使用`DataLoader`，我们只需使用以下代码：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: First, we create an instance of a `transforms` object that we’ll call `transform`.
    This will determine the rules for how we modify the images. It resizes the image
    to 150 × 150 and then normalizes it into a tensor. Note that the raw images are
    actually 300 × 300, but to make training quicker for the purposes of learning,
    I’ve resized them to 150 × 150.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个名为`transform`的`transforms`对象实例。这将确定我们修改图像的规则。它将图像调整大小到150 × 150，然后将其归一化到一个张量中。请注意，原始图像实际上是300
    × 300，但为了学习目的加快训练速度，我已经将它们调整大小到150 × 150。
- en: Then, we specify the `dataset` objects to be `datasets.ImageFolder` types and
    point them to the required directory, and that will generate images for the training
    process by flowing them from that directory while applying the transform. The
    directory for training is `training_dir`, and the directory for validation is
    `validation_dir`, as specified earlier.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们指定`dataset`对象为`datasets.ImageFolder`类型，并将它们指向所需的目录，这样就会在应用转换的同时从该目录生成用于训练过程的图像。训练目录是`training_dir`，验证目录是`validation_dir`，如前所述。
- en: CNN Architecture for “Horses or Humans”
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “马或人”的CNN架构
- en: 'There are several major differences between this dataset and the Fashion MNIST
    one, and you have to take them into account when designing an architecture for
    classifying the images. First, the images are much larger—150 × 150 pixels—so
    more layers may be needed. Second, the images are in full color, not grayscale,
    so each image will have three channels instead of one. Third, there are only two
    image types, so we can actually classify them with only *one* output neuron. To
    do this, we’ll drive the value of that neuron toward 0 for one of the labels and
    toward 1 for the other. The `sigmoid` function is ideal for this process of driving
    the value to one of these extremes. You can see this at the bottom of the `forward`
    function:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与Fashion MNIST数据集相比，这个数据集有几个主要的不同之处，在设计用于分类图像的架构时，你必须考虑到它们。首先，图像要大得多——150 ×
    150像素，所以可能需要更多的层。其次，图像是全彩色的，而不是灰度的，所以每个图像将有三通道而不是一个。第三，只有两种图像类型，因此我们可以实际上只用*一个*输出神经元来对它们进行分类。为此，我们将该神经元的值驱动到其中一个标签的0，并驱动到另一个标签的1。`sigmoid`函数非常适合这个过程，可以将值驱动到这两个极端之一。你可以在`forward`函数的底部看到这一点：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There are a number of things to note here. First of all, take a look at the
    very first layer. We’re defining 16 filters, each of which has a `kernel_size`
    of 3, but the input shape is 3\. Remember that this is because our input image
    is in color: there are three channels, instead of just one for the monochrome
    Fashion MNIST dataset we were using earlier.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个需要注意的地方。首先，看看最顶层。我们定义了16个过滤器，每个过滤器的`kernel_size`为3，但输入形状是3。记住这是因为我们的输入图像是彩色的：有三个通道，而不是我们之前使用的单色Fashion
    MNIST数据集的一个通道。
- en: At the other end, notice that there’s only one neuron in the output layer. This
    is because we’re using a binary classifier, and we can get a binary classification
    with just a single neuron if we activate it with a sigmoid function. The purpose
    of the sigmoid function is to drive one set of values toward 0 and the other toward
    1, which is perfect for binary classification.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一端，注意输出层中只有一个神经元。这是因为我们使用的是二元分类器，如果我们用sigmoid函数激活它，我们就可以只用一个神经元得到二元分类。sigmoid函数的目的是将一组值驱动到0，另一组驱动到1，这对于二元分类来说非常完美。
- en: 'Next, notice how we stack several more convolutional layers. We do this because
    our image source is quite large and we want, over time, to have many smaller images,
    each with features highlighted. If we take a look at the results of a `summary`,
    we’ll see this in action:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，注意我们如何堆叠更多的卷积层。我们这样做是因为我们的图像源相当大，我们希望随着时间的推移，拥有许多较小的图像，每个图像都突出显示了一些特征。如果我们查看`summary`的结果，我们会看到这一点：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that by the time the data has gone through all the convolutional and pooling
    layers, it ends up as 18 × 18 items. The theory is that these will be activated
    feature maps that are relatively simple because they will contain just 324 pixels.
    We can then pass these feature maps to the dense neural network to match them
    to the appropriate labels.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当数据通过所有卷积和池化层后，最终会变成18 × 18个项。理论上是这样的，这些将是相对简单的激活特征图，因为它们将只包含324个像素。然后我们可以将这些特征图传递给密集神经网络，以匹配适当的标签。
- en: This, of course, leads this network to have many more parameters than the previous
    network, so it will be slower to train. With this architecture, we’re going to
    learn over 10 million parameters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这使得这个网络比之前的网络有更多的参数，所以训练会慢一些。使用这种架构，我们将学习超过1000万个参数。
- en: Tip
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The code in this section, as well as in many other places in this book, may
    require you to import Python libraries. To find the correct imports, you can check
    out [the book’s repository](https://github.com/lmoroney/PyTorch-Book-FIles).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分的代码，以及本书中的许多其他地方，可能需要你导入Python库。要找到正确的导入，你可以查看[本书的仓库](https://github.com/lmoroney/PyTorch-Book-FIles)。
- en: 'To train the network, we’ll have to compile it with a loss function and an
    optimizer. In this case, the loss function can be the `BCELoss,` where `BCE` stands
    for *binary cross entropy*. As the name suggests, because there are only two classes
    in this scenario, this is a loss function that is designed for it. For the `optimizer`,
    we can continue using the same `Adam` that we used earlier. Here’s the code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练网络，我们需要使用损失函数和优化器来编译它。在这种情况下，损失函数可以是`BCELoss`，其中`BCE`代表*二元交叉熵*。正如其名所示，因为在这个场景中只有两个类别，这是一个为此设计的损失函数。对于`optimizer`，我们可以继续使用之前使用的相同的`Adam`。以下是代码：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then train in the usual way:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后以通常的方式进行训练：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: One thing to note is that the labels are converted to `floats` because of the
    binary cross entropy, where the value of the final output node will be a float
    value.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，由于二元交叉熵，标签被转换为`floats`，最终输出节点的值将是一个浮点值。
- en: Over just 15 epochs, this architecture gives us a very impressive 95%+ accuracy
    on the training set. Of course, this is just with the training data, and this
    performance isn’t an indication of the network’s potential performance on data
    that it hasn’t previously seen.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅仅15个epoch之后，这个架构在训练集上就给我们带来了非常令人印象深刻的95%以上的准确率。当然，这仅仅是基于训练数据，这种性能并不能表明网络在之前未见过数据上的潜在性能。
- en: Next, we’ll look at adding the validation set and measuring its performance
    to give us a good indication of how this model might perform in real life.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看添加验证集并衡量其性能，以给我们一个很好的指示，了解这个模型在现实生活中的表现可能如何。
- en: Adding Validation to the “Horses or Humans” Dataset
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将验证添加到“马或人”数据集中
- en: To add validation, you’ll need a validation dataset that’s separate from the
    training one. In some cases, you’ll get a master dataset that you have to split
    yourself, but in the case of “Horses or Humans,” there’s a separate validation
    set that you can download. In the preceding code snippet, you’ve already downloaded
    the training and validation datasets, put them in directories, and set up data
    loaders for each of them. However, for training, you only used one of these datasets—the
    one that was set up to load the training data. So next, we’ll switch the model
    into evaluation mode and explore how well it did with the validation data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加验证，你需要一个与训练集分开的验证数据集。在某些情况下，你会得到一个主数据集，你需要自己将其分割，但在“马或人”的情况下，有一个单独的验证集可以下载。在前面的代码片段中，你已经下载了训练和验证数据集，将它们放入目录中，并为每个数据集设置了数据加载器。然而，对于训练，你只使用了这些数据集中的一个——即设置为加载训练数据的那个。所以接下来，我们将模型切换到评估模式，并探索它在验证数据上的表现如何。
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You may be wondering why we’re talking about a validation dataset here, rather
    than a test dataset, and whether the two are the same thing. For simple models
    like the ones developed in the previous chapters, it’s often sufficient to split
    the dataset into two parts: one for training and one for testing. But for more
    complex models like the one we’re building here, you’ll want to create separate
    validation and test sets.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们在这里讨论的是验证数据集，而不是测试数据集，以及这两个是否是同一件事。对于像前几章中开发的简单模型，通常将数据集分成两部分：一部分用于训练，另一部分用于测试，就足够了。但对我们正在构建的更复杂模型，你将需要创建单独的验证和测试集。
- en: What’s the difference? *Training data* is the data that is used to teach the
    network how the data and labels fit together, while *validation data* is used
    to see how the network is doing with previously unseen data *while* you are training
    (i.e., it isn’t used to fit data to labels but to inspect how well the fitting
    is going). Also, *test data* is used after training to evaluate how the network
    does with data it has never previously seen. Some datasets come with a three-way
    split, and in other cases, you’ll want to separate the test set into two parts
    for validation and testing. Here, you’ll download some additional images for testing
    the model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 差别在哪里？*训练数据*是用于教会网络数据和标签如何匹配在一起的数据，而*验证数据*是在训练过程中用于查看网络在之前未见过的数据上的表现（即，它不是用于将数据拟合到标签，而是用于检查拟合进行得如何）。此外，*测试数据*是在训练后用于评估网络在从未见过数据上的表现。一些数据集带有三方分割，而在其他情况下，你可能希望将测试集分割成两部分用于验证和测试。在这里，你将下载一些额外的图像来测试模型。
- en: To download the validation set and unzip it into a different directory, you
    can use code that’s very similar to that used for the training images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载验证集并将其解压缩到不同的目录中，你可以使用与用于训练图像相似的代码。
- en: 'Then, to perform the validation, you simply update your `train_model` method
    to perform a validation at the end of each training loop (or epoch) and report
    on the results. For example, you can do this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了进行验证，你只需更新你的`train_model`方法，在每个训练循环（或epoch）结束时执行验证，并报告结果。例如，你可以这样做：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: I added code here to do *both* the training and the validation and report on
    accuracy. Note that this is really just for learning purposes, so you can compare.
    In a real-world scenario, checking the accuracy of training data is a waste of
    processing time!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里添加了代码来同时进行训练和验证，并报告准确率。请注意，这只是为了学习目的，所以你可以进行比较。在现实世界的场景中，检查训练数据的准确率是浪费处理时间！
- en: 'After training for 10 epochs, you should see that your model is 99%+ accurate
    on the training set but only about 88% on the validation set:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10个epoch的训练后，你应该看到你的模型在训练集上的准确率达到了99%以上，但在验证集上只有大约88%：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is an indication that the model is overfitting, which is something we also
    saw in the previous chapter.It’s easy to be lulled into a false sense of security
    by the 100% accuracy, but the other figure is more representative of how your
    model will behave in the real world.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明模型正在过拟合，这在上一章我们也看到了。100%的准确率很容易让人产生一种虚假的安全感，但其他数据更能代表你的模型在现实世界中的表现。
- en: Still, the performance isn’t bad, considering how few images it was trained
    on and how diverse those images were. You’re beginning to hit a wall caused by
    lack of data, but there are some techniques that you can use to improve your model’s
    performance. We’ll explore them later in this chapter, but before that, let’s
    take a look at how to *use* this model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到它训练的图片数量很少，以及这些图片的多样性，性能还算不错。你开始遇到由数据不足造成的瓶颈，但有一些技术可以帮助你提高模型的表现。我们将在本章后面探讨这些技术，但在那之前，让我们看看如何*使用*这个模型。
- en: Testing “Horses or Humans” Images
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试“马或人类”图片
- en: It’s all very well and good to be able to build a model, but of course, you
    want to try it out. A major frustration of mine when I was starting my AI journey
    was that I could find lots of code that showed me how to build models and charts
    of how those models were performing, but very rarely was there code to help me
    kick the tires of the model myself to try it out. I’ll try to help you avoid that
    problem in this book!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 能够构建模型固然很好，但你当然想尝试一下。在我开始我的AI之旅时，我最大的挫折是能够找到很多代码，它们向我展示了如何构建模型和那些模型的表现图表，但很少能找到代码帮助我自己尝试模型。在这本书中，我会尽力帮助你避免这个问题！
- en: Testing the model is perhaps easiest using Colab. I’ve provided a “Horses or
    Humans” notebook on GitHub that you can open directly in [Colab](http://bit.ly/horsehuman).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Colab测试模型可能是最简单的方法。我在GitHub上提供了一个“马或人类”笔记本，你可以直接在[Colab](http://bit.ly/horsehuman)中打开。
- en: Once you’ve trained the model, you’ll see a section called “Running the Model.”
    Before running it, you should find a few pictures of horses or humans online and
    download them to your computer. I recommend you go to [Pixabay.com](http://pixabay.com),
    which is a really good site to check out for royalty-free images. It’s also a
    good idea to get your test images together first, because the node can time out
    while you’re searching.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练了模型，你将看到一个名为“运行模型”的部分。在运行之前，你应该在网上找到一些马或人类的图片并将它们下载到你的电脑上。我建议你访问[Pixabay.com](http://pixabay.com)，这是一个寻找免费版权图片的好网站。在搜索时，节点可能会超时，所以先收集好测试图片是个好主意。
- en: '[Figure 3-9](#ch03_figure_9_1748570891060144) shows a few pictures of horses
    and humans that I downloaded from Pixabay to test the model.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-9](#ch03_figure_9_1748570891060144)展示了我从Pixabay下载的一些马和人类的图片，以测试模型。'
- en: '![](assets/aiml_0309.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0309.png)'
- en: Figure 3-9\. Test images
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. 测试图片
- en: When they were uploaded, as you can see in [Figure 3-10](#ch03_figure_10_1748570891060159),
    the model correctly classified one image as a human and another as a horse—but
    despite the fact that the third image was obviously of a human, the model incorrectly
    classified it as a horse!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当它们被上传时，正如你在[图3-10](#ch03_figure_10_1748570891060159)中可以看到的，模型正确地将一张图片分类为人类，另一张分类为马——但尽管第三张图片显然是人类的，模型却错误地将其分类为马！
- en: You can also upload multiple images simultaneously and have the model make predictions
    for all of them. You may also notice that it tends to overfit toward horses. If
    the human isn’t fully posed (i.e., if you can’t see their full body), the model
    can skew toward horses. That’s what happened in this case. The first human model
    is fully posed, and the image resembles many of the poses in the dataset, so the
    model was able to classify her correctly. On the other hand, the second human
    model is facing the camera, but only her upper half is in the image. There was
    no training data that looked like that, so the model couldn’t correctly identify
    her.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以同时上传多张图片，让模型对它们所有进行预测。你可能也会注意到它倾向于过拟合到马。如果人类没有完全摆好姿势（即，如果你看不到他们的全身），模型可能会偏向于马。这就是这个案例发生的情况。第一个人类模型是全摆姿势的，图像与数据集中的许多姿势相似，因此模型能够正确地分类她。另一方面，第二个人类模型面向相机，但图像中只有她的上半身。没有这样的训练数据，所以模型无法正确地识别她。
- en: '![](assets/aiml_0310.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0310.png)'
- en: Figure 3-10\. Executing the model
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. 执行模型
- en: 'Let’s now explore the code to see what it’s doing. Perhaps the most important
    part is this chunk:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探索代码，看看它在做什么。也许最重要的部分是这一段：
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we are loading the image from the path that Colab wrote it to. Note that
    we specify a `transform` to apply to the image. The images being uploaded can
    be any shape, but if we are going to feed them into the model, they *must* be
    the same size that the model was trained on. So, if we use the same `transform`
    that we defined when performing the training, we’ll know it’s in the same dimensions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在从Colab写入图像的路径中加载图像。请注意，我们指定了一个要应用于图像的`transform`。上传的图像可以是任何形状，但如果我们打算将它们输入到模型中，它们*必须*与模型训练时的大小相同。所以，如果我们使用在训练时定义的相同的`transform`，我们就知道它在相同的维度上。
- en: 'At the end is this strange command: `image = image.unsqueeze(0)`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是这条奇怪的命令：`image = image.unsqueeze(0)`。
- en: When you look back at how the model was trained, the DataLoader objects batched
    the images going into it. If you think of an image as a 2D array of pixels, then
    the batch is an array of 2D arrays, which of course is then a 3D array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当你回顾模型是如何训练的时候，DataLoader对象将输入的图像分批处理。如果你把图像想象成一个像素的二维数组，那么批次就是一个二维数组的数组，这当然就是一个三维数组。
- en: But when we’re using this code with one image at a time, there’s no batch, so
    to make this a 3D array (which is technically a batch with one item in it), we
    can just unsqueeze the image along axis 0 to simulate this.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们一次使用一张图像时，没有批次，所以为了使它成为一个三维数组（技术上是一个包含一个项目的批次），我们只需沿着轴0对图像进行unsqueeze来模拟这一点。
- en: 'With our image in the right format, it’s easy to do the classification:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的图像格式正确时，进行分类变得很容易：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The model then returns an array containing the classifications for the batch.
    Because there’s only one classification in this case, it’s effectively an array
    containing an array. You can see this back in [Figure 3-10](#ch03_figure_10_1748570891060159),
    where for the first human model, the array looks like `tensor([[2.1368e-05]],
    device='cuda:0').`
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型随后返回一个包含批次分类的数组。因为在这种情况下只有一个分类，所以它实际上是一个包含数组的数组。你可以在[图3-10](#ch03_figure_10_1748570891060159)中看到这一点，其中第一个人类模型的数组看起来像`tensor([[2.1368e-05]],
    device='cuda:0')`。
- en: 'So now, it’s simply a matter of inspecting the value of the first element in
    that array. If it’s greater than 0.5, we’re looking at a human:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在，我们只需检查那个数组中第一个元素的值。如果它大于0.5，我们就在看一个人类：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are a few important points to consider here. First, even though the network
    was trained on synthetic, computer-generated imagery, it performs quite well at
    spotting horses and humans and differentiating them in real photographs. This
    is a potential boon in that you may not need thousands of photographs to train
    a model, and you can do it relatively cheaply with CGI.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里有几个重要的点需要考虑。首先，尽管网络是在合成、计算机生成的图像上训练的，但它非常擅长在真实照片中识别马和人类以及区分它们。这是一个潜在的优点，你可能不需要成千上万的照片来训练模型，你可以用CGI相对便宜地完成它。
- en: But this dataset also demonstrates a fundamental issue you will face. Your training
    set cannot hope to represent *every* possible scenario your model might face in
    the wild, and thus, the model will always have some level of overspecialization
    toward the training set. We saw a clear and simple example of this earlier in
    this section, when the model mischaracterized the human in the center of [Figure 3-9](#ch03_figure_9_1748570891060144).
    The training set didn’t include a human in that pose, and thus, the model didn’t
    “learn” that a human could look like that. As a result, there was every chance
    it might see the figure as a horse, and in this case, it did.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个数据集也展示了一个你将面临的基本问题。你的训练集无法代表模型在野外可能遇到的*每一种*可能的情况，因此，模型将始终对训练集有一定的过度专业化。我们在这个部分前面已经看到了一个清晰且简单的例子，当模型错误地将[图3-9](#ch03_figure_9_1748570891060144)中心的人类描述为其他事物时。训练集没有包括那种姿势的人类，因此，模型没有“学习”到人类可以看起来那样。结果，它有可能将这个图像误认为是马，在这种情况下，它确实是这样做的。
- en: What’s the solution? The obvious one is to add more training data, with humans
    in that particular pose and others that weren’t initially represented. That isn’t
    always possible, though. Fortunately, there’s a neat trick in PyTorch that you
    can use to virtually extend your dataset—it’s called *image augmentation*, and
    we’ll explore that next.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是什么？显然的一个方法是添加更多的训练数据，包括特定姿势的人类和其他最初未表示的人类。但这并不总是可能的。幸运的是，PyTorch中有一个巧妙的技巧可以用来虚拟扩展你的数据集——它被称为*图像增强*，我们将在下一节中探讨。
- en: Image Augmentation
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像增强
- en: In the previous section, you built a horse-or-human classifier model that was
    trained on a relatively small dataset. As a result, you soon began to hit problems
    classifying some previously unseen images, such as the miscategorization of a
    woman as a horse because the training set didn’t include any images of people
    in that pose.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你构建了一个在相对较小的数据集上训练的“马或人”分类器模型。因此，你很快就开始遇到分类一些之前未见过的图像的问题，例如，由于训练集没有包含任何该姿势的人的图像，将女士误分类为马。
- en: One way to deal with such problems is with *image augmentation*. The idea behind
    this technique is that as PyTorch is loading your data, it can create additional
    new data by amending what it has using a number of transforms. For example, take
    a look at [Figure 3-11](#fig-3-11). While there is nothing in the dataset that
    looks like the woman on the right, the image on the left is somewhat similar.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 处理此类问题的一种方法是通过*图像增强*。这种技术的理念是，当PyTorch加载数据时，它可以通过使用多个转换来修改现有数据，从而创建额外的新的数据。例如，看看[图3-11](#fig-3-11)。虽然数据集中没有像右侧的女士那样的东西，但左侧的图像在某种程度上是相似的。
- en: '![](assets/aiml_0311.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0311.png)'
- en: Figure 3-11\. Dataset similarities
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-11. 数据集相似性
- en: So, if you could, for example, zoom into the image on the left as you are training,
    as shown in [Figure 3-12](#fig-3-12), you would increase the chances of the model
    being able to correctly classify the image on the right as a person.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你在训练时能够放大左侧的图像，就像[图3-12](#fig-3-12)中所示，你将增加模型正确分类右侧图像为人的机会。
- en: '![](assets/aiml_0312.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0312.png)'
- en: Figure 3-12\. Zooming in on the training set data
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12. 放大训练集数据
- en: 'In a similar way, you can broaden the training set with a variety of other
    transformations, including the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，你可以通过多种其他转换来扩展训练集，包括以下内容：
- en: Rotation (turning the image)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转（旋转图像）
- en: Shifting horizontally (moving the pixels horizontally with wrapping)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平偏移（将像素水平移动并循环）
- en: Shifting vertically (moving the pixels vertically with wrapping)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直偏移（将像素垂直移动并循环）
- en: Shearing (moving the pixels either horizontally or vertically but offsetting
    so that the image would look like parallelogram)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉伸（将像素水平或垂直移动，但偏移以使图像看起来像平行四边形）
- en: Zooming (magnifying a particular region)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放（放大特定区域）
- en: Flipping (vertically or horizontally)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻转（垂直或水平）
- en: 'Because you’ve been using the `datasets.ImageFolder` and a `DataLoader` to
    load the images, you’ve seen the model do a transform already—when it normalized
    the images like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你一直在使用`datasets.ImageFolder`和`DataLoader`来加载图像，你已经看到模型已经执行了一个转换——当它像这样归一化图像时：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Many other transforms are easily available within the torchvision.transforms
    library, so, for example, you could do something like this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: torchvision.transforms库中提供了许多其他转换，例如，你可以做如下操作：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here, in addition to rescaling the image to normalize it, you’re doing the
    following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，除了将图像缩放以进行归一化之外，你还在做以下事情：
- en: Randomly flipping horizontally
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机水平翻转
- en: Randomly rotating up to 20 degrees left or right
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机旋转最多20度左或右
- en: Randomly cropping a 150 × 150 window instead of resizing
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机裁剪一个150 × 150的窗口而不是调整大小
- en: 'In addition, the transforms.RandomAffine library gives you the facility to
    do all of these things, as well as adding stuff like scaling the image (zooming
    in or out), shearing the image, etc. Here’s an example:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，transforms.RandomAffine库为你提供了执行所有这些操作的功能，以及添加诸如缩放图像（放大或缩小）、拉伸图像等操作。以下是一个示例：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When you retrain with these parameters, one of the first things you’ll notice
    is that training takes longer because of all the image processing. Also, your
    model’s accuracy may not be as high as it was, because previously it was overfitting
    to a largely uniform set of data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用这些参数重新训练时，你首先会注意到训练时间变长了，因为所有这些图像处理。此外，你的模型精度可能不如之前，因为之前它过度拟合了一个大体上均匀的数据集。
- en: In my case, when I was training with these augmentations, my accuracy went down
    from 99% to 94% after 15 epochs, with validation much lower at 64%. This likely
    indicates overfitting in the model, but it warrants investigation by training
    with more epochs! One other thing to note is that random cropping might also be
    an issue—the CGI images generally center the subject, so random cropping will
    give partial subjects.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，当我使用这些增强进行训练时，我的准确率在15个epoch后从99%下降到94%，验证准确率更低，为64%。这很可能表明模型过拟合，但需要通过更多epoch的训练来调查！还有一点需要注意，随机裁剪可能也是一个问题——CGI图像通常将主题居中，因此随机裁剪会给出部分主题。
- en: But what about the image from [Figure 3-9](#ch03_figure_9_1748570891060144)
    that the model misclassified earlier? This time, the model gets it right. Thanks
    to the image augmentations, the training set now has sufficient coverage for the
    model to understand that this particular image is a human too (see [Figure 3-13](#ch03_figure_11_1748570891060175)).
    This is just a single data point, and it may not be representative of the results
    for real data, but it’s a small step in the right direction.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 但关于模型之前错误分类的[图3-9](#ch03_figure_9_1748570891060144)中的图像怎么办？这次，模型分类正确了。多亏了图像增强，训练集现在对模型来说覆盖范围足够，能够理解这张特定的图像也是一个人（参见[图3-13](#ch03_figure_11_1748570891060175)）。这只是一个单一的数据点，可能并不代表真实数据的分类结果，但这是朝着正确方向迈出的一小步。
- en: '![](assets/aiml_0313.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0313.png)'
- en: Figure 3-13\. The woman is now correctly classified
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13。这位女士现在被正确分类了
- en: As you can see, even with a relatively small dataset like “Horses or Humans,”
    you can start to build a pretty decent classifier. With larger datasets, you could
    take this further. Another way you can improve the model is by using features
    that the model has already learned elsewhere. Many researchers with massive resources
    (millions of images) and huge models that have been trained on thousands of classes
    have shared their models, and by using a concept called *transfer learning*, you
    can use the features those models learned and apply them to your data. We’ll explore
    that next!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，即使像“马或人”这样的相对较小的数据集，你也能开始构建一个相当不错的分类器。有了更大的数据集，你可以做得更多。另一种改进模型的方法是使用模型已经在其他地方学习到的特征。许多拥有大量资源（数百万张图片）和经过数千个类别训练的巨大模型的科研人员已经分享了他们的模型，通过使用一个称为**迁移学习**的概念，你可以使用这些模型学习到的特征并将它们应用到你的数据上。我们将在下一节中探讨这一点！
- en: Transfer Learning
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: As we’ve already seen in this chapter, the use of convolutions to extract features
    can be a powerful tool for identifying the contents of an image. If we use this
    tool, we can then feed the resulting feature maps into the dense layers of a neural
    network to match them to the labels and give us a more accurate way of determining
    the contents of an image. Using this approach with a simple fast-to-train neural
    network and some image augmentation techniques, we built a model that was 80–90%
    accurate at distinguishing between a horse and a human when it was trained on
    a very small dataset.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中已经看到的，使用卷积来提取特征可以是一个强大的工具，用于识别图像的内容。如果我们使用这个工具，然后我们可以将得到的特征图输入到神经网络的密集层中，将它们与标签匹配，并给我们一个更准确的方法来确定图像的内容。通过使用这种方法，结合一个简单快速训练的神经网络和一些图像增强技术，我们构建了一个模型，当在非常小的数据集上训练时，在区分马和人方面准确率达到80-90%。
- en: 'However, we can improve our model even further by using a method called *transfer
    learning*. The idea behind it is simple: instead of having our model learn a set
    of filters from scratch for our dataset, why not have it use a set of filters
    that were learned on a much larger dataset, with many more features than we can
    “afford” to build from scratch? We can place these filters in our network and
    then train a model with our data using the pre-learned filters. For example, while
    our “Horses or Humans” dataset has only two classes, we can use an existing model
    that has been pretrained for one thousand classes—but at some point, we’ll have
    to throw away some of the preexisting network and add the layers that will let
    us have a classifier for two classes.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以通过使用一种称为**迁移学习**的方法来进一步改进我们的模型。其背后的理念很简单：不是让我们的模型从头开始为我们数据集学习一组过滤器，为什么不使用在更大数据集上学习的一组过滤器，这些数据集具有比我们“负担得起”从头开始构建的更多特征？我们可以将这些过滤器放入我们的网络中，然后使用预学习的过滤器用我们的数据进行模型训练。例如，虽然我们的“马或人”数据集只有两个类别，但我们可以使用一个为1000个类别预训练的现有模型——但最终，我们不得不丢弃一些预存在的网络并添加允许我们有两个类别分类器的层。
- en: '[Figure 3-14](#ch03_figure_12_1748570891060190) shows what a CNN architecture
    for a classification task like ours might look like. We have a series of convolutional
    layers that lead to a dense layer, which in turn leads to an output layer.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-14](#ch03_figure_12_1748570891060190)展示了我们这样的分类任务可能看起来像的CNN架构。我们有一系列卷积层，它们通向一个密集层，然后又通向输出层。'
- en: '![](assets/aiml_0314.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0314.png)'
- en: Figure 3-14\. A CNN architecture
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14\. CNN架构
- en: We’ve seen that we can build a pretty good classifier using this architecture.
    But what if we could use transfer learning to take the pre-learned layers from
    another model, freeze or lock them so that they aren’t trainable, and then put
    them on top of our model, like in [Figure 3-15](#ch03_figure_13_1748570891060207)?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，我们可以使用这种架构构建一个相当好的分类器。但如果我们可以使用迁移学习来从另一个模型中提取预训练的层，冻结或锁定它们以便它们不可训练，然后将它们放在我们的模型之上，就像[图3-15](#ch03_figure_13_1748570891060207)中那样呢？
- en: '![](assets/aiml_0315.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0315.png)'
- en: Figure 3-15\. Taking and locking layers from another architecture via transfer
    learning
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15\. 通过迁移学习从另一个架构中提取和锁定层
- en: When we consider that once they’ve been trained, all these layers are just a
    set of numbers indicating the filter values, weights, and biases along with a
    known architecture (the number of filters per layer, the size of the filter, etc.),
    the idea of reusing them is pretty straightforward.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到一旦训练完成，所有这些层都只是一组表示滤波器值、权重和偏置的数字，以及一个已知的架构（每层的滤波器数量、滤波器大小等）时，重新使用它们的想法相当直接。
- en: Let’s look at how this would appear in code. There are several pretrained models
    already available from a variety of sources, so we’ll use version 3 of the popular
    Inception model from Google, which is trained on more than a million images from
    a database called ImageNet. Inception has dozens of layers, and it can classify
    images into one thousand categories.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在代码中会是什么样子。已经有来自各种来源的几个预训练模型可用，所以我们将使用Google流行的Inception模型的第3版，它在ImageNet数据库中的超过一百万张图片上进行了训练。Inception有数十层，可以将图像分类为一千个类别。
- en: 'The torchvision.models library contains a number of models, including Inception
    V3, so we can easily get access to the pretrained model:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: torchvision.models库包含了许多模型，包括Inception V3，因此我们可以轻松地访问预训练模型：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we have a full Inception model that’s pretrained. If you want to inspect
    its architecture, you can do so with this code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个完全预训练的Inception模型。如果你想检查其架构，可以使用以下代码：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Be warned—this model is huge! Still, you should take a look through it to see
    the layers and their names. I like to use the one called `Mixed7_c` because its
    output is nice and small—it consists of 8 × 8 images—but you should feel free
    to experiment with others.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 警告——这个模型非常大！尽管如此，你应该浏览一下它，看看层和它们的名称。我喜欢使用名为`Mixed7_c`的层，因为它的输出既好又小——它由8×8的图像组成——但你当然可以自由地尝试其他层。
- en: 'Next, we’ll freeze the entire network from retraining and then set a variable
    to point to `mixed7`’s output as where we want to crop the network. We can do
    that with this code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将冻结整个网络以防止重新训练，并设置一个变量以指向`mixed7`的输出，这是我们想要裁剪网络的地方。我们可以用以下代码做到这一点：
- en: '[PRE23]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You’ll notice that we’re printing the output shape of the last layer, and you’ll
    also see that we’re getting 8 × 8 images at this point. This indicates that by
    the time the images have been fed through to `Mixed_7c`, the output images from
    the filters are 8 × 8 in size, so they’re pretty easy to manage. Again, you don’t
    have to choose that specific layer; you’re welcome to experiment with others.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们在打印最后一层的输出形状，你也会看到我们此时正在得到8×8的图像。这表明当图像被输入到`Mixed_7c`时，滤波器的输出图像大小为8×8，因此它们相当容易管理。再次强调，你不必选择那个特定的层；你可以自由地尝试其他层。
- en: Now, let’s see how to modify the model for transfer learning. It’s pretty straightforward—if
    you go back to the output from the custom `print_model_summary` from a moment
    ago, you’ll see that the *last* layer in the model is called `fc`. As you might
    expect, *fc* stands for *fully connected*, which is effectively a Linear layer
    with our densely connected neurons.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何修改模型以进行迁移学习。这相当直接——如果你回到刚才的`print_model_summary`的输出，你会看到模型中的**最后一层**被称作`fc`。正如你所预期的那样，`fc`代表**全连接**，这实际上是一个具有密集连接神经元的线性层。
- en: 'So now, it becomes as simple as replacing that layer with a new layer called
    `fc`. We don’t need to *know* the input shape for it ahead of time—we can inspect
    its `in_features` property to find that. So now, to create a new layer of 1,024
    neurons that outputs to another layer of two neurons and replace the `fc` from
    Inception, all we have to do is this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这变得简单了，只需用一个新的层`fc`替换那个层。我们不需要提前知道它的输入形状——我们可以检查它的`in_features`属性来找到它。因此，现在要创建一个输出到另一个两个神经元层的1024个神经元的新的层，并替换Inception中的`fc`，我们只需做以下操作：
- en: '[PRE24]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It’s as simple as creating a new set of Linear layers from the last output,
    because we’ll be feeding the results into a dense layer. So, we then add a Linear
    layer of 1,024 neurons and a dense layer with two neurons for our output. Also,
    you’ve probably noticed that in the previous model, we did it with one neuron
    and used sigmoid activation for the two classes—so you’re probably wondering why
    we’re going to two neurons in the output layer now. This was primarily a stylistic
    choice. Inception was designed for *n* neurons to output for *n* classes, and
    I wanted to keep that approach.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像从最后一个输出创建一组新的线性层一样简单，因为我们将会把结果喂入一个密集层。因此，我们添加了一个有1024个神经元的线性层和一个有两个神经元的密集层作为输出。此外，你可能已经注意到在之前的模型中，我们只用了一个神经元，并为两个类别使用了sigmoid激活函数——所以你可能想知道为什么现在输出层要使用两个神经元。这主要是一个风格选择。Inception是为输出*n*个神经元到*n*个类别而设计的，我想保持那种方法。
- en: Training the model on this architecture over only three epochs gave us an accuracy
    of 99%+, with a validation accuracy of 95%+. Clearly, that’s a vast improvement.
    Also, remember that Inception learned a massive set of features that it could
    use to classify the many classes it was trained on. It turns out that that feature
    set is also incredibly useful for learning how to classify any other images—not
    least, those from “Horses or Humans.”
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构上仅用三个epoch训练模型，我们得到了99%+的准确率，验证准确率达到了95%+。显然，这是一个巨大的提升。此外，请记住，Inception学习到了一个庞大的特征集，它可以用来对训练的多个类别进行分类。结果证明，这个特征集对于学习如何对任何其他图像进行分类也非常有用——尤其是那些来自“马或人”的图像。
- en: The results we got from this model are much better than those we got from our
    previous model, but you can continue to tweak and improve it. You can also explore
    how the model will work with a much larger dataset, like the famous “[Dogs vs.
    Cats”](https://oreil.ly/UhWMk) from Kaggle. It’s an extremely varied dataset consisting
    of 25,000 images of cats and dogs, often with the subjects somewhat obscured—for
    example, if they are held by a human.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个模型得到的结果比我们从之前的模型得到的结果要好得多，但你可以继续调整和改进它。你还可以探索模型在更大的数据集上如何工作，比如来自Kaggle的著名数据集“[狗与猫”](https://oreil.ly/UhWMk)。这是一个极其多样化的数据集，包含25000张猫和狗的图片，通常主题有些模糊——例如，如果它们被人类拿着。
- en: Using the same algorithm and model design as before, you can train a “Dogs vs.
    Cats” classifier on Colab, using a GPU at about 3 minutes per epoch.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前相同的算法和模型设计，你可以在Colab上训练一个“狗与猫”分类器，使用GPU大约每个epoch需要3分钟。
- en: When I tested with very complex pictures like those in [Figure 3-16](#ch03_figure_14_1748570891060222),
    this classifier got them all correct. I chose one picture of a dog with catlike
    ears and one with its back turned. Both pictures of cats were nontypical.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当我用[图3-16](#ch03_figure_14_1748570891060222)中那些非常复杂的图片进行测试时，这个分类器将它们全部正确分类。我选择了一张长着猫耳的狗的图片和一张背对着人的图片。两张猫的图片都不典型。
- en: '![](assets/aiml_0316.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0316.png)'
- en: Figure 3-16\. Unusual dogs and cats that the model classified correctly
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-16\. 模型正确分类的不寻常的狗和猫
- en: 'To parse the results, you can use code like this:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要解析结果，你可以使用如下代码：
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note the lines where I’m printing the output of the image, calculating the prediction
    from that, and printing that.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我打印图像输出、计算预测结果并打印输出的那些行。
- en: When you upload some images to Colab, you can see how they predict in [Figure 3-17](#ch03_figure_15_1748570891060236).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将一些图像上传到Colab时，你可以看到它们在[图3-17](#ch03_figure_15_1748570891060236)中的预测结果。
- en: '![](assets/aiml_0317.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0317.png)'
- en: Figure 3-17\. Classifying the cat washing its paw
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-17\. 对洗爪子的猫进行分类
- en: The first image uploaded was “labrador,” which, as its name suggests, is of
    a dog. The tensor returned from the model contained [–14.9642, 18.3943], meaning
    a very low number for the first label and a very high one for the second. Given
    that we used an image directory when training, the labels ended up being in alphabetical
    order, so it was low for cat and high for dog. Then, when we called `torch.max`,
    it gave us [1]. That indicates that neuron 1 is the one for this classification—thus,
    the image is a dog.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 上传的第一张图像是“拉布拉多”，正如其名称所暗示的，这是一只狗。从模型返回的张量包含[–14.9642, 18.3943]，这意味着第一个标签的数值非常低，而第二个标签的数值非常高。鉴于我们使用图像目录进行训练，标签最终是按字母顺序排列的，所以猫的数值低，狗的数值高。然后，当我们调用`torch.max`时，它给了我们[1]。这表明神经元1是用于这个分类的——因此，这张图像是一只狗。
- en: The second image had [5.3486, –4.8260], with the first neuron being higher.
    Thus, it detected a cat. The size of these numbers indicates the strength of the
    prediction. For example, it was much surer that the first image is a dog than
    it was that the second image is a cat.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图像的数值为[5.3486, –4.8260]，第一个神经元的数值更高。因此，它检测到的是猫。这些数字的大小表示预测的强度。例如，它对第一张图像是狗的判断比对第二张图像是猫的判断要更有信心。
- en: You can find the complete code for the “Horses or Humans” and “Dogs vs. Cats”
    classifiers in the book’s [GitHub repository](https://github.com/lmoroney/tfbook).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在书的[GitHub仓库](https://github.com/lmoroney/tfbook)中找到“马或人”和“狗对猫”分类器的完整代码。
- en: Multiclass Classification
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多分类
- en: In all of the examples so far, you’ve been building *binary* classifiers—ones
    that choose between two options (horses or humans, cats or dogs). On the other
    hand, when you’re building *multiclass classifiers*, the models are almost the
    same but there are a few important differences. Instead of a single neuron that
    is sigmoid activated or two neurons that are binary activated, your output layer
    will now require *n* neurons, where *n* is the number of classes you want to classify.
    You’ll also have to change your loss function to an appropriate one for multiple
    categories.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的所有示例中，你一直在构建*二元*分类器——选择两个选项（马或人，猫或狗）的分类器。另一方面，当你构建*多类分类器*时，模型几乎相同，但有一些重要差异。你的输出层现在将需要*n*个神经元，其中*n*是你想要分类的类别数量。你还需要将你的损失函数更改为适用于多个类别的适当函数。
- en: A neat feature of the `nn.CrossEntropyLoss` loss function in PyTorch is that
    it can handle multiple categories, so the “Cats vs. Dogs” and “Horses or Humans”
    transfer learning classifiers you’ve built thus far in this chapter can use it
    without modification. But the “Horses or Humans” classifier that you built at
    the beginning with a *single* output neuron will not be able to because it can’t
    handle more than two classes. This is always something to look out for, and it’s
    a common bug when you start writing code for classification.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中`nn.CrossEntropyLoss`损失函数的一个整洁特性是它可以处理多个类别，因此你在这个章节中构建的“猫对狗”和“马或人”迁移学习分类器无需修改就可以使用它。但是，你最初用单个输出神经元构建的“马或人”分类器将无法使用，因为它无法处理超过两个类别。这总是一件需要注意的事情，当你开始编写分类代码时，这是一个常见的错误。
- en: To go beyond two-class classification, consider, for example, the game Rock,
    Paper, Scissors. If you wanted to train a dataset to recognize the different hand
    gestures used in this game, you’d need to handle three categories. Fortunately,
    there’s a [simple dataset](https://oreil.ly/VHhmS) you can use for this.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要超越二分类，考虑例如石头、剪刀、布这样的游戏。如果你想要训练一个数据集来识别这个游戏中使用的不同手势，你需要处理三个类别。幸运的是，有一个[简单数据集](https://oreil.ly/VHhmS)你可以使用。
- en: 'There are two downloads: a training set of many diverse hands, with different
    sizes, shapes, colors, and details such as nail polish; and a testing set of equally
    diverse hands, none of which are in the training set. You can see some examples
    in [Figure 3-18](#ch03_figure_16_1748570891060251).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个下载：一个是包含许多不同大小、形状、颜色和细节（如指甲油）的手的训练集；另一个是同样多样化的手的测试集，其中没有任何手在训练集中。你可以在[图3-18](#ch03_figure_16_1748570891060251)中看到一些示例。
- en: '![Examples of Rock/Paper/Scissors gestures](assets/aiml_0318.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![石头/剪刀/布手势的示例](assets/aiml_0318.png)'
- en: Figure 3-18\. Examples of Rock, Paper, Scissors gestures
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-18. 石头、剪刀、布手势的示例
- en: 'Using the dataset is simple. You can download and unzip it—the sorted subdirectories
    are already present in the ZIP file—and then use it to initialize an `ImageFolder`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据集很简单。你可以下载并解压它——排序的子目录已经存在于ZIP文件中——然后使用它来初始化`ImageFolder`：
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Be sure to use a `transform` that fits the input shape of your model. In the
    last few examples, we were using Inception, and it’s 299 × 299.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 确保使用适合你模型输入形状的`transform`。在最后几个示例中，我们使用的是Inception，其大小为299 × 299。
- en: 'You can use the ImageFolder for your DataLoader in the usual way:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像往常一样使用ImageFolder作为你的DataLoader：
- en: '[PRE27]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Earlier, when we tweaked the Inception model for “Horses or Humans” or “Cats
    vs. Dogs,” there were only *two* classes and thus *two* output neurons. Given
    that this data has *three* classes, we need to be sure that we change the new
    fully connected layer at the bottom accordingly:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，当我们调整Inception模型以用于“马或人”或“猫对狗”时，只有*两个*类别，因此只有*两个*输出神经元。鉴于这个数据有三个类别，我们需要确保相应地更改底部的全新连接层：
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, training the model works as before: you specify the loss function and
    optimizer, and you call the `train_model()` function. For good repetition, this
    function is the same as the one used in the “Horses or Humans” and “Cats vs. Dogs”
    examples:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练模型的工作方式与之前相同：你指定损失函数和优化器，然后调用`train_model()`函数。为了良好的重复性，这个函数与“马或人”和“猫对狗”示例中使用的函数相同：
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Your code for testing predictions will also need to change somewhat. There are
    now three output neurons, and they will output a high value for the predicted
    class and lower values for the other classes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你用于测试预测的代码也需要做些改变。现在有三个输出神经元，它们将为预测类别输出高值，为其他类别输出低值。
- en: Note also that when you’re using the `ImageFolder`, the classes are loaded in
    alphabetical order—so while you might expect the output neurons to be in the order
    of the name of the game, the order will in fact be Paper, Rock, Scissors.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，当你使用`ImageFolder`时，类别是按字母顺序加载的——所以虽然你可能期望输出神经元按照游戏名称的顺序排列，但实际上顺序将是纸、石头、剪刀。
- en: 'Code that you can use to try out predictions in a Colab notebook will look
    like the following. It’s very similar to what you saw earlier:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Colab笔记本中尝试预测的代码如下。它与之前看到的内容非常相似：
- en: '[PRE30]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that it doesn’t parse the output; it just prints the classes. [Figure 3-19](#ch03_figure_17_1748570891060264)
    shows what it looks like in actual use.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它不会解析输出；它只是打印类别。[图3-19](#ch03_figure_17_1748570891060264)显示了实际使用中的样子。
- en: '![](assets/aiml_0319.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0319.png)'
- en: Figure 3-19\. Testing the Rock, Paper, Scissors classifier
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-19\. 测试剪刀、石头、布分类器
- en: You can see from the filenames what the images were.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从文件名中看到图像的内容。
- en: If you explore this a little deeper, you can see that the file named *scissors4.png*
    had an output of –2.5582, –1.7362, 3.8465]. The largest number is the third one,
    and if you think alphabetically, you can see that the third neuron represents
    scissors, so it was classified correctly. Similar results were achieved for the
    other files.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进一步探索，你可以看到名为*scissors4.png*的文件输出为-2.5582, -1.7362, 3.8465]。最大的数字是第三个，如果你按字母顺序思考，你可以看到第三个神经元代表剪刀，所以它被正确分类。其他文件也取得了类似的结果。
- en: Some images that you can use to test the dataset [are available to download](https://oreil.ly/dEUpx).
    Alternatively, of course, you can try your own. Note that the training images
    are all done against a plain white background, though, so there may be some confusion
    if there is a lot of detail in the background of the photos you take.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用的一些测试数据集的图片[可供下载](https://oreil.ly/dEUpx)。当然，你也可以尝试自己的。注意，训练图像都是在纯白色背景上完成的，所以如果你的照片背景有很多细节，可能会有些混淆。
- en: Dropout Regularization
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout正则化
- en: Earlier in this chapter, we discussed overfitting, in which a network may become
    too specialized in a particular type of input data and thus fare poorly on others.
    One technique to help overcome this is use of *dropout regularization*.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们讨论了过拟合问题，其中网络可能对特定类型的输入数据过于专业化，因此在其他数据上表现不佳。帮助克服这一问题的技术之一是使用*dropout正则化*。
- en: When a neural network is being trained, each individual neuron will have an
    effect on neurons in subsequent layers. Over time, particularly in larger networks,
    some neurons can become overspecialized—and that feeds downstream, potentially
    causing the network as a whole to become overspecialized and thus leading to overfitting.
    Additionally, neighboring neurons can end up with similar weights and biases,
    and if not monitored, this condition can lead the overall model to become overspecialized
    on the features activated by those neurons.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络正在训练时，每个单独的神经元都会对后续层的神经元产生影响。随着时间的推移，尤其是在较大的网络中，一些神经元可能会变得过度专业化——这会向下传递，可能导致整个网络变得过度专业化，从而导致过拟合。此外，相邻的神经元可能会具有相似的权重和偏差，如果不加监控，这种状况可能导致整体模型对那些神经元激活的特征变得过度专业化。
- en: For example, consider the neural network in [Figure 3-20](#ch03_figure_18_1748570891060278),
    in which there are layers of 2, 6, 6, and 2 neurons. The neurons in the middle
    layers might end up with very similar weights and biases.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑[图3-20](#ch03_figure_18_1748570891060278)中的神经网络，其中包含2、6、6和2个神经元的层。中间层的神经元可能会具有非常相似的权重和偏差。
- en: '![](assets/aiml_0320.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0320.png)'
- en: Figure 3-20\. A simple neural network
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-20\. 一个简单的神经网络
- en: While training, if you remove a random number of neurons and ignore them, then
    their contribution to the neurons in the next layer is temporarily blocked (see
    [Figure 3-21](#ch03_figure_19_1748570891060292)). They are effectively dropped
    out, leading to the term *dropout regularization*.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，如果你移除一定数量的随机神经元并忽略它们，那么它们对下一层神经元的影响将暂时被阻断（参见[图3-21](#ch03_figure_19_1748570891060292)）。它们实际上是被丢弃了，导致术语*dropout
    regularization*。
- en: '![](assets/aiml_0321.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0321.png)'
- en: Figure 3-21\. A neural network with dropouts
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-21\. 带有dropout的神经网络
- en: This reduces the chances of the neurons becoming overspecialized. The network
    will still learn the same number of parameters, but it should be better at generalization—that
    is, it should be more resilient to different inputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这减少了神经元变得过度专业化的可能性。网络将仍然学习相同数量的参数，但它应该更擅长泛化——也就是说，它应该对不同输入有更强的抵抗力。
- en: Note
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The concept of dropouts was proposed by Nitish Srivastava et al. in their 2014
    paper “[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://oreil.ly/673CJ).”'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dropout的概念是由Nitish Srivastava等人于2014年在他们的论文“[Dropout: A Simple Way to Prevent
    Neural Networks from Overfitting](https://oreil.ly/673CJ)”中提出的。'
- en: 'To implement dropouts in PyTorch, you can just use a simple layer like this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中实现dropout，你可以简单地使用这样的层：
- en: '[PRE31]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will drop out, at random, the specified percentage of neurons (here, 50%)
    in the specified layer. Note that it may take some experimentation to find the
    correct percentage for your network.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在随机情况下，丢弃指定层中指定百分比的神经元（这里，50%）。请注意，可能需要一些实验来找到适合你网络的正确百分比。
- en: For a simple example that demonstrates this, consider the new fully connected
    layers we added to the bottom of Inception with the transfer learning example
    in this chapter.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这一点，考虑我们在本章中添加到Inception底部的新的全连接层，以及迁移学习示例。
- en: 'Here it is for Rock, Paper, Scissors with three output neurons:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Rock, Paper, Scissors的三个输出神经元的例子：
- en: '[PRE32]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'With dropout added, it would look like this:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 添加dropout后，它看起来会是这样：
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The examples that we used in this chapter for transfer learning are already
    learning really well without the use of dropouts. However, I’d recommend that
    you always consider dropouts when building your models because they can greatly
    reduce waste in the ML process—letting your model learn just as well but much
    faster!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中用于迁移学习的示例已经在没有使用dropout的情况下学习得很好。然而，我建议你在构建模型时始终考虑dropout，因为它们可以大大减少ML过程中的浪费——让你的模型学习得同样好，但速度更快！
- en: Additionally, as you design your neural networks, keep in mind that getting
    great results on your training set is not always a good thing because it could
    be a sign of overfitting. Introducing dropouts can help you remove that problem
    so that you can optimize your network in other areas without that false sense
    of security.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在设计你的神经网络时，请记住，在训练集上获得很好的结果并不总是好事，因为这可能是过拟合的迹象。引入dropout可以帮助你消除这个问题，这样你就可以在其他区域优化你的网络，而不会产生那种虚假的安全感。
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced you to a more advanced way of achieving computer vision
    by using convolutional neural networks. You saw how to use convolutions to apply
    filters that can extract features from images, and you designed your first neural
    networks to deal with more complex vision scenarios than those you encountered
    with the MNIST and Fashion MNIST datasets. You also explored techniques to improve
    your network’s accuracy and avoid overfitting, such as the use of image augmentation
    and dropouts.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向你介绍了通过使用卷积神经网络实现更高级计算机视觉的方法。你看到了如何使用卷积来应用过滤器，从而从图像中提取特征，并且你设计了你的第一个神经网络来处理比MNIST和Fashion
    MNIST数据集中遇到的更复杂的视觉场景。你还探索了提高网络准确性和避免过拟合的技术，例如使用图像增强和丢弃法。
- en: Before we explore further scenarios, in [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246),
    you’ll get an introduction to PyTorch data, which is a technology that makes it
    much easier for you to get access to data for training and testing your networks.
    In this chapter, you downloaded ZIP files and extracted images, but that’s not
    always going to be possible. With PyTorch datasets, you’ll be able to access lots
    of datasets with a standard API.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步探索场景之前，在[第4章](ch04.html#ch04_using_data_with_pytorch_1748548966496246)中，你将了解到PyTorch数据的基础，这是一种使你更容易获取用于训练和测试网络数据的技术的介绍。在这一章中，你下载了ZIP文件并提取了图像，但这种情况并不总是可行的。使用PyTorch数据集，你将能够通过标准API访问大量数据集。
