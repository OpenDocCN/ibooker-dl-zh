- en: 'Chapter 3\. Going Beyond the Basics: Detecting Features in Images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    you learned how to get started with computer vision by creating a simple neural
    network that matched the input pixels of the Fashion MNIST dataset to 10 labels,
    each of which represented a type (or class) of clothing. And while you created
    a network that was pretty good at detecting clothing types, there was a clear
    drawback. Your neural network was trained on small monochrome images, each of
    which contained only a single item of clothing, and each item was centered within
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: To take the model to the next level, you need it to be able to detect *features*
    in images. So, for example, instead of looking merely at the raw pixels in the
    image, what if we could filter the images down to constituent elements? Matching
    those elements, instead of raw pixels, would help the model detect the contents
    of images more effectively. For example, consider the Fashion MNIST dataset that
    we used in the last chapter. When detecting a shoe, the neural network may have
    been activated by lots of dark pixels clustered at the bottom of the image, which
    it would see as the sole of the shoe. But if the shoe were not centered and filling
    the frame, this logic wouldn’t hold.
  prefs: []
  type: TYPE_NORMAL
- en: One method of detecting features comes from photography and image processing
    methodologies that you may already be familiar with. If you’ve ever used a tool
    like Photoshop or GIMP to sharpen an image, you’ve used a mathematical filter
    that works on the pixels of the image. Another word for what these filters do
    is *convolution*, and by using such filters in a neural network, you’ll create
    a *convolutional neural network* (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll start by learning about how to use convolutions to detect
    features in an image. Then, you’ll dig deeper into classifying images based on
    the features within. We’ll also explore augmentation of images to get more features
    and transfer learning to take preexisting features that were learned by others,
    and then we’ll look briefly into optimizing your models by using dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *convolution* is simply a filter of weights that are used to multiply a pixel
    by its neighbors to get a new value for the pixel. For example, consider the ankle
    boot image from Fashion MNIST and the pixel values for it (see [Figure 3-1](#ch03_figure_1_1748570891059985)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Ankle boot with convolution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we look at the pixel in the middle of the selection, we can see that it has
    the value 192\. (Recall that Fashion MNIST uses monochrome images with pixel values
    from 0 to 255.) The pixel above and to the left has the value 0, the one immediately
    above has the value 64, etc.
  prefs: []
  type: TYPE_NORMAL
- en: If we then define a filter in the same 3 × 3 grid, as shown below the original
    values, we can transform that pixel by calculating a new value for it. We do this
    by multiplying the current value of each pixel in the grid by the value in the
    same position in the filter grid and then summing up the total amount. This total
    will be the new value for the current pixel, and we then repeat this calculation
    for all pixels in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this case, while the current value of the pixel in the center of the
    selection is 192, we calculate the new value after applying the filter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The result equals 577, which will be the new value for the pixel. Repeating
    this process for every pixel in the image will give us a filtered image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider the impact of applying a filter on a more complicated image:
    specifically, the [ascent image](https://oreil.ly/wP8TE) that’s built into SciPy
    for easy testing. This is a 512 × 512 grayscale image that shows two people climbing
    a staircase.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a filter with negative values on the left, positive values on the right,
    and zeros in the middle will end up removing most of the information from the
    image except for vertical lines (see [Figure 3-2](#ch03_figure_2_1748570891060023)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Using a filter to derive vertical lines
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, a small change to the filter can emphasize the horizontal lines (see
    [Figure 3-3](#ch03_figure_3_1748570891060045)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Using a filter to derive horizontal lines
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These examples also show that the amount of information in the image is reduced.
    Therefore, we can potentially *learn* a set of filters that reduce the image to
    features, and those features can be matched to labels as before. Previously, we
    learned parameters that were used in neurons to match inputs to outputs, and similarly,
    we can learn the best filters to match inputs to outputs over time.
  prefs: []
  type: TYPE_NORMAL
- en: When we combine convolution with pooling, we can reduce the amount of information
    in the image while maintaining the features. We’ll explore that next.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Pooling* is the process of eliminating pixels in your image while maintaining
    the semantics of the content within the image. It’s best explained visually. [Figure 3-4](#ch03_figure_4_1748570891060063)
    depicts the concept of *max pooling*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. An example of max pooling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case, consider the box on the left to be the pixels in a monochrome
    image. We group them into 2 × 2 arrays, so in this case, the 16 pixels are grouped
    into four 2 × 2 arrays. These arrays are called *pools*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we select the *maximum* value in each of the groups and reassemble them
    into a new image. Thus, the pixels on the left are reduced by 75% (from 16 to
    4), with the maximum value from each pool making up the new image. [Figure 3-5](#ch03_figure_5_1748570891060080)
    shows the version of ascent from [Figure 3-2](#ch03_figure_2_1748570891060023),
    with the vertical lines enhanced, after max pooling has been applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Ascent after applying vertical filter and max pooling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note how the filtered features have not just been maintained but have been further
    enhanced. Also, the image size has changed from 512 × 512 to 256 × 256—making
    it a quarter of the original size.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are other approaches to pooling. These include *min pooling*, which takes
    the smallest pixel value from the pool, and *average pooling*, which takes the
    overall average value from the pool.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    you created a neural network that recognized fashion images. For convenience,
    here’s the code to define the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1] `# Training process` `epochs` `=` `5` `for` `t` `in` `range``(``epochs``):`     `print``(``f``"Epoch`
    `{``t``+``1``}``\n``-------------------------------"``)`     `train``(``train_loader``,`
    `model``,` `loss_function``,` `optimizer``)` `print``(``"Done!"``)` [PRE2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]`To convert this to a CNN, you simply use convolutional layers in our
    model definition on top of the current linear ones. You’ll also add pooling layers.    To
    implement a convolutional layer, you’ll use the `nn.Conv2D` type. It accepts as
    parameters the number of convolutions to use in the layer, the size of the convolutions,
    the activation function, etc.    For example, here’s a convolutional layer that
    uses this type:    [PRE4]    In this case, we want the layer to learn `64` convolutions.
    It will randomly initialize them, and over time, it will learn the filter values
    that work best to match the input values to their labels. The `kernel_size = 3`
    indicates the size of the filter. Earlier, we showed you 3 × 3 filters, and that’s
    what we’re specifying here. The 3 × 3 filter is the most common size of filter.
    You can change it as you see fit, but you’ll typically see an odd number of axes
    like 5 × 5 or 7 × 7 because of how filters remove pixels from the borders of the
    image, as you’ll see later.    Here’s how to use a pooling layer in the neural
    network. You’ll typically do this immediately after the convolutional layer:    [PRE5]    In
    the example back in [Figure 3-4](#ch03_figure_4_1748570891060063), we split the
    image into 2 × 2 pools and picked the maximum value in each. However, we could
    have used the parameters that you see here to define the pool size. The `kernel_size=2`
    parameter indicates that our pools are 2 × 2, and the `stride=2` parameter means
    that the filter will jump over two pixels to get the next pool.    Now, let’s
    explore the full code to define a model for Fashion MNIST with a CNN:    [PRE6]    Here,
    we see that the class has two functions, one for initialization and one that will
    be called during the forward pass in each epoch during training.    The `init`
    simply defines what each of the layers in our neural network will look like. The
    first layer (`self.layer1`) will take in the one-dimensional input, have `64`
    convolutions, a `kernel_size` of `3`, and `padding` of `1`. It will then ReLU
    the output before max pooling it.    The next layer (`self.layer2`) will take
    the 64 convolutions of output from the previous layer and then output `64` of
    its own before ReLUing them and max pooling them. Its output will now be `64 ×
    6 × 6` because the `MaxPool` halves the size of the image.    The data is then
    fed to the next layer (`self.fc1`, where `fc` stands for *fully connected*), with
    the input being the shape of the output of the previous layer. The output is 128,
    which is the same number of neurons we used in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    for the deep neural network (DNN).    Finally, these 128 are fed into the final
    layer (`self.fc1`) with 10 outputs—that represent the 10 classes.    ###### Note    In
    the DNN, we ran the input through a `Flatten` layer prior to feeding it into the
    first `Dense` layer. We’ve lost that in the input layer here—instead, we’ve just
    specified the 1-D input shape. Note that prior to the first `Linear` layer, after
    convolutions and pooling, the data will be flattened.    Then, we stack these
    layers in the `forward` function. We can see that we get the data `x` and pass
    it through `layer1` to get `out`, which is passed to `layer2` to get a new `out`.
    At this point, we have the convolutions that we’ve learned, but we need to flatten
    them before loading them into the `Linear` layers `fc1` and `fc2`. The `out =
    out.view(out.size(0), -1)` achieves this.    If we train this network on the same
    data for the same 50 epochs as we used when training the network shown in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    we will see that it works nicely. We can get to 91% accuracy on the test set quite
    easily:    [PRE7]    So, we can see that adding convolutions to the neural network
    definitely increases its ability to classify images. Next, let’s take a look at
    the journey an image takes through the network so we can get a little bit more
    of an understanding of why this process works.    ###### Note    If you are using
    the accompanying code from my GitHub, you’ll notice that I’m using model.to(device)
    a lot. In PyTorch, if an accelerator is available, you can request that the model
    and/or its data use the accelerator with this command.[PRE8]``  [PRE9]` [PRE10]
    # Exploring the Convolutional Network    With the torchsummary library, you can
    inspect your model. When you run it on the Fashion MNIST convolutional network
    we’ve been working on, you’ll see something like this:    [PRE11]py    Let’s first
    take a look at the `Output Shape` column to get an understanding of what’s going
    on here. Our first layer will have 28 × 28 images and apply 64 filters to them.
    But because our filter is 3 × 3, a one-pixel border around the image would typically
    be lost, reducing our overall information to 26 × 26 pixels. However, because
    we used the `padding=1` parameter, the image was artificially inflated to 30 ×
    30, meaning that its output would be the correct 28 × 28 and no information would
    be lost.    If you don’t pad the image, you’ll end up with a result like the one
    in [Figure 3-6](#ch03_figure_6_1748570891060097). If we take each of the boxes
    as a pixel in the image, the first possible filter we can use starts in the second
    row and the second column. The same would happen on the right side and at the
    bottom of the diagram.  ![](assets/aiml_0306.png)  ###### Figure 3-6\. Losing
    pixels when running a filter    Thus, an image that is *a* × *b* pixels in shape
    when run through a 3 × 3 filter will become (*a* – 2) × (*b* – 2) pixels in shape.
    Similarly, a 5 × 5 filter would make it (*a* – 4) × (*b* – 4), and so on. As we’re
    using a 28 × 28 image and a 3 × 3 filter, our output would now be 26 × 26\. But
    because we padded the image up to 30 × 30 (again, to prevent loss of information),
    the output is now 28 × 28.    After that, the pooling layer will be 2 × 2, so
    the size of the image will halve on each axis, and it will then become 14 × 14\.
    The next convolutional layer does *not* use padding, so it will reduce this further
    to 12 × 12, and the next pooling will output 6 x 6.    So, by the time the image
    has gone through two convolutional layers, the result will be many 6 × 6 images.
    How many? We can see that in the `Param #` (number of parameters) column.    Each
    convolution is a 3 × 3 filter, plus a bias. Remember earlier, with our dense layers,
    when each layer was *y* = *wx* + *b*, where *w* was our parameter (aka weight)
    and *b* was our bias? This case is very similar, except that because the filter
    is 3 × 3, there are 9 parameters to learn. Given that we have 64 convolutions
    defined, we’ll have 640 overall parameters. (Each convolution has 9 parameters
    plus a bias, for a total of 10, and there are 64 of them.)    The `ReLU and MaxPooling`
    layers don’t learn anything; they just reduce the image, so there are no learned
    parameters there—hence, 0 are reported.    The next convolutional layer has 64
    filters, but each of them is multiplied across the *previous* 64 filters, each
    of which has 9 parameters. We have a bias on each of the new 64 filters, so our
    number of parameters should be (64 × (64 × 9)) + 64, which gives us 36,928 parameters
    the network needs to learn.    If this is confusing, try changing the number of
    convolutions in the first layer to something else—for example, 10\. You’ll see
    that the number of parameters in the second layer becomes 5,824, which is (64
    × (10 × 9)) + 64).    By the time we get through the second convolution, our images
    are 6 × 6, and we have 64 of them. If we multiply this out, we’ll have 1,600 values,
    which we’ll feed into a dense layer of 128 neurons. Each neuron has a weight and
    a bias, and we’ll have 128 of them, so the number of parameters the network will
    learn is ((6 × 6 × 64) × 128) + 128, giving us 295,040 parameters.    Then, our
    final dense layer of 10 neurons will take in the output of the previous 128, so
    the number of parameters learned will be (128 × 10) + 10, which is 1,290.    The
    total number of parameters will be the sum of all of these: 333,898.    Training
    this network requires us to learn the best set of these 333,898 parameters to
    match the input images to their labels. It’s a slower process because there are
    more parameters, but as we can see from the results, it also builds a more accurate
    model!    Of course, with this dataset, we still have the limitation that the
    images are 28 × 28, monochrome, and centered. So next we’ll take a look at using
    convolutions to explore a more complex dataset comprising color pictures of horses
    and humans, and we’ll try to make the model determine whether an image contains
    one or the other. In this case, the subject won’t always be centered in the image
    like with Fashion MNIST, so we’ll have to rely on convolutions to spot distinguishing
    features.    # Building a CNN to Distinguish Between Horses and Humans    In this
    section, we’ll explore a more complex scenario than the Fashion MNIST classifier.
    We’ll extend what we’ve learned about convolutions and CNNs to try to classify
    the contents of images in which the location of a feature isn’t always in the
    same place. I’ve created the “Horses or Humans” dataset for this purpose.    ##
    The “Horses or Humans” Dataset    [The dataset for this section](https://oreil.ly/8VXwy)
    contains over a thousand 300 × 300–pixel images. Approximately half the images
    are of horses, and the other half are of humans—and all are rendered in different
    poses. You can see some examples in [Figure 3-7](#ch03_figure_7_1748570891060112).  ![](assets/aiml_0307.png)  ######
    Figure 3-7\. Horses and humans    As you can see, the subjects have different
    orientations and poses, and the image composition varies. Consider the two horses,
    for example—their heads are oriented differently, and one image is zoomed out
    (showing the complete animal), while the other is zoomed in (showing just the
    head and part of the body). Similarly, the humans are lit differently, have different
    skin tones, and are posed differently. The man has his hands on his hips, while
    the woman has hers outstretched. The images also contain backgrounds such as trees
    and beaches, so a classifier will have to determine which parts of the image are
    the important features that determine what makes a horse a horse and a human a
    human, without being affected by the background.    While the previous examples
    of predicting *y* = 2*x* – 1 or classifying small monochrome images of clothing
    *might* have been possible with traditional coding, it’s clear that this example
    is far more difficult and that you are crossing the line into where ML is essential
    to solve a problem.    An interesting side note is that these images are all computer
    generated. The theory is that features spotted in a CGI image of a horse should
    apply to a real image, and you’ll see how well this works later in this chapter.    ##
    Handling the Data    The Fashion MNIST dataset that you’ve been using up to this
    point comes with labels, and every image file has an associated file with the
    label details. Many image-based datasets do not have this, and “Horses or Humans”
    is no exception. Instead of labels, the images are sorted into subdirectories
    of each type, and with the DataLoader in PyTorch, you can use this structure to
    *automatically* assign labels to images.    First, you simply need to ensure that
    your directory structure has a set of named subdirectories, with each subdirectory
    being a label. For example, the “Horses or Humans” dataset is available as a set
    of ZIP files, one of which contains the training data (1,000+ images) and another
    of which contains the validation data (256 images). When you download and unpack
    them into a local directory for training and validation, you need to ensure that
    they are in a file structure like the one in [Figure 3-8](#ch03_figure_8_1748570891060128).    Here’s
    the code to get the training data and extract it into the appropriately named
    subdirectories, as shown in [Figure 3-8](#ch03_figure_8_1748570891060128):    [PRE12]py
    `file_name` `=` `"horse-or-human.zip"` `training_dir` `=` `''horse-or-human/training/''`
    `urllib``.``request``.``urlretrieve``(``url``,` `file_name``)`   `zip_ref` `=`
    `zipfile``.``ZipFile``(``file_name``,` `''r''``)` `zip_ref``.``extractall``(``training_dir``)`
    `zip_ref``.``close``()` [PRE13]py  [PRE14] [PRE15] class HorsesHumansCNN(nn.Module):     def
    __init__(self):         super(HorsesHumansCNN, self).__init__()         self.conv1
    = nn.Conv2d(3, 16, kernel_size=3, padding=1)         self.conv2 = nn.Conv2d(16,
    32, kernel_size=3, padding=1)         self.conv3 = nn.Conv2d(32, 64, kernel_size=3,
    padding=1)         self.pool = nn.MaxPool2d(2, 2)         self.fc1 = nn.Linear(64
    * 18 * 18, 512)         self.drop = nn.Dropout(0.25)         self.fc2 = nn.Linear(512,
    1)       def forward(self, x):         x = self.pool(F.relu(self.conv1(x)))         x
    = self.pool(F.relu(self.conv2(x)))         x = self.pool(F.relu(self.conv3(x)))         x
    = x.view(–1, 64 * 18 * 18)         x = F.relu(self.fc1(x))         x = self.drop(x)         x
    = self.fc2(x)         x = torch.sigmoid(x)  # Use sigmoid to output probabilities         return
    x [PRE16] ----------------------------------------------------------------         Layer
    (type)               Output Shape         Param # ================================================================             Conv2d-1         [–1,
    16, 150, 150]             448          MaxPool2d-2           [–1, 16, 75, 75]               0             Conv2d-3           [–1,
    32, 75, 75]           4,640          MaxPool2d-4           [–1, 32, 37, 37]               0             Conv2d-5           [–1,
    64, 37, 37]          18,496          MaxPool2d-6           [–1, 64, 18, 18]               0             Linear-7                  [–1,
    512]      10,617,344            Dropout-8                  [–1, 512]               0             Linear-9                    [–1,
    1]             513 ================================================================
    Total params: 10,641,441 Trainable params: 10,641,441 Non-trainable params: 0
    ---------------------------------------------------------------- Input size (MB):
    0.26 Forward/backward pass size (MB): 5.98 Params size (MB): 40.59 Estimated Total
    Size (MB): 46.83 ----------------------------------------------------------------
    [PRE17] criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001)
    [PRE18] def train_model(num_epochs):     for epoch in range(num_epochs):         model.train()         running_loss
    = 0.0         for images, labels in train_loader:             images, labels =
    images.to(device), labels.to(device).float()               optimizer.zero_grad()             outputs
    = model(images).view(–1)             loss = criterion(outputs, labels)             loss.backward()             optimizer.step()             running_loss
    += loss.item() [PRE19] def train_model(num_epochs):     for epoch in range(num_epochs):         model.train()         running_loss
    = 0.0         for images, labels in train_loader:             images, labels =
    images.to(device), labels.to(device).float()               optimizer.zero_grad()             outputs
    = model(images).view(–1)             loss = criterion(outputs, labels)             loss.backward()             optimizer.step()             running_loss
    += loss.item()           print(f''Epoch {epoch + 1}, Loss: {running_loss /                  len(train_loader)}'')       #
    Evaluate on training set         model.eval()         with torch.no_grad():             correct
    = 0             total = 0             for images, labels in train_loader:                 images,
    labels = images.to(device),                                   labels.to(device).float()                 outputs
    = model(images).view(–1)                 predicted = outputs > 0.5  # Threshold
    predictions                 total += labels.size(0)                 correct +=
    (predicted == labels).sum().item()             print(f''Test Set Accuracy: {100
    * correct / total}%'')           # Evaluate on validation set         model.eval()         with
    torch.no_grad():             correct = 0             total = 0             for
    images, labels in val_loader:                 images, labels = images.to(device),                                   labels.to(device).float()                 outputs
    = model(images).view(–1)                 predicted = outputs > 0.5  # Threshold
    predictions                 total += labels.size(0)                 correct +=
    (predicted == labels).sum().item()             print(f''Validation Set Accuracy:
    {100 * correct / total}%'')       train_model(50) [PRE20] Epoch 7, Loss: 0.0016404045829699512
    Training Set Accuracy: 100.0% Validation Set Accuracy: 88.28125% Epoch 8, Loss:
    0.0010613293736610378 Training Set Accuracy: 100.0% Validation Set Accuracy: 89.0625%
    Epoch 9, Loss: 0.0008372313717332979 Training Set Accuracy: 100.0% Validation
    Set Accuracy: 86.328125% Epoch 10, Loss: 0.0006578459407812646 Training Set Accuracy:
    100.0% Validation Set Accuracy: 87.5% [PRE21] def load_image(image_path, transform):     #
    Load image     image = Image.open(image_path).convert(''RGB'')  # Convert to RGB     #
    Apply transformations     image = transform(image)     # Add batch dimension,
    as the model expects batches     image = image.unsqueeze(0)     return image [PRE22]
    with torch.no_grad():     output = model(image) [PRE23] class_name = "Human" if
    prediction.item() == 1 else "Horse" [PRE24]` [PRE25]`` [PRE26] # Define transformations
    transform = transforms.Compose([     transforms.Resize((150, 150)),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5,
    0.5, 0.5], std=[0.5, 0.5, 0.5]) ]) [PRE27] # Transforms for the training data
    train_transforms = transforms.Compose([     transforms.RandomHorizontalFlip(),     transforms.RandomRotation(20),     transforms.RandomResizedCrop(150),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5,
    0.5, 0.5], std=[0.5, 0.5, 0.5])   ])   # Transforms for the validation data val_transforms
    = transforms.Compose([     transforms.Resize(150),     transforms.CenterCrop(150),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5,
    0.5, 0.5], std=[0.5, 0.5, 0.5]) ]) [PRE28] transforms.RandomAffine(     degrees=0,  #
    No rotation     translate=(0.2, 0.2),  # Translate up to 20% vert and horizontally     scale=(0.8,
    1.2),  # Zoom in or out by 20%     shear=20,  # Shear by up to 20 degrees ), [PRE29]
    import torch import torch.nn as nn from torchvision import models, transforms
    from torch.utils.data import DataLoader from torchvision.datasets import ImageFolder
    from torch.optim import RMSprop   # Load the pretrained Inception V3 model pre_trained_model
    = models.inception_v3(pretrained=True, aux_logits=True) [PRE30] def print_model_summary(model):     for
    name, module in model.named_modules():         print(f"{name} : {module.__class__.__name__}")   #
    Example of how to use the function with your pretrained model print_model_summary(pre_trained_model)
    [PRE31] # Freeze all layers up to and including the ''Mixed_7c'' for name, parameter
    in pre_trained_model.named_parameters():     parameter.requires_grad = False     if
    ''Mixed_7c'' in name:         break [PRE32] # Modify the existing fully connected
    layer num_ftrs = pre_trained_model.fc.in_features pre_trained_model.fc = nn.Sequential(     nn.Linear(num_ftrs,
    1024),  # New fully connected layer      nn.ReLU(),                # Activation
    layer     nn.Linear(1024, 2)         # Final layer for binary classification )
    [PRE33]      def load_image(image_path, transform):     # Load image     image
    = Image.open(image_path).convert(''RGB'')  # Convert to RGB      # Apply transformations     image
    = transform(image)     # Add batch dimension, as the model expects batches     image
    = image.unsqueeze(0)     return image      # Prediction function def predict(image_path,
    model, device, transform):     model.eval()     image = load_image(image_path,
    transform)     image = image.to(device)     with torch.no_grad():         output
    = model(image)         print(output)         prediction = torch.max(output, 1)         print(prediction)
    [PRE34] !wget --no-check-certificate \  https://storage.googleapis.com/learning-datasets/rps.zip
    \  -O /tmp/rps.zip local_zip = ''/tmp/rps.zip'' zip_ref = zipfile.ZipFile(local_zip,
    ''r'') zip_ref.extractall(''/tmp/'') zip_ref.close() training_dir = "/tmp/rps/"   train_dataset
    = ImageFolder(root=training_dir, transform=transform) [PRE35] train_loader = DataLoader(train_dataset,
    batch_size=32, shuffle=True) [PRE36] # Modify the existing fully connected layer
    num_ftrs = pre_trained_model.fc.in_features pre_trained_model.fc = nn.Sequential(     nn.Linear(num_ftrs,
    1024),  # New fully connected layer      nn.ReLU(),                  # Activation
    layer     nn.Linear(1024, 3)         # Final layer for binary classification )
    [PRE37] # Only optimize parameters that are set to be trainable optimizer = RMSprop(filter(lambda
    p: p.requires_grad,                      pre_trained_model.parameters()), lr=0.001)   criterion
    = nn.CrossEntropyLoss()   # Train the model train_model(pre_trained_model, criterion,
    optimizer, train_loader, num_epochs=3) [PRE38] def load_image(image_path, transform):     #
    Load image     image = Image.open(image_path).convert(''RGB'')  # Convert to RGB      #
    Apply transformations     image = transform(image)     # Add batch dimension,
    as the model expects batches     image = image.unsqueeze(0)     return image       #
    Prediction function def predict(image_path, model, device, transform):     model.eval()     image
    = load_image(image_path, transform)     image = image.to(device)     with torch.no_grad():         output
    = model(image)         print(output)         prediction = torch.max(output, 1)         print(prediction)
    [PRE39] nn.Dropout(0.5) [PRE40] num_ftrs = pre_trained_model.fc.in_features pre_trained_model.fc
    = nn.Sequential(     nn.Linear(num_ftrs, 1024),  # New fully connected layer      nn.ReLU(),                #
    Activation layer     nn.Linear(1024, 3)         # Final layer for RPS ) [PRE41]
    num_ftrs = model.fc.in_features model.fc = nn.Sequential(     nn.Dropout(0.5),  #
    Adding dropout before the final FC layer     nn.Linear(num_ftrs, 1024),  # Reduce
    dimensionality to 1024     nn.ReLU(),     nn.Dropout(0.5),  # Adding another dropout
    layer after ReLU activation     nn.Linear(1024, 3)  # Final layer for RPS ) [PRE42]`
    [PRE43][PRE44]````'
  prefs: []
  type: TYPE_NORMAL
