- en: 'Chapter 3\. Going Beyond the Basics: Detecting Features in Images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    you learned how to get started with computer vision by creating a simple neural
    network that matched the input pixels of the Fashion MNIST dataset to 10 labels,
    each of which represented a type (or class) of clothing. And while you created
    a network that was pretty good at detecting clothing types, there was a clear
    drawback. Your neural network was trained on small monochrome images, each of
    which contained only a single item of clothing, and each item was centered within
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: To take the model to the next level, you need it to be able to detect *features*
    in images. So, for example, instead of looking merely at the raw pixels in the
    image, what if we could filter the images down to constituent elements? Matching
    those elements, instead of raw pixels, would help the model detect the contents
    of images more effectively. For example, consider the Fashion MNIST dataset that
    we used in the last chapter. When detecting a shoe, the neural network may have
    been activated by lots of dark pixels clustered at the bottom of the image, which
    it would see as the sole of the shoe. But if the shoe were not centered and filling
    the frame, this logic wouldn’t hold.
  prefs: []
  type: TYPE_NORMAL
- en: One method of detecting features comes from photography and image processing
    methodologies that you may already be familiar with. If you’ve ever used a tool
    like Photoshop or GIMP to sharpen an image, you’ve used a mathematical filter
    that works on the pixels of the image. Another word for what these filters do
    is *convolution*, and by using such filters in a neural network, you’ll create
    a *convolutional neural network* (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll start by learning about how to use convolutions to detect
    features in an image. Then, you’ll dig deeper into classifying images based on
    the features within. We’ll also explore augmentation of images to get more features
    and transfer learning to take preexisting features that were learned by others,
    and then we’ll look briefly into optimizing your models by using dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *convolution* is simply a filter of weights that are used to multiply a pixel
    by its neighbors to get a new value for the pixel. For example, consider the ankle
    boot image from Fashion MNIST and the pixel values for it (see [Figure 3-1](#ch03_figure_1_1748570891059985)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Ankle boot with convolution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we look at the pixel in the middle of the selection, we can see that it has
    the value 192\. (Recall that Fashion MNIST uses monochrome images with pixel values
    from 0 to 255.) The pixel above and to the left has the value 0, the one immediately
    above has the value 64, etc.
  prefs: []
  type: TYPE_NORMAL
- en: If we then define a filter in the same 3 × 3 grid, as shown below the original
    values, we can transform that pixel by calculating a new value for it. We do this
    by multiplying the current value of each pixel in the grid by the value in the
    same position in the filter grid and then summing up the total amount. This total
    will be the new value for the current pixel, and we then repeat this calculation
    for all pixels in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this case, while the current value of the pixel in the center of the
    selection is 192, we calculate the new value after applying the filter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The result equals 577, which will be the new value for the pixel. Repeating
    this process for every pixel in the image will give us a filtered image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider the impact of applying a filter on a more complicated image:
    specifically, the [ascent image](https://oreil.ly/wP8TE) that’s built into SciPy
    for easy testing. This is a 512 × 512 grayscale image that shows two people climbing
    a staircase.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a filter with negative values on the left, positive values on the right,
    and zeros in the middle will end up removing most of the information from the
    image except for vertical lines (see [Figure 3-2](#ch03_figure_2_1748570891060023)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Using a filter to derive vertical lines
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, a small change to the filter can emphasize the horizontal lines (see
    [Figure 3-3](#ch03_figure_3_1748570891060045)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Using a filter to derive horizontal lines
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These examples also show that the amount of information in the image is reduced.
    Therefore, we can potentially *learn* a set of filters that reduce the image to
    features, and those features can be matched to labels as before. Previously, we
    learned parameters that were used in neurons to match inputs to outputs, and similarly,
    we can learn the best filters to match inputs to outputs over time.
  prefs: []
  type: TYPE_NORMAL
- en: When we combine convolution with pooling, we can reduce the amount of information
    in the image while maintaining the features. We’ll explore that next.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Pooling* is the process of eliminating pixels in your image while maintaining
    the semantics of the content within the image. It’s best explained visually. [Figure 3-4](#ch03_figure_4_1748570891060063)
    depicts the concept of *max pooling*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. An example of max pooling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case, consider the box on the left to be the pixels in a monochrome
    image. We group them into 2 × 2 arrays, so in this case, the 16 pixels are grouped
    into four 2 × 2 arrays. These arrays are called *pools*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we select the *maximum* value in each of the groups and reassemble them
    into a new image. Thus, the pixels on the left are reduced by 75% (from 16 to
    4), with the maximum value from each pool making up the new image. [Figure 3-5](#ch03_figure_5_1748570891060080)
    shows the version of ascent from [Figure 3-2](#ch03_figure_2_1748570891060023),
    with the vertical lines enhanced, after max pooling has been applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Ascent after applying vertical filter and max pooling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note how the filtered features have not just been maintained but have been further
    enhanced. Also, the image size has changed from 512 × 512 to 256 × 256—making
    it a quarter of the original size.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are other approaches to pooling. These include *min pooling*, which takes
    the smallest pixel value from the pool, and *average pooling*, which takes the
    overall average value from the pool.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    you created a neural network that recognized fashion images. For convenience,
    here’s the code to define the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To convert this to a CNN, you simply use convolutional layers in our model definition
    on top of the current linear ones. You’ll also add pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: To implement a convolutional layer, you’ll use the `nn.Conv2D` type. It accepts
    as parameters the number of convolutions to use in the layer, the size of the
    convolutions, the activation function, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here’s a convolutional layer that uses this type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we want the layer to learn `64` convolutions. It will randomly
    initialize them, and over time, it will learn the filter values that work best
    to match the input values to their labels. The `kernel_size = 3` indicates the
    size of the filter. Earlier, we showed you 3 × 3 filters, and that’s what we’re
    specifying here. The 3 × 3 filter is the most common size of filter. You can change
    it as you see fit, but you’ll typically see an odd number of axes like 5 × 5 or
    7 × 7 because of how filters remove pixels from the borders of the image, as you’ll
    see later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to use a pooling layer in the neural network. You’ll typically do
    this immediately after the convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the example back in [Figure 3-4](#ch03_figure_4_1748570891060063), we split
    the image into 2 × 2 pools and picked the maximum value in each. However, we could
    have used the parameters that you see here to define the pool size. The `kernel_size=2`
    parameter indicates that our pools are 2 × 2, and the `stride=2` parameter means
    that the filter will jump over two pixels to get the next pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s explore the full code to define a model for Fashion MNIST with a
    CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see that the class has two functions, one for initialization and one
    that will be called during the forward pass in each epoch during training.
  prefs: []
  type: TYPE_NORMAL
- en: The `init` simply defines what each of the layers in our neural network will
    look like. The first layer (`self.layer1`) will take in the one-dimensional input,
    have `64` convolutions, a `kernel_size` of `3`, and `padding` of `1`. It will
    then ReLU the output before max pooling it.
  prefs: []
  type: TYPE_NORMAL
- en: The next layer (`self.layer2`) will take the 64 convolutions of output from
    the previous layer and then output `64` of its own before ReLUing them and max
    pooling them. Its output will now be `64 × 6 × 6` because the `MaxPool` halves
    the size of the image.
  prefs: []
  type: TYPE_NORMAL
- en: The data is then fed to the next layer (`self.fc1`, where `fc` stands for *fully
    connected*), with the input being the shape of the output of the previous layer.
    The output is 128, which is the same number of neurons we used in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)
    for the deep neural network (DNN).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, these 128 are fed into the final layer (`self.fc1`) with 10 outputs—that
    represent the 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the DNN, we ran the input through a `Flatten` layer prior to feeding it into
    the first `Dense` layer. We’ve lost that in the input layer here—instead, we’ve
    just specified the 1-D input shape. Note that prior to the first `Linear` layer,
    after convolutions and pooling, the data will be flattened.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we stack these layers in the `forward` function. We can see that we get
    the data `x` and pass it through `layer1` to get `out`, which is passed to `layer2`
    to get a new `out`. At this point, we have the convolutions that we’ve learned,
    but we need to flatten them before loading them into the `Linear` layers `fc1`
    and `fc2`. The `out = out.view(out.size(0), -1)` achieves this.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we train this network on the same data for the same 50 epochs as we used
    when training the network shown in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    we will see that it works nicely. We can get to 91% accuracy on the test set quite
    easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So, we can see that adding convolutions to the neural network definitely increases
    its ability to classify images. Next, let’s take a look at the journey an image
    takes through the network so we can get a little bit more of an understanding
    of why this process works.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are using the accompanying code from my GitHub, you’ll notice that I’m
    using model.to(device) a lot. In PyTorch, if an accelerator is available, you
    can request that the model and/or its data use the accelerator with this command.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Convolutional Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the torchsummary library, you can inspect your model. When you run it
    on the Fashion MNIST convolutional network we’ve been working on, you’ll see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let’s first take a look at the `Output Shape` column to get an understanding
    of what’s going on here. Our first layer will have 28 × 28 images and apply 64
    filters to them. But because our filter is 3 × 3, a one-pixel border around the
    image would typically be lost, reducing our overall information to 26 × 26 pixels.
    However, because we used the `padding=1` parameter, the image was artificially
    inflated to 30 × 30, meaning that its output would be the correct 28 × 28 and
    no information would be lost.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t pad the image, you’ll end up with a result like the one in [Figure 3-6](#ch03_figure_6_1748570891060097).
    If we take each of the boxes as a pixel in the image, the first possible filter
    we can use starts in the second row and the second column. The same would happen
    on the right side and at the bottom of the diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Losing pixels when running a filter
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thus, an image that is *a* × *b* pixels in shape when run through a 3 × 3 filter
    will become (*a* – 2) × (*b* – 2) pixels in shape. Similarly, a 5 × 5 filter would
    make it (*a* – 4) × (*b* – 4), and so on. As we’re using a 28 × 28 image and a
    3 × 3 filter, our output would now be 26 × 26\. But because we padded the image
    up to 30 × 30 (again, to prevent loss of information), the output is now 28 ×
    28.
  prefs: []
  type: TYPE_NORMAL
- en: After that, the pooling layer will be 2 × 2, so the size of the image will halve
    on each axis, and it will then become 14 × 14\. The next convolutional layer does
    *not* use padding, so it will reduce this further to 12 × 12, and the next pooling
    will output 6 x 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, by the time the image has gone through two convolutional layers, the result
    will be many 6 × 6 images. How many? We can see that in the `Param #` (number
    of parameters) column.'
  prefs: []
  type: TYPE_NORMAL
- en: Each convolution is a 3 × 3 filter, plus a bias. Remember earlier, with our
    dense layers, when each layer was *y* = *wx* + *b*, where *w* was our parameter
    (aka weight) and *b* was our bias? This case is very similar, except that because
    the filter is 3 × 3, there are 9 parameters to learn. Given that we have 64 convolutions
    defined, we’ll have 640 overall parameters. (Each convolution has 9 parameters
    plus a bias, for a total of 10, and there are 64 of them.)
  prefs: []
  type: TYPE_NORMAL
- en: The `ReLU and MaxPooling` layers don’t learn anything; they just reduce the
    image, so there are no learned parameters there—hence, 0 are reported.
  prefs: []
  type: TYPE_NORMAL
- en: The next convolutional layer has 64 filters, but each of them is multiplied
    across the *previous* 64 filters, each of which has 9 parameters. We have a bias
    on each of the new 64 filters, so our number of parameters should be (64 × (64
    × 9)) + 64, which gives us 36,928 parameters the network needs to learn.
  prefs: []
  type: TYPE_NORMAL
- en: If this is confusing, try changing the number of convolutions in the first layer
    to something else—for example, 10\. You’ll see that the number of parameters in
    the second layer becomes 5,824, which is (64 × (10 × 9)) + 64).
  prefs: []
  type: TYPE_NORMAL
- en: By the time we get through the second convolution, our images are 6 × 6, and
    we have 64 of them. If we multiply this out, we’ll have 1,600 values, which we’ll
    feed into a dense layer of 128 neurons. Each neuron has a weight and a bias, and
    we’ll have 128 of them, so the number of parameters the network will learn is
    ((6 × 6 × 64) × 128) + 128, giving us 295,040 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Then, our final dense layer of 10 neurons will take in the output of the previous
    128, so the number of parameters learned will be (128 × 10) + 10, which is 1,290.
  prefs: []
  type: TYPE_NORMAL
- en: 'The total number of parameters will be the sum of all of these: 333,898.'
  prefs: []
  type: TYPE_NORMAL
- en: Training this network requires us to learn the best set of these 333,898 parameters
    to match the input images to their labels. It’s a slower process because there
    are more parameters, but as we can see from the results, it also builds a more
    accurate model!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, with this dataset, we still have the limitation that the images are
    28 × 28, monochrome, and centered. So next we’ll take a look at using convolutions
    to explore a more complex dataset comprising color pictures of horses and humans,
    and we’ll try to make the model determine whether an image contains one or the
    other. In this case, the subject won’t always be centered in the image like with
    Fashion MNIST, so we’ll have to rely on convolutions to spot distinguishing features.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN to Distinguish Between Horses and Humans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll explore a more complex scenario than the Fashion MNIST
    classifier. We’ll extend what we’ve learned about convolutions and CNNs to try
    to classify the contents of images in which the location of a feature isn’t always
    in the same place. I’ve created the “Horses or Humans” dataset for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: The “Horses or Humans” Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[The dataset for this section](https://oreil.ly/8VXwy) contains over a thousand
    300 × 300–pixel images. Approximately half the images are of horses, and the other
    half are of humans—and all are rendered in different poses. You can see some examples
    in [Figure 3-7](#ch03_figure_7_1748570891060112).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Horses and humans
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the subjects have different orientations and poses, and the
    image composition varies. Consider the two horses, for example—their heads are
    oriented differently, and one image is zoomed out (showing the complete animal),
    while the other is zoomed in (showing just the head and part of the body). Similarly,
    the humans are lit differently, have different skin tones, and are posed differently.
    The man has his hands on his hips, while the woman has hers outstretched. The
    images also contain backgrounds such as trees and beaches, so a classifier will
    have to determine which parts of the image are the important features that determine
    what makes a horse a horse and a human a human, without being affected by the
    background.
  prefs: []
  type: TYPE_NORMAL
- en: While the previous examples of predicting *y* = 2*x* – 1 or classifying small
    monochrome images of clothing *might* have been possible with traditional coding,
    it’s clear that this example is far more difficult and that you are crossing the
    line into where ML is essential to solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting side note is that these images are all computer generated. The
    theory is that features spotted in a CGI image of a horse should apply to a real
    image, and you’ll see how well this works later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Handling the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Fashion MNIST dataset that you’ve been using up to this point comes with
    labels, and every image file has an associated file with the label details. Many
    image-based datasets do not have this, and “Horses or Humans” is no exception.
    Instead of labels, the images are sorted into subdirectories of each type, and
    with the DataLoader in PyTorch, you can use this structure to *automatically*
    assign labels to images.
  prefs: []
  type: TYPE_NORMAL
- en: First, you simply need to ensure that your directory structure has a set of
    named subdirectories, with each subdirectory being a label. For example, the “Horses
    or Humans” dataset is available as a set of ZIP files, one of which contains the
    training data (1,000+ images) and another of which contains the validation data
    (256 images). When you download and unpack them into a local directory for training
    and validation, you need to ensure that they are in a file structure like the
    one in [Figure 3-8](#ch03_figure_8_1748570891060128).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code to get the training data and extract it into the appropriately
    named subdirectories, as shown in [Figure 3-8](#ch03_figure_8_1748570891060128):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/aiml_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Ensuring that images are in named subdirectories
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This code simply downloads the ZIP of the training data and unzips it into a
    directory at *horse-or-human/training*. (We’ll deal with downloading the validation
    data shortly.) This is the parent directory that will contain subdirectories for
    the image types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to use the `DataLoader`, we simply use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: First, we create an instance of a `transforms` object that we’ll call `transform`.
    This will determine the rules for how we modify the images. It resizes the image
    to 150 × 150 and then normalizes it into a tensor. Note that the raw images are
    actually 300 × 300, but to make training quicker for the purposes of learning,
    I’ve resized them to 150 × 150.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we specify the `dataset` objects to be `datasets.ImageFolder` types and
    point them to the required directory, and that will generate images for the training
    process by flowing them from that directory while applying the transform. The
    directory for training is `training_dir`, and the directory for validation is
    `validation_dir`, as specified earlier.
  prefs: []
  type: TYPE_NORMAL
- en: CNN Architecture for “Horses or Humans”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several major differences between this dataset and the Fashion MNIST
    one, and you have to take them into account when designing an architecture for
    classifying the images. First, the images are much larger—150 × 150 pixels—so
    more layers may be needed. Second, the images are in full color, not grayscale,
    so each image will have three channels instead of one. Third, there are only two
    image types, so we can actually classify them with only *one* output neuron. To
    do this, we’ll drive the value of that neuron toward 0 for one of the labels and
    toward 1 for the other. The `sigmoid` function is ideal for this process of driving
    the value to one of these extremes. You can see this at the bottom of the `forward`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of things to note here. First of all, take a look at the
    very first layer. We’re defining 16 filters, each of which has a `kernel_size`
    of 3, but the input shape is 3\. Remember that this is because our input image
    is in color: there are three channels, instead of just one for the monochrome
    Fashion MNIST dataset we were using earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: At the other end, notice that there’s only one neuron in the output layer. This
    is because we’re using a binary classifier, and we can get a binary classification
    with just a single neuron if we activate it with a sigmoid function. The purpose
    of the sigmoid function is to drive one set of values toward 0 and the other toward
    1, which is perfect for binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, notice how we stack several more convolutional layers. We do this because
    our image source is quite large and we want, over time, to have many smaller images,
    each with features highlighted. If we take a look at the results of a `summary`,
    we’ll see this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that by the time the data has gone through all the convolutional and pooling
    layers, it ends up as 18 × 18 items. The theory is that these will be activated
    feature maps that are relatively simple because they will contain just 324 pixels.
    We can then pass these feature maps to the dense neural network to match them
    to the appropriate labels.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, leads this network to have many more parameters than the previous
    network, so it will be slower to train. With this architecture, we’re going to
    learn over 10 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The code in this section, as well as in many other places in this book, may
    require you to import Python libraries. To find the correct imports, you can check
    out [the book’s repository](https://github.com/lmoroney/PyTorch-Book-FIles).
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the network, we’ll have to compile it with a loss function and an
    optimizer. In this case, the loss function can be the `BCELoss,` where `BCE` stands
    for *binary cross entropy*. As the name suggests, because there are only two classes
    in this scenario, this is a loss function that is designed for it. For the `optimizer`,
    we can continue using the same `Adam` that we used earlier. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: One thing to note is that the labels are converted to `floats` because of the
    binary cross entropy, where the value of the final output node will be a float
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Over just 15 epochs, this architecture gives us a very impressive 95%+ accuracy
    on the training set. Of course, this is just with the training data, and this
    performance isn’t an indication of the network’s potential performance on data
    that it hasn’t previously seen.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at adding the validation set and measuring its performance
    to give us a good indication of how this model might perform in real life.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Validation to the “Horses or Humans” Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To add validation, you’ll need a validation dataset that’s separate from the
    training one. In some cases, you’ll get a master dataset that you have to split
    yourself, but in the case of “Horses or Humans,” there’s a separate validation
    set that you can download. In the preceding code snippet, you’ve already downloaded
    the training and validation datasets, put them in directories, and set up data
    loaders for each of them. However, for training, you only used one of these datasets—the
    one that was set up to load the training data. So next, we’ll switch the model
    into evaluation mode and explore how well it did with the validation data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You may be wondering why we’re talking about a validation dataset here, rather
    than a test dataset, and whether the two are the same thing. For simple models
    like the ones developed in the previous chapters, it’s often sufficient to split
    the dataset into two parts: one for training and one for testing. But for more
    complex models like the one we’re building here, you’ll want to create separate
    validation and test sets.'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the difference? *Training data* is the data that is used to teach the
    network how the data and labels fit together, while *validation data* is used
    to see how the network is doing with previously unseen data *while* you are training
    (i.e., it isn’t used to fit data to labels but to inspect how well the fitting
    is going). Also, *test data* is used after training to evaluate how the network
    does with data it has never previously seen. Some datasets come with a three-way
    split, and in other cases, you’ll want to separate the test set into two parts
    for validation and testing. Here, you’ll download some additional images for testing
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: To download the validation set and unzip it into a different directory, you
    can use code that’s very similar to that used for the training images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, to perform the validation, you simply update your `train_model` method
    to perform a validation at the end of each training loop (or epoch) and report
    on the results. For example, you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: I added code here to do *both* the training and the validation and report on
    accuracy. Note that this is really just for learning purposes, so you can compare.
    In a real-world scenario, checking the accuracy of training data is a waste of
    processing time!
  prefs: []
  type: TYPE_NORMAL
- en: 'After training for 10 epochs, you should see that your model is 99%+ accurate
    on the training set but only about 88% on the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is an indication that the model is overfitting, which is something we also
    saw in the previous chapter.It’s easy to be lulled into a false sense of security
    by the 100% accuracy, but the other figure is more representative of how your
    model will behave in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Still, the performance isn’t bad, considering how few images it was trained
    on and how diverse those images were. You’re beginning to hit a wall caused by
    lack of data, but there are some techniques that you can use to improve your model’s
    performance. We’ll explore them later in this chapter, but before that, let’s
    take a look at how to *use* this model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing “Horses or Humans” Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s all very well and good to be able to build a model, but of course, you
    want to try it out. A major frustration of mine when I was starting my AI journey
    was that I could find lots of code that showed me how to build models and charts
    of how those models were performing, but very rarely was there code to help me
    kick the tires of the model myself to try it out. I’ll try to help you avoid that
    problem in this book!
  prefs: []
  type: TYPE_NORMAL
- en: Testing the model is perhaps easiest using Colab. I’ve provided a “Horses or
    Humans” notebook on GitHub that you can open directly in [Colab](http://bit.ly/horsehuman).
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve trained the model, you’ll see a section called “Running the Model.”
    Before running it, you should find a few pictures of horses or humans online and
    download them to your computer. I recommend you go to [Pixabay.com](http://pixabay.com),
    which is a really good site to check out for royalty-free images. It’s also a
    good idea to get your test images together first, because the node can time out
    while you’re searching.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-9](#ch03_figure_9_1748570891060144) shows a few pictures of horses
    and humans that I downloaded from Pixabay to test the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Test images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When they were uploaded, as you can see in [Figure 3-10](#ch03_figure_10_1748570891060159),
    the model correctly classified one image as a human and another as a horse—but
    despite the fact that the third image was obviously of a human, the model incorrectly
    classified it as a horse!
  prefs: []
  type: TYPE_NORMAL
- en: You can also upload multiple images simultaneously and have the model make predictions
    for all of them. You may also notice that it tends to overfit toward horses. If
    the human isn’t fully posed (i.e., if you can’t see their full body), the model
    can skew toward horses. That’s what happened in this case. The first human model
    is fully posed, and the image resembles many of the poses in the dataset, so the
    model was able to classify her correctly. On the other hand, the second human
    model is facing the camera, but only her upper half is in the image. There was
    no training data that looked like that, so the model couldn’t correctly identify
    her.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Executing the model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s now explore the code to see what it’s doing. Perhaps the most important
    part is this chunk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are loading the image from the path that Colab wrote it to. Note that
    we specify a `transform` to apply to the image. The images being uploaded can
    be any shape, but if we are going to feed them into the model, they *must* be
    the same size that the model was trained on. So, if we use the same `transform`
    that we defined when performing the training, we’ll know it’s in the same dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end is this strange command: `image = image.unsqueeze(0)`.'
  prefs: []
  type: TYPE_NORMAL
- en: When you look back at how the model was trained, the DataLoader objects batched
    the images going into it. If you think of an image as a 2D array of pixels, then
    the batch is an array of 2D arrays, which of course is then a 3D array.
  prefs: []
  type: TYPE_NORMAL
- en: But when we’re using this code with one image at a time, there’s no batch, so
    to make this a 3D array (which is technically a batch with one item in it), we
    can just unsqueeze the image along axis 0 to simulate this.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our image in the right format, it’s easy to do the classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The model then returns an array containing the classifications for the batch.
    Because there’s only one classification in this case, it’s effectively an array
    containing an array. You can see this back in [Figure 3-10](#ch03_figure_10_1748570891060159),
    where for the first human model, the array looks like `tensor([[2.1368e-05]],
    device='cuda:0').`
  prefs: []
  type: TYPE_NORMAL
- en: 'So now, it’s simply a matter of inspecting the value of the first element in
    that array. If it’s greater than 0.5, we’re looking at a human:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There are a few important points to consider here. First, even though the network
    was trained on synthetic, computer-generated imagery, it performs quite well at
    spotting horses and humans and differentiating them in real photographs. This
    is a potential boon in that you may not need thousands of photographs to train
    a model, and you can do it relatively cheaply with CGI.
  prefs: []
  type: TYPE_NORMAL
- en: But this dataset also demonstrates a fundamental issue you will face. Your training
    set cannot hope to represent *every* possible scenario your model might face in
    the wild, and thus, the model will always have some level of overspecialization
    toward the training set. We saw a clear and simple example of this earlier in
    this section, when the model mischaracterized the human in the center of [Figure 3-9](#ch03_figure_9_1748570891060144).
    The training set didn’t include a human in that pose, and thus, the model didn’t
    “learn” that a human could look like that. As a result, there was every chance
    it might see the figure as a horse, and in this case, it did.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the solution? The obvious one is to add more training data, with humans
    in that particular pose and others that weren’t initially represented. That isn’t
    always possible, though. Fortunately, there’s a neat trick in PyTorch that you
    can use to virtually extend your dataset—it’s called *image augmentation*, and
    we’ll explore that next.
  prefs: []
  type: TYPE_NORMAL
- en: Image Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, you built a horse-or-human classifier model that was
    trained on a relatively small dataset. As a result, you soon began to hit problems
    classifying some previously unseen images, such as the miscategorization of a
    woman as a horse because the training set didn’t include any images of people
    in that pose.
  prefs: []
  type: TYPE_NORMAL
- en: One way to deal with such problems is with *image augmentation*. The idea behind
    this technique is that as PyTorch is loading your data, it can create additional
    new data by amending what it has using a number of transforms. For example, take
    a look at [Figure 3-11](#fig-3-11). While there is nothing in the dataset that
    looks like the woman on the right, the image on the left is somewhat similar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Dataset similarities
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, if you could, for example, zoom into the image on the left as you are training,
    as shown in [Figure 3-12](#fig-3-12), you would increase the chances of the model
    being able to correctly classify the image on the right as a person.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Zooming in on the training set data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In a similar way, you can broaden the training set with a variety of other
    transformations, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Rotation (turning the image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting horizontally (moving the pixels horizontally with wrapping)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting vertically (moving the pixels vertically with wrapping)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shearing (moving the pixels either horizontally or vertically but offsetting
    so that the image would look like parallelogram)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zooming (magnifying a particular region)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flipping (vertically or horizontally)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because you’ve been using the `datasets.ImageFolder` and a `DataLoader` to
    load the images, you’ve seen the model do a transform already—when it normalized
    the images like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Many other transforms are easily available within the torchvision.transforms
    library, so, for example, you could do something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, in addition to rescaling the image to normalize it, you’re doing the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly flipping horizontally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly rotating up to 20 degrees left or right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly cropping a 150 × 150 window instead of resizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, the transforms.RandomAffine library gives you the facility to
    do all of these things, as well as adding stuff like scaling the image (zooming
    in or out), shearing the image, etc. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When you retrain with these parameters, one of the first things you’ll notice
    is that training takes longer because of all the image processing. Also, your
    model’s accuracy may not be as high as it was, because previously it was overfitting
    to a largely uniform set of data.
  prefs: []
  type: TYPE_NORMAL
- en: In my case, when I was training with these augmentations, my accuracy went down
    from 99% to 94% after 15 epochs, with validation much lower at 64%. This likely
    indicates overfitting in the model, but it warrants investigation by training
    with more epochs! One other thing to note is that random cropping might also be
    an issue—the CGI images generally center the subject, so random cropping will
    give partial subjects.
  prefs: []
  type: TYPE_NORMAL
- en: But what about the image from [Figure 3-9](#ch03_figure_9_1748570891060144)
    that the model misclassified earlier? This time, the model gets it right. Thanks
    to the image augmentations, the training set now has sufficient coverage for the
    model to understand that this particular image is a human too (see [Figure 3-13](#ch03_figure_11_1748570891060175)).
    This is just a single data point, and it may not be representative of the results
    for real data, but it’s a small step in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. The woman is now correctly classified
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, even with a relatively small dataset like “Horses or Humans,”
    you can start to build a pretty decent classifier. With larger datasets, you could
    take this further. Another way you can improve the model is by using features
    that the model has already learned elsewhere. Many researchers with massive resources
    (millions of images) and huge models that have been trained on thousands of classes
    have shared their models, and by using a concept called *transfer learning*, you
    can use the features those models learned and apply them to your data. We’ll explore
    that next!
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve already seen in this chapter, the use of convolutions to extract features
    can be a powerful tool for identifying the contents of an image. If we use this
    tool, we can then feed the resulting feature maps into the dense layers of a neural
    network to match them to the labels and give us a more accurate way of determining
    the contents of an image. Using this approach with a simple fast-to-train neural
    network and some image augmentation techniques, we built a model that was 80–90%
    accurate at distinguishing between a horse and a human when it was trained on
    a very small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can improve our model even further by using a method called *transfer
    learning*. The idea behind it is simple: instead of having our model learn a set
    of filters from scratch for our dataset, why not have it use a set of filters
    that were learned on a much larger dataset, with many more features than we can
    “afford” to build from scratch? We can place these filters in our network and
    then train a model with our data using the pre-learned filters. For example, while
    our “Horses or Humans” dataset has only two classes, we can use an existing model
    that has been pretrained for one thousand classes—but at some point, we’ll have
    to throw away some of the preexisting network and add the layers that will let
    us have a classifier for two classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-14](#ch03_figure_12_1748570891060190) shows what a CNN architecture
    for a classification task like ours might look like. We have a series of convolutional
    layers that lead to a dense layer, which in turn leads to an output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. A CNN architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ve seen that we can build a pretty good classifier using this architecture.
    But what if we could use transfer learning to take the pre-learned layers from
    another model, freeze or lock them so that they aren’t trainable, and then put
    them on top of our model, like in [Figure 3-15](#ch03_figure_13_1748570891060207)?
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15\. Taking and locking layers from another architecture via transfer
    learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When we consider that once they’ve been trained, all these layers are just a
    set of numbers indicating the filter values, weights, and biases along with a
    known architecture (the number of filters per layer, the size of the filter, etc.),
    the idea of reusing them is pretty straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how this would appear in code. There are several pretrained models
    already available from a variety of sources, so we’ll use version 3 of the popular
    Inception model from Google, which is trained on more than a million images from
    a database called ImageNet. Inception has dozens of layers, and it can classify
    images into one thousand categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'The torchvision.models library contains a number of models, including Inception
    V3, so we can easily get access to the pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a full Inception model that’s pretrained. If you want to inspect
    its architecture, you can do so with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Be warned—this model is huge! Still, you should take a look through it to see
    the layers and their names. I like to use the one called `Mixed7_c` because its
    output is nice and small—it consists of 8 × 8 images—but you should feel free
    to experiment with others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll freeze the entire network from retraining and then set a variable
    to point to `mixed7`’s output as where we want to crop the network. We can do
    that with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You’ll notice that we’re printing the output shape of the last layer, and you’ll
    also see that we’re getting 8 × 8 images at this point. This indicates that by
    the time the images have been fed through to `Mixed_7c`, the output images from
    the filters are 8 × 8 in size, so they’re pretty easy to manage. Again, you don’t
    have to choose that specific layer; you’re welcome to experiment with others.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how to modify the model for transfer learning. It’s pretty straightforward—if
    you go back to the output from the custom `print_model_summary` from a moment
    ago, you’ll see that the *last* layer in the model is called `fc`. As you might
    expect, *fc* stands for *fully connected*, which is effectively a Linear layer
    with our densely connected neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'So now, it becomes as simple as replacing that layer with a new layer called
    `fc`. We don’t need to *know* the input shape for it ahead of time—we can inspect
    its `in_features` property to find that. So now, to create a new layer of 1,024
    neurons that outputs to another layer of two neurons and replace the `fc` from
    Inception, all we have to do is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It’s as simple as creating a new set of Linear layers from the last output,
    because we’ll be feeding the results into a dense layer. So, we then add a Linear
    layer of 1,024 neurons and a dense layer with two neurons for our output. Also,
    you’ve probably noticed that in the previous model, we did it with one neuron
    and used sigmoid activation for the two classes—so you’re probably wondering why
    we’re going to two neurons in the output layer now. This was primarily a stylistic
    choice. Inception was designed for *n* neurons to output for *n* classes, and
    I wanted to keep that approach.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model on this architecture over only three epochs gave us an accuracy
    of 99%+, with a validation accuracy of 95%+. Clearly, that’s a vast improvement.
    Also, remember that Inception learned a massive set of features that it could
    use to classify the many classes it was trained on. It turns out that that feature
    set is also incredibly useful for learning how to classify any other images—not
    least, those from “Horses or Humans.”
  prefs: []
  type: TYPE_NORMAL
- en: The results we got from this model are much better than those we got from our
    previous model, but you can continue to tweak and improve it. You can also explore
    how the model will work with a much larger dataset, like the famous “[Dogs vs.
    Cats”](https://oreil.ly/UhWMk) from Kaggle. It’s an extremely varied dataset consisting
    of 25,000 images of cats and dogs, often with the subjects somewhat obscured—for
    example, if they are held by a human.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same algorithm and model design as before, you can train a “Dogs vs.
    Cats” classifier on Colab, using a GPU at about 3 minutes per epoch.
  prefs: []
  type: TYPE_NORMAL
- en: When I tested with very complex pictures like those in [Figure 3-16](#ch03_figure_14_1748570891060222),
    this classifier got them all correct. I chose one picture of a dog with catlike
    ears and one with its back turned. Both pictures of cats were nontypical.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0316.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. Unusual dogs and cats that the model classified correctly
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To parse the results, you can use code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note the lines where I’m printing the output of the image, calculating the prediction
    from that, and printing that.
  prefs: []
  type: TYPE_NORMAL
- en: When you upload some images to Colab, you can see how they predict in [Figure 3-17](#ch03_figure_15_1748570891060236).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0317.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. Classifying the cat washing its paw
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first image uploaded was “labrador,” which, as its name suggests, is of
    a dog. The tensor returned from the model contained [–14.9642, 18.3943], meaning
    a very low number for the first label and a very high one for the second. Given
    that we used an image directory when training, the labels ended up being in alphabetical
    order, so it was low for cat and high for dog. Then, when we called `torch.max`,
    it gave us [1]. That indicates that neuron 1 is the one for this classification—thus,
    the image is a dog.
  prefs: []
  type: TYPE_NORMAL
- en: The second image had [5.3486, –4.8260], with the first neuron being higher.
    Thus, it detected a cat. The size of these numbers indicates the strength of the
    prediction. For example, it was much surer that the first image is a dog than
    it was that the second image is a cat.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete code for the “Horses or Humans” and “Dogs vs. Cats”
    classifiers in the book’s [GitHub repository](https://github.com/lmoroney/tfbook).
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In all of the examples so far, you’ve been building *binary* classifiers—ones
    that choose between two options (horses or humans, cats or dogs). On the other
    hand, when you’re building *multiclass classifiers*, the models are almost the
    same but there are a few important differences. Instead of a single neuron that
    is sigmoid activated or two neurons that are binary activated, your output layer
    will now require *n* neurons, where *n* is the number of classes you want to classify.
    You’ll also have to change your loss function to an appropriate one for multiple
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: A neat feature of the `nn.CrossEntropyLoss` loss function in PyTorch is that
    it can handle multiple categories, so the “Cats vs. Dogs” and “Horses or Humans”
    transfer learning classifiers you’ve built thus far in this chapter can use it
    without modification. But the “Horses or Humans” classifier that you built at
    the beginning with a *single* output neuron will not be able to because it can’t
    handle more than two classes. This is always something to look out for, and it’s
    a common bug when you start writing code for classification.
  prefs: []
  type: TYPE_NORMAL
- en: To go beyond two-class classification, consider, for example, the game Rock,
    Paper, Scissors. If you wanted to train a dataset to recognize the different hand
    gestures used in this game, you’d need to handle three categories. Fortunately,
    there’s a [simple dataset](https://oreil.ly/VHhmS) you can use for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two downloads: a training set of many diverse hands, with different
    sizes, shapes, colors, and details such as nail polish; and a testing set of equally
    diverse hands, none of which are in the training set. You can see some examples
    in [Figure 3-18](#ch03_figure_16_1748570891060251).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Examples of Rock/Paper/Scissors gestures](assets/aiml_0318.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. Examples of Rock, Paper, Scissors gestures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using the dataset is simple. You can download and unzip it—the sorted subdirectories
    are already present in the ZIP file—and then use it to initialize an `ImageFolder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to use a `transform` that fits the input shape of your model. In the
    last few examples, we were using Inception, and it’s 299 × 299.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the ImageFolder for your DataLoader in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Earlier, when we tweaked the Inception model for “Horses or Humans” or “Cats
    vs. Dogs,” there were only *two* classes and thus *two* output neurons. Given
    that this data has *three* classes, we need to be sure that we change the new
    fully connected layer at the bottom accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, training the model works as before: you specify the loss function and
    optimizer, and you call the `train_model()` function. For good repetition, this
    function is the same as the one used in the “Horses or Humans” and “Cats vs. Dogs”
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Your code for testing predictions will also need to change somewhat. There are
    now three output neurons, and they will output a high value for the predicted
    class and lower values for the other classes.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that when you’re using the `ImageFolder`, the classes are loaded in
    alphabetical order—so while you might expect the output neurons to be in the order
    of the name of the game, the order will in fact be Paper, Rock, Scissors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code that you can use to try out predictions in a Colab notebook will look
    like the following. It’s very similar to what you saw earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that it doesn’t parse the output; it just prints the classes. [Figure 3-19](#ch03_figure_17_1748570891060264)
    shows what it looks like in actual use.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0319.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-19\. Testing the Rock, Paper, Scissors classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see from the filenames what the images were.
  prefs: []
  type: TYPE_NORMAL
- en: If you explore this a little deeper, you can see that the file named *scissors4.png*
    had an output of –2.5582, –1.7362, 3.8465]. The largest number is the third one,
    and if you think alphabetically, you can see that the third neuron represents
    scissors, so it was classified correctly. Similar results were achieved for the
    other files.
  prefs: []
  type: TYPE_NORMAL
- en: Some images that you can use to test the dataset [are available to download](https://oreil.ly/dEUpx).
    Alternatively, of course, you can try your own. Note that the training images
    are all done against a plain white background, though, so there may be some confusion
    if there is a lot of detail in the background of the photos you take.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, we discussed overfitting, in which a network may become
    too specialized in a particular type of input data and thus fare poorly on others.
    One technique to help overcome this is use of *dropout regularization*.
  prefs: []
  type: TYPE_NORMAL
- en: When a neural network is being trained, each individual neuron will have an
    effect on neurons in subsequent layers. Over time, particularly in larger networks,
    some neurons can become overspecialized—and that feeds downstream, potentially
    causing the network as a whole to become overspecialized and thus leading to overfitting.
    Additionally, neighboring neurons can end up with similar weights and biases,
    and if not monitored, this condition can lead the overall model to become overspecialized
    on the features activated by those neurons.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the neural network in [Figure 3-20](#ch03_figure_18_1748570891060278),
    in which there are layers of 2, 6, 6, and 2 neurons. The neurons in the middle
    layers might end up with very similar weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0320.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-20\. A simple neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While training, if you remove a random number of neurons and ignore them, then
    their contribution to the neurons in the next layer is temporarily blocked (see
    [Figure 3-21](#ch03_figure_19_1748570891060292)). They are effectively dropped
    out, leading to the term *dropout regularization*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0321.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-21\. A neural network with dropouts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This reduces the chances of the neurons becoming overspecialized. The network
    will still learn the same number of parameters, but it should be better at generalization—that
    is, it should be more resilient to different inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The concept of dropouts was proposed by Nitish Srivastava et al. in their 2014
    paper “[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://oreil.ly/673CJ).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement dropouts in PyTorch, you can just use a simple layer like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This will drop out, at random, the specified percentage of neurons (here, 50%)
    in the specified layer. Note that it may take some experimentation to find the
    correct percentage for your network.
  prefs: []
  type: TYPE_NORMAL
- en: For a simple example that demonstrates this, consider the new fully connected
    layers we added to the bottom of Inception with the transfer learning example
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here it is for Rock, Paper, Scissors with three output neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'With dropout added, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The examples that we used in this chapter for transfer learning are already
    learning really well without the use of dropouts. However, I’d recommend that
    you always consider dropouts when building your models because they can greatly
    reduce waste in the ML process—letting your model learn just as well but much
    faster!
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, as you design your neural networks, keep in mind that getting
    great results on your training set is not always a good thing because it could
    be a sign of overfitting. Introducing dropouts can help you remove that problem
    so that you can optimize your network in other areas without that false sense
    of security.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced you to a more advanced way of achieving computer vision
    by using convolutional neural networks. You saw how to use convolutions to apply
    filters that can extract features from images, and you designed your first neural
    networks to deal with more complex vision scenarios than those you encountered
    with the MNIST and Fashion MNIST datasets. You also explored techniques to improve
    your network’s accuracy and avoid overfitting, such as the use of image augmentation
    and dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: Before we explore further scenarios, in [Chapter 4](ch04.html#ch04_using_data_with_pytorch_1748548966496246),
    you’ll get an introduction to PyTorch data, which is a technology that makes it
    much easier for you to get access to data for training and testing your networks.
    In this chapter, you downloaded ZIP files and extracted images, but that’s not
    always going to be possible. With PyTorch datasets, you’ll be able to access lots
    of datasets with a standard API.
  prefs: []
  type: TYPE_NORMAL
