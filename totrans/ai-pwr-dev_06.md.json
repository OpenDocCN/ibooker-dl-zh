["```py\n-- Question: What is the command to run a docker container, running PostgreSQL for a database called itam_db. I would like to specify the password for this database.\n```", "```py\n--Answer: docker run --name itam_db \n    -e POSTGRES_PASSWORD=postgres -d \n    -p 5432:5432 postgres\n```", "```py\n-- create a schema called itam\ncreate schema itam;\n```", "```py\n-- create two users called itam_user and itam_admin. itam_user should be able to read from and write to the tables in the itam schema. itam_admin should be able to read from and write to the tables in the itam schema and create and drop tables in the itam schema.\ncreate user itam_user with password 'itam_user';\ncreate user itam_admin with password 'itam_admin';\ngrant usage on schema itam to itam_user;\ngrant usage on schema itam to itam_admin;\ngrant select, insert, update, delete on all tables in schema itam to itam_user;\ngrant select, insert, update, delete on all tables in schema itam to itam_admin;\ngrant usage, drop on schema itam to itam_admin;\n```", "```py\n-- grant ownership of the itam schema to itam_admin\nalter schema itam owner to itam_admin;\n```", "```py\n-- create a table called depreciation_strategy in the itam schema. the table should have the following columns: id (int), name (varchar), and description (varchar). the table should have a primary key on id.\n-- id needs to be in quotes because it is a reserved word in postgresql\n-- there are two values for depreciation_strategy: straight line and double declining balance\ncreate table itam.depreciation_strategy (\n    \"id\" int primary key,\n    \"name\" varchar,\n    \"description\" varchar\n);\n```", "```py\n-- create a sequence called depreciation_strategy_seq, which should start at 1 and increment by 1 and should be used as the primary key for the depreciation_strategy table.\ncreate sequence itam.depreciation_strategy_seq start 1 increment 1;\n```", "```py\n-- question: how do I make the sequence the primary key for the depreciation_strategy table?\n\n-- answer: use the following command\nalter table itam.depreciation_strategy \n    alter column \"id\" \n    set default\n    nextval('itam.depreciation_strategy_seq'\n            ::regclass);\n```", "```py\ninsert into depreciation_strategy (id, name, description) \n    values (1, 'straight line', \n    'straight line');\n\ninsert into depreciation_strategy (id, name, description) \n    values (2, 'double declining balance', \n    'double declining balance');\n```", "```py\n-- create a table called funding_details in the itam schema. the table should have the following columns: id (int), name (varchar),depreciation_strategy_id (int) and depreciation_rate (float). the table should have a primary key on id.\n-- depreciation_strategy_id is a foreign key to the depreciation_strategy table.\n-- id needs to be in quotes because it is a reserved word in postgresql\ncreate table itam.funding_details (\n    \"id\" int primary key,\n    \"name\" varchar,\n    \"depreciation_strategy_id\" int,\n    \"depreciation_rate\" float\n);\n\n-- create a sequence called funding_details_seq, which should start at 1 and increment by 1 and should be used as the primary key for the funding_details table.\ncreate sequence itam.funding_details_seq start 1 increment 1;\nalter table itam.funding_details \nalter column \"id\" \nset default \nnextval('itam.funding_details_seq'\n    ::regclass);\n```", "```py\n-- create a table called assets in the itam schema. the table should have the following columns: \n-- id (int), name (varchar), status (varchar), category (varchar), cost (float), useful_life (int), salvage_value (float), purchase_date (date), funding_details_id (int). The table should have a primary key on id and a foreign key on funding_details_id.\n-- id needs to be in quotes because it is a reserved word in postgresql\n-- the table should have a sequence called assets_id_seq, which should start at 1 and increment by 1 and should be used as the primary key for the assets table.\ncreate table itam.assets (\n    \"id\" int primary key,\n    \"name\" varchar,\n    \"status\" varchar,\n    \"category\" varchar,\n    \"cost\" float,\n    \"useful_life\" int,\n    \"salvage_value\" float,\n    \"purchase_date\" date,\n    \"funding_details_id\" int\n);\n\n-- create a sequence called assets_seq, which should start at 1 and increment by 1 and should be used as the primary key for the assets table.\ncreate sequence itam.assets_seq start 1 increment 1;\n\nalter table itam.assets alter column \"id\" \nset default \nnextval('itam.assets_seq'::\n    regclass);\n```", "```py\n-- Generate a dataset of assets for an ITAM system. The dataset should include the following columns: id (int), name (varchar), status (varchar), category (varchar), cost (float), useful_life (int), salvage_value (float), purchase_date (date), funding_details_id (int). The dataset should have 1000 rows, sorted by id. Each row should have the following characteristics:\n-- - id should be a unique integer and sequential starting at 1.\n-- - name should be a random string of characters between 1 and 50 characters long.\n-- - status should be a random selection from the following valid asset statuses: in use, in storage, disposed of, in repair, in transit, other.\n-- - category should be a random selection from the following valid categories: hardware, software, other.\n-- - cost should be a random float between 0 and 100000.\n-- - useful_life should be a random int between 1 and 10.\n-- - salvage_value should be a random float greater than 0 but less than the cost of the asset.\n-- - purchase_date should be a random date between 1/1/2019 and 12/31/2022.\n-- - funding_details_id should be a random integer either 1 or 2.\n-- The dataset should be saved as a CSV file named assets.csv in the data directory. The file should have a header row and the columns should have the following data types: id (int), name (varchar), status (varchar), category (varchar), cost (float), useful_life (float), salvage_value (float), funding_details_id (int)\n```", "```py\ninsert into itam.assets (id, name, status, \n    category, cost, useful_life, salvage_value, \n    purchase_date, funding_details_id)\nselect\n    id,\n    name,\n    status,\n    category,\n    cost,\n    useful_life,\n    salvage_value,\n    purchase_date,\n    funding_details_id\nfrom (\n    select\n        row_number() over (order by random()) as id,\n        md5(random()::text) as name,\n        case\n            when random() < 0.2 then 'in use'\n            when random() < 0.4 then 'in storage'\n            when random() < 0.6 then 'disposed of'\n            when random() < 0.8 then 'in repair'\n            when random() < 0.9 then 'in transit'\n            else 'other'\n        end as status,\n        case\n            when random() < 0.5 then 'hardware'\n            when random() < 0.9 then 'software'\n            else 'other'\n        end as category,\n        random() * 100000 as cost,\n        (random() * 100)::int as useful_life,\n        random() * (random() * 100000) as salvage_value,\n        -- generate a random date between 1/1/2019 and 12/31/2022\n        -- this does not work please fix\n        -- '2019-01-01'::date + random() * \n            ('2022-12-31'::date - '2019-01-01'::date) \n            as purchase_date,\n        '2019-01-01'::date + (random() * \n          (DATE '2022-12-31' - DATE '2019-01-01')\n          ::integer)::integer as purchase_date\n        case\n            when random() < 0.5 then 1\n            else 2\n        end as funding_details_id\n    from generate_series(1, 1000)\n) as assets;\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport random\nfrom faker import Faker\n\n# Create Faker object\nfake = Faker()\n\n# Define our valid statuses, categories, funding_details and depreciation_strategies\nvalid_statuses = ['in use', 'in storage', \n'disposed', 'in repair', 'in transit', 'other']\nvalid_categories = ['hardware']\nvalid_funding_details = ['purchased']\nvalid_depreciation_strategies = ['straight line']\n\n# Generate the data\ndata = {\n    'id': list(range(1, 1001)),\n    'name': [fake.first_name() for _ in range(1000)],\n    'status': [random.choice(valid_statuses) for _ in range(1000)],\n    'category': [random.choice(valid_categories) for _ in range(1000)],\n    'cost': np.random.uniform(0, 100000, 1000),\n    'useful_life': np.random.uniform(1, 10, 1000),\n    'salvage_value': np.random.uniform(0, 10000, 1000),\n    'funding_details': [random.choice(valid_funding_details)\n for _ in range(1000)],\n    'depreciation_strategy': [random.choice(\n    valid_depreciation_strategies) \n    for _ in range(1000)],\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Ensure the DataFrame is sorted by id\ndf = df.sort_values('id')\n\n# Show the DataFrame\nprint(df)\n```", "```py\n-- Generate a dataset of funding details for an ITAM system. The dataset should include the following columns: id (int), name (varchar), depreciation_strategy_id (int), depreciation_rate (float). The dataset should have 1000 rows, sorted by id. \n-- Each row should have the following characteristics:\n-- - id should be a unique integer and sequential starting at 1.\n-- - name should be a random string of characters between 1 and 10 characters long.\n-- - depreciation_strategy_id should be a random integer either 1 or 2.\n-- - depreciation_rate should be a random float between 0 and .4.\n\ninsert into itam.funding_details (id, name, \n    depreciation_strategy_id, depreciation_rate)\nselect\n    id,\n    name,\n    depreciation_strategy_id,\n    depreciation_rate\nfrom (\n    select\n        row_number() over (order by random()) as id,\n        md5(random()::text) as name,\n        case\n            when random() < 0.5 then 1\n            else 2\n        end as depreciation_strategy_id,\n        random() * 0.4 as depreciation_rate\n    from generate_series(1, 1000)\n) as funding_details;\n```", "```py\nBase  = declarative_base()\n\nclass FundingDetailsModel(Base):\n    __tablename__ = 'funding_details'\n    id = Column(Integer, primary_key=True)\n    depreciation_rate = Column(Float)\n    depreciation_strategy_id = Column(Integer)\n\n    def get_depreciation_strategy(self) -> DepreciationStrategy:\n        if self.depreciation_strategy_id is 1:\n            return StraightLineDepreciationStrategy()\n        else:\n            return DoubleDecliningDepreciationStrategy()\n\nclass AssetModel(Base):\n    __tablename__ = 'assets'\n    id = Column(Integer, primary_key=True)\n    name = Column(String)\n    status = Column(String)\n    category = Column(String)\n    cost = Column(Float)\n    useful_life = Column(Float)\n    salvage_value = Column(Float)\n    purchase_date = Column(Date)\n    funding_details_id = Column(Integer, ForeignKey('funding_details.id'))\n    funding_details = relationship('FundingDetailsModel')\n```", "```py\nversion: '2.1'\n\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.3.2\n    container_name: zookeeper\n    ports:\n      - \"2181:2181\"\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_SERVER_ID: 1\n      ZOOKEEPER_SERVERS: zoo1:2888:3888\n\n  kafka:\n    image: confluentinc/cp-kafka:7.3.2\n    hostname: kafka\n    container_name: kafka\n    ports:\n      - \"9092:9092\"\n      - \"29092:29092\"\n      - \"9999:9999\"\n    environment:\n      KAFKA_ADVERTISED_LISTENERS: \n            INTERNAL://kafka:19092,EXTERNAL://\n            ${DOCKER_HOST_IP:127.0.0.1}:9092,\n            DOCKER://host.docker.internal:29092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,\nDOCKER:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL\n      KAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"\n      KAFKA_BROKER_ID: 1\n      KAFKA_LOG4J_LOGGERS: \"kafka.controller=\n            INFO,kafka.producer.async\n            .DefaultEventHandler=INFO,\n            state.change.logger=INFO\"\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      KAFKA_JMX_PORT: 9999\n      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}\n      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer\n      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: \"true\"\n    depends_on:\n      - zookeeper\n```", "```py\nfrom kafka import KafkaConsumer\nfrom asset_manager import AssetManager\n\nconsumer = KafkaConsumer(\n    'asset_location',\n    bootstrap_servers=['localhost:9092'],\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    group_id='my-group',\n    value_deserializer=lambda x: \n            tuple(map(float, x.decode('utf-8')\n            .split(',')))\n)\n\nasset_manager = AssetManager()\n\nfor message in consumer:\n    asset_id, latitude, longitude = message.value\n    asset_manager.update_asset_location(asset_id, (latitude, longitude))\n```", "```py\nclass AssetLocationMediator:\n    def __init__(self):\n        self.handlers = {}\n\n    def register_handler(self, event_type, handler):\n        if event_type not in self.handlers:\n            self.handlers[event_type] = []\n        self.handlers[event_type].append(handler)\n\n    def publish(self, event):\n        event_type = type(event)\n        if event_type in self.handlers:\n            for handler in self.handlers[event_type]:\n                handler(event)\n```", "```py\nfrom kafka import KafkaConsumer\nfrom itam.domain.events.asset_location_updated import AssetLocationUpdated\nimport json\n\nclass AssetLocationKafkaConsumer:\n    def __init__(self, mediator):\n        self.mediator = mediator\n\n        self.consumer = KafkaConsumer(\n            'asset_location',\n            bootstrap_servers=['localhost:9092'],\n            enable_auto_commit=True,\n            group_id='itam-group',\n            value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n        )\n\n    def poll(self):\n        print(\"Polling for asset location updates\")\n        for message in self.consumer:\n            asset_id = message.value['asset_id']\n            latitude = message.value['latitude']\n            longitude = message.value['longitude']\n            timestamp = message.value['timestamp']\n            event = AssetLocationUpdated(asset_id, latitude, longitude, timestamp)\n            self.mediator.publish(event)\n```", "```py\nfrom itam.infrastructure.mediators.asset_location_mediator import \n\nclass AssetManager:\n    def __init__(self, base_repository: \n            BaseRepository[Asset], \n            mediator: AssetLocationMediator):\n        self._repository = base_repository\n        self.mediator = mediator\n        self.mediator.register_handler(\n            AssetLocationUpdated, \n            self.update_asset_location)\n\n    def update_asset_location(self, event: AssetLocationUpdated) -> None:\n        asset = self.read(event.asset_id)\n        asset.add_location(event.latitude, \n            event.longitude, event.timestamp)\n        #self.update(asset)\n        print(f\"Asset {asset.id} location updated \n            to {event.latitude}, {event.longitude} \n            at {event.timestamp}\")\n```", "```py\ndocker exec -it kafka /bin/bash\n```", "```py\nkafka-topics --list --bootstrap-server localhost:9092\n```", "```py\nkafka-topics --create --bootstrap-server localhost:9092 \n    --replication-factor 1 \n    --partitions 1 \n    --topic asset_location\n```", "```py\nkafka-console-producer --broker-list localhost:9092 --topic asset_location\n```", "```py\n{\"asset_id\": 1, \"latitude\": 41.8781, \"longitude\": -87.6298, \n    \"timestamp\": \"2022-01-01T00:00:00Z\"}\n{\"asset_id\": 1, \"latitude\": 41.9000, \"longitude\": -87.6244, \n    \"timestamp\": \"2022-01-01T00:10:00Z\"}\n{\"asset_id\": 1, \"latitude\": 41.8676, \"longitude\": -87.6270, \n    \"timestamp\": \"2022-01-01T00:20:00Z\"}\n{\"asset_id\": 1, \"latitude\": 41.8788, \"longitude\": -87.6359, \n    \"timestamp\": \"2022-01-01T00:30:00Z\"}\n{\"asset_id\": 1, \"latitude\": 41.8740, \"longitude\": -87.6298, \"timestamp\": \"2022-01-01T00:40:00Z\"}\n```", "```py\nkafka-topics --delete --topic asset_location --bootstrap-server \nlocalhost:9092\n```", "```py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col, expr\nfrom pyspark.sql.types import StructType, \n    StructField, IntegerType, DoubleType, \n    TimestampType\nfrom geopy.distance import distance\n```", "```py\nclass AssetLocationSparkAdapter:\n    def __init__(self):\n        # Define the schema for the incoming JSON data\n        self.schema = StructType([\n            StructField(\"asset_id\", IntegerType()),\n            StructField(\"latitude\", DoubleType()),\n            StructField(\"longitude\", DoubleType()),\n            StructField(\"timestamp\", TimestampType())\n        ])\n\n        # Create a SparkSession\n        self.spark = SparkSession.builder \\\n            .appName(\"AssetLocationSparkAdapter\") \\\n            .getOrCreate()\n\n        # Create a streaming DataFrame from the asset_location topic\n        self.df = self.spark \\\n            .readStream \\\n            .format(\"kafka\") \\\n            .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n            .option(\"subscribe\", \"asset_location\") \\\n            .option(\"startingOffsets\", \"earliest\") \\\n            .load() \\\n            .selectExpr(\"CAST(value AS STRING)\")\n\n        # Parse the incoming JSON data\n        self.parsed_stream = self.df \\\n            .select(from_json(col(\"value\"), self.schema).alias(\"data\")) \\\n            .select(\"data.*\")\n```", "```py\n        # Calculate the distance between the current location and Chicago for each asset\n        self.distance = self.parsed_stream \\\n            .withColumn(\"distance\", \n            expr(\"calculate_distance(latitude, \n            longitude, 41.8781, -87.6298)\")) \\\n            .select(col(\"asset_id\"), col(\"timestamp\"), col(\"distance\")) \\\n            .filter(col(\"distance\") > 25)\n\n        # Write the results to the console\n        self.query = self.distance \\\n            .writeStream \\\n            .outputMode(\"append\") \\\n            .format(\"console\") \\\n            .start()\n\n    def run(self):\n        # Start the streaming query\n        self.query.awaitTermination()\n\n    def stop(self):\n        # Stop the streaming query and SparkSession\n        self.query.stop()\n        self.spark.stop()\n```", "```py\ndef calculate_distance(lat1, lon1, lat2, lon2):\n    return distance((lat1, lon1), (lat2, lon2)).miles\n```", "```py\nos.environ['PYSPARK_SUBMIT_ARGS'] = \n    '--packages org.apache.spark:\n         spark-streaming-kafka-0-10_2.12:3.2.0,\n         org.apache.spark:\n         spark-sql-kafka-0-10_2.12:3.2.0 \n         pyspark-shell'\n\nclass AssetLocationSparkAdapter:\n    def __init__(self):\n        # Create a SparkSession\n        self.spark = SparkSession.builder \\\n            .appName(\"AssetLocationSparkAdapter\") \\\n            .getOrCreate()\n         self.spark.udf.register(\"calculate_distance\", calculate_distance)\n```", "```py\nif __name__ == \"__main__\":\n    adapter = AssetLocationSparkAdapter()\n    adapter.run()\n```", "```py\n+--------+-------------------+------------------+\n|asset_id|          timestamp|          distance|\n+--------+-------------------+------------------+\n|       1|2021-12-31 20:30:00| 712.8314662207446|\n+--------+-------------------+------------------+\n```"]