- en: 3 Selecting and evaluating AI tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distinctions among different types of AI, or ways of using AI, and how to select
    the most appropriate one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to assess AI’s performance and select models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common ways to measure AI’s performance at a task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter provides guidance on selecting an AI model or tool and assessing
    its performance at a given task. We kick off by discussing three common distinctions
    between different types of AI: proprietary versus open source AI, off-the-shelf
    versus fine-tuned AI, and AI apps versus foundation models. We explain what these
    mean and how to pick the most suitable type. Afterward, we discuss a common process
    to assess AI’s performance, which uses different datasets for validation and testing.
    We also discuss some common performance measures such as accuracy. The appendix
    includes a catalog of popular generative AI tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Proprietary vs. open source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In proprietary AI, the user isn’t allowed to modify or even see the code that
    powers the underlying ML models. The inner workings of the technology are kept
    secret to prevent others from copying it. One common way of using proprietary
    AI is through customer-facing apps such as ChatGPT. These tend to charge users
    a monthly subscription to access the service, although some provide a free tier
    that grants access to a reduced number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Another common way of using proprietary AI is via APIs. These let users interact
    with AI programmatically to build apps that utilize it. The AI software runs on
    a remote server behind closed doors, so the user can’t see the code. APIs are
    typically billed based on usage (e.g., the number of input and output tokens).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, in open source AI, the provider publicly discloses the internal
    details of the ML model, including the code to use it and the values of all the
    model’s parameters. The user is often authorized to modify or customize the model
    if needed. In addition, users can self-host these models using their own infrastructure;
    for example, you can download a copy of the model to your local computer or your
    own cloud computing instance and run the code yourself. This doesn’t mean you
    *must* self-host the model as it may also be available through APIs, but you have
    the option to self-host it. An example of open source AI is the family of Llama
    models produced by Meta, which are openly available for download on multiple websites.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open source AI is sometimes not quite as open as it may sound. For starters,
    their manufacturers don’t disclose the data used to train these models. So, while
    you can see the parameters of the final model, you’d be unable to train that exact
    model yourself as you wouldn’t know which data to use. Mistral AI, a company that
    provides open source AI, explains ([https://mng.bz/rKQy](https://mng.bz/rKQy)):'
  prefs: []
  type: TYPE_NORMAL
- en: We do not communicate on our training datasets. We keep proprietary some intermediary
    assets (code and resources) required to produce both the Open-Source models and
    the Optimized models. Among others, this involves the training logic for models,
    and the datasets used in training.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that, just like proprietary models, open source models are improved (or
    aligned) by using reinforcement learning with human feedback (see chapter 1).
    This is performed using data created manually by human labelers, which remains
    undisclosed in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: The licenses to use open source AI often come with restrictions. For example,
    you are not allowed to use a Llama model—even your own copy—for an app with more
    than 700 million monthly users (see [https://mng.bz/VVoG](https://mng.bz/VVoG)).
    In that case, you would have to discuss licensing options with Meta, and you may
    be asked to pay. Moreover, you’re not allowed to use a Llama model or its outputs
    to improve other LLMs; in other words, you can’t use Llama to build products that
    compete with it.
  prefs: []
  type: TYPE_NORMAL
- en: Building large ML models is expensive, so the most powerful open source AI is
    built by for-profit companies that charge or intend to charge for services. These
    services often include consulting or access to premium, proprietary models.
  prefs: []
  type: TYPE_NORMAL
- en: How to decide
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proprietary AI is most suitable when you need a done-for-you solution. Using
    proprietary AI doesn’t usually require specialized knowledge, such as machine
    learning, coding, and DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main reasons to use open source AI is to be able to self-host it
    (run it on your own servers), which can provide better transparency and governance,
    as you have full visibility over the code and full control over which data exits
    the organization. Your company may not want to send any sensitive data to a third
    party, such as OpenAI, or it may want to audit the code to ensure it doesn’t do
    anything it’s not supposed to.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of self-hosting AI, however, tends to be higher than paying for APIs,
    as you need to maintain the required infrastructure, so it is usually not cost-effective
    unless done at a very large scale. You also need to be very careful—malicious
    open source models have been published in the past that executed unintended code
    in the user’s machine (see [https://mng.bz/xKeX](https://mng.bz/xKeX)).
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to use open source AI is customization. If you want to modify
    a model (e.g., by fine-tuning it, which is covered in the next section), then
    open source AI lets you do so most freely. Table 3.1 summarizes the best uses
    of proprietary and open source AI.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1  Proprietary vs. open source AI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Proprietary AIBest for . . . | Open source AIBest for . . . |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| • Done-for-you solution• Easy start• No specialized knowledge required• Small-scale
    use, in which pay-as-you-go AI is cheaper than maintaining your own infrastructure
    | • Self-hosting so that you enjoy better governance and transparency• Large-scale
    use, in which maintaining your own infrastructure is cheaper than pay-as-you-go
    AI• Model customization (e.g., fine-tuning) |'
  prefs: []
  type: TYPE_TB
- en: In terms of the quality of outputs, proprietary AI used to hold an edge over
    open source AI. However, the gap has been narrowing, and many people claim that
    open source AI is already or will soon be as capable as its proprietary counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Off-the-shelf vs. fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to improving the performance of generative AI at a certain task,
    there are two main schools of thought. One of them is using off-the-shelf models—without
    any alterations—and it relies on prompt engineering techniques to make them more
    performant and customized to your intended task. For example, it has become popular
    to include a few demonstrations of how to perform a task inside the prompt, which
    is known as *few-shot prompting* (as opposed to *zero-shot prompting* in which
    you don’t provide any examples). This helps disambiguate the request. Researchers
    from OpenAI argued ([https://arxiv.org/pdf/2005.14165](https://arxiv.org/pdf/2005.14165)):'
  prefs: []
  type: TYPE_NORMAL
- en: If someone is asked to “make a table of world records for the 200m dash”, this
    request can be ambiguous, as it may not be clear exactly what format the table
    should have or what should be included (and even with careful clarification, understanding
    precisely what is desired can be difficult).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The researchers went on to show that including a few examples of how to perform
    the task within the prompt steered the LLM in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the RAG approach (see chapter 1) has become a popular way of providing
    the LLM with a large amount of contextual information to help it perform a task.
    The increasingly large context window of state-of-the-art LLMs has made RAG particularly
    effective.
  prefs: []
  type: TYPE_NORMAL
- en: Improved prompts can help customize image generation. For example, the image
    generator Midjourney lets users upload images as part of their prompts to indicate
    the desired style of the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: The other school of thought suggests altering the model to make it more suitable
    for the intended task, which is known as *fine-tuning.* The model’s internal parameters
    are adjusted, so you utilize an altered copy of the original model to generate
    your outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning requires training data, which is used to continue the training of
    the original model for a little longer. For example, to fine-tune an LLM, you
    must create a sample of text in your intended style. This data is fed to the training
    algorithm to refine the LLM. The amount of data used for fine-tuning is usually
    much smaller compared to the data used to train the original LLM—you may need
    just a handful of documents to do so. Open source models are ideal for fine-tuning
    as you have access to the entire model with its parameters, and you can then alter
    the parameters to better suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the biggest challenge of fine-tuning is overdoing it—if you specialize
    your model too much on your fine-tuning training data, it might end up memorizing
    specific examples present in the data and not perform well with other instances.
    This is known as *overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: There are a handful of techniques to prevent overfitting (see the sidebar).
    You need to be mindful of these techniques and configure the fine-tuning algorithm
    appropriately to prevent overfitting. We’ll discuss later in this chapter how
    you can use validation and test sets to evaluate and compare different AI models,
    which can help select the best strategy to fine-tune a model and ensure the final
    model hasn’t overfitted the data.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques to control overfitting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Early stopping**—*You train the model on your fine-tuning data only for a
    few iterations. You stop once performance stops improving, as measured on a separate
    piece of data (called the *validation set*).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Limited scope of updates**—*You only allow some parts of the model to be updated.
    For example, one popular method called LoRA inserts small layers with new learnable
    parameters into the model, while keeping its original parameters intact.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Regularization**—*You add a term to the loss function that penalizes too high
    or too low parameter values. This reduces the risk of overfitting by preventing
    parameters from being overly specialized to specific training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dropout**—*Pieces of the model are randomly removed on each iteration of the
    training process, which prevents internal units of the model from overly specializing
    to the training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: A method known as LoRA has become popular for fine-tuning (see [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)).
    LoRA inserts small layers with new learnable parameters to adjust the existing
    model, instead of modifying its original parameters. This makes fine-tuning faster
    as few parameter updates must be calculated on each iteration. It also helps control
    overfitting as you only modify a limited number of parameters (see “Limited scope
    of updates” in the sidebar).
  prefs: []
  type: TYPE_NORMAL
- en: The libraries developed by Hugging Face are very popular for fine-tuning existing
    models ([https://huggingface.co/docs/trl/main/en/index](https://huggingface.co/docs/trl/main/en/index)).
    Hugging Face also contains a large inventory of open source models you can fine-tune.
    Many users run their fine-tuning using Jupyter notebooks connected to cloud-computing
    instances. Google Collab is particularly commonly used for this, as it provides
    easy-to-access notebooks and lets you use some of its computing power for free,
    which might be enough to fine-tune some models.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning requires some specialized machine learning knowledge, so I recommend
    you learn the basics of ML to get it right. You might also require infrastructure
    to run the fine-tuning process, and you’ll then have to use your own customized
    copy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, it is also possible to fine-tune proprietary AI. For example,
    OpenAI lets you upload your own fine-tuning dataset and create a fine-tuned version
    of its models, which you can access through the API. The company charges a premium
    for using fine-tuned models compared to using OpenAI’s original models. The process
    is friendly, although not as customizable as fine-tuning open source models.
  prefs: []
  type: TYPE_NORMAL
- en: How to decide
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt engineering is the most straightforward way of improving a model’s performance.
    Common advice is that it’s the first thing you should try (check out [https://mng.bz/AQZx](https://mng.bz/AQZx)
    and [https://mng.bz/ZlQA](https://mng.bz/ZlQA) for more info). As context windows
    have become large, prompts can be quite rich. So, it is often advisable to use
    fine-tuning as a last resort when the output still isn’t quite what you expect,
    even after trying multiple ways of improving the prompts. Note, however, that
    prompt engineering works best with the most advanced and costly models, as they
    can adapt better to a wider range of tasks and fit longer prompts within their
    context windows. Table 3.2 compares off-the-shelf with fine-tuned AI.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2  Off-the-shelf vs. fine-tuned AI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Off-the-shelf AIBest when . . . | Fine-tuned AIBest when . . . |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| • Prompt engineering techniques work well.• It is okay to use proprietary
    AI.• You can afford large models.• You prioritize ease of use. | • You want highly
    customized outputs, and you’ve exhausted other options.• You need to use smaller
    models (for example, for self-hosting them).• You have ML expertise and access
    to computing resources. |'
  prefs: []
  type: TYPE_TB
- en: Fine-tuning can be a good choice for smaller models, for example, because you
    want to reduce your costs. This is particularly relevant when you must self-host
    your own models. In this case, using a small, fine-tuned model might be more effective
    than using prompt engineering with a larger model.
  prefs: []
  type: TYPE_NORMAL
- en: Customer-facing AI apps vs. foundation models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Customer-facing AI apps help final customers perform tasks. These include general-purpose
    commercial chatbots such as ChatGPT and special-purpose apps such as GitHub Copilot
    and Cursor, which help software engineers write code.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, foundation models are large, multipurpose AI models. These models
    are used behind the scenes to power customer-facing apps. For example, foundation
    models such as GPT-4o are used to power customer-facing ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Some companies build both customer-facing apps and provide access to their underlying
    foundation models through APIs so that software developers can build their own
    apps on top.
  prefs: []
  type: TYPE_NORMAL
- en: How to decide
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Customer-facing apps are the most suitable choice when you want AI to assist
    you in performing a specific task, as they’re friendly to use and particularly
    tailored to the task. Foundation models are best used as a building block when
    you want to create your own app based on powerful AI. Table 3.3 compares customer-facing
    AI apps with foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.3  Customer-facing AI apps vs. foundation models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Customer-facing AI appsSuitable for . . . | Foundation modelsSuitable for
    . . . |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| • Assistance with a specific task• End users | • Powering AI-based apps•
    Engineers |'
  prefs: []
  type: TYPE_TB
- en: Model validation, selection, and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to accurately compare and select AI models, it’s a good idea to
    build a benchmark to assess their respective performances. Also, for reasons that
    will become apparent soon, we often overestimate machine learning’s performance,
    so it’s good to follow a well-designed assessment process to prevent bad surprises.
  prefs: []
  type: TYPE_NORMAL
- en: This section describes the ideal protocol to evaluate AI’s performance at a
    task. In this protocol, AI models are built and evaluated using three different
    collections of data, known as *datasets.* In the following, we describe the role
    of each type of dataset and how it should be used.
  prefs: []
  type: TYPE_NORMAL
- en: Training set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training set is the dataset used to build the model. It contains a large
    collection of examples of how to perform the task. For example, for image generation,
    it comprises numerous images paired with captions that describe their content.
    For text generation, it comprises a large amount of text. A much smaller training
    set is also used to fine-tune a model.
  prefs: []
  type: TYPE_NORMAL
- en: During training or fine-tuning, the training algorithm tries to find model parameters
    that minimize the loss on the training set (see chapter 1). The loss is a mathematical
    function that quantifies how far off the model is from performing the required
    task well, such as predicting the next token in the case of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The loss is usually designed to have nice mathematical properties, such as differentiability,
    so it’s not always the most intuitive way of understanding a model’s performance.
    In addition, the loss does not always quantify how good the model is at your intended
    task. For example, if you want to use AI to solve coding problems, the training
    loss does not explicitly quantify its coding abilities; instead, it quantifies
    how well it autocompletes text, which is only indirectly related to coding abilities.
  prefs: []
  type: TYPE_NORMAL
- en: You don’t have to worry much about creating a training set unless you’re fine-tuning
    a model or training one from scratch. However, you might need to be mindful of
    what data was used for training when creating the validation and test sets (more
    on this in a minute).
  prefs: []
  type: TYPE_NORMAL
- en: Validation set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The validation set is used to compare the performance of different models. For
    example, you could use a validation set to compare the performance of GPT-4o and
    Llama 3 at performing a task. This helps you pick the best model, which is known
    as *model selection.*
  prefs: []
  type: TYPE_NORMAL
- en: The performance on the validation set is usually calculated using a measure
    close to your actual business objective. For example, you could calculate how
    often the model solves coding problems correctly. Note this is often different
    from the loss function used for training or fine-tuning the model. There’s a list
    of common performance measures later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important that data in the validation set is not present inside the training
    set. Otherwise, you might overestimate the model’s performance. This is because
    a poor model that overfits the training data (it memorizes specific instances)
    may go undetected, as some of the memorized data will also appear in the validation
    set it’s evaluated on. If the validation set is included in the training set,
    it’s a bit like an exam that contains questions present verbatim in the textbook—students
    could memorize answers without genuinely learning and pass the exam.
  prefs: []
  type: TYPE_NORMAL
- en: You need to be particularly careful about this when using LLMs as they’re trained
    on a huge amount of publicly available data that includes solutions to many problems.
    Suppose you want to use an LLM to help you solve crossword puzzles. You create
    a validation set by gathering clues from real *New York Times* crosswords published
    in the past. You then count how often the LLM identifies the right word based
    on the clues. The problem is that there are numerous websites that explicitly
    provide the solutions to all past *New York Times* crosswords, clue by clue. So,
    at least in theory, an LLM could memorize the exact solution to each past clue.
    Your validation data would thus assess the LLM’s performance at solving problems
    whose solution it had the answer to. A better way of doing this would be to create
    a validation set containing new clues that haven’t appeared in past puzzles. This
    way, the LLM wouldn’t be able to “cheat.” Alternatively, you could make sure that
    the puzzles in the validation set were published after the LLM’s training data
    cut-off date.
  prefs: []
  type: TYPE_NORMAL
- en: The validation set can also be used to help you make high-level decisions when
    you’re training or fine-tuning your own model. For example, you can train two
    models with different numbers of layers or different learning rates (how much
    the model’s parameters are updated on every training iteration), and then pick
    the model with highest performance on the validation set. You could also use a
    validation set to compare different prompt engineering approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Test set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using a validation set is not enough to properly assess a model’s performance.
    Because you’re specifically selecting the model that works best on the validation
    set alone, you might get an overly optimistic idea of its performance. After all,
    you discarded the models that weren’t as good on that specific piece of data.
    What if the selected model only works well on the validation data by chance and
    is not a better model in general?
  prefs: []
  type: TYPE_NORMAL
- en: So, after you’re done picking the best model using the validation set, you must
    perform a final check using *another* dataset, called the *test set.* The test
    set gives you an idea of the model’s performance on data that has genuinely never
    been used to make modeling decisions. This final assessment is a sanity check.
  prefs: []
  type: TYPE_NORMAL
- en: The test set can only be used once. If after the test you find performance disappointing
    and want to update the model or consider alternatives, you must collect a new
    test set to perform a new assessment. Otherwise, you end up using the test set
    repeatedly for model selection, so it turns into a validation set.
  prefs: []
  type: TYPE_NORMAL
- en: It is up to you to choose how thorough you want to be when following this process.
    I know of hedge funds that are very stringent about following it, as a lot of
    money is at stake. For example, they try not to even look at the data inside the
    test set to, say, plot a graph. This way, they prevent knowledge about the test
    data from creeping into modeling decisions, so the test data is as independent
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Performance measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes some common performance measures that can be used to
    evaluate AI’s performance at an intended task.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Accuracy* is the percentage of tasks performed correctly. For example, 90%
    accuracy means that 9 out of 10 solutions are correct, as measured on the validation
    or test sets.'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is commonly used for classification tasks. For example, it is often
    used to assess how good AI is at categorizing an image or detecting a tweet’s
    sentiment. You can also use it for other problem-solving tasks. For instance,
    you could use accuracy to measure an LLM’s ability to solve coding problems—you’d
    need to count the number of correctly solved problems and divide it by the total
    number of problems in your validation or test set.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In information retrieval, we are interested in identifying relevant instances
    out of a much larger pool. For example, a law firm may use a RAG approach to retrieve
    relevant legal cases, according to a query, from a large database of past cases.
    As another example, a bank may want to identify fraudulent transactions out of
    a (hopefully) much larger pool of transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Two common performance measures are recall and precision. However, as we’ll
    discuss in a minute, neither can be used by itself.
  prefs: []
  type: TYPE_NORMAL
- en: '*Recall* measures how many relevant instances are identified. For example,
    90% recall means that 9 out of 10 relevant instances are retrieved, the remaining
    being missed.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision* measures how relevant the retrieved instances are. For example,
    90% precision means that 9 out of 10 retrieved instances are truly relevant, the
    remaining being irrelevant or a false positive.'
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is that there is a tradeoff between precision and recall. Consider
    a system that retrieves too much stuff. For example, it could determine that almost
    every past legal case is relevant to every query. This system would achieve very
    high recall, perhaps close to 100%. However, it would be plagued with false positives,
    so its precision would be very low.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, consider a system that doesn’t retrieve much stuff at all. For
    example, it may consider almost every past legal case irrelevant regardless of
    the query. This system would achieve close to 100% precision, but its recall would
    be very low.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to properly quantify AI’s performance at information retrieval, you must
    somehow combine recall and precision into a single measure. A popular way to do
    this is to calculate the *F-measure*, which is the harmonic mean (a sort of average)
    between precision *P* and recall *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: F = 2(P R)/(P + R)
  prefs: []
  type: TYPE_NORMAL
- en: The higher the F-measure becomes, the higher the recall and precision. It takes
    its maximum value when recall and precision are both 100%.
  prefs: []
  type: TYPE_NORMAL
- en: I’m not a big fan of the F-measure for two reasons. First, it gives equal importance
    to recall and precision. This is arbitrary. In reality, a business may not care
    equally about them. I advise you to be wary of any promises of a measure that
    is universally good for information retrieval, be it the F-measure or something
    else, as the relative appetite for precision and recall is business specific.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the F-measure is difficult to interpret, as the harmonic mean is not
    very intuitive. Technically, the F-measure is the reciprocal of the average of
    the reciprocals, which leads to the above formula after some algebraic manipulation.
    Good luck at communicating that to the business!
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, your best bet is to try to understand the business’s preferences
    with respect to precision and recall and come up with a custom measure that considers
    that. In the following paragraphs, I explain one of my preferred ways of doing
    this.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to understand the business’s minimum desirable level of recall
    (it can also be done with precision, but we’ll use recall here). For example,
    the business may want to make sure to always recall at least 95% of relevant legal
    cases or fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, you tune the system so that it attains the desired level of recall.
    One way to do this is to have AI output relevance as a numerical score, with values
    ranging from 0 (totally irrelevant) to 1 (totally relevant). Instances above a
    certain relevance threshold are considered relevant. You pick the threshold that
    helps you attain the desired level of recall. For example, it could be that setting
    a threshold of, say, 0.7, above which an instance is considered relevant, helps
    you attain the required 95% recall (you can use the validation set to calculate
    the threshold).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you use the other measure—precision in this case—to report performance.
    You can thus compare different models (all attaining the desired recall) by how
    precise they are.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error and root mean squared error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you use AI to make a numerical prediction, such as the amount of rainfall,
    you must calculate how far off predictions are from actual values. One straightforward
    way of doing this is to calculate the absolute difference between predicted and
    known values in the training or test sets and average the results. This is known
    as the *mean absolute error*, or MAE.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to square the differences, which makes them all positive,
    average the results, and then take the square root of this number to (sort of)
    undo the effect of squaring. This is known as the *root mean squared error*, or
    RMSE. This measure is quite popular owing to its nice mathematical properties
    (in particular, its differentiability) and because it penalizes larger deviations
    more due to the squaring of the difference. However, it’s not as easy to interpret
    as MAE.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Proprietary AI is a good choice when you need an easy-to-use, done-for-you solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source AI is a good choice when you need to self-host or customize models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If AI isn’t working quite the way you expect, or you need to customize it, it’s
    usually recommended to still use off-the-shelf models and enhance your prompts.
    If that doesn’t work, you may want to fine-tune a model to your own data. Fine-tuning
    is also a good option when you prefer to use a smaller model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer-facing AI apps are designed to be friendly and useful to end users.
    They’re powered by foundation models behind the scenes, which are large, general-purpose
    AI models you can use to build your own AI-based apps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure to use a validation set (with data not present in the training set)
    to compare and select models. You should also perform a sanity check afterward
    using a separate test set, once you’ve selected your favorite model. Do not use
    the test set twice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy of a model measures how often it performs a task correctly. Measures
    such as precision and recall are used for information retrieval (e.g., fetching
    relevant legal cases according to a query from a much larger pool of legal cases).
    Precision and recall cannot be used by themselves; they must be combined in a
    way that matches business preferences about their relative importance. You can
    use the mean absolute error (MAE) or the root mean squared error (RMSE) to evaluate
    the performance of a model at predicting a number (such as the amount of rainfall).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
