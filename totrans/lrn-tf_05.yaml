- en: 'Chapter 5\. Text I: Working with Text and Sequences, and TensorBoard Visualization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we show how to work with sequences in TensorFlow, and in particular
    text. We begin by introducing recurrent neural networks (RNNs), a powerful class
    of deep learning algorithms particularly useful and popular in natural language
    processing (NLP). We show how to implement RNN models from scratch, introduce
    some important TensorFlow capabilities, and visualize the model with the interactive
    TensorBoard. We then explore how to use an RNN in a supervised text classification
    problem with word-embedding training. Finally, we show how to build a more advanced
    RNN model with long short-term memory (LSTM) networks and how to handle sequences
    of variable length.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Sequence Data
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in the previous chapter that using the spatial structure of images can
    lead to advanced models with excellent results. As discussed in that chapter,
    exploiting structure is the key to success. As we will see shortly, an immensely
    important and useful type of structure is the sequential structure. Thinking in
    terms of data science, this fundamental structure appears in many datasets, across
    all domains. In computer vision, video is a sequence of visual content evolving
    over time. In speech we have audio signals, in genomics gene sequences; we have
    longitudinal medical records in healthcare, financial data in the stock market,
    and so on (see [Figure 5-1](#ubiquity_of_sequence_data)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0501.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. The ubiquity of sequence data.
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A particularly important type of data with strong sequential structure is natural
    language—text data. Deep learning methods that exploit the sequential structure
    inherent in texts—characters, words, sentences, paragraphs, documents—are at the
    forefront of natural language understanding (NLU) systems, often leaving more
    traditional methods in the dust. There are a great many types of NLU tasks that
    are of interest to solve, ranging from document classification to building powerful
    language models, from answering questions automatically to generating human-level
    conversation agents. These tasks are fiendishly difficult, garnering the efforts
    and attention of the entire AI community in both academia and industry.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focus on the basic building blocks and tasks, and show how
    to work with sequences—primarily of text—in TensorFlow. We take a detailed deep
    dive into the core elements of sequence models in TensorFlow, implementing some
    of them from scratch, to gain a thorough understanding. In the next chapter we
    show more advanced text modeling techniques with TensorFlow, and in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications)
    we use abstraction libraries that offer simpler, high-level ways to implement
    our models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the most important and popular class of deep learning models
    for sequences (in particular, text): recurrent neural networks.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Recurrent Neural Networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are a powerful and widely used class of neural network
    architectures for modeling sequence data. The basic idea behind RNN models is
    that each new element in the sequence contributes some new information, which
    updates the current state of the model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, which explored computer vision with CNN models, we
    discussed how those architectures are inspired by the current scientific perceptions
    of the way the human brain processes visual information. These scientific perceptions
    are often rather close to our commonplace intuition from our day-to-day lives
    about how we process sequential information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: When we receive new information, clearly our “history” and “memory” are not
    wiped out, but instead “updated.” When we read a sentence in some text, with each
    new word, our current state of information is updated, and it is dependent not
    only on the new observed word but on the words that preceded it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental mathematical construct in statistics and probability, which is
    often used as a building block for modeling sequential patterns via machine learning
    is the Markov chain model. Figuratively speaking, we can view our data sequences
    as “chains,” with each node in the chain dependent in some way on the previous
    node, so that “history” is not erased but carried on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学和概率论中的一个基本数学构造，通常被用作通过机器学习建模顺序模式的基本构件是马尔可夫链模型。比喻地说，我们可以将我们的数据序列视为“链”，链中的每个节点在某种程度上依赖于前一个节点，因此“历史”不会被抹去，而是被延续。
- en: RNN models are also based on this notion of chain structure, and vary in how
    exactly they maintain and update information.  As their name implies, recurrent
    neural nets apply some form of “loop.” As seen in [Figure 5-2](#recurrent_neural_nets),
    at some point in time *t*, the network observes an input *x[t]* (a word in a sentence)
    and updates its “state vector” to *h[t]* from the previous vector *h[t-1]*. When
    we process new input (the next word), it will be done in some manner that is dependent
    on *h[t]* and thus on the history of the sequence (the previous words we’ve seen
    affect our understanding of the current word). As seen in the illustration, this
    recurrent structure can simply be viewed as one long unrolled chain, with each
    node in the chain performing the same kind of processing “step” based on the “message”
    it obtains from the output of the previous node. This, of course, is very related
    to the Markov chain models discussed previously and their hidden Markov model
    (HMM) extensions, which are not discussed in this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RNN模型也基于这种链式结构的概念，并且在如何确切地维护和更新信息方面有所不同。正如它们的名称所示，循环神经网络应用某种形式的“循环”。如[图5-2](#recurrent_neural_nets)所示，在某个时间点*t*，网络观察到一个输入*x[t]*（句子中的一个单词），并将其“状态向量”从上一个向量*h[t-1]*更新为*h[t]*。当我们处理新的输入（下一个单词）时，它将以某种依赖于*h[t]*的方式进行，因此依赖于序列的历史（我们之前看到的单词影响我们对当前单词的理解）。如图所示，这种循环结构可以简单地被视为一个长长的展开链，链中的每个节点执行相同类型的处理“步骤”，基于它从前一个节点的输出获得的“消息”。当然，这与先前讨论的马尔可夫链模型及其隐马尔可夫模型（HMM）扩展密切相关，这些内容在本书中没有讨论。
- en: '![](assets/letf_0502.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0502.png)'
- en: Figure 5-2\. Recurrent neural networks updating with new information received
    over time.
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2。随时间更新的循环神经网络。
- en: Vanilla RNN Implementation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础RNN实现
- en: In this section we implement a basic RNN from scratch, explore its inner workings,
    and gain insight into how TensorFlow can work with sequences. We introduce some
    powerful, fairly low-level tools that TensorFlow provides for working with sequence
    data, which you can use to implement your own systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从头开始实现一个基本的RNN，探索其内部工作原理，并了解TensorFlow如何处理序列。我们介绍了一些强大的、相当低级的工具，TensorFlow提供了这些工具用于处理序列数据，您可以使用这些工具来实现自己的系统。
- en: In the next sections, we will show how to use higher-level TensorFlow RNN modules.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将展示如何使用更高级别的TensorFlow RNN模块。
- en: We begin with defining our basic model mathematically. This mainly consists
    of defining the recurrence structure—the RNN update step.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数学上定义我们的基本模型开始。这主要包括定义循环结构 - RNN 更新步骤。
- en: The update step for our simple vanilla RNN is
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单的基础vanilla RNN的更新步骤是
- en: '*h[t]* = *tanh*(*W[x]**x[t]* + *W[h]h[t-1]* + *b*)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*h[t]* = *tanh*(*W[x]**x[t]* + *W[h]h[t-1]* + *b*)'
- en: where *W[h]*, *W[x]*, and *b* are weight and bias variables we learn, *tanh*(·)
    is the hyperbolic tangent function that has its range in [–1,1] and is strongly
    connected to the sigmoid function used in previous chapters, and *x[t]* and *h[t]*
    are the input and state vectors as defined previously. Finally, the hidden state
    vector is multiplied by another set of weights, yielding the outputs that appear
    in [Figure 5-2](#recurrent_neural_nets).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*W[h]*，*W[x]*和*b*是我们学习的权重和偏置变量，*tanh*(·)是双曲正切函数，其范围在[-1,1]之间，并且与前几章中使用的sigmoid函数密切相关，*x[t]*和*h[t]*是之前定义的输入和状态向量。最后，隐藏状态向量乘以另一组权重，产生出现在[图5-2](#recurrent_neural_nets)中的输出。
- en: MNIST images as sequences
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MNIST图像作为序列
- en: To get a first taste of the power and general applicability of sequence models,
    in this section we implement our first RNN to solve the MNIST image classification
    task that you are by now familiar with.  Later in this chapter we will focus on
    sequences of text, and see how neural sequence models can powerfully manipulate
    them and extract information to solve NLU tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了初尝序列模型的强大和普适性，在本节中，我们实现我们的第一个RNN来解决您现在熟悉的MNIST图像分类任务。在本章的后面，我们将专注于文本序列，并看看神经序列模型如何强大地操纵它们并提取信息以解决NLU任务。
- en: But, you may ask, what have images got to do with sequences?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你可能会问，图像与序列有什么关系？
- en: As we saw in the previous chapter, the architecture of convolutional neural
    networks makes use of the spatial structure of images. While the structure of
    natural images is well suited for CNN models,  it is revealing to look at the
    structure of images from different angles. In a trend in cutting-edge deep learning
    research, advanced models attempt to exploit various kinds of sequential structures
    in images, trying to capture in some sense the “generative process” that created
    each image. Intuitively, this all comes down to the notion that nearby areas in
    images are somehow related, and trying to model this structure.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，卷积神经网络的架构利用了图像的空间结构。虽然自然图像的结构非常适合CNN模型，但从不同角度查看图像的结构是有启发性的。在前沿深度学习研究的趋势中，先进模型尝试利用图像中各种顺序结构，试图以某种方式捕捉创造每个图像的“生成过程”。直观地说，这一切归结为图像中相邻区域在某种程度上相关，并试图对这种结构建模。
- en: 'Here, to introduce basic RNNs and how to work with sequences, we take a simple
    sequential view of images: we look at each image in our data as a sequence of
    rows (or columns). In our MNIST data, this just means that each 28×28-pixel image
    can be viewed as a sequence of length 28, each element in the sequence a vector
    of 28 pixels (see [Figure 5-3](#sequence_of_pixel_columns)). Then, the temporal
    dependencies in the RNN can be imaged as a scanner head, scanning the image from
    top to bottom (rows) or left to right (columns).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，为了介绍基本的RNN以及如何处理序列，我们将图像简单地视为序列：我们将数据中的每个图像看作是一系列的行（或列）。在我们的MNIST数据中，这意味着每个28×28像素的图像可以被视为长度为28的序列，序列中的每个元素是一个包含28个像素的向量（参见[图5-3](#sequence_of_pixel_columns)）。然后，RNN中的时间依赖关系可以被想象成一个扫描头，从上到下（行）或从左到右（列）扫描图像。
- en: '![](assets/letf_0503.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0503.png)'
- en: Figure 5-3\. An image as a sequence of pixel columns.
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3。图像作为像素列的序列。
- en: 'We start by loading data, defining some parameters, and creating placeholders
    for our data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据，定义一些参数，并为我们的数据创建占位符：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`element_size` is the dimension of each vector in our sequence—in our case,
    a row/column of 28 pixels. `time_steps` is the number of such elements in a sequence.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`element_size`是我们序列中每个向量的维度，在我们的情况下是28个像素的行/列。`time_steps`是序列中这样的元素的数量。'
- en: 'As we saw in previous chapters, when we load data with the built-in MNIST data
    loader, it comes in unrolled form—a vector of 784 pixels. When we load batches
    of data during training (we’ll get to that later in this section), we simply reshape
    each unrolled vector to [`batch_size`, `time_steps`, `element_size`]:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的章节中看到的，当我们使用内置的MNIST数据加载器加载数据时，它以展开的形式呈现，即一个包含784个像素的向量。在训练期间加载数据批次时（我们稍后将在本节中介绍），我们只需将每个展开的向量重塑为[`batch_size`,
    `time_steps`, `element_size`]：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We set `hidden_layer_size` (arbitrarily to `128`, controlling the size of the
    hidden RNN state vector discussed earlier.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`hidden_layer_size`设置为`128`（任意值），控制之前讨论的隐藏RNN状态向量的大小。
- en: '`LOG_DIR` is the directory to which we save model summaries for TensorBoard
    visualization. You will learn what this means as we go.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`LOG_DIR`是我们保存模型摘要以供TensorBoard可视化的目录。随着我们的学习，您将了解这意味着什么。'
- en: TensorBoard visualizations
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorBoard可视化
- en: In this chapter, we will also briefly introduce TensorBoard visualizations.
    TensorBoard allows you to monitor and explore the model structure, weights, and
    training process, and requires some very simple additions to the code. More details
    are provided throughout this chapter and further along in the book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还将简要介绍TensorBoard可视化。TensorBoard允许您监视和探索模型结构、权重和训练过程，并需要对代码进行一些非常简单的添加。更多细节将在本章和本书后续部分提供。
- en: Finally, our input and label placeholders are created with the suitable dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建了适当维度的输入和标签占位符。
- en: The RNN step
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN步骤
- en: Let’s implement the mathematical model for the RNN step.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现RNN步骤的数学模型。
- en: 'We first create a function used for logging summaries, which we will use later
    in TensorBoard to visualize our model and training process (it is not important
    to understand its technicalities at this stage):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个用于记录摘要的函数，稍后我们将在TensorBoard中使用它来可视化我们的模型和训练过程（在这个阶段理解其技术细节并不重要）：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we create the weight and bias variables used in the RNN step:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建在RNN步骤中使用的权重和偏置变量：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Applying the RNN step with tf.scan()
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用tf.scan()应用RNN步骤
- en: 'We now create a function that implements the vanilla RNN step we saw in the
    previous section using the variables we created. It should by now be straightforward
    to understand the TensorFlow code used here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个函数，实现了我们在前一节中看到的基本RNN步骤，使用我们创建的变量。现在应该很容易理解这里使用的TensorFlow代码：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we apply this function across all 28 time steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这个函数应用到所有的28个时间步上：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this small code block, there are some important elements to understand. First,
    we reshape the inputs from `[batch_size, time_steps, element_size]` to `[time_steps,
    batch_size, element_size]`. The `perm` argument to `tf.transpose()` tells TensorFlow
    which axes we want to switch around. Now that the first axis in our input Tensor
    represents the time axis, we can iterate across all time steps by using the built-in
    `tf.scan()` function, which repeatedly applies a callable (function) to a sequence
    of elements in order, as explained in the following note.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小的代码块中，有一些重要的元素需要理解。首先，我们将输入从`[batch_size, time_steps, element_size]`重塑为`[time_steps,
    batch_size, element_size]`。`tf.transpose()`的`perm`参数告诉TensorFlow我们想要交换的轴。现在，我们的输入张量中的第一个轴代表时间轴，我们可以通过使用内置的`tf.scan()`函数在所有时间步上进行迭代，该函数重复地将一个可调用（函数）应用于序列中的元素，如下面的说明所述。
- en: tf.scan()
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.scan()
- en: This important function was added to TensorFlow to allow us to introduce loops
    into the computation graph, instead of just “unrolling” the loops explicitly by
    adding more and more replications of the same operations. More technically, it
    is a higher-order function very similar to the reduce operator, but it returns
    all intermediate accumulator values over time. There are several advantages to
    this approach, chief among them the ability to have a dynamic number of iterations
    rather than fixed, computational speedups and optimizations for graph construction.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个重要的函数被添加到TensorFlow中，允许我们在计算图中引入循环，而不仅仅是通过添加更多和更多的操作复制来“展开”循环。更技术上地说，它是一个类似于reduce操作符的高阶函数，但它返回随时间的所有中间累加器值。这种方法有几个优点，其中最重要的是能够具有动态数量的迭代而不是固定的，以及用于图构建的计算速度提升和优化。
- en: 'To demonstrate the use of this function, consider the following simple example
    (which is separate from the overall RNN code in this section):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个函数的使用，考虑以下简单示例（这与本节中的整体RNN代码是分开的）：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s see what we get:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this case, we use `tf.scan()` to sequentially concatenate characters to a
    string, in a manner analogous to the arithmetic cumulative sum.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用`tf.scan()`将字符顺序连接到一个字符串中，类似于算术累积和。
- en: Sequential outputs
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 顺序输出
- en: 'As we saw earlier, in an RNN we get a state vector for each time step, multiply
    it by some weights, and get an output vector—our new representation of the data.
    Let’s implement this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，在RNN中，我们为每个时间步获得一个状态向量，将其乘以一些权重，然后获得一个输出向量——我们数据的新表示。让我们实现这个：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our input to the RNN is sequential, and so is our output. In this sequence classification
    example, we take the last state vector and pass it through a fully connected linear
    layer to extract an output vector (which will later be passed through a softmax
    activation function to generate predictions). This is common practice in basic
    sequence classification, where we assume that the last state vector has “accumulated”
    information representing the entire sequence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RNN的输入是顺序的，输出也是如此。在这个序列分类示例中，我们取最后一个状态向量，并通过一个全连接的线性层将其传递，以提取一个输出向量（稍后将通过softmax激活函数传递以生成预测）。这在基本序列分类中是常见的做法，我们假设最后一个状态向量已经“积累”了代表整个序列的信息。
- en: To implement this, we first define the linear layer’s weights and bias term
    variables, and create a factory function for this layer. Then we apply this layer
    to all outputs with `tf.map_fn()`, which is pretty much the same as the typical
    map function that applies functions to sequences/iterables in an element-wise
    manner, in this case on each element in our sequence.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们首先定义线性层的权重和偏置项变量，并为该层创建一个工厂函数。然后我们使用`tf.map_fn()`将此层应用于所有输出，这与典型的map函数几乎相同，该函数以元素方式将函数应用于序列/可迭代对象，本例中是在我们序列的每个元素上。
- en: Finally, we extract the last output for each instance in the batch, with negative
    indexing (similarly to ordinary Python). We will see some more ways to do this
    later and investigate outputs and states in some more depth.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提取批次中每个实例的最后输出，使用负索引（类似于普通Python）。稍后我们将看到一些更多的方法来做这个，并深入研究输出和状态。
- en: RNN classification
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN分类
- en: 'We’re now ready to train a classifier, much in the same way we did in the previous
    chapters. We define the ops for loss function computation, optimization, and prediction,
    add some more summaries for TensorBoard, and merge all these summaries into one
    operation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练一个分类器，方式与前几章相同。我们定义损失函数计算、优化和预测的操作，为TensorBoard添加一些更多摘要，并将所有这些摘要合并为一个操作：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By now you should be familiar with most of the components used for defining
    the loss function and optimization. Here, we used the `RMSPropOptimizer`, implementing
    a well-known and strong gradient descent algorithm, with some standard hyperparameters.
    Of course, we could have used any other optimizer (and do so throughout this book!).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该熟悉用于定义损失函数和优化的大多数组件。在这里，我们使用`RMSPropOptimizer`，实现了一个众所周知且强大的梯度下降算法，带有一些标准的超参数。当然，我们可以使用任何其他优化器（并在本书中一直这样做！）。
- en: We create a small test set with unseen MNIST images, and add some more technical
    ops and commands for logging summaries that we will use in TensorBoard.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个包含未见过的MNIST图像的小测试集，并添加一些用于记录摘要的技术操作和命令，这些将在TensorBoard中使用。
- en: 'Let’s run the model and check out the results:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行模型并查看结果：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we print some training and testing accuracy results:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印一些训练和测试准确率的结果：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To summarize this section, we started off with the raw MNIST pixels and regarded
    them as sequential data—each column (or row) of 28 pixels as a time step. We then
    applied the vanilla RNN to extract outputs corresponding to each time-step and
    used the last output to perform classification of the entire sequence (image).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分，我们从原始的MNIST像素开始，并将它们视为顺序数据——每列（或行）的28个像素作为一个时间步。然后，我们应用了vanilla RNN来提取对应于每个时间步的输出，并使用最后的输出来执行整个序列（图像）的分类。
- en: Visualizing the model with TensorBoard
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化模型
- en: TensorBoard is an interactive browser-based tool that allows us to visualize
    the learning process, as well as explore our trained model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一个交互式基于浏览器的工具，允许我们可视化学习过程，以及探索我们训练的模型。
- en: 'To run TensorBoard, go to the command terminal and tell TensorBoard where the
    relevant summaries you logged are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 运行TensorBoard，转到命令终端并告诉TensorBoard相关摘要的位置：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, `*LOG_DIR*` should be replaced with your log directory. If you are on
    Windows and this is not working, make sure you are running the terminal from the
    same drive where the log data is, and add a name to the log directory as follows
    in order to bypass a bug in the way TensorBoard parses the path:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`*LOG_DIR*`应替换为您的日志目录。如果您在Windows上并且这不起作用，请确保您从存储日志数据的相同驱动器运行终端，并按以下方式向日志目录添加名称，以绕过TensorBoard解析路径的错误：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'TensorBoard allows us to assign names to individual log directories by putting
    a colon between the name and the path, which may be useful when working with multiple
    log directories. In such a case, we pass a comma-separated list of log directories
    as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard允许我们为单独的日志目录分配名称，方法是在名称和路径之间放置一个冒号，当使用多个日志目录时可能会有用。在这种情况下，我们将传递一个逗号分隔的日志目录列表，如下所示：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In our example (with one log directory), once you have run the `tensorboard`
    command, you should get something like the following, telling you where to navigate
    in your browser:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中（有一个日志目录），一旦您运行了`tensorboard`命令，您应该会得到类似以下内容的信息，告诉您在浏览器中导航到哪里：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If the address does not work, go to *localhost:6006*, which should always work.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果地址无法使用，请转到*localhost:6006*，这个地址应该始终有效。
- en: TensorBoard recursively walks the directory tree rooted at `*LOG_DIR*` looking
    for subdirectories that contain tfevents log data. If you run this example multiple
    times, make sure to either delete the `*LOG_DIR*` folder you created after each
    run, or write the logs to separate subdirectories within `*LOG_DIR*`, such as
    `*LOG_DIR*`*/run1/train*, `*LOG_DIR*`*/run2/train*, and so forth, to avoid issues
    with overwriting log files, which may lead to some “funky” plots.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard递归地遍历以`*LOG_DIR*`为根的目录树，寻找包含tfevents日志数据的子目录。如果您多次运行此示例，请确保在每次运行后要么删除您创建的`*LOG_DIR*`文件夹，要么将日志写入`*LOG_DIR*`内的单独子目录，例如`*LOG_DIR*`*/run1/train*，`*LOG_DIR*`*/run2/train*等，以避免覆盖日志文件，这可能会导致一些“奇怪”的图形。
- en: Let’s take a look at some of the visualizations we can get. In the next section,
    we will explore interactive visualization of high-dimensional data with TensorBoard—for
    now, we focus on plotting training process summaries and trained weights.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些我们可以获得的可视化效果。在下一节中，我们将探讨如何使用TensorBoard对高维数据进行交互式可视化-现在，我们专注于绘制训练过程摘要和训练权重。
- en: First, in your browser, go to the Scalars tab. Here TensorBoard shows us summaries
    of all scalars, including not only training and testing accuracy, which are usually
    most interesting, but also some summary statistics we logged about variables (see
    [Figure 5-4](#tensorboard_scalar_summaries)). Hovering over the plots, we can
    see some numerical figures.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在浏览器中，转到标量选项卡。在这里，TensorBoard向我们显示所有标量的摘要，包括通常最有趣的训练和测试准确性，以及我们记录的有关变量的一些摘要统计信息（请参见[图5-4](#tensorboard_scalar_summaries)）。将鼠标悬停在图表上，我们可以看到一些数字。
- en: '![](assets/letf_0504.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/letf_0504.png)
- en: Figure 5-4\. TensorBoard scalar summaries.
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。TensorBoard标量摘要。
- en: In the Graphs tab we can get an interactive visualization of our computation
    graph, from a high-level view down to the basic ops, by zooming in (see [Figure 5-5](#zooming_into_computation_graph)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在图形选项卡中，我们可以通过放大来获得计算图的交互式可视化，从高级视图到基本操作（请参见[图5-5](#zooming_into_computation_graph)）。
- en: '![](assets/letf_0505.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/letf_0505.png)
- en: Figure 5-5\. Zooming in on the computation graph.
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5。放大计算图。
- en: Finally, in the Histograms tab we see histograms of our weights across the training
    process (see [Figure 5-6](#weights_throughout_learning_process)). Of course, we
    had to explicitly add these histograms to our logging in order to view them, with
    `tf.summary.histogram()`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在直方图选项卡中，我们可以看到在训练过程中权重的直方图（请参见[图5-6](#weights_throughout_learning_process)）。当然，我们必须明确将这些直方图添加到我们的日志记录中才能查看它们，使用`tf.summary.histogram()`。
- en: '![](assets/letf_0506.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/letf_0506.png)
- en: Figure 5-6\. Histograms of weights throughout the learning process.
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6。学习过程中权重的直方图。
- en: TensorFlow Built-in RNN Functions
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow内置的RNN函数
- en: The preceding example taught us some of the fundamental and powerful ways we
    can work with sequences, by implementing our graph pretty much from scratch. In
    practice, it is of course a good idea to use built-in higher-level modules and
    functions. This not only makes the code shorter and easier to write, but exploits
    many low-level optimizations afforded by TensorFlow implementations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例教会了我们一些使用序列的基本和强大的方法，通过几乎从头开始实现我们的图。在实践中，当然最好使用内置的高级模块和函数。这不仅使代码更短，更容易编写，而且利用了TensorFlow实现提供的许多低级优化。
- en: 'In this section we first present a new, shorter version of the code in its
    entirety. Since most of the overall details have not changed, we focus on the
    main new elements, `tf.contrib.rnn.BasicRNNCell` and `tf.nn.dynamic_rnn()`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先以完整的新代码的新版本呈现。由于大部分整体细节没有改变，我们将重点放在主要的新元素`tf.contrib.rnn.BasicRNNCell`和`tf.nn.dynamic_rnn()`上：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: tf.contrib.rnn.BasicRNNCell and tf.nn.dynamic_rnn()
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tf.contrib.rnn.BasicRNNCell和tf.nn.dynamic_rnn()
- en: TensorFlow’s RNN cells are abstractions that represent the basic operations
    each recurrent “cell” carries out (see [Figure 5-2](#recurrent_neural_nets) at
    the start of this chapter for an illustration), and its associated state. They
    are, in general terms, a “replacement” of the `rnn_step()` function and the associated
    variables it required. Of course, there are many variants and types of cells,
    each with many methods and properties. We will see some more advanced cells toward
    the end of this chapter and later in the book.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的RNN单元是表示每个循环“单元”执行的基本操作（请参见本章开头的[图5-2](#recurrent_neural_nets)进行说明），以及其关联状态的抽象。它们通常是`rnn_step()`函数及其所需的相关变量的“替代”。当然，有许多变体和类型的单元，每个单元都有许多方法和属性。我们将在本章末尾和本书后面看到一些更高级的单元。
- en: Once we have created the `rnn_cell`, we feed it into `tf.nn.dynamic_rnn()`.
    This function replaces `tf.scan()` in our vanilla implementation and creates an
    RNN specified by `rnn_cell`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了`rnn_cell`，我们将其输入到`tf.nn.dynamic_rnn()`中。此函数替换了我们基本实现中的`tf.scan()`，并创建了由`rnn_cell`指定的RNN。
- en: As of this writing, in early 2017, TensorFlow includes a static and a dynamic
    function for creating an RNN. What does this mean? The static version creates
    an unrolled graph (as in [Figure 5-2](#recurrent_neural_nets)) of fixed length.
    The dynamic version uses a `tf.While` loop to dynamically construct the graph
    at execution time, leading to faster graph creation, which can be significant.
    This dynamic construction can also be very useful in other ways, some of which
    we will touch on when we discuss variable-length sequences toward the end of this
    chapter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，即2017年初，TensorFlow包括用于创建RNN的静态和动态函数。这是什么意思？静态版本创建一个固定长度的展开图（如[图5-2](#recurrent_neural_nets)）。动态版本使用`tf.While`循环在执行时动态构建图，从而加快图的创建速度，这可能是显著的。这种动态构建在其他方面也非常有用，其中一些我们将在本章末尾讨论变长序列时提及。
- en: Note that *contrib* refers to the fact that code in this library is contributed
    and still requires testing. We discuss the `contrib` library in much more detail
    in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications). `BasicRNNCell`
    was moved to `contrib` in TensorFlow 1.0 as part of ongoing development. In version
    1.2, many of the RNN functions and classes were moved back to the core namespace
    with aliases kept in `contrib` for backward compatibiliy, meaning that the preceding
    code works for all versions 1.X as of this writing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*contrib*指的是这个库中的代码是由贡献者贡献的，并且仍需要测试。我们将在第7章中更详细地讨论`contrib`库。`BasicRNNCell`在TensorFlow
    1.0中被移动到`contrib`作为持续开发的一部分。在1.2版本中，许多RNN函数和类被移回核心命名空间，并在`contrib`中保留别名以实现向后兼容性，这意味着在撰写本文时，前面的代码适用于所有1.X版本。
- en: RNN for Text Sequences
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本序列的RNN
- en: We began this chapter by learning how to implement RNN models in TensorFlow.
    For ease of exposition, we showed how to implement and use an RNN for a sequence
    made of pixels in MNIST images. We next show how to use these sequence models
    on text sequences.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时学习了如何在TensorFlow中实现RNN模型。为了便于说明，我们展示了如何在由MNIST图像中的像素组成的序列上实现和使用RNN。接下来我们将展示如何在文本序列上使用这些序列模型。
- en: Text data has some properties distinctly different from image data, which we
    will discuss here and later in this book. These properties can make it somewhat
    difficult to handle text data at first, and text data always requires at least
    some basic pre-processing steps for us to be able to work with it. To introduce
    working with text in TensorFlow, we will thus focus on the core components and
    create a minimal, contrived text dataset that will let us get straight to the
    action. In [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications),
    we will apply RNN models to movie review sentiment classification.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据具有与图像数据明显不同的一些属性，我们将在这里和本书的后面进行讨论。这些属性可能使得一开始处理文本数据有些困难，而文本数据总是需要至少一些基本的预处理步骤才能让我们能够处理它。为了介绍在TensorFlow中处理文本，我们将专注于核心组件并创建一个最小的、人为的文本数据集，这将让我们直接开始行动。在第7章中，我们将应用RNN模型进行电影评论情感分类。
- en: Let’s get started, presenting our example data and discussing some key properties
    of text datasets as we go.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧，展示我们的示例数据并在进行的过程中讨论文本数据集的一些关键属性。
- en: Text Sequences
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本序列
- en: In the MNIST RNN example we saw earlier, each sequence was of fixed size—the
    width (or height) of an image. Each element in the sequence was a dense vector
    of 28 pixels. In NLP tasks and datasets, we have a different kind of “picture.”
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前看到的MNIST RNN示例中，每个序列的大小是固定的——图像的宽度（或高度）。序列中的每个元素都是一个由28个像素组成的密集向量。在NLP任务和数据集中，我们有一种不同类型的“图片”。
- en: Our sequences could be of words forming a sentence, of sentences forming a paragraph,
    or even of characters forming words or paragraphs forming whole documents.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的序列可以是由单词组成的句子，由句子组成的段落，甚至由字符组成的单词或段落组成的整个文档。
- en: 'Consider the following sentence: “Our company provides smart agriculture solutions
    for farms, with advanced AI, deep-learning.” Say we obtain this sentence from
    an online news blog, and wish to process it as part of our machine learning system.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下句子：“我们公司为农场提供智能农业解决方案，具有先进的人工智能、深度学习。”假设我们从在线新闻博客中获取这个句子，并希望将其作为我们机器学习系统的一部分进行处理。
- en: Each of the words in this sentence would be represented with an ID—an integer,
    commonly referred to as a token ID in NLP. So, the word “agriculture” could, for
    instance, be mapped to the integer 3452, the word “farm” to 12, “AI” to 150, and
    “deep-learning” to 0\. This representation in terms of integer identifiers is
    very different from the vector of pixels in image data, in multiple ways. We will
    elaborate on this important point shortly when we discuss word embeddings, and
    in [Chapter 6](ch06.html#text_ii).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子中的每个单词都将用一个ID表示——一个整数，通常在NLP中被称为令牌ID。因此，例如单词“agriculture”可以映射到整数3452，单词“farm”到12，“AI”到150，“deep-learning”到0。这种以整数标识符表示的表示形式与图像数据中的像素向量在多个方面都非常不同。我们将在讨论词嵌入时很快详细阐述这一重要观点，并在第6章中进行讨论。
- en: To make things more concrete, let’s start by creating our simplified text data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情更具体，让我们从创建我们简化的文本数据开始。
- en: Our simulated data consists of two classes of very short “sentences,” one composed
    of odd digits and the other of even digits (with numbers written in English).
    We generate sentences built of words representing even and odd numbers. Our goal
    is to learn to classify each sentence as either odd or even in a supervised text-classification
    task.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模拟数据由两类非常短的“句子”组成，一类由奇数组成，另一类由偶数组成（数字用英文书写）。我们生成由表示偶数和奇数的单词构建的句子。我们的目标是在监督文本分类任务中学习将每个句子分类为奇数或偶数。
- en: Of course, we do not really need any machine learning for this simple task—we
    use this contrived example only for illustrative purposes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于这个简单的任务，我们实际上并不需要任何机器学习——我们只是为了说明目的而使用这个人为的例子。
- en: 'First, we define some constants, which will be explained as we go:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一些常量，随着我们的进行将会解释：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we create sentences. We sample random digits and map them to the corresponding
    “words” (e.g., 1 is mapped to “One,” 7 to “Seven,” etc.).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建句子。我们随机抽取数字并将其映射到相应的“单词”（例如，1映射到“One”，7映射到“Seven”等）。
- en: Text sequences typically have variable lengths, which is of course the case
    for all real natural language data (such as in the sentences appearing on this
    page).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 文本序列通常具有可变长度，这当然也适用于所有真实的自然语言数据（例如在本页上出现的句子）。
- en: To make our simulated sentences have different lengths, we sample for each sentence
    a random length between 3 and 6 with `np.random.choice(range(3, 7))`—the lower
    bound is inclusive, and the upper bound is exclusive.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们模拟的句子具有不同的长度，我们为每个句子抽取一个介于3和6之间的随机长度，使用`np.random.choice(range(3, 7))`——下限包括，上限不包括。
- en: 'Now, to put all our input sentences in one tensor (per batch of data instances),
    we need them to somehow be of the same size—so we pad sentences with a length
    shorter than 6 with zeros (or *PAD* symbols) to make all sentences equally sized
    (artificially). This pre-processing step is known as *zero-padding*. The following
    code accomplishes all of this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了将所有输入句子放入一个张量中（每个数据实例的批次），我们需要它们以某种方式具有相同的大小—因此，我们用零（或*PAD*符号）填充长度短于6的句子，使所有句子大小相等（人为地）。这个预处理步骤称为*零填充*。以下代码完成了所有这些：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s take a look at our sentences, each padded to length 6:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们的句子，每个都填充到长度6：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Notice that we add the *PAD* word (token) to our data and `digit_to_word_map`
    dictionary, and separately store even and odd sentences and their original lengths
    (before padding).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们向我们的数据和`digit_to_word_map`字典中添加了*PAD*单词（标记），并分别存储偶数和奇数句子及其原始长度（填充之前）。
- en: 'Let’s take a look at the original sequence lengths for the sentences we printed:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们打印的句子的原始序列长度：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Why keep the original sentence lengths? By zero-padding, we solved one technical
    problem but created another: if we naively pass these padded sentences through
    our RNN model as they are, it will process useless `PAD` symbols. This would both
    harm model correctness by processing “noise” and increase computation time. We
    resolve this issue by first storing the original lengths in the `seqlens` array
    and then telling TensorFlow’s `tf.nn.dynamic_rnn()` where each sentence ends.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么保留原始句子长度？通过零填充，我们解决了一个技术问题，但又创建了另一个问题：如果我们简单地将这些填充的句子通过我们的RNN模型，它将处理无用的`PAD`符号。这将通过处理“噪音”损害模型的正确性，并增加计算时间。我们通过首先将原始长度存储在`seqlens`数组中，然后告诉TensorFlow的`tf.nn.dynamic_rnn()`每个句子的结束位置来解决这个问题。
- en: In this chapter, our data is simulated—generated by us. In real applications,
    we would start off by getting a collection of documents (e.g., one-sentence tweets)
    and then mapping each word to an integer ID.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的数据是模拟的——由我们生成。在实际应用中，我们将首先获得一系列文档（例如，一句话的推文），然后将每个单词映射到一个整数ID。
- en: 'So, we now map words to indices—word *identifiers*—by simply creating a dictionary
    with words as keys and indices as values. We also create the inverse map. Note
    that there is no correspondence between the word IDs and the digits each word
    represents—the IDs carry no semantic meaning, just as in any NLP application with
    real data:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在将单词映射到索引—单词*标识符*—通过简单地创建一个以单词为键、索引为值的字典。我们还创建了反向映射。请注意，单词ID和每个单词代表的数字之间没有对应关系—ID没有语义含义，就像在任何具有真实数据的NLP应用中一样：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is a supervised classification task—we need an array of labels in the one-hot
    format, train and test sets, a function to generate batches of instances, and
    placeholders, as usual.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个监督分类任务—我们需要一个以one-hot格式的标签数组，训练和测试集，一个生成实例批次的函数和占位符，和通常一样。
- en: 'First, we create the labels and split the data into train and test sets:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建标签并将数据分为训练集和测试集：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we create a function that generates batches of sentences. Each sentence
    in a batch is simply a list of integer IDs corresponding to words:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个生成句子批次的函数。每个批次中的句子只是一个对应于单词的整数ID列表：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we create placeholders for data:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为数据创建占位符：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that we have created a placeholder for the original sequence lengths. We
    will see how to make use of these in our RNN shortly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经为原始序列长度创建了占位符。我们将很快看到如何在我们的RNN中使用这些。
- en: Supervised Word Embeddings
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督词嵌入
- en: Our text data is now encoded as lists of word IDs—each sentence is a sequence
    of integers corresponding to words. This type of *atomic* representation, where
    each word is represented with an ID,  is not scalable for training deep learning
    models with large vocabularies that occur in real problems. We could end up with
    millions of such word IDs, each encoded in one-hot (binary) categorical form, leading
    to great data sparsity and computational issues. We will discuss this in more
    depth in [Chapter 6](ch06.html#text_ii).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文本数据现在被编码为单词ID列表—每个句子是一个对应于单词的整数序列。这种*原子*表示，其中每个单词用一个ID表示，对于训练具有大词汇量的深度学习模型来说是不可扩展的，这在实际问题中经常出现。我们可能会得到数百万这样的单词ID，每个以one-hot（二进制）分类形式编码，导致数据稀疏和计算问题。我们将在[第6章](ch06.html#text_ii)中更深入地讨论这个问题。
- en: A powerful approach to work around this issue is to use word embeddings. The
    embedding is, in a nutshell, simply a mapping from high-dimensional one-hot vectors
    encoding words to lower-dimensional dense vectors. So, for example, if our vocabulary
    has size 100,000, each word in one-hot representation would be of the same size.
     The corresponding word vector—or *word embedding*—would be of size 300, say.
    The high-dimensional one-hot vectors are thus “embedded” into a continuous vector
    space with a much lower dimensionality.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个强大方法是使用词嵌入。嵌入本质上只是将编码单词的高维度one-hot向量映射到较低维度稠密向量。因此，例如，如果我们的词汇量大小为100,000，那么每个单词在one-hot表示中的大小将相同。相应的单词向量或*词嵌入*大小为300。因此，高维度的one-hot向量被“嵌入”到具有更低维度的连续向量空间中。
- en: In [Chapter 6](ch06.html#text_ii) we dive deeper into word embeddings, exploring
    a popular method to train them in an “unsupervised” manner known as word2vec.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.html#text_ii)中，我们深入探讨了词嵌入，探索了一种流行的无监督训练方法，即word2vec。
- en: Here, our end goal is to solve a text classification problem, and we will train
    word vectors in a supervised framework, tuning the embedded word vectors to solve
    the downstream classification task.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的最终目标是解决文本分类问题，并且我们将在监督框架中训练词向量，调整嵌入的词向量以解决下游分类任务。
- en: 'It is helpful to think of word embeddings as basic hash tables or lookup tables,
    mapping words to their dense vector values. These vectors are optimized as part
    of the training process. Previously, we gave each word an integer index, and sentences
    are then represented as sequences of these indices. Now, to obtain a word’s vector,
    we use the built-in `tf.nn.embedding_lookup()` function, which efficiently retrieves
    the vectors for each word in a given sequence of word indices:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词嵌入视为基本的哈希表或查找表是有帮助的，将单词映射到它们的密集向量值。这些向量是作为训练过程的一部分进行优化的。以前，我们给每个单词一个整数索引，然后句子表示为这些索引的序列。现在，为了获得一个单词的向量，我们使用内置的`tf.nn.embedding_lookup()`函数，它有效地检索给定单词索引序列中每个单词的向量：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We will see examples of and visualizations of our vector representations of
    words shortly.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将看到单词向量表示的示例和可视化。
- en: LSTM and Using Sequence Length
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM和使用序列长度
- en: In the introductory RNN example with which we began, we implemented and used
    the basic vanilla RNN model. In practice, we often use slightly more advanced
    RNN models, which differ mainly by how they update their hidden state and propagate
    information through time. A very popular recurrent network is the long short-term
    memory (LSTM) network. It differs from vanilla RNN by having some special *memory
    mechanisms* that enable the recurrent cells to better store information for long
    periods of time, thus allowing them to capture long-term dependencies better than
    plain RNN.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始的介绍性RNN示例中，我们实现并使用了基本的vanilla RNN模型。在实践中，我们经常使用略微更高级的RNN模型，其主要区别在于它们如何更新其隐藏状态并通过时间传播信息。一个非常流行的循环网络是长短期记忆（LSTM）网络。它通过具有一些特殊的*记忆机制*与vanilla
    RNN不同，这些机制使得循环单元能够更好地存储信息长时间，从而使它们能够比普通RNN更好地捕获长期依赖关系。
- en: There is nothing mysterious about these memory mechanisms; they simply consist
    of some more parameters added to each recurrent cell, enabling the RNN to overcome
    optimization issues and propagate information. These trainable parameters act
    as filters that select what information is worth “remembering” and passing on,
    and what is worth “forgetting.” They are trained in exactly the same way as any
    other parameter in a network, with gradient-descent algorithms and backpropagation.
    We don’t go into the more technical mathematical formulations here, but there
    are plenty of great resources out there delving into the details.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些记忆机制并没有什么神秘之处；它们只是由一些添加到每个循环单元的更多参数组成，使得RNN能够克服优化问题并传播信息。这些可训练参数充当过滤器，选择哪些信息值得“记住”和传递，哪些值得“遗忘”。它们的训练方式与网络中的任何其他参数完全相同，使用梯度下降算法和反向传播。我们在这里不深入讨论更多技术数学公式，但有很多很好的资源深入探讨细节。
- en: 'We create an LSTM cell with `tf.contrib.rnn.BasicLSTMCell()` and feed it to
    `tf.nn.dynamic_rnn()`, just as we did at the start of this chapter. We also give
    `dynamic_rnn()` the length of each sequence in a batch of examples, using the
    `_seqlens` placeholder we created earlier. TensorFlow uses this to stop all RNN
    steps beyond the last real sequence element. It also returns all output vectors
    over time (in the `outputs` tensor), which are all zero-padded beyond the true
    end of the sequence. So, for example, if the length of our original sequence is
    5 and we zero-pad it to a sequence of length 15, the output for all time steps
    beyond 5 will be zero:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`tf.contrib.rnn.BasicLSTMCell()`创建一个LSTM单元，并将其提供给`tf.nn.dynamic_rnn()`，就像我们在本章开始时所做的那样。我们还使用我们之前创建的`_seqlens`占位符给`dynamic_rnn()`提供每个示例批次中每个序列的长度。TensorFlow使用这个长度来停止超出最后一个真实序列元素的所有RNN步骤。它还返回所有随时间的输出向量（在`outputs`张量中），这些向量在真实序列的真实结尾之后都是零填充的。因此，例如，如果我们原始序列的长度为5，并且我们将其零填充为长度为15的序列，则超过5的所有时间步的输出将为零：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We take the last valid output vector—in this case conveniently available for
    us in the `states` tensor returned by `dynamic_rnn()`—and pass it through a linear
    layer (and the softmax function), using it as our final prediction. We will explore
    the concepts of last relevant output and zero-padding further in the next section,
    when we look at some outputs generated by `dynamic_rnn()` for our example sentences.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取最后一个有效的输出向量——在这种情况下，方便地在`dynamic_rnn()`返回的`states`张量中可用，并通过一个线性层（和softmax函数）传递它，将其用作我们的最终预测。在下一节中，当我们查看`dynamic_rnn()`为我们的示例句子生成的一些输出时，我们将进一步探讨最后相关输出和零填充的概念。
- en: Training Embeddings and the LSTM Classifier
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练嵌入和LSTM分类器
- en: 'We have all the pieces in the puzzle. Let’s put them together, and complete
    an end-to-end training of both word vectors and a classification model:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了拼图的所有部分。让我们把它们放在一起，完成端到端的单词向量和分类模型的训练：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As we can see, this is a pretty simple toy text classification problem:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，这是一个非常简单的玩具文本分类问题：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We’ve also computed an example batch of outputs generated by `dynamic_rnn()`,
    to further illustrate the concepts of zero-padding and last relevant outputs discussed
    in the previous section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还计算了由`dynamic_rnn()`生成的一个示例批次的输出，以进一步说明在前一节中讨论的零填充和最后相关输出的概念。
- en: 'Let’s take a look at one example of these outputs, for a sentence that was
    zero-padded (in your random batch of data you may see different output, of course—look
    for a sentence whose `seqlen` was lower than the maximal 6):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个这些输出的例子，对于一个被零填充的句子（在您的随机数据批次中，您可能会看到不同的输出，当然要寻找一个`seqlen`小于最大6的句子）：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This output has, as expected, six time steps, each a vector of size 32\. Let’s
    take a glimpse at its values (printing only the first few dimensions to avoid
    clutter):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出有如预期的六个时间步，每个大小为32的向量。让我们看一眼它的值（只打印前几个维度以避免混乱）：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We see that for this sentence, whose original length was 4, the last two time
    steps have zero vectors due to padding.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，对于这个句子，原始长度为4，最后两个时间步由于填充而具有零向量。
- en: 'Finally, we look at the states vector returned by `dynamic_rnn()`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看一下`dynamic_rnn()`返回的状态向量：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We can see that it conveniently stores for us the last relevant output vector—its
    values match the last relevant output vector before zero-padding.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它方便地为我们存储了最后一个相关输出向量——其值与零填充之前的最后一个相关输出向量匹配。
- en: At this point, you may be wondering how to access and manipulate the word vectors
    and explore the trained representation. We show how to do so, including interactive
    embedding visualization, in the next chapter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能想知道如何访问和操作单词向量，并探索训练后的表示。我们将展示如何做到这一点，包括交互式嵌入可视化，在下一章中。
- en: Stacking multiple LSTMs
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠多个LSTMs
- en: Earlier, we focused on a one-layer LSTM network for ease of exposition. Adding
    more layers is straightforward, using the `MultiRNNCell()` wrapper that combines
    multiple RNN cells into one multilayer cell.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们专注于一个单层LSTM网络以便更容易解释。添加更多层很简单，使用`MultiRNNCell()`包装器将多个RNN单元组合成一个多层单元。
- en: 'Say, for example, we wanted to stack two LSTM layers in the preceding example.
    We can do this as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们想在前面的例子中堆叠两个LSTM层。我们可以这样做：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We first define an LSTM cell as before, and then feed it into the `tf.contrib.rnn.MultiRNNCell()`
    wrapper.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先像以前一样定义一个LSTM单元，然后将其馈送到`tf.contrib.rnn.MultiRNNCell()`包装器中。
- en: 'Now our network has two layers of LSTM, causing some shape issues when trying
    to extract the final state vectors. To get the final state of the second layer,
    we simply adapt our indexing a bit:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的网络有两层LSTM，当尝试提取最终状态向量时会出现一些形状问题。为了获得第二层的最终状态，我们只需稍微调整我们的索引：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we introduced sequence models in TensorFlow. We saw how to implement
    a basic RNN model from scratch by using `tf.scan()` and built-in modules, as well
    as more advanced LSTM networks, for both text and image data. Finally, we trained
    an end-to-end text classification RNN with word embeddings, and showed how to
    handle sequences of variable length. In the next chapter, we dive deeper into
    word embeddings and word2vec. In [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications),
    we will see some cool abstraction layers over TensorFlow, and how they can be
    used to train advanced text classification RNN models with considerably less effort.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了在TensorFlow中的序列模型。我们看到如何通过使用`tf.scan()`和内置模块来实现基本的RNN模型，以及更高级的LSTM网络，用于文本和图像数据。最后，我们训练了一个端到端的文本分类RNN模型，使用了词嵌入，并展示了如何处理可变长度的序列。在下一章中，我们将深入探讨词嵌入和word2vec。在[第7章](ch07.html#tensorflow_abstractions_and_simplifications)中，我们将看到一些很酷的TensorFlow抽象层，以及它们如何用于训练高级文本分类RNN模型，而且付出的努力要少得多。
