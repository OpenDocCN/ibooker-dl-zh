- en: 'Chapter 5\. Text I: Working with Text and Sequences, and TensorBoard Visualization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we show how to work with sequences in TensorFlow, and in particular
    text. We begin by introducing recurrent neural networks (RNNs), a powerful class
    of deep learning algorithms particularly useful and popular in natural language
    processing (NLP). We show how to implement RNN models from scratch, introduce
    some important TensorFlow capabilities, and visualize the model with the interactive
    TensorBoard. We then explore how to use an RNN in a supervised text classification
    problem with word-embedding training. Finally, we show how to build a more advanced
    RNN model with long short-term memory (LSTM) networks and how to handle sequences
    of variable length.
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Sequence Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in the previous chapter that using the spatial structure of images can
    lead to advanced models with excellent results. As discussed in that chapter,
    exploiting structure is the key to success. As we will see shortly, an immensely
    important and useful type of structure is the sequential structure. Thinking in
    terms of data science, this fundamental structure appears in many datasets, across
    all domains. In computer vision, video is a sequence of visual content evolving
    over time. In speech we have audio signals, in genomics gene sequences; we have
    longitudinal medical records in healthcare, financial data in the stock market,
    and so on (see [Figure 5-1](#ubiquity_of_sequence_data)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. The ubiquity of sequence data.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A particularly important type of data with strong sequential structure is natural
    language—text data. Deep learning methods that exploit the sequential structure
    inherent in texts—characters, words, sentences, paragraphs, documents—are at the
    forefront of natural language understanding (NLU) systems, often leaving more
    traditional methods in the dust. There are a great many types of NLU tasks that
    are of interest to solve, ranging from document classification to building powerful
    language models, from answering questions automatically to generating human-level
    conversation agents. These tasks are fiendishly difficult, garnering the efforts
    and attention of the entire AI community in both academia and industry.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focus on the basic building blocks and tasks, and show how
    to work with sequences—primarily of text—in TensorFlow. We take a detailed deep
    dive into the core elements of sequence models in TensorFlow, implementing some
    of them from scratch, to gain a thorough understanding. In the next chapter we
    show more advanced text modeling techniques with TensorFlow, and in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications)
    we use abstraction libraries that offer simpler, high-level ways to implement
    our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the most important and popular class of deep learning models
    for sequences (in particular, text): recurrent neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are a powerful and widely used class of neural network
    architectures for modeling sequence data. The basic idea behind RNN models is
    that each new element in the sequence contributes some new information, which
    updates the current state of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, which explored computer vision with CNN models, we
    discussed how those architectures are inspired by the current scientific perceptions
    of the way the human brain processes visual information. These scientific perceptions
    are often rather close to our commonplace intuition from our day-to-day lives
    about how we process sequential information.
  prefs: []
  type: TYPE_NORMAL
- en: When we receive new information, clearly our “history” and “memory” are not
    wiped out, but instead “updated.” When we read a sentence in some text, with each
    new word, our current state of information is updated, and it is dependent not
    only on the new observed word but on the words that preceded it.
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental mathematical construct in statistics and probability, which is
    often used as a building block for modeling sequential patterns via machine learning
    is the Markov chain model. Figuratively speaking, we can view our data sequences
    as “chains,” with each node in the chain dependent in some way on the previous
    node, so that “history” is not erased but carried on.
  prefs: []
  type: TYPE_NORMAL
- en: RNN models are also based on this notion of chain structure, and vary in how
    exactly they maintain and update information.  As their name implies, recurrent
    neural nets apply some form of “loop.” As seen in [Figure 5-2](#recurrent_neural_nets),
    at some point in time *t*, the network observes an input *x[t]* (a word in a sentence)
    and updates its “state vector” to *h[t]* from the previous vector *h[t-1]*. When
    we process new input (the next word), it will be done in some manner that is dependent
    on *h[t]* and thus on the history of the sequence (the previous words we’ve seen
    affect our understanding of the current word). As seen in the illustration, this
    recurrent structure can simply be viewed as one long unrolled chain, with each
    node in the chain performing the same kind of processing “step” based on the “message”
    it obtains from the output of the previous node. This, of course, is very related
    to the Markov chain models discussed previously and their hidden Markov model
    (HMM) extensions, which are not discussed in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Recurrent neural networks updating with new information received
    over time.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Vanilla RNN Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we implement a basic RNN from scratch, explore its inner workings,
    and gain insight into how TensorFlow can work with sequences. We introduce some
    powerful, fairly low-level tools that TensorFlow provides for working with sequence
    data, which you can use to implement your own systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will show how to use higher-level TensorFlow RNN modules.
  prefs: []
  type: TYPE_NORMAL
- en: We begin with defining our basic model mathematically. This mainly consists
    of defining the recurrence structure—the RNN update step.
  prefs: []
  type: TYPE_NORMAL
- en: The update step for our simple vanilla RNN is
  prefs: []
  type: TYPE_NORMAL
- en: '*h[t]* = *tanh*(*W[x]**x[t]* + *W[h]h[t-1]* + *b*)'
  prefs: []
  type: TYPE_NORMAL
- en: where *W[h]*, *W[x]*, and *b* are weight and bias variables we learn, *tanh*(·)
    is the hyperbolic tangent function that has its range in [–1,1] and is strongly
    connected to the sigmoid function used in previous chapters, and *x[t]* and *h[t]*
    are the input and state vectors as defined previously. Finally, the hidden state
    vector is multiplied by another set of weights, yielding the outputs that appear
    in [Figure 5-2](#recurrent_neural_nets).
  prefs: []
  type: TYPE_NORMAL
- en: MNIST images as sequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get a first taste of the power and general applicability of sequence models,
    in this section we implement our first RNN to solve the MNIST image classification
    task that you are by now familiar with.  Later in this chapter we will focus on
    sequences of text, and see how neural sequence models can powerfully manipulate
    them and extract information to solve NLU tasks.
  prefs: []
  type: TYPE_NORMAL
- en: But, you may ask, what have images got to do with sequences?
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, the architecture of convolutional neural
    networks makes use of the spatial structure of images. While the structure of
    natural images is well suited for CNN models,  it is revealing to look at the
    structure of images from different angles. In a trend in cutting-edge deep learning
    research, advanced models attempt to exploit various kinds of sequential structures
    in images, trying to capture in some sense the “generative process” that created
    each image. Intuitively, this all comes down to the notion that nearby areas in
    images are somehow related, and trying to model this structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, to introduce basic RNNs and how to work with sequences, we take a simple
    sequential view of images: we look at each image in our data as a sequence of
    rows (or columns). In our MNIST data, this just means that each 28×28-pixel image
    can be viewed as a sequence of length 28, each element in the sequence a vector
    of 28 pixels (see [Figure 5-3](#sequence_of_pixel_columns)). Then, the temporal
    dependencies in the RNN can be imaged as a scanner head, scanning the image from
    top to bottom (rows) or left to right (columns).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. An image as a sequence of pixel columns.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We start by loading data, defining some parameters, and creating placeholders
    for our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`element_size` is the dimension of each vector in our sequence—in our case,
    a row/column of 28 pixels. `time_steps` is the number of such elements in a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in previous chapters, when we load data with the built-in MNIST data
    loader, it comes in unrolled form—a vector of 784 pixels. When we load batches
    of data during training (we’ll get to that later in this section), we simply reshape
    each unrolled vector to [`batch_size`, `time_steps`, `element_size`]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We set `hidden_layer_size` (arbitrarily to `128`, controlling the size of the
    hidden RNN state vector discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '`LOG_DIR` is the directory to which we save model summaries for TensorBoard
    visualization. You will learn what this means as we go.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will also briefly introduce TensorBoard visualizations.
    TensorBoard allows you to monitor and explore the model structure, weights, and
    training process, and requires some very simple additions to the code. More details
    are provided throughout this chapter and further along in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, our input and label placeholders are created with the suitable dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The RNN step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s implement the mathematical model for the RNN step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create a function used for logging summaries, which we will use later
    in TensorBoard to visualize our model and training process (it is not important
    to understand its technicalities at this stage):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the weight and bias variables used in the RNN step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Applying the RNN step with tf.scan()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now create a function that implements the vanilla RNN step we saw in the
    previous section using the variables we created. It should by now be straightforward
    to understand the TensorFlow code used here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we apply this function across all 28 time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this small code block, there are some important elements to understand. First,
    we reshape the inputs from `[batch_size, time_steps, element_size]` to `[time_steps,
    batch_size, element_size]`. The `perm` argument to `tf.transpose()` tells TensorFlow
    which axes we want to switch around. Now that the first axis in our input Tensor
    represents the time axis, we can iterate across all time steps by using the built-in
    `tf.scan()` function, which repeatedly applies a callable (function) to a sequence
    of elements in order, as explained in the following note.
  prefs: []
  type: TYPE_NORMAL
- en: tf.scan()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This important function was added to TensorFlow to allow us to introduce loops
    into the computation graph, instead of just “unrolling” the loops explicitly by
    adding more and more replications of the same operations. More technically, it
    is a higher-order function very similar to the reduce operator, but it returns
    all intermediate accumulator values over time. There are several advantages to
    this approach, chief among them the ability to have a dynamic number of iterations
    rather than fixed, computational speedups and optimizations for graph construction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the use of this function, consider the following simple example
    (which is separate from the overall RNN code in this section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we use `tf.scan()` to sequentially concatenate characters to a
    string, in a manner analogous to the arithmetic cumulative sum.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we saw earlier, in an RNN we get a state vector for each time step, multiply
    it by some weights, and get an output vector—our new representation of the data.
    Let’s implement this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Our input to the RNN is sequential, and so is our output. In this sequence classification
    example, we take the last state vector and pass it through a fully connected linear
    layer to extract an output vector (which will later be passed through a softmax
    activation function to generate predictions). This is common practice in basic
    sequence classification, where we assume that the last state vector has “accumulated”
    information representing the entire sequence.
  prefs: []
  type: TYPE_NORMAL
- en: To implement this, we first define the linear layer’s weights and bias term
    variables, and create a factory function for this layer. Then we apply this layer
    to all outputs with `tf.map_fn()`, which is pretty much the same as the typical
    map function that applies functions to sequences/iterables in an element-wise
    manner, in this case on each element in our sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we extract the last output for each instance in the batch, with negative
    indexing (similarly to ordinary Python). We will see some more ways to do this
    later and investigate outputs and states in some more depth.
  prefs: []
  type: TYPE_NORMAL
- en: RNN classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re now ready to train a classifier, much in the same way we did in the previous
    chapters. We define the ops for loss function computation, optimization, and prediction,
    add some more summaries for TensorBoard, and merge all these summaries into one
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By now you should be familiar with most of the components used for defining
    the loss function and optimization. Here, we used the `RMSPropOptimizer`, implementing
    a well-known and strong gradient descent algorithm, with some standard hyperparameters.
    Of course, we could have used any other optimizer (and do so throughout this book!).
  prefs: []
  type: TYPE_NORMAL
- en: We create a small test set with unseen MNIST images, and add some more technical
    ops and commands for logging summaries that we will use in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the model and check out the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print some training and testing accuracy results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To summarize this section, we started off with the raw MNIST pixels and regarded
    them as sequential data—each column (or row) of 28 pixels as a time step. We then
    applied the vanilla RNN to extract outputs corresponding to each time-step and
    used the last output to perform classification of the entire sequence (image).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the model with TensorBoard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorBoard is an interactive browser-based tool that allows us to visualize
    the learning process, as well as explore our trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run TensorBoard, go to the command terminal and tell TensorBoard where the
    relevant summaries you logged are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `*LOG_DIR*` should be replaced with your log directory. If you are on
    Windows and this is not working, make sure you are running the terminal from the
    same drive where the log data is, and add a name to the log directory as follows
    in order to bypass a bug in the way TensorBoard parses the path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorBoard allows us to assign names to individual log directories by putting
    a colon between the name and the path, which may be useful when working with multiple
    log directories. In such a case, we pass a comma-separated list of log directories
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example (with one log directory), once you have run the `tensorboard`
    command, you should get something like the following, telling you where to navigate
    in your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If the address does not work, go to *localhost:6006*, which should always work.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard recursively walks the directory tree rooted at `*LOG_DIR*` looking
    for subdirectories that contain tfevents log data. If you run this example multiple
    times, make sure to either delete the `*LOG_DIR*` folder you created after each
    run, or write the logs to separate subdirectories within `*LOG_DIR*`, such as
    `*LOG_DIR*`*/run1/train*, `*LOG_DIR*`*/run2/train*, and so forth, to avoid issues
    with overwriting log files, which may lead to some “funky” plots.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at some of the visualizations we can get. In the next section,
    we will explore interactive visualization of high-dimensional data with TensorBoard—for
    now, we focus on plotting training process summaries and trained weights.
  prefs: []
  type: TYPE_NORMAL
- en: First, in your browser, go to the Scalars tab. Here TensorBoard shows us summaries
    of all scalars, including not only training and testing accuracy, which are usually
    most interesting, but also some summary statistics we logged about variables (see
    [Figure 5-4](#tensorboard_scalar_summaries)). Hovering over the plots, we can
    see some numerical figures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. TensorBoard scalar summaries.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the Graphs tab we can get an interactive visualization of our computation
    graph, from a high-level view down to the basic ops, by zooming in (see [Figure 5-5](#zooming_into_computation_graph)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Zooming in on the computation graph.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, in the Histograms tab we see histograms of our weights across the training
    process (see [Figure 5-6](#weights_throughout_learning_process)). Of course, we
    had to explicitly add these histograms to our logging in order to view them, with
    `tf.summary.histogram()`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Histograms of weights throughout the learning process.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorFlow Built-in RNN Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding example taught us some of the fundamental and powerful ways we
    can work with sequences, by implementing our graph pretty much from scratch. In
    practice, it is of course a good idea to use built-in higher-level modules and
    functions. This not only makes the code shorter and easier to write, but exploits
    many low-level optimizations afforded by TensorFlow implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section we first present a new, shorter version of the code in its
    entirety. Since most of the overall details have not changed, we focus on the
    main new elements, `tf.contrib.rnn.BasicRNNCell` and `tf.nn.dynamic_rnn()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: tf.contrib.rnn.BasicRNNCell and tf.nn.dynamic_rnn()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorFlow’s RNN cells are abstractions that represent the basic operations
    each recurrent “cell” carries out (see [Figure 5-2](#recurrent_neural_nets) at
    the start of this chapter for an illustration), and its associated state. They
    are, in general terms, a “replacement” of the `rnn_step()` function and the associated
    variables it required. Of course, there are many variants and types of cells,
    each with many methods and properties. We will see some more advanced cells toward
    the end of this chapter and later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have created the `rnn_cell`, we feed it into `tf.nn.dynamic_rnn()`.
    This function replaces `tf.scan()` in our vanilla implementation and creates an
    RNN specified by `rnn_cell`.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, in early 2017, TensorFlow includes a static and a dynamic
    function for creating an RNN. What does this mean? The static version creates
    an unrolled graph (as in [Figure 5-2](#recurrent_neural_nets)) of fixed length.
    The dynamic version uses a `tf.While` loop to dynamically construct the graph
    at execution time, leading to faster graph creation, which can be significant.
    This dynamic construction can also be very useful in other ways, some of which
    we will touch on when we discuss variable-length sequences toward the end of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note that *contrib* refers to the fact that code in this library is contributed
    and still requires testing. We discuss the `contrib` library in much more detail
    in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications). `BasicRNNCell`
    was moved to `contrib` in TensorFlow 1.0 as part of ongoing development. In version
    1.2, many of the RNN functions and classes were moved back to the core namespace
    with aliases kept in `contrib` for backward compatibiliy, meaning that the preceding
    code works for all versions 1.X as of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: RNN for Text Sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began this chapter by learning how to implement RNN models in TensorFlow.
    For ease of exposition, we showed how to implement and use an RNN for a sequence
    made of pixels in MNIST images. We next show how to use these sequence models
    on text sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Text data has some properties distinctly different from image data, which we
    will discuss here and later in this book. These properties can make it somewhat
    difficult to handle text data at first, and text data always requires at least
    some basic pre-processing steps for us to be able to work with it. To introduce
    working with text in TensorFlow, we will thus focus on the core components and
    create a minimal, contrived text dataset that will let us get straight to the
    action. In [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications),
    we will apply RNN models to movie review sentiment classification.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started, presenting our example data and discussing some key properties
    of text datasets as we go.
  prefs: []
  type: TYPE_NORMAL
- en: Text Sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the MNIST RNN example we saw earlier, each sequence was of fixed size—the
    width (or height) of an image. Each element in the sequence was a dense vector
    of 28 pixels. In NLP tasks and datasets, we have a different kind of “picture.”
  prefs: []
  type: TYPE_NORMAL
- en: Our sequences could be of words forming a sentence, of sentences forming a paragraph,
    or even of characters forming words or paragraphs forming whole documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following sentence: “Our company provides smart agriculture solutions
    for farms, with advanced AI, deep-learning.” Say we obtain this sentence from
    an online news blog, and wish to process it as part of our machine learning system.'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the words in this sentence would be represented with an ID—an integer,
    commonly referred to as a token ID in NLP. So, the word “agriculture” could, for
    instance, be mapped to the integer 3452, the word “farm” to 12, “AI” to 150, and
    “deep-learning” to 0\. This representation in terms of integer identifiers is
    very different from the vector of pixels in image data, in multiple ways. We will
    elaborate on this important point shortly when we discuss word embeddings, and
    in [Chapter 6](ch06.html#text_ii).
  prefs: []
  type: TYPE_NORMAL
- en: To make things more concrete, let’s start by creating our simplified text data.
  prefs: []
  type: TYPE_NORMAL
- en: Our simulated data consists of two classes of very short “sentences,” one composed
    of odd digits and the other of even digits (with numbers written in English).
    We generate sentences built of words representing even and odd numbers. Our goal
    is to learn to classify each sentence as either odd or even in a supervised text-classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we do not really need any machine learning for this simple task—we
    use this contrived example only for illustrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define some constants, which will be explained as we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create sentences. We sample random digits and map them to the corresponding
    “words” (e.g., 1 is mapped to “One,” 7 to “Seven,” etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Text sequences typically have variable lengths, which is of course the case
    for all real natural language data (such as in the sentences appearing on this
    page).
  prefs: []
  type: TYPE_NORMAL
- en: To make our simulated sentences have different lengths, we sample for each sentence
    a random length between 3 and 6 with `np.random.choice(range(3, 7))`—the lower
    bound is inclusive, and the upper bound is exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to put all our input sentences in one tensor (per batch of data instances),
    we need them to somehow be of the same size—so we pad sentences with a length
    shorter than 6 with zeros (or *PAD* symbols) to make all sentences equally sized
    (artificially). This pre-processing step is known as *zero-padding*. The following
    code accomplishes all of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at our sentences, each padded to length 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we add the *PAD* word (token) to our data and `digit_to_word_map`
    dictionary, and separately store even and odd sentences and their original lengths
    (before padding).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the original sequence lengths for the sentences we printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Why keep the original sentence lengths? By zero-padding, we solved one technical
    problem but created another: if we naively pass these padded sentences through
    our RNN model as they are, it will process useless `PAD` symbols. This would both
    harm model correctness by processing “noise” and increase computation time. We
    resolve this issue by first storing the original lengths in the `seqlens` array
    and then telling TensorFlow’s `tf.nn.dynamic_rnn()` where each sentence ends.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, our data is simulated—generated by us. In real applications,
    we would start off by getting a collection of documents (e.g., one-sentence tweets)
    and then mapping each word to an integer ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we now map words to indices—word *identifiers*—by simply creating a dictionary
    with words as keys and indices as values. We also create the inverse map. Note
    that there is no correspondence between the word IDs and the digits each word
    represents—the IDs carry no semantic meaning, just as in any NLP application with
    real data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This is a supervised classification task—we need an array of labels in the one-hot
    format, train and test sets, a function to generate batches of instances, and
    placeholders, as usual.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the labels and split the data into train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a function that generates batches of sentences. Each sentence
    in a batch is simply a list of integer IDs corresponding to words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create placeholders for data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have created a placeholder for the original sequence lengths. We
    will see how to make use of these in our RNN shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Word Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our text data is now encoded as lists of word IDs—each sentence is a sequence
    of integers corresponding to words. This type of *atomic* representation, where
    each word is represented with an ID,  is not scalable for training deep learning
    models with large vocabularies that occur in real problems. We could end up with
    millions of such word IDs, each encoded in one-hot (binary) categorical form, leading
    to great data sparsity and computational issues. We will discuss this in more
    depth in [Chapter 6](ch06.html#text_ii).
  prefs: []
  type: TYPE_NORMAL
- en: A powerful approach to work around this issue is to use word embeddings. The
    embedding is, in a nutshell, simply a mapping from high-dimensional one-hot vectors
    encoding words to lower-dimensional dense vectors. So, for example, if our vocabulary
    has size 100,000, each word in one-hot representation would be of the same size.
     The corresponding word vector—or *word embedding*—would be of size 300, say.
    The high-dimensional one-hot vectors are thus “embedded” into a continuous vector
    space with a much lower dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.html#text_ii) we dive deeper into word embeddings, exploring
    a popular method to train them in an “unsupervised” manner known as word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: Here, our end goal is to solve a text classification problem, and we will train
    word vectors in a supervised framework, tuning the embedded word vectors to solve
    the downstream classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is helpful to think of word embeddings as basic hash tables or lookup tables,
    mapping words to their dense vector values. These vectors are optimized as part
    of the training process. Previously, we gave each word an integer index, and sentences
    are then represented as sequences of these indices. Now, to obtain a word’s vector,
    we use the built-in `tf.nn.embedding_lookup()` function, which efficiently retrieves
    the vectors for each word in a given sequence of word indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We will see examples of and visualizations of our vector representations of
    words shortly.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM and Using Sequence Length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the introductory RNN example with which we began, we implemented and used
    the basic vanilla RNN model. In practice, we often use slightly more advanced
    RNN models, which differ mainly by how they update their hidden state and propagate
    information through time. A very popular recurrent network is the long short-term
    memory (LSTM) network. It differs from vanilla RNN by having some special *memory
    mechanisms* that enable the recurrent cells to better store information for long
    periods of time, thus allowing them to capture long-term dependencies better than
    plain RNN.
  prefs: []
  type: TYPE_NORMAL
- en: There is nothing mysterious about these memory mechanisms; they simply consist
    of some more parameters added to each recurrent cell, enabling the RNN to overcome
    optimization issues and propagate information. These trainable parameters act
    as filters that select what information is worth “remembering” and passing on,
    and what is worth “forgetting.” They are trained in exactly the same way as any
    other parameter in a network, with gradient-descent algorithms and backpropagation.
    We don’t go into the more technical mathematical formulations here, but there
    are plenty of great resources out there delving into the details.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create an LSTM cell with `tf.contrib.rnn.BasicLSTMCell()` and feed it to
    `tf.nn.dynamic_rnn()`, just as we did at the start of this chapter. We also give
    `dynamic_rnn()` the length of each sequence in a batch of examples, using the
    `_seqlens` placeholder we created earlier. TensorFlow uses this to stop all RNN
    steps beyond the last real sequence element. It also returns all output vectors
    over time (in the `outputs` tensor), which are all zero-padded beyond the true
    end of the sequence. So, for example, if the length of our original sequence is
    5 and we zero-pad it to a sequence of length 15, the output for all time steps
    beyond 5 will be zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We take the last valid output vector—in this case conveniently available for
    us in the `states` tensor returned by `dynamic_rnn()`—and pass it through a linear
    layer (and the softmax function), using it as our final prediction. We will explore
    the concepts of last relevant output and zero-padding further in the next section,
    when we look at some outputs generated by `dynamic_rnn()` for our example sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Training Embeddings and the LSTM Classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have all the pieces in the puzzle. Let’s put them together, and complete
    an end-to-end training of both word vectors and a classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, this is a pretty simple toy text classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We’ve also computed an example batch of outputs generated by `dynamic_rnn()`,
    to further illustrate the concepts of zero-padding and last relevant outputs discussed
    in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at one example of these outputs, for a sentence that was
    zero-padded (in your random batch of data you may see different output, of course—look
    for a sentence whose `seqlen` was lower than the maximal 6):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This output has, as expected, six time steps, each a vector of size 32\. Let’s
    take a glimpse at its values (printing only the first few dimensions to avoid
    clutter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We see that for this sentence, whose original length was 4, the last two time
    steps have zero vectors due to padding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we look at the states vector returned by `dynamic_rnn()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We can see that it conveniently stores for us the last relevant output vector—its
    values match the last relevant output vector before zero-padding.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may be wondering how to access and manipulate the word vectors
    and explore the trained representation. We show how to do so, including interactive
    embedding visualization, in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking multiple LSTMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, we focused on a one-layer LSTM network for ease of exposition. Adding
    more layers is straightforward, using the `MultiRNNCell()` wrapper that combines
    multiple RNN cells into one multilayer cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, for example, we wanted to stack two LSTM layers in the preceding example.
    We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We first define an LSTM cell as before, and then feed it into the `tf.contrib.rnn.MultiRNNCell()`
    wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now our network has two layers of LSTM, causing some shape issues when trying
    to extract the final state vectors. To get the final state of the second layer,
    we simply adapt our indexing a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we introduced sequence models in TensorFlow. We saw how to implement
    a basic RNN model from scratch by using `tf.scan()` and built-in modules, as well
    as more advanced LSTM networks, for both text and image data. Finally, we trained
    an end-to-end text classification RNN with word embeddings, and showed how to
    handle sequences of variable length. In the next chapter, we dive deeper into
    word embeddings and word2vec. In [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications),
    we will see some cool abstraction layers over TensorFlow, and how they can be
    used to train advanced text classification RNN models with considerably less effort.
  prefs: []
  type: TYPE_NORMAL
