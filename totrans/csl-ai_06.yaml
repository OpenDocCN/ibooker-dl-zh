- en: 5 Connecting causality and deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 连接因果性和深度学习
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Incorporating deep learning into a causal graphical model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将深度学习融入因果图模型
- en: Training a causal graphical model with a variational autoencoder
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用变分自动编码器训练因果图模型
- en: Using causal methods to enhance machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因果方法来增强机器学习
- en: 'The title of this book is *Causal AI*, but how exactly does causality connect
    to AI? More specifically, how does causality connect with deep learning, the dominant
    paradigm in AI? In this chapter, I look at this question from two perspectives:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的标题是《因果AI》，但因果性究竟是如何与AI联系起来的？更具体地说，因果性是如何与深度学习联系起来的，深度学习是AI的主导范式？在本章中，我从两个角度来探讨这个问题：
- en: '*How to incorporate deep learning into a causal model*—We’ll look at a causal
    model of a computer vision problem (section 5.1) and then train the deep causal
    image model (section 5.2).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何将深度学习融入因果模型*——我们将研究一个计算机视觉问题的因果模型（第5.1节），然后训练深度因果图像模型（第5.2节）。'
- en: '*How to use causal reasoning to do better deep learning*—We’ll look at a case
    study on independence of mechanism and semi-supervised learning (section 5.3.1
    and 5.3.2), and we’ll demystify deep learning with causality (section 5.3.3).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何利用因果推理进行更好的深度学习*——我们将研究关于机制独立性和半监督学习的案例研究（第5.3.1节和5.3.2节），并使用因果性来揭示深度学习的神秘面纱（第5.3.3节）。'
- en: The term *deep learning* broadly refers to applications of deep neural networks.
    It’s a machine learning approach that stacks many nonlinear models together in
    sequential layers, emulating the connections of neurons in brains. “Deep” refers
    to stacking many layers to achieve more modeling power, particularly in terms
    of modeling high-dimensional and nonlinear data, such as visual media and natural
    language text. Neural nets have been around for a while, but relatively recent
    advancements in hardware and automatic differentiation have made it possible to
    scale deep neural networks to extremely large sizes. That scaling is why, in recent
    years, there have been multiple cases of deep learning outperforming humans on
    many advanced inference and decision-making tasks, such as image recognition,
    natural language processing, game playing, medical diagnosis, autonomous driving,
    and generating lifelike text, images, and video.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: “深度学习”一词广泛指代深度神经网络的运用。这是一种机器学习方法，通过在序列层中堆叠许多非线性模型，模拟大脑中神经元的连接。“深度”指的是堆叠许多层以实现更强的建模能力，尤其是在建模高维和非线性数据方面，如视觉媒体和自然语言文本。“深度”一词指的是堆叠许多层以达到更强的建模能力，尤其是在建模高维和非线性数据方面，如视觉媒体和自然语言文本。神经网络已经存在了一段时间，但相对较近的硬件和自动微分技术的进步使得深度神经网络可以扩展到极其大的规模。正是这种扩展使得近年来，深度学习在许多高级推理和决策任务上超越了人类，例如图像识别、自然语言处理、游戏、医疗诊断、自动驾驶以及生成逼真的文本、图像和视频。
- en: But asking how deep learning connects to causality can elicit frustrating answers.
    AI company CEOs and leaders in big tech fuel hype about the power of deep learning
    models and even claim they can learn the causal structure of the world. On the
    other hand, some leading researchers claim these models are merely “stochastic
    parrots” that can echo patterns of correlation that, while nuanced and complex,
    still fall short of true causal understanding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但询问深度学习如何与因果性联系起来可能会得到令人沮丧的答案。AI公司的首席执行官和大型科技公司的领导者们炒作深度学习模型的力量，甚至声称它们可以学习世界的因果结构。另一方面，一些领先的研究人员声称这些模型仅仅是“随机鹦鹉”，它们可以回声相关模式，尽管这些模式细微而复杂，但仍然不足以达到真正的因果理解。
- en: Our goal in this chapter is to reconcile these perspectives. But skipping ahead,
    the main takeaway is that deep learning architecture can be integrated into a
    causal model and we can train the model using deep learning training techniques.
    But also, we can use causal reasoning to build better deep learning models and
    improve how we train them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是调和这些观点。但跳过前面的内容，主要的收获是深度学习架构可以集成到因果模型中，我们可以使用深度学习训练技术来训练模型。但不仅如此，我们还可以使用因果推理来构建更好的深度学习模型，并改进我们的训练方法。
- en: 'We’ll anchor this idea in two case studies:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过两个案例研究来巩固这一想法：
- en: Building a causal DAG for computer vision using a variational autoencoder
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用变分自动编码器为计算机视觉构建因果DAG
- en: Implementing better semi-supervised learning using independence of mechanism
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用机制独立性实现更好的半监督学习
- en: Other examples of the interplay of causality and AI that you’ll see in the rest
    of the book will build on the intuition we get from these case studies. For example,
    chapter 9 will illustrate counterfactual reasoning using a variational autoencoder
    like the one we’ll build in this chapter. In chapter 11, we’ll explore machine
    learning and probabilistic deep learning approaches for causal effect inference.
    Chapter 13 will show how to combine large language models and causal reasoning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，你将看到的因果与AI相互作用的其他示例将建立在我们从这些案例研究中获得的直觉之上。例如，第9章将使用本章我们将构建的变分自动编码器来展示反事实推理。在第11章中，我们将探讨因果效应推断的机器学习和概率深度学习方法。第13章将展示如何结合大型语言模型和因果推理。
- en: We’ll start by considering how to incorporate deep learning into a causal model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先考虑如何将深度学习融入因果模型中。
- en: 5.1 A causal model of a computer vision problem
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 计算机视觉问题的因果模型
- en: Let’s look at a computer vision problem that we can approach with a causal DAG.
    Recall the MNIST data from chapter 1, composed of images of digits and their labels,
    illustrated in figure 5.1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们可以用因果DAG来处理的一个计算机视觉问题。回忆一下第1章中的MNIST数据，由数字图像及其标签组成，如图5.1所示。
- en: '![figure](../Images/CH05_F01_Ness.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F01_Ness.png)'
- en: Figure 5.1 MNIST data featuring images of handwritten digits and their digit
    labels
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 MNIST数据，包括手写数字图像及其数字标签
- en: There is a related dataset called Typeface MNIST (TMNIST) that also features
    digit images and their digit labels. However, instead of handwritten digits, the
    images are digits rendered in 2,990 different fonts, illustrated in figure 5.2\.
    For each image, in addition to a digit label, there is a font label. Examples
    of the font labels include “GrandHotel-Regular,” “KulimPark-Regular,” and “Gorditas-Bold.”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个相关的数据集叫做Typeface MNIST（TMNIST），它也包含数字图像及其数字标签。然而，与手写数字不同，图像是渲染在2,990种不同字体中的数字，如图5.2所示。对于每个图像，除了数字标签外，还有一个字体标签。字体标签的例子包括“GrandHotel-Regular”、“KulimPark-Regular”和“Gorditas-Bold”。
- en: '![figure](../Images/CH05_F02_Ness.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F02_Ness.png)'
- en: Figure 5.2 Examples from the Typeface MNIST, which is composed of typed digits
    with different typefaces. In addition to a digit label for each digit, there is
    a label for one of 2,990 different typefaces (fonts).
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2 Typeface MNIST的示例，它由不同字体的数字组成。除了每个数字的数字标签外，还有一个标签表示2,990种不同字体（字体）中的一种。
- en: In this analysis, we’ll combine these datasets into one and build a simple deep
    causal generative model on that data. We’ll simplify the “fonts” label into a
    sample binary label that indicates “handwritten” for MNIST images and “typed”
    for the TMNIST images.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次分析中，我们将将这些数据集合并为一个，并在该数据上构建一个简单的深度因果生成模型。我们将“字体”标签简化为一个样本二进制标签，表示MNIST图像为“手写”，TMNIST图像为“打字”。
- en: We have seen how to build a causal generative model on top of a DAG. We factorized
    the joint distribution into a product of *causal Markov kernels* representing
    the conditional probability distributions for each node, conditional on their
    parents in the DAG. In our previous examples in pgmpy, we fit a conditional probability
    table for each of these kernels.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何在DAG之上构建因果生成模型。我们将联合分布分解为表示每个节点条件概率分布的因果马尔可夫核的乘积，这些核在DAG中基于其父节点。在我们之前的pgmpy示例中，我们为这些核中的每一个都拟合了一个条件概率表。
- en: You can imagine how hard it would be to use a conditional probability table
    to represent the conditional probability distribution of pixels in an image. But
    there is nothing stopping us from modeling the causal Markov kernel with a deep
    neural net, which we know is flexible enough to work with high-dimensional features
    like pixels. In this section, I’ll demonstrate how to use deep neural nets to
    model the causal Markov kernels defined by a causal DAG.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象使用条件概率表来表示图像中像素的条件概率分布会有多困难。但是，没有任何阻止我们用深度神经网络来建模因果马尔可夫核，我们知道它足够灵活，可以处理像像素这样的高维特征。在本节中，我将演示如何使用深度神经网络来建模由因果DAG定义的因果马尔可夫核。
- en: 5.1.1 Leveraging the universal function approximator
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 利用通用函数逼近器
- en: Deep learning is a highly effective universal function approximator. Let’s imagine
    there is a function that maps some set of inputs to some set of outputs, but we
    either don’t know the function or it’s too hard to write down in math or code.
    Given enough examples of those inputs and outputs, deep learning can approximate
    that function with high precision. Even if that function is nonlinear and high-dimensional,
    with enough data, deep learning will learn a good approximation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种高度有效的通用函数逼近器。让我们想象有一个函数将一组输入映射到一组输出，但我们要么不知道这个函数，要么很难用数学或代码表达它。给定足够的输入和输出的例子，深度学习可以以高精度近似该函数。即使该函数是非线性和高维的，只要有足够的数据，深度学习将学会一个好的近似。
- en: We regularly work with functions in causal modeling and inference, and sometimes
    it makes sense to approximate them, so long as the approximations preserve the
    causal information we care about. For example, the causal Markov property makes
    us interested in functions that map values of a node’s parents in the causal DAG
    to values (or probability values) of that node.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常在因果建模和推理中使用函数，有时对它们进行近似是有意义的，只要这些近似保留了我们关心的因果信息。例如，因果马尔可夫性质使我们感兴趣的是将因果有向图中节点父节点的值映射到该节点值（或概率值）的函数。
- en: In this section, we’ll do this mapping between a node and its parents with the
    variational autoencoder (VAE) framework. We’ll train two deep neural nets in the
    VAE, one of which maps parent cause variables to a distribution of the outcome
    variable, and another that maps the outcome variable to a distribution of the
    cause variables. This example will showcase the use of deep learning when causality
    is nonlinear and high-dimensional; the effect variable will be an image represented
    as a high-dimensional array, and the cause variables will represent the contents
    of the image.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用变分自编码器（VAE）框架来执行节点与其父节点之间的映射。在VAE中，我们将训练两个深度神经网络，其中一个将父原因变量映射到结果变量的分布，另一个将结果变量映射到原因变量的分布。这个例子将展示在因果是非线性和高维时深度学习的应用；效应变量将是一个表示为高维数组的图像，原因变量将代表图像的内容。
- en: 5.1.2 Causal abstraction and plate models
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 因果抽象和板模型
- en: But what does it mean to build a causal model of an image? Images are comprised
    of pixels arranged in a grid. As data, we can represent that pixel grid as a matrix
    of numerical values corresponding to color. In the case of both MNIST and TMNIST,
    the image is a 28 × 28 matrix of grayscale values, as illustrated in figure 5.3\.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但构建图像的因果模型意味着什么呢？图像由排列成网格的像素组成。作为数据，我们可以将像素网格表示为数值矩阵，这些数值对应于颜色。在MNIST和TMNIST的情况下，图像是一个28
    × 28的灰度值矩阵，如图5.3所示。
- en: '![figure](../Images/CH05_F03_Ness.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F03_Ness.png)'
- en: Figure 5.3 An MNIST image of “6” (left) and a TMNIST image of “7”. In their
    raw form, these are 28 × 28 matrices of numeric values corresponding to grayscale
    values.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3展示了“6”的MNIST图像（左侧）和“7”的TMNIST图像。在它们的原始形式中，这些是28 × 28的数值矩阵，对应于灰度值。
- en: A typical machine learning model looks at this 28 × 28 matrix of pixels as 28
    × 28 = 784 features. The machine learning algorithm learns statistical patterns
    connecting the pixels to one another and their labels. Based on this fact, one
    might be tempted to treat each individual pixel as a node in the naive causal
    DAG, as in figure 5.4, where for visual simplicity I’ve drawn 16 pixels (an arbitrary
    number) instead of all 784\.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的机器学习模型将这个28 × 28的像素矩阵视为784个特征。机器学习算法学习将像素及其标签连接起来的统计模式。基于这个事实，人们可能会倾向于将每个单独的像素视为朴素因果有向图中的一个节点，如图5.4所示，为了视觉上的简单，我画了16个像素（一个任意数）而不是所有的784个。
- en: '![figure](../Images/CH05_F04_Ness.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F04_Ness.png)'
- en: Figure 5.4 What a naive causal DAG might look like for an image represented
    by a 4 × 4 matrix
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4一个由4 × 4矩阵表示的图像的朴素因果有向图可能看起来是什么样子
- en: In figure 5.4, there are edges from the *digit* and *is-handwritten* variables
    to each pixel. Further, there are examples of edges representing possible causal
    relationships *between* pixels. Causal edges between pixels imply the color of
    one pixel is a cause of another. Perhaps most of these relationships are between
    nodes that are close, with a few far-reaching edges. But how would we know if
    one pixel causes another? If two pixels are connected, how would we know the direction
    of causality?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.4中，存在从*数字*和*手写*变量到每个像素的边缘。此外，还有一些表示像素之间可能因果关系的边缘示例。像素之间的因果边缘意味着一个像素的颜色是另一个像素的原因。也许这些关系大多数都在节点之间，只有少数是远距离的。但我们如何知道一个像素是否导致另一个像素？如果两个像素相连，我们如何知道因果关系的方向？
- en: Working at the right level of abstraction
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在正确的抽象层次上工作
- en: With these connections among only 16 pixels, the naive DAG in figure 5.4 is
    already quite unwieldy. It would be much worse with 784 pixels. Aside from the
    unwieldiness of a DAG, the problem with a pixel-level model is that our causal
    questions are generally not at the pixel level—we’d probably never ask “what is
    the causal effect of this pixel on that pixel?” In other words, the pixel is too
    low a level of abstraction, which is why thinking about causal relationships between
    individual pixels feels a bit absurd.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用16个像素之间的这些连接，图5.4中的朴素DAG就已经相当难以处理了。如果用784个像素，情况会更糟。除了DAG的难以处理之外，像素级模型的问题在于我们的因果问题通常不是在像素级别——我们可能永远不会问“这个像素对这个像素的因果效应是什么？”换句话说，像素的抽象层次太低，这就是为什么考虑单个像素之间的因果关系感觉有点荒谬。
- en: In applied statistics domains, such as econometrics, social science, public
    health, and business, our data has variables like per capita income, revenue,
    location, age, etc. These variables are typically already at the level of abstraction
    we want to think about when we get the data. But modern machine learning focuses
    on many perception problems from raw media, such as images, video, text, and sensor
    data. We don’t generally want to do causal reasoning at the low level of these
    features. Our causal questions are usually about the high-level abstractions behind
    these low-level features. We need to model at these higher abstraction levels.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用统计学领域，如计量经济学、社会科学、公共卫生和商业，我们的数据有诸如人均收入、收入、位置、年龄等变量。这些变量通常已经是我们获取数据时想要思考的抽象层次。但现代机器学习专注于从原始媒体（如图像、视频、文本和传感器数据）中提取的许多感知问题。我们通常不希望在低级特征上进行因果推理。我们的因果问题通常关于这些低级特征背后的高级抽象。我们需要在这些更高抽象层次上建模。
- en: Instead of thinking about individual pixels, we’ll think about the entire image.
    We’ll define a variable *X* to represent how the image appears; i.e., *X* is a
    matrix random variable representing pixels. Figure 5.5 illustrates a causal DAG
    for the TMNIST case. Simply put, the identity of the digits (0–9) and the font
    (2,990 possible values) are the causes, and the image is the effect.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会考虑单个像素，而是会考虑整个图像。我们将定义一个变量*X*来表示图像的外观；即，*X*是一个表示像素的矩阵随机变量。图5.5说明了TMNIST案例的因果DAG。简单来说，数字（0-9）和字体（2,990种可能值）是原因，图像是结果。
- en: '![figure](../Images/CH05_F05_Ness.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F05_Ness.png)'
- en: Figure 5.5 A simple causal DAG that represents the implied DGP behind Typeface
    MNIST
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 表示Typeface MNIST背后隐含DGP的简单因果DAG
- en: In this case, we are using the causal DAG to make an assertion that the label
    causes the image. That is not always the case, as we’ll discuss in our case study
    on semi-supervised learning in section 5.3\. As with all causal models, it depends
    on the data generating process (DGP) within a domain.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用因果DAG来做出一个断言，即标签导致图像。这并不总是情况，正如我们将在5.3节讨论的半监督学习案例研究中讨论的那样。与所有因果模型一样，它取决于一个领域内的数据生成过程（DGP）。
- en: Why say that the digit *causes* the image?
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么说数字*导致*了图像？
- en: Plato’s allegory of the cave describes a group of people who have lived in a
    cave all their lives, without seeing the world. They face a blank cave wall and
    watch shadows projected on the wall from objects passing in front of a fire behind
    them. The shadows are simplified and sometimes distorted representations of the
    true objects passing in front of the fire. In this case, we can think of the form
    of the objects as being the cause of the shadow.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 柏拉图的洞穴寓言描述了一群人，他们一生都在洞穴中生活，从未见过世界。他们面对一个空白的洞穴墙壁，并观看从他们身后火堆前经过的物体在墙壁上投射的影子。这些影子是真实物体经过火堆前的简化且有时扭曲的表示。在这种情况下，我们可以认为物体的形式是影子的原因。
- en: Analogously, the true form of the digit label causes the representation in the
    image. The MNIST images were written by people, and they have some *Platonic ideal*
    of the digit in their head that they want to render onto paper. In the process,
    that ideal is distorted by motor variation in the hand, the angle of the paper,
    the friction of the pen on the paper, and other factors—the rendered image is
    a “shadow” caused by that “ideal.”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，数字标签的真实形式导致图像中的表示。MNIST图像是由人们书写的，他们心中有一个想要渲染到纸上的数字的柏拉图理想。在这个过程中，这个理想受到手的运动变化、纸张的角度、笔在纸上的摩擦以及其他因素的影响——渲染的图像是那个“理想”的“影子”。
- en: This idea is related to a concept called “vision as inverse graphics” in computer
    vision (see [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for sources with more information). In causal terms, the takeaway is that when
    we are analyzing images rendered from raw signals from the environment, and the
    task is to infer the actual objects or events that resulted in those signals,
    causality flows from those objects or events to the signals. The inference task
    is to use the observed signals (shadows on the cave wall) to infer the nature
    of the causes (objects in front of the fire).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法与计算机视觉中称为“视觉作为逆图形”的概念相关（有关更多信息，请参阅[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)）。在因果的术语中，要点是当我们分析从环境原始信号渲染的图像时，如果任务是推断导致这些信号的实际物体或事件，因果性从这些物体或事件流向信号。推断任务是使用观察到的信号（洞穴墙壁上的影子）来推断原因的性质（火堆前的物体）。
- en: That said, images can be causes too. For example, if you were modeling how people
    behave *after* seeing an image in a mobile app (e.g., whether they “click”, “like”,
    or “swipe left”), you could model the image as a cause of the behavior.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，图像也可以是原因。例如，如果你正在模拟人们在移动应用中看到图像后的行为（例如，他们是否“点击”、“点赞”或“向左滑动”），你可以将图像建模为导致该行为的原因。
- en: Plate modeling
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 板模型
- en: Modeling 2,990 fonts in our TMNIST data is overkill for our purposes here. Instead,
    I combined these datasets into one—half from MNIST and half from Typeface MNIST.
    Along with the “digit” label, I’m just going to have a simple binary label called
    “is-handwritten”, which is 1 (true) for images of handwritten digits from MNIST
    and 0 (false) for images of “typed” digits from TMNIST. We can modify our causal
    DAG to get figure 5.6.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的TMNIST数据中建模2,990种字体对于我们的目的来说过于冗余。相反，我将这些数据集合并为一个——一半来自MNIST，一半来自Typeface
    MNIST。除了“数字”标签外，我还要有一个简单的二进制标签，称为“is-handwritten”，对于MNIST的手写数字图像为1（真实），对于TMNIST的“打字”数字图像为0（假）。我们可以修改我们的因果DAG以获得图5.6。
- en: '![figure](../Images/CH05_F06_Ness.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F06_Ness.png)'
- en: Figure 5.6 A causal DAG representing the combined MNIST and TMNIST data, where
    “is-handwritten” is 1 (MNIST images) or 0 (TMNIST images)
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6 表示MNIST和TMNIST数据组合的因果DAG，其中“is-handwritten”为1（MNIST图像）或0（TMNIST图像）
- en: Plate modeling is a visualizing technique used in probabilistic machine learning
    that provides an excellent way to visualize the higher-level abstractions while
    preserving the lower-level dimensional detail. Plate notation is a method of visually
    representing variables that repeat in a DAG (e.g., *X*[1] to *X*[16] in figure
    5.4)—in our case, we have repetition of the pixels.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 板模型是一种在概率机器学习中使用的可视化技术，它提供了一种在保留低级维度细节的同时，可视化高级抽象的绝佳方法。板符号是一种在DAG（例如，图5.4中的*X*[1]到*X*[16]）中视觉表示重复变量的方法——在我们的情况下，我们有像素的重复。
- en: Instead of drawing each of the 784 pixels as an individual node, we use a rectangle
    or “plate” to group repeating variables into subgraphs. We then write a number
    on the plate to represent the number of repetitions of the entities on the plate.
    Plates can nest within one another to indicate repeated entities nested within
    repeated entities. Each plate gets a letter subscript indexing the elements on
    that plate. The causal DAG in figure 5.7 represents one image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是将每个784个像素作为单独的节点来绘制，而是使用矩形或“板”将重复变量分组到子图中。然后我们在板上写一个数字来表示板上实体的重复次数。板可以嵌套在另一个板中，以表示嵌套在重复实体中的重复实体。每个板都有一个字母下标，用于索引该板上的元素。图5.7中的因果DAG代表一个图像。
- en: '![figure](../Images/CH05_F07_Ness.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F07_Ness.png)'
- en: Figure 5.7 A plate model representation of the causal DAG. Plates represent
    repeating variables, in this case 28 × 28 = 784 pixels. *X**[j]* is the *j*^(th)
    pixel.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7展示了因果DAG的板模型表示。板代表重复变量，在这种情况下是28 × 28 = 784像素。*X**[j]*是第*j*个像素。
- en: During training, we’ll have a large set of training images. Next, we’ll modify
    the DAG to capture all the images in the training data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将有一大批训练图像。接下来，我们将修改DAG以捕获训练数据中的所有图像。
- en: 5.2 Training a neural causal model
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 训练神经因果模型
- en: To train our neural causal model, we need to load and prepare the training data,
    create the architecture of our model, write a training procedure, and implement
    some tools for evaluating how well training is progressing. We’ll start by loading
    and preparing the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的神经因果模型，我们需要加载数据并准备，创建我们模型的架构，编写训练过程，并实现一些评估训练进展的工具。我们将首先加载数据并准备。
- en: 5.2.1 Setting up the training data
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 设置训练数据
- en: Our training data has *N* example images, so we need our plate model to represent
    all *N* images in the training data, half handwritten and half typed. We’ll add
    another plate corresponding to repeating *N* sets of images and labels, as in
    figure 5.8.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据包含*N*个示例图像，因此我们需要我们的板模型来表示训练数据中的所有*N*个图像，一半是手写的，一半是打印的。我们将在图5.8中添加另一个板，对应于重复*N*组图像和标签。
- en: '![figure](../Images/CH05_F08_Ness.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F08_Ness.png)'
- en: Figure 5.8 The causal model with an additional plate for the *N* images in the
    data
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8展示了包含数据中*N*个图像的因果模型，并添加了一个额外的板。
- en: Now we have a causal DAG that illustrates both our desired level of causal abstraction
    as well as the dimensional information we need to start training the neural nets
    in the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个因果DAG，它展示了我们希望达到的因果抽象水平以及我们开始训练模型中的神经网络所需的维度信息。
- en: Let’s first load Pyro and some other libraries and set some hyperparameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先加载Pyro和其他一些库，并设置一些超参数。
- en: Setting up your environment
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置你的环境
- en: This code was written using Python version 3.10.12 and tested in Google Colab.
    The versions of the main libraries include Pyro (pyro-ppl) version 1.8.4, torch
    version 2.2.1, torchvision version 0.18.0+cu121, and pandas version 2.0.3\. We’ll
    also use matplotlib for plotting.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用Python 3.10.12版本编写，并在Google Colab中进行测试。主要库的版本包括Pyro (pyro-ppl) 1.8.4、torch
    2.2.1、torchvision 0.18.0+cu121和pandas 2.0.3。我们还将使用matplotlib进行绘图。
- en: Visit [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to a notebook that will load in Google Colab.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 访问[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)获取一个笔记本的链接，该笔记本将在Google
    Colab中加载。
- en: If GPUs are available on your device, it will be faster to train the neural
    nets with CUDA (a platform for parallel computing on GPUs). We’ll run a bit of
    code that lets us toggle it on. If you don’t have GPUs or aren’t sure if you do,
    leave `USE_CUDA` set to `False`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的设备上有GPU，使用CUDA（一个在GPU上并行计算的平台）训练神经网络将更快。我们将运行一些代码来切换它。如果您没有GPU或者不确定是否有，请将`USE_CUDA`设置为`False`。
- en: Listing 5.1 Setting up for GPU training
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1 设置GPU训练
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Use CUDA if it is available.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 如果可用，使用CUDA。'
- en: First, we’ll make a subclass of the `Dataset` class (a class for loading and
    preprocessing data) that will let us combine the MNIST and TMNIST datasets.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个`Dataset`类的子类（用于加载数据和预处理数据的类），这将使我们能够结合MNIST和TMNIST数据集。
- en: Listing 5.2 Combining the data
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.2 合并数据
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 This class loads and processes a dataset that combines MNIST and Typeface
    MNIST. The output is a torch.utils.data.Dataset object.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 此类加载并处理结合MNIST和Typeface MNIST的数据集。输出是一个torch.utils.data.Dataset对象。'
- en: '#2 Load, normalize, and reshape the images to 28 × 28 pixels.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 加载、归一化和重塑图像为28 × 28像素。'
- en: '#3 Get and process the digit labels, 0–9.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取和处理数字标签，0–9。'
- en: '#4 1 for handwritten digits (MNIST), and 0 for “typed” digits (TMNIST)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 1 对于手写数字（MNIST），以及 0 对于“输入”数字（TMNIST）'
- en: '#5 Return a tuple of the image, the digit label, and the is_handwritten label.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 返回一个包含图像、数字标签和 is_handwritten 标签的元组。'
- en: Next, we’ll use the `DataLoader` class (which allows for efficient data iteration
    and batching during training) to load the data from a CSV file in GitHub and split
    it into training and test sets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `DataLoader` 类（它允许在训练期间高效地进行数据迭代和批处理）从 GitHub 中的 CSV 文件加载数据，并将其分为训练集和测试集。
- en: Listing 5.3 Downloading, splitting, and loading the data
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3 下载、分割和加载数据
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Set up the data loader that loads the data and splits it into training and
    test sets.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置数据加载器，用于加载数据并将其分为训练集和测试集。'
- en: '#2 Allot 80% of the data to training data and the remaining 20% to test data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将 80% 的数据分配给训练数据，剩余的 20% 分配给测试数据。'
- en: '#3 Create training and test loaders.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 创建训练和测试加载器。'
- en: Next, we’ll set up the full variational autoencoder.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将设置完整的变分自编码器。
- en: 5.2.2 Setting up the variational autoencoder
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 设置变分自编码器
- en: The variational autoencoder (VAE) is perhaps the simplest deep probabilistic
    machine learning modeling approach. In the typical setup for applying VAE to images,
    we introduce a latent continuous variable *Z* that has a smaller dimension than
    the image data. Here, *dimensionality* refers to the number of elements in a vector
    representation of the data. For instance, our image is a 28 × 28 matrix of pixels,
    or alternatively a vector with dimension 28 × 28 = 784\. By having a much smaller
    dimension than the image dimension, the latent variable *Z* represents a compressed
    encoding of the image information. For each image in the dataset, there is a corresponding
    latent *Z* value that represents an encoding of that image. This setup is illustrated
    in figure 5.9.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）可能是最简单的深度概率机器学习建模方法。在将 VAE 应用于图像的典型设置中，我们引入一个比图像数据维度小的潜在连续变量 *Z*。在这里，*维度*指的是数据向量表示中的元素数量。例如，我们的图像是一个
    28 × 28 的像素矩阵，或者也可以是一个维度为 28 × 28 = 784 的向量。由于比图像维度小得多，潜在变量 *Z* 代表了图像信息的压缩编码。对于数据集中的每个图像，都有一个相应的潜在
    *Z* 值，它代表了该图像的编码。这种设置如图 5.9 所示。
- en: '![figure](../Images/CH05_F09_Ness.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F09_Ness.png)'
- en: Figure 5.9 The causal DAG plate model, extended to include an “encoding” variable
    *Z*. During training, the variable is latent, indicated by the dashed line. (After
    the model is deployed, *digit* and *is-handwritten*are also latent).
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.9 因果 DAG 盘模型，扩展以包括一个“编码”变量 *Z*。在训练期间，该变量是潜在的，由虚线表示。（在模型部署后，*digit* 和 *is-handwritten*
    也都是潜在的）。
- en: '*Z* appears as a new parent in the causal DAG, but it’s important to note that
    the classical VAE framework does not define *Z* as causal. Now that we are thinking
    causally, we’ll give *Z* a causal interpretation. Specifically, as parents of
    the image node in the DAG, we view *digit* and *is-handwritten* as causal drivers
    of what we see in the image. Yet there are other elements of the image (e.g.,
    the stroke thickness of a handwritten character, or the font of a typed character)
    that are also causes of what we see in the image. We’ll think of *Z* as a continuous
    latent *stand-in* for all of these other causes of the image that we are not explicitly
    modeling, like *digit* and *is-handwritten*. Examples of these causes include
    the nuance of the various fonts in the TMNIST labels and all of the variations
    in the handwritten digits due to different writers and motor movements as they
    wrote. With that in mind, we can view *P*(*X*| *digit*, *is-handwritten*, *Z*)
    as the causal Markov kernel of *X*. That said, it is important to remember that
    the representation we learn for *Z* is a stand-in for latent causes and is not
    the same as learning the actual latent causes.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*Z* 在因果 DAG 中表现为一个新的父节点，但需要注意的是，经典的 VAE 框架并没有将 *Z* 定义为因果的。现在我们正在进行因果思考，我们将
    *Z* 给予因果解释。具体来说，作为 DAG 中图像节点的父节点，我们将 *digit* 和 *is-handwritten* 视为图像中我们所看到的因果驱动因素。然而，图像中还有其他元素（例如，手写字符的笔画粗细或输入字符的字体）也是我们所看到的图像的因果因素。我们将
    *Z* 视为所有这些我们未明确建模的图像因果因素的连续潜在“替身”，例如 *digit* 和 *is-handwritten*。这些因果因素的例子包括 TMNIST
    标签中各种字体的细微差别以及由于不同作者和书写时的运动方式而导致的书写数字的所有变化。考虑到这一点，我们可以将 *P*(*X*| *digit*, *is-handwritten*,
    *Z*) 视为 *X* 的因果马尔可夫核。话虽如此，重要的是要记住，我们为 *Z* 学习到的表示是潜在因果的替身，并不等同于学习实际的潜在因果。'
- en: 'The VAE setup will train two deep neural networks: One called an “encoder”,
    which encodes an image into a value for *Z*. The other neural network, called
    the “decoder,” will align with our DAG. The decoder generates an image from the
    *digit* label, the *is-handwritten* label, and a *Z* value, as in figure 5.10.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: VAE 设置将训练两个深度神经网络：一个称为“编码器”，它将图像编码为 *Z* 的一个值。另一个神经网络，称为“解码器”，将与我们的 DAG 对齐。解码器将从
    *digit* 标签、*is-handwritten* 标签和 *Z* 值生成图像，如图 5.10 所示。
- en: The decoder acts like a rendering engine; given a *Z* encoding value and the
    values for *digit* and *is-handwritten*, it renders an image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的作用像一个渲染引擎；给定一个 *Z* 编码值和 *digit* 以及 *is-handwritten* 的值，它将渲染一个图像。
- en: '![figure](../Images/CH05_F10_Ness.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F10_Ness.png)'
- en: Figure 5.10 The decoder neural network generates as output an image *X* from
    inputs *Z* and the labels *is-handwritten* and *digit*. As with any neural net,
    the inputs are processed through one or more “hidden layers.”
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10 解码器神经网络从输入 *Z* 和标签 *is-handwritten* 以及 *digit* 生成输出图像 *X*。与任何神经网络一样，输入通过一个或多个“隐藏层”进行处理。
- en: Key VAE concepts so far
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 到目前为止的关键 VAE 概念
- en: '*Variational autoencoder (VAE)*—A popular framework in deep generative modeling.
    We’re using it to model a causal Markov kernel in a causal model.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*变分自编码器 (VAE)*——深度生成建模中的一种流行框架。我们正在使用它来模拟因果马尔可夫核中的因果模型。'
- en: '*Decoder*—We use the decoder as the model of the causal Markov kernel. It maps
    the observed causes *is-handwritten* and *digit*, and the latent variable *Z*,
    to our image outcome variable *X*.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*解码器*——我们使用解码器作为因果马尔可夫核的模型。它将观察到的原因 *is-handwritten* 和 *digit* 以及潜在变量 *Z* 映射到我们的图像结果变量
    *X*。'
- en: This VAE approach allows us to use a neural net, a la the decoder, to capture
    the complex and nonlinear relations needed to model the image as an effect caused
    by *digit* and *is-handwritten*. Modeling images would be difficult with the conditional
    probability tables and other simple parameterizations of causal Markov kernels
    we’ve discussed previously.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 VAE 方法允许我们使用神经网络，类似于解码器，来捕捉建模图像作为由 *digit* 和 *is-handwritten* 造成的效应所需的复杂和非线性关系。使用我们之前讨论过的条件概率表和其他简单的因果马尔可夫核参数化来建模图像将是困难的。
- en: First, let’s implement the decoder. We’ll pass in arguments `z_dim` for the
    dimension of *Z* and `hidden_dim` for the dimension (width) of the hidden layers.
    We’ll specify these variables when we instantiate the full VAE. The decoder combines
    the latent vector *Z* with additional inputs—the variable representing the *digit,*
    and *is-handwritten* (a binary indicator of whether the digit is handwritten).
    It will produce a 784-dimensional output vector representing an image of size
    28 × 28 pixels. This output vector contains the parameters for a Bernoulli distribution
    for each pixel, essentially modeling the likelihood of each pixel being “on.”
    The class uses two fully connected layers (`fc1` and `fc2`), and employs `Softplus`
    and `Sigmoid` “activation functions,” which are the hallmarks of how neural nets
    emulate neurons.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实现解码器。我们将传递 `z_dim` 作为 *Z* 的维度和 `hidden_dim` 作为隐藏层（宽度）的维度。当我们实例化完整的 VAE
    时将指定这些变量。解码器将潜在向量 *Z* 与额外的输入相结合——代表 *digit* 的变量和 *is-handwritten*（一个指示数字是否手写的二进制指示符）。它将生成一个
    784 维的输出向量，代表大小为 28 × 28 像素的图像。这个输出向量包含每个像素的伯努利分布的参数，本质上是对每个像素“开启”的可能性的建模。该类使用两个全连接层（`fc1`
    和 `fc2`），并采用 `Softplus` 和 `Sigmoid` “激活函数”，这是神经网络模拟神经元的特点。
- en: Listing 5.4 Implement the decoder
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4 实现解码器
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 A class for the decoder used in the VAE'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于 VAE 中的解码器的类'
- en: '#2 Image is 28 × 28 pixels.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 图像大小为 28 × 28 像素。'
- en: '#3 Digit is one-hot encoded digits 0–9, i.e., a vector of length 10.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 数字是 0-9 的一热编码数字，即长度为 10 的向量。'
- en: '#4 An indicator for whether the digit is handwritten that has size 1'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 一个指示数字是否手写的指示符，大小为 1'
- en: '#5 Softplus and sigmoid are nonlinear transforms (activation functions) used
    in mapping between layers.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 Softplus 和 sigmoid 是在层之间映射时使用的非线性变换（激活函数）。'
- en: '#6 fc1 is a linear function that maps the Z vector, the digit, and is_handwritten
    to a linear output, which is passed through a softplus activation function to
    create a hidden layer-a vector whose length is given by hidden_layer.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 fc1 是一个线性函数，它将 Z 向量、数字和 is_handwritten 映射到一个线性输出，该输出通过 softplus 激活函数传递，创建一个隐藏层——其长度由
    hidden_layer 给出。'
- en: '#7 fc2 linearly maps the hidden layer to an output passed to a sigmoid function.
    The resulting value is between 0 and 1.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 fc2 线性地将隐藏层映射到传递给 sigmoid 函数的输出。得到的值介于 0 和 1 之间。'
- en: '#8 Define the forward computation from the latent Z variable value to a generated
    X variable value.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 定义从潜在变量Z的值到生成变量X的值的正向计算。'
- en: '#9 Combine Z and the labels.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 结合Z和标签。'
- en: '#10 Compute the hidden layer.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 计算隐藏层。'
- en: '#11 Pass the hidden layer to a linear transform and then to a sigmoid transform
    to output a parameter vector of length 784\. Each element of the vector corresponds
    to a Bernoulli parameter value for an image pixel.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 将隐藏层传递给线性变换，然后传递给sigmoid变换以输出长度为784的参数向量。向量的每个元素对应于图像像素的伯努利参数值。'
- en: We use the decoder in the causal model. Our causal DAG acts as the scaffold
    for a causal probabilistic machine learning model that, with the help of the decoder,
    defines a joint probability distribution on {*is-handwritten*, *digit*, *X*, *Z*},
    where *Z* is latent. We can use the model to calculate the likelihood of the training
    data for a given value of *Z*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在因果模型中使用解码器。我们的因果DAG作为因果概率机器学习模型的支架，该模型在解码器的帮助下定义了关于{*is-handwritten*，*digit*，*X*，*Z*}的联合概率分布，其中*Z*是潜在的。我们可以使用该模型来计算给定*Z*值的训练数据的似然。
- en: The latent variable `z`, the digit identity represented as a one-hot vector
    `digit`, and a binary indicator `is_handwritten` are modeled as samples from standard
    distributions. These variables are then fed into the decoder to produce parameters
    (`img_param`) for a Bernoulli distribution representing individual pixel probabilities
    of an image.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在变量`z`，以一位热向量`digit`表示的数字身份，以及二进制指示符`is_handwritten`被建模为来自标准分布的样本。然后这些变量被输入到解码器中，以产生表示图像单个像素概率的伯努利分布的参数（`img_param`）。
- en: Note, using the Bernoulli distribution to model the pixels is a bit of a hack.
    The pixels are not binary black and white outcomes—they have grayscale values.
    The line `dist.enable_validation(False)` lets us cheat by getting Bernoulli log
    likelihoods for the images given a decoder’s `img_param` output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用伯努利分布来模拟像素是一种折衷的方法。像素不是二进制的黑白结果——它们有灰度值。`dist.enable_validation(False)`这一行让我们通过解码器的`img_param`输出为图像获得伯努利对数似然而作弊。
- en: The following model code is a class method for a PyTorch neural network module.
    We’ll see the entire class later.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型代码是PyTorch神经网络模块的一个类方法。我们稍后会看到整个类。
- en: Listing 5.5 The causal model
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.5 因果模型
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Disabling distribution validation lets Pyro calculate log likelihoods for
    pixels even though the pixels are not binary values.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 禁用分布验证使Pyro能够在像素不是二进制值的情况下计算对数似然。'
- en: '#2 The model of a single image. Within the method, we register the decoder,
    a PyTorch module, with Pyro. This lets Pyro know about the parameters inside of
    the decoder network.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 单个图像的模型。在方法内部，我们将解码器，一个PyTorch模块，注册到Pyro。这使Pyro了解解码器网络内部的参数。'
- en: '#3 We model the joint probability of Z, digit, and is_handwritten, sampling
    each from canonical distributions. We sample Z from a multivariate normal with
    location parameter z_loc (all zeros) and scale parameter z_scale (all ones).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们对Z、数字和is_handwritten的联合概率进行建模，每个都从标准分布中进行采样。我们从具有位置参数z_loc（所有为零）和尺度参数z_scale（所有为一）的多变量正态分布中采样Z。'
- en: '#4 We also sample the digit from a one-hot categorical distribution. Equal
    probability is assigned to each digit.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 我们还从一位热分类分布中采样数字。每个数字被分配相同的概率。'
- en: '#5 We similarly sample the is_handwritten variable from a Bernoulli distribution.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 我们同样从伯努利分布中采样is_handwritten变量。'
- en: '#6 The decoder maps digit, is_handwritten, and Z to a probability parameter
    vector.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 解码器将数字、is_handwritten和Z映射到一个概率参数向量。'
- en: '#7 The parameter vector is passed to the Bernoulli distribution, which models
    the pixel values in the data. The pixels are not technically Bernoulli binary
    variables, but we’ll relax this assumption.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 参数向量被传递到伯努利分布，该分布模拟数据中的像素值。像素在技术上不是伯努利二元变量，但我们将放宽这个假设。'
- en: The preceding `model` method represents the DGP for one image. The `training_
    model` method in the following listing applies that `model` method to the *N*
    images in the training data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`model`方法表示一个图像的DGP。以下列表中的`training_model`方法将`model`方法应用于训练数据中的*N*个图像。
- en: Listing 5.6 Method for applying `model` to *N* images in data
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.6 应用`model`到数据中*N*个图像的方法
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 The model represents the DGP for one image. The training_model applies that
    model to the N images in the training data.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 模型表示一个图像的DGP。`training_model`方法将此模型应用于训练数据中的N个图像。'
- en: '#2 Now we condition the model on the evidence in the training data.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 现在我们将模型条件化在训练数据中的证据上。'
- en: '#3 This context manager represents the N-size plate representing repeating
    IID examples in the data in figure 5.9\. In this case, N is the batch size. It
    works like a for loop, iterating over each data unit in the batch.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 此上下文管理器代表图5.9中数据中重复的独立同分布(IID)示例的N大小板。在这种情况下，N是批大小。它像一个for循环，遍历批处理中的每个数据单元。'
- en: Our probabilistic machine learning model models the joint distribution of {*Z*,
    *X*, *digit*, *is-handwritten*}. But since *Z* is latent, the model will need
    to learn *P*(*Z*|*X*, *digit*, *is-handwritten*). Given that we use the decoder
    neural net to go from *Z* and the labels to *X*, the distribution of *Z*, given
    *X* and the labels will be complex. We will use *variational inference*, a technique
    where we first define an approximating distribution *Q*(*Z*|*X*, *digit*, *is-handwritten*),
    and try to make that distribution as close to *P*(*Z*|*X*, *digit*, *is-handwritten*)
    as we can.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的概率机器学习模型建模了联合分布{*Z*, *X*, *digit*, *is-handwritten*}。但由于*Z*是潜在的，该模型将需要学习*P*(*Z*|*X*,
    *digit*, *is-handwritten*)。鉴于我们使用解码器神经网络从*Z*和标签到*X*，给定*X*和标签的*Z*的分布将是复杂的。我们将使用*变分推断*，这是一种技术，我们首先定义一个近似分布*Q*(*Z*|*X*,
    *digit*, *is-handwritten*)，并尝试使该分布尽可能接近*P*(*Z*|*X*, *digit*, *is-handwritten*)。
- en: The main ingredient of the approximating distribution is the second neural net
    in the VAE framework, the encoder, illustrated in figure 5.11\. The encoder maps
    an observed image and its labels in the training data to a latent *Z* variable.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 近似分布的主要成分是VAE框架中的第二个神经网络，即编码器，如图5.11所示。编码器将训练数据中的观察图像及其标签映射到潜在*Z*变量。
- en: '![figure](../Images/CH05_F11_Ness.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F11_Ness.png)'
- en: Figure 5.11 The encoder maps actual images as input to the latent *Z* variable
    as output.
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.11 编码器将实际图像作为输入映射到潜在*Z*变量作为输出。
- en: The encoder does the work of compressing the information in the image into a
    lower-dimensional encoding.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器负责将图像中的信息压缩到低维编码。
- en: Key VAE concepts so far
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 到目前为止的关键VAE概念
- en: '*Variational autoencoder (VAE)*—A popular framework in deep generative modeling.
    We’re using it to model a causal Markov kernel in our causal model.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*变分自动编码器(VAE)*—深度生成建模中流行的框架。我们正在使用它来模拟因果马尔可夫核。'
- en: '*Decoder*—We use the decoder as the model of the causal Markov kernel. It maps
    observed causes *is-handwritten* and *digit*, and the latent variable *Z*, to
    our image outcome variable *X*.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*解码器*—我们将解码器用作因果马尔可夫核的模型。它将观察到的原因*is-handwritten*和*digit*，以及潜在变量*Z*，映射到我们的图像结果变量*X*。'
- en: '*Encoder*—The encoder maps the image, *digit*, and *is-handwritten* indicator
    to the parameters of a distribution where we can draw samples of *Z*.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*编码器*—编码器将图像、*digit*和*is-handwritten*指示符映射到分布的参数，我们可以从中抽取*Z*的样本。'
- en: In the following code, the encoder takes as input an image, a digit label, and
    the *is-handwritten* indicator. These inputs are concatenated and passed through
    a series of fully connected layers with Softplus activation functions. The final
    output of the encoder consists of two vectors representing the location (`z_loc`)
    and scale (`z_scale`) parameters of the latent space distribution on *Z*, given
    observed values for *image* (`img`), *digit* (`digit`), and *is-handwritten* (`is_handwritten`).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，编码器将图像、数字标签和*is-handwritten*指示符作为输入。这些输入被连接并通过一系列具有Softplus激活函数的全连接层传递。编码器的最终输出由两个向量组成，分别代表潜在空间分布*Z*上的位置(`z_loc`)和尺度(`z_scale`)参数，给定观察到的*image*(`img`)、*digit*(`digit`)和*is-handwritten*(`is_handwritten`)值。
- en: Listing 5.7 Implement the encoder
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.7 实现编码器
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 The encoder is an instance of a PyTorch module.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 编码器是PyTorch模块的一个实例。'
- en: '#2 The input image is 28 × 28 = 784 pixels.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 输入图像是28 × 28 = 784像素。'
- en: '#3 The digit dimension is 10.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 数字维度为10。'
- en: '#4 In the encoder, we’ll only use the softplus transform (activation function).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 在编码器中，我们只使用softplus变换（激活函数）。'
- en: '#5 The linear transform fc1 combines with the softplus to map the 784-dimensional
    pixel vector, 10-dimensional digit label vector, and 2-dimensional is_handwritten
    vector to the hidden layer.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 线性变换fc1与softplus结合，将784维像素向量、10维数字标签向量和2维is_handwritten向量映射到隐藏层。'
- en: '#6 The linear transforms, fc21 and fc22, will combine with the softplus to
    map the hidden vector to Z’s vector space.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 线性变换fc21和fc22将与softplus结合，将隐藏向量映射到Z的向量空间。'
- en: '#7 Define the reverse computation from an observed X variable value to a latent
    Z variable value.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 定义从观察到的X变量值到潜在Z变量值的反向计算。'
- en: '#8 Combine the image vector, digit label, and is_handwritten label into one
    input.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 将图像向量、数字标签和is_handwritten标签合并为一个输入。'
- en: '#9 Map the input to the hidden layer.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 将输入映射到隐藏层。'
- en: '#10 The VAE framework will sample Z from a normal distribution that approximates
    P(Z|img, digit, is_handwritten). The final transforms map the hidden layer to
    a location and scale parameter for that normal distribution.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 VAE框架将从近似P(Z|img, digit, is_handwritten)的正态分布中采样Z。最终的转换将隐藏层映射到该正态分布的位置和尺度参数。'
- en: The output of the encoder produces the parameters of a distribution on *Z*.
    During training, given an image and its labels (*is-handwritten* and *digit*),
    we want to get a good value of *Z*, so we write a *guide function* that will use
    the encoder to sample values of *Z*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的输出产生*Z*上的分布的参数。在训练过程中，给定一个图像及其标签（*is-handwritten*和*digit*），我们希望得到一个良好的*Z*值，因此我们编写了一个*指导函数*，该函数将使用编码器来采样*Z*的值。
- en: Listing 5.8 The guide function
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.8 指导函数
- en: '[PRE7]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 training_guide is a method of the VAE that will use the encoder.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 training_guide是VAE的一个方法，它将使用编码器。'
- en: '#2 Register the encoder so Pyro is aware of its weight parameters.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 注册编码器，使Pyro了解其权重参数。'
- en: '#3 This is the same plate context manager for iterating over the batch data
    that we see in the training_model.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 这是迭代批处理数据的相同plate上下文管理器，我们在training_model中看到。'
- en: '#4 Use the encoder to map an image and its labels to parameters of a normal
    distribution.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用编码器将图像及其标签映射到正态分布的参数。'
- en: '#5 Sample Z from that normal distribution'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从该正态分布中采样Z。'
- en: We combine these elements into one PyTorch neural network module representing
    the VAE. We’ll initialize the latent dimension of *Z* to be 50\. We’ll set our
    hidden layer dimension to 400 in both the encoder and decoder. That means that
    given a dimension of 28 × 28 for the image, 1 for the binary *is-handwritten*,
    and 10 for the one-hot-encoded *digit* variable, we’ll take a 28 × 28 + 1 + 10
    = 795-dimensional feature vector and compress it down to a 400-dimensional hidden
    layer, and then compress that down to a 50-dimensional location and scale parameter
    for *Z*’s multivariate normal (Gaussian) distribution. The decoder takes as input
    the values of *digit*, *is-handwritten*, and *Z* and maps these to a 400-dimensional
    hidden layer and to the 28 × 28–dimensional image. These architectural choices
    of latent variable dimension, number of layers, activation functions, and hidden
    layer dimensions depend on the problem and are typically selected by convention
    or by experimenting with different values.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些元素组合成一个表示VAE的PyTorch神经网络模块。我们将*Z*的潜在维度初始化为50。我们将编码器和解码器的隐藏层维度都设置为400。这意味着对于图像的维度28
    × 28，二进制*is-handwritten*为1，以及one-hot编码的*digit*变量为10，我们将一个28 × 28 + 1 + 10 = 795维的特征向量压缩到400维的隐藏层，然后将其压缩到*Z*的多变量正态（高斯）分布的50维位置和尺度参数。解码器将*digit*、*is-handwritten*和*Z*的值作为输入，并将这些映射到400维的隐藏层和28
    × 28维的图像。这些关于潜在变量维度、层数、激活函数和隐藏层维度的架构选择取决于问题，通常是通过惯例或通过尝试不同的值来选择的。
- en: Now we’ll put these pieces together into the full VAE class.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将这些部分组合成完整的VAE类。
- en: Listing 5.9 Full VAE class
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.9 完整的VAE类
- en: '[PRE8]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Set the latent dimension to 50.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将潜在维度设置为50。'
- en: '#2 Set the hidden layers to have a dimension of 400.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将隐藏层设置为400维。'
- en: '#3 Set up the encoder and decoder.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 设置编码器和解码器。'
- en: '#4 Add in the methods for model, training_model, and training_guide.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 添加模型、训练模型和训练指南的方法。'
- en: Having specified the VAE, we can now move on to training.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定了VAE之后，我们现在可以继续进行训练。
- en: 5.2.3 The training procedure
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 训练过程
- en: We know we have a good generative model when the encoder can encode an image
    into a latent value of *Z*, and then decode it into a *reconstructed* version
    of the image. We can minimize the *reconstruction error*—the difference between
    original and reconstructed images—in the training data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当编码器可以将图像编码为潜在值*Z*，然后解码成图像的*重构*版本时，我们知道我们有一个好的生成模型。我们可以在训练数据中最小化*重构误差*——原始图像和重构图像之间的差异。
- en: A bit of perspective on the “variational inference” training algorithm
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对“变分推断”训练算法的一些看法
- en: In this section, you’ll see a bunch of jargon relating to variational inference,
    which is the algorithm we’ll use for training. It helps to zoom out and examine
    why we’re using this algorithm. There are many statistical estimators and algorithms
    both for fitting neural net weights and other parameters and for causal inference.
    One of these is variational inference.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将看到许多与变分推断相关的术语，这是我们用于训练的算法。了解我们为什么使用这个算法是有帮助的。对于拟合神经网络权重和其他参数以及因果推断，有许多统计估计量和算法。其中之一就是变分推断。
- en: To be clear, variational inference is not a “causal” idea. It is just another
    probabilistic inference algorithm. In this book, I favor this inference algorithm
    more than others because it scales well even when variables in the DAG are latent
    in the training data, and it works with deep neural nets and leverages deep learning
    frameworks like PyTorch. This opens the door to reasoning causally about richer
    modalities such as text, images, video, etc., whereas traditional causal inference
    estimators were developed for numerical data. Further, we can tailor the method
    to different problems (see the discussion of “commodification of inference” in
    chapter 1) and leverage domain knowledge during inference (such as by using knowledge
    of conditional independence in the guide). Finally, the core concepts of variational
    inference show up across many deep generative modeling approaches (such as latent
    diffusion models).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，变分推断不是一个“因果”概念。它只是另一种概率推断算法。在这本书中，我更倾向于这种推断算法，因为它即使在DAG中的变量在训练数据中是潜在的情况下也能很好地扩展，并且可以与深度神经网络一起工作，利用像PyTorch这样的深度学习框架。这为对更丰富的模态（如文本、图像、视频等）进行因果推理打开了大门，而传统的因果推断估计器是为数值数据开发的。此外，我们可以根据不同的问题调整这种方法（参见第1章中关于“推断的商品化”的讨论），并在推断过程中利用领域知识（例如，通过在指南中使用条件独立性的知识）。最后，变分推断的核心概念出现在许多深度生成建模方法中（如潜在扩散模型）。
- en: 'In practice, solely minimizing reconstruction error leads to overfitting and
    other issues, so we’ll opt for a probabilistic approach: given an image, we’ll
    use our guide function to sample a value of *Z* from *P*(*Z*|*image*, *is-handwritten*,
    *digi**t*). Then we’ll plug that value into our model’s decoder, and the output
    parameterizes *P*(*image*|*is-handwritten*, *digit*, *Z*). Our probabilistic approach
    to minimizing reconstruction error optimizes the encoder and decoder such that
    we’ll maximize the likelihood of *Z* with respect to *P*(*Z*|*image*, *is-handwritten*,
    *digit*) and the likelihood of the original image with respect to *P*(*image*|*is-handwritten*,
    *digit*, *Z*).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，仅仅最小化重建误差会导致过拟合和其他问题，因此我们将选择一种概率方法：给定一个图像，我们将使用我们的指南函数从*P*(*Z*|*image*,
    *is-handwritten*, *digi**t*)中采样一个*Z*的值。然后我们将该值插入到我们模型的解码器中，输出参数化*P*(*image*|*is-handwritten*,
    *digit*, *Z*)。我们最小化重建误差的概率方法优化了编码器和解码器，以便我们最大化*Z*相对于*P*(*Z*|*image*, *is-handwritten*,
    *digit*)的似然性，以及原始图像相对于*P*(*image*|*is-handwritten*, *digit*, *Z*)的似然性。
- en: But typically we can’t directly sample from or get likelihoods from the distribution
    *P*(*Z*|*image*, *is-handwritten*, *digit*). So, instead, our guide function attempts
    to approximate it. The guide represents a *variational distribution*, denoted
    *Q*(*Z*|*X*, *is-handwritten*, *digit*). A change in the weights of the encoder
    represents a shifting of the variational distribution. Training will optimize
    the weights of the encoder such that the variational distribution shifts toward
    *P*(*Z*|*image*, *is-handwritten*, *digit*). That training approach is called
    *variational inference*, and it works by minimizing the *Kullback–Leibler divergence*
    (KL divergence) between the two distributions; KL divergence is a way of quantifying
    how two distributions differ.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常我们无法直接从分布*P*(*Z*|*image*, *is-handwritten*, *digit*)中采样或获取似然性。因此，我们的指南函数试图近似它。指南代表一个*变分分布*，表示为*Q*(*Z*|*X*,
    *is-handwritten*, *digit*)。编码器权重的变化代表变分分布的移动。训练将优化编码器的权重，使变分分布向*P*(*Z*|*image*,
    *is-handwritten*, *digit*)移动。这种训练方法被称为*变分推断*，它通过最小化两个分布之间的*Kullback–Leibler散度*（KL散度）来实现；KL散度是一种量化两个分布差异的方法。
- en: Our variational inference procedure optimizes a quantity called *ELBO*, which
    means *expected lower bound on the log-likelihood of the data*. Minimizing negative
    ELBO loss indirectly minimizes reconstruction error and KL divergence between
    *Q*(*Z*|…) and *P*(*Z*|…). Pyro implements ELBO in a utility called `Trace_ELBO`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可变推断过程优化了一个称为 *ELBO* 的量，它表示 *数据的对数似然期望下界*。间接最小化负 ELBO 损失可以最小化重建误差和 *Q*(*Z*|…)
    与 *P*(*Z*|…) 之间的 KL 散度。Pyro 在一个名为 `Trace_ELBO` 的实用工具中实现了 ELBO。
- en: Our procedure will use *stochastic* variational inference (SVI), which simply
    means doing variational inference with a training procedure that works with randomly
    selected subsets of the data, or “batches”, rather than the full dataset, which
    reduces memory use and helps scale to larger data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的程序将使用 *随机* 可变推断 (SVI)，这意味着使用与随机选择的数据子集（或“批次”）一起工作的训练程序进行可变推断，而不是整个数据集，这可以减少内存使用并有助于扩展到更大的数据。
- en: Key VAE concepts so far
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 到目前为止的关键 VAE 概念
- en: '*Variational autoencoder (VAE)*—A popular framework in deep generative modeling.
    We’re using it to model a causal Markov kernel in our causal model.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*变分自动编码器 (VAE)*—深度生成模型中的一种流行框架。我们正在使用它来模拟因果模型中的因果马尔可夫核。'
- en: '*Decoder*—We use the decoder as the model of the causal Markov kernel. It maps
    the observed causes *is-handwritten* and *digit*, and the latent variable *Z*,
    to our image outcome variable *X*.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*解码器*—我们将解码器作为因果马尔可夫核的模型。它将观察到的原因 *is-handwritten* 和 *digit*，以及潜在变量 *Z*，映射到我们的图像结果变量
    *X*。'
- en: '*Encoder*—The encoder maps the *image*, *digit*, and *is-handwritten* to the
    parameters of a distribution where we can draw samples of *Z*.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*编码器*—编码器将 *图像*、*digit* 和 *is-handwritten* 映射到我们可以从中抽取 *Z* 样本的分布的参数。'
- en: '*Guide function*—During training, we want values of *Z* that represent an image,
    given *is-handwritten* and *digit*; i.e., we want to generate *Z*s from *P*(*Z*|*image*,
    *is-handwritten*, *digit*). But we can’t sample from this distribution directly.
    So we write a *guide function* that uses the encoder and convenient canonical
    distributions like the multivariate normal to sample values of *Z*.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*引导函数*—在训练过程中，我们希望 *Z* 的值代表一个图像，给定 *is-handwritten* 和 *digit*；即，我们希望从 *P*(*Z*|*image*，*is-handwritten*，*digit*)
    中生成 *Z*。但我们不能直接从这个分布中采样。因此，我们编写了一个 *引导函数*，该函数使用编码器和方便的正态分布等标准分布来采样 *Z* 的值。'
- en: '*Variational distribution*—The guide function represents a distribution called
    *the variational distribution*, denoted *Q*(*Z*|*image*, *is-handwritten*, *digit*).
    During inference, we want to sample from *Q*(*Z*|…) in a way that is representative
    of *P*(*Z*|*image*, *is-handwritten*, *digit*).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*变分分布*—引导函数表示一个称为 *变分分布* 的分布，表示为 *Q*(*Z*|*image*，*is-handwritten*，*digit*）。在推理过程中，我们希望以代表
    *P*(*Z*|*image*，*is-handwritten*，*digit*) 的方式从 *Q*(*Z*|…) 中采样。'
- en: '*Variational inference*—This is the training procedure that seeks to maximize
    the closeness between *Q*(*Z*|…) and *P*(*Z*|…) so sampling from *Q*(*Z*|…) produces
    samples representative of *P*(*Z*|…) (e.g., by minimizing KL divergence).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*变分推断*—这是寻求最大化 *Q*(*Z*|…) 和 *P*(*Z*|…) 之间接近度的训练过程，以便从 *Q*(*Z*|…) 中采样的样本代表 *P*(*Z*|…)（例如，通过最小化
    KL 散度）。'
- en: '*Stochastic variational inference (SVI)*—Variational inference where training
    relies on randomly selected subsets of the data, rather than on the full data,
    in order to make training faster and more scalable.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机变分推断 (SVI)*—变分推断，其中训练依赖于随机选择的数据子集，而不是整个数据，以便使训练更快、更可扩展。'
- en: Before we get started, we’ll make a helper function for plotting images so we
    can see how we are doing during training.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们将创建一个用于绘制图像的辅助函数，这样我们就可以在训练过程中看到我们的进展情况。
- en: Listing 5.10 Helper function for plotting images
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.10 绘制图像的辅助函数
- en: '[PRE9]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Helper function for plotting an image'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于绘制图像的辅助函数'
- en: Next, we’ll create a `reconstruct_img` helper function that will *reconstruct*
    an image, given its labels, where “reconstruct” means encoding the image into
    a latent representation and then decoding the latent representation back into
    an image. We can then compare the original image and its reconstruction to see
    how well the encoder and decoder have been trained. We’ll create a `compare_images`
    function to do that comparison.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 `reconstruct_img` 辅助函数，它将根据其标签 *重建* 一个图像，其中“重建”意味着将图像编码到潜在表示中，然后解码潜在表示回到图像。然后我们可以比较原始图像及其重建，以查看编码器和解码器训练得有多好。我们将创建一个
    `compare_images` 函数来进行该比较。
- en: Listing 5.11 Define a helper function for reconstructing and viewing the images
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.11 定义一个用于重建和查看图像的辅助函数
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Given an input image, this function reconstructs the image by passing it
    through the encoder and then through the decoder.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 给定一个输入图像，此函数通过编码器然后通过解码器传递它来重建图像。'
- en: '#2 Plots the two images side by side for comparison'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将两个图像并排绘制以进行比较。'
- en: Next, we’ll create some helper functions for handling the data. We’ll use `get_random_example`
    to grab random images from the dataset. The `reshape_data` function will convert
    an image and its labels into input for the encoder. And we’ll use `generate_data`
    and `generate_coded_data` to simulate an image from the model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一些处理数据的辅助函数。我们将使用`get_random_example`从数据集中获取随机图像。`reshape_data`函数将图像及其标签转换为编码器的输入。我们将使用`generate_data`和`generate_coded_data`从模型中模拟图像。
- en: Listing 5.12 Data processing helper functions for training
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.12 训练数据处理的辅助函数
- en: '[PRE11]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Choose a random example from the dataset.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从数据集中选择一个随机示例。'
- en: '#2 Reshape the data.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 重新塑形数据。'
- en: '#3 Generate data that is encoded.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 生成已编码的数据。'
- en: '#4 Generate (unencoded) data.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 生成（未编码）数据。'
- en: Finally, we can run the training procedure. First, we’ll set up stochastic variational
    inference. We’ll first set up an instance of the Adam optimizer, which will handle
    optimization of the parameters in `training_guide`. Then we’ll pass `training_model`,
    `training_guide`, the optimizer, and the ELBO loss function to the SVI constructor
    to get an SVI instance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以运行训练过程。首先，我们将设置随机变分推理。我们首先设置一个Adam优化器的实例，该实例将处理`training_guide`中的参数优化。然后我们将`training_model`、`training_guide`、优化器和ELBO损失函数传递给SVI构造函数以获取SVI实例。
- en: Listing 5.13 Set up the training procedure
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.13 设置训练过程
- en: '[PRE12]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Clear any values of the parameters in the guide memory.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 清除指南内存中参数的任何值。'
- en: '#2 Initialize the VAE.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 初始化VAE。'
- en: '#3 Load the data.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 加载数据。'
- en: '#4 Initialize the optimizer.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 初始化优化器。'
- en: '#5 Initialize the SVI loss calculator. Loss negative “expected lower bound”
    (ELBO).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 初始化SVI损失计算器。损失负“期望下界”（ELBO）。'
- en: When training generative models, it is useful to set up a procedure that uses
    test data to evaluate how well training is progressing. You can include anything
    you think is useful to monitor during training. Here, I calculate and print the
    loss function on the test data, just to make sure the test loss is progressively
    decreasing along with training loss (a flattening of test loss while training
    loss continues to decrease would indicate overfitting).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练生成模型时，设置一个使用测试数据来评估训练进展的流程是有用的。你可以包括你认为在训练期间有用的任何内容。在这里，我计算并打印测试数据上的损失函数，只是为了确保测试损失随着训练损失的逐渐降低而逐渐降低（测试损失平坦化而训练损失继续降低将表明过拟合）。
- en: A more direct way of determining how well our model is training is to generate
    and view images. In my test evaluation procedure, I produce two visualizations.
    First, I inspect how well it can reconstruct a random image from the test data.
    I pass the image through the encoder and then through the decoder, creating a
    “reconstruction” of the image. Then I plot the original and reconstructed images
    side by side and compare them visually, looking to see that they are close to
    identical.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 确定我们的模型训练得有多好的一种更直接的方法是生成并查看图像。在我的测试评估过程中，我生成了两个可视化。首先，我检查它从测试数据中重建随机图像的能力。我将图像通过编码器然后通过解码器传递，创建了一个“重建”的图像。然后我将原始图像和重建图像并排绘制并比较它们，看它们是否非常相似。
- en: Next, I visualize how well it is performing as an overall generative model by
    generating and plotting an image from scratch. I run this code once each time
    a certain number of epochs are run.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我通过从头生成并绘制图像来可视化它作为一个整体生成模型的性能。每次运行一定数量的epoch时，我都会运行此代码一次。
- en: Listing 5.14 Setting up a test evaluation procedure
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.14 设置测试评估过程
- en: '[PRE13]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Calculate and print test loss.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 计算并打印测试损失。'
- en: '#2 Compare a random test image to its reconstruction.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将随机测试图像与其重建图像进行比较。'
- en: '#3 Generate a random image from the model.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从模型中生成一个随机图像。'
- en: Now we’ll run the training. For a single epoch, we’ll iteratively get a batch
    of data from the training data loader and pass it to the step method and run a
    training step. After a certain number of epochs (a number set by `TEST_FREQUENCY`),
    we’ll use our helper functions to compare a random image to its reconstruction,
    as well as simulate an image from scratch and plot it.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将运行训练。对于单个epoch，我们将从训练数据加载器中迭代地获取一批数据并将其传递到step方法中，运行一个训练步骤。在一定的epoch数（由`TEST_FREQUENCY`设置）之后，我们将使用辅助函数将随机图像与其重建图像进行比较，以及从头模拟图像并绘制它。
- en: Listing 5.15 Running training and plotting progress
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.15 运行训练和绘制进度
- en: '[PRE14]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Run the training procedure for a certain number of epochs.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 运行一定数量的epoch的训练过程。'
- en: '#2 Run a training step on one batch in one epoch.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在一个epoch中对一个批次运行一个训练步骤。'
- en: '#3 The test data evaluation procedure runs every 10 epochs.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 测试数据评估过程每10个epoch运行一次。'
- en: Again, see [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to a Jupyter notebook with the full VAE, encoder/decoder, and training
    code, including a link for running it in Google Colab.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，参见[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)获取一个包含完整VAE、编码器/解码器和训练代码的Jupyter笔记本链接，包括在Google
    Colab中运行的链接。
- en: 5.2.4 Evaluating training
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 评估训练
- en: At certain points during training, we randomly choose an image and “reconstruct”
    it by passing the image through the encoder to get a latent value of *Z*, and
    passing that value back through the decoder. In one run, the first image I see
    is a non-handwritten number 6\. Figure 5.12 shows this image and its reconstruction.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的某些点上，我们随机选择一个图像，并通过将图像通过编码器来“重构”它以获得*Z*的潜在值，然后将该值通过解码器返回。在一次运行中，我看到的第一个图像是一个非手写的数字6。图5.12显示了这张图像及其重构。
- en: During training, we also simulate random images from the generative model and
    plot it. Figure 5.13 shows the first simulated image in one run—in this case,
    the number 3.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们还从生成模型中模拟随机图像并绘制出来。图5.13显示了在一次运行中生成的第一张模拟图像——在这种情况下，数字3。
- en: '![figure](../Images/CH05_F12_Ness.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F12_Ness.png)'
- en: Figure 5.12 The first attempt to reconstruct an image during training shows
    the model has learned something but still has much progress to make.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.12 训练过程中第一次尝试重构图像显示了模型已经学到了一些东西，但仍有很大的进步空间。
- en: '![figure](../Images/CH05_F13_Ness.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F13_Ness.png)'
- en: Figure 5.13 The first instance of an image generated from the generative model
    during training
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13 训练过程中从生成模型生成的第一个图像实例
- en: But the model learns quickly. By 130 epochs, we get the results in figure 5.14.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 但模型学习得很快。到130个epoch时，我们在图5.14中得到了结果。
- en: After training is complete, we can see a visualization of loss over training
    (negative ELBO) in figure 5.15.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以在图5.15中看到训练过程中的损失可视化（负ELBO）。
- en: The code will train the parameters of the encoder that maps images and the labels
    to the latent variable. It will also train the decoder that maps the latent variable
    and the labels to the image. That latent variable is a fundamental feature of
    the VAE, but we should take a closer look at how to interpret the latent variable
    in causal terms.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将训练将图像和标签映射到潜在变量的编码器参数。它还将训练将潜在变量和标签映射到图像的解码器。这个潜在变量是VAE的基本特征，但我们应该更仔细地看看如何从因果角度解释潜在变量。
- en: '![figure](../Images/CH05_F14_Ness.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F14_Ness.png)'
- en: Figure 5.14 Reconstructed and randomly generated images from the model after
    130 epochs of training look much better.
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.14 经过130个epoch的训练后，模型重构和随机生成的图像看起来要好得多。
- en: '![figure](../Images/CH05_F15_Ness.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F15_Ness.png)'
- en: Figure 5.15 Test loss as training progresses. The *x*-axis is the epoch.
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.15 随着训练的进行，测试损失。*x*轴是epoch。
- en: 5.2.5 How should we causally interpret Z?
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.5 我们应该如何因果解释Z？
- en: I said we can view *Z* as a “stand-in” for all the independent latent causes
    of the object in the image. *Z* is a representation we learn from the pixels in
    the images. It is tempting to treat that representation like a higher-level causal
    abstraction of those latent causes, but it is probably not doing a great job as
    a causal abstraction. The autoencoder paradigm trains an encoder that can take
    an image and embed it into a low-dimensional representation *Z*. It tries to do
    so in a way that enables it to reconstruct the original image as well as possible.
    In order to reconstruct the image with little loss, the framework tries to encode
    as much information from the original image as it can in that lower dimensional
    representation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我说过我们可以将*Z*视为图像中所有独立潜在原因的“替身”。*Z*是从图像像素中学习到的表示。将其视为那些潜在原因的高级因果抽象的诱惑很大，但它可能并不是一个好的因果抽象。自动编码器范式训练一个编码器，可以将图像嵌入到低维表示*Z*中。它试图以能够尽可能好地重构原始图像的方式做到这一点。为了以最小的损失重构图像，框架试图在低维表示中尽可能多地编码原始图像的信息。
- en: A good *causal* representation, however, shouldn’t try to capture as much information
    as possible. Rather, it should strive to capture only the *causal* information
    in the images and ignore everything else. Indeed, the task of “disentangling”
    the causal and non-causal factors in *Z* is generally impossible when *Z* is unsupervised
    (meaning we lack labels for *Z*). However, domain knowledge, interventions, and
    semi-supervision can help. See [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references on *causal representation learning* and *disentanglement of causal
    factors*. As we progress through the book, we’ll develop intuition for what the
    “causal information” in such a representation should look like.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个好的*因果*表示不应该试图捕捉尽可能多的信息。相反，它应该努力只捕捉图像中的*因果*信息，忽略其他所有信息。实际上，当*Z*是无监督的（意味着我们缺乏*Z*的标签）时，通常无法在*Z*中“分离”因果和非因果因素。然而，领域知识、干预和半监督可以帮助。有关*因果表示学习*和*因果因素的分离*的参考文献，请参阅[https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)。随着我们继续阅读本书，我们将对这种表示中的“因果信息”应该是什么样子形成直觉。
- en: 5.2.6 Advantages of this causal interpretation
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.6 这种因果解释的优势
- en: There is nothing inherently causal about our VAE’s setup and training procedure;
    it is typical of a vanilla supervised VAE you’d see in many machine learning settings.
    The only causal element of our approach was our interpretation. We say that the
    *digit* and *is-handwritten* are causes, and *Z* is a stand-in for latent causes,
    and the image is the outcome. Applying the causal Markov property, our causal
    model factorizes the joint distribution into *P*(*Z*), *P*(*is-handwritten*),
    *P*(*digit*), and *P*(*image*|*Z*, *is-handwritten*, *digit*), where the latter
    factor is the causal Markov kernel of the image.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们VAE的设置和训练过程并没有固有的因果性；这在许多机器学习场景中常见的普通监督VAE中很典型。我们方法中唯一的因果元素是我们的解释。我们说*数字*和*是否手写*是原因，*Z*是潜在原因的替代品，图像是结果。应用因果马尔可夫性质，我们的因果模型将联合分布分解为*P*(*Z*)、*P*(*is-handwritten*)、*P*(*digit*)和*P*(*image*|*Z*,
    *is-handwritten*, *digit*)，其中后者是图像的因果马尔可夫核。
- en: What can we do with this causal interpretation? First, we can use it to improve
    deep learning and general machine learning workflows and tasks. We’ll see an example
    of this with *semi-supervised learning* in the next section.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用这种因果解释做什么？首先，我们可以用它来改进深度学习和通用机器学习的工作流程和任务。我们将在下一节中看到一个关于*半监督学习*的例子。
- en: Incorporating generative AI in causal models is not limited to VAEs
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在因果模型中融入生成式AI不仅限于VAE
- en: I demonstrated how to use a VAE framework to fit a causal Markov kernel entailed
    by a causal DAG, but a VAE was just one approach to achieving this end. We could
    have used another deep probabilistic machine learning framework, such as a generative
    adversarial network (GAN) or a diffusion model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我演示了如何使用VAE框架来拟合由因果有向无环图（DAG）蕴含的因果马尔可夫核，但VAE只是实现这一目标的一种方法。我们还可以使用其他深度概率机器学习框架，例如生成对抗网络（GAN）或扩散模型。
- en: In this section, we incorporated deep learning into a causal graphical model.
    Next, we investigate how to use causal ideas to enhance deep learning.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深度学习融入了因果图模型。接下来，我们将探讨如何利用因果思想来增强深度学习。
- en: 5.3 Using causal inference to enhance deep learning
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 利用因果推理增强深度学习
- en: We can use causal insights to improve how we set up and train deep learning
    models. These insights tend to lead to benefits such as improved sample efficiency
    (i.e., doing more with less data), the ability to do transfer learning (using
    what a model learned in solving one task to improve performance on another), data
    fusion (combining different datasets), and enabling more robust predictions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用因果洞察来改进我们设置和训练深度学习模型的方式。这些洞察往往导致诸如提高样本效率（即用更少的数据做更多的事情）、进行迁移学习（使用模型在解决一个任务中学到的知识来提高另一个任务的性能）、数据融合（结合不同的数据集）以及使预测更加鲁棒等好处。
- en: Much of the work of deep learning is trial and error. For example, when training
    a VAE or other deep learning models, you typically experiment with different approaches
    (VAE vs. another framework), architectural choices (latent variable and hidden
    layer dimension, activation functions, number of layers, etc.), and training approaches
    (choice of loss function, learning rate, optimizer, etc.) before you get a good
    result. These experiments cost time, effort, and resources. In some cases, causal
    modeling can help you make better choices about what might work and what is unlikely
    to work, leading to cost savings. In this section, we’ll look at a particular
    example of this case in the context of semi-supervised learning.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的大部分工作都是试错。例如，在训练 VAE 或其他深度学习模型时，你通常会在得到好结果之前尝试不同的方法（VAE 与其他框架的比较）、架构选择（潜在变量和隐藏层维度、激活函数、层数等）以及训练方法（损失函数的选择、学习率、优化器等）。这些实验耗费时间、精力和资源。在某些情况下，因果建模可以帮助你更好地选择可能有效和可能无效的方法，从而节省成本。在本节中，我们将探讨半监督学习背景下这种特定情况的例子。
- en: 5.3.1 Independence of mechanism as an inductive bias
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 机制的独立性作为归纳偏差
- en: 'Suppose we had a DAG with two variables: “cause” *C* and “outcome” *O*. The
    DAG is simply *C* → *O*. Our causal Markov kernels are *P*(*C*) and *P*(*O*|*C*).
    Recall the idea of *independence of mechanism* from chapter 3—the causal Markov
    kernel *P*(*O*|*C*) represents a mechanism of how the cause *C* drives the outcome
    *O*. That mechanism is distinct from other mechanisms in the system, such that
    changes to those mechanisms have no effect on *P*(*O*|*C*). Thus, knowing about
    *P*(*O*|*C*) tells you nothing about the distribution of the cause *P*(*C*) and
    vice versa. However, knowing something about the distribution of the outcome *P*(*O*)
    might tell you something about the distribution of the cause given the outcome
    *P*(*C*|*O*), and vice versa.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含两个变量的 DAG（有向无环图）：“原因” *C* 和“结果” *O*。该 DAG 简单地是 *C* → *O*。我们的因果马尔可夫核是
    *P*(*C*) 和 *P*(*O*|*C*)。回忆一下第 3 章中关于 *机制独立性* 的概念——因果马尔可夫核 *P*(*O*|*C*) 代表了原因 *C*
    导致结果 *O* 的机制。这种机制与系统中的其他机制不同，即对这些机制的改变不会影响 *P*(*O*|*C*)。因此，了解 *P*(*O*|*C*) 并不会告诉你关于原因分布
    *P*(*C*) 的任何信息，反之亦然。然而，了解关于结果分布 *P*(*O*) 的某些信息可能会告诉你关于给定结果的原因分布 *P*(*C*|*O*) 的某些信息，反之亦然。
- en: To illustrate, consider a scenario where *C* represents sunscreen usage and
    *O* indicates whether someone has sunburn. You understand the *mechanism* by which
    sunscreen protects against sunburn (UV rays, SPF levels, regular application,
    the perils of sweat and swimming, etc.), and by extension, the chances of getting
    sunburn given how one uses sunscreen, captured by *P*(*O*|*C*). However, this
    understanding of the mechanism doesn’t provide any information about how *common*
    sunscreen use is, denoted by *P*(*C*).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，考虑一个场景，其中 *C* 代表防晒霜的使用，而 *O* 表示某人是否晒伤。你理解防晒霜防止晒伤的机制（紫外线、SPF 水平、定期涂抹、汗水和游泳的潜在危险等），并且通过扩展，根据一个人如何使用防晒霜，由
    *P*(*O*|*C*) 捕获的晒伤的可能性。然而，这种对机制的这种理解并不提供任何关于防晒霜使用有多普遍的信息，这由 *P*(*C*) 表示。
- en: Now, suppose you’re trying to guess whether a sunburned person used sunscreen,
    i.e., you’re mentally modeling *P*(*C*|*O*). In this case, knowing the prevalence
    of sunburns, *P*(*O*), could help. Consider whether the sunburned individual was
    a case of someone who did use sunscreen but got a sunburn anyway. That case would
    be more likely if sunburns were a common problem than if sunburns were rare—if
    sunburns are common, sunscreen use is probably common, but if sunburns were uncommon,
    people would be less cautious about prevention.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你正在尝试猜测一个晒伤的人是否使用了防晒霜，即你正在心理上建模 *P*(*C*|*O*)。在这种情况下，了解晒伤的普遍性，*P*(*O*)，可能会有所帮助。考虑晒伤的个体是否是那种使用了防晒霜但仍然晒伤的情况。如果晒伤是一个常见问题，那么这种情况更有可能发生，如果晒伤很少见，那么人们就会对预防措施不那么谨慎。
- en: Similarly, suppose *C* represents study effort and *O* represents test scores.
    You know the causal mechanism behind how studying more causes higher test scores,
    captured by *P*(*O*|*C*). But this doesn’t tell you how common it is for students
    to study hard, captured by *P*(*C*). Suppose a student got a low test score, and
    you are trying to infer whether they studied hard—you are mentally modeling *P*(*C*|*O*).
    Again, knowing the typical distribution of test scores *P*(*O*) can help. If low
    scores are rare, students might be complacent, and thus more likely not to study
    hard. You can use that insight as an *inductive bias*—a way to constrain your
    mental model of *P*(*C*|*O*).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，假设 *C* 代表学习努力程度，*O* 代表考试成绩。你知道学习更多导致更高考试成绩的因果机制，这由 *P*(*O*|*C*) 捕获。但这并不告诉你学生努力学习有多普遍，这由
    *P*(*C*) 捕获。假设一个学生考试成绩低，你正在尝试推断他们是否努力学习——你正在心理上建模 *P*(*C*|*O*)。再次，知道考试成绩的典型分布
    *P*(*O*) 可以有所帮助。如果低分很少见，学生可能会自满，因此不太可能努力学习。你可以利用这个洞察作为 *归纳偏差*——一种约束你心理模型 *P*(*C*|*O*)
    的方法。
- en: Causal inductive bias
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因果归纳偏差
- en: “Inductive bias” refers to the assumptions (explicit or implicit) that lead
    an inference algorithm to prefer certain inferences or predictions over others.
    Examples of inductive bias include Occam’s Razor and the assumption in forecasting
    that trends in the past will continue into the future.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: “归纳偏差”指的是导致推理算法偏好某些推理或预测而不是其他推理或预测的假设（显式或隐式）。归纳偏差的例子包括奥卡姆剃刀和预测中过去趋势将继续到未来的假设。
- en: Modern deep learning relies on using neural network architectures and training
    objectives to encode inductive bias. For example, “convolutions” and “max pooling”
    are architectural elements in convolutional neural networks for computer vision
    that encode an inductive bias called “translation invariance”; i.e., a kitten
    is still a kitten regardless of whether it appears on the left or right of an
    image.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习依赖于使用神经网络架构和训练目标来编码归纳偏差。例如，“卷积”和“最大池化”是计算机视觉卷积神经网络中的架构元素，它们编码了一个称为“平移不变性”的归纳偏差；即，无论一只小猫出现在图像的左边还是右边，它仍然是一只小猫。
- en: Causal models provide inductive biases in the form of causal assumptions about
    the DGP (such as a causal DAG). Deep learning can leverage these causal inductive
    biases to attain better results just as it does with other types of inductive
    biases. For example, independence of mechanism suggests that knowing *P*(*O*)
    could provide a useful inductive bias in learning *P*(*C*|*O*).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 因果模型以因果假设的形式提供归纳偏差，这些假设关于 DGP（例如因果图）。深度学习可以利用这些因果归纳偏差，就像它利用其他类型的归纳偏差一样，以获得更好的结果。例如，机制的独立性表明，知道
    *P*(*O*) 可以在学习 *P*(*C*|*O*) 时提供有用的归纳偏差。
- en: Now consider two variables *X* and *Y* (which can be vectors) with joint distribution
    *P*(*X*, *Y*). We want to design an algorithm that solves a task by learning from
    data observed from *P*(*X*, *Y*). The chain rule of probability tells us that
    *P*(*X*=*x*, *Y*=*y*) = *P*(*X*=*x*|*Y*=*y*)*P*(*Y*=*y*) = *P*(*Y*=*y*|*X*=*x*)*P*(*X*=*x*).
    So, from that basic probabilistic perspective, modeling the set {*P*(*X*|*Y*),
    *P*(*Y*)} is equivalent to modeling the set {*P*(*Y*|*X*), *P*(*X*)}. But consider
    the cases where either *X* is a cause of *Y* or where *Y* is a cause of *X*. Under
    these circumstances, the independence of mechanism gives us an asymmetry between
    sets {*P*(*X*|*Y*), *P*(*Y*)} and {*P*(*Y*|*X*), *P*(*X*)} (specifically, {*P*(*Y*|*X*),
    *P*(*X*)} represents the independent mechanism behind *X*’s causal influence on
    *Y*, and {*P*(*X*|*Y*), *P*(*Y*)} does not) that we can possibly leverage as an
    inductive bias in these algorithms. Semi-supervised learning is a good example.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑两个变量 *X* 和 *Y*（可以是向量）的联合分布 *P*(*X*, *Y*)。我们希望设计一个算法，通过从观察到的 *P*(*X*, *Y*)
    数据中学习来解决问题。概率的链式法则告诉我们 *P*(*X*=*x*, *Y*=*y*) = *P*(*X*=*x*|*Y*=*y*)*P*(*Y*=*y*)
    = *P*(*Y*=*y*|*X*=*x*)*P*(*X*=*x*)。因此，从基本概率的角度来看，对集合 {*P*(*X*|*Y*), *P*(*Y*)}
    的建模等同于对集合 {*P*(*Y*|*X*), *P*(*X*)} 的建模。但是考虑以下情况：要么 *X* 是 *Y* 的原因，要么 *Y* 是 *X*
    的原因。在这些情况下，机制的独立性在集合 {*P*(*X*|*Y*), *P*(*Y*)} 和 {*P*(*Y*|*X*), *P*(*X*)} 之间产生了不对称性（具体来说，集合
    {*P*(*Y*|*X*), *P*(*X*)} 代表了 *X* 对 *Y* 的因果影响背后的独立机制，而集合 {*P*(*X*|*Y*), *P*(*Y*)}
    则没有），我们可以在这些算法中利用这种不对称性作为归纳偏差。半监督学习是一个很好的例子。
- en: '5.3.2 Case study: Semi-supervised learning'
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 案例研究：半监督学习
- en: Returning to our TMNIST-MNIST VAE-based causal model, suppose we had, in addition
    to our original data, a large set of images of digits that were unlabeled (i.e.,
    *digit* and *is-handwritten* are not observed). Our causal interpretation of our
    model suggests we can leverage this data during training using semi-supervised
    learning.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的基于TMNIST-MNIST VAE的因果模型，假设除了我们的原始数据外，我们还有一个大量未标记的数字图像集（即，*digit*和*is-handwritten*没有被观察到）。我们模型因果解释的暗示表明，我们可以在训练过程中利用这些数据，使用半监督学习。
- en: Independence of mechanism can help you determine when semi-supervised learning
    will be effective. In *supervised learning*, the training data consists of *N*
    samples of *X*, *Y* pairs; (*x*[1], *y*[1]), (*x*[2], *y*[2]), …, (*x*[*N*], *y*[*N*]).
    *X* is the *feature data* used to predict the *labels* *Y*. The data is “supervised”
    because every *x* is paired with a *y*. We can use these pairs to learn *P*(*Y*|*X*).
    In *unsupervised learning*, the data *X* is unsupervised, meaning we have no labels,
    no observed value of *Y*. Our data looks like (*x*[1]), (*x*[2]), …, (*x*[*N*]).
    With this data alone, we can’t directly learn anything about *P*(*Y*|*X*); we
    can only learn about *P*(*X*). Semi-supervised learning asks the question, suppose
    we had a combination of supervised and unsupervised data. Could these two sets
    of data be combined in a way such that our ability to predict *Y* was better than
    if we only used the supervised data? In other words, can learning more about *P*(*X*)
    from the unsupervised data somehow augment our learning of *P*(*Y*|*X*) from the
    supervised data?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 机制独立性可以帮助你确定何时半监督学习将有效。在*监督学习*中，训练数据由*N*个*X*，*Y*对的样本组成；（*x*[1]，*y*[1]），（*x*[2]，*y*[2]），…，（*x*[*N*]，*y*[*N*]）。*X*是用于预测*标签*
    *Y* 的*特征数据*。数据是“监督的”，因为每个*x*都与一个*y*配对。我们可以使用这些对来学习*P*(*Y*|*X*)。在*无监督学习*中，数据*X*是无监督的，这意味着我们没有标签，没有观察到*y*的值。我们的数据看起来像（*x*[1]），（*x*[2]），…，（*x*[*N*]）。仅凭这些数据，我们无法直接学习有关*P*(*Y*|*X*)的任何内容；我们只能学习有关*P*(*X*)的内容。半监督学习提出的问题是，假设我们有一个监督和无监督数据的组合。这两组数据能否以某种方式结合，使得我们预测*y*的能力比仅使用监督数据时更好？换句话说，从无监督数据中更多地了解*P*(*X*)是否能够以某种方式增强我们从监督数据中学习*P*(*Y*|*X*)的能力？
- en: The semi-supervised question is quite practical. It is common to have abundant
    unsupervised examples if labeling those examples is costly. For example, suppose
    you worked at a social media site and were tasked with building an algorithm that
    classified whether an uploaded image depicted gratuitous violence. The first step
    is to create supervised data by having humans manually label images as gratuitously
    violent or not. Not only does this cost many people-hours, but it is mentally
    stressful for the labelers. A successful semi-supervised approach would mean you
    could minimize the amount of labeling work you need to do.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督问题非常实用。如果标记这些示例的成本很高，通常会有大量的无监督示例。例如，假设你在一家社交媒体网站上工作，并被分配构建一个算法来分类上传的图像是否描绘了无节制的暴力。第一步是通过让人类手动标记图像为无节制的暴力或不暴力来创建监督数据。这不仅需要很多人工时，而且对标记者来说精神压力很大。一个成功的半监督方法意味着你可以最小化你需要做的标记工作。
- en: Our task is to learn a representation of *P*(*X* ,*Y*) and use it to predict
    from *P*(*Y*|*X*). For semi-supervised learning to work, the unlabeled values
    of *X* must update the representation of *P*(*X* , *Y*) in a way that provides
    information about *P*(*Y*|*X*). However, independence of mechanism means the task
    of learning *P*(*X* ,*Y*) decomposes into learning distinct representations of
    the causal Markov kernels, where the parameter vector of each representation is
    orthogonal to the others. That parameter modularity (see section 3.2) can block
    flow of parameter updating information from the unlabeled observations of *X*
    to the learned representation of *P*(*Y*|*X*). To illustrate, let's consider two
    possibilities, one where *Y* is a cause of *X*, and one where *X* is a cause of
    *Y*. If *Y* is a cause of *X*, such as in our MNIST-TMNIST example (*Y* is the
    is-handwritten and digit variables, and *X* is the image), then our learning task
    decomposes into learning distinct representations of *P*(*X*|*Y*) and P(Y). Unlabeled
    observations of *X* can give us a better representation of *P*(*X*), we can use
    to flip *P*(*X*|*Y*) into *P*(*Y*|*X*) by way of Bayes rule. However, when *X*
    is a cause of *Y*, our learning task decomposes into learning distinct representations
    of *P*(*X*) and *P*(*Y*|*X*). That parameter modularity means those unlabeled
    values of *X* will help us update *P*(*X*)’s representation but not that of *P*(*Y*|*X*).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是学习 *P*(*X*，*Y*) 的表示，并使用它来预测 *P*(*Y*|*X*)。为了使半监督学习起作用，*X* 的未标记值必须以提供关于
    *P*(*Y*|*X*) 的信息的方式更新 *P*(*X*，*Y*) 的表示。然而，机制独立性意味着学习 *P*(*X*，*Y*) 的任务分解为学习因果马尔可夫核的独立表示，其中每个表示的参数向量与其他参数向量正交。这种参数模块化（见第3.2节）可以阻止参数更新信息从
    *X* 的未标记观察结果流向学习到的 *P*(*Y*|*X*) 表示。为了说明，让我们考虑两种可能性，一种情况是 *Y* 是 *X* 的原因，另一种情况是
    *X* 是 *Y* 的原因。如果 *Y* 是 *X* 的原因，例如在我们的MNIST-TMNIST例子中（*Y* 是手写和数字变量，*X* 是图像），那么我们的学习任务分解为学习
    *P*(*X*|*Y*) 和 *P*(*Y*) 的独立表示。未标记的 *X* 观察结果可以给我们一个更好的 *P*(*X*) 表示，我们可以通过贝叶斯规则将其转换为
    *P*(*Y*|*X*)。然而，当 *X* 是 *Y* 的原因时，我们的学习任务分解为学习 *P*(*X*) 和 *P*(*Y*|*X*) 的独立表示。这种参数模块化意味着那些未标记的
    *X* 值将帮助我们更新 *P*(*X*) 的表示，但不会更新 *P*(*Y*|*X*) 的表示。
- en: '![figure](../Images/CH05_F16_Ness.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F16_Ness.png)'
- en: Figure 5.16 In causal learning, the features cause the label. In anti-causal
    learning, the label causes the features.
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.16 在因果学习中，特征导致标签。在反因果学习中，标签导致特征。
- en: The case where the feature causes the label is sometimes called *causal learning*
    because the direction of the prediction is from the cause to the effect. *Anti-causal
    learning* refers to the case when the label causes the feature. The two cases
    are illustrated in figure 5.16.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征导致标签的情况有时被称为 *因果学习*，因为预测的方向是从原因到效果。*反因果学习*指的是标签导致特征的情况。这两种情况在图5.16中得到了说明。
- en: Independence of mechanism suggests semi-supervised learning can achieve performance
    gains (relative to a baseline of supervised learning on only the labeled data)
    only in the anti-causal case. See the chapter notes at www.altdeep.ai/causalAIbook
    for a more detailed explanation and references. But intuitively, we can see that
    this mirrors the the sunscreen and sunburn example—knowing the prevalence of sunburns
    *P*(*O*) helped in learning how to guess sunscreen use when you know if someone
    has a sunburn *P*(*C*|*O*). In this same anti-causal learning case, having only
    observations from *P*(*X*) can still be helpful in learning a good model of *P*(*Y*|*X*).
    But in the causal learning case, it would be a waste of effort and resources.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 机制独立性的假设表明，半监督学习只能在反因果情况下实现性能提升（相对于仅对标记数据进行监督学习的基线）。有关更详细解释和参考文献，请参阅www.altdeep.ai/causalAIbook中的章节注释。但直观上，我们可以看到这反映了防晒霜和晒伤的例子——知道晒伤的普遍性
    *P*(*O*) 有助于在知道某人是否晒伤 *P*(*C*|*O*) 的情况下猜测防晒霜的使用情况。在这个相同的反因果学习案例中，仅从 *P*(*X*) 的观察结果中仍然可以帮助学习一个良好的
    *P*(*Y*|*X*) 模型。但在因果学习案例中，这将是一种浪费努力和资源。
- en: In practice, the causal structure between *X* and *Y* could be more nuanced
    and complicated than these simple *X*→*Y* and *X*←*Y* cases. For example, there
    could be unobserved common causes of *X* and *Y*. The takeaway here is that when
    you know something about the causal relationships between the variables in your
    machine learning problem, you can leverage that knowledge to model more effectively,
    even if the task is not a causal inference task (e.g., simply predicting *Y* given
    *X*). This could help you avoid spending time and resources on an approach that
    is not likely to work, as in the semi-supervised case. Or it could enable more
    efficient, robust, or better performing inferences.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，X和Y之间的因果结构可能比这些简单的X→Y和X←Y案例更为复杂和微妙。例如，可能存在X和Y未观察到的共同原因。这里的启示是，当你了解机器学习问题中变量之间的因果关系时，你可以利用这些知识来更有效地建模，即使任务不是因果推理任务（例如，仅根据X预测Y）。这可以帮助你避免在不太可能有效的方法上浪费时间和资源，就像半监督学习的情况一样。或者它可能使推理更加高效、鲁棒或表现更好。
- en: 5.3.3 Demystifying deep learning with causality
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 通过因果性揭开深度学习的神秘面纱
- en: Our semi-supervised learning example highlights how a causal perspective can
    explain when we’d expect semi-supervised learning to work and when to fail. In
    other words, it somewhat *demystifies* semi-supervised learning.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的半监督学习示例突出了因果视角如何解释我们何时期望半监督学习能够成功，何时会失败。换句话说，它在某种程度上*揭开了*半监督学习的神秘面纱。
- en: That mystery around the effectiveness of deep learning methods led AI researcher
    Ali Rahimi to compare modern machine learning to alchemy.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法有效性的神秘性导致AI研究人员阿里·拉希米将现代机器学习与炼金术进行比较。
- en: Alchemy worked. Alchemists invented metallurgy, ways to dye textiles, modern
    glass-making processes, and medications. Then again, alchemists also believed
    they could cure diseases with leeches and transmute base metals into gold.
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 炼金术成功了。炼金术士发明了冶金术、染料纺织的方法、现代玻璃制造工艺和药物。然而，炼金术士也相信他们可以用蜥蜴治愈疾病，将普通金属转化为黄金。
- en: In other words, alchemy works, but alchemists lacked an understanding of the
    underlying scientific principles that made it work when it did. That *mystery*
    made it hard to know when it would fail. As a result, alchemists wasted considerable
    effort on dead ends (philosopher’s stones, immortality elixirs, etc.).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，炼金术是有效的，但炼金术士缺乏理解其工作背后的科学原理，这使其在有效时有效。这种*神秘性*使得很难知道它何时会失败。因此，炼金术士在死胡同（哲学家石，长生不老药等）上浪费了大量的努力。
- en: Chapter checkpoint
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 章节检查点
- en: '*Incorporating deep learning into a causal model:*'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '*将深度学习融入因果模型：*'
- en: ✓ A causal model of a computer vision problem
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 计算机视觉问题的因果模型
- en: ✓ Training the deep causal image model
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 训练深度因果图像模型
- en: '*Using causal reasoning to enhance machine learning:*'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用因果推理来增强机器学习：*'
- en: ✓ Case study on independence of mechanism and semi-supervised learning
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ✓ 机制独立与半监督学习案例研究
- en: 👉 Demystifying deep learning with causality
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 通过因果性揭开深度学习的神秘面纱
- en: Similarly, deep learning “works” in that it achieves good performance on a wide
    variety of prediction and inference tasks. But we often have an incomplete understanding
    of why and when it works. That *mystery* has led to problems with reproducibility,
    robustness, and safety. It also leads to irresponsible applications of AI, such
    as published work that attempts to predict behavior (e.g., criminality) from profile
    photos. Such efforts are the machine learning analog of the alchemical immortality
    elixirs that contained toxins like mercury; they don’t work *and* they cause harm.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，深度学习“有效”是因为它在广泛的预测和推理任务上取得了良好的性能。但我们通常对为什么以及何时有效缺乏完整理解。这种*神秘性*导致了可重复性、鲁棒性和安全性的问题。它还导致了AI的不负责任应用，例如试图从个人资料照片预测行为（例如犯罪性）的发表工作。这种努力是机器学习中的炼金术长生不老药类似物，其中含有水银等毒素；它们不仅不工作，而且还会造成伤害。
- en: We often hear about the “superhuman” performance of deep learning. Speaking
    of superhuman ability, imagine an alternative telling of Superman’s origin story.
    Imagine if, when Superman made his first public appearance, his superhuman abilities
    were unreliable? Suppose he demonstrated astounding superhuman feats like flight,
    super strength, and laser vision, but sometimes his flight ability failed and
    his super strength faltered. Sometimes his laser vision was dangerously unfocused,
    resulting in terrible collateral damage. The public would be impressed and hopeful
    that he could do some good, but unsure if it would be safe to rely on him when
    the stakes were high.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常听到深度学习的“超人类”性能。说到超人类能力，想象一下超人起源故事的另一种说法。想象一下，当超人第一次公开露面时，他的超人类能力是不可靠的？假设他展示了惊人的超人类壮举，如飞行、超级力量和激光视力，但有时他的飞行能力会失败，他的超级力量会减弱。有时他的激光视力会极度不聚焦，造成严重的附带损害。公众会感到印象深刻，并希望他能做一些好事，但不确定在高风险的情况下是否可以依赖他。
- en: Now imagine that his adoptive Midwestern parents, experts in causal inference,
    used causal analysis to model the *how* and *why* of his powers. Having demystified
    the mechanisms underlying his superpowers, they were able to engineer a pill that
    stabilized those powers. The pill wouldn’t so much give Superman new powers; it
    would just make his existing powers more reliable. The work of developing that
    pill would get fewer headlines than flight and laser vision, but it would be the
    difference between merely having superpowers and being Superman.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，他的养父母，来自中西部的因果推断专家，用因果分析来模拟他能力的**如何**和**为什么**。在揭开了他超能力背后的机制之后，他们能够制造出一种稳定这些能力的药片。这种药片并不会赋予超人新的能力；它只是让他的现有能力更加可靠。开发这种药片的工作可能不会像飞行和激光视力那样成为头条新闻，但它却是仅仅拥有超能力与成为超人之间的区别。
- en: This analogy helps us understand the impact of using causal methods to demystify
    deep learning and other machine learning methods. Less mystery leads to more robust
    methods and helps us avoid wasteful or harmful applications.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类比帮助我们理解使用因果方法来揭示深度学习和其他机器学习方法的影响。减少神秘感导致更稳健的方法，并帮助我们避免浪费或有害的应用。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning can be used to enhance causal modeling and inference. Causal reasoning
    can enhance the setup, training, and performance of deep learning models.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可以用来增强因果建模和推理。因果推理可以增强深度学习模型的设置、训练和性能。
- en: Causal models can leverage the ability of deep learning to scale and work with
    high-dimensional nonlinear relationships.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果模型可以利用深度学习扩展和操作高维非线性关系的能力。
- en: You can use generative AI frameworks like the variational autoencoder to build
    a causal generative model on a DAG just as we did with pgmpy.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用变分自动编码器等生成式AI框架，在DAG上构建因果生成模型，就像我们使用pgmpy一样。
- en: The decoder maps the outcomes of direct parents (the labels of an image) to
    the outcomes of the child (the image).
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器将直接父代（图像的标签）的结果映射到子代（图像）的结果。
- en: In other words, the decoder gives us a nonlinear high-dimensional representation
    of the causal Markov kernel for the image.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 换句话说，解码器为我们提供了因果马尔可夫核的非线性高维表示，用于图像。
- en: The encoder maps the image variable and the causes (labels) back to the latent
    variable *Z*.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器将图像变量和原因（标签）映射回潜在变量 *Z*。
- en: We can view the learned representation of the latent variable as a stand-in
    for unmodeled causes, but it still lacks the qualities we’d expect from an ideal
    causal representation. Learning latent causal representations is an active area
    of research.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将潜在变量的学习表示视为未建模原因的替代品，但它仍然缺乏我们期望的理想因果表示所具有的品质。学习潜在因果表示是一个活跃的研究领域。
- en: Causality often enhances deep learning and other machine learning methods by
    helping elucidate the underlying principles that make it work. For example, causal
    analysis shows semi-supervised learning should work in the case of *anti-causal
    learning* (when the features are *caused by* the label) but not in the case of
    *causal learning* (when the features cause the label).
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果性通常通过帮助阐明使其工作的基本原理来增强深度学习和其他机器学习方法。例如，因果分析表明，在**反因果学习**的情况下（当特征**由**标签引起时），半监督学习应该有效，但在**因果学习**的情况下（当特征引起标签时）则不适用。
- en: Such causal insights can help the modeler avoid spending time, compute, person-hours,
    and other resources on a given algorithm when it is not likely to work in a given
    problem setting.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这样的因果洞察可以帮助模型构建者避免在某个算法不太可能在给定问题设置中工作的情况下，浪费时间、计算资源、人力小时和其他资源。
- en: Causal insights can demystify elements of building and training deep learning
    models, such that they become more robust, efficient, and safe.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因果洞察可以揭开构建和训练深度学习模型元素的面纱，从而使它们变得更加稳健、高效和安全。
