- en: 5 Connecting causality and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incorporating deep learning into a causal graphical model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a causal graphical model with a variational autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using causal methods to enhance machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The title of this book is *Causal AI*, but how exactly does causality connect
    to AI? More specifically, how does causality connect with deep learning, the dominant
    paradigm in AI? In this chapter, I look at this question from two perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How to incorporate deep learning into a causal model*—We’ll look at a causal
    model of a computer vision problem (section 5.1) and then train the deep causal
    image model (section 5.2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How to use causal reasoning to do better deep learning*—We’ll look at a case
    study on independence of mechanism and semi-supervised learning (section 5.3.1
    and 5.3.2), and we’ll demystify deep learning with causality (section 5.3.3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term *deep learning* broadly refers to applications of deep neural networks.
    It’s a machine learning approach that stacks many nonlinear models together in
    sequential layers, emulating the connections of neurons in brains. “Deep” refers
    to stacking many layers to achieve more modeling power, particularly in terms
    of modeling high-dimensional and nonlinear data, such as visual media and natural
    language text. Neural nets have been around for a while, but relatively recent
    advancements in hardware and automatic differentiation have made it possible to
    scale deep neural networks to extremely large sizes. That scaling is why, in recent
    years, there have been multiple cases of deep learning outperforming humans on
    many advanced inference and decision-making tasks, such as image recognition,
    natural language processing, game playing, medical diagnosis, autonomous driving,
    and generating lifelike text, images, and video.
  prefs: []
  type: TYPE_NORMAL
- en: But asking how deep learning connects to causality can elicit frustrating answers.
    AI company CEOs and leaders in big tech fuel hype about the power of deep learning
    models and even claim they can learn the causal structure of the world. On the
    other hand, some leading researchers claim these models are merely “stochastic
    parrots” that can echo patterns of correlation that, while nuanced and complex,
    still fall short of true causal understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal in this chapter is to reconcile these perspectives. But skipping ahead,
    the main takeaway is that deep learning architecture can be integrated into a
    causal model and we can train the model using deep learning training techniques.
    But also, we can use causal reasoning to build better deep learning models and
    improve how we train them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll anchor this idea in two case studies:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a causal DAG for computer vision using a variational autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing better semi-supervised learning using independence of mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other examples of the interplay of causality and AI that you’ll see in the rest
    of the book will build on the intuition we get from these case studies. For example,
    chapter 9 will illustrate counterfactual reasoning using a variational autoencoder
    like the one we’ll build in this chapter. In chapter 11, we’ll explore machine
    learning and probabilistic deep learning approaches for causal effect inference.
    Chapter 13 will show how to combine large language models and causal reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by considering how to incorporate deep learning into a causal model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 A causal model of a computer vision problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a computer vision problem that we can approach with a causal DAG.
    Recall the MNIST data from chapter 1, composed of images of digits and their labels,
    illustrated in figure 5.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F01_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 MNIST data featuring images of handwritten digits and their digit
    labels
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There is a related dataset called Typeface MNIST (TMNIST) that also features
    digit images and their digit labels. However, instead of handwritten digits, the
    images are digits rendered in 2,990 different fonts, illustrated in figure 5.2\.
    For each image, in addition to a digit label, there is a font label. Examples
    of the font labels include “GrandHotel-Regular,” “KulimPark-Regular,” and “Gorditas-Bold.”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F02_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 Examples from the Typeface MNIST, which is composed of typed digits
    with different typefaces. In addition to a digit label for each digit, there is
    a label for one of 2,990 different typefaces (fonts).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this analysis, we’ll combine these datasets into one and build a simple deep
    causal generative model on that data. We’ll simplify the “fonts” label into a
    sample binary label that indicates “handwritten” for MNIST images and “typed”
    for the TMNIST images.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to build a causal generative model on top of a DAG. We factorized
    the joint distribution into a product of *causal Markov kernels* representing
    the conditional probability distributions for each node, conditional on their
    parents in the DAG. In our previous examples in pgmpy, we fit a conditional probability
    table for each of these kernels.
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine how hard it would be to use a conditional probability table
    to represent the conditional probability distribution of pixels in an image. But
    there is nothing stopping us from modeling the causal Markov kernel with a deep
    neural net, which we know is flexible enough to work with high-dimensional features
    like pixels. In this section, I’ll demonstrate how to use deep neural nets to
    model the causal Markov kernels defined by a causal DAG.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Leveraging the universal function approximator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning is a highly effective universal function approximator. Let’s imagine
    there is a function that maps some set of inputs to some set of outputs, but we
    either don’t know the function or it’s too hard to write down in math or code.
    Given enough examples of those inputs and outputs, deep learning can approximate
    that function with high precision. Even if that function is nonlinear and high-dimensional,
    with enough data, deep learning will learn a good approximation.
  prefs: []
  type: TYPE_NORMAL
- en: We regularly work with functions in causal modeling and inference, and sometimes
    it makes sense to approximate them, so long as the approximations preserve the
    causal information we care about. For example, the causal Markov property makes
    us interested in functions that map values of a node’s parents in the causal DAG
    to values (or probability values) of that node.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll do this mapping between a node and its parents with the
    variational autoencoder (VAE) framework. We’ll train two deep neural nets in the
    VAE, one of which maps parent cause variables to a distribution of the outcome
    variable, and another that maps the outcome variable to a distribution of the
    cause variables. This example will showcase the use of deep learning when causality
    is nonlinear and high-dimensional; the effect variable will be an image represented
    as a high-dimensional array, and the cause variables will represent the contents
    of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Causal abstraction and plate models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: But what does it mean to build a causal model of an image? Images are comprised
    of pixels arranged in a grid. As data, we can represent that pixel grid as a matrix
    of numerical values corresponding to color. In the case of both MNIST and TMNIST,
    the image is a 28 × 28 matrix of grayscale values, as illustrated in figure 5.3\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F03_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 An MNIST image of “6” (left) and a TMNIST image of “7”. In their
    raw form, these are 28 × 28 matrices of numeric values corresponding to grayscale
    values.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A typical machine learning model looks at this 28 × 28 matrix of pixels as 28
    × 28 = 784 features. The machine learning algorithm learns statistical patterns
    connecting the pixels to one another and their labels. Based on this fact, one
    might be tempted to treat each individual pixel as a node in the naive causal
    DAG, as in figure 5.4, where for visual simplicity I’ve drawn 16 pixels (an arbitrary
    number) instead of all 784\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F04_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 What a naive causal DAG might look like for an image represented
    by a 4 × 4 matrix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In figure 5.4, there are edges from the *digit* and *is-handwritten* variables
    to each pixel. Further, there are examples of edges representing possible causal
    relationships *between* pixels. Causal edges between pixels imply the color of
    one pixel is a cause of another. Perhaps most of these relationships are between
    nodes that are close, with a few far-reaching edges. But how would we know if
    one pixel causes another? If two pixels are connected, how would we know the direction
    of causality?
  prefs: []
  type: TYPE_NORMAL
- en: Working at the right level of abstraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With these connections among only 16 pixels, the naive DAG in figure 5.4 is
    already quite unwieldy. It would be much worse with 784 pixels. Aside from the
    unwieldiness of a DAG, the problem with a pixel-level model is that our causal
    questions are generally not at the pixel level—we’d probably never ask “what is
    the causal effect of this pixel on that pixel?” In other words, the pixel is too
    low a level of abstraction, which is why thinking about causal relationships between
    individual pixels feels a bit absurd.
  prefs: []
  type: TYPE_NORMAL
- en: In applied statistics domains, such as econometrics, social science, public
    health, and business, our data has variables like per capita income, revenue,
    location, age, etc. These variables are typically already at the level of abstraction
    we want to think about when we get the data. But modern machine learning focuses
    on many perception problems from raw media, such as images, video, text, and sensor
    data. We don’t generally want to do causal reasoning at the low level of these
    features. Our causal questions are usually about the high-level abstractions behind
    these low-level features. We need to model at these higher abstraction levels.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of thinking about individual pixels, we’ll think about the entire image.
    We’ll define a variable *X* to represent how the image appears; i.e., *X* is a
    matrix random variable representing pixels. Figure 5.5 illustrates a causal DAG
    for the TMNIST case. Simply put, the identity of the digits (0–9) and the font
    (2,990 possible values) are the causes, and the image is the effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F05_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 A simple causal DAG that represents the implied DGP behind Typeface
    MNIST
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this case, we are using the causal DAG to make an assertion that the label
    causes the image. That is not always the case, as we’ll discuss in our case study
    on semi-supervised learning in section 5.3\. As with all causal models, it depends
    on the data generating process (DGP) within a domain.
  prefs: []
  type: TYPE_NORMAL
- en: Why say that the digit *causes* the image?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Plato’s allegory of the cave describes a group of people who have lived in a
    cave all their lives, without seeing the world. They face a blank cave wall and
    watch shadows projected on the wall from objects passing in front of a fire behind
    them. The shadows are simplified and sometimes distorted representations of the
    true objects passing in front of the fire. In this case, we can think of the form
    of the objects as being the cause of the shadow.
  prefs: []
  type: TYPE_NORMAL
- en: Analogously, the true form of the digit label causes the representation in the
    image. The MNIST images were written by people, and they have some *Platonic ideal*
    of the digit in their head that they want to render onto paper. In the process,
    that ideal is distorted by motor variation in the hand, the angle of the paper,
    the friction of the pen on the paper, and other factors—the rendered image is
    a “shadow” caused by that “ideal.”
  prefs: []
  type: TYPE_NORMAL
- en: This idea is related to a concept called “vision as inverse graphics” in computer
    vision (see [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for sources with more information). In causal terms, the takeaway is that when
    we are analyzing images rendered from raw signals from the environment, and the
    task is to infer the actual objects or events that resulted in those signals,
    causality flows from those objects or events to the signals. The inference task
    is to use the observed signals (shadows on the cave wall) to infer the nature
    of the causes (objects in front of the fire).
  prefs: []
  type: TYPE_NORMAL
- en: That said, images can be causes too. For example, if you were modeling how people
    behave *after* seeing an image in a mobile app (e.g., whether they “click”, “like”,
    or “swipe left”), you could model the image as a cause of the behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Plate modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modeling 2,990 fonts in our TMNIST data is overkill for our purposes here. Instead,
    I combined these datasets into one—half from MNIST and half from Typeface MNIST.
    Along with the “digit” label, I’m just going to have a simple binary label called
    “is-handwritten”, which is 1 (true) for images of handwritten digits from MNIST
    and 0 (false) for images of “typed” digits from TMNIST. We can modify our causal
    DAG to get figure 5.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F06_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 A causal DAG representing the combined MNIST and TMNIST data, where
    “is-handwritten” is 1 (MNIST images) or 0 (TMNIST images)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Plate modeling is a visualizing technique used in probabilistic machine learning
    that provides an excellent way to visualize the higher-level abstractions while
    preserving the lower-level dimensional detail. Plate notation is a method of visually
    representing variables that repeat in a DAG (e.g., *X*[1] to *X*[16] in figure
    5.4)—in our case, we have repetition of the pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of drawing each of the 784 pixels as an individual node, we use a rectangle
    or “plate” to group repeating variables into subgraphs. We then write a number
    on the plate to represent the number of repetitions of the entities on the plate.
    Plates can nest within one another to indicate repeated entities nested within
    repeated entities. Each plate gets a letter subscript indexing the elements on
    that plate. The causal DAG in figure 5.7 represents one image.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F07_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 A plate model representation of the causal DAG. Plates represent
    repeating variables, in this case 28 × 28 = 784 pixels. *X**[j]* is the *j*^(th)
    pixel.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: During training, we’ll have a large set of training images. Next, we’ll modify
    the DAG to capture all the images in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Training a neural causal model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train our neural causal model, we need to load and prepare the training data,
    create the architecture of our model, write a training procedure, and implement
    some tools for evaluating how well training is progressing. We’ll start by loading
    and preparing the data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Setting up the training data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our training data has *N* example images, so we need our plate model to represent
    all *N* images in the training data, half handwritten and half typed. We’ll add
    another plate corresponding to repeating *N* sets of images and labels, as in
    figure 5.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F08_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 The causal model with an additional plate for the *N* images in the
    data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now we have a causal DAG that illustrates both our desired level of causal abstraction
    as well as the dimensional information we need to start training the neural nets
    in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first load Pyro and some other libraries and set some hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your environment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This code was written using Python version 3.10.12 and tested in Google Colab.
    The versions of the main libraries include Pyro (pyro-ppl) version 1.8.4, torch
    version 2.2.1, torchvision version 0.18.0+cu121, and pandas version 2.0.3\. We’ll
    also use matplotlib for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for links to a notebook that will load in Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: If GPUs are available on your device, it will be faster to train the neural
    nets with CUDA (a platform for parallel computing on GPUs). We’ll run a bit of
    code that lets us toggle it on. If you don’t have GPUs or aren’t sure if you do,
    leave `USE_CUDA` set to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Setting up for GPU training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Use CUDA if it is available.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll make a subclass of the `Dataset` class (a class for loading and
    preprocessing data) that will let us combine the MNIST and TMNIST datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 Combining the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This class loads and processes a dataset that combines MNIST and Typeface
    MNIST. The output is a torch.utils.data.Dataset object.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Load, normalize, and reshape the images to 28 × 28 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Get and process the digit labels, 0–9.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 1 for handwritten digits (MNIST), and 0 for “typed” digits (TMNIST)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Return a tuple of the image, the digit label, and the is_handwritten label.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll use the `DataLoader` class (which allows for efficient data iteration
    and batching during training) to load the data from a CSV file in GitHub and split
    it into training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 Downloading, splitting, and loading the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set up the data loader that loads the data and splits it into training and
    test sets.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Allot 80% of the data to training data and the remaining 20% to test data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Create training and test loaders.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll set up the full variational autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Setting up the variational autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The variational autoencoder (VAE) is perhaps the simplest deep probabilistic
    machine learning modeling approach. In the typical setup for applying VAE to images,
    we introduce a latent continuous variable *Z* that has a smaller dimension than
    the image data. Here, *dimensionality* refers to the number of elements in a vector
    representation of the data. For instance, our image is a 28 × 28 matrix of pixels,
    or alternatively a vector with dimension 28 × 28 = 784\. By having a much smaller
    dimension than the image dimension, the latent variable *Z* represents a compressed
    encoding of the image information. For each image in the dataset, there is a corresponding
    latent *Z* value that represents an encoding of that image. This setup is illustrated
    in figure 5.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F09_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 The causal DAG plate model, extended to include an “encoding” variable
    *Z*. During training, the variable is latent, indicated by the dashed line. (After
    the model is deployed, *digit* and *is-handwritten*are also latent).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Z* appears as a new parent in the causal DAG, but it’s important to note that
    the classical VAE framework does not define *Z* as causal. Now that we are thinking
    causally, we’ll give *Z* a causal interpretation. Specifically, as parents of
    the image node in the DAG, we view *digit* and *is-handwritten* as causal drivers
    of what we see in the image. Yet there are other elements of the image (e.g.,
    the stroke thickness of a handwritten character, or the font of a typed character)
    that are also causes of what we see in the image. We’ll think of *Z* as a continuous
    latent *stand-in* for all of these other causes of the image that we are not explicitly
    modeling, like *digit* and *is-handwritten*. Examples of these causes include
    the nuance of the various fonts in the TMNIST labels and all of the variations
    in the handwritten digits due to different writers and motor movements as they
    wrote. With that in mind, we can view *P*(*X*| *digit*, *is-handwritten*, *Z*)
    as the causal Markov kernel of *X*. That said, it is important to remember that
    the representation we learn for *Z* is a stand-in for latent causes and is not
    the same as learning the actual latent causes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The VAE setup will train two deep neural networks: One called an “encoder”,
    which encodes an image into a value for *Z*. The other neural network, called
    the “decoder,” will align with our DAG. The decoder generates an image from the
    *digit* label, the *is-handwritten* label, and a *Z* value, as in figure 5.10.'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder acts like a rendering engine; given a *Z* encoding value and the
    values for *digit* and *is-handwritten*, it renders an image.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F10_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 The decoder neural network generates as output an image *X* from
    inputs *Z* and the labels *is-handwritten* and *digit*. As with any neural net,
    the inputs are processed through one or more “hidden layers.”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Key VAE concepts so far
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Variational autoencoder (VAE)*—A popular framework in deep generative modeling.
    We’re using it to model a causal Markov kernel in a causal model.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Decoder*—We use the decoder as the model of the causal Markov kernel. It maps
    the observed causes *is-handwritten* and *digit*, and the latent variable *Z*,
    to our image outcome variable *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: This VAE approach allows us to use a neural net, a la the decoder, to capture
    the complex and nonlinear relations needed to model the image as an effect caused
    by *digit* and *is-handwritten*. Modeling images would be difficult with the conditional
    probability tables and other simple parameterizations of causal Markov kernels
    we’ve discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s implement the decoder. We’ll pass in arguments `z_dim` for the
    dimension of *Z* and `hidden_dim` for the dimension (width) of the hidden layers.
    We’ll specify these variables when we instantiate the full VAE. The decoder combines
    the latent vector *Z* with additional inputs—the variable representing the *digit,*
    and *is-handwritten* (a binary indicator of whether the digit is handwritten).
    It will produce a 784-dimensional output vector representing an image of size
    28 × 28 pixels. This output vector contains the parameters for a Bernoulli distribution
    for each pixel, essentially modeling the likelihood of each pixel being “on.”
    The class uses two fully connected layers (`fc1` and `fc2`), and employs `Softplus`
    and `Sigmoid` “activation functions,” which are the hallmarks of how neural nets
    emulate neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 Implement the decoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A class for the decoder used in the VAE'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Image is 28 × 28 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Digit is one-hot encoded digits 0–9, i.e., a vector of length 10.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 An indicator for whether the digit is handwritten that has size 1'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Softplus and sigmoid are nonlinear transforms (activation functions) used
    in mapping between layers.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 fc1 is a linear function that maps the Z vector, the digit, and is_handwritten
    to a linear output, which is passed through a softplus activation function to
    create a hidden layer-a vector whose length is given by hidden_layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 fc2 linearly maps the hidden layer to an output passed to a sigmoid function.
    The resulting value is between 0 and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Define the forward computation from the latent Z variable value to a generated
    X variable value.'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Combine Z and the labels.'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Compute the hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Pass the hidden layer to a linear transform and then to a sigmoid transform
    to output a parameter vector of length 784\. Each element of the vector corresponds
    to a Bernoulli parameter value for an image pixel.'
  prefs: []
  type: TYPE_NORMAL
- en: We use the decoder in the causal model. Our causal DAG acts as the scaffold
    for a causal probabilistic machine learning model that, with the help of the decoder,
    defines a joint probability distribution on {*is-handwritten*, *digit*, *X*, *Z*},
    where *Z* is latent. We can use the model to calculate the likelihood of the training
    data for a given value of *Z*.
  prefs: []
  type: TYPE_NORMAL
- en: The latent variable `z`, the digit identity represented as a one-hot vector
    `digit`, and a binary indicator `is_handwritten` are modeled as samples from standard
    distributions. These variables are then fed into the decoder to produce parameters
    (`img_param`) for a Bernoulli distribution representing individual pixel probabilities
    of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Note, using the Bernoulli distribution to model the pixels is a bit of a hack.
    The pixels are not binary black and white outcomes—they have grayscale values.
    The line `dist.enable_validation(False)` lets us cheat by getting Bernoulli log
    likelihoods for the images given a decoder’s `img_param` output.
  prefs: []
  type: TYPE_NORMAL
- en: The following model code is a class method for a PyTorch neural network module.
    We’ll see the entire class later.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 The causal model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Disabling distribution validation lets Pyro calculate log likelihoods for
    pixels even though the pixels are not binary values.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The model of a single image. Within the method, we register the decoder,
    a PyTorch module, with Pyro. This lets Pyro know about the parameters inside of
    the decoder network.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 We model the joint probability of Z, digit, and is_handwritten, sampling
    each from canonical distributions. We sample Z from a multivariate normal with
    location parameter z_loc (all zeros) and scale parameter z_scale (all ones).'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 We also sample the digit from a one-hot categorical distribution. Equal
    probability is assigned to each digit.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 We similarly sample the is_handwritten variable from a Bernoulli distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 The decoder maps digit, is_handwritten, and Z to a probability parameter
    vector.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 The parameter vector is passed to the Bernoulli distribution, which models
    the pixel values in the data. The pixels are not technically Bernoulli binary
    variables, but we’ll relax this assumption.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding `model` method represents the DGP for one image. The `training_
    model` method in the following listing applies that `model` method to the *N*
    images in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Method for applying `model` to *N* images in data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The model represents the DGP for one image. The training_model applies that
    model to the N images in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Now we condition the model on the evidence in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 This context manager represents the N-size plate representing repeating
    IID examples in the data in figure 5.9\. In this case, N is the batch size. It
    works like a for loop, iterating over each data unit in the batch.'
  prefs: []
  type: TYPE_NORMAL
- en: Our probabilistic machine learning model models the joint distribution of {*Z*,
    *X*, *digit*, *is-handwritten*}. But since *Z* is latent, the model will need
    to learn *P*(*Z*|*X*, *digit*, *is-handwritten*). Given that we use the decoder
    neural net to go from *Z* and the labels to *X*, the distribution of *Z*, given
    *X* and the labels will be complex. We will use *variational inference*, a technique
    where we first define an approximating distribution *Q*(*Z*|*X*, *digit*, *is-handwritten*),
    and try to make that distribution as close to *P*(*Z*|*X*, *digit*, *is-handwritten*)
    as we can.
  prefs: []
  type: TYPE_NORMAL
- en: The main ingredient of the approximating distribution is the second neural net
    in the VAE framework, the encoder, illustrated in figure 5.11\. The encoder maps
    an observed image and its labels in the training data to a latent *Z* variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F11_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 The encoder maps actual images as input to the latent *Z* variable
    as output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The encoder does the work of compressing the information in the image into a
    lower-dimensional encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Key VAE concepts so far
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Variational autoencoder (VAE)*—A popular framework in deep generative modeling.
    We’re using it to model a causal Markov kernel in our causal model.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Decoder*—We use the decoder as the model of the causal Markov kernel. It maps
    observed causes *is-handwritten* and *digit*, and the latent variable *Z*, to
    our image outcome variable *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Encoder*—The encoder maps the image, *digit*, and *is-handwritten* indicator
    to the parameters of a distribution where we can draw samples of *Z*.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, the encoder takes as input an image, a digit label, and
    the *is-handwritten* indicator. These inputs are concatenated and passed through
    a series of fully connected layers with Softplus activation functions. The final
    output of the encoder consists of two vectors representing the location (`z_loc`)
    and scale (`z_scale`) parameters of the latent space distribution on *Z*, given
    observed values for *image* (`img`), *digit* (`digit`), and *is-handwritten* (`is_handwritten`).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 Implement the encoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The encoder is an instance of a PyTorch module.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The input image is 28 × 28 = 784 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The digit dimension is 10.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 In the encoder, we’ll only use the softplus transform (activation function).'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The linear transform fc1 combines with the softplus to map the 784-dimensional
    pixel vector, 10-dimensional digit label vector, and 2-dimensional is_handwritten
    vector to the hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 The linear transforms, fc21 and fc22, will combine with the softplus to
    map the hidden vector to Z’s vector space.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Define the reverse computation from an observed X variable value to a latent
    Z variable value.'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Combine the image vector, digit label, and is_handwritten label into one
    input.'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Map the input to the hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 The VAE framework will sample Z from a normal distribution that approximates
    P(Z|img, digit, is_handwritten). The final transforms map the hidden layer to
    a location and scale parameter for that normal distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the encoder produces the parameters of a distribution on *Z*.
    During training, given an image and its labels (*is-handwritten* and *digit*),
    we want to get a good value of *Z*, so we write a *guide function* that will use
    the encoder to sample values of *Z*.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 The guide function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 training_guide is a method of the VAE that will use the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Register the encoder so Pyro is aware of its weight parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 This is the same plate context manager for iterating over the batch data
    that we see in the training_model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Use the encoder to map an image and its labels to parameters of a normal
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Sample Z from that normal distribution'
  prefs: []
  type: TYPE_NORMAL
- en: We combine these elements into one PyTorch neural network module representing
    the VAE. We’ll initialize the latent dimension of *Z* to be 50\. We’ll set our
    hidden layer dimension to 400 in both the encoder and decoder. That means that
    given a dimension of 28 × 28 for the image, 1 for the binary *is-handwritten*,
    and 10 for the one-hot-encoded *digit* variable, we’ll take a 28 × 28 + 1 + 10
    = 795-dimensional feature vector and compress it down to a 400-dimensional hidden
    layer, and then compress that down to a 50-dimensional location and scale parameter
    for *Z*’s multivariate normal (Gaussian) distribution. The decoder takes as input
    the values of *digit*, *is-handwritten*, and *Z* and maps these to a 400-dimensional
    hidden layer and to the 28 × 28–dimensional image. These architectural choices
    of latent variable dimension, number of layers, activation functions, and hidden
    layer dimensions depend on the problem and are typically selected by convention
    or by experimenting with different values.
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll put these pieces together into the full VAE class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.9 Full VAE class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Set the latent dimension to 50.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Set the hidden layers to have a dimension of 400.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Set up the encoder and decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Add in the methods for model, training_model, and training_guide.'
  prefs: []
  type: TYPE_NORMAL
- en: Having specified the VAE, we can now move on to training.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 The training procedure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We know we have a good generative model when the encoder can encode an image
    into a latent value of *Z*, and then decode it into a *reconstructed* version
    of the image. We can minimize the *reconstruction error*—the difference between
    original and reconstructed images—in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: A bit of perspective on the “variational inference” training algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this section, you’ll see a bunch of jargon relating to variational inference,
    which is the algorithm we’ll use for training. It helps to zoom out and examine
    why we’re using this algorithm. There are many statistical estimators and algorithms
    both for fitting neural net weights and other parameters and for causal inference.
    One of these is variational inference.
  prefs: []
  type: TYPE_NORMAL
- en: To be clear, variational inference is not a “causal” idea. It is just another
    probabilistic inference algorithm. In this book, I favor this inference algorithm
    more than others because it scales well even when variables in the DAG are latent
    in the training data, and it works with deep neural nets and leverages deep learning
    frameworks like PyTorch. This opens the door to reasoning causally about richer
    modalities such as text, images, video, etc., whereas traditional causal inference
    estimators were developed for numerical data. Further, we can tailor the method
    to different problems (see the discussion of “commodification of inference” in
    chapter 1) and leverage domain knowledge during inference (such as by using knowledge
    of conditional independence in the guide). Finally, the core concepts of variational
    inference show up across many deep generative modeling approaches (such as latent
    diffusion models).
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, solely minimizing reconstruction error leads to overfitting and
    other issues, so we’ll opt for a probabilistic approach: given an image, we’ll
    use our guide function to sample a value of *Z* from *P*(*Z*|*image*, *is-handwritten*,
    *digi**t*). Then we’ll plug that value into our model’s decoder, and the output
    parameterizes *P*(*image*|*is-handwritten*, *digit*, *Z*). Our probabilistic approach
    to minimizing reconstruction error optimizes the encoder and decoder such that
    we’ll maximize the likelihood of *Z* with respect to *P*(*Z*|*image*, *is-handwritten*,
    *digit*) and the likelihood of the original image with respect to *P*(*image*|*is-handwritten*,
    *digit*, *Z*).'
  prefs: []
  type: TYPE_NORMAL
- en: But typically we can’t directly sample from or get likelihoods from the distribution
    *P*(*Z*|*image*, *is-handwritten*, *digit*). So, instead, our guide function attempts
    to approximate it. The guide represents a *variational distribution*, denoted
    *Q*(*Z*|*X*, *is-handwritten*, *digit*). A change in the weights of the encoder
    represents a shifting of the variational distribution. Training will optimize
    the weights of the encoder such that the variational distribution shifts toward
    *P*(*Z*|*image*, *is-handwritten*, *digit*). That training approach is called
    *variational inference*, and it works by minimizing the *Kullback–Leibler divergence*
    (KL divergence) between the two distributions; KL divergence is a way of quantifying
    how two distributions differ.
  prefs: []
  type: TYPE_NORMAL
- en: Our variational inference procedure optimizes a quantity called *ELBO*, which
    means *expected lower bound on the log-likelihood of the data*. Minimizing negative
    ELBO loss indirectly minimizes reconstruction error and KL divergence between
    *Q*(*Z*|…) and *P*(*Z*|…). Pyro implements ELBO in a utility called `Trace_ELBO`.
  prefs: []
  type: TYPE_NORMAL
- en: Our procedure will use *stochastic* variational inference (SVI), which simply
    means doing variational inference with a training procedure that works with randomly
    selected subsets of the data, or “batches”, rather than the full dataset, which
    reduces memory use and helps scale to larger data.
  prefs: []
  type: TYPE_NORMAL
- en: Key VAE concepts so far
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Variational autoencoder (VAE)*—A popular framework in deep generative modeling.
    We’re using it to model a causal Markov kernel in our causal model.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Decoder*—We use the decoder as the model of the causal Markov kernel. It maps
    the observed causes *is-handwritten* and *digit*, and the latent variable *Z*,
    to our image outcome variable *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Encoder*—The encoder maps the *image*, *digit*, and *is-handwritten* to the
    parameters of a distribution where we can draw samples of *Z*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Guide function*—During training, we want values of *Z* that represent an image,
    given *is-handwritten* and *digit*; i.e., we want to generate *Z*s from *P*(*Z*|*image*,
    *is-handwritten*, *digit*). But we can’t sample from this distribution directly.
    So we write a *guide function* that uses the encoder and convenient canonical
    distributions like the multivariate normal to sample values of *Z*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Variational distribution*—The guide function represents a distribution called
    *the variational distribution*, denoted *Q*(*Z*|*image*, *is-handwritten*, *digit*).
    During inference, we want to sample from *Q*(*Z*|…) in a way that is representative
    of *P*(*Z*|*image*, *is-handwritten*, *digit*).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Variational inference*—This is the training procedure that seeks to maximize
    the closeness between *Q*(*Z*|…) and *P*(*Z*|…) so sampling from *Q*(*Z*|…) produces
    samples representative of *P*(*Z*|…) (e.g., by minimizing KL divergence).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stochastic variational inference (SVI)*—Variational inference where training
    relies on randomly selected subsets of the data, rather than on the full data,
    in order to make training faster and more scalable.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we get started, we’ll make a helper function for plotting images so we
    can see how we are doing during training.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 Helper function for plotting images
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Helper function for plotting an image'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll create a `reconstruct_img` helper function that will *reconstruct*
    an image, given its labels, where “reconstruct” means encoding the image into
    a latent representation and then decoding the latent representation back into
    an image. We can then compare the original image and its reconstruction to see
    how well the encoder and decoder have been trained. We’ll create a `compare_images`
    function to do that comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 Define a helper function for reconstructing and viewing the images
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Given an input image, this function reconstructs the image by passing it
    through the encoder and then through the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Plots the two images side by side for comparison'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll create some helper functions for handling the data. We’ll use `get_random_example`
    to grab random images from the dataset. The `reshape_data` function will convert
    an image and its labels into input for the encoder. And we’ll use `generate_data`
    and `generate_coded_data` to simulate an image from the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 Data processing helper functions for training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Choose a random example from the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Reshape the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Generate data that is encoded.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Generate (unencoded) data.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can run the training procedure. First, we’ll set up stochastic variational
    inference. We’ll first set up an instance of the Adam optimizer, which will handle
    optimization of the parameters in `training_guide`. Then we’ll pass `training_model`,
    `training_guide`, the optimizer, and the ELBO loss function to the SVI constructor
    to get an SVI instance.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.13 Set up the training procedure
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Clear any values of the parameters in the guide memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Initialize the VAE.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Load the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Initialize the optimizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Initialize the SVI loss calculator. Loss negative “expected lower bound”
    (ELBO).'
  prefs: []
  type: TYPE_NORMAL
- en: When training generative models, it is useful to set up a procedure that uses
    test data to evaluate how well training is progressing. You can include anything
    you think is useful to monitor during training. Here, I calculate and print the
    loss function on the test data, just to make sure the test loss is progressively
    decreasing along with training loss (a flattening of test loss while training
    loss continues to decrease would indicate overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: A more direct way of determining how well our model is training is to generate
    and view images. In my test evaluation procedure, I produce two visualizations.
    First, I inspect how well it can reconstruct a random image from the test data.
    I pass the image through the encoder and then through the decoder, creating a
    “reconstruction” of the image. Then I plot the original and reconstructed images
    side by side and compare them visually, looking to see that they are close to
    identical.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I visualize how well it is performing as an overall generative model by
    generating and plotting an image from scratch. I run this code once each time
    a certain number of epochs are run.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.14 Setting up a test evaluation procedure
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calculate and print test loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Compare a random test image to its reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Generate a random image from the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ll run the training. For a single epoch, we’ll iteratively get a batch
    of data from the training data loader and pass it to the step method and run a
    training step. After a certain number of epochs (a number set by `TEST_FREQUENCY`),
    we’ll use our helper functions to compare a random image to its reconstruction,
    as well as simulate an image from scratch and plot it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.15 Running training and plotting progress
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Run the training procedure for a certain number of epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Run a training step on one batch in one epoch.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The test data evaluation procedure runs every 10 epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, see [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for a link to a Jupyter notebook with the full VAE, encoder/decoder, and training
    code, including a link for running it in Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Evaluating training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At certain points during training, we randomly choose an image and “reconstruct”
    it by passing the image through the encoder to get a latent value of *Z*, and
    passing that value back through the decoder. In one run, the first image I see
    is a non-handwritten number 6\. Figure 5.12 shows this image and its reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: During training, we also simulate random images from the generative model and
    plot it. Figure 5.13 shows the first simulated image in one run—in this case,
    the number 3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F12_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 The first attempt to reconstruct an image during training shows
    the model has learned something but still has much progress to make.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F13_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 The first instance of an image generated from the generative model
    during training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: But the model learns quickly. By 130 epochs, we get the results in figure 5.14.
  prefs: []
  type: TYPE_NORMAL
- en: After training is complete, we can see a visualization of loss over training
    (negative ELBO) in figure 5.15.
  prefs: []
  type: TYPE_NORMAL
- en: The code will train the parameters of the encoder that maps images and the labels
    to the latent variable. It will also train the decoder that maps the latent variable
    and the labels to the image. That latent variable is a fundamental feature of
    the VAE, but we should take a closer look at how to interpret the latent variable
    in causal terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F14_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 Reconstructed and randomly generated images from the model after
    130 epochs of training look much better.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F15_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 Test loss as training progresses. The *x*-axis is the epoch.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.2.5 How should we causally interpret Z?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I said we can view *Z* as a “stand-in” for all the independent latent causes
    of the object in the image. *Z* is a representation we learn from the pixels in
    the images. It is tempting to treat that representation like a higher-level causal
    abstraction of those latent causes, but it is probably not doing a great job as
    a causal abstraction. The autoencoder paradigm trains an encoder that can take
    an image and embed it into a low-dimensional representation *Z*. It tries to do
    so in a way that enables it to reconstruct the original image as well as possible.
    In order to reconstruct the image with little loss, the framework tries to encode
    as much information from the original image as it can in that lower dimensional
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: A good *causal* representation, however, shouldn’t try to capture as much information
    as possible. Rather, it should strive to capture only the *causal* information
    in the images and ignore everything else. Indeed, the task of “disentangling”
    the causal and non-causal factors in *Z* is generally impossible when *Z* is unsupervised
    (meaning we lack labels for *Z*). However, domain knowledge, interventions, and
    semi-supervision can help. See [https://www.altdeep.ai/p/causalaibook](https://www.altdeep.ai/p/causalaibook)
    for references on *causal representation learning* and *disentanglement of causal
    factors*. As we progress through the book, we’ll develop intuition for what the
    “causal information” in such a representation should look like.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.6 Advantages of this causal interpretation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is nothing inherently causal about our VAE’s setup and training procedure;
    it is typical of a vanilla supervised VAE you’d see in many machine learning settings.
    The only causal element of our approach was our interpretation. We say that the
    *digit* and *is-handwritten* are causes, and *Z* is a stand-in for latent causes,
    and the image is the outcome. Applying the causal Markov property, our causal
    model factorizes the joint distribution into *P*(*Z*), *P*(*is-handwritten*),
    *P*(*digit*), and *P*(*image*|*Z*, *is-handwritten*, *digit*), where the latter
    factor is the causal Markov kernel of the image.
  prefs: []
  type: TYPE_NORMAL
- en: What can we do with this causal interpretation? First, we can use it to improve
    deep learning and general machine learning workflows and tasks. We’ll see an example
    of this with *semi-supervised learning* in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating generative AI in causal models is not limited to VAEs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: I demonstrated how to use a VAE framework to fit a causal Markov kernel entailed
    by a causal DAG, but a VAE was just one approach to achieving this end. We could
    have used another deep probabilistic machine learning framework, such as a generative
    adversarial network (GAN) or a diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we incorporated deep learning into a causal graphical model.
    Next, we investigate how to use causal ideas to enhance deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Using causal inference to enhance deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use causal insights to improve how we set up and train deep learning
    models. These insights tend to lead to benefits such as improved sample efficiency
    (i.e., doing more with less data), the ability to do transfer learning (using
    what a model learned in solving one task to improve performance on another), data
    fusion (combining different datasets), and enabling more robust predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Much of the work of deep learning is trial and error. For example, when training
    a VAE or other deep learning models, you typically experiment with different approaches
    (VAE vs. another framework), architectural choices (latent variable and hidden
    layer dimension, activation functions, number of layers, etc.), and training approaches
    (choice of loss function, learning rate, optimizer, etc.) before you get a good
    result. These experiments cost time, effort, and resources. In some cases, causal
    modeling can help you make better choices about what might work and what is unlikely
    to work, leading to cost savings. In this section, we’ll look at a particular
    example of this case in the context of semi-supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Independence of mechanism as an inductive bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we had a DAG with two variables: “cause” *C* and “outcome” *O*. The
    DAG is simply *C* → *O*. Our causal Markov kernels are *P*(*C*) and *P*(*O*|*C*).
    Recall the idea of *independence of mechanism* from chapter 3—the causal Markov
    kernel *P*(*O*|*C*) represents a mechanism of how the cause *C* drives the outcome
    *O*. That mechanism is distinct from other mechanisms in the system, such that
    changes to those mechanisms have no effect on *P*(*O*|*C*). Thus, knowing about
    *P*(*O*|*C*) tells you nothing about the distribution of the cause *P*(*C*) and
    vice versa. However, knowing something about the distribution of the outcome *P*(*O*)
    might tell you something about the distribution of the cause given the outcome
    *P*(*C*|*O*), and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, consider a scenario where *C* represents sunscreen usage and
    *O* indicates whether someone has sunburn. You understand the *mechanism* by which
    sunscreen protects against sunburn (UV rays, SPF levels, regular application,
    the perils of sweat and swimming, etc.), and by extension, the chances of getting
    sunburn given how one uses sunscreen, captured by *P*(*O*|*C*). However, this
    understanding of the mechanism doesn’t provide any information about how *common*
    sunscreen use is, denoted by *P*(*C*).
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose you’re trying to guess whether a sunburned person used sunscreen,
    i.e., you’re mentally modeling *P*(*C*|*O*). In this case, knowing the prevalence
    of sunburns, *P*(*O*), could help. Consider whether the sunburned individual was
    a case of someone who did use sunscreen but got a sunburn anyway. That case would
    be more likely if sunburns were a common problem than if sunburns were rare—if
    sunburns are common, sunscreen use is probably common, but if sunburns were uncommon,
    people would be less cautious about prevention.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, suppose *C* represents study effort and *O* represents test scores.
    You know the causal mechanism behind how studying more causes higher test scores,
    captured by *P*(*O*|*C*). But this doesn’t tell you how common it is for students
    to study hard, captured by *P*(*C*). Suppose a student got a low test score, and
    you are trying to infer whether they studied hard—you are mentally modeling *P*(*C*|*O*).
    Again, knowing the typical distribution of test scores *P*(*O*) can help. If low
    scores are rare, students might be complacent, and thus more likely not to study
    hard. You can use that insight as an *inductive bias*—a way to constrain your
    mental model of *P*(*C*|*O*).
  prefs: []
  type: TYPE_NORMAL
- en: Causal inductive bias
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: “Inductive bias” refers to the assumptions (explicit or implicit) that lead
    an inference algorithm to prefer certain inferences or predictions over others.
    Examples of inductive bias include Occam’s Razor and the assumption in forecasting
    that trends in the past will continue into the future.
  prefs: []
  type: TYPE_NORMAL
- en: Modern deep learning relies on using neural network architectures and training
    objectives to encode inductive bias. For example, “convolutions” and “max pooling”
    are architectural elements in convolutional neural networks for computer vision
    that encode an inductive bias called “translation invariance”; i.e., a kitten
    is still a kitten regardless of whether it appears on the left or right of an
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Causal models provide inductive biases in the form of causal assumptions about
    the DGP (such as a causal DAG). Deep learning can leverage these causal inductive
    biases to attain better results just as it does with other types of inductive
    biases. For example, independence of mechanism suggests that knowing *P*(*O*)
    could provide a useful inductive bias in learning *P*(*C*|*O*).
  prefs: []
  type: TYPE_NORMAL
- en: Now consider two variables *X* and *Y* (which can be vectors) with joint distribution
    *P*(*X*, *Y*). We want to design an algorithm that solves a task by learning from
    data observed from *P*(*X*, *Y*). The chain rule of probability tells us that
    *P*(*X*=*x*, *Y*=*y*) = *P*(*X*=*x*|*Y*=*y*)*P*(*Y*=*y*) = *P*(*Y*=*y*|*X*=*x*)*P*(*X*=*x*).
    So, from that basic probabilistic perspective, modeling the set {*P*(*X*|*Y*),
    *P*(*Y*)} is equivalent to modeling the set {*P*(*Y*|*X*), *P*(*X*)}. But consider
    the cases where either *X* is a cause of *Y* or where *Y* is a cause of *X*. Under
    these circumstances, the independence of mechanism gives us an asymmetry between
    sets {*P*(*X*|*Y*), *P*(*Y*)} and {*P*(*Y*|*X*), *P*(*X*)} (specifically, {*P*(*Y*|*X*),
    *P*(*X*)} represents the independent mechanism behind *X*’s causal influence on
    *Y*, and {*P*(*X*|*Y*), *P*(*Y*)} does not) that we can possibly leverage as an
    inductive bias in these algorithms. Semi-supervised learning is a good example.
  prefs: []
  type: TYPE_NORMAL
- en: '5.3.2 Case study: Semi-supervised learning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Returning to our TMNIST-MNIST VAE-based causal model, suppose we had, in addition
    to our original data, a large set of images of digits that were unlabeled (i.e.,
    *digit* and *is-handwritten* are not observed). Our causal interpretation of our
    model suggests we can leverage this data during training using semi-supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Independence of mechanism can help you determine when semi-supervised learning
    will be effective. In *supervised learning*, the training data consists of *N*
    samples of *X*, *Y* pairs; (*x*[1], *y*[1]), (*x*[2], *y*[2]), …, (*x*[*N*], *y*[*N*]).
    *X* is the *feature data* used to predict the *labels* *Y*. The data is “supervised”
    because every *x* is paired with a *y*. We can use these pairs to learn *P*(*Y*|*X*).
    In *unsupervised learning*, the data *X* is unsupervised, meaning we have no labels,
    no observed value of *Y*. Our data looks like (*x*[1]), (*x*[2]), …, (*x*[*N*]).
    With this data alone, we can’t directly learn anything about *P*(*Y*|*X*); we
    can only learn about *P*(*X*). Semi-supervised learning asks the question, suppose
    we had a combination of supervised and unsupervised data. Could these two sets
    of data be combined in a way such that our ability to predict *Y* was better than
    if we only used the supervised data? In other words, can learning more about *P*(*X*)
    from the unsupervised data somehow augment our learning of *P*(*Y*|*X*) from the
    supervised data?
  prefs: []
  type: TYPE_NORMAL
- en: The semi-supervised question is quite practical. It is common to have abundant
    unsupervised examples if labeling those examples is costly. For example, suppose
    you worked at a social media site and were tasked with building an algorithm that
    classified whether an uploaded image depicted gratuitous violence. The first step
    is to create supervised data by having humans manually label images as gratuitously
    violent or not. Not only does this cost many people-hours, but it is mentally
    stressful for the labelers. A successful semi-supervised approach would mean you
    could minimize the amount of labeling work you need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Our task is to learn a representation of *P*(*X* ,*Y*) and use it to predict
    from *P*(*Y*|*X*). For semi-supervised learning to work, the unlabeled values
    of *X* must update the representation of *P*(*X* , *Y*) in a way that provides
    information about *P*(*Y*|*X*). However, independence of mechanism means the task
    of learning *P*(*X* ,*Y*) decomposes into learning distinct representations of
    the causal Markov kernels, where the parameter vector of each representation is
    orthogonal to the others. That parameter modularity (see section 3.2) can block
    flow of parameter updating information from the unlabeled observations of *X*
    to the learned representation of *P*(*Y*|*X*). To illustrate, let's consider two
    possibilities, one where *Y* is a cause of *X*, and one where *X* is a cause of
    *Y*. If *Y* is a cause of *X*, such as in our MNIST-TMNIST example (*Y* is the
    is-handwritten and digit variables, and *X* is the image), then our learning task
    decomposes into learning distinct representations of *P*(*X*|*Y*) and P(Y). Unlabeled
    observations of *X* can give us a better representation of *P*(*X*), we can use
    to flip *P*(*X*|*Y*) into *P*(*Y*|*X*) by way of Bayes rule. However, when *X*
    is a cause of *Y*, our learning task decomposes into learning distinct representations
    of *P*(*X*) and *P*(*Y*|*X*). That parameter modularity means those unlabeled
    values of *X* will help us update *P*(*X*)’s representation but not that of *P*(*Y*|*X*).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F16_Ness.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 In causal learning, the features cause the label. In anti-causal
    learning, the label causes the features.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The case where the feature causes the label is sometimes called *causal learning*
    because the direction of the prediction is from the cause to the effect. *Anti-causal
    learning* refers to the case when the label causes the feature. The two cases
    are illustrated in figure 5.16.
  prefs: []
  type: TYPE_NORMAL
- en: Independence of mechanism suggests semi-supervised learning can achieve performance
    gains (relative to a baseline of supervised learning on only the labeled data)
    only in the anti-causal case. See the chapter notes at www.altdeep.ai/causalAIbook
    for a more detailed explanation and references. But intuitively, we can see that
    this mirrors the the sunscreen and sunburn example—knowing the prevalence of sunburns
    *P*(*O*) helped in learning how to guess sunscreen use when you know if someone
    has a sunburn *P*(*C*|*O*). In this same anti-causal learning case, having only
    observations from *P*(*X*) can still be helpful in learning a good model of *P*(*Y*|*X*).
    But in the causal learning case, it would be a waste of effort and resources.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the causal structure between *X* and *Y* could be more nuanced
    and complicated than these simple *X*→*Y* and *X*←*Y* cases. For example, there
    could be unobserved common causes of *X* and *Y*. The takeaway here is that when
    you know something about the causal relationships between the variables in your
    machine learning problem, you can leverage that knowledge to model more effectively,
    even if the task is not a causal inference task (e.g., simply predicting *Y* given
    *X*). This could help you avoid spending time and resources on an approach that
    is not likely to work, as in the semi-supervised case. Or it could enable more
    efficient, robust, or better performing inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Demystifying deep learning with causality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our semi-supervised learning example highlights how a causal perspective can
    explain when we’d expect semi-supervised learning to work and when to fail. In
    other words, it somewhat *demystifies* semi-supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: That mystery around the effectiveness of deep learning methods led AI researcher
    Ali Rahimi to compare modern machine learning to alchemy.
  prefs: []
  type: TYPE_NORMAL
- en: Alchemy worked. Alchemists invented metallurgy, ways to dye textiles, modern
    glass-making processes, and medications. Then again, alchemists also believed
    they could cure diseases with leeches and transmute base metals into gold.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, alchemy works, but alchemists lacked an understanding of the
    underlying scientific principles that made it work when it did. That *mystery*
    made it hard to know when it would fail. As a result, alchemists wasted considerable
    effort on dead ends (philosopher’s stones, immortality elixirs, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Chapter checkpoint
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Incorporating deep learning into a causal model:*'
  prefs: []
  type: TYPE_NORMAL
- en: ✓ A causal model of a computer vision problem
  prefs: []
  type: TYPE_NORMAL
- en: ✓ Training the deep causal image model
  prefs: []
  type: TYPE_NORMAL
- en: '*Using causal reasoning to enhance machine learning:*'
  prefs: []
  type: TYPE_NORMAL
- en: ✓ Case study on independence of mechanism and semi-supervised learning
  prefs: []
  type: TYPE_NORMAL
- en: 👉 Demystifying deep learning with causality
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, deep learning “works” in that it achieves good performance on a wide
    variety of prediction and inference tasks. But we often have an incomplete understanding
    of why and when it works. That *mystery* has led to problems with reproducibility,
    robustness, and safety. It also leads to irresponsible applications of AI, such
    as published work that attempts to predict behavior (e.g., criminality) from profile
    photos. Such efforts are the machine learning analog of the alchemical immortality
    elixirs that contained toxins like mercury; they don’t work *and* they cause harm.
  prefs: []
  type: TYPE_NORMAL
- en: We often hear about the “superhuman” performance of deep learning. Speaking
    of superhuman ability, imagine an alternative telling of Superman’s origin story.
    Imagine if, when Superman made his first public appearance, his superhuman abilities
    were unreliable? Suppose he demonstrated astounding superhuman feats like flight,
    super strength, and laser vision, but sometimes his flight ability failed and
    his super strength faltered. Sometimes his laser vision was dangerously unfocused,
    resulting in terrible collateral damage. The public would be impressed and hopeful
    that he could do some good, but unsure if it would be safe to rely on him when
    the stakes were high.
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine that his adoptive Midwestern parents, experts in causal inference,
    used causal analysis to model the *how* and *why* of his powers. Having demystified
    the mechanisms underlying his superpowers, they were able to engineer a pill that
    stabilized those powers. The pill wouldn’t so much give Superman new powers; it
    would just make his existing powers more reliable. The work of developing that
    pill would get fewer headlines than flight and laser vision, but it would be the
    difference between merely having superpowers and being Superman.
  prefs: []
  type: TYPE_NORMAL
- en: This analogy helps us understand the impact of using causal methods to demystify
    deep learning and other machine learning methods. Less mystery leads to more robust
    methods and helps us avoid wasteful or harmful applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning can be used to enhance causal modeling and inference. Causal reasoning
    can enhance the setup, training, and performance of deep learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal models can leverage the ability of deep learning to scale and work with
    high-dimensional nonlinear relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use generative AI frameworks like the variational autoencoder to build
    a causal generative model on a DAG just as we did with pgmpy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder maps the outcomes of direct parents (the labels of an image) to
    the outcomes of the child (the image).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, the decoder gives us a nonlinear high-dimensional representation
    of the causal Markov kernel for the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder maps the image variable and the causes (labels) back to the latent
    variable *Z*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can view the learned representation of the latent variable as a stand-in
    for unmodeled causes, but it still lacks the qualities we’d expect from an ideal
    causal representation. Learning latent causal representations is an active area
    of research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causality often enhances deep learning and other machine learning methods by
    helping elucidate the underlying principles that make it work. For example, causal
    analysis shows semi-supervised learning should work in the case of *anti-causal
    learning* (when the features are *caused by* the label) but not in the case of
    *causal learning* (when the features cause the label).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such causal insights can help the modeler avoid spending time, compute, person-hours,
    and other resources on a given algorithm when it is not likely to work in a given
    problem setting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal insights can demystify elements of building and training deep learning
    models, such that they become more robust, efficient, and safe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
