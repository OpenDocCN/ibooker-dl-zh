- en: 12 Fine-tuning LLMs with business domain knowledge
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 使用商业领域知识微调LLMs
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The fine-tuning process for an LLM
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的微调过程
- en: Preparing a data set to use for fine-tuning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备用于微调的数据集
- en: Using fine-tuning tools to better understand the process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用微调工具更好地理解过程
- en: Although the effects of large language models (LLMs) in various industries have
    been covered extensively in the mainstream media, the ubiquity and popularity
    of LLMs have contributed to a quiet revolution in the AI open source community.
    Through the spirit of open collaboration and the support of big technology companies,
    the ability to fine-tune AI models has become increasingly more accessible to
    AI enthusiasts. This opportunity has resulted in a vibrant community that is experimenting
    and sharing a wide range of processes and tools that can be used to better understand
    how fine-tuning works and how we can tune models ourselves or in teams.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在各个行业中的影响在主流媒体中得到了广泛的报道，但LLMs的普遍性和受欢迎程度已经为AI开源社区带来了一场静悄悄的革命。通过开放协作的精神和大型科技公司的支持，微调AI模型的能力越来越容易为AI爱好者所获得。这个机会产生了一个充满活力的社区，该社区正在尝试和分享各种过程和工具，这些过程和工具可以更好地理解微调是如何工作的，以及我们如何自己或团队合作来调整模型。
- en: The topic of fine-tuning is vast, and getting into each important detail would
    require an entire book of its own. However, by taking advantage of the models,
    data sets, platforms, and tools created by and for the open source community,
    we can establish an appreciation for the fine-tuning process. These open source
    resources can prepare us for a future in which we might find ourselves fine-tuning
    our models within our organizations to help build context-based LLMs. However,
    fine-tuning is as much about the approach we take as it is about the tools we
    use. So, in this chapter, we’ll go through each important part of the fine-tuning
    process and learn how to fine-tune our model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的主题非常广泛，深入探讨每个重要细节可能需要一本整本书。然而，通过利用开源社区创建和为开源社区创建的模型、数据集、平台和工具，我们可以对微调过程有一个欣赏。这些开源资源可以为我们准备一个未来，在那个未来中，我们可能会在我们的组织中微调我们的模型，以帮助构建基于上下文的LLMs。然而，微调不仅关乎我们采取的方法，也关乎我们使用的工具。因此，在本章中，我们将详细了解微调过程的每个重要部分，并学习如何微调我们的模型。
- en: 12.1 Exploring the fine-tuning process
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 探索微调过程
- en: Before we learn more about fine-tuning tools, we should first discuss what fine-tuning
    entails and reflect on what we hope to achieve. As we’ll see, fine-tuning involves
    a series of steps that play an important role in the wider process. Knowing what
    we hope to achieve helps us not only with the evaluation of a model once it’s
    been tuned, but also guides us in our tuning approach. Each step of the fine-tuning
    process includes distinct activities and challenges. And although we might not
    be able to cover all the details, we’ll learn enough to understand what happens
    as we tune models and the challenges that we might face.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习更多关于微调工具之前，我们首先应该讨论微调包含的内容，并反思我们希望实现的目标。正如我们将看到的，微调涉及一系列步骤，这些步骤在更广泛的过程中发挥着重要作用。了解我们希望实现的目标不仅有助于我们评估微调后的模型，而且还能指导我们的微调方法。微调过程的每一步都包括不同的活动和挑战。尽管我们可能无法涵盖所有细节，但我们将学习到足够多的知识，以了解我们在微调模型时会发生什么，以及我们可能面临的挑战。
- en: 12.1.1 A map of the fine-tuning process
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 微调过程的地图
- en: As we discovered in chapter 10, fine-tuning is the process of taking an existing
    model that has already gone through some sort of training, known as a foundational
    model, and training it further with additional data sets to
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第10章中发现的，微调是将已经经过某种训练的现有模型（称为基础模型）进一步使用额外的数据集进行训练的过程
- en: Make a model more attuned to contextual information
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使模型更能适应上下文信息
- en: Change the tone of a model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变模型的声音
- en: Help it respond to specific queries or instructions
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助它响应特定的查询或指令
- en: Just as a foundational model goes through a series of steps to be trained, there
    is a series of steps to go through during a fine-tuning session. Figure 12.1 summarizes
    these steps and how they feed into one another.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就像基础模型需要经过一系列步骤进行训练一样，在微调会话期间也有一系列步骤需要遵循。图12.1总结了这些步骤以及它们是如何相互关联的。
- en: '![](../../OEBPS/Images/CH12_F01_Winteringham2.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH12_F01_Winteringham2.png)'
- en: Figure 12.1 A visual representation of the different steps taken during fine-tuning
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 精调过程中所采取的不同步骤的视觉表示
- en: Figure 12.1 doesn’t necessarily cover every nuance of fine-tuning, but it captures
    the core steps we would typically expect to go through to successfully create
    a tuned model. Throughout this chapter, we’ll examine each step in more detail,
    but let’s first reflect on what might be the most important part of the process,
    identifying what we hope to achieve from fine-tuning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1不一定涵盖微调的每一个细微之处，但它捕捉了我们通常期望成功创建一个微调模型的核心步骤。在本章中，我们将更详细地检查每个步骤，但首先让我们反思一下这个过程可能最重要的部分，即确定我们希望通过微调实现的目标。
- en: 12.1.2 Goal setting
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.2 目标设定
- en: One of the biggest mistakes that we can make when tuning LLMs is not having
    a clear idea of what we want our tuned LLM to do. Failing to set out clear goals
    for what problems a tuned model is supposed to help with can affect every part
    of the fine-tuning process, from data preparation to testing. Given the indeterministic
    nature of LLMs, this doesn’t mean setting goals around specific information we
    expect to get from an LLM. But we do have to ask ourselves what type of behavior
    we want and how it fits into our wider context.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调LLM时，我们可能犯的最大错误之一就是没有明确的想法，不知道我们希望微调后的LLM做什么。未能为微调模型应该帮助解决的问题设定明确的目标可能会影响微调过程的各个方面，从数据准备到测试。鉴于LLM的不确定性，这并不意味着围绕我们期望从LLM中获得的具体信息来设定目标。但我们必须问自己我们想要什么类型的行为，以及它如何融入更广泛的背景。
- en: 'To illustrate this, let’s consider two different goals. One is to create a
    code-completion tool that has been fine-tuned on our code base to use an LLM’s
    generative capabilities, without revealing intellectual property to a third party.
    Another is a Q&A/chat-based LLM that offers support to users, which has been tuned
    on support documentation and customer data to help answer questions. Depending
    on which goal we want to pursue, we would need to consider details such as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们考虑两个不同的目标。一个是创建一个代码补全工具，该工具已经在我们的代码库上微调，以使用LLM的生成能力，同时不向第三方泄露知识产权。另一个是一个基于问答/聊天的LLM，它为用户提供支持，该LLM已经在支持文档和客户数据上微调，以帮助回答问题。根据我们想要追求的目标，我们需要考虑以下细节：
- en: '*What datasets to use?* For our code completion scenario, we likely want to
    create a data set that consists of code broken down into logical sections to tune
    our model with. We may also be interested in using other data sets that include
    open source code. This would differ from the Q&A chat scenario in which we would
    create a corpus of data that includes help guides and documentation.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用什么数据集？* 对于我们的代码补全场景，我们可能希望创建一个由逻辑部分组成的代码数据集，以微调我们的模型。我们也可能对使用包含开源代码的其他数据集感兴趣。这与问答聊天场景不同，在问答聊天场景中，我们会创建一个包括帮助指南和文档的数据集。'
- en: '*What model to use?* At the time of writing, Hugging Face, an AI open source
    community site, is currently hosting 500,000+ different models, all of which are
    designed to serve different purposes such as generation, classification, and translation.
    When considering our fine-tuning goals, we will need to select models that suit
    our needs. In our code scenario, we would likely pick a model that has already
    been trained on a large corpus of code data, making it easier to further tune
    with our code base. In our Q&A/chat LLM scenario, we would likely want a model
    that has already been trained to act as a capable chat-based LLM.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用什么模型？* 在撰写本文时，Hugging Face，一个AI开源社区网站，目前托管了500,000+个不同的模型，这些模型都旨在服务于不同的目的，如生成、分类和翻译。在考虑我们的微调目标时，我们需要选择适合我们需求的模型。在我们的代码场景中，我们可能会选择一个已经在大量代码数据集上训练过的模型，这使得它更容易与我们的代码库进一步微调。在我们的问答/聊天LLM场景中，我们可能会想要一个已经训练成能够作为有能力的基于聊天的LLM的模型。'
- en: '*How big a model to use?* Another question to ask ourselves is how large a
    model do we need? Depending on the size of the model we want, usually defined
    by its parameter size, will also determine our hardware requirements. There is
    a tradeoff that must be considered. If we want our models to be accurate in their
    responses, then a large amount of hardware needs to be dedicated to the hosting
    and running of a larger model. If our budget is limited, or the location we have
    to deploy our model isn’t performant, then we may need to consider a smaller model
    that might not be as accurate or respond as quickly to our requests.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Hugging Face?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face is a platform for the AI open source community that allows members
    to host, deploy, and collaborate on their AI work. Notably, Hugging Face offers
    a place to host data sets and models, as well as the opportunity to let others
    deploy and interact with AI applications through their space features. There are
    also other paid-for features such as auto training, which is designed to make
    fine-tuning easier, as well as increased hardware space to deploy more complex
    and resource-demanding models. Similar to how GitHub enables teams to collaborate
    in addition to offering the ability to host code, Hugging Face offers a place
    to share and learn from AI community members and work together on future AI projects.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: This is by no means an exhaustive list of considerations, and as we explore
    fine-tuning more, we’ll learn how there are different options we need to decide
    on. Fortunately (if you have budget), many tools have been created and put in
    place to make the experimentation with fine-tuning models easier and faster. Therefore,
    although it’s good to have a goal in mind when setting out on a fine-tuning journey,
    we are free to switch out models, data sets, and more to learn how to create the
    most optimal fine-tuned model for a given context.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Executing a fine-tuning session
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keeping in mind the importance of a goal for a fine-tuned model, in this chapter,
    we’re going to attempt to fine-tune a model using the code base of a project to
    create a model that can support future analysis. In real terms, this means having
    a model that, when asked questions about our code base, can give answers that
    are context-sensitive to our project. To do this, we’ll be using the code from
    the open source project restful-booker-platform ([https://mng.bz/vJRJ](https://mng.bz/vJRJ)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Preparing data for training
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although fine-tuning doesn’t require the same volumes of data as a pretrained
    model, the success of fine-tuning relies heavily on the size and quality of the
    data we use, which brings us to the question of what sort of data we need. To
    help us understand the size, scope, and formatting of data sets, we can learn
    a lot from sites such as Hugging Face, where open source data sets are stored
    ([https://huggingface.co/datasets](https://huggingface.co/datasets)). Some notable
    data sets used at the time of writing include:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '*The Stack*—546 million rows of code examples that have been scraped from open
    source projects on sites such as GitHub ([https://huggingface.co/datasets/bigcode/the-stack](https://huggingface.co/datasets/bigcode/the-stack))'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*堆栈*—从GitHub等开源项目上抓取的5.46亿行代码示例（[https://huggingface.co/datasets/bigcode/the-stack](https://huggingface.co/datasets/bigcode/the-stack)）'
- en: '*Alpaca*—52,000 rows of synthetic data that have been generated using an existing
    LLM ([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca))'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*羊驼*—使用现有的大型语言模型（LLM）生成的52,000行合成数据（[https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)）'
- en: '*OpenOrca*—2.91 million rows of question and response data ([https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca))'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenOrca*—290万行问题和回答数据（[https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)）'
- en: Looking through each of these data sets, we can see they contain different types
    of information, from code examples to questions and responses, created in different
    ways. Data sets such as The Stack are based on real information scraped off the
    internet, whereas Alpaca has been synthetically generated by an AI. We can also
    see that Alpaca is a much smaller data set compared with the others in the list,
    but that doesn’t mean it’s not useful.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这些数据集的每一个，我们可以看到它们包含不同类型的信息，从代码示例到问题和回答，以不同的方式创建。例如，The Stack这样的数据集是基于从互联网上抓取的真实信息，而Alpaca是由人工智能合成的。我们还可以看到，与列表中的其他数据集相比，Alpaca是一个规模较小的数据集，但这并不意味着它没有用。
- en: What is synthetic data?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是合成数据？
- en: In the context of AI training and fine-tuning, synthetic data is the process
    of artificially generating data that, while looking like real user data, is not
    based on real-life data. Using synthetic data is a useful technique for training
    and fine-tuning AIs because it can provide the necessary data required for carrying
    out training or fine-tuning. There are side effects to using synthetic data, though.
    First, there is a cost in generating the test data. Tools such as gretel.ai, mostly.ai,
    and tonic.ai offer data-generation tools, but they come at a price. Second, and
    perhaps more important, studies have shown that training models on purely synthetic
    data can impact the quality of a model’s responses. This makes sense because real
    data will have variances and randomness that are hard to simulate in AI-generated
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能训练和微调的背景下，合成数据是指人工生成看起来像真实用户数据，但实际上并非基于真实生活数据的过程。使用合成数据是训练和微调人工智能的有用技术，因为它可以提供进行训练或微调所需的数据。尽管如此，使用合成数据也存在副作用。首先，生成测试数据会有成本。例如，gretel.ai、mostly.ai和tonic.ai等工具提供数据生成工具，但它们是有价格的。其次，也许更重要的是，研究表明，在纯合成数据上训练模型可能会影响模型响应的质量。这很有道理，因为真实数据将具有难以在人工智能生成数据中模拟的变异性随机性。
- en: So, when setting out our requirements, we need to consider what we want, what
    is currently available, and what we might need to build ourselves. Let’s return
    to our code assistant and Q&A model examples. For our code assistant LLM, we are
    likely going to want a corpus of data that is mostly code based, whereas with
    our Q&A model, we would need data written in natural language and containing questions
    and answers in a key–value format (the question being the key and the answer being
    the value). As we can see, our goals inform our decisions regarding the type of
    data we require, but with it come additional questions, such as where is the data
    going to come from and how are we going to format it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在制定我们的要求时，我们需要考虑我们想要什么，目前有什么可用，以及我们可能需要自己构建什么。让我们回到我们的代码助手和问答模型示例。对于我们的代码助手LLM，我们可能需要一个主要以代码为基础的数据集，而我们的问答模型则需要以自然语言编写的数据，并包含以键值格式（问题作为键，答案作为值）编写的问题和答案。正如我们所看到的，我们的目标指导我们关于所需数据类型的决策，但随之而来的是更多的问题，例如数据从何而来以及我们将如何格式化它。
- en: 'We’ve already seen that there are lots of publicly available data sets from
    sites such as Hugging Face (Kaggle available at [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
    is also a great source of data sets). But if we’re attempting to fine-tune a model
    so it is more aligned with our context, chances are we will want to tune it using
    data that belongs to our organization. Therefore, an exercise in what data is
    available, its quality, and how can we access it is required before we decide
    how we’re going to convert it into a format that is suitable for training. Consider
    the format in which the Alpaca data set has been structured ([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)).
    The data set consists of the following four columns: instruction, input, output,
    and text. As we’ll learn in the next step, depending on the way we are fine-tuning
    a model, we will require different aspects of a data set. For example, if we wanted
    to fine-tune a Q&A model, we would require, as a minimum, the instruction and
    output columns to help tune it toward what type of questions to expect and what
    types of answers to respond with.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，来自Hugging Face（Kaggle的网址[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)也是一个优秀的数据集来源）等网站上有许多公开的数据集。但如果我们试图微调一个模型，使其更符合我们的上下文，那么我们很可能会希望使用属于我们组织的数据来调整它。因此，在决定如何将其转换为适合训练的格式之前，我们需要进行一项关于可用数据、其质量以及我们如何获取它的练习。考虑Alpaca数据集的结构格式([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca))。该数据集包括以下四个列：指令、输入、输出和文本。正如我们将在下一步学习的那样，根据我们微调模型的方式，我们将需要数据集的不同方面。例如，如果我们想微调一个问答模型，我们至少需要指令和输出列，以帮助调整它，使其能够回答预期的问题类型和回答类型。
- en: The challenge is getting raw data into a structured format like the one we saw
    in the Alpaca data set. For example, if we consider our Q&A model scenario, we
    would likely want to train it on our documentation and support documents. Some
    of this raw data might be in a Q&A format, such as FAQs, but the majority of our
    data wouldn’t be so straightforward. Therefore, we would need to work out a way
    to parse our data in a way that fits our data set structure. To make matters more
    complicated, we would also need to do this automatically to generate enough of
    a corpus of data so that it would be useful. Doing it manually is also an option,
    but it could be a costly one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于将原始数据转换为像我们在Alpaca数据集中看到的那种结构化格式。例如，如果我们考虑我们的问答模型场景，我们可能希望用我们的文档和支持文档来训练它。其中一些原始数据可能以问答格式存在，例如常见问题解答（FAQs），但我们的大部分数据可能不会这么直接。因此，我们需要找出一种方法来解析我们的数据，使其适合我们的数据集结构。更复杂的是，我们还需要自动完成这项工作，以生成足够的数据量，使其有用。手动完成也是一项选择，但可能会很昂贵。
- en: A case study in data preparation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备案例研究
- en: 'The data set we are going to use for our fine-tuning session presents a good
    example of the challenges we may encounter when building even small data sets.
    For our fine-tuning session, we’ll be using a previously created data set I built
    that can be found on Hugging Face at [https://mng.bz/4pXa](https://mng.bz/4pXa).
    The data set is a JSONL-formatted document that contains parsed sections of the
    Java portion of restful-booker-platform (RBP), paired with generated instructions,
    a sample of which is provided here (line breaks have been added for readability):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要用于微调会话的数据集展示了我们在构建小型数据集时可能遇到的挑战。在我们的微调会话中，我们将使用一个之前创建的数据集，该数据集可以在Hugging
    Face上找到，网址为[https://mng.bz/4pXa](https://mng.bz/4pXa)。该数据集是一个JSONL格式的文档，包含restful-booker-platform
    (RBP)的Java部分的解析部分，以及生成的指令，其中一部分如下所示（为了便于阅读，已添加换行符）：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This snippet will give us everything we need data-wise for tuning, but before
    we begin, let’s explore how it was made. Consider a class like the following and
    ask yourself how would you break this code up for fine-tuning in a logical fashion?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段将为我们提供调整所需的所有数据，但在我们开始之前，让我们探索一下它是如何制作的。考虑以下类，然后问问自己你会如何以逻辑的方式将这段代码拆分以进行微调？
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Would you fine-tune on a file-by-file basis, a line-by-line basis, or some
    other way? On a first attempt at testing out fine-tuning of a model on the RBP
    code base, I opted for the line-by-line approach. It created a script that would
    iterate through each file in a project and add each line of the file into its
    row resulting in a table of data that looked similar to the following example
    table:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你会选择按文件逐个微调、按行逐个微调，还是其他方式？在尝试测试基于RBP代码库的模型微调的第一尝试中，我选择了逐行的方法。它创建了一个脚本，该脚本将遍历项目中的每个文件，并将文件的每一行添加到其行中，从而生成一个类似于以下示例表的数据表：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The problem with this approach was that, although it was easy to parse and store
    the data, I ended up with entries that were lacking context, meaning I was tuning
    with entries such as `}` or `@Autowired`. These don’t provide a lot of context
    or detail about the RBP project, and they raised another question. What type of
    instruction can be paired with these types of entries? If you recall, during instruction-based
    fine-tuning, we send an instruction (sometimes with additional input data) and
    then compare the response with our expected output. The type of instruction we
    would add to an entry such as `}` would contain no hints toward our context and
    potentially contribute to a fine-tuned model that responds in unusual and undesired
    ways. This is exactly what happened when I attempted to tune a model based on
    line-by-line material.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，尽管解析和存储数据很容易，但我最终得到的条目缺乏上下文，这意味着我调整的是像`}`或`@Autowired`这样的条目。这些条目并没有提供很多关于RBP项目的上下文或细节，并且引发了一个新的问题。可以与这些类型的条目配对的指令类型是什么？如果你还记得，在基于指令的微调过程中，我们会发送一个指令（有时附带额外的输入数据），然后比较响应与我们的预期输出。我们添加到像`}`这样的条目中的指令类型将不包含任何关于我们上下文的提示，并可能对微调模型产生响应异常和不希望的方式。这正是我在尝试基于逐行材料调整模型时发生的情况。
- en: 'Instead, what I opted for (and what can be found in the dataset) is an approach
    that breaks down the code into logical sections. This means that rather than slicing
    things line by line, a file would be sliced up based on different attributes within
    a Java class. For example, a selection of slices from the class I shared earlier
    would look like the following in an example table:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我选择的方法（并在数据集中可以找到）是将代码分解成逻辑部分。这意味着，而不是逐行切割事物，一个文件将基于Java类中的不同属性进行切割。例如，从之前分享的类中选择的切片在示例表中看起来如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Instead of each entry in the data set being a line of code, each entry might
    contain details of how the class was declared, what variables are being declared
    in the class, each method in the class, and its containing code. The goal was
    to keep it detailed enough so the fine-tune would result in a model that had a
    greater awareness of the code base, but not so granular to lose the context completely.
    The result was a more successful tune, but the process of parsing became more
    complex. It required the creation of additional code that would iterate through
    each file, using JavaParser ([https://javaparser.org/](https://javaparser.org/))
    to ingest the code, build a semantic tree, and then query said tree to extract
    the information required for the data set (the code of which can be found at [https://mng.bz/QVpw](https://mng.bz/QVpw)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据集中的每个条目都是一行代码不同，每个条目可能包含如何声明类、在类中声明的变量、类中的每个方法及其包含的代码的详细信息。目标是保持足够详细，以便微调的结果是一个对代码库有更高意识的模型，但又不至于过于细化而完全失去上下文。结果是更成功的调整，但解析过程变得更加复杂。这需要创建额外的代码，该代码将遍历每个文件，使用JavaParser
    ([https://javaparser.org/](https://javaparser.org/)) 读取代码，构建语义树，然后查询该树以提取数据集所需的信息（代码可以在[https://mng.bz/QVpw](https://mng.bz/QVpw)找到）。
- en: This example is a very basic one when it comes to preparing a data set for tuning
    (or training). However, after reflecting on it, it becomes clear that organizing
    and preparing even a simple data set from scratch has its complexities and challenges.
    The raw data for this data set was easily parsable with the right tools, but how
    do we manage data that is diverse in structure or has no discernible structure
    in the first place? This exploration into data sets highlights that the identification
    and creation of a data set is a complicated process. How we structure our data
    and what we put in it is critical for the success of fine-tuned models, and it’s
    where most of the work of fine-tuning and experimenting with LLMs can be found.
    Therefore, it’s important to have the necessary processes and tooling in place
    so that we can rapidly experiment with different data sets to see how they impact
    the result of a fine-tuned model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到为微调（或训练）准备数据集时，这个例子是一个非常基础的例子。然而，经过反思，我们可以清楚地看到，从头开始组织和准备即使是简单数据集也有其复杂性和挑战。这个数据集的原始数据可以用合适的工具轻松解析，但我们如何管理结构多样或一开始就没有明显结构的数据呢？对数据集的这种探索突出了识别和创建数据集是一个复杂的过程。我们如何组织数据以及我们放入其中的内容对于微调模型的成功至关重要，这也是微调和实验LLMs的大部分工作所在。因此，确保我们有必要的流程和工具非常重要，这样我们就可以快速实验不同的数据集，看看它们如何影响微调模型的结果。
- en: 12.2.2 Preprocessing and setup
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 预处理和设置
- en: With our data set in place, we next need to preprocess our data before tuning
    and get our tuning tools in place. We’ll get to tool setup soon, but first, to
    understand the preprocessing activities for fine-tuning, we need to skip ahead
    a little and talk about what happens during a fine-tuning session. Perhaps not
    surprisingly, given the size of data sets, a fine-tuning session consists of a
    specific loop, visualized in figure 12.2, which is run multiple times during fine-tuning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集就绪后，我们接下来需要在微调之前预处理我们的数据，并准备好我们的微调工具。我们将很快介绍工具设置，但首先，为了了解微调的预处理活动，我们需要稍微跳一下，谈谈微调会话期间发生的事情。考虑到数据集的大小，微调会话由一个特定的循环组成，如图12.2所示，这个循环在微调过程中会多次运行。
- en: Stepping through the visualization, we start with our data set. Assuming it’s
    structured in a way similar to the `_Alpaca_ dataset` we looked at earlier (it
    contains a column of instructions, inputs, and outputs), the data stored in the
    instruction and input columns are added to a prompt. That prompt is then sent
    to the model we are fine-tuning and a response is sent back from the model. We
    then compare the response from the model with the output stored in our data set
    to determine the *sentiment*. The sentiment indicates how closely aligned the
    response is to our expected output. The sentiment score is then used to inform
    what tweaks need to be made to the model’s parameters to fine-tune it toward how
    we want our model to respond. If the sentiment score indicates it’s responding
    desirably, then changes will be minimal. On the other hand, if the sentiment score
    indicates an undesirable response, then bigger changes will be made.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化过程中，我们首先从我们的数据集开始。假设它以与我们之前查看的 `_Alpaca_` 数据集类似的方式组织（它包含一个指令、输入和输出的列），指令和输入列中存储的数据将被添加到一个提示中。然后，这个提示被发送到我们正在微调的模型，模型会返回一个响应。我们随后将模型的响应与数据集中存储的输出进行比较，以确定*情感*。情感表示响应与我们的预期输出之间的匹配程度。然后，情感分数被用来告知需要调整模型参数以微调模型以实现我们希望模型如何响应。如果情感分数表明响应是可取的，那么变化将很小。另一方面，如果情感分数表明响应不可取，那么将进行更大的调整。
- en: '![](../../OEBPS/Images/CH12_F02_Winteringham2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH12_F02_Winteringham2.png)'
- en: Figure 12.2 A visualization of what happens during fine-tuning
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 微调过程中的可视化
- en: This whole process is done programmatically using different tools and is run
    multiple times across each entry in a data set. The data set itself is also usually
    iterated through multiple times known as an *epoch*. It’s through iterating across
    multiple epochs in the fine-tuning process that the model is tuned toward how
    we want it to respond. This approach to tuning is known as instruction-based fine-tuning,
    and it’s important to understand how it works before we execute our fine-tuning
    because there are steps we need to take before the tuning can begin. First, we
    need to design the type of prompt we want to send to our model. Second, we need
    to determine how we are going to codify our prompt so that our model can read
    it. Similar to curating our data set, the choices we make for these two steps
    can also have a dramatic effect on the result of our fine-tuning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程都是通过不同的工具以编程方式完成的，并在数据集中的每个条目上多次运行。数据集本身通常也会多次迭代，这被称为一个*epoch*。通过在微调过程中迭代多个epoch，模型被调整到我们希望它如何响应的方式。这种调整方法被称为基于指令的微调，在我们执行微调之前理解它的工作方式非常重要，因为我们需要在调整开始之前采取一些步骤。首先，我们需要设计我们想要发送给模型的提示类型。其次，我们需要确定我们如何将我们的提示编码化，以便我们的模型可以读取它。类似于我们选择数据集，我们在这两个步骤中做出的选择也可能对我们的微调结果产生重大影响。
- en: Prompt design
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提示设计
- en: 'Once we know the format of our data set, we need to create an instructional
    prompt that works with the data within the data set and any additional instructions
    we want to add. For example, consider these two instruction prompts from the deeplearning.ai
    course *Finetuning Large Language Models* ([https://mng.bz/XV9G](https://mng.bz/XV9G)).
    First is a prompt that takes an instruction and an input:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了数据集的格式，我们需要创建一个指导提示，使其能够与数据集中的数据以及我们想要添加的任何附加指令一起工作。例如，考虑deeplearning.ai课程《微调大型语言模型》中的这两个指导提示（[https://mng.bz/XV9G](https://mng.bz/XV9G)）。第一个提示接受一个指令和一个输入：
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction that describes a task, paired with an input that
    provides further context. Write a response that appropriately completes the request.###
    Instruction:{instruction}### Input:{input}### Response: |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一个描述任务的指令，配有一个提供更多上下文的输入。请编写一个响应，以适当地完成请求。### Instruction:{instruction}###
    Input:{input}### Response: |'
- en: 'The second is a prompt that contains just an instruction:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个提示只包含一个指令：
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:{instruction}### Response: |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一个描述任务的指令。请编写一个响应，以适当地完成请求。### Instruction:{instruction}### Response:
    |'
- en: 'Notice how each prompt contains a combination of static, instructional text
    that contextualizes what information is being sent, and then tags such as `{instruction}`
    to inject data from the data set. Based on the RBP data set we want to use, we
    could configure our prompt for fine-tuning like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到每个提示都包含静态的、指导性的文本，它为发送的信息提供了上下文，然后是像`{instruction}`这样的标签来注入数据集中的数据。根据我们想要使用的RBP数据集，我们可以这样配置我们的提示以进行微调：
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction, delimited by three hashes, that asks a question
    about the restful booker platform code base. Respond with the necessary code to
    answer the question. Check that the code compiles correctly before outputting
    it.###{instruction}### |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一个由三个井号分隔的指令，它询问关于restful booker平台代码库的问题。请提供必要的代码来回答问题。在输出之前检查代码是否正确编译。###{instruction}###
    |'
- en: 'The prompt follows some of the prompting tactics we’ve explored in earlier
    chapters. We can use those to help us instruct the model clearly in what to expect
    in our prompt and what we want to see it responds with. To help us better understand
    the fine-tuning loop, let’s imagine we have the following entry in a data set:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示遵循我们在前面章节中探讨的一些提示策略。我们可以使用这些策略来帮助我们清楚地指导模型在提示中期待什么，以及我们希望它做出什么响应。为了更好地理解微调循环，让我们想象我们在数据集中有以下条目：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: During fine-tuning, the instruction portion of this data set would be injected
    into the prompt to create the following prompt
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，这个数据集的指令部分将被注入到提示中，以创建以下提示
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Below is an instruction, delimited by three hashes, that asks a question
    about the restful booker platform code base. Respond with the necessary code to
    answer the question. Check that the code compiles correctly before outputting
    it.###How does the method i`nitialiseMocks` work for `BrandingServiceTest`?###
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: 'This might result in the model responding with a code example written like
    this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'However, our output in our data set is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: This means that the sentiment score between the two sets of data is a middling
    one because, although the response is code and it has some similarities to our
    expected output in solution, the code isn’t entirely the same. This sentiment
    score would then be factored into what tweaks need to be made to the model’s parameters
    so that when this specific row in our data set comes around again, the results
    are more closely aligned. The prompt template we use affects the results of a
    fine-tuned model, and the instructions we add make a difference. However, we need
    to be mindful that what we add to a prompt template doesn’t affect just the result
    of fine-tuning but also has an impact on what is sent to the model in the first
    place. This brings us to how we turn a text-based prompt into a language that
    a model understands.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: A *token* is a numerical representation of a word, phrase, or character. We
    covered the topic of tokens in chapter 10\. So why do we need to be aware of tokenization
    as part of the fine-tuning process? First, there are many different tokenizers
    available that can be utilized during data preprocessing that will tokenize the
    text in different ways. The type of model we are using will influence the type
    of tokenizer. Choosing one that doesn’t align with the model we’re tuning would
    result in our prompts being converted into token identifiers that don’t align
    with the parameters that exist inside the model we’re tuning. Roughly speaking,
    it would be like being taught a course by a teacher who was speaking in a different
    or completely made-up language.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The second reason, which relates to our fine-tuning prompt and our data set,
    is context length. Context length is the total amount of tokens a model can process
    at once. This is important because if we create a prompt that has a large number
    of tokens within it or attempt to tune using data containing a large number of
    tokens within each entry, then our prompts risk breaching the context length,
    meaning our prompt will be truncated. Every token that is over the context length
    limit would simply be discarded or ignored, and the result would be a model being
    fine-tuned on partially complete prompts, which might create unexpected or undesired
    side effects.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to keep our context length in mind both when curating our
    dataset and designing our prompt for tuning. This might mean removing any entries
    from our data set that have the potential to overflow our context length, writing
    a prompt that has clear instructions but doesn’t overflow the token count, or
    looking for a new model that contains a larger context length.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Tooling and hardware
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Given the many steps required for processing data and executing the fine-tuning
    process, necessary tooling needs to be in place to execute each phase. Fortunately,
    tooling for fine-tuning has progressed greatly recently. Originally, it would
    require extensive experience with tools such as Python and libraries such as PyTorch,
    Tensorflow, or Keras. And though these tools are designed to be as easy to use
    as possible, the learning curve could be quite steep and require us to build our
    fine-tuning frameworks from the ground up. If we are comfortable with this type
    of approach or are working with those who have experience with these types of
    tools, then it’s worthwhile using them. However, as interest has grown around
    fine-tuning, new tooling built on the aforementioned tools has begun to appear
    to make fine-tuning more accessible. Frameworks such as Axolotl and platforms
    such as Hugging Face let us set up fine-tuning quickly in a way that requires
    minimal tool development. The tradeoff is that these frameworks are either opinionated,
    for example selecting what tokenizers we should use, or they come at a cost.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: It is not only the tooling around AI tuning that has seen growth, but also the
    infrastructure that supports it. Training models is a hardware-intensive exercise
    and requires access to graphical processing units (GPUs). This means either purchasing
    hardware that has a generous amount of CPU, RAM, and GPU for tuning, or provisioning
    computing from cloud providers. The latter is the popular option for many teams
    and a massive area of growth as it keeps the costs down for hardware requirements,
    ensuring access to newer, updated GPUs. Unsurprisingly, the big cloud computing
    companies such as Google, Microsoft, and Amazon all offer access to dedicated
    services that are designed specifically for tuning and hosting of LLMs. But some
    alternatives have begun to appear, such as RunPod, Latitude.sh, and Lambda Labs,
    which are specialist GPU cloud providers. These are options that can be used in
    conjunction with the tools we select for fine-tuning, but some services provide
    both the frameworks for fine-tuning and the computing resources to run the tuning
    for you.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The market landscape for what we can use for and where we can run our fine-tuning
    is a fast-growing space. But what it highlights is that research is required to
    determine what tooling and infrastructure best suits the experience of our team
    and the type of budget we have available for tuning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our fine-tuning tooling
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools available for fine-tuning that offer different levels of
    control of the tuning process, PyTorch being a popular choice. But as mentioned
    before, there is a learning curve to setting and using these tools. If we were
    working in a context in which we wanted full control of our prompts, tokenizers,
    and tuning tools, we might choose these more granular tools. But for new starters,
    like us, who are happy using tools that trade opinions on approaches for ease
    of use, we can once again look to the AI open source community. Therefore, for
    our tuning session, we’ll be running our fine-tuning with Axolotl, a tool *designed
    to streamline the fine-tuning of various AI models, offering support for multiple
    configurations and architectures.*
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: We can think of Axolotl as a framework for fine-tuning that contains all the
    necessary tools and processes to carry out our tuning. That means for our fine-tuning
    session, the prompting approach and tokenizer have been taken care of for us,
    allowing us to get into our fine-tuning quickly and without a massive learning
    curve.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Hardware requirements for using Axolotl
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, it’s important to note that to carry out a fine-tuning session,
    you will need a system that has access to a GPU. If, however, you don’t have access
    to a GPU, there are cost-effective cloud platforms designed to support AI fine-tuning.
    Axolotl’s ReadMe contains links to two providers: RunPod and Latitude.sh.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'As someone who doesn’t have access to a GPU, I found RunPod easy to get set
    up with and reasonably priced to run multiple training sessions for less than
    $10\. If this is the approach you want to take, here are some steps to follow
    to get set up:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Create an account and add credit to it via [https://www.runpod.io/console/user/billing](https://www.runpod.io/console/user/billing).
    I have found the minimum transaction of $10 to be enough.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Head to GPU cloud [https://www.runpod.io/console/gpu-cloud](https://www.runpod.io/console/gpu-cloud),
    click `Choose Template` at the top of the page, find the `winglian/axolotl-runpod:main-latest`
    Docker image, and select it.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, choose a pod to deploy. Depending on the time of day and demand, you will
    see which ones can be deployed and which can’t. At the time of writing, a 1x RTX
    4090 will suffice for our tuning exercise. However, if we want the tuning to go
    faster, we can choose more GPUs or a larger box.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click deploy and go through the setup wizard to fire off the creation of your
    pod. Head to [https://www.runpod.io/console/pods](https://www.runpod.io/console/pods)
    and wait for your pod to deploy.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the pod is deployed, click on connect to reveal the details to SSH into
    your pod (This will require you to add an SSH public key before connecting, which
    can be done here: [https://www.runpod.io/console/user/settings](https://www.runpod.io/console/user/settings).)'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you are logged into the pod, you’ll find Axolotl is installed and ready
    to use.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by setting up Axolotl on our machine of choice (if you have chosen
    the RunPod option this can be skipped). The documentation and code for Axolotl
    can be found at [https://mng.bz/yoRG](https://mng.bz/yoRG), and it contains comprehensive
    instructions on how to install the application, offering the option of installing
    it directly on our machine or via Docker.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Working with fine-tuning tools
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once Axolotl is set up, we can begin configuring our session. As mentioned before,
    we’ll be using the RBP data set, which can be found on Hugging Face ([https://mng.bz/M1M7](https://mng.bz/M1M7)).
    For our model, we’ll also be using a version of Meta’s Llama-2 model, which contains
    7 billion parameters and a context window of 4k. Much like the prompts and tokenizer,
    the model settings have been taken care of in an example file that can be found
    inside the Axolotl project `examples/llama-2/lora.yml.` However, to train the
    model on our data set, we need to update the `dataset.path` in the YAML file to
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'with `path` dictating where to find the data set on Hugging Face (that is,
    where it will be downloaded from) and `type` setting out which template prompt
    we want to use. Looking at the top of the YAML file, we can also see references
    to the model and the tokenizer that will be used. Again, these can be modified
    if we want to experiment with other approaches:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Several other settings can be found in the file and are beyond the scope of
    this chapter, but there are two we do want to highlight: `sample_packing` and
    `num_epochs`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As for `sample_packing,` we set it to `false` because the data set isn’t large
    enough for splitting into training and testing sets (more on that later). `num_epochs`
    determines how many times we want to iterate through our data set. The default
    is `4`, meaning the fine-tuning process will loop through the whole data set four
    times before it completes. With these changes made to the YAML file, we can save,
    quit, and begin our fine-tuning.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 Setting off a fine-tuning run
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With our configuration in place, we can begin fine-tuning. To do this, we’ll
    be following the steps found in Axolotl’s ReadMe. To start, we trigger a preprocessing
    step:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preprocessing step downloads the data set and runs it through the tokenizer,
    translating the data from text to tokens, ready for fine-tuning.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: What is LORA?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that the YAML file we’ve been configuring and are now using
    to start our fine-tuning session is named lora.yml. LORA is an approach to fine-tuning
    in which instead of attempting to directly tune the parameters within a model,
    a smaller subset of parameters that approximate the parameters of the model are
    created and fine-tuned, creating a LORA adapter. This means that when we deploy
    the model once it’s fine-tuned, the model is loaded with the LORA adapter inside
    it to give us the tuned behaviors we are looking for. It has become popular because
    it speeds up the fine-tuning process and allows communities and teams to share
    their adapters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the preprocessing is complete, we are ready to start the fine-tuning process.
    Keep in mind that depending on the hardware being used, this process can take
    anywhere from 30 minutes to 4 hours or more, so pick a time in which the fine-tune
    can be left to run while you work on other tasks. To trigger the fine-tuning,
    we run the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will pull in the YAML file that we’ve edited and kick-start the fine-tuning
    process. As the tuning begins, we’ll start to see details on its progress of the
    tune like the following example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Each entry in the console details the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss*—This score indicates the alignment between the expected output in our
    data set and the output of the model. The lower the score, the better aligned
    the expected and actual responses. In this example, the loss score is relatively
    high because it was taken at the start of a tuning. As the tuning progresses,
    we would hope to see the loss score reduce.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning rate*—This is a numerical representation of the step change made
    to the parameters in the model. Smaller steps mean more gradual changes in tuning.
    How big a step is made is determined by the sentiment score, as well as the learning
    rate hyper-parameter. In the context of AI training, a hyper-parameter is a configuration
    option that we can set before the tuning begins, which impacts the outcome of
    training or tuning. So, in the case of learning rate, we can increase the step
    range, which can result in more dramatic tunes to a model. This again may or may
    not result in a more optimal tune.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Epoch*—Earlier, we learned how we can iterate through a data set multiple
    times during fine-tuning and that each iteration is known as an epoch. The epoch
    value shown in the console output simply informs us how far through a given epoch
    we are.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These metrics are useful to give us an indication of the quality and the progress
    of a fine-tuning session. Depending on the size of the data set, the model and
    hardware supporting the tune will determine how long a tune might take. However,
    it’s not uncommon for a tune to take multiple hours, given the volume of work
    required for tuning. This is why more experienced model tuners will set up processes
    and tooling so that multiple models can be tuned at once for comparison once the
    tuning is complete.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.5 Testing the results of a fine-tune
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once our fine-tuning has been completed, we’ll want to check whether the changes
    we’ve made align with the goal that we set at the start of the fine-tuning process.
    If you recall, during the fine-tuning process, an instruction is sent to a model,
    and it returns an output. Sentiment analysis then determines how closely aligned
    the model’s output and our expected output are, which informs what tuning to the
    model’s parameters takes place. Consequently, the parameters within a model should
    now be more biased toward our context. Therefore, to test whether a model has
    been successfully tuned, we want to check what happens when we ask the model for
    new instructions that differ from the ones it was tuned on. We can do this in
    one of the two ways: inference and/or human validation.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Given the numerous parameters within a model and the options in instructions
    to send and outputs to receive, inference employs an automated approach to testing
    the output of a model. Inference works very similar to fine-tuning. We either
    take a slice out of a larger data set or employ a new data set that follows the
    same structure as the data set we used for fine-tuning that contains instructions
    and outputs that are different to the ones in our original tuning data. Then we
    send each of these sets of instructions to a model, capture the response, and
    then use sentiment analysis to compare what we expect the model to respond against
    what it responded. (The key difference between tuning and inference comes after
    the sentiment analysis. Tuning will make changes to the model, whereas in case
    of inference, the model is left alone). If the returned sentiment score is high,
    we can assume the model has been tuned in a way that meets our goals. If it doesn’t,
    then we can start to consider what our next steps are for a future fine-tuning
    session.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Human validation
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Although sentiment analysis is useful, it is based on mathematical models to
    determine alignment. Therefore, it’s also sensible to explore the outputs of a
    model manually, through human validation. This might be through using prompts
    saved in the inference data set and evaluating responses, or by testing out different
    responses by generating new prompts to see how the model reacts. To get the same
    level of scope and coverage as with the inference testing would be too expensive
    for a team, but it can provide a more human perspective that can spot discrepancies
    and/or hallucinations that inference wouldn’t.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what we learn from both, the automated processes of inference and
    our experimentation will inform our next decisions. Perhaps the fine-tuning resulted
    in a model that is acceptable, and we can release it for wider use. However, we
    will often likely conclude that the resulting tuned model isn’t right for us.
    This would trigger further analysis to determine what our next steps are. Perhaps
    our data set needs changing, or we want to modify our prompt further, or perhaps
    the tokenizer could be replaced. Regardless of the choice—again, a decision-making
    process that is beyond the scope of this chapter—this highlights that fine-tuning
    projects require to experiment many times over to discover an optimal result.
    This is why teams that have a mature process around fine-tuning (or training in
    general) will run multiple fine-tuning experiments at once, tuning multiple models
    at once, with slightly different parameters to compare the results.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Testing out our fine-tuning session
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our fine-tuned model, although we don’t have enough data to run
    an inference testing session, we can launch our model to test it out manually.
    To do this, we run the following command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With this command, we are loading up the Llama-2 model and loading in the LORA
    adapter we have created as part of our tuning. The command `--gradio` allows us
    to host a user interface for our model using the gradio library ([https://www.gradio.app/](https://www.gradio.app/))
    so that we can start to test out our model via a web browser.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'With our model running, we can begin to check the results of our fine-tuning
    session. First, we can check the tuning by selecting an instruction and output
    set from our data set such as:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: '| Instruction:How does the method `initialiseMocks` work for `BrandingServiceTest`?Output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the prompt from the tuning session, we can send the following request
    to our tuned model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:How does the method `initialiseMocks` work
    for `BrandingServiceTest`?### Response: |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: 'Upon sending this, we would receive a response such as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:How does the method `initialiseMocks` work
    for `BrandingServiceTest`?### Response:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the model has been tuned to the point that it is returning a
    strongly aligned response based on the instructions we’ve sent. Now we can turn
    our attention to how the model copes with new untested prompts, such as this basic,
    example prompt:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: '| What are the annotations found in the `BrandingResult` java class? |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: 'Sending this to an example tuned model returns a response like this:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: '| What are the annotations found in the `BrandingResult` java class?'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Comparing this response to the original code on which the model was tuned (which
    can be found at [https://mng.bz/aVlz](https://mng.bz/aVlz)), we can see that the
    model has demonstrated some success in listing aspects of the `BrandingResult`
    class such as the variables used and the getter/setter methods. However, it is
    also missing details such as the class constructor, and it got the names of variables
    wrong (although it is at least consistent across the code). It could also be argued
    that the prompt wasn’t answered correctly as we were requesting details on annotations
    and not on the class as a whole.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: So, in conclusion, we have seen some success with this tuning session, but more
    work is required. The tuning process has rebalanced the parameters within the
    model in a way that our context has become more dominant within it. However, the
    missing items and incorrect details mean that further tweaking is required for
    the tuning process. Perhaps we could look to improve the quality of the instructions
    in the data set or reconsider the prompt we use for tuning. Equally, we could
    look at more technical aspects of the tuning, such as choosing a model with a
    larger parameter count, or tweak hyper-parameters such as the amount of epochs
    we use for training or the learning rate.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.6 Lessons learned with fine-tuning
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter gave us a taste of how the fine-tuning process works. At first,
    it may seem like an overwhelming activity. But although there is specific tooling
    and terminology to understand, by taking the fine-tuning process step by step,
    we can tackle each challenge as it comes. Ultimately, fine-tuning is very much
    about experimentation. What data we use, models we tune, tooling we employ, and
    hyper-parameters we set, they all affect the result.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the cost of experimentation is not something that can
    be dismissed. Teams that want to carry out fine-tuning sessions require substantial
    financial backing for resources and experience. But as both private companies
    and the open source community grow, fine-tuning will become more accessible, and
    the price of hardware will likely decrease, making this a growing space in organizations
    and a challenge for teams to deliver high-quality models that can assist us and
    our organizations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning is the process of training a pre-existing model further, which is
    sometimes known as a foundational model.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fine-tuning process involves multiple steps such as goal setting, data preparation,
    processing, tuning, and testing.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting clear goals around what we want a fine-tuned LLM to do informs how we
    approach the fine-tuning process.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned models require specifying and preparation of data.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sets massively influence the results of a fine-tuned model. This means
    finding data relevant to our goals and formatting it in a way that helps maximize
    the output of a model after fine-tuning.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning relies on repeatedly sending prompts embedded with training data
    to get a response that we want to bias toward aligning with an expected output.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models need prompts to be converted into a machine-readable language. This is
    achieved through the tokenization process.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization is the process in which data is sliced into smaller tokens.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models have a context length, which is the maximum number of tokens it can process
    at once. Send too many tokens at once, and some will be discarded, affecting the
    fine-tuning process.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When fine-tuning, we can either build our frameworks, which require experience,
    or utilize existing frameworks, which are opinionated and/or cost money to use.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axolotl is a great framework for fine-tuning that is accessible to those with
    limited experience and looking to get started.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing of fine-tuned models can be done in an automated fashion, using inference,
    or manually.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning is becoming increasingly accessible to teams for use in the AI assistant
    tooling.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
