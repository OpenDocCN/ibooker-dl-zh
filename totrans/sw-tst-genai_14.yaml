- en: 12 Fine-tuning LLMs with business domain knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: The fine-tuning process for an LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a data set to use for fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using fine-tuning tools to better understand the process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the effects of large language models (LLMs) in various industries have
    been covered extensively in the mainstream media, the ubiquity and popularity
    of LLMs have contributed to a quiet revolution in the AI open source community.
    Through the spirit of open collaboration and the support of big technology companies,
    the ability to fine-tune AI models has become increasingly more accessible to
    AI enthusiasts. This opportunity has resulted in a vibrant community that is experimenting
    and sharing a wide range of processes and tools that can be used to better understand
    how fine-tuning works and how we can tune models ourselves or in teams.
  prefs: []
  type: TYPE_NORMAL
- en: The topic of fine-tuning is vast, and getting into each important detail would
    require an entire book of its own. However, by taking advantage of the models,
    data sets, platforms, and tools created by and for the open source community,
    we can establish an appreciation for the fine-tuning process. These open source
    resources can prepare us for a future in which we might find ourselves fine-tuning
    our models within our organizations to help build context-based LLMs. However,
    fine-tuning is as much about the approach we take as it is about the tools we
    use. So, in this chapter, we’ll go through each important part of the fine-tuning
    process and learn how to fine-tune our model.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Exploring the fine-tuning process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we learn more about fine-tuning tools, we should first discuss what fine-tuning
    entails and reflect on what we hope to achieve. As we’ll see, fine-tuning involves
    a series of steps that play an important role in the wider process. Knowing what
    we hope to achieve helps us not only with the evaluation of a model once it’s
    been tuned, but also guides us in our tuning approach. Each step of the fine-tuning
    process includes distinct activities and challenges. And although we might not
    be able to cover all the details, we’ll learn enough to understand what happens
    as we tune models and the challenges that we might face.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 A map of the fine-tuning process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discovered in chapter 10, fine-tuning is the process of taking an existing
    model that has already gone through some sort of training, known as a foundational
    model, and training it further with additional data sets to
  prefs: []
  type: TYPE_NORMAL
- en: Make a model more attuned to contextual information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the tone of a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Help it respond to specific queries or instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just as a foundational model goes through a series of steps to be trained, there
    is a series of steps to go through during a fine-tuning session. Figure 12.1 summarizes
    these steps and how they feed into one another.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH12_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 A visual representation of the different steps taken during fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 doesn’t necessarily cover every nuance of fine-tuning, but it captures
    the core steps we would typically expect to go through to successfully create
    a tuned model. Throughout this chapter, we’ll examine each step in more detail,
    but let’s first reflect on what might be the most important part of the process,
    identifying what we hope to achieve from fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 Goal setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the biggest mistakes that we can make when tuning LLMs is not having
    a clear idea of what we want our tuned LLM to do. Failing to set out clear goals
    for what problems a tuned model is supposed to help with can affect every part
    of the fine-tuning process, from data preparation to testing. Given the indeterministic
    nature of LLMs, this doesn’t mean setting goals around specific information we
    expect to get from an LLM. But we do have to ask ourselves what type of behavior
    we want and how it fits into our wider context.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, let’s consider two different goals. One is to create a
    code-completion tool that has been fine-tuned on our code base to use an LLM’s
    generative capabilities, without revealing intellectual property to a third party.
    Another is a Q&A/chat-based LLM that offers support to users, which has been tuned
    on support documentation and customer data to help answer questions. Depending
    on which goal we want to pursue, we would need to consider details such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What datasets to use?* For our code completion scenario, we likely want to
    create a data set that consists of code broken down into logical sections to tune
    our model with. We may also be interested in using other data sets that include
    open source code. This would differ from the Q&A chat scenario in which we would
    create a corpus of data that includes help guides and documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What model to use?* At the time of writing, Hugging Face, an AI open source
    community site, is currently hosting 500,000+ different models, all of which are
    designed to serve different purposes such as generation, classification, and translation.
    When considering our fine-tuning goals, we will need to select models that suit
    our needs. In our code scenario, we would likely pick a model that has already
    been trained on a large corpus of code data, making it easier to further tune
    with our code base. In our Q&A/chat LLM scenario, we would likely want a model
    that has already been trained to act as a capable chat-based LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How big a model to use?* Another question to ask ourselves is how large a
    model do we need? Depending on the size of the model we want, usually defined
    by its parameter size, will also determine our hardware requirements. There is
    a tradeoff that must be considered. If we want our models to be accurate in their
    responses, then a large amount of hardware needs to be dedicated to the hosting
    and running of a larger model. If our budget is limited, or the location we have
    to deploy our model isn’t performant, then we may need to consider a smaller model
    that might not be as accurate or respond as quickly to our requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Hugging Face?
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face is a platform for the AI open source community that allows members
    to host, deploy, and collaborate on their AI work. Notably, Hugging Face offers
    a place to host data sets and models, as well as the opportunity to let others
    deploy and interact with AI applications through their space features. There are
    also other paid-for features such as auto training, which is designed to make
    fine-tuning easier, as well as increased hardware space to deploy more complex
    and resource-demanding models. Similar to how GitHub enables teams to collaborate
    in addition to offering the ability to host code, Hugging Face offers a place
    to share and learn from AI community members and work together on future AI projects.
  prefs: []
  type: TYPE_NORMAL
- en: This is by no means an exhaustive list of considerations, and as we explore
    fine-tuning more, we’ll learn how there are different options we need to decide
    on. Fortunately (if you have budget), many tools have been created and put in
    place to make the experimentation with fine-tuning models easier and faster. Therefore,
    although it’s good to have a goal in mind when setting out on a fine-tuning journey,
    we are free to switch out models, data sets, and more to learn how to create the
    most optimal fine-tuned model for a given context.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Executing a fine-tuning session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keeping in mind the importance of a goal for a fine-tuned model, in this chapter,
    we’re going to attempt to fine-tune a model using the code base of a project to
    create a model that can support future analysis. In real terms, this means having
    a model that, when asked questions about our code base, can give answers that
    are context-sensitive to our project. To do this, we’ll be using the code from
    the open source project restful-booker-platform ([https://mng.bz/vJRJ](https://mng.bz/vJRJ)).
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Preparing data for training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although fine-tuning doesn’t require the same volumes of data as a pretrained
    model, the success of fine-tuning relies heavily on the size and quality of the
    data we use, which brings us to the question of what sort of data we need. To
    help us understand the size, scope, and formatting of data sets, we can learn
    a lot from sites such as Hugging Face, where open source data sets are stored
    ([https://huggingface.co/datasets](https://huggingface.co/datasets)). Some notable
    data sets used at the time of writing include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Stack*—546 million rows of code examples that have been scraped from open
    source projects on sites such as GitHub ([https://huggingface.co/datasets/bigcode/the-stack](https://huggingface.co/datasets/bigcode/the-stack))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alpaca*—52,000 rows of synthetic data that have been generated using an existing
    LLM ([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenOrca*—2.91 million rows of question and response data ([https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking through each of these data sets, we can see they contain different types
    of information, from code examples to questions and responses, created in different
    ways. Data sets such as The Stack are based on real information scraped off the
    internet, whereas Alpaca has been synthetically generated by an AI. We can also
    see that Alpaca is a much smaller data set compared with the others in the list,
    but that doesn’t mean it’s not useful.
  prefs: []
  type: TYPE_NORMAL
- en: What is synthetic data?
  prefs: []
  type: TYPE_NORMAL
- en: In the context of AI training and fine-tuning, synthetic data is the process
    of artificially generating data that, while looking like real user data, is not
    based on real-life data. Using synthetic data is a useful technique for training
    and fine-tuning AIs because it can provide the necessary data required for carrying
    out training or fine-tuning. There are side effects to using synthetic data, though.
    First, there is a cost in generating the test data. Tools such as gretel.ai, mostly.ai,
    and tonic.ai offer data-generation tools, but they come at a price. Second, and
    perhaps more important, studies have shown that training models on purely synthetic
    data can impact the quality of a model’s responses. This makes sense because real
    data will have variances and randomness that are hard to simulate in AI-generated
    data.
  prefs: []
  type: TYPE_NORMAL
- en: So, when setting out our requirements, we need to consider what we want, what
    is currently available, and what we might need to build ourselves. Let’s return
    to our code assistant and Q&A model examples. For our code assistant LLM, we are
    likely going to want a corpus of data that is mostly code based, whereas with
    our Q&A model, we would need data written in natural language and containing questions
    and answers in a key–value format (the question being the key and the answer being
    the value). As we can see, our goals inform our decisions regarding the type of
    data we require, but with it come additional questions, such as where is the data
    going to come from and how are we going to format it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve already seen that there are lots of publicly available data sets from
    sites such as Hugging Face (Kaggle available at [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
    is also a great source of data sets). But if we’re attempting to fine-tune a model
    so it is more aligned with our context, chances are we will want to tune it using
    data that belongs to our organization. Therefore, an exercise in what data is
    available, its quality, and how can we access it is required before we decide
    how we’re going to convert it into a format that is suitable for training. Consider
    the format in which the Alpaca data set has been structured ([https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)).
    The data set consists of the following four columns: instruction, input, output,
    and text. As we’ll learn in the next step, depending on the way we are fine-tuning
    a model, we will require different aspects of a data set. For example, if we wanted
    to fine-tune a Q&A model, we would require, as a minimum, the instruction and
    output columns to help tune it toward what type of questions to expect and what
    types of answers to respond with.'
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is getting raw data into a structured format like the one we saw
    in the Alpaca data set. For example, if we consider our Q&A model scenario, we
    would likely want to train it on our documentation and support documents. Some
    of this raw data might be in a Q&A format, such as FAQs, but the majority of our
    data wouldn’t be so straightforward. Therefore, we would need to work out a way
    to parse our data in a way that fits our data set structure. To make matters more
    complicated, we would also need to do this automatically to generate enough of
    a corpus of data so that it would be useful. Doing it manually is also an option,
    but it could be a costly one.
  prefs: []
  type: TYPE_NORMAL
- en: A case study in data preparation
  prefs: []
  type: TYPE_NORMAL
- en: 'The data set we are going to use for our fine-tuning session presents a good
    example of the challenges we may encounter when building even small data sets.
    For our fine-tuning session, we’ll be using a previously created data set I built
    that can be found on Hugging Face at [https://mng.bz/4pXa](https://mng.bz/4pXa).
    The data set is a JSONL-formatted document that contains parsed sections of the
    Java portion of restful-booker-platform (RBP), paired with generated instructions,
    a sample of which is provided here (line breaks have been added for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This snippet will give us everything we need data-wise for tuning, but before
    we begin, let’s explore how it was made. Consider a class like the following and
    ask yourself how would you break this code up for fine-tuning in a logical fashion?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Would you fine-tune on a file-by-file basis, a line-by-line basis, or some
    other way? On a first attempt at testing out fine-tuning of a model on the RBP
    code base, I opted for the line-by-line approach. It created a script that would
    iterate through each file in a project and add each line of the file into its
    row resulting in a table of data that looked similar to the following example
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The problem with this approach was that, although it was easy to parse and store
    the data, I ended up with entries that were lacking context, meaning I was tuning
    with entries such as `}` or `@Autowired`. These don’t provide a lot of context
    or detail about the RBP project, and they raised another question. What type of
    instruction can be paired with these types of entries? If you recall, during instruction-based
    fine-tuning, we send an instruction (sometimes with additional input data) and
    then compare the response with our expected output. The type of instruction we
    would add to an entry such as `}` would contain no hints toward our context and
    potentially contribute to a fine-tuned model that responds in unusual and undesired
    ways. This is exactly what happened when I attempted to tune a model based on
    line-by-line material.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, what I opted for (and what can be found in the dataset) is an approach
    that breaks down the code into logical sections. This means that rather than slicing
    things line by line, a file would be sliced up based on different attributes within
    a Java class. For example, a selection of slices from the class I shared earlier
    would look like the following in an example table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Instead of each entry in the data set being a line of code, each entry might
    contain details of how the class was declared, what variables are being declared
    in the class, each method in the class, and its containing code. The goal was
    to keep it detailed enough so the fine-tune would result in a model that had a
    greater awareness of the code base, but not so granular to lose the context completely.
    The result was a more successful tune, but the process of parsing became more
    complex. It required the creation of additional code that would iterate through
    each file, using JavaParser ([https://javaparser.org/](https://javaparser.org/))
    to ingest the code, build a semantic tree, and then query said tree to extract
    the information required for the data set (the code of which can be found at [https://mng.bz/QVpw](https://mng.bz/QVpw)).
  prefs: []
  type: TYPE_NORMAL
- en: This example is a very basic one when it comes to preparing a data set for tuning
    (or training). However, after reflecting on it, it becomes clear that organizing
    and preparing even a simple data set from scratch has its complexities and challenges.
    The raw data for this data set was easily parsable with the right tools, but how
    do we manage data that is diverse in structure or has no discernible structure
    in the first place? This exploration into data sets highlights that the identification
    and creation of a data set is a complicated process. How we structure our data
    and what we put in it is critical for the success of fine-tuned models, and it’s
    where most of the work of fine-tuning and experimenting with LLMs can be found.
    Therefore, it’s important to have the necessary processes and tooling in place
    so that we can rapidly experiment with different data sets to see how they impact
    the result of a fine-tuned model.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Preprocessing and setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our data set in place, we next need to preprocess our data before tuning
    and get our tuning tools in place. We’ll get to tool setup soon, but first, to
    understand the preprocessing activities for fine-tuning, we need to skip ahead
    a little and talk about what happens during a fine-tuning session. Perhaps not
    surprisingly, given the size of data sets, a fine-tuning session consists of a
    specific loop, visualized in figure 12.2, which is run multiple times during fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Stepping through the visualization, we start with our data set. Assuming it’s
    structured in a way similar to the `_Alpaca_ dataset` we looked at earlier (it
    contains a column of instructions, inputs, and outputs), the data stored in the
    instruction and input columns are added to a prompt. That prompt is then sent
    to the model we are fine-tuning and a response is sent back from the model. We
    then compare the response from the model with the output stored in our data set
    to determine the *sentiment*. The sentiment indicates how closely aligned the
    response is to our expected output. The sentiment score is then used to inform
    what tweaks need to be made to the model’s parameters to fine-tune it toward how
    we want our model to respond. If the sentiment score indicates it’s responding
    desirably, then changes will be minimal. On the other hand, if the sentiment score
    indicates an undesirable response, then bigger changes will be made.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH12_F02_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 A visualization of what happens during fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: This whole process is done programmatically using different tools and is run
    multiple times across each entry in a data set. The data set itself is also usually
    iterated through multiple times known as an *epoch*. It’s through iterating across
    multiple epochs in the fine-tuning process that the model is tuned toward how
    we want it to respond. This approach to tuning is known as instruction-based fine-tuning,
    and it’s important to understand how it works before we execute our fine-tuning
    because there are steps we need to take before the tuning can begin. First, we
    need to design the type of prompt we want to send to our model. Second, we need
    to determine how we are going to codify our prompt so that our model can read
    it. Similar to curating our data set, the choices we make for these two steps
    can also have a dramatic effect on the result of our fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt design
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we know the format of our data set, we need to create an instructional
    prompt that works with the data within the data set and any additional instructions
    we want to add. For example, consider these two instruction prompts from the deeplearning.ai
    course *Finetuning Large Language Models* ([https://mng.bz/XV9G](https://mng.bz/XV9G)).
    First is a prompt that takes an instruction and an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction that describes a task, paired with an input that
    provides further context. Write a response that appropriately completes the request.###
    Instruction:{instruction}### Input:{input}### Response: |'
  prefs: []
  type: TYPE_TB
- en: 'The second is a prompt that contains just an instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:{instruction}### Response: |'
  prefs: []
  type: TYPE_TB
- en: 'Notice how each prompt contains a combination of static, instructional text
    that contextualizes what information is being sent, and then tags such as `{instruction}`
    to inject data from the data set. Based on the RBP data set we want to use, we
    could configure our prompt for fine-tuning like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction, delimited by three hashes, that asks a question
    about the restful booker platform code base. Respond with the necessary code to
    answer the question. Check that the code compiles correctly before outputting
    it.###{instruction}### |'
  prefs: []
  type: TYPE_TB
- en: 'The prompt follows some of the prompting tactics we’ve explored in earlier
    chapters. We can use those to help us instruct the model clearly in what to expect
    in our prompt and what we want to see it responds with. To help us better understand
    the fine-tuning loop, let’s imagine we have the following entry in a data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: During fine-tuning, the instruction portion of this data set would be injected
    into the prompt to create the following prompt
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction, delimited by three hashes, that asks a question
    about the restful booker platform code base. Respond with the necessary code to
    answer the question. Check that the code compiles correctly before outputting
    it.###How does the method i`nitialiseMocks` work for `BrandingServiceTest`?###
    |'
  prefs: []
  type: TYPE_TB
- en: 'This might result in the model responding with a code example written like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, our output in our data set is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the sentiment score between the two sets of data is a middling
    one because, although the response is code and it has some similarities to our
    expected output in solution, the code isn’t entirely the same. This sentiment
    score would then be factored into what tweaks need to be made to the model’s parameters
    so that when this specific row in our data set comes around again, the results
    are more closely aligned. The prompt template we use affects the results of a
    fine-tuned model, and the instructions we add make a difference. However, we need
    to be mindful that what we add to a prompt template doesn’t affect just the result
    of fine-tuning but also has an impact on what is sent to the model in the first
    place. This brings us to how we turn a text-based prompt into a language that
    a model understands.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs: []
  type: TYPE_NORMAL
- en: A *token* is a numerical representation of a word, phrase, or character. We
    covered the topic of tokens in chapter 10\. So why do we need to be aware of tokenization
    as part of the fine-tuning process? First, there are many different tokenizers
    available that can be utilized during data preprocessing that will tokenize the
    text in different ways. The type of model we are using will influence the type
    of tokenizer. Choosing one that doesn’t align with the model we’re tuning would
    result in our prompts being converted into token identifiers that don’t align
    with the parameters that exist inside the model we’re tuning. Roughly speaking,
    it would be like being taught a course by a teacher who was speaking in a different
    or completely made-up language.
  prefs: []
  type: TYPE_NORMAL
- en: The second reason, which relates to our fine-tuning prompt and our data set,
    is context length. Context length is the total amount of tokens a model can process
    at once. This is important because if we create a prompt that has a large number
    of tokens within it or attempt to tune using data containing a large number of
    tokens within each entry, then our prompts risk breaching the context length,
    meaning our prompt will be truncated. Every token that is over the context length
    limit would simply be discarded or ignored, and the result would be a model being
    fine-tuned on partially complete prompts, which might create unexpected or undesired
    side effects.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to keep our context length in mind both when curating our
    dataset and designing our prompt for tuning. This might mean removing any entries
    from our data set that have the potential to overflow our context length, writing
    a prompt that has clear instructions but doesn’t overflow the token count, or
    looking for a new model that contains a larger context length.
  prefs: []
  type: TYPE_NORMAL
- en: Tooling and hardware
  prefs: []
  type: TYPE_NORMAL
- en: Given the many steps required for processing data and executing the fine-tuning
    process, necessary tooling needs to be in place to execute each phase. Fortunately,
    tooling for fine-tuning has progressed greatly recently. Originally, it would
    require extensive experience with tools such as Python and libraries such as PyTorch,
    Tensorflow, or Keras. And though these tools are designed to be as easy to use
    as possible, the learning curve could be quite steep and require us to build our
    fine-tuning frameworks from the ground up. If we are comfortable with this type
    of approach or are working with those who have experience with these types of
    tools, then it’s worthwhile using them. However, as interest has grown around
    fine-tuning, new tooling built on the aforementioned tools has begun to appear
    to make fine-tuning more accessible. Frameworks such as Axolotl and platforms
    such as Hugging Face let us set up fine-tuning quickly in a way that requires
    minimal tool development. The tradeoff is that these frameworks are either opinionated,
    for example selecting what tokenizers we should use, or they come at a cost.
  prefs: []
  type: TYPE_NORMAL
- en: It is not only the tooling around AI tuning that has seen growth, but also the
    infrastructure that supports it. Training models is a hardware-intensive exercise
    and requires access to graphical processing units (GPUs). This means either purchasing
    hardware that has a generous amount of CPU, RAM, and GPU for tuning, or provisioning
    computing from cloud providers. The latter is the popular option for many teams
    and a massive area of growth as it keeps the costs down for hardware requirements,
    ensuring access to newer, updated GPUs. Unsurprisingly, the big cloud computing
    companies such as Google, Microsoft, and Amazon all offer access to dedicated
    services that are designed specifically for tuning and hosting of LLMs. But some
    alternatives have begun to appear, such as RunPod, Latitude.sh, and Lambda Labs,
    which are specialist GPU cloud providers. These are options that can be used in
    conjunction with the tools we select for fine-tuning, but some services provide
    both the frameworks for fine-tuning and the computing resources to run the tuning
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: The market landscape for what we can use for and where we can run our fine-tuning
    is a fast-growing space. But what it highlights is that research is required to
    determine what tooling and infrastructure best suits the experience of our team
    and the type of budget we have available for tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our fine-tuning tooling
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools available for fine-tuning that offer different levels of
    control of the tuning process, PyTorch being a popular choice. But as mentioned
    before, there is a learning curve to setting and using these tools. If we were
    working in a context in which we wanted full control of our prompts, tokenizers,
    and tuning tools, we might choose these more granular tools. But for new starters,
    like us, who are happy using tools that trade opinions on approaches for ease
    of use, we can once again look to the AI open source community. Therefore, for
    our tuning session, we’ll be running our fine-tuning with Axolotl, a tool *designed
    to streamline the fine-tuning of various AI models, offering support for multiple
    configurations and architectures.*
  prefs: []
  type: TYPE_NORMAL
- en: We can think of Axolotl as a framework for fine-tuning that contains all the
    necessary tools and processes to carry out our tuning. That means for our fine-tuning
    session, the prompting approach and tokenizer have been taken care of for us,
    allowing us to get into our fine-tuning quickly and without a massive learning
    curve.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware requirements for using Axolotl
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, it’s important to note that to carry out a fine-tuning session,
    you will need a system that has access to a GPU. If, however, you don’t have access
    to a GPU, there are cost-effective cloud platforms designed to support AI fine-tuning.
    Axolotl’s ReadMe contains links to two providers: RunPod and Latitude.sh.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As someone who doesn’t have access to a GPU, I found RunPod easy to get set
    up with and reasonably priced to run multiple training sessions for less than
    $10\. If this is the approach you want to take, here are some steps to follow
    to get set up:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an account and add credit to it via [https://www.runpod.io/console/user/billing](https://www.runpod.io/console/user/billing).
    I have found the minimum transaction of $10 to be enough.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Head to GPU cloud [https://www.runpod.io/console/gpu-cloud](https://www.runpod.io/console/gpu-cloud),
    click `Choose Template` at the top of the page, find the `winglian/axolotl-runpod:main-latest`
    Docker image, and select it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, choose a pod to deploy. Depending on the time of day and demand, you will
    see which ones can be deployed and which can’t. At the time of writing, a 1x RTX
    4090 will suffice for our tuning exercise. However, if we want the tuning to go
    faster, we can choose more GPUs or a larger box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click deploy and go through the setup wizard to fire off the creation of your
    pod. Head to [https://www.runpod.io/console/pods](https://www.runpod.io/console/pods)
    and wait for your pod to deploy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the pod is deployed, click on connect to reveal the details to SSH into
    your pod (This will require you to add an SSH public key before connecting, which
    can be done here: [https://www.runpod.io/console/user/settings](https://www.runpod.io/console/user/settings).)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you are logged into the pod, you’ll find Axolotl is installed and ready
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by setting up Axolotl on our machine of choice (if you have chosen
    the RunPod option this can be skipped). The documentation and code for Axolotl
    can be found at [https://mng.bz/yoRG](https://mng.bz/yoRG), and it contains comprehensive
    instructions on how to install the application, offering the option of installing
    it directly on our machine or via Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Working with fine-tuning tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once Axolotl is set up, we can begin configuring our session. As mentioned before,
    we’ll be using the RBP data set, which can be found on Hugging Face ([https://mng.bz/M1M7](https://mng.bz/M1M7)).
    For our model, we’ll also be using a version of Meta’s Llama-2 model, which contains
    7 billion parameters and a context window of 4k. Much like the prompts and tokenizer,
    the model settings have been taken care of in an example file that can be found
    inside the Axolotl project `examples/llama-2/lora.yml.` However, to train the
    model on our data set, we need to update the `dataset.path` in the YAML file to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'with `path` dictating where to find the data set on Hugging Face (that is,
    where it will be downloaded from) and `type` setting out which template prompt
    we want to use. Looking at the top of the YAML file, we can also see references
    to the model and the tokenizer that will be used. Again, these can be modified
    if we want to experiment with other approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Several other settings can be found in the file and are beyond the scope of
    this chapter, but there are two we do want to highlight: `sample_packing` and
    `num_epochs`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As for `sample_packing,` we set it to `false` because the data set isn’t large
    enough for splitting into training and testing sets (more on that later). `num_epochs`
    determines how many times we want to iterate through our data set. The default
    is `4`, meaning the fine-tuning process will loop through the whole data set four
    times before it completes. With these changes made to the YAML file, we can save,
    quit, and begin our fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 Setting off a fine-tuning run
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With our configuration in place, we can begin fine-tuning. To do this, we’ll
    be following the steps found in Axolotl’s ReadMe. To start, we trigger a preprocessing
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preprocessing step downloads the data set and runs it through the tokenizer,
    translating the data from text to tokens, ready for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: What is LORA?
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that the YAML file we’ve been configuring and are now using
    to start our fine-tuning session is named lora.yml. LORA is an approach to fine-tuning
    in which instead of attempting to directly tune the parameters within a model,
    a smaller subset of parameters that approximate the parameters of the model are
    created and fine-tuned, creating a LORA adapter. This means that when we deploy
    the model once it’s fine-tuned, the model is loaded with the LORA adapter inside
    it to give us the tuned behaviors we are looking for. It has become popular because
    it speeds up the fine-tuning process and allows communities and teams to share
    their adapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the preprocessing is complete, we are ready to start the fine-tuning process.
    Keep in mind that depending on the hardware being used, this process can take
    anywhere from 30 minutes to 4 hours or more, so pick a time in which the fine-tune
    can be left to run while you work on other tasks. To trigger the fine-tuning,
    we run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will pull in the YAML file that we’ve edited and kick-start the fine-tuning
    process. As the tuning begins, we’ll start to see details on its progress of the
    tune like the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Each entry in the console details the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss*—This score indicates the alignment between the expected output in our
    data set and the output of the model. The lower the score, the better aligned
    the expected and actual responses. In this example, the loss score is relatively
    high because it was taken at the start of a tuning. As the tuning progresses,
    we would hope to see the loss score reduce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning rate*—This is a numerical representation of the step change made
    to the parameters in the model. Smaller steps mean more gradual changes in tuning.
    How big a step is made is determined by the sentiment score, as well as the learning
    rate hyper-parameter. In the context of AI training, a hyper-parameter is a configuration
    option that we can set before the tuning begins, which impacts the outcome of
    training or tuning. So, in the case of learning rate, we can increase the step
    range, which can result in more dramatic tunes to a model. This again may or may
    not result in a more optimal tune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Epoch*—Earlier, we learned how we can iterate through a data set multiple
    times during fine-tuning and that each iteration is known as an epoch. The epoch
    value shown in the console output simply informs us how far through a given epoch
    we are.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These metrics are useful to give us an indication of the quality and the progress
    of a fine-tuning session. Depending on the size of the data set, the model and
    hardware supporting the tune will determine how long a tune might take. However,
    it’s not uncommon for a tune to take multiple hours, given the volume of work
    required for tuning. This is why more experienced model tuners will set up processes
    and tooling so that multiple models can be tuned at once for comparison once the
    tuning is complete.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.5 Testing the results of a fine-tune
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once our fine-tuning has been completed, we’ll want to check whether the changes
    we’ve made align with the goal that we set at the start of the fine-tuning process.
    If you recall, during the fine-tuning process, an instruction is sent to a model,
    and it returns an output. Sentiment analysis then determines how closely aligned
    the model’s output and our expected output are, which informs what tuning to the
    model’s parameters takes place. Consequently, the parameters within a model should
    now be more biased toward our context. Therefore, to test whether a model has
    been successfully tuned, we want to check what happens when we ask the model for
    new instructions that differ from the ones it was tuned on. We can do this in
    one of the two ways: inference and/or human validation.'
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs: []
  type: TYPE_NORMAL
- en: Given the numerous parameters within a model and the options in instructions
    to send and outputs to receive, inference employs an automated approach to testing
    the output of a model. Inference works very similar to fine-tuning. We either
    take a slice out of a larger data set or employ a new data set that follows the
    same structure as the data set we used for fine-tuning that contains instructions
    and outputs that are different to the ones in our original tuning data. Then we
    send each of these sets of instructions to a model, capture the response, and
    then use sentiment analysis to compare what we expect the model to respond against
    what it responded. (The key difference between tuning and inference comes after
    the sentiment analysis. Tuning will make changes to the model, whereas in case
    of inference, the model is left alone). If the returned sentiment score is high,
    we can assume the model has been tuned in a way that meets our goals. If it doesn’t,
    then we can start to consider what our next steps are for a future fine-tuning
    session.
  prefs: []
  type: TYPE_NORMAL
- en: Human validation
  prefs: []
  type: TYPE_NORMAL
- en: Although sentiment analysis is useful, it is based on mathematical models to
    determine alignment. Therefore, it’s also sensible to explore the outputs of a
    model manually, through human validation. This might be through using prompts
    saved in the inference data set and evaluating responses, or by testing out different
    responses by generating new prompts to see how the model reacts. To get the same
    level of scope and coverage as with the inference testing would be too expensive
    for a team, but it can provide a more human perspective that can spot discrepancies
    and/or hallucinations that inference wouldn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what we learn from both, the automated processes of inference and
    our experimentation will inform our next decisions. Perhaps the fine-tuning resulted
    in a model that is acceptable, and we can release it for wider use. However, we
    will often likely conclude that the resulting tuned model isn’t right for us.
    This would trigger further analysis to determine what our next steps are. Perhaps
    our data set needs changing, or we want to modify our prompt further, or perhaps
    the tokenizer could be replaced. Regardless of the choice—again, a decision-making
    process that is beyond the scope of this chapter—this highlights that fine-tuning
    projects require to experiment many times over to discover an optimal result.
    This is why teams that have a mature process around fine-tuning (or training in
    general) will run multiple fine-tuning experiments at once, tuning multiple models
    at once, with slightly different parameters to compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: Testing out our fine-tuning session
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our fine-tuned model, although we don’t have enough data to run
    an inference testing session, we can launch our model to test it out manually.
    To do this, we run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With this command, we are loading up the Llama-2 model and loading in the LORA
    adapter we have created as part of our tuning. The command `--gradio` allows us
    to host a user interface for our model using the gradio library ([https://www.gradio.app/](https://www.gradio.app/))
    so that we can start to test out our model via a web browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our model running, we can begin to check the results of our fine-tuning
    session. First, we can check the tuning by selecting an instruction and output
    set from our data set such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Instruction:How does the method `initialiseMocks` work for `BrandingServiceTest`?Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the prompt from the tuning session, we can send the following request
    to our tuned model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:How does the method `initialiseMocks` work
    for `BrandingServiceTest`?### Response: |'
  prefs: []
  type: TYPE_TB
- en: 'Upon sending this, we would receive a response such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Below is an instruction that describes a task. Write a response that appropriately
    completes the request.### Instruction:How does the method `initialiseMocks` work
    for `BrandingServiceTest`?### Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the model has been tuned to the point that it is returning a
    strongly aligned response based on the instructions we’ve sent. Now we can turn
    our attention to how the model copes with new untested prompts, such as this basic,
    example prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What are the annotations found in the `BrandingResult` java class? |'
  prefs: []
  type: TYPE_TB
- en: 'Sending this to an example tuned model returns a response like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What are the annotations found in the `BrandingResult` java class?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing this response to the original code on which the model was tuned (which
    can be found at [https://mng.bz/aVlz](https://mng.bz/aVlz)), we can see that the
    model has demonstrated some success in listing aspects of the `BrandingResult`
    class such as the variables used and the getter/setter methods. However, it is
    also missing details such as the class constructor, and it got the names of variables
    wrong (although it is at least consistent across the code). It could also be argued
    that the prompt wasn’t answered correctly as we were requesting details on annotations
    and not on the class as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: So, in conclusion, we have seen some success with this tuning session, but more
    work is required. The tuning process has rebalanced the parameters within the
    model in a way that our context has become more dominant within it. However, the
    missing items and incorrect details mean that further tweaking is required for
    the tuning process. Perhaps we could look to improve the quality of the instructions
    in the data set or reconsider the prompt we use for tuning. Equally, we could
    look at more technical aspects of the tuning, such as choosing a model with a
    larger parameter count, or tweak hyper-parameters such as the amount of epochs
    we use for training or the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.6 Lessons learned with fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter gave us a taste of how the fine-tuning process works. At first,
    it may seem like an overwhelming activity. But although there is specific tooling
    and terminology to understand, by taking the fine-tuning process step by step,
    we can tackle each challenge as it comes. Ultimately, fine-tuning is very much
    about experimentation. What data we use, models we tune, tooling we employ, and
    hyper-parameters we set, they all affect the result.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the cost of experimentation is not something that can
    be dismissed. Teams that want to carry out fine-tuning sessions require substantial
    financial backing for resources and experience. But as both private companies
    and the open source community grow, fine-tuning will become more accessible, and
    the price of hardware will likely decrease, making this a growing space in organizations
    and a challenge for teams to deliver high-quality models that can assist us and
    our organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning is the process of training a pre-existing model further, which is
    sometimes known as a foundational model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fine-tuning process involves multiple steps such as goal setting, data preparation,
    processing, tuning, and testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting clear goals around what we want a fine-tuned LLM to do informs how we
    approach the fine-tuning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned models require specifying and preparation of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sets massively influence the results of a fine-tuned model. This means
    finding data relevant to our goals and formatting it in a way that helps maximize
    the output of a model after fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning relies on repeatedly sending prompts embedded with training data
    to get a response that we want to bias toward aligning with an expected output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models need prompts to be converted into a machine-readable language. This is
    achieved through the tokenization process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization is the process in which data is sliced into smaller tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models have a context length, which is the maximum number of tokens it can process
    at once. Send too many tokens at once, and some will be discarded, affecting the
    fine-tuning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When fine-tuning, we can either build our frameworks, which require experience,
    or utilize existing frameworks, which are opinionated and/or cost money to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axolotl is a great framework for fine-tuning that is accessible to those with
    limited experience and looking to get started.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing of fine-tuned models can be done in an automated fashion, using inference,
    or manually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning is becoming increasingly accessible to teams for use in the AI assistant
    tooling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
