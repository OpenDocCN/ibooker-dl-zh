<html><head></head><body>
  <div class="readable-text" id="p1"> &#13;
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span> </span> <span class="chapter-title-text">Unsupervised learning for text data</span></h1> &#13;
  </div> &#13;
  <div class="introduction-summary"> &#13;
   <h3 class="introduction-header">This chapter covers</h3> &#13;
   <ul> &#13;
    <li class="readable-text" id="p2">Text data analysis: use cases and challenges</li> &#13;
    <li class="readable-text" id="p3">Preprocessing and cleaning text data</li> &#13;
    <li class="readable-text" id="p4">Vector representation methods for text data</li> &#13;
    <li class="readable-text" id="p5">Sentiment analysis and text clustering using Python</li> &#13;
    <li class="readable-text" id="p6">Generative AI applications for text data </li> &#13;
   </ul> &#13;
  </div> &#13;
  <div class="readable-text" id="p7"> &#13;
   <blockquote>&#13;
    <div>&#13;
     Everybody smiles in the same language. &#13;
     <div class=" quote-cite">&#13;
       —George Carlin &#13;
     </div>&#13;
    </div>&#13;
   </blockquote> &#13;
  </div> &#13;
  <div class="readable-text" id="p8"> &#13;
   <p>Our world has so many languages. These languages are the most common medium of communication to express our thoughts and emotions. These words can be written into text. In this chapter, we explore the sorts of analysis we can do on text data. Text data falls under unstructured data and carries a lot of useful information and hence is a useful source of insights for businesses. We use natural language processing (NLP) to analyze the text data.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p9"> &#13;
   <p>At the same time, to analyze text data, we have to make the data analysis-ready. Or, in very simple terms, since our algorithms and processors can only understand numbers, we have to represent the text data in numbers or <em>vectors</em>. We will explore all these steps in this chapter. Text data holds the key to quite a few important use cases, such as sentiment analysis, document categorization, and language translation, to name a few. We will cover the use cases using a case study and develop a Python solution on the same.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p10"> &#13;
   <p>The chapter starts with defining text data, sources of text data, and various use cases of text data. We will then move on to the steps and processes to clean and handle the text data. We cover the concepts of NLP, mathematical foundations, and methods to represent text data into vectors. We will create Python codes for the use cases. Toward the end, we share a case study on text data. Finally, we will also look into the generative AI-based (GenAI) solutions. We have not covered GenAI concepts yet in the book, as they are in part 3. But here we introduce the concepts in the light of text data. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p11"> &#13;
   <p>Welcome to the seventh chapter, and all the very best!</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p12"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.1</span> Technical toolkit</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p13"> &#13;
   <p>We will continue to use the same version of Python and Jupyter Notebook as we have used so far. The codes and datasets used in this chapter have been checked in at the same GitHub location. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p14"> &#13;
   <p>You need to install the following Python libraries for this chapter: <code>re</code>, <code>string</code>, <code>nltk</code>, <code>lxml</code>, <code>requests</code>, <code>pandas</code>, <code>textblob</code>, <code>matplotlib</code>, <code>sys</code>, <code>sklearn</code>, <code>scikitlearn</code>, and <code>warnings</code>. Along with these, you will need <code>numpy</code> and <code>pandas</code>. With libraries, we can use the algorithms very quickly. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p15"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.2</span> Text data is everywhere</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p16"> &#13;
   <p>Recall in the very first chapter of the book we explored structured and unstructured datasets. Unstructured data can be text, audio, image, or video. Examples of unstructured data and their respective sources are given in figure 7.1, where we explain the primary types of unstructured data—text, images, audio, and video—along with examples. The focus of this chapter is on text data.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p17">  &#13;
   <img alt="figure" src="../Images/CH07_F01_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.1</span> Unstructured data can be text, images, audio, or video. We deal with text data in this chapter. This list is not exhaustive.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p18"> &#13;
   <p>Language is perhaps our greatest tool for communication. When in written form, this becomes text data. Today, thanks to widely accessible computers and smartphones, text is everywhere. It is generated by writing blogs and social media posts, tweets, comments, stories, reviews, chats, and comments, to name a few. Text data is generally much more direct than images and can be emotionally expressive. It is useful for businesses to unlock the potential of text data and derive insights from it. They can understand customers better, explore the business processes, and gauge the quality of services offered.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p19"> &#13;
   <p>Have you ever reviewed a product or a service on Amazon? You award stars to a product; at the same time, you can also input free text. Go to Amazon and look at some of the reviews. You might find some reviews have a good amount of text as the feedback. This text is useful for the product/service providers to enhance their offerings. Also, you might have participated in a few surveys that ask you to share your feedback. Moreover, with the advent of Alexa, Siri, Cortona, etc., the voice command acts as an interface between humans and machines—which is again a rich source of data. Even the customer calls we make to a call center can be transcribed so that they become a source of text data. These calls can be recorded, and using speech-to-text conversion, we can generate a huge amount of text data.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p20"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.3</span> Use cases of text data</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p21"> &#13;
   <p>Not all the use cases discussed in this section can implement unsupervised learning. Some require supervised learning too. Nevertheless, for your knowledge, we share both types of use cases, based on supervised learning and unsupervised learning:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p22"> <em>Sentiment analysis</em><em> </em>—You might have participated in surveys or given your feedback on products/surveys. These surveys generate tons of text data. That text data can be analyzed, and we can determine whether the sentiment in the review is positive or negative. In simple words, sentiment analysis gauges the positivity or negativity of the text data. Hence, we can see the sentiment about a product or service in the minds of the customers. We can use both supervised and unsupervised learning for sentiment analysis. </li> &#13;
   <li class="readable-text" id="p23"> <em>News categorization or document categorization</em><em> </em>—Look at the Google News web page and you will find that each news item has been categorized to sports, politics, science, business, or another category. Incoming news is classified based on the content of the news, which is the actual text. Imagine the thousands of documents that are sorted in this manner. In this use case, it is clear that machine learning is ideal, given the unfeasible amount of time and effort that would be required to sort such items manually. Supervised learning solutions work well for such problems. </li> &#13;
   <li class="readable-text" id="p24"> <em>Language translation</em><em> </em>—Translation of text from one language to another is a very interesting use case. Using NLP, we can translate between languages. Language translation is very tricky, as different languages have different grammatical rules. Generally, deep learning–based solutions are the best fit for language translation. </li> &#13;
   <li class="readable-text" id="p25"> <em>Spam filtering</em><em> </em>—Email spam filters can be set up using NLP and supervised machine learning. A supervised learning algorithm can analyze incoming mail parameters and give a prediction if that email belongs to a spam folder or not. The prediction can be based on various parameters like sender email ID, subject line, body of the mail, attachments, time of mail, etc. Generally, supervised learning algorithms are used here. </li> &#13;
   <li class="readable-text" id="p26"> <em>Part-of-speech tagging</em><em> </em>—This is one of the popular use cases. It means that we can distinguish the nouns, adjectives, verbs, adverbs, etc., in a sentence. Named-entity recognition is also one of the famous applications of NLP. It involves identifying a person, place, organization, time, or number in a sentence. For example, John lives in London and works for Google. Named-entity recognition can generate understanding like [John]<sub>Person</sub> lives in [London]<sub>Location</sub> and works for [Google]<sub>organization</sub>. </li> &#13;
   <li class="readable-text" id="p27"> <em>Sentence generation, captioning the images, speech-to-text or text-to-speech tasks, and handwriting recognition</em><em> </em>—These are a few other significant and popular use cases. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p28"> &#13;
   <p>The use cases listed here are not exhaustive. There are tons of other use cases that can be implemented using NLP. NLP is a very popular research field too. We share some significant papers at the end of the chapter. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p29"> &#13;
   <p>You might have also heard about large language models (LLMs) like ChatGPT, Bard, and Claude. They are algorithms that process natural language inputs and predict the next word based on what they have already seen. With GenAI in the picture, a lot of the use cases can be solved by simply calling the API. ChatGPT can communicate like a human with memory and serves as customer support for many services. LLMs can summarize hundreds of pdf documents. You can even create applications that can be used for getting answers from multiple documents and websites. Certainly, GenAI has enhanced the power here. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p30"> &#13;
   <p>While text data is very important, at the same time it is quite difficult to analyze. Remember, our computers and processors understand only numbers. So the text needs to be represented as numbers so we can perform mathematical and statistical calculations on it. Before diving into the preparation of text data, we cover some of the challenges we face while working on text datasets.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p31"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.4</span> Challenges with text data</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p32"> &#13;
   <p>Text is a difficult data type to work with. There are a large number of permutations to express the same thought. For example, I might ask, “Hey buddy, what is your age?” or “Hello there, may I know how old are you?”—they mean the same, right? The answer to both the questions is the same, and it is quite easy for humans to decipher, but it can be a daunting task for a machine. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p33"> &#13;
   <p>Some of the most common challenges we face in this area are as follows:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p34"> Text data can be complex to handle. There can be a lot of junk characters like $^%*&amp; present in the text. </li> &#13;
   <li class="readable-text" id="p35"> With the advent of modern communications, we have started to use short forms of words; for example, “u” can be used for “you,” “brb” for “be right back,” and so on. Additionally, the challenge is where the same word might mean something different to different people, or misspelling a single letter can change the complete meaning of the word. </li> &#13;
   <li class="readable-text" id="p36"> Language is changing, unbounded, and ever-evolving. It changes every day and new words are added to the language. If you do a simple Google search, you will find that quite a few words are added to the dictionary each year. </li> &#13;
   <li class="readable-text" id="p37"> The world has close to 6,500 languages, and each one carries its own unique characteristics. Each and every one completes our world. Each language follows its own rules and grammar, which are unique in usage and pattern. Even the writing can be different: some are written left to right, some right to left, and some even vertically. The same emotion might take fewer or more words in different languages. </li> &#13;
   <li class="readable-text buletless-item" id="p38"> The meaning of a word is dependent on the context. A word can be both an adjective and a noun, depending on the context. Consider these examples: &#13;
    <ul> &#13;
     <li> “This book is a must-read” and “Please book a room for me.” </li> &#13;
     <li> “Tommy” can be a name, but when used as “Tommy Hilfiger” its usage is completely changed. </li> &#13;
     <li> “Apple” is both a fruit and a company. </li> &#13;
     <li> “April” is a month and can be a name too. </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text" id="p39"> Look at one more example: “Mark traveled from the UK to France and is working with John over there. He misses his friends.” Humans can easily understand that “he” in the second sentence is Mark and not John, which might not be that simple for a machine. </li> &#13;
   <li class="readable-text" id="p40"> There can be many synonyms for the same word, like “good” can be replaced by “positive,” “wonderful,” “superb,” or “exceptional” in different scenarios. Words like “studying,” “studious,” and “studies” are related to the same root word “study.” </li> &#13;
   <li class="readable-text" id="p41"> The size of text data can be daunting too. Managing a text dataset, storing it, cleaning it, and refreshing it is a herculean task. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p42"> &#13;
   <p>Like any other machine learning project, text analytics follows the principles of machine learning, albeit the precise process is slightly different. Recall in chapter 1 we examined the process of a machine learning project, as shown in figure 7.2. You are advised to refresh your memory on the process from chapter 1 if needed. </p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p43">  &#13;
   <img alt="figure" src="../Images/CH07_F02_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.2</span> The overall steps in a data science project are the same for text data. The preprocessing of text data is very different from the structured dataset.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p44"> &#13;
   <p>Defining the business problem, data collection and monitoring, etc., remain the same. The major difference is in the processing of the text, which involves data cleaning, creation of features, representation of text data, etc. We will cover this in the next section.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p45"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 7.1</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p46"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p47"> Note the three most effective use cases for the text data. </li> &#13;
    <li class="readable-text" id="p48"> Why is working on text data so tedious? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p49"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.5</span> Preprocessing the text data</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p50"> &#13;
   <p>Text data, like any other data source, can be messy and noisy. We clean some of it in the data discovery phase and a lot of it in the preprocessing phase. At the same time, we should extract the features from our dataset. Some of the steps in the cleaning process are common and can be implemented on most text datasets. Some text datasets might require a customized approach. We start with cleaning the raw text data.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p51"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.6</span> Data cleaning</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p52"> &#13;
   <p>As with any form of data analysis, ensuring good data quality is vital. The cleaner the text data, the better the analysis. At the same time, preprocessing is not a straightforward task but rather is complex and time-consuming.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p53"> &#13;
   <p>Text data must be cleaned as it contains a lot of junk characters, irrelevant words, noise and punctuation, URLs, etc. The primary ways of cleaning the text data are</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p54"> <em>Stopping word removal</em><em> </em>—Out of all the words that are used in any language, there are some words that are most common. Stop words are the most common words in a vocabulary that carry less importance than key words. Examples are “is,” “an,” “the,” “a,” “be,” “has,” “had,” “it,” etc. Once we remove the stop words from the text, the dimensions of the data are reduced and hence the complexity of the solution is reduced.  </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p55"> &#13;
   <p>We can define a customized list of stop words and remove them that way, or there are standard libraries to remove the stop words.</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p56"> &#13;
   <p>At the same time, it is imperative that we understand the context very well while removing the stop words. For example, if we ask a question “is it raining?” then the answer “it is” is a complete answer in itself. When we are working with solutions where contextual information is important, we do not remove stop words.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p57"> <em>Frequency-based removal of words</em><em> </em>—Sometimes you might wish to remove the words that are most common in your text or that are very unique. The process is to get the frequency of the words in the text and then set a threshold of frequency. We can remove the most common ones. Or maybe you wish to remove the ones that have occurred only once/twice in the entire dataset. Based on the requirements, you will decide. At the same time, we should be cautious and observe due diligence while removing the words. </li> &#13;
   <li class="readable-text" id="p58"> <em>Library-based cleaning</em><em> </em>—This is done when we wish to clean the data using a predefined and customized library. We can create a repository of words that we do not want in our text and iteratively remove them from the text data. This approach allows us flexibility to implement the cleaning of our own choice. </li> &#13;
   <li class="readable-text" id="p59"> <em>Junk or unwanted characters</em><em> </em>—Text data, particularly tweets, comments, etc., might contain a lot of URLs, hashtags, numbers, punctuations, social media mentions, special characters, etc. We might need to clean them from the text. At the same time, we should be careful as some words that are not important for one domain might be required for a different domain. If data has been scraped from websites or HTML/XML sources, we need to get rid of all the HTML entities, punctuations, nonalphabet characters, and so on.  </li> &#13;
  </ul> &#13;
  <div class="readable-text print-book-callout" id="p60"> &#13;
   <p><span class="print-book-callout-head">TIP</span>  Always keep business context in mind while cleaning the text data.</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p61"> &#13;
   <p>As we know, a lot of new types of expressions have entered the language—for example, lol, hahahaha, brb, rofl, etc. These expressions are to be converted to their original meanings. Even emojis like :-), ;-), etc., should be converted to their original meanings.</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p62"> <em>Data encoding</em><em> </em>—There are a few data encodings available like ISO/IEC, UTF-8, etc. Generally, UTF-8 is the most popular one. But it is not a hard and fast rule to always use UTF-8 only. </li> &#13;
   <li class="readable-text" id="p63"> <em>Lexicon normalization</em><em> </em>—Depending on the context and usage, the same word might get represented in different ways. During lexicon normalization, we clean such ambiguities. The basic idea is to reduce the word to its root form. Hence, words that are derived from each other can be mapped to the central word, provided they have the same core meaning. </li> &#13;
  </ul> &#13;
  <div class="readable-text list-body-item" id="p64"> &#13;
   <p>Figure 7.3 shows that the same word, “eat,” has been used in various forms. The root word is “eat,” but these different forms demonstrate the many different representations for “eat.”<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p65">  &#13;
   <img alt="figure" src="../Images/CH07_F03_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.3</span> “Ate,” “eaten,” “eats,” and “eating” all have the same root word: “eat.” Stemming and lemmatization can be used to get the root word. </h5>&#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p66"> &#13;
   <p>Here, we wish to map all these words like “eating,” “eaten,” etc., to their central word, “eat,” as they have the same core meaning. There are two primary methods to work on this:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class=" buletless-item" style="list-style-type: none;"> &#13;
    <ul> &#13;
     <li class="readable-text" id="p67"> Stemming is a basic rule-based approach for mapping a word to its core word. It removes “es,” “ing,” “ly,” “ed,” etc., from the end of the word. For example, studies will become “studi” and “studying” will become “study.” Being a rule-based approach, the output spellings might not always be accurate. </li> &#13;
     <li class="readable-text" id="p68"> Lemmatization is an organized approach that reduces words to their dictionary form. The <em>lemma</em> of a word is its dictionary or canonical form. For example, “eats,” “eating,” “eaten,” etc., all have the same root word “eat.” Lemmatization provides better results than stemming, but it takes more time. </li> &#13;
    </ul> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p69"> &#13;
   <p>These are only some of the methods to clean text data. These techniques will help, but business acumen is required to further make sense of the dataset. We will clean the text data using these approaches by developing a Python solution.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p70"> &#13;
   <p>Once the data is cleaned, we start with the representation of data so that it can be processed by machine learning algorithms, which is our next topic.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p71"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.7</span> Extracting features from the text dataset</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p72"> &#13;
   <p>We have explored the concepts and techniques to clean up messy text data. Now we have cleaned the data, and it is ready to be used. The next step is to represent this data in a format that can be understood by our algorithms. As we know, our algorithms can only understand numbers. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p73"> &#13;
   <p>A very simple technique to encode text data in a way that it can be useful for machine learning can be to simply perform one-hot encoding on our words and represent them in a matrix—but certainly not a scalable one if you have a complete document.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p74"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> One-hot encoding is covered in the appendix.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p75"> &#13;
   <p>The words can be first converted to lowercase and then sorted in alphabetical order. Then a numeric label can be assigned to them. Finally, words are converted to binary vectors. Let us understand using an example. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p76"> &#13;
   <p>If the text is “It is raining heavily,” we will use these steps:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p77"> Lowercase the words so the output will be “it is raining heavily.” </li> &#13;
   <li class="readable-text" id="p78"> Arrange them in alphabetical order. The result is heavily, is, it, raining. </li> &#13;
   <li class="readable-text" id="p79"> Assign place values to each word as heavily:0, is:1, it:2, raining:3. </li> &#13;
   <li class="readable-text" id="p80"> Transform them into binary vectors as shown here: </li> &#13;
  </ol> &#13;
  <div class="readable-text list-body-item" id="p81"> &#13;
   <p>[0. 0. 1. 0.] #it</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p82"> &#13;
   <p>[0. 1. 0. 0.] #is</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p83"> &#13;
   <p>[0. 0. 0. 1.] #raining</p> &#13;
  </div> &#13;
  <div class="readable-text list-body-item" id="p84"> &#13;
   <p>[1. 0. 0. 0.]] #heavily</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p85"> &#13;
   <p>As we can see, we are able to represent each of the words in binary vectors, where 0 or 1 is the representation for each of the words. Though this approach is quite intuitive and simple to comprehend, it is pragmatically not possible when we have a massive corpus and vocabulary. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p86"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Corpus refers to a collection of texts. It is Latin for “body.” It can be a body of written words or spoken words, which can be used to perform a linguistic analysis. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p87"> &#13;
   <p>Moreover, handling massive data sizes with so many dimensions will be computationally very expensive. The resulting matrix thus created will be very sparse too. Hence, we should consider other means and ways to represent our text data. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p88"> &#13;
   <p>There are better alternatives than one-hot encoding. These techniques focus on the frequency of the word or the context in which the word is being used. This scientific method of text representation is much more accurate, robust, and explanatory. There are multiple such techniques like term frequency-inverse document frequency (TF-IDF), the bag of words approach, etc. We discuss a few of these techniques later in the chapter. First, we need to examine the important concept of tokenization.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p89"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.8</span> Tokenization</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p90"> &#13;
   <p>Tokenization is simply breaking a text or a set of text into individual tokens. It is the building block of NLP. Look at the example in figure 7.4, where we have created individual tokens for each word of the sentence. Tokenization is an important step as it allows us to assign unique identifiers or tokens to each of the words. Once we have allocated each word a specific token, the analysis becomes less complex.</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p91">  &#13;
   <img alt="figure" src="../Images/CH07_F04_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.4</span> Tokenization can be used to break a sentence into different tokens of words. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p92"> &#13;
   <p>Tokens are usually used on individual words, but this is not always necessary. We are allowed to tokenize a word or the subwords or characters in a word. In the case of subwords, the same sentence can have subword tokens as rain-ing (i.e., rain and ing as separate subtokens). </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p93"> &#13;
   <p>If we wish to perform tokenization at a character level, it can be r-a-i-n-i-n-g. In fact, in the first step of the one-hot encoding approach discussed in the last section, tokenization was done on the words. Tokenization at a character level might not always be used.</p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p94"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> Tokenization is the building block for NPL solutions.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p95"> &#13;
   <p>Once we have obtained the tokens, the tokens can be used to prepare a vocabulary. A vocabulary is the set of unique tokens in the corpus. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p96"> &#13;
   <p>There are multiple libraries for tokenization. <em>Regexp</em> tokenization uses the given pattern arguments to match the tokens or separators between the tokens. <em>Whitespace</em> tokenization treats any sequence of whitespace characters as a separator. Then we have <em>blankline,</em> which uses a sequence of blank lines as a separator. Finally, <em>wordpunct</em> tokenizes by matching a sequence of alphabetic characters and a sequence of nonalphabetic and nonwhitespace characters. We will perform tokenization when we create Python solutions for our text data.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p97"> &#13;
   <p>Next, we will explore more methods to represent text data. The first such method is the bag of words (BOW) approach.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p98"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.9</span> BOW approach</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p99"> &#13;
   <p>As the name suggests, all the words in the corpus are used. In the BOW approach, the text data is tokenized for each word in the corpus, and then the respective frequency of each token is calculated. During this process, we disregard the grammar, the order, and the context of the word. We simply focus on the simplicity. Hence, we will represent each text (sentence or document) as a <em>bag of its own words</em>. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p100"> &#13;
   <p>In the BOW approach for the entire document, we define the vocabulary of the corpus as all the unique words present in the corpus. Please note we use all the unique words in the corpus. If we want, we can also set a threshold (i.e., the upper and lower limit for the frequency of the words to be selected). Once we have the unique words, each of the sentences can be represented by a vector of the same dimension as the base vocabulary vector. This vector representation contains the frequency of each word of the sentence in the vocabulary. It might sound complicated, but it is actually a straightforward approach.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p101"> &#13;
   <p>Let us understand this approach with an example. Let’s say that we have two sentences: “It is raining heavily” and “We should eat fruits.” To represent these two sentences, we calculate the frequency of each of the words in these sentences, as shown in figure 7.5.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p102">  &#13;
   <img alt="figure" src="../Images/CH07_F05_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.5</span> The frequency of each word has been calculated. In this example, we have two sentences.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p103"> &#13;
   <p>Now if we assume that the words in these two sentences represent the entire vocabulary, we can represent the first sentence as shown in figure 7.6. Note that the table contains all the words, but the words that are not present in the sentence have received a value of 0.<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p104">  &#13;
   <img alt="figure" src="../Images/CH07_F06_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.6</span> We are assuming that in the vocabulary only two sentences are present and the first sentence will be represented as shown.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p105"> &#13;
   <p>In this example, we examined how the BOW approach has been used to represent a sentence as a vector. But the BOW approach has not considered the order of the words or the context. It focuses only on the frequency of the word. Hence, it is a very fast approach to represent the data and is computationally less expensive compared to its peers. Since it is frequency based, it is commonly used for document classifications. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p106"> &#13;
   <p>But, due to its pure frequency-based calculation and representation, solution accuracy using the BOW approach can take a hit. In language, the context of the word plays a significant role. As we have seen earlier, apple is both a fruit as well as a well-known brand and organization. That is why we have other advanced methods that consider more parameters than frequency alone. One such method is TF-IDF, which we will study next.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p107"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 7.2</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p108"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p109"> Explain tokenization in simple language as if you are explaining it to a person who does not know NLP. </li> &#13;
    <li class="readable-text" id="p110"> The bag of words approach uses the context of the words and not frequency alone. True or False? </li> &#13;
    <li class="readable-text" id="p111"> Lemmatization is a less rigorous approach than stemming. True or False? </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p112"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.10</span> Term frequency and inverse document frequency</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p113"> &#13;
   <p>In the BOW approach, we give importance to the frequency of a word only. But the words that have a higher frequency might not always offer meaningful information as compared to words that are rare but carry more importance. For example, say we have a collection of medical documents, and we wish to compare two words: “disease” and “diabetes.” Since the corpus consists of medical documents, the word “disease” is bound to be more frequent, while the word “diabetes” will be less frequent but more important to identify the documents that deal with diabetes. The term frequency and inverse document frequency (TF-IDF) approach allows us to resolve this problem and extract information on the more important words.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p114"> &#13;
   <p>In TF-IDF, we consider the relative importance of the word. TF means term frequency, and IDF means inverse document frequency. We can define TF-IDF in this way:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p115"> TF is the count of a term in the entire document (for example, the count of the word “a” in document “D”). </li> &#13;
   <li class="readable-text" id="p116"> IDF is the log of the ratio of total documents (<em>N</em>) in the entire corpus and the number of documents (<em>d</em><em>f</em>) that contain the word “a.” </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p117"> &#13;
   <p>So the TF-IDF formula will give us the relative importance of a word in the entire corpus. The mathematical formula is the multiplication of TF and IDF and is given by equation 7.1:</p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p118">  &#13;
   <img alt="figure" src="../Images/verdhan-ch7-eqs-0x.png"/> &#13;
   <h5 class=" figure-container-h5">(7.1)</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p119"> &#13;
   <p>where <em>N</em> is the total number of documents in the corpus, <em>tf</em><sub><em>i</em></sub><sub>,</sub><sub><em>j</em></sub><sub> </sub> is the frequency of the word in the document, and <em>df</em><sub><em>i</em></sub> is the number of documents in the corpus that contain that word.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p120"> &#13;
   <p>The concept might sound complex. Let’s understand this with an example. Say we have a collection of 1 million sports journals. These sports journals contain many articles of various lengths. We also assume that all the articles are in the English language only. So, let’s say, in these documents, we want to calculate the TF-IDF value for the words “ground” and “backhand.”</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p121"> &#13;
   <p>Let’s assume we have a document of 100 words with the word “ground” appearing five times and “backhand” only twice. So the TF for ground is 5/100 = 0.05, and for backhand, it is 2/100 = 0.02.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p122"> &#13;
   <p>We understand that the word “ground” is quite a common word in sports, while the word “backhand” will be used less often. Now we assume that “ground” appears in 100,000 documents out of 1 million documents while “backhand” appears only in 10. So the IDF for “ground” is log (1,000,000/100,000) = log (10) = 1. For “backhand” it will be log (1,000,000/10) = log (100,000) = 5.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p123"> &#13;
   <p>To get the final values for “ground,” we multiply TF and IDF = 0.05 × 1 = 0.05. To get the final values for “backhand,” we multiply TF and IDF = 0.02 × 5 = 0.1.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p124"> &#13;
   <p>We can observe in this case that the relative importance of the word “backhand” is more than the relative importance of the word “ground.” This is the advantage of TF-IDF over the frequency-based BOW approach. But TF-IDF takes more time to compute as compared to BOW, since all the TF and IDF have to be calculated. Nevertheless, TF-IDF offers a better and more mature solution as compared to the BOW approach in such cases. So, in scenarios where the relative importance of a word is in discussion, we can use TF-IDF. For example, if the task is to shortlist medical documents on cardiology, the importance of the word “angiogram” will be higher as it is much more related to cardiology. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p125"> &#13;
   <p>We have so far covered BOW and the TF-IDF approach. But in neither of these approaches did we take the sequence of the words into consideration, which is covered in language models. We cover language models next.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p126"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.11</span> Language models</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p127"> &#13;
   <p>Language models assign probabilities to the sequence of words. N-grams are the simplest in language models. We know that to analyze the text data, they must be converted to feature vectors. N-gram models create the feature vectors so that text can be represented in a format that can be analyzed further. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p128"> &#13;
   <p>N-gram is a probabilistic language model. In an n-gram model, we calculate the probability of the <em>N</em><sup>th</sup> word given the sequence of (<em>N</em> – 1) words. To be more spe- cific, an n-gram model will predict the next word <em>x</em><sub><em>i</em></sub> based on the words <em>x</em><sub><em>i</em></sub><sub>–(</sub><sub><em>n–</em></sub><sub>1</sub><sub>)</sub>, <em>x</em><sub><em>i</em></sub><sub>–(</sub><sub><em>n–</em></sub><sub>2</sub><sub>)</sub>…<em>x</em><sub><em>i</em></sub><sub>–1</sub>. If we wish to use the probability terms, we can represent them as the conditional probability of <em>x</em><sub><em>i</em></sub> given the previous words, which can be represented as <em>P</em>(<em>x</em><sub><em>i</em></sub> | <em>x</em><sub><em>i</em></sub><sub>–(</sub><sub><em>n–</em></sub><sub>1</sub><sub>)</sub>, <em>x</em><sub><em>i</em></sub><sub>–(</sub><sub><em>n–</em></sub><sub>2</sub><sub>)</sub>…<em>x</em><sub><em>i</em></sub><sub>–1</sub>). The probability is calculated by using the relative frequency of the sequence occurring in the text corpus. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p129"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> If the items are words, n-grams may be referred to as <em>shingles</em>. </p> &#13;
  </div> &#13;
  <div class="readable-text" id="p130"> &#13;
   <p>Let’s study this using an example. We will take a sentence and then break down the meaning by using words in the sentence. Consider we have the sentence “It is raining heavily.” We show the respective representations of this sentence by using different values of <em>n</em> in figure 7.6. You should note how the sequence of words and their respective combinations are getting changed for different values of <em>n</em>. If we wish to use <em>n</em> = 1 or a single word to make a prediction, the representation will be as shown in figure 7.7. Note that each word is used separately here. They are referred to as <em>unigrams</em>. <span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p131">  &#13;
   <img alt="figure" src="../Images/CH07_F07_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.7</span> Unigrams, bigrams, and trigrams can be used to represent the same sentence. The concept can be extended to n-grams too. </h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p132"> &#13;
   <p>If we wish to use <em>n</em> = 2, the number of words used will become two. They are referred to as <em>bigrams</em>. If we use <em>n</em> = 3, the number of words becomes three, and they are referred to as <em>trigrams,</em> and so on.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p133"> &#13;
   <p>Hence, if we have a unigram, it is a sequence of one word; for two words, it is a bigram; for three words, it is a trigram; and so on. So, a trigram model will approximate the probability of a word given all the previous words by using the conditional probability of only the preceding two words, whereas a bigram will do the same by considering only the preceding word. This is a valid assumption, indeed, that the probability of a word will depend only on the preceding word and is referred to as the <em>Markov</em> assumption. Generally, <em>n</em> &gt; 1 is considered to be much more informative than unigrams. But obviously, the computation time will increase too.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p134"> &#13;
   <p>The n-gram approach is very sensitive to the choice of <em>n</em>. It also depends significantly on the training corpus that has been used, which makes the probabilities heavily dependent on the training corpus. So, if an unknown word is encountered, it will be difficult for the model to work on that new word.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p135"> &#13;
   <p>Next we create a Python example. We will show a few examples of text cleaning using Python.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p136"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.12</span> Text cleaning using Python</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p137"> &#13;
   <p>There are a few libraries you may need to install. We will show a few small code snippets. You are advised to use them as per the examples. We are also including the respective screenshots of the code snippets and their results:</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p138"> &#13;
   <p>Code 1: Remove the blank spaces in the text. Import the library <code>re</code>; it is called the Regular Expression (<code>Regex</code>) expression. The text is “It is raining outside” with a lot of blank spaces in between (see figure 7.8):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p139"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import re&#13;
doc = "It is     raining       outside"&#13;
new_doc = re.sub("\s+"," ", doc)&#13;
print(new_doc)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p140">  &#13;
   <img alt="figure" src="../Images/CH07_F08_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.8</span> Removing the blank spaces</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p141"> &#13;
   <p>Code 2: Now we will remove the punctuation in the text data (see figure 7.9):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p142"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">text_d = "Hey!!! How are you doing? And how is your health! Bye, take care."&#13;
re.sub("[^-9A-Za-z ]", "" , text_d)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p143">  &#13;
   <img alt="figure" src="../Images/CH07_F09_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.9</span> Removing the punctuation</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p144"> &#13;
   <p>Code 3: Here is one more method to remove the punctuation (see figure 7.10):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p145"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import string&#13;
text_d = "Hey!!! How are you doing? And how is your health! Bye, take care."&#13;
cleaned_text = "".join([i for i in text_d if i not in string.punctuation])&#13;
cleaned_text<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p146">  &#13;
   <img alt="figure" src="../Images/CH07_F10_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.10</span> An alternative way to remove punctuation</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p147"> &#13;
   <p>Code 4: We will now remove the punctuation as well as convert the text to lowercase (see figure 7.11):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p148"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">text_d = "Hey!!! How are you doing? And how is your health! Bye, take care."&#13;
cleaned_text = "".join([i.lower() for i in text_d if i not in &#13;
string.punctuation])&#13;
cleaned_text<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p149">  &#13;
   <img alt="figure" src="../Images/CH07_F11_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.11</span> Converting the text to lowercase</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p150"> &#13;
   <p>Code 5: Tokenization is done here using the standard <code>nltk</code> library (see figure 7.12):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p151"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import nltk&#13;
text_d = "Hey!!! How are you doing? And how is your health! Bye, take care."&#13;
nltk.tokenize.word_tokenize(text_d)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p152">  &#13;
   <img alt="figure" src="../Images/CH07_F12_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.12</span> Tokenization</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p153"> &#13;
   <p>Note that in the output of the code, we have all the words, including the punctuation marks, as different tokens. If you wish to exclude the punctuation, you can clean the punctuation marks using the code snippets shared earlier. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p154"> &#13;
   <p>Code 6: Next comes the stop words. We will remove the stop words using the <code>nltk</code> library. After that, we tokenize the words (see figure 7.13):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p155"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">stopwords = nltk.corpus.stopwords.words('english')&#13;
text_d = "Hey!!! How are you doing? And how is your health! Bye, take care."&#13;
text_new = "".join([i for i in text_d if i not in string.punctuation])&#13;
print(text_new)&#13;
words = nltk.tokenize.word_tokenize(text_new)&#13;
print(words)&#13;
words_new = [i for i in words if i not in stopwords]&#13;
print(words_new)<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p156">  &#13;
   <img alt="figure" src="../Images/CH07_F13_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.13</span> Removing stop words and tokenizing words</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p157"> &#13;
   <p>Code 7: We will now perform stemming on a text example. We use <code>nltk</code> library for it. The words are first tokenized, and then we apply stemming (see figure 7.14):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p158"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import nltk&#13;
from nltk.stem import PorterStemmer&#13;
stem = PorterStemmer()&#13;
text = "eats eating studies study"&#13;
tokenization = nltk.word_tokenize(text)&#13;
for word in tokenization:&#13;
    print("Stem for {} is {}".format(word, stem.stem(word)))<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p159">  &#13;
   <img alt="figure" src="../Images/CH07_F14_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.14</span> Tokenizing and then stemming the words</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p160"> &#13;
   <p>Code 8: We now perform lemmatization on a text example. We use the <code>nltk</code> library for it. The words are first tokenized, and then we apply lemmatization (see figure 7.15):</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p161"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">import nltk&#13;
from nltk.stem import WordNetLemmatizer&#13;
wordnet_lemmatizer = WordNetLemmatizer()&#13;
text = "eats eating studies study"&#13;
tokenization = nltk.word_tokenize(text)&#13;
for word in tokenization:&#13;
    print("Lemma for {} is {}".format(word, wordnet_lemmatizer.lemmatize(word)))<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p162">  &#13;
   <img alt="figure" src="../Images/CH07_F15_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.15</span> Tokenizing and then lemmatizing the words</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p163"> &#13;
   <p>Observe and compare the difference between the two outputs of stemming and lemmatization. For “studies” and “studying,” stemming generated the output as “studi” while lemmatization generated the correct output as “study.”</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p164"> &#13;
   <p>We have covered BOW, TF-IDF, and n-gram approaches so far. But in all these techniques, the relationship between words has been neglected. This relationship is used in word embeddings, our next topic.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p165"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.13</span> Word embeddings</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p166"> &#13;
   <blockquote>&#13;
    <div>&#13;
     A word is characterized by the company it keeps.  &#13;
     <div class=" quote-cite">&#13;
       —John Rupert Firth &#13;
     </div>&#13;
    </div>&#13;
   </blockquote> &#13;
  </div> &#13;
  <div class="readable-text" id="p167"> &#13;
   <p>So far we have studied several approaches, but all the techniques ignore the contextual relationship between words. Let’s take a closer look using an example. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p168"> &#13;
   <p>Imagine we have 100,000 words in our vocabulary, starting from “aa” (the basaltic lava) to “zoom.” Now, if we perform one-hot encoding, all these words can be represented in a vector form. Each word will have a unique vector. For example, if the position of the word “king” is 21000, the vector will have a shape like the following vector, which has 1 at the 21,000th position and the rest of the values as 0:</p> &#13;
  </div> &#13;
  <div class="browsable-container listing-container" id="p169"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…………………1, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p170"> &#13;
   <p>There are a few glaring problems with this approach:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p171"> The number of dimensions is very high, and it is complex to compute. </li> &#13;
   <li class="readable-text" id="p172"> The data is very sparse in nature. </li> &#13;
   <li class="readable-text" id="p173"> If <em>n</em> new words have to be entered, the vocabulary increases by <em>n</em>, and hence each vector dimensionality increases by <em>n</em>. </li> &#13;
   <li class="readable-text" id="p174"> This approach ignores the relationship between words. We know that “ruler,” “king,” and “monarch” are sometimes used interchangeably. In the one-hot-encoding approach, any such relationships are ignored. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p175"> &#13;
   <p>If we wish to perform language translation, or generate a chat-bot, we need to pass such knowledge to the machine learning solution. Word embeddings provide a solution to the problem. They convert the high-dimensional word features into lower dimensions while maintaining the contextual relationship. Word embeddings allow us to create much more generalized models. We can understand the meaning by looking at an example. </p> &#13;
  </div> &#13;
  <div class="readable-text print-book-callout" id="p176"> &#13;
   <p><span class="print-book-callout-head">NOTE </span> In an LLM-enabled solution, you might not need to do a lot of these steps.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p177"> &#13;
   <p>In the example shown in figure 7.16, the relation of “man” to “woman” is similar to “king” to “queen”; “good” to “nice” is similar to “bad” to “awful”; or the relationship of “UK” to “London” is similar to “Japan” to “Tokyo.”<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p178">  &#13;
   <img alt="figure" src="../Images/CH07_F16_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.16</span> Word embeddings can be used to represent the relationships between words. For example, there is a relation from “men” to “women” that is similar to “king” to “queen” as both “men-women” and “king-queen” represent the male-female gender relationship.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p179"> &#13;
   <p>In simple terms, using word embeddings, we can represent the words that have similar meanings. Word embeddings can be thought of as a class of techniques where we represent each of the individual words in a predefined vector space. Each of the words in the corpus is mapped to one vector. The distributed representation is understood based on the word’s usage. Hence, words that can be used similarly have similar representations. This allows the solution to capture the underlying meaning of the words and their relationships. Hence, the meaning of the word plays a significant role. This representation is more intelligent as compared to the BOW approach where each word is treated differently, irrespective of its usage. Also, the number of dimensions is fewer as compared to one-hot encoding. Each word is represented by 10s or 100s of dimensions, which is significantly less than the one-hot encoding approach where 1000s of dimensions are used for representation.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p180"> &#13;
   <p>We cover the two most popular techniques—Word2Vec and global vectors for word representation (GloVe)—in the next section. The mathematical foundation for Word2Vec and GloVe are beyond the scope of this book. We provide an understanding of the working mechanism of the solutions and then develop Python code using Word2Vec and GloVe. This section is more technically involved, so if you are interested only in the application of the solutions, you can skip the next section.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p181"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.14</span> Word2Vec and GloVe</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p182"> &#13;
   <p>Word2Vec was first published in 2013. It was developed by Tomas Mikolov and others at Google. We share the link to the paper at the end of the chapter. You are advised to study the paper thoroughly if you wish to learn about the more technical elements in detail.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p183"> &#13;
   <p>Word2Vec is a group of models used to produce word embeddings. The input is a large corpus of text. The output is a vector space with a very large number of dimensions. In this output, each of the words in the corpus is assigned a unique and corresponding vector. The most important point is that the words that have a similar or common context in the corpus are located nearby in the vector space produced.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p184"> &#13;
   <p>In Word2Vec, the researchers introduced two different learning models—the continuous bag of words (CBOW) and the continuous skip-gram model:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p185"> In CBOW, the model makes a prediction of the current word from a window of surrounding context words. So the CBOW model predicts a target word based on the context of the surrounding words in the text. Recall that in the BOW approach, the order of the words does not play any part. In contrast, in CBOW, the order of the words is significant. </li> &#13;
   <li class="readable-text" id="p186"> The continuous skip-gram model uses the current word to predict the surrounding window of context words. While doing so, it allocates more weight to the neighboring words as compared to the distant words. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p187"> &#13;
   <p>GloVe is an unsupervised learning algorithm for generating vector representation for words. It was developed by Pennington and others at Stanford and launched in 2014. It is a combination of two techniques: matrix factorization techniques and local context-based learning used in Word2Vec. GloVe can be used to find relationships like zip codes and cities, synonyms, etc. It generates a single set of vectors for words with the same morphological structure.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p188"> &#13;
   <p>Both Word2Vec and GloVe learn and understand vector representation of their words from the co-occurrence information. Co-occurrence means how frequently the words appear together in a large corpus. The prime difference is that Word2Vec is a prediction-based model, while GloVe is a frequency-based model. Word2Vec predicts the context given a word while GloVe learns the context by creating a co-occurrence matrix on how frequently a word appears in a given context.</p> &#13;
  </div> &#13;
  <div class="callout-container sidebar-container"> &#13;
   <div class="readable-text" id="p189"> &#13;
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 7.3</h5> &#13;
   </div> &#13;
   <div class="readable-text" id="p190"> &#13;
    <p>Answer these questions to check your understanding:</p> &#13;
   </div> &#13;
   <ol> &#13;
    <li class="readable-text" id="p191"> BOW is more rigorous than the TF-IDF approach. True or False? </li> &#13;
    <li class="readable-text" id="p192"> Differentiate between Word2Vec and GloVe. </li> &#13;
   </ol> &#13;
  </div> &#13;
  <div class="readable-text" id="p193"> &#13;
   <p>We will now move to the case study and Python implementation.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p194"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.15</span> Sentiment analysis case study with Python implementation </h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p195"> &#13;
   <p>So far, we have discussed a lot of concepts on NLP and text data. In this section, we first explore a business case and then develop a Python solution based on it. Here we are working on sentiment analysis. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p196"> &#13;
   <p>Product reviews are a rich source of information—both for customers and organizations. Whenever we wish to buy any new product or service, we tend to look at the reviews by fellow customers. You might have reviewed products and services yourself. These reviews are available at Amazon and on blogs, surveys, etc.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p197"> &#13;
   <p>Let’s consider a case. A telecom operator receives complaints from its customers, reviews about the service, and comments about the overall experience. The streams can be product quality, pricing, onboarding experience, ease of registration, payment process, general reviews, customer service, etc. We want to determine the general context of the review—whether it is positive, negative, or neutral. The reviews include the number of stars allocated, actual text reviews, pros and cons about the product/service, attributes, etc. However, there are a few business problems—for instance,</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p198"> Sometimes the number of stars received by a product/service is very high, while the actual reviews are quite negative. </li> &#13;
   <li class="readable-text" id="p199"> The organizations and the product owners need to know which features are appreciated by the customers and which features are disliked by the customers. The team can then work on improving the features that are disliked. </li> &#13;
   <li class="readable-text" id="p200"> There is a need to gauge and keep an eye on the competition! The organizations need to know the attributes of the popular products of their competitors. </li> &#13;
   <li class="readable-text" id="p201"> The product owners want to better plan for the upcoming features they wish to release in the future. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p202"> &#13;
   <p>So the business teams will be able to answer these important questions:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p203"> What are our customers’ satisfaction levels for the products and services? </li> &#13;
   <li class="readable-text" id="p204"> What are the major pain points and dissatisfactions of the customers? </li> &#13;
   <li class="readable-text" id="p205"> What drives the customers’ engagement? </li> &#13;
   <li class="readable-text" id="p206"> Which services are complex and time-consuming, and which are the most liked services/products? </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p207"> &#13;
   <p>This business use case will drive the following business benefits:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p208"> The products and services that are most satisfactory and are the most liked should be continued. </li> &#13;
   <li class="readable-text" id="p209"> The ones that are not liked and are receiving a negative score should be improved and challenges mitigated. </li> &#13;
   <li class="readable-text" id="p210"> The respective teams, like finance, operations, complaints, CRM, etc., can be notified, and they can work individually to improve the customer experience. </li> &#13;
   <li class="readable-text" id="p211"> The precise reasons for liking or disliking the services will be useful for the respective teams to work in the right direction. </li> &#13;
   <li class="readable-text" id="p212"> Overall, it will provide a benchmark to measure the Net Promoter Score for the customer base. The business can strive to enhance the overall customer experience. </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p213"> &#13;
   <p>We might want to represent these findings by means of a dashboard. This dashboard will be refreshed on a regular cycle, like monthly or quarterly.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p214"> &#13;
   <p>To solve this business problem, the teams can collect relevant data from websites, surveys, Amazon, blogs, etc. Then an analysis can be done on that dataset. It is relatively easy to analyze the structured data. In this example, we work on text data.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p215"> &#13;
   <p>The Python Jupyter notebook is pushed to the GitHub location. You are advised to use the Jupyter notebook from the GitHub location as it contains more steps. The steps are as follows:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p216"> Import all the libraries: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p217"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">#### Loading all the required libraries here&#13;
from lxml import html  &#13;
import requests&#13;
import pandas as pd&#13;
from nltk.corpus import stopwords&#13;
from textblob import TextBlob&#13;
import matplotlib.pyplot as plt&#13;
import sys&#13;
import numpy as np&#13;
import pandas as pd&#13;
import matplotlib&#13;
import matplotlib.pyplot as plt&#13;
import sklearn&#13;
import scikitplot as skplt&#13;
import nltk&#13;
#to ignore warnings&#13;
import warnings&#13;
warnings.filterwarnings("ignore")&#13;
nltk.download('stopwords')&#13;
nltk.download('punkt')&#13;
nltk.download('wordnet')</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p218"><span class="faux-ol-li-counter">2. </span> Define the tags. These tags are used to get the attributes of the product from the reviews: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p219"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">xpath_reviews = '//div[@data-hook="review"]'&#13;
reviews = parser.xpath(xpath_reviews)&#13;
xpath_rating  = './/i[@data-hook="review-star-rating"]//text()' &#13;
xpath_title   = './/a[@data-hook="review-title"]//text()'&#13;
xpath_author  = './/a[@data-hook="review-author"]//text()'&#13;
xpath_date    = './/span[@data-hook="review-date"]//text()'&#13;
xpath_body    = './/span[@data-hook="review-body"]//text()'&#13;
xpath_helpful = './/span[@data-hook="helpful-vote-statement"]//text()'</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p220"><span class="faux-ol-li-counter">3. </span> Make everything ready to extract the data. We create a dataframe to store the customer reviews. Then we iterate through all the reviews and extract the information: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p221"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area"># Create a dataframe here. &#13;
&#13;
reviews_df = pd.DataFrame()&#13;
for review in reviews:&#13;
    rating  = review.xpath(xpath_rating)&#13;
    title   = review.xpath(xpath_title)&#13;
    author  = review.xpath(xpath_author)&#13;
    date    = review.xpath(xpath_date)&#13;
    body    = review.xpath(xpath_body)&#13;
    helpful = review.xpath(xpath_helpful)&#13;
&#13;
    review_dict = {'rating': rating,&#13;
                   'title': title,&#13;
                   'author': author,             &#13;
                   'date': date,&#13;
                   'body': body,&#13;
                   'helpful': helpful}&#13;
    reviews_df = reviews_df.append(review_dict, ignore_index=True)&#13;
all_reviews = pd.DataFrame()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p222"><span class="faux-ol-li-counter">4. </span> Iterate through the reviews and then fill in the details: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p223"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area"># Fill the values of the reviews here. &#13;
&#13;
for i in range(1,90):&#13;
    amazon_url = 'https://www.amazon.co.uk/Hive-Heating-Thermostat-Professional-Installation/product-reviews/B011B3J6KY/ref=cm_cr_othr_d_show_all?ie=UTF8&amp;reviewerType=all_revie&#13;
ws&amp;pageNumber='+str(i)&#13;
    headers = {'User-Agent': user_agent}&#13;
    page = requests.get(amazon_url, headers = headers)&#13;
    parser = html.fromstring(page.content)&#13;
    xpath_reviews = '//div[@data-hook="review"]'&#13;
    reviews = parser.xpath(xpath_reviews)&#13;
    reviews_df = pd.DataFrame()&#13;
    xpath_rating  = './/i[@data-hook="review-star-rating"]//text()' &#13;
    xpath_title   = './/a[@data-hook="review-title"]//text()'&#13;
    xpath_author  = './/a[@data-hook="review-author"]//text()'&#13;
    xpath_date    = './/span[@data-hook="review-date"]//text()'&#13;
    xpath_body    = './/span[@data-hook="review-body"]//text()'&#13;
    xpath_helpful = './/span[@data-hook="helpful-vote-statement"]//text()'&#13;
    #print(i)&#13;
    for review in reviews:&#13;
        rating  = review.xpath(xpath_rating)&#13;
        title   = review.xpath(xpath_title)&#13;
        author  = review.xpath(xpath_author)&#13;
        date    = review.xpath(xpath_date)&#13;
        body    = review.xpath(xpath_body)&#13;
        helpful = review.xpath(xpath_helpful)&#13;
&#13;
        review_dict = {'rating': rating,&#13;
                       'title': title,&#13;
                       'author': author,             &#13;
                       'date': date,&#13;
                       'body': body,&#13;
                       'helpful': helpful}&#13;
        reviews_df = reviews_df.append(review_dict, ignore_index=True)&#13;
    #print(reviews_df)&#13;
    all_reviews = all_reviews.append(reviews_df)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p224"><span class="faux-ol-li-counter">5. </span> Have a look at the output we generated: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p225"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">all_reviews.head()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p226"><span class="faux-ol-li-counter">6. </span> Save the output to a path. You can give your own path: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p227"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">out_folder = '/Users/Data/'&#13;
all_reviews.to_csv(out_folder + 'Reviews.csv')</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p228"><span class="faux-ol-li-counter">7. </span> Load the data and analyze it: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p229"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">#Load the data now and analyse it&#13;
data_path = '/Users/vaibhavverdhan/Book/UnsupervisedLearningBookFinal/'&#13;
reviewDataCSV = 'Reviews.csv'&#13;
reviewData = (pd.read_csv(data_path+reviewDataCSV,index_col=0,))</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p230"><span class="faux-ol-li-counter">8. </span> Look at the basic information about the dataset: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p231"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">reviewData.shape&#13;
reviewData.rating.unique()&#13;
reviewData.rating.value_counts()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p232"><span class="faux-ol-li-counter">9. </span> Look at the distribution of the stars given in the reviews. This will allow us to understand the reviews given by the customers: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p233"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">labels = '5 Stars', '1 Star', '4 Stars', '3 Stars', '2 Stars'&#13;
sizes = [reviewData.rating.value_counts()[0], reviewData.rating.value_counts()[1],reviewData.rating.value_counts()[2],rev&#13;
iewData.rating.value_counts()[3],reviewData.rating.value_counts()[4]]&#13;
colors = ['green', 'yellowgreen', 'coral', 'lightblue', 'grey']&#13;
explode = (0, 0, 0, 0, 0)  # explode 1st slice&#13;
&#13;
# Plot&#13;
plt.pie(sizes, explode=explode, labels=labels, colors=colors,&#13;
        autopct='%1.1f%%', shadow=True, startangle=140)&#13;
&#13;
plt.axis('equal')&#13;
plt.show()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p234"><span class="faux-ol-li-counter">10. </span> Make the text lowercase, and then remove the stop words and the words that have the highest frequency: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p235"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">reviewData.body = reviewData.body.str.lower()&#13;
reviewData.body = reviewData.body.str.replace('[^\w\s]','')&#13;
stop = stopwords.words('english')&#13;
reviewData.body = reviewData.body.apply(lambda x: " ".join(x for x in &#13;
x.split() if x not in stop))&#13;
freq = list(freq.index)&#13;
reviewData.body = reviewData.body.apply(lambda x: " ".join(x for x in x.split() if x not in freq))&#13;
freq = pd.Series(' '.join(reviewData.body).split()).value_counts()[-10:]&#13;
freq = list(freq.index)&#13;
reviewData.body = reviewData.body.apply(lambda x: " ".join(x for x in &#13;
x.split() if x not in freq))</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p236"><span class="faux-ol-li-counter">11. </span> Tokenize the data: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p237"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from nltk.tokenize import word_tokenize&#13;
tokens = word_tokenize(reviewData.iloc[1,1])&#13;
print(tokens)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p238"><span class="faux-ol-li-counter">12. </span> Perform lemmatization: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p239"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from textblob import Word&#13;
reviewData.body = reviewData.body.apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))&#13;
reviewData.body.head()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p240"><span class="faux-ol-li-counter">13. </span> Append all the reviews to the string: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p241"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">sentimentString = reviewData.iloc[1,1]&#13;
# append to this string &#13;
for i in range(2,len(reviewData)):&#13;
    sentimentString = sentimentString + reviewData.iloc[i,1]</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p242"><span class="faux-ol-li-counter">14. </span> Do the sentiment analysis. From <code>textblob</code>, we take the sentiment method. It generates polarity and subjectivity for a sentiment. Sentiment polarity for an element is the orientation of the sentiment in the expression; that is, it tells us if the text expresses a negative, positive, or neutral sentiment in the text. It subjectively measures and quantifies the amount of opinion and factual information in the text. If the subjectivity is high, it means that the text contains more opinion than facts: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p243"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area"># the functions generates polarity and subjectivity here, subsetting the &#13;
polarity only here&#13;
allReviewsSentiment = reviewData.body[:900].apply(lambda x: &#13;
TextBlob(x).sentiment[0])&#13;
# this contains boths subjectivity and polarity&#13;
allReviewsSentimentComplete = reviewData.body[:900].apply(lambda x: &#13;
TextBlob(x).sentiment)&#13;
allReviewsSentimentComplete.head()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p244"><span class="faux-ol-li-counter">15. </span> Save the sentiment to a .csv file: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p245"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">allReviewsSentiment.to_csv(out_folder + 'ReviewsSentiment.csv')</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p246"><span class="faux-ol-li-counter">16. </span> Allocate a meaning or a tag to the sentiment. We classify each of the scores from extremely satisfied to extremely dissatisfied: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p247"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">allReviewsSentimentDF = allReviewsSentiment.to_frame()&#13;
# Create a list to store the data&#13;
grades = []&#13;
&#13;
# For each row in the column,&#13;
for row in allReviewsSentimentDF['body']:&#13;
    # if more than a value,&#13;
    if row &gt;= 0.75:&#13;
       grades.append('Extremely Satisfied')&#13;
    elif (row &gt;= 0.5) &amp; (row &lt; 0.75):&#13;
        grades.append('Satisfied')&#13;
    elif (row &gt;= 0.2) &amp; (row &lt; 0.5):&#13;
        grades.append('Nice')&#13;
    elif (row &gt;= -0.2) &amp; (row &lt; 0.2):&#13;
        grades.append('Neutral')&#13;
    elif (row &gt; -0.5) &amp; (row &lt;= -0.2):&#13;
        grades.append('Bad')&#13;
    elif (row &gt;= -0.75) &amp; (row &lt; -0.5):&#13;
        grades.append('Dis-satisfied')&#13;
    elif  row &lt; -0.75:&#13;
        grades.append('Extremely Dis-satisfied')&#13;
    else:&#13;
        # Append a failing grade&#13;
        grades.append('No Sentiment')&#13;
&#13;
# Create a column from the list&#13;
allReviewsSentimentDF['SentimentScore'] = grades&#13;
allReviewsSentimentDF.head()</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p248"><span class="faux-ol-li-counter">17. </span> Look at the sentiment scores and plot them too. Finally, we merge them with the main dataset: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p249"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">allReviewsSentimentDF.SentimentScore.value_counts()&#13;
allReviewsSentimentDF['SentimentScore'].value_counts().plot(kind='bar')&#13;
#### Merge the review data with Sentiment generated&#13;
&#13;
reviewData['polarityScore'] = allReviewsSentimentDF['body']    <span class="aframe-location"/> #1</pre> &#13;
    <div class="code-annotations-overlay-container"> #1 Adds column polarityScore&#13;
     <br/>&#13;
    </div> &#13;
   </div> &#13;
  </div> &#13;
  <div class="readable-text" id="p250"> &#13;
   <p>In this case study, you not only scraped the reviews from the website but you also analyzed the dataset. If we compare the sentiments, we can see that the stars given to a product do not represent a true picture.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p251"> &#13;
   <p>Figure 7.17 compares the actual stars and the output from sentiment analysis. We can observe that 73% of customers have given five stars and 7% have given four stars, while in the sentiment analysis most of the reviews have been classified as neutral. This is the real power of sentiment analysis!<span class="aframe-location"/></p> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p252">  &#13;
   <img alt="figure" src="../Images/CH07_F17_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.17</span> Compare the original distribution of number of stars on the left side and the real results from the sentiment analysis on the right.</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p253"> &#13;
   <p>Sentiment analysis is quite an important use case. It is very useful for business and product teams. The preceding code can be scaled to any such business problem at hand.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p254"> &#13;
   <p>We now move to the second case study on document classification using Python.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p255"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.16</span> Text clustering using Python</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p256"> &#13;
   <p>Consider this: we have a bunch of text datasets or documents, but they all are mixed up. We do not know which text belongs to which class. In this case, we will assume that we have two types of text datasets: one that has all the data related to football and one that is related to travel. We will develop a model that can segregate these two classes. To do that, we follow these steps:</p> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p257"> Import all the libraries: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p258"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">from sklearn.feature_extraction.text import TfidfVectorizer&#13;
from sklearn.cluster import KMeans&#13;
import numpy as np&#13;
import pandas as pd</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p259"><span class="faux-ol-li-counter">2. </span> Create a dummy dataset. This text data has a few sentences we have written ourselves. There are two categories: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p260"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">text = ["It is a good place to travel", &#13;
        "Football is a nice game", "Lets go for holidays and travel to &#13;
Egypt", &#13;
        "It is a goal, a great game.", "Enjoy your journey and forget &#13;
the rest", "The teams are ready for the same" ]</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p261"><span class="faux-ol-li-counter">3. </span> Use TF-IDF to vectorize the data: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p262"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">tfidf_vectorizer = TfidfVectorizer(stop_words='english')&#13;
X = tfidf_vectorizer.fit_transform(text)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p263"><span class="faux-ol-li-counter">4. </span> Do the clustering: </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p264"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">k = 2&#13;
model = KMeans(n_clusters=k, init='k-means++', max_iter=10, n_init=2)&#13;
model.fit(X)</pre>  &#13;
   </div> &#13;
  </div> &#13;
  <ol class=" faux-ol-li" style="list-style: none;"> &#13;
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p265"><span class="faux-ol-li-counter">5. </span> Represent the centroids and print the outputs (see figure 7.18): </li> &#13;
  </ol> &#13;
  <div class="browsable-container listing-container" id="p266"> &#13;
   <div class="code-area-container"> &#13;
    <pre class="code-area">centroids = model.cluster_centers_.argsort()[:, ::-1]&#13;
features = vectorizer.get_feature_names()&#13;
&#13;
for i in range(k):&#13;
    print("Cluster %d:" % i),&#13;
    for ind in centroids[i, :10]:&#13;
        print("%s" % terms[ind])<span class="aframe-location"/></pre>  &#13;
   </div> &#13;
  </div> &#13;
  <div class="browsable-container figure-container" id="p267">  &#13;
   <img alt="figure" src="../Images/CH07_F18_Verdhan.png"/> &#13;
   <h5 class=" figure-container-h5"><span class="num-string">Figure 7.18</span> Printed output</h5>&#13;
  </div> &#13;
  <div class="readable-text" id="p268"> &#13;
   <p>You can extend this example to other datasets too. Get the datasets from the internet and replicate the code in the preceding example.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p269"> &#13;
   <p>We have pushed the code to the GitHub location of the book. You are advised to use it. It is really an important source to represent text data.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p270"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.17</span> GenAI for text data</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p271"> &#13;
   <p>GenAI solutions are a new kind of unsupervised solution. You surely have heard about ChatGPT and LLMs. They have revolutionized the world. GenAI for text data uses machine learning models to create human-like text. It is trained on large-scale data patterns and hence can generate a variety of content pieces—for example, essays, technical reports, and summaries of a book—and can act like a human chat interface. Even the complex translation of languages is made easy with GenAI. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p272"> &#13;
   <p>GenAI for text data involves the use of advanced algorithms, like transformers, to generate coherent, contextually appropriate text. These algorithms are trained on mammoth datasets. Imagine we feed tons of content present on the internet to the algorithms. By learning patterns and relationships between the words and the sentences, the grammar used, syntax, and semantics, they can create human-like responses. These models, such as OpenAI’s GPT or Google’s BERT, are very powerful for drafting emails with correct language and grammar, creating detailed reports, writing code modules in a language like Java/C++, and many other tasks. Using this power, content creators, writers and copyrighters, brand managers and marketers, and business owners can produce high-quality text in a much more scalable and efficient manner.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p273"> &#13;
   <p>Despite its amazing potential, GenAI still has some areas in need of improvement. Sometimes it generates inaccurate information, also known as hallucinations. Ensuring that the output remains unbiased and ethical is another hurdle, as models can inadvertently reflect societal biases present in the data they were trained on. AI-generated text is increasingly being used in customer service, automating responses while still maintaining a personal tone. Researchers are also exploring its use in the healthcare and legal fields, where it can help with documentation and drafting. While GenAI is revolutionizing the way text is produced, the need for human oversight remains critical to ensure quality, accuracy, and fairness.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p274"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.18</span> Concluding thoughts</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p275"> &#13;
   <p>Text data is one of the most useful datasets. A lot of intelligence is hidden in the texts: logs, blogs, reviews, posts, tweets, complaints, comments, articles, and so on—the sources of text data are many. Organizations are investing in setting up the infrastructure for accessing text data and storing it. Analyzing text data requires better processing powers and better machines than our standard laptops. It requires special skill sets and a deeper understanding of the concepts. NLP is an evolving field, and a lot of research is underway. At the same time, we cannot ignore the importance of sound business acumen and knowledge.</p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p276"> &#13;
   <p>Data analysis and machine learning are not easy. We have to understand a lot of concepts around data cleaning, exploration, representation, and modeling. But analyzing unstructured data might be even more complex than analyzing structured datasets. We worked on an images dataset in the last chapter, and in the current chapter, we worked on text data. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p277"> &#13;
   <p>Text data is one of the most difficult datasets to analyze. There are so many permutations and combinations for text data. Cleaning the text data is a difficult and complex task. In this chapter, we discussed a few important techniques to clean text data. We also covered some methods to represent text data in vector forms. You are advised to practice each of these methods and compare the performances by applying each of the techniques. We also introduced the concept of GenAI for text data. </p> &#13;
  </div> &#13;
  <div class="readable-text intended-text" id="p278"> &#13;
   <p>With this, we come to the end of chapter 7. This also marks an end to part 2. In the next part, the complexity increases. We will be studying even deeper concepts of unsupervised learning algorithms.</p> &#13;
  </div> &#13;
  <div class="readable-text" id="p279"> &#13;
   <h2 class=" readable-text-h2"><span class="num-string">7.19</span> Practical next steps and suggested readings</h2> &#13;
  </div> &#13;
  <div class="readable-text" id="p280"> &#13;
   <p>The following provides suggestions for what to do next and offers some helpful reading:</p> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text buletless-item" id="p281"> Get the datasets from the following link. You will find a lot of text datasets here. You are advised to implement clustering and dimensionality reduction solutions: &#13;
    <ul> &#13;
     <li>  50 Free Machine Learning Datasets: Natural Language Processing: <a href="https://mng.bz/ZljO">https://mng.bz/ZljO</a> </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text" id="p282"> You will find a lot of useful datasets at Kaggle as well: <a href="https://www.kaggle.com/datasets?search=text">https://www.kaggle.com/datasets?search=text</a> </li> &#13;
   <li class="readable-text buletless-item" id="p283"> Go through the following research papers: &#13;
    <ul> &#13;
     <li> Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. <a href="https://arxiv.org/pdf/1301.3781.pdf">https://arxiv.org/pdf/1301.3781.pdf</a> </li> &#13;
     <li> Pennington, J., Socher, R., and Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. <a href="https://nlp.stanford.edu/pubs/glove.pdf">https://nlp.stanford.edu/pubs/glove.pdf</a> </li> &#13;
     <li> Das, B., and Chakraborty, S. (2018). An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation. <a href="https://arxiv.org/pdf/1806.06407.pdf">https://arxiv.org/pdf/1806.06407.pdf</a> </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text buletless-item" id="p284"> Consider these widely quoted papers: &#13;
    <ul> &#13;
     <li> Blum, A., and Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. <a href="https://dl.acm.org/doi/10.1145/279943.279962">https://dl.acm.org/doi/10.1145/279943.279962</a> </li> &#13;
     <li> Knight, K. (2009). Bayesian Inference with Tears. <a href="https://mng.bz/RVp0">https://mng.bz/RVp0</a> </li> &#13;
     <li> Hofmann, T. (1999). Probabilistic latent semantic indexing. <a href="https://dl.acm.org/doi/10.1145/312624.312649">https://dl.acm.org/doi/10.1145/312624.312649</a> </li> &#13;
     <li> Hindle, D., and Rooth, M. (1993). Structural Ambiguity and Lexical Relations. <a href="https://aclanthology.org/J93-1005.pdf">https://aclanthology.org/J93-1005.pdf</a> </li> &#13;
     <li> Collins and Singer. (1999). Unsupervised Models for Named Entity Classification. <a href="https://aclanthology.org/W99-0613.pdf">https://aclanthology.org/W99-0613.pdf</a> </li> &#13;
    </ul> </li> &#13;
   <li class="readable-text" id="p285"> See the comprehensive study on TF-IDF feature weighting: Das, M., Selvakumar, K., and Alphonse, J. P. A. (2023). A Comparative Study on TF-IDF Feature Weighting Method and its Analysis using Unstructured Dataset. <a href="https://arxiv.org/abs/2308.04037">https://arxiv.org/abs/2308.04037</a> </li> &#13;
  </ul> &#13;
  <div class="readable-text" id="p286"> &#13;
   <h2 class=" readable-text-h2">Summary</h2> &#13;
  </div> &#13;
  <ul> &#13;
   <li class="readable-text" id="p287"> Text data’s omnipresence in blogs, social media, surveys, and more, and its capacity to express emotions, emphasizes the importance of this form of data. </li> &#13;
   <li class="readable-text" id="p288"> Applications of text analysis include sentiment analysis, document categorization, language translation, spam filtering, and named-entity recognition. </li> &#13;
   <li class="readable-text" id="p289"> Challenges in text data include handling junk characters, multiple languages, evolving language, synonyms, and context-based meanings. </li> &#13;
   <li class="readable-text" id="p290"> Data preprocessing and cleaning involves removing stop words and unwanted characters and normalizing text through stemming and lemmatization. </li> &#13;
   <li class="readable-text" id="p291"> Within text representation techniques, one-hot encoding is basic but not scalable; advanced techniques consider frequency and context. </li> &#13;
   <li class="readable-text" id="p292"> Tokenization involves breaking down text into tokens and is fundamental for creating analysis-ready datasets. </li> &#13;
   <li class="readable-text" id="p293"> The BOW approach is a fast, frequency-based method that ignores word order and context. </li> &#13;
   <li class="readable-text" id="p294"> TF-IDF weighs words based on importance over mere frequency, offering more insightful analysis than BOW. </li> &#13;
   <li class="readable-text" id="p295"> Language models and n-grams use word sequences for probabilistic predictions, with variations like unigrams, bigrams, and trigrams. </li> &#13;
   <li class="readable-text" id="p296"> Python for text parsing illustrates cleaning and preprocessing text data using Python libraries like <code>nltk</code>. </li> &#13;
   <li class="readable-text" id="p297"> Techniques like Word2Vec and GloVe maintain contextual relationships between words for better semantic understanding. </li> &#13;
   <li class="readable-text" id="p298"> Word2Vec is prediction based, while GloVe is frequency based; both create compact and meaningful word representations. </li> &#13;
   <li class="readable-text" id="p299"> LLMs have revolutionized the entire landscape for text datasets. </li> &#13;
  </ul>&#13;
 </body></html>