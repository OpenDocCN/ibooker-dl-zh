- en: Chapter 7\. Finetuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。微调
- en: Finetuning is the process of adapting a model to a specific task by further
    training the whole model or part of the model. Chapters [5](ch05.html#ch05a_prompt_engineering_1730156991195551)
    and [6](ch06.html#ch06_rag_and_agents_1730157386571386) discuss prompt-based methods,
    which adapt a model by giving it instructions, context, and tools. Finetuning
    adapts a model by adjusting its weights.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是通过进一步训练整个模型或模型的一部分来适应特定任务的过程。第[5](ch05.html#ch05a_prompt_engineering_1730156991195551)章和第[6](ch06.html#ch06_rag_and_agents_1730157386571386)章讨论了基于提示的方法，这些方法通过提供指令、上下文和工具来调整模型。微调通过调整其权重来调整模型。
- en: Finetuning can enhance various aspects of a model. It can improve the model’s
    domain-specific capabilities, such as coding or medical question answering, and
    can also strengthen its safety. However, it is most often used to improve the
    model’s instruction-following ability, particularly to ensure it adheres to specific
    output styles and formats.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以增强模型的各个方面。它可以提高模型在特定领域的功能，例如编码或医学问答，也可以加强其安全性。然而，它最常用于提高模型的指令遵循能力，尤其是确保它遵循特定的输出样式和格式。
- en: While finetuning can help create models that are more customized to your needs,
    it also requires more up-front investment. A question I hear very often is when
    to finetune and when to do RAG. After an overview of finetuning, this chapter
    will discuss the reasons for finetuning and the reasons for not finetuning, as
    well as a simple framework for thinking about choosing between finetuning and
    alternate methods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然微调可以帮助创建更符合您需求的模型，但它也要求更多的前期投资。我经常听到的一个问题是何时进行微调以及何时进行RAG。在概述微调之后，本章将讨论微调的原因以及不微调的原因，以及一个关于在微调和替代方法之间进行选择的简单框架。
- en: Compared to prompt-based methods, finetuning incurs a much higher memory footprint.
    At the scale of today’s foundation models, naive finetuning often requires more
    memory than what’s available on a single GPU. This makes finetuning expensive
    and challenging to do. As discussed throughout this chapter, reducing memory requirements
    is a primary motivation for many finetuning techniques. This chapter dedicates
    one section to outlining factors contributing to a model’s memory footprint, which
    is important for understanding these techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于提示的方法相比，微调的内存占用要大得多。在当今基础模型的规模上，简单的微调通常需要的内存比单个GPU上可用的内存还要多。这使得微调变得昂贵且具有挑战性。正如本章所讨论的，降低内存需求是许多微调技术的首要动机。本章专门用一节概述了导致模型内存占用大小的因素，这对于理解这些技术很重要。
- en: 'A memory-efficient approach that has become dominant in the finetuning space
    is PEFT (parameter-efficient finetuning). This chapter explores PEFT and how it
    differs from traditional finetuning; this chapter also provides an overview of
    its evolving techniques. I’ll focus particularly on one compelling category: adapter-based
    techniques.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调领域已成为主流的一种内存高效方法是PEFT（参数高效微调）。本章探讨了PEFT以及它与传统微调的不同之处；本章还概述了其演变的技术。我将特别关注一个引人入胜的类别：基于适配器的技术。
- en: With prompt-based methods, knowledge about how ML models operate under the hood
    is recommended but not strictly necessary. However, finetuning brings you to the
    realm of model training, where ML knowledge is required. ML basics are beyond
    the scope of this book. If you want a quick refresh, the book’s [GitHub repository](https://github.com/chiphuyen/aie-book)
    has pointers to helpful resources. In this chapter, I’ll cover a few core concepts
    immediately relevant to the discussion.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于提示的方法中，了解ML模型在底层如何运行的知识是推荐的，但并非绝对必要。然而，微调将您带入模型训练的领域，在那里需要ML知识。ML基础知识超出了本书的范围。如果您想快速复习，本书的[GitHub仓库](https://github.com/chiphuyen/aie-book)提供了指向有用资源的链接。在本章中，我将立即介绍与讨论直接相关的几个核心概念。
- en: This chapter is the most technically challenging one for me to write, not because
    of the complexity of the concepts, but because of the broad scope these concepts
    cover. I suspect it might also be technically challenging to read. If, at any
    point, you feel like you’re diving too deep into details that aren’t relevant
    to your work, feel free to skip.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这章对我来说是写作中最具技术挑战性的章节，不是因为概念本身的复杂性，而是因为这些概念覆盖的范围很广。我怀疑它可能对阅读来说也可能具有技术挑战性。如果您在任何时候感觉您正在深入研究与您的工作无关的细节，请随时跳过。
- en: There’s a lot to discuss. Let’s dive in!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多要讨论的内容。让我们深入探讨吧！
- en: Finetuning Overview
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调概述
- en: To finetune, you start with a base model that has some, but not all, of the
    capabilities you need. The goal of finetuning is to get this model to perform
    well enough for your specific task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行微调，你从一个具有所需部分能力但并非全部能力的基础模型开始。微调的目标是使该模型在特定任务上表现良好。
- en: 'Finetuning is one way to do *transfer learning*, a concept first introduced
    by [Bozinovski and Fulgosi](https://oreil.ly/Udw0Z) in 1976\. Transfer learning
    focuses on how to transfer the knowledge gained from one task to accelerate learning
    for a new, related task. This is conceptually similar to how humans transfer skills:
    for example, knowing how to play the piano can make it easier to learn another
    musical instrument.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是进行迁移学习的一种方式，这一概念最早由[Bozinovski和Fulgosi](https://oreil.ly/Udw0Z)在1976年提出。迁移学习关注的是如何将从一个任务中获得的知识转移到加速新相关任务的学习。这在概念上类似于人类技能的迁移：例如，知道如何弹钢琴可以使得学习另一种乐器更容易。
- en: An early large-scale success in transfer learning was Google’s multilingual
    translation system ([Johnson et. al, 2016](https://arxiv.org/abs/1611.04558)).
    The model transferred its knowledge of Portuguese–English and English–Spanish
    translation to directly translate Portuguese to Spanish, even though there were
    no Portuguese–Spanish examples in the training data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习早期的一个大规模成功案例是谷歌的多语言翻译系统([Johnson et. al, 2016](https://arxiv.org/abs/1611.04558))。该模型将葡萄牙语-英语和英语-西班牙语的翻译知识转移到直接将葡萄牙语翻译成西班牙语，尽管训练数据中没有葡萄牙语-西班牙语的例子。
- en: Since the early days of deep learning, transfer learning has offered a solution
    for tasks with limited or expensive training data. By training a base model on
    tasks with abundant data, you can then transfer that knowledge to a target task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自从深度学习的早期以来，迁移学习为数据有限或昂贵的任务提供了一种解决方案。通过在数据丰富的任务上训练基础模型，然后将这些知识转移到目标任务。
- en: For LLMs, knowledge gained from pre-training on text completion (a task with
    abundant data) is transferred to more specialized tasks, like legal question answering
    or text-to-SQL, which often have less available data. This capability for transfer
    learning makes foundation models particularly valuable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型（LLMs）来说，从文本补全（一个数据量丰富的任务）中预训练获得的知识被转移到更多专业化的任务，如法律问答或文本到SQL，这些任务通常可用的数据较少。这种迁移学习的能力使得基础模型特别有价值。
- en: Transfer learning improves *sample efficiency*, allowing a model to learn the
    same behavior with fewer examples. A *sample-efficient* model learns effectively
    from fewer samples. For example, while training a model from scratch for legal
    question answering may need millions of examples, finetuning a good base model
    might only require a few hundred.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习提高了样本效率，允许模型用更少的示例学习相同的行为。一个样本高效的模型可以从更少的样本中有效地学习。例如，从头开始训练法律问答模型可能需要数百万个示例，而微调一个优秀的基模型可能只需要几百个。
- en: Ideally, much of what the model needs to learn is already present in the base
    model, and finetuning just refines the model’s behavior. OpenAI’s [InstructGPT
    paper](https://oreil.ly/5-5lw) (2022) suggested viewing finetuning as unlocking
    the capabilities a model already has but that are difficult for users to access
    via prompting alone.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，模型需要学习的大部分内容已经存在于基础模型中，微调只是对模型行为的优化。OpenAI的[InstructGPT论文](https://oreil.ly/5-5lw)（2022）建议将微调视为解锁模型已经具备但用户仅通过提示难以访问的能力。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Finetuning isn’t the only way to do transfer learning. Another approach is *feature-based
    transfer*. In this approach, a model is trained to extract features from the data,
    usually as embedding vectors, which are then used by another model. I mention
    feature-based transfer briefly in [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359),
    when discussing how part of a foundation model can be reused for a classification
    task by *adding a classifier head*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 微调并不是进行迁移学习的唯一方式。另一种方法是基于特征的迁移。在这种方法中，模型被训练来从数据中提取特征，通常作为嵌入向量，然后这些特征被另一个模型使用。我在[第二章](ch02.html#ch02_understanding_foundation_models_1730147895571359)中简要提到了基于特征的迁移，当时讨论了如何通过添加分类器头部，将基础模型的一部分用于分类任务。
- en: Feature-based transfer is very common in computer vision. For instance, in the
    second half of the 2010s, many people used models trained on the ImagetNet dataset
    to extract features from images and use these features in other computer vision
    tasks such as object detection or image segmentation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的学习迁移在计算机视觉中非常常见。例如，在2010年代后半期，许多人使用在ImageNet数据集上训练的模型从图像中提取特征，并将这些特征用于其他计算机视觉任务，如目标检测或图像分割。
- en: 'Finetuning is part of a model’s training process. It’s an extension of model
    pre-training. Because any training that happens after pre-training is finetuning,
    finetuning can take many different forms. [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)
    already discussed two types of finetuning: supervised finetuning and preference
    finetuning. Let’s do a quick recap of these methods and how you might leverage
    them as an application developer.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是模型训练过程的一部分。它是模型预训练的扩展。因为预训练之后发生的任何训练都是微调，所以微调可以采取多种不同的形式。[第2章](ch02.html#ch02_understanding_foundation_models_1730147895571359)已经讨论了两种微调类型：监督微调和偏好微调。让我们快速回顾这些方法以及您作为应用程序开发者如何利用它们。
- en: Recall that a model’s training process starts with *pre-training*, which is
    usually done with self-supervision. Self-supervision allows the model to learn
    from a large amount of unlabeled data. For language models, self-supervised data
    is typically just *sequences of text* that don’t need annotations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，一个模型的训练过程从*预训练*开始，这通常是通过自监督完成的。自监督使模型能够从大量未标记的数据中学习。对于语言模型，自监督数据通常是*文本序列*，不需要标注。
- en: Before finetuning this pre-trained model with expensive task-specific data,
    you can finetune it with self-supervision using cheap task-related data. For example,
    to finetune a model for legal question answering, before finetuning it on expensive
    annotated (question, answer) data, you can finetune it on raw legal documents.
    Similarly, to finetune a model to do book summarization in Vietnamese, you can
    first finetune it on a large collection of Vietnamese text. *Self-supervised finetuning*
    is also called *continued pre-training*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用昂贵的特定任务数据微调此预训练模型之前，您可以使用廉价的与任务相关的数据进行自监督微调。例如，为了微调一个用于法律问答的模型，在微调昂贵的标注（问题，答案）数据之前，您可以在原始法律文件上对其进行微调。同样，为了微调一个用于越南语书籍摘要的模型，您可以先在大量的越南语文本集合上对其进行微调。*自监督微调*也被称为*持续预训练*。
- en: As discussed in [Chapter 1](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319),
    language models can be autoregressive or masked. An autoregressive model predicts
    the next token in a sequence using the previous tokens as the context. A masked
    model fills in the blank using the tokens both before and after it. Similarly,
    with supervised finetuning, you can also finetune a model to predict the next
    token or fill in the blank. The latter, also known as *infilling finetuning*,
    is especially useful for tasks such as text editing and code debugging. You can
    finetune a model for infilling even if it was pre-trained autoregressively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#ch01_introduction_to_building_ai_applications_with_foun_1730130814984319)所述，语言模型可以是自回归的或掩码的。自回归模型使用前一个标记作为上下文来预测序列中的下一个标记。掩码模型使用其前后标记来填充空白。同样，在监督微调中，您也可以微调模型以预测下一个标记或填充空白。后者也称为*填充微调*，对于文本编辑和代码调试等任务特别有用。即使模型是自回归预训练的，您也可以为填充微调微调模型。
- en: The massive amount of data a model can learn from during self-supervised learning
    outfits the model with a rich understanding of the world, but it might be hard
    for users to extract that knowledge for their tasks, or the way the model behaves
    might be misaligned with human preference. Supervised finetuning uses high-quality
    annotated data to refine the model to align with human usage and preference.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在自监督学习期间可以学习的大量数据为模型提供了对世界的丰富理解，但用户可能难以提取这些知识用于他们的任务，或者模型的行为可能与人类偏好不一致。监督微调使用高质量的标注数据来细化模型，使其与人类使用和偏好保持一致。
- en: 'During *supervised finetuning*, the model is trained using (input, output)
    pairs: the input can be an instruction and the output can be a response. A response
    can be open-ended, such as for the task of book summarization. A response can
    be also close-ended, such as for a classification task. High-quality instruction
    data can be challenging and expensive to create, especially for instructions that
    require factual consistency, domain expertise, or political correctness. [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888)
    discusses how to acquire instruction data.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在*监督微调*期间，模型使用（输入，输出）对进行训练：输入可以是指令，输出可以是响应。响应可以是开放式的，例如用于书籍摘要任务。响应也可以是封闭式的，例如用于分类任务。高质量的指令数据可能难以创建且成本高昂，特别是对于需要事实一致性、领域专业知识或政治正确的指令。
    [第8章](ch08.html#ch08_dataset_engineering_1730130932019888) 讨论了如何获取指令数据。
- en: A model can also be finetuned with reinforcement learning to generate responses
    that maximize human preference. Preference finetuning requires comparative data
    that typically follows the format (instruction, winning response, losing response).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 模型还可以通过强化学习进行调优，以生成最大化人类偏好的响应。偏好调优需要比较数据，通常遵循以下格式（指令，获胜响应，失败响应）。
- en: It’s possible to finetune a model to extend its context length. *Long-context
    finetuning* typically requires modifying the model’s architecture, such as adjusting
    the positional embeddings. A long sequence means more possible positions for tokens,
    and positional embeddings should be able to handle them. Compared to other finetuning
    techniques, long-context finetuning is harder to do. The resulting model might
    also degrade on shorter sequences.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能通过调优来扩展模型上下文的长度。*长上下文调优*通常需要修改模型的架构，例如调整位置嵌入。长序列意味着有更多可能的标记位置，位置嵌入应该能够处理它们。与其他调优技术相比，长上下文调优更难实现。结果模型在较短序列上也可能退化。
- en: '[Figure 7-1](#ch07a_figure_1_1730160615799658) shows the making of different
    Code Llama models ([Rozière et al., 2024](https://arxiv.org/abs/2308.12950)),
    from the base model Llama 2, using different finetuning techniques. Using long-context
    finetuning, they were able to increase the model’s maximum context length from
    4,096 tokens to 16,384 tokens to accommodate longer code files. In the image,
    instruction finetuning refers to supervised finetuning.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-1](#ch07a_figure_1_1730160615799658) 展示了不同Code Llama模型的制作过程（Rozière等人，2024年[https://arxiv.org/abs/2308.12950]），从基础模型Llama
    2开始，使用不同的调优技术。通过使用长上下文调优，他们能够将模型的最大上下文长度从4,096个标记增加到16,384个标记，以适应更长的代码文件。在图像中，指令调优指的是监督调优。'
- en: Finetuning can be done by both model developers and application developers.
    Model developers typically post-train a model with different finetuning techniques
    before releasing it. A model developer might also release different model versions,
    each finetuned to a different extent, so that application developers can choose
    the version that works best for them.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 调优可以由模型开发者和应用开发者共同完成。模型开发者通常在发布模型之前，使用不同的调优技术对模型进行后训练。模型开发者还可能发布不同版本的模型，每个版本都根据不同的程度进行调优，以便应用开发者可以选择最适合他们的版本。
- en: '![A diagram of a program'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![程序图'
- en: Description automatically generated](assets/aien_0701.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0701.png)
- en: Figure 7-1\. Different finetuning techniques used to make different Code Llama
    models. Image from the Rozière et al. (2024). Adapted from an original image licensed
    under CC BY 4.0.
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 制作不同Code Llama模型所使用的不同调优技术。图片来自Rozière等人（2024年）。改编自原始图片，许可协议为CC BY 4.0。
- en: As an application developer, you might finetune a pre-trained model, but most
    likely, you’ll finetune a model that has been post-trained. The more refined a
    model is and the more relevant its knowledge is to your task, the less work you’ll
    have to do to adapt it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名应用开发者，你可能会对预训练模型进行调优，但更有可能的是，你会对已经过后训练的模型进行调优。模型越精细，其知识与你任务的相关性越强，你适应它所需的工作就越少。
- en: When to Finetune
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时进行调优
- en: Before jumping into different finetuning techniques, it’s necessary to consider
    whether finetuning is the right option for you. Compared to prompt-based methods,
    finetuning requires significantly more resources, not just in data and hardware,
    but also in ML talent. Therefore, finetuning is generally attempted *after* extensive
    experiments with prompt-based methods. However, finetuning and prompting aren’t
    mutually exclusive. Real-world problems often require both approaches.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究不同的调优技术之前，考虑是否调优是你正确的选择是必要的。与基于提示的方法相比，调优需要显著更多的资源，不仅包括数据和硬件，还包括机器学习人才。因此，调优通常是在对基于提示的方法进行广泛实验之后尝试的。然而，调优和提示并不是相互排斥的。现实世界的问题通常需要这两种方法。
- en: Reasons to Finetune
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调优的理由
- en: The primary reason for finetuning is to improve a model’s quality, in terms
    of both general capabilities and task-specific capabilities. Finetuning is commonly
    used to improve a model’s ability to generate outputs following specific structures,
    such as JSON or YAML formats.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 调优的主要原因是提高模型的质量，无论是从一般能力还是特定任务能力来看。调优通常用于提高模型生成遵循特定结构输出的能力，例如JSON或YAML格式。
- en: A general-purpose model that performs well on a wide range of benchmarks might
    not perform well on your specific task. If the model you want to use wasn’t sufficiently
    trained on your task, finetuning it with your data can be especially useful.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用模型在广泛的基准测试中表现良好，但可能不会在你特定的任务上表现良好。如果你想要使用的模型在任务上训练不足，使用你的数据进行微调可能特别有用。
- en: For example, an out-of-the-box model might be good at converting from text to
    the standard SQL dialect but might fail with a less common SQL dialect. In this
    case, finetuning this model on data containing this SQL dialect will help. Similarly,
    if the model works well on standard SQL for common queries but often fails for
    customer-specific queries, finetuning the model on customer-specific queries might
    help.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个开箱即用的模型可能在将文本转换为标准SQL方言方面做得很好，但可能无法处理较少见的SQL方言。在这种情况下，在包含这种SQL方言的数据上微调该模型将有所帮助。同样，如果模型在标准SQL上对常见查询表现良好，但经常在客户特定查询上失败，那么在客户特定查询上微调模型可能有所帮助。
- en: One especially interesting use case of finetuning is bias mitigation. The idea
    is that if the base model perpetuates certain biases from its training data, exposing
    it to carefully curated data during finetuning can counteract these biases ([Wang
    and Russakovsky, 2023](https://oreil.ly/iPwB_)). For example, if a model consistently
    assigns CEOs male-sounding names, finetuning it on a dataset with many female
    CEOs can mitigate this bias. [Garimella et al. (2022)](https://oreil.ly/RoPL4)
    found that finetuning BERT-like language models on text authored by women can
    reduce these models’ gender biases, while finetuning them on texts by African
    authors can reduce racial biases.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的一个特别有趣的用例是偏差缓解。其想法是，如果基模型在其训练数据中持续存在某些偏差，那么在微调期间暴露于精心挑选的数据可以抵消这些偏差([王和Russakovsky，2023](https://oreil.ly/iPwB_))。例如，如果一个模型始终将CEO分配给听起来像男性的名字，那么在包含许多女性CEO的数据集上微调它可以缓解这种偏差。[Garimella等人(2022)](https://oreil.ly/RoPL4)发现，在由女性撰写的文本上微调BERT类语言模型可以减少这些模型的性别偏差，而在非洲作者撰写的文本上微调可以减少种族偏差。
- en: You can finetune a big model to make it even better, but finetuning smaller
    models is much more common. Smaller models require less memory, and, therefore,
    are easier to finetune. They are also cheaper and faster to use in production.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以微调一个大模型以使其变得更好，但微调较小的模型更为常见。较小的模型需要的内存更少，因此更容易微调。它们在生产中也更便宜、更快。
- en: A common approach is to finetune a small model to imitate the behavior of a
    larger model using data generated by this large model. Because this approach distills
    the larger model’s knowledge into the smaller model, it’s called *distillation*.
    This is discussed in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888)
    together with other data synthesis techniques.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是使用由大型模型生成的数据来微调一个小型模型，以模仿大型模型的行为。因为这种方法将大型模型的知识提炼到小型模型中，所以被称为*蒸馏*。这在与其他数据合成技术一起在第8章(ch08.html#ch08_dataset_engineering_1730130932019888)中进行了讨论。
- en: A small model, finetuned on a specific task, might outperform a much larger
    out-of-the-box model on that task. For example, Grammarly found that their finetuned
    Flan-T5 models ([Chung et al., 2022](https://arxiv.org/abs/2210.11416)) outperformed
    a GPT-3 variant specialized in text editing across a wide range of writing assistant
    tasks despite being 60 times smaller. The finetuning process used only 82,000
    (instruction, output) pairs, which is smaller than the data typically needed to
    train a text-editing model from scratch.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在特定任务上微调的小型模型可能在那个任务上优于一个更大的开箱即用的模型。例如，Grammarly发现他们的微调Flan-T5模型([Chung等人，2022](https://arxiv.org/abs/2210.11416))在广泛的写作助手任务上优于一个专门用于文本编辑的GPT-3变种，尽管它只有60分之一的大小。微调过程只使用了82,000对(指令，输出)，这比从头开始训练一个文本编辑模型所需的数据要少。
- en: In the early days of foundation models, when the strongest models were commercial
    with limited finetuning access, there weren’t many competitive models available
    for finetuning. However, as the open source community proliferates with high-quality
    models of all sizes, tailored for a wide variety of domains, finetuning has become
    a lot more viable and attractive.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础模型早期，当最强的模型是商业化的且微调访问有限时，可用于微调的竞争性模型并不多。然而，随着开源社区中各种大小的高质量模型增多，这些模型针对广泛的领域定制，微调变得更多可行和吸引人。
- en: Reasons Not to Finetune
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不进行微调的原因
- en: While finetuning can improve a model in many ways, many of these improvements
    can also be achieved, to a certain extent, without finetuning. Finetuning can
    improve a model’s performance, but so do carefully crafted prompts and context.
    Finetuning can help with structured outputs, but many other techniques, as discussed
    in [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359),
    can also do that.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然微调可以从许多方面改善模型，但这些改进在一定程度上也可以在不进行微调的情况下实现。微调可以提高模型的表现，但精心设计的提示和上下文也可以做到这一点。微调可以帮助生成结构化输出，但正如在第2章[2](ch02.html#ch02_understanding_foundation_models_1730147895571359)中讨论的，许多其他技术也可以做到这一点。
- en: First, while finetuning a model for a specific task can improve its performance
    for that task, it can degrade its performance for other tasks.^([1](ch07.html#id1369))
    This can be frustrating when you intend this model for an application that expects
    diverse prompts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，虽然为特定任务微调模型可以改善其在该任务上的性能，但它可能会降低其在其他任务上的性能。[1](ch07.html#id1369) 这在你打算将此模型应用于期望多样化提示的应用时可能会令人沮丧。
- en: 'Imagine you need a model for three types of queries: product recommendations,
    changing orders, and general feedback. Originally, the model works well for product
    recommendations and general feedback but poorly for changing orders. To fix this,
    you finetune the model on a dataset of (query, response) pairs about changing
    orders. The finetuned model might indeed perform better for this type of query,
    but worse for the two other tasks.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要一个模型来处理三种类型的查询：产品推荐、更改订单和一般反馈。最初，该模型在产品推荐和一般反馈方面表现良好，但在更改订单方面表现不佳。为了解决这个问题，你需要在关于更改订单的（查询，响应）对数据集上微调模型。微调后的模型可能确实能更好地处理这类查询，但在其他两个任务上表现可能更差。
- en: What do you do in this situation? You can finetune the model on all the queries
    you care about, not just changing orders. If you can’t seem to get a model to
    perform well on all your tasks, consider using separate models for different tasks.
    If you wish to combine these separate models into one to make serving them easier,
    you can also consider merging them together, as discussed later in this chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你该怎么办？你可以对所有你关心的查询进行微调，而不仅仅是更改订单。如果你似乎无法使模型在所有任务上都表现良好，考虑为不同的任务使用不同的模型。如果你希望将这些单独的模型合并为一个以简化服务，你也可以考虑将它们合并在一起，正如本章后面所讨论的。
- en: If you’re just starting to experiment with a project, finetuning is rarely the
    first thing you should attempt. Finetuning requires high up-front investments
    and continual maintenance. First, you need data. Annotated data can be slow and
    expensive to acquire manually, especially for tasks that demand critical thinking
    and domain expertise. Open source data and AI-generated data can mitigate the
    cost, but their effectiveness is highly variable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始尝试一个项目，微调通常不是你应该尝试的第一件事。微调需要大量的前期投资和持续的维护。首先，你需要数据。手动获取标注数据可能既慢又贵，尤其是对于需要批判性思维和领域专业知识的工作。开源数据和AI生成数据可以减轻成本，但它们的有效性差异很大。
- en: Second, finetuning requires the knowledge of how to train models. You need to
    evaluate base models to choose one to finetune. Depending on your needs and resources,
    options might be limited. While finetuning frameworks and APIs can automate many
    steps in the actual finetuning process, you still need to understand the different
    training knobs you can tweak, monitor the learning process, and debug when something
    is wrong. For example, you need to understand how an optimizer works, what learning
    rate to use, how much training data is needed, how to address overfitting/underfitting,
    and how to evaluate your models throughout the process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，微调需要了解如何训练模型。你需要评估基础模型以选择一个进行微调。根据你的需求和资源，选项可能有限。虽然微调框架和API可以自动化微调过程中的许多步骤，但你仍然需要了解你可以调整的不同训练旋钮，监控学习过程，并在出现问题时代码调试。例如，你需要了解优化器是如何工作的，应该使用什么学习率，需要多少训练数据，如何处理过拟合/欠拟合，以及如何在整个过程中评估你的模型。
- en: Third, once you have a finetuned model, you’ll need to figure out how to serve
    it. Will you host it yourself or use an API service? As discussed in [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301),
    inference optimization for large models, especially LLMs, isn’t trivial. Finetuning
    requires less of a technical leap if you’re already hosting your models in-house
    and familiar with how to operate models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，一旦你有了微调模型，你需要弄清楚如何提供服务。你会自己托管它还是使用API服务？正如第9章中讨论的[Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301)，大型模型（尤其是LLM）的推理优化并不简单。如果你已经在内部托管模型并且熟悉如何操作模型，微调需要的技术飞跃就会小得多。
- en: More importantly, you need to establish a policy and budget for monitoring,
    maintaining, and updating your model. As you iterate on your finetuned model,
    new base models are being developed at a rapid pace. These base models may improve
    faster than you can enhance your finetuned model. If a new base model outperforms
    your finetuned model on your specific task, how significant does the performance
    improvement have to be before you switch to the new base model? What if a new
    base model doesn’t immediately outperform your existing model but has the potential
    to do so after finetuning—would you experiment with it?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，你需要制定监控、维护和更新你的模型的政策和预算。随着你对微调模型进行迭代，新的基础模型正在以快速的速度开发。这些基础模型可能比你能增强你的微调模型的速度更快。如果一个新基础模型在你的特定任务上优于你的微调模型，那么在切换到新基础模型之前，性能改进需要有多显著？如果一个新的基础模型在微调后没有立即优于你的现有模型，但有可能做到，你会对其进行实验吗？
- en: In many cases, switching to a better model would provide only a small incremental
    improvement, and your task might be given a lower priority than projects with
    larger returns, like enabling new use cases.^([2](ch07.html#id1370))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，切换到更好的模型只会带来微小的增量改进，你的任务可能比具有更大回报的项目（如启用新用例）的优先级低。[2](ch07.html#id1370)
- en: AI engineering experiments should start with prompting, following the best practices
    discussed in [Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386). Explore
    more advanced solutions only if prompting alone proves inadequate. Ensure you
    have thoroughly tested various prompts, as a model’s performance can vary greatly
    with different prompts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: AI工程实验应该从提示开始，遵循第6章中讨论的最佳实践。[Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386)。只有当单独使用提示不足以解决问题时，才探索更高级的解决方案。确保你已经彻底测试了各种提示，因为模型在不同提示下的性能可能会有很大差异。
- en: Many practitioners I’ve spoken with share a similar story that goes like this.
    Someone complains that prompting is ineffective and insists on finetuning. Upon
    investigation, it turns out that prompt experiments were minimal and unsystematic.
    Instructions were unclear, examples didn’t represent actual data, and metrics
    were poorly defined. After refining the prompt experiment process, the prompt
    quality improved enough to be sufficient for their application.^([3](ch07.html#id1371))
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我与许多从业者交谈过，他们分享了一个类似的故事。有人抱怨提示无效，并坚持微调。经过调查，发现提示实验很少且缺乏系统性。指令不明确，示例没有代表实际数据，指标定义不明确。在完善提示实验流程后，提示质量得到了足够的提升，足以满足他们的应用需求。[3](ch07.html#id1371)
- en: Both finetuning and prompting experiments require systematic processes. Doing
    prompt experiments enables developers to build an evaluation pipeline, data annotation
    guideline, and experiment tracking practices that will be stepping stones for
    finetuning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 微调和提示实验都需要系统化的流程。进行提示实验可以使开发者构建评估流程、数据标注指南和实验跟踪实践，这些将成为微调的垫脚石。
- en: One benefit of finetuning, before prompt caching was introduced, was that it
    can help optimize token usage. The more examples you add to a prompt, the more
    input tokens the model will use, which increases both latency and cost. Instead
    of including your examples in each prompt, you can finetune a model on these examples.
    This allows you to use shorter prompts with the finetuned model, as shown in [Figure 7-2](#ch07a_figure_2_1730160615799676).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示缓存引入之前，微调的一个好处是它可以帮助优化令牌使用。你添加到提示中的示例越多，模型将使用的输入令牌就越多，这会增加延迟和成本。你不必在每个提示中包含你的示例，而可以在这些示例上微调模型。这样，你可以使用更短的提示与微调后的模型一起使用，如图[图7-2](#ch07a_figure_2_1730160615799676)所示。
- en: With prompt caching, where repetitive prompt segments can be cached for reuse,
    this is no longer a strong benefit. Prompt caching is discussed further in [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301).
    However, the number of examples you can use with a prompt is still limited by
    the maximum context length. With finetuning, there’s no limit to how many examples
    you can use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提示缓存，其中重复的提示段可以缓存以供重用，这不再是强大的优势。提示缓存将在第9章中进一步讨论。[第9章](ch09.html#ch09_inference_optimization_1730130963006301)。然而，你可以使用提示的示例数量仍然受最大上下文长度的限制。使用微调，你可以使用示例的数量没有限制。
- en: '![A diagram of a model'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![模型图'
- en: Description automatically generated](assets/aien_0702.png)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](assets/aien_0702.png)'
- en: Figure 7-2\. Instead of including examples in each prompt, which increases cost
    and latency, you finetune a model on these examples.
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2。不是在每个提示中包含示例，这会增加成本和延迟，而是在这些示例上微调模型。
- en: Finetuning and RAG
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调和RAG
- en: Once you’ve maximized the performance gains from prompting, you might wonder
    whether to do RAG or finetuning next. The answer depends on whether your model’s
    failures are information-based or behavior-based.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你已经从提示中获得了性能提升的最大化，你可能会想知道接下来是进行RAG还是微调。答案取决于你的模型失败是基于信息还是基于行为。
- en: '*If the model fails because it lacks information, a RAG system that gives the
    model access to the relevant sources of information can help*. Information-based
    failures happen when the outputs are factually wrong or outdated. Here are two
    example scenarios in which information-based failures happen:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果模型因为缺乏信息而失败，一个允许模型访问相关信息源的RAG系统可以提供帮助*。信息性失败发生在输出事实错误或过时的情况下。以下是在两种情况下发生信息性失败的示例场景：'
- en: The model doesn’t have the information.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型没有信息。
- en: Public models are unlikely to have information private to you or your organization.
    When a model doesn’t have the information, it either tells you so or hallucinates
    an answer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 公共模型不太可能拥有你或你组织特有的信息。当模型没有信息时，它会告诉你，或者编造一个答案。
- en: The model has outdated information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型有陈旧的信息。
- en: 'If you ask: “How many studio albums has Taylor Swift released?” and the correct
    answer is 11, but the model answers 10, it can be because the model’s cut-off
    date was before the release of the latest album.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问：“泰勒·斯威夫特发行了多少张录音室专辑？”正确的答案是11张，但模型回答的是10张，这可能是因为模型的截止日期在最新专辑发行之前。
- en: The paper [“Fine-Tuning or Retrieval?”](https://oreil.ly/t9HTH) by Ovadia et
    al. (2024) demonstrated that for tasks that require up-to-date information, such
    as questions about current events, RAG outperformed finetuned models. Not only
    that, RAG with the base model outperformed RAG with finetuned models, as shown
    in [Table 7-2](#ch07a_table_2_1730160615803962). This finding indicates that *while
    finetuning can enhance a model’s performance on a specific task, it may also lead
    to a decline in performance in other areas.*
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Ovadia等人（2024）的论文[“微调还是检索？”](https://oreil.ly/t9HTH)表明，对于需要最新信息的任务，如关于当前事件的问题，RAG优于微调模型。不仅如此，使用基础模型的RAG优于使用微调模型的RAG，如[表7-2](#ch07a_table_2_1730160615803962)所示。这一发现表明，*虽然微调可以提高模型在特定任务上的性能，但它也可能导致其他领域的性能下降*。
- en: Table 7-2\. RAG outperforms finetuning on a question-answering task about current
    events, curated by Ovadia et al. (2024). FT-reg and FT-par refer to two different
    finetuning approaches the author used.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-2。在Ovadia等人（2024）编纂的关于当前事件的问答任务中，RAG优于微调。FT-reg和FT-par指的是作者使用的两种不同的微调方法。
- en: '|  | Base model | Base model + RAG | FT-reg | FT-par | FT-reg + RAG | FT-par
    + RAG |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | 基础模型 | 基础模型 + RAG | FT-reg | FT-par | FT-reg + RAG | FT-par + RAG |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Mistral-7B | 0.481 | 0.875 | 0.504 | 0.588 | 0.810 | 0.830 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 0.481 | 0.875 | 0.504 | 0.588 | 0.810 | 0.830 |'
- en: '| Llama 2-7B | 0.353 | 0.585 | 0.219 | 0.392 | 0.326 | 0.520 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2-7B | 0.353 | 0.585 | 0.219 | 0.392 | 0.326 | 0.520 |'
- en: '| Orca 2-7B | 0.456 | 0.876 | 0.511 | 0.566 | 0.820 | 0.826 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Orca 2-7B | 0.456 | 0.876 | 0.511 | 0.566 | 0.820 | 0.826 |'
- en: On the other hand, *if the model has behavioral issues, finetuning might help*.
    One behavioral issue is when the model’s outputs are factually correct but irrelevant
    to the task. For example, you ask the model to generate technical specifications
    for a software project to provide to your engineering teams. While accurate, the
    generated specs lack the details your teams need. Finetuning the model with well-defined
    technical specifications can make the outputs more relevant.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*如果模型存在行为问题，微调可能有所帮助*。一个行为问题是当模型的输出在事实上是正确的，但与任务无关。例如，你要求模型为软件项目生成技术规范以供工程团队使用。虽然准确，但生成的规范缺少团队需要的细节。使用定义良好的技术规范微调模型可以使输出更加相关。
- en: Another issue is when it fails to follow the expected output format. For example,
    if you asked the model to write HTML code, but the generated code didn’t compile,
    it might be because the model wasn’t sufficiently exposed to HTML in its training
    data. You can correct this by exposing the model to more HTML code during finetuning.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是没有遵循预期的输出格式。例如，如果你要求模型编写HTML代码，但生成的代码无法编译，可能是因为模型在训练数据中没有充分接触HTML。你可以在微调期间通过让模型接触更多HTML代码来纠正这一点。
- en: Semantic parsing is a category of tasks whose success hinges on the model’s
    ability to generate outputs in the expected format and, therefore, often requires
    finetuning. Semantic parsing is discussed briefly in Chapters [2](ch02.html#ch02_understanding_foundation_models_1730147895571359)
    and [6](ch06.html#ch06_rag_and_agents_1730157386571386). As a reminder, semantic
    parsing means converting natural language into a structured format like JSON.
    Strong off-the-shelf models are generally good for common, less complex syntaxes
    like JSON, YAML, and regex. However, they might not be as good for syntaxes with
    fewer available examples on the internet, such as a domain-specific language for
    a less popular tool or a complex syntax.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 语义解析是一类任务，其成功取决于模型生成预期格式的输出的能力，因此通常需要微调。语义解析在第二章[2](ch02.html#ch02_understanding_foundation_models_1730147895571359)和第六章[6](ch06.html#ch06_rag_and_agents_1730157386571386)中简要讨论。提醒一下，语义解析意味着将自然语言转换为结构化格式，如JSON。现成的强大模型通常适用于常见的、较简单的语法，如JSON、YAML和regex。然而，它们可能不适合互联网上可用示例较少的语法，例如不太受欢迎的工具的特定领域语言或复杂的语法。
- en: '*In short, finetuning is for form, and RAG is for facts*. A RAG system gives
    your model external knowledge to construct more accurate and informative answers.
    A RAG system can help mitigate your model’s hallucinations. Finetuning, on the
    other hand, helps your model understand and follow syntaxes and styles.^([5](ch07.html#id1377))
    While finetuning can potentially reduce hallucinations if done with enough high-quality
    data, it can also worsen hallucinations if the data quality is low.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*简而言之，微调是为了形式，而RAG是为了事实*。RAG系统为你的模型提供外部知识，以构建更准确和更有信息量的答案。RAG系统可以帮助减轻模型的幻觉。另一方面，微调有助于模型理解和遵循语法和风格。[5](ch07.html#id1377)
    虽然微调在足够高质量的数据下可能有助于减少幻觉，但如果数据质量低，也可能加剧幻觉。'
- en: If your model has both information and behavior issues, start with RAG. RAG
    is typically easier since you won’t have to worry about curating training data
    or hosting the finetuned models. When doing RAG, start with simple term-based
    solutions such as BM25 instead of jumping straight into something that requires
    vector databases.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型既有信息问题又有行为问题，可以从RAG开始。RAG通常更容易，因为你不必担心整理训练数据或托管微调模型。在进行RAG时，从简单的基于术语的解决方案开始，例如BM25，而不是直接跳入需要向量数据库的东西。
- en: 'RAG can also introduce a more significant performance boost than finetuning.
    Ovadia et al. (2024) showed that for almost all question categories in the [MMLU
    benchmark](https://arxiv.org/abs/2009.03300), RAG outperforms finetuning for three
    different models: Mistral 7B, Llama 2-7B, and Orca 2-7B.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RAG还可以比微调带来更显著的性能提升。Ovadia等人（2024）表明，在[MMLU基准](https://arxiv.org/abs/2009.03300)的几乎所有问题类别中，对于Mistral
    7B、Llama 2-7B和Orca 2-7B这三种不同的模型，RAG都优于微调。
- en: However, RAG and finetuning aren’t mutually exclusive. They can sometimes be
    used together to maximize your application’s performance. In the same experiment,
    [Ovadia et al. (2024)](https://oreil.ly/t9HTH) showed that incorporating RAG on
    top of a finetuned model can boost its performance on the MMLU benchmark 43% of
    the time. It’s important to note that in this experiment, using RAG with finetuned
    models doesn’t improve the performance 57% of the time, compared to using RAG
    alone.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RAG和微调并不是相互排斥的。它们有时可以一起使用以最大化您应用程序的性能。在相同的实验中，[Ovadia等人（2024）](https://oreil.ly/t9HTH)
    展示了在微调模型之上结合RAG可以提高其在MMLU基准测试上的性能43%的时间。重要的是要注意，在这个实验中，与单独使用RAG相比，使用RAG与微调模型并不总是提高性能57%。
- en: There’s no universal workflow for all applications. [Figure 7-3](#ch07a_figure_3_1730160615799691)
    shows some paths an application development process might follow over time. The
    arrow indicates what next step you might try. This figure is inspired by an example
    workflow shown by [OpenAI](https://oreil.ly/Ny1WI) (2023).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 没有适用于所有应用程序的通用工作流程。[图7-3](#ch07a_figure_3_1730160615799691) 显示了应用程序开发过程可能随时间遵循的一些路径。箭头指示您可能尝试的下一步。这个图是受[OpenAI](https://oreil.ly/Ny1WI)（2023）展示的示例工作流程启发的。
- en: '![A diagram of a process'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![流程图'
- en: Description automatically generated](assets/aien_0703.png)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0703.png)
- en: Figure 7-3\. Example application development flows. After simple retrieval (such
    as term-based retrieval), whether to experiment with more complex retrieval (such
    as hybrid search) or finetuning depends on each application and its failure modes.
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3。示例应用程序开发流程。在简单的检索（如基于术语的检索）之后，是否尝试更复杂的检索（如混合搜索）或微调取决于每个应用程序及其失败模式。
- en: 'So the workflow to adapt a model to a task might work as follows. Note that
    before any of the adaptation steps, you should define your evaluation criteria
    and design your evaluation pipeline, as discussed in [Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863).
    This evaluation pipeline is what you’ll use to benchmark your progress as you
    develop your application. Evaluation doesn’t happen only in the beginning. It
    should be present during every step of the process:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将模型适应特定任务的流程可能如下所示。请注意，在执行任何适应步骤之前，您应该定义您的评估标准并设计您的评估流程，如第 [4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)
    中所述。这个评估流程是您在开发应用程序时用来衡量您进度的方式。评估不仅仅发生在开始时，它应该贯穿整个过程的每一步：
- en: Try to get a model to perform your task with prompting alone. Use the prompt
    engineering best practices covered in [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551),
    including systematically versioning your prompts.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试仅通过提示来让模型执行您的任务。使用第 [5章](ch05.html#ch05a_prompt_engineering_1730156991195551)
    中涵盖的提示工程最佳实践，包括系统地版本化您的提示。
- en: Add more examples to the prompt. Depending on the use case, the number of examples
    needed might be between 1 and 50.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提示中添加更多示例。根据用例，所需的示例数量可能在1到50之间。
- en: If your model frequently fails due to missing information, connect it to data
    sources that can supply relevant information. When starting with RAG, begin by
    using basic retrieval methods like term-based search. Even with simple retrieval,
    adding relevant and accurate knowledge should lead to some improvement in your
    model’s performance.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的模型经常由于信息缺失而失败，将其连接到可以提供相关信息的数据源。在开始使用RAG时，首先使用基本的检索方法，如基于术语的搜索。即使使用简单的检索，添加相关和准确的知识也应该导致您模型性能的某些改进。
- en: 'Depending on your model’s failure modes, you might explore one of these next
    steps:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您模型的失败模式，您可能想要探索以下这些下一步：
- en: If the model continues having information-based failures, you might want to
    try even more advanced RAG methods, such as embedding-based retrieval.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型继续出现基于信息失败的，您可能想要尝试更先进的RAG方法，例如基于嵌入的检索。
- en: If the model continues having behavioral issues, such as it keeps generating
    irrelevant, malformatted, or unsafe responses, you can opt for finetuning. Embedding-based
    retrieval increases inference complexity by introducing additional components
    into the pipeline, while finetuning increases the complexity of model development
    but leaves inference unchanged.
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型继续出现行为问题，例如持续生成无关、格式错误或不安全的响应，您可以选择微调。基于嵌入的检索通过在流程中引入额外的组件来增加推理复杂性，而微调则增加了模型开发的复杂性，但保持了推理不变。
- en: Combine both RAG and finetuning for even more performance boost.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结合RAG和微调以获得更多的性能提升。
- en: 'If, after considering all the pros and cons of finetuning and other alternate
    techniques, you decide to finetune your model, the rest of the chapter is for
    you. First, let’s look into the number one challenge of finetuning: its memory
    bottleneck.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑了微调和其他替代技术的利弊之后，如果你决定微调你的模型，本章的其余部分就是为你准备的。首先，让我们来看看微调的首要挑战：其内存瓶颈。
- en: Memory Bottlenecks
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存瓶颈
- en: Because finetuning is memory-intensive, many finetuning techniques aim to minimize
    their memory footprint. Understanding what causes this memory bottleneck is necessary
    to understand why and how these techniques work. This understanding, in turn,
    can help you select a finetuning method that works best for you.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微调对内存需求较高，许多微调技术旨在最小化它们的内存占用。了解导致这种内存瓶颈的原因对于理解为什么以及如何使用这些技术是必要的。这种理解反过来可以帮助你选择最适合你的微调方法。
- en: Besides explaining finetuning’s memory bottleneck, this section also introduces
    formulas for back-of-the-napkin calculation of the memory usage of each model.
    This calculation is useful in estimating what hardware you’d need to serve or
    finetune a model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解释微调的内存瓶颈外，本节还介绍了计算每个模型内存使用的便签公式。这种计算对于估计你需要什么硬件来服务或微调模型非常有用。
- en: Because memory calculation requires a breakdown of low-level ML and computing
    concepts, this section is technically dense. If you’re already familiar with these
    concepts, feel free to skip them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存计算需要对低级机器学习和计算概念进行分解，因此本节在技术上较为密集。如果你已经熟悉这些概念，请随意跳过它们。
- en: Backpropagation and Trainable Parameters
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播和可训练参数
- en: A key factor that determines a model’s memory footprint during finetuning is
    its number of *trainable parameters*. A trainable parameter is a parameter that
    can be updated during finetuning. During pre-training, all model parameters are
    updated. During inference, no model parameters are updated. During finetuning,
    some or all model parameters may be updated. The parameters that are kept unchanged
    are *frozen parameters*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 决定模型在微调期间内存占用大小的关键因素是其*可训练参数*的数量。可训练参数是在微调期间可以更新的参数。在预训练期间，所有模型参数都会更新。在推理期间，没有模型参数会更新。在微调期间，某些或所有模型参数可能会更新。保持不变的参数被称为*冻结参数*。
- en: 'The memory needed for each trainable parameter results from the way a model
    is trained. As of this writing, neural networks are typically trained using a
    mechanism called *backpropagation*.^([6](ch07.html#id1382)) With backpropagation,
    each training step consists of two phases:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每个可训练参数所需的内存源于模型训练的方式。截至本文写作时，神经网络通常使用称为*反向传播*的机制进行训练。^([6](ch07.html#id1382))
    使用反向传播，每个训练步骤由两个阶段组成：
- en: 'Forward pass: the process of computing the output from the input.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向传递：从输入计算输出的过程。
- en: 'Backward pass: the process of updating the model’s weights using the aggregated
    signals from the forward pass.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传递：使用前向传递的聚合信号更新模型权重的过程。
- en: 'During inference, only the forward pass is executed. During training, both
    passes are executed. At a high level, the backward pass works as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，只执行前向传递。在训练过程中，执行两个传递。从高层次来看，反向传递的工作原理如下：
- en: Compare the computed output from the forward pass against the expected output
    (ground truth). If they are different, the model made a mistake, and the parameters
    need to be adjusted. The difference between the computed output and the expected
    output is called the *loss*.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前向传递计算出的输出与预期输出（真实值）进行比较。如果它们不同，模型犯了错误，参数需要调整。计算输出与预期输出之间的差异被称为*损失*。
- en: Compute how much each trainable parameter contributes to the mistake. This value
    is called the *gradient*. Mathematically, gradients are computed by taking the
    derivative of the loss with respect to each trainable parameter. There’s one gradient
    value per trainable parameter.^([7](ch07.html#id1383)) If a parameter has a high
    gradient, it significantly contributes to the loss and should be adjusted more.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个可训练参数对错误的贡献程度。这个值被称为*梯度*。从数学上讲，梯度是通过计算损失相对于每个可训练参数的导数来计算的。每个可训练参数都有一个梯度值。^([7](ch07.html#id1383))
    如果一个参数的梯度值较高，它对损失有显著贡献，应该进行更多调整。
- en: Adjust trainable parameter values using their corresponding gradient. How much
    each parameter should be readjusted, given its gradient value, is determined by
    the *optimizer*. Common optimizers include SGD (stochastic gradient descent) and
    Adam. For transformer-based models, Adam is, by far, the most widely used optimizer.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用它们对应的梯度来调整可训练参数的值。给定每个参数的梯度值，每个参数应该调整多少，由**优化器**决定。常见的优化器包括SGD（随机梯度下降）和Adam。对于基于transformer的模型，Adam到目前为止是最广泛使用的优化器。
- en: The forward and backward pass for a hypothetical neural network with three parameters
    and one nonlinear activation function is visualized in [Figure 7-4](#ch07b_figure_1_1730159634220258).
    I use this dummy neural network to simplify the visualization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有三个参数和一个非线性激活函数的假设神经网络的前向和反向传播过程在[图7-4](#ch07b_figure_1_1730159634220258)中进行了可视化。我使用这个虚拟神经网络来简化可视化。
- en: '![A diagram of a flowchart'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![流程图示意图'
- en: Description automatically generated](assets/aien_0704.png)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0704.png)
- en: Figure 7-4\. The forward and backward pass of a simple neural network.
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 简单神经网络的正向和反向传播。
- en: During the backward pass, each trainable parameter comes with additional values,
    its gradient, and its optimizer states. Therefore, the more trainable parameters
    there are, the more memory is needed to store these additional values.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，每个可训练参数都带有额外的值，其梯度及其优化器状态。因此，可训练参数越多，存储这些额外值所需的内存就越多。
- en: Memory Math
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存数学
- en: It’s useful to know how much memory a model needs so that you can use the right
    hardware for it. Often, you might already have the hardware and need to calculate
    whether you can afford to run a certain model. If a model requires 30 GB of memory
    to do inference, a chip with 24 GB of memory won’t be sufficient.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 了解模型需要多少内存是有用的，这样你就可以为它使用合适的硬件。通常，你可能已经拥有了硬件，需要计算你是否能够运行某个模型。如果一个模型需要30 GB的内存来进行推理，那么一个24
    GB内存的芯片将不足以满足需求。
- en: A model’s memory footprint depends on the model as well as the workload and
    the different optimization techniques used to reduce its memory usage. Because
    it’s impossible to account for all optimization techniques and workloads, in this
    section, I’ll outline only the formulas for approximate calculations, which should
    give you a rough idea of how much memory you need to operate a model, both during
    inference and training.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的内存占用取决于模型本身以及工作负载和用于减少其内存使用的不同优化技术。由于无法考虑所有优化技术和工作负载，在本节中，我将仅概述近似计算的公式，这应该能给你一个大致的了解，你需要多少内存来操作模型，无论是推理还是训练。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Inference and training having distinct memory profiles is one of the reasons
    for the divergence in chips for training and inference, as discussed in [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 推理和训练具有不同的内存配置文件是训练和推理芯片差异的原因之一，如第9章[第9章](ch09.html#ch09_inference_optimization_1730130963006301)中所述。
- en: Memory needed for inference
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理所需的内存
- en: 'During inference, only the forward pass is executed. The forward pass requires
    memory for the model’s weights. Let N be the model’s parameter count and M be
    the memory needed for each parameter; the memory needed to load the model’s parameters
    is:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，只执行前向传播。前向传播需要模型的权重内存。设N为模型的参数数量，M为每个参数所需的内存；加载模型参数所需的内存为：
- en: '[PRE0]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The forward pass also requires memory for activation values. Transformer models
    need memory for key-value vectors for the attention mechanism. The memory for
    both activation values and key-value vectors grows linearly with sequence length
    and batch size.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播还需要激活值的内存。Transformer模型需要内存来存储注意力机制中的键值向量。激活值和键值向量的内存随着序列长度和批量大小的增加而线性增长。
- en: 'For many applications, the memory for activation and key-value vectors can
    be assumed to be 20% of the memory for the model’s weights. If your application
    uses a longer context or larger batch size, the actual memory needed will be higher.
    This assumption brings the model’s memory footprint to:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用，激活和键值向量的内存可以假设是模型权重内存的20%。如果你的应用使用更长的上下文或更大的批量大小，实际需要的内存将更高。这个假设将模型的内存占用降低到：
- en: '[PRE1]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Consider a 13B-parameter model. If each parameter requires 2 bytes, the model’s
    weights will require 13B × 2 bytes = 26 GB. The total memory for inference will
    be 26 GB × 1.2 = 31.2 GB.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个13B参数的模型。如果每个参数需要2字节，模型的权重将需要13B × 2字节 = 26 GB。推理的总内存将是26 GB × 1.2 = 31.2
    GB。
- en: A model’s memory footprint grows rapidly with its size. As models become bigger,
    memory becomes a bottleneck for operating them.^([8](ch07.html#id1389)) A 70B-parameter
    model with 2 bytes per parameter will require a whooping 140 GB of memory just
    for its weights.^([9](ch07.html#id1390))
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的内存占用随着其大小的增加而迅速增长。随着模型变得更大，内存成为操作它们的瓶颈。[8](ch07.html#id1389) 一个70B参数的模型，每个参数占用2字节，仅其权重就需要高达140GB的内存。[9](ch07.html#id1390))
- en: Memory needed for training
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练所需内存
- en: To train a model, you need memory for the model’s weights and activations, which
    has already been discussed. Additionally, you need memory for gradients and optimizer
    states, which scales with the number of trainable parameters.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个模型，你需要为模型的权重和激活存储内存，这已经在前面讨论过。此外，你还需要为梯度优化器状态存储内存，这会随着可训练参数数量的增加而增加。
- en: 'Overall, the memory needed for training is calculated as:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，训练所需的内存计算如下：
- en: Training memory = model weights + activations + gradients + optimizer states
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练内存 = 模型权重 + 激活 + 梯度 + 优化器状态
- en: Tip
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'During the backward pass, each trainable parameter requires one value for gradient
    plus zero to two values for optimizer states, depending on the optimizer:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，每个可训练参数需要一个梯度值以及根据优化器需要零到两个优化器状态值。
- en: A vanilla SGD optimizer has no state.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纯SGD优化器没有状态。
- en: A momentum optimizer stores one value per trainable parameter.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量优化器为每个可训练参数存储一个值。
- en: An Adam optimizer stores two values per trainable parameter.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam优化器为每个可训练参数存储两个值。
- en: 'Imagine you’re updating all parameters in a 13B-parameter model using the Adam
    optimizer. Because each trainable parameter has three values for its gradient
    and optimizer states, if it takes two bytes to store each value, the memory needed
    for gradients and optimizer states will be:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在使用Adam优化器更新一个13B参数模型的所有参数。因为每个可训练参数有三个梯度值和优化器状态值，如果每个值占用两个字节存储，那么梯度优化器状态所需的内存将是：
- en: '[PRE2]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'However, if you only have 1B trainable parameters, the memory needed for gradients
    and optimizer states will be only:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你只有1B个可训练参数，梯度优化器状态所需的内存将仅为：
- en: '[PRE3]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One important thing to note is that in the previous formula, I assumed that
    the memory needed for activations is less than the memory needed for the model’s
    weights. However, in reality, the activation memory can be much larger. If activations
    are stored for gradient computation, the memory needed for activations can dwarf
    the memory needed for the model’s weights. [Figure 7-5](#ch07b_figure_2_1730159634220278)
    shows the memory needed for activations compared to the memory needed for the
    model’s weights for different Megatron models at different scales, according to
    the paper [“Reducing Activation Recomputation in Large Transformer Models”](https://arxiv.org/abs/2205.05198),
    by Korthikanti et al. (2022).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个重要事项是，在前面的公式中，我假设激活所需的内存小于模型权重所需的内存。然而，在现实中，激活内存可能要大得多。如果激活被存储用于梯度计算，激活所需的内存可能会远大于模型权重所需的内存。[图7-5](#ch07b_figure_2_1730159634220278)展示了根据Korthikanti等人（2022年）的论文“Reducing
    Activation Recomputation in Large Transformer Models”，在不同规模的不同Megatron模型中，激活所需的内存与模型权重所需的内存的比较。[链接](https://arxiv.org/abs/2205.05198)。
- en: One way to reduce the memory needed for activations is not to store them. Instead
    of storing activations for reuse, you recompute activations when necessary. This
    technique is called *gradient checkpointing* or *activation recomputation*. While
    this reduces the memory requirements, it increases the time needed for training
    due to the recomputation.^([10](ch07.html#id1395))
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 减少激活所需内存的一种方法是不存储它们。不是存储激活以供重用，而是在需要时重新计算激活。这种技术称为*梯度检查点*或*激活重新计算*。虽然这减少了内存需求，但由于重新计算，它增加了训练所需的时间。[10](ch07.html#id1395))
- en: '![A graph of a graph'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图的图'
- en: Description automatically generated with medium confidence](assets/aien_0705.png)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，置信度中等](assets/aien_0705.png)
- en: Figure 7-5\. The memory needed for activations can dwarf the memory needed for
    the model’s weights. Image from Korthikanti et al., 2022.
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5. 激活所需的内存可能远大于模型权重的内存需求。图片来自Korthikanti等人，2022年。
- en: Numerical Representations
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值表示
- en: In the memory calculation so far, I’ve assumed that each value takes up two
    bytes of memory. The memory required to represent each value in a model contributes
    directly to the model’s overall memory footprint. If you reduce the memory needed
    for each value by half, the memory needed for the model’s weights is also reduced
    by half.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在迄今为止的内存计算中，我假设每个值占用两个字节的内存。表示模型中每个值的内存需求直接影响到模型的总体内存占用。如果你将每个值所需的内存减半，模型权重的内存需求也将减半。
- en: 'Before discussing how to reduce the memory needed for each value, it’s useful
    to understand numerical representations. Numerical values in neural networks are
    traditionally represented as [float numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic).
    The most common family of floating point formats is the FP family, which adheres
    to the Institute of Electrical and Electronics Engineers (IEEE) standard for Floating-Point
    Arithmetic ([IEEE 754](https://en.wikipedia.org/wiki/IEEE_754)):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论如何减少每个值所需的内存之前，了解数值表示是有用的。神经网络中的数值传统上表示为[浮点数](https://en.wikipedia.org/wiki/Floating-point_arithmetic)。最常用的浮点格式家族是FP家族，它遵循电气和电子工程师协会（IEEE）的浮点算术标准（[IEEE
    754](https://en.wikipedia.org/wiki/IEEE_754)）：
- en: FP32 uses 32 bits (4 bytes) to represent a float. This format is called single
    precision.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP32使用32位（4字节）来表示浮点数。这种格式称为单精度。
- en: FP64 uses 64 bits (8 bytes) and is called double precision.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP64使用64位（8字节）并称为双精度。
- en: FP16 uses 16 bits (2 bytes) and is called half precision.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP16使用16位（2字节）并称为半精度。
- en: While FP64 is still used in many computations—as of this writing, FP64 is the
    default format for NumPy and pandas—it’s rarely used in neural networks because
    of its memory footprint. FP32 and FP16 are more common. Other popular floating
    point formats in AI workloads include *BF16* (BFloat16) and *TF32* (TensorFloat-32).
    BF16 was designed by Google to optimize AI performance on [TPUs](https://oreil.ly/BGXtn)
    and TF32 was designed by NVIDIA for [GPUs](https://oreil.ly/0pZgw).^([11](ch07.html#id1396))
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 FP64 在许多计算中仍然被使用——截至本文撰写时，FP64 是 NumPy 和 pandas 的默认格式——但由于其内存占用，它很少在神经网络中使用。FP32
    和 FP16 更常见。在人工智能工作负载中，其他流行的浮点格式包括 *BF16*（BFloat16）和 *TF32*（TensorFloat-32）。BF16
    是由 Google 设计的，用于优化 [TPUs](https://oreil.ly/BGXtn) 上的 AI 性能，而 TF32 是由 NVIDIA 为
    [GPUs](https://oreil.ly/0pZgw) 设计的.^([11](ch07.html#id1396))
- en: Numbers can also be represented as integers. Even though not yet as common as
    floating formats, integer representations are becoming increasingly popular. Common
    integer formats are INT8 (8-bit integers) and INT4 (4-bit integers).^([12](ch07.html#id1397))
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数字也可以表示为整数。尽管目前不如浮点格式常见，但整数表示正在变得越来越流行。常见的整数格式是 INT8（8位整数）和 INT4（4位整数).^([12](ch07.html#id1397))
- en: Each float format usually has 1 bit to represent the number’s sign, i.e., negative
    or positive. The rest of the bits are split between *range* and *precision*:^([13](ch07.html#id1398))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个浮点格式通常有1位用于表示数字的符号，即负数或正数。其余位在 *范围* 和 *精度* 之间分配:^([13](ch07.html#id1398))
- en: Range
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 范围
- en: The number of range bits determines the range of values the format can represent.
    More bits means a wider range. This is similar to how having more digits lets
    you represent a wider range of numbers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 范围位数的数量决定了该格式可以表示的值的范围。位数越多，范围越广。这类似于拥有更多数字可以表示更广泛的数字范围。
- en: Precision
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 精度
- en: The number of precision bits determines how precisely a number can be represented.
    Reducing the number of precision bits makes a number less precise. For example,
    if you convert 10.1234 to a format that can support only two decimal digits, this
    value becomes 10.12, which is less precise than the original value.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 精度位的数量决定了数字可以表示的精确程度。减少精度位会使数字的精确度降低。例如，如果你将10.1234转换为只能支持两位小数的格式，这个值变为10.12，这比原始值精确度低。
- en: '[Figure 7-6](#ch07b_figure_3_1730159634220288) shows different floating point
    formats along with their range and precision bits.^([14](ch07.html#id1401))'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-6](#ch07b_figure_3_1730159634220288)显示了不同的浮点格式及其范围和精度位.^([14](ch07.html#id1401))'
- en: '![A graph with numbers and text'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![包含数字和文本的图表'
- en: Description automatically generated with medium confidence](assets/aien_0706.png)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成中等置信度的描述](assets/aien_0706.png)
- en: Figure 7-6\. Different numerical formats with their range and precision.
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6\. 带有范围和精度位的不同数值格式。
- en: Formats with more bits are considered *higher precision*. Converting a number
    with a high-precision format into a low-precision format (e.g., from FP32 to FP16)
    means *reducing its precision*. Reducing precision can cause a value to change
    or result in errors. [Table 7-3](#ch07b_table_1_1730159634233580) shows how FP32
    values can be converted into FP16, BF16, and TF32.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 位数更多的格式被认为是*高精度*。将具有高精度格式的数字转换为低精度格式（例如，从FP32转换为FP16）意味着*降低其精度*。降低精度可能会导致数值变化或产生错误。[表7-3](#ch07b_table_1_1730159634233580)
    展示了如何将FP32值转换为FP16、BF16和TF32，并显示结果的不准确性。
- en: Table 7-3\. Convert from FP32 values to lower-precision formats. Resultant inaccuracies
    are in italics.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-3\. 将FP32值转换为低精度格式。结果的不准确性以斜体显示。
- en: '| FP32 | FP16 | BF16 | TF32 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| FP32 | FP16 | BF16 | TF32 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0.0123456789 | 0.01234*43603515625* | 0.0123*291* | 0.01234*43603515625*
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 0.0123456789 | 0.01234*43603515625* | 0.0123*291* | 0.01234*43603515625*
    |'
- en: '| 0.123456789 | 0.1234*7412109375* | 0.123*535* | 0.1234*130859375* |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 0.123456789 | 0.1234*7412109375* | 0.123*535* | 0.1234*130859375* |'
- en: '| 1.23456789 | 1.234*375* | 1.234*38* | 1.234*375* |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 1.23456789 | 1.234*375* | 1.234*38* | 1.234*375* |'
- en: '| 12.3456789 | 12.34*375* | 12.3*75* | 12.34*375* |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 12.3456789 | 12.34*375* | 12.3*75* | 12.34*375* |'
- en: '| 123.456789 | 123.4*375* | 123.*5* | 123.4*375* |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 123.456789 | 123.4*375* | 123.*5* | 123.4*375* |'
- en: '| 1234.56789 | 123*5.0* | 123*2.0* | 1234.*0* |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 1234.56789 | 123*5.0* | 123*2.0* | 1234.*0* |'
- en: '| 12345.6789 | 1234*4.0* | 123*52.0* | 1234*4.0* |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 12345.6789 | 1234*4.0* | 123*52.0* | 1234*4.0* |'
- en: '| 123456.789 | *INF*^([a](ch07.html#id1402)) | 123*392.0* | 123456.*0* |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 123456.789 | *INF*^([a](ch07.html#id1402)) | 123*392.0* | 123456.*0* |'
- en: '| 1234567.89 | *INF* | 123*6990.0* | 123*3920.0* |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 1234567.89 | *INF* | 123*6990.0* | 123*3920.0* |'
- en: '| ^([a](ch07.html#id1402-marker)) Values out of bound in FP16 are rounded to
    infinity. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch07.html#id1402-marker)) FP16格式中的数值超出范围时将被四舍五入到无穷大。 |'
- en: Note in [Table 7-3](#ch07b_table_1_1730159634233580) that even though BF16 and
    FP16 have the same number of bits, BF16 has more bits for range and fewer bits
    for precision. This allows BF16 to represent large values that are out-of-bound
    for FP16\. However, this also makes BF16 less precise than FP16\. For example,
    1234.56789 is 1235.0 in FP16 (0.035% value change) but 1232.0 in BF16 (0.208%
    value change).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意[表7-3](#ch07b_table_1_1730159634233580)中，尽管BF16和FP16具有相同的位数，但BF16在范围上有更多的位数，而在精度上有更少的位数。这使得BF16可以表示FP16范围之外的较大值。然而，这也使得BF16比FP16精度更低。例如，1234.56789在FP16中为1235.0（0.035%的数值变化），但在BF16中为1232.0（0.208%的数值变化）。
- en: Warning
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When using a model, make sure to load the model in the format it’s intended
    for. Loading a model into the wrong numerical format can cause the model to change
    significantly. For example, Llama 2 had its weights set to BF16 when it came out.
    However, many teams loaded the model in FP16 and were subsequently frustrated
    to find the model’s quality much worse than advertised.^([15](ch07.html#id1404))
    While this misunderstanding wasted a lot of people’s time, the upside is that
    it forced many people to learn about numerical representations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用模型时，请确保以模型预期格式加载模型。将模型加载到错误的数值格式可能会导致模型发生显著变化。例如，Llama 2 在发布时其权重设置为BF16。然而，许多团队将模型加载为FP16，随后失望地发现模型的质量远低于宣传的。[^15](ch07.html#id1404)
    这种误解浪费了很多人时间，但好处是迫使很多人学习关于数值表示的知识。
- en: The right format for you depends on the distribution of numerical values of
    your workload (such as the range of values you need), how sensitive your workload
    is to small numerical changes, and the underlying hardware.^([16](ch07.html#id1405))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 适合您的格式取决于您的工作负载的数值分布（例如，您需要的值的范围），以及您的工作负载对微小数值变化的敏感度，以及底层硬件。[^16](ch07.html#id1405)
- en: Quantization
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: The fewer bits needed to represent a model’s values, the lower the model’s memory
    footprint will be. A 10B-parameter model in a 32-bit format requires 40 GB for
    its weights, but the same model in a 16-bit format will require only 20 GB. Reducing
    precision, also known as quantization, is a cheap and extremely effective way
    to reduce a model’s memory footprint. It’s straightforward to do and generalizes
    over tasks and architectures. In the context of ML, low precision generally refers
    to any format with fewer bits than the standard FP32.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 需要的位数越少来表示模型值，模型的内存占用就越低。一个10B参数的模型在32位格式下需要40GB的权重，但在16位格式下只需要20GB。降低精度，也称为量化，是一种成本低廉且极为有效的减少模型内存占用的方法。它操作简单，并且可以泛化到各种任务和架构。在机器学习（ML）的上下文中，低精度通常指任何位数少于标准FP32的格式。
- en: 'To do quantization, you need to decide what to quantize and when:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行量化，您需要决定量化什么以及何时量化：
- en: What to quantize
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 量化内容
- en: Ideally, you want to quantize whatever is consuming most of your memory, but
    it also depends on what you can quantize without hurting performance too much.
    As discussed in [“Memory Math”](#ch07b_memory_math_1730159634259402), major contributors
    to a model’s memory footprint during inference are the model’s weights and activations.^([17](ch07.html#id1406))
    Weight quantization is more common than activation quantization, since weight
    activation tends to have a more stable impact on performance with less accuracy
    loss.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你希望量化消耗你大部分内存的任何内容，但这同时也取决于你可以在不太多损害性能的情况下进行量化的内容。正如在[“内存数学”](#ch07b_memory_math_1730159634259402)中讨论的那样，模型在推理过程中的内存占用主要贡献者是模型的权重和激活。[17](ch07.html#id1406)
    权重量化比激活量化更为常见，因为权重激活通常对性能的影响更为稳定，且精度损失较小。
- en: When to quantize
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 何时进行量化
- en: Quantization can happen during training or post-training. Post-training quantization
    (PTQ) means quantizing a model after it’s been fully trained. PTQ is by far the
    most common. It’s also more relevant to AI application developers who don’t usually
    train models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以在训练期间或训练后进行。训练后量化（PTQ）意味着在模型完全训练后对其进行量化。PTQ到目前为止是最常见的。它也与通常不训练模型的AI应用开发者更为相关。
- en: Inference quantization
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理量化
- en: In the early days of deep learning, it was standard to train and serve models
    using 32 bits with FP32\. Since the late 2010s, it has become increasingly common
    to serve models in 16 bits and in even lower precision. For example, [Dettmers
    et al. (2022)](https://arxiv.org/abs/2208.07339) have done excellent work quantizing
    LLMs into 8 bits with LLM.int8() and 4 bits with QLoRA ([Dettmers et al., 2023](https://arxiv.org/abs/2305.14314)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的早期，使用32位FP32训练和提供服务是标准做法。自2010年代末以来，以16位甚至更低精度提供服务变得越来越普遍。例如，[Dettmers
    et al. (2022)](https://arxiv.org/abs/2208.07339)使用LLM.int8()将LLM量化到8位，使用QLoRA([Dettmers
    et al., 2023](https://arxiv.org/abs/2305.14314))将LLM量化到4位，他们做了出色的工作。
- en: A model can also be served in *mixed precision*, where values are reduced in
    precision when possible and maintained in higher precision when necessary. To
    serve models on the devices, [Apple](https://oreil.ly/lqLfv) (2024) leveraged
    a quantization scheme that uses a mixture of 2-bit and 4-bit formats, averaging
    3.5 bits-per-weight. Also in 2024, in anticipation of 4-bit neural networks, NVIDIA
    announced their new GPU architecture, [Blackwell](https://oreil.ly/FIP9V), that
    supports model inference in 4-bit float.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 模型也可以以*混合精度*提供服务，其中在可能的情况下降低精度，在必要时保持更高精度。为了在设备上提供服务，[苹果](https://oreil.ly/lqLfv)（2024）利用了一种混合2位和4位格式的量化方案，平均每位权重3.5位。同样在2024年，为了应对4位神经网络，英伟达宣布了其新的GPU架构[Blackwell](https://oreil.ly/FIP9V)，该架构支持4位浮点数模型推理。
- en: Once you get to 8 bits and under, numerical representations get more tricky.
    You can keep parameter values as floats using one of the [minifloat](https://en.wikipedia.org/wiki/Minifloat)
    formats, such as FP8 (8 bits) and FP4 (4 bits).^([18](ch07.html#id1407)) More
    commonly, however, parameter values are converted into an integer format, such
    as INT8 or INT4.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦达到8位以下，数值表示就会变得更加复杂。你可以使用[小浮点](https://en.wikipedia.org/wiki/Minifloat)格式之一，如FP8（8位）和FP4（4位），将参数值保持为浮点数。[18](ch07.html#id1407)
    然而，更常见的是将参数值转换为整数格式，例如INT8或INT4。
- en: Quantization is effective, but there’s a limit to how far it can go. You can’t
    have fewer than 1 bit per value, and some have attempted the 1-bit representation,
    e.g., BinaryConnect ([Courbariaux et al., 2015](https://arxiv.org/abs/1511.00363)),
    Xnor-Net ([Rastegari et al., 2016](https://arxiv.org/abs/1603.05279)), and BitNet
    ([Wang et al., 2023](https://arxiv.org/abs/2310.11453)).^([19](ch07.html#id1408))
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是有效的，但它有一个极限。每个值不能少于1位，有些人尝试了1位表示，例如BinaryConnect([Courbariaux et al., 2015](https://arxiv.org/abs/1511.00363))、Xnor-Net([Rastegari
    et al., 2016](https://arxiv.org/abs/1603.05279))和BitNet([Wang et al., 2023](https://arxiv.org/abs/2310.11453))。[19](ch07.html#id1408)
- en: In 2024, Microsoft researchers ([Ma et al.](https://arxiv.org/abs/2402.17764))
    declared that we’re entering the era of 1-bit LLMs by introducing BitNet b1.58,
    a transformer-based language model that requires only 1.58 bits per parameter
    and whose performance is comparable to 16-bit Llama 2 ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288))
    up to 3.9B parameters, as shown in [Table 7-4](#ch07b_table_2_1730159634233604).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年，微软研究人员([Ma et al.](https://arxiv.org/abs/2402.17764))通过引入BitNet b1.58，一个基于Transformer的语言模型，该模型每参数仅需1.58位，其性能与16位Llama
    2([Touvron et al., 2023](https://arxiv.org/abs/2307.09288))相当，参数量高达3.9B，如[表7-4](#ch07b_table_2_1730159634233604)所示。
- en: Table 7-4\. BitNet b1.58’s performance compared to that of Llama 2 16-bit on
    different benchmarks and at different model sizes, up to 3.9B parameters. Results
    from Ma et al. (2024).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-4\. BitNet b1.58在不同基准和不同模型大小（高达3.9B参数）上的性能与Llama 2 16位的比较。结果来自Ma等人（2024年）。
- en: '| Model | Size | ARCe | ARCc | HS | BQ | OQ | PQ | WGe | Avg. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Model | Size | ARCe | ARCc | HS | BQ | OQ | PQ | WGe | Avg. |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Llama LLM | 700M | 54.7 | 23.0 | 37.0 | 60.0 | 20.2 | 68.9 | 54.8 | 45.5
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Llama LLM | 700M | 54.7 | 23.0 | 37.0 | 60.0 | 20.2 | 68.9 | 54.8 | 45.5
    |'
- en: '| BitNet b1.58 | 700M | 51.8 | 21.4 | 35.1 | 58.2 | 20.0 | 68.1 | 55.2 | 44.3
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| BitNet b1.58 | 700M | 51.8 | 21.4 | 35.1 | 58.2 | 20.0 | 68.1 | 55.2 | 44.3
    |'
- en: '| Llama LLM | 1.3B | 56.9 | 23.5 | 38.5 | 59.1 | 21.6 | 70.0 | 53.9 | 46.2
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Llama LLM | 1.3B | 56.9 | 23.5 | 38.5 | 59.1 | 21.6 | 70.0 | 53.9 | 46.2
    |'
- en: '| BitNet b1.58 | 1.3B | 54.9 | 24.2 | 37.7 | 56.7 | 19.6 | 68.8 | 55.8 | 45.4
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| BitNet b1.58 | 1.3B | 54.9 | 24.2 | 37.7 | 56.7 | 19.6 | 68.8 | 55.8 | 45.4
    |'
- en: '| Llama LLM | 3B | 62.1 | 25.6 | 43.3 | 61.8 | 24.6 | 72.1 | 58.2 | 49.7 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Llama LLM | 3B | 62.1 | 25.6 | 43.3 | 61.8 | 24.6 | 72.1 | 58.2 | 49.7 |'
- en: '| BitNet b1.58 | 3B | 61.4 | 28.3 | 42.9 | 61.5 | 26.6 | 71.5 | 59.3 | 50.2
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| BitNet b1.58 | 3B | 61.4 | 28.3 | 42.9 | 61.5 | 26.6 | 71.5 | 59.3 | 50.2
    |'
- en: '| BitNet b1.58 | 3.9B | 64.2 | 28.7 | 44.2 | 63.5 | 24.2 | 73.2 | 60.5 | 51.2
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| BitNet b1.58 | 3.9B | 64.2 | 28.7 | 44.2 | 63.5 | 24.2 | 73.2 | 60.5 | 51.2
    |'
- en: Reduced precision not only reduces the memory footprint but also often improves
    computation speed. First, it allows a larger batch size, enabling the model to
    process more inputs in parallel. Second, reduced precision speeds up computation,
    which further reduces inference latency and training time. To illustrate this,
    consider the addition of two numbers. If we perform the addition bit by bit, and
    each takes *t* nanoseconds, it’ll take *32t* nanoseconds for 32 bits but only
    *16t* nanoseconds for 16 bits. However, reducing precision doesn’t always reduce
    latency due to the added computation needed for format conversion.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 降低精度不仅减少了内存占用，而且通常还提高了计算速度。首先，它允许更大的批量大小，使模型能够并行处理更多的输入。其次，降低精度加快了计算速度，这进一步减少了推理延迟和训练时间。为了说明这一点，考虑两个数的加法。如果我们逐位进行加法，并且每次加法需要*t*纳秒，那么32位需要*32t*纳秒，而16位只需要*16t*纳秒。然而，由于格式转换所需的额外计算，降低精度并不总是减少延迟。
- en: There are downsides to reduced precision. Each conversion often causes a small
    value change, and many small changes can cause a big performance change. If a
    value is outside the range the reduced precision format can represent, it might
    be converted to infinity or an arbitrary value, causing the model’s quality to
    further degrade. How to reduce precision with minimal impact on model performance
    is an active area of research, pursued by model developers as well as by hardware
    makers and application developers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 降低精度也有一些缺点。每次转换通常会导致小数值的变化，许多小的变化可能导致性能的大幅变化。如果一个值超出了降低精度格式可以表示的范围，它可能被转换为无穷大或任意值，从而进一步降低模型的质量。如何以最小的模型性能影响来降低精度是一个活跃的研究领域，由模型开发者、硬件制造商和应用开发者共同追求。
- en: Inference in lower precision has become a standard. A model is trained using
    a higher-precision format to maximize performance, then its precision is reduced
    for inference. Major ML frameworks, including PyTorch, TensorFlow, and Hugging
    Face’s transformers, offer PTQ for free with a few lines of code.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在低精度下的推理已经成为标准。模型使用高精度格式进行训练以最大化性能，然后在推理时降低其精度。包括PyTorch、TensorFlow和Hugging
    Face的transformers在内的主要机器学习框架，只需几行代码即可免费提供PTQ。
- en: Some edge devices only support quantized inference. Therefore, frameworks for
    on-device inference, such as TensorFlow Lite and PyTorch Mobile, also offer PTQ.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一些边缘设备只支持量化推理。因此，用于设备上推理的框架，如TensorFlow Lite和PyTorch Mobile，也提供了PTQ。
- en: Training quantization
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练量化
- en: 'Quantization during training is not yet as common as PTQ, but it’s gaining
    traction. There are two distinct goals for training quantization:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的量化还不像PTQ那样普遍，但它正在获得关注。训练量化有两个不同的目标：
- en: To produce a model that can perform well in low precision during inference.
    This is to address the challenge that a model’s quality might degrade during post-training
    quantization.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了生成一个在推理过程中能够良好表现的低精度模型。这是为了解决模型在训练后量化过程中质量可能下降的挑战。
- en: To reduce training time and cost. Quantization reduces a model’s memory footprint,
    allowing a model to be trained on cheaper hardware or allowing the training of
    a larger model on the same hardware. Quantization also speeds up computation,
    which further reduces costs.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少训练时间和成本。量化减少了模型的内存占用，使得模型可以在更便宜的硬件上训练，或者允许在相同的硬件上训练更大的模型。量化还加快了计算速度，从而进一步降低了成本。
- en: A quantization technique might help achieve one or both of these goals.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术可能有助于实现一个或两个这些目标。
- en: Quantization-aware training (QAT) aims to create a model with high quality in
    low precision for inference. With QAT, the model simulates low-precision (e.g.,
    8-bit) behavior during training, which allows the model to learn to produce high-quality
    outputs in low precision. However, QAT doesn’t reduce a model’s training time
    since its computations are still performed in high precision. QAT can even increase
    training time due to the extra work of simulating low-precision behavior.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练（QAT）旨在创建一个在低精度下具有高质量推理的模型。使用QAT，模型在训练期间模拟低精度（例如，8位）的行为，这使得模型能够学会在低精度下产生高质量的输出。然而，QAT不会减少模型的训练时间，因为其计算仍然以高精度执行。QAT甚至可能由于模拟低精度行为的额外工作而增加训练时间。
- en: On the other hand, training a model directly in lower precision can help with
    both goals. People attempted to train models in reduced precision as early as
    2016; see [Hubara et al. (2016)](https://oreil.ly/D-wIG) and [Jacob et al. (2017)](https://arxiv.org/abs/1712.05877).
    [Character.AI (2024)](https://oreil.ly/J7kVB) shared that they were able to train
    their models entirely in INT8, which helped eliminate the training/serving precision
    mismatch while also significantly improving training efficiency. However, training
    in lower precision is harder to do, as backpropgation is more sensitive to lower
    precision.^([20](ch07.html#id1415))
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，直接在较低精度下训练模型可以帮助实现这两个目标。人们早在2016年就尝试在降低精度下训练模型；参见 [Hubara et al. (2016)](https://oreil.ly/D-wIG)
    和 [Jacob et al. (2017)](https://arxiv.org/abs/1712.05877)。[Character.AI (2024)](https://oreil.ly/J7kVB)
    分享说，他们能够完全在INT8下训练他们的模型，这有助于消除训练/服务精度不匹配，同时也显著提高了训练效率。然而，在较低精度下训练更难实现，因为反向传播对较低精度更敏感。^([20](ch07.html#id1415))
- en: Lower-precision training is often done in [*mixed precision*](https://oreil.ly/pBaQM),
    where a copy of the weights is kept in higher precision but other values, such
    as gradients and activations, are kept in lower precision.^([21](ch07.html#id1416))
    You can also have less-sensitive weight values computed in lower precision and
    more-sensitive weight values computed in higher precision. For example, LLM-QAT
    ([Liu et al., 2023](https://arxiv.org/abs/2305.17888)) quantizes weights and activations
    into 4 bits but keeps embeddings in 16 bits.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 较低精度的训练通常在[*混合精度*](https://oreil.ly/pBaQM)下进行，其中权重的一个副本保留在高精度，但其他值，如梯度和激活，则保留在较低精度。^([21](ch07.html#id1416))
    你还可以在较低精度下计算对权重值更敏感的值，而在较高精度下计算对权重值更敏感的值。例如，LLM-QAT ([Liu et al., 2023](https://arxiv.org/abs/2305.17888))
    将权重和激活量化为4位，但将嵌入保留在16位。
- en: The portions of the model that should be in lower precision can be set automatically
    using the [*automatic mixed precision*](https://oreil.ly/JZRsd) (AMP) functionality
    offered by many ML frameworks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中应该使用较低精度的部分可以通过许多机器学习框架提供的[*自动混合精度*](https://oreil.ly/JZRsd) (AMP)功能自动设置。
- en: It’s also possible to have different phases of training in different precision
    levels. For example, a model can be trained in higher precision but finetuned
    in lower precision. This is especially common with foundation models, where the
    team training a model from scratch might be an organization with sufficient compute
    for higher precision training. Once the model is published, developers with less
    compute access can finetune that model in lower precision.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的精度级别中，也可以有不同的训练阶段。例如，一个模型可以在更高的精度下进行训练，但在较低的精度下进行微调。这在基础模型中尤为常见，因为从头开始训练模型的团队可能拥有足够的计算能力来进行高精度训练。一旦模型发布，计算能力较低的开发者可以在较低的精度下微调该模型。
- en: Finetuning Techniques
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调技术
- en: I hope that the previous section has made clear why finetuning large-scale models
    is so memory-intensive. The more memory finetuning requires, the fewer people
    who can afford to do it. Techniques that reduce a model’s memory footprint make
    finetuning more accessible, allowing more people to adapt models to their applications.
    This section focuses on memory-efficient finetuning techniques, which centers
    around parameter-efficient finetuning.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望前一部分已经清楚地说明了为什么微调大规模模型如此内存密集。微调所需的内存越多，能够负担得起的人就越少。减少模型内存占用的技术使得微调更加容易获得，允许更多的人将模型适应到他们的应用中。本节重点介绍内存高效的微调技术，这主要集中在参数高效的微调上。
- en: I’ll also cover model merging, an exciting but more experimental approach to
    creating custom models. While model merging is generally not considered finetuning,
    I include it in this section because it’s complementary to finetuning. Finetuning
    tailors one model to specific needs. Model merging combines multiple models, often
    finetuned models, for the same purpose.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我还会介绍模型合并，这是一种创建自定义模型令人兴奋但更具实验性的方法。虽然模型合并通常不被视为微调，但我将其包含在本节中，因为它与微调互补。微调是根据特定需求定制模型。模型合并将多个模型（通常是微调过的模型）结合在一起，以实现相同的目的。
- en: While combining multiple models isn’t a new concept, new types of models and
    finetuning techniques have inspired many creative model-merging techniques, making
    this section especially fun to write about.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然结合多个模型并不是一个新概念，但新型模型和微调技术激发了众多创造性的模型合并技术，使得本节特别有趣来撰写。
- en: Parameter-Efficient Finetuning
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数高效微调
- en: In the early days of finetuning, models were small enough that people could
    finetune entire models. This approach is called *full finetuning*. In full finetuning,
    the number of trainable parameters is exactly the same as the number of parameters.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调的早期阶段，模型足够小，以至于人们可以微调整个模型。这种方法被称为*完全微调*。在完全微调中，可训练参数的数量与参数的数量完全相同。
- en: Full finetuning can look similar to training. The main difference is that training
    starts with randomized model weights, whereas finetuning starts with model weights
    that have been previously trained.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 完全微调看起来可能和训练相似。主要区别在于训练是从随机初始化的模型权重开始的，而微调则是从先前训练过的模型权重开始的。
- en: 'As discussed in [“Memory Math”](#ch07b_memory_math_1730159634259402), the more
    trainable parameters there are, the more memory is needed. Consider a 7B-parameter
    model:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[“记忆数学”](#ch07b_memory_math_1730159634259402)中讨论的那样，可训练参数越多，所需的内存就越多。考虑一个70亿参数的模型：
- en: If you use a 16-bit format like FP16, loading the model’s weights alone requires
    14 GB for memory.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用16位格式如FP16，仅加载模型的权重就需要14 GB的内存。
- en: Full finetuning this model with the Adam optimizer, also in a 16-bit format,
    requires an additional 7B × 3 × 2 bytes = 42 GB of memory.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Adam优化器完全微调此模型，并且也在16位格式下，需要额外的7B × 3 × 2字节= 42 GB的内存。
- en: The total memory needed for the model’s weights, gradients, and optimizer states
    is then 14 GB + 42 GB = 56 GB.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，模型权重、梯度和优化器状态所需的总内存是14 GB + 42 GB = 56 GB。
- en: 56 GB exceeds the memory capacity of most consumer GPUs, which typically come
    with 12–24 GB of memory, with higher-end GPUs offering up to 48 GB. And this memory
    estimation doesn’t yet take into account the memory required for activations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 56 GB超出了大多数消费级GPU的内存容量，这些GPU通常配备12-24 GB的内存，高端GPU提供高达48 GB。而且这个内存估计还没有考虑到激活所需的内存。
- en: Note
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To fit a model on a given hardware, you can either reduce the model’s memory
    footprint or find ways to use the hardware’s memory more efficiently. Techniques
    like quantization and PEFT help minimize the total memory footprint. Techniques
    that focus on making better use of hardware memory include *CPU offloading*. Instead
    of trying to fit the whole model on GPUs, you can offload the excess memory onto
    CPUs, as demonstrated by DeepSpeed ([Rasley et al., 2020](https://oreil.ly/Np1Hn)).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 要在给定的硬件上适配模型，你可以要么减少模型的内存占用，要么找到更有效地使用硬件内存的方法。量化（quantization）和PEFT（Parameter
    Efficient Fine-tuning）等技术有助于最小化总内存占用。专注于更好地利用硬件内存的技术包括*CPU卸载*。你不必试图将整个模型放在GPU上，可以将多余的内存卸载到CPU上，正如DeepSpeed（[Rasley等人，2020](https://oreil.ly/Np1Hn)）所展示的那样。
- en: We also haven’t touched on the fact that full finetuning, especially supervised
    finetuning and preference finetuning, typically requires a lot of high-quality
    annotated data that most people can’t afford. Due to the high memory and data
    requirements of full finetuning, people started doing *partial finetuning*. In
    partial finetuning, only some of the model’s parameters are updated. For example,
    if a model has ten layers, you might freeze the first nine layers and finetune
    only the last layer,^([22](ch07.html#id1429)) reducing the number of trainable
    parameters to 10% of full finetuning.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有提到这样一个事实：全量微调，尤其是监督微调和偏好微调，通常需要大量高质量标注数据，而这对于大多数人来说是无法负担的。由于全量微调对内存和数据的高要求，人们开始进行**部分微调**。在部分微调中，只有模型的一些参数被更新。例如，如果一个模型有十层，你可能冻结前九层，只微调最后一层，^([22](ch07.html#id1429))将可训练参数的数量减少到全量微调的10%。
- en: While partial finetuning can reduce the memory footprint, it’s *parameter-inefficient*.
    Partial finetuning requires many trainable parameters to achieve performance close
    to that of full finetuning. A study by [Houlsby et al. (2019)](https://arxiv.org/abs/1902.00751)
    shows that with BERT large ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)),
    you’d need to update approximately 25% of the parameters to achieve performance
    comparable to that of full finetuning on the GLUE benchmark ([Wang et al., 2018](https://arxiv.org/abs/1804.07461)).
    [Figure 7-7](#ch07b_figure_4_1730159634220299) shows the performance curve of
    partial finetuning with different numbers of trainable parameters.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然部分微调可以减少内存占用，但它**参数效率低**。部分微调需要许多可训练参数才能达到接近全量微调的性能。Houlsby等人（2019）的研究表明，使用BERT大模型([Devlin等人，2018](https://arxiv.org/abs/1810.04805))，你需要更新大约25%的参数才能在GLUE基准测试([Wang等人，2018](https://arxiv.org/abs/1804.07461))上达到与全量微调相当的性能。[图7-7](#ch07b_figure_4_1730159634220299)显示了不同数量可训练参数的部分微调性能曲线。
- en: '![A graph of a number of objects'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![对象数量图](assets/aien_0707.png)'
- en: Description automatically generated with medium confidence](assets/aien_0707.png)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成，中等置信度](assets/aien_0707.png)
- en: Figure 7-7\. The blue line shows that partial finetuning requires many trainable
    parameters to achieve a performance comparable to full finetuning. Image from
    Houlsby et al. (2019).
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7。蓝色线显示，部分微调需要许多可训练参数才能达到与全量微调相当的性能。图片来自Houlsby等人（2019）。
- en: 'This brings up the question: How to achieve performance close to that of full
    finetuning while using significantly fewer trainable parameters? Finetuning techniques
    resulting from this quest are parameter-efficient. There’s no clear threshold
    that a finetuning method has to pass to be considered parameter-efficient. However,
    in general, a technique is considered parameter-efficient if it can achieve performance
    close to that of full finetuning while using several orders of magnitude fewer
    trainable parameters.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个问题：如何在显著减少可训练参数数量的同时，实现接近全量微调的性能？由此产生的微调技术是参数高效的。没有明确的阈值来衡量一个微调方法是否被认为是参数高效的。然而，一般来说，如果一个技术能够在使用几个数量级更少的可训练参数的情况下实现接近全量微调的性能，那么它被认为是参数高效的。
- en: The idea of PEFT (parameter-efficient finetuning) was introduced by Houlsby
    et al. (2019). The authors showed that by inserting additional parameters into
    the model in the right places, you can achieve strong finetuning performance using
    a small number of trainable parameters. They inserted two adapter modules into
    each transformer block of a BERT model, as shown in [Figure 7-8](#ch07b_figure_5_1730159634220312).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT（参数高效微调）的概念由Houlsby等人（2019）提出。作者展示了通过在模型中适当位置插入额外的参数，可以使用少量可训练参数实现强大的微调性能。他们在一个BERT模型的每个Transformer块中插入两个适配器模块，如图[图7-8](#ch07b_figure_5_1730159634220312)所示。
- en: '![A diagram of a layer'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '![层图](assets/aien_0708.png)'
- en: Description automatically generated](assets/aien_0708.png)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0708.png)
- en: Figure 7-8\. By inserting two adapter modules into each transformer layer for
    a BERT model and updating only the adapters, Houlsby et al. (2019) were able to
    achieve strong finetuning performance using a small number of trainable parameters.
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8。通过在每个BERT模型的Transformer层中插入两个适配器模块，并只更新适配器，Houlsby等人（2019）能够使用少量可训练参数实现强大的微调性能。
- en: During finetuning, they kept the model’s original parameters unchanged and only
    updated the adapters. The number of trainable parameters is the number of parameters
    in the adapters. On the GLUE benchmark, they achieved a performance within 0.4%
    of full finetuning using only 3% of the number of trainable parameters. The orange
    line in [Figure 7-7](#ch07b_figure_4_1730159634220299) shows the performance delta
    between full finetuning and finetuning using different adapter sizes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，他们保持了模型原始参数不变，只更新了适配器。可训练参数的数量是适配器中参数的数量。在GLUE基准测试中，他们仅使用3%的可训练参数数量就实现了与全微调相差0.4%的性能。图7-7中橙色线显示了全微调和使用不同适配器大小微调之间的性能差异。
- en: However, the downside of this approach is that it increases the inference latency
    of the finetuned model. The adapters introduce additional layers, which add more
    computational steps to the forward pass, slowing inference.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的缺点是它增加了微调模型的推理延迟。适配器引入了额外的层，这增加了正向传递的计算步骤，从而减慢了推理。
- en: PEFT enables finetuning on more affordable hardware, making it accessible to
    many more developers. PEFT methods are generally not only parameter-efficient
    but also sample-efficient. While full finetuning may need tens of thousands to
    millions of examples to achieve notable quality improvements, some PEFT methods
    can deliver strong performance with just a few thousand examples.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT使微调更易于在更经济的硬件上实现，使更多开发者能够访问。PEFT方法通常不仅参数高效，而且样本高效。虽然全微调可能需要成千上万甚至数百万个示例才能实现显著的质量改进，但一些PEFT方法只需几千个示例就能提供强大的性能。
- en: 'Given PEFT’s obvious appeal, PEFT techniques are being rapidly developed. The
    next section will give an overview of these techniques before diving deeper into
    the most common PEFT technique: LoRA.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PEFT的明显吸引力，PEFT技术正在迅速发展。下一节将概述这些技术，然后再深入探讨最常见的PEFT技术：LoRA。
- en: PEFT techniques
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PEFT技术
- en: 'The existing prolific world of PEFT generally falls into two buckets: *adapter-based
    methods* and *soft prompt-based methods*. However, it’s likely that newer buckets
    will be introduced in the future.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的PEFT丰富世界通常分为两大类：*基于适配器的方 法*和*基于软提示的方 法*。然而，未来可能还会引入新的类别。
- en: '*Adapter-based methods* refer to all methods that involve additional modules
    to the model weights, such as the one developed by [Houlsby et al. (2019)](https://arxiv.org/abs/1902.00751).
    Because adapter-based methods involve adding parameters, they are also called
    *additive methods*.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于适配器的方 法*指的是所有涉及向模型权重添加额外模块的方法，例如Houlsby等人（2019年）开发的方法。[Houlsby等人（2019年）](https://arxiv.org/abs/1902.00751)。由于基于适配器的方
    法涉及添加参数，因此它们也被称为*加性方法*。'
- en: As of this writing, LoRA ([Hu et al., 2021](https://arxiv.org/abs/2106.09685))
    is by far the most popular adapter-based method, and it will be the topic of the
    following section. Other adapter-based methods include BitFit ([Zaken et al.,
    2021](https://arxiv.org/abs/2106.10199)), which came out around the same time
    LoRA did. Newer adapter methods include IA3 ([Liu et al., 2022](https://oreil.ly/avDPk)),
    whose efficient mixed-task batching strategy makes it particularly attractive
    for multi-task finetuning. It’s been shown to outperform LoRA and even full finetuning
    in some cases. LongLoRA ([Chen et al., 2023](https://arxiv.org/abs/2309.12307))
    is a LoRA variant that incorporates attention-modification techniques to expand
    context length.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，LoRA([Hu等人，2021年](https://arxiv.org/abs/2106.09685))是最受欢迎的基于适配器的方 法，它将是下一节的主题。其他基于适配器的方
    法包括BitFit([Zaken等人，2021年](https://arxiv.org/abs/2106.10199))，它与LoRA同时出现。较新的适配器方法包括IA3([Liu等人，2022年](https://oreil.ly/avDPk))，它具有高效的混合任务批处理策略，使其特别适用于多任务微调。它已被证明在某些情况下优于LoRA甚至全微调。LongLoRA([Chen等人，2023年](https://arxiv.org/abs/2309.12307))是LoRA的一个变体，它结合了注意力修改技术来扩展上下文长度。
- en: 'If adapter-based methods add trainable parameters to the model’s architecture,
    soft prompt-based methods modify how the model processes the input by introducing
    special trainable tokens. These additional tokens are fed into the model alongside
    the input tokens. They are called *soft prompts* because, like the inputs (hard
    prompts), soft prompts also guide the model’s behaviors. However, soft prompts
    differ from hard prompts in two ways:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果基于适配器的方 法向模型架构中添加可训练参数，则基于软提示的方法通过引入特殊的可训练标记来修改模型处理输入的方式。这些额外的标记与输入标记一起输入到模型中。它们被称为*软提示*，因为，像输入（硬提示）一样，软提示也指导模型的行为。然而，软提示与硬提示在两个方面有所不同：
- en: Hard prompts are human-readable. They typically contain *discrete* tokens such
    as “I”, “write”, “a”, and “lot”. In contrast, soft prompts are continuous vectors,
    resembling embedding vectors, and are not human-readable.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬提示是可读的。它们通常包含*离散*标记，如“我”、“写”、“一个”和“很多”。相比之下，软提示是连续的向量，类似于嵌入向量，并且不可读。
- en: Hard prompts are static and not trainable, whereas soft prompts can be optimized
    through backpropagation during the tuning process, allowing them to be adjusted
    for specific tasks.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬提示是静态的，不可训练，而软提示可以在调优过程中通过反向传播进行优化，这使得它们可以根据特定任务进行调整。
- en: Some people describe soft prompting as a crossover between prompt engineering
    and finetuning. [Figure 7-9](#ch07b_figure_6_1730159634220324) visualizes how
    you can use soft prompts together with hard prompts to guide a model’s behaviors.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人将软提示描述为提示工程和微调之间的交叉。图[7-9](#ch07b_figure_6_1730159634220324)展示了你如何结合使用软提示和硬提示来引导模型的行为。
- en: '![A diagram of a model'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![模型的图'
- en: Description automatically generated](assets/aien_0709.png)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](assets/aien_0709.png)
- en: Figure 7-9\. Hard prompts and soft prompts can be combined to change a model’s
    behaviors.
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9。硬提示和软提示可以结合使用，以改变模型的行为。
- en: Soft prompt tuning as a subfield is characterized by a series of similar-sounding
    techniques that can be confusing, such as prefix-tuning ([Li and Liang, 2021](https://arxiv.org/abs/2101.00190)),
    P-Tuning ([Liu et al., 2021](https://arxiv.org/abs/2103.10385)), and prompt tuning
    ([Lester et al., 2021](https://arxiv.org/abs/2104.08691)).^([23](ch07.html#id1431))
    They differ mainly on the locations where the soft prompts are inserted. For example,
    prefix tuning prepends soft prompt tokens to the input at every transformer layer,
    whereas prompt tuning prepends soft prompt tokens to only the embedded input.
    If you want to use any of them, many PEFT frameworks will implement them out of
    the box for you.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 作为子领域，软提示调优以一系列听起来相似但可能令人困惑的技术为特征，例如前缀调优([Li和Liang, 2021](https://arxiv.org/abs/2101.00190))、P-Tuning([Liu等，2021](https://arxiv.org/abs/2103.10385))和提示调优([Lester等，2021](https://arxiv.org/abs/2104.08691))^([23](ch07.html#id1431))。它们的主要区别在于软提示插入的位置。例如，前缀调优在每个Transformer层将软提示标记添加到输入之前，而提示调优只将软提示标记添加到嵌入输入之前。如果你想使用其中任何一种，许多PEFT框架都会为你实现它们。
- en: To get a sense of what PEFT methods are being used, I analyzed over 1,000 open
    issues on the [GitHub repository huggingface/peft](https://github.com/huggingface/peft)
    in October 2024\. The assumption is that if someone uses a technique, they are
    more likely to report issues or ask questions about it. [Figure 7-10](#ch07b_figure_7_1730159634220334)
    shows the result. For “P-Tuning”, I searched for keywords “p_tuning” and “p tuning”
    to account for different spellings.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解正在使用哪些PEFT方法，我在2024年10月分析了GitHub仓库[huggingface/peft](https://github.com/huggingface/peft)上的1000多个公开issue。假设如果有人使用一种技术，他们更有可能报告有关该技术的问题或提出相关问题。[图7-10](#ch07b_figure_7_1730159634220334)显示了结果。对于“P-Tuning”，我搜索了“p_tuning”和“p
    tuning”关键词，以考虑到不同的拼写。
- en: '![A graph of a graph with different colored bars'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '![不同颜色条形图的图表'
- en: Description automatically generated with medium confidence](assets/aien_0710.png)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成，置信度中等](assets/aien_0710.png)
- en: Figure 7-10\. The number of issues corresponding to different finetuning techniques
    from the GitHub repository huggingface/peft. This is a proxy to estimate the popularity
    of each technique.
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。GitHub仓库huggingface/peft中对应不同微调技术的issue数量。这是估计每种技术流行度的代理。
- en: From this analysis, it’s clear that LoRA dominates. Soft prompts are less common,
    but there seems to be growing interest from those who want more customization
    than what is afforded by prompt engineering but who don’t want to invest in finetuning.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次分析中可以看出，LoRA占据主导地位。软提示不太常见，但似乎越来越多的人对那些想要比提示工程提供的更多定制化，但又不想投资微调的人越来越感兴趣。
- en: Because of LoRA’s popularity, the next section focuses on how LoRA works and
    how it solves the challenge posed by early adapter-based methods. Even if you
    don’t use LoRA, this deep dive should provide a framework for you to explore other
    finetuning methods.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LoRA的流行，下一节将重点介绍LoRA的工作原理以及它是如何解决早期基于适配器方法的挑战。即使你不使用LoRA，这次深入探讨也应该为你提供一个框架，让你探索其他微调方法。
- en: LoRA
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoRA
- en: Unlike the original adapter method by [Houlsby et al. (2019)](https://arxiv.org/abs/1902.00751),
    LoRA (Low-Rank Adaptation) ([Hu et al., 2021](https://arxiv.org/abs/2106.09685))
    incorporates additional parameters in a way that doesn’t incur extra inference
    latency. Instead of introducing additional layers to the base model, LoRA uses
    modules that can be merged back to the original layers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 与[Houlsby等人（2019）](https://arxiv.org/abs/1902.00751)的原始适配器方法不同，LoRA（低秩适配）([Hu等人，2021](https://arxiv.org/abs/2106.09685))以不增加额外推理延迟的方式引入了额外的参数。LoRA不是向基础模型引入额外的层，而是使用可以合并回原始层的模块。
- en: You can apply LoRA to individual weight matrices. Given a weight matrix, LoRA
    decomposes this matrix into the product of two smaller matrices, then updates
    these two smaller matrices before merging them back to the original matrix.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将LoRA应用于单个权重矩阵。给定一个权重矩阵，LoRA将其分解为两个较小矩阵的乘积，然后更新这两个较小的矩阵，最后将它们合并回原始矩阵。
- en: 'Consider the weight matrix *W* of the dimension *n* × *m*. LoRA works as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑维度为*n* × *m*的权重矩阵*W*。LoRA的工作原理如下：
- en: 'First, choose the dimension of the smaller matrices. Let *r* be the chosen
    value. Construct two matrices: *A* (dimension *n* × *r*) and *B* (dimension *r*
    × *m*). Their product is *W*[*AB*], which is of the same dimension as *W*. *r*
    is the LoRA *rank*.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，选择较小矩阵的维度。设*r*为所选值。构建两个矩阵：*A*（维度*n* × *r*）和*B*（维度*r* × *m*）。它们的乘积是*W*[*AB*]，其维度与*W*相同。*r*是LoRA的*秩*。
- en: 'Add *W*[*AB*] to the original weight matrix *W* to create a new weight matrix
    *W*ʹ. Use *W*ʹ in place of *W* as part of the model. You can use a hyperparameter
    ɑ to determine how much *W*[*AB*] should contribute to the new matrix: $upper
    W prime equals upper W plus StartFraction alpha Over r EndFraction upper W Subscript
    upper A upper B$'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*W*[*AB*]添加到原始权重矩阵*W*中，创建一个新的权重矩阵*W*ʹ。将*W*ʹ作为模型的一部分替代*W*。您可以使用超参数ɑ来确定*W*[*AB*]应在新矩阵中占多少贡献：$upper
    W prime equals upper W plus StartFraction alpha Over r EndFraction upper W Subscript
    upper A upper B$
- en: During finetuning, update only the parameters in *A* and *B*. *W* is kept intact.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在微调期间，仅更新*A*和*B*中的参数。*W*保持不变。
- en: '[Figure 7-11](#ch07b_figure_8_1730159634220345) visualizes this process.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-11](#ch07b_figure_8_1730159634220345)展示了此过程。'
- en: '![A diagram of a diagram'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图的图'
- en: Description automatically generated](assets/aien_0711.png)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0711.png)
- en: Figure 7-11\. To apply LoRA to a weight matrix W, decompose it into the product
    of two matrices A and B. During finetuning, only A and B are updated. W is kept
    intact.
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11\. 要将LoRA应用于权重矩阵W，将其分解为两个矩阵A和B的乘积。在微调期间，仅更新A和B。W保持不变。
- en: Note
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: LoRA (Low-Rank Adaptation) is built on the concept of *low-rank factorization*,
    a long-standing dimensionality reduction technique. The key idea is that you can
    factorize a large matrix into a product of two smaller matrices to reduce the
    number of parameters, which, in turn, reduces both the computation and memory
    requirements. For example, a `9 × 9` matrix can be factorized into the product
    of two matrices of dimensions `9 × 1` and `1 × 9`. The original matrix has 81
    parameters, but the two product matrices have only 18 parameters combined.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA（低秩适配）建立在*低秩分解*的概念之上，这是一种长期存在的降维技术。关键思想是你可以将一个大矩阵分解为两个较小矩阵的乘积以减少参数数量，这反过来又减少了计算和内存需求。例如，一个`9
    × 9`的矩阵可以被分解为两个矩阵的乘积，其维度为`9 × 1`和`1 × 9`。原始矩阵有81个参数，但两个乘积矩阵的总参数只有18个。
- en: The number of columns in the first factorized matrix and the number of columns
    in the second factorized matrix correspond to the rank of the factorization. The
    original matrix is *full-rank*, while the two smaller matrices represent a low-rank
    approximation.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个因子分解矩阵的列数和第二个因子分解矩阵的列数对应于因子分解的秩。原始矩阵是*满秩*的，而两个较小的矩阵代表低秩近似。
- en: While factorization can significantly reduce the number of parameters, it’s
    lossy because it only approximates the original matrix. The higher the rank, the
    more information from the original matrix the factorization can preserve.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然因子分解可以显著减少参数数量，但它是有损的，因为它仅近似原始矩阵。秩越高，因子分解可以保留的原始矩阵信息越多。
- en: Like the original adapter method, LoRA is parameter-efficient and sample-efficient.
    The factorization enables LoRA to use even fewer trainable parameters. The LoRA
    paper showed that, for GPT-3, LoRA achieves comparable or better performance with
    full finetuning on several tasks while using only ~4.7M trainable parameters,
    0.0027% of full finetuning.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的适配器方法一样，LoRA在参数效率和样本效率方面都很高。分解使得LoRA可以使用更少的可训练参数。LoRA论文显示，对于GPT-3，LoRA在几个任务上实现了与全微调相当或更好的性能，同时只使用了大约4.7M个可训练参数，占全微调的0.0027%。
- en: Why does LoRA work?
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LoRA为什么有效？
- en: Parameter-efficient methods like LoRA have become so popular that many people
    take them for granted. *But why is parameter efficiency possible at all?* If a
    model requires a lot of parameters to learn certain behaviors during pre-training,
    shouldn’t it also require a lot of parameters to change its behaviors during finetuning?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LoRA这样的参数高效方法已经变得如此流行，以至于许多人认为这是理所当然的。*但参数效率为什么可能呢？* 如果一个模型在预训练期间需要大量参数来学习某些行为，那么在微调期间改变其行为时，它是否也应该需要大量参数？
- en: The same question can be raised for data. If a model requires a lot of data
    to learn a behavior, shouldn’t it also require a lot of data to meaningfully change
    this behavior? How is it possible that you need millions or billions of examples
    to pre-train a model, but only a few hundreds or thousands of examples to finetune
    it?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据，也可以提出同样的问题。如果一个模型需要大量数据来学习某种行为，那么它是否也应该需要大量数据来有意义地改变这种行为？为什么你需要数百万或数十亿个示例来预训练一个模型，但只需要几百或几千个示例来微调它？
- en: Many papers have argued that while LLMs have many parameters, they have very
    low intrinsic dimensions; see [Li et al. (2018)](https://arxiv.org/abs/1804.08838);
    [Aghajanyan et al. (2020)](https://arxiv.org/abs/2012.13255); and [Hu et al. (2021)](https://arxiv.org/abs/2106.09685).
    They showed that *pre-training implicitly minimizes the model’s intrinsic dimension*.
    Surprisingly, larger models tend to have lower intrinsic dimensions after pre-training.
    This suggests that pre-training acts as a compression framework for downstream
    tasks. In other words, the better trained an LLM is, the easier it is to finetune
    the model using a small number of trainable parameters and a small amount of data.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 许多论文都认为，尽管LLMs有很多参数，但它们的内在维度非常低；参见[Li等（2018）](https://arxiv.org/abs/1804.08838)；[Aghajanyan等（2020）](https://arxiv.org/abs/2012.13255)；以及[Hu等（2021）](https://arxiv.org/abs/2106.09685)。他们展示了*预训练隐式地最小化了模型的内在维度*。令人惊讶的是，在预训练后，较大的模型往往具有更低的内在维度。这表明预训练充当了下游任务的压缩框架。换句话说，LLM训练得越好，使用少量可训练参数和少量数据微调模型就越容易。
- en: You might wonder, if low-rank factorization works so well, *why don’t we use
    LoRA for pre-training as well?* Instead of pre-training a large model and applying
    low-rank factorization only during finetuning, could we factorize a model from
    the start for pre-training? Low-rank pre-training can significantly reduce the
    model’s number of parameters, significantly reducing the model’s pre-training
    time and cost.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，如果低秩分解效果如此之好，*为什么不也用LoRA进行预训练呢？* 我们是否可以从一开始就分解模型以进行预训练，而不是在微调期间仅对大模型进行低秩分解？低秩预训练可以显著减少模型参数数量，从而显著减少模型的预训练时间和成本。
- en: Throughout the 2010s, many people tried training low-rank neural networks, exemplified
    in studies such as “Low-Rank Matrix Factorization for Deep Neural Network Training
    with High-Dimensional Output Targets” ([Sainath et al., 2013](https://oreil.ly/xzdiG)),
    “Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks” ([Povey
    et al., 2018](https://oreil.ly/LHLNz)), and “Speeding up Convolutional Neural
    Networks with Low Rank Expansions” ([Jaderberg et al., 2014](https://oreil.ly/BR63I)).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '在2010年代，许多人尝试训练低秩神经网络，例如在“用于具有高维输出目标的深度神经网络训练的低秩矩阵分解”（[Sainath等，2013](https://oreil.ly/xzdiG)）、“用于深度神经网络的半正交低秩矩阵分解”（[Povey等，2018](https://oreil.ly/LHLNz)）和“使用低秩扩展加速卷积神经网络”（[Jaderberg等，2014](https://oreil.ly/BR63I)）等研究中。 '
- en: Low-rank factorization has proven to be effective at smaller scales. For example,
    by applying various factorization strategies, including replacing 3 × 3 convolution
    with 1 × 1 convolution, SqueezeNet ([Iandola et al., 2016](https://arxiv.org/abs/1602.07360))
    achieves AlexNet-level accuracy on ImageNet using 50 times fewer parameters.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩分解在较小规模上已被证明是有效的。例如，通过应用各种分解策略，包括用1×1卷积替换3×3卷积，SqueezeNet ([Iandola等，2016](https://arxiv.org/abs/1602.07360))
    在ImageNet上实现了AlexNet级别的准确率，同时参数数量减少了50倍。
- en: More recent attempts to train low-rank LLMs include ReLoRA ([Lialin et al.,
    2023](https://arxiv.org/abs/2307.05695)) and GaLore ([Zhao et al., 2024](https://arxiv.org/abs/2403.03507)).
    ReLoRA works for transformer-based models of up to 1.3B parameters. GaLore achieves
    performance comparable to that of a full-rank model at 1B parameters and promising
    performance at 7B parameters.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的尝试训练低秩 LLM 包括 ReLoRA ([Lialin 等人，2023](https://arxiv.org/abs/2307.05695))
    和 GaLore ([Zhao 等人，2024](https://arxiv.org/abs/2403.03507))。ReLoRA 适用于参数量高达 1.3B
    的基于 transformer 的模型。GaLore 在 1B 参数时实现了与全秩模型相当的性能，在 7B 参数时表现出有希望的性能。
- en: It’s possible that one day not too far in the future, researchers will develop
    a way to scale up low-rank pre-training to hundreds of billions of parameters.
    However, if [Aghajanyan et al.’s argument](https://arxiv.org/abs/2012.13255) is
    correct—that pre-training implicitly compresses a model’s intrinsic dimension—full-rank
    pre-training is still necessary to sufficiently reduce the model’s intrinsic dimension
    to a point where low-rank factorization can work. It would be interesting to study
    exactly how much full-rank training is necessary before it’s possible to switch
    to low-rank training.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能在未来不久的某一天，研究人员将开发一种方法来扩展低秩预训练到数百亿参数。然而，如果 [Aghajanyan 等人的论点](https://arxiv.org/abs/2012.13255)是正确的——即预训练隐式压缩了模型的内禀维度——那么全秩预训练仍然是必要的，以充分减少模型的内禀维度到低秩分解可以工作的程度。研究在切换到低秩训练之前需要多少全秩训练将是有趣的。
- en: LoRA configurations
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LoRA 配置
- en: To apply LoRA, you need to decide what weight matrices to apply LoRA to and
    the rank of each factorization. This section will discuss the considerations for
    each of these decisions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用 LoRA，您需要决定应用于哪些权重矩阵以及每个因子的秩。本节将讨论这些决策的考虑因素。
- en: LoRA can be applied to each individual weight matrix. The efficiency of LoRA,
    therefore, depends not only on what matrices LoRA is applied to but also on the
    model’s architecture, as different architectures have different weight matrices.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 可以应用于每个单独的权重矩阵。因此，LoRA 的效率不仅取决于应用于哪些矩阵，还取决于模型的架构，因为不同的架构有不同的权重矩阵。
- en: 'While there have been examples of LoRA with other architectures, such as convolutional
    neural networks ([Dutt et al., 2023](https://arxiv.org/abs/2305.08252); [Zhong
    et al., 2024](https://arxiv.org/abs/2401.17868); [Aleem et al., 2024](https://arxiv.org/abs/2402.04964)),
    LoRA has been primarily used for transformer models.^([24](ch07.html#id1439))
    LoRA is most commonly applied to the four weight matrices in the attention modules:
    the query (*W*[*q*]), key (*W*[*k*]), value (*W*[*v*]), and output projection
    (*W*[*o*]) matrices.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有其他架构（如卷积神经网络）使用 LoRA 的例子，例如 [Dutt 等人，2023](https://arxiv.org/abs/2305.08252)；[Zhong
    等人，2024](https://arxiv.org/abs/2401.17868)；[Aleem 等人，2024](https://arxiv.org/abs/2402.04964)），LoRA
    主要用于 transformer 模型。[24](ch07.html#id1439) LoRA 最常应用于注意力模块中的四个权重矩阵：查询 (*W*[*q*])、键
    (*W*[*k*])、值 (*W*[*v*]) 和输出投影 (*W*[*o*]) 矩阵。
- en: Typically, LoRA is applied uniformly to all matrices of the same type within
    a model. For example, applying LoRA to the query matrix means applying LoRA to
    all query matrices in the model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，LoRA 会统一应用于模型中同一类型的所有矩阵。例如，将 LoRA 应用于查询矩阵意味着将 LoRA 应用于模型中的所有查询矩阵。
- en: Naively, you can apply LoRA to all these attention matrices. However, often,
    you’re constrained by your hardware’s memory and can accommodate only a fixed
    number of trainable parameters. Given a fixed budget of trainable parameters,
    what matrices should you apply LoRA to, to maximize performance?
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，您可以将 LoRA 应用于所有这些注意力矩阵。然而，通常您会受到硬件内存的限制，只能容纳固定数量的可训练参数。在固定的可训练参数预算下，您应该将
    LoRA 应用于哪些矩阵以最大化性能？
- en: 'When finetuning GPT-3 175B, Hu et al. (2021) set their trainable parameter
    budget at 18M, which is 0.01% of the model’s total number of parameters. This
    budget allows them to apply LoRA to the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调 GPT-3 175B 时，Hu 等人（2021）将他们的可训练参数预算设置为 18M，这是模型总参数数量的 0.01%。这个预算允许他们将 LoRA
    应用于以下内容：
- en: One matrix with the rank of 8
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个秩为 8 的矩阵
- en: Two matrices with the rank of 4
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个秩为 4 的矩阵
- en: All four matrices with the rank of 2
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有四个矩阵的秩为 2
- en: Note
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: GPT-3 175B has 96 transformer layers with a model dimension of 12,288\. Applying
    LoRA with rank = 2 to all four matrices would yield (12,288 × 2 × 2) × 4 = 196,608
    trainable parameters per layer, or 18,874,368 trainable parameters for the whole
    model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 175B 拥有 96 层 transformer，模型维度为 12,288。将 LoRA（秩 = 2）应用于所有四个矩阵将产生 (12,288
    × 2 × 2) × 4 = 196,608 个可训练参数每层，或者整个模型有 18,874,368 个可训练参数。
- en: They found that applying LoRA to all four matrices with rank = 2 yields the
    best performance on the WikiSQL ([Zhong et al., 2017](https://arxiv.org/abs/1709.00103))
    and MultiNLI (Multi-Genre Natural Language Inference) benchmarks ([Williams et
    al., 2017](https://oreil.ly/mqHMU)). [Table 7-5](#ch07b_table_3_1730159634233616)
    shows their results. However, the authors suggested that if you can choose only
    two attention matrices, the query and value matrices generally yield the best
    results.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-5\. LoRA performance with the budget of 18M trainable parameters. Results
    from LoRA (Hu et al., 2021).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Number of trainable parameters = 18M |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| Weight type | W[q] | W[k] | W[v] | W[o] | W[q], W[k] | W[q], W[v] | W[q],
    W[k], W[v], W[o] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| Rank r | 8 | 8 | 8 | 8 | 4 | 4 | 2 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| WikiSQL (± 0.5%) | 70.4 | 70.0 | 73.0 | 73.2 | 71.4 | **73.7** | **73.7**
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| MultiNLI (± 0.1%) | 91.0 | 90.8 | 91.0 | 91.3 | 91.3 | 91.3 | **91.7** |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: Empirical observations suggest that applying LoRA to more weight matrices, including
    the feedforward matrices, yields better results. For example, Databricks showed
    that the biggest performance boost they got was from applying LoRA to all feedforward
    layers ([Sooriyarachchi, 2023](https://oreil.ly/zzREV)). [Fomenko et al. (2024)](https://arxiv.org/html/2404.05086v1)
    noted that feedforward-based LoRA can be complementary to attention-based LoRA,
    though attention-based LoRA typically offers greater efficacy within memory constraints.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of LoRA is that while its performance depends on its rank, studies
    have shown that *a small r, such as between 4 and 64, is usually sufficient for
    many use cases*. A smaller *r* means fewer LoRA parameters, which translates to
    a lower memory footprint.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The LoRA authors observed that, to their surprise, increasing the value of *r*
    doesn’t increase finetuning performance. This observation is consistent with Databricks’
    report that “increasing *r* beyond a certain value may not yield any discernible
    increase in quality of model output” (Sooriyarachchi, 2023).^([25](ch07.html#id1441))
    Some argue that a higher *r* might even hurt as it can lead to overfitting. However,
    in some cases, a higher rank might be necessary. [Raschka (2023)](https://oreil.ly/A-d5f)
    found that *r* = 256 achieved the best performance on his tasks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Another LoRA hyperparameter you can configure is the value $alpha$ that determines
    how much the product *W*[*AB*] should contribute to the new matrix during merging:
    $upper W prime equals upper W plus StartFraction alpha Over r EndFraction upper
    W Subscript upper A upper B$ . In practice, I’ve often seen ɑ chosen so that the
    ratio $alpha colon r$ is typically between 1:8 and 8:1, but the optimal ratio
    varies. For example, if *r* is small, you might want $alpha$ to be larger, and
    if *r* is large, you might want $alpha$ to be smaller. Experimentation is needed
    to determine the best $left-parenthesis r comma alpha right-parenthesis$ combination
    for your use case.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Serving LoRA adapters
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署LoRA适配器
- en: LoRA not only lets you finetune models using less memory and data, but it also
    simplifies serving multiple models due to its modularity. To understand this benefit,
    let’s examine how to serve a LoRA-finetuned model.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA不仅让您可以使用更少的内存和数据微调模型，而且由于其模块化，它还简化了多个模型的部署。为了理解这一好处，让我们看看如何部署一个经过LoRA微调的模型。
- en: 'In general, there are two ways to serve a LoRA-finetuned model:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有两种方式来部署经过LoRA微调的模型：
- en: Merge the LoRA weights *A* and *B* into the original model to create the new
    matrix Wʹ prior to serving the finetuned model. Since no extra computation is
    done during inference, no extra latency is added.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在部署微调后的模型之前，将LoRA权重 *A* 和 *B* 合并到原始模型中，创建新的矩阵 Wʹ。由于推理过程中没有进行额外的计算，因此不会增加额外的延迟。
- en: Keep *W*, *A*, and *B* separate during serving. The process of merging *A* and
    *B* back to *W* happens during inference, which adds extra latency.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在部署过程中，将 *W*，*A* 和 *B* 保持分离。将 *A* 和 *B* 合并回 *W* 的过程发生在推理过程中，这会增加额外的延迟。
- en: The first option is generally better if you have only one LoRA model to serve,
    whereas the second is generally better for *multi-LoRA serving—*serving multiple
    LoRA models that share the same base model. [Figure 7-12](#ch07b_figure_9_1730159634220354)
    visualizes multi-LoRA serving if you keep the LoRA adapters separate.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只有一个LoRA模型需要部署，第一种选项通常更好，而第二种选项对于 *多LoRA部署*（部署多个共享相同基模型的LoRA模型）通常更好。[图7-12](#ch07b_figure_9_1730159634220354)
    展示了在保持LoRA适配器分离的情况下多LoRA部署的情况。
- en: '![A diagram of a flowchart'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '![流程图示意图'
- en: Description automatically generated](assets/aien_0712.png)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0712.png)
- en: Figure 7-12\. Keeping LoRA adapters separate allows reuse of the same full-rank
    matrix *W* in multi-LoRA serving.
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12\. 在多LoRA部署中保持LoRA适配器分离，允许重用相同的满秩矩阵 *W*。
- en: For multi-LoRA serving, while option 2 adds latency overhead, it significantly
    reduces the storage needed. Consider the scenario in which you finetune a model
    for each of your customers using LoRA. With 100 customers, you end up with 100
    finetuned models, all sharing the same base model. With option 1, you have to
    store 100 full-rank matrices *W*ʹ. With option 2, you only have to store one full-rank
    matrix *W*, and 100 sets of smaller matrices (*A*, *B*).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多LoRA部署，虽然选项2会增加延迟开销，但它显著减少了所需的存储空间。考虑这样一个场景：您使用LoRA为每位客户微调一个模型。如果有100位客户，您将拥有100个微调模型，它们都共享相同的基模型。在选项1中，您必须存储100个满秩矩阵
    *W*ʹ。在选项2中，您只需要存储一个满秩矩阵 *W*，以及100组较小的矩阵（*A*，*B*）。
- en: 'To put this in perspective, let’s say that the original matrix *W* is of the
    dimension `4096 × 4096` (16.8M parameters). If the LoRA’s rank is 8, the number
    of parameters in *A* and *B* is `4096 × 8 × 2 = 65,536`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，让我们假设原始矩阵 *W* 的维度为 `4096 × 4096`（16.8M个参数）。如果LoRA的秩为8，那么 *A* 和 *B*
    中的参数数量为 `4096 × 8 × 2 = 65,536`：
- en: In option 1, 100 full-rank matrices *W*ʹ totals `16.8M × 100 = 1.68B` parameters.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选项1中，100个满秩矩阵 *W*ʹ 总计 `16.8M × 100 = 1.68B` 个参数。
- en: 'In option 2, one full-rank matrix *W* and 100 sets of small matrices (*A*,
    *B*) totals: `16.8M + 65,536 × 100 = 23.3M` parameters.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选项2中，一个满秩矩阵 *W* 和100组小矩阵（*A*，*B*）总共：`16.8M + 65,536 × 100 = 23.3M` 个参数。
- en: Option 2 also makes it faster to switch between tasks. Let’s say you’re currently
    serving customer *X* using this customer’s model. To switch to serving customer
    *Y*, instead of loading this customer’s full weight matrix, you only need to load
    Y’s LoRA adapter, which can significantly reduce the loading time. While keeping
    *A* and *B* separate incurs additional latency, there are optimization techniques
    to minimize the added latency. The [book’s GitHub repository](https://github.com/chiphuyen/aie-book)
    contains a walkthrough of how to do so.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 选项2还可以加快任务之间的切换速度。假设您目前正在使用这位客户的模型为客户 *X* 提供服务。要切换到为客户 *Y* 提供服务，您只需要加载Y的LoRA适配器，而不是加载这位客户的完整权重矩阵，这可以显著减少加载时间。虽然保持
    *A* 和 *B* 分离会增加额外的延迟，但有一些优化技术可以最小化增加的延迟。[本书的GitHub仓库](https://github.com/chiphuyen/aie-book)
    包含了如何做到这一点的教程。
- en: Multi-LoRA serving makes it easy to combine multiple specialized models. Instead
    of having one big powerful model for multiple tasks, you can have one LoRA adapter
    for each task. For example, Apple used multiple [LoRA adapters](https://oreil.ly/vfXqE)
    to adapt the same 3B-parameter base model to different iPhone features (2024).
    They utilized quantization techniques to further reduce the memory footprint of
    this base model and adapters, allowing the serving of all of them on-device.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 多LoRA服务使得结合多个专用模型变得容易。你不需要有一个用于多个任务的大而强大的模型，而是可以为每个任务有一个LoRA适配器。例如，苹果公司使用了多个[LoRA适配器](https://oreil.ly/vfXqE)来适应相同的300亿参数基础模型，以适应不同的iPhone功能（2024）。他们利用量化技术进一步减少了基础模型和适配器的内存占用，使得它们都可以在设备上提供服务。
- en: The modularity of LoRA adapters means that LoRA adapters can be shared and reused.
    There are publicly available finetuned LoRA adapters that you can use the way
    you’d use pre-trained models. You can find them on [Hugging Face](https://oreil.ly/T08JJ)^([26](ch07.html#id1444))
    or initiatives like [AdapterHub](https://adapterhub.ml).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA适配器的模块化意味着LoRA适配器可以被共享和重用。你可以像使用预训练模型一样使用公开可用的微调LoRA适配器。你可以在[Hugging Face](https://oreil.ly/T08JJ)^([26](ch07.html#id1444))或类似的项目[AdapterHub](https://adapterhub.ml)上找到它们。
- en: 'You might be wondering: “LoRA sounds great, but what’s the catch?” The main
    drawback of LoRA is that it doesn’t offer performance as strong as full finetuning.
    It’s also more challenging to do than full finetuning as it involves modifying
    the model’s implementation, which requires an understanding of the model’s architecture
    and coding skills. However, this is usually only an issue for less popular base
    models. PEFT frameworks—such as [Hugging Face’s PEFT](https://github.com/huggingface/peft),
    [Axolotl](https://github.com/axolotl-ai-cloud/axolotl), [unsloth](https://github.com/unslothai/unsloth),
    and [LitGPT](https://github.com/Lightning-AI/litgpt)—likely support LoRA for popular
    base models right out of the box.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：“LoRA听起来很棒，但有什么缺点吗？”LoRA的主要缺点是它提供的性能不如全微调。由于它涉及修改模型的实现，这需要理解模型的架构和编程技能，因此它比全微调更具挑战性。然而，这通常只适用于不太受欢迎的基础模型。PEFT框架，如[Hugging
    Face的PEFT](https://github.com/huggingface/peft)、[Axolotl](https://github.com/axolotl-ai-cloud/axolotl)、[unsloth](https://github.com/unslothai/unsloth)和[LitGPT](https://github.com/Lightning-AI/litgpt)，可能直接支持流行基础模型的LoRA。
- en: Quantized LoRA
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化LoRA
- en: The rapid rise of LoRA has led to the development of numerous LoRA variations.
    Some aim to reduce the number of trainable parameters even further. However, as
    illustrated in [Table 7-6](#ch07b_table_4_1730159634233626), the memory of a LoRA
    adapter is minimal compared to the memory of the model’s weights. Reducing the
    number of LoRA parameters decreases the overall memory footprint by only a small
    percentage.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA的快速崛起导致了众多LoRA变体的开发。其中一些旨在进一步减少可训练参数的数量。然而，如[表7-6](#ch07b_table_4_1730159634233626)所示，与模型权重的内存相比，LoRA适配器的内存最小。减少LoRA参数的数量只能将整体内存占用减少一小部分。
- en: Table 7-6\. The memory needed by LoRA weights compared to that needed by the
    model’s weights.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-6\. 与模型权重所需的内存相比，LoRA权重所需的内存。
- en: '|  | Model’s weights memory (16 bits) | LoRA trainable params (r=2, query &
    key matrices) | LoRA adapter memory (16 bits) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型权重内存（16位） | LoRA可训练参数（r=2，查询与键矩阵） | LoRA适配器内存（16位） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama 2 (13B) | 26 GB | 3.28M | 6.55 MB |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 (13B) | 26 GB | 3.28M | 6.55 MB |'
- en: '| GPT-3 (175B) | 350 GB | 18.87M | 37.7 MB |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 (175B) | 350 GB | 18.87M | 37.7 MB |'
- en: Rather than trying to reduce LoRA’s number of parameters, you can reduce the
    memory usage more effectively by quantizing the model’s weights, activations,
    and/or gradients during finetuning. An early promising quantized version of LoRA
    is QLoRA ([Dettmers et al., 2023](https://arxiv.org/abs/2305.14314)).^([27](ch07.html#id1447))
    In the original LoRA paper, during finetuning, the model’s weights are stored
    using 16 bits. QLoRA stores the model’s weights in 4 bits but dequantizes (converts)
    them back into BF16 when computing the forward and backward pass.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是试图减少LoRA的参数数量，你可以在微调期间通过量化模型的权重、激活和/或梯度来更有效地减少内存使用。LoRA的一个早期有希望的量化版本是QLoRA
    ([Dettmers et al., 2023](https://arxiv.org/abs/2305.14314)).^([27](ch07.html#id1447))
    在原始的LoRA论文中，在微调期间，模型的权重使用16位存储。QLoRA将模型的权重存储在4位，但在计算前向和反向传递时将它们反量化（转换）回BF16。
- en: The 4-bit format that QLoRA uses is NF4 (NormalFloat-4), which quantizes values
    based on the insight that pre-trained weights usually follow a normal distribution
    with a median of zero. On top of 4-bit quantization, QLoRA also uses paged optimizers
    to automatically transfer data between the CPU and GPU when the GPU runs out of
    memory, especially with long sequence lengths. These techniques allow a 65B-parameter
    model to be finetuned on a single 48 GB GPU.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA使用的4位格式是NF4（NormalFloat-4），它根据预训练权重通常遵循以零为均值的正态分布的见解来量化值。在4位量化之上，QLoRA还使用分页优化器，当GPU内存不足时自动在CPU和GPU之间传输数据，特别是对于长序列长度。这些技术使得一个65B参数的模型可以在单个48
    GB GPU上进行微调。
- en: The authors finetuned a variety of models, including Llama 7B to 65B, in the
    4-bit mode. The resulting family of models, called Guanaco, showed competitive
    performance on both public benchmarks and comparative evaluation. [Table 7-7](#ch07b_table_5_1730159634233637)
    shows the Elo ratings of Guanaco models, GPT-4, and ChatGPT in May 2023, as judged
    by GPT-4\. While Guanaco 65B didn’t outperform GPT-4, it was often preferred to
    ChatGPT.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在4位模式下微调了各种模型，包括从Llama 7B到65B。这个被称为Guanaco的模型系列在公共基准测试和比较评估中均表现出竞争力。[表7-7](#ch07b_table_5_1730159634233637)显示了2023年5月Guanaco模型、GPT-4和ChatGPT的Elo评分，由GPT-4评判。虽然Guanaco
    65B没有超过GPT-4，但它通常比ChatGPT更受欢迎。
- en: Table 7-7\. Elo ratings of Guanaco models compared to popular models in May
    2023 using GPT-4 as a judge. The experiment is from QLoRA (Dettmers et al., 2023).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-7\. 使用GPT-4作为评判者，与2023年5月流行的模型相比，Guanaco模型的Elo评分。实验来自QLoRA（Dettmers等人，2023年）。
- en: '| Model | Size | Elo |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | Elo |'
- en: '| --- | --- | --- |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4 | - | 1348 ± 1 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | - | 1348 ± 1 |'
- en: '| Guanaco 65B | 41 GB | 1022 ± 1 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 65B | 41 GB | 1022 ± 1 |'
- en: '| Guanaco 33B | 21 GB | 992 ± 1 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 33B | 21 GB | 992 ± 1 |'
- en: '| Vicuna 13B | 26 GB | 974 ± 1 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna 13B | 26 GB | 974 ± 1 |'
- en: '| ChatGPT | - | 966 ± 1 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | - | 966 ± 1 |'
- en: '| Guanaco 13B | 10 GB | 916 ± 1 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 13B | 10 GB | 916 ± 1 |'
- en: '| Bard | - | 902 ± 1 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Bard | - | 902 ± 1 |'
- en: '| Guanaco 7B | 6 GB | 879 ± 1 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco 7B | 6 GB | 879 ± 1 |'
- en: The main limitation of QLoRA is that NF4 quantization is expensive. While QLoRA
    can reduce the memory footprint, it might increase training time due to the extra
    time required by quantization and dequantization steps.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA的主要限制是NF4量化成本高昂。虽然QLoRA可以减少内存占用，但由于量化和解量化步骤所需额外时间，它可能会增加训练时间。
- en: Due to its memory-saving promise, quantized LoRA is an active area of research.
    Other than QLoRA, quantized LoRA works include QA-LoRA ([Xu et al., 2023](https://arxiv.org/abs/2309.14717)),
    ModuLoRA ([Yin et al., 2023](https://arxiv.org/abs/2309.16119)), and IR-QLoRA
    ([Qin et al., 2024](https://arxiv.org/abs/2402.05445)).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其节省内存的承诺，量化LoRA是一个活跃的研究领域。除了QLoRA之外，其他量化LoRA的工作包括QA-LoRA（[Xu等人，2023](https://arxiv.org/abs/2309.14717)）、ModuLoRA（[Yin等人，2023](https://arxiv.org/abs/2309.16119)）和IR-QLoRA（[Qin等人，2024](https://arxiv.org/abs/2402.05445)）。
- en: Model Merging and Multi-Task Finetuning
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型合并和多任务微调
- en: If finetuning allows you to create a custom model by altering a single model,
    model merging allows you to create a custom model by combining multiple models.
    Model merging offers you greater flexibility than finetuning alone. You can take
    two available models and merge them together to create a new, hopefully more useful,
    model. You can also finetune any or all of the constituent models before merging
    them together.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果微调允许您通过改变单个模型来创建自定义模型，那么模型合并允许您通过组合多个模型来创建自定义模型。模型合并比单独的微调提供了更大的灵活性。您可以将两个可用的模型合并在一起，以创建一个新模型，希望它更有用。您还可以在合并之前对构成模型中的任何一个或所有模型进行微调。
- en: While you don’t have to further finetune the merged model, its performance can
    often be improved by finetuning. Without finetuning, model merging can be done
    without GPUs, making merging particularly attractive to indie model developers
    that don’t have access to a lot of compute.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您不需要进一步微调合并的模型，但通过微调通常可以提高其性能。不进行微调，可以在没有GPU的情况下进行模型合并，这使得合并对没有大量计算能力的独立模型开发者特别有吸引力。
- en: The goal of model merging is to create a single model that provides more value
    than using all the constituent models separately. The added value can come from
    improved performance. For example, if you have two models that are good at different
    things on the same task, you can merge them into a single model that is better
    than both of them on that task. Imagine one model that can answer the first 60%
    of the questions and another model that can answer the last 60% of the questions.
    Combined, perhaps they can answer 80% of the questions.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并的目标是创建一个单一模型，其提供的价值大于单独使用所有组成模型。增加的价值可以来自性能的提升。例如，如果你有两个在相同任务上擅长不同事物的模型，你可以将它们合并成一个在该任务上比两者都好的单一模型。想象一下，一个模型可以回答前60%的问题，另一个模型可以回答最后60%的问题。结合在一起，它们可能可以回答80%的问题。
- en: The added value can also come from a reduced memory footprint, which leads to
    reduced costs. For example, if you have two models that can do different tasks,
    they can be merged into one model that can do both tasks but with fewer parameters.
    This is particularly attractive for adapter-based models. Given two models that
    were finetuned on top of the same base model, you can combine their adapters into
    a single adapter.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 增加的价值也可以来自减少的内存占用，这导致成本降低。例如，如果你有两个可以执行不同任务的模型，它们可以被合并成一个既能执行这两个任务但参数更少的模型。这对于基于适配器的模型尤其有吸引力。给定两个在相同基础模型上微调的模型，你可以将它们的适配器合并成一个单一的适配器。
- en: 'One important use case of model merging is multi-task finetuning. Without model
    merging, if you want to a finetune a model for multiple tasks, you generally have
    to follow one of these approaches:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并的一个重要用例是多任务微调。如果没有模型合并，如果你想为多个任务微调一个模型，你通常必须遵循以下方法之一：
- en: Simultaneous finetuning
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 同时微调
- en: You create a dataset with examples for all the tasks and finetune the model
    on this dataset to make the model learn all the tasks simultaneously. However,
    because it’s generally harder to learn multiple skills at the same time, this
    approach typically requires more data and more training.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建一个包含所有任务示例的数据集，并在该数据集上微调模型，使模型能够同时学习所有任务。然而，由于同时学习多项技能通常更困难，这种方法通常需要更多的数据和更多的训练。
- en: Sequential finetuning
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序微调
- en: You can finetune the model on each task separately but sequentially. After training
    a model on task A, train it on task B, and so on. The assumption is that it’s
    easier for models to learn one task at a time. Unfortunately, neural networks
    are prone to catastrophic forgetting ([Kirkpatrick et al., 2016](https://arxiv.org/abs/1612.00796)).
    A model can forget how to do an old task when it’s trained on a new task, leading
    to a significant performance drop on earlier tasks.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在每个任务上分别但顺序地微调模型。在训练完任务A的模型后，再在任务B上训练，依此类推。假设模型一次学习一个任务更容易。不幸的是，神经网络容易发生灾难性遗忘（[Kirkpatrick
    et al., 2016](https://arxiv.org/abs/1612.00796)）。当模型在新的任务上训练时，可能会忘记如何执行旧任务，导致早期任务性能显著下降。
- en: Model merging offers another method for multi-task finetuning. You can finetune
    the model on different tasks separately but in parallel. Once done, these different
    models are merged together. Finetuning on each task separately allows the model
    to learn that task better. Because there’s no sequential learning, there’s less
    risk of catastrophic forgetting.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并为多任务微调提供了另一种方法。你可以在不同的任务上分别但并行地微调模型。一旦完成，这些不同的模型就会被合并在一起。分别对每个任务进行微调允许模型更好地学习该任务。因为没有顺序学习，所以发生灾难性遗忘的风险更小。
- en: Model merging is also appealing when you have to deploy models to devices such
    as phones, laptops, cars, smartwatches, and warehouse robots. On-device deployment
    is often challenging because of limited on-device memory capacity. Instead of
    squeezing multiple models for different tasks onto a device, you can merge these
    models together into one model that can perform multiple tasks while requiring
    much less memory.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 当你必须将模型部署到手机、笔记本电脑、汽车、智能手表和仓库机器人等设备上时，模型合并也很吸引人。由于设备上的内存容量有限，设备上的部署通常具有挑战性。与其将多个用于不同任务的模型压缩到设备上，不如将这些模型合并成一个可以执行多个任务但所需内存更少的单一模型。
- en: On-device deployment is necessary for use cases where data can’t leave the device
    (often due to privacy), or where there’s limited or unreliable internet access.
    On-device deployment can also significantly reduce inference costs. The more computation
    you can offload to user devices, the less you have to pay to data centers.^([28](ch07.html#id1463))
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备上部署对于数据不能离开设备的使用案例是必要的（通常由于隐私原因），或者当存在有限或不稳定的互联网接入时。设备上的部署还可以显着降低推理成本。您可以将更多计算卸载到用户设备上，那么您需要支付给数据中心的钱就越少。[28](ch07.html#id1463)
- en: Model merging is one way to do *federated learning* ([McMahan et al., 2016](https://arxiv.org/abs/1602.05629)),
    in which multiple devices train the same model using separate data. For example,
    if you deploy model X to multiple devices, each copy of X can continue learning
    separately from the on-device data. After a while, you have multiple copies of
    X, all trained on different data. You can merge these copies together into one
    new base model that contains the learning of all constituent models.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并是进行**联邦学习**的一种方法([McMahan et al., 2016](https://arxiv.org/abs/1602.05629))，其中多个设备使用不同的数据训练相同的模型。例如，如果您将模型X部署到多个设备上，X的每个副本都可以从设备上的数据继续单独学习。过了一段时间，您就有了X的多个副本，它们都在不同的数据上进行了训练。您可以将这些副本合并成一个新基础模型，该模型包含所有构成模型的训练。
- en: The idea of combining models together to obtain better performance started with
    *model ensemble methods*. According to [Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning),
    ensembling combines “multiple learning algorithms to obtain better predictive
    performance than could be obtained from any of the constituent learning algorithms
    alone.” If model merging typically involves mixing parameters of constituent models
    together, ensembling typically combines only model outputs while keeping each
    constituent model intact.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型组合起来以获得更好的性能的想法始于**模型集成方法**。根据[Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)，集成结合了“多个学习算法，以获得比任何单个构成学习算法单独获得的更好的预测性能。”如果模型合并通常涉及混合构成模型的参数，那么集成通常仅结合模型输出，同时保持每个构成模型完整。
- en: For example, in ensembling, given a query, you might use three models to generate
    three different answers. Then, a final answer is generated based on these three
    answers, using a simple majority vote or another trainable ML module.^([29](ch07.html#id1465))
    While ensembling can generally improve performance, it has a higher inference
    cost since it requires multiple inference calls per request.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在集成中，给定一个查询，您可能会使用三个模型生成三个不同的答案。然后，基于这三个答案生成一个最终答案，使用简单的多数投票或另一个可训练的机器学习模块。[29](ch07.html#id1465)
    而集成通常可以提高性能，但由于它需要每个请求进行多次推理调用，因此具有更高的推理成本。
- en: '[Figure 7-13](#ch07b_figure_10_1730159634220361) compares ensembling and model
    merging. Just like model ensembles used to dominate leaderboards, many models
    on top of the [Hugging Face’s Open LLM Leaderboard](https://oreil.ly/hRV9P) are
    merged models.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-13](#ch07b_figure_10_1730159634220361)比较了集成和模型合并。就像模型集成曾经主导排行榜一样，[Hugging
    Face的Open LLM排行榜](https://oreil.ly/hRV9P)上的许多模型都是合并模型。'
- en: '![A diagram of a model'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '![模型图'
- en: Description automatically generated](assets/aien_0713.png)
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0713.png)
- en: Figure 7-13\. How ensembling and model merging work.
  id: totrans-372
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-13\. 集成和模型合并的工作方式。
- en: Many model-merging techniques are experimental and might become outdated as
    the community gains a better understanding of the underlying theory. For this
    reason, I’ll focus on the high-level merging approaches instead of any individual
    technique.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型合并技术是实验性的，并且随着社区对底层理论的更好理解，可能会变得过时。因此，我将专注于高级合并方法，而不是任何个别技术。
- en: Model merging approaches differ in how the constituent parameters are combined.
    Three approaches covered here are summing, layer stacking, and concatenation.
    [Figure 7-14](#ch07b_figure_11_1730159634220368) shows their high-level differences.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 模型合并方法在如何组合构成参数方面有所不同。这里涵盖的三种方法是相加、层堆叠和连接。[图7-14](#ch07b_figure_11_1730159634220368)显示了它们的高级差异。
- en: '![A diagram of different colored bricks'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '![不同颜色砖块的图'
- en: Description automatically generated](assets/aien_0714.png)
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0714.png)
- en: 'Figure 7-14\. Three main approaches to model merging: summing, layer stacking,
    and concatenation.'
  id: totrans-377
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-14\. 模型合并的三个主要方法：相加、层堆叠和连接。
- en: You can mix these approaches when merging models, e.g., summing some layers
    and stacking others. Let’s explore each of these approaches.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并模型时，您可以混合这些方法，例如，将一些层相加，将其他层堆叠。让我们探索这些方法中的每一个。
- en: Summing
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相加
- en: 'This approach involves adding the weight values of constituent models together.
    I’ll discuss two summing methods: linear combination and spherical linear interpolation.
    If the parameters in two models are in different scales, e.g., one model’s parameter
    values are much larger than the other’s, you can rescale the models before summing
    so that their parameter values are in the same range.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法涉及将构成模型的权重值相加。我将讨论两种求和方法：线性组合和球面线性插值。如果两个模型的参数处于不同的尺度上，例如，一个模型的参数值比另一个大得多，您可以在求和之前重新缩放模型，以便它们的参数值处于相同的范围内。
- en: Linear combination
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性组合
- en: 'Linear combination includes both an average and a weighted average. Given two
    models, A and B, their weighted average is:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组合包括平均和加权平均。给定两个模型A和B，它们的加权平均是：
- en: $Merge left-parenthesis upper A comma upper B right-parenthesis equals StartFraction
    upper W Subscript upper A Baseline upper A plus upper W Subscript upper B Baseline
    upper B Over upper W Subscript upper A Baseline plus upper W Subscript upper B
    Baseline EndFraction$
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: $Merge\ left-parenthesis\ upper A\ comma\ upper B\ right-parenthesis\ equals\
    StartFraction\ upper W Subscript upper A Baseline\ upper A plus\ upper W Subscript
    upper B Baseline\ upper B\ Over\ upper W Subscript upper A Baseline\ plus\ upper
    W Subscript upper B Baseline\ EndFraction$
- en: '[Figure 7-15](#ch07b_figure_12_1730159634220376) shows how to linearly combine
    two layers when *w*[*A*] = *w*[*B*] = 1.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-15](#ch07b_figure_12_1730159634220376)展示了当*w*[*A*] = *w*[*B*] = 1时如何线性组合两个层。'
- en: '![A diagram of yellow circles with numbers'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '![黄色圆圈带数字的图]'
- en: Description automatically generated](assets/aien_0715.png)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0715.png)
- en: Figure 7-15\. Merging parameters by averaging them.
  id: totrans-387
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-15. 通过平均合并参数。
- en: Linear combination works surprisingly well, given how simple it is.^([30](ch07.html#id1466))
    The idea that multiple models can be linearly combined to create a better one
    was studied as early as the early 1990s ([Perrone, 1993](https://oreil.ly/eXC02)).
    Linear combination is often used in federated learning ([Wang et al., 2020](https://oreil.ly/ZKRPR)).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组合非常简单，但效果惊人.^([30](ch07.html#id1466))早在20世纪90年代初，就研究了多个模型可以线性组合以创建更好的模型的想法([Perrone，1993](https://oreil.ly/eXC02))。线性组合在联邦学习中经常被使用([Wang等人，2020](https://oreil.ly/ZKRPR))。
- en: You can linearly combine entire models or parts of models. Model soups ([Wortsman
    et al., 2022](https://arxiv.org/abs/2203.05482)) showed how averaging the entire
    weights of multiple finetuned models can improve accuracy without increasing inference
    time. However, it’s more common to merge models by linearly combining specific
    components, such as their adapters.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以线性组合整个模型或模型的部分。模型汤([Wortsman等人，2022](https://arxiv.org/abs/2203.05482))展示了通过平均多个微调模型的全部权重可以提高准确度，而不会增加推理时间。然而，更常见的是通过线性组合特定的组件来合并模型，例如它们的适配器。
- en: While you can linearly combine any set of models, *linear combination is the
    most effective for models finetuned on top of the same base model.* In this case,
    linear combination can be viewed through the concept of *task vectors*. The idea
    is that once you’ve finetuned a model for a specific task, subtracting the base
    model from it should give you a vector that captures the essence of the task.
    Task vectors are also called *delta parameters*. If you finetune using LoRA, you
    can construct the task vector from the LoRA weights.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以线性组合任何一组模型，但*线性组合对于在相同基础模型上微调的模型最为有效*。在这种情况下，线性组合可以通过*任务向量*的概念来理解。想法是，一旦您为特定任务微调了一个模型，从其中减去基础模型应该会得到一个捕捉任务本质的向量。任务向量也称为*delta参数*。如果您使用LoRA进行微调，您可以从LoRA权重中构建任务向量。
- en: Task vectors allow us to do *task arithmetic* ([Ilharco et al., 2022](https://arxiv.org/abs/2212.04089)),
    such as adding two task vectors to combine task capabilities or subtracting a
    task vector to reduce specific capabilities. Task subtraction can be useful for
    removing undesirable model behaviors, such as invasive capabilities like facial
    recognition or biases obtained during pre-training.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 任务向量使我们能够进行*任务算术*([Ilharco等人，2022](https://arxiv.org/abs/2212.04089))，例如将两个任务向量相加以组合任务能力或减去一个任务向量以减少特定能力。任务减法可以用于移除不希望出现的模型行为，例如面部识别等侵入性能力或预训练期间获得的偏差。
- en: Linear combination is straightforward when the components to be merged are of
    the same architecture and of the same size. However, it can also work for models
    that don’t share the same architecture or the same size. For example, if one model’s
    layer is larger than that of the other model, you can project one or both layers
    into the same dimension.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 当要合并的组件具有相同的架构和大小的时候，线性组合是直接的。然而，它也可以适用于不共享相同架构或大小的模型。例如，如果一个模型的层比另一个模型大，你可以将一个或两个层投影到相同的维度。
- en: 'Some people proposed aligning models before averaging to ensure that functionally
    related parameters are averaged together, such as in “Model Fusion via Optimal
    Transport” ([Singh and Jaggi, 2020](https://arxiv.org/abs/1910.05653)), “Git Re-Basin:
    Merging Models Modulo Permutation Symmetries” ([Ainsworth et al., 2022](https://arxiv.org/abs/2209.04836)),
    and “Merging by Matching Models in Task Parameter Subspaces” ([Tam et al., 2023](https://arxiv.org/abs/2312.04339)).
    While it makes sense to combine aligned parameters, aligning parameters can be
    challenging to do, and, therefore, this approach is less common on naive linear
    combinations.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人提出在平均之前对齐模型，以确保功能相关的参数被一起平均，例如在“通过最优传输进行模型融合”（[Singh和Jaggi，2020](https://arxiv.org/abs/1910.05653)），“Git
    Re-Basin：模态重基：模态合并模态对称性”（[Ainsworth等人，2022](https://arxiv.org/abs/2209.04836)）和“在任务参数子空间中通过匹配模型进行合并”（[Tam等人，2023](https://arxiv.org/abs/2312.04339)）中。虽然结合对齐参数是有意义的，但参数对齐可能具有挑战性，因此这种方法在简单的线性组合中不太常见。
- en: Spherical linear interpolation (SLERP)
  id: totrans-394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 球面线性插值（SLERP）
- en: Another common model summing method is SLERP, which is based on the mathematical
    operator of the same name, Spherical LinEar inteRPolation.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的模型求和方法是SLERP，它基于同名的数学运算符，即球面线性插值。
- en: Note
  id: totrans-396
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Interpolation means estimating unknown values based on known values. In the
    case of model merging, the unknown value is the merged model, and the known values
    are the constituent models. Linear combination is one interpolation technique.
    SLERP is another.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 插值意味着根据已知值估计未知值。在模型合并的情况下，未知值是合并后的模型，已知值是构成模型。线性组合是一种插值技术。SLERP是另一种。
- en: Because the formula for SLERP is mathy, and model-merging tools typically implement
    it for you, I won’t go into the details here. Intuitively, you can think of each
    component (vector) to be merged as a point on a sphere. To merge two vectors,
    you first draw the shortest path between these two points along the sphere’s surface.
    This is similar to drawing the shortest path between two cities along the Earth’s
    surface. The merged vector of these two vectors is a point along their shortest
    path. Where exactly the point falls along the path depends on the interpolation
    factor, which you can set to be between 0 and 1\. Factor values less than 0.5
    bring the merged vector closer to the first vector, which means that the first
    task vector will contribute more to the result. A factor of 0.5 means that you
    pick a point exactly halfway. This middle point is the blue point in [Figure 7-16](#ch07b_figure_13_1730159634220389).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 因为SLERP的公式很数学，而且模型合并工具通常为你实现它，所以我不会在这里详细介绍。直观上，你可以将每个要合并的组件（向量）想象成一个球面上的点。要合并两个向量，你首先在球面上画出这两个点之间的最短路径。这类似于在地球表面上画出两个城市之间的最短路径。这两个向量的合并向量是它们最短路径上的一个点。这个点在路径上的确切位置取决于插值因子，你可以将其设置为介于0和1之间。小于0.5的因子值将合并向量更靠近第一个向量，这意味着第一个任务向量将对结果贡献更多。0.5的因子意味着你选择一个正好在中间的点。这个中间点就是[图7-16](#ch07b_figure_13_1730159634220389)中的蓝色点。
- en: SLERP, as a mathematical operation, is defined with only two vectors, which
    means that you can merge only two vectors at a time. If you want to merge more
    than two vectors, you can potentially do SLERP sequentially, i.e., merging A with
    B, and then merging that result with C.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种数学运算，SLERP（球面线性插值）仅使用两个向量定义，这意味着你一次只能合并两个向量。如果你想合并超过两个向量，你可以按顺序进行SLERP，即先合并A和B，然后将结果与C合并。
- en: '![A circle with arrows and a red circle'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '![带有箭头和红色圆圈的圆圈'
- en: Description automatically generated](assets/aien_0716.png)
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0716.png)
- en: Figure 7-16\. How SLERP works for two vectors t1 and t2\. The red line is their
    shortest path on the spherical surface. Depending on the interpolation, the merged
    vector can be any point along this path. The blue vector is the resulting merged
    vector when the interpolation factor is 0.5.
  id: totrans-402
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-16\. SLERP如何作用于两个向量t1和t2。红色线是它们在球面上的最短路径。根据插值，合并向量可以是这条路径上的任何一点。蓝色向量是当插值因子为0.5时的结果合并向量。
- en: Pruning redundant task-specific parameters
  id: totrans-403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 剪枝冗余任务特定参数
- en: During finetuning, many models’ parameters are adjusted. However, most of these
    adjustments are minor and don’t significantly contribute to the model’s performance
    on the task.^([31](ch07.html#id1473)) Adjustments that don’t contribute to the
    model’s performance are considered *redundant*.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，许多模型的参数都会进行调整。然而，这些调整中的大多数都是微小的，并且不会对模型在任务上的性能产生显著贡献。[31](ch07.html#id1473)
    对模型性能没有贡献的调整被认为是*冗余*的。
- en: 'In the paper “TIES-Merging: Resolving Interference When Merging Models”, [Yadav
    et al. (2023)](https://arxiv.org/abs/2306.01708) showed that you can reset a large
    portion of task vector parameters with minimal performance degradation, as shown
    in [Figure 7-17](#ch07b_figure_14_1730159634220402). Resetting means changing
    the finetuned parameter to its original value in the base model, effectively setting
    the corresponding task vector parameter to zero. (Recall that the task vector
    can be obtained by subtracting the base model from the finetuned model.)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '在论文“TIES-Merging: Resolving Interference When Merging Models”中，[Yadav et al.
    (2023)](https://arxiv.org/abs/2306.01708)展示了你可以通过最小化性能下降来重置大量任务向量参数，如图[图7-17](#ch07b_figure_14_1730159634220402)所示。重置意味着将微调参数更改为其在基础模型中的原始值，有效地将相应的任务向量参数设置为零。（回想一下，任务向量可以通过从微调模型中减去基础模型来获得。）'
- en: '![A graph with a line and a dotted line'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个带有直线和虚线的图'
- en: Description automatically generated](assets/aien_0717.png)
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0717.png)
- en: Figure 7-17\. In Yadav et al.’s experiments, keeping the top 20% of the task
    vector parameters gives comparable performance to keeping 100% of the parameters.
  id: totrans-408
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-17\. 在Yadav et al.的实验中，保留任务向量参数的前20%与保留100%的参数具有可比的性能。
- en: These redundant parameters, while not harmful to one model, might be harmful
    to the merged model. Merging techniques such as TIES (Yadav et al., 2023) and
    DARE ([Yu et al., 2023](https://arxiv.org/abs/2311.03099)) first prune the redundant
    parameters from task vectors before merging them.^([32](ch07.html#id1474)) Both
    papers showed that this practice can significantly improve the quality of the
    final merged models. The more models there are to merge, the more important pruning
    is because there are more opportunities for redundant parameters in one task to
    interfere with other tasks.^([33](ch07.html#id1477))
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 这些冗余参数，虽然对单个模型没有害处，但可能对合并模型有害。例如，TIES（Yadav et al., 2023）和DARE（[Yu et al., 2023](https://arxiv.org/abs/2311.03099)）等合并技术首先从任务向量中剪枝冗余参数，然后再进行合并。[32](ch07.html#id1474)
    两篇论文都表明，这种做法可以显著提高最终合并模型的质量。要合并的模型越多，剪枝就越重要，因为一个任务中的冗余参数有更多机会干扰其他任务。[33](ch07.html#id1477)
- en: Layer stacking
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层堆叠
- en: In this approach, you take different layers from one or more models and stack
    them on top of each other. For example, you might take the first layer from model
    1 and the second layer from model 2\. This approach is also called *passthrough*
    or *frankenmerging*. It can create models with unique architectures and numbers
    of parameters. Unlike the merging by summing approach, the merged models resulting
    from layer stacking typically require further finetuning to achieve good performance.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，你从一个或多个模型中取出不同的层并将它们堆叠在一起。例如，你可能从模型1中取出第一层，从模型2中取出第二层。这种方法也被称为*透传*或*弗兰肯斯坦合并*。它可以创建具有独特架构和参数数量的模型。与通过求和合并的方法不同，通过层堆叠得到的合并模型通常需要进一步微调以达到良好的性能。
- en: One early success of frankenmerging is [Goliath-120B](https://oreil.ly/IM0Jc)
    (alpindale, 2023), which was merged from two finetuned Llama 2-70B models, [Xwin](https://oreil.ly/URfbk)
    and [Euryale](https://oreil.ly/Ftnxd). It took 72 out of 80 layers from each model
    and merged them together.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 弗兰肯斯坦合并的一个早期成功是[Goliath-120B](https://oreil.ly/IM0Jc)（alpindale，2023），它是由两个微调后的Llama
    2-70B模型[Xwin](https://oreil.ly/URfbk)和[Euryale](https://oreil.ly/Ftnxd)合并而成的。它从每个模型中取出了72层中的80层并将它们合并在一起。
- en: 'Layer stacking can be used to train mixture-of-experts (MoE) models, as introduced
    in “Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints” ([Komatsuzaki
    et al., 2022](https://arxiv.org/abs/2212.05055)). Rather than training an MOE
    from scratch, you take a pre-trained model and make multiple copies of certain
    layers or modules. A router is then added to send each input to the most suitable
    copy. You then further train the merged model along with the router to refine
    their performance. [Figure 7-18](#ch07b_figure_15_1730159634220410) illustrates
    this process.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 层堆叠可用于训练混合专家（MoE）模型，如“稀疏升级：从密集检查点训练混合专家”（Komatsuzaki 等人，2022年）中所述（[Komatsuzaki
    等人，2022](https://arxiv.org/abs/2212.05055)）。与其从头开始训练一个MoE模型，你取一个预训练模型并复制某些层或模块。然后添加一个路由器，将每个输入发送到最合适的副本。然后进一步训练合并的模型以及路由器，以提升它们的性能。[图7-18](#ch07b_figure_15_1730159634220410)展示了这个过程。
- en: Komatsuzaki et al. showed that layer stacking can produce models that outperform
    MoE models trained from scratch. Using this approach, Together AI mixed six weaker
    open source models together to create Mixture-of-Agents, which achieved comparable
    performance to OpenAI’s GPT-4o in some benchmarks ([Wang et al., 2024](https://arxiv.org/abs/2406.04692)).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: Komatsuzaki 等人表明，层堆叠可以产生优于从头开始训练的MoE模型的模型。使用这种方法，Together AI将六个较弱的开源模型混合在一起，创建了混合代理模型，在某些基准测试中达到了与OpenAI的GPT-4o相当的性能（[王等人，2024](https://arxiv.org/abs/2406.04692)）。
- en: '![A diagram of a machine'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](assets/aien_0718.png)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0718.png)
- en: Figure 7-18\. You can create an MoE model from a pre-trained model. Image adapted
    from Komatsuzaki et al. (2022).
  id: totrans-417
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-18。您可以从预训练模型创建一个MoE模型。图像改编自Komatsuzaki 等人（2022年）。
- en: An interesting use case of layer stacking is *model upscaling*. Model upscaling
    is the study of how to create larger models using fewer resources. Sometimes,
    you might want a bigger model than what you already have, presumably because bigger
    models give better performance. For example, your team might have originally trained
    a model to fit on your 40 GB GPU. However, you obtained a new machine with 80
    GB, which allows you to serve a bigger model. Instead of training a new model
    from scratch, you can use layer stacking to create a larger model from the existing
    model.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 层堆叠的一个有趣用例是*模型升级*。模型升级是研究如何使用更少的资源创建更大模型的研究。有时，你可能需要一个比你已有的更大的模型，可能是因为更大的模型能提供更好的性能。例如，你的团队最初训练了一个模型以适应40
    GB的GPU。然而，你获得了一台新的机器，拥有80 GB的内存，这允许你服务一个更大的模型。而不是从头开始训练新模型，你可以使用层堆叠从现有模型创建一个更大的模型。
- en: 'One approach to layer upscaling is *depthwise scaling*. [Kim et al. (2023)](https://arxiv.org/abs/2312.15166)
    used this technique to create SOLAR 10.7B from one 7B-parameter model with 32
    layers. The procedure works as follows:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 层升级的一种方法是*深度缩放*。[Kim 等人（2023）](https://arxiv.org/abs/2312.15166)使用这种技术从具有32层的7B参数模型创建SOLAR
    10.7B。该过程如下：
- en: Make a copy of the original pre-trained model.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制原始预训练模型。
- en: Merge these two copies by summing certain layers (summing two layers and turning
    them into one layer) and stacking the rest. The layers to be summed are carefully
    selected to match the target model size. For SOLAR 10.7B, 16 layers are summed,
    leaving the final model with 32 × 2 - 16 = 48 layers.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过相加某些层（相加两层并合并成一层）和堆叠其余部分来合并这两个副本。要相加的层被仔细选择以匹配目标模型大小。对于SOLAR 10.7B，相加了16层，最终模型有32
    × 2 - 16 = 48层。
- en: Further train this upscaled model toward the target performance.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进一步训练这个升级后的模型以实现目标性能。
- en: '[Figure 7-19](#ch07b_figure_16_1730159634220419) illustrates this process.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-19](#ch07b_figure_16_1730159634220419)展示了这个过程。'
- en: '![A screenshot of a computer program'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机程序的截图'
- en: Description automatically generated](assets/aien_0719.png)
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](assets/aien_0719.png)
- en: Figure 7-19\. Use depthwise scaling to create a 48-layer model from a 32-layer
    model. The image is licensed under CC BY 4.0 and was slightly modified for readability.
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-19。使用深度缩放从32层模型创建48层模型。此图像根据CC BY 4.0许可发布，并稍作修改以提高可读性。
- en: Concatenation
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接
- en: Instead of adding the parameters of the constituent models together in different
    manners, you can also concatenate them. The merged component’s number of parameters
    will be the sum of the number of parameters from all constituent components. If
    you merge two LoRA adapters of ranks *r*[1] and *r*[2], the merged adapter’s rank
    will be *r*[1] + *r*[2], as shown in [Figure 7-20](#ch07b_figure_17_1730159634220429).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 除了以不同方式将构成模型的参数相加外，你还可以将它们连接起来。合并组件的参数数量将是所有构成组件参数数量的总和。如果你合并秩为 *r*[1] 和 *r*[2]
    的两个 LoRA 适配器，合并适配器的秩将是 *r*[1] + *r*[2]，如图 7-20 所示。
- en: '![A diagram of a algorithm'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '![算法图'
- en: Description automatically generated](assets/aien_0720.png)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成描述](assets/aien_0720.png)
- en: Figure 7-20\. If you merge two LoRA adapters using concatenation, the rank of
    the merged adapter will be the sum of both adapters’ ranks.
  id: totrans-431
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-20\. 如果你使用连接合并两个 LoRA 适配器，合并适配器的秩将是两个适配器秩的总和。
- en: Concatenation isn’t recommended because it doesn’t reduce the memory footprint
    compared to serving different models separately. Concatenation might give better
    performance, but the incremental performance might not be worth the number of
    extra parameters.^([34](ch07.html#id1487))
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 连接不建议使用，因为它与分别提供不同模型相比，并不会减少内存占用。连接可能会提供更好的性能，但增量性能可能不值得增加的额外参数数量.^([34](ch07.html#id1487))
- en: Finetuning Tactics
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调策略
- en: This chapter has discussed multiple finetuning approaches, what problems they
    solve, and how they work. In this last section, I’ll focus on more practical finetuning
    tactics.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了多种微调方法，它们解决的问题以及它们的工作原理。在本节的最后，我将关注更实用的微调策略。
- en: Finetuning frameworks and base models
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调框架和基础模型
- en: 'While many things around finetuning—deciding whether to finetune, acquiring
    data, and maintaining finetuned models—are hard, the actual process of finetuning
    is more straightforward. There are three things you need to choose: a base model,
    a finetuning method, and a framework for finetuning.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微调周围的许多事情——决定是否微调、获取数据和维护微调模型——都很困难，但实际的微调过程更为直接。你需要选择三件事：一个基础模型、一个微调方法和一个微调框架。
- en: Base models
  id: totrans-437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础模型
- en: '[Chapter 4](ch04.html#ch04_evaluate_ai_systems_1730130866187863) already covered
    the criteria for model selection that can be applied to both prompt-based methods
    and finetuning. Some of the criteria discussed include model size, licenses, and
    benchmark performance. At the beginning of an AI project, when you’re still exploring
    the feasibility of your task, it’s useful to start with the most powerful model
    you can afford. If this model struggles to produce good results, weaker models
    are likely to perform even worse. If the strongest model meets your needs, you
    can then explore weaker models, using the initial model as a benchmark for comparison.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 4 章](ch04.html#ch04_evaluate_ai_systems_1730130866187863) 已经讨论了可以应用于基于提示方法和微调的模型选择标准。讨论的一些标准包括模型大小、许可证和基准性能。在
    AI 项目开始时，当你仍在探索任务可行性时，从你负担得起的最高性能模型开始是有用的。如果这个模型难以产生好的结果，较弱的模型可能表现得更差。如果最强模型满足你的需求，然后你可以探索较弱的模型，将初始模型作为比较的基准。'
- en: 'For finetuning, the starting models vary for different projects. [OpenAI’s
    finetuning best practices document](https://oreil.ly/7I6Ch) gives examples of
    two development paths: the progression path and the distillation path.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，起始模型因不同项目而异。[OpenAI 的微调最佳实践文档](https://oreil.ly/7I6Ch)提供了两种开发路径的示例：进展路径和蒸馏路径。
- en: 'The progression path looks like this:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 进展路径看起来是这样的：
- en: Test your finetuning code using the cheapest and fastest model to make sure
    the code works as expected.^([35](ch07.html#id1489))
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最便宜、最快的模型测试你的微调代码，以确保代码按预期工作.^([35](ch07.html#id1489))
- en: Test your data by finetuning a middling model. If the training loss doesn’t
    go down with more data, something might be wrong.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过微调一个中等模型来测试你的数据。如果增加数据后训练损失没有下降，可能存在问题。
- en: Run a few more experiments with the best model to see how far you can push performance.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳模型进行更多实验，看看你能将性能提升到什么程度。
- en: Once you have good results, do a training run with all models to map out the
    price/performance frontier and select the model that makes the most sense for
    your use case.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你得到好的结果，对所有模型进行一次训练运行，以绘制价格/性能前沿，并选择对你用例最有意义的模型。
- en: 'The distillation path might look as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏路径可能如下所示：
- en: Start with a small dataset and the strongest model you can afford. Train the
    best possible model with this small dataset. Because the base model is already
    strong, it requires less data to achieve good performance.
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个小数据集和你可以负担的最强大的模型开始。使用这个小数据集训练出最好的模型。因为基础模型已经很强，所以需要更少的数据来实现良好的性能。
- en: Use this finetuned model to generate more training data.
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个微调模型来生成更多的训练数据。
- en: Use this new dataset to train a cheaper model.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个新数据集来训练一个更便宜的模型。
- en: Because finetuning usually comes after experiments with prompt engineering,
    by the time you start to finetune, ideally, you should have a pretty good understanding
    of different models’ behaviors. You should plan your finetuning development path
    based on this understanding.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微调通常在提示工程实验之后进行，当你开始微调时，理想情况下，你应该对不同模型的特性有相当好的理解。你应该根据这种理解来规划你的微调开发路径。
- en: Finetuning methods
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调方法
- en: Recall that adapter techniques like LoRA are cost-effective but typically don’t
    deliver the same level of performance as full finetuning. If you’re just starting
    with finetuning, try something like LoRA, and attempt full finetuning later.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，像LoRA这样的适配器技术虽然成本效益高，但通常无法提供与全量微调相同水平的性能。如果你是微调的初学者，可以尝试LoRA这样的方法，稍后再尝试全量微调。
- en: The finetuning methods to use also depend on your data volume. Depending on
    the base model and the task, full finetuning typically requires at least thousands
    of examples and often many more. PEFT methods, however, can show good performance
    with a much smaller dataset. If you have a small dataset, such as a few hundred
    examples, full finetuning might not outperform LoRA.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的微调方法也取决于你的数据量。根据基础模型和任务的不同，全量微调通常至少需要数千个示例，有时甚至更多。然而，PEFT方法却可以在更小的数据集上展现出良好的性能。如果你有一个小数据集，比如几百个示例，全量微调可能不会优于LoRA。
- en: Take into account how many finetuned models you need and how you want to serve
    them when deciding on a finetuning method. Adapter-based methods like LoRA allow
    you to more efficiently serve multiple models that share the same base model.
    With LoRA, you only need to serve a single full model, whereas full finetuning
    requires serving multiple full models.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定微调方法时，考虑到你需要多少个微调模型以及你希望如何提供服务。基于适配器的LoRA等方法允许你更有效地服务多个共享相同基础模型的模型。使用LoRA，你只需要服务一个完整的模型，而全量微调则需要服务多个完整的模型。
- en: Finetuning frameworks
  id: totrans-454
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调框架
- en: The easiest way to finetune is to use a finetuning API where you can upload
    data, select a base model, and get back a finetuned model. Like model inference
    APIs, finetuning APIs can be provided by model providers, cloud service providers,
    and third-party providers. A limitation of this approach is that you’re limited
    to the base models that the API supports. Another limitation is that the API might
    not expose all the knobs you can use for optimal finetuning performance. Finetuning
    APIs are suitable for those who want something quick and easy, but they might
    be frustrating for those who want more customization.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的微调方式是使用微调API，你可以上传数据，选择基础模型，然后获取一个微调模型。像模型推理API一样，微调API可以由模型提供商、云服务提供商和第三方提供商提供。这种方法的局限性在于，你受限于API支持的基模型。另一个局限性是，API可能不会暴露出所有你可以用来实现最佳微调性能的控件。微调API适合那些想要快速简单的人，但对于那些想要更多定制的人来说可能会令人沮丧。
- en: You can also finetune using one of many great finetuning frameworks available,
    such as [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory), [unsloth](https://github.com/unslothai/unsloth),
    [PEFT](https://github.com/huggingface/peft), [Axolotl](https://github.com/axolotl-ai-cloud/axolotl),
    and [LitGPT](https://github.com/Lightning-AI/litgpt). They support a wide range
    of finetuning methods, especially adapter-based techniques. If you want to do
    full finetuning, many base models provide their open source training code on GitHub
    that you can clone and run with your own data. [Llama Police](https://huyenchip.com/llama-police)
    has a more comprehensive and up-to-date list of finetuning frameworks and model
    repositories.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用许多优秀的微调框架之一来进行微调，例如[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)、[unsloth](https://github.com/unslothai/unsloth)、[PEFT](https://github.com/huggingface/peft)、[Axolotl](https://github.com/axolotl-ai-cloud/axolotl)和[LitGPT](https://github.com/Lightning-AI/litgpt)。它们支持广泛的微调方法，特别是基于适配器的技术。如果你想要进行全量微调，许多基础模型在GitHub上提供了开源的训练代码，你可以克隆并使用自己的数据进行训练。[Llama
    Police](https://huyenchip.com/llama-police)有一个更全面和更新的微调框架和模型仓库列表。
- en: Doing your own finetuning gives you more flexibility, but you’ll have to provision
    the necessary compute. If you do only adapter-based techniques, a mid-tier GPU
    might suffice for most models. If you need more compute, you can choose a framework
    that integrates seamlessly with your cloud provider.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 进行自己的微调给你更多的灵活性，但你需要提供必要的计算资源。如果你只使用基于适配器的技术，中端GPU可能对大多数模型就足够了。如果你需要更多的计算资源，你可以选择一个与你的云提供商无缝集成的框架。
- en: To finetune a model using more than one machine, you’ll need a framework that
    helps you do distributed training, such as [DeepSpeed](https://github.com/microsoft/DeepSpeed),
    [PyTorch Distributed](https://oreil.ly/hxUAk), and [ColossalAI](https://github.com/microsoft/DeepSpeed).
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用多台机器微调模型，你需要一个帮助你进行分布式训练的框架，例如[DeepSpeed](https://github.com/microsoft/DeepSpeed)、[PyTorch
    Distributed](https://oreil.ly/hxUAk)和[ColossalAI](https://github.com/microsoft/DeepSpeed)。
- en: Finetuning hyperparameters
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调超参数
- en: Depending on the base model and the finetuning method, there are many hyperparameters
    you can tune to improve finetuning efficiency. For specific hyperparameters for
    your use case, check out the documentation of the base model or the finetuning
    framework you use. Here, I’ll cover a few important hyperparameters that frequently
    appear.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 根据基础模型和微调方法，有许多超参数你可以调整以提高微调效率。对于你用例的具体超参数，请查看你使用的基模型或微调框架的文档。在这里，我将介绍一些经常出现的重要超参数。
- en: Learning rate
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习率
- en: The learning rate determines how fast the model’s parameters should change with
    each learning step. If you think of learning as finding a path toward a goal,
    the learning rate is the step size. If the step size is too small, it might take
    too long to get to the goal. If the step size is too big, you might overstep the
    goal, and, hence, the model might never converge.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率决定了模型参数在每次学习步骤中应该变化多快。如果你把学习看作是寻找通往目标路径的过程，那么学习率就是步长。如果步长太小，可能需要太长时间才能达到目标。如果步长太大，你可能会超过目标，因此模型可能永远不会收敛。
- en: A universal optimal learning rate doesn’t exist. You’ll have to experiment with
    different learning rates, typically between the range of 1e-7 to 1e-3, to see
    which one works best. A common practice is to take the learning rate at the end
    of the pre-training phase and multiply it with a constant between 0.1 and 1.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 并不存在通用的最优学习率。你需要对不同学习率进行实验，通常在1e-7到1e-3的范围内，以确定哪个效果最好。常见的做法是在预训练阶段的末尾取学习率，并将其乘以0.1到1之间的常数。
- en: The loss curve can give you hints about the learning rate. If the loss curve
    fluctuates a lot, it’s likely that the learning rate is too big. If the loss curve
    is stable but takes a long time to decrease, the learning is likely too small.
    Increase the learning rate as high as the loss curve remains stable.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 损失曲线可以给你关于学习率的提示。如果损失曲线波动很大，那么很可能是学习率太大。如果损失曲线稳定但下降时间过长，那么学习率可能太小。提高学习率直到损失曲线保持稳定。
- en: You can vary learning rates during the training process. You can use larger
    learning rates in the beginning and smaller learning rates near the end. Algorithms
    that determine how learning rates should change throughout the training process
    are called learning rate schedules.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在训练过程中调整学习率。你可以在开始时使用较大的学习率，在接近结束时使用较小的学习率。决定学习率在整个训练过程中如何变化的算法称为学习率调度。
- en: Batch size
  id: totrans-466
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批大小
- en: The batch size determines how many examples a model learns from in each step
    to update its weights. A batch size that is too small, such as fewer than eight,
    can lead to unstable training.^([36](ch07.html#id1497)) A larger batch size helps
    aggregate the signals from different examples, resulting in more stable and reliable
    updates.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 批大小决定了模型在每一步中从多少个示例中学习以更新其权重。批大小太小，例如少于八个，可能导致训练不稳定。[36](ch07.html#id1497) 批大小越大，有助于汇总不同示例的信号，从而实现更稳定和可靠的更新。
- en: In general, the larger the batch size, the faster the model can go through training
    examples. However, the larger the batch size, the more memory is needed to run
    your model. Thus, batch size is limited by the hardware you use.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，批大小越大，模型通过训练示例的速度越快。然而，批大小越大，运行你的模型所需的内存就越多。因此，批大小受你所使用的硬件限制。
- en: This is where you see the cost versus efficiency trade-off. More expensive compute
    allows faster finetuning.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你看到成本与效率权衡的地方。更昂贵的计算资源允许更快的微调。
- en: As of this writing, compute is still a bottleneck for finetuning. Often, models
    are so large, and memory is so constrained, that only small batch sizes can be
    used. This can lead to unstable model weight updates. To address this, instead
    of updating the model weights after each batch, you can accumulate gradients across
    several batches and update the model weights once enough reliable gradients are
    accumulated. This technique is called *gradient accumulation*.^([37](ch07.html#id1498))
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，计算仍然是调优的瓶颈。通常，模型很大，内存受限，只能使用小批量大小。这可能导致模型权重更新不稳定。为了解决这个问题，你可以在积累足够可靠的梯度后，而不是在每次批量更新模型权重，可以在几个批量之间累积梯度，然后更新模型权重。这种技术称为*梯度累积*^([37](ch07.html#id1498))。
- en: When compute cost isn’t the most important factor, you can experiment with different
    batch sizes to see which gives the best model performance.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算成本不是最重要的因素时，你可以尝试不同的批量大小，看看哪个能给出最佳模型性能。
- en: Number of epochs
  id: totrans-472
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Epoch数量
- en: An epoch is a pass over the training data. The number of epochs determines how
    many times each training example is trained on.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 一个epoch是对训练数据的遍历。epoch的数量决定了每个训练示例被训练的次数。
- en: Small datasets may need more epochs than large datasets. For a dataset with
    millions of examples, 1–2 epochs might be sufficient. A dataset with thousands
    of examples might still see performance improvement after 4–10 epochs.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 小数据集可能需要比大数据集更多的epoch。对于有数百万个示例的数据集，1-2个epoch可能就足够了。对于有数千个示例的数据集，在4-10个epoch后可能仍然看到性能提升。
- en: The difference between the training loss and the validation loss can give you
    hints about epochs. If both the training loss and the validation loss still steadily
    decrease, the model can benefit from more epochs (and more data). If the training
    loss still decreases but the validation loss increases, the model is overfitting
    to the training data, and you might try lowering the number of epochs.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失和验证损失之间的差异可以给你关于epoch的提示。如果训练损失和验证损失仍然稳步下降，模型可以从更多的epoch（和更多数据）中受益。如果训练损失仍然下降，但验证损失增加，这意味着模型对训练数据过拟合，你可能需要尝试减少epoch的数量。
- en: Prompt loss weight
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示损失权重
- en: For instruction finetuning, each example consists of a prompt and a response,
    both of which can contribute to the model’s loss during training. During inference,
    however, prompts are usually provided by users, and the model only needs to generate
    responses. Therefore, response tokens should contribute more to the model’s loss
    during training than prompt tokens.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指令调优，每个示例都由一个提示和一个响应组成，这两个都可以在训练期间对模型的损失做出贡献。然而，在推理期间，提示通常由用户提供，模型只需要生成响应。因此，响应标记在训练期间对模型损失的贡献应该比提示标记更多。
- en: The prompt model weight determines how much prompts should contribute to this
    loss compared to responses. If this weight is 100%, prompts contribute to the
    loss as much as responses, meaning that the model learns equally from both. If
    this weight is 0%, the model learns only from responses. Typically, this weight
    is set to 10% by default, meaning that the model should learn some from prompts
    but mostly from responses.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模型权重决定了提示相对于响应对损失贡献的程度。如果这个权重是100%，提示对损失的贡献与响应一样多，这意味着模型从两者中学习得同样多。如果这个权重是0%，模型只从响应中学习。通常，这个权重默认设置为10%，这意味着模型应该从提示中学习一些，但主要从响应中学习。
- en: Summary
  id: totrans-479
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Outside of the evaluation chapters, finetuning has been the most challenging
    chapter to write. It touched on a wide range of concepts, both old (transfer learning)
    and new (PEFT), fundamental (low-rank factorization) and experimental (model merging),
    mathematical (memory calculation) and tactical (hyperparameter tuning). Arranging
    all these different aspects into a coherent structure while keeping them accessible
    was difficult.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估章节之外，调优一直是写作最具挑战性的章节。它涉及了广泛的概念，既有旧的（迁移学习）也有新的（PEFT），既有基础的（低秩分解）也有实验性的（模型合并），既有数学的（内存计算）也有策略性的（超参数调整）。在保持它们易于理解的同时，将这些不同的方面安排成一个连贯的结构是困难的。
- en: The process of finetuning itself isn’t hard. Many finetuning frameworks handle
    the training process for you. These frameworks can even suggest common finetuning
    methods with sensible default hyperparameters.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 调优过程本身并不难。许多调优框架会为你处理训练过程。这些框架甚至可以建议一些常见的调优方法，并带有合理的默认超参数。
- en: 'However, the context surrounding finetuning is complex. It starts with whether
    you should even finetune a model. This chapter started with the reasons for finetuning
    and the reasons for not finetuning. It also discussed one question that I have
    been asked many times: when to finetune and when to do RAG.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，微调的背景是复杂的。它始于你是否应该甚至微调一个模型。本章从微调的原因和不应微调的原因开始。它还讨论了一个我经常被问到的问题：何时微调，何时进行RAG。
- en: In its early days, finetuning was similar to pre-training—both involved updating
    the model’s entire weights. However, as models increased in size, full finetuning
    became impractical for most practitioners. The more parameters to update during
    finetuning, the more memory finetuning needs. Most practitioners don’t have access
    to sufficient resources (hardware, time, and data) to do full finetuning with
    foundation models.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调的早期，微调与预训练类似——两者都涉及更新模型的全部权重。然而，随着模型规模的增加，对于大多数从业者来说，完全微调变得不切实际。微调时需要更新的参数越多，微调所需的内存就越多。大多数从业者没有足够的资源（硬件、时间和数据）来使用基础模型进行完全微调。
- en: 'Many finetuning techniques have been developed with the same motivation: to
    achieve strong performance on a minimal memory footprint. For example, PEFT reduces
    finetuning’s memory requirements by reducing the number of trainable parameters.
    Quantized training, on the other hand, mitigates this memory bottleneck by reducing
    the number of bits needed to represent each value.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 许多微调技术都是出于相同的动机：在最小的内存占用下实现强大的性能。例如，PEFT通过减少可训练参数的数量来降低微调的内存需求。另一方面，量化训练通过减少表示每个值所需的位数来缓解这个内存瓶颈。
- en: After giving an overview of PEFT, the chapter zoomed into LoRA—why and how it
    works. LoRA has many properties that make it popular among practitioners. On top
    of being parameter-efficient and data-efficient, it’s also modular, making it
    much easier to serve and combine multiple LoRA models.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在概述了PEFT之后，本章聚焦于LoRA——为什么以及它是如何工作的。LoRA具有许多使它在从业者中受欢迎的特性。除了参数高效和数据高效之外，它还具有模块化特性，这使得服务和使用多个LoRA模型变得更加容易。
- en: The idea of combining finetuned models brought the chapter to model merging;
    its goal is to combine multiple models into one model that works better than these
    models separately. This chapter discussed the many use cases of model merging,
    from on-device deployment to model upscaling, and general approaches to model
    merging.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 将微调模型结合起来的想法将本章引向了模型合并；其目标是合并多个模型，使其比单独使用这些模型表现得更好。本章讨论了模型合并的许多用例，从设备上部署到模型升级，以及模型合并的一般方法。
- en: A comment I often hear from practitioners is that finetuning is easy, but getting
    data for finetuning is hard. Obtaining high-quality annotated data, especially
    instruction data, is challenging. The next chapter will dive into these challenges.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常听到从业者的一个评论是，微调很容易，但获取微调数据却很困难。获取高质量的标注数据，尤其是指令数据，是一项挑战。下一章将深入探讨这些挑战。
- en: ^([1](ch07.html#id1369-marker)) Some people call this phenomenon an alignment
    tax ([Bai et al., 2020](https://arxiv.org/abs/2204.05862)), but this term can
    be confused with penalties against human preference alignment.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#id1369-marker)) 有些人称这种现象为对齐税([Bai et al., 2020](https://arxiv.org/abs/2204.05862))，但这个术语可能会与针对人类偏好对齐的惩罚相混淆。
- en: ^([2](ch07.html#id1370-marker)) Many businesses resist changing technologies
    they consider “good enough.” If all companies were quick to adopt more optimal
    solutions, fax machines would have become obsolete by now.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.html#id1370-marker)) 许多企业抵制改变他们认为“足够好”的技术。如果所有公司都迅速采用更优的解决方案，传真机现在可能已经过时了。
- en: ^([3](ch07.html#id1371-marker)) I’ve also noticed a few cases when engineers
    know that finetuning isn’t strictly necessary but still insist on doing it because
    they want to learn how to finetune. As an engineer who likes learning new skills,
    I appreciate this mindset. However, if you’re in a leadership position, it can
    be hard to differentiate whether finetuning is needed or wanted.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.html#id1371-marker)) 我还注意到一些情况，当工程师知道微调并非绝对必要，但仍然坚持这样做，因为他们想学习如何微调。作为一个喜欢学习新技能的工程师，我欣赏这种心态。然而，如果你处于领导职位，区分微调是必需的还是想要的可能会很困难。
- en: ^([4](ch07.html#id1374-marker)) 0314 denotes the date this GPT-4 version came
    out, March 14, 2024\. The specific date stamp matters because different versions
    vary significantly in performance.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch07.html#id1374-marker)) 0314代表这个GPT-4版本发布的日期，2024年3月14日。具体的日期戳很重要，因为不同版本的性能差异很大。
- en: ^([5](ch07.html#id1377-marker)) Some people, such as the authors of the Llama
    3.1 paper ([Dubey et al., 2024](https://arxiv.org/abs/2407.21783)), adhere to
    “the principle that post-training should align the model to ‘know what it knows’
    rather than add knowledge.”
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.html#id1377-marker)) 一些人士，例如Llama 3.1论文的作者（Dubey等人，2024年[1](https://arxiv.org/abs/2407.21783)），坚持“训练后应使模型与‘知道它知道什么’对齐，而不是添加知识”的原则。
- en: ^([6](ch07.html#id1382-marker)) Other than backpropagation, a promising approach
    to training neural networks is evolutionary strategy. One example, described by
    [Maheswaranathan et al.](https://oreil.ly/B59ci), combines random search with
    surrogate gradients, instead of using real gradients, to update model weights.
    Another interesting approach is direct feedback alignment ([Arild Nøkland, 2016](https://arxiv.org/abs/1609.01596)).
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.html#id1382-marker)) 除了反向传播之外，训练神经网络的一个有前景的方法是进化策略。一个例子，由Maheswaranathan等人描述，结合了随机搜索和代理梯度，而不是使用真实梯度来更新模型权重。另一种有趣的方法是直接反馈对齐（Arild
    Nøkland，2016年[4](https://arxiv.org/abs/1609.01596)）。
- en: ^([7](ch07.html#id1383-marker)) If a parameter is not trainable, it doesn’t
    need to be updated and, therefore, there’s no need to compute its gradient.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.html#id1383-marker)) 如果一个参数不可训练，则不需要更新它，因此不需要计算其梯度。
- en: '^([8](ch07.html#id1389-marker)) Some might say that you’re not doing AI until
    you’ve seen a “RuntimeError: CUDA out of memory” error.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch07.html#id1389-marker)) 有些人可能会说，直到你看到“RuntimeError: CUDA out of memory”错误，你才算真正在做AI。'
- en: ^([9](ch07.html#id1390-marker)) To learn more about inference memory calculation,
    check out Carol Chen’s [“Transformer Inference Arithmetic”](https://oreil.ly/u7wYx),
    kipply’s blog (March 2022).
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch07.html#id1390-marker)) 想了解更多关于推理内存计算的信息，请查看Carol Chen的“Transformer
    Inference Arithmetic”（[3](https://oreil.ly/u7wYx)），kipply的博客（2022年3月）。
- en: ^([10](ch07.html#id1395-marker)) To learn more about training memory calculation,
    check out EleutherAI’s [“Transformer Math 101”](https://oreil.ly/Xe7h6) (Anthony
    et al., April 2023).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch07.html#id1395-marker)) 想了解更多关于训练内存计算的信息，请查看EleutherAI的“Transformer
    Math 101”（Anthony等人，2023年4月[2](https://oreil.ly/Xe7h6)）。
- en: ^([11](ch07.html#id1396-marker)) Google introduced BFloat16 as [“the secret
    to high performance on Cloud TPUs”](https://oreil.ly/atIgi).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch07.html#id1396-marker)) Google推出了BFloat16，将其称为“在Cloud TPUs上实现高性能的秘密”。
- en: ^([12](ch07.html#id1397-marker)) Integer formats are also called *fixed point*
    formats.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch07.html#id1397-marker)) 整数格式也被称为*定点*格式。
- en: ^([13](ch07.html#id1398-marker)) Range bits are called *exponents*. Precision
    bits are called significand*s*.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch07.html#id1398-marker)) 范围位被称为*指数*。精度位被称为*尾数*。
- en: ^([14](ch07.html#id1401-marker)) Note that usually the number at the end of
    a format’s name signifies how many bits it occupies, but TF32 actually has 19
    bits, not 32 bits. I believe it was named so to suggest its functional compatibility
    with FP32\. But honestly, why it’s called TF32 and not TF19 keeps me up at night.
    An ex-coworker at NVIDIA volunteered his conjecture that people might be skeptical
    of weird formats (19-bit), so naming this format TF32 makes it look more friendly.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch07.html#id1401-marker)) 注意，通常格式名称末尾的数字表示它占用多少位，但TF32实际上有19位，而不是32位。我相信它之所以被命名为TF32，而不是TF19，是为了暗示它与FP32的功能兼容性。但说实话，为什么叫TF32而不是TF19让我整夜辗转反侧。一位前同事在NVIDIA自愿提出了他的猜想，认为人们可能对奇怪的格式（19位）持怀疑态度，因此将此格式命名为TF32使其看起来更友好。
- en: '^([15](ch07.html#id1404-marker)) The FP16 and BF16 confusion continued with
    Llama 3.1\. See X and Threads discussions: [1](https://en.wikipedia.org/wiki/IEEE_754);
    [2](https://x.com/abacaj/status/1695334296792264792?s=20), [3](https://oreil.ly/U8L4d),
    [4](https://oreil.ly/8ush1); and llama.cpp’s [benchmark between BF16 and FP16](https://github.com/ggerganov/llama.cpp/pull/7150),
    [Bloke’s writeup](https://oreil.ly/0vuze), and [Raschka’s writeup](https://oreil.ly/WK_zT).'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch07.html#id1404-marker)) FP16和BF16的混淆在Llama 3.1中继续存在。参见X和Threads讨论：[1](https://en.wikipedia.org/wiki/IEEE_754)；[2](https://x.com/abacaj/status/1695334296792264792?s=20)，[3](https://oreil.ly/U8L4d)，[4](https://oreil.ly/8ush1)；以及llama.cpp在BF16和FP16之间的[基准测试](https://github.com/ggerganov/llama.cpp/pull/7150)，[Bloke的总结](https://oreil.ly/0vuze)，和[Raschka的总结](https://oreil.ly/WK_zT)。
- en: ^([16](ch07.html#id1405-marker)) Designing numerical formats is a fascinating
    discipline. Being able to create a lower-precision format that doesn’t compromise
    a system’s quality can make that system much cheaper and faster, enabling new
    use cases.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch07.html#id1405-marker)) 设计数值格式是一门迷人的学科。能够创建一个不降低系统质量且精度较低的格式，可以使该系统更加便宜和快速，从而启用新的用例。
- en: ^([17](ch07.html#id1406-marker)) Another major contributor to the memory footprint
    of transformer-based models is the KV cache, which is discussed in [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301).
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch07.html#id1406-marker)) 另一个对基于transformer的模型内存占用有重大贡献的因素是KV缓存，这在[第9章](ch09.html#ch09_inference_optimization_1730130963006301)中有讨论。
- en: ^([18](ch07.html#id1407-marker)) The smallest possible float size that follows
    all IEEE principles is 4-bit.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch07.html#id1407-marker)) 符合所有IEEE原则的最小浮点数大小是4位。
- en: ^([19](ch07.html#id1408-marker)) The authors of the Xnor-Net paper spun off
    Xnor.ai, a startup that focused on model compression. [In early 2020, it was acquired
    by Apple for a reported $200M](https://oreil.ly/V4pma).
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch07.html#id1408-marker)) Xnor-Net论文的作者成立了Xnor.ai，一家专注于模型压缩的初创公司。[在2020年初，它以2亿美元的价格被苹果公司收购](https://oreil.ly/V4pma)。
- en: ^([20](ch07.html#id1415-marker)) During training, the model’s weights are updated
    via multiple steps. Small rounding changes can compound during the training process,
    making it difficult for the model to achieve the desirable performance. On top
    of that, loss values require precise computation. Small changes in the loss value
    can point parameter updates in the wrong direction.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch07.html#id1415-marker)) 在训练过程中，模型的权重通过多个步骤进行更新。小的舍入变化在训练过程中可能会累积，使得模型难以达到期望的性能。此外，损失值需要精确计算。损失值的小变化可能会将参数更新指向错误的方向。
- en: '^([21](ch07.html#id1416-marker)) Personal anecdote: much of my team’s work
    at NVIDIA was on mixed precision training. See [“Mixed Precision Training for
    NLP and Speech Recognition with OpenSeq2Seq”](https://oreil.ly/QL2gL) (Huyen et
    al., NVIDIA Developer Technical Blog, October 2018).'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch07.html#id1416-marker)) 个人轶事：我在NVIDIA团队的大部分工作都集中在混合精度训练上。参见[“使用OpenSeq2Seq进行NLP和语音识别的混合精度训练”](https://oreil.ly/QL2gL)（Huyen等人，NVIDIA开发者技术博客，2018年10月）。
- en: ^([22](ch07.html#id1429-marker)) In partial finetuning, it’s common to finetune
    the layers closest to the output layer because those layers are usually more task-specific,
    whereas earlier layers tend to capture more general features.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch07.html#id1429-marker)) 在部分微调中，通常会对靠近输出层的层进行微调，因为这些层通常更具有任务特定性，而早期层则倾向于捕获更通用的特征。
- en: ^([23](ch07.html#id1431-marker)) I’ve never met a single person who could explain
    to me, on the spot, the differences between these techniques.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch07.html#id1431-marker)) 我从未遇到过能立即向我解释这些技术之间差异的人。
- en: ^([24](ch07.html#id1439-marker)) To effectively use LoRA for a model, it’s necessary
    to understand that model’s architecture. [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)
    already covered the weight composition of some transformer-based models. For the
    exact weight composition of a model, refer to its paper.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch07.html#id1439-marker)) 要有效地使用LoRA来训练模型，有必要了解该模型的架构。[第2章](ch02.html#ch02_understanding_foundation_models_1730147895571359)已经涵盖了某些基于transformer的模型的权重组成。对于模型的精确权重组成，请参阅其论文。
- en: ^([25](ch07.html#id1441-marker)) As of this writing, some finetuning frameworks
    like [Fireworks](https://oreil.ly/82-jJ) only allow a maximum LoRA rank of 32\.
    However, this constraint is unlikely due to performance and more likely due to
    their hardware’s memory constraint.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch07.html#id1441-marker)) 在撰写本文时，一些微调框架如[Fireworks](https://oreil.ly/82-jJ)仅允许最大LoRA秩为32。然而，这种限制不太可能是由于性能问题，而更有可能是由于他们硬件的内存限制。
- en: ^([26](ch07.html#id1444-marker)) Search for these adapters by tags “adapter”,
    “peft”, or “LoRA”.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch07.html#id1444-marker)) 通过标签“适配器”、“peft”或“LoRA”搜索这些适配器。
- en: ^([27](ch07.html#id1447-marker)) QLoRA isn’t the only quantized LoRA work. Many
    research labs have been working on quantized LoRA without publicly discussing
    it.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch07.html#id1447-marker)) QLoRA并不是唯一的量化LoRA工作。许多研究实验室都在进行量化LoRA的研究，但没有公开讨论。
- en: ^([28](ch07.html#id1463-marker)) My book, [*Designing Machine Learning Systems*](https://oreil.ly/u_cVP)
    has a section on “ML on the Cloud and on the Edge.”
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch07.html#id1463-marker)) 我的书[*设计机器学习系统*](https://oreil.ly/u_cVP)中有一个关于“云和边缘上的机器学习”的部分。
- en: ^([29](ch07.html#id1465-marker)) You can read more about ensemble methods in
    my book [*Designing Machine Learning Systems*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/).
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch07.html#id1465-marker)) 你可以在我的书[*设计机器学习系统*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)中了解更多关于集成方法的内容。
- en: ^([30](ch07.html#id1466-marker)) Averaging works not just with weights but also
    with embeddings. For example, given a sentence, you can use a word embedding algorithm
    to generate an embedding vector for each word in the sentence, then average all
    these word embeddings into a sentence embedding. When I started out in ML, I couldn’t
    believe that averaging seems to just work. It’s magical when simple components,
    when used correctly, can create something so wonderfully perplexing, like AI.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch07.html#id1466-marker)) 平均化不仅适用于权重，也适用于嵌入。例如，给定一个句子，你可以使用一个词嵌入算法为句子中的每个单词生成一个嵌入向量，然后将所有这些词嵌入平均成一个句子嵌入。当我刚开始机器学习时，我简直不敢相信平均化似乎总是有效。当简单的组件被正确使用时，可以创造出如此美妙而令人困惑的东西，就像AI一样，这真是神奇。
- en: ^([31](ch07.html#id1473-marker)) The assumption is that the parameters that
    undergo the most substantial changes during finetuning are the ones most crucial
    for the target task.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch07.html#id1473-marker)) 假设，在微调过程中发生最大变化的参数是对于目标任务最重要的参数。
- en: ^([32](ch07.html#id1474-marker)) TIES is abbreviated from “TrIm, Elect Sign,
    and merge,” while DARE is from “Drop And REscale.” I know, these abbreviations
    pain me too.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch07.html#id1474-marker)) TIES是从“TrIm, Elect Sign, and merge”缩写而来，而DARE是从“Drop
    And REscale”缩写而来。我知道，这些缩写也让我感到痛苦。
- en: ^([33](ch07.html#id1477-marker)) When task vectors are pruned, they become more
    sparse, but the finetuned model doesn’t. Pruning, in this case, isn’t to reduce
    the memory footprint or inference latency, but to improve performance.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch07.html#id1477-marker)) 当任务向量被剪枝时，它们变得更加稀疏，但微调后的模型不是。在这种情况下，剪枝不是为了减少内存占用或推理延迟，而是为了提高性能。
- en: ^([34](ch07.html#id1487-marker)) I debated for a long time whether to include
    the concatenation technique in this book, and decided to include it for completeness.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch07.html#id1487-marker)) 我长时间争论是否应该将连接技术包含在这本书中，并最终决定为了完整性而包含它。
- en: ^([35](ch07.html#id1489-marker)) In college, I made the painful mistake of letting
    my model train overnight, only to have it crash after eight hours because I tried
    to save the checkpoint in a nonexistent folder. All that progress was lost.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch07.html#id1489-marker)) 在大学时，我犯了一个痛苦的错误，让我的模型整夜训练，结果在八小时后崩溃，因为我试图在一个不存在的文件夹中保存检查点。所有的进步都丢失了。
- en: ^([36](ch07.html#id1497-marker)) While it’s commonly acknowledged that small
    batch sizes lead to unstable training, I wasn’t able to find good explanations
    for why that’s the case. If you have references about this, please feel free to
    send them my way.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ^([36](ch07.html#id1497-marker)) 虽然普遍认为小批量大小会导致训练不稳定，但我无法找到关于这种情况的良好解释。如果你有这方面的参考资料，请随时发送给我。
- en: '^([37](ch07.html#id1498-marker)) I tried to find the first paper where gradient
    accumulation was introduced but couldn’t. Its use in deep learning was mentioned
    as early as 2016 in [“Ako: Decentralised Deep Learning with Partial Gradient Exchange”](https://oreil.ly/GFeC7)
    (Watcharapichat et al., *Proceedings of the Seventh ACM Symposium on Cloud Computing*,
    2016). The concept seems to come from distributed training, where gradients computed
    on different machines need to be accumulated and used to update the model’s weights.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '^([37](ch07.html#id1498-marker)) 我试图找到介绍梯度累积的第一篇论文，但未能找到。它在2016年的[“Ako: Decentralised
    Deep Learning with Partial Gradient Exchange”](https://oreil.ly/GFeC7)（Watcharapichat等人，《第七届ACM云计算研讨会论文集》，2016）中被提及，其使用在深度学习中的应用最早可以追溯到2016年。这个概念似乎来自分布式训练，其中不同机器上计算的梯度需要累积并用于更新模型的权重。'
