<html><head></head><body><section data-pdf-bookmark="Chapter 7. Recurrent Neural Networks for Natural Language Processing" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648">&#13;
      <h1><span class="label">Chapter 7. </span>Recurrent Neural Networks for <span class="keep-together">Natural Language Processing</span></h1>&#13;
      <p>In <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a>, you saw how to tokenize and sequence text, turning sentences into tensors of numbers that could then be fed into a neural network.<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks" data-type="indexterm" id="ch7rnnall"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="ch7rnnall2"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks" data-tertiary="about" data-type="indexterm" id="id1279"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="about" data-type="indexterm" id="id1280"/> You then extended that in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a> by looking at embeddings, which constitute a way to have words with similar meanings cluster together to enable the calculation of sentiment. This worked really well, as you saw by building a sarcasm classifier. But there’s a limitation to that: namely, sentences aren’t just collections of words—and often, the <em>order</em> in which the words appear will dictate their overall meaning. Also, adjectives can add to or change the meaning of the nouns they appear beside. For example, the word <em>blue</em> might be meaningless from a sentiment perspective, as might <em>sky</em>, but when you put them together to get <em>blue sky</em>, it indicates a clear sentiment that’s usually positive. Finally, some nouns may qualify others, such as in <em>rain cloud</em>, <em>writing desk</em>, and <em>coffee mug</em>.</p>&#13;
      <p>To take sequences like this into account, you need to take an additional approach: you need to factor <em>recurrence</em> into the model architecture. In this chapter, you’ll look at different ways of doing this. We’ll explore how sequence information can be learned and how you can use this information to create a type of model that is better able to understand text: the <em>recurrent neural network</em> (RNN).</p>&#13;
      <section data-pdf-bookmark="The Basis of Recurrence" data-type="sect1"><div class="sect1" id="ch07_the_basis_of_recurrence_1748549654891942">&#13;
        <h1>The Basis of Recurrence</h1>&#13;
        <p>To understand how recurrence<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks" data-tertiary="recurrence" data-type="indexterm" id="ch7recur"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="recurrence" data-type="indexterm" id="ch7recur2"/><a contenteditable="false" data-primary="neurons" data-secondary="recurrent neurons" data-type="indexterm" id="ch7recur3"/> might work, let’s first consider the limitations of the models used thus far in the book. Ultimately, creating a model looks a little bit like <a data-type="xref" href="#ch07_figure_1_1748549654877957">Figure 7-1</a>. You provide data and labels and define a model architecture, and the model learns the rules that fit the data to the labels. Those rules then become available to you as an application programming interface (API) that will give you back predicted labels for future data.</p>&#13;
        <figure><div class="figure" id="ch07_figure_1_1748549654877957">&#13;
          <img alt="" src="assets/aiml_0701.png"/>&#13;
          <h6><span class="label">Figure 7-1. </span>High-level view of model creation</h6>&#13;
        </div></figure>&#13;
        <p>But, as you can see, the data is lumped in wholesale. There’s no granularity involved and no effort to understand the sequence in which that data occurs. This means the words <em>blue</em> and <em>sky</em> have no different meaning in sentences such as, “Today I am blue, because the sky is gray,” and “Today I am happy, and there’s a beautiful blue sky.” To us, the difference in the use of these words is obvious, but to a model, with the architecture shown here, there really is no difference.</p>&#13;
        <p>So, how do we fix this? Let’s first explore the nature of recurrence, and from there, you’ll be able to see how a basic RNN can work.</p>&#13;
        <p>Consider the famous<a contenteditable="false" data-primary="Fibonacci sequence" data-type="indexterm" id="id1281"/> Fibonacci sequence of numbers. In case you aren’t familiar with it, I’ve put some of them into <a data-type="xref" href="#ch07_figure_2_1748549654878009">Figure 7-2</a>.</p>&#13;
        <figure><div class="figure" id="ch07_figure_2_1748549654878009">&#13;
          <img alt="" src="assets/aiml_0702.png"/>&#13;
          <h6><span class="label">Figure 7-2. </span>The first few numbers in the Fibonacci sequence</h6>&#13;
        </div></figure>&#13;
        <p>The idea behind this sequence is that every number is the sum of the two numbers preceding it. So if we start with 1 and 2, the next number is 1 + 2, which is 3. The one after that is 2 + 3, which is 5, and then there’s 3 + 5, which is 8, and so on.</p>&#13;
        <p>We can place this in a computational graph to get <a data-type="xref" href="#ch07_figure_3_1748549654878036">Figure 7-3</a>.</p>&#13;
        <figure><div class="figure" id="ch07_figure_3_1748549654878036">&#13;
          <img alt="" src="assets/aiml_0703.png"/>&#13;
          <h6><span class="label">Figure 7-3. </span>A computational graph representation of the Fibonacci sequence</h6>&#13;
        </div></figure>&#13;
        <p class="pagebreak-before">Here, you can see that we feed 1 and 2 into the function and get 3 as the output. We then carry the second parameter (2) over to the next step and feed it into the function along with the output from the previous step (3). The output of this is 5, and it gets fed into the function with the second parameter from the previous step (3) to produce an output of 8. This process continues indefinitely, with every operation depending on those before it. The 1 at the top left sort of “survives” through the process—it’s an element of the 3 that gets fed into the second operation, it’s an element of the 5 that gets fed into the third operation, and so on. Thus, some of the essence of the 1 is preserved throughout the sequence, though its impact on the overall value is diminished.</p>&#13;
        <p>This is analogous to how a recurrent neuron is architected. You can see the typical representation of a recurrent neuron in <a data-type="xref" href="#ch07_figure_4_1748549654878060">Figure 7-4</a>.</p>&#13;
        <figure><div class="figure" id="ch07_figure_4_1748549654878060">&#13;
          <img alt="" src="assets/aiml_0704.png"/>&#13;
          <h6><span class="label">Figure 7-4. </span>A recurrent neuron</h6>&#13;
        </div></figure>&#13;
        <p>A value <em>x</em> is fed into the function <em>F</em> at a time step, so it’s typically labeled <em>x</em><sub><em>t</em></sub>. This produces an output <em>y</em> at that time step, which is typically labeled <em>y</em><sub><em>t</em></sub>. It also produces a value that is fed forward to the next step, which is indicated by the arrow from <em>F</em> to itself.</p>&#13;
        <p>This is made a little clearer if you look at how recurrent neurons work beside one another across time steps, which you can see in <a data-type="xref" href="#ch07_figure_5_1748549654878082">Figure 7-5</a>.</p>&#13;
        <p>Here, <em>x</em><sub>0</sub> is operated on to get <em>y</em><sub>0</sub> and a value that’s passed forward. The next step gets that value and <em>x</em><sub>1</sub> and produces <em>y</em><sub>1</sub> and a value that’s passed forward. The next one gets that value and <em>x</em><sub>2</sub> and produces <em>y</em><sub>2</sub> and a passed-forward value, and so on down the line. This is similar to what we saw with the Fibonacci sequence, and I always find it to be a handy mnemonic when trying to remember how an RNN works.<a contenteditable="false" data-primary="" data-startref="ch7recur" data-type="indexterm" id="id1282"/><a contenteditable="false" data-primary="" data-startref="ch7recur2" data-type="indexterm" id="id1283"/><a contenteditable="false" data-primary="" data-startref="ch7recur3" data-type="indexterm" id="id1284"/></p>&#13;
                <figure><div class="figure" id="ch07_figure_5_1748549654878082">&#13;
          <img alt="" src="assets/aiml_0705.png"/>&#13;
          <h6><span class="label">Figure 7-5. </span>Recurrent neurons in time steps</h6>&#13;
        </div></figure>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Extending Recurrence for Language" data-type="sect1"><div class="sect1" id="ch07_extending_recurrence_for_language_1748549654892020">&#13;
        <h1>Extending Recurrence for Language</h1>&#13;
        <p>In the previous section, you saw<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks" data-tertiary="recurrence extended for language" data-type="indexterm" id="ch7reclan"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="recurrence extended for language" data-type="indexterm" id="ch7reclan2"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-type="indexterm" id="ch7reclan3"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks" data-tertiary="long short-term memory" data-type="indexterm" id="ch7reclan4"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="long short-term memory" data-type="indexterm" id="ch7reclan5"/> how an RNN operating over several time steps can help maintain context across a sequence. Indeed, we’ll use RNNs for sequence modeling later in this book—but there’s a nuance when it comes to language that you can miss when using a simple RNN like those shown in <a data-type="xref" href="#ch07_figure_4_1748549654878060">Figure 7-4</a> and <a data-type="xref" href="#ch07_figure_5_1748549654878082">Figure 7-5</a>. As in the Fibonacci sequence example mentioned earlier, the amount of context that’s carried over will diminish over time. The effect of the output of the neuron at step 1 is huge at step 2, smaller at step 3, smaller still at step 4, and so on. So, if we have a sentence like “Today has a beautiful blue &lt;something&gt;,” the word <em>blue</em> will have a strong impact on what the next word could be: we can guess that it’s likely to be <em>sky</em>. But what about context that comes from earlier in a sentence? For example, consider the sentence “I lived in Ireland, so in high school, I had to learn how to speak and write &lt;something&gt;.”</p>&#13;
        <p>That &lt;something&gt; is <em>Gaelic</em>, but the word that really gives us that context is <em>Ireland</em>, which is much earlier in the sentence. Thus, for us to be able to recognize what &lt;something&gt; should be, we need a way to preserve context across a longer distance. The short-term memory of an RNN needs to get longer, and in recognition of this, an enhancement to the architecture called <em>long short-term memory</em> (LSTM) was <span class="keep-together">invented.</span></p>&#13;
        <p class="pagebreak-before">While I won’t go into detail on the underlying architecture of how LSTMs work, the high-level diagram shown in <a data-type="xref" href="#ch07_figure_6_1748549654878103">Figure 7-6</a> gets the main point across. <a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="information online" data-type="indexterm" id="id1285"/><a contenteditable="false" data-primary="online resources" data-secondary="long short-term memory information" data-type="indexterm" id="id1286"/>To learn more about the internal operations of LSTM, check out Christopher Olah’s excellent <a href="https://oreil.ly/6KcFA">blog post on the subject</a>.</p>&#13;
        <p>The LSTM architecture enhances the basic RNN by adding a “cell state” that enables context to be maintained not just from step to step but across the entire sequence of steps. Remembering that these are neurons that learn in the way neurons do, you can see that this enhancement ensures that the context that is important will be learned over time.</p>&#13;
        <figure><div class="figure" id="ch07_figure_6_1748549654878103">&#13;
          <img alt="" src="assets/aiml_0706.png"/>&#13;
          <h6><span class="label">Figure 7-6. </span>High-level view of LSTM architecture</h6>&#13;
        </div></figure>&#13;
        <p>An important part of an LSTM<a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="bidirectionality" data-type="indexterm" id="id1287"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="long short-term memory" data-tertiary="bidirectionality" data-type="indexterm" id="id1288"/> is that it can be <em>bidirectional</em>—the time steps can be iterated both forward and backward so that context can be learned in both directions. Often, context for a word can come <em>after</em> it in the sentence and not just before.</p>&#13;
        <p>See <a data-type="xref" href="#ch07_figure_7_1748549654878124">Figure 7-7</a> for a high-level view of this.</p>&#13;
        <figure><div class="figure" id="ch07_figure_7_1748549654878124">&#13;
          <img alt="" src="assets/aiml_0707.png"/>&#13;
          <h6><span class="label">Figure 7-7. </span>High-level view of LSTM bidirectional architecture</h6>&#13;
        </div></figure>&#13;
        <p>This is how evaluation in the direction from 0 to <code>number_of_steps</code> is done, and it’s also how evaluation from <code>number_of_steps</code> to 0 is done. At each step, the <em>y</em> result is an aggregation of the “forward” pass and the “backward” pass. You can see this in <a data-type="xref" href="#ch07_figure_8_1748549654878143">Figure 7-8</a>.</p>&#13;
        <figure><div class="figure" id="ch07_figure_8_1748549654878143">&#13;
          <img alt="" src="assets/aiml_0708.png"/>&#13;
          <h6><span class="label">Figure 7-8. </span>Bidirectional LSTM</h6>&#13;
        </div></figure>&#13;
        <p>It’s easy to confuse the bidirectional nature of the LSTM with the terms <em>forward</em> and <em>backward</em> when it comes to the training of the network, but they’re very different. When I refer to the forward and backward pass, I’m referring to the setting of the parameters of the neurons and their updating from the learning process, respectively. Don’t confuse this with the values that the LSTM is paying attention to as being the next or previous tokens in the sequence.</p>&#13;
        <p>Also, consider each neuron at each time step to be F0, F1, F2, etc. The direction of the time step is shown, so the calculation at F1 in the forward direction is F1(-&gt;), and in the reverse direction, it’s (&lt;-)F1. The values of these are aggregated to give the <em>y</em> value for that time step. Additionally, the cell state is bidirectional, and this can be really useful for managing context in sentences. Again, considering the sentence “I lived in Ireland, so in high school, I had to learn how to speak and write &lt;something&gt;,” you can see how the &lt;something&gt; was qualified to be <em>Gaelic</em> by the context word <em>Ireland</em>. But what if it were the other way around: “I lived in &lt;this country&gt;, so in high school, I had to learn how to speak and write Gaelic”? You can see that by going <em>backward</em> through the sentence, we can learn about what &lt;this country&gt; should be. Thus, using bidirectional LSTMs can be very powerful for understanding sentiment in text. (And as you’ll see in <a data-type="xref" href="ch08.html#ch08_using_ml_to_create_text_1748549671852453">Chapter 8</a>, they’re really powerful for generating text, too!)</p>&#13;
        <p>Of course, there’s a lot going on with LSTMs, in particular bidirectional ones, so expect training to be slow. Here’s where it’s worth investing in a GPU or at the very least using a hosted one in Google Colab if you can.<a contenteditable="false" data-primary="" data-startref="ch7reclan" data-type="indexterm" id="id1289"/><a contenteditable="false" data-primary="" data-startref="ch7reclan2" data-type="indexterm" id="id1290"/><a contenteditable="false" data-primary="" data-startref="ch7reclan3" data-type="indexterm" id="id1291"/><a contenteditable="false" data-primary="" data-startref="ch7reclan4" data-type="indexterm" id="id1292"/><a contenteditable="false" data-primary="" data-startref="ch7reclan5" data-type="indexterm" id="id1293"/></p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Creating a Text Classifier with RNNs" data-type="sect1"><div class="sect1" id="ch07_creating_a_text_classifier_with_rnns_1748549654892097">&#13;
        <h1>Creating a Text Classifier with RNNs</h1>&#13;
        <p>In <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>, you experimented<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks" data-tertiary="text classifier via" data-type="indexterm" id="ch7sarc"/><a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="text classifier via" data-type="indexterm" id="ch7sarc2"/><a contenteditable="false" data-primary="text classifier via recurrent neural networks" data-type="indexterm" id="ch7sarc3"/><a contenteditable="false" data-primary="sarcasm detector" data-secondary="RNNs for text classifier" data-type="indexterm" id="ch7sarc4"/><a contenteditable="false" data-primary="embeddings" data-secondary="sarcasm detector" data-tertiary="RNNs for text classifier" data-type="indexterm" id="ch7sarc5"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="sarcasm detector via RNNs" data-type="indexterm" id="ch7sarc6"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="sarcasm detector via RNNs" data-type="indexterm" id="ch7sarc7"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="sarcasm detector via RNNs" data-type="indexterm" id="ch7sarc8"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="text classifier using" data-type="indexterm" id="ch7sarc9"/> with creating a classifier for the Sarcasm dataset by using embeddings. In that case, you turn words into vectors before aggregating them and then feeding them into dense layers for classification. But when you’re using an RNN layer such as an LSTM, you don’t do the aggregation, and you can feed the output of the embedding layer directly into the recurrent layer. <a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="recurrent layer same size as embedding" data-type="indexterm" id="id1294"/><a contenteditable="false" data-primary="text classifier via recurrent neural networks" data-secondary="recurrent layer same size as embedding" data-type="indexterm" id="id1295"/>When it comes to the dimensionality of the recurrent layer, a rule of thumb you’ll often see is that it’s the same size as the embedding dimension. This isn’t necessary, but it can be a good starting point. Also note that while in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a> I mentioned that the embedding dimension is often the fourth root of the size of the vocabulary, when using RNNs, you’ll often see that that rule may be ignored because it would make the size of the recurrent layer too small. </p>&#13;
        <p>For this example, I have used the number of neurons in the hidden layer as a starting point, and you can experiment from there.</p>&#13;
        <p>So, for example, you could update the simple model architecture for the sarcasm classifier you developed in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a> to the following to use a bidirectional LSTM:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">TextClassificationModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">,</code> &#13;
                 <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">24</code><code class="p">,</code> <code class="n">lstm_layers</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>&#13;
        <code class="nb">super</code><code class="p">(</code><code class="n">TextClassificationModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>&#13;
 &#13;
        <code class="c1"># Embedding layer</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>&#13;
 &#13;
        <code class="c1"># LSTM layer</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">lstm</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code>&#13;
            <code class="n">input_size</code><code class="o">=</code><code class="n">embedding_dim</code><code class="p">,</code>&#13;
            <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_dim</code><code class="p">,</code>&#13;
            <code class="n">num_layers</code><code class="o">=</code><code class="n">lstm_layers</code><code class="p">,</code>&#13;
            <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
            <code class="n">bidirectional</code><code class="o">=</code><code class="kc">True</code>&#13;
        <code class="p">)</code>&#13;
 &#13;
        <code class="c1"># Global pooling</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool1d</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>&#13;
 &#13;
        <code class="c1"># Fully connected layers</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code> <code class="o">*</code> <code class="mi">2</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
 &#13;
        <code class="c1"># Activation functions</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>&#13;
 &#13;
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>&#13;
        <code class="c1"># x shape: (batch_size, sequence_length)</code>&#13;
 &#13;
        <code class="c1"># Get embeddings</code>&#13;
        <code class="n">embedded</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>  &#13;
        <code class="c1"># Shape: (batch_size, sequence_length, embedding_dim)</code>&#13;
 &#13;
        <code class="c1"># Pass through LSTM</code>&#13;
        <code class="n">lstm_out</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">lstm</code><code class="p">(</code><code class="n">embedded</code><code class="p">)</code>  &#13;
        <code class="c1"># Shape: (batch_size, sequence_length, hidden_dim)</code>&#13;
 &#13;
        <code class="c1"># Transpose for global pooling </code>&#13;
        <code class="c1"># (expecting: batch, channels, sequence_length)</code>&#13;
        <code class="n">lstm_out</code> <code class="o">=</code> <code class="n">lstm_out</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>  &#13;
        <code class="c1"># Shape: (batch_size, hidden_dim, sequence_length)</code>&#13;
 &#13;
        <code class="c1"># Apply global pooling</code>&#13;
        <code class="n">pooled</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code><code class="p">(</code><code class="n">lstm_out</code><code class="p">)</code>  &#13;
        <code class="c1"># Shape: (batch_size, hidden_dim, 1)</code>&#13;
        <code class="n">pooled</code> <code class="o">=</code> <code class="n">pooled</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># Shape: (batch_size, hidden_dim)</code>&#13;
 &#13;
        <code class="c1"># Pass through fully connected layers</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">pooled</code><code class="p">))</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>&#13;
 &#13;
        <code class="k">return</code> <code class="n">x</code></pre>&#13;
        <p>You can then set the loss function and classifier to this. (Note that the LR is 0.001, or 1e–3.):</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define loss function and optimizer</code>&#13;
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>&#13;
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> &#13;
                       <code class="n">betas</code><code class="o">=</code><code class="p">(</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.999</code><code class="p">),</code> <code class="n">amsgrad</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>&#13;
        <p>When you print out the model architecture summary, you’ll see something like the following:</p>&#13;
        &#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Layer</code> <code class="p">(</code><code class="nb">type</code><code class="p">:</code><code class="n">depth</code><code class="o">-</code><code class="n">idx</code><code class="p">)</code>                   <code class="n">Output</code> <code class="n">Shape</code>              <code class="n">Param</code> <code class="c1">#</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">TextClassificationModel</code>                  <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Embedding</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">1</code>                         <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">85</code><code class="p">,</code> <code class="mi">7</code><code class="p">]</code>               <code class="mi">14</code><code class="p">,</code><code class="mi">000</code>&#13;
<code class="err">├─</code><code class="n">LSTM</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">2</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">85</code><code class="p">,</code> <code class="mi">48</code><code class="p">]</code>              <code class="mi">6</code><code class="p">,</code><code class="mi">336</code>&#13;
<code class="err">├─</code><code class="n">AdaptiveAvgPool1d</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">3</code>                 <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">48</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>               <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">4</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="mi">1</code><code class="p">,</code><code class="mi">176</code>&#13;
<code class="err">├─</code><code class="n">ReLU</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">5</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">6</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="mi">25</code>&#13;
<code class="err">├─</code><code class="n">Sigmoid</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">7</code>                           <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Total</code> <code class="n">params</code><code class="p">:</code> <code class="mi">21</code><code class="p">,</code><code class="mi">537</code>&#13;
<code class="n">Trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">21</code><code class="p">,</code><code class="mi">537</code>&#13;
<code class="n">Non</code><code class="o">-</code><code class="n">trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">0</code>&#13;
<code class="n">Total</code> <code class="n">mult</code><code class="o">-</code><code class="n">adds</code> <code class="p">(</code><code class="n">M</code><code class="p">):</code> <code class="mf">17.72</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Input</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.02</code>&#13;
<code class="n">Forward</code><code class="o">/</code><code class="n">backward</code> <code class="k">pass</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">1.20</code>&#13;
<code class="n">Params</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.09</code>&#13;
<code class="n">Estimated</code> <code class="n">Total</code> <code class="n">Size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">1.31</code>&#13;
<code class="o">==========================================================================</code></pre>&#13;
        <p>Note that the vocab size is 2,000 and the embedding dimension is 7. This gives 14,000 parameters in the embedding layer, and the bidirectional layer will have 48 neurons (24 out, 24 back) with a sequence length of 85 characters</p>&#13;
        <p><a data-type="xref" href="#ch07_figure_9_1748549654878164">Figure 7-9</a> shows the results of training with this over three hundred epochs.</p>&#13;
        <p>This gives us a network with only 21,537 parameters. As you can see, the accuracy of the network on training data rapidly climbs toward 85%, but the validation data plateaus at around 75%. This is similar to the figures we got earlier, but inspecting the loss chart in <a data-type="xref" href="#ch07_figure_10_1748549654878185">Figure 7-10</a> shows that while the loss for the test set diverged after 15 epochs, the validation loss turned to increase, indicating we have overfitting.</p>&#13;
        <figure><div class="figure" id="ch07_figure_9_1748549654878164">&#13;
          <img alt="" src="assets/aiml_0709.png"/>&#13;
          <h6><span class="label">Figure 7-9. </span>Accuracy for LSTM over 30 epochs</h6>&#13;
        </div></figure>&#13;
        <figure><div class="figure" id="ch07_figure_10_1748549654878185">&#13;
          <img alt="" src="assets/aiml_0710.png"/>&#13;
          <h6><span class="label">Figure 7-10. </span>Loss with LSTM over 30 epochs</h6>&#13;
        </div></figure>&#13;
        <p>However, this was just using a single LSTM layer with a hidden layer of 24 neurons. In the next section, you’ll see how to use stacked LSTMs and explore the impact on the accuracy of classifying this dataset.</p>&#13;
        <section data-pdf-bookmark="Stacking LSTMs" data-type="sect2"><div class="sect2" id="ch07_stacking_lstms_1748549654892175">&#13;
          <h2>Stacking LSTMs</h2>&#13;
          <p>In the previous section, you saw<a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="text classifier via" data-tertiary="stacking LSTMs" data-type="indexterm" id="ch7stk"/><a contenteditable="false" data-primary="text classifier via recurrent neural networks" data-secondary="stacking LSTMs" data-type="indexterm" id="ch7stk2"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="text classifier using" data-tertiary="stacking LSTMs" data-type="indexterm" id="ch7stk3"/> how to use an LSTM layer after the embedding layer to help classify the contents of the sarcasm dataset. But LSTMs can be stacked on top of one another, and this approach is used in many state-of-the-art NLP models.</p>&#13;
          <p>Stacking LSTMs with PyTorch is pretty straightforward. You add them as extra layers just like you would with any other layer, but you will need to be careful in specifying the dimensions. So, for example, if the first LSTM has <em>x</em> number of hidden layers, then the next LSTM will have <em>x </em>number of inputs. If the LST is bidirectional, then the next will need to double the size. Here’s an example:</p>&#13;
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># First LSTM layer</code>&#13;
<code class="bp">self</code><code class="o">.</code><code class="n">lstm1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code>&#13;
    <code class="n">input_size</code><code class="o">=</code><code class="n">embedding_dim</code><code class="p">,</code>&#13;
    <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_dim</code><code class="p">,</code>&#13;
    <code class="n">num_layers</code><code class="o">=</code><code class="n">lstm_layers</code><code class="p">,</code>&#13;
    <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
    <code class="n">bidirectional</code><code class="o">=</code><code class="kc">True</code>&#13;
<code class="p">)</code>&#13;
 &#13;
<code class="c1"># Second LSTM layer</code>&#13;
<code class="c1"># Note: Input size is hidden_dim*2 because first LSTM is bidirectional.</code>&#13;
<code class="bp">self</code><code class="o">.</code><code class="n">lstm2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code>&#13;
    <code class="n">input_size</code><code class="o">=</code><code class="n">hidden_dim</code> <code class="o">*</code> <code class="mi">2</code><code class="p">,</code>&#13;
    <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_dim</code><code class="p">,</code>&#13;
    <code class="n">num_layers</code><code class="o">=</code><code class="n">lstm_layers</code><code class="p">,</code>&#13;
    <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
    <code class="n">bidirectional</code><code class="o">=</code><code class="kc">True</code>&#13;
<code class="p">)</code>&#13;
 </pre>&#13;
          <p>Note that the <code>input_size</code> for the first layer is the embedding dimension because it’s preceded by the embedding layer. The second LSTM then has its input size as <span class="keep-together">(<code>hidden_dim * 2</code>)</span> because the output from the first LSTM is that size, given that it’s bidirectional.</p>&#13;
          <p>The model architecture will look like this:</p>&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Layer</code> <code class="p">(</code><code class="nb">type</code><code class="p">:</code><code class="n">depth</code><code class="o">-</code><code class="n">idx</code><code class="p">)</code>                   <code class="n">Output</code> <code class="n">Shape</code>              <code class="n">Param</code> <code class="c1">#</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">TextClassificationModel</code>                  <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Embedding</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">1</code>                         <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">85</code><code class="p">,</code> <code class="mi">7</code><code class="p">]</code>               <code class="mi">14</code><code class="p">,</code><code class="mi">000</code>&#13;
<code class="err">├─</code><code class="n">LSTM</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">2</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">85</code><code class="p">,</code> <code class="mi">48</code><code class="p">]</code>              <code class="mi">6</code><code class="p">,</code><code class="mi">336</code>&#13;
<code class="err">├─</code><code class="n">LSTM</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">3</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">85</code><code class="p">,</code> <code class="mi">48</code><code class="p">]</code>              <code class="mi">14</code><code class="p">,</code><code class="mi">208</code>&#13;
<code class="err">├─</code><code class="n">AdaptiveAvgPool1d</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">4</code>                 <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">48</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>               <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">5</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="mi">1</code><code class="p">,</code><code class="mi">176</code>&#13;
<code class="err">├─</code><code class="n">ReLU</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">6</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">7</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="mi">25</code>&#13;
<code class="err">├─</code><code class="n">Sigmoid</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">8</code>                           <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Total</code> <code class="n">params</code><code class="p">:</code> <code class="mi">35</code><code class="p">,</code><code class="mi">745</code>&#13;
<code class="n">Trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">35</code><code class="p">,</code><code class="mi">745</code>&#13;
<code class="n">Non</code><code class="o">-</code><code class="n">trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">0</code>&#13;
<code class="n">Total</code> <code class="n">mult</code><code class="o">-</code><code class="n">adds</code> <code class="p">(</code><code class="n">M</code><code class="p">):</code> <code class="mf">56.37</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Input</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.02</code>&#13;
<code class="n">Forward</code><code class="o">/</code><code class="n">backward</code> <code class="k">pass</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">2.25</code>&#13;
<code class="n">Params</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.14</code>&#13;
<code class="n">Estimated</code> <code class="n">Total</code> <code class="n">Size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">2.41</code>&#13;
<code class="o">==========================================================================</code>&#13;
 </pre>&#13;
          <p>Adding the extra layer will give us roughly 14,000 extra parameters that need to be learned, which is an increase of about 75%. So, it might slow the network down, but the cost is relatively low if there’s a reasonable benefit.</p>&#13;
          <p class="pagebreak-before">After training for three hundred epochs, the result looks like <a data-type="xref" href="#ch07_figure_11_1748549654878205">Figure 7-11</a>. While the accuracy on the validation set is flat, examining the loss (shown in <a data-type="xref" href="#ch07_figure_12_1748549654878225">Figure 7-12</a>) tells a different story. As you can see in <a data-type="xref" href="#ch07_figure_12_1748549654878225">Figure 7-12</a>, while the accuracy for both training and validation looked good, the validation loss quickly took off upward, which is a clear sign of overfitting.</p>&#13;
          <figure><div class="figure" id="ch07_figure_11_1748549654878205">&#13;
            <img alt="" src="assets/aiml_0711.png"/>&#13;
            <h6><span class="label">Figure 7-11. </span>Accuracy for stacked LSTM architecture</h6>&#13;
          </div></figure>&#13;
<p>This overfitting (which is indicated by the training accuracy climbing toward 100% as the loss falls smoothly while the validation accuracy is relatively steady and the loss increases drastically) is a result of the model getting overspecialized for the training set. As with the examples in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>, this shows that it’s easy to be lulled into a false sense of security if you just look at the accuracy metrics without examining the loss.</p>&#13;
          <figure><div class="figure" id="ch07_figure_12_1748549654878225">&#13;
            <img alt="" src="assets/aiml_0712.png"/>&#13;
            <h6><span class="label">Figure 7-12. </span>Loss for stacked LSTM architecture</h6>&#13;
          </div></figure>&#13;
          <section data-pdf-bookmark="Optimizing stacked LSTMs" data-type="sect3"><div class="sect3" id="ch07_optimizing_stacked_lstms_1748549654892243">&#13;
            <h3>Optimizing stacked LSTMs</h3>&#13;
            <p>In <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>, you saw that<a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="text classifier via" data-tertiary="LR to optimize stacked LSTMs" data-type="indexterm" id="ch7opt"/><a contenteditable="false" data-primary="text classifier via recurrent neural networks" data-secondary="stacking LSTMs" data-tertiary="LR to optimize stacked LSTMs" data-type="indexterm" id="ch7opt2"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="text classifier using" data-tertiary="LR to optimize stacked LSTMs" data-type="indexterm" id="ch7opt3"/><a contenteditable="false" data-primary="learning rate (LR)" data-secondary="optimizing stacked LSTMs" data-type="indexterm" id="ch7opt4"/><a contenteditable="false" data-primary="Adam optimizer" data-secondary="learning rate parameter adjustment" data-type="indexterm" id="id1296"/> a very effective method of reducing overfitting was to reduce the LR. It’s worth exploring here whether that will have a positive effect on an RNN, too.</p>&#13;
            <p>For example, the following code reduces the LR by 50%, from 0.0001 to 0.00005:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define loss function and optimizer</code><code>&#13;
</code><code class="n">criterion</code><code> </code><code class="o">=</code><code> </code><code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code class="n">optimizer</code><code> </code><code class="o">=</code><code> </code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code> </code><strong><code class="n">lr</code><code class="o">=</code></strong><strong><code class="mf">0.00005</code></strong><code class="p">,</code><code> </code><code>&#13;
</code><code>                       </code><code class="n">betas</code><code class="o">=</code><code class="p">(</code><code class="mf">0.9</code><code class="p">,</code><code> </code><code class="mf">0.999</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">amsgrad</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code><code>&#13;
</code><code> </code></pre>&#13;
            <p class="pagebreak-before"><a data-type="xref" href="#ch07_figure_13_1748549654878247">Figure 7-13</a> demonstrates the impact of this on training. As you can see, there’s a small difference in the validation accuracy, indicating that we’re overfitting a bit less.</p>&#13;
            <figure><div class="figure" id="ch07_figure_13_1748549654878247">&#13;
              <img alt="" src="assets/aiml_0713.png"/>&#13;
              <h6><span class="label">Figure 7-13. </span>Impact of reduced LR on accuracy with stacked LSTMs</h6>&#13;
            </div></figure>&#13;
            <p>While an initial look at <a data-type="xref" href="#ch07_figure_14_1748549654878268">Figure 7-14</a> similarly suggests a decent impact on loss due to the reduced LR, with the curve not moving up so sharply, it’s worth looking a little closer. We see that the loss on the training set is actually a little higher (~0.35 versus ~0.27) than the previous example, while the loss on the validation set is lower (~0.5 versus 0.6).</p>&#13;
            <p>Adjusting the LR hyperparameter certainly seems worth investigation.</p>&#13;
   <p>Indeed, further experimentation with the LR showed a marked improvement in getting training and validation curves to converge, indicating that while the network was less accurate after training, we could tell that it was generalizing better. Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_15_1748549654878288">7-15</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_16_1748549654878310">7-16</a> show the impact of using a lower LR (.0003 rather than .0005).</p>&#13;
            <figure><div class="figure" id="ch07_figure_14_1748549654878268">&#13;
              <img alt="" src="assets/aiml_0714.png"/>&#13;
              <h6><span class="label">Figure 7-14. </span>Impact of reduced LR on loss with stacked LSTMs</h6>&#13;
            </div></figure>&#13;
            <figure><div class="figure" id="ch07_figure_15_1748549654878288">&#13;
              <img src="assets/aiml_0715.png"/>&#13;
              <h6><span class="label">Figure 7-15. </span>Accuracy with further-reduced LR with stacked LSTM</h6>&#13;
            </div></figure>&#13;
            <figure><div class="figure" id="ch07_figure_16_1748549654878310">&#13;
              <img src="assets/aiml_0716.png"/>&#13;
              <h6><span class="label">Figure 7-16. </span>Loss with further-reduced LR and stacked LSTM</h6>&#13;
            </div></figure>&#13;
            <p>Indeed, reducing the LR even further, to .00001, gave potentially even better results, as shown in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_17_1748549654878331">7-17</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ch07_figure_18_1748549654878358">7-18</a>. As with the previous diagrams, while the overall accuracy isn’t as good and the loss is higher, that’s an indication that we’re getting closer to a “realistic” result for this network architecture and not being led into having a false sense of security by overfitting on the training data.</p>&#13;
             <p>In addition to changing the LR parameter,<a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="text classifier via" data-tertiary="dropout in stacked LSTMs" data-type="indexterm" id="ch7drop"/><a contenteditable="false" data-primary="text classifier via recurrent neural networks" data-secondary="stacking LSTMs" data-tertiary="dropout in stacked LSTMs" data-type="indexterm" id="ch7drop2"/><a contenteditable="false" data-primary="long short-term memory (LSTM)" data-secondary="text classifier using" data-tertiary="dropout in stacked LSTMs" data-type="indexterm" id="ch7drop3"/><a contenteditable="false" data-primary="dropout regularization" data-secondary="stacked LSTMs" data-type="indexterm" id="ch7drop4"/> you should also consider using dropout in the LSTM layers. It works exactly the same as for dense layers, as discussed in <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a>, where random neurons are dropped to prevent a proximity bias from impacting the learning. That being said, you should be careful about setting it <em>too</em> low, because when you start tweaking with different architectures, you might freeze the ability of the network to learn.<a contenteditable="false" data-primary="" data-startref="ch7opt" data-type="indexterm" id="id1297"/><a contenteditable="false" data-primary="" data-startref="ch7opt2" data-type="indexterm" id="id1298"/><a contenteditable="false" data-primary="" data-startref="ch7opt3" data-type="indexterm" id="id1299"/><a contenteditable="false" data-primary="" data-startref="ch7opt4" data-type="indexterm" id="id1300"/> </p>&#13;
            <figure><div class="figure" id="ch07_figure_17_1748549654878331">&#13;
              <img src="assets/aiml_0717.png"/>&#13;
              <h6><span class="label">Figure 7-17. </span>Accuracy with lower LR</h6>&#13;
            </div></figure>&#13;
            <figure><div class="figure" id="ch07_figure_18_1748549654878358">&#13;
              <p>&#13;
                <strong/>&#13;
              </p>&#13;
              <img src="assets/aiml_0718.png"/>&#13;
              <h6><span class="label">Figure 7-18. </span>Loss with lower LR</h6>&#13;
            </div></figure>&#13;
          </div></section>&#13;
          <section data-pdf-bookmark="Using dropout" data-type="sect3"><div class="sect3" id="ch07_using_dropout_1748549654892305">&#13;
            <h3>Using dropout</h3>&#13;
            <p>In addition to changing the LR parameter, you should also consider using dropout in the LSTM layers. It works exactly the same as for dense layers, as discussed in <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a>, where random neurons are dropped to prevent a proximity bias from impacting the learning.</p>&#13;
            <p>You can implement dropout by using <code>nn.Dropout</code>. Here’s an example:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout_rate</code><code class="p">)</code>&#13;
<code class="bp">self</code><code class="o">.</code><code class="n">lstm_dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout_rate</code><code class="p">)</code>&#13;
<code class="bp">self</code><code class="o">.</code><code class="n">final_dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout_rate</code><code class="p">)</code></pre>&#13;
            <p>Then, in your forward pass, you can apply the dropouts at the appropriate levels, like this:</p>&#13;
            <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>&#13;
    <code class="c1"># Get embeddings</code>&#13;
    <code class="n">embedded</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>  &#13;
 &#13;
    <code class="c1"># Apply first dropout after embedding layer</code>&#13;
    <code class="n">embedded</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dropout</code><code class="p">(</code><code class="n">embedded</code><code class="p">)</code>&#13;
 &#13;
    <code class="n">lstm1_out</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">lstm1</code><code class="p">(</code><code class="n">embedded</code><code class="p">)</code>&#13;
 &#13;
    <code class="c1"># Apply dropout between LSTM layers</code>&#13;
    <code class="n">lstm1_out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">lstm_dropout</code><code class="p">(</code><code class="n">lstm1_out</code><code class="p">)</code>&#13;
 &#13;
    <code class="n">lstm2_out</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">lstm2</code><code class="p">(</code><code class="n">lstm1_out</code><code class="p">)</code>&#13;
 &#13;
    <code class="c1"># Apply final dropout</code>&#13;
    <code class="n">lstm2_out</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">final_dropout</code><code class="p">(</code><code class="n">lstm2_out</code><code class="p">)</code>&#13;
 &#13;
    <code class="n">lstm_out</code> <code class="o">=</code> <code class="n">lstm2_out</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>&#13;
 &#13;
    <code class="n">pooled</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code><code class="p">(</code><code class="n">lstm_out</code><code class="p">)</code>  &#13;
    <code class="n">pooled</code> <code class="o">=</code> <code class="n">pooled</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="err">–</code><code class="mi">1</code><code class="p">)</code> &#13;
 &#13;
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">pooled</code><code class="p">))</code>&#13;
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>&#13;
 &#13;
    <code class="k">return</code> <code class="n">x</code></pre>&#13;
            <p>When I ran this with the lowest LR I had tested prior to dropout, the network didn’t learn. So, I moved the LR back up to 0.0003 and ran for 300 epochs using this dropout (note that the dropout rate is 0.2, so about 20% of neurons are dropped at random). The accuracy results can be seen in <a data-type="xref" href="#ch07_figure_19_1748549654878378">Figure 7-19</a>. The curves for training and validation are still close to each other, but they’re hitting greater than 75% accuracy, whereas without dropout, it was hard to get above 70%.</p>&#13;
            <figure><div class="figure" id="ch07_figure_19_1748549654878378">&#13;
              <img src="assets/aiml_0719.png"/>&#13;
              <h6><span class="label">Figure 7-19. </span>Accuracy of stacked LSTMs using dropout</h6>&#13;
            </div></figure>&#13;
            <p>As you can see, using dropout can have a positive impact on the accuracy of the network, which is good! There’s always a worry that losing neurons will make your model perform worse, but as we can see here, that’s not the case. But do be careful when using dropout because it can lead to underfitting or overfitting if not used appropriately.</p>&#13;
            <p>There’s also a positive impact on loss, as you can see in <a data-type="xref" href="#ch07_figure_20_1748549654878397">Figure 7-20</a>. While the curves are clearly diverging, they are closer than they were previously, and the validation set is flattening out at a loss of about 0.45, which also demonstrates an improvement! As this example shows, dropout is another handy technique that you can use to improve the performance of LSTM-based RNNs.</p>&#13;
            <p>It’s worth exploring these techniques for avoiding overfitting in your data, and it’s also worth exploring the techniques for preprocessing your data that we covered in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>. But there’s one thing that we haven’t yet tried: a form of transfer learning in which you can use pre-learned embeddings for words instead of trying to learn your own. We’ll explore that next.<a contenteditable="false" data-primary="" data-startref="ch7sarc" data-type="indexterm" id="id1301"/><a contenteditable="false" data-primary="" data-startref="ch7sarc2" data-type="indexterm" id="id1302"/><a contenteditable="false" data-primary="" data-startref="ch7sarc3" data-type="indexterm" id="id1303"/><a contenteditable="false" data-primary="" data-startref="ch7sarc4" data-type="indexterm" id="id1304"/><a contenteditable="false" data-primary="" data-startref="ch7sarc5" data-type="indexterm" id="id1305"/><a contenteditable="false" data-primary="" data-startref="ch7sarc6" data-type="indexterm" id="id1306"/><a contenteditable="false" data-primary="" data-startref="ch7sarc7" data-type="indexterm" id="id1307"/><a contenteditable="false" data-primary="" data-startref="ch7sarc8" data-type="indexterm" id="id1308"/><a contenteditable="false" data-primary="" data-startref="ch7sarc9" data-type="indexterm" id="id1309"/><a contenteditable="false" data-primary="" data-startref="ch7stk" data-type="indexterm" id="id1310"/><a contenteditable="false" data-primary="" data-startref="ch7stk2" data-type="indexterm" id="id1311"/><a contenteditable="false" data-primary="" data-startref="ch7stk3" data-type="indexterm" id="id1312"/><a contenteditable="false" data-primary="" data-startref="ch7drop" data-type="indexterm" id="id1313"/><a contenteditable="false" data-primary="" data-startref="ch7drop2" data-type="indexterm" id="id1314"/><a contenteditable="false" data-primary="" data-startref="ch7drop3" data-type="indexterm" id="id1315"/><a contenteditable="false" data-primary="" data-startref="ch7drop4" data-type="indexterm" id="id1316"/></p>&#13;
                        <figure><div class="figure" id="ch07_figure_20_1748549654878397">&#13;
              <img src="assets/aiml_0720.png"/>&#13;
              <h6><span class="label">Figure 7-20. </span>Loss curves for dropout-enabled LSTMs</h6>&#13;
            </div></figure>&#13;
          </div></section>&#13;
        </div></section>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Using Pretrained Embeddings with RNNs" data-type="sect1"><div class="sect1" id="ch07_using_pretrained_embeddings_with_rnns_1748549654892383">&#13;
        <h1>Using Pretrained Embeddings with RNNs</h1>&#13;
        <p>In all the previous examples,<a contenteditable="false" data-primary="recurrent neural networks (RNNs)" data-secondary="pretrained embeddings with" data-type="indexterm" id="ch7pre"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks" data-tertiary="pretrained embeddings with" data-type="indexterm" id="ch7pre2"/><a contenteditable="false" data-primary="pretrained embeddings" data-secondary="recurrent neural networks with" data-type="indexterm" id="ch7pre3"/><a contenteditable="false" data-primary="embeddings" data-secondary="pretrained embeddings" data-tertiary="recurrent neural networks with" data-type="indexterm" id="ch7pre4"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="pretrained embeddings with RNNs" data-type="indexterm" id="ch7pre5"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="pretrained embeddings with RNNs" data-type="indexterm" id="ch7pre6"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="pretrained embeddings with RNNs" data-type="indexterm" id="ch7pre7"/><a contenteditable="false" data-primary="Global Vectors for Word Representation (GloVe) pretrained embeddings" data-type="indexterm" id="ch7pre8"/><a contenteditable="false" data-primary="Stanford Global Vectors for Word Representation (GloVe) pretrained embeddings" data-type="indexterm" id="ch7pre9"/> you gathered the full set of words to be used in the training set and then trained embeddings with them. You initially aggregated them before feeding them into a dense network, and in this chapter, you explored how to improve the results using an RNN. While doing this, you were restricted to the words in your dataset and how their embeddings could be learned by using the labels from that dataset.</p>&#13;
        <p>Now, think back to <a data-type="xref" href="ch04.html#ch04_using_data_with_pytorch_1748548966496246">Chapter 4</a>, where we discussed transfer learning. What if instead of learning the embeddings for yourself, you could use pre-learned embeddings, where researchers have already done the hard work of turning words into vectors and those vectors are proven? <a contenteditable="false" data-primary="Pennington, Jeffrey" data-type="indexterm" id="id1317"/><a contenteditable="false" data-primary="Socher, Richard" data-type="indexterm" id="id1318"/><a contenteditable="false" data-primary="Manning, Christopher" data-type="indexterm" id="id1319"/>One example of this, as we saw in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a>, is the <a href="https://oreil.ly/4ENdQ">GloVe (Global Vectors for Word Representation) model</a> developed by Jeffrey Pennington, Richard Socher, and Christopher Manning at Stanford.</p>&#13;
        <p>In this case, the researchers have shared their pretrained word vectors for a variety of datasets:</p>&#13;
        <ul class="less_space">&#13;
          <li>&#13;
            <p>A 6-billion-token, 400,000-word vocabulary set in 50, 100, 200, and 300 dimensions with words taken from Wikipedia and Gigaword</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>A 42-billion-token, 1.9-million-word vocabulary in 300 dimensions from a common crawl</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>An 840-billion-token, 2.2-million-word vocabulary in 300 dimensions from a common crawl</p>&#13;
          </li>&#13;
          <li>&#13;
            <p>A 27-billion-token, 1.2-million-word vocabulary in 25, 50, 100, and 200 dimensions from a Twitter crawl of 2 billion tweets</p>&#13;
          </li>&#13;
        </ul>&#13;
        <p>Given that the vectors are already pretrained, it’s simple for you to reuse them in your PyTorch code, instead of learning them from scratch. First, you’ll have to download the GloVe data. I’ve opted to use the 6-billion-word version, in 50 dimensions, using this code to download and unzip it:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">urllib.request</code>&#13;
<code class="kn">import</code> <code class="nn">zipfile</code>&#13;
 &#13;
<code class="c1"># Download GloVe embeddings</code>&#13;
<code class="n">url</code> <code class="o">=</code> <code class="s2">"https://nlp.stanford.edu/data/glove.6B.zip"</code>&#13;
<code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlretrieve</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="s2">"glove.6B.zip"</code><code class="p">)</code>&#13;
 &#13;
<code class="c1"># Unzip</code>&#13;
<code class="k">with</code> <code class="n">zipfile</code><code class="o">.</code><code class="n">ZipFile</code><code class="p">(</code><code class="s2">"glove.6B.zip"</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">)</code> <code class="k">as</code> <code class="n">zip_ref</code><code class="p">:</code>&#13;
    <code class="n">zip_ref</code><code class="o">.</code><code class="n">extractall</code><code class="p">()</code>&#13;
 &#13;
<code class="c1"># You can use glove.6B.50d.txt (50 dimensions)</code>&#13;
<code class="c1"># or glove.6B.100d.txt (100 dimensions)</code></pre>&#13;
        <p>Each entry in the file is a word, followed by the dimensional coefficients that were learned for it. The easiest way to use this is to create a dictionary where the key is the word and the values are the embeddings. You can set up this dictionary like this:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="n">glove_embeddings</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">()</code>&#13;
<code class="n">f</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="s1">'glove.6B.50d.txt'</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">f</code><code class="p">:</code>&#13;
    <code class="n">values</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>&#13;
    <code class="n">word</code> <code class="o">=</code> <code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
    <code class="n">coefs</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">values</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">dtype</code><code class="o">=</code><code class="s1">'float32'</code><code class="p">)</code>&#13;
    <code class="n">glove_embeddings</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="o">=</code> <code class="n">coefs</code>&#13;
<code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre>&#13;
        <p>At this point, you’ll be able to look up the set of coefficients for any word simply by using it as the key. So, for example, to see the embeddings for the word <em>frog</em>, you could use this:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">glove_embeddings</code><code class="p">[</code><code class="s1">'frog'</code><code class="p">]</code></pre>&#13;
        <p>With these pretrained embeddings in hand, you can now load them into the <span class="keep-together">embeddings</span> layer in your neural architecture and use them as pretrained embeddings instead of learning them from scratch. See the following model architecture <span class="keep-together">definition.</span> If the <code>pretrained_embeddings</code> value is not null, then the weights for the embedding layer will be loaded from that. If <code>freeze_embeddings</code> is <code>True</code>, then they’ll be frozen; otherwise, they’ll be used as the starting point for learning (i.e., you’ll fine-tune the embeddings based on your corpus):</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">TextClassificationModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code> &#13;
                 <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.25</code><code class="p">,</code> <code class="n">pretrained_embeddings</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> &#13;
                 <code class="n">freeze_embeddings</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">lstm_layers</code><code class="o">=</code><code class="mi">2</code><code class="p">):</code>&#13;
        <code class="nb">super</code><code class="p">(</code><code class="n">TextClassificationModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>&#13;
 &#13;
        <code class="c1"># Initialize embedding layer</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>&#13;
 &#13;
        <code class="c1"># Load pretrained embeddings if provided</code>&#13;
        <code class="k">if</code> <code class="n">pretrained_embeddings</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">copy_</code><code class="p">(</code><code class="n">pretrained_embeddings</code><code class="p">)</code>&#13;
            <code class="k">if</code> <code class="n">freeze_embeddings</code><code class="p">:</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">False</code>&#13;
 &#13;
        <code class="c1"># LSTM layer</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">lstm</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code>&#13;
            <code class="n">input_size</code><code class="o">=</code><code class="n">embedding_dim</code><code class="p">,</code>&#13;
            <code class="n">hidden_size</code><code class="o">=</code><code class="n">hidden_dim</code><code class="p">,</code>&#13;
            <code class="n">num_layers</code><code class="o">=</code><code class="n">lstm_layers</code><code class="p">,</code>&#13;
            <code class="n">batch_first</code><code class="o">=</code><code class="kc">True</code>&#13;
        <code class="p">)</code>&#13;
 &#13;
        <code class="c1"># Global pooling</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool1d</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>&#13;
 &#13;
        <code class="c1"># Fully connected layers</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
 &#13;
        <code class="c1"># Activation functions</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code></pre>&#13;
        <p>This model shows a total of 406.817 parameters of which only 6,817 are trainable, so training will be fast!</p>&#13;
        &#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Layer</code> <code class="p">(</code><code class="nb">type</code><code class="p">:</code><code class="n">depth</code><code class="o">-</code><code class="n">idx</code><code class="p">)</code>                   <code class="n">Output</code> <code class="n">Shape</code>              <code class="n">Param</code> <code class="c1">#</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">TextClassificationModel</code>                  <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Embedding</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">1</code>                         <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">60</code><code class="p">,</code> <code class="mi">50</code><code class="p">]</code>              <code class="p">(</code><code class="mi">400</code><code class="p">,</code><code class="mi">000</code><code class="p">)</code>&#13;
<code class="err">├─</code><code class="n">Dropout</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">2</code>                           <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">60</code><code class="p">,</code> <code class="mi">50</code><code class="p">]</code>              <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">LSTM</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">3</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">60</code><code class="p">,</code> <code class="mi">16</code><code class="p">]</code>              <code class="mi">6</code><code class="p">,</code><code class="mi">528</code>&#13;
<code class="err">├─</code><code class="n">Dropout</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">4</code>                           <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">60</code><code class="p">,</code> <code class="mi">16</code><code class="p">]</code>              <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">AdaptiveAvgPool1d</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">5</code>                 <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>               <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">6</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code><code class="p">]</code>                  <code class="mi">272</code>&#13;
<code class="err">├─</code><code class="n">ReLU</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">7</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code><code class="p">]</code>                  <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Dropout</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">8</code>                           <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">16</code><code class="p">]</code>                  <code class="o">--</code>&#13;
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">9</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="mi">17</code>&#13;
<code class="err">├─</code><code class="n">Sigmoid</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">10</code>                          <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Total</code> <code class="n">params</code><code class="p">:</code> <code class="mi">406</code><code class="p">,</code><code class="mi">817</code>&#13;
<code class="n">Trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">6</code><code class="p">,</code><code class="mi">817</code>&#13;
<code class="n">Non</code><code class="o">-</code><code class="n">trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">400</code><code class="p">,</code><code class="mi">000</code>&#13;
<code class="n">Total</code> <code class="n">mult</code><code class="o">-</code><code class="n">adds</code> <code class="p">(</code><code class="n">M</code><code class="p">):</code> <code class="mf">25.34</code>&#13;
<code class="o">==========================================================================</code>&#13;
<code class="n">Input</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.02</code>&#13;
<code class="n">Forward</code><code class="o">/</code><code class="n">backward</code> <code class="k">pass</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">1.02</code>&#13;
<code class="n">Params</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">1.63</code>&#13;
<code class="n">Estimated</code> <code class="n">Total</code> <code class="n">Size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">2.66</code>&#13;
<code class="o">==========================================================================</code>&#13;
</pre>&#13;
        <p>You can now train as before, and you can see how this architecture, with the pretrained embeddings and stacked LSTMs, reduces overfitting really nicely! <a data-type="xref" href="#ch07_figure_21_1748549654878419">Figure 7-21</a> shows the Training versus Validation accuracy on the sarcasm dataset using LSTMs and pretrained GloVe embeddings, while <a data-type="xref" href="#ch07_figure_22_1748549654878440">Figure 7-22</a> shows the loss on training versus validation, where the closeness of the curves demonstrates that we’re not <span class="keep-together">overfitting.</span></p>&#13;
        <figure><div class="figure" id="ch07_figure_21_1748549654878419">&#13;
          <img src="assets/aiml_0721.png"/>&#13;
          <h6><span class="label">Figure 7-21. </span>Training versus validation accuracy on the sarcasm dataset with LSTMs and GloVe</h6>&#13;
        </div></figure>&#13;
        <figure><div class="figure" id="ch07_figure_22_1748549654878440">&#13;
          <img src="assets/aiml_0722.png"/>&#13;
          <h6><span class="label">Figure 7-22. </span>Training and validation loss on the sarcasm dataset with LSTMs and GloVe</h6>&#13;
        </div></figure>&#13;
        <p>For further analysis, you’ll want to consider your vocab size. One of the optimizations you did in the previous chapter to avoid overfitting was intended to prevent the embeddings becoming overburdened with learning low-frequency words: you avoided overfitting by using a smaller vocabulary of frequently used words. In this case, as the word embeddings have already been learned for you with GloVe, you could expand the vocabulary—but by how much?</p>&#13;
        <p>The first thing to explore is how many of the words in your corpus are actually in the GloVe set. It has 1.2 million words, but there’s no guarantee it has <em>all</em> of your words.</p>&#13;
        <p>When building the <code>word_index</code>, you can call <code>build_vocab_glove</code> with a <em>really</em> large number and it will ignore any words over the total amount. So, for example, say you call this:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">word_index</code> <code class="o">=</code> <code class="n">build_vocab_glove</code><code class="p">(</code><code class="n">training_sentences</code><code class="p">,</code> <code class="n">max_vocab_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code><code class="mi">000</code><code class="p">)</code></pre>&#13;
        <p>With the sarcasm dataset, you’ll get a vocab_size of 22,457 returned. If you like, you can then explore the GloVe embeddings to see just how many of these words are present in GloVE. Start by creating a dictionary for the embeddings and reading the GloVE file into it:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">embeddings_dict</code> <code class="o">=</code> <code class="p">{}</code>&#13;
<code class="n">embedding_dim</code> <code class="o">=</code> <code class="mi">50</code>&#13;
<code class="n">glove_file</code> <code class="o">=</code> <code class="sa">f</code><code class="s1">'glove.6B.</code><code class="si">{</code><code class="n">embedding_dim</code><code class="si">}</code><code class="s1">d.txt'</code>&#13;
 &#13;
<code class="c1"># Read GloVe embeddings</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Loading GloVe embeddings from </code><code class="si">{</code><code class="n">glove_file</code><code class="si">}</code><code class="s2">..."</code><code class="p">)</code>&#13;
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">glove_file</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">f</code><code class="p">:</code>&#13;
        <code class="n">values</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>&#13;
        <code class="n">word</code> <code class="o">=</code> <code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
        <code class="n">vector</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">values</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">dtype</code><code class="o">=</code><code class="s1">'float32'</code><code class="p">)</code>&#13;
        <code class="n">embeddings_dict</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="o">=</code> <code class="n">vector</code></pre>&#13;
        <p>Then, you can compare this with your <code>word_index</code> that you created from the entire corpus with the preceding line:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">found_words</code> <code class="o">=</code> <code class="mi">0</code>&#13;
<code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">idx</code> <code class="ow">in</code> <code class="n">word_index</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>&#13;
    <code class="k">if</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">embeddings_dict</code><code class="p">:</code>&#13;
        <code class="n">found_words</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">found_words</code><code class="p">)</code></pre>&#13;
        <p>In the case of sarcasm, 21,291 of the words were found in GloVE, which is the vast majority, so the principles you used in <a data-type="xref" href="ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">Chapter 6</a> to choose how many you should train on (i.e., picking those with sufficient frequency to have a signal) will still apply!</p>&#13;
        <p>Using this method, I chose to use a vocabulary size of 8,000 (instead of the 2,000 that was previously used to avoid overfitting) to get the results you saw just now. I then tested it with headlines from <em>The Onion</em>, the source of the sarcastic headlines in the sarcasm dataset, against other sentences, as shown here:</p>&#13;
        <pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">test_sentences</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"It Was, For, Uh, Medical Reasons, Says Doctor To </code><code class="w"/>&#13;
                   <code class="n">Boris</code> <code class="n">Johnson</code><code class="p">,</code> <code class="n">Explaining</code> <code class="n">Why</code> <code class="n">They</code> <code class="n">Had</code> <code class="n">To</code> <code class="n">Give</code> <code class="n">Him</code> <code class="n">Haircut</code><code class="s2">",</code><code class="w"/>&#13;
                  <code class="s2">"It's a beautiful sunny day"</code><code class="p">,</code>&#13;
                  <code class="s2">"I lived in Ireland, so in high school they made me </code><code class="w"/>&#13;
                   <code class="n">learn</code> <code class="n">to</code> <code class="n">speak</code> <code class="ow">and</code> <code class="n">write</code> <code class="ow">in</code> <code class="n">Gaelic</code><code class="s2">",</code><code class="w"/>&#13;
                  <code class="s2">"Census Foot Soldiers Swarm Neighborhoods, Kick Down </code><code class="w"/>&#13;
                   <code class="n">Doors</code> <code class="n">To</code> <code class="n">Tally</code> <code class="n">Household</code> <code class="n">Sizes</code><code class="s2">"]</code><code class="w"/>&#13;
 </pre>&#13;
        <p>The results for these headlines are as follows—remember that values close to 50% (0.5) are considered neutral, those close to 0 are considered nonsarcastic, and those close to 1 are considered sarcastic:</p>&#13;
        <pre data-code-language="python" data-type="programlisting"><code class="n">tensor</code><code class="p">([[</code><code class="mf">0.9316</code><code class="p">],</code>&#13;
        <code class="p">[</code><code class="mf">0.1603</code><code class="p">],</code>&#13;
        <code class="p">[</code><code class="mf">0.6959</code><code class="p">],</code>&#13;
        <code class="p">[</code><code class="mf">0.9594</code><code class="p">]],</code> <code class="n">device</code><code class="o">=</code><code class="s1">'cuda:0'</code><code class="p">)</code>&#13;
 &#13;
<code class="n">Text</code><code class="p">:</code> <code class="n">It</code> <code class="n">Was</code><code class="p">,</code> <code class="n">For</code><code class="p">,</code> <code class="n">Uh</code><code class="p">,</code> <code class="n">Medical</code> <code class="n">Reasons</code><code class="p">,</code> <code class="n">Says</code> <code class="n">Doctor</code> <code class="n">To</code> <code class="n">Boris</code> <code class="n">Johnson</code><code class="p">,</code> &#13;
      <code class="n">Explaining</code> <code class="n">Why</code> <code class="n">They</code> <code class="n">Had</code> <code class="n">To</code> <code class="n">Give</code> <code class="n">Him</code> <code class="n">Haircut</code>&#13;
<code class="n">Probability</code><code class="p">:</code> <code class="mf">0.9316</code>&#13;
<code class="n">Classification</code><code class="p">:</code> <code class="n">Sarcastic</code>&#13;
<code class="o">--------------------------------------------------------------------------</code>&#13;
 &#13;
<code class="n">Text</code><code class="p">:</code> <code class="n">It</code><code class="s1">'s a beautiful sunny day</code><code class="w"/>&#13;
<code class="n">Probability</code><code class="p">:</code> <code class="mf">0.1603</code>&#13;
<code class="n">Classification</code><code class="p">:</code> <code class="n">Not</code> <code class="n">Sarcastic</code>&#13;
<code class="o">--------------------------------------------------------------------------</code>&#13;
 &#13;
<code class="n">Text</code><code class="p">:</code> <code class="n">I</code> <code class="n">lived</code> <code class="ow">in</code> <code class="n">Ireland</code><code class="p">,</code> <code class="n">so</code> <code class="ow">in</code> <code class="n">high</code> <code class="n">school</code> <code class="n">they</code> <code class="n">made</code> <code class="n">me</code> <code class="n">learn</code> <code class="n">to</code> <code class="n">speak</code> &#13;
      <code class="ow">and</code> <code class="n">write</code> <code class="ow">in</code> <code class="n">Gaelic</code>&#13;
<code class="n">Probability</code><code class="p">:</code> <code class="mf">0.6959</code>&#13;
<code class="n">Classification</code><code class="p">:</code> <code class="n">Sarcastic</code>&#13;
<code class="o">--------------------------------------------------------------------------</code>&#13;
 &#13;
<code class="n">Text</code><code class="p">:</code> <code class="n">Census</code> <code class="n">Foot</code> <code class="n">Soldiers</code> <code class="n">Swarm</code> <code class="n">Neighborhoods</code><code class="p">,</code> <code class="n">Kick</code> <code class="n">Down</code> <code class="n">Doors</code> <code class="n">To</code> <code class="n">Tally</code> &#13;
      <code class="n">Household</code> <code class="n">Sizes</code>&#13;
<code class="n">Probability</code><code class="p">:</code> <code class="mf">0.9594</code>&#13;
<code class="n">Classification</code><code class="p">:</code> <code class="n">Sarcastic</code>&#13;
<code class="o">--------------------------------------------------------------------------</code></pre>&#13;
        <p>The first and fourth sentences, which are taken from <em>The Onion</em>, showed 93%+ likelihood of sarcasm. The statement about the weather was strongly nonsarcastic (16%), and the sentence about going to high school in Ireland was deemed to be potentially sarcastic but not with high confidence (69%).<a contenteditable="false" data-primary="" data-startref="ch7rnnall" data-type="indexterm" id="id1320"/><a contenteditable="false" data-primary="" data-startref="ch7rnnall2" data-type="indexterm" id="id1321"/><a contenteditable="false" data-primary="" data-startref="ch7pre" data-type="indexterm" id="id1322"/><a contenteditable="false" data-primary="" data-startref="ch7pre2" data-type="indexterm" id="id1323"/><a contenteditable="false" data-primary="" data-startref="ch7pre3" data-type="indexterm" id="id1324"/><a contenteditable="false" data-primary="" data-startref="ch7pre4" data-type="indexterm" id="id1325"/><a contenteditable="false" data-primary="" data-startref="ch7pre5" data-type="indexterm" id="id1326"/><a contenteditable="false" data-primary="" data-startref="ch7pre6" data-type="indexterm" id="id1327"/><a contenteditable="false" data-primary="" data-startref="ch7pre7" data-type="indexterm" id="id1328"/><a contenteditable="false" data-primary="" data-startref="ch7pre8" data-type="indexterm" id="id1329"/><a contenteditable="false" data-primary="" data-startref="ch7pre9" data-type="indexterm" id="id1330"/></p>&#13;
      </div></section>&#13;
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch07_summary_1748549654892447">&#13;
        <h1>Summary</h1>&#13;
        <p>This chapter introduced you to recurrent neural networks, which use sequence-oriented logic in their design and can help you understand the sentiment in sentences based not only on the words they contain but also on the order in which they appear. You saw how a basic RNN works, as well as how an LSTM can build on this to enable context to be preserved over the long term. These models are the precursors to the popular and famous “transformers” models used to underpin generative AI. </p>&#13;
        <p>You also used LSTMs to improve the sentiment analysis model you’ve been working on, and you then looked into overfitting issues with RNNs and techniques to improve them, including by using transfer learning from pretrained embeddings. </p>&#13;
        <p>In <a data-type="xref" href="ch08.html#ch08_using_ml_to_create_text_1748549671852453">Chapter 8</a>, you’ll use what you’ve learned so far to explore how to predict words, and from there, you’ll be able to create a model that creates text and writes poetry for you!</p>&#13;
      </div></section>&#13;
    </div></section></body></html>