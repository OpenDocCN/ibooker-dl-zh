<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Autonomous Background Coding Agents"><div class="chapter" id="ch10_autonomous_background_coding_agents_1752630045087844">
<h1><span class="label">Chapter 10. </span>Autonomous Background Coding Agents</h1>

<p><em>Autonomous background coding agents</em> are rapidly emerging as the next evolution of AI coding tools.<a contenteditable="false" data-primary="autonomous background coding agents" data-type="indexterm" id="id1075"/><a contenteditable="false" data-primary="background coding agents" data-seealso="autonomous background coding agents" data-type="indexterm" id="id1076"/>  Unlike familiar “copilot” assistants that suggest code while you type, these agents operate more like background junior developers you can dispatch to handle entire tasks asynchronously. Code is generated in an isolated environment spun up for the agent, tests can be run, and the result often comes back as a fully formed pull request for you to review.</p>

<p>In this section, I’ll explore what background coding agents are, how they work, the current landscape of tools (OpenAI Codex, Google Jules, Cursor, Devin, and more), and how they compare to traditional in-IDE assistants. I’ll also examine their capabilities, limitations, and the pragmatic changes they signal for the future of software engineering.</p>

<section data-type="sect1" data-pdf-bookmark="From Copilots to Autonomous Agents: What Are Background Coding Agents?"><div class="sect1" id="ch10_from_copilots_to_autonomous_agents_what_are_backg_1752630045087997">
<h1>From Copilots to Autonomous Agents: <span class="keep-together">What Are Background Coding Agents?</span></h1>

<p>Traditional AI coding assistants (like Cursor, GitHub Copilot, or VSCode extensions like Cline) are <em>supervised coding agents</em>—interactive helpers that respond to a developer’s prompts or inline context. <a contenteditable="false" data-primary="VSCode (Visual Studio Code)" data-type="indexterm" id="id1077"/><a contenteditable="false" data-primary="Cline" data-type="indexterm" id="id1078"/><a contenteditable="false" data-primary="Cursor IDE" data-type="indexterm" id="id1079"/><a contenteditable="false" data-primary="supervised coding agents" data-type="indexterm" id="id1080"/><a contenteditable="false" data-primary="GitHub Copilot" data-type="indexterm" id="id1081"/><a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="about" data-type="indexterm" id="id1082"/>They’re essentially autocomplete on <span class="keep-together">steroids</span>, generating suggestions in a chat or as you write, but the human developer is in the driver’s seat guiding every step.</p>

<p>In contrast, autonomous background coding agents operate with much greater independence. You give them a high-level task or goal, then “send them off” to work through the problem on their own, without constant supervision. These agents will read and modify your codebase, formulate a plan, execute code (even running tests or commands), and produce a result (often a commit or pull request)—all in an asynchronous workflow.</p>

<p>Think of the difference between a copilot and an autopilot: your copilot (much like GitHub Copilot) is always in the cockpit beside you, awaiting your input; the autopilot (background agent) can fly the plane on its own for a while. This autonomy means that background agents can tackle multistep coding tasks while you focus elsewhere.<a contenteditable="false" data-primary="async agents" data-seealso="autonomous background coding agents" data-type="indexterm" id="id1083"/> Using async agents like Codex and <a contenteditable="false" data-primary="Jules" data-type="indexterm" id="id1084"/>Jules is <a contenteditable="false" data-primary="Codex" data-type="indexterm" id="id1085"/>like expanding your cognitive bandwidth: you can fire off a task to the AI and forget about it until it’s done. Instead of a single-threaded back-and-forth with an AI, you suddenly have a multithreaded workflow: the agent works in parallel with you, much like a competent junior dev working in the background.</p>

<p>Crucially, background agents operate in isolated development environments (often cloud VMs or containers) rather than directly in your editor. <a contenteditable="false" data-primary="dependencies" data-secondary="background coding agents installing" data-type="indexterm" id="id1086"/>They typically clone your repository into a sandbox, install dependencies, and have the tools needed to build and test the project. For security, these sandboxes are restricted (with rules like “No internet access unless explicitly allowed”) and ephemeral. The agent can run compilers, tests, linters, and the like without any risk to your local machine. When the task is complete, the agent outputs the code changes (diffs) and a summary of what it did. Usually this comes through as a pull request (with code diffs, commit message, and sometimes an explanation), which you can then review and merge.</p>

<p>To sum up, a background coding agent is an AI-powered autonomous coder that understands your intent, works through an entire task in a sandbox environment by reading and writing code and testing it, and then delivers the results for you to review. It’s not just suggesting a line or two—it can handle larger-scope tasks:</p>

<ul>
	<li>
	<p>Write a new feature X across the codebase.</p>
	</li>
	<li>
	<p>Refactor module Y for efficiency.</p>
	</li>
	<li>
	<p>Upgrade this project’s dependencies.</p>
	</li>
</ul>

<p>This is a significant shift in how we might incorporate AI into development workflows, moving from assistive suggestions to delegating <em>actual implementation work</em>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="How Do Autonomous Coding Agents Work?"><div class="sect1" id="ch10_how_do_autonomous_coding_agents_work_1752630045088058">
<h1>How Do Autonomous Coding Agents Work?</h1>

<p>Under the hood, most background agents follow a similar pattern of operation: <em>plan, execute, verify,</em> and <em>report</em>. <a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="how they work" data-type="indexterm" id="ix_autobkcdhow"/>Let’s walk through these steps and their capabilities.</p>

<section class="notoc" data-type="sect2" data-pdf-bookmark="Plan"><div class="sect2" id="id119">
<h2>Plan</h2>

<p>When you give an agent a task (typically via a prompt or command describing what you want), the agent first parses the request and formulates a plan of attack.<a contenteditable="false" data-primary="planning stage, autonomous coding agents" data-type="indexterm" id="id1087"/> Some agents explicitly show you this plan before proceeding. For example, <a href="https://oreil.ly/jxDhZ">Google’s Jules</a> presents an execution plan that you can review and tweak before it starts coding, which “prevents the anxiety of wondering whether the agent understood your request correctly.” A good agent will break the task into substeps:</p>

<blockquote>
<p>Step 1: search the codebase for relevant sections; Step 2: make changes in files A, B, C; Step 3: run tests; Step 4: commit changes.</p>
</blockquote>

<p>This planning stage is key to effective autonomy: it’s the AI’s way of reasoning about <em>how</em> to accomplish your goal before diving in.</p>

<p>The agent launches a dedicated development environment for the task. Jules, for instance, “clones your codebase into a secure Google Cloud VM” and works asynchronously there. OpenAI’s Codex similarly runs each task in its own cloud sandbox, preloaded with your repository. <a contenteditable="false" data-primary="Cursor IDE" data-secondary="background coding gents" data-type="indexterm" id="id1088"/>Tools like Cursor’s background agents use a remote Ubuntu-based machine that has internet access to install packages and can be customized via Docker or snapshots. Ensuring the environment has all needed dependencies (like the correct language runtimes and build tools) is both critical and nontrivial. As I noted in a previous analysis, “Figuring out a smooth experience to spin up just the right environment for an agent is key…and the user experience to configure it is as frustrating, if not more, than it can be for CI pipelines.” Nonetheless, agents are tackling this by allowing configuration files to specify setup steps. The goal is to create a <em>dev environment in the cloud</em> that mirrors what a human developer would need to successfully run the project’s code and tests.</p>

<p>Notably, many agents disable internet access to their code after setup, so they can sandbox the run without unauthorized data exfiltration or unrestricted internet calls. Some allow controlled internet use for specific needs: for example, OpenAI recently enabled optional internet access for Codex tasks like fetching package updates or documentation.</p>
</div></section>

<section class="notoc" data-type="sect2" data-pdf-bookmark="Execute"><div class="sect2" id="id120">
<h2>Execute</h2>

<p>Next comes the main show: the agent starts writing and modifying code according to the plan.<a contenteditable="false" data-primary="execution stage, autonomous coding agents" data-type="indexterm" id="id1089"/> Armed with a large language model (or a mix of models) fine-tuned for coding, it can read multiple files, generate new code, and even create new files if needed. This is where the agent essentially acts like a programmer: locating where changes should be made, editing code, and inserting new logic.</p>

<p>One interesting observation from early runs is that agents often use brute-force text search (like the Unix <code>grep</code> command) to find relevant parts of the codebase.<a contenteditable="false" data-primary="brute-force text searches by autonomous coding agents" data-type="indexterm" id="id1090"/> For example, an agent might search for a function name or a keyword to figure out where in the repository to make changes. This seems surprisingly simplistic—shouldn’t they use fancy semantic code search or AST-based analysis? Yet, it’s effective and reliable.<a contenteditable="false" data-primary="full-text searches, coding agents defaulting to" data-type="indexterm" id="id1091"/> As <a href="https://oreil.ly/wDSkr">Birgitta Böckeler notes</a>, many coding agents default to straightforward full-text search, perhaps finding it the most broadly effective method despite more advanced techniques existing.</p>

<p>As the agent edits code, some systems provide real-time logs or status updates so you can follow along if you want.<a contenteditable="false" data-primary="Codex" data-type="indexterm" id="id1092"/><a contenteditable="false" data-primary="OpenAI's Codex" data-type="indexterm" id="id1093"/> OpenAI Codex exposes a log of the agent’s “thoughts” and commands (summarized) as it works through a task.<a contenteditable="false" data-primary="Cursor IDE" data-type="indexterm" id="id1094"/> Cursor allows you to “view their status and enter the machine the agent is running in” to observe or even intervene midtask. In practice, though, the idea is you don’t need to babysit—you can let the agent run on autopilot.</p>
</div></section>

<section class="notoc" data-type="sect2" data-pdf-bookmark="Verify"><div class="sect2" id="id121">
<h2>Verify</h2>

<p>A defining capability of these agents is that they don’t stop at writing code—they often compile the code and <em>run tests to verify</em> their changes.<a contenteditable="false" data-primary="verification" data-secondary="by autonomous coding agents" data-secondary-sortas="autonomous" data-type="indexterm" id="id1095"/> For instance, OpenAI’s Codex is designed to iteratively run tests until it receives a passing result. If an agent can run the project’s test suite (or at least a relevant subset of tests), it can catch mistakes and automatically correct them in subsequent iterations. This is huge: it moves the AI from just <em>generating</em> code to also <em>debugging</em> and <em>validating</em> its code.</p>

<p>In theory, an agent with a robust test harness can attempt a fix, see a test fail, adjust the code, and loop until tests pass—without a human in the loop. In practice, environment issues sometimes thwart this. In one case I studied, Codex wasn’t able to run the full test suite due to environment mismatches (certain tools were missing), resulting in a pull request that still had two failing tests. Had the environment been fully aligned, the agent could have fixed those trivial issues before making the PR.</p>

<p>This underscores why environment setup is so important for autonomous agents: if they can run everything a developer would (linters, tests, builds), they can self-correct many errors automatically.<a contenteditable="false" data-primary="Devin" data-type="indexterm" id="id1096"/> Agents like Devin emphasize this loop—Devin “writes code, finds bugs in the code, corrects the code, and runs its own end-to-end tests to verify it works” as a normal part of its operation. In fact, Devin will even spin up a live preview deployment of a frontend app it built so you can manually verify a feature in the browser, which is a clever extension of the verification step.</p>
</div></section>

<section class="notoc" data-type="sect2" data-pdf-bookmark="Report"><div class="sect2" id="id122">
<h2>Report</h2>

<p>Once the agent has a candidate solution (all tests have passed, or it deems the code ready), it prepares the results for you.<a contenteditable="false" data-primary="reporting by autonomous coding agents" data-type="indexterm" id="id1097"/> Depending on the platform, this might come as a PR on GitHub, a diff and explanation<a contenteditable="false" data-primary="diffs" data-type="indexterm" id="id1098"/> in chat, or files ready to merge.</p>

<p class="pagebreak-before less_space">At this point, you—the human—do a review. Here we come back to “Trust but verify”: you trust the agent to produce something useful, but you verify the changes through code review and additional testing. <a contenteditable="false" data-primary="PR (pull request) review process" data-secondary="autonomous coding agents integrating with" data-type="indexterm" id="id1099"/>Many agent systems explicitly integrate with the PR review process because it’s a familiar workflow for developers. Jules, for example, plugs into your GitHub and will open a branch and PR with its changes. OpenAI’s Codex presents the diff inside ChatGPT for you to approve or ask follow-up questions. If you find issues or have change requests, you can often feed that back to the agent for another iteration.</p>

<p>Some agents handle this via chat (Devin can take feedback from a linked Slack thread: if you point out a problem or ask for tweaks, it will “start working on a reply” to address it). Others might require a new run with an adjusted prompt or use a review comment interface. Impressively, Devin even responded to a GitHub PR comment asking <em>why</em> it made certain changes—it reacted with an “eyes” emoji to signal it saw the comment, then posted a detailed explanation of its reasoning. (The explanation turned out to be not entirely correct in that case, but the fact that it can discuss PRs says something about how interactive these agents can become.)</p>

<p>If all looks good, you merge the agent’s PR or integrate the changes. If not, you might discard it or have the agent try again. One pragmatic question teams face is what to do if an agent’s output is <em>almost</em> good but not quite. Do you spend time fixing up the last 10%–20% of an agent-generated patch, even if it was a low-priority task you <span class="keep-together">offloaded</span> to the AI? This is what I call the “sunk cost” dilemma for AI contributions. <a href="https://oreil.ly/IdJ9d">Birgitta Böckeler</a> muses that if an agent PR only partly succeeds, teams will have to decide “in which situations would [they] discard the pull request, and in which situations would they invest the time to get it the last 20% there” for a task that originally wasn’t worth much dev time. There’s no one answer—it depends on the context and value of the change—but it’s a new kind of trade-off introduced by autonomous agents.</p>

<p>In summary, background coding agents handle the end-to-end cycle of coding tasks: <em>understand → plan → code → test → deliver</em>. They essentially simulate what a diligent, methodical developer might do when assigned a task, albeit within the current limits of AI (see <a data-type="xref" href="#ch10_figure_1_1752630045078635">Figure 10-1</a>).</p>

<figure><div id="ch10_figure_1_1752630045078635" class="figure"><img src="assets/bevc_1001.png" width="569" height="886"/>
<h6><span class="label">Figure 10-1. </span>Autonomous AI agent workflow: self-directed agents plan tasks, execute solutions, verify results, and report outcomes with minimal human intervention.</h6>
</div></figure>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="How Do Background Agents Compare to In-IDE AI Assistants?"><div class="sect1" id="ch10_how_do_background_agents_compare_to_in_ide_ai_assi_1752630045088118">
<h1>How Do Background Agents Compare <span class="keep-together">to In-IDE AI Assistants?</span></h1>

<p>It’s worth drawing a clear line between the coding<a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="how they work" data-startref="ix_autobkcdhow" data-type="indexterm" id="id1100"/> AI tools we’ve<a contenteditable="false" data-primary="IDEs (integrated development environments)" data-secondary="AI assistants in, comparison to autonomous coding agents" data-type="indexterm" id="id1101"/> had <a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="comparison to in-IDE AI assistants" data-type="indexterm" id="id1102"/>for a couple years (GitHub Copilot, ChatGPT coding mode, etc.) and this new generation of autonomous agents. Both are useful, but they play different roles and have different strengths/weaknesses.</p>

<p>The most obvious difference is their <em>level of autonomy</em>. <a contenteditable="false" data-primary="VSCode (Visual Studio Code)" data-secondary="AI extensions" data-type="indexterm" id="id1103"/>In-IDE assistants<a contenteditable="false" data-primary="Copilot" data-type="indexterm" id="id1104"/> like Copilot or VSCode’s AI extensions work <em>synchronously</em> with you—they generate suggestions or answer questions when invoked, and their scope is usually limited to the immediate context (like the file or function you’re editing or a specific prompt you gave). <em>You</em> decide when to accept a suggestion, ask for another, or apply a change.</p>

<p>With background agents, once you hit “go” on a task, the agent will autonomously perform potentially hundreds of actions (file edits, runs, searches) without further confirmation. It’s operating <em>asynchronously</em>. This requires a higher degree of trust (you’re letting it change things on its own) but also frees you from micromanaging. I often describe it as the difference between having an AI <em>pair programmer</em> versus an AI <em>assistant developer</em> on the team.<a contenteditable="false" data-primary="pair programming" data-secondary="AI pair programmer versus AI assistant developer" data-type="indexterm" id="id1105"/><a contenteditable="false" data-primary="AI pair programmer versus AI assistant developer" data-type="indexterm" id="id1106"/> The pair programmer (Copilot) is with you keystroke by keystroke; the assistant dev (Codex/Jules/etc.) works in parallel on another issue.</p>

<p>The copilot style of AI tools means they excel at microtasks—writing a function, completing a line, generating a small snippet, answering a question about how to use an API. They don’t maintain a long narrative or project-wide understanding, beyond what’s in your editor’s open files or a limited window.</p>

<p>Autonomous agents operate at the <em>project level</em>. <a contenteditable="false" data-primary="project level, autonomous coding agents operating at" data-type="indexterm" id="id1107"/>They load your entire repository (or at least index it) and can make coordinated changes across multiple modules. They keep track of a multistep plan. For example, GitHub Copilot might help you write a unit test if you prompt it, but a background agent could, on its own, decide to add the corresponding implementation in one file, the test in another, and a modified a config in a third—all as part of one unified task. This makes agents far better suited for things like refactoring a cross-cutting concern (logging, error handling), performing upgrades (which often involve many files), or implementing a feature that touches backend and frontend. IDE assistants couldn’t easily handle those because they lack long-term task memory and whole-repo visibility.</p>

<p>Copilot-style assistants are <em>reactive</em>—they respond to your code or queries. <a contenteditable="false" data-primary="Copilot" data-secondary="reactive nature of Copilot-style assistants" data-type="indexterm" id="id1108"/>They don’t initiate actions. Background agents are <em>proactive</em> in the sense that once activated, they will take initiative to reach the goal.<a contenteditable="false" data-primary="Jules" data-type="indexterm" id="id1109"/> A Jules or <a contenteditable="false" data-primary="Devin" data-type="indexterm" id="id1110"/>Devin agent might decide, “I need to create a new file here” or “Let me run the tests now,” without being explicitly told at each step. They also can <em>notify you of things proactively</em>, like:</p>

<blockquote>
<p>I found another place to apply this change, so I’ll include that too.</p>
</blockquote>

<p>They behave more like an employee, who might say, “I noticed X while I was in the code, so I fixed that as well.” That said, autonomy also means they might do something you didn’t expect or necessarily want. The supervised nature of this style of tool means it will only do exactly what you accept (except maybe for subtle missuggestions you didn’t notice). So with great power (proactivity) comes the need for greater oversight.</p>

<p>A major difference is that <a contenteditable="false" data-primary="execution of code and commands by autonomous coding agents" data-type="indexterm" id="id1111"/>background agents can <em>execute code and commands</em>, whereas traditional IDE assistants usually cannot (unless you count things like ChatGPT’s Code Interpreter mode, but that’s more for data analysis, not integrated with your project’s build).</p>

<p>Agents will run your test suite, start your dev server, compile the app, maybe even deploy it. They operate in a sandbox, but it’s effectively like having an automated developer who can use the terminal. This is a game changer—it closes the loop of verify/fix. An IDE helper might generate code that looks plausible, but if it didn’t actually run it, there could be runtime issues or failing tests.</p>

<p>With an agent that runs the code, you have a higher chance the output is actually functional. <a contenteditable="false" data-primary="debugging" data-secondary="by autonomous coding agents" data-secondary-sortas="autonomous" data-type="indexterm" id="id1112"/>It also offloads the debugging step; if something fails, the agent can try to fix it immediately. The flip side is this requires the agent’s environment to be correct (as discussed earlier), and it opens the door to potential side effects. Imagine an agent running a database migration or modifying data—usually they’re in sandbox mode, so this doesn’t affect production, but be careful.</p>

<p>GitHub Copilot and tools like it live in the editor, which is great for in-the-flow coding. <a contenteditable="false" data-primary="GitHub Copilot" data-secondary="living in the editor" data-type="indexterm" id="id1113"/>Agents often<a contenteditable="false" data-primary="DevOps" data-secondary="background coding agents integrating with tools" data-type="indexterm" id="id1114"/> integrate with project management and DevOps tools, too. For example, you might create a GitHub issue and have an agent pick it up and generate a PR, or trigger an agent run from a<a contenteditable="false" data-primary="CI/CD (continuous integration and continuous deployment) pipelines" data-secondary="autonomous coding agents and" data-type="indexterm" id="id1115"/> CI pipeline for certain tasks (like autofixing lint errors on PRs). In fact, CodeGen advertises its agents’ ability to attach to issue trackers so that when an issue moves to “In Progress,” the AI agent works on it. This kind of integration is beyond what IDE tools do. It hints that AI agents could become part of the CI/CD loop—for instance, automatically attempting to fix build failures or automatically creating follow-up PRs for minor issues. That’s a different mode of collaboration: not just helping a dev write code but acting as a bot user in the team’s toolchain.</p>

<p>Using copilot-type assistants often still feels like programming, just faster—you type, they suggest, you accept, you test. Using a background agent feels more like delegation followed by review. <a contenteditable="false" data-primary="code reviews" data-secondary="code produced by autonomous coding agents" data-type="indexterm" id="id1116"/>The human effort shifts from writing code to writing a good task description and then reviewing the code produced.<a contenteditable="false" data-primary="generator versus reviewer asymmetry" data-type="indexterm" id="id1117"/> I call this <em>“</em>generator versus reviewer asymmetry”—generating a solution (or code) from scratch is hard, but reviewing and refining it is easier.<a contenteditable="false" data-primary="reviewing code, generator versus reviewer asymmetry" data-type="indexterm" id="id1118"/> Async agents capitalize on this: they handle the bulk generation, leaving you with the (typically faster) job of vetting and tweaking. This can be a productivity boon, but it also means as an engineer you need to sharpen your code review and verification skills.</p>

<p>Code review has always been important, but now it’s not just for other human colleagues’ code—it’s for AI-generated code as well, which might have different patterns of mistakes. My mantra is that you should treat agent-produced code as if it were written by a slightly overeager junior developer: assume good intentions and decent competence, but verify everything and don’t hesitate to request changes or reject if it’s not up to standards.</p>

<p>In practice, I find that I use copilot-style tools and background agents <em>together</em>. For instance, I might use Copilot or Cursor’s inline suggestions while I’m actively coding a complex piece of logic, because I want tight control over that logic. Meanwhile, I might delegate a peripheral but time-consuming task (like updating all our API client libraries for new endpoints) to a background agent to handle in parallel. They fill different niches. One doesn’t necessarily replace the other. In fact, I foresee IDEs offering a unified experience: a palette of options from “Complete this line” to “Generate a function” to “Hey, AI, please implement this entire ticket for me.” You’d choose the tool depending on the scope.<a contenteditable="false" data-primary="background coding agents" data-startref="ix_bckgrcd" data-type="indexterm" id="id1119"/></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Combining Multiple AI Models to Maximize Strengths"><div class="sect1" id="ch10_combining_multiple_ai_models_to_maximize_strengths_1752630045088174">
<h1>Combining Multiple AI Models to Maximize Strengths</h1>

<p>So far, I’ve often referred to “the AI” as if it’s one monolithic assistant. In reality, there are many AI models, each with different strengths. <a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="combining multiple models to maximize strengths" data-type="indexterm" id="ix_AImods"/><a contenteditable="false" data-primary="models (AI)" data-secondary="combining multiple models to maximize strengths" data-type="indexterm" id="ix_modcmb"/>Some are great at natural language understanding, others excel at generating code, and some might be specialized in certain domains (like a math problem solver or a UI generator). An advanced practitioner of vibe coding can orchestrate multiple AIs together, using each for what it’s best at. This is like having a team of specialists rather than a single generalist.</p>

<p>Consider a future workflow where you have:</p>

<ul>
	<li>
	<p>A CodeGen AI highly trained on programming that can produce code and fix code efficiently</p>
	</li>
	<li>
	<p>A TestGen AI, specialized in generating test cases and finding edge cases</p>
	</li>
	<li>
	<p>A Doc AI that writes clear documentation and explanations</p>
	</li>
	<li>
	<p>A Design AI that’s skilled at generating UI layouts or graphics</p>
	</li>
	<li>
	<p>An Optimization AI focused on performance tuning and perhaps even aware of low-level details</p>
	</li>
</ul>

<p>You can pipe your task through several of these AIs. For example, you ask CodeGen AI to write an implementation. Immediately, you feed that output to TestGen AI to generate tests for it (or to critique it). Then feed both code and tests to Doc AI to produce documentation or a usage guide. If the code involves user interface, maybe Design AI is used earlier to propose the layout structure that CodeGen AI then implements. By chaining them, you leverage each model’s domain expertise. This is analogous to a software pipeline or assembly line, but instead of different human roles, it’s different AI roles.</p>

<p>Even among similar models, combining them can improve reliability. If you have two code-generation models from different providers or of different architectures, you can have them both attempt the solution and then compare or test both outputs. If one model’s output passes all tests and the other doesn’t, you pick the passing one. If both pass but have different approaches, you might manually choose the more readable one. If one fails, you can even show the failing one the successful code as a hint to learn from. This kind of AI cross-talk can reduce errors since it’s less likely that two different models will make the exact same mistake. It’s like getting a second opinion. You can already find research and tools that use one AI to check another’s reasoning⁠—for instance, one generates an answer and another judges it.</p>

<section class="pagebreak-before" data-type="sect2" data-pdf-bookmark="Differentiate Models by Task Type"><div class="sect2" id="ch10_differentiate_models_by_task_type_1752630045088227">
<h2 class="less_space">Differentiate Models by Task Type</h2>

<p>Use the right tool for the job.<a contenteditable="false" data-primary="models (AI)" data-secondary="combining multiple models to maximize strengths" data-tertiary="differentiating models by task type" data-type="indexterm" id="id1120"/> Large language models (LLMs) are good generalists, but sometimes smaller, specialized models or tools do better.<a contenteditable="false" data-primary="LLMs (large language models)" data-type="indexterm" id="id1121"/> For example, for arithmetic or certain algorithms, a deterministic tool (or an AI that’s more constrained) might be better. Some advanced dev setups use symbolic solvers or older rule-based AI for specific subtasks and LLMs for others. As an advanced vibe coder, you might maintain a toolbox: when you need regex, you call a regex-specific generator; when you need a commit message, maybe a model fine-tuned for summarization is used. The beauty is these can be integrated via simple scripts or prompt wrappers. For instance, you could have a local script like <code>ai_regex_generator</code> that internally prompts an AI but with some pre- and postprocessing to ensure the output is a valid regex, and maybe tests it on provided examples.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Use an Orchestration System"><div class="sect2" id="ch10_use_an_orchestration_system_1752630045088275">
<h2>Use an Orchestration System</h2>

<p>If you find yourself frequently combining models, you <a contenteditable="false" data-primary="models (AI)" data-secondary="combining multiple models to maximize strengths" data-tertiary="using an orchestration system" data-type="indexterm" id="id1122"/>might use <a contenteditable="false" data-primary="orchestration system for multiple AI models" data-type="indexterm" id="id1123"/>or build an orchestration system, an emerging category of frameworks often referred to as <em>AI orchestration</em> or <em>agents</em>. These systems allow you to define a flow; for example:</p>

<blockquote>
<p>Step 1: Use Model A to interpret user request.</p>

<p>Step 2: If request is about data analysis, use Model B to generate SQL; if about text, use Model C…</p>

<p>Step 3: Feed the result to Model D to explain it.</p>
</blockquote>

<p>This is more relevant if you’re building an app or service powered by multiple AI steps. But even in personal dev, you can script a multistep approach. For example, one custom CLI tool, <code>ai_dev_assist</code>, takes a prompt and behind the scenes uses an AI to classify the prompt into categories like <code>code</code>, <code>design</code>, <code>test</code>, and <code>optimize</code>. Based on the category, it forwards the prompt to the appropriate specialist AI. When it receives the result, it can optionally pipe the result into another AI for review or improvement.</p>

<p>This kind of meta-AI coordinating other AIs sounds complex, but an advanced user can set it up with current technology. It will likely get easier as we begin to see dedicated support in IDEs or cloud platforms.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Human-AI Hybrid Teams"><div class="sect2" id="ch10_human_ai_hybrid_teams_1752630045088325">
<h2>Human-AI Hybrid Teams</h2>

<p>While on the subject of multiple intelligences, let’s not forget <em>human</em> collaborators. An advanced <a contenteditable="false" data-primary="human-AI hybrid teams" data-type="indexterm" id="id1124"/><a contenteditable="false" data-primary="models (AI)" data-secondary="combining multiple models to maximize strengths" data-tertiary="human-AI hybrid teams" data-type="indexterm" id="id1125"/>vibe coder also knows when to involve fellow human developers in the loop. For example, you might use AI to generate two or three different design prototypes for a feature, then bring those to your team’s UX designer for feedback. Which one aligns with our brand? Which feels intuitive? If an AI writes a complex piece of code, you might do a code review session with a colleague focusing on that piece, acknowledging that “an AI helped write this, so I want another pair of human eyes on it too.” In a sense, the “multiple model” approach can include humans as just highly advanced models—each entity (human or AI) has unique strengths. The future of development might often be human + AI pair programming or even team programming where some “team members” are AI.</p>

<p>Imagine building a small web application through vibe coding.<a contenteditable="false" data-primary="web applications, building with AI" data-secondary="multimodel approach" data-type="indexterm" id="id1126"/> Your workflow might look like this:</p>

<ol>
	<li>
	<p>You use a UI Layout AI to generate the HTML/CSS for your page given a description (specialized in frontend).<a contenteditable="false" data-primary="HTML/CSS" data-secondary="UI Layout AI generating" data-type="indexterm" id="id1127"/></p>
	</li>
	<li>
	<p>You use a Content AI to generate some placeholder text or images needed (like marketing text, maybe using a model geared for copywriting).</p>
	</li>
	<li>
	<p>You then use your main Code AI to generate the interactive functionality in JavaScript, feeding it the HTML so it knows which element IDs to hook into.</p>
	</li>
	<li>
	<p>You then ask a Testing AI to generate Selenium or Playwright tests for the interface interactions.</p>
	</li>
	<li>
	<p>Finally, you use a Security AI to scan the code for common vulnerabilities. This could be a model or simply a static-analysis tool augmented with AI.</p>
	</li>
</ol>

<p>This multimodel approach covers frontend, backend (if there is one), content, testing, and security in one integrated process. Each AI handled its portion and you, as the orchestrator, ensured they all align.</p>

<p>While today you might have to manually copy outputs from one tool to another or use some glue scripts, tomorrow’s IDEs might let you configure this pipeline so it feels seamless. The key takeaway is: <em>don’t rely on just one AI model if you have access to several</em>. Use the best one for each job and make them work together. It leads to better outcomes and also reduces single-point failure—if one model isn’t good at something, another might cover that weakness.</p>

<p>Combining AI models is an advanced move, but it’s a logical extension of specialization, a principle well known in software engineering (think microservices, each service doing one thing well). Here, each AI service does one thing well. As a vibe coder, your role expands to AI conductor, not just AI prompter. It requires a bit more setup and thought, but the payoff is a symphony of AI collaborators each contributing to a high-quality end product.</p>

<p>Now that you know how they work, let’s meet some of the leading examples and see how they stack up.<a contenteditable="false" data-primary="AI (artificial intelligence)" data-secondary="combining multiple models to maximize strengths" data-startref="ix_AImods" data-type="indexterm" id="id1128"/><a contenteditable="false" data-primary="models (AI)" data-secondary="combining multiple models to maximize strengths" data-startref="ix_modcmb" data-type="indexterm" id="id1129"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Major Players in Autonomous Coding Agents"><div class="sect1" id="ch10_major_players_in_autonomous_coding_agents_1752630045088375">
<h1>Major Players in Autonomous Coding Agents</h1>

<p>As I write this in 2025, the autonomous<a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="major players in" data-type="indexterm" id="ix_autobkcdmjr"/> coding agent landscape has rapidly evolved over the past year, with distinct approaches emerging across different platforms. These tools represent a shift from passively completing code to acting as active development partners that can execute complex tasks independently.</p>

<dl>
	<dt>Cloud-based command-line agents: OpenAI Codex</dt>
	<dd>
	<p><a href="https://oreil.ly/Ml-NU">OpenAI’s Codex</a> exemplifies the cloud-based agent approach, operating through ChatGPT’s interface or an open source CLI.<a contenteditable="false" data-primary="cloud-based command-line agents" data-type="indexterm" id="id1130"/><a contenteditable="false" data-primary="Codex" data-type="indexterm" id="id1131"/><a contenteditable="false" data-primary="OpenAI's Codex" data-type="indexterm" id="id1132"/> It spins up isolated sandboxes to execute coding tasks in parallel, handling everything from React upgrades to unit test creation. What distinguishes Codex is its reinforcement-learning training on real coding tasks, enabling it to follow best practices, like running tests iteratively until they pass. While results can vary between runs, Codex typically converges on working solutions for well-bounded tasks. Its strength lies in actual code execution within CI-like environments, representing the first wave of agents that truly “pair” with development pipelines.</p>
	</dd>
	<dt>Workflow-integrated agents: Google Jules</dt>
	<dd>
	<p><a href="https://jules.google">Google Jules</a> takes a different approach by deeply integrating with GitHub workflows.<a contenteditable="false" data-primary="workflow-integrated agents" data-type="indexterm" id="id1133"/><a contenteditable="false" data-primary="Google Jules" data-seealso="Jules" data-type="indexterm" id="id1134"/><a contenteditable="false" data-primary="Jules" data-type="indexterm" id="id1135"/> Running on Google Cloud VMs with full repository clones, Jules emphasizes visible, structured planning—presenting its reasoning and allowing plan modifications before execution. This “plan, then execute” philosophy, combined with real-time feedback capabilities, positions Jules as a supervised assistant rather than a black-box automation. Its GitHub-native design means it operates directly where teams work, creating branches and PRs without context switching. The agent even experiments with novel features like audio changelogs, pointing toward more accessible code review processes.</p>
	</dd>
	<dt>IDE-integrated agents: Cursor</dt>
	<dd>
	<p><a href="https://oreil.ly/V-Pci">Cursor’s background agents</a> represent the IDE-centric approach, launched directly from the editor but executing on remote machines.<a contenteditable="false" data-primary="Cursor IDE" data-secondary="background coding agents" data-type="indexterm" id="id1136"/><a contenteditable="false" data-primary="IDEs (integrated development environments)" data-secondary="autonomous background agents integrated with" data-type="indexterm" id="id1137"/> This hybrid model lets developers orchestrate multiple AI workers from their command center while maintaining local control. Cursor provisions Ubuntu instances with customizable environments (via <em>environment.json</em> or Dockerfiles), giving agents full internet access and package installation capabilities. The key innovation is seamless IDE integration: developers can monitor agent progress, intervene when needed, and immediately access changes locally when complete. This approach blurs the line between local AI assistance and cloud execution power.</p>
	</dd>
	<dt>Team-integrated agents: Devin</dt>
	<dd>
	<p><a href="https://devin.ai">Devin</a> positions itself as an “AI teammate” rather than just a tool, integrating with Slack, GitHub, and issue trackers like Jira. <a contenteditable="false" data-primary="Devin" data-secondary="as AI teammate" data-seealso="AI" data-type="indexterm" id="id1138"/><a contenteditable="false" data-primary="team-integrated autonomous coding agents" data-type="indexterm" id="id1139"/>Built by <span class="keep-together">Cognition</span> Labs, it uses custom AI models tuned for long-term reasoning and multistep execution. Devin excels at parallel execution of small maintenance tasks like bugfixes, test additions, and linter cleanups that often get deprioritized. Its collaborative design includes status updates, clarification requests, and even automatic preview deployments. While it handles straightforward tasks well, complex issues can still require significant human intervention, highlighting the current boundaries of autonomous coding.</p>
	</dd>
</dl>

<p>The field is expanding rapidly, with both established players and startups racing to define the category. Microsoft has hinted at “Copilot++,” moving beyond inline suggestions to agent capabilities. Enterprises are being courted by startups like CodeGen (which uses Anthropic’s Claude) promising “SWEs that never sleep.” Meanwhile, open source projects and academic research continue pushing boundaries, exploring how to make code generation more reliable and contextual.</p>

<p>This proliferation suggests that we’re witnessing the birth of a new development paradigm where individual developers orchestrate multiple AI agents, each specialized for different aspects of the software lifecycle. The key differentiators emerging are:</p>

<ul>
	<li>
	<p>Execution environment (local versus cloud)</p>
	</li>
	<li>
	<p>Integration depth (IDE versus workflow tools)</p>
	</li>
	<li>
	<p>Autonomy level (supervised versus independent)</p>
	</li>
	<li>
	<p>Target use cases (maintenance versus feature development)</p>
	</li>
</ul>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Challenges and Limitations"><div class="sect1" id="ch10_challenges_and_limitations_1752630045088425">
<h1>Challenges and Limitations</h1>

<p>While <a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="major players in" data-startref="ix_autobkcdmjr" data-type="indexterm" id="id1140"/>autonomous coding agents inherit the<a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="challenges and limitations" data-type="indexterm" id="ix_autobkcdchal"/> foundational challenges of AI-assisted development, as discussed throughout this book—particularly the 70% problem, explored in <a data-type="xref" href="ch03.html#ch03_the_70_problem_ai_assisted_workflows_that_actual_1752630043200933">Chapter 3</a>—their autonomous nature introduces distinct complications that warrant separate examination:</p>

<dl>
	<dt>The compounding effect of sequential decisions</dt>
	<dd>
	<p>Unlike interactive AI assistance where humans intervene at each step, autonomous agents make chains of decisions that can compound errors in unique ways. When an agent misinterprets the initial requirements, it doesn’t just generate one flawed function: it builds an <em>entire implementation architecture</em> on that misunderstanding. Each subsequent decision reinforces the original error, creating what I call “coherent incorrectness”: code that’s internally consistent but fundamentally misaligned with actual needs.</p>
	</dd>
	<dd>
	<p>This sequential decision making particularly challenges agents that tackle multifile changes. An agent implementing a new feature might correctly modify the backend API but then propagate incorrect assumptions through the frontend, database schema, and test suites. By the time you review the complete pull request, untangling these interconnected mistakes tends to require more effort than the interactive, incremental corrections that are possible with traditional AI assistance.</p>
	</dd>
	<dt>Environmental brittleness at scale</dt>
	<dd>
	<p>While <a data-type="xref" href="ch08.html#ch08_security_maintainability_and_reliability_1752630044621528">Chapter 8</a> discusses general environment configuration challenges, autonomous agents face unique complications from their sandbox execution model. Each agent run requires spinning up an isolated environment that <em>precisely</em> mirrors your development setup—a challenge that scales poorly. When you’re running multiple agents concurrently, even slight variations in the environment can lead to dramatically different outcomes.</p>
	</dd>
	<dd>
	<p>Consider a scenario where five agents work on different features simultaneously. Agent A might have a slightly older Node version in its container, Agent B might lack a specific system library, and Agent C might have different time zone settings. These variations, invisible during execution, surface as subtle bugs that only appear when you begin integrating their work. This “environmental drift” between agent sandboxes represents a new class of integration challenge that is absent from single-developer workflows.</p>
	</dd>
	<dt>The async coordination paradox</dt>
	<dd>
	<p>Autonomous agents promise parallel development, but this introduces coordination challenges that are quite distinct from human team dynamics. When multiple agents modify overlapping code sections, they lack the implicit communication channels humans use—there’s no quick Slack message asking, “Are you touching the auth module?” or informal awareness of what colleagues are working on.</p>
	</dd>
	<dd>
	<p>This creates<a contenteditable="false" data-primary="async agents" data-secondary="coordination paradox" data-type="indexterm" id="id1141"/> what I term the <em>async coordination paradox</em>: the more agents you run in parallel to increase productivity, the more complex integrating them becomes. Unlike human developers, who naturally coordinate through standups and informal communication, agents operate in isolation. You might discover that Agent A has refactored a utility function, while Agent B was busy adding new calls to the old version, creating conflicts that wouldn’t occur if agents had human developers’ natural awareness of each other’s work.</p>
	</dd>
	<dt>The review bottleneck—amplified</dt>
	<dd>
	<p>While code review <a contenteditable="false" data-primary="code reviews" data-secondary="bottleneck amplified by autonomous coding agents" data-type="indexterm" id="id1142"/>remains essential for all AI-generated code (as discussed in previous chapters), autonomous agents amplify this challenge through sheer volume and timing. Unlike interactive AI assistance, where code arrives incrementally as you work, agent-generated PRs appear as complete implementations—often as multiple PRs arriving simultaneously after overnight runs.</p>
	</dd>
	<dd>
	<p class="pagebreak-before less_space">This creates a kind of cognitive overload that’s distinct from the kind you get when reviewing human PRs. With human contributions, you can often rely on commit messages and PR descriptions to reflect a coder’s actual thought processes.<a contenteditable="false" data-primary="PRs (pull requests)" data-secondary="autonomous coding agent PRs" data-type="indexterm" id="id1143"/> Agent PRs, however, require you to reverse-engineer the agent’s “reasoning” from the code itself. When five agents each deliver PRs of 500 lines or more on Monday morning, the review burden shifts from being a collaborative quality check to something more like an archaeological expedition.</p>
	</dd>
	<dt>Delegating to agents requires trust</dt>
	<dd>
	<p>Perhaps most significantly, autonomous agents challenge our trust models in ways interactive AI tools don’t. When you delegate a task to an agent and walk away, you’re making an implicit bet about acceptable risk. This differs fundamentally from supervised AI assistance, where you maintain moment-by-moment control.</p>
	</dd>
	<dd>
	<p>Consider agentic technologies’ security implications. Autonomous agents with repository write access and execution capabilities present unique attack surfaces. A compromised or misdirected agent doesn’t just <em>suggest</em> bad code—it actively <em>commits</em> it and potentially even <em>deploys</em> it. Our sandboxing and access controls for agents must be correspondingly more sophisticated than for suggestion-based tools (covered in <a data-type="xref" href="ch08.html#ch08_security_maintainability_and_reliability_1752630044621528">Chapter 8</a>).</p>
	</dd>
	<dt>Emerging organizational challenges</dt>
	<dd>
	<p>As teams scale up their agent usage, new organizational patterns are emerging that don’t exist with traditional AI assistance.<a contenteditable="false" data-primary="organizational challenges with autonomous coding agents" data-type="indexterm" id="id1144"/><a contenteditable="false" data-primary="ownership of code" data-secondary="code generated by autonomous coding agents" data-type="indexterm" id="id1145"/> Who “owns” agent-generated code when the requesting developer is out sick? How do you track agent resource usage across teams? What happens when an agent’s monthlong refactoring project conflicts with urgent feature development?</p>
	</dd>
	<dd>
	<p>These aren’t technical limitations but organizational challenges, and they’re unique to autonomous systems. They require new roles (agent coordinators?), new processes (agent impact assessments?), and new tools (agent fleet management?) that extend beyond the individual developer considerations this book has addressed in earlier chapters.</p>
	</dd>
</dl>

<p>The autonomous nature of these agents—their ability to work independently, make sequential decisions, and operate at scale—transforms them from productivity tools into something approaching team members. This shift demands not just the technical practices discussed throughout this book but entirely new frameworks for coordination, trust, and integration that we’re only beginning to understand.<a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="challenges and limitations" data-startref="ix_autobkcdchal" data-type="indexterm" id="id1146"/></p>
</div></section>

<section class="pagebreak-before" data-type="sect1" data-pdf-bookmark="Best Practices for Using AI Coding Agents Effectively"><div class="sect1" id="ch10_best_practices_for_using_ai_coding_agents_effectiv_1752630045088479">
<h1 class="less_space">Best Practices for Using AI Coding Agents Effectively</h1>

<p>While many general AI development practices apply to<a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="best practices for effective use" data-type="indexterm" id="ix_autobkcdbstprc"/> autonomous coding agents, certain aspects of agent-based development require specific consideration. Based on collective experience with tools like Codex, Jules, Devin, and Cursor’s background agents, these practices address the unique challenges of delegating entire development tasks to AI systems operating independently.</p>

<section data-type="sect2" data-pdf-bookmark="Strategically Select the Tasks Autonomous Agents Are Going to Implement"><div class="sect2" id="ch10_strategically_select_the_tasks_autonomous_agents_a_1752630045088529">
<h2>Strategically Select the Tasks Autonomous <span class="keep-together">Agents Are Going to Implement</span></h2>

<p>The fundamental difference between AI assistants and autonomous agents lies in their scope and independence.<a contenteditable="false" data-primary="tasks for autonomous coding agents, selecting strategically" data-type="indexterm" id="id1147"/> Agents excel at well-defined, encapsulated tasks with clear success criteria—particularly those involving parallel execution of many small tasks. Ideal agent assignments include comprehensive test coverage improvements, systematic dependency updates, bulk refactoring operations, and standardized feature implementations across multiple components.</p>

<p>Consider the difference between asking an AI assistant to help write a single test versus tasking an agent to achieve 80% test coverage across an entire module. The agent can methodically work through each untested function, generate appropriate test cases, run them to verify correctness, and iterate until the coverage target is met. This type of systematic, measurable work is the sweet spot for autonomous agents.</p>

<p>Conversely, tasks that require making significant architectural decisions, interpreting complex stakeholder requirements, or designing novel algorithms remain better suited to human-led development with AI assistance. The key lies in recognizing which aspects of a larger task can be effectively delegated to agents and which require human judgment and creativity.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Leverage Agent-Specific Planning and Oversight Features"><div class="sect2" id="ch10_leverage_agent_specific_planning_and_oversight_fea_1752630045088580">
<h2>Leverage Agent-Specific Planning and Oversight Features</h2>

<p>Modern autonomous agents  distinguish <a contenteditable="false" data-primary="planning and oversight features, agent-specific, leveraging" data-type="indexterm" id="id1148"/>themselves through sophisticated planning and execution transparency features that demand active engagement. When Jules presents its execution plan before beginning work or when Cursor displays real-time logs of agent activity, these represent critical intervention points that are unique to agent-based development.</p>

<p>The <em>planning phase</em> serves as your primary quality gate. Review proposed plans not just for correctness but for efficiency and alignment with your codebase conventions. If Jules plans to update a Next.js application but omits critical webpack configuration changes, catching this during planning prevents extensive rework later on. This proactive review differs fundamentally from reactive code review and represents a new skill in the developer toolkit.</p>

<p><em>Runtime monitoring</em> provides another layer of agent-specific oversight. While you need not watch every operation, periodic checks can prevent agents from pursuing inefficient solutions or making unnecessarily broad changes. Cursor’s ability to “enter” the agent’s environment midtask exemplifies how modern tools support intervention without completely abandoning the autonomous workflow. To maximize efficiency, you’ll need to learn when to intervene and when to let the agent self-correct.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Manage Concurrent Agent Operations"><div class="sect2" id="ch10_manage_concurrent_agent_operations_1752630045088628">
<h2>Manage Concurrent Agent Operations</h2>

<p>Unlike traditional <a contenteditable="false" data-primary="concurrent agent operations, managing" data-type="indexterm" id="id1149"/>development, where a single developer works on one task at a time, agents enable true parallel development. This capability requires new coordination strategies. When running multiple agents simultaneously—perhaps one updating dependencies while another adds logging infrastructure—you must consider the potential conflicts and dependencies between their work.</p>

<p>Establish clear boundaries for each agent’s scope to minimize merge conflicts. Assign agents to different modules or layers of the application when possible. Consider the order of integration: an agent that is adding new features might need to wait for another agent’s infrastructure improvements to complete. This orchestration resembles managing a distributed team more than it does traditional solo development.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Evolve Your Team Practices to Integrate Agents"><div class="sect2" id="ch10_evolve_your_team_practices_to_integrate_agents_1752630045088675">
<h2>Evolve Your Team Practices to Integrate Agents</h2>

<p>The introduction of autonomous agents fundamentally alters team dynamics and review processes. <a contenteditable="false" data-primary="team practices, evolving to integrate autonomous coding agents" data-type="indexterm" id="id1150"/>Unlike reviewing a colleague’s carefully crafted PR, agent-generated PRs may contain technically correct but stylistically inconsistent code. Teams must develop new review practices that account for this difference.</p>

<p>Consider establishing agent-specific review checklists that emphasize not just correctness but also alignment with team conventions and architectural patterns. Document common quirks you spot as you work with the agent: perhaps your chosen agent consistently uses certain antipatterns or misses specific optimization opportunities. This institutional knowledge helps reviewers quickly identify and address recurring issues.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Build Feedback Loops with Autonomous Systems"><div class="sect2" id="ch10_build_feedback_loops_with_autonomous_systems_1752630045088722">
<h2>Build Feedback Loops with Autonomous Systems</h2>

<p>Perhaps most importantly, autonomous agents enable a new form of iterative development in which the feedback loop<a contenteditable="false" data-primary="feedback" data-secondary="feedback loops with autonomous systems" data-type="indexterm" id="id1151"/> extends beyond mere code review. When an agent’s pull request needs refinement, you can often send it back and ask for another iteration with specific guidance. This differs from traditional development, where sending work back to a human colleague carries social and time costs.</p>

<p>Work to develop prompting patterns that work well with your chosen agents. When you find successful prompt formulations that consistently yield high-quality results, document them. Create templates for common task types that include all necessary context and constraints. This is a kind of prompt engineering specifically for agents that considers their planning, execution, and revision cycles, and it represents a distinct skill from general AI interaction.</p>

<p>The goal remains unchanged: delivering high-quality software efficiently. Autonomous agents simply provide a new tool for achieving this goal, one you should integrate into your existing practices thoughtfully rather than replacing established methods wholesale. By understanding these agents and leveraging their unique capabilities while maintaining rigorous quality standards, teams can realize significant productivity gains without sacrificing code quality or architectural integrity.<a contenteditable="false" data-primary="autonomous background coding agents" data-secondary="best practices for effective use" data-startref="ix_autobkcdbstprc" data-type="indexterm" id="id1152"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Summary and Next Steps"><div class="sect1" id="ch10_summary_and_next_steps_1752630045088770">
<h1>Summary and Next Steps</h1>

<p>To wrap up, I’ll echo a sentiment from <a data-type="xref" href="ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362">Chapter 4</a>: AI won’t replace developers, but developers who can use AI effectively may well replace those who can’t. The advent of autonomous coding agents is a leap in that direction—those who learn to harness these “headless colleagues” will be able to do more in less time. It’s an exciting time to be a software engineer, as long as we adapt and continue to hold our work to high standards. The tools may be changing, but the goals remain: build reliable, efficient, and innovative software. With AI agents at our side (or in the background), we have new ways to reach those goals—and perhaps get a good night’s sleep while the bots burn the midnight oil.</p>

<p>Next, the final chapter of this book takes a broader look at the future of AI in coding, including the future of agentic AI.</p>
</div></section>
</div></section></div></div></body></html>