["```py\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n)\nfrom transformers import BitsAndBytesConfig\nimport torch\nfrom datasets import load_dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"ai21labs/Jamba-v0.1\", device_map=\"auto\"\n)\n\ndataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n```", "```py\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    learning_rate=2e-3,\n)\nlora_config = LoraConfig(\n    r=8,\n    target_modules=[\"embed_tokens\", \"x_proj\", \"in_proj\", \"out_proj\"],\n    task_type=\"CAUSAL_LM\",\n    bias=\"none\",\n)\n```", "```py\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    peft_config=lora_config,\n    train_dataset=dataset,\n    dataset_text_field=\"quote\",\n)\n\ntrainer.train()\n\ntokenizer.save_pretrained(\"./JAMBA/\")\nmodel.save_pretrained(\"./JAMBA/\")\n\ndel model\n```", "```py\nquantization_config = BitsAndBytesConfig(\n    load_in_8bit=True, llm_int8_skip_modules=[\"mamba\"]\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"ai21labs/Jamba-v0.1\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n    quantization_config=quantization_config,\n)\ninput_ids = tokenizer(\n    \"In the recent Super Bowl LVIII,\", return_tensors=\"pt\"\n).to(model.device)[\"input_ids\"]\n\noutputs = model.generate(input_ids, max_new_tokens=216)\n\nprint(tokenizer.batch_decode(outputs))\n```", "```py\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoModelForSpeechSeq2Seq,\n    AutoProcessor,\n)\nimport torch\nfrom datasets import load_dataset\n\nfrom time import perf_counter\nfrom tqdm import tqdm\n\nfrom evaluate import load\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nattention = \"sdpa\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\nassistant_model_id = \"distil-whisper/distil-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=False,\n    use_safetensors=True,\n    attn_implementation=attention,\n    torch_dtype=torch_dtype,\n).to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\nassistant_model = AutoModelForCausalLM.from_pretrained(\n    assistant_model_id,\n    low_cpu_mem_usage=False,\n    use_safetensors=True,\n    attn_implementation=attention,\n    torch_dtype=torch_dtype,\n).to(device)\n\ndataset = load_dataset(\n    \"hf-internal-testing/librispeech_asr_dummy\",\n    \"clean\",\n    split=\"validation\",\n    trust_remote_code=True,\n)\nwer = load(\"wer\")\n\ngenerate_kwargs_1 = {\n    \"language\": \"en\",\n    \"task\": \"transcribe\",\n}\ngenerate_kwargs_2 = {\n    \"language\": \"en\",\n    \"task\": \"transcribe\",\n    \"assistant_model\": assistant_model,\n}\n\nspec_decoding = False\nfor i, generate_kwargs in enumerate([generate_kwargs_1, generate_kwargs_2]):\n    all_time = 0\n    predictions = []\n    references = []\n    for sample in tqdm(dataset):\n        audio = sample[\"audio\"]\n        inputs = processor(\n            audio[\"array\"],\n            sampling_rate=audio[\"sampling_rate\"],\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(device=device, dtype=torch_dtype)\n        start_time = perf_counter()\n        output = model.generate(\n            **inputs,\n            **generate_kwargs,\n        )\n        gen_time = perf_counter() - start_time\n        all_time += gen_time\n        predictions.append(\n            processor.batch_decode(\n                output, skip_special_tokens=True, normalize=True\n            )[0]\n        )\n        references.append(processor.tokenizer.normalize(sample[\"text\"]))\n    score = wer.compute(predictions=predictions, references=references)\n    if i > 0:\n        spec_decoding = True\n    print(f\"Speculative Decoding: {spec_decoding}\")\n    print(f\"Time: {all_time}\")\n    print(f\"Word Error Rate: {score}\")\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import BitsAndBytesConfig\nimport torch\nimport dspy\nfrom dspy.datasets.gsm8k import GSM8K, gsm8k_metric\nfrom dsp.modules.lm import LM\nfrom dspy.evaluate import Evaluate\nfrom dspy.teleprompt import BootstrapFewShot\n\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    device_map=\"auto\", \n    quantization_config=quantization_config,\n    attn_implementation=\"sdpa\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True,)\n\ngms8k = GSM8K()\ngsm8k_trainset, gsm8k_devset = gms8k.train[:30], gms8k.dev[:100]\n```", "```py\ndef openai_to_hf(**kwargs):\n    hf_kwargs = {}\n    for k, v in kwargs.items():\n        if k == \"n\":\n            hf_kwargs[\"num_return_sequences\"] = v\n        elif k == \"frequency_penalty\":\n            hf_kwargs[\"repetition_penalty\"] = 1.0 - v\n        elif k == \"presence_penalty\":\n            hf_kwargs[\"diversity_penalty\"] = v\n        elif k == \"max_tokens\":\n            hf_kwargs[\"max_new_tokens\"] = v\n        elif k == \"model\":\n            pass\n        else:\n            hf_kwargs[k] = v\n\n    return hf_kwargs\n\nclass HFModel(LM):\n    def __init__(\n        self,\n        model: AutoModelForCausalLM,\n        tokenizer: AutoTokenizer,\n        **kwargs\n    ):\n        \"\"\"wrapper for Hugging Face models\n        Args:\n            model (AutoModelForCausalLM): HF model identifier to load and use\n            tokenizer: AutoTokenizer\n        \"\"\"\n        super().__init__(model)\n        self.model = model\n        self.tokenizer = tokenizer\n        self.drop_prompt_from_output = True\n        self.history = []\n        self.is_client = False\n        self.device = model.device\n        self.kwargs = {\n            \"temperature\": 0.3,\n            \"max_new_tokens\": 300,\n        }\n\n    def basic_request(self, prompt, **kwargs):\n        raw_kwargs = kwargs\n        kwargs = {**self.kwargs, **kwargs}\n        response = self._generate(prompt, **kwargs)\n\n        history = {\n            \"prompt\": prompt,\n            \"response\": response,\n            \"kwargs\": kwargs,\n            \"raw_kwargs\": raw_kwargs,\n        }\n        self.history.append(history)\n\n        return response\n\n    def _generate(self, prompt, **kwargs):\n        kwargs = {**openai_to_hf(**self.kwargs), **openai_to_hf(**kwargs)}\n        if isinstance(prompt, dict):\n            try:\n                prompt = prompt[\"messages\"][0][\"content\"]\n            except (KeyError, IndexError, TypeError):\n                print(\"Failed to extract 'content' from the prompt.\")\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n\n        outputs = self.model.generate(**inputs, **kwargs)    \n        if self.drop_prompt_from_output:\n            input_length = inputs.input_ids.shape[1]\n            outputs = outputs[:, input_length:]\n        completions = [\n            {\"text\": c}\n            for c in self.tokenizer.batch_decode(\n                outputs, skip_special_tokens=True\n            )\n        ]\n        response = {\n            \"prompt\": prompt,\n            \"choices\": completions,\n        }\n        return response\n    def __call__(\n        self, prompt, only_completed=True, return_sorted=False, **kwargs\n    ):\n        assert only_completed, \"for now\"\n        assert return_sorted is False, \"for now\"\n\n        if kwargs.get(\"n\", 1) > 1 or kwargs.get(\"temperature\", 0.0) > 0.1:\n            kwargs[\"do_sample\"] = True\n\n        response = self.request(prompt, **kwargs)\n        return [c[\"text\"] for c in response[\"choices\"]]\n\nprint(\"Model set up!\")              #1\nllama = HFModel(model, tokenizer)\n\ndspy.settings.configure(lm=llama)     #2\n```", "```py\nclass QASignature(dspy.Signature):         #1\n    (\n    \"\"\"You are given a question and answer\"\"\"\n    \"\"\"and you must think step by step to answer the question. \"\"\"\n    \"\"\"Only include the answer as the output.\"\"\"\n    )\n    question = dspy.InputField(desc=\"A math question\")\n    answer = dspy.OutputField(desc=\"An answer that is a number\")\n\nclass ZeroShot(dspy.Module):\n    def __init__(self):\n    super().__init__()\n    self.prog = dspy.Predict(QASignature, max_tokens=1000)\n\n    def forward(self, question):\n    return self.prog(question=question)\n\nevaluate = Evaluate(        #2\n    devset=gsm8k_devset,\n    metric=gsm8k_metric,\n    num_threads=4,\n    display_progress=True,\n    display_table=0,\n)\n\nprint(\"Evaluating Zero Shot\")      #3\nevaluate(ZeroShot())\n```", "```py\n29/200 14.5%\n```", "```py\nconfig = dict(max_bootstrapped_demos=2)     #1\n\nclass CoT(dspy.Module):\n    def __init__(self):\n    super().__init__()\n    self.prog = dspy.ChainOfThought(QASignature, max_tokens=1000)\n\n    def forward(self, question):\n    return self.prog(question=question)\n\nprint(\"Creating Bootstrapped Few Shot Prompt\")         #2\nteleprompter = BootstrapFewShot(metric=gsm8k_metric, **config)\noptimized_cot = teleprompter.compile(\n    CoT(), trainset=gsm8k_trainset, valset=gsm8k_devset\n)\noptimized_cot.save(\"optimized_llama3_math_cot.json\")\n\nprint(\"Evaluating Optimized CoT Prompt\")            #3\nevaluate(optimized_cot)                  \n#149/200 74.5%\n```", "```py\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\nimport torch\nfrom time import perf_counter\n\nmodel_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=quantization_config,\n    low_cpu_mem_usage=True,\n    use_safetensors=True,\n    attn_implementation=\"sdpa\",\n    torch_dtype=torch.float16,\n)\n system = (                                                                  #1\n    \"This is a chat between a user and an artificial intelligence \"          #1\n    \"assistant. The assistant gives helpful, detailed, and polite answers \"  #1\n    \"to the user's questions based on the context. The assistant should \"    #1\n    \"also indicate when the answer cannot be found in the context.\"          #1\n)                                                                            #1\nquestion = ( #1\n    \"Please give a full and complete answer for the question. \"\n    \"Can you help me find a place to eat?\"\n)\nresponse = (\n    \"Sure, there are many locations near you that are wonderful \"\n    \"to eat at, have you tried La Dolce Vite?\"\n)\nquestion_2 = (\n    \"Please give a full and complete answer for the question. \"\n    \"I'm looking for somewhere near me that serves noodles.\"\n)\n\nprompt = f\"\"\"System: {system}\n\nUser: {question}\n\nAssistant: {response}\n\nUser: {question_2}\n\nAssistant:\"\"\"\nstart = perf_counter()\ninputs = tokenizer(tokenizer.bos_token + prompt, return_tensors=\"pt\").to(\n    device\n)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n]\ntext_outputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=128,\n    eos_token_id=terminators,\n)\nresponse = text_outputs[0][inputs.input_ids.shape[-1] :]\nend = perf_counter() - start\n print(\n    f\"\\n\\nFull Response: {tokenizer.batch_decode(text_outputs)}\"\n    f\"\\n\\nOnly Answer Response: {tokenizer.decode(response)}\"\n)\nprint(f\"\\nTime to execute: {end}\\n\")\n\nstart = perf_counter()\nwith torch.no_grad():          \n    hidden_outputs = model(         #2\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    output_hidden_states=True,\n    )\n\n    embeddings_to_cache = hidden_outputs.hidden_states[-1]\n\nend = perf_counter() - start\nprint(f\"Embeddings: {embeddings_to_cache}\")\nprint(f\"\\nTime to execute: {end}\\n\")\n\nfor key, module in model._modules.items():\n    if key == \"lm_head\":                         #3\n    print(f\"This is the layer to pass to by itself:\\n{module}\")\nwith torch.no_grad():\n    start = perf_counter()\n    outputs = model._modules[\"lm_head\"](embeddings_to_cache)\n    end = perf_counter() - start\n    print(f\"Outputs: {outputs}\")\n    print(f\"\\nTime to execute: {end}\\n\")\n\nclass CustomLinearClassifier(torch.nn.Module):        #4\n    def __init__(self, num_labels):\n        super(CustomLinearClassifier, self).__init__()\n        self.num_labels = num_labels\n        self.dropout = torch.nn.Dropout(0.1)\n        self.ff = torch.nn.Linear(4096, num_labels, dtype=torch.float16)\n\n    def forward(self, input_ids=None, targets=None):\n    sequence = self.dropout(input_ids)\n\n    logits = self.ff(sequence[:, 0, :].view(-1, 4096))\n\n    if targets is not None:\n            loss = torch.nn.functional.cross_entropy(\n                logits.view(-1, self.num_labels), targets.view(-1)\n            )\n            return logits, loss\n    else:\n            return logits\n\ncustom_LMHead = CustomLinearClassifier(128256).to(device)\n\nwith torch.no_grad():\n    start = perf_counter()\n    outputs = custom_LMHead(embeddings_to_cache)\n    end = perf_counter() - start\n    print(f\"Outputs: {outputs}\")\n    print(f\"\\nTime to execute: {end}\\n\")\n```"]