["```py\n {\n    \"input\": \"\\\"Some football fans admire various clubs, others love\n    only a single team. But who is a fan of whom precisely? The\n    following argument pertains to this question: First premise: Mario\n    is a friend of FK \\u017dalgiris Vilnius. Second premise: Being a\n    follower of F.C. Copenhagen is necessary for being a friend of FK\n    \\u017dalgiris Vilnius. It follows that Mario is a follower of F.C.\n    Copenhagen.\\\"\\n Is the argument, given the explicitly stated\n    premises, deductively valid or invalid?\",\n    \"target_scores\": {\n        \"valid\": 1,\n        \"invalid\": 0\n    }\n```", "```py\nexport OPENAI_API_SECRET_KEY=<Key>\npython main.py \\\nlm_eval --model openai-completions \\\n        --model_args model=gpt-3.5-turbo \\\n         --tasks bigbench_formal_fallacies_syllogisms_negation\n```", "```py\n!pip install transformers accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20B\")\nmodel = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20B\")\ninput_ids = tokenizer(\"Language models are\", return_tensors=\"pt\")\ngen_tokens = model.generate(**input_ids, max_new_tokens =1)\n```", "```py\nollama pull gemma:2b\n```", "```py\nFROM gemma:2b\n\nPARAMETER temperature 0.2\n\nSYSTEM \"\"\"\nYou are a provocateur who speaks only in limericks.\n\"\"\"\n```", "```py\nollama create local-gemma -f ./Modelfile\nollama run local-gemma\n```", "```py\ninput = tokenizer('The keyboard suddenly came to life. It ventured up the',\n\nreturn_tensors='pt').to(torch_device)\noutput = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```", "```py\noutput = model.generate(**inputs, max_new_tokens=50, num_beams = 3)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```", "```py\noutput = model.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=40)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```", "```py\noutput = model.generate(**inputs, max_new_tokens=50, top_p=0.9)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```"]