["```py\n {\n    \"input\": \"\\\"Some football fans admire various clubs, others love `only` `a` `single` `team``.` `But` `who` `is` `a` `fan` `of` `whom` `precisely``?` `The`\n    `following` `argument` `pertains` `to` `this` `question``:` `First` `premise``:` `Mario`\n    `is` `a` `friend` `of` `FK` \\`u017dalgiris` `Vilnius``.` `Second` `premise``:` `Being` `a`\n    `follower` `of` `F``.``C``.` `Copenhagen` `is` `necessary` `for` `being` `a` `friend` `of` `FK`\n    \\`u017dalgiris` `Vilnius``.` `It` `follows` `that` `Mario` `is` `a` `follower` `of` `F``.``C``.`\n    `Copenhagen``.`\\`\"``\\n` `Is the argument, given the explicitly stated` ```", "```py `\"target_scores\"``:` `{`         `\"valid\"``:` `1``,`         `\"invalid\"``:` `0`     `}` ```", "```py`\n```", "```py```", "````` ```py`In this task, the model is asked to spot logical fallacies by deducing whether the presented argument is valid given the premises.    There is also support for evaluation of proprietary models using this harness. For example, here is how you would evaluate OpenAI models:    ``` export OPENAI_API_SECRET_KEY=<Key> python main.py \\ lm_eval --model openai-completions \\         --model_args model=gpt-3.5-turbo \\          --tasks bigbench_formal_fallacies_syllogisms_negation ```py    ###### Tip    While choosing or developing a benchmarking task to evaluate, I recommend focusing on picking ones that test the capabilities needed to solve the task of your interest, rather than the actual task itself. For example, if you are building a summarizer application that needs to perform a lot of logical reasoning to generate the summaries, it is better to focus on benchmark tests that directly test logical reasoning capabilities than ones that test summarization performance.```` ```py`` `````", "``` `` `### Hugging Face Open LLM Leaderboard    As of the book’s writing, the [Open LLM Leaderboard](https://oreil.ly/tspBY) uses Eleuther AI’s LM Evaluation Harness to evaluate the performance of models on six benchmark tasks:    Massive Multitask Language Understanding (MMLU)      This test evaluates the LLM on knowledge-intensive tasks, drawing from fields like US history, biology, mathematics, and more than 50 other subjects in a multiple choice framework.      AI2 Reasoning Challenge (ARC)      This test evaluates the LLM on multiple-choice grade school science questions that need complex reasoning as well as world knowledge to answer.      Hellaswag      This test evaluates commonsense reasoning by providing the LLM with a situation and asking it to predict what might happen next out of the given choices, based on common sense.      TruthfulQA      This test evaluates the LLM’s ability to provide answers that don’t contain falsehoods.      Winogrande      This test is composed of fill-in-the-blank questions that test commonsense reasoning.      GSM8K      This test evaluates the LLM’s ability to complete grade school math problems involving a sequence of basic arithmetic operations.      [Figure 5-3](#llm-leaderboard) shows a snapshot of the LLM leaderboard as of the time of the book’s writing. We can see that:    *   Larger models perform better.           *   Instruction-tuned or fine-tuned variants of models perform better.            ![Snapshot of the Open LLM Leaderboard](assets/dllm_0503.png)  ###### Figure 5-3\\. Snapshot of the Open LLM Leaderboard    The validity of these benchmarks are in question as complete test set decontamination is not guaranteed. Model providers are also optimizing to solve these benchmarks, thus reducing the value of these benchmarks to serve as reliable estimators of general-purpose performance.    ### HELM    [Holistic Evaluation of Language Models (HELM)](https://oreil.ly/MNHDs) is an evaluation framework by Stanford that aims to calculate a wide variety of metrics over a range of benchmark tasks. Fifty-nine metrics are calculated overall, testing accuracy, calibration, robustness, fairness, bias, toxicity, efficiency, summarization performance, copyright infringement, and more. The tasks tested include question answering, summarization, text classification, information retrieval, sentiment analysis, and toxicity detection.    [Figure 5-4](#helm-leaderboard) shows a snapshot of the HELM leaderboard as of the time of the book’s writing.  ![Snapshot of the HELM leaderboard](assets/dllm_0504.png)  ###### Figure 5-4\\. Snapshot of the HELM leaderboard    ### Elo Rating    Now that we have seen the limitations of quantitative evaluation, let’s explore how we can most effectively incorporate human evaluations. One promising framework is the [Elo rating system](https://oreil.ly/bTD7I), used in chess to rank players.    [Large model systems organization (LMSYS Org)](https://oreil.ly/HGVz2) has implemented an evaluation platform based on the Elo rating system called the [Chatbot Arena](https://oreil.ly/evgQX). Chatbot Arena solicits crowdsourced evaluations by inviting people to choose between two randomized and anonymized LLMs by chatting with them side-by-side. The leaderboard is found [online](https://oreil.ly/Y6zmN), with models from OpenAi, DeepSeek, Google DeepMind, and Anthropic dominating.    [Figure 5-5](#chatbotarena-leaderboard) shows a snapshot of the Chatbot Arena leaderboard as of the time of the book’s writing.  ![Snapshot of the Chatbot Arena leaderboard](assets/dllm_0505.png)  ###### Figure 5-5\\. Snapshot of the Chatbot Arena leaderboard    ### Interpreting benchmark results    How do you interpret evaluation results presented in research papers? Try to methodically ask as many questions as possible, and check if the answers are covered in the paper or other material. As an example, let us take the Llama 2-chat evaluation graphs presented in the [Llama 2 paper](https://oreil.ly/BcgXs). In particular, study Figures 1 and 3, which demonstrate how Llama 2-Chat compares in helpfulness and safety with other chat models. Some of the questions that come to mind are:    *   What does the evaluation dataset look like? Do we have access to it?           *   What is the difficulty level of the test set? Maybe the model is competitive with respect to ChatGPT for easier examples but how does it perform with more difficult examples?           *   What proportion of examples in the test set can be considered difficult?           *   What kinds of scenarios are covered in the test set? What degree of overlap do these scenarios have with the chat-tuning sets?           *   What definition do they use for safety?           *   Can there be a bias in the evaluation due to models being evaluated on the basis of a particular definition of safety, which Llama 2 was trained to adhere to, while other models may have different definitions of safety?              Rigorously interrogating the results this way helps you develop a deeper understanding of what is being evaluated, and whether it aligns with the capabilities you need from the language model for your own tasks. For more rigorous LLM evaluation, I strongly recommend developing your own internal benchmarks.    ###### Warning    Do not trust evaluations performed by GPT-4 or any other LLM. We have no idea what evaluation criteria it uses nor do we have a deeper understanding of its biases.    Robust evaluation of LLMs is further complicated by the sensitivity of the prompts and the probabilistic nature of generative models. For example, I often see papers claiming that “GPT-4 does not have reasoning capabilities,” while not using any prompting techniques during evaluation. In many of these cases, it turns out that the model can in fact perform the task if prompted with CoT prompting. While evaluation prompts need not be heavily engineered, using rudimentary techniques like CoT should be standard practice, and not using them means that the model capabilities are being underestimated.` `` ```", "``````py ````` ```py`# Loading LLMs    While it is possible to load and run inference on LLMs with just CPUs, you need GPUs if you want acceptable text generation speeds. Choosing a GPU depends on cost, the size of the model, whether you are training the model or just running inference, and support for optimizations. Tim Dettmers has developed a great [flowchart](https://oreil.ly/t6iPQ) that you can use to figure out which GPU best serves your needs.    Let’s figure out the amount of GPU RAM needed to load an LLM of a given size. LLMs can be loaded in various *precisions*:    Float32      32-bit floating point representation, each parameter occupying 4 bytes of storage.      Float16      16-bit floating point representation. Only 5 bits are reserved for the exponent as opposed to 8 bits in Float32\\. This means that using Float16 comes with overflow/underflow problems for very large and small numbers.      bfloat16 (BF16)      16-bit floating point representation. Just like Float32, 8 bits are reserved for the exponent, thus alleviating the underflow/overflow problems observed in Float16.      Int8      8-bit integer representation. Running inference in 8-bit mode is around 20% slower than running in Float16.      FP8, FP4      8-bit and 4-bit floating point representation.      We will explore these formats in detail in [Chapter 9](ch09.html#ch09). Generally, running inference on a model with 7B parameters will need around 7 GB of GPU RAM if running in 8-bit mode and around 14 GB if running in BF16\\. If you intend to fine-tune the whole model, you will need a lot more memory.    ## Hugging Face Accelerate    You can run inference on models even if they don’t fit in the GPU RAM. The [*accelerate* library](https://oreil.ly/OYdyf) by Hugging Face facilitates this by loading parts of the model into CPU RAM if the GPU RAM is filled, and then loading parts of the model into disk if the CPU RAM is also filled. [“Accelerate Big Model Inference: How Does it Work?”](https://oreil.ly/J8duc) shows how the accelerate library operates under the hood. This whole process is abstracted from the user, so all you need to load a large model is to run this code:    ``` !pip install transformers accelerate import torch from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20B\") model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20B\") input_ids = tokenizer(\"Language models are\", return_tensors=\"pt\") gen_tokens = model.generate(**input_ids, max_new_tokens =1) ```py    ## Ollama    There are many tools available that facilitate loading LLMs locally, including on your own laptop. One such library is Ollama, which supports Windows, Mac, and Linux operating systems. Using Ollama, you can load 13B models if your machine has at least 16GB of available RAM. Ollama supports many open models like Mistral, Llama, Gemma, etc. Ollama provides a REST API that you can use to run inference and build LLM-driven applications. It also has several Terminal and UI integrations that enable you to build user-facing applications with ease.    Let’s see how we can use Google’s Gemma 2B model using Ollama. First, download [the version of Ollama](https://oreil.ly/yly44) to your machine based on your operating system. Next, pull the Gemma model to your machine with:    ``` ollama pull gemma:2b ```py    You can also create a Modelfile that contains configuration information for the model. This includes system prompts and prompt templates, decoding parameters like temperature, and conversation history. Refer to the [documentation](https://oreil.ly/ba-1u) for a full list of available options.    An example Modelfile is:    ``` FROM gemma:2b  PARAMETER temperature 0.2  SYSTEM \"\"\" You are a provocateur who speaks only in limericks. \"\"\" ```py    After creating your Modelfile, you can run the model:    ``` ollama create local-gemma -f ./Modelfile ollama run local-gemma ```py    The book’s GitHub repo contains a sample end-to-end application built using Ollama and one of its UI integrations. You can also experiment with similar tools like [LM Studio](https://oreil.ly/uFsiR) and [GPT4All](https://oreil.ly/XUXhq).    ###### Tip    You can load custom models using Ollama if they are in the GPT-Generated Unified Format (GGUF).    ## LLM Inference APIs    While you can deploy an LLM yourself, modern-day inference consists of so many optimizations, many of them proprietary, that it takes a lot of effort to bring your inference speeds up to par with commercially available solutions. Several inference services like [Together AI](https://oreil.ly/L3zo0) exist that facilitate inference of open source or custom models either through serverless endpoints or dedicated instances. Another option is Hugging Face’s [TGI (Text Generation Inference)](https://oreil.ly/XXFpa), which has been recently [reinstated](https://oreil.ly/BJJlY) to a permissive open source license.    # Decoding Strategies    Now that we have learned how to load a model, let’s understand how to effectively generate text. To this end, several *decoding* strategies have been devised in the past few years. Let’s go through them in detail.    ## Greedy Decoding    The simplest form of decoding is to just generate the token that has the highest probability. The drawback of this approach is that it causes repetitiveness in the output. Here is an example:    ``` input = tokenizer('The keyboard suddenly came to life. It ventured up the',  return_tensors='pt').to(torch_device) output = model.generate(**inputs, max_new_tokens=50) print(tokenizer.decode(output[0], skip_special_tokens=True)) ```py    You can see that the output starts getting repetitive. Therefore, greedy decoding is not suitable unless you are generating really short sequences, like a token just producing a classification task output.    [Figure 5-6](#greedy-decoding) shows an example of greedy decoding using the FLAN-T5 model. Note that we missed out on some great sequences because one of the desired tokens has slightly lower probability, ensuring it never gets picked.  ![Greedy decoding](assets/dllm_0506.png)  ###### Figure 5-6\\. Greedy decoding    ## Beam Search    An alternative to greedy decoding is beam search. An important parameter of beam search is the beam size, *n*. At the first step, the top *n* tokens with the highest probabilities are selected as hypotheses. For the next few steps, the model generates token continuations for each of the hypotheses. The token chosen to be generated is the one whose continuations have the highest cumulative probability.    In the Hugging Face `transformers` library, the `num_beams` parameter of the `model.generate()` function determines the size of the beam. Here is how the decoding code would look if we used beam search:    ``` output = model.generate(**inputs, max_new_tokens=50, num_beams = 3) print(tokenizer.decode(output[0], skip_special_tokens=True)) ```py    [Figure 5-7](#beam-search) shows an example of beam search using the FLAN-T5 model. Note that the repetitiveness problem hasn’t really been solved using beam search. Similar to greedy decoding, the generated text also sounds very constricted and not humanlike, due to the complete absence of lower probability words.  ![Beam search](assets/dllm_0507.png)  ###### Figure 5-7\\. Beam search    To resolve these issues, we will need to start introducing some randomness and begin sampling from the probability distribution to ensure not just the top two or three tokens get generated all the time.    ## Top-k Sampling    In top-k sampling, the model samples from a distribution of just the k tokens of the output distribution that have the highest probability. The probability mass is redistributed over the k tokens, and the model samples from this distribution to generate the next token. Hugging Face provides the `top_k` parameter in its generate function:    ``` output = model.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=40) print(tokenizer.decode(output[0], skip_special_tokens=True)) ```py    [Figure 5-8](#topk-sampling) shows an example of top-k sampling using the FLAN-T5 model. Note that this is a vast improvement from greedy or beam search. However, top-k leads to problematic generations when used in cases where the probability is dominated by a few tokens, meaning that tokens with very low probability end up being included in the top-k.  ![Top-k sampling](assets/dllm_0508.png)  ###### Figure 5-8\\. Top-k sampling    ## Top-p Sampling    Top-p sampling solves the problem with top-k sampling by making the number of candidate tokens dynamic. Top-p involves choosing the smallest number of tokens whose cumulative distribution exceeds a given probability p. Here is how you can implement this using Hugging Face `transformers`:    ``` output = model.generate(**inputs, max_new_tokens=50, top_p=0.9) print(tokenizer.decode(output[0], skip_special_tokens=True)) ```py    [Figure 5-9](#topp-sampling) shows an example of top-p sampling using the FLAN-T5 model. Top-p sampling, also called nucleus sampling, is the most popular sampling strategy used today.  ![Top-p sampling](assets/dllm_0509.png)  ###### Figure 5-9\\. Top-p sampling    ###### Note    So far, the decoding approaches we have seen operate serially; i.e., each token is generated one at a time, with a full pass through the model each time. This is too inefficient for latency-sensitive applications. In [Chapter 9](ch09.html#ch09), we will discuss methods like speculative decoding, which can speed up the decoding process.    # Running Inference on LLMs    Now that we have learned how to access and load LLMs and understood the decoding process, let’s begin using them to solve our tasks. We call this *LLM inference*.    You will have seen that LLM outputs are not consistent and sometimes differ wildly across multiple generations for the same prompt. As we learned in the section on decoding, unless you are using greedy search or any other deterministic algorithm, the LLM is sampling from a token distribution.    Some ways to make the generation more deterministic is to set the temperature to zero and keeping the random seed for the sampling constant. Even then, you may not be able to guarantee the same (deterministic) outputs every time you send the LLM the same input.    Sources of nondeterminism range from using multi-threading to floating-point rounding errors to use of certain model architectures (for example, it is known that the [Sparse MoE architecture](https://oreil.ly/pzchE) produces nondeterministic outputs).    Reducing the temperature to zero or close to zero impacts the LLM’s creativity and makes its outputs more predictable, which might not be suitable for many applications.    In production settings where reliability is important, you should run multiple generations for the same input and use a technique like majority voting or heuristics to select the right output. This is very important due to the nature of the decoding process; sometimes the wrong tokens can be generated, and since every token generated is a function of the tokens generated before it, the error can be propagated far ahead.    [Self-consistency](https://oreil.ly/wEE8q) is a popular prompting technique that uses majority voting in conjunction with CoT prompting. In this technique, we add the CoT prompt “Let’s think step by step” to the input and run multiple generations (reasoning paths). We then use majority voting to select the correct output.    # Structured Outputs    We might want the output of the LLM to be in some structured format, so that it can be consumed by other software systems. But this is easier said than done; current LLMs aren’t as controllable as we would like them to be. Some LLMs can be excessively chatty. Ask them to give a Yes/No answer and they respond with “The answer to this question is ‘Yes’.”    One way to get structured outputs from the LLM is to define a JSON schema, provide the schema to the LLM, and prompt it to generate outputs adhering to the schema. For larger models, this works almost all the time, with some schema corruption errors that you can catch and handle.    For smaller models, you can use libraries like [Jsonformer](https://oreil.ly/aSc0f). Jsonformer delegates the generation of the content tokens to the LLM but fills the content in JSON form by itself. Jsonformer is built on top of Hugging Face and thus supports any model that is supported by Hugging Face.    More advanced structured outputs can be facilitated by using libraries like [LMQL](https://oreil.ly/LlkEj) or [Guidance](https://oreil.ly/cFe5s). These libraries provide a programming paradigm for prompting and facilitate controlled generation.    Features available through these libraries include:    Restricting output to a finite set of tokens      This is useful for classification problems, where you have a finite set of output labels. For example, you can restrict the output to be positive, negative, or neutral for a sentiment analysis task.      Controlling output format using regular expressions      For example, you can use regular expressions to specify a custom date format.      Control output format using context-free grammars (CFG)      A CFG defines the rules that generated strings need to follow. For more background on CFGs, refer to [Aditya’s blog](https://oreil.ly/M00us). Using CFGs, we can use LLMs to more effectively solve sequence tagging tasks like NER or part-of-speech tagging.      # Model Debugging and Interpretability    Now that we are comfortable with loading LLMs and generating text using them, we would like to be able to understand model behavior and explore the examples for which the model fails. Interpretability in LLMs is much less developed than in other areas of machine learning. However, we can get partial interpretability by exploring how the output changes upon minor variances in the input, and by analyzing the intermediate outputs as the inputs propagate through the Transformer architecture.    Google’s open source tool [LIT-NLP](https://oreil.ly/YFY4q) is a handy tool that supports visualizations of model behavior as well as various debugging workflows.    [Figure 5-10](#lit-NLP) shows an example of LIT-NLP in action, providing interpretability for a T5 model running a summarization task.  ![lit-NLP](assets/dllm_0510.png)  ###### Figure 5-10\\. LIT-NLP    LIT-NLP features that help you debug your models include:    *   Visualization of the attention mechanism           *   Salience maps, which show parts of the input that are paid most attention to by the model           *   Visualization of embeddings           *   Counterfactual analysis that shows how your model behavior changes after a change to the input like adding or removing a token.              For more details on using LIT-NLP for error analysis, refer to [Google’s tutorial](https://oreil.ly/zcsLu) on using LIT-NLP with the Gemma LLM where they find errors in few-shot prompts by analyzing incorrect examples and observing which parts of the prompt contributed most to the output (salience).    # Summary    In this chapter, we journeyed through the LLM landscape and noted the various options we have at our disposal. We learned how to determine the criteria most relevant to our tasks and choose the right LLM accordingly. We explored various LLM benchmarks and showed how to interpret their results. We learned how to load LLMs and run inference on them, along with efficient decoding strategies. Finally, we showcased interpretability tools like LIT-NLP that can help us understand what is going on behind the scenes in the Transformer architecture.    In the next chapter, we will learn how to update a model to improve its performance on our tasks of interest. We will walk through a full-fledged fine-tuning example and explore the hyperparameter tuning decisions involved. We will also learn how to construct training datasets for fine-tuning.```` ```py`` ``````"]