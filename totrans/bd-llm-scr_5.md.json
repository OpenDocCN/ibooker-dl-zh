["```py\nimport torch\nfrom chapter04 import GPTModel\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,\n    \"context_length\": 256,  #A\n    \"emb_dim\": 768,\n    \"n_heads\": 12,\n    \"n_layers\": 12, \n    \"drop_rate\": 0.1,     #B\n    \"qkv_bias\": False\n}\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\n```", "```py\nimport tiktoken\nfrom chapter04 import generate_text_simple\n\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\nstart_context = \"Every effort moves you\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\n Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n```", "```py\ninputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n                       [40,    1107, 588]])   #  \"I really like\"]\n```", "```py\ntargets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n                        [588,  428,  11311]]) #  \" really like chocolate\"]\n```", "```py\nwith torch.no_grad(): #A\n    logits = model(inputs)\nprobas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\nprint(probas.shape)\n```", "```py\ntorch.Size([2, 3, 50257])\n```", "```py\ntoken_ids = torch.argmax(probas, dim=-1, keepdim=True)\nprint(\"Token IDs:\\n\", token_ids)\n```", "```py\nToken IDs:\n tensor([[[16657], # First batch\n         [  339],\n         [42826]],\n        [[49906], # Second batch\n         [29669],\n         [41751]]])\nFinally, step 5 converts the token IDs back into text:\nprint(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\nprint(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n```", "```py\nTargets batch 1:  effort moves you\nOutputs batch 1:  Armed heNetflix\n```", "```py\ntext_idx = 0\ntarget_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(\"Text 1:\", target_probas_1)\n\ntext_idx = 1\ntarget_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(\"Text 2:\", target_probas_2)\n```", "```py\nText 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\nText 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\n```", "```py\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\nprint(log_probas)\n```", "```py\ntensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])\n```", "```py\navg_log_probas = torch.mean(log_probas)\nprint(avg_log_probas)\n```", "```py\ntensor(-10.7722)\n```", "```py\nneg_avg_log_probas = avg_log_probas * -1\nprint(neg_avg_log_probas)\n```", "```py\nprint(\"Logits shape:\", logits.shape)\nprint(\"Targets shape:\", targets.shape)\n```", "```py\nLogits shape: torch.Size([2, 3, 50257])\nTargets shape: torch.Size([2, 3])\n```", "```py\nlogits_flat = logits.flatten(0, 1)\ntargets_flat = targets.flatten()\nprint(\"Flattened logits:\", logits_flat.shape)\nprint(\"Flattened targets:\", targets_flat.shape)\n```", "```py\nFlattened logits: torch.Size([6, 50257])\nFlattened targets: torch.Size([6])\n```", "```py\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\nprint(loss)\n```", "```py\ntensor(10.7722)\n```", "```py\nfile_path = \"the-verdict.txt\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    text_data = file.read()\n```", "```py\ntotal_characters = len(text_data)\ntotal_tokens = len(tokenizer.encode(text_data))\nprint(\"Characters:\", total_characters)\nprint(\"Tokens:\", total_tokens)\n```", "```py\nCharacters: 20479\nTokens: 5145\n```", "```py\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n```", "```py\nfrom chapter02 import create_dataloader_v1\ntorch.manual_seed(123)\n\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True\n)\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False\n)\n```", "```py\nprint(\"Train loader:\")\nfor x, y in train_loader:\n    print(x.shape, y.shape)\n\nprint(\"\\nValidation loader:\")\nfor x, y in val_loader:\n    print(x.shape, y.shape)\n```", "```py\nTrain loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\n\nValidation loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\n```", "```py\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device) #A\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(\n        logits.flatten(0, 1), target_batch.flatten()\n    )\n    return loss\n```", "```py\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if num_batches is None:\n        num_batches = len(data_loader) #A\n    else:\n        num_batches = min(num_batches, len(data_loader)) #B\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item() #C\n        else:\n            break\n    return total_loss / num_batches #D\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #A\nmodel.to(device)\ntrain_loss = calc_loss_loader(train_loader, model, device) #B\nval_loss = calc_loss_loader(val_loader, model, device)\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)\n```", "```py\nTraining loss: 10.98758347829183\nValidation loss: 10.98110580444336\n```", "```py\ndef train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n                       eval_freq, eval_iter, start_context):\n    train_losses, val_losses, track_tokens_seen = [], [], [] #A\n    tokens_seen, global_step = 0, -1\n\n    for epoch in range(num_epochs): #B\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() #C\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward() #D\n            optimizer.step() #E\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            if global_step % eval_freq == 0: #F\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n\n        generate_and_print_sample(  #G\n            model, train_loader.dataset.tokenizer, device, start_context\n        )\n    return train_losses, val_losses, track_tokens_seen\n```", "```py\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval() #A\n    with torch.no_grad(): #B\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n```", "```py\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded,\n            max_new_tokens=50, context_size=context_size\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n    model.train()\n```", "```py\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #A\nnum_epochs = 10\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n    start_context=\"Every effort moves you\"\n)\n```", "```py\nEp 1 (Step 000000): Train loss 9.781, Val loss 9.933\nEp 1 (Step 000005): Train loss 8.111, Val loss 8.339\nEvery effort moves you,,,,,,,,,,,,.                                     \nEp 2 (Step 000010): Train loss 6.661, Val loss 7.048\nEp 2 (Step 000015): Train loss 5.961, Val loss 6.616\nEvery effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n[...]  #A\nEp 9 (Step 000080): Train loss 0.541, Val loss 6.393\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\nEp 10 (Step 000085): Train loss 0.391, Val loss 6.452\nEvery effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n```", "```py\nimport matplotlib.pyplot as plt\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax2 = ax1.twiny()  #A\n    ax2.plot(tokens_seen, train_losses, alpha=0)  #B\n    ax2.set_xlabel(\"Tokens seen\")\n    fig.tight_layout()\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n```", "```py\nmodel.to(\"cpu\")\nmodel.eval()\n```", "```py\ntokenizer = tiktoken.get_encoding(\"gpt2\")\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\nEvery effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n```", "```py\nvocab = { \n    \"closer\": 0,\n    \"every\": 1, \n    \"effort\": 2, \n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5, \n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8,\n} \ninverse_vocab = {v: k for k, v in vocab.items()}\n```", "```py\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n```", "```py\nprobas = torch.softmax(next_token_logits, dim=0)\nnext_token_id = torch.argmax(probas).item()\nprint(inverse_vocab[next_token_id])\n```", "```py\ntorch.manual_seed(123) \nnext_token_id = torch.multinomial(probas, num_samples=1).item()\nprint(inverse_vocab[next_token_id])\n```", "```py\ndef print_sampled_tokens(probas):\n    torch.manual_seed(123)\n    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n    sampled_ids = torch.bincount(torch.tensor(sample))\n    for i, freq in enumerate(sampled_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\nprint_sampled_tokens(probas)\nThe sampling output is as follows:\n73 x closer\n0 x every\n0 x effort\n582 x forward\n2 x inches\n0 x moves\n0 x pizza\n343 x toward\n```", "```py\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits / temperature\n    return torch.softmax(scaled_logits, dim=0)\n```", "```py\ntemperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\nscaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\nx = torch.arange(len(vocab))\nbar_width = 0.15\nfig, ax = plt.subplots(figsize=(5, 3))\nfor i, T in enumerate(temperatures):\n    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n                   bar_width, label=f'Temperature = {T}')\nax.set_ylabel('Probability')\nax.set_xticks(x)\nax.set_xticklabels(vocab.keys(), rotation=90)\nax.legend()\nplt.tight_layout()\nplt.show()\n```", "```py\ntop_k = 3\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\nprint(\"Top logits:\", top_logits)\nprint(\"Top positions:\", top_pos)\n```", "```py\nTop logits: tensor([6.7500, 6.2800, 4.5100])\nTop positions: tensor([3, 7, 0])\n```", "```py\nnew_logits = torch.where(\n    condition=next_token_logits < top_logits[-1],  #A\n    input=torch.tensor(float('-inf')),  #B\n    other=next_token_logits  #C\n)\nprint(new_logits)\n```", "```py\ntensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n```", "```py\ntopk_probas = torch.softmax(new_logits, dim=0)\nprint(topk_probas)\n```", "```py\ntensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n```", "```py\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):  #A\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n        if top_k is not None:  #B\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:  #C\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:  #D\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx\n```", "```py\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=15,\n    context_size=GPT_CONFIG_124M[\"context_length\"],\n    top_k=25,\n    temperature=1.4\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\n Every effort moves you stand to work on surprise, a one of us had gone with random-\n```", "```py\ntorch.save(model.state_dict(), \"model.pth\")\n```", "```py\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\"))\nmodel.eval()\n```", "```py\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n    }, \n    \"model_and_optimizer.pth\"\n)\n```", "```py\ncheckpoint = torch.load(\"model_and_optimizer.pth\")\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nmodel.train();\n```", "```py\npip install tensorflow>=2.15.0  tqdm>=4.66\n```", "```py\nimport urllib.request\nurl = (\n    \"https://raw.githubusercontent.com/rasbt/\"\n    \"LLMs-from-scratch/main/ch05/\"\n    \"01_main-chapter-code/gpt_download.py\"\n)\nfilename = url.split('/')[-1]\nurllib.request.urlretrieve(url, filename)\n```", "```py\nfrom gpt_download import download_and_load_gpt2\nsettings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n```", "```py\ncheckpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 63.9kiB/s]\nencoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 2.20MiB/s]\nhprams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 78.3kiB/s]\nmodel.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [01:09<00:00, 7.16MiB/s]\nmodel.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21k/5.21k [00:00<00:00, 3.24MiB/s]\nmodel.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471k/471k [00:00<00:00, 2.46MiB/s]\nvocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 1.70MiB/s]\n```", "```py\nprint(\"Settings:\", settings)\nprint(\"Parameter dictionary keys:\", params.keys())\n```", "```py\nSettings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\nParameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n```", "```py\nprint(params[\"wte\"])\nprint(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)\n```", "```py\n[[-0.11010301 ... -0.1363697   0.01506208   0.04531523]\n [ 0.04034033 ...  0.08605453  0.00253983   0.04318958]\n [-0.12746179  ...  0.08991534 -0.12972379 -0.08785918]\n ...\n [-0.04453601 ...   0.10435229  0.09783269 -0.06952604]\n [ 0.1860082  ...  -0.09625227  0.07847701 -0.02245961]\n [ 0.05135201 ...   0.00704835  0.15519823  0.12067825]]\nToken embedding weight tensor dimensions: (50257, 768)\n```", "```py\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n```", "```py\nmodel_name = \"gpt2-small (124M)\"\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\n```", "```py\nNEW_CONFIG.update({\"context_length\": 1024})\n```", "```py\nNEW_CONFIG.update({\"qkv_bias\": True})\n```", "```py\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()\n```", "```py\ndef assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n    return torch.nn.Parameter(torch.tensor(right))\n```", "```py\nimport numpy as np\n\ndef load_weights_into_gpt(gpt, params):\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])  #A\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n\n    for b in range(len(params[\"blocks\"])):  #B\n        q_w, k_w, v_w = np.split(  #C\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.weight = assign(\n            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n        gpt.trf_blocks[b].att.W_key.weight = assign(\n            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n        gpt.trf_blocks[b].att.W_value.weight = assign(\n            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.bias = assign(\n            gpt.trf_blocks[b].att.W_query.bias, q_b)\n        gpt.trf_blocks[b].att.W_key.bias = assign(\n            gpt.trf_blocks[b].att.W_key.bias, k_b)\n        gpt.trf_blocks[b].att.W_value.bias = assign(\n            gpt.trf_blocks[b].att.W_value.bias, v_b)\n\n        gpt.trf_blocks[b].att.out_proj.weight = assign(\n            gpt.trf_blocks[b].att.out_proj.weight, \n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].att.out_proj.bias = assign(\n            gpt.trf_blocks[b].att.out_proj.bias, \n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight, \n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias, \n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight, \n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias, \n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n\n        gpt.trf_blocks[b].norm1.scale = assign(\n            gpt.trf_blocks[b].norm1.scale, \n            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n        gpt.trf_blocks[b].norm1.shift = assign(\n            gpt.trf_blocks[b].norm1.shift, \n            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n        gpt.trf_blocks[b].norm2.scale = assign(\n            gpt.trf_blocks[b].norm2.scale, \n            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n        gpt.trf_blocks[b].norm2.shift = assign(\n            gpt.trf_blocks[b].norm2.shift, \n            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n\n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])  #D\n```", "```py\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\n```", "```py\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=gpt,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=NEW_CONFIG[\"context_length\"],\n    top_k=50,\n    temperature=1.5\n)\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n```", "```py\nOutput text:\n Every effort moves you toward finding an ideal new way to practice something!\nWhat makes us want to be on top of that?\n```"]