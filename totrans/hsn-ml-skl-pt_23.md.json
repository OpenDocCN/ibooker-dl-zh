["```py\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1))\n# [...] pretend the 32-bit model is trained here\nmodel.half()  # convert the model parameters to half precision (16 bits)\n```", "```py\nX = torch.rand(3, 10, dtype=torch.float16)  # some 16-bit input\ny_pred = model(X)  # 16-bit output\n```", "```py\nmodel = nn.Sequential(nn.Linear(10, 100, dtype=torch.float16), nn.ReLU(),\n                      nn.Linear(100, 1, dtype=torch.float16))\n```", "```py\nfrom torch.amp import GradScaler\n\ndef train_mpt(model, optimizer, criterion, train_loader, n_epochs,\n              dtype=torch.float16, init_scale=2.0**16):\n    grad_scaler = GradScaler(device=device, init_scale=init_scale)\n    model.train()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            with torch.autocast(device_type=device, dtype=dtype):\n                y_pred = model(X_batch)\n                loss = criterion(y_pred, y_batch)\n            grad_scaler.scale(loss).backward()\n            grad_scaler.step(optimizer)\n            grad_scaler.update()\n            optimizer.zero_grad()\n```", "```py\n>>> w = torch.tensor([0.1, -0.1, 0.6, 0.0])  # 32-bit floats `>>>` `s` `=` `(``w``.``max``()` `-` `w``.``min``())` `/` `255.`  `# compute the scale` ```", "```py `>>>` `z` `=` `-``(``w``.``min``()` `/` `s``)``.``round``()`  `# compute the zero point` ```", "```py` `>>>` `qw`  `# this is a quantized tensor internally represented using integers` ```", "```py ```", "```py`` ```", "```py\n```", "```py```", "```py```", "````` Quantizing a model to 8-bits divides its size by almost 4\\. For example, suppose we have a convolutional layer with 64 kernels, 3 × 3 each, and it has 32 input channels. This layer requires 64 × 32 × 3 × 3 = 18,432 parameters (ignoring the bias terms). That’s 18,432 × 4 = 73,728 bytes before quantization, and just 18,432 bytes after quantization, plus 2 × 4 = 8 bytes to store *s* and *z* (indeed, they are both stored as 32-bit floats, so 4 bytes each).    ###### Tip    PyTorch also has a `torch.quantize_per_channel()` function which quantizes each channel separately: this offers better precision but requires a bit more space for the additional quantization parameters.    When the float values are approximately symmetric around zero, we can use *symmetric linear quantization*, where the values are mapped between –127 and +127, or more generally between –*r* and +*r* with *r* = 2^(*n*–1) – 1, using [Equation B-2](#symmetric_linear_quantization_equation).    ##### Equation B-2\\. Symmetric linear quantization  $q Subscript i Baseline equals round left-parenthesis StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis with s equals StartFraction max Underscript i Endscripts StartAbsoluteValue w Subscript i Baseline EndAbsoluteValue Over 2 Superscript n minus 1 Baseline minus 1 EndFraction$  To implement symmetric linear quantization in PyTorch, we can use the `torch.quantize_per_tensor()` function again, but using a zero point equal to 0, and data type `qint8` (quantized signed 8-bit integer):    ```py >>> w = torch.tensor([0.0, -0.94, 0.92, 0.93])  # 32-bit floats `>>>` `s` `=` `w``.``abs``()``.``max``()` `/` `127.` ```` `>>>` `qw` `=` `torch``.``quantize_per_tensor``(``w``,` `scale``=``s``,` `zero_point``=``0``,` `dtype``=``torch``.``qint8``)` ```py `>>>` `qw` `` `tensor([ 0.0000, -0.9400,  0.9178,  0.9326], size=(4,), dtype=torch.qint8,`  `quantization_scheme=torch.per_tensor_affine, scale=0.007401574868708849,`  `zero_point=0)` `` ``` ```py` ```   ```py` ``` ``[Figure B-3](#symmetric_quantization_diagram) shows some floats ranging between –0.94 and +0.93, quantized to signed bytes (i.e., 8-bits) ranging between –127 and +127,⁠^([5](app02.html#id4409)) using symmetric linear quantization. Notice that float 0.0 is always mapped to integer 0.  ![Diagram showing the mapping of weights from floats between -0.94 and 0.93 to quantized bytes ranging from -127 to 127 using symmetric linear quantization.](assets/hmls_ab03.png)  ###### Figure B-3\\. Symmetric linear quantization    Symmetric mode is often a bit faster than asymmetric mode, because there’s no zero point *z* to worry about. However, if the values are not symmetric, part of the integer range will be wasted. For example, if all the weights are positive, then symmetric mode will only use bytes 0 to 127 (rather than –127 to 127). As a result, symmetric mode can be a bit less precise than asymmetric mode. In practice, symmetric mode is generally preferred for weights (which are often fairly symmetric), and asymmetric mode for activations (especially when using ReLU, since it outputs only nonnegative values).    Let’s now see how to quantize your models in practice using PyTorch’s `torch.​ao.quantization` package. The first approach is to quantize a trained model, which is called *post-training quantization* (PTQ). The second is to train (or fine-tune) your model with some fake quantization to get it used to the noise: this is called *quantization-aware training* (QAT). Let’s start with PTQ.`` ```py ```` ```py`` `````", "``````py`  ``````", "````` ```py`## Post-Training Quantization Using torch.ao.quantization    The `torch.ao` package contains tools for architecture optimization (hence the name), including pruning, sparsity, and quantization. The `torch.ao.quantization` package offers two solutions to quantize trained models: dynamic quantization and static quantization. Let’s see how to implement both.    ### Dynamic quantization    Dynamic quantization is best for MLPs, RNNs, and transformers. To implement it using PyTorch’s `torch.ao.quantization` package, you must first choose a quantization engine: PyTorch currently supports the *Facebook General Matrix Multiplication* (FBGEMM) engine for x86 CPUs, plus a newer x86 engine that supports recent x86 CPUs but is less battle-tested, and finally the *Quantized Neural Networks Package* (QNNPACK) engine for ARM/mobile. This code will pick the appropriate engine depending on the platform:    ``` import platform  machine = platform.machine().lower() engine = \"qnnpack\" if (\"arm\" in machine or \"aarch64\" in machine) else \"x86\" ```py    ###### Warning    PyTorch does not offer an engine for CUDA or other hardware accelerators, but other libraries do, such as the bitsandbytes library (as we will see shortly).    Once you have selected an engine, you can use the `quantize_dynamic()` function from the `torch.ao.quantization` package; just pass it your trained model, tell it the types of layers to quantize (typically just the `Linear` and RNN layers), specify the quantized data type, and boom, you have a ready-to-use quantized model:    ``` from torch.ao.quantization import quantize_dynamic  model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1)) # [...] pretend the 32-bit model is trained here torch.backends.quantized.engine = engine qmodel = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8) X = torch.randn(3, 10) y_pred = qmodel(X)  # float inputs and outputs, but quantized internally ```py    The `quantize_dynamic()` function replaces each `Linear` layer with a `DynamicQuantizedLinear` layer, with int8 weights. This layer behaves just like a regular linear layer, with float inputs and outputs, but it quantizes its inputs on the fly (recomputing the zero points and scales for each batch), performs matrix multiplication using integers only (with 32-bit integer accumulators), and dequantizes the result so the next layer gets float inputs. Now let’s look at static quantization.    ### Static quantization    This option is best for CNNs, and max inference speed. It’s also compulsory for edge devices without a *floating-point unit* (FPU), as they don’t support floats at all. Both the weights and activations are prepared for quantization ahead of time, for all layers. As we discussed earlier, weights are constant so they can be quantized once, while activations require a calibration step to determine their typical range. The model is then converted to a fully quantized model. Here is how to implement it:    ``` from torch.ao.quantization import get_default_qconfig, QuantStub, DeQuantStub  model = nn.Sequential(QuantStub(),                       nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1),                       DeQuantStub()) # [...] pretend the 32-bit model is trained here model.qconfig = get_default_qconfig(engine) torch.ao.quantization.prepare(model, inplace=True) for X_batch, _ in calibration_loader:     model(X_batch) torch.ao.quantization.convert(model, inplace=True) ```py    Let’s go through this code step by step:    *   After the imports, we create our 32-bit model, but this time we add a `QuantStub` layer as the first layer, and a `DeQuantStub` layer as the last. Both layers are just passthrough for now.           *   Next, the model can be trained normally (another option would be to take a pretrained model and place it between a `QuantStub` layer and a `DeQuantStub` layer).           *   Next, we set the model’s `qconfig` to the output of the `get_default_qconfig()` function: this function takes the name of the desired quantization engine and returns a `QConfig` object containing a default quantization configuration for this engine. It specifies the quantization data type (e.g., `torch.qint8`), the quantization scheme (e.g., symmetric linear quantization per tensor), and two functions that will observe the weights and activations to determine their ranges.           *   Next we call the `torch.ao.quantization.prepare()` function: it uses the weight observer specified in the configuration to determine the weights range, which it immediately uses to compute the zero points and scales for the weights. Since we don’t know what the input data looks like at this point, the function cannot compute the quantization parameters for the activations yet, so it inserts activation observers in the model itself: these are attached to the outputs of the `QuantStub` and `Linear` layers. The observer appended to the `QuantStub` layer is responsible for tracking the input range.           *   Next, we take a representative sample of input batches (i.e., the kind the model will get in production), and we pass these batches through the model: this allows the activation observers to track the activations.           *   Once we have given the model enough data, we finally call the `torch.ao.​quanti⁠zation.convert()` function, which removes the observers from the model and replaces the layers with quantized versions. The `QuantStub` layer is replaced with a `Quantize` layer which will quantize the inputs. The `Linear` layers are replaced with `QuantizedLinear` layers. And the `DeQuantStub` layer is replaced with a `DeQuantize` layer which will dequantize the outputs.              ###### Note    There are a few observers to choose from: they can just keep track of the minimum and maximum values for each tensor (`MinMaxObserver`), or for each channel (`PerChannelMinMaxObserver`), or they can compute an exponential moving average of the min/max values, which reduces the impact of a few outliers. Finally, they can even record a histogram of the observed values (`HistogramObserver`), making it possible to find an optimal quantization range that minimizes the quantization error. That said, the default observers are usually fine.    We now have a model that we can use normally, with float inputs and outputs, but which works entirely with integers internally, making it lightweight and fast. To deploy it to mobile or embedded devices, there are many options to choose from (which are beyond the scope of this book), including:    *   Use ExecuTorch, which is PyTorch’s lightweight edge runtime           *   Export the model to ONNX and run it with ONNX Runtime (cross-platform)           *   Convert it to TFLite or TFLite Micro           *   Compile it for the target device using TVM or microTVM              Moreover, the PyTorch team has released a separate library named [*PyTorch-native Architecture Optimization* (TorchAO)](https://homl.info/torchao), designed to be a robust and extensible model optimization framework. Over time, many features in PyTorch’s `torch.ao` package are expected to be migrated to—or superseded by—TorchAO. The library already includes advanced features such as 4-bit weight support and *per-block quantization*, in which each tensor is split into small blocks and each block is quantized independently, trading space for improved precision.    Post-training quantization (either dynamic or static) can shrink and speed up your models significantly, but it will also degrade their accuracy. This is particularly the case when quantizing down to 4 bits or less, and it’s worse for static quantization than for dynamic quantization (which can at least adapt to each input batch independently). When the accuracy drop is unacceptable, you can try quantization-aware training, as we will discuss now.    ## Quantization-Aware Training (QAT)    QAT was introduced in a [2017 paper](https://homl.info/qat) by Google researchers.⁠^([6](app02.html#id4433)) It rests upon a simple idea: why not introduce some fake quantization noise during training so the model can learn to cope with it? After training, we can then quantize the model for real, and it should remain fairly accurate. QAT also makes it possible to quantize more aggressively without losing too much accuracy, down to 4 bits, or even less. Sound promising? Let’s see how it can be done.    To add fake quantization noise to weights, we can simply quantize them and immediately dequantize them. For example, a weight equal to 0.42 might be quantized to the 4-bit integer 7, and immediately dequantized back to 0.39: we’ve successfully introduced quantization noise, and it’s precisely the quantization noise that we would get if the model were really quantized. This fake quantization operation can be executed at each training step, and it can also be applied to some of the activations (e.g., to each layer output).    However, there is one little problem: quantization involves rounding to the nearest integer, and the rounding operation has gradients equal to zero (or undefined at integer boundaries), so gradient descent cannot make any progress. Luckily, we can sidestep this issue by using the *straight-through estimator* (STE) trick: during the backward phase, we pretend that the fake quantization operation was just the identity function, so the gradients flow straight through it untouched. This works because the loss landscape is generally fairly smooth locally, so gradients are likely to be similar within a small region around the quantized value, including at the original value.    Implementing QAT in PyTorch is fairly straightforward:    ``` from torch.ao.quantization import get_default_qat_qconfig  model = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1)) model.qconfig = get_default_qat_qconfig(engine) torch.ao.quantization.prepare_qat(model, inplace=True) train(model, optimizer, [...])  # train the model normally torch.ao.quantization.convert(model.eval(), inplace=True) ```py    After the import, we create our model, set its `qconfig` attribute to the default QAT configuration object for the chosen quantization engine, then we call the `prepare_qat()` function to add fake quantization operations to the model. This step also adds observers to determine the usual range of activation values. Next, we can train the model normally. Lastly, we switch the model to eval mode, and we call the `convert()` function to truly quantize it.    ###### Tip    QAT doesn’t have to be used during all of training: you can take a pretrained model and just fine-tune it for a few epochs using QAT, typically using a lower learning rate to avoid damaging the pretrained weights.    We’ve seen how to implement PTQ and QAT using PyTorch’s `torch.ao` package. However, it’s primarily designed for CPUs. What if you want to run an LLM on a GPU that doesn’t quite have enough RAM? One option is to use the TorchAO library, which has growing GPU support. Another is to use the bitsandbytes library: let’s discuss it now.    ## Quantizing LLMs Using the bitsandbytes Library    The bitsandbytes library (bnb), created by Tim Dettmers, is designed to make it easier to train and run large models on GPUs with limited VRAM. For this, it offers:    *   Quantization tools, including 4-bit quantization, block-wise quantization, and more           *   Memory-efficient versions of popular optimizers such as Adam or AdamW, that operate on 8-bit tensors           *   Custom CUDA kernels written specifically for 8-bit or 4-bit quantized models, for maximum speed              ###### Warning    The bitsandbytes library is designed for Nvidia GPUs. It also has some limited support for CPUs and AMD GPUs.    For example, let’s see how to implement post-training static quantization down to 4 bits. If you are using Colab, you must first install the bitsandbytes library using `%pip install bitsandbytes`, then run this code:    ``` from transformers import AutoModelForCausalLM, BitsAndBytesConfig  model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",                                 bnb_4bit_compute_dtype=torch.bfloat16) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",                                              quantization_config=bnb_config) ```py    This code starts by importing the necessary classes from the Transformers library (introduced in [Chapter 14](ch14.html#nlp_chapter)), then it creates a `BitsAndBytesConfig` object, which I will explain shortly. Lastly, it downloads a pretrained model (in this case a 1.1 billion parameter version of Llama named TinyLlama, fine-tuned for chat), specifying the desired quantization configuration.    Under the hood, the Transformers library uses the bitsandbytes library to quantize the model weights down to 4 bits just as they are loaded into the GPU: no extra step is required. You can now use this model normally to generate text (see [Chapter 15](ch15.html#transformer_chapter)). During inference, whenever some weights are needed, they are dequantized on the fly to the type specified by the `bnb_4bit_compute_dtype` argument (`bfloat16` in this case), and the computations are performed in this higher precision. As soon as the dequantized weights are no longer needed, they are dropped, so memory usage remains low.    In this example, the `BitsAndBytesConfig` object specifies *4-bit Normal Float* (NF4) quantization using `bfloat16` for computations. NF4 is a nonlinear 4-bit scheme where each of the 16 possible integer values represents a specific float value between –1 and +1\\. Instead of being equally spaced (as in linear quantization), these values correspond to the quantiles of the normal distribution centered on zero: this means that they are closer together near zero. This improves accuracy because model weights tend to follow a normal distribution centered on zero, so having more precision near zero is helpful.    NF4 was introduced as part of [QLoRA](https://homl.info/qlora),⁠^([7](app02.html#id4445)) a technique that quantizes a frozen pretrained model with NF4, then uses LoRA adapters (see [Chapter 17](ch17.html#speedup_chapter)) for fine-tuning, along with activation checkpointing (see [Chapter 12](ch12.html#cnn_chapter)). This approach drastically reduces VRAM usage and compute: the authors managed to fine-tune a 65-billion parameter model using a single GPU with 48 GB of RAM, with only a small accuracy drop. Although activation checkpointing reduces VRAM usage overall, it can lead to memory spikes when processing batches with long sequences. To deal with such spikes, the QLoRA authors also introduced *paged optimizers* which take advantage of Nvidia unified memory: the CUDA driver automatically moves pages of data from GPU VRAM to CPU RAM whenever needed. Lastly, the authors also used *double quantization*, meaning that the quantization parameters themselves were quantized to save a bit more VRAM.    For more details on 4-bit quantization in the Hugging Face ecosystem, check out this [great post by the QLoRA authors and other contributors](https://huggingface.co/blog/4bit-transformers-bitsandbytes).```` ```py`` `````", "```` ```py ``# Using Pre-Quantized Models    Many popular pretrained models have already been quantized and published online, in particular on the Hugging Face Hub. For example, Tom Jobbins, better known by his Hugging Face username TheBloke, has published thousands of quantized models available at [*https://huggingface.co/TheBloke*](https://huggingface.co/TheBloke). Many of these models were quantized using one of the following modern methods:    *Generative pre-training quantization* (GPTQ)      [GPTQ](https://homl.info/gptq)⁠^([8](app02.html#id4454)) is a post-training quantization method, usually down to 4 bits, that treats quantization as an optimization problem. GPTQ goes through each layer, one by one, and optimizes the 4-bit weights to minimize the MSE between the layer’s original outputs (i.e., using the full precision weights) and the approximate outputs (i.e., using the 4-bit weights). Once the optimal 4-bit weights are found, the approximate outputs are passed to the next layer, and the process is repeated all the way to the output layer. During inference, the weights are dequantized whenever they are needed. GPTQ only quantizes the weights, not the activations: this is called *weight-only quantization*, which is great for inference, not for training. You can use the [Hugging Face Optimum library](https://huggingface.co/docs/optimum) or the [GPTQModel library](https://github.com/ModelCloud/GPTQModel) to quantize your models with GPTQ.      *Activation-aware Weight Quantization* (AWQ)      [AWQ](https://homl.info/awq)⁠^([9](app02.html#id4458)) aims to improve the accuracy of block-wise weight-only quantization (typically 4-bit quantization). The idea is to preserve the precision of the most important weights. To identify these so-called *salient weights*, the algorithm runs a calibration dataset through the model and finds the largest activations for each quantization group (e.g., the largest 0.1% to 1% activations), and the corresponding weights are considered salient. The authors observed that storing the salient weights using float16 greatly reduces the model’s *perplexity* (a common metric equal to the exponential of the cross-entropy). However, mixing 4-bit and 16-bit weights is not hardware-friendly, so AWQ uses another method to preserve the salient weight’s precision: they simply scale them up by some factor and add an operation in the model to scale down the corresponding activations (but this operation can generally be fused into the previous operation). Rather than using a fixed scaling factor, AWQ performs a search for the optimal factor, leading to the lowest quantization error. To implement AWQ, you can use the Hugging Face Optimum library.      Llama.cpp quantization using the *GPT-Generated Unified Format* (GGUF)      [GGUF](https://homl.info/gguf) is a binary file format designed to store LLMs efficiently. It was introduced by Georgi Gerganov, the creator of llama.cpp, and it supersedes previous file formats such as GGML, GGMF, and GGJT. A GGUF file includes the weights, the tokenizer, special tokens, the model architecture, the vocabulary size, and other metadata. Llama.cpp offers quantizers (e.g., using the `quantize` tool) to convert the model weights to one of GGUF’s supported quantized formats, such as Q4_K_M. Q4 stands for 4-bit quantization, K stands for per-block quantization (typically 32 or 64 weights per block depending on the chosen format), and M means medium size and precision for this quantization level (other options are S = Small and L = Large). There are also more recent and efficient quantization options such as Importance-aware Quantization (IQ), which uses various techniques to improve accuracy (e.g., nonlinear quantization), and Ternary Quantization (TQ).      ###### Note    On the Hugging Face Hub, every repository is backed by Git, so it has branches and commits. When you call `from_pretrained()`, the model is fetched from the default branch, which is almost always `main`. But quantized models are often placed in a different branch. When calling `from_pretrained()`, you can choose a branch, a tag, or even a commit hash, by using the `revision` argument. Check the model card for the list of available files and versions. For GGUF models, you must specify the filename using the `gguf_file` argument.    In conclusion, reduced precision, mixed-precision training, and quantization are arguably the most important tools to allow large models to run on limited hardware. But there are many more, including the following:    *   You could tweak the model’s architecture before training, by reducing the number of layers, or the number of neurons per layer, or by sharing weights across layers (e.g., as in the ALBERT model, introduced in [Chapter 15](ch15.html#transformer_chapter)).           *   If you have a large trained model, you can shrink it by removing some of its weights, for example the ones with the smallest magnitude, or the ones with the smallest effect on the loss. You can also remove whole channels, layers, or attention heads. This is called *model pruning*, and you can implement it using the `torch.nn.utils.prune` module, or the Hugging Face Optimum library.           *   As we saw in [Chapter 15](ch15.html#transformer_chapter), you can also use a large trained model as a teacher to train a smaller model: this is called distillation.           *   A trained model can also be shrunk by fusing some of its layers, removing redundancy. For example, a batch-norm layer (introduced in [Chapter 11](ch11.html#deep_chapter)) performs a linear operation, so if it comes immediately after a linear layer, you can fuse both layers into a single linear layer. Similarly, you can fuse a convolutional layer followed by a batch-norm layer into a single convolutional layer. This only works after training, since the batch-norm layer must compute running averages during training. You can implement layer fusion with the `torch.quantization.fuse_modules()` function, or with the Hugging Face Optimum library. In any case, make sure to fuse layers *before* quantizing your model: less layers means less quantization noise.           *   You can use low-rank approximations, where a large matrix is replaced by the product of two smaller ones. For example, replace a large linear layer such as `Linear(10_000, 20_000)` with two linear layers `Linear(10_000, 100)` and `Linear(100, 20_000)`. This reduces the number of parameters from about 200 million down to just three million, and also drastically reduces computations. The intermediate dimensionality (100 in this example) is a hyperparameter you can tune to balance accuracy and model size. This technique can be performed after training by factorizing the weight matrix using SVD (see the notebook for an example).              Give these techniques a try: shrink the models!    ###### Note    Chapter 17 and Appendices C, D, and E are available online at [*https://homl.info*](https://homl.info).    ^([1](app02.html#id4346-marker)) In general, –0 and +0 are considered equal, but some operations give different results, for example 1 / –0 = –infinity, while 1 / +0 = +infinity.    ^([2](app02.html#id4353-marker)) Some high-performance computing applications deactivate subnormal numbers because they slow down computations, and normalized numbers are generally sufficient (e.g., normalized fp32 can represent numbers as small as ±1.2e^(–38)).    ^([3](app02.html#id4362-marker)) The *M* stands for *mantissa*, which is a term often used as a synonym for fraction. Unfortunately, it’s also used as a synonym for significand, leading to some confusion. This is why IEEE 754 no longer uses the term mantissa.    ^([4](app02.html#id4388-marker)) P. Micikevicius et al., “Mixed Precision Training”, arXiv preprint 2017, ICLR (2018).    ^([5](app02.html#id4409-marker)) PyTorch implements *restricted symmetric quantization*, meaning that it excludes the lowest possible signed integer (e.g., –128 for 8-bit integers) to ensure that the range is symmetric (e.g., –127 to +127). Some other implementations allow the full signed byte range (from –128 to +127): this is called *unrestricted symmetric quantization*. These implementations also subtract 0.5 instead of 1 in the denominator of [Equation B-2](#symmetric_linear_quantization_equation).    ^([6](app02.html#id4433-marker)) Benoit Jacob et al., “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference”, arXiv preprint arXiv:1712.05877 (2017)”.    ^([7](app02.html#id4445-marker)) Tim Dettmers et al., “QLORA: Efficient Finetuning of Quantized LLMs”, arXiv preprint arXiv:2305.14314 (2023).    ^([8](app02.html#id4454-marker)) Elias Frantar et al., “GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers”, arXiv preprint arXiv:2210.17323 (2022).    ^([9](app02.html#id4458-marker)) Ji Lin et al., “AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration”, arXiv preprint arXiv:2306.00978 (2023).`` ``` ````"]