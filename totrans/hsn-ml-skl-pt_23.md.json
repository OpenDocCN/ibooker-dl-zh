["```py\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1))\n# [...] pretend the 32-bit model is trained here\nmodel.half()  # convert the model parameters to half precision (16 bits)\n```", "```py\nX = torch.rand(3, 10, dtype=torch.float16)  # some 16-bit input\ny_pred = model(X)  # 16-bit output\n```", "```py\nmodel = nn.Sequential(nn.Linear(10, 100, dtype=torch.float16), nn.ReLU(),\n                      nn.Linear(100, 1, dtype=torch.float16))\n```", "```py\nfrom torch.amp import GradScaler\n\ndef train_mpt(model, optimizer, criterion, train_loader, n_epochs,\n              dtype=torch.float16, init_scale=2.0**16):\n    grad_scaler = GradScaler(device=device, init_scale=init_scale)\n    model.train()\n    for epoch in range(n_epochs):\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            with torch.autocast(device_type=device, dtype=dtype):\n                y_pred = model(X_batch)\n                loss = criterion(y_pred, y_batch)\n            grad_scaler.scale(loss).backward()\n            grad_scaler.step(optimizer)\n            grad_scaler.update()\n            optimizer.zero_grad()\n```", "```py\n>>> w = torch.tensor([0.1, -0.1, 0.6, 0.0])  # 32-bit floats\n>>> s = (w.max() - w.min()) / 255.  # compute the scale\n>>> z = -(w.min() / s).round()  # compute the zero point\n>>> qw = torch.quantize_per_tensor(w, scale=s, zero_point=z, dtype=torch.quint8)\n>>> qw  # this is a quantized tensor internally represented using integers\ntensor([ 0.0988, -0.0988,  0.6012,  0.0000], size=(4,), dtype=torch.quint8,\n quantization_scheme=torch.per_tensor_affine, scale=0.002745098201557994,\n zero_point=36)\n>>> qw.dequantize()  # back to 32-bit floats (close to the original tensor)\ntensor([ 0.0988, -0.0988,  0.6012,  0.0000])\n```", "```py\n>>> w = torch.tensor([0.0, -0.94, 0.92, 0.93])  # 32-bit floats\n>>> s = w.abs().max() / 127.\n>>> qw = torch.quantize_per_tensor(w, scale=s, zero_point=0, dtype=torch.qint8)\n>>> qw\ntensor([ 0.0000, -0.9400,  0.9178,  0.9326], size=(4,), dtype=torch.qint8,\n quantization_scheme=torch.per_tensor_affine, scale=0.007401574868708849,\n zero_point=0)\n```", "```py\nimport platform\n\nmachine = platform.machine().lower()\nengine = \"qnnpack\" if (\"arm\" in machine or \"aarch64\" in machine) else \"x86\"\n```", "```py\nfrom torch.ao.quantization import quantize_dynamic\n\nmodel = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1))\n# [...] pretend the 32-bit model is trained here\ntorch.backends.quantized.engine = engine\nqmodel = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\nX = torch.randn(3, 10)\ny_pred = qmodel(X)  # float inputs and outputs, but quantized internally\n```", "```py\nfrom torch.ao.quantization import get_default_qconfig, QuantStub, DeQuantStub\n\nmodel = nn.Sequential(QuantStub(),\n                      nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1),\n                      DeQuantStub())\n# [...] pretend the 32-bit model is trained here\nmodel.qconfig = get_default_qconfig(engine)\ntorch.ao.quantization.prepare(model, inplace=True)\nfor X_batch, _ in calibration_loader:\n    model(X_batch)\ntorch.ao.quantization.convert(model, inplace=True)\n```", "```py\nfrom torch.ao.quantization import get_default_qat_qconfig\n\nmodel = nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 1))\nmodel.qconfig = get_default_qat_qconfig(engine)\ntorch.ao.quantization.prepare_qat(model, inplace=True)\ntrain(model, optimizer, [...])  # train the model normally\ntorch.ao.quantization.convert(model.eval(), inplace=True)\n```", "```py\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_compute_dtype=torch.bfloat16)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",\n                                             quantization_config=bnb_config)\n```"]