- en: Chapter 20\. Privacy, Security, and Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After working through the previous chapters in this book, you should hopefully
    be able to build an embedded application that relies on machine learning. You’ll
    still need to navigate a lot of challenges, though, to turn your project into
    a product that can be successfully deployed into the world. Two key challenges
    are protecting the privacy and the security of your users. This chapter covers
    some of the approaches we’ve found useful for overcoming those challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning on-device relies on sensor inputs. Some of these sensors, like
    microphones and cameras, [raise obvious privacy concerns](https://oreil.ly/CEcsR),
    but even others, like accelerometers, can be abused; for example, to identify
    individuals from their gait when wearing your product. We all have a responsibility
    as engineers to safeguard our users from damage that our products can cause, so
    it’s vital to think about privacy at all stages of the design. There are also
    legal implications to handling sensitive user data that are beyond the scope of
    our coverage but about which you should consult your lawyers. If you’re part of
    a large organization, you might have privacy specialists and processes that can
    help you with specialist knowledge. Even if you don’t have access to those resources,
    you should spend some time running your own privacy review at the outset of the
    project, and periodically revisit it until you launch. There isn’t widespread
    agreement on what a “privacy review” actually is, but we discuss some best practices,
    most of which revolve around building a strong Privacy Design Document (PDD).
  prefs: []
  type: TYPE_NORMAL
- en: The Privacy Design Document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field of [privacy engineering](https://oreil.ly/MEwUE) is still very new,
    and it can be difficult to find documentation on how to work through the privacy
    implications of a product. The way that many large companies handle the process
    of ensuring privacy in their applications is to create a Privacy Design Document.
    This is a single place where you can cover the important privacy aspects of your
    product. Your document should include information about all the topics raised
    in the subsections that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first section of the PDD should cover what data you’ll be gathering, how
    it will be gathered, and why. You should be as specific as possible and use plain
    English—for example, “collecting temperature and humidity” rather than “obtaining
    environmental atmospheric information.” While working on this section, you also
    have the opportunity to think about what you’re actually gathering, and ensure
    that it’s the minimum you need for your product. If you’re only listening for
    loud noises to wake up a more complex device, do you really need to sample audio
    at 16 KHz using a microphone, or can you use a cruder sensor that ensures you
    won’t be able to record speech even if there’s a security breach? A simple system
    diagram can be useful in this section, showing how the information flows between
    the different components in your product (including any cloud APIs). The overall
    goal of this section is to provide a good overview of what you’ll be collecting
    to a nontechnical audience, whether it’s your lawyers, executives, or board members.
    One way to think about it is how it would look on the front page of a newspaper,
    in a story written by an unsympathetic journalist. Make sure you’ve done everything
    you can to minimize your users’ exposure to malicious actions by others. In concrete
    terms, think through scenarios like “What could an abusive ex-partner do using
    this technology?” and try to be as imaginative as possible to ensure there’s as
    much protection built in as you can offer.
  prefs: []
  type: TYPE_NORMAL
- en: Data usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is done with any data after you’ve collected it? For example, many startups
    are tempted to leverage user data to train their machine learning models, but
    this is an extremely fraught process from a privacy perspective, because it requires
    storage and processing of potentially very sensitive information for long periods
    of time for only indirect user benefits. We strongly suggest treating training
    data acquisition as an entirely separate program, using paid providers with clear
    consent rather than collecting data as a side effect of product usage.
  prefs: []
  type: TYPE_NORMAL
- en: One of the benefits of on-device machine learning is that you have the ability
    to process sensitive data locally and share only aggregated results. For example,
    you might have a pedestrian-counting device that captures images every second,
    but the only data that’s transmitted is a count of people and vehicles seen. If
    you can, try to engineer your hardware to ensure that these guarantees can’t be
    broken. If you’re using only 224 × 224–pixel images as inputs to a classification
    algorithm, use a camera sensor that’s also low-resolution so that it’s physically
    impossible to recognize faces or license plates. If you plan on transmitting only
    a few values as a summary (like the pedestrian counts), support only a wireless
    technology with low bit rates to avoid being able to transmit the source video
    even if your device is hacked. We’re hoping that in the future, [special-purpose
    hardware](https://oreil.ly/6E2Ya) will help enforce these guarantees, but even
    now there’s still a lot you can do at the system design level to avoid overengineering
    and make abuse more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Data sharing and storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Who has access to the data you’ve gathered? What systems are in place to ensure
    that only those people can see it? How long is it kept, either on-device or in
    the cloud? If it is kept for any length of time, what are the policies on deleting
    it? You might think that storing information stripped of obvious user IDs like
    email addresses or names is safe, but identity can be derived from many sources,
    like IP addresses, recognizable voices, or even gaits, so you should assume that
    any sensor data you gather is personally identifiable information (PII). The best
    policy is to treat this kind of PII like radioactive waste. Avoid gathering it
    if you possibly can, keep it well guarded while you do need it, and dispose of
    it as quickly as possible after you’re done.
  prefs: []
  type: TYPE_NORMAL
- en: When you think about who has access, don’t forget that all your permission systems
    can be overridden by government pressure, which can cause your users serious harm
    in repressive countries. That’s another reason to limit what is transmitted and
    stored to the bare minimum possible, to avoid that responsibility and limit your
    users’ exposure.
  prefs: []
  type: TYPE_NORMAL
- en: Consent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Do the people using your product understand what information it’s gathering,
    and have they agreed to how you’ll use it? There’s a narrow legal question here
    that you might think can be answered by a click-through end-user license agreement,
    but we’d encourage you to think about this more broadly as a marketing challenge.
    Presumably you are convinced that the product benefits are worth the trade-off
    of gathering more data, so how can you communicate that to prospective customers
    clearly so that they make an informed choice? If you’re having trouble coming
    up with that message, that’s a sign you should rethink your design to reduce the
    privacy implications or increase the benefits of your product.
  prefs: []
  type: TYPE_NORMAL
- en: Using a PDD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should treat the PDD as a living document, updated constantly as your product
    evolves. It’s clearly useful for communicating product details to your lawyers
    and other business stakeholders, but it can also be useful in a lot of other contexts.
    For instance, you should collaborate with your marketing team to ensure that its
    messaging is informed by what you’re doing, and with any providers of third-party
    services (like advertising) to ensure they’re complying with what you’re promising.
    All of the engineers on the team should have access to it and be able to add comments,
    given that there might well be some hidden privacy implications that are visible
    only at the implementation level. For example, you might be using a geocoding
    cloud API that leaks the IP address of your device, or there might be a WiFi chip
    on your microcontroller that you’re not using but that could theoretically be
    enabled to transmit sensitive data.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensuring total security of an embedded device is very hard. An attacker can
    easily gain physical possession of a system, and then use all sorts of intrusive
    techniques to extract information. Your first line of defense is ensuring that
    as little sensitive information as possible is retained on your embedded system,
    which is why the PDD is so important. If you are relying on secure communications
    with a cloud service, you should think about investigating [secure cryptoprocessors](https://oreil.ly/lGLzA)
    to ensure that any keys are held safely. These chips can also be used for secure
    booting, to make sure only the program you’ve flashed will run on the device.
  prefs: []
  type: TYPE_NORMAL
- en: As with privacy, you should try to craft your hardware design to limit the opportunities
    for any attackers. If you don’t need WiFi or Bluetooth, build a device that doesn’t
    have those capabilities. [Don’t offer debug interfaces like SWD](https://oreil.ly/X1I7x)
    on shipping products, and look into [disabling code readout on Arm platforms](https://oreil.ly/ag5Vc).
    Even though these measures [aren’t perfect](https://oreil.ly/R3YG-), they will
    raise the cost of an attack.
  prefs: []
  type: TYPE_NORMAL
- en: You should also try to rely on established libraries and services for security
    and encryption. Rolling your own cryptography is a very bad idea, because it’s
    very easy to make mistakes that are difficult to spot but destroy the security
    of your system. The full challenge of embedded system security is beyond the scope
    of this book, but you should think about creating a security design document,
    similar to the one we recommend for privacy. You should cover what you think likely
    attacks are, their impacts, and how you’ll defend against them.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We often hear from engineers who are concerned about protecting their machine
    learning models from unscrupulous competitors, because they require a lot of work
    to create but are shipped on-device and are usually in an easy-to-understand format.
    The bad news is that there is no absolute protection against copying. In this
    sense, models are like any other software: they can be stolen and examined just
    like regular machine code. Like with software, though, the problem is not as bad
    as it might seem at first. Just as disassembling a procedural program doesn’t
    reveal the true source code, examining a quantized model doesn’t offer any access
    to the training algorithm or data, so attackers won’t be able to effectively modify
    the model for any other use. It should also be pretty easy to spot a direct copy
    of a model if it’s shipped on a competitor’s device and prove legally that the
    competitor stole your intellectual property, just as you can with any other software.'
  prefs: []
  type: TYPE_NORMAL
- en: It can still be worthwhile to make it harder for casual attackers to access
    your model. A simple technique is to store your serialized model in flash after
    XOR-ing it with a private key and then copy it into RAM and unencrypt it before
    use. That will prevent a simple dump of flash from revealing your model, but an
    attacker with access to RAM at runtime will still be able to access it. You might
    think that switching away from a TensorFlow Lite FlatBuffer to a proprietary format
    would help, but because the weight parameters themselves are large arrays of numerical
    values and it’s obvious from stepping through a debugger what operations are called
    in which order, we’ve found the value of this kind of obfuscation very limited.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One fun approach to use for spotting misappropriation of models is deliberately
    building in subtle flaws as part of the training process, and then looking out
    for them when checking suspected infringements. As an example, you could train
    a wake-word detection model to not only listen out for “Hello,” but also secretly
    “Ahoy, sailor!” It’s extremely unlikely that an independently trained model would
    show a response for that same phrase, so if there is one, it’s a strong signal
    that the model was copied, even if you can’t access the internal workings of a
    device. This technique is based on the old idea of including a fictitious entry
    in reference works such as maps, directories, and dictionaries to help spot copyright
    infringements; it has come to be known as *mountweazeling* after the practice
    of placing [a fictitious mountain](https://oreil.ly/OpY2G), “Mountweazel,” on
    maps to help identify copies.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With modern microcontrollers it’s very tempting to enable over-the-air updates
    so you have the ability to revise the code that’s running on your device at any
    time, even long after shipping. This opens up such a wide attack surface for security
    and privacy violations that we urge you to consider whether it is truly essential
    for your product. It’s difficult to ensure that only you have the ability to upload
    new code without a well-designed secure booting system and other protections,
    and if you make a mistake, you’ve handed complete control of your device to malicious
    actors. As a default, we recommend that you don’t allow any kind of code updating
    after a device has been manufactured. This might sound draconian, given that it
    prevents updates that fix security holes, for example, but in almost all cases
    removing the possibility of attackers’ code being run on the system will help
    security much more than it hurts. It also simplifies the network architecture,
    because there’s no longer a need for any protocol to “listen” for updates; the
    device might effectively be able to operate in a transmit-only mode, which also
    greatly reduces the attack surface.
  prefs: []
  type: TYPE_NORMAL
- en: This does mean that there’s much more of a burden on you to get the code right
    before a device is released, especially with regard to the model accuracy. We
    talked earlier about approaches like unit tests and verifying overall model accuracy
    against a dedicated test set, but they won’t catch all problems. When you’re preparing
    for a release, we highly recommend using a dog-fooding approach in which you try
    the devices in real-world environments, but under the supervision of organization
    insiders. These experiments are a lot more likely to reveal unexpected behaviors
    than engineering tests, because tests are limited by the imagination of their
    creators, and the real world is much more surprising than any of us can predict
    ahead of time. The good news is that after you have encountered undesirable behaviors,
    you can then turn them into test cases that can be tackled as part of your normal
    development process. In fact, developing this kind of institutional memory of
    the deep requirements of your product, codified into tests, can be one of your
    biggest competitive advantages, in so much as the only way to acquire it is by
    painful trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from a Development Board to a Product
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The full process of turning an application running on a development board into
    a shipping product is beyond the scope of this book, but there are some things
    worth thinking about during the development process. You should research the bulk
    prices of the microcontroller you’re considering using—for example, on sites like
    [Digi-Key](https://digikey.com)—to make sure that the system you’re targeting
    will fit your budget in the end. It should be fairly straightforward to move your
    code to a production device assuming that it’s the same chip you were using during
    development, so from a programming perspective, the main imperative is to ensure
    that your development board matches your production target. Debugging any issues
    that arise will become a lot harder after your code is deployed in a final form
    factor, especially if you’ve taken the steps described earlier to secure your
    platform, so it’s worth delaying that step as long as you can.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Safeguarding our users’ privacy and security is one of our most important responsibilities
    as engineers, but it’s not always clear how to decide on the best approaches.
    In this chapter, we covered the basic process of thinking about and designing
    in protections, and some more advanced security considerations. With that, we’ve
    completed the foundations of building and deploying an embedded machine learning
    application, but we know that there’s far more to this area than we could cover
    in a single book. To finish off, the final chapter discusses resources that you
    can use to continue learning more.
  prefs: []
  type: TYPE_NORMAL
