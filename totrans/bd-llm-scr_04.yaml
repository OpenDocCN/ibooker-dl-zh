- en: 5 Pretraining on unlabeled data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 在未标记数据上的预训练
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Computing the training and validation set losses to assess the quality of LLM-generated
    text during training
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算训练和验证集损失以评估训练过程中LLM生成文本的质量
- en: Implementing a training function and pretraining the LLM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现训练函数和预训练LLM
- en: Saving and loading model weights to continue training an LLM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载模型权重以继续训练LLM
- en: Loading pretrained weights from OpenAI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从OpenAI加载预训练权重
- en: Thus far, we have implemented the data sampling and attention mechanism and
    coded the LLM architecture. It is now time to implement a training function and
    pretrain the LLM. We will learn about basic model evaluation techniques to measure
    the quality of the generated text, which is a requirement for optimizing the LLM
    during the training process. Moreover, we will discuss how to load pretrained
    weights, giving our LLM a solid starting point for fine-tuning. Figure 5.1 lays
    out our overall plan, highlighting what we will discuss in this chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了数据采样和注意力机制，并编写了LLM架构的代码。现在是时候实现训练函数并预训练LLM了。我们将学习基本模型评估技术来衡量生成文本的质量，这是在训练过程中优化LLM的要求。此外，我们将讨论如何加载预训练权重，为我们的LLM提供一个坚实的微调起点。图5.1概述了我们的整体计划，突出了本章将讨论的内容。
- en: '![figure](../Images/5-1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-1.png)'
- en: 'Figure 5.1 The three main stages of coding an LLM. This chapter focuses on
    stage 2: pretraining the LLM (step 4), which includes implementing the training
    code (step 5), evaluating the performance (step 6), and saving and loading model
    weights (step 7).'
  id: totrans-8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 编码LLM的三个主要阶段。本章重点介绍第2阶段：预训练LLM（步骤4），包括实现训练代码（步骤5）、评估性能（步骤6）以及保存和加载模型权重（步骤7）。
- en: Weight parameters
  id: totrans-9
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重参数
- en: In the context of LLMs and other deep learning models, *weights* refer to the
    trainable parameters that the learning process adjusts. These weights are also
    known as *weight parameters* or simply *parameters*. In frameworks like PyTorch,
    these weights are stored in linear layers; we used these to implement the multi-head
    attention module in chapter 3 and the `GPTModel` in chapter 4\. After initializing
    a layer (`new_layer` `=` `torch.nn.Linear(...)`), we can access its weights through
    the `.weight` attribute, `new_layer.weight`. Additionally, for convenience, PyTorch
    allows direct access to all a model’s trainable parameters, including weights
    and biases, through the method `model.parameters()`, which we will use later when
    implementing the model training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM和其他深度学习模型的背景下，*权重*指的是学习过程调整的可训练参数。这些权重也被称为*权重参数*或简单地称为*参数*。在PyTorch等框架中，这些权重存储在线性层中；我们在第3章中使用了这些来实现多头注意力模块，在第4章中实现了`GPTModel`。初始化一个层（`new_layer`
    `=` `torch.nn.Linear(...)`）后，我们可以通过`.weight`属性访问其权重，即`new_layer.weight`。此外，为了方便起见，PyTorch允许通过方法`model.parameters()`直接访问模型的所有可训练参数，包括权重和偏差，我们将在实现模型训练时使用此方法。
- en: 5.1 Evaluating generative text models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 评估生成文本模型
- en: After briefly recapping the text generation from chapter 4, we will set up our
    LLM for text generation and then discuss basic ways to evaluate the quality of
    the generated text. We will then calculate the training and validation losses.
    Figure 5.2 shows the topics covered in this chapter, with these first three steps
    highlighted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要回顾了第4章中的文本生成后，我们将设置我们的LLM进行文本生成，然后讨论评估生成文本质量的基本方法。然后我们将计算训练和验证损失。图5.2展示了本章涵盖的主题，其中前三个步骤被突出显示。
- en: '![figure](../Images/5-2.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-2.png)'
- en: Figure 5.2 An overview of the topics covered in this chapter. We begin by recapping
    text generation (step 1) before moving on to discuss basic model evaluation techniques
    (step 2) and training and validation losses (step 3).
  id: totrans-14
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2本章涵盖主题的概述。我们首先回顾文本生成（步骤1），然后继续讨论基本模型评估技术（步骤2）和训练与验证损失（步骤3）。
- en: 5.1.1 Using GPT to generate text
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 使用GPT生成文本
- en: 'Let’s set up the LLM and briefly recap the text generation process we implemented
    in chapter 4\. We begin by initializing the GPT model that we will later evaluate
    and train using the `GPTModel` class and `GPT_CONFIG_124M` dictionary (see chapter
    4):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置LLM并简要回顾我们在第4章中实现的文本生成过程。我们首先通过`GPTModel`类和`GPT_CONFIG_124M`字典（见第4章）初始化我们将要评估和训练的GPT模型：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 We shorten the context length from 1,024 to 256 tokens.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 我们将上下文长度从1,024缩短到256个标记。'
- en: '#2 It’s possible and common to set dropout to 0.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将dropout设置为0是可能且常见的。'
- en: Considering the `GPT_CONFIG_124M` dictionary, the only adjustment we have made
    compared to the previous chapter is that we have reduced the context length (`context_
    length`) to 256 tokens. This modification reduces the computational demands of
    training the model, making it possible to carry out the training on a standard
    laptop computer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到`GPT_CONFIG_124M`字典，与我们上一章相比，我们唯一做出的调整是将上下文长度（`context_length`）减少到256个token。这种修改降低了训练模型的计算需求，使得在标准笔记本电脑上执行训练成为可能。
- en: Originally, the GPT-2 model with 124 million parameters was configured to handle
    up to 1,024 tokens. After the training process, we will update the context size
    setting and load pretrained weights to work with a model configured for a 1,024-token
    context length.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GPT-2模型有1240万个参数，配置为处理最多1,024个token。在训练过程之后，我们将更新上下文大小设置并加载预训练的权重，以便与配置为1,024个token上下文长度的模型一起工作。
- en: 'Using the `GPTModel` instance, we adopt the `generate_text_simple` function
    from chapter 4 and introduce two handy functions: `text_to_token_` `ids` and `token_ids_
    to_text`. These functions facilitate the conversion between text and token representations,
    a technique we will utilize throughout this chapter.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`GPTModel`实例，我们采用第4章中的`generate_text_simple`函数，并引入两个实用的函数：`text_to_token_ids`和`token_ids_to_text`。这些函数便于在文本和token表示之间进行转换，这是我们将在本章中利用的技术。
- en: '![figure](../Images/5-3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-3.png)'
- en: Figure 5.3 Generating text involves encoding text into token IDs that the LLM
    processes into logit vectors. The logit vectors are then converted back into token
    IDs, detokenized into a text representation.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3 文本生成涉及将文本编码成LLM处理的token ID，然后将logit向量转换回token ID，并反序列化为文本表示。
- en: Figure 5.3 illustrates a three-step text generation process using a GPT model.
    First, the tokenizer converts input text into a series of token IDs (see chapter
    2). Second, the model receives these token IDs and generates corresponding logits,
    which are vectors representing the probability distribution for each token in
    the vocabulary (see chapter 4). Third, these logits are converted back into token
    IDs, which the tokenizer decodes into human-readable text, completing the cycle
    from textual input to textual output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3展示了使用GPT模型的三步文本生成过程。首先，分词器将输入文本转换为一系列token ID（参见第2章）。其次，模型接收这些token ID并生成相应的logit，这些logit是表示词汇表中每个token概率分布的向量（参见第4章）。第三，这些logit被转换回token
    ID，分词器将其解码为可读文本，从而完成从文本输入到文本输出的循环。
- en: We can implement the text generation process, as shown in the following listing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以实施如以下列表所示的文本生成过程。
- en: Listing 5.1 Utility functions for text to token ID conversion
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1 文本到token ID转换的实用函数
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 .unsqueeze(0) adds the batch dimension'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 .unsqueeze(0) 添加批处理维度'
- en: '#2 Removes batch dimension'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 移除批处理维度'
- en: 'Using this code, the `model` generates the following text:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，`model`生成了以下文本：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Clearly, the model isn’t yet producing coherent text because it hasn’t undergone
    training. To define what makes text “coherent” or “high quality,” we have to implement
    a numerical method to evaluate the generated content. This approach will enable
    us to monitor and enhance the model’s performance throughout its training process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型还没有生成连贯的文本，因为它还没有经过训练。为了定义什么使文本“连贯”或“高质量”，我们必须实现一个数值方法来评估生成的内容。这种方法将使我们能够在整个训练过程中监控和提升模型的表现。
- en: Next, we will calculate a *loss metric* for the generated outputs. This loss
    serves as a progress and success indicator of the training progress. Furthermore,
    in later chapters, when we fine-tune our LLM, we will review additional methodologies
    for assessing model quality.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算生成输出的*损失指标*。这个损失作为训练进度和成功的指标。此外，在后面的章节中，当我们微调我们的LLM时，我们将回顾评估模型质量的额外方法。
- en: 5.1.2 Calculating the text generation loss
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 计算文本生成损失
- en: Next, let’s explore techniques for numerically assessing text quality generated
    during training by calculating a *text generation loss*. We will go over this
    topic step by step with a practical example to make the concepts clear and applicable,
    beginning with a short recap of how the data is loaded and how the text is generated
    via the `generate_text_simple` function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过计算*文本生成损失*来探索在训练过程中评估文本质量的技术。我们将通过一个实际例子逐步讲解这个主题，以使概念清晰并具有实用性，首先简要回顾如何通过`generate_text_simple`函数加载数据和生成文本。
- en: Figure 5.4 illustrates the overall flow from input text to LLM-generated text
    using a five-step procedure. This text-generation process shows what the `generate_text_simple`
    function does internally. We need to perform these same initial steps before we
    can compute a loss that measures the generated text quality later in this section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4展示了从输入文本到LLM生成文本的整体流程，使用五步程序。这个文本生成过程显示了`generate_text_simple`函数内部执行的操作。在我们能够计算衡量生成文本质量的损失之前，我们需要执行这些相同的初始步骤。
- en: '![figure](../Images/5-4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-4.png)'
- en: Figure 5.4 For each of the three input tokens, shown on the left, we compute
    a vector containing probability scores corresponding to each token in the vocabulary.
    The index position of the highest probability score in each vector represents
    the most likely next token ID. These token IDs associated with the highest probability
    scores are selected and mapped back into a text that represents the text generated
    by the model.
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4 对于左侧显示的每个三个输入标记，我们计算一个包含与词汇表中每个标记对应的概率分数的向量。每个向量中最高概率分数的索引位置代表最可能的下一个标记ID。与最高概率分数关联的这些标记ID被选中并映射回表示模型生成的文本的文本。
- en: Figure 5.4 outlines the text generation process with a small seven-token vocabulary
    to fit this image on a single page. However, our `GPTModel` works with a much
    larger vocabulary consisting of 50,257 words; hence, the token IDs in the following
    code will range from 0 to 50,256 rather than 0 to 6\.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4概述了使用小七标记词汇表来适应单页的文本生成过程。然而，我们的`GPTModel`使用一个包含50,257个单词的更大词汇表；因此，以下代码中的标记ID将范围从0到50,256，而不是0到6。
- en: Also, figure 5.4 only shows a single text example (`"every` `effort` `moves"`)
    for simplicity. In the following hands-on code example that implements the steps
    in the figure, we will work with two input examples for the GPT model (`"every`
    `effort` `moves"` and `"I` `really` `like"`).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图5.4仅为了简化展示了单个文本示例（`"every` `effort` `moves"`）。在以下实现图中步骤的动手代码示例中，我们将使用两个GPT模型的输入示例（`"every`
    `effort` `moves"`和`"I` `really` `like"`）。
- en: 'Consider these two input examples, which have already been mapped to token
    IDs (figure 5.4, step 1):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这两个已经映射到标记ID的输入示例（图5.4，步骤1）：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Matching these inputs, the `targets` contain the token IDs we want the model
    to produce:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些输入匹配，`targets`包含我们希望模型生成的标记ID：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the targets are the inputs but shifted one position forward, a concept
    we covered in chapter 2 during the implementation of the data loader. This shifting
    strategy is crucial for teaching the model to predict the next token in a sequence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，目标输入是但向前移动了一个位置，这是我们在第2章实现数据加载器时讨论的概念。这种移动策略对于教会模型预测序列中的下一个标记至关重要。
- en: 'Now we feed the inputs into the model to calculate logits vectors for the two
    input examples, each comprising three tokens. Then we apply the `softmax` function
    to transform these logits into probability scores (`probas`; figure 5.4, step
    2):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将输入输入到模型中，为两个输入示例计算logits向量，每个示例包含三个标记。然后我们应用`softmax`函数将这些logits转换为概率分数（`probas`；图5.4，步骤2）：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Disables gradient tracking since we are not training yet'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 禁用梯度跟踪，因为我们还没有开始训练'
- en: '#2 Probability of each token in vocabulary'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 词汇表中每个标记的概率'
- en: The resulting tensor dimension of the probability score (`probas`) tensor is
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分数张量（`probas`）的结果维度是
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first number, 2, corresponds to the two examples (rows) in the inputs, also
    known as batch size. The second number, 3, corresponds to the number of tokens
    in each input (row). Finally, the last number corresponds to the embedding dimensionality,
    which is determined by the vocabulary size. Following the conversion from logits
    to probabilities via the `softmax` function, the `generate_text_simple` function
    then converts the resulting probability scores back into text (figure 5.4, steps
    3–5).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数字，2，对应于输入中的两个示例（行），也称为批量大小。第二个数字，3，对应于每个输入（行）中的标记数量。最后，最后一个数字对应于嵌入维度性，它由词汇表大小决定。通过`softmax`函数将logits转换为概率后，`generate_text_simple`函数随后将结果概率分数转换回文本（图5.4，步骤3–5）。
- en: 'We can complete steps 3 and 4 by applying the `argmax` function to the probability
    scores to obtain the corresponding token IDs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对概率分数应用`argmax`函数来完成步骤3和4，以获得相应的标记ID：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Given that we have two input batches, each containing three tokens, applying
    the `argmax` function to the probability scores (figure 5.4, step 3) yields two
    sets of outputs, each with three predicted token IDs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有两个输入批次，每个批次包含三个标记，将`argmax`函数应用于概率分数（图5.4，步骤3）会产生两组输出，每组有三个预测标记ID：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 First batch'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第一个批次'
- en: '#2 Second batch'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第二个批次'
- en: 'Finally, step 5 converts the token IDs back into text:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，步骤5将标记ID转换回文本：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When we decode these tokens, we find that these output tokens are quite different
    from the target tokens we want the model to generate:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们解码这些标记时，我们发现这些输出标记与我们希望模型生成的目标标记相当不同：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The model produces random text that is different from the target text because
    it has not been trained yet. We now want to evaluate the performance of the model’s
    generated text numerically via a loss (figure 5.5). Not only is this useful for
    measuring the quality of the generated text, but it’s also a building block for
    implementing the training function, which we will use to update the model’s weight
    to improve the generated text.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型尚未经过训练，它产生的文本与目标文本不同。我们现在想通过损失（图5.5）来数值评估模型生成文本的性能。这不仅有助于衡量生成文本的质量，也是实现训练函数的基石，我们将使用它来更新模型的权重，以改进生成的文本。
- en: '![figure](../Images/5-5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-5.png)'
- en: Figure 5.5 An overview of the topics covered in this chapter. We have completed
    step 1\. We are now ready to implement the text evaluation function (step 2).
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 本章节涵盖的主题概述。我们已经完成了步骤1。我们现在准备实现文本评估函数（步骤2）。
- en: Part of the text evaluation process that we implement, as shown in figure 5.5,
    is to measure “how far” the generated tokens are from the correct predictions
    (targets). The training function we implement later will use this information
    to adjust the model weights to generate text that is more similar to (or, ideally,
    matches) the target text.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实施的部分文本评估过程，如图5.5所示，是测量生成的标记与正确预测（目标）之间的“距离”。我们稍后实施的训练函数将使用这些信息来调整模型权重，以生成更接近（或理想情况下匹配）目标文本的文本。
- en: 'The model training aims to increase the softmax probability in the index positions
    corresponding to the correct target token IDs, as illustrated in figure 5.6\.
    This softmax probability is also used in the evaluation metric we will implement
    next to numerically assess the model’s generated outputs: the higher the probability
    in the correct positions, the better.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的目标是增加与正确目标标记ID对应的softmax概率，如图5.6所示。此softmax概率也用于我们将在下一节实施的评估指标中，以数值评估模型的生成输出：正确位置的概率越高，越好。
- en: '![figure](../Images/5-6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-6.png)'
- en: Figure 5.6 Before training, the model produces random next-token probability
    vectors. The goal of model training is to ensure that the probability values corresponding
    to the highlighted target token IDs are maximized.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6 在训练之前，模型产生随机的下一个标记概率向量。模型训练的目标是确保与突出显示的目标标记ID对应的概率值最大化。
- en: Remember that figure 5.6 displays the softmax probabilities for a compact seven-token
    vocabulary to fit everything into a single figure. This implies that the starting
    random values will hover around 1/7, which equals approximately 0.14\. However,
    the vocabulary we are using for our GPT-2 model has 50,257 tokens, so most of
    the initial probabilities will hover around 0.00002 (1/50,257).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，图5.6显示了紧凑的七个标记词汇表的softmax概率，以便将所有内容放入单个图中。这表明起始的随机值将围绕1/7，即大约0.14。然而，我们用于我们的GPT-2模型的词汇表有50,257个标记，所以大部分初始概率将围绕0.00002（1/50,257）。
- en: 'For each of the two input texts, we can print the initial softmax probability
    scores corresponding to the target tokens using the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个输入文本中的每一个，我们可以使用以下代码打印出对应于目标标记的初始softmax概率分数：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The three target token ID probabilities for each batch are
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个批次的目标标记ID概率有三个
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The goal of training an LLM is to maximize the likelihood of the correct token,
    which involves increasing its probability relative to other tokens. This way,
    we ensure the LLM consistently picks the target token—essentially the next word
    in the sentence—as the next token it generates.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个大型语言模型（LLM）的目标是最大化正确标记的可能性，这涉及到增加其相对于其他标记的概率。这样，我们确保LLM始终选择目标标记——本质上句子的下一个单词——作为它生成的下一个标记。
- en: Backpropagation
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反向传播
- en: How do we maximize the softmax probability values corresponding to the target
    tokens? The big picture is that we update the model weights so that the model
    outputs higher values for the respective token IDs we want to generate. The weight
    update is done via a process called *backpropagation*, a standard technique for
    training deep neural networks (see sections A.3 to A.7 in appendix A for more
    details about backpropagation and model training).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何最大化对应于目标标记的softmax概率值？总体来说，我们更新模型权重，使得模型对于我们想要生成的相应标记ID输出更高的值。权重更新是通过称为*反向传播*的过程完成的，这是一种训练深度神经网络的标准化技术（有关反向传播和模型训练的更多详细信息，请参阅附录A中的A.3到A.7节）。
- en: Backpropagation requires a loss function, which calculates the difference between
    the model’s predicted output (here, the probabilities corresponding to the target
    token IDs) and the actual desired output. This loss function measures how far
    off the model’s predictions are from the target values.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播需要一个损失函数，该函数计算模型预测输出（在这里，是针对目标标记ID的概率）与实际期望输出之间的差异。这个损失函数衡量模型的预测与目标值之间的偏差程度。
- en: 'Next, we will calculate the loss for the probability scores of the two example
    batches, `target_probas_1` and `target_probas_2`. The main steps are illustrated
    in figure 5.7\. Since we already applied steps 1 to 3 to obtain `target_probas_1`
    and `target_ probas_2`, we proceed with step 4, applying the *logarithm* to the
    probability scores:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算两个示例批次`target_probas_1`和`target_probas_2`的概率得分的损失。主要步骤如图5.7所示。由于我们已经对步骤1到3进行了处理以获得`target_probas_1`和`target_probas_2`，我们继续进行步骤4，对概率得分应用*对数*：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![figure](../Images/5-7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-7.png)'
- en: Figure 5.7 Calculating the loss involves several steps. Steps 1 to 3, which
    we have already completed, calculate the token probabilities corresponding to
    the target tensors. These probabilities are then transformed via a logarithm and
    averaged in steps 4 to 6.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7 计算损失涉及几个步骤。步骤1到3，我们已经完成，计算了与目标张量对应的标记概率。然后，在步骤4到6中，这些概率通过对数变换并平均。
- en: 'This results in the following values:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下值：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Working with logarithms of probability scores is more manageable in mathematical
    optimization than handling the scores directly. This topic is outside the scope
    of this book, but I’ve detailed it further in a lecture, which can be found in
    appendix B.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学优化中处理概率得数的对数比直接处理得分更容易管理。这个主题超出了本书的范围，但我已经在附录B中的讲座中进一步详细说明了这一点。
- en: 'Next, we combine these log probabilities into a single score by computing the
    average (step 5 in figure 5.7):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这些对数概率合并成一个单一得分，通过计算平均值（图5.7中的步骤5）：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The resulting average log probability score is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的平均对数概率得分是
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The goal is to get the average log probability as close to 0 as possible by
    updating the model’s weights as part of the training process. However, in deep
    learning, the common practice isn’t to push the average log probability up to
    0 but rather to bring the negative average log probability down to 0\. The negative
    average log probability is simply the average log probability multiplied by –1,
    which corresponds to step 6 in figure 5.7:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过对模型权重进行更新作为训练过程的一部分，将平均对数概率尽可能接近0。然而，在深度学习中，常见的做法不是将平均对数概率推到0，而是将负的平均对数概率降低到0。负的平均对数概率仅仅是平均对数概率乘以-1，这对应于图5.7中的步骤6：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This prints `tensor(10.7940)`. In deep learning, the term for turning this negative
    value, –10.7940, into 10.7940, is known as the *cross entropy* loss. PyTorch comes
    in handy here, as it already has a built-in `cross_entropy` function that takes
    care of all these six steps in figure 5.7 for us.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印出`tensor(10.7940)`。在深度学习中，将这个负值-10.7940转换为10.7940的术语被称为*交叉熵*损失。PyTorch在这里很有用，因为它已经内置了一个`cross_entropy`函数，可以为我们处理图5.7中的所有这六个步骤。
- en: Cross entropy loss
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: At its core, the cross entropy loss is a popular measure in machine learning
    and deep learning that measures the difference between two probability distributions—typically,
    the true distribution of labels (here, tokens in a dataset) and the predicted
    distribution from a model (for instance, the token probabilities generated by
    an LLM).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，交叉熵损失是机器学习和深度学习中一种流行的度量方法，它衡量两个概率分布之间的差异——通常，是标签的真实分布（在这里，是数据集中的标记）和模型预测的分布（例如，由大型语言模型生成的标记概率）。
- en: In the context of machine learning and specifically in frameworks like PyTorch,
    the `cross_entropy` function computes this measure for discrete outcomes, which
    is similar to the negative average log probability of the target tokens given
    the model’s generated token probabilities, making the terms “cross entropy” and
    “negative average log probability” related and often used interchangeably in practice.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，特别是在像PyTorch这样的框架中，`cross_entropy`函数计算离散结果这一度量，这与给定模型生成的标记概率的目标标记的负平均对数概率相似，使得“交叉熵”和“负平均对数概率”这两个术语在实践中相关且经常互换使用。
- en: 'Before we apply the `cross_entropy` function, let’s briefly recall the shape
    of the logits and target tensors:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们应用`cross_entropy`函数之前，让我们简要回顾一下logits和目标张量的形状：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The resulting shapes are
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 结果形状是
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we can see, the `logits` tensor has three dimensions: batch size, number
    of tokens, and vocabulary size. The `targets` tensor has two dimensions: batch
    size and number of tokens.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`logits`张量有三个维度：批次大小、标记数量和词汇表大小。`targets`张量有两个维度：批次大小和标记数量。
- en: 'For the `cross_entropy` loss function in PyTorch, we want to flatten these
    tensors by combining them over the batch dimension:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PyTorch中的`cross_entropy`损失函数，我们希望通过组合批次维度来展平这些张量：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The resulting tensor dimensions are
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结果张量维度是
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Remember that the `targets` are the token IDs we want the LLM to generate, and
    the `logits` contain the unscaled model outputs before they enter the `softmax`
    function to obtain the probability scores.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`targets`是我们希望LLM生成的标记ID，而`logits`包含在进入`softmax`函数以获得概率分数之前的未缩放模型输出。
- en: 'Previously, we applied the `softmax` function, selected the probability scores
    corresponding to the target IDs, and computed the negative average log probabilities.
    PyTorch’s `cross_entropy` function will take care of all these steps for us:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们应用了`softmax`函数，选择了对应于目标ID的概率分数，并计算了负平均对数概率。PyTorch的`cross_entropy`函数将为我们处理所有这些步骤：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting loss is the same that we obtained previously when applying the
    individual steps in figure 5.7 manually:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 结果损失与我们在手动应用图5.7中的各个步骤时获得的结果相同：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Perplexity
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混淆度
- en: '*Perplexity* is a measure often used alongside cross entropy loss to evaluate
    the performance of models in tasks like language modeling. It can provide a more
    interpretable way to understand the uncertainty of a model in predicting the next
    token in a sequence.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*混淆度*是常与交叉熵损失一起使用来评估模型在语言建模等任务中性能的度量。它可以提供一种更可解释的方式来理解模型在预测序列中下一个标记时的不确定性。'
- en: Perplexity measures how well the probability distribution predicted by the model
    matches the actual distribution of the words in the dataset. Similar to the loss,
    a lower perplexity indicates that the model predictions are closer to the actual
    distribution.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆度衡量模型预测的概率分布与数据集中单词的实际分布之间的匹配程度。与损失类似，较低的混淆度表明模型预测更接近实际分布。
- en: Perplexity can be calculated as `perplexity` `=` `torch.exp(loss)`, which returns
    `tensor(48725.8203)` when applied to the previously calculated loss.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆度可以计算为`perplexity` `=` `torch.exp(loss)`，当应用于之前计算的损失时，返回`tensor(48725.8203)`。
- en: Perplexity is often considered more interpretable than the raw loss value because
    it signifies the effective vocabulary size about which the model is uncertain
    at each step. In the given example, this would translate to the model being unsure
    about which among 48,725 tokens in the vocabulary to generate as the next token.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆度通常被认为比原始损失值更可解释，因为它表示模型在每一步对有效词汇表大小的不确定性。在给定示例中，这相当于模型不确定在词汇表中的48,725个标记中应该生成哪个作为下一个标记。
- en: We have now calculated the loss for two small text inputs for illustration purposes.
    Next, we will apply the loss computation to the entire training and validation
    sets.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已计算了两个小型文本输入的损失以供说明。接下来，我们将应用损失计算到整个训练和验证集。
- en: 5.1.3 Calculating the training and validation set losses
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 计算训练和验证集损失
- en: We must first prepare the training and validation datasets that we will use
    to train the LLM. Then, as highlighted in figure 5.8, we will calculate the cross
    entropy for the training and validation sets, which is an important component
    of the model training process.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须首先准备我们将用于训练LLM的训练和验证数据集。然后，如图5.8所示，我们将计算训练和验证集的交叉熵，这是模型训练过程中的一个重要组成部分。
- en: '![figure](../Images/5-8.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-8.png)'
- en: Figure 5.8 Having completed steps 1 and 2, including computing the cross entropy
    loss, we can now apply this loss computation to the entire text dataset that we
    will use for model training.
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 完成步骤 1 和 2，包括计算交叉熵损失后，我们现在可以将这种损失计算应用于整个用于模型训练的文本数据集。
- en: To compute the loss on the training and validation datasets, we use a very small
    text dataset, the “The Verdict” short story by Edith Wharton, which we have already
    worked with in chapter 2\. By selecting a text from the public domain, we circumvent
    any concerns related to usage rights. Additionally, using such a small dataset
    allows for the execution of code examples on a standard laptop computer in a matter
    of minutes, even without a high-end GPU, which is particularly advantageous for
    educational purposes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练集和验证集上计算损失，我们使用一个非常小的文本数据集，即 Edith Wharton 的短篇小说“The Verdict”，我们在第 2 章中已经使用过它。通过选择公共领域的文本，我们绕过了任何与使用权利相关的担忧。此外，使用如此小的数据集允许在标准笔记本电脑上几分钟内执行代码示例，即使没有高端
    GPU，这对于教育目的尤其有利。
- en: Note  Interested readers can also use the supplementary code for this book to
    prepare a larger-scale dataset consisting of more than 60,000 public domain books
    from Project Gutenberg and train an LLM on these (see appendix D for details).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对本书感兴趣的读者也可以使用本书的补充代码来准备一个更大规模的由 Project Gutenberg 的 60,000 多本公共领域书籍组成的数据库，并在这些数据上训练
    LLM（详细信息请见附录 D）。
- en: The cost of pretraining LLMs
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 预训练大型语言模型（LLM）的成本
- en: To put the scale of our project into perspective, consider the training of the
    7 billion parameter Llama 2 model, a relatively popular openly available LLM.
    This model required 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion
    tokens. At the time of writing, running an 8 × A100 cloud server on AWS costs
    around $30 per hour. A rough estimate puts the total training cost of such an
    LLM at around $690,000 (calculated as 184,320 hours divided by 8, then multiplied
    by $30).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们项目的规模放在正确的视角中，考虑一下训练 700 亿参数的 Llama 2 模型，这是一个相对流行的公开可用的 LLM。这个模型在昂贵的 A100
    GPU 上需要 184,320 个 GPU 小时，处理了 2 万亿个标记。在撰写本文时，在 AWS 上运行一个 8 × A100 云服务器每小时大约花费 30
    美元。粗略估计，这样一个 LLM 的总训练成本大约为 69 万美元（计算为 184,320 小时除以 8，然后乘以 30）。
- en: 'The following code loads the “The Verdict” short story:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载了“The Verdict”短篇小说：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After loading the dataset, we can check the number of characters and tokens
    in the dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据集后，我们可以检查数据集中的字符和标记数量：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output is
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With just 5,145 tokens, the text might seem too small to train an LLM, but as
    mentioned earlier, it’s for educational purposes so that we can run the code in
    minutes instead of weeks. Plus, later we will load pretrained weights from OpenAI
    into our `GPTModel` code.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用 5,145 个标记，文本可能看起来太小，不足以训练 LLM，但如前所述，这是出于教育目的，以便我们可以在几分钟内而不是几周内运行代码。此外，稍后我们将从
    OpenAI 加载预训练的权重到我们的 `GPTModel` 代码中。
- en: Next, we divide the dataset into a training and a validation set and use the
    data loaders from chapter 2 to prepare the batches for LLM training. This process
    is visualized in figure 5.9. Due to spatial constraints, we use a `max_length=6`.
    However, for the actual data loaders, we set the `max_length` equal to the 256-token
    context length that the LLM supports so that the LLM sees longer texts during
    training.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据集分为训练集和验证集，并使用第 2 章中的数据加载器为 LLM 训练准备批次。这一过程在图 5.9 中进行了可视化。由于空间限制，我们使用
    `max_length=6`。然而，对于实际的数据加载器，我们将 `max_length` 设置为 LLM 支持的 256 个标记的上下文长度，这样 LLM
    在训练期间可以看到更长的文本。
- en: '![figure](../Images/5-9.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-9.png)'
- en: Figure 5.9 When preparing the data loaders, we split the input text into training
    and validation set portions. Then we tokenize the text (only shown for the training
    set portion for simplicity) and divide the tokenized text into chunks of a user-specified
    length (here, 6). Finally, we shuffle the rows and organize the chunked text into
    batches (here, batch size 2), which we can use for model training.
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.9 在准备数据加载器时，我们将输入文本分为训练集和验证集部分。然后我们对文本进行分词（为了简单起见，这里只展示了训练集部分）并将分词后的文本划分为用户指定的长度块（这里为
    6）。最后，我们打乱行顺序并将分块后的文本组织成批次（这里批次大小为 2），这些批次可以用于模型训练。
- en: Note  We are training the model with training data presented in similarly sized
    chunks for simplicity and efficiency. However, in practice, it can also be beneficial
    to train an LLM with variable-length inputs to help the LLM to better generalize
    across different types of inputs when it is being used.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们以类似大小的块呈现训练数据以进行训练，以简化并提高效率。然而，在实践中，训练一个具有可变长度输入的 LLM 也有助于 LLM 在使用时更好地泛化到不同类型的输入。
- en: 'To implement the data splitting and loading, we first define a `train_ratio`
    to use 90% of the data for training and the remaining 10% as validation data for
    model evaluation during training:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现数据拆分和加载，我们首先定义一个 `train_ratio`，使用 90% 的数据用于训练，剩余的 10% 作为训练期间模型评估的验证数据：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Using the `train_data` and `val_data` subsets, we can now create the respective
    data loader reusing the `create_dataloader_v1` code from chapter 2:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `train_data` 和 `val_data` 子集，我们现在可以创建相应的数据加载器，重用第 2 章中的 `create_dataloader_v1`
    代码：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We used a relatively small batch size to reduce the computational resource demand
    because we were working with a very small dataset. In practice, training LLMs
    with batch sizes of 1,024 or larger is not uncommon.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个相对较小的批次大小，以减少计算资源的需求，因为我们正在处理一个非常小的数据集。在实践中，使用 1,024 或更大的批次大小来训练 LLM
    并不罕见。
- en: 'As an optional check, we can iterate through the data loaders to ensure that
    they were created correctly:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作为可选的检查，我们可以遍历数据加载器以确保它们被正确创建：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We should see the following outputs:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Based on the preceding code output, we have nine training set batches with two
    samples and 256 tokens each. Since we allocated only 10% of the data for validation,
    there is only one validation batch consisting of two input examples. As expected,
    the input data (`x`) and target data (`y`) have the same shape (the batch size
    times the number of tokens in each batch) since the targets are the inputs shifted
    by one position, as discussed in chapter 2.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码输出，我们有九个训练集批次，每个批次包含两个样本和 256 个标记。由于我们只分配了 10% 的数据用于验证，因此只有一个包含两个输入示例的验证批次。正如预期的那样，输入数据
    (`x`) 和目标数据 (`y`) 具有相同的形状（批次大小乘以每个批次的标记数），因为目标数据是按照第 2 章中讨论的偏移一个位置的目标。
- en: 'Next, we implement a utility function to calculate the cross entropy loss of
    a given batch returned via the training and validation loader:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个实用函数来计算通过训练和验证加载器返回的给定批次的交叉熵损失：
- en: '[PRE31]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 The transfer to a given device allows us to transfer the data to a GPU.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将数据传输到指定设备允许我们将数据传输到 GPU。'
- en: We can now use this `calc_loss_batch` utility function, which computes the loss
    for a single batch, to implement the following `calc_loss_loader` function that
    computes the loss over all the batches sampled by a given data loader.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这个 `calc_loss_batch` 工具函数，它计算单个批次的损失，来实现以下 `calc_loss_loader` 函数，该函数计算给定数据加载器采样的所有批次的损失。
- en: Listing 5.2 Function to compute the training and validation loss
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.2 计算训练和验证损失的函数
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '#1 Iteratives over all batches if no fixed num_batches is specified'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 如果未指定固定的 num_batches，则迭代所有批次'
- en: '#2 Reduces the number of batches to match the total number of batches in the
    data loader if num_batches exceeds the number of batches in the data loader'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果 num_batches 超过数据加载器中的批次总数，则减少批次数量以匹配数据加载器中的批次总数'
- en: '#3 Sums loss for each batch'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 对每个批次的损失求和'
- en: '#4 Averages the loss over all batches'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 平均所有批次的损失'
- en: By default, the `calc_loss_loader` function iterates over all batches in a given
    data loader, accumulates the loss in the `total_loss` variable, and then computes
    and averages the loss over the total number of batches. Alternatively, we can
    specify a smaller number of batches via `num_batches` to speed up the evaluation
    during model training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`calc_loss_loader` 函数遍历给定数据加载器中的所有批次，将损失累积在 `total_loss` 变量中，然后计算并平均所有批次的损失。或者，我们可以通过
    `num_batches` 指定更少的批次数量，以加快模型训练期间的评估速度。
- en: 'Let’s now see this `calc_loss_loader` function in action, applying it to the
    training and validation set loaders:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个 `calc_loss_loader` 函数的实际应用，将其应用于训练集和验证集加载器：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#1 If you have a machine with a CUDA-supported GPU, the LLM will train on the
    GPU without making any changes to the code.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 如果你有一台支持 CUDA 的 GPU 的机器，LLM 将在没有对代码进行任何更改的情况下在 GPU 上进行训练。'
- en: '#2 Disables gradient tracking for efficiency because we are not training yet'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 由于我们尚未开始训练，因此禁用梯度跟踪以提高效率'
- en: '#3 Via the “device” setting, we ensure the data is loaded onto the same device
    as the LLM model.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 通过“device”设置，我们确保数据被加载到与LLM模型相同的设备上。'
- en: The resulting loss values are
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 产生的损失值是
- en: '[PRE34]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The loss values are relatively high because the model has not yet been trained.
    For comparison, the loss approaches 0 if the model learns to generate the next
    tokens as they appear in the training and validation sets.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值相对较高，因为模型尚未经过训练。为了比较，如果模型学会生成与训练和验证集中出现的下一个标记，损失值将接近0。
- en: Now that we have a way to measure the quality of the generated text, we will
    train the LLM to reduce this loss so that it becomes better at generating text,
    as illustrated in figure 5.10.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了衡量生成文本质量的方法，我们将训练LLM以减少这种损失，使其在生成文本方面变得更好，如图5.10所示。
- en: '![figure](../Images/5-10.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-10.png)'
- en: Figure 5.10 We have recapped the text generation process (step 1) and implemented
    basic model evaluation techniques (step 2) to compute the training and validation
    set losses (step 3). Next, we will go to the training functions and pretrain the
    LLM (step 4).
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.10 我们回顾了文本生成过程（步骤1）并实现了基本的模型评估技术（步骤2）来计算训练集和验证集的损失（步骤3）。接下来，我们将进入训练函数并预训练LLM（步骤4）。
- en: Next, we will focus on pretraining the LLM. After model training, we will implement
    alternative text generation strategies and save and load pretrained model weights.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将专注于预训练LLM。在模型训练后，我们将实现替代文本生成策略，并保存和加载预训练模型权重。
- en: 5.2 Training an LLM
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 训练LLM
- en: It is finally time to implement the code for pretraining the LLM, our `GPTModel`.
    For this, we focus on a straightforward training loop to keep the code concise
    and readable.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候实现预训练LLM，即我们的`GPTModel`的代码了。为此，我们关注一个简单的训练循环，以保持代码简洁易读。
- en: Note  Interested readers can learn about more advanced techniques, including
    *learning rate warmup*, *cosine annealing*, and *gradient clipping*, in appendix
    D.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对更高级技术感兴趣的用户可以在附录D中了解有关*学习率预热*、*余弦退火*和*梯度裁剪*等内容。
- en: '![figure](../Images/5-11.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-11.png)'
- en: Figure 5.11 A typical training loop for training deep neural networks in PyTorch
    consists of numerous steps, iterating over the batches in the training set for
    several epochs. In each loop, we calculate the loss for each training set batch
    to determine loss gradients, which we use to update the model weights so that
    the training set loss is minimized.
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.11 PyTorch中训练深度神经网络的典型训练循环包括多个步骤，迭代训练集中的批次数个epoch。在每个循环中，我们计算每个训练集批次的损失以确定损失梯度，我们使用这些梯度来更新模型权重，以最小化训练集损失。
- en: The flowchart in figure 5.11 depicts a typical PyTorch neural network training
    workflow, which we use for training an LLM. It outlines eight steps, starting
    with iterating over each epoch, processing batches, resetting gradients, calculating
    the loss and new gradients, and updating weights and concluding with monitoring
    steps like printing losses and generating text samples.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11中的流程图描述了PyTorch神经网络的典型训练工作流程，我们用它来训练LLM。它概述了八个步骤，从迭代每个epoch开始，处理批次，重置梯度，计算损失和新梯度，更新权重，最后以打印损失和生成文本样本等监控步骤结束。
- en: Note  If you are relatively new to training deep neural networks with PyTorch
    and any of these steps are unfamiliar, consider reading sections A.5 to A.8 in
    appendix A.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你相对较新于使用PyTorch训练深度神经网络，并且对其中任何步骤不熟悉，请考虑阅读附录A中的A.5到A.8节。
- en: We can implement this training flow via the `train_model_simple` function in
    code.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过代码中的`train_model_simple`函数实现这个训练流程。
- en: Listing 5.3 The main function for pretraining LLMs
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.3 预训练LLM的主要函数
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '#1 Initializes lists to track losses and tokens seen'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 初始化列表以跟踪损失和已看到的标记'
- en: '#2 Starts the main training loop'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 启动主训练循环'
- en: '#3 Resets loss gradients from the previous batch iteration'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从上一批次的迭代中重置损失梯度'
- en: '#4 Calculates loss gradients'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 计算损失梯度'
- en: '#5 Updates model weights using loss gradients'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 使用损失梯度更新模型权重'
- en: '#6 Optional evaluation step'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 可选评估步骤'
- en: '#7 Prints a sample text after each epoch'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 在每个epoch后打印一个样本文本'
- en: 'Note that the `train_model_simple` function we just created uses two functions
    we have not defined yet: `evaluate_model` and `generate_and_print_sample`.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们刚刚创建的`train_model_simple`函数使用了我们尚未定义的两个函数：`evaluate_model`和`generate_and_print_sample`。
- en: 'The `evaluate_model` function corresponds to step 7 in figure 5.11\. It prints
    the training and validation set losses after each model update so we can evaluate
    whether the training improves the model. More specifically, the `evaluate_model`
    function calculates the loss over the training and validation set while ensuring
    the model is in evaluation mode with gradient tracking and dropout disabled when
    calculating the loss over the training and validation sets:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_model`函数对应于图5.11中的步骤7。它在每次模型更新后打印训练集和验证集的损失，以便我们可以评估训练是否改善了模型。更具体地说，`evaluate_model`函数在计算训练集和验证集的损失时，确保模型处于评估模式，并且禁用了梯度跟踪和Dropout。'
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '#1 Dropout is disabled during evaluation for stable, reproducible results.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在评估期间禁用Dropout以获得稳定、可重复的结果。'
- en: '#2 Disables gradient tracking, which is not required during evaluation, to
    reduce the computational overhead'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 禁用梯度跟踪，这在评估期间不是必需的，以减少计算开销'
- en: 'Similar to `evaluate_model`, the `generate_and_print_sample` function is a
    convenience function that we use to track whether the model improves during the
    training. In particular, the `generate_and_print_sample` function takes a text
    snippet (`start_context`) as input, converts it into token IDs, and feeds it to
    the LLM to generate a text sample using the `generate_text_simple` function we
    used earlier:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与`evaluate_model`类似，`generate_and_print_sample`函数是一个方便的函数，我们用它来跟踪模型在训练过程中的改进情况。特别是，`generate_and_print_sample`函数接受一个文本片段（`start_context`）作为输入，将其转换为token
    ID，并使用我们之前使用的`generate_text_simple`函数将其输入到LLM中生成文本样本：
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#1 Compact print format'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 紧凑打印格式'
- en: While the `evaluate_model` function gives us a numeric estimate of the model’s
    training progress, this `generate_and_print_sample` text function provides a concrete
    text example generated by the model to judge its capabilities during training.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`evaluate_model`函数给我们提供了一个模型训练进度的数值估计，但这个`generate_and_print_sample`文本函数提供了一个由模型生成的具体文本示例，我们可以用它来判断模型在训练过程中的能力。
- en: AdamW
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: AdamW
- en: '*Adam* optimizers are a popular choice for training deep neural networks. However,
    in our training loop, we opt for the *AdamW* optimizer. AdamW is a variant of
    Adam that improves the weight decay approach, which aims to minimize model complexity
    and prevent overfitting by penalizing larger weights. This adjustment allows AdamW
    to achieve more effective regularization and better generalization; thus, AdamW
    is frequently used in the training of LLMs.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adam*优化器是训练深度神经网络时的一个流行选择。然而，在我们的训练循环中，我们选择了*AdamW*优化器。AdamW是Adam的一个变体，它改进了权重衰减方法，旨在通过惩罚较大的权重来最小化模型复杂度并防止过拟合。这种调整使得AdamW能够实现更有效的正则化和更好的泛化；因此，AdamW经常用于LLM的训练。'
- en: 'Let’s see this all in action by training a `GPTModel` instance for 10 epochs
    using an `AdamW` optimizer and the `train_model_simple` function we defined earlier:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`AdamW`优化器和之前定义的`train_model_simple`函数训练一个`GPTModel`实例10个epoch来观察这一切的实际效果：
- en: '[PRE38]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '#1 The .parameters() method returns all trainable weight parameters of the
    model.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 `.parameters()`方法返回模型的所有可训练权重参数。'
- en: 'Executing the `train_model_simple` function starts the training process, which
    takes about 5 minutes to complete on a MacBook Air or a similar laptop. The output
    printed during this execution is as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`train_model_simple`函数开始训练过程，这个过程在MacBook Air或类似笔记本电脑上大约需要5分钟才能完成。在此执行期间打印的输出如下：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#1 Intermediate results removed to save space'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 删除中间结果以节省空间'
- en: As we can see, the training loss improves drastically, starting with a value
    of 9.781 and converging to 0.391\. The language skills of the model have improved
    quite a lot. In the beginning, the model is only able to append commas to the
    start context (`Every` `effort` `moves` `you,,,,,,,,,,,,`) or repeat the word
    `and`. At the end of the training, it can generate grammatically correct text.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，训练损失显著改善，从9.781的值开始，收敛到0.391。该模型的语言技能有了很大的提升。一开始，模型只能将逗号添加到起始上下文（`Every`
    `effort` `moves` `you,,,,,,,,,,,,`）或重复单词`and`。在训练结束时，它可以生成语法正确的文本。
- en: Similar to the training set loss, we can see that the validation loss starts
    high (9.933) and decreases during the training. However, it never becomes as small
    as the training set loss and remains at 6.452 after the 10th epoch.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练集损失类似，我们可以看到验证损失开始较高（9.933）并在训练过程中下降。然而，它从未变得像训练集损失那样小，并在第10个epoch后保持在6.452。
- en: 'Before discussing the validation loss in more detail, let’s create a simple
    plot that shows the training and validation set losses side by side:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在更详细地讨论验证损失之前，让我们创建一个简单的图，该图显示了训练集和验证集损失并排显示：
- en: '[PRE40]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '#1 Creates a second x-axis that shares the same y-axis'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建一个与同一y轴共享的第二个x轴'
- en: '#2 Invisible plot for aligning ticks'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 用于对齐刻度的不可见图'
- en: The resulting training and validation loss plot is shown in figure 5.12\. As
    we can see, both the training and validation losses start to improve for the first
    epoch. However, the losses start to diverge past the second epoch. This divergence
    and the fact that the validation loss is much larger than the training loss indicate
    that the model is overfitting to the training data. We can confirm that the model
    memorizes the training data verbatim by searching for the generated text snippets,
    such as `quite` `insensible` `to` `the` `irony` in the “The Verdict” text file.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的训练和验证损失图如图5.12所示。如图所示，训练和验证损失在第一个epoch开始时开始改善。然而，损失在第二个epoch之后开始发散。这种发散以及验证损失远大于训练损失的事实表明，模型对训练数据过度拟合。我们可以通过搜索生成的文本片段来确认模型逐字逐句地记住了训练数据，例如在“The
    Verdict”文本文件中的“相当” “无感觉” “对” “讽刺”。
- en: '![figure](../Images/5-12.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-12.png)'
- en: Figure 5.12 At the beginning of the training, both the training and validation
    set losses sharply decrease, which is a sign that the model is learning. However,
    the training set loss continues to decrease past the second epoch, whereas the
    validation loss stagnates. This is a sign that the model is still learning, but
    it’s overfitting to the training set past epoch 2.
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.12 在训练开始时，训练集和验证集损失急剧下降，这是模型正在学习的标志。然而，训练集损失在第二个epoch之后继续下降，而验证损失停滞不前。这是模型仍在学习，但在epoch
    2之后对训练集过度拟合的标志。
- en: This memorization is expected since we are working with a very, very small training
    dataset and training the model for multiple epochs. Usually, it’s common to train
    a model on a much larger dataset for only one epoch.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在使用一个非常小的训练数据集，并且对模型进行多轮训练，因此这种记忆是预期的。通常，在只有一个epoch的情况下，在更大的数据集上训练模型是常见的。
- en: Note  As mentioned earlier, interested readers can try to train the model on
    60,000 public domain books from Project Gutenberg, where this overfitting does
    not occur; see appendix B for details.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如前所述，感兴趣的读者可以尝试在Project Gutenberg的60,000本公共领域书籍上训练模型，在那里不会发生过度拟合；参见附录B的详细信息。
- en: '![figure](../Images/5-13.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-13.png)'
- en: Figure 5.13 Our model can generate coherent text after implementing the training
    function. However, it often memorizes passages from the training set verbatim.
    Next, we will discuss strategies to generate more diverse output texts.
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13 在实现训练函数后，我们的模型可以生成连贯的文本。然而，它经常逐字逐句地记住训练集中的段落。接下来，我们将讨论生成更多样化输出文本的策略。
- en: As illustrated in figure 5.13, we have completed four of our objectives for
    this chapter. Next, we will cover text generation strategies for LLMs to reduce
    training data memorization and increase the originality of the LLM-generated text
    before we cover weight loading and saving and loading pretrained weights from
    OpenAI’s GPT model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.13所示，我们已经完成了本章的四个目标。接下来，在我们介绍权重加载和保存以及从OpenAI的GPT模型加载预训练权重之前，我们将讨论LLM的文本生成策略，以减少训练数据记忆并提高LLM生成文本的原创性。
- en: 5.3 Decoding strategies to control randomness
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 控制随机性的解码策略
- en: Let’s look at text generation strategies (also called decoding strategies) to
    generate more original text. First, we will briefly revisit the `generate_text_simple`
    function that we used inside `generate_and_print_sample` earlier. Then we will
    cover two techniques, *temperature scaling* and *top-k sampling*, to improve this
    function.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看文本生成策略（也称为解码策略），以生成更多原创文本。首先，我们将简要回顾我们之前在`generate_and_print_sample`中使用的`generate_text_simple`函数。然后，我们将介绍两种技术，*温度缩放*和*top-k采样*，以改进此函数。
- en: 'We begin by transferring the model back from the GPU to the CPU since inference
    with a relatively small model does not require a GPU. Also, after training, we
    put the model into evaluation mode to turn off random components such as dropout:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将模型从GPU转移到CPU，因为使用相对较小的模型进行推理不需要GPU。此外，在训练后，我们将模型放入评估模式以关闭随机组件，如dropout：
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we plug the `GPTModel` instance (`model`) into the `generate_text_simple`
    function, which uses the LLM to generate one token at a time:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将`GPTModel`实例（`model`）插入到`generate_text_simple`函数中，该函数使用LLM逐个生成标记：
- en: '[PRE42]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The generated text is
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As explained earlier, the generated token is selected at each generation step
    corresponding to the largest probability score among all tokens in the vocabulary.
    This means that the LLM will always generate the same outputs even if we run the
    preceding `generate_text_simple` function multiple times on the same start context
    (`Every` `effort` `moves` `you`).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，生成的标记是在每个生成步骤中根据词汇表中所有标记的最大概率分数选择的。这意味着即使我们多次在相同的起始上下文（`Every` `effort`
    `moves` `you`）上运行前面的`generate_text_simple`函数，LLM也会始终生成相同的输出。
- en: 5.3.1 Temperature scaling
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 温度缩放
- en: Let’s now look at temperature scaling, a technique that adds a probabilistic
    selection process to the next-token generation task. Previously, inside the `generate_text_simple`
    function, we always sampled the token with the highest probability as the next
    token using `torch.argmax`, also known as *greedy decoding*. To generate text
    with more variety, we can replace `argmax` with a function that samples from a
    probability distribution (here, the probability scores the LLM generates for each
    vocabulary entry at each token generation step).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看温度缩放技术，这是一种将概率选择过程添加到下一个标记生成任务中的技术。之前，在`generate_text_simple`函数内部，我们总是使用`torch.argmax`（也称为*贪婪解码*）来采样概率最高的标记作为下一个标记。为了生成更多样化的文本，我们可以用从概率分布中采样的函数（在这里，是LLM在每次标记生成步骤为每个词汇条目生成的概率分数）来替换`argmax`。
- en: 'To illustrate the probabilistic sampling with a concrete example, let’s briefly
    discuss the next-token generation process using a very small vocabulary for illustration
    purposes:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用具体例子说明概率抽样，让我们简要讨论使用一个非常小的词汇表来演示的下一个标记生成过程：
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, assume the LLM is given the start context `"every` `effort` `moves` `you"`
    and generates the following next-token logits:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设LLM被给定了起始上下文`"every"` `effort` `moves` `you"`并生成了以下下一个标记的logits：
- en: '[PRE45]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'As discussed in chapter 4, inside `generate_text_simple`, we convert the logits
    into probabilities via the `softmax` function and obtain the token ID corresponding
    to the generated token via the `argmax` function, which we can then map back into
    text via the inverse vocabulary:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如第4章所述，在`generate_text_simple`内部，我们通过`softmax`函数将logits转换为概率，并通过`argmax`函数获得生成的标记对应的标记ID，然后我们可以通过逆词汇将其映射回文本：
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Since the largest logit value and, correspondingly, the largest softmax probability
    score are in the fourth position (index position 3 since Python uses 0 indexing),
    the generated word is `"forward"`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最大的logit值以及相应的最大的softmax概率分数位于第四位（Python使用0索引，因此索引位置为3），生成的单词是`"forward"`。
- en: 'To implement a probabilistic sampling process, we can now replace `argmax`
    with the `multinomial` function in PyTorch:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现概率抽样过程，我们现在可以在PyTorch中将`argmax`替换为`multinomial`函数：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The printed output is `"forward"` just like before. What happened? The `multinomial`
    function samples the next token proportional to its probability score. In other
    words, `"forward"` is still the most likely token and will be selected by `multinomial`
    most of the time but not all the time. To illustrate this, let’s implement a function
    that repeats this sampling 1,000 times:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的输出与之前一样是`"forward"`。发生了什么？`multinomial`函数按概率分数成比例地采样下一个标记。换句话说，`"forward"`仍然是概率最高的标记，并且大多数时候会被`multinomial`选中，但不是每次都会被选中。为了说明这一点，让我们实现一个重复此采样1,000次的函数：
- en: '[PRE48]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The sampling output is
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 采样输出是
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As we can see, the word `forward` is sampled most of the time (582 out of 1,000
    times), but other tokens such as `closer`, `inches`, and `toward` will also be
    sampled some of the time. This means that if we replaced the `argmax` function
    with the `multinomial` function inside the `generate_and_print_sample` function,
    the LLM would sometimes generate texts such as `every` `effort` `moves` `you`
    `toward`, `every` `effort` `moves` `you` `inches`, and `every` `effort` `moves`
    `you` `closer` instead of `every` `effort` `moves` `you` `forward`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，单词`forward`在大多数时候被采样（1,000次中有582次），但其他标记如`closer`、`inches`和`toward`也会被采样一些时候。这意味着如果我们将`generate_and_print_sample`函数中的`argmax`函数替换为`multinomial`函数，LLM有时会生成如`every`
    `effort` `moves` `you` `toward`、`every` `effort` `moves` `you` `inches`和`every`
    `effort` `moves` `you` `closer`这样的文本，而不是`every` `effort` `moves` `you` `forward`。
- en: 'We can further control the distribution and selection process via a concept
    called *temperature scaling.* Temperature scaling is just a fancy description
    for dividing the logits by a number greater than 0:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个称为**温度缩放**的概念进一步控制分布和选择过程。温度缩放只是将 logits 除以一个大于 0 的数的花哨说法：
- en: '[PRE50]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Temperatures greater than 1 result in more uniformly distributed token probabilities,
    and temperatures smaller than 1 will result in more confident (sharper or more
    peaky) distributions. Let’s illustrate this by plotting the original probabilities
    alongside probabilities scaled with different temperature values:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 大于 1 的温度会导致标记概率分布更加均匀，而小于 1 的温度会导致更加自信（更尖锐或更峰值）的分布。让我们通过将原始概率与不同温度值缩放的概率一起绘制来展示这一点：
- en: '[PRE51]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#1 Original, lower, and higher confidence'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 原始、较低和较高置信度'
- en: The resulting plot is shown in figure 5.14.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如图 5.14 所示。
- en: '![figure](../Images/5-14.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-14.png)'
- en: Figure 5.14 A temperature of 1 represents the unscaled probability scores for
    each token in the vocabulary. Decreasing the temperature to 0.1 sharpens the distribution,
    so the most likely token (here, “forward”) will have an even higher probability
    score. Likewise, increasing the temperature to 5 makes the distribution more uniform.
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.14 温度为 1 表示词汇表中每个标记的未缩放概率分数。将温度降低到 0.1 会使分布更加尖锐，这样最可能的标记（在这里是“forward”）将具有更高的概率分数。同样，将温度增加到
    5 会使分布更加均匀。
- en: A temperature of 1 divides the logits by 1 before passing them to the `softmax`
    function to compute the probability scores. In other words, using a temperature
    of 1 is the same as not using any temperature scaling. In this case, the tokens
    are selected with a probability equal to the original softmax probability scores
    via the `multinomial` sampling function in PyTorch. For example, for the temperature
    setting 1, the token corresponding to “forward” would be selected about 60% of
    the time, as we can see in figure 5.14.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 温度为 1 时，在将 logits 传递给 `softmax` 函数以计算概率分数之前，将其除以 1。换句话说，使用温度为 1 等同于不使用任何温度缩放。在这种情况下，通过
    PyTorch 中的 `multinomial` 采样函数，以与原始 softmax 概率分数相等的概率选择标记。例如，对于温度设置 1，对应于“forward”的标记大约有
    60% 的时间被选中，如图 5.14 所示。
- en: Also, as we can see in figure 5.14, applying very small temperatures, such as
    0.1, will result in sharper distributions such that the behavior of the `multinomial`
    function selects the most likely token (here, `"forward"`) almost 100% of the
    time, approaching the behavior of the `argmax` function. Likewise, a temperature
    of 5 results in a more uniform distribution where other tokens are selected more
    often. This can add more variety to the generated texts but also more often results
    in nonsensical text. For example, using the temperature of 5 results in texts
    such as `every` `effort` `moves` `you` `pizza` about 4% of the time.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如图 5.14 所示，应用非常小的温度，如 0.1，将导致分布更加尖锐，这样 `multinomial` 函数的行为几乎 100% 选中最可能的标记（在这里是
    `"forward"`），接近 `argmax` 函数的行为。同样，温度为 5 导致分布更加均匀。这可以为生成的文本添加更多多样性，但也会更频繁地产生无意义的文本。例如，使用温度为
    5 时，文本中出现“every” “effort” “moves” “you” “pizza”的频率大约为 4%。
- en: Exercise 5.1
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 5.1
- en: Use the `print_sampled_tokens` function to print the sampling frequencies of
    the softmax probabilities scaled with the temperatures shown in figure 5.14\.
    How often is the word `pizza` sampled in each case? Can you think of a faster
    and more accurate way to determine how often the word `pizza` is sampled?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `print_sampled_tokens` 函数打印出与图 5.14 中显示的温度缩放的 softmax 概率采样频率。在每种情况下，“pizza”这个词被采样的频率是多少？你能想到一种更快更准确的方法来确定“pizza”这个词被采样的频率吗？
- en: 5.3.2 Top-k sampling
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 Top-k 采样
- en: We’ve now implemented a probabilistic sampling approach coupled with temperature
    scaling to increase the diversity of the outputs. We saw that higher temperature
    values result in more uniformly distributed next-token probabilities, which result
    in more diverse outputs as it reduces the likelihood of the model repeatedly selecting
    the most probable token. This method allows for the exploring of less likely but
    potentially more interesting and creative paths in the generation process. However,
    one downside of this approach is that it sometimes leads to grammatically incorrect
    or completely nonsensical outputs such as `every` `effort` `moves` `you` `pizza`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经实现了一种结合温度缩放的概率采样方法，以增加输出的多样性。我们注意到，较高的温度值会导致下一个标记的概率分布更加均匀，这减少了模型反复选择最可能标记的可能性，从而产生了更多样化的输出。这种方法允许在生成过程中探索不太可能但可能更有趣和创造性的路径。然而，这种方法的一个缺点是，有时会导致语法错误或完全不合理的输出，例如`every`
    `effort` `moves` `you` `pizza`。
- en: '*Top-k sampling*, when combined with probabilistic sampling and temperature
    scaling, can improve the text generation results. In top-k sampling, we can restrict
    the sampled tokens to the top-k most likely tokens and exclude all other tokens
    from the selection process by masking their probability scores, as illustrated
    in figure 5.15.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*Top-k采样*，当与概率采样和温度缩放结合使用时，可以提高文本生成结果。在top-k采样中，我们可以将采样的标记限制为最可能的top-k标记，并通过屏蔽它们的概率分数来排除所有其他标记的选择过程，如图5.15所示。'
- en: '![figure](../Images/5-15.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-15.png)'
- en: Figure 5.15 Using top-k sampling with k = 3, we focus on the three tokens associated
    with the highest logits and mask out all other tokens with negative infinity (`–inf`)
    before applying the `softmax` function. This results in a probability distribution
    with a probability value 0 assigned to all non-top-k tokens. (The numbers in this
    figure are truncated to two digits after the decimal point to reduce visual clutter.
    The values in the “Softmax” row should add up to 1.0.)
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.15 使用k = 3的top-k采样，我们关注与最高logits值相关的三个标记，在应用`softmax`函数之前，用负无穷大(`–inf`)屏蔽所有其他标记。这导致一个概率分布，其中所有非top-k标记的概率值被分配为0。（图中数字在小数点后截断为两位以减少视觉杂乱。在“Softmax”行中的值应加起来为1.0。）
- en: The top-k approach replaces all nonselected logits with negative infinity value
    (`-inf`), such that when computing the softmax values, the probability scores
    of the non-top-k tokens are 0, and the remaining probabilities sum up to 1\. (Careful
    readers may remember this masking trick from the causal attention module we implemented
    in chapter 3, section 3.5.1.)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: top-k方法将所有未选择的logits替换为负无穷大值(`-inf`)，这样在计算softmax值时，非top-k标记的概率分数为0，剩余的概率加起来为1。（仔细的读者可能会记得我们在第3章第3.5.1节中实现的因果注意力模块中的这个屏蔽技巧。）
- en: 'In code, we can implement the top-k procedure in figure 5.15 as follows, starting
    with the selection of the tokens with the largest logit values:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以按照以下方式实现图5.15中的top-k过程，从选择具有最大logits值的标记开始：
- en: '[PRE52]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The logits values and token IDs of the top three tokens, in descending order,
    are
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 按降序排列，top三个标记的logits值和标记ID如下
- en: '[PRE53]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Subsequently, we apply PyTorch’s `where` function to set the logit values of
    tokens that are below the lowest logit value within our top-three selection to
    negative infinity (`-inf`):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们应用PyTorch的`where`函数将位于我们top-three选择中最低logits值以下的所有标记的logits值设置为负无穷大(`-inf`)：
- en: '[PRE54]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '#1 Identifies logits less than the minimum in the top 3'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 识别出top 3中低于最小值的logits'
- en: '#2 Assigns –inf to these lower logits'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将这些较低logits分配为–inf'
- en: '#3 Retains the original logits for all other tokens'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 保留所有其他标记的原始logits'
- en: The resulting logits for the next token in the nine-token vocabulary are
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在九个标记词汇中的下一个标记的logits值如下
- en: '[PRE55]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Lastly, let’s apply the `softmax` function to turn these into next-token probabilities:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们应用`softmax`函数将这些转换为下一个标记的概率：
- en: '[PRE56]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'As we can see, the result of this top-three approach are three non-zero probability
    scores:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这种top-three方法的结果是三个非零概率分数：
- en: '[PRE57]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We can now apply the temperature scaling and multinomial function for probabilistic
    sampling to select the next token among these three non-zero probability scores
    to generate the next token. We do this next by modifying the text generation function.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以应用温度缩放和多项式函数进行概率采样，从这三个非零概率分数中选择下一个标记来生成下一个标记。我们接下来通过修改文本生成函数来实现这一点。
- en: 5.3.3 Modifying the text generation function
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 修改文本生成函数
- en: Now, let’s combine temperature sampling and top-k sampling to modify the `generate_
    text_simple` function we used to generate text via the LLM earlier, creating a
    new `generate` function.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们结合温度采样和top-k采样来修改我们之前用于通过LLM生成文本的`generate_text_simple`函数，创建一个新的`generate`函数。
- en: Listing 5.4 A modified text generation function with more diversity
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.4 一个具有更多多样性的修改后的文本生成函数
- en: '[PRE58]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '#1 The for loop is the same as before: gets logits and only focuses on the
    last time step.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 循环结构与之前相同：获取logits并仅关注最后一个时间步。'
- en: '#2 Filters logits with top_k sampling'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用top_k采样过滤logits'
- en: '#3 Applies temperature scaling'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 应用温度缩放'
- en: '#4 Carries out greedy next-token selection as before when temperature scaling
    is disabled'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 当禁用温度缩放时，与之前一样执行贪婪的下一个标记选择'
- en: '#5 Stops generating early if end-of-sequence token is encountered'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 遇到序列结束标记时提前停止生成'
- en: 'Let’s now see this new `generate` function in action:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看这个新的`generate`函数的实际应用：
- en: '[PRE59]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The generated text is
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE60]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As we can see, the generated text is very different from the one we previously
    generated via the `generate_simple` function in section 5.3 (`"Every` `effort`
    `moves` `you` `know,"` `was` `one` `of` `the` `axioms` `he` `laid...!` ), which
    was a memorized passage from the training set.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，生成的文本与我们之前在5.3节中通过`generate_simple`函数生成的文本非常不同（`"Every` `effort` `moves`
    `you` `know,"` `was` `one` `of` `the` `axioms` `he` `laid...!`），这是训练集中的记忆段落。
- en: Exercise 5.2
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.2
- en: Play around with different temperatures and top-k settings. Based on your observations,
    can you think of applications where lower temperature and top-k settings are desired?
    Likewise, can you think of applications where higher temperature and top-k settings
    are preferred? (It’s recommended to also revisit this exercise at the end of the
    chapter after loading the pretrained weights from OpenAI.)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的温度和top-k设置。根据你的观察，你能想到哪些需要较低温度和top-k设置的用例？同样，你能想到哪些需要较高温度和top-k设置的用例？（建议在加载OpenAI的预训练权重后，在章节末尾再次回顾这个练习。）
- en: Exercise 5.3
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.3
- en: What are the different combinations of settings for the `generate` function
    to force deterministic behavior, that is, disabling the random sampling such that
    it always produces the same outputs similar to the `generate_simple` function?
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate`函数的不同设置组合有哪些，可以强制确定性行为，即禁用随机采样，使其总是产生相同的输出，类似于`generate_simple`函数？'
- en: 5.4 Loading and saving model weights in PyTorch
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 在PyTorch中加载和保存模型权重
- en: Thus far, we have discussed how to numerically evaluate the training progress
    and pretrain an LLM from scratch. Even though both the LLM and dataset were relatively
    small, this exercise showed that pretraining LLMs is computationally expensive.
    Thus, it is important to be able to save the LLM so that we don’t have to rerun
    the training every time we want to use it in a new session.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何数值评估训练进度和从头开始预训练一个LLM。尽管LLM和数据集相对较小，但这个练习表明预训练LLM是计算密集型的。因此，能够保存LLM非常重要，这样我们就不必每次在新会话中使用它时都重新运行训练。
- en: So, let’s discuss how to save and load a pretrained model, as highlighted in
    figure 5.16\. Later, we will load a more capable pretrained GPT model from OpenAI
    into our `GPTModel` instance.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们讨论如何保存和加载预训练模型，如图5.16所示。稍后，我们将从OpenAI加载一个更强大的预训练GPT模型到我们的`GPTModel`实例中。
- en: '![figure](../Images/5-16.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-16.png)'
- en: Figure 5.16 After training and inspecting the model, it is often helpful to
    save the model so that we can use or continue training it later (step 6).
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.16 在训练和检查模型后，保存模型通常很有帮助，这样我们就可以稍后使用或继续训练它（步骤6）。
- en: 'Fortunately, saving a PyTorch model is relatively straightforward. The recommended
    way is to save a model’s `state_dict`, a dictionary mapping each layer to its
    parameters, using the `torch.save` function:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，保存PyTorch模型相对简单。推荐的方法是使用`torch.save`函数保存模型的`state_dict`，这是一个将每一层映射到其参数的字典：
- en: '[PRE61]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`"model.pth"` is the filename where the `state_dict` is saved. The `.pth` extension
    is a convention for PyTorch files, though we could technically use any file extension.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`"model.pth"`是保存`state_dict`的文件名。`.pth`扩展名是PyTorch文件的约定，尽管技术上我们可以使用任何文件扩展名。'
- en: 'Then, after saving the model weights via the `state_dict`, we can load the
    model weights into a new `GPTModel` model instance:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在通过`state_dict`保存模型权重之后，我们可以将模型权重加载到一个新的`GPTModel`模型实例中：
- en: '[PRE62]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: As discussed in chapter 4, dropout helps prevent the model from overfitting
    to the training data by randomly “dropping out” of a layer’s neurons during training.
    However, during inference, we don’t want to randomly drop out any of the information
    the network has learned. Using `model.eval()` switches the model to evaluation
    mode for inference, disabling the dropout layers of the `model`. If we plan to
    continue pretraining a model later—for example, using the `train_model_simple`
    function we defined earlier in this chapter—saving the optimizer state is also
    recommended.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如第4章所述，dropout通过在训练过程中随机“丢弃”层中的神经元来帮助防止模型过度拟合训练数据。然而，在推理过程中，我们不希望随机丢弃网络学习到的任何信息。使用`model.eval()`将模型切换到评估模式进行推理，禁用`model`的dropout层。如果我们计划稍后继续预训练模型——例如，使用本章前面定义的`train_model_simple`函数——保存优化器状态也是推荐的。
- en: 'Adaptive optimizers such as AdamW store additional parameters for each model
    weight. AdamW uses historical data to adjust learning rates for each model parameter
    dynamically. Without it, the optimizer resets, and the model may learn suboptimally
    or even fail to converge properly, which means it will lose the ability to generate
    coherent text. Using `torch.save`, we can save both the model and optimizer `state_dict`
    contents:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应优化器，如AdamW，为每个模型权重存储额外的参数。AdamW使用历史数据来动态调整每个模型参数的学习率。没有它，优化器会重置，模型可能学习不佳，甚至无法正确收敛，这意味着它将失去生成连贯文本的能力。使用`torch.save`，我们可以保存模型和优化器的`state_dict`内容：
- en: '[PRE63]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then we can restore the model and optimizer states by first loading the saved
    data via `torch.load` and then using the `load_state_dict` method:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过首先使用`torch.load`加载保存的数据，然后使用`load_state_dict`方法来恢复模型和优化器状态：
- en: '[PRE64]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Exercise 5.4
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.4
- en: After saving the weights, load the model and optimizer in a new Python session
    or Jupyter notebook file and continue pretraining it for one more epoch using
    the `train_model_simple` function.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 保存权重后，在新的Python会话或Jupyter笔记本文件中加载模型和优化器，并使用`train_model_simple`函数继续进行一个epoch的预训练。
- en: 5.5 Loading pretrained weights from OpenAI
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 从OpenAI加载预训练权重
- en: Previously, we trained a small GPT-2 model using a limited dataset comprising
    a short-story book. This approach allowed us to focus on the fundamentals without
    the need for extensive time and computational resources.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前，我们使用一个包含短篇小说集的有限数据集训练了一个小的GPT-2模型。这种方法使我们能够专注于基础，而无需大量的时间和计算资源。
- en: Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus eliminating
    the need to invest tens to hundreds of thousands of dollars in retraining the
    model on a large corpus ourselves. So, let’s load these weights into our `GPTModel`
    class and use the model for text generation. Here, *weights* refer to the weight
    parameters stored in the `.weight` attributes of PyTorch’s `Linear` and `Embedding`
    layers, for example. We accessed them earlier via `model.parameters()` when training
    the model. In chapter 6, will reuse these pretrained weights to fine-tune the
    model for a text classification task and follow instructions similar to ChatGPT.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，OpenAI公开分享了他们的GPT-2模型的权重，从而消除了我们自己在大型语料库上重新训练模型所需的数十万甚至数百万美元的投资。因此，让我们将这些权重加载到我们的`GPTModel`类中，并使用该模型进行文本生成。在这里，*权重*指的是存储在PyTorch的`Linear`和`Embedding`层的`.weight`属性中的权重参数，例如。我们在训练模型时通过`model.parameters()`访问过它们。在第6章中，我们将重用这些预训练的权重来微调模型以进行文本分类任务，并遵循类似于ChatGPT的说明。
- en: Note that OpenAI originally saved the GPT-2 weights via TensorFlow, which we
    have to install to load the weights in Python. The following code will use a progress
    bar tool called `tqdm` to track the download process, which we also have to install.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，OpenAI最初通过TensorFlow保存了GPT-2的权重，我们必须安装TensorFlow才能在Python中加载权重。以下代码将使用名为`tqdm`的进度条工具来跟踪下载过程，我们同样需要安装它。
- en: 'You can install these libraries by executing the following command in your
    terminal:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在终端中执行以下命令来安装这些库：
- en: '[PRE65]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The download code is relatively long, mostly boilerplate, and not very interesting.
    Hence, instead of devoting precious space to discussing Python code for fetching
    files from the internet, we download the `gpt_download.py` Python module directly
    from this chapter’s online repository:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 下载代码相对较长，主要是样板代码，并不很有趣。因此，我们不会在讨论从互联网上获取文件的Python代码上浪费宝贵空间，而是直接从本章的在线仓库下载`gpt_download.py`
    Python模块：
- en: '[PRE66]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Next, after downloading this file to the local directory of your Python session,
    you should briefly inspect the contents of this file to ensure that it was saved
    correctly and contains valid Python code.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在将此文件下载到Python会话的本地目录后，您应该简要检查此文件的内容，以确保它已正确保存并包含有效的Python代码。
- en: 'We can now import the `download_and_load_gpt2` function from the `gpt_download
    .py` file as follows, which will load the GPT-2 architecture settings (`settings`)
    and weight parameters (`params`) into our Python session:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按照以下方式从`gpt_download.py`文件中导入`download_and_load_gpt2`函数，这将把GPT-2架构设置（`settings`）和权重参数（`params`）加载到我们的Python会话中：
- en: '[PRE67]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Executing this code downloads the following seven files associated with the
    `124M` parameter GPT-2 model:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将下载与`124M`参数GPT-2模型相关的以下七个文件：
- en: '[PRE68]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Note  If the download code does not work for you, it could be due to intermittent
    internet connection, server problems, or changes in how OpenAI shares the weights
    of the open-source GPT-2 model. In this case, please visit this chapter’s online
    code repository at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
    for alternative and updated instructions, and reach out via the Manning Forum
    for further questions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果下载代码对您不起作用，可能是由于间歇性互联网连接、服务器问题或OpenAI分享开源GPT-2模型权重的变化方式。在这种情况下，请访问本章的在线代码仓库[https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)，以获取替代和更新的说明，并通过Manning论坛提出进一步的问题。
- en: 'Assuming the execution of the previous code has completed, let’s inspect the
    contents of `settings` and `params`:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 假设上一段代码已执行完成，让我们检查`settings`和`params`的内容：
- en: '[PRE69]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The contents are
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 内容如下
- en: '[PRE70]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Both `settings` and `params` are Python dictionaries. The `settings` dictionary
    stores the LLM architecture settings similarly to our manually defined `GPT_CONFIG_124M`
    settings. The `params` dictionary contains the actual weight tensors. Note that
    we only printed the dictionary keys because printing the weight contents would
    take up too much screen space; however, we can inspect these weight tensors by
    printing the whole dictionary via `print(params)` or by selecting individual tensors
    via the respective dictionary keys, for example, the embedding layer weights:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`settings`和`params`都是Python字典。`settings`字典存储了LLM架构设置，类似于我们手动定义的`GPT_CONFIG_124M`设置。`params`字典包含实际的权重张量。请注意，我们只打印了字典键，因为打印权重内容会占用太多的屏幕空间；然而，我们可以通过打印整个字典`print(params)`或通过选择单个张量，例如通过相应的字典键来检查这些权重张量，例如嵌入层权重：'
- en: '[PRE71]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The weights of the token embedding layer are
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌嵌入层的权重是
- en: '[PRE72]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We downloaded and loaded the weights of the smallest GPT-2 model via the `download_
    and_load_gpt2(model_size="124M",` `...)` setting. OpenAI also shares the weights
    of larger models: `355M`, `774M`, and `1558M`. The overall architecture of these
    differently sized GPT models is the same, as illustrated in figure 5.17, except
    that different architectural elements are repeated different numbers of times
    and the embedding size differs. The remaining code in this chapter is also compatible
    with these larger models.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`download_and_load_gpt2(model_size="124M", ...)`设置下载并加载了最小的GPT-2模型权重。OpenAI还分享了更大模型的权重：`355M`、`774M`和`1558M`。这些不同尺寸的GPT模型的总体架构是相同的，如图5.17所示，除了不同的架构元素重复的次数不同，以及嵌入大小不同。本章剩余的代码也与这些更大的模型兼容。
- en: '![figure](../Images/5-17.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-17.png)'
- en: Figure 5.17 GPT-2 LLMs come in several different model sizes, ranging from 124
    million to 1,558 million parameters. The core architecture is the same, with the
    only difference being the embedding sizes and the number of times individual components
    like the attention heads and transformer blocks are repeated.
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.17 GPT-2 LLM有几种不同的模型尺寸，从1.24亿到1.558亿参数不等。核心架构是相同的，唯一的区别是嵌入大小以及注意力头和Transformer块等单个组件重复的次数。
- en: 'After loading the GPT-2 model weights into Python, we still need to transfer
    them from the `settings` and `params` dictionaries into our `GPTModel` instance.
    First, we create a dictionary that lists the differences between the different
    GPT model sizes in figure 5.17:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在将GPT-2模型权重加载到Python中之后，我们仍然需要将它们从`settings`和`params`字典转移到我们的`GPTModel`实例中。首先，我们创建一个字典，列出图5.17中不同GPT模型尺寸之间的差异：
- en: '[PRE73]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Suppose we are interested in loading the smallest model, `"gpt2-small` `(124M)"`.
    We can use the corresponding settings from the `model_configs` table to update
    our full-length `GPT_CONFIG_124M` we defined and used earlier:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们感兴趣的是加载最小的模型`"gpt2-small"`（124M）。我们可以使用`model_configs`表中的相应设置来更新我们之前定义并使用的完整长度的`GPT_CONFIG_124M`：
- en: '[PRE74]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Careful readers may remember that we used a 256-token length earlier, but the
    original GPT-2 models from OpenAI were trained with a 1,024-token length, so we
    have to update the `NEW_CONFIG` accordingly:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的读者可能还记得我们之前使用的是256标记长度，但OpenAI的原始GPT-2模型是在1,024标记长度下训练的，因此我们必须相应地更新`NEW_CONFIG`：
- en: '[PRE75]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Also, OpenAI used bias vectors in the multi-head attention module’s linear
    layers to implement the query, key, and value matrix computations. Bias vectors
    are not commonly used in LLMs anymore as they don’t improve the modeling performance
    and are thus unnecessary. However, since we are working with pretrained weights,
    we need to match the settings for consistency and enable these bias vectors:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，OpenAI在多头注意力模块的线性层中使用了偏置向量来实现查询、键和值矩阵的计算。偏置向量在LLMs中不再常用，因为它们不会提高建模性能，因此是不必要的。然而，由于我们正在使用预训练的权重，我们需要匹配设置以保持一致性并启用这些偏置向量：
- en: '[PRE76]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We can now use the updated `NEW_CONFIG` dictionary to initialize a new `GPTModel`
    instance:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用更新的`NEW_CONFIG`字典来初始化一个新的`GPTModel`实例：
- en: '[PRE77]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'By default, the `GPTModel` instance is initialized with random weights for
    pretraining. The last step to using OpenAI’s model weights is to override these
    random weights with the weights we loaded into the `params` dictionary. For this,
    we will first define a small `assign` utility function that checks whether two
    tensors or arrays (`left` and `right`) have the same dimensions or shape and returns
    the right tensor as trainable PyTorch parameters:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`GPTModel`实例使用随机权重进行预训练。使用OpenAI模型权重的最后一步是用我们加载到`params`字典中的权重覆盖这些随机权重。为此，我们首先定义一个小的`assign`实用函数，该函数检查两个张量或数组（`left`和`right`）是否具有相同的维度或形状，并返回正确的张量作为可训练的PyTorch参数：
- en: '[PRE78]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Next, we define a `load_weights_into_gpt` function that loads the weights from
    the `params` dictionary into a `GPTModel` instance `gpt.`
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`load_weights_into_gpt`函数，该函数将`params`字典中的权重加载到`GPTModel`实例`gpt`中。
- en: Listing 5.5 Loading OpenAI weights into our GPT model code
  id: totrans-349
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.5 将OpenAI权重加载到我们的GPT模型代码中
- en: '[PRE79]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '#1 Sets the model’s positional and token embedding weights to those specified
    in params.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将模型的定位和标记嵌入权重设置为`params`中指定的那些。'
- en: '#2 Iterates over each transformer block in the model'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 遍历模型中的每个transformer块'
- en: '#3 The np.split function is used to divide the attention and bias weights into
    three equal parts for the query, key, and value components.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用`np.split`函数将注意力和偏置权重分为查询、键和值组件的三个相等部分。'
- en: '#4 The original GPT-2 model by OpenAI reused the token embedding weights in
    the output layer to reduce the total number of parameters, which is a concept
    known as weight tying.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 OpenAI的原始GPT-2模型在输出层中重用了标记嵌入权重以减少参数总数，这是一个称为权重绑定的概念。'
- en: In the `load_weights_into_gpt` function, we carefully match the weights from
    OpenAI’s implementation with our `GPTModel` implementation. To pick a specific
    example, OpenAI stored the weight tensor for the output projection layer for the
    first transformer block as `params["blocks"][0]["attn"]["c_proj"]["w"]`. In our
    implementation, this weight tensor corresponds to `gpt.trf_blocks[b].att.out_proj
    .weight`, where `gpt` is a `GPTModel` instance.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在`load_weights_into_gpt`函数中，我们仔细地将来自OpenAI实现的权重与我们的`GPTModel`实现进行匹配。以一个具体的例子来说，OpenAI将第一个transformer块的输出投影层的权重张量存储为`params["blocks"][0]["attn"]["c_proj"]["w"]`。在我们的实现中，这个权重张量对应于`gpt.trf_blocks[b].att.out_proj
    .weight`，其中`gpt`是一个`GPTModel`实例。
- en: Developing the `load_weights_into_gpt` function took a lot of guesswork since
    OpenAI used a slightly different naming convention from ours. However, the `assign`
    function would alert us if we try to match two tensors with different dimensions.
    Also, if we made a mistake in this function, we would notice this, as the resulting
    GPT model would be unable to produce coherent text.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 开发`load_weights_into_gpt`函数花费了很多猜测工作，因为OpenAI使用的命名约定与我们略有不同。然而，如果尝试匹配两个维度不同的张量，`assign`函数会提醒我们。此外，如果我们在这个函数中犯了错误，我们会注意到这一点，因为生成的GPT模型将无法生成连贯的文本。
- en: 'Let’s now try the `load_weights_into_gpt` out in practice and load the OpenAI
    model weights into our `GPTModel` instance `gpt`:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实际尝试使用`load_weights_into_gpt`并将OpenAI模型权重加载到我们的`GPTModel`实例`gpt`中：
- en: '[PRE80]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'If the model is loaded correctly, we can now use it to generate new text using
    our previous `generate` function:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型加载正确，我们现在可以使用它通过之前的`generate`函数生成新的文本：
- en: '[PRE81]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The resulting text is as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 产生的文本如下：
- en: '[PRE82]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: We can be confident that we loaded the model weights correctly because the model
    can produce coherent text. A tiny mistake in this process would cause the model
    to fail. In the following chapters, we will work further with this pretrained
    model and fine-tune it to classify text and follow instructions.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确信我们已经正确加载了模型权重，因为模型可以生成连贯的文本。在这个过程中出现微小的错误会导致模型失败。在接下来的章节中，我们将进一步使用这个预训练模型，并对其进行微调以分类文本和遵循指令。
- en: Exercise 5.5
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.5
- en: Calculate the training and validation set losses of the `GPTModel` with the
    pretrained weights from OpenAI on the “The Verdict” dataset.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在“The Verdict”数据集上，使用OpenAI提供的预训练权重计算`GPTModel`的训练和验证集损失。
- en: Exercise 5.6
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.6
- en: Experiment with GPT-2 models of different sizes—for example, the largest 1,558
    million parameter model—and compare the generated text to the 124 million model.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同大小的GPT-2模型——例如，最大的1,558百万参数模型——并将生成的文本与1.24亿参数模型进行比较。
- en: Summary
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: When LLMs generate text, they output one token at a time.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当LLM生成文本时，它们一次输出一个标记。
- en: By default, the next token is generated by converting the model outputs into
    probability scores and selecting the token from the vocabulary that corresponds
    to the highest probability score, which is known as “greedy decoding.”
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，下一个标记是通过将模型输出转换为概率分数并选择与最高概率分数对应的词汇表中的标记来生成的，这被称为“贪婪解码”。
- en: Using probabilistic sampling and temperature scaling, we can influence the diversity
    and coherence of the generated text.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过概率采样和温度缩放，我们可以影响生成文本的多样性和连贯性。
- en: Training and validation set losses can be used to gauge the quality of text
    generated by LLM during training.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，LLM生成的文本的质量可以通过训练和验证集的损失来衡量。
- en: Pretraining an LLM involves changing its weights to minimize the training loss.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练一个大型语言模型（LLM）涉及调整其权重以最小化训练损失。
- en: The training loop for LLMs itself is a standard procedure in deep learning,
    using a conventional cross entropy loss and AdamW optimizer.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的训练循环本身是深度学习中的一个标准程序，使用传统的交叉熵损失和AdamW优化器。
- en: Pretraining an LLM on a large text corpus is time- and resource-intensive, so
    we can load openly available weights as an alternative to pretraining the model
    on a large dataset ourselves.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型文本语料库上预训练一个LLM既耗时又耗资源，因此我们可以加载公开可用的权重，作为我们自己在大数据集上预训练模型的替代方案。
