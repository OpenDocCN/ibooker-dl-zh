- en: Chapter 12\. Prospects and Perspectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The life sciences are advancing at a remarkable rate, perhaps faster than any
    other branch of science. The same can be said of deep learning: it is one of the
    most exciting, rapidly advancing areas of computer science. The combination of
    the two has the potential to change the world in dramatic, far-reaching ways.
    The effects are already starting to be felt, but those are trivial compared to
    what will likely happen over the next few decades. The union of deep learning
    with biology can do enormous good, but also great harm.'
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter we will set aside the mechanics of training deep models
    and take a broader view of the future of the field. Where does it have the greatest
    potential to solve important problems in the coming years? What obstacles must
    be overcome for that to happen? And what risks associated with this work must
    we strive to avoid?
  prefs: []
  type: TYPE_NORMAL
- en: Medical Diagnosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diagnosing disease will likely be one of the first places where deep learning
    makes its mark. In just the last few years, models have been published that match
    or exceed the accuracy of expert humans at diagnosing many important diseases.
    Examples include pneumonia, skin cancer, diabetic retinopathy, age-related macular
    degeneration, heart arrhythmia, breast cancer, and more. That list is expected
    to grow very rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of these models are based on image data: X-rays, MRIs, microscope images,
    etc. This makes sense. Deep learning’s first great successes were in the field
    of computer vision, and years of research have produced sophisticated architectures
    for analyzing image data. Applying those architectures to medical images is obvious
    low-hanging fruit. But not all of the applications are image-based. Any data that
    can be represented in numeric form is a valid input for deep models: electrocardiograms,
    blood chemistry panels, DNA sequences, gene expression profiles, vital signs,
    and much more.'
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the biggest challenge will be creating the datasets, not designing
    the architectures. Training a deep model requires lots of consistent, cleanly
    labeled data. If you want to diagnose cancer from microscope images, you need
    lots of images from patients both with and without cancer, labeled to indicate
    which are which. If you want to diagnose it from gene expression, you need lots
    of labeled gene expression profiles. The same is true for every disease you hope
    to diagnose, for every type of data you hope to diagnose it from.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, many of those datasets don’t exist. And even when appropriate datasets
    do exist, they are often smaller than we would like. The data may be noisy, collected
    from many sources with systematic differences between them. Many of the labels
    may be inaccurate. The data may only exist in a human-readable form, not one that
    is easily machine-readable: for example, free-form text written by doctors into
    patients’ medical records.'
  prefs: []
  type: TYPE_NORMAL
- en: Progress in using deep learning for medical diagnosis will depend on creating
    better datasets. In some cases, that will mean assembling and curating existing
    data. In other cases, it will mean collecting new data that is designed from the
    start to be suitable for machine learning. The latter approach will often produce
    better results, but it also is much more expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, creating those datasets could easily be disastrous for patient
    privacy. Medical records contain some of our most sensitive, most intimate information.
    If you were diagnosed with a disease, would you want your employer to know? Your
    neighbors? Your credit card company? What about advertisers who would see it as
    an opportunity to sell you health-related products?
  prefs: []
  type: TYPE_NORMAL
- en: 'Privacy concerns are especially acute for genome sequences, because they have
    a unique property: they are shared between relatives. Your parent, your child,
    your sibling each share 50% of your DNA. It is impossible to give away one person’s
    sequence without also giving away lots of information about all their relatives.
    It is also impossible to anonymize this data. Your DNA sequence identifies you
    far more precisely than your name or your fingerprint. Figuring out how to get
    the benefits of genetic data without destroying privacy will be a huge challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the factors that make data most useful for machine learning. First,
    of course, there should be lots of it. You want as much data as you can get. It
    should be clean, detailed, and precisely labeled. It should also be easily available.
    Lots of researchers will want to use it for training lots of models. And it should
    be easy to cross reference against other datasets so you can combine lots of data
    together. If DNA sequences and gene expression profiles and medical history are
    each individually useful, think how much more you can do when you have all of
    them for the same patient!
  prefs: []
  type: TYPE_NORMAL
- en: Now consider the factors that make data most prone to abuse. We don’t need to
    list them, because we just did. The factors that make data useful are exactly
    the same as the ones that make it easy to abuse. Balancing these two concerns
    will be a major challenge in the coming years.
  prefs: []
  type: TYPE_NORMAL
- en: Personalized Medicine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step beyond diagnosing an illness is deciding how to treat it. Traditionally
    this has been done in a “one size fits all” manner: a drug is recommended for
    a disease if it helps some reasonable fraction of patients with that diagnosis
    while not producing too many side effects. Your doctor might first ask if you
    have any known allergies, but that is about the limit of personalization.'
  prefs: []
  type: TYPE_NORMAL
- en: This ignores all the complexities of biology. Every person is unique. A drug
    might be effective in some people, but not in others. It might produce severe
    side effects in some people, but not in others. Some people might have enzymes
    that break the drug down very quickly, and thus require a large dose, while others
    might need a much smaller dose.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnoses are only very rough descriptions. When a doctor declares that a patient
    has diabetes or cancer, that can mean many different things. In fact, every cancer
    is unique, a different person’s cells with a different set of mutations that have
    caused them to become cancerous. A treatment that works for one might not work
    for another.
  prefs: []
  type: TYPE_NORMAL
- en: '*Personalized medicine* is an attempt to go beyond this. It tries to take into
    account every patient’s unique genetics and biochemistry to select the best treatment
    for that particular person, the one that will produce the greatest benefit with
    the fewest side effects. In principle, this could lead to a dramatic improvement
    in the quality of healthcare.'
  prefs: []
  type: TYPE_NORMAL
- en: If personalized medicine achieves its potential, computers will play a central
    role. It requires analyzing huge volumes of data, far more than a human could
    process, to predict how each possible treatment will interact with a patient’s
    unique biology and disease condition. Deep learning excels at that kind of problem.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 10](ch10.xhtml#interpretation_of_deep_models), interpretability
    and explainability are critical for this application. When the computer outputs
    a diagnosis and recommends a treatment, the doctor needs a way to double check
    those results and decide whether or not to trust them. The model must explain
    why it arrived at its conclusion, presenting the evidence in a way the doctor
    can understand and verify.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the volumes of data involved and the complexity of biological
    systems will eventually overwhelm the ability of any human to understand the explanations.
    If a model “explains” that a patient’s unique combination of mutations to 17 genes
    will make a particular treatment effective for them, no doctor can realistically
    be expected to double-check that. This creates practical, legal, and ethical issues
    that will need to be addressed. When is it right for a doctor to prescribe a treatment
    without understanding why it’s recommended? When is it right for them to ignore
    the computer’s recommendation and prescribe something else? In either case, who
    is responsible if the prescribed treatment doesn’t work or has life-threatening
    side effects?
  prefs: []
  type: TYPE_NORMAL
- en: The field is likely to develop through a series of stages. At first, computers
    will only be assistants to doctors, helping them to better understand the data.
    Eventually the computers will become so much better than humans at selecting treatments
    that it would be totally unethical for any doctor to contradict them. But that
    will take a long time, and there will be a long transition period. During that
    transition, doctors will often be tempted to trust computer models that perhaps
    shouldn’t be trusted, and to rely on their recommendations more than is justified.
    As a person creating those models, you have a responsibility to consider carefully
    how they will be used. Think critically about what results should be given, and
    how those results should be presented to minimize the chance of someone misunderstanding
    them or putting too much weight on an unreliable result.
  prefs: []
  type: TYPE_NORMAL
- en: Pharmaceutical Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of developing a new drug is hugely long and complicated. Deep learning
    can assist at many points in the process, some of which we have already discussed
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: It is also a hugely expensive process. A recent study estimated that pharmaceutical
    companies spend an average of $2.6 billion on research and development for every
    drug that gets approved. That doesn’t mean it costs billions of dollars to develop
    a single drug, of course. It means that most drug candidates fail. For every drug
    that gets approved, the company spent money investigating lots of others before
    ultimately abandoning them.
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice to say that deep learning is about to sweep in and fix all
    the problems, but that seems unlikely. Pharmaceutical development is simply too
    complicated. When a drug enters your body, it comes into contact with a hundred
    thousand other molecules. You need it to interact with the right one in just the
    right way to have the desired effect, while *not* interacting with any other molecule
    to produce toxicity or other unwanted side effects. It also needs to be sufficiently
    soluble to get into the blood, and in some cases must cross the blood–brain barrier.
    Then consider that once in the body, many drugs undergo chemical reactions that
    change them in various ways. You must consider not just the effects of the original
    drug, but also the effects of all products produced from it! Finally, add in requirements
    that it must be inexpensive to produce, have a long shelf life, be easy to administer,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Drug development is very, very hard. There are so many things to optimize for
    all at once. A deep learning model might help with one of them, but each one represents
    only a tiny part of the process.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, you can look at this in a different way. The incredible cost
    of drug development means that even small improvements can have a large impact.
    Consider that 5% of $2.6 billion is $130 million. If deep learning can lower the
    cost of drug development by 5%, that will quickly add up to billions of dollars
    saved.
  prefs: []
  type: TYPE_NORMAL
- en: The drug development process can be thought of as a funnel, as shown in [Figure 12-1](#the_drug_development_funnel).
    The earliest stages might involve screening tens or hundreds of thousands of compounds
    for desired properties. Although the number of compounds is huge, the cost of
    each assay is tiny. A few hundred of the most promising compounds might be selected
    for the much more expensive preclinical studies involving animals or cultured
    cells. Of those, perhaps 10 or fewer might advance to clinical trials on humans.
    And of those, if we are lucky, one might eventually reach the market as an approved
    drug. At each stage the number of candidate compounds shrinks, but the cost of
    each experiment grows more quickly, so most of the expense is in the later stages.
  prefs: []
  type: TYPE_NORMAL
- en: '![The drug development funnel.](Images/dlls_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. The drug development funnel.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A good strategy for reducing the cost of drug development can therefore be
    summarized as: “Fail sooner.” If a compound will ultimately be rejected, try to
    filter it out in the early stages of the development process before hundreds of
    millions of dollars have been spent on clinical trials. Deep learning has great
    potential to help with this problem. If it can more accurately predict which compounds
    will ultimately become successful drugs, the cost savings will be enormous.'
  prefs: []
  type: TYPE_NORMAL
- en: Biology Research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to its medical applications, deep learning has great potential
    to assist basic research. Modern experimental techniques tend to be high-throughput:
    they produce lots of data, thousands or millions of numbers at a time. Making
    sense of that data is a huge challenge. Deep learning is a powerful tool for analyzing
    experimental data and identifying patterns in it. We have seen some examples of
    this, such as with genomic data and microscope images.'
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting possibility is that neural networks can directly serve as
    models of biological systems. The most prominent application of this idea is to
    neurobiology. After all, “neural networks” were directly inspired by neural circuits
    in the brain. How far does the similarity go? If you train a neural network to
    perform a task, does it do it in the same way that the brain performs the task?
  prefs: []
  type: TYPE_NORMAL
- en: At least in some cases, the answer turns out to be yes! This has been demonstrated
    for a few different brain functions, including processing visual,^([1](ch12.xhtml#idm45806159783544))
    auditory,^([2](ch12.xhtml#idm45806159781320)) and movement sensations. In each
    case, a neural network was trained to perform a task. It was then compared to
    the corresponding brain region and found to match its behavior well. For example,
    particular layers in the network could be used to accurately predict the behavior
    of specific areas in the visual or auditory cortex.
  prefs: []
  type: TYPE_NORMAL
- en: This is rather remarkable. The models were not “designed” to match any particular
    brain region. In each case, the researchers simply created a generic model and
    trained it with gradient descent optimization to perform some function—and the
    solution found by the optimizer turned out to be essentially the same as the one
    discovered by millions of years of evolution. In fact, the neural network turned
    out to more closely match the brain system than other models that had been specifically
    designed to represent it!
  prefs: []
  type: TYPE_NORMAL
- en: 'To push this approach further, we will probably need to develop entirely new
    architectures. Convolutional networks were directly inspired by the visual cortex,
    so it makes sense that a CNN can serve as a model of it. But presumably there
    are other brain regions that work in very different ways. Perhaps this will lead
    to a steady back and forth between neuroscience and deep learning: discoveries
    about the brain will suggest useful new architectures for deep learning, and those
    architectures in turn can serve as models for better understanding the brain.'
  prefs: []
  type: TYPE_NORMAL
- en: And of course, there are other complicated systems in biology. What about the
    immune system? Or gene regulation? Each of these can be viewed as a “network,”
    with a huge number of parts sending information back and forth to each other.
    Can deep models be used to represent these systems and better understand how they
    work? At present, it is still an open question.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a powerful and rapidly advancing tool. If you work in the life
    sciences, you need to be aware of it, because it’s going to transform your field.
  prefs: []
  type: TYPE_NORMAL
- en: Equally, if you work in deep learning, the life sciences are an incredibly important
    domain that deserves your attention. They offer the combination of huge datasets,
    complex systems that traditional techniques struggle to describe, and problems
    that directly impact human welfare in important ways.
  prefs: []
  type: TYPE_NORMAL
- en: Whichever side you come from, we hope this book has given you the necessary
    background to start making important contributions in applying deep learning to
    the life sciences. We are at a remarkable moment in history when a set of new
    technologies is coming together to change the world. We are all privileged to
    be part of that process.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch12.xhtml#idm45806159783544-marker)) Yamins, Daniel L. K. et al. “Performance-Optimized
    Hierarchical Models Predict Neural Responses in Higher Visual Cortex.” Proceedings
    of the National Academy of Sciences 111:8619–8624\. [*https://doi.org/10.1073/pnas.1403112111*](https://doi.org/10.1073/pnas.1403112111).
    2014.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch12.xhtml#idm45806159781320-marker)) Kell, Alexander J. E. et al. “A
    Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain
    Responses, and Reveals a Cortical Processing Hierarchy.” *Neuron* 98:630–644\.
    [*https://doi.org/10.1016/j.neuron.2018.03.044*](https://doi.org/10.1016/j.neuron.2018.03.044).
    2018.
  prefs: []
  type: TYPE_NORMAL
