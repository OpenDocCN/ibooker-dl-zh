- en: Chapter 6\. Ensemble Learning and Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章\. 集成学习和随机森林
- en: Suppose you pose a complex question to thousands of random people, then aggregate
    their answers. In many cases you will find that this aggregated answer is better
    than an expert’s answer. This is called the *wisdom of the crowd*. Similarly,
    if you aggregate the predictions of a group of predictors (such as classifiers
    or regressors), you will often get better predictions than with the best individual
    predictor. A group of predictors is called an *ensemble*; thus, this technique
    is called *ensemble learning*, and an ensemble learning algorithm is called an
    *ensemble method*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你向成千上万的随机人群提出一个复杂问题，然后汇总他们的答案。在许多情况下，你会发现这个汇总的答案比专家的答案更好。这被称为*群众的智慧*。同样，如果你汇总一组预测者的预测（例如分类器或回归器），你通常会得到比最佳单个预测者更好的预测。一组预测者被称为*集成*；因此，这种技术被称为*集成学习*，集成学习算法被称为*集成方法*。
- en: As an example of an ensemble method, you can train a group of decision tree
    classifiers, each on a different random subset of the training set. You can then
    obtain the predictions of all the individual trees, and the class that gets the
    most votes is the ensemble’s prediction (see the last exercise in [Chapter 5](ch05.html#trees_chapter)).
    Such an ensemble of decision trees is called a *random forest*, and despite its
    simplicity, this is one of the most powerful machine learning algorithms available
    today.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为集成方法的一个例子，你可以训练一组决策树分类器，每个分类器在不同的随机子集上。然后你可以获得所有单个树的预测，得到最多投票的类别就是集成预测（参见第5章的最后一个练习[Chapter 5](ch05.html#trees_chapter)）。这样的一组决策树被称为*随机森林*，尽管它很简单，但这是目前最强大的机器学习算法之一。
- en: 'As discussed in [Chapter 2](ch02.html#project_chapter), you will often use
    ensemble methods near the end of a project, once you have already built a few
    good predictors, to combine them into an even better predictor. In fact, the winning
    solutions in machine learning competitions often involve several ensemble methods—most
    famously in the [Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize).
    There are some downsides, however: ensemble learning requires much more computing
    resources than using a single model (both for training and for inference), it
    can be more complex to deploy and manage, and the predictions are harder to interpret.
    But the pros often outweigh the cons.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如第2章[Chapter 2](ch02.html#project_chapter)中所述，你通常会在项目后期使用集成方法，一旦你已经建立了一些好的预测者，将它们组合成一个更好的预测者。事实上，机器学习竞赛中的获胜解决方案通常涉及几个集成方法——最著名的是在[Netflix
    Prize竞赛](https://en.wikipedia.org/wiki/Netflix_Prize)中。然而，也有一些缺点：集成学习比使用单个模型需要更多的计算资源（无论是训练还是推理），它可能更复杂，部署和管理起来更困难，而且预测更难解释。但优点通常大于缺点。
- en: In this chapter we will examine the most popular ensemble methods, including
    voting classifiers, bagging and pasting ensembles, random forests, boosting, and
    stacking ensembles.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨最流行的集成方法，包括投票分类器、袋装和粘贴集成、随机森林、提升和堆叠集成。
- en: Voting Classifiers
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票分类器
- en: Suppose you have trained a few classifiers, each one achieving about 80% accuracy.
    You may have a logistic regression classifier, an SVM classifier, a random forest
    classifier, a *k*-nearest neighbors classifier, and perhaps a few more (see [Figure 6-1](#voting_classifier_training_diagram)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经训练了一些分类器，每个分类器的准确率大约为80%。你可能有一个逻辑回归分类器、一个SVM分类器、一个随机森林分类器、一个*k*最近邻分类器，可能还有更多（参见[图6-1](#voting_classifier_training_diagram)）。
- en: '![Diagram illustrating the training of diverse classifiers, including logistic
    regression, SVM, random forest, and others, highlighting their role in ensemble
    learning.](assets/hmls_0601.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![展示不同分类器训练的图表，包括逻辑回归、SVM、随机森林等，突出它们在集成学习中的作用。](assets/hmls_0601.png)'
- en: Figure 6-1\. Training diverse classifiers
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 多样化分类器的训练
- en: 'A very simple way to create an even better classifier is to aggregate the predictions
    of each classifier: the class that gets the most votes is the ensemble’s prediction.
    This majority-vote classifier is called a *hard voting* classifier (see [Figure 6-2](#voting_classifier_prediction_diagram)).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个更好的分类器的一个非常简单的方法是汇总每个分类器的预测：得到最多投票的类别就是集成预测。这种多数投票分类器被称为*硬投票*分类器（参见[图6-2](#voting_classifier_prediction_diagram)）。
- en: '![Diagram illustrating a hard voting classifier, where diverse predictors contribute
    their predictions, with the majority vote determining the ensemble''s final prediction
    for a new instance.](assets/hmls_0602.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![说明硬投票分类器的图，其中不同的预测器贡献他们的预测，多数投票决定对新的实例的集成最终预测。](assets/hmls_0602.png)'
- en: Figure 6-2\. Hard voting classifier predictions
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. 硬投票分类器预测
- en: Somewhat surprisingly, this voting classifier often achieves a higher accuracy
    than the best classifier in the ensemble. In fact, even if each classifier is
    a *weak learner* (meaning it does only slightly better than random guessing),
    the ensemble can still be a *strong learner* (achieving high accuracy), provided
    there are a sufficient number of weak learners in the ensemble and they are sufficiently
    diverse (i.e., if they focus on different aspects of the data and make different
    kinds of errors).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 比较令人惊讶的是，这种投票分类器通常比集成中的最佳分类器具有更高的准确性。事实上，即使每个分类器都是一个**弱学习器**（意味着它只比随机猜测略好），只要集成中有足够数量的弱学习器，并且它们足够多样化（即，如果它们关注数据的不同方面并犯不同类型的错误），集成仍然可以是一个**强学习器**（实现高准确性）。
- en: 'How is this possible? The following analogy can help shed some light on this
    mystery. Suppose you have a slightly biased coin that has a 51% chance of coming
    up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will
    generally get more or less 510 heads and 490 tails, and hence a majority of heads.
    If you do the math, you will find that the probability of obtaining a majority
    of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher
    the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This
    is due to the *law of large numbers*: as you keep tossing the coin, the ratio
    of heads gets closer and closer to the probability of heads (51%). [Figure 6-3](#law_of_large_numbers_plot)
    shows 10 series of biased coin tosses. You can see that as the number of tosses
    increases, the ratio of heads approaches 51%. Eventually all 10 series end up
    so close to 51% that they are consistently above 50%.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能呢？以下类比可以帮助揭示这个谜团。假设你有一个略微倾斜的硬币，有51%的概率出现正面，49%的概率出现反面。如果你抛掷它1000次，你通常会得到大约510个正面和490个反面，因此大多数是正面。如果你做数学计算，你会发现1000次抛掷后获得大多数正面的概率接近75%。你抛掷硬币的次数越多，概率就越高（例如，抛掷10000次，概率超过97%）。这是由于**大数定律**：随着你不断地抛掷硬币，正面的比例越来越接近正面的概率（51%）。[图6-3](#law_of_large_numbers_plot)显示了10次倾斜硬币抛掷的系列。你可以看到，随着抛掷次数的增加，正面的比例接近51%。最终，所有10个系列都接近51%，它们始终高于50%。
- en: '![A line chart illustrating 10 series of biased coin tosses, showing that as
    the number of tosses increases, the heads ratio approaches 51%, demonstrating
    the law of large numbers.](assets/hmls_0603.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![展示10次倾斜硬币抛掷的系列线图，显示随着抛掷次数的增加，正面比例接近51%，证明了大数定律。](assets/hmls_0603.png)'
- en: Figure 6-3\. The law of large numbers
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 大数定律
- en: Similarly, suppose you build an ensemble containing 1,000 classifiers that are
    individually correct only 51% of the time (barely better than random guessing).
    If you predict the majority voted class, you can hope for up to 75% accuracy!
    However, this is only true if all classifiers are perfectly independent, making
    uncorrelated errors, which is clearly not the case because they are trained on
    the same data. They are likely to make the same types of errors, so there will
    be many majority votes for the wrong class, reducing the ensemble’s accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，假设你构建了一个包含1000个分类器的集成，这些分类器各自只有51%的时间是正确的（几乎不比随机猜测好）。如果你预测多数投票的类别，你可以希望达到高达75%的准确性！然而，这只在所有分类器完全独立，做出不相关的错误的情况下才成立，这显然是不可能的，因为它们是在相同的数据上训练的。它们很可能会犯相同类型的错误，因此会有许多多数投票错误类别，从而降低集成的准确性。
- en: Tip
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Ensemble methods work best when the predictors are as independent from one another
    as possible. One way to get diverse classifiers is to train them using very different
    algorithms. This increases the chance that they will make very different types
    of errors, improving the ensemble’s accuracy. You can also play with the model
    hyperparameters to get diverse models, or train the models on different subsets
    of the data, as we will see.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在预测器尽可能相互独立时效果最好。获得不同分类器的一种方法是通过使用非常不同的算法来训练它们。这增加了它们做出非常不同类型错误的概率，从而提高了集成的准确性。你也可以调整模型超参数来获得不同的模型，或者在不同的数据子集上训练模型，正如我们将看到的。
- en: 'Scikit-Learn provides a `VotingClassifier` class that’s quite easy to use:
    just give it a list of name/predictor pairs, and use it like a normal classifier.
    Let’s try it on the moons dataset (introduced in [Chapter 5](ch05.html#trees_chapter)).
    We will load and split the moons dataset into a training set and a test set, then
    we’ll create and train a voting classifier composed of three diverse classifiers:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 提供了一个非常容易使用的 `VotingClassifier` 类：只需给它一个名称/预测器对的列表，就可以像使用普通分类器一样使用它。让我们在月亮数据集（在第
    5 章中介绍）上试一试。我们将加载并分割月亮数据集为训练集和测试集，然后我们将创建并训练一个由三个不同分类器组成的投票分类器：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When you fit a `VotingClassifier`, it clones every estimator and fits the clones.
    The original estimators are available via the `estimators` attribute, while the
    fitted clones are available via the `estimators_` attribute. If you prefer a dict
    rather than a list, you can use `named_estimators` or `named_estimators_` instead.
    To begin, let’s look at each fitted classifier’s accuracy on the test set:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拟合一个 `VotingClassifier` 时，它会克隆每个估计器并拟合克隆。原始估计器可以通过 `estimators` 属性访问，而拟合的克隆可以通过
    `estimators_` 属性访问。如果你更喜欢字典而不是列表，可以使用 `named_estimators` 或 `named_estimators_`。首先，让我们看看每个拟合分类器在测试集上的准确率：
- en: '[PRE1] `...` `` `lr = 0.864` `rf = 0.896` `svc = 0.896` `` [PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] `...` `` `lr = 0.864` `rf = 0.896` `svc = 0.896` `` [PRE2]'
- en: '[PRE3][PRE4]``py[PRE5][PRE6] >>> voting_clf.predict(X_test[:1]) `array([1])`
    `>>>` `[``clf``.``predict``(``X_test``[:``1``])` `for` `clf` `in` `voting_clf``.``estimators_``]`
    `` `[array([1]), array([1]), array([0])]` `` [PRE7][PRE8][PRE9] >>> voting_clf.score(X_test,
    y_test) `0.912` [PRE10]`` There you have it! The voting classifier outperforms
    all the individual classifiers.    If all classifiers are able to estimate class
    probabilities (i.e., if they all have a `predict_proba()` method), then you should
    generally tell Scikit-Learn to predict the class with the highest class probability,
    averaged over all the individual classifiers. This is called *soft voting*. It
    often achieves higher performance than hard voting because it gives more weight
    to highly confident votes. All you need to do is set the voting classifier’s `voting`
    hyperparameter to `"soft"`, and ensure that all classifiers can estimate class
    probabilities. This is not the case for the `SVC` class by default, so you need
    to set its `probability` hyperparameter to `True` (this will make the `SVC` class
    use cross-validation to estimate class probabilities, slowing down training, and
    it will add a `predict_proba()` method). Let’s try that:    [PRE11]` `>>>` `voting_clf``.``fit``(``X_train``,`
    `y_train``)` [PRE12] [PRE13]   [PRE14] ``We reach 92% accuracy simply by using
    soft voting—not bad!    ###### Tip    Soft voting works best when the estimated
    probabilities are well-calibrated. If they are not, you can use `sklearn.calibration.CalibratedClassifierCV`
    to calibrate them (see [Chapter 3](ch03.html#classification_chapter)).`` [PRE15]`
    [PRE16][PRE17][PRE18][PRE19][PRE20] [PRE21]`py[PRE22]``py[PRE23][PRE24] # Bagging
    and Pasting    One way to get a diverse set of classifiers is to use very different
    training algorithms, as just discussed. Another way is to use the same training
    algorithm for every predictor but train them on different random subsets of the
    training set. When sampling is performed *with* replacement,⁠^([1](ch06.html#id1744))
    this method is called [*bagging*](https://homl.info/20)⁠^([2](ch06.html#id1745))
    (short for *bootstrap aggregating*⁠^([3](ch06.html#id1746))). When sampling is
    performed *without* replacement, it is called [*pasting*](https://homl.info/21).⁠^([4](ch06.html#id1747))    In
    other words, both bagging and pasting allow training instances to be sampled several
    times across multiple predictors, but only bagging allows training instances to
    be sampled several times for the same predictor. This sampling and training process
    is represented in [Figure 6-4](#bagging_training_diagram).  ![Diagram illustrating
    the bagging process, where multiple predictors are trained on random samples with
    replacement from the training set.](assets/hmls_0604.png)  ###### Figure 6-4\.
    Bagging and pasting involve training several predictors on different random samples
    of the training set    Once all predictors are trained, the ensemble can make
    a prediction for a new instance by simply aggregating the predictions of all predictors.
    For classification, the aggregation function is typically the *statistical mode*
    (i.e., the most frequent prediction, just like with a hard voting classifier),
    and for regression it’s usually just the average. Each individual predictor has
    a higher bias than if it were trained on the original training set, but aggregation
    reduces both bias and variance.⁠^([5](ch06.html#id1749))    To get an intuition
    of why this is the case, imagine that you trained two regressors to predict house
    prices. The first underestimates the prices by $40,000 on average, while the second
    overestimates them by $50,000 on average. Assuming these regressors are 100% independent
    and their predictions follow a normal distribution, if you compute the average
    of the two predictions, the result will overestimate the prices by only (–40,000
    + 50,000)/2 = $5,000 on average: that’s a much lower bias! Similarly, if both
    predictors have a $10,000 standard deviation (i.e., a variance of 100,000,000),
    then the average prediction will have a variance of (10,000² + 10,000²)/2² = 50,000,000
    (i.e., the standard deviation will be $7,071). The variance is halved!    In practice,
    the ensemble often ends up with a similar bias but a lower variance than a single
    predictor trained on the original training set. Therefore it works best with high-variance
    and low-bias models (e.g., ensembles of decision trees, not ensembles of linear
    regressors).    ###### Tip    Prefer bagging when your dataset is noisy or your
    model is prone to overfitting (e.g., deep decision tree). Otherwise, prefer pasting
    as it avoids redundancy during training, making it a bit more computationally
    efficient.    As you can see in [Figure 6-4](#bagging_training_diagram), predictors
    can all be trained in parallel, via different CPU cores or even different servers.
    Similarly, predictions can be made in parallel. This is one of the reasons bagging
    and pasting are such popular methods: they scale very well.    ## Bagging and
    Pasting in Scikit-Learn    Scikit-Learn offers a simple API for both bagging and
    pasting: the `BaggingClassifier` class (or `BaggingRegressor` for regression).
    The following code trains an ensemble of 500 decision tree classifiers:⁠^([6](ch06.html#id1752))
    each is trained on 100 training instances randomly sampled from the training set
    with replacement (this is an example of bagging, but if you want to use pasting
    instead, just set `bootstrap=False`). The `n_jobs` parameter tells Scikit-Learn
    the number of CPU cores to use for training and predictions, and `–1` tells Scikit-Learn
    to use all available cores:    [PRE25]py    ###### Note    A `BaggingClassifier`
    automatically performs soft voting instead of hard voting if the base classifier
    can estimate class probabilities (i.e., if it has a `predict_proba()` method),
    which is the case with decision tree classifiers.    [Figure 6-5](#decision_tree_without_and_with_bagging_plot)
    compares the decision boundary of a single decision tree with the decision boundary
    of a bagging ensemble of 500 trees (from the preceding code), both trained on
    the moons dataset. As you can see, the ensemble’s predictions will likely generalize
    much better than the single decision tree’s predictions: the ensemble has a comparable
    bias but a smaller variance (it makes roughly the same number of errors on the
    training set, but the decision boundary is less irregular).    Bagging introduces
    a bit more diversity in the subsets that each predictor is trained on, so bagging
    ends up with a slightly higher bias than pasting; but the extra diversity also
    means that the predictors end up being less correlated, so the ensemble’s variance
    is reduced. Overall, bagging often results in better models, which explains why
    it’s generally preferred. But if you have spare time and CPU power, you can use
    cross-validation to evaluate both bagging and pasting, and select the one that
    works best.  ![Comparison of a single decision tree''s decision boundaries versus
    those of a bagging ensemble with 500 trees, illustrating variance reduction through
    bagging.](assets/hmls_0605.png)  ###### Figure 6-5\. A single decision tree (left)
    versus a bagging ensemble of 500 trees (right)    ## Out-of-Bag Evaluation    With
    bagging, some training instances may be sampled several times for any given predictor,
    while others may not be sampled at all. By default, a `BaggingClassifier` samples
    *m* training instances with replacement (`bootstrap=True`), where *m* is the size
    of the training set. With this process, it can be shown mathematically that only
    about 63% of the training instances are sampled on average for each predictor.⁠^([7](ch06.html#id1757))
    The remaining 37% of the training instances that are not sampled are called *out-of-bag*
    (OOB) instances. Note that they are not the same 37% for all predictors.    A
    bagging ensemble can be evaluated using OOB instances, without the need for a
    separate validation set: indeed, if there are enough estimators, then each instance
    in the training set will likely be an OOB instance of several estimators, so these
    estimators can be used to make a fair ensemble prediction for that instance. Once
    you have a prediction for each instance, you can compute the ensemble’s prediction
    accuracy (or any other metric).    In Scikit-Learn, you can set `oob_score=True`
    when creating a `BaggingClassifier` to request an automatic OOB evaluation after
    training. The following code demonstrates this. The resulting evaluation score
    is available in the `oob_score_` attribute:    [PRE26]py`` `...` [PRE27]` [PRE28][PRE29]
    [PRE30]py` According to this OOB evaluation, this `BaggingClassifier` is likely
    to achieve about 89.6% accuracy on the test set. Let’s verify this:    [PRE31]py
    `>>>` `accuracy_score``(``y_test``,` `y_pred``)` `` `0.92` `` [PRE32]py   [PRE33]py
    [PRE34]py`` [PRE35]py[PRE36]`py` [PRE37]py` `>>>` `rnd_clf` `=` `RandomForestClassifier``(``n_estimators``=``500``,`
    `random_state``=``42``)` [PRE38]py [PRE39] [PRE40]`` [PRE41]` # Boosting    *Boosting*
    (originally called *hypothesis boosting*) refers to any ensemble method that can
    combine several weak learners into a strong learner. The general idea of most
    boosting methods is to train predictors sequentially, each trying to correct its
    predecessor. There are many boosting methods available, but by far the most popular
    are [*AdaBoost*](https://homl.info/26)⁠^([13](ch06.html#id1792)) (short for *adaptive
    boosting*) and *gradient boosting*. Let’s start with AdaBoost.    ## AdaBoost    One
    way for a new predictor to correct its predecessor is to pay a bit more attention
    to the training instances that the predecessor underfit. This results in new predictors
    focusing more and more on the hard cases. This is the technique used by AdaBoost.    For
    example, when training an AdaBoost classifier, the algorithm first trains a base
    classifier (such as a decision tree) and uses it to make predictions on the training
    set. The algorithm then increases the relative weight of misclassified training
    instances. Then it trains a second classifier, using the updated weights, and
    again makes predictions on the training set, updates the instance weights, and
    so on (see [Figure 6-7](#adaboost_training_diagram)).  ![Diagram illustrating
    the sequential training process of AdaBoost, showing how instance weights and
    predictions are updated through stages.](assets/hmls_0607.png)  ###### Figure
    6-7\. AdaBoost sequential training with instance weight updates    [Figure 6-8](#boosting_plot)
    shows the decision boundaries of five consecutive predictors on the moons dataset
    (in this example, each predictor is a highly regularized SVM classifier with an
    RBF kernel).⁠^([14](ch06.html#id1793)) The first classifier gets many instances
    wrong, so their weights get boosted. The second classifier therefore does a better
    job on these instances, and so on. The plot on the right represents the same sequence
    of predictors, except that the learning rate is halved (i.e., the misclassified
    instance weights are boosted much less at every iteration). As you can see, this
    sequential learning technique has some similarities with gradient descent, except
    that instead of tweaking a single predictor’s parameters to minimize a cost function,
    AdaBoost adds predictors to the ensemble, gradually making it better.  ![Two diagrams
    showing decision boundaries of five consecutive SVM classifiers on the moons dataset,
    with learning rates 1 and 0.5, illustrating the impact of learning rate on AdaBoost’s
    performance.](assets/hmls_0608.png)  ###### Figure 6-8\. Decision boundaries of
    consecutive predictors    ###### Warning    There is one important drawback to
    this sequential learning technique: training cannot be parallelized since each
    predictor can only be trained after the previous predictor has been trained and
    evaluated. As a result, it does not scale as well as bagging or pasting.    Once
    all predictors are trained, the ensemble makes predictions very much like bagging
    or pasting, except that predictors have different weights depending on their overall
    accuracy on the weighted training set.    Let’s take a closer look at the AdaBoost
    algorithm. Each instance weight *w*^((*i*)) is initially set to 1/*m*, so their
    sum is 1\. A first predictor is trained, and its weighted error rate *r*[1] is
    computed on the training set; see [Equation 6-1](#weighted_error_rate).    #####
    Equation 6-1\. Weighted error rate of the j^(th) predictor  <mrow><msub><mi>r</mi><mi>j</mi></msub>
    <mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo>
    <mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup>
    <mo>≠</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></mstyle>
    <mi>m</mi></munderover> <msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle>
    <mtext>where</mtext> <msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mtext>is</mtext>
    <mtext>the</mtext> <msup><mi>j</mi><mtext>th</mtext></msup> <mtext>predictor’s</mtext>
    <mtext>prediction</mtext><mtext>for</mtext> <mtext>the</mtext> <msup><mi>i</mi>
    <mtext>th</mtext></msup> <mtext>instance</mtext></mrow>  The predictor’s weight
    *α*[*j*] is then computed using [Equation 6-2](#predictor_weight), where *η* is
    the learning rate hyperparameter (defaults to 1).⁠^([15](ch06.html#id1795)) The
    more accurate the predictor is, the higher its weight will be. If it is just guessing
    randomly, then its weight will be close to zero. However, if it is most often
    wrong (i.e., less accurate than random guessing), then its weight will be negative.    #####
    Equation 6-2\. Predictor weight  $dollar-sign alpha Subscript j Baseline equals
    eta log StartFraction 1 minus r Subscript j Baseline Over r Subscript j Baseline
    EndFraction dollar-sign$  Next, the AdaBoost algorithm updates the instance weights,
    using [Equation 6-3](#instance_weight_update), which boosts the weights of the
    misclassified instances and encourages the next predictor to pay more attention
    to them.    ##### Equation 6-3\. Weight update rule  $dollar-sign StartLayout
    1st Row 1st Column Blank 2nd Column for i equals 1 comma 2 comma period period
    period comma m 2nd Row 1st Column Blank 2nd Column w Superscript left-parenthesis
    i right-parenthesis Baseline left-arrow StartLayout Enlarged left-brace 1st Row  w
    Superscript left-parenthesis i right-parenthesis Baseline if ModifyingAbove y
    With caret Superscript left-parenthesis i right-parenthesis Baseline equals y
    Superscript left-parenthesis i right-parenthesis Baseline 2nd Row  w Superscript
    left-parenthesis i right-parenthesis Baseline exp left-parenthesis alpha Subscript
    j Baseline right-parenthesis if ModifyingAbove y With caret Superscript left-parenthesis
    i right-parenthesis Baseline not-equals y Superscript left-parenthesis i right-parenthesis
    EndLayout EndLayout dollar-sign$  Then all the instance weights are normalized
    to ensure that their sum is once again 1 (i.e., they are divided by $sigma-summation
    Underscript i equals 1 Overscript m Endscripts w Superscript left-parenthesis
    i right-parenthesis$ ).    Finally, a new predictor is trained using the updated
    weights, and the whole process is repeated: the new predictor’s weight is computed,
    the instance weights are updated, then another predictor is trained, and so on.
    The algorithm stops when the desired number of predictors is reached, or when
    a perfect predictor is found.    To make predictions, AdaBoost simply computes
    the predictions of all the predictors and weighs them using the predictor weights
    *α*[*j*]. The predicted class is the one that receives the majority of weighted
    votes (see [Equation 6-4](#adaboost_prediction)).    ##### Equation 6-4\. AdaBoost
    predictions  <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mrow><mo>(</mo>
    <mi mathvariant="bold">x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo form="prefix">argmax</mo>
    <mi>k</mi></munder> <mrow><munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle
    scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle>
    <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>j</mi></msub> <mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac>
    <mi>N</mi></munderover> <msub><mi>α</mi> <mi>j</mi></msub></mrow> <mtext>where</mtext>
    <mi>N</mi> <mtext>is</mtext> <mtext>the</mtext> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>predictors</mtext></mrow>  Scikit-Learn uses a multiclass version of AdaBoost
    called [*SAMME*](https://homl.info/27)⁠^([16](ch06.html#id1797)) (which stands
    for *Stagewise Additive Modeling using a Multiclass Exponential loss function*).
    When there are just two classes, SAMME is equivalent to AdaBoost.    The following
    code trains an AdaBoost classifier based on 30 *decision stumps* using Scikit-Learn’s
    `AdaBoostClassifier` class (as you might expect, there is also an `AdaBoostRegressor`
    class). A decision stump is a decision tree with `max_depth=1`—in other words,
    a tree composed of a single decision node plus two leaf nodes. This is the default
    base estimator for the `AdaBoostClassifier` class:    [PRE42]    ###### Tip    If
    your AdaBoost ensemble is overfitting the training set, you can try reducing the
    number of estimators or more strongly regularizing the base estimator.    ## Gradient
    Boosting    Another very popular boosting algorithm is [*gradient boosting*](https://homl.info/28).⁠^([17](ch06.html#id1803))
    Just like AdaBoost, gradient boosting works by sequentially adding predictors
    to an ensemble, each one correcting its predecessor. However, instead of tweaking
    the instance weights at every iteration like AdaBoost does, this method tries
    to fit the new predictor to the *residual errors* made by the previous predictor.    Let’s
    go through a simple regression example, using decision trees as the base predictors;
    this is called *gradient tree boosting*, or *gradient boosted regression trees*
    (GBRT). First, let’s generate a noisy quadratic dataset and fit a `DecisionTreeRegressor`
    to it:    [PRE43]    Next, we’ll train a second `DecisionTreeRegressor` on the
    residual errors made by the first predictor:    [PRE44]    And then we’ll train
    a third regressor on the residual errors made by the second predictor:    [PRE45]    Now
    we have an ensemble containing three trees. It can make predictions on a new instance
    simply by adding up the predictions of all the trees:    [PRE46]   [PRE47]` [Figure 6-9](#gradient_boosting_plot)
    represents the predictions of these three trees in the left column, and the ensemble’s
    predictions in the right column. In the first row, the ensemble has just one tree,
    so its predictions are exactly the same as the first tree’s predictions. In the
    second row, a new tree is trained on the residual errors of the first tree. On
    the right you can see that the ensemble’s predictions are equal to the sum of
    the predictions of the first two trees. Similarly, in the third row another tree
    is trained on the residual errors of the second tree. You can see that the ensemble’s
    predictions gradually get better as trees are added to the ensemble.    You can
    use Scikit-Learn’s `GradientBoostingRegressor` class to train GBRT ensembles more
    easily (there’s also a `GradientBoostingClassifier` class for classification).
    Much like the `RandomForestRegressor` class, it has hyperparameters to control
    the growth of decision trees (e.g., `max_depth`, `min_samples_leaf`), as well
    as hyperparameters to control the ensemble training, such as the number of trees
    (`n_estimators`). The following code creates the same ensemble as the previous
    one:    [PRE48]  ![Diagram illustrating gradient boosting; the left column shows
    individual predictors trained on residuals, and the right column shows the ensemble''s
    progressive predictions.](assets/hmls_0609.png)  ###### Figure 6-9\. In this depiction
    of gradient boosting, the first predictor (top left) is trained normally, then
    each consecutive predictor (middle left and lower left) is trained on the previous
    predictor’s residuals; the right column shows the resulting ensemble’s predictions    The
    `learning_rate` hyperparameter scales the contribution of each tree. If you set
    it to a low value, such as `0.05`, you will need more trees in the ensemble to
    fit the training set, but the predictions will usually generalize better. This
    is a regularization technique called *shrinkage*. [Figure 6-10](#gbrt_learning_rate_plot)
    shows two GBRT ensembles trained with different hyperparameters: the one on the
    left does not have enough trees to fit the training set, while the one on the
    right has about the right amount. If we added more trees, the GBRT would start
    to overfit the training set. As usual, you can use cross-validation to find the
    optimal learning rate, using `GridSearchCV` or `RandomizedSearchCV`.  ![Comparison
    of GBRT ensemble predictions with insufficient predictors (left graph) versus
    sufficient predictors (right graph).](assets/hmls_0610.png)  ###### Figure 6-10\.
    GBRT ensembles with not enough predictors (left) and just enough (right)    To
    find the optimal number of trees, you could also perform cross-validation, but
    there’s a simpler way: if you set the `n_iter_no_change` hyperparameter to an
    integer value, say 10, then the `GradientBoostingRegressor` will automatically
    stop adding more trees during training if it sees that the last 10 trees didn’t
    help. This is simply early stopping (introduced in [Chapter 4](ch04.html#linear_models_chapter)),
    but with a little bit of patience: it tolerates having no progress for a few iterations
    before it stops. Let’s train the ensemble using early stopping:    [PRE49]    If
    you set `n_iter_no_change` too low, training may stop too early and the model
    will underfit. But if you set it too high, it will overfit instead. We also set
    a fairly small learning rate and a high number of estimators, but the actual number
    of estimators in the trained ensemble is much lower, thanks to early stopping:    [PRE50]   ``When
    `n_iter_no_change` is set, the `fit()` method automatically splits the training
    set into a smaller training set and a validation set: this allows it to evaluate
    the model’s performance each time it adds a new tree. The size of the validation
    set is controlled by the `validation_fraction` hyperparameter, which is 10% by
    default. The `tol` hyperparameter determines the maximum performance improvement
    that still counts as negligible. It defaults to 0.0001.    The `GradientBoostingRegressor`
    class also supports a `subsample` hyperparameter, which specifies the fraction
    of training instances to be used for training each tree. For example, if `subsample=0.25`,
    then each tree is trained on 25% of the training instances, selected randomly.
    As you can probably guess by now, this technique trades a higher bias for a lower
    variance. It also speeds up training considerably. This is called *stochastic
    gradient boosting*.`` [PRE51]``  [PRE52] [PRE53]`py  [PRE54]py [PRE55]py`` [PRE56]py[PRE57][PRE58][PRE59][PRE60]py[PRE61]py`
    [PRE62]```'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
