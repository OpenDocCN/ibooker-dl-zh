<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">9</span></span> <span class="chapter-title-text">Mastering agent prompts with prompt flow</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Understanding systematic prompt engineering and setting up your first prompt flow</li> 
    <li class="readable-text" id="p3">Crafting an effective profile/persona prompt</li> 
    <li class="readable-text" id="p4">Evaluating profiles: Rubrics and grounding</li> 
    <li class="readable-text" id="p5">Grounding evaluation of a large language model profile</li> 
    <li class="readable-text" id="p6">Comparing prompts: Getting the perfect profile </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In this chapter, we delve into the Test Changes Systematically prompt engineering strategy. If you recall, we covered the grand strategies of the OpenAI prompt engineering framework in chapter 2. These strategies are instrumental in helping us build better prompts and, consequently, better agent profiles and personas. Understanding this role is key to our prompt engineering journey.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>Test Changes Systematically is such a core facet of prompt engineering that Microsoft developed a tool around this strategy called <em>prompt flow</em>, described later in this chapter. Before getting to prompt flow, we need to understand why we need systemic prompt engineering.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_121"><span class="num-string">9.1</span> Why we need systematic prompt engineering</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Prompt engineering, by its nature, is an iterative process. When building a prompt, you’ll often iterate and evaluate. To see this concept in action, consider the simple application of prompt engineering to a ChatGPT question.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>You can follow along by opening your browser to ChatGPT (<a href="https://chat.openai.com/">https://chat.openai.com/</a>), entering the following (text) prompt into ChatGPT, and clicking the Send Message button (an example of this conversation is shown in figure 9.1, on the left side):</p> 
  </div> 
  <div class="readable-text prompt" id="p12"> 
   <p>can you recommend something<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p13">  
   <img alt="figure" src="../Images/9-1.png" width="1012" height="474"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.1</span> The differences in applying prompt engineering and iterating</h5>
  </div> 
  <div class="readable-text" id="p14"> 
   <p>We can see that the response from ChatGPT is asking for more information. Go ahead and open a new conversation with ChatGPT, and enter the following prompt, as shown in figure 9.1, on the right side:</p> 
  </div> 
  <div class="readable-text prompt" id="p15"> 
   <p>Can you please recommend a time travel movie set in the medieval period.</p> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>The results in figure 9.1 show a clear difference between leaving out details and being more specific in your request. We just applied the tactic of politely Writing Clear Instructions, and ChatGPT provided us with a good recommendation. But also notice how ChatGPT itself guides the user into better prompting. The refreshed screen shown in figure 9.2 shows the OpenAI prompt engineering strategies.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p17">  
   <img alt="figure" src="../Images/9-2.png" width="1100" height="885"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.2</span> OpenAI prompt engineering strategies, broken down by agent component</h5>
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>We just applied simple iteration to improve our prompt. We can extend this example by using a system prompt/message. Figure 9.3 demonstrates the use and role of the system prompt in iterative communication. In chapter 2, we used the system message/prompt in various examples.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p19">  
   <img alt="figure" src="../Images/9-3.png" width="1100" height="496"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.3</span> The messages to and from an LLM conversation and the iteration of messages</h5>
  </div> 
  <div class="readable-text" id="p20"> 
   <p>You can also try this in ChatGPT. This time, enter the following prompt and include the word <em>system</em> in lowercase, followed by a new line (enter a new line in the message window without sending the message by pressing Shift-Enter):</p> 
  </div> 
  <div class="readable-text prompt" id="p21"> 
   <p>system</p> 
  </div> 
  <div class="readable-text prompt" id="p22"> 
   <p>You are an expert on time travel movies.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>ChatGPT will respond with some pleasant comments, as shown in figure 9.4. Because of this, it’s happy to accept its new role and asks for any follow-up questions. Now enter the following generic prompt as we did previously:</p> 
  </div> 
  <div class="readable-text prompt" id="p24"> 
   <p>can you recommend something<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p25">  
   <img alt="figure" src="../Images/9-4.png" width="902" height="872"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.4</span> The effect of adding a system prompt to our previous conversation</h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <p>We’ve just seen the iteration of refining a prompt, the prompt engineering, to extract a better response. This was accomplished over three different conversations using the ChatGPT UI. While not the most efficient way, it works.</p> 
  </div> 
  <div class="readable-text intended-text" id="p27"> 
   <p>However, we haven’t defined the iterative flow for evaluating the prompt and determining when a prompt is effective. Figure 9.5 shows a systemic method of prompt engineering using a system of iteration and evaluation.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p28">  
   <img alt="figure" src="../Images/9-5.png" width="1012" height="489"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.5</span> The systemic method of prompt engineering</h5>
  </div> 
  <div class="readable-text" id="p29"> 
   <p>The system of iterating and evaluating prompts covers the broad Test Changes Systemically strategy. Evaluating the performance and effectiveness of prompts is still new, but we’ll use techniques from education, such as rubrics and grounding, which we’ll explore in a later section of this chapter. However, as spelled out in the next section, we need to understand the difference between a persona and an agent profile before we do so.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_122"><span class="num-string">9.2</span> Understanding agent profiles and personas</h2> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>An <em>agent profile</em> is an encapsulation of component prompts or messages that describe an agent. It includes the agent’s persona, special instructions, and other strategies that can guide the user or other agent consumers.</p> 
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>Figure 9.6 shows the main elements of an agent profile. These elements map to prompt engineering strategies described in this book. Not all agents will use all the elements of a full agent profile.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p33">  
   <img alt="figure" src="../Images/9-6.png" width="1100" height="953"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.6</span> The component parts of an agent profile</h5>
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>At a basic level, an <em>agent profile</em> is a set of prompts describing the agent. It may include other external elements related to actions/tools, knowledge, memory, reasoning, evaluation, planning, and feedback. The combination of these elements comprises an entire agent prompt profile.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>Prompts are the heart of an agent’s function. A prompt or set of prompts drives each of the agent components in the profile. For actions/tools, these prompts are well defined, but as we’ve seen, prompts for memory and knowledge can vary significantly by use case.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>The definition of an AI agent profile is more than just a system prompt. Prompt flow can allow us to construct the prompts and code comprising the agent profile but also include the ability to evaluate its effectiveness. In the next section, we’ll open up prompt flow and start using it.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_123"><span class="num-string">9.3</span> Setting up your first prompt flow</h2> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Prompt flow is a tool developed by Microsoft within its Azure Machine Learning Studio platform. The tool was later released as an open source project on GitHub, where it has attracted more attention and use. While initially intended as an application platform, it has since shown its strength in developing and evaluating prompts/ profiles.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>Because prompt flow was initially developed to run on Azure as a service, it features a robust core architecture. The tool supports multi-threaded batch processing, which makes it ideal for evaluating prompts at scale. The following section will examine the basics of starting with prompt flow.</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_124"><span class="num-string">9.3.1</span> Getting started</h3> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>There are a few prerequisites to undertake before working through the exercises in this book. The relevant prerequisites for this section and chapter are shown in the following list; make sure to complete them before attempting the exercises:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p42"> <em>Visual Studio Code (VS Code)</em> —Refer to appendix A for installation instructions, including additional extensions. </li> 
   <li class="readable-text" id="p43"> <em>Prompt flow, VS Code extension</em> —Refer to appendix A for details on installing extensions. </li> 
   <li class="readable-text" id="p44"> <em>Python virtual environment</em> —Refer to appendix A for details on setting up a virtual environment. </li> 
   <li class="readable-text" id="p45"> <em>Install prompt flow packages</em> —Within your virtual environment, do a quick <code>pip</code> <code>install</code>, as shown here: </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p46"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install promptflow promptflow-tools</pre>  
   </div> 
  </div> 
  <ul> 
   <li class="readable-text" id="p47"> <em>LLM (GPT-4 or above)</em> —You’ll need access to GPT-4 or above through OpenAI or Azure OpenAI Studio. Refer to appendix B if you need assistance accessing these resources. </li> 
   <li class="readable-text" id="p48"> <em>Book’s source code</em> —Clone the book’s source code to a local folder; refer to appendix A if you need help cloning the repository. </li> 
  </ul> 
  <div class="readable-text" id="p49"> 
   <p>Open up VS Code to the book’s source code folder, <code>chapter</code> <code>3</code>. Ensure that you have a virtual environment connected and have installed the prompt flow packages and extension.</p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>First, you’ll want to create a connection to your LLM resource within the prompt flow extension. Open the prompt flow extension within VS Code, and then click to open the connections. Then, click the plus sign beside the LLM resource to create a new connection, as shown in figure 9.7.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p51">  
   <img alt="figure" src="../Images/9-7.png" width="1012" height="779"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.7</span> Creating a new prompt flow LLM connection</h5>
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>This will open a YAML file where you’ll need to populate the connection name and other information relevant to your connection. Follow the directions, and don’t enter API keys into the document, as shown in figure 9.8.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p53">  
   <img alt="figure" src="../Images/9-8.png" width="1012" height="444"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.8</span> Setting the connection information for your LLM resource</h5>
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>When the connection information is entered, click the Create Connection link at the bottom of the document. This will open a terminal prompt below the document, asking you to enter your key. Depending on your terminal configuration, you may be unable to paste (Ctrl-V, Cmd-V). Alternatively, you can paste the key by hovering the mouse cursor over the terminal and right-clicking on Windows.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>We’ll now test the connection by first opening the simple flow in the <code>chapter_09/promptflow/simpleflow</code> folder. Then, open the <code>flow.dag.yaml</code> file in VS Code. This is a YAML file, but the prompt flow extension provides a visual editor that is accessible by clicking the Visual Editor link at the top of the file, as shown in figure 9.9.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p56">  
   <img alt="figure" src="../Images/9-9.png" width="1012" height="464"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.9</span> Opening the prompt flow visual editor</h5>
  </div> 
  <div class="readable-text" id="p57"> 
   <p>After the visual editor window is opened, you’ll see a graph representing the flow and the flow blocks. Double-click the recommender block, and set the connection name, API type, and model or deployment name, as shown in figure 9.10.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p58">  
   <img alt="figure" src="../Images/9-10.png" width="1012" height="449"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.10</span> Setting the LLM connection details</h5>
  </div> 
  <div class="readable-text" id="p59"> 
   <p>A prompt flow is composed of a set of blocks starting with an <code>Inputs</code> block and terminating in an <code>Outputs</code> block. Within this simple flow, the <code>recommender</code> block represents the LLM connection and the prompt used to converse with the model. The <code>echo</code> block for this simple example echoes the input.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p> When creating a connection to an LLM, either in prompt flow or through an API, here are the crucial parameters we always need to consider (prompt flow documentation: <a href="https://microsoft.github.io/promptflow">https://microsoft.github.io/promptflow</a>):</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p61"> <em>Connection</em> —This is the connection name, but it also represents the service you’re connecting to. Prompt flow supports multiple services, including locally deployed LLMs. </li> 
   <li class="readable-text" id="p62"> <em>API</em> —This is the API type. The options are <code>chat</code> for a chat completion API, such as GPT-4, or <code>completion</code> for the older completion models, such as the OpenAI Davinci. </li> 
   <li class="readable-text" id="p63"> <em>Model</em> —This may be the model or deployment name, depending on your service connection. For OpenAI, this will be the model’s name, and for Azure OpenAI, it will represent the deployment name. </li> 
   <li class="readable-text" id="p64"> <em>Temperature</em> —This represents the stochasticity or variability of the model response. A value of <code>1</code> represents a high variability of responses, while <code>0</code> indicates a desire for no variability. This is a critical parameter to understand and, as we’ll see, will vary by use case. </li> 
   <li class="readable-text" id="p65"> <em>Stop</em> —This optional setting tells the call to the LLM to stop creating tokens. It’s more appropriate for older and open source models. </li> 
   <li class="readable-text" id="p66"> <em>Max tokens</em> —This limits the number of tokens used in a conversation. Knowledge of how many tokens you use is crucial to evaluating how your LLM interactions will work when scaled. Counting tokens may not be a concern if you’re exploring and conducting research. However, in production systems, tokens represent the load on the LLM, and connections using numerous tokens may not scale well. </li> 
   <li class="readable-text" id="p67"> <em>Advanced parameters</em> —You can set a few more options to tune your interaction with the LLM, but we’ll cover that topic in later sections of the book. </li> 
  </ul> 
  <div class="readable-text" id="p68"> 
   <p>After configuring the LLM block, scroll up to the Inputs block section, and review the primary input shown in the user_input field, as shown in figure 9.11. Leave it as the default, and then click the Play button at the top of the window.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p69">  
   <img alt="figure" src="../Images/9-11.png" width="1012" height="392"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.11</span> Setting the inputs and starting the flow</h5>
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>All the blocks in the flow will run, and the results will be shown in the terminal window. What you should find interesting is that the output shows recommendations for time travel movies. This is because the recommender block already has a simple profile set, and we’ll see how that works in the next section.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_125"><span class="num-string">9.3.2</span> Creating profiles with Jinja2 templates</h3> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>The flow responds with time travel movie recommendations because of the prompt or profile it uses. By default, prompt flow uses Jinja2 templates to define the content of the prompt or what we’ll call a <em>profile.</em> For the purposes of this book and our exploration of AI agents, we’ll refer to these templates as the profile of a flow or agent.</p> 
  </div> 
  <div class="readable-text intended-text" id="p73"> 
   <p>While prompt flow doesn’t explicitly refer to itself as an assistant or agent engine, it certainly meets the criteria of producing a proxy and general types of agents. As you’ll see, prompt flow even supports deployments of flows into containers and as services.</p> 
  </div> 
  <div class="readable-text intended-text" id="p74"> 
   <p>Open VS Code to <code>chapter_09/promptflow/simpleflow/flow.dag.yaml</code>, and open the file in the visual editor. Then, locate the Prompt field, and click the <code>recommended</code> <code>.jinja2</code> link, as shown in figure 9.12.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p75">  
   <img alt="figure" src="../Images/9-12.png" width="1012" height="424"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.12</span> Opening the prompt Jinja2 template and examining the parts of the profile/prompt</h5>
  </div> 
  <div class="readable-text" id="p76"> 
   <p>Jinja is a templating engine, and Jinja2 is a particular version of that engine. Templates are an excellent way of defining the layout and parts of any form of text document. They have been extensively used to produce HTML, JSON, CSS, and other document forms. In addition, they support the ability to apply code directly into the template. While there is no standard way to construct prompts or agent profiles, our preference in this book is to use templating engines such as Jinja.</p> 
  </div> 
  <div class="readable-text intended-text" id="p77"> 
   <p>At this point, change the role within the system prompt of the <code>recommended.jinja2</code> template. Then, run all blocks of the flow by opening the flow in the visual editor and clicking the Play button. The next section will look at other ways of running prompt flow for testing or actual deployment.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_126"><span class="num-string">9.3.3</span> Deploying a prompt flow API</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>Because prompt flow was also designed to be deployed as a service, it supports a couple of ways to deploy as an app or API quickly. Prompt flow can be deployed as a local web application and API running from the terminal or as a Docker container.</p> 
  </div> 
  <div class="readable-text intended-text" id="p80"> 
   <p>Return to the <code>flow.dag.yaml</code> file in the visual editor from VS Code. At the top of the window beside the Play button are several options we’ll want to investigate further. Click the Build button as shown in figure 9.13, and then select to deploy as a local app. A new YAML file will be created to configure the app. Leave the defaults, and click the Start Local App link.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p81">  
   <img alt="figure" src="../Images/9-13.png" width="1012" height="319"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.13</span> Building and starting the flow as a local app</h5>
  </div> 
  <div class="readable-text" id="p82"> 
   <p>This will launch the flow as a local web application, and you’ll see a browser tab open, as shown in figure 9.14. Enter some text into the user_input field, which is marked as required with a red asterisk. Click Enter and wait a few seconds for the reply.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p83">  
   <img alt="figure" src="../Images/9-14.png" width="1012" height="724"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.14</span> Running the flow as a local web application</h5>
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>You should see a reply like the one shown earlier in figure 9.12, where the flow or agent replies with a list of time travel movies. This is great—we’ve just developed our first agent profile and the equivalent of a proxy agent. However, we need to determine how successful or valuable the recommendations are. In the next section, we explore how to evaluate prompts and profiles.</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_127"><span class="num-string">9.4</span> Evaluating profiles: Rubrics and grounding</h2> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>A key element of any prompt or agent profile is how well it performs its given task. As we see in our recommendation example, prompting an agent profile to give a list of recommendations is relatively easy, but knowing whether those recommendations are helpful requires us to evaluate the response.</p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>Fortunately, prompt flow has been designed to evaluate prompts/profiles at scale. The robust infrastructure allows for the evaluation of LLM interactions to be parallelized and managed as workers, allowing hundreds of profile evaluations and variations to happen quickly.</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>In the next section, we look at how prompt flow can be configured to run prompt/ profile variations against each other. We’ll need to understand this before evaluating profiles’ performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p89"> 
   <p>Prompt flow provides a mechanism to allow for multiple variations within an LLM prompt/profile. This tool is excellent for comparing subtle or significant differences between profile variations. When used in performing bulk evaluations, it can be invaluable for quickly assessing the performance of a profile.</p> 
  </div> 
  <div class="readable-text intended-text" id="p90"> 
   <p>Open the <code>recommender_with_variations/flow.dag.yaml</code> file in VS Code and the flow visual editor, as shown in figure 9.15. This time, we’re making the profile more generalized and allowing for customization at the input level. This allows us to expand our recommendations to anything and not just time travel movies.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p91">  
   <img alt="figure" src="../Images/9-15.png" width="1012" height="764"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.15</span> The recommender, with variations in flow and expanded inputs</h5>
  </div> 
  <div class="readable-text" id="p92"> 
   <p>The new inputs Subject, Genre, Format, and Custom allow us to define a profile that can easily be adjusted to any recommendation. This also means that we must prime the inputs based on the recommendation use case. There are multiple ways to prime these inputs; two examples of priming inputs are shown in figure 9.16. The figure shows two options, options A and B, for priming inputs. Option A represents the classic UI; perhaps there are objects for the user to select the subject or genre, for example. Option B places a proxy/chat agent to interact with the user better to understand the desired subject, genre, and so on.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p93">  
   <img alt="figure" src="../Images/9-16.png" width="1012" height="579"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><strong><span class="num-string">Figure 9.16</span> The user interaction options for interfacing with the agent profile to prime inputs to the agent profile</strong></h5>
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Even considering the power of LLMs, you may still want or need to use option A. The benefit of option A is that you can constrain and validate the inputs much like you do with any modern UI. Alternatively, the downside of option A is that the constrained behavior may limit and restrict future use cases.</p> 
  </div> 
  <div class="readable-text intended-text" id="p95"> 
   <p>Option B represents a more fluid and natural way without a traditional UI. It’s far more powerful and extensible than option A but also introduces more unknowns for evaluation. However, if the proxy agent that option B uses is written well, it can assist a lot in gathering better information from the user.</p> 
  </div> 
  <div class="readable-text intended-text" id="p96"> 
   <p>The option you choose will dictate how you need to evaluate your profiles. If you’re okay with a constrained UI, then it’s likely that the inputs will also be constrained to a set of discrete values. For now, we’ll assume option B for input priming, meaning the input values will be defined by their name.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>To get back to VS Code and the visual view of the recommender with variants flow, click the icon shown earlier in figure 9.15 to open the variants and allow editing. Then, click the <code>recommend.jinja2</code> and <code>recommender_variant_1.jinja2</code> links to open the files side by side, as shown in figure 9.17.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p98">  
   <img alt="figure" src="../Images/9-17.png" width="1012" height="434"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.17</span> Side-by-side comparison of variant profile templates for the recommender</h5>
  </div> 
  <div class="readable-text intended-text" id="p99"> 
   <p>Figure 9.17 demonstrates the difference between the variant profiles. One profile injects the inputs into the user prompt, and the other injects them into the system prompt. However, it’s essential to understand that variations can encompass more than profile design, as identified in table 9.1.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p100"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 9.1</span> LLM variation options in prompt flow</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Option 
       </div></th> 
      <th> 
       <div>
         Evaluation option examples 
       </div></th> 
      <th> 
       <div>
         Notes 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Jinja2 prompt template <br/></td> 
      <td>  Compare system prompt variations, user prompt variations, or mixed prompt variations. <br/></td> 
      <td>  Some endless combinations and techniques can be applied here. Prompt engineering is evolving all the time. <br/></td> 
     </tr> 
     <tr> 
      <td>  LLM <br/></td> 
      <td>  Compare GPT-9.5 to GPT-4. <br/>  Compare GPT-4 to GPT-4 Turbo. <br/>  Compare open source models to commercial models. <br/></td> 
      <td>  This is a useful way to evaluate and ground model performance against a prompt. It can also help you tune your profile to work with open source and/or cheaper models. <br/></td> 
     </tr> 
     <tr> 
      <td>  Temperature <br/></td> 
      <td>  Compare a 0 temperature (no randomness) to a 1 (maximum randomness). <br/></td> 
      <td>  Changes to the temperature can significantly change the responses of some prompts, which may improve or degrade performance. <br/></td> 
     </tr> 
     <tr> 
      <td>  Max tokens <br/></td> 
      <td>  Compare limited tokens to larger token sizes. <br/></td> 
      <td>  This can allow you to reduce and maximize token usage. <br/></td> 
     </tr> 
     <tr> 
      <td>  Advanced parameters <br/></td> 
      <td>  Compare differences to options such as <code>top_p</code>, <code>presence_penalty</code>, <code>frequency_penalty</code>, and <code>logit_bias</code>. <br/></td> 
      <td>  We’ll cover the use of these advanced parameters in later chapters. <br/></td> 
     </tr> 
     <tr> 
      <td>  Function calls <br/></td> 
      <td>  Compare alternative function calls. <br/></td> 
      <td>  Function calls will be addressed later in this chapter. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>For this simple example, we’re just going to use prompt variations by varying the input to reflect in either the system or user prompt. Refer to figure 9.17 for what this looks like. We can then quickly run both variations by clicking the Play (Run All) button at the top and choosing both, as shown in figure 9.18.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p102">  
   <img alt="figure" src="../Images/9-18.png" width="1012" height="424"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.18</span> Running both prompt variations at the same time</h5>
  </div> 
  <div class="readable-text" id="p103"> 
   <p>In the terminal window, you’ll see the results of both runs. The results will likely look similar, so now we must move on to how we evaluate the difference between variations in the next section.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_128"><span class="num-string">9.5</span> Understanding rubrics and grounding</h2> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>Evaluation of prompt/profile performance isn’t something we can typically do using a measure of accuracy or correct percentage. Measuring the performance of a profile depends on the use case and desired outcome. If that is as simple as determining if the response was right or wrong, all the better. However, in most cases, evaluation won’t be that simple.</p> 
  </div> 
  <div class="readable-text intended-text" id="p106"> 
   <p>In education, the <em>rubric</em> concept defines a structured set of criteria and standards a student must establish to receive a particular grade. A rubric can also be used to define a guide for the performance of a profile or prompt. We can follow these steps to define a rubric we can use to evaluate the performance of a profile or prompt:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p107"> <em>Identify the purpose and objectives.</em> Determine the goals you want the profile or agent to accomplish. For example, do you want to evaluate the quality of recommendations for a given audience or overall quality for a given subject, format, or other input? </li> 
   <li class="readable-text" id="p108"> <em>Define criteria.</em> Develop a set of criteria or dimensions that you’ll use to evaluate the profile. These criteria should align with your objectives and provide clear guidelines for assessment. Each criterion should be specific and measurable. For example, you may want to measure a recommendation by how well it fits with the genre and then by subject and format. </li> 
   <li class="readable-text" id="p109"> <em>Create a scale</em>. Establish a rating scale that describes the levels of performance for each criterion. Standard scales include numerical scales (e.g., 1–5) or descriptive scales (e.g., Excellent, Good, Fair, Poor). </li> 
   <li class="readable-text" id="p110"> <em>Provide descriptions.</em> For each level on the scale, provide clear and concise descriptions that indicate what constitutes a strong performance and what represents a weaker performance for each criterion. </li> 
   <li class="readable-text" id="p111"> <em>Apply the rubric.</em> When assessing a prompt or profile, use the rubric to evaluate the prompt’s performance based on the established criteria. Assign scores or ratings for each criterion, considering the descriptions for each level. </li> 
   <li class="readable-text" id="p112"> <em>Calculate the total score.</em> Depending on your rubric, you may calculate a total score by summing up the scores for each criterion or using a weighted average if some criteria are more important than others. </li> 
   <li class="readable-text" id="p113"> <em>Ensure evaluation consistency.</em> If multiple evaluators are assessing the profile, it’s crucial to ensure consistency in grading. </li> 
   <li class="readable-text" id="p114"> <em>Review, revise, and iterate.</em> Periodically review and revise the rubric to ensure it aligns with your assessment goals and objectives. Adjust as needed to improve its effectiveness. </li> 
  </ol> 
  <div class="readable-text" id="p115"> 
   <p><em>Grounding</em> is a concept that can be applied to profile and prompt evaluation—it defines how well a response is aligned with a given rubric’s specific criteria and standards. You can also think of grounding as the baseline expectation of a prompt or profile output.</p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>This list summarizes some other important considerations when using grounding with profile evaluation:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p117"> Grounding refers to aligning responses with the criteria, objectives, and context defined by the rubric and prompt. </li> 
   <li class="readable-text" id="p118"> Grounding involves assessing whether the response directly addresses the rubric criteria, stays on topic, and adheres to any provided instructions. </li> 
   <li class="readable-text" id="p119"> Evaluators and evaluations gauge the accuracy, relevance, and adherence to standards when assessing grounding. </li> 
   <li class="readable-text" id="p120"> Grounding ensures that the response output is firmly rooted in the specified context, making the assessment process more objective and meaningful. </li> 
  </ul> 
  <div class="readable-text" id="p121"> 
   <p>A well-grounded response aligns with all the rubric criteria within the given context and objectives. Poorly grounded responses will fail or miss the entire criteria, context, and objectives.</p> 
  </div> 
  <div class="readable-text intended-text" id="p122"> 
   <p>As the concepts of rubrics and grounding may still be abstract, let’s look at applying them to our current recommender example. Following is a list that follows the process for defining a rubric as applied to our recommender example:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p123"> <em>Identify the purpose and objectives.</em> The purpose of our profile/prompt is to recommend three top items given a subject, format, genre, and custom input. </li> 
   <li class="readable-text" id="p124"> <em>Define criteria.</em> For simplicity, we’ll evaluate how a particular recommendation aligns with the given input criteria, subject, format, and genre. For example, if a profile recommends a book when asked for a movie format, we expect a low score in the format criteria. </li> 
   <li class="readable-text" id="p125"> <em>Create a scale.</em> Again, keeping things simple, we’ll use a scale of 1–5 (1 is poor, and 5 is excellent). </li> 
   <li class="readable-text" id="p126"> <em>Provide descriptions.</em> See the general descriptions for the rating scale shown in table 9.2. </li> 
   <li class="readable-text" id="p127"> <em>Apply the rubric.</em> With the rubric assigned at this stage, it’s an excellent exercise to evaluate the rubric against recommendations manually. </li> 
   <li class="readable-text" id="p128"> <em>Calculate the total score.</em> For our rubric, we’ll average the score for all criteria to provide a total score. </li> 
   <li class="readable-text" id="p129"> <em>Ensure evaluation consistency.</em> The technique we’ll use for evaluation will provide very consistent results. </li> 
   <li class="readable-text" id="p130"> <em>Review, revise, and iterate.</em> We’ll review, compare, and iterate on our profiles, rubrics, and the evaluations themselves. </li> 
  </ol> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p131"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 9.2</span> Rubric ratings </h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Rating 
       </div></th> 
      <th> 
       <div>
         Description 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  1 <br/></td> 
      <td>  Poor alignment: this is the opposite of what is expected given the criteria. <br/></td> 
     </tr> 
     <tr> 
      <td>  2 <br/></td> 
      <td>  Bad alignment: this isn’t a good fit for the given criteria. <br/></td> 
     </tr> 
     <tr> 
      <td>  3 <br/></td> 
      <td>  Mediocre alignment: it may or may not fit well with the given criteria. <br/></td> 
     </tr> 
     <tr> 
      <td>  4 <br/></td> 
      <td>  Good alignment: it may not align 100% with the criteria but is a good fit otherwise. <br/></td> 
     </tr> 
     <tr> 
      <td>  5 <br/></td> 
      <td>  Excellent alignment: this is a good recommendation for the given criteria. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>This basic rubric can now be applied to evaluate the responses for our profile. You can do this manually, or as you’ll see in the next section, using a second LLM profile.</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_129"><span class="num-string">9.6</span> Grounding evaluation with an LLM profile</h2> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>This section will employ another LLM prompt/profile for evaluation and grounding. This second LLM prompt will add another block after the recommendations are generated. It will process the generated recommendations and evaluate each one, given the previous rubric.</p> 
  </div> 
  <div class="readable-text intended-text" id="p135"> 
   <p>Before GPT-4 and other sophisticated LLMs came along, we would have never considered using another LLM prompt to evaluate or ground a<span class="aframe-location"/> profile. You often want to use a different model when using LLMs to ground a profile. However, if you’re comparing profiles against each other, using the same LLM for evaluation and grounding is appropriate.</p> 
  </div> 
  <div class="readable-text intended-text" id="p136"> 
   <p>Open the <code>recommender_with_LLM_evaluation\flow.dag.yaml</code> file in the prompt flow visual editor, scroll down to the <code>evaluate_recommendation</code> block, and click the <code>evaluate_recommendation.jinja2</code> link to open the file, as shown in figure 9.19. Each section of the rubric is identified in the figure.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p137">  
   <img alt="figure" src="../Images/9-19.png" width="1012" height="564"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.19</span> The evaluation prompt, with each of the parts of the rubric outlined</h5>
  </div> 
  <div class="readable-text" id="p138"> 
   <p>We have a rubric that is not only well defined but also in the form of a prompt that can be used to evaluate recommendations. This allows us to evaluate the effectiveness of recommendations for a given profile—automatically. Of course, you can also use the rubric to score and evaluate the recommendations manually for a better baseline.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p139"> 
   <p><span class="print-book-callout-head">Note</span>  Using LLMs to evaluate prompts and profiles provides a strong baseline for comparing the performance of a profile. It can also do this without human bias in a controlled and repeatable manner. This provides an excellent mechanism to establish baseline groundings for any profile or prompt.</p> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>Returning to the <code>recommender_with_LLM_evaluation</code> flow visual editor, we can run the flow by clicking the Play button and observing the output.<span class="aframe-location"/> You can run a single recommendation or run both variations when prompted. The output of a single evaluation using the default inputs is shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p141"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.1</span> LLM rubric evaluation output</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
    "recommendations": "Title: The Butterfly Effect
Subject: 5
Format: 5
Genre: 4

Title: Primer
Subject: 5
Format: 5
Genre: 4

Title: Time Bandits
Subject: 5
Format: 5
Genre: 5"
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>We now have a rubric for grounding our recommender, and the evaluation is run automatically using a second LLM prompt. In the next section, we look at how to perform multiple evaluations simultaneously and then at a total score for everything.</p> 
  </div> 
  <div class="readable-text" id="p143"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_130"><span class="num-string">9.7</span> Comparing profiles: Getting the perfect profile</h2> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>With our understanding of rubrics and grounding, we can now move on to evaluating and iterating the perfect profile. Before we do that, though, we need to clean up the output from the LLM evaluation block. This will require us to parse the recommendations into something more Pythonic, which we’ll tackle in the next section.</p> 
  </div> 
  <div class="readable-text" id="p145"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_131"><span class="num-string">9.7.1</span> Parsing the LLM evaluation output</h3> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>As the raw output from the evaluation block is text, we now want to parse that into something more usable. Of course, writing parsing functions is simple, but there are better ways to cast responses automagically. We covered better methods for returning responses in chapter 5, on agent actions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>Open <code>chapter_09\prompt_flow\recommender_with_parsing\flow.dag.yaml</code> in VS Code, and look at the flow in the visual editor. Locate the <code>parsing_results</code> block, and click the link to open the Python file in the editor, as shown in figure 9.20.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p148">  
   <img alt="figure" src="../Images/9-20.png" width="1012" height="519"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.20</span> Opening the <code>parsing_results.py</code> file in VS Code</h5>
  </div> 
  <div class="readable-text intended-text" id="p149"> 
   <p>The code for the <code>parsing_results.py</code> file is shown in listing 9.2.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p150"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.2</span> <code>parsing_results.py</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from promptflow import tool

@tool     <span class="aframe-location"/> #1
def parse(input: str) -&gt; str:
    # Splitting the recommendations into individual movie blocks
    rblocks = input.strip().split("\n\n")     <span class="aframe-location"/> #2

    # Function to parse individual recommendation block into dictionary
    def parse_block(block):
        lines = block.split('\n')
        rdict = {}
        for line in lines:
            kvs = line.split(': ')
            key, value = kvs[0], kvs[1]
            rdict[key.lower()] = value    <span class="aframe-location"/> #3
        return rdict

    parsed = [parse_block(block) for block in rblocks]   <span class="aframe-location"/> #4

    return parsed</pre> 
    <div class="code-annotations-overlay-container">
     #1 Special decorator to denote the tool block
     <br/>#2 Splits the input and double new lines
     <br/>#3 Creates a dictionary entry and sets the value
     <br/>#4 Loops through each block and parses into key/value dictionary
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p151"> 
   <p>We’re converting the recommendations output from listing 9.1, which is just a string, into a dictionary. So this code will convert this string into the JSON block shown next:</p> 
  </div> 
  <div class="readable-text" id="p152"> 
   <p><em>Before parsing:</em></p> 
  </div> 
  <div class="browsable-container listing-container" id="p153"> 
   <div class="code-area-container"> 
    <pre class="code-area">"Title: The Butterfly Effect
Subject: 5
Format: 5
Genre: 4

Title: Primer
Subject: 5
Format: 5
Genre: 4

Title: Time Bandits
Subject: 5
Format: 5
Genre: 5"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p154"> 
   <p><em>After parsing:</em></p> 
  </div> 
  <div class="browsable-container listing-container" id="p155"> 
   <div class="code-area-container"> 
    <pre class="code-area">       {
            "title": " The Butterfly Effect
            "subject": "5",
            "format": "5",
            "genre": "4"
        },
        {
            "title": " Primer",
            "subject": "5",
            "format": "5",
            "genre": "4"
        },
        {
            "title": " Time Bandits",
            "subject": "5",
            "format": "5",
            "genre": "5"
        }</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p156"> 
   <p>The output of this <code>parsing_results</code> block now gets passed to the output and is wrapped in a list of recommendations. We can see what all this looks like by running the flow.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>Open <code>flow.dag.yaml</code> for the flow in the visual editor, and click the Play (Run All) button. Be sure to select to use both recommender variants. You’ll see both variations run and output to the terminal.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>At this point, we have a full working recommendation and LLM evaluation flow that outputs a score for each criterion on each output. However, to do comprehensive evaluations of a particular profile, we want to generate multiple recommendations with various criteria. We’ll see how to do batch processing of flows in the next section.</p> 
  </div> 
  <div class="readable-text" id="p159"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_132"><span class="num-string">9.7.2</span> Running batch processing in prompt flow</h3> 
  </div> 
  <div class="readable-text" id="p160"> 
   <p>In our generic recommendation profile, we want to evaluate how various input criteria can affect the generated recommendations. Fortunately, prompt flow can batch-process any variations we want to test. The limit is only the time and money we want to spend.</p> 
  </div> 
  <div class="readable-text intended-text" id="p161"> 
   <p>To perform batch processing, we must first create a JSON Lines (JSONL) or JSON list document of our input criteria. If you recall, our input criteria looked like the following in JSON format:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p162"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">{
    "<strong>subject</strong>": "time travel",
    "<strong>format</strong>": "books",
    "<strong>genre</strong>": "fantasy",
    "<strong>custom</strong>": "don't include any R rated content"
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p163"> 
   <p>We want to create a list of JSON objects like that just shown, preferably in a random manner. Of course, the simple way to do this is to prompt ChatGPT to create a JSONL document using the following prompt:</p> 
  </div> 
  <div class="readable-text prompt" id="p164"> 
   <p>I am developing a recommendation agent. The agent will recommend anything given the following criteria:</p> 
  </div> 
  <div class="readable-text prompt" id="p165"> 
   <p>1. subject - examples: time travel, cooking, vacation</p> 
  </div> 
  <div class="readable-text prompt" id="p166"> 
   <p>2. format - examples: books, movies, games</p> 
  </div> 
  <div class="readable-text prompt" id="p167"> 
   <p>3. genre: documentary, action, romance</p> 
  </div> 
  <div class="readable-text prompt" id="p168"> 
   <p>4. custom: don't include any R rated content</p> 
  </div> 
  <div class="readable-text prompt" id="p169"> 
   <p>Can you please generate a random list of these criteria and output it in the format of a JSON Lines file, JSONL. Please include 10 items in the list.</p> 
  </div> 
  <div class="readable-text" id="p170"> 
   <p>Try this out by going to ChatGPT and entering the preceding prompt. A previously generated file can be found in the flow folder, called <code>\bulk_recommend.jsonl</code>. The contents of this file have been shown here for reference:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p171"> 
   <div class="code-area-container"> 
    <pre class="code-area">{
  "subject": "time travel",
  "format": "books",
  "genre": "fantasy",
  "custom": "don't include any R rated content"
}
{
  "subject": "space exploration",
  "format": "podcasts",
  "genre": "sci-fi",
  "custom": "include family-friendly content only"
}
{
  "subject": "mystery",
  "format": "podcasts",
  "genre": "fantasy",
  "custom": "don't include any R rated content"
}
{
  "subject": "space exploration",
  "format": "podcasts",
  "genre": "action",
  "custom": "include family-friendly content only"
}
{
  "subject": "vacation",
  "format": "books",
  "genre": "thriller",
  "custom": "don't include any R rated content"
}
{
  "subject": "mystery",
  "format": "books",
  "genre": "sci-fi",
  "custom": "don't include any R rated content"
}
{
  "subject": "mystery",
  "format": "books",
  "genre": "romance",
  "custom": "don't include any R rated content"
}
{
  "subject": "vacation",
  "format": "movies",
  "genre": "fantasy",
  "custom": "don't include any R rated content"
}
{
  "subject": "cooking",
  "format": "TV shows",
  "genre": "thriller",
  "custom": "include family-friendly content only"
}
{
  "subject": "mystery",
  "format": "movies",
  "genre": "romance",
  "custom": "include family-friendly content only"
}</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>With this bulk file, we can run both variants using the various input criteria in the bulk JSONL file. Open the <code>flow.dag.yaml</code> file in the visual editor, click Batch (the beaker icon) to start the bulk-data loading process, and select the file as shown in figure 9.21. For some operating systems, this may appear as <code>Local</code> <code>Data</code> <code>File</code>.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p173">  
   <img alt="figure" src="../Images/9-21.png" width="1012" height="339"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.21</span> Loading the bulk JSONL file to run the flow on multiple input variations</h5>
  </div> 
  <div class="readable-text" id="p174"> 
   <p>After the bulk file is selected, a new YAML document will open with a Run link added at the bottom of the file, as shown in figure 9.22. Click the link to do the batch run of inputs.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p175">  
   <img alt="figure" src="../Images/9-22.png" width="1012" height="439"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.22</span> Running the batch run of inputs</h5>
  </div> 
  <div class="readable-text" id="p176"> 
   <p>At this point, a few things will happen. The flow visual editor will appear, and beside that a log file will open, showing the progress of the run. In the terminal window, you’ll see the various worker processes spawning and running.</p> 
  </div> 
  <div class="readable-text intended-text" id="p177"> 
   <p>Be patient. The batch run, even for 10 items, may take a few minutes or seconds, depending on various factors such as hardware, previous calls, and so on. Wait for the run to complete, and you’ll see a summary of results in the terminal.</p> 
  </div> 
  <div class="readable-text intended-text" id="p178"> 
   <p>You can also view the run results by opening the prompt flow extension and selecting the last run, as shown in figure 9.23. Then, you dig into each run by clicking the table cells. A lot of information is exposed in this dialog, which can help you troubleshoot flows and profiles.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p179">  
   <img alt="figure" src="../Images/9-23.png" width="1012" height="624"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.23</span> An opening run visualization and an examination of a batch run</h5>
  </div> 
  <div class="readable-text" id="p180"> 
   <p>A lot of information is captured during a batch run, and you can explore much of it through the visualizer. More information can be found by clicking the output folder link from the terminal window. This will open another session of VS Code with the output folder allowing you to review the run logs and other details.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>Now that we’ve completed the batch run for each variant, we can apply grounding and evaluate the results of both prompts. The next section will use a new flow to perform the profile/prompt evaluation.</p> 
  </div> 
  <div class="readable-text" id="p182"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_133"><span class="num-string">9.7.3</span> Creating an evaluation flow for grounding</h3> 
  </div> 
  <div class="readable-text" id="p183"> 
   <p>Open <code>chapter_3\prompt_flow\evaluate_groundings\flow.dag.yaml</code> in the visual editor, as shown in figure 9.24. There are no LLM blocks in the evaluation flow—just Python code blocks that will run the scoring and then aggregate the scores.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p184">  
   <img alt="figure" src="../Images/9-24.png" width="1012" height="674"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.24</span> Looking at the <code>evaluate_groundings</code> flow used to ground recommendation runs</h5>
  </div> 
  <div class="readable-text intended-text" id="p185"> 
   <p>We can now look at the code for the <code>scoring</code> and <code>aggregate</code> blocks, starting with the scoring code in listing 9.3. This scoring code averages the score for each criterion into an average score. The output of the function is a list of processed recommendations.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p186"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.3</span> <code>line_process.py</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">@tool
def line_process(recommendations: str):    <span class="aframe-location"/> #1
    inputs = recommendations
    output = []
    for data_dict in inputs:                     <span class="aframe-location"/> #2
        total_score = 0
        score_count = 0

        for key, value in data_dict.items():     #2
                if key != "title":    <span class="aframe-location"/> #3
                    try:
                        total_score += float(value)
                        score_count += 1
                        data_dict[key] = float(value)    <span class="aframe-location"/> #4
                    except:
                        pass

        avg_score = total_score / score_count if score_count &gt; 0 else 0

        data_dict["avg_score"] = round(avg_score, 2)   <span class="aframe-location"/> #5
        output.append(data_dict)

    return output</pre> 
    <div class="code-annotations-overlay-container">
     #1 A set of three recommendations is input into the function.
     <br/>#2 Loops over each recommendation and criterion
     <br/>#3 Title isn’t a criterion, so ignore it.
     <br/>#4 Totals the score for all criteria and sets the float value to key
     <br/>#5 Adds the average score as a grounding score of the recommendation
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p187"> 
   <p>From the grounded recommendations, we can move on to aggregating the scores with the <code>aggregate</code> block—the code for the <code>aggregate</code> block is shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p188"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 9.4</span> <code>aggregate.py</code> </h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">@tool
def aggregate(processed_results: List[str]):
    items = [item for sublist in processed_results 
              <span class="">↪</span> for item in sublist]    <span class="aframe-location"/> #1

    aggregated = {}

    for item in items:
        for key, value in item.items():
            if key == 'title':
                continue

            if isinstance(value, (float, int)):     <span class="aframe-location"/> #2
                if key in aggregated:
                    aggregated[key] += value
                else:
                    aggregated[key] = value

    for key, value in aggregated.items():     <span class="aframe-location"/> #3
        value = value / len(items)
        log_metric(key=key, value=value)    <span class="aframe-location"/> #4
        aggregated[key] = value

    return aggregated</pre> 
    <div class="code-annotations-overlay-container">
     #1 The input is a list of lists; flatten to a list of items.
     <br/>#2 Checks to see if the value is numeric and accumulates scores for each criterion key
     <br/>#3 Loops over aggregated criterion scores
     <br/>#4 Logs the criterion as a metric
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p189"> 
   <p>The result of the aggregations will be a summary score for each criterion and the average score. Since the evaluation/grounding flow is separate, it can be run over any recommendation run we perform. This will allow us to use the batch run results for any variation to compare results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p190"> 
   <p>We can run the grounding flow by opening <code>flow.dag.yaml</code> in the visual editor and clicking Batch (beaker icon). Then, when prompted, we select an existing run and then select the run we want to evaluate, as shown in figure 9.25. This will open a YAML file with the Run link at the bottom, as we’ve seen before. Click the Run link to run the evaluation.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p191">  
   <img alt="figure" src="../Images/9-25.png" width="1012" height="250"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.25</span> Loading a previous run to be grounded and evaluated</h5>
  </div> 
  <div class="readable-text intended-text" id="p192"> 
   <p>After the run is completed, you’ll see a summary of the results in the terminal window. You can click the output link to open the folder in VS Code and analyze the results, but there is a better way to compare them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p193"> 
   <p>Open the prompt flow extension, focus on the Batch Run History window, and scroll down to the Run against Run section, as shown in figure 9.26. Select the runs you want to compare—likely the ones near the top—so that the checkmark appears. Then, right-click the run, and select the Visualize Runs option. The Batch Run Visualization window opens, and you’ll see the metrics for each of the runs at the top.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p194">  
   <img alt="figure" src="../Images/9-26.png" width="1017" height="459"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 9.26</span> Visualizing the metrics for multiple runs and comparing them</h5>
  </div> 
  <div class="readable-text" id="p195"> 
   <p>We can now see a significant difference between profile/prompt variation 0, the user prompt, and variation 1, the system prompt. Refer to figure 9.15 if you need a refresher on what the prompts/profiles look like. At this point, it should be evident that injecting the input parameters into the system prompt provides better recommendations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p196"> 
   <p>You can now go back and try other profiles or other variant options to see what effect this has on your recommendations. The possibilities are virtually endless, but hopefully you can see what an excellent tool prompt flow will be for building agent profiles and prompts.</p> 
  </div> 
  <div class="readable-text" id="p197"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_134"><span class="num-string">9.7.4</span> Exercises</h3> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>Use the following exercises to improve your knowledge of the material:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p199"> <em>Exercise 1</em> —Create a New Prompt Variant for Recommender Flow (Intermediate) </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p200"> 
   <p><em>Objective</em> —Improve the recommendation results by creating and testing a new prompt variant in prompt flow.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p201"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p202"> Create a new prompt variant for the recommender flow in prompt flow. </li> 
     <li class="readable-text" id="p203"> Run the flow in batch mode. </li> 
     <li class="readable-text" id="p204"> Evaluate the results to determine if they are better or worse compared to the original prompt. </li> 
    </ul></li> 
   <li class="readable-text" id="p205"> <em>Exercise 2</em> —Add a Custom Field to the Rubric and Evaluate (Intermediate) </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p206"> 
   <p><em>Objective</em> —Enhance the evaluation criteria by incorporating a custom field into the rubric and updating the evaluation flow.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p207"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p208"> Add the custom field as a new criterion to the rubric. </li> 
     <li class="readable-text" id="p209"> Update the evaluation flow to score the new criterion. </li> 
     <li class="readable-text" id="p210"> Evaluate the results, and analyze the effect of the new criterion on the evaluation. </li> 
    </ul></li> 
   <li class="readable-text" id="p211"> <em>Exercise 3</em> —Develop a New Use Case and Evaluation Rubric (Advanced) </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p212"> 
   <p><em>Objective</em> —Expand the application of prompt engineering by developing a new use case and creating an evaluation rubric.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p213"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p214"> Develop a new use case aside from the recommendation. </li> 
     <li class="readable-text" id="p215"> Build the prompt for the new use case. </li> 
     <li class="readable-text" id="p216"> Create a rubric for evaluating the new prompt. </li> 
     <li class="readable-text" id="p217"> Update or alter the evaluation flow to aggregate and compare the results of the new use case with existing ones. </li> 
    </ul></li> 
   <li class="readable-text" id="p218"> <em>Exercise 4</em> —Evaluate Other LLMs Using LM Studio (Intermediate) </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p219"> 
   <p><em>Objective</em> —Assess the performance of different open source LLMs by hosting a local server with LM Studio.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p220"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p221"> Use LM Studio to host a local server for evaluating LLMs. </li> 
     <li class="readable-text" id="p222"> Evaluate other open source LLMs. </li> 
     <li class="readable-text" id="p223"> Consult chapter 2 if assistance is needed for setting up the server and performing the evaluations. </li> 
    </ul></li> 
   <li class="readable-text" id="p224"> <em>Exercise 5</em> —Build and Evaluate Prompts Using Prompt Flow (Intermediate) </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p225"> 
   <p><em>Objective</em> —Apply prompt engineering strategies to build and evaluate new prompts or profiles using prompt flow.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p226"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p227"> Build new prompts or profiles for evaluation using prompt flow. </li> 
     <li class="readable-text" id="p228"> Apply the Write Clear Instructions prompt engineering strategy from chapter 2. </li> 
     <li class="readable-text" id="p229"> Evaluate the prompts and profiles using prompt flow. </li> 
     <li class="readable-text" id="p230"> Refer to chapter 2 for tactics and implementation details if a refresher is needed. </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p231"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_135">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p232"> An agent profile consists of several other component prompts that can drive functions such as actions/tools, knowledge, memory, evaluation, reasoning, feedback, and planning. </li> 
   <li class="readable-text" id="p233"> Prompt flow can be used to evaluate an agent’s component prompts. </li> 
   <li class="readable-text" id="p234"> Systemic prompt engineering is an iterative process evaluating a prompt and agent profile. </li> 
   <li class="readable-text" id="p235"> The Test Changes Systematically strategy describes iterating and evaluating prompts, and system prompt engineering implements this strategy. </li> 
   <li class="readable-text" id="p236"> Agent profiles and prompt engineering have many similarities. We define an agent profile as the combination of prompt engineering elements that guide and help an agent through its task. </li> 
   <li class="readable-text" id="p237"> Prompt flow is an open source tool from Microsoft that provides several features for developing and evaluating profiles and prompts. </li> 
   <li class="readable-text" id="p238"> An LLM connection in prompt flow supports additional parameters, including temperature, stop token, max tokens, and other advanced parameters. </li> 
   <li class="readable-text" id="p239"> LLM blocks support prompt and profile variants, which allow for evaluating changes to the prompt/profile or other connection parameters. </li> 
   <li class="readable-text" id="p240"> A rubric applied to an LLM prompt is the criteria and standards a prompt/profile must fulfill to be grounded. Grounding is the scoring and evaluation of a rubric. </li> 
   <li class="readable-text" id="p241"> Prompt flow supports running multiple variations as single runs or batch runs. </li> 
   <li class="readable-text" id="p242"> In prompt flow, an evaluation flow is run after a generative flow to score and aggregate the results. The Visualize Runs option can compare the aggregated criteria from scoring the rubric across multiple runs. </li> 
  </ul>
 </div></div></body></html>