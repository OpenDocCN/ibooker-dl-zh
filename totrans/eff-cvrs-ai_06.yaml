- en: 5 Improving weak understanding for traditional AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 提升对传统人工智能的薄弱理解
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Identifying the types of errors a classifier can make
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别分类器可能犯的错误类型
- en: Establishing a baseline of current classifier performance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立当前分类器性能的基线
- en: Using data science methodologies to identify and prioritize improvements
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据科学方法来识别和优先处理改进
- en: Infusing your traditional AI with generated content to enhance understanding
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将生成内容融入你的传统人工智能以增强理解
- en: In this chapter, we will demonstrate a methodical, iterative approach to improving
    the understanding of a classification-based conversational solution. This chapter
    builds on the concepts introduced in the previous chapter and uses the output
    produced by the final exercise in section 4.4 (where you created a test set with
    the golden intent assigned to each utterance in a format that can be used by your
    testing tool). Later in this chapter, we’ll explore how large language models
    can supplement intent-driven output responses to deliver a more robust experience.
    (If you’re looking for generative AI improvement techniques, feel free to skip
    ahead to the next chapter.)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示一种系统性的、迭代的方法来提高基于分类的对话解决方案的理解。本章建立在上一章中介绍的概念之上，并使用第4.4节中产生的最终输出（你在其中创建了一个测试集，每个话语都分配了黄金意图，并且格式可以用于你的测试工具）。在本章的后面部分，我们将探讨大型语言模型如何补充意图驱动的输出响应，以提供更稳健的体验。（如果你在寻找生成式人工智能改进技术，可以自由地跳到下一章。）
- en: We will start by building an improvement plan and identifying the types of errors
    your classifier may be committing. Next, we’ll iterate through seven improvement
    cycles to solve the various problems you might see in your own text classifier.
    Although data science techniques are used, you do not need to be a data scientist
    to extract meaningful insights about your data using the methodologies presented
    in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先制定一个改进计划，并确定你的分类器可能犯的错误类型。接下来，我们将通过七个改进周期来解决你可能在自己的文本分类器中看到的各种问题。尽管使用了数据科学技术，但你不需要是数据科学家，就可以使用本章中介绍的方法从你的数据中提取有意义的见解。
- en: 5.1 Building your improvement plan
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 制定你的改进计划
- en: If you built a blind test set using a sample from your production logs, you
    should have a reliable “representative distribution” test set. This means that
    the topics that are most frequently asked by your users are represented with corresponding
    volume in your testing data. This will be a key factor in prioritizing any problems
    that are surfaced by your test results.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用生产日志中的样本构建了一个盲测试集，你应该有一个可靠的“代表性分布”测试集。这意味着用户最常询问的主题在你的测试数据中都有相应的数量表示。这将是在优先处理测试结果中暴露出来的任何问题的一个关键因素。
- en: If you are working with the results of a *k*-fold test (discussed in chapter
    4), you won’t know for certain which topics are the most important, so the most
    egregious accuracy scores are a logical starting point.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理第4章中讨论的*k*-折测试的结果，你将无法确定哪些主题是最重要的，因此最严重的准确度得分是一个合理的起点。
- en: In either case, it’s now time to dig into those test results. An improvement
    plan starts with identifying the biggest problem spots in the bot’s training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，现在是时候深入挖掘那些测试结果了。改进计划从识别机器人训练中的最大问题区域开始。
- en: 5.1.1 Identify problematic patterns in misunderstood utterances
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 识别误解话语中的问题模式
- en: The first score that will grab your attention is the overall accuracy of your
    test results. This is a lot like getting back a spelling or math test and looking
    at the red ink at the top of the page. If your test had 100 questions and you
    got 79 of them correct, your accuracy score would be 79%. For classifiers, this
    number is good for an “at a glance” view of the model, but it doesn’t give a complete
    picture of what is going on or where to start making improvements. For that, we
    need to understand the possible outcomes and types of errors our classifier may
    be committing. This is revealed in the measurements of recall, precision, and
    F1 score.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个会吸引你注意的分数是测试结果的整体准确率。这就像拿到一份拼写或数学测试，看到页面顶部的红墨水。如果你的测试有100个问题，你答对了79个，你的准确率就是79%。对于分类器来说，这个数字可以提供一个“一目了然”的模型视图，但它并不提供关于正在发生的事情或从哪里开始改进的完整画面。为了做到这一点，我们需要了解我们分类器可能犯的错误和可能的结果类型。这体现在回收率、精确率和F1分数的测量中。
- en: A brief explanation of recall, precision, and F1 scores
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回收率、精确率和F1分数的简要说明
- en: 'In chapter 4, we described *recall* as the classifier’s ability to predict
    a correct intent and *precision* as the ability to refrain from predicting a wrong
    intent. You can think of this in terms of positive and negative predictions. For
    every utterance that we test against the model, there are four possible outcomes,
    and they are not mutually exclusive, meaning that every prediction is going to
    have two or three of these outcomes happening simultaneously. Figure 5.1 shows
    a confusion matrix that visualizes these possible outcomes:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们将*召回率*描述为分类器预测正确意图的能力，将*精确率*描述为避免预测错误意图的能力。你可以从正面和负面预测的角度来考虑这一点。对于我们要测试的每个话语，都有四种可能的结果，它们不是互斥的，这意味着每个预测都会同时发生两个或三个这些结果。图5.1显示了可视化这些可能结果的混淆矩阵：
- en: '![figure](../Images/CH05_F01_Freed2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F01_Freed2.png)'
- en: Figure 5.1 In a 2 × 2 confusion matrix, the possible outcomes are derived by
    comparing the predicted intent to the actual intent.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 在一个2×2的混淆矩阵中，可能的结果是通过比较预测意图和实际意图得出的。
- en: '*True positive*—A prediction that matches the correct intent'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阳性*—与正确意图匹配的预测'
- en: '*True negative*—A prediction that does not match an incorrect intent'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阴性*—不匹配错误意图的预测'
- en: '*False positive*—A prediction that matches an incorrect intent'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阳性*—匹配错误意图的预测'
- en: '*False negative*—A prediction that does not match the correct intent'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阴性*—不匹配正确意图的预测'
- en: 'The first metric that might interest us is the recall of our intents. For this,
    we need to know the true positives and the false negatives. An intent that is
    returning false negatives is committing an error of under-selection. When measured
    per intent, this looks like an accuracy score. If our test had five questions
    for the `#Request_Agent` intent, and the classifier got those questions correct
    four times, the intent’s recall would be 80%:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能感兴趣的第一个指标是意图的召回率。为此，我们需要知道真阳性和假阴性。返回假阴性的意图正在犯选择不足的错误。按意图衡量，这看起来像是一个准确率分数。如果我们的测试有五个关于`#Request_Agent`意图的问题，分类器正确回答了四个，那么该意图的召回率将是80%：
- en: '*Recall = True positives / (True positives + False negatives)*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率 = 真阳性 / (真阳性 + 假阴性)*'
- en: 'The next metric that helps us understand our classifier is precision. This
    measures how good our classifier is at refraining from giving a false positive.
    An intent that is returning false positives is committing an error of over-selection.
    An example of over-selection can be seen in the last two rows of table 5.1:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助我们理解分类器的下一个指标是精确率。这衡量我们的分类器在避免给出假阳性方面的好坏。返回假阳性的意图正在犯选择过多的错误。表5.1的最后两行提供了一个选择过多的例子：
- en: '*Precision = True positives / (True positives + False positives)*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确率 = 真阳性 / (真阳性 + 假阳性)*'
- en: 'Table 5.1 Test results show seven utterances, five of which are labeled with
    the correct `#Request_Agent` intent. The first four predictions were true positives.
    The last two rows show where `#Request_Agent` was predicted twice for utterances
    where it shouldn’t have been (“What can I ask you” and “Somebody hit my car”).
    These false positives contribute to our precision calculation: 4 / (4 + 2) = 0.66.'
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1 测试结果显示有七个话语，其中五个被标记为正确的`#Request_Agent`意图。前四个预测是真阳性。最后两行显示了在不应预测`#Request_Agent`意图的话语中预测了两次（“我可以问您什么？”和“有人撞了我的车”）。这些假阳性贡献于我们的精确率计算：4
    / (4 + 2) = 0.66。
- en: '| Utterance | Correct intent | Predicted intent | Correct |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 话语 | 正确意图 | 预测意图 | 正确 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Customer service  | `Request_Agent`  | `Request_Agent`  | 1  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 客户服务 | `Request_Agent` | `Request_Agent` | 1 |'
- en: '| Speak with an agent  | `Request_Agent`  | `Request_Agent`  | 1  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 与代理交谈 | `Request_Agent` | `Request_Agent` | 1 |'
- en: '| Can I please speak with somebody?  | `Request_Agent`  | `Request_Agent`  |
    1  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 我可以和某人说话吗？ | `Request_Agent` | `Request_Agent` | 1 |'
- en: '| Talk with a human  | `Request_Agent`  | `Request_Agent`  | 1  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 与真人交谈 | `Request_Agent` | `Request_Agent` | 1 |'
- en: '| When will I get a live person?  | `Request_Agent`  | `Office_Hours`  | 0  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 我什么时候能接到真人？ | `Request_Agent` | `Office_Hours` | 0 |'
- en: '| What can I ask you?  | `VA_Capabilities`  | `Request_Agent`  | 0  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 我可以问您什么？ | `VA_Capabilities` | `Request_Agent` | 0 |'
- en: '| Somebody hit my car  | `Report_Accident`  | `Request_Agent`  | 0  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 有人撞了我的车 | `Report_Accident` | `Request_Agent` | 0 |'
- en: A full analysis of all possible outcomes for `#Request_Agent` is shown in figure
    5.2\. It also shows the true negatives (which are not used in our calculations
    but have been included to demonstrate the range of other outcomes).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`#Request_Agent` 的所有可能结果的全面分析显示在图 5.2 中。它还显示了真阴性值（这些值在我们的计算中未使用，但已包括以展示其他结果的范围）。'
- en: '![figure](../Images/CH05_F02_Freed2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图表](../Images/CH05_F02_Freed2.png)'
- en: Figure 5.2 The highlighted columns are used for calculating precision and recall
    for `#Request_Agent`.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2 用于计算 `#Request_Agent` 的精确度和召回率的突出列。
- en: 'Now that we know the recall and precision, we can also calculate the F1 score,
    which is the harmonic mean of recall and precision. This calculation is made as
    follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了召回率和精确度，我们还可以计算 F1 分数，它是召回率和精确率的调和平均值。这个计算如下：
- en: '*F1 score = (2 *×* Precision *×* Recall) / (Precision + Recall)*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*F1 分数 = (2 × 精确度 × 回忆) / (精确度 + 回忆)*'
- en: For our `#Request_Agent` intent, this would be calculated as (2 × 0.66 × 0.8)
    / (0.66 + 0.8) = 0.72\. Table 5.2 shows all three scores.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 `#Request_Agent` 目标，这将计算为 (2 × 0.66 × 0.8) / (0.66 + 0.8) = 0.72。表 5.2
    显示了所有三个分数。
- en: Table 5.2 Recall, precision, and F1 score for `#Request_Agent`
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 5.2 `#Request_Agent` 的召回率、精确度和 F1 分数
- en: '| Intent | Recall | Precision | F1 score |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 回忆 | 精确度 | F1 分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `Request_Agent`  | 0.80  | 0.66  | 0.72  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `Request_Agent` | 0.80 | 0.66 | 0.72 |'
- en: What about the true negatives?
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 真阴性值如何？
- en: Earlier in this section, we mentioned true negatives—a prediction that does
    not match an incorrect intent. True negatives occur whenever we have more than
    one trained intent. However, they are not a useful measurement in our methods.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节前面，我们提到了真阴性值——一个不匹配错误意图的预测。只要我们有一个以上的训练目标，就会发生真阴性值。然而，它们并不是我们方法中的有用度量。
- en: Why not? Well, for every prediction the model makes, there is only one way for
    it to be right, but there are two ways for it to be wrong. This seems a little
    unfair, and it’s hard to see why if you’re just looking at two intents. But imagine
    we have a model that was trained with 20 intents. Whenever we make a single prediction
    that returns a true positive, we will also get 19 true negatives. And for every
    false positive prediction,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不呢？好吧，对于模型做出的每一个预测，它只有一种正确的方式，但有两种错误的方式。这似乎有点不公平，如果你只是看两个目标，就很难理解这一点。但想象一下，我们有一个用20个目标训练的模型。每次我们做出一个返回真阳性的单个预测时，我们也会得到19个真阴性值。而对于每一个假阳性预测，
- en: we have 1 false negative and 18 true negatives. So all those true negatives
    add up to a very large number that, for our purposes, doesn’t give us much insight.
    Therefore, we don’t factor true negatives into our calculations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有1个假阴性值和18个真阴性值。所以所有这些真阴性值加起来是一个非常大的数字，对于我们来说，这并不提供太多洞察。因此，我们不将真阴性值纳入我们的计算中。
- en: Deciding which metric is important
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 决定哪个指标很重要
- en: 'Recall, precision, F1 score: Which number should we care about? That’s a great
    question! The answer is that it depends on what your organization values most
    in terms of what the solution needs to deliver. Here are some considerations to
    guide you to an answer:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率、精确度、F1 分数：我们应该关注哪个数字？这是一个很好的问题！答案是，这取决于你的组织在解决方案需要交付的内容中最重视什么。以下是一些指导你找到答案的考虑因素：
- en: Recall is useful when there is a high cost associated with false negatives.
    Imagine the effect if a fraud detection tool missed 25% of the fraudulent transactions
    it evaluated. (For a chatbot, this would look like a correct intent that is not
    predicted 25% of the time.)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与假阴性相关的高成本时，召回率是有用的。想象一下，如果欺诈检测工具错过了它评估的25%的欺诈交易，会有什么影响。（对于聊天机器人来说，这看起来像是一个正确的意图，但25%的时间没有被预测到。）
- en: Precision is useful when there is a high cost associated with false positives.
    Think of the gameshow Jeopardy!, which penalizes a contestant for attempting to
    answer and getting it wrong (or a chatbot that over-selects the `#Request_ Agent`
    intent, resulting in unnecessary escalations).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与假阳性相关的高成本时，精确度是有用的。想想《危险边缘》这个游戏节目，它会惩罚试图回答并答错题目的参赛者（或者一个过度选择 `#Request_Agent`
    目标的聊天机器人，导致不必要的升级）。
- en: The F1 score is useful when there is a high cost associated with both false
    positives and false negatives. We like to use this for most implementations because
    it reflects a good balance of the recall and precision scores.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与假阳性和假阴性都相关的高成本时，F1 分数是有用的。我们喜欢使用这个指标，因为它反映了召回率和精确率分数的良好平衡。
- en: Visualizing your data with a confusion matrix
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用混淆矩阵可视化你的数据
- en: Earlier in this section, we showed a 2 × 2 confusion matrix to demonstrate the
    potential outcomes. A confusion matrix can help you assess the performance of
    a classification model by visualizing a summary of the predictions made by your
    model. Some testing tools produce this with their results output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节前面，我们展示了2×2混淆矩阵以展示潜在的结果。混淆矩阵可以帮助你通过可视化模型做出的预测摘要来评估分类模型的性能。一些测试工具会在其结果输出中生成此信息。
- en: Figure 5.3 shows a fictional scenario where a classifier model made ten perfect
    predictions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3显示了一个虚构场景，其中分类器模型做出了十个完美的预测。
- en: '![figure](../Images/CH05_F03_Freed2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F03_Freed2.png)'
- en: Figure 5.3 A solid diagonal line shows that each predicted intent (represented
    by a single letter) matched to the actual intent.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3 一条实线对角线表明每个预测意图（用一个字母表示）都与实际意图相匹配。
- en: Shaded boxes that stray from the diagonal provide useful insights about where
    your model is confused, as shown in figure 5.4.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从对角线偏离的阴影框提供了关于模型在哪里混淆的有用见解，如图5.4所示。
- en: '![figure](../Images/CH05_F04_Freed2.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F04_Freed2.png)'
- en: Figure 5.4 This model had nine correct predictions, but wrongly predicted intent
    G when the actual intent was E.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4显示，该模型有九个正确的预测，但在实际意图是E时错误地预测了意图G。
- en: 5.1.2 Incremental improvements
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 逐步改进
- en: An incremental improvement approach will affect measurable change in a manageable
    way. Every change you make to a classifier has the potential to affect multiple
    intents. Sometimes this effect is positive, but sometimes it’s not. You might
    get away with updating several intents all at once, but if the testing shows a
    performance decline, it can be difficult to track down the culprit. You will have
    to balance the need for efficiency with your tolerance for rework.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步改进的方法将以可管理的方式影响可衡量的变化。你对分类器所做的每一次更改都可能影响多个意图。有时这种影响是积极的，但有时则不然。你可能会同时更新多个意图而不会出现问题，但如果测试显示性能下降，追踪问题根源可能会很困难。你将不得不在效率需求与对返工的容忍度之间取得平衡。
- en: '5.1.3 Where to start: Identifying the biggest problems'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 从哪里开始：识别最大的问题
- en: Generally, the best place to start is with the highest volume intents that have
    the lowest F1 scores. The business may also weigh in on priorities. If a lower
    volume intent fails to recognize the type of request it was designed to handle,
    but this failure incurs costly human intervention, it might take priority.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最好的起点是具有最低F1分数的最高量意图。企业也可能对优先级提出意见。如果一个低量意图未能识别其设计用来处理的那种请求，但这种失败导致了昂贵的人工干预，它可能会被优先考虑。
- en: 'For the rest of this chapter, we will explore a fictional use case: a chatbot
    that serves a population that interacts with a state’s Bureau of Motor Vehicles
    (a type of US government agency that regulates and manages the issuance of state
    identification cards, driver’s licenses, certain permits, and vehicle registrations).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将探讨一个虚构的用例：一个服务于与州机动车管理局（一种美国政府机构，负责监管和管理州身份证、驾驶执照、某些许可证和车辆注册的发放）互动的人群的聊天机器人。
- en: To begin, let’s follow the advice given in chapter 4 and take a quick, high-level
    look at our current training data, as laid out in table 5.3.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们遵循第4章中给出的建议，快速、高层次地查看我们的当前训练数据，如表5.3所示。
- en: Table 5.3 Intents with example counts in a baseline training set
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.3 基线训练集中具有示例计数的意图
- en: '| Intent name | Number of examples |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 意图名称 | 示例数量 |'
- en: '| --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Accident_Report`  | 2  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `Accident_Report`  | 2  |'
- en: '| `Appointment`  | 6  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `Appointment`  | 6  |'
- en: '| `Change_Contact_Records`  | 3  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `Change_Contact_Records`  | 3  |'
- en: '| `Chitchat_Goodbye`  | 3  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `Chitchat_Goodbye`  | 3  |'
- en: '| `Chitchat_Hello`  | 4  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `Chitchat_Hello`  | 4  |'
- en: '| `Chitchat_Thanks`  | 2  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `Chitchat_Thanks`  | 2  |'
- en: '| `Chitchat_VA_About`  | 8  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `Chitchat_VA_About`  | 8  |'
- en: '| `Fee_Info`  | 5  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `Fee_Info`  | 5  |'
- en: '| `General_Negative_Feedback`  | 6  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `General_Negative_Feedback`  | 6  |'
- en: '| `General_Request_Agent`  | 5  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `General_Request_Agent`  | 5  |'
- en: '| `Get_ID_Number`  | 4  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `Get_ID_Number`  | 4  |'
- en: '| `Item_Not_Received`  | 8  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `Item_Not_Received`  | 8  |'
- en: '| `License_or_ID`  | 5  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `License_or_ID`  | 5  |'
- en: '| `License_Reinstatement`  | 4  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `License_Reinstatement`  | 4  |'
- en: '| `Login_Issue`  | 4  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `Login_Issue`  | 4  |'
- en: '| `Name_Change`  | 6  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `Name_Change`  | 6  |'
- en: '| `Office_Information`  | 6  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `Office_Information`  | 6  |'
- en: '| `Payment_Methods`  | 3  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `Payment_Methods`  | 3  |'
- en: '| `Refund_Overcharge`  | 4  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `Refund_Overcharge`  | 4  |'
- en: '| `Report_Sold_Vehicle`  | 6  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `Report_Sold_Vehicle`  | 6  |'
- en: '| `Report_Stolen_License_Permit_ID`  | 5  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `Report_Stolen_License_Permit_ID`  | 5  |'
- en: '| `Report_Stolen_Plates_Registration`  | 3  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `Report_Stolen_Plates_Registration`  | 3  |'
- en: '| `Report_Stolen_Vehicle`  | 2  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `Report_Stolen_Vehicle`  | 2  |'
- en: '| `Request_Receipt`  | 4  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 5  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 6  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 6  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
- en: '| **Grand Total**  | **125**  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: We can make some quantitative statements about this training set. It has 27
    intents with a grand total of 125 training examples. The examples are distributed
    fairly evenly. As a qualitative assessment, we might say that many of the intents
    appear to be unique, but a few of them might have some overlap. Some terms definitely
    overlap across intent names. A peek at the full set of training utterances (not
    shown) revealed that many terms appear in multiple intents, such as “ID,” “title,”
    “permit,” “vehicle,” “stolen.” However, as shown in table 5.4, the contexts in
    which these words appeared were judged to be appropriately labeled.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.4 Utterances extracted from the baseline training set show a variety
    of terms overlapping across multiple intents.
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Labeled intent |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| How much is an ID?  | `Fee_Info`  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| I need to find out my ID number  | `Get_ID_Number`  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| I didn’t receive my ID  | `Item_Not_Received`  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| Title never came  | `Item_Not_Received`  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| Add a person to the title  | `Vehicle_Title`  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| How do I get a driving permit?  | `License_or_ID`  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| Replace my program parking permit  | `Vehicle_Permit`  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| I sold a vehicle  | `Report_Sold_Vehicle`  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| I need to report a stolen car  | `Report_Stolen_Vehicle`  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| My ID was stolen  | `Report_Stolen_License_Permit_ID`  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: Overall, it seems that the range of topics is reasonable for the chatbot’s purpose,
    which in this case is to answer questions a user might have when dealing with
    a state’s Bureau of Motor Vehicles.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a baseline
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we have made an initial assessment of our training data, we need to
    understand how it is currently performing. We’ll start by running a *k*-fold cross
    validation test to establish a baseline. The results, our first version (V1) shown
    in table 5.5, are not that bad, considering the low volume of data present in
    the training set.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.5 Baseline (V1) *k*-fold results
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 2  | 2  | 1  | 1  | 1  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 6  | 8  | 1  | 0.75  | 0.8571  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 3  | 0  | 0  | 0  | 0  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 3  | 0  | 0  | 0  | 0  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 4  | 6  | 1  | 0.6667  | 0.80  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 2  | 2  | 1  | 1  | 1  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 8  | 8  | 1  | 1  | 1  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 5  | 2  | 0.40  | 1  | 0.5714  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 6  | 7  | 1  | 0.8571  | 0.9231  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 5  | 4  | 0.80  | 1  | 0.8889  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 4  | 6  | 1  | 0.6667  | 0.80  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 8  | 6  | 0.6250  | 0.8333  | 0.7143  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 4  | 4  | 1  | 1  | 1  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 5  | 5  | 0.60  | 0.60  | 0.60  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 4  | 4  | 1  | 1  | 1  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 6  | 8  | 1  | 0.75  | 0.8571  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 6  | 6  | 1  | 1  | 1  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 3  | 3  | 1  | 1  | 1  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 4  | 4  | 1  | 1  | 1  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 6  | 5  | 0.8333  | 1  | 0.9091  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '| 5  | 6  | 1  | 0.8333  | 0.9091  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '| 3  | 3  | 0.3333  | 0.3333  | 0.3333  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 2  | 3  | 1  | 0.6667  | 0.80  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 4  | 4  | 1  | 1  | 1.0000  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 5  | 6  | 1  | 0.8333  | 0.9091  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 6  | 9  | 1  | 0.6667  | 0.80  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 6  | 4  | 0.6667  | 1  | 0.80  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: Our *k*-fold test had a total of 125 questions (the grand total of our training
    set), and it got 105 of them correct, for an overall accuracy of 84%. Several
    intents had perfect recall and perfect precision (which is often a hallmark of
    a manufactured data set). There were two intents that had a recall of 0; they
    each had only three training examples. This reveals one of the flaws of *k*-fold
    testing—there simply weren’t enough examples to distribute across the auto-generated
    train and test sets. More than likely, those intents will perform better than
    0 in production. However, the intents with perfect recall will probably not perform
    quite as well. If you are launching a pilot and have no other training data available,
    these results are generally good enough to go live, with a strong caution to the
    stakeholders that they should expect lower actual performance until representative
    data is available for use in training updates.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Once the solution is live, a new baseline should be taken using the blind test
    set you created from the logs. We have an example of this in table 5.6, and it
    really emphasizes the gap in performance predicted by our *k*-fold test compared
    to real user inputs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.6 Baseline (V1) blind results
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 2  | 2  | 1  | 1  | 1  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 7  | 5  | 0.7143  | 1  | 0.8333  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 4  | 4  | 1  | 1  | 1  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 1  | 1  | 1  | 1  | 1  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 1  | 1  | 1  | 1  | 1  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 1  | 1  | 1  | 1  | 1  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 1  | 2  | 1  | 0.50  | 0.6667  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 11  | 9  | 0.8182  | 1  | 0.90  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 2  | 3  | 1  | 0.6667  | 0.80  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 3  | 2  | 0.6667  | 1  | 0.80  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 3  | 5  | 1  | 0.60  | 0.75  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 16  | 9  | 0.4375  | 0.7778  | 0.56  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 5  | 5  | 0.60  | 0.60  | 0.60  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 7  | 5  | 0.5714  | 0.80  | 0.6667  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 9  | 5  | 0.4444  | 1  | 0.6153  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 9  | 9  | 1  | 1  | 1  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 9  | 11  | 1  | 0.8182  | 0.90  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 2  | 2  | 1  | 1  | 1  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 3  | 4  | 1  | 0.7500  | 0.8571  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| `Refund_Overcharge`  | 3  | 4  | 1  | 0.7500  | 0.8571  |'
- en: '| `Report_Sold_Vehicle`  | 6  | 7  | 1  | 0.8571  | 0.9231  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| `Report_Sold_Vehicle`  | 6  | 7  | 1  | 0.8571  | 0.9231  |'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE2]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '| 7  | 8  | 0.8571  | 0.75  | 0.80  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 7  | 8  | 0.8571  | 0.75  | 0.80  |'
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE3]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '| 5  | 4  | 0.80  | 1  | 0.8889  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 5  | 4  | 0.80  | 1  | 0.8889  |'
- en: '| `Report_Stolen_Vehicle`  | 2  | 2  | 0.50  | 0.50  | 0.50  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| `Report_Stolen_Vehicle`  | 2  | 2  | 0.50  | 0.50  | 0.50  |'
- en: '| `Request_Receipt`  | 4  | 4  | 1  | 1  | 1  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| `Request_Receipt`  | 4  | 4  | 1  | 1  | 1  |'
- en: '| `Vehicle_Permit`  | 4  | 5  | 1  | 0.80  | 0.8889  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| `Vehicle_Permit`  | 4  | 5  | 1  | 0.80  | 0.8889  |'
- en: '| `Vehicle_Title`  | 2  | 8  | 1  | 0.25  | 0.40  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| `Vehicle_Title`  | 2  | 8  | 1  | 0.25  | 0.40  |'
- en: '| `Walk_In`  | 8  | 5  | 0.3750  | 0.60  | 0.4615  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| `Walk_In`  | 8  | 5  | 0.3750  | 0.60  | 0.4615  |'
- en: On the first run of our blind test, 102 questions were correct out of 134, for
    an overall accuracy of 76%—8 points lower than the 84% predicted by our *k*-fold
    test.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们盲测试的第一轮中，134个问题中有102个是正确的，总体准确率为76%——比我们预测的84%低8个百分点。
- en: Validating your initial training strategy
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证您的初始训练策略
- en: Once you have obtained annotated logs and taken some baseline performance measurements,
    you can validate the decisions that informed your initial training strategy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您获得了标注日志并进行了某些基线性能测量，您就可以验证那些指导您初始训练策略的决定。
- en: Scarcity of representative training data is a very common problem for conversational
    AI projects. Just like many other newly launched chatbots, our initial training
    set was developed by subject matter experts (SMEs) who manufactured training examples
    for the topics they believed would occur most frequently. In figure 5.5, we can
    compare the number of examples trained for each intent to the number of examples
    that were present in the randomly selected logs used for testing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性训练数据的稀缺是对话式人工智能项目非常常见的问题。就像许多其他新推出的聊天机器人一样，我们的初始训练集是由主题专家（SMEs）开发的，他们为他们认为最可能发生的话题制造了训练示例。在图5.5中，我们可以比较为每个意图训练的示例数量与用于测试的随机选择的日志中存在的示例数量。
- en: '![figure](../Images/CH05_F05_Freed2.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F05_Freed2.png)'
- en: Figure 5.5 A comparison of training examples to the utterances in our representative
    blind test set shows that there is some disparity in volume for many of the most
    popular intents (the representative blind utterances) on the left side of the
    graph. We also see disparity across several of the least popular intents (those
    on the right).
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 将训练示例与我们的代表性盲测试集中的话语进行比较，显示在图表左侧的许多最流行意图（代表性的盲话语）在音量上存在一些差异。我们还在几个不太受欢迎的意图（右侧的意图）中看到了差异。
- en: A side-by-side volume comparison of training data to representative blind utterances
    per intent can help us understand if our solution’s topic coverage is in alignment
    with the real-world interactions. One of the first observations we noted was that
    `#Item_Not_Received` was the most popular real-world intent. This validated the
    initial build strategy of supplying that intent with a higher number of training
    examples (relative to most other intents). We also noted that `#Chitchat_VA_About`
    had a high number of training examples compared to how infrequently this topic
    came up in the logs. This intent may be over-trained. It certainly doesn’t seem
    to be as popular as we thought it might be. Yet, until we look at the performance
    metrics for these intents, we cannot draw any solid conclusions. Rather, these
    observations might inform our improvement recommendations.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 每个意图的训练数据与代表性盲话语的并排音量比较可以帮助我们了解我们的解决方案的主题覆盖范围是否与实际世界的交互一致。我们首先注意到的是`#Item_Not_Received`是最受欢迎的实际意图。这验证了为该意图提供更多训练示例（相对于大多数其他意图）的初始构建策略。我们还注意到，与日志中该主题出现的频率相比，`#Chitchat_VA_About`有大量的训练示例。这个意图可能过度训练了。它显然不像我们想象的那么受欢迎。然而，在我们查看这些意图的性能指标之前，我们无法得出任何明确的结论。相反，这些观察结果可能会为我们的改进建议提供信息。
- en: Exercises
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习
- en: Run a representative blind test using your own data, and identify which intents,
    if any, exhibit poor performance.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您自己的数据运行代表性盲测试，并确定哪些意图（如果有的话）表现出较差的性能。
- en: Does your training volume align with the intent volume seen in your logs?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的训练音量与日志中看到的意图音量一致吗？
- en: How would you prioritize improvements for the poorest-performing intents?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会如何优先考虑对表现最差的意图进行改进？
- en: 5.2 Solving “wrong intent matched”
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 解决“意图匹配错误”
- en: 'When your chatbot returns the wrong intent, it has committed two categories
    of errors: false positives (predicting the wrong intent), and false negatives
    (failing to predict the right intent). Let’s walk through an improvement cycle
    to demonstrate how we would approach this problem.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的聊天机器人返回错误的意图时，它犯了两种类型的错误：假阳性（预测错误的意图）和假阴性（未能预测正确的意图）。让我们通过一个改进周期来演示我们将如何处理这个问题。
- en: 5.2.1 Improve recall for one intent
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1提高一个意图的召回率
- en: We will start with `#Login_Issue`, which was the fifth most popular topic but
    had a considerably low recall of 0.44\. There were nine test utterances in our
    blind set; it got four questions correct (true positives) and five incorrect (false
    negatives). This intent had a perfect precision score, which means it never showed
    up as a wrong prediction for other intents. Table 5.7 shows the summary metrics.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`#Login_Issue`开始，这是第五个最受欢迎的主题，但召回率相当低，为0.44。在我们的盲测集中有九个测试话语；它正确回答了四个问题（真阳性）和五个错误（假阴性）。这个意图有一个完美的精确度分数，这意味着它从未作为其他意图的错误预测出现。表5.7显示了总结指标。
- en: Table 5.7 Summary metrics for `#Login_Issue`; a blind test set run against our
    baseline classifier shows low recall but perfect precision.
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.7 `#Login_Issue`的总结指标；对基线分类器进行的盲测集运行显示召回率低但精确度完美。
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 意图 | 样本数量 | 预测数量 | 召回率 | 精确度 | F1分数 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| `Login_Issue`  | 9  | 5  | 0.4444  | 1  | 0.6153  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| `Login_Issue` | 9 | 5 | 0.4444 | 1 | 0.6153 |'
- en: In table 5.8, we can drill down to the result details of the blind test. Our
    classifier failed to predict a correct intent five times. Three of those were
    predictions of a wrong intent. Two were instances where confidence was so low
    that the classifier did not return a prediction.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在表5.8中，我们可以深入到盲测的结果细节。我们的分类器五次未能预测正确的意图。其中三次是错误意图的预测。两次是置信度如此之低，以至于分类器没有返回预测。
- en: Table 5.8 Baseline blind result details for `#Login_Issue` show that we had
    a recall score of 44%. Out of nine utterances, the correct (aka *golden*) intent
    was predicted five times.
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.8基线盲测结果细节显示，我们有一个召回率为44%的分数。在九个话语中，正确的（即*金色*）意图被预测了五次。
- en: '| Utterance | Golden intent | Predicted intent | Confidence |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 话语 | 金色意图 | 预测意图 | 置信度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BMV portal password reset  | `Login_Issue`  | <none>  | n/a  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| BMV门户密码重置 | `Login_Issue` | <none> | n/a |'
- en: '| I can’t get on my profile  | `Login_Issue`  | `Item_Not_Received`  | 0.8131  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 我无法进入我的个人资料 | `Login_Issue` | `Item_Not_Received` | 0.8131 |'
- en: '| I need help logging into my BMV profile  | `Login_Issue`  | <none>  | n/a  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 我需要帮助登录我的BMV个人资料 | `Login_Issue` | <none> | n/a |'
- en: '| I never got my security verification code  | `Login_Issue`  | `Item_Not_Received`  |
    0.2358  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 我从未收到我的安全验证码 | `Login_Issue` | `Item_Not_Received` | 0.2358 |'
- en: '| I tried logging in and it didn’t work  | `Login_Issue`  | `Login_Issue`  |
    0.8033  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 我尝试登录但失败了 | `Login_Issue` | `Login_Issue` | 0.8033 |'
- en: '| I’m not able to get into the portal  | `Login_Issue`  | `Login_Issue`  |
    0.6680  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 我无法进入门户 | `Login_Issue` | `Login_Issue` | 0.6680 |'
- en: '| Password locked out  | `Login_Issue`  | `Login_Issue`  | 0.5520  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 密码被锁定 | `Login_Issue` | `Login_Issue` | 0.5520 |'
- en: '| Password reset  | `Login_Issue`  | `Login_Issue`  | 0.4875  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 密码重置 | `Login_Issue` | `Login_Issue` | 0.4875 |'
- en: '| You never sent a security code  | `Login_Issue`  | `Item_Not_Received`  |
    0.2091  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 你从未发送安全码 | `Login_Issue` | `Item_Not_Received` | 0.2091 |'
- en: 'If we look at our current trained examples, it’s easy to see why so many questions
    were missed. There were only four examples:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看看我们当前的训练示例，很容易看出为什么错过了这么多问题。只有四个示例：
- en: I’m unable to log in on the website
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我无法在网站上登录
- en: Online account problem
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线账户问题
- en: Online problems
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线问题
- en: Problem signing onto my account
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 登录我的账户有问题
- en: 'Our training examples lack the variety of meaningful words and phrases seen
    in interactions with real users. Users might refer to their account as their “profile.”
    They list explicit problems such as being “locked out,” needing a “password reset,”
    and failing to receive a “security code.” We should expect to see an improvement
    if we add a few representative examples (obtained from our logs):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练示例缺乏在与真实用户互动中看到的丰富词汇和短语。用户可能会将他们的账户称为他们的“个人资料”。他们列出明确的问题，例如“被锁定”、“需要密码重置”和“未能收到安全码”。如果我们添加一些代表性的示例（从我们的日志中获取），我们应该会看到改进：
- en: Help signing in to online portal
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助登录在线门户
- en: I need to reset my password
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我需要重置我的密码
- en: I need a security code to log on
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these additions, we updated our classifier to V2 and reran the blind test
    set. Let’s look at how this affected the recall for `#Login_Issue` in table 5.9.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.9 Blind test result details show improved recall for our newest classifier
    version (V2). Out of nine utterances, the correct (aka *golden*) intent was predicted
    eight times.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Golden intent | Predicted intent | Confidence |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| BMV portal password reset  | `Login_Issue`  | `Login_Issue`  | 0.8253  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| I can’t get on my profile  | `Login_Issue`  | `Item_Not_Received`  | 0.8131  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| I need help logging into my BMV profile  | `Login_Issue`  | `Login_Issue`  |
    0.6846  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| I never got my security verification code  | `Login_Issue`  | `Login_Issue`  |
    0.7179  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| I tried logging in and it didn’t work  | `Login_Issue`  | `Login_Issue`  |
    0.8899  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| I’m not able to get into the portal  | `Login_Issue`  | `Login_Issue`  |
    0.7840  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| Password locked out  | `Login_Issue`  | `Login_Issue`  | 0.9083  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| Password reset  | `Login_Issue`  | `Login_Issue`  | 0.9204  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| You never sent a security code  | `Login_Issue`  | `Login_Issue`  | 0.2551  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: Our overall accuracy improved from 76% to 79% (106 out of 134 correct), and
    table 5.10 shows a dramatic improvement in the recall and F1 score. The precision
    score also remained steady.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.10 A comparison of summary metrics; our V2 classifier shows an overall
    improvement compared to the baseline (V1) for `#Login_Issue`.
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—Baseline (V1)  | 9  | 5  | 0.4444  | 1  | 0.6153  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—V2  | 9  | 8  | 0.8889  | 1  | 0.9412  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: 5.2.2 Improve precision for one intent
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s experiment with improving the precision for an intent. The `#Chitchat_VA_About`
    intent remained unchanged between the baseline test results and the V2 test results.
    (It is important to look at the newest results after each change.) Table 5.11
    shows that the recall was perfect, but the precision was only 50%. This means
    our classifier is placing a bit more importance on this topic, and it is showing
    up as a false positive (over-selecting) in another intent.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.11 Metrics after the V2 update show that `#Chitchat_VA_About` has perfect
    recall but poor precision.
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 1  | 2  | 1  | 0.50  | 0.6667  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: In table 5.12, we see that there was only one test question in our blind set
    for this intent, but our classifier predicted the intent twice.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.12 V2 blind result details show an over-selection for `#Chitchat_VA_About`.
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Golden intent | Predicted intent | Confidence |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| Do you have a name?  | `Chitchat_VA_About`  | `Chitchat_VA_About`  | 0.8042  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| Where are my tags?  | `Item_Not_Received`  | `Chitchat_VA_About`  | 0.3015  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: Our training has eight examples. We knew that these examples were manufactured
    (in fact, they were provided by a template), but our logs show that this is not
    a very common topic. Our blind test set only contained one utterance for this
    intent.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'One strategy for improving precision is to prune the training examples. This
    tells our classifier that the intent isn’t quite as dominant as the other intents
    within our solution. We’ll discard three examples because they are either overly
    redundant or, in the case of “Where are you from,” there was no evidence in the
    logs that this was a relevant question:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Are you a robot?
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can I ask you?
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can you do?
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can you help me with? (REMOVE FROM TRAINING)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s your name?
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where are you from? (REMOVE FROM TRAINING)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who am I talking to? (REMOVE FROM TRAINING)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are you?
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the training was updated (now V3), we ran the blind test again and reviewed
    the results. We saw an improvement to the precision for the `#Chitchat_VA_About`
    intent from V2 to V3—it was a perfect score across all metrics. Oddly enough,
    our overall accuracy dropped to 78% (from 79%), and one of the questions we lost
    was from our `#Login_Issue` intent. Table 5.13 shows the changes in metrics from
    V2 to V3 for both intents.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.13 Metrics before and after V3 update for `#Chitchat_VA_About` and `#Login_Issue`
    show that changing one intent can have an effect on another intent.
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`—V2  | 1  | 2  | 1  | 0.50  | 0.6667  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`—V3  | 1  | 1  | 1  | 1  | 1  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—V2  | 9  | 8  | 0.8889  | 1  | 0.9412  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—V3  | 9  | 7  | 0.7777  | 1  | 0.875  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: Although `#Login_Issue` had a slight decline, the current F1 score of 0.875
    is still far better than the baseline F1 score of 0.6153\. Keep in mind that smaller
    datasets are more sensitive to small changes, and a change to any intent can potentially
    affect every intent. Those changes may have negative or positive results. Instead
    of focusing on this, however, we will make a few more changes elsewhere and check
    back to see if the intent improves.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Improve the F1 score for one intent
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s move forward with improving the F1 score for `#Item_Not_Received`. Table
    5.14 shows that it had an F1 score of 56% after our V3 update.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.14 After V3 update, the F1 score remained unchanged at 0.56 for `#Item_Not_Received`.
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V2  | 16  | 9  | 0.4375  | 0.7777  | 0.56  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V3  | 16  | 9  | 0.4375  | 0.7777  | 0.56  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: The intent had eight training examples, but our logs showed that this is a very
    popular topic, so we need it to perform much better. We’ll add 10 more examples
    from our logs to that intent (now V4) and run another experiment.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.15 shows that our recall for this intent has now more than doubled,
    and though the precision fell slightly, the F1 score is greatly improved. The
    classifier’s overall accuracy also increased from 78% to 81%.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.15 Before and after metrics for `#Item_Not_Received` show an improved
    F1 score.
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V3  | 16  | 9  | 0.4375  | 0.7777  | 0.56  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V4  | 16  | 19  | 0.875  | 0.7368  | 0.8  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: 5.2.4 Improve precision and recall for multiple intents
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes there is confusion due to a heavy overlap of terms across intents
    that have similar goals. Figure 5.6 shows the confusion matrix that our testing
    tool provided.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: In our model, we see a fair amount of confusion across the intents that relate
    to stolen items. One solution to this problem is to merge intents. This must be
    considered carefully. The intents were probably created separately by design,
    as they all have different answers. However, entity detection can be used to route
    the flow to the appropriate answer.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F06_Freed2.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Confusion matrix after the V4 update. The density in shading represents
    the volume of questions predicted for a given intent. If a classifier test had
    a perfect accuracy score, you would see a solid black diagonal line running from
    the upper left corner to the lower right corner. The shaded squares that stray
    away from this diagonal line mark the areas of confusion within your model.
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll merge all of these into a single intent called `#Report_Stolen`. These
    examples are listed in table 5.16\. Don’t forget that the blind test set will
    need to reflect this change, as well as the related dialogue flows.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.16 Examples from three intents to be merged into a new `#Report_Stolen`
    intent
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent name | Training example |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | Report a stolen car  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | I need to report a stolen car  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | My plates were stolen  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | My registration was stolen  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | License plate stolen off vehicle  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | Stolen real ID  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | Wallet was stolen  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | My drivers license was stolen  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | My ID was stolen  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | My permit was stolen  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: The conversational flow will be updated so that when a defined entity value
    or synonym is detected in an utterance, the corresponding original answer is provided.
    You may also need a default condition to disambiguate or provide a generic answer
    in case an utterance triggers the new intent but no entity is detected. Table
    5.17 is an example of what that might look like.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.17 Dialogue updates using entity detection for the new `#Report_Stolen`
    intent
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Entity/synonym detected | Treatment |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| vehicle, car, truck, motorcycle  | Routes to original answer for `#Report
    Stolen Vehicle`  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| plates, registration, tags  | Routes to original answer for `#Report_Stolen_Plates_Registration`  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| ID, license, permit  | Routes to original answer for `#Report_Stolen_License_Permit_ID`  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| (none detected)  | Disambiguate (“It sounds like something was stolen; can
    you tell me what it was?”)  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: With these changes, our classifier is now on V5\. Table 5.18 shows the metrics
    for the three old intents under V4 and the metrics for our new intent in V5.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.18 Metrics before and after V5 update show that merging three intents
    into the single `#Report_Stolen` intent results in perfect scores across the board
    for this topic.
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: —V4
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '| 7  | 5  | 0.5714  | 0.8  | 0.6666  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: —V4
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '| 5  | 4  | 0.8  | 1  | 0.8888  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`—V4  | 2  | 2  | 0.5  | 0.5  | 0.5  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen`—new intent in V5  | 14  | 14  | 1  | 1  | 1  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: Our latest change dramatically improved the performance of this topic, and it
    bumped the overall accuracy to 85%, which is now higher than our baseline *k*-fold
    (which was 84%).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: With that update complete, we can move on to other intents that need improvement.
    Following the iterative processes, we updated the remaining intents that showed
    the poorest performance by adding a few more examples from the logs. This became
    V6 of our classifier. Table 5.19 is an overview of the intents that were updated.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.19 Training example counts increase from V5 to V6 and more closely align
    with the volume present in the representative blind test set.
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | V5 training example count | V6 training example count | Test utterances
    in representative blind |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 5  | 6  | 7  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 4  | 6  | 5  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 7  | 8  | 9  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 6  | 8  | 8  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: This update resulted in an overall accuracy of 92% for the latest classifier
    (now on V6). In the world of natural language classification, this is a very good
    score for a representative blind test set. You will never achieve 100%; even human-to-human
    communications don’t come close to that.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Every data set is different, and we could spend several more cycles tweaking
    our training if there is plenty of data available. However, there are diminishing
    returns associated with pursuing results that approach 100%. There is also a risk
    of over-fitting your model to the current blind test set. Once additional logs
    become available and a new test set is created, you may discover additional gaps
    (or your overfitting will be exposed).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.20 shows a comparison of the blind test F1 scores of the baseline classifier
    against our latest updates. Twelve of the intents did not change (and they were
    already performing very well). One intent decreased from 90% to 80%, and the remaining
    14 intents showed improvement. We felt that this was a good and reasonable tradeoff,
    improving more than half of our intents at the cost of one intent showing a slight
    decline.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.20 Comparison of baseline F1 scores and V6 F1 scores
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Baseline (V1) F1 score | V6 F1 score | Change |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 1  | 1  | (no change)  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 0.8333  | 0.833  | (no change)  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 1  | 1  | (no change)  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 1  | 1  | (no change)  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 1  | 1  | (no change)  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 1  | 1  | (no change)  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 0.6667  | 0.9524  | + 0.2857  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 0.90  | 0.80  | - 0.1  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 0.80  | 0.80  | (no change)  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 0.80  | 0.80  | (no change)  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 0.75  | 0.8571  | + 0.1071  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 0.56  | 0.8750  | + 0.315  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 0.60  | 0.75  | + 0.15  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 0.6667  | 1  | + 0.3333  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 0.6153  | 0.9412  | + 0.3259  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 1  | 1  | (no change)  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 0.90  | 1  | + 0.1  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 1  | 1  | (no change)  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 0.8571  | 0.8571  | (no change)  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 0.9231  | 1  | + 0.0769  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | 0.80  | (n/a - merged)  | + 0.2  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | 0.8889  | (n/a - merged)  | + 0.1111  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 0.50  | (n/a - merged)  | + 0.5  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen`  | n/a  | 1  | (n/a – merged)  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 1  | 1  | (no change)  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 0.8889  | 1  | + 0.1111  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 0.40  | 0.8  | + 0.4  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 0.4615  | 0.75  | + 0.2885  |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: Exercises
  id: totrans-376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Using the output from the previous exercise (a prioritized list of your poorest-performing
    intents), identify the category of error each intent is committing: recall, precision,
    or both.'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make iterative training adjustments to improve each intent.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure each change to verify that
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The intended effect is achieved
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No other intents were negatively affected
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.3 Solving “no intent matched”
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our classifier in good shape for the current scope, we can
    focus on expanding the domain, if needed. During an initial review of your production
    logs, you will almost surely encounter topics that were not included in the initial
    training set. Some of these topics will be obvious, but perhaps there wasn’t enough
    data to train an intent at the time of the initial launch. Maybe the business
    wasn’t ready to write answers for some topics. Sometimes a seasonal topic is not
    included because it was not in the forefront of anyone’s mind (e.g., tax season,
    hurricane season, fiscal year end, etc.). Other topics may be completely unexpected
    (e.g., a data breach).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Although you don’t have any intents defined to match these utterances, the classifier
    will always attempt to make a prediction; it doesn’t know what it doesn’t know,
    so it does its best to match an utterance to what it does know. In an ideal world,
    the classifier would return very low confidence, and this would trigger an “anything_else”
    or “no action matches” type of response. In reality, such user utterances often
    contain words that appear somewhere in your training, so it is possible that the
    classifier will predict an intent that has training examples with similar words.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Clustering utterances for new intents
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the guidelines described in chapter 4, we recommended setting aside utterances
    that were related to the domain but not included in the original scope. It’s time
    to address these.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: One of the topics our logs revealed was related to users wanting to cancel their
    license or registration. We know from our logs how the classifier predicted each
    utterance at the time the utterance was asked. Now we can test them against our
    latest classifier (V6) to get new model predictions.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: In table 5.21, we see that our classifier exhibited low confidence and/or was
    incorrect whenever an utterance contained a form of the word “cancel.”
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.21 Unmatched utterances from logs with predictions from the V6 classifier
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Predicted intent | Confidence |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| Cancel a registration  | `Appointment`  | 0.2681  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| Cancel my car registration  | `License_or_ID`  | 0.3651  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| Cancel a drivers license  | `License_Reinstatement`  | 0.3042  |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Canceling a registration  | `Appointment`  | 0.2417  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| Cancellation of registration  | `Fee_Info`  | 0.2786  |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| Cancelling my registration  | `Item_Not_Received`  | 0.3004  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| Cancel a replacement license  | `Vehicle_Permit`  | 0.3264  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| Cancel the license  | `License_Reinstatement`  | 0.3237  |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| Cancel a title or registration  | `Vehicle_Title`  | 0.5913  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| Cancel vehicle registrations  | `Item_Not_Received`  | 0.2914  |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| Commercial drivers license cancel  | `License_Reinstatement`  | 0.2995  |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| Driver’s license cancellation  | `Get_ID_Number`  | 0.3387  |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| How do I cancel my vehicle registration?  | `License_or_ID`  | 0.4324  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| I need to cancel a vehicle registration  | `License_or_ID`  | 0.3481  |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| I need to cancel my ID  | `Get_ID_Number`  | 0.3205  |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| I would like to cancel the registration on my car  | `Change_Contact_Records`  |
    0.3147  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| I would like to cancel my car registration  | `License_or_ID`  | 0.3447  |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| I would like to cancel my state identification card  | `Change_Contact_Records`  |
    0.2982  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| I wanted to cancel a registration  | `Item_Not_Received`  | 0.3155  |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| I want to confirm cancellation of my registration  | `Item_Not_Received`  |
    0.4092  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| Questions about cancelling registration for a vehicle  | `Fee_Info`  | 0.2795  |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| I want to cancel my registration on my pickup  | `Item_Not_Received`  | 0.4761  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: We’ll randomly divide these into a training set of nine utterances under a new
    `#Cancel_Registration_or_License` intent and add the remaining thirteen to our
    blind test set.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: When we run the updated blind test set against our updated classifier (now V7),
    we get an overall accuracy of 92%, which is usually a very good, if not ideal,
    outcome. This will not always be the case, so if your overall performance drastically
    drops, you will need to iterate through the applicable improvement steps (depending
    on whether the problem was recall, precision, or both) for the intents that were
    affected.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through one more example of adding a new intent. The logs contained
    several utterances referring to a data breach. This is an example of how a chatbot
    can exhibit declining performance due to new information in the world. In this
    case, the organization had never experienced a data breach before. But when it
    did, and this news became public, users suddenly had a lot of questions about
    it. This manifested as unmatched and incorrect predictions, as seen in table 5.22.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.22 Unmatched utterances on the topic of “data breach” from logs, with
    predictions from the V7 classifier. The classifier didn’t have enough confidence
    to match most of the utterances referring to “hack” or “data breach,” which is
    good because we hadn’t yet taught it anything about that topic. But most of the
    utterances that contain the word “stolen” match strongly against our `#Report_Stolen`
    intent. This may not go so well for the user because our solution doesn’t have
    any answers yet concerning data that was stolen.
  id: totrans-417
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Predicted intent | Confidence |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| I want to know about that hacking on the BMV  | <none>  | n/a  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| I want to know about the breach in information at the BMV and if I’m at risk  |
    <none>  | n/a  |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| My identity has been stolen  | `Report_Stolen`  | 0.9483  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| My license number was stolen  | `Report_Stolen`  | 0.9240  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| Need questions answered about data breach  | <none>  | n/a  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| No I’m curious about the current breach of stolen IDs  | `Report_Stolen`  |
    0.8604  |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| Someone hacked my information  | `Report_Stolen`  | 0.4662  |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| Someone is using my drivers license number  | `Get_ID_Number`  | 0.4067  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| Someone stole my identity  | `Report_Stolen`  | 0.7705  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| Someone stole my information  | `Report_Stolen`  | 0.8043  |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| Stolen personal identity  | `Report_Stolen`  | 0.9263  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| Stolen personal information  | `Report_Stolen`  | 0.9166  |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '| Stolen social security number  | `Report_Stolen`  | 0.7515  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| Was my account affected by the recent data hack?  | <none>  | n/a  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| Was my account hacked?  | `Login_Issue`  | 0.3998  |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| Was there a data breach?  | <none>  | n/a  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| Yeah I’d like to know if my driver’s license has been breached  | `Report_Stolen`  |
    0.4092  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| Yes what do I do about the data breach at the BMV?  | <none>  | n/a  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| Was my social security number stolen in the hack?  | `Report_Stolen`  | 0.7806  |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| I want to know if my information was stolen  | `Report_Stolen`  | 0.9198  |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: To resolve the problem of this unmatched intent, we selected seven representative
    utterances from the logs to create a new intent called `#Data_Breach`. Our selection
    ensured that a variety of important terms, such as “hack,” “breach,” and “stolen,”
    were added to our new training set. The remaining utterances were added to our
    blind test set, and we tested our newest classifier, V8\. The new `#Data_Breach`
    intent returned a perfect score, and the F1 score comparisons in table 5.23 show
    that nearly all others remained steady or improved since our baseline reading.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.23 Final score comparison between the baseline (V1) and the final version
    (V8)
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Baseline (V1) F1 score | V8 F1 score |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 1  | 1  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 0.8333  | 0.8333  |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| `Cancel_Registration_or_License`  | n/a (NEW)  | 0.9630  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 1  | 0.8889  |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 1  | 1  |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 1  | 1  |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 1  | 1  |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 0.6667  | 1  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| `Data_Breach`  | n/a (NEW)  | 1  |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 0.90  | 0.9524  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 0.80  | 0.80  |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 0.80  | 0.80  |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 0.75  | 1  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 0.56  | 0.8750  |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 0.60  | 0.75  |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 0.6667  | 1  |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 0.6153  | 0.9412  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 1  | 1  |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 0.90  | 1  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 1  | 1  |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 0.8571  | 0.80  |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 0.9231  | 1  |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | 0.80  | (n/a - merged)  |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | 0.8889  | (n/a - merged)  |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 0.50  | (n/a - merged)  |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen`  | n/a  | 0.9630  |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 1  | 1  |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 0.8889  | 1  |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 0.40  | 0.6667  |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 0.4615  | 0.75  |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: Our overall accuracy score remained steady at 92%. (Our updated blind test set
    has 160 questions, and 147 were correct.) You might recall that our very first
    blind test had an overall accuracy of 76%, so this is quite an improvement. Our
    V8 confusion matrix, shown in figure 5.7, also looks improved, with a fairly dark
    diagonal line.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F07_Freed2.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 Comparison of baseline (V1) confusion matrix to the V8 update
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We could iterate further to try to get a little higher, but for this use case,
    the classifier’s accuracy is more than good enough for the time being. Any further
    tweaks with the limited data we have at present are likely to over-fit our model
    to the current blind test set. Remember that a healthy strategy is to plan to
    iterate over the life of the bot, using newer logs and refreshed blind test sets.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 When to stop adding intents
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When reviewing your logs, you may have encountered a diverse range of other
    questions that are perfectly reasonable for the domain, but very infrequent. In
    our logs, we saw questions like the following, but no additional utterances with
    similar goals:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: I need a form for a doctor to fill out saying a driver is not safe to drive
    anymore.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have a question about electronic signatures.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the process for getting a specialty license plate?
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we know when to stop adding intents? It’s best to let the data from our
    human-annotated logs guide us. We can total up all of the examples by intent and
    render them as a chart, as in figure 5.8\.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F08_Freed2.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 Example of a longtail chart. The terms we use to describe the volume
    distribution of our available training data are “short head” and “long tail.”
    These terms describe the visual representation of rendering our data on a bar
    chart. The heavier-volume intents are on the left (the short head), and as the
    volume decreases for each intent, the data has the appearance of a long tail falling
    off to the right.
  id: totrans-485
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In our longtail chart, we picked a point to divide between what should be in
    scope versus out of scope. This point isn’t a static, prescriptive position. It’s
    a decision that should be made with the business by establishing a minimum number
    of training examples required to create a new intent. Everything that falls on
    the left of this line should probably be included in the training, as there is
    evidence that these topics will be asked more frequently. Everything to the right
    will not be trained in the current classifier. Over time, you may find enough
    data in the logs to justify adding a new intent. Until then, your solution will
    have to handle such topics with one of the following strategies: give a response
    saying the bot doesn’t understand, fall back to an agent escalation, add a search
    integration to find answers in a document repository, or implement a retrieval-augmented
    generation (RAG) or large language model (LLM) component to generate answers.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-487
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Identify new topics based on your logs, and build new intents from the utterances
    found in the logs.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add utterances to your blind set, and test your changes.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is your classifier able to recognize the new intent without negatively affecting
    the performance of your existing intents?
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.4 Supplementing traditional AI with generative content
  id: totrans-491
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conversational AI, we typically think of delivering either a static answer
    (as in classic intent-driven implementations) or an answer that is entirely generated
    (as in a RAG pattern). Static answers fill a need where an answer must maintain
    consistency, either in content or in structure. Although personalization is possible,
    it is generally limited to defined entities or other context-driven dialogue conditions.
    This tends to result in colder, less personalized bot responses. Figure 5.9 shows
    how three users with the same general goal, but very different personal situations,
    all receive the same bot response.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F09_Freed2.png)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 In a traditional (classification-based) dialogue pattern, an intent
    is identified, and the dialogue is configured to give a static or minimally personalized
    answer.
  id: totrans-494
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.4.1 Combining traditional and generative AI for an intent
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can enhance the user experience using a hybrid response pattern, which combines
    personalized generated content with the static predefined answers written for
    our intent. Our goal is to acknowledge the user’s problem while ensuring that
    important information is delivered with consistency. Many large language models
    excel at summarization tasks, so a model can be prompted to craft an empathetic
    message that conveys a personalized level of understanding. Figure 5.10 shows
    what this looks like from the user’s perspective.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F10_Freed2.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 An output response identifies the correct intent using traditional
    AI and then prepends generated text to the static output response configured for
    the intent. The generated greeting and summary convey to the user that the bot
    understands their goal and the particular details of the user’s situation.
  id: totrans-498
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This pattern employs an API call to the LLM as a dialogue step. Content is generated
    by the LLM and delivered just before the predefined output response. Figure 5.11
    shows the high-level steps for such a pattern.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F11_Freed2.png)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 LLMs can be called within traditional dialogue patterns to greet
    a user and summarize their problem before delivering a predefined or static answer.
  id: totrans-501
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.4.2 Prompting to convey understanding
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In conversational AI, your bot’s role is typically to be a representative of
    your company. They are a “digital” resource, as opposed to a “human” resource.
    Still, their job is to be the face of the company. Human agents are great at conveying
    empathy and understanding. In fact, they will often restate the user’s problem
    to demonstrate that they understand. LLMs can be prompted to simulate this summarization
    behavior.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Since our traditional AI has already classified the user’s intent under this
    pattern, we can craft a prompt that instructs the LLM to perform a specific task.
    In this case, we want the LLM to generate a personalized, empathetic greeting
    that can be paired with additional static content. The next listing shows a prompt
    instruction for summarizing a user’s input.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Prompting a model to greet and summarize a user problem
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exercises
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Collect a set of user utterances to test and tune an LLM prompt that can greet
    a user where appropriate and summarize their problem.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with a variety of instruction prompts. The goal is to create an efficient
    prompt instruction that will produce good results for the majority of your utterance
    test set.
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classifier’s performance can be measured in terms of accuracy, recall, precision,
    and F1 score. These measurements reflect the types of errors a classifier may
    be committing.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance metrics produced by your testing will inform your next steps
    toward improving classifier performance. Higher volume intents with low performance
    are a good place to start.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterative test and train cycles will show you the effects of your changes.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A chatbot can use additional strategies, such as disambiguation, clarifying
    questions, and entity detection to overcome confusion or route answers for merged
    intents.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A chatbot with a strong classifier can deliver more business value by delivering
    the right answers on the first try and deflecting work that would otherwise be
    handled by a human agent. You should plan to monitor and retrain your solution
    throughout the life of the bot.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI can supplement a traditional AI solution by infusing static chatbot
    responses with personalization and empathy, which enhances the perception of understanding.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
