- en: 5 Improving weak understanding for traditional AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identifying the types of errors a classifier can make
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing a baseline of current classifier performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using data science methodologies to identify and prioritize improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infusing your traditional AI with generated content to enhance understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will demonstrate a methodical, iterative approach to improving
    the understanding of a classification-based conversational solution. This chapter
    builds on the concepts introduced in the previous chapter and uses the output
    produced by the final exercise in section 4.4 (where you created a test set with
    the golden intent assigned to each utterance in a format that can be used by your
    testing tool). Later in this chapter, we’ll explore how large language models
    can supplement intent-driven output responses to deliver a more robust experience.
    (If you’re looking for generative AI improvement techniques, feel free to skip
    ahead to the next chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: We will start by building an improvement plan and identifying the types of errors
    your classifier may be committing. Next, we’ll iterate through seven improvement
    cycles to solve the various problems you might see in your own text classifier.
    Although data science techniques are used, you do not need to be a data scientist
    to extract meaningful insights about your data using the methodologies presented
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Building your improvement plan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you built a blind test set using a sample from your production logs, you
    should have a reliable “representative distribution” test set. This means that
    the topics that are most frequently asked by your users are represented with corresponding
    volume in your testing data. This will be a key factor in prioritizing any problems
    that are surfaced by your test results.
  prefs: []
  type: TYPE_NORMAL
- en: If you are working with the results of a *k*-fold test (discussed in chapter
    4), you won’t know for certain which topics are the most important, so the most
    egregious accuracy scores are a logical starting point.
  prefs: []
  type: TYPE_NORMAL
- en: In either case, it’s now time to dig into those test results. An improvement
    plan starts with identifying the biggest problem spots in the bot’s training.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Identify problematic patterns in misunderstood utterances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first score that will grab your attention is the overall accuracy of your
    test results. This is a lot like getting back a spelling or math test and looking
    at the red ink at the top of the page. If your test had 100 questions and you
    got 79 of them correct, your accuracy score would be 79%. For classifiers, this
    number is good for an “at a glance” view of the model, but it doesn’t give a complete
    picture of what is going on or where to start making improvements. For that, we
    need to understand the possible outcomes and types of errors our classifier may
    be committing. This is revealed in the measurements of recall, precision, and
    F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: A brief explanation of recall, precision, and F1 scores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In chapter 4, we described *recall* as the classifier’s ability to predict
    a correct intent and *precision* as the ability to refrain from predicting a wrong
    intent. You can think of this in terms of positive and negative predictions. For
    every utterance that we test against the model, there are four possible outcomes,
    and they are not mutually exclusive, meaning that every prediction is going to
    have two or three of these outcomes happening simultaneously. Figure 5.1 shows
    a confusion matrix that visualizes these possible outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F01_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 In a 2 × 2 confusion matrix, the possible outcomes are derived by
    comparing the predicted intent to the actual intent.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*True positive*—A prediction that matches the correct intent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*True negative*—A prediction that does not match an incorrect intent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*False positive*—A prediction that matches an incorrect intent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*False negative*—A prediction that does not match the correct intent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first metric that might interest us is the recall of our intents. For this,
    we need to know the true positives and the false negatives. An intent that is
    returning false negatives is committing an error of under-selection. When measured
    per intent, this looks like an accuracy score. If our test had five questions
    for the `#Request_Agent` intent, and the classifier got those questions correct
    four times, the intent’s recall would be 80%:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recall = True positives / (True positives + False negatives)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next metric that helps us understand our classifier is precision. This
    measures how good our classifier is at refraining from giving a false positive.
    An intent that is returning false positives is committing an error of over-selection.
    An example of over-selection can be seen in the last two rows of table 5.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision = True positives / (True positives + False positives)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5.1 Test results show seven utterances, five of which are labeled with
    the correct `#Request_Agent` intent. The first four predictions were true positives.
    The last two rows show where `#Request_Agent` was predicted twice for utterances
    where it shouldn’t have been (“What can I ask you” and “Somebody hit my car”).
    These false positives contribute to our precision calculation: 4 / (4 + 2) = 0.66.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Correct intent | Predicted intent | Correct |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Customer service  | `Request_Agent`  | `Request_Agent`  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| Speak with an agent  | `Request_Agent`  | `Request_Agent`  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| Can I please speak with somebody?  | `Request_Agent`  | `Request_Agent`  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| Talk with a human  | `Request_Agent`  | `Request_Agent`  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| When will I get a live person?  | `Request_Agent`  | `Office_Hours`  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| What can I ask you?  | `VA_Capabilities`  | `Request_Agent`  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| Somebody hit my car  | `Report_Accident`  | `Request_Agent`  | 0  |'
  prefs: []
  type: TYPE_TB
- en: A full analysis of all possible outcomes for `#Request_Agent` is shown in figure
    5.2\. It also shows the true negatives (which are not used in our calculations
    but have been included to demonstrate the range of other outcomes).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F02_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 The highlighted columns are used for calculating precision and recall
    for `#Request_Agent`.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now that we know the recall and precision, we can also calculate the F1 score,
    which is the harmonic mean of recall and precision. This calculation is made as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*F1 score = (2 *×* Precision *×* Recall) / (Precision + Recall)*'
  prefs: []
  type: TYPE_NORMAL
- en: For our `#Request_Agent` intent, this would be calculated as (2 × 0.66 × 0.8)
    / (0.66 + 0.8) = 0.72\. Table 5.2 shows all three scores.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.2 Recall, precision, and F1 score for `#Request_Agent`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Recall | Precision | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Request_Agent`  | 0.80  | 0.66  | 0.72  |'
  prefs: []
  type: TYPE_TB
- en: What about the true negatives?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Earlier in this section, we mentioned true negatives—a prediction that does
    not match an incorrect intent. True negatives occur whenever we have more than
    one trained intent. However, they are not a useful measurement in our methods.
  prefs: []
  type: TYPE_NORMAL
- en: Why not? Well, for every prediction the model makes, there is only one way for
    it to be right, but there are two ways for it to be wrong. This seems a little
    unfair, and it’s hard to see why if you’re just looking at two intents. But imagine
    we have a model that was trained with 20 intents. Whenever we make a single prediction
    that returns a true positive, we will also get 19 true negatives. And for every
    false positive prediction,
  prefs: []
  type: TYPE_NORMAL
- en: we have 1 false negative and 18 true negatives. So all those true negatives
    add up to a very large number that, for our purposes, doesn’t give us much insight.
    Therefore, we don’t factor true negatives into our calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding which metric is important
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall, precision, F1 score: Which number should we care about? That’s a great
    question! The answer is that it depends on what your organization values most
    in terms of what the solution needs to deliver. Here are some considerations to
    guide you to an answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall is useful when there is a high cost associated with false negatives.
    Imagine the effect if a fraud detection tool missed 25% of the fraudulent transactions
    it evaluated. (For a chatbot, this would look like a correct intent that is not
    predicted 25% of the time.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision is useful when there is a high cost associated with false positives.
    Think of the gameshow Jeopardy!, which penalizes a contestant for attempting to
    answer and getting it wrong (or a chatbot that over-selects the `#Request_ Agent`
    intent, resulting in unnecessary escalations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F1 score is useful when there is a high cost associated with both false
    positives and false negatives. We like to use this for most implementations because
    it reflects a good balance of the recall and precision scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing your data with a confusion matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Earlier in this section, we showed a 2 × 2 confusion matrix to demonstrate the
    potential outcomes. A confusion matrix can help you assess the performance of
    a classification model by visualizing a summary of the predictions made by your
    model. Some testing tools produce this with their results output.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 shows a fictional scenario where a classifier model made ten perfect
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F03_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 A solid diagonal line shows that each predicted intent (represented
    by a single letter) matched to the actual intent.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Shaded boxes that stray from the diagonal provide useful insights about where
    your model is confused, as shown in figure 5.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F04_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 This model had nine correct predictions, but wrongly predicted intent
    G when the actual intent was E.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.1.2 Incremental improvements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An incremental improvement approach will affect measurable change in a manageable
    way. Every change you make to a classifier has the potential to affect multiple
    intents. Sometimes this effect is positive, but sometimes it’s not. You might
    get away with updating several intents all at once, but if the testing shows a
    performance decline, it can be difficult to track down the culprit. You will have
    to balance the need for efficiency with your tolerance for rework.
  prefs: []
  type: TYPE_NORMAL
- en: '5.1.3 Where to start: Identifying the biggest problems'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, the best place to start is with the highest volume intents that have
    the lowest F1 scores. The business may also weigh in on priorities. If a lower
    volume intent fails to recognize the type of request it was designed to handle,
    but this failure incurs costly human intervention, it might take priority.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the rest of this chapter, we will explore a fictional use case: a chatbot
    that serves a population that interacts with a state’s Bureau of Motor Vehicles
    (a type of US government agency that regulates and manages the issuance of state
    identification cards, driver’s licenses, certain permits, and vehicle registrations).'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, let’s follow the advice given in chapter 4 and take a quick, high-level
    look at our current training data, as laid out in table 5.3.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.3 Intents with example counts in a baseline training set
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent name | Number of examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 2  |'
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 3  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 3  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 2  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 8  |'
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 8  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 3  |'
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | 3  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 2  |'
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 6  |'
  prefs: []
  type: TYPE_TB
- en: '| **Grand Total**  | **125**  |'
  prefs: []
  type: TYPE_TB
- en: We can make some quantitative statements about this training set. It has 27
    intents with a grand total of 125 training examples. The examples are distributed
    fairly evenly. As a qualitative assessment, we might say that many of the intents
    appear to be unique, but a few of them might have some overlap. Some terms definitely
    overlap across intent names. A peek at the full set of training utterances (not
    shown) revealed that many terms appear in multiple intents, such as “ID,” “title,”
    “permit,” “vehicle,” “stolen.” However, as shown in table 5.4, the contexts in
    which these words appeared were judged to be appropriately labeled.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.4 Utterances extracted from the baseline training set show a variety
    of terms overlapping across multiple intents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Labeled intent |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| How much is an ID?  | `Fee_Info`  |'
  prefs: []
  type: TYPE_TB
- en: '| I need to find out my ID number  | `Get_ID_Number`  |'
  prefs: []
  type: TYPE_TB
- en: '| I didn’t receive my ID  | `Item_Not_Received`  |'
  prefs: []
  type: TYPE_TB
- en: '| Title never came  | `Item_Not_Received`  |'
  prefs: []
  type: TYPE_TB
- en: '| Add a person to the title  | `Vehicle_Title`  |'
  prefs: []
  type: TYPE_TB
- en: '| How do I get a driving permit?  | `License_or_ID`  |'
  prefs: []
  type: TYPE_TB
- en: '| Replace my program parking permit  | `Vehicle_Permit`  |'
  prefs: []
  type: TYPE_TB
- en: '| I sold a vehicle  | `Report_Sold_Vehicle`  |'
  prefs: []
  type: TYPE_TB
- en: '| I need to report a stolen car  | `Report_Stolen_Vehicle`  |'
  prefs: []
  type: TYPE_TB
- en: '| My ID was stolen  | `Report_Stolen_License_Permit_ID`  |'
  prefs: []
  type: TYPE_TB
- en: Overall, it seems that the range of topics is reasonable for the chatbot’s purpose,
    which in this case is to answer questions a user might have when dealing with
    a state’s Bureau of Motor Vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a baseline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we have made an initial assessment of our training data, we need to
    understand how it is currently performing. We’ll start by running a *k*-fold cross
    validation test to establish a baseline. The results, our first version (V1) shown
    in table 5.5, are not that bad, considering the low volume of data present in
    the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.5 Baseline (V1) *k*-fold results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 2  | 2  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 6  | 8  | 1  | 0.75  | 0.8571  |'
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 3  | 0  | 0  | 0  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 3  | 0  | 0  | 0  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 4  | 6  | 1  | 0.6667  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 2  | 2  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 8  | 8  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 5  | 2  | 0.40  | 1  | 0.5714  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 6  | 7  | 1  | 0.8571  | 0.9231  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 5  | 4  | 0.80  | 1  | 0.8889  |'
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 4  | 6  | 1  | 0.6667  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 8  | 6  | 0.6250  | 0.8333  | 0.7143  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 4  | 4  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 5  | 5  | 0.60  | 0.60  | 0.60  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 4  | 4  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 6  | 8  | 1  | 0.75  | 0.8571  |'
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 6  | 6  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 3  | 3  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 4  | 4  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 6  | 5  | 0.8333  | 1  | 0.9091  |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '| 5  | 6  | 1  | 0.8333  | 0.9091  |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '| 3  | 3  | 0.3333  | 0.3333  | 0.3333  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 2  | 3  | 1  | 0.6667  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 4  | 4  | 1  | 1  | 1.0000  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 5  | 6  | 1  | 0.8333  | 0.9091  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 6  | 9  | 1  | 0.6667  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 6  | 4  | 0.6667  | 1  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: Our *k*-fold test had a total of 125 questions (the grand total of our training
    set), and it got 105 of them correct, for an overall accuracy of 84%. Several
    intents had perfect recall and perfect precision (which is often a hallmark of
    a manufactured data set). There were two intents that had a recall of 0; they
    each had only three training examples. This reveals one of the flaws of *k*-fold
    testing—there simply weren’t enough examples to distribute across the auto-generated
    train and test sets. More than likely, those intents will perform better than
    0 in production. However, the intents with perfect recall will probably not perform
    quite as well. If you are launching a pilot and have no other training data available,
    these results are generally good enough to go live, with a strong caution to the
    stakeholders that they should expect lower actual performance until representative
    data is available for use in training updates.
  prefs: []
  type: TYPE_NORMAL
- en: Once the solution is live, a new baseline should be taken using the blind test
    set you created from the logs. We have an example of this in table 5.6, and it
    really emphasizes the gap in performance predicted by our *k*-fold test compared
    to real user inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.6 Baseline (V1) blind results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 2  | 2  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 7  | 5  | 0.7143  | 1  | 0.8333  |'
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 4  | 4  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 1  | 1  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 1  | 1  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 1  | 1  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 1  | 2  | 1  | 0.50  | 0.6667  |'
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 11  | 9  | 0.8182  | 1  | 0.90  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 2  | 3  | 1  | 0.6667  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 3  | 2  | 0.6667  | 1  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 3  | 5  | 1  | 0.60  | 0.75  |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 16  | 9  | 0.4375  | 0.7778  | 0.56  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 5  | 5  | 0.60  | 0.60  | 0.60  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 7  | 5  | 0.5714  | 0.80  | 0.6667  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 9  | 5  | 0.4444  | 1  | 0.6153  |'
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 9  | 9  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 9  | 11  | 1  | 0.8182  | 0.90  |'
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 2  | 2  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 3  | 4  | 1  | 0.7500  | 0.8571  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 6  | 7  | 1  | 0.8571  | 0.9231  |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '| 7  | 8  | 0.8571  | 0.75  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '| 5  | 4  | 0.80  | 1  | 0.8889  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 2  | 2  | 0.50  | 0.50  | 0.50  |'
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 4  | 4  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 4  | 5  | 1  | 0.80  | 0.8889  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 2  | 8  | 1  | 0.25  | 0.40  |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 8  | 5  | 0.3750  | 0.60  | 0.4615  |'
  prefs: []
  type: TYPE_TB
- en: On the first run of our blind test, 102 questions were correct out of 134, for
    an overall accuracy of 76%—8 points lower than the 84% predicted by our *k*-fold
    test.
  prefs: []
  type: TYPE_NORMAL
- en: Validating your initial training strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once you have obtained annotated logs and taken some baseline performance measurements,
    you can validate the decisions that informed your initial training strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Scarcity of representative training data is a very common problem for conversational
    AI projects. Just like many other newly launched chatbots, our initial training
    set was developed by subject matter experts (SMEs) who manufactured training examples
    for the topics they believed would occur most frequently. In figure 5.5, we can
    compare the number of examples trained for each intent to the number of examples
    that were present in the randomly selected logs used for testing.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F05_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 A comparison of training examples to the utterances in our representative
    blind test set shows that there is some disparity in volume for many of the most
    popular intents (the representative blind utterances) on the left side of the
    graph. We also see disparity across several of the least popular intents (those
    on the right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A side-by-side volume comparison of training data to representative blind utterances
    per intent can help us understand if our solution’s topic coverage is in alignment
    with the real-world interactions. One of the first observations we noted was that
    `#Item_Not_Received` was the most popular real-world intent. This validated the
    initial build strategy of supplying that intent with a higher number of training
    examples (relative to most other intents). We also noted that `#Chitchat_VA_About`
    had a high number of training examples compared to how infrequently this topic
    came up in the logs. This intent may be over-trained. It certainly doesn’t seem
    to be as popular as we thought it might be. Yet, until we look at the performance
    metrics for these intents, we cannot draw any solid conclusions. Rather, these
    observations might inform our improvement recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Run a representative blind test using your own data, and identify which intents,
    if any, exhibit poor performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does your training volume align with the intent volume seen in your logs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you prioritize improvements for the poorest-performing intents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.2 Solving “wrong intent matched”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When your chatbot returns the wrong intent, it has committed two categories
    of errors: false positives (predicting the wrong intent), and false negatives
    (failing to predict the right intent). Let’s walk through an improvement cycle
    to demonstrate how we would approach this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Improve recall for one intent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will start with `#Login_Issue`, which was the fifth most popular topic but
    had a considerably low recall of 0.44\. There were nine test utterances in our
    blind set; it got four questions correct (true positives) and five incorrect (false
    negatives). This intent had a perfect precision score, which means it never showed
    up as a wrong prediction for other intents. Table 5.7 shows the summary metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.7 Summary metrics for `#Login_Issue`; a blind test set run against our
    baseline classifier shows low recall but perfect precision.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 9  | 5  | 0.4444  | 1  | 0.6153  |'
  prefs: []
  type: TYPE_TB
- en: In table 5.8, we can drill down to the result details of the blind test. Our
    classifier failed to predict a correct intent five times. Three of those were
    predictions of a wrong intent. Two were instances where confidence was so low
    that the classifier did not return a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.8 Baseline blind result details for `#Login_Issue` show that we had
    a recall score of 44%. Out of nine utterances, the correct (aka *golden*) intent
    was predicted five times.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Golden intent | Predicted intent | Confidence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BMV portal password reset  | `Login_Issue`  | <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| I can’t get on my profile  | `Login_Issue`  | `Item_Not_Received`  | 0.8131  |'
  prefs: []
  type: TYPE_TB
- en: '| I need help logging into my BMV profile  | `Login_Issue`  | <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| I never got my security verification code  | `Login_Issue`  | `Item_Not_Received`  |
    0.2358  |'
  prefs: []
  type: TYPE_TB
- en: '| I tried logging in and it didn’t work  | `Login_Issue`  | `Login_Issue`  |
    0.8033  |'
  prefs: []
  type: TYPE_TB
- en: '| I’m not able to get into the portal  | `Login_Issue`  | `Login_Issue`  |
    0.6680  |'
  prefs: []
  type: TYPE_TB
- en: '| Password locked out  | `Login_Issue`  | `Login_Issue`  | 0.5520  |'
  prefs: []
  type: TYPE_TB
- en: '| Password reset  | `Login_Issue`  | `Login_Issue`  | 0.4875  |'
  prefs: []
  type: TYPE_TB
- en: '| You never sent a security code  | `Login_Issue`  | `Item_Not_Received`  |
    0.2091  |'
  prefs: []
  type: TYPE_TB
- en: 'If we look at our current trained examples, it’s easy to see why so many questions
    were missed. There were only four examples:'
  prefs: []
  type: TYPE_NORMAL
- en: I’m unable to log in on the website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online account problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem signing onto my account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our training examples lack the variety of meaningful words and phrases seen
    in interactions with real users. Users might refer to their account as their “profile.”
    They list explicit problems such as being “locked out,” needing a “password reset,”
    and failing to receive a “security code.” We should expect to see an improvement
    if we add a few representative examples (obtained from our logs):'
  prefs: []
  type: TYPE_NORMAL
- en: Help signing in to online portal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I need to reset my password
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I need a security code to log on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these additions, we updated our classifier to V2 and reran the blind test
    set. Let’s look at how this affected the recall for `#Login_Issue` in table 5.9.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.9 Blind test result details show improved recall for our newest classifier
    version (V2). Out of nine utterances, the correct (aka *golden*) intent was predicted
    eight times.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Golden intent | Predicted intent | Confidence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BMV portal password reset  | `Login_Issue`  | `Login_Issue`  | 0.8253  |'
  prefs: []
  type: TYPE_TB
- en: '| I can’t get on my profile  | `Login_Issue`  | `Item_Not_Received`  | 0.8131  |'
  prefs: []
  type: TYPE_TB
- en: '| I need help logging into my BMV profile  | `Login_Issue`  | `Login_Issue`  |
    0.6846  |'
  prefs: []
  type: TYPE_TB
- en: '| I never got my security verification code  | `Login_Issue`  | `Login_Issue`  |
    0.7179  |'
  prefs: []
  type: TYPE_TB
- en: '| I tried logging in and it didn’t work  | `Login_Issue`  | `Login_Issue`  |
    0.8899  |'
  prefs: []
  type: TYPE_TB
- en: '| I’m not able to get into the portal  | `Login_Issue`  | `Login_Issue`  |
    0.7840  |'
  prefs: []
  type: TYPE_TB
- en: '| Password locked out  | `Login_Issue`  | `Login_Issue`  | 0.9083  |'
  prefs: []
  type: TYPE_TB
- en: '| Password reset  | `Login_Issue`  | `Login_Issue`  | 0.9204  |'
  prefs: []
  type: TYPE_TB
- en: '| You never sent a security code  | `Login_Issue`  | `Login_Issue`  | 0.2551  |'
  prefs: []
  type: TYPE_TB
- en: Our overall accuracy improved from 76% to 79% (106 out of 134 correct), and
    table 5.10 shows a dramatic improvement in the recall and F1 score. The precision
    score also remained steady.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.10 A comparison of summary metrics; our V2 classifier shows an overall
    improvement compared to the baseline (V1) for `#Login_Issue`.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—Baseline (V1)  | 9  | 5  | 0.4444  | 1  | 0.6153  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—V2  | 9  | 8  | 0.8889  | 1  | 0.9412  |'
  prefs: []
  type: TYPE_TB
- en: 5.2.2 Improve precision for one intent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s experiment with improving the precision for an intent. The `#Chitchat_VA_About`
    intent remained unchanged between the baseline test results and the V2 test results.
    (It is important to look at the newest results after each change.) Table 5.11
    shows that the recall was perfect, but the precision was only 50%. This means
    our classifier is placing a bit more importance on this topic, and it is showing
    up as a false positive (over-selecting) in another intent.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.11 Metrics after the V2 update show that `#Chitchat_VA_About` has perfect
    recall but poor precision.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 1  | 2  | 1  | 0.50  | 0.6667  |'
  prefs: []
  type: TYPE_TB
- en: In table 5.12, we see that there was only one test question in our blind set
    for this intent, but our classifier predicted the intent twice.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.12 V2 blind result details show an over-selection for `#Chitchat_VA_About`.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Golden intent | Predicted intent | Confidence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Do you have a name?  | `Chitchat_VA_About`  | `Chitchat_VA_About`  | 0.8042  |'
  prefs: []
  type: TYPE_TB
- en: '| Where are my tags?  | `Item_Not_Received`  | `Chitchat_VA_About`  | 0.3015  |'
  prefs: []
  type: TYPE_TB
- en: Our training has eight examples. We knew that these examples were manufactured
    (in fact, they were provided by a template), but our logs show that this is not
    a very common topic. Our blind test set only contained one utterance for this
    intent.
  prefs: []
  type: TYPE_NORMAL
- en: 'One strategy for improving precision is to prune the training examples. This
    tells our classifier that the intent isn’t quite as dominant as the other intents
    within our solution. We’ll discard three examples because they are either overly
    redundant or, in the case of “Where are you from,” there was no evidence in the
    logs that this was a relevant question:'
  prefs: []
  type: TYPE_NORMAL
- en: Are you a robot?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can I ask you?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can you do?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can you help me with? (REMOVE FROM TRAINING)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s your name?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where are you from? (REMOVE FROM TRAINING)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who am I talking to? (REMOVE FROM TRAINING)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are you?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the training was updated (now V3), we ran the blind test again and reviewed
    the results. We saw an improvement to the precision for the `#Chitchat_VA_About`
    intent from V2 to V3—it was a perfect score across all metrics. Oddly enough,
    our overall accuracy dropped to 78% (from 79%), and one of the questions we lost
    was from our `#Login_Issue` intent. Table 5.13 shows the changes in metrics from
    V2 to V3 for both intents.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.13 Metrics before and after V3 update for `#Chitchat_VA_About` and `#Login_Issue`
    show that changing one intent can have an effect on another intent.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`—V2  | 1  | 2  | 1  | 0.50  | 0.6667  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`—V3  | 1  | 1  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—V2  | 9  | 8  | 0.8889  | 1  | 0.9412  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`—V3  | 9  | 7  | 0.7777  | 1  | 0.875  |'
  prefs: []
  type: TYPE_TB
- en: Although `#Login_Issue` had a slight decline, the current F1 score of 0.875
    is still far better than the baseline F1 score of 0.6153\. Keep in mind that smaller
    datasets are more sensitive to small changes, and a change to any intent can potentially
    affect every intent. Those changes may have negative or positive results. Instead
    of focusing on this, however, we will make a few more changes elsewhere and check
    back to see if the intent improves.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Improve the F1 score for one intent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s move forward with improving the F1 score for `#Item_Not_Received`. Table
    5.14 shows that it had an F1 score of 56% after our V3 update.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.14 After V3 update, the F1 score remained unchanged at 0.56 for `#Item_Not_Received`.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V2  | 16  | 9  | 0.4375  | 0.7777  | 0.56  |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V3  | 16  | 9  | 0.4375  | 0.7777  | 0.56  |'
  prefs: []
  type: TYPE_TB
- en: The intent had eight training examples, but our logs showed that this is a very
    popular topic, so we need it to perform much better. We’ll add 10 more examples
    from our logs to that intent (now V4) and run another experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.15 shows that our recall for this intent has now more than doubled,
    and though the precision fell slightly, the F1 score is greatly improved. The
    classifier’s overall accuracy also increased from 78% to 81%.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.15 Before and after metrics for `#Item_Not_Received` show an improved
    F1 score.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V3  | 16  | 9  | 0.4375  | 0.7777  | 0.56  |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`—V4  | 16  | 19  | 0.875  | 0.7368  | 0.8  |'
  prefs: []
  type: TYPE_TB
- en: 5.2.4 Improve precision and recall for multiple intents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes there is confusion due to a heavy overlap of terms across intents
    that have similar goals. Figure 5.6 shows the confusion matrix that our testing
    tool provided.
  prefs: []
  type: TYPE_NORMAL
- en: In our model, we see a fair amount of confusion across the intents that relate
    to stolen items. One solution to this problem is to merge intents. This must be
    considered carefully. The intents were probably created separately by design,
    as they all have different answers. However, entity detection can be used to route
    the flow to the appropriate answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F06_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Confusion matrix after the V4 update. The density in shading represents
    the volume of questions predicted for a given intent. If a classifier test had
    a perfect accuracy score, you would see a solid black diagonal line running from
    the upper left corner to the lower right corner. The shaded squares that stray
    away from this diagonal line mark the areas of confusion within your model.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll merge all of these into a single intent called `#Report_Stolen`. These
    examples are listed in table 5.16\. Don’t forget that the blind test set will
    need to reflect this change, as well as the related dialogue flows.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.16 Examples from three intents to be merged into a new `#Report_Stolen`
    intent
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent name | Training example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | Report a stolen car  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | I need to report a stolen car  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | My plates were stolen  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | My registration was stolen  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | License plate stolen off vehicle  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | Stolen real ID  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | Wallet was stolen  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | My drivers license was stolen  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | My ID was stolen  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | My permit was stolen  |'
  prefs: []
  type: TYPE_TB
- en: The conversational flow will be updated so that when a defined entity value
    or synonym is detected in an utterance, the corresponding original answer is provided.
    You may also need a default condition to disambiguate or provide a generic answer
    in case an utterance triggers the new intent but no entity is detected. Table
    5.17 is an example of what that might look like.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.17 Dialogue updates using entity detection for the new `#Report_Stolen`
    intent
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Entity/synonym detected | Treatment |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| vehicle, car, truck, motorcycle  | Routes to original answer for `#Report
    Stolen Vehicle`  |'
  prefs: []
  type: TYPE_TB
- en: '| plates, registration, tags  | Routes to original answer for `#Report_Stolen_Plates_Registration`  |'
  prefs: []
  type: TYPE_TB
- en: '| ID, license, permit  | Routes to original answer for `#Report_Stolen_License_Permit_ID`  |'
  prefs: []
  type: TYPE_TB
- en: '| (none detected)  | Disambiguate (“It sounds like something was stolen; can
    you tell me what it was?”)  |'
  prefs: []
  type: TYPE_TB
- en: With these changes, our classifier is now on V5\. Table 5.18 shows the metrics
    for the three old intents under V4 and the metrics for our new intent in V5.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.18 Metrics before and after V5 update show that merging three intents
    into the single `#Report_Stolen` intent results in perfect scores across the board
    for this topic.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Number of samples | Number of predictions | Recall | Precision |
    F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: —V4
  prefs: []
  type: TYPE_NORMAL
- en: '| 7  | 5  | 0.5714  | 0.8  | 0.6666  |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: —V4
  prefs: []
  type: TYPE_NORMAL
- en: '| 5  | 4  | 0.8  | 1  | 0.8888  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`—V4  | 2  | 2  | 0.5  | 0.5  | 0.5  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen`—new intent in V5  | 14  | 14  | 1  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: Our latest change dramatically improved the performance of this topic, and it
    bumped the overall accuracy to 85%, which is now higher than our baseline *k*-fold
    (which was 84%).
  prefs: []
  type: TYPE_NORMAL
- en: With that update complete, we can move on to other intents that need improvement.
    Following the iterative processes, we updated the remaining intents that showed
    the poorest performance by adding a few more examples from the logs. This became
    V6 of our classifier. Table 5.19 is an overview of the intents that were updated.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.19 Training example counts increase from V5 to V6 and more closely align
    with the volume present in the representative blind test set.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | V5 training example count | V6 training example count | Test utterances
    in representative blind |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 5  | 6  | 7  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 4  | 6  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 7  | 8  | 9  |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 6  | 8  | 8  |'
  prefs: []
  type: TYPE_TB
- en: This update resulted in an overall accuracy of 92% for the latest classifier
    (now on V6). In the world of natural language classification, this is a very good
    score for a representative blind test set. You will never achieve 100%; even human-to-human
    communications don’t come close to that.
  prefs: []
  type: TYPE_NORMAL
- en: Every data set is different, and we could spend several more cycles tweaking
    our training if there is plenty of data available. However, there are diminishing
    returns associated with pursuing results that approach 100%. There is also a risk
    of over-fitting your model to the current blind test set. Once additional logs
    become available and a new test set is created, you may discover additional gaps
    (or your overfitting will be exposed).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.20 shows a comparison of the blind test F1 scores of the baseline classifier
    against our latest updates. Twelve of the intents did not change (and they were
    already performing very well). One intent decreased from 90% to 80%, and the remaining
    14 intents showed improvement. We felt that this was a good and reasonable tradeoff,
    improving more than half of our intents at the cost of one intent showing a slight
    decline.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.20 Comparison of baseline F1 scores and V6 F1 scores
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Baseline (V1) F1 score | V6 F1 score | Change |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 0.8333  | 0.833  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 0.6667  | 0.9524  | + 0.2857  |'
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 0.90  | 0.80  | - 0.1  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 0.80  | 0.80  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 0.80  | 0.80  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 0.75  | 0.8571  | + 0.1071  |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 0.56  | 0.8750  | + 0.315  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 0.60  | 0.75  | + 0.15  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 0.6667  | 1  | + 0.3333  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 0.6153  | 0.9412  | + 0.3259  |'
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 0.90  | 1  | + 0.1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 0.8571  | 0.8571  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 0.9231  | 1  | + 0.0769  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | 0.80  | (n/a - merged)  | + 0.2  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | 0.8889  | (n/a - merged)  | + 0.1111  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 0.50  | (n/a - merged)  | + 0.5  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen`  | n/a  | 1  | (n/a – merged)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 1  | 1  | (no change)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 0.8889  | 1  | + 0.1111  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 0.40  | 0.8  | + 0.4  |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 0.4615  | 0.75  | + 0.2885  |'
  prefs: []
  type: TYPE_TB
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Using the output from the previous exercise (a prioritized list of your poorest-performing
    intents), identify the category of error each intent is committing: recall, precision,
    or both.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make iterative training adjustments to improve each intent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure each change to verify that
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The intended effect is achieved
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No other intents were negatively affected
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.3 Solving “no intent matched”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our classifier in good shape for the current scope, we can
    focus on expanding the domain, if needed. During an initial review of your production
    logs, you will almost surely encounter topics that were not included in the initial
    training set. Some of these topics will be obvious, but perhaps there wasn’t enough
    data to train an intent at the time of the initial launch. Maybe the business
    wasn’t ready to write answers for some topics. Sometimes a seasonal topic is not
    included because it was not in the forefront of anyone’s mind (e.g., tax season,
    hurricane season, fiscal year end, etc.). Other topics may be completely unexpected
    (e.g., a data breach).
  prefs: []
  type: TYPE_NORMAL
- en: Although you don’t have any intents defined to match these utterances, the classifier
    will always attempt to make a prediction; it doesn’t know what it doesn’t know,
    so it does its best to match an utterance to what it does know. In an ideal world,
    the classifier would return very low confidence, and this would trigger an “anything_else”
    or “no action matches” type of response. In reality, such user utterances often
    contain words that appear somewhere in your training, so it is possible that the
    classifier will predict an intent that has training examples with similar words.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Clustering utterances for new intents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the guidelines described in chapter 4, we recommended setting aside utterances
    that were related to the domain but not included in the original scope. It’s time
    to address these.
  prefs: []
  type: TYPE_NORMAL
- en: One of the topics our logs revealed was related to users wanting to cancel their
    license or registration. We know from our logs how the classifier predicted each
    utterance at the time the utterance was asked. Now we can test them against our
    latest classifier (V6) to get new model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In table 5.21, we see that our classifier exhibited low confidence and/or was
    incorrect whenever an utterance contained a form of the word “cancel.”
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.21 Unmatched utterances from logs with predictions from the V6 classifier
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Predicted intent | Confidence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel a registration  | `Appointment`  | 0.2681  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel my car registration  | `License_or_ID`  | 0.3651  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel a drivers license  | `License_Reinstatement`  | 0.3042  |'
  prefs: []
  type: TYPE_TB
- en: '| Canceling a registration  | `Appointment`  | 0.2417  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancellation of registration  | `Fee_Info`  | 0.2786  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancelling my registration  | `Item_Not_Received`  | 0.3004  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel a replacement license  | `Vehicle_Permit`  | 0.3264  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel the license  | `License_Reinstatement`  | 0.3237  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel a title or registration  | `Vehicle_Title`  | 0.5913  |'
  prefs: []
  type: TYPE_TB
- en: '| Cancel vehicle registrations  | `Item_Not_Received`  | 0.2914  |'
  prefs: []
  type: TYPE_TB
- en: '| Commercial drivers license cancel  | `License_Reinstatement`  | 0.2995  |'
  prefs: []
  type: TYPE_TB
- en: '| Driver’s license cancellation  | `Get_ID_Number`  | 0.3387  |'
  prefs: []
  type: TYPE_TB
- en: '| How do I cancel my vehicle registration?  | `License_or_ID`  | 0.4324  |'
  prefs: []
  type: TYPE_TB
- en: '| I need to cancel a vehicle registration  | `License_or_ID`  | 0.3481  |'
  prefs: []
  type: TYPE_TB
- en: '| I need to cancel my ID  | `Get_ID_Number`  | 0.3205  |'
  prefs: []
  type: TYPE_TB
- en: '| I would like to cancel the registration on my car  | `Change_Contact_Records`  |
    0.3147  |'
  prefs: []
  type: TYPE_TB
- en: '| I would like to cancel my car registration  | `License_or_ID`  | 0.3447  |'
  prefs: []
  type: TYPE_TB
- en: '| I would like to cancel my state identification card  | `Change_Contact_Records`  |
    0.2982  |'
  prefs: []
  type: TYPE_TB
- en: '| I wanted to cancel a registration  | `Item_Not_Received`  | 0.3155  |'
  prefs: []
  type: TYPE_TB
- en: '| I want to confirm cancellation of my registration  | `Item_Not_Received`  |
    0.4092  |'
  prefs: []
  type: TYPE_TB
- en: '| Questions about cancelling registration for a vehicle  | `Fee_Info`  | 0.2795  |'
  prefs: []
  type: TYPE_TB
- en: '| I want to cancel my registration on my pickup  | `Item_Not_Received`  | 0.4761  |'
  prefs: []
  type: TYPE_TB
- en: We’ll randomly divide these into a training set of nine utterances under a new
    `#Cancel_Registration_or_License` intent and add the remaining thirteen to our
    blind test set.
  prefs: []
  type: TYPE_NORMAL
- en: When we run the updated blind test set against our updated classifier (now V7),
    we get an overall accuracy of 92%, which is usually a very good, if not ideal,
    outcome. This will not always be the case, so if your overall performance drastically
    drops, you will need to iterate through the applicable improvement steps (depending
    on whether the problem was recall, precision, or both) for the intents that were
    affected.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through one more example of adding a new intent. The logs contained
    several utterances referring to a data breach. This is an example of how a chatbot
    can exhibit declining performance due to new information in the world. In this
    case, the organization had never experienced a data breach before. But when it
    did, and this news became public, users suddenly had a lot of questions about
    it. This manifested as unmatched and incorrect predictions, as seen in table 5.22.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.22 Unmatched utterances on the topic of “data breach” from logs, with
    predictions from the V7 classifier. The classifier didn’t have enough confidence
    to match most of the utterances referring to “hack” or “data breach,” which is
    good because we hadn’t yet taught it anything about that topic. But most of the
    utterances that contain the word “stolen” match strongly against our `#Report_Stolen`
    intent. This may not go so well for the user because our solution doesn’t have
    any answers yet concerning data that was stolen.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Predicted intent | Confidence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| I want to know about that hacking on the BMV  | <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| I want to know about the breach in information at the BMV and if I’m at risk  |
    <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| My identity has been stolen  | `Report_Stolen`  | 0.9483  |'
  prefs: []
  type: TYPE_TB
- en: '| My license number was stolen  | `Report_Stolen`  | 0.9240  |'
  prefs: []
  type: TYPE_TB
- en: '| Need questions answered about data breach  | <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| No I’m curious about the current breach of stolen IDs  | `Report_Stolen`  |
    0.8604  |'
  prefs: []
  type: TYPE_TB
- en: '| Someone hacked my information  | `Report_Stolen`  | 0.4662  |'
  prefs: []
  type: TYPE_TB
- en: '| Someone is using my drivers license number  | `Get_ID_Number`  | 0.4067  |'
  prefs: []
  type: TYPE_TB
- en: '| Someone stole my identity  | `Report_Stolen`  | 0.7705  |'
  prefs: []
  type: TYPE_TB
- en: '| Someone stole my information  | `Report_Stolen`  | 0.8043  |'
  prefs: []
  type: TYPE_TB
- en: '| Stolen personal identity  | `Report_Stolen`  | 0.9263  |'
  prefs: []
  type: TYPE_TB
- en: '| Stolen personal information  | `Report_Stolen`  | 0.9166  |'
  prefs: []
  type: TYPE_TB
- en: '| Stolen social security number  | `Report_Stolen`  | 0.7515  |'
  prefs: []
  type: TYPE_TB
- en: '| Was my account affected by the recent data hack?  | <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| Was my account hacked?  | `Login_Issue`  | 0.3998  |'
  prefs: []
  type: TYPE_TB
- en: '| Was there a data breach?  | <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| Yeah I’d like to know if my driver’s license has been breached  | `Report_Stolen`  |
    0.4092  |'
  prefs: []
  type: TYPE_TB
- en: '| Yes what do I do about the data breach at the BMV?  | <none>  | n/a  |'
  prefs: []
  type: TYPE_TB
- en: '| Was my social security number stolen in the hack?  | `Report_Stolen`  | 0.7806  |'
  prefs: []
  type: TYPE_TB
- en: '| I want to know if my information was stolen  | `Report_Stolen`  | 0.9198  |'
  prefs: []
  type: TYPE_TB
- en: To resolve the problem of this unmatched intent, we selected seven representative
    utterances from the logs to create a new intent called `#Data_Breach`. Our selection
    ensured that a variety of important terms, such as “hack,” “breach,” and “stolen,”
    were added to our new training set. The remaining utterances were added to our
    blind test set, and we tested our newest classifier, V8\. The new `#Data_Breach`
    intent returned a perfect score, and the F1 score comparisons in table 5.23 show
    that nearly all others remained steady or improved since our baseline reading.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.23 Final score comparison between the baseline (V1) and the final version
    (V8)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Intent | Baseline (V1) F1 score | V8 F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Accident_Report`  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Appointment`  | 0.8333  | 0.8333  |'
  prefs: []
  type: TYPE_TB
- en: '| `Cancel_Registration_or_License`  | n/a (NEW)  | 0.9630  |'
  prefs: []
  type: TYPE_TB
- en: '| `Change_Contact_Records`  | 1  | 0.8889  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Goodbye`  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Hello`  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_Thanks`  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Chitchat_VA_About`  | 0.6667  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Data_Breach`  | n/a (NEW)  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Fee_Info`  | 0.90  | 0.9524  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Negative_Feedback`  | 0.80  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `General_Request_Agent`  | 0.80  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `Get_ID_Number`  | 0.75  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Item_Not_Received`  | 0.56  | 0.8750  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_Reinstatement`  | 0.60  | 0.75  |'
  prefs: []
  type: TYPE_TB
- en: '| `License_or_ID`  | 0.6667  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Login_Issue`  | 0.6153  | 0.9412  |'
  prefs: []
  type: TYPE_TB
- en: '| `Name_Change`  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Office_Information`  | 0.90  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Payment_Methods`  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Refund_Overcharge`  | 0.8571  | 0.80  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Sold_Vehicle`  | 0.9231  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_License_Permit_ID`  | 0.80  | (n/a - merged)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Plates_Registration`  | 0.8889  | (n/a - merged)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen_Vehicle`  | 0.50  | (n/a - merged)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Report_Stolen`  | n/a  | 0.9630  |'
  prefs: []
  type: TYPE_TB
- en: '| `Request_Receipt`  | 1  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Permit`  | 0.8889  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| `Vehicle_Title`  | 0.40  | 0.6667  |'
  prefs: []
  type: TYPE_TB
- en: '| `Walk_In`  | 0.4615  | 0.75  |'
  prefs: []
  type: TYPE_TB
- en: Our overall accuracy score remained steady at 92%. (Our updated blind test set
    has 160 questions, and 147 were correct.) You might recall that our very first
    blind test had an overall accuracy of 76%, so this is quite an improvement. Our
    V8 confusion matrix, shown in figure 5.7, also looks improved, with a fairly dark
    diagonal line.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F07_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 Comparison of baseline (V1) confusion matrix to the V8 update
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We could iterate further to try to get a little higher, but for this use case,
    the classifier’s accuracy is more than good enough for the time being. Any further
    tweaks with the limited data we have at present are likely to over-fit our model
    to the current blind test set. Remember that a healthy strategy is to plan to
    iterate over the life of the bot, using newer logs and refreshed blind test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 When to stop adding intents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When reviewing your logs, you may have encountered a diverse range of other
    questions that are perfectly reasonable for the domain, but very infrequent. In
    our logs, we saw questions like the following, but no additional utterances with
    similar goals:'
  prefs: []
  type: TYPE_NORMAL
- en: I need a form for a doctor to fill out saying a driver is not safe to drive
    anymore.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have a question about electronic signatures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the process for getting a specialty license plate?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we know when to stop adding intents? It’s best to let the data from our
    human-annotated logs guide us. We can total up all of the examples by intent and
    render them as a chart, as in figure 5.8\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F08_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 Example of a longtail chart. The terms we use to describe the volume
    distribution of our available training data are “short head” and “long tail.”
    These terms describe the visual representation of rendering our data on a bar
    chart. The heavier-volume intents are on the left (the short head), and as the
    volume decreases for each intent, the data has the appearance of a long tail falling
    off to the right.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In our longtail chart, we picked a point to divide between what should be in
    scope versus out of scope. This point isn’t a static, prescriptive position. It’s
    a decision that should be made with the business by establishing a minimum number
    of training examples required to create a new intent. Everything that falls on
    the left of this line should probably be included in the training, as there is
    evidence that these topics will be asked more frequently. Everything to the right
    will not be trained in the current classifier. Over time, you may find enough
    data in the logs to justify adding a new intent. Until then, your solution will
    have to handle such topics with one of the following strategies: give a response
    saying the bot doesn’t understand, fall back to an agent escalation, add a search
    integration to find answers in a document repository, or implement a retrieval-augmented
    generation (RAG) or large language model (LLM) component to generate answers.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Identify new topics based on your logs, and build new intents from the utterances
    found in the logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add utterances to your blind set, and test your changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is your classifier able to recognize the new intent without negatively affecting
    the performance of your existing intents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.4 Supplementing traditional AI with generative content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conversational AI, we typically think of delivering either a static answer
    (as in classic intent-driven implementations) or an answer that is entirely generated
    (as in a RAG pattern). Static answers fill a need where an answer must maintain
    consistency, either in content or in structure. Although personalization is possible,
    it is generally limited to defined entities or other context-driven dialogue conditions.
    This tends to result in colder, less personalized bot responses. Figure 5.9 shows
    how three users with the same general goal, but very different personal situations,
    all receive the same bot response.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F09_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 In a traditional (classification-based) dialogue pattern, an intent
    is identified, and the dialogue is configured to give a static or minimally personalized
    answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.4.1 Combining traditional and generative AI for an intent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can enhance the user experience using a hybrid response pattern, which combines
    personalized generated content with the static predefined answers written for
    our intent. Our goal is to acknowledge the user’s problem while ensuring that
    important information is delivered with consistency. Many large language models
    excel at summarization tasks, so a model can be prompted to craft an empathetic
    message that conveys a personalized level of understanding. Figure 5.10 shows
    what this looks like from the user’s perspective.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F10_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 An output response identifies the correct intent using traditional
    AI and then prepends generated text to the static output response configured for
    the intent. The generated greeting and summary convey to the user that the bot
    understands their goal and the particular details of the user’s situation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This pattern employs an API call to the LLM as a dialogue step. Content is generated
    by the LLM and delivered just before the predefined output response. Figure 5.11
    shows the high-level steps for such a pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F11_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 LLMs can be called within traditional dialogue patterns to greet
    a user and summarize their problem before delivering a predefined or static answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.4.2 Prompting to convey understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In conversational AI, your bot’s role is typically to be a representative of
    your company. They are a “digital” resource, as opposed to a “human” resource.
    Still, their job is to be the face of the company. Human agents are great at conveying
    empathy and understanding. In fact, they will often restate the user’s problem
    to demonstrate that they understand. LLMs can be prompted to simulate this summarization
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Since our traditional AI has already classified the user’s intent under this
    pattern, we can craft a prompt that instructs the LLM to perform a specific task.
    In this case, we want the LLM to generate a personalized, empathetic greeting
    that can be paired with additional static content. The next listing shows a prompt
    instruction for summarizing a user’s input.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 Prompting a model to greet and summarize a user problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Collect a set of user utterances to test and tune an LLM prompt that can greet
    a user where appropriate and summarize their problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with a variety of instruction prompts. The goal is to create an efficient
    prompt instruction that will produce good results for the majority of your utterance
    test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classifier’s performance can be measured in terms of accuracy, recall, precision,
    and F1 score. These measurements reflect the types of errors a classifier may
    be committing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance metrics produced by your testing will inform your next steps
    toward improving classifier performance. Higher volume intents with low performance
    are a good place to start.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterative test and train cycles will show you the effects of your changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A chatbot can use additional strategies, such as disambiguation, clarifying
    questions, and entity detection to overcome confusion or route answers for merged
    intents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A chatbot with a strong classifier can deliver more business value by delivering
    the right answers on the first try and deflecting work that would otherwise be
    handled by a human agent. You should plan to monitor and retrain your solution
    throughout the life of the bot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI can supplement a traditional AI solution by infusing static chatbot
    responses with personalization and empathy, which enhances the perception of understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
