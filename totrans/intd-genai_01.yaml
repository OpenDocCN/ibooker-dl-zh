- en: 2 Training large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Explaining how LLMs are trained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the emergent properties of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the harms and vulnerabilities that come from training LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For decades, the digital economy has run on the currency of data. The digital
    economy of collecting and trading information about who we are and what we do
    online is worth trillions of dollars, and as more of our daily activities have
    moved on to the internet, the mill has ever more grist to grind through. Large
    language models (LLMs) are inventions of the internet age, emulating human language
    by vacuuming up terabytes of text data found online.
  prefs: []
  type: TYPE_NORMAL
- en: The process has yielded both predictable and unpredictable results. Notably,
    there are significant questions about both what is in the datasets used by LLMs
    and how to prevent the models from replicating some of the more objectionable
    text they hold in their training sets. With data collection at this scale, the
    collection of personal information and low-quality, spammy, or offensive content
    is expected, but how to address the problem is another challenge. LLMs at the
    scale we’re now seeing have exhibited a host of capabilities that don’t seem to
    be available to smaller language models. These properties make LLMs more attractive
    for a variety of uses and ensure that the race toward more and more data and bigger
    and bigger models won’t end anytime soon.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn more about how LLMs are trained to understand
    what makes them unique compared to previous models and how these characteristics
    result in both new capabilities and potential vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: How are LLMs trained?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 1, we introduced some of the concepts involved in training LLMs.
    We covered transformer architecture, a specific type of neural network used in
    LLMs, and talked about some of the sources of data that LLMs use. We also explained
    the self-supervised task they are trained to complete—generate the next most probable
    word or character, also known as token prediction. Here we’ll examine the training
    process in greater detail and discuss perhaps the most surprising and exciting
    aspect of LLMs—their emergent properties, that is, things they weren’t trained
    to do, but do well anyway.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of creating an LLM, often called the *pre-training* step, is
    training on some token prediction task (for a generative model, autoregression
    or causal token prediction) with a gigantic corpus of data. It is called pre-training
    because even though this is a training phase, the knowledge encoded by the model
    during this phase is foundational to any subsequent natural language task. Then,
    the model is fine-tuned on one or many additional tasks, that is, trained with
    labeled data and a specific objective. Dialogue agents such as ChatGPT might be
    fine-tuned on conversational data; many generative models are fine-tuned on instruction
    datasets to improve their capability to follow instructions (e.g., “Write me a
    poem”); others might be fine-tuned for code generation. This process is pictured
    in figure 2.1, but it’s worthwhile to take a deeper look at each of these stages.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F01_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 The high-level training process for LLMs
  prefs: []
  type: TYPE_NORMAL
- en: Exploring open web data collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To model natural language and then generate language convincingly, LLMs need
    lots and lots of examples. Let’s consider all the implicit knowledge that goes
    into question-answering tasks. First, the model must have an accurate representation
    of both the question and the context (what the question is being asked about),
    which in turn means having a representation for each of the tokens in the question
    and context—analogous to knowing what the words themselves mean. The model must
    also be able to parse the question syntactically to identify what is being asked
    and then produce an answer, either from the context (the open-book case) or from
    its internal representation of external concepts (the closed-book case). Because
    LLMs have seen so much text from the internet, most would be able to answer a
    question like, “Who was the first president of the United States?” correctly without
    any provided context. More obscure information might result in an incorrect or
    made-up answer because the model wouldn’t have a high-probability response. Notably,
    if we ask ChatGPT, “Who was the first president?” without specifying that we are
    asking about the United States, ChatGPT responds, “The first president of the
    United States was George Washington.”
  prefs: []
  type: TYPE_NORMAL
- en: LLMs use data from the open web, which refers to all public web pages on the
    internet, including sites such as Wikipedia and Reddit, but also possibly non-password-protected
    blogs, news aggregators, and non-private forums. Why does ChatGPT assume we’re
    asking about the United States? To be fair, the answer might be different if the
    request came from an IP address in another country, but the assumption also belies
    an indisputable fact about internet data—most of it is in English, and a disproportionate
    amount of it is from the United States and Western Europe. In chapter 1, we mentioned
    that Wikipedia is one of the classic data sources for LLMs. While the encyclopedia’s
    global geographic coverage continues to improve, there are more than 6.6 million
    articles in the English Wikipedia, whereas the next-highest total is 2.5 million
    articles in the French Wikipedia. The downstream effects of this are that the
    LLMs are better at understanding, generating, and completing tasks in English.
    They also better understand topics relevant to North America and Western Europe,
    and therefore serve these audiences better.
  prefs: []
  type: TYPE_NORMAL
- en: To get a sense of other types of text datasets in use, we can look at open data
    repositories, such as that of the open source AI company Hugging Face (see [https://huggingface.co/datasets](https://huggingface.co/datasets)).
    Open data is available for anyone to download and use for their projects, although
    sometimes the type of permissible use is restricted by the data’s license; for
    example, a dataset provider might specify that the dataset should be used for
    academic or research purposes only, not in commercial applications. One dataset
    for language models consists of millions of Reddit posts (with non-English posts
    filtered out). Others include collections of news articles, reviews from sites
    such as Amazon and Rotten Tomatoes (review-aggregation website for movies and
    TV shows), or questions and answers from the community Q&A site Stack Exchange.
    Common Crawl is a nonprofit that maintains a massive repository of web page data
    and provides it for public use (see [https://commoncrawl.org/](https://commoncrawl.org/)).
    In short, anywhere that people are writing online is a potential data source.
  prefs: []
  type: TYPE_NORMAL
- en: Companies that develop LLMs might use a combination of open datasets such as
    those on Hugging Face, datasets that they purchase from third-party vendors, datasets
    that they collect themselves by scraping the web, or datasets that they create
    themselves by writing examples for the models to learn from. Although the initial
    training of the LLM might not require any manual intervention, as we’ll see, crowdsourcing
    and conversational collection are important in improving the model’s performance
    in specific domains, such as dialogue for chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying autoregression and bidirectional token prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some of the first LLMs, such as Google’s BERT, were focused much more heavily
    on natural language understanding, as compared to generative use cases such as
    chatbots. Because of this objective, BERT is known as a bidirectional model, meaning
    BERT was trained to predict the missing word (token) within a sentence and has
    access to both the left and right contexts (the bidirectional part). This is ideal
    for natural language understanding because the model picks up more information
    about the contexts a particular word is used in. However, if a model is used for
    text generation, it shouldn’t be trained on anything that comes after the missing
    token because it would only ever have access to the text that preceded it. This
    type of model is called *autoregressive*, because future predictions are dependent
    on the model’s past data. All the models in the GPT family, as well as Google’s
    Pathways Language Model (PaLM), are autoregressive.
  prefs: []
  type: TYPE_NORMAL
- en: Autoregressive means that future predictions are dependent on the model’s past
    data.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider the sentence, “For their honeymoon, they flew to _____
    and had a romantic dinner in front of the Eiffel Tower.” The correct word for
    a model to predict here is “Paris”. In this case, the right context (what happens
    after the missing word) is especially informative, and a bidirectional model would
    very likely answer correctly. But when a model is asked to generate text, such
    as, “A good location for a romantic honeymoon is ______,” the task is structured
    such that the model’s completion is at the end of the context. Therefore, the
    model’s training should only use the left context (what comes before the missing
    word) to predict the missing tokens. The model learns by self-supervision, repeatedly
    guessing the final token in billions of examples from the text and adjusting its
    weights based on the correct token, until the model’s performance on guessing
    missing tokens in the training data is optimal. When we chat with ChatGPT, it
    doesn’t appear to look like a formal task to the user, but under the hood, the
    model is predicting what should come next after each message. When I type, “Hey!
    What’s up?” the logical and likeliest completions are to answer the question and
    return the greeting.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once trained on the token completion task, a model can generate words, phrases,
    or complete sentences. At this stage the models are called foundation or base
    models because they provide the foundational knowledge, due to their complex representations
    of thousands of different words and concepts, for performing natural language
    processing (NLP) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Although these base models aren’t that impressive out of the box, they can be
    easily adapted to do well in specific tasks through *fine-tuning*, that is collecting
    labeled datasets that demonstrate the specific task or tasks the model needs to
    improve on. These tasks might be very narrow, such as a classification problem
    requiring specific domain expertise, or quite broad. Many commercial LLMs are
    fine-tuned on instruction-following data so that the models can better respond
    to inputs such as “Write a song,” or “Tell me a joke.” Other fine-tuning tasks
    are also common uses for LLMs, such as summarization and question answering. From
    a technical perspective, fine-tuning trains a neural network in a supervised fashion,
    but instead of starting from scratch, the neural network is initialized with the
    weights of the foundation model. Whereas training the foundation model takes weeks
    and uses large amounts of computing resources, fine-tuning can be done in minutes.
    The fine-tuned model uses the representations of the original but then adjusts
    its own weights and parameters to best fit the new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unexpected: Emergent properties of LLMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some respects, LLMs are natural extensions of predecessor neural network
    models. Before the transformer architecture made it efficient to build larger
    and larger models, it was well-known that model size correlates with model performance
    on a range of common NLP tasks, and, in many cases, such performance improvements
    could be predicted based on empirically derived scaling laws. However, LLMs have
    also yielded behaviors, called emergent properties, that no one could have predicted
    via a scaling law. In a 2022 survey on the emergent abilities of LLMs, emergence
    is defined as “when quantitative changes in a system result in qualitative changes
    in behavior” [[1]](http://arxiv.org/abs/2206.07682). In other words, we might
    expect that for a particular task, a model with 100 billion parameters would achieve
    10% higher accuracy than a model with 100 million parameters. But the model with
    100 billion parameters—an LLM—can now do tasks that the smaller model can’t and
    in somewhat unpredictable and unexpected ways.
  prefs: []
  type: TYPE_NORMAL
- en: Emergent properties are abilities that LLMs begin to exhibit at very large model
    sizes with behaviors that are qualitatively different from those of smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick study: Learning with few examples'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When talking about the emergent capabilities of LLMs, it’s useful to compare
    them to the capabilities derived from the process described in the previous section.
    In the standard case, the model is pre-trained and fine-tuned for one or many
    natural language abilities, such as translation or analogy completion. These abilities
    are part of the training pipeline and are considered predictable—not in exactly
    how the model will perform but in how the model improves as it’s trained.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the primary examples of emergent abilities are zero-shot
    and few-shot learning. The terms *zero-shot* and *few-shot* refer to the number
    of examples that the model is given before being asked to perform a task. For
    instance, let’s say that a restaurateur wants to add visual indicators for vegetarian
    dishes on their restaurant’s menu. Using ChatGPT, they might write something like,
    “Please rewrite this menu and put an asterisk next to all dishes that do not contain
    any meat,” and then copy and paste the menu. This might seem like a trivial task
    for a human, but the model must first interpret the request, then classify each
    written menu item according to whether or not it contains meat, and finally produce
    the output in the corresponding format. The level of natural language understanding
    and generative ability required to complete such a task with no previous examples
    (we can safely assume that the model was never trained explicitly to do this)
    isn’t observed in previous language models, and yet LLMs can produce impressive
    results on many such zero-shot tasks, where the model has never seen the task
    before.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot or few-shot refers to the number of examples that the model is given
    before being asked to perform a task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the few-shot case, the model is given a few examples of the task in the
    *prompt*, the text that the model takes as input to determine what output it should
    generate. In the previous zero-shot example, the user’s request constituted part
    or all of the model’s prompt (models are sometimes deployed with a base prompt,
    which might provide generic instructions on how to respond to inputs but isn’t
    relevant to this discussion). Another user might want the model to perform a slightly
    more complex task. Let’s say a freelance writer is working on three different
    pieces—one about dog breeding, one about exoplanets, and one about Pittsburgh—and
    wants to organize a list of articles by topic. In this case, they might write
    something like:'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the following articles is related to one of “dog breeding,” “exoplanets,”
    or “Pittsburgh”. For each article, write the most likely related topic from those
    three topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'This could be structured as a zero-shot task as well. However, it’s generally
    beneficial to model performance to provide a few examples, so if the response
    wasn’t exactly what the writer wanted, they might try to provide additional guidance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: “The latest discovery of space telescopes”: Exoplanets; Example: “Why
    pugs have breathing problems”: Dog breeding; and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2 shows how zero-shot and few-shot prompts differ from fine-tuning
    a model for a task. If you’ve used an LLM to perform one of these tasks, you might
    have tried zero-shot and few-shot learning without even thinking about it or realizing
    it. This is one of the great strengths of LLMs: because the interface with these
    chatbots is simply natural language, we can often tweak the inputs to achieve
    the desired outputs in a much more intuitive way than we might with other models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F02_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 A comparison of fine-tuning, zero-shot learning, and one-shot learning
    on a machine translation task
  prefs: []
  type: TYPE_NORMAL
- en: In addition to zero-shot and few-shot examples in the model’s prompts, other
    changes to the model’s prompt have uncovered additional emergent abilities. A
    technique called chain-of-thought prompting, or directing the model to break apart
    challenging problems into multiple steps, has been shown to improve model performance
    (in its simplest version, prefacing a prompt with “Let’s think step-by-step” has
    been shown to make the model generations more accurate in reasoning problems).
    People have also tested detailed instructions on zero-shot tasks, as well as asking
    the model about its level of confidence in its own response, each of which can
    improve responses in certain settings.
  prefs: []
  type: TYPE_NORMAL
- en: In the previously mentioned study exploring the emergent abilities of LLMs,
    the authors examined the performance of LLMs of various sizes on few-shot tasks.
    In particular, the researchers looked for tasks where the performance of “small”
    LLMs was random, but then jumped sharply at the larger sizes. They found that
    language models’ ability to do addition, subtraction, and multiplication was emergent,
    with GPT-3 getting answers correct in almost no cases until the 13-billion-parameter
    model size; similarly, GPT-3 and other models were found to significantly improve
    their ability to answer questions about miscellaneous academic topics, including
    math, history, and law, after reaching about 70 billion or more parameters. Because
    these emergent abilities don’t follow the scaling law, it’s difficult to say with
    certainty whether larger sizes would promote even greater capabilities, at what
    size improvement would stop, or even how to reason about these tasks as compared
    to those where accuracy maps predictably to model size.
  prefs: []
  type: TYPE_NORMAL
- en: Sparks of artificial general intelligence?
  prefs: []
  type: TYPE_NORMAL
- en: According to an evaluation by a team at Microsoft, “beyond its mastery of language,
    GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision,
    medicine, law, psychology and more, without needing any special prompting” [[2]](http://arxiv.org/abs/2304.15004).
    These emergent abilities led them to provocatively title the paper, “Sparks of
    Artificial General Intelligence,” and write that “Given the depth and breadth
    of GPT-4’s capabilities, we believe it could be reasonably viewed as an early
    (yet still incomplete) version of an artificial general intelligence (AGI) system.”
    AGI has been the long-sought goal of many scientists in AI, and it’s understood
    to be intelligence that can learn as well as humans who have historically been
    much better at generalizing knowledge and adapting to unseen problems. The question
    of AGI, and whether any LLMs possess it, is outside the scope of this chapter,
    but we’ll discuss it and related questions in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: Is emergence an illusion?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although several studies have documented evidence of emergent abilities, there
    isn’t yet a consensus about emergence within the machine learning community. A
    team of computer scientists at Stanford University argued that these so-called
    emergent abilities appear less because of some qualitative change in model behavior
    at certain scales and more because of the way that researchers are evaluating
    the models [[2]](http://arxiv.org/abs/2304.15004). In particular, the sharp increases
    in performance that characterize emergence in some tasks seem to be at least partially
    attributable to the choice of metric on the task, the amount of test data used
    for evaluation (because testing on less data will give a noisier estimate of model
    performance), and the number of large-scale models in the evaluation (because
    there are fewer large-scale models available than small-scale models). In other
    words, the authors don’t dispute the actual performance of the LLMs on any of
    these tasks, just the idea that the LLMs, in cases where emergent abilities were
    claimed, represented a fundamental change from previous versions. The emergence
    behavior depends on the performance metric selected, and while it’s not clear
    whether one metric is better than another, caution is warranted before we assume
    that *other* capabilities might readily emerge with more or different data and
    bigger models.
  prefs: []
  type: TYPE_NORMAL
- en: What’s in the training data?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve previously discussed, LLMs are trained on *massive* amounts of noncurated
    data from the web. Just how much information have these LLMs been fed? Quite a
    lot. The general-purpose LLM, GPT-3, was trained on 45 terabytes (TB) of text
    data [[3]](https://arxiv.org/pdf/2005.14165.pdf), where 1 TB is generally estimated
    to contain 75 million pages [[4]](https://cloudnine.com/ediscoverydaily/electronic-discovery/ediscovery-best-practices-perspective-on-the-amount-of-data-contained-in-1-gigabyte/).
    When working with unfathomable amounts of noncurated and undocumented training
    data, no one is quite sure what the data includes, resulting in LLMs encoding
    and amplifying stereotypical and derogatory associations, as well as sometimes
    containing sensitive data, such as personally identifiable information (PII).
    In this section, we’ll talk more about the challenges that come with training
    language models on immeasurable amounts of text data.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perpetuating harmful stereotypes and discriminatory language along the lines
    of gender, sexual orientation, race, ethnicity, religion, age, and disability
    status is a well-documented form of harm in LLMs [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922).
    Internet-based datasets encode bias and harmful stereotypes for different reasons.
    The first is that these associations are largely a reflection of the characteristics
    found in the training data. Here, as the LLM learns the characteristics and patterns
    of a language in order to generate human-like text, it also inherits human-like
    prejudices, historical injustice, and cultural associations that can be harmful
    and offensive. The second is the lack of diversity in training data. The dataset
    can be biased because some communities may be better represented than others,
    and the dataset may not be broadly representative of how different groups of people
    view the world. The third is that developing and changing social views can result
    in LLMs misrepresenting social movements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 1, we briefly discussed how word embeddings mirror the inequities
    that exist in society. In an early study of bias in word embeddings, the authors
    considered NLP applications that use word embeddings to determine this potential
    effect [[6]](https://doi.org/10.1126/science.aal4230). First, they looked at sentiment
    analysis, which classifies text as positive, negative, or neutral. The task was
    calculating a sentiment score for movie reviews, which can be helpful for marketing
    purposes. Their results showed that movie reviews containing European American
    names had more positive sentiment scores on average in comparison to those with
    African American names, even when the reviews were otherwise similar; that is,
    the sentiment scores exhibited racial bias for character and actor names in the
    movie reviews. Next, they looked at machine translation where they concluded that
    translations from many gender-neutral languages to English result in gender-stereotyped
    sentences. In their paper, they show how Google Translate converts Turkish sentences
    with genderless pronouns to English: *“O bir doktor. O bir hemşire.”* to “He is
    a doctor. She is a nurse.”'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, LLMs not only reinforce stereotypes but also amplify them. In a study
    exploring religious bias in language models, authors determined that OpenAI’s
    GPT-3 captures Muslim-violence bias, as well as anti-Semitic bias [[7]](https://arxiv.org/pdf/2101.05783.pdf).
    They show that prompts including the word “Muslim” yield text that maps to “terrorist”
    23% of the time, while “Jewish” maps to “money” 5% of the time. They further show
    that replacing “Muslim” in the prompt with other religious groups significantly
    reduces GPT-3 from including violence-related keywords and phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminatory gender, race, profession, and religion biases are also exaggerated
    in LLMs. In fictional stories generated by GPT-3, it was found that feminine characters
    were described as less powerful when compared to masculine characters, as well
    as more likely to be associated with family and appearance [[8]](https://aclanthology.org/2021.nuse-1.5.pdf).
    Other LLMs, such as BERT and GPT-2, also demonstrate strong stereotypical biases.
    For example, attribute words for *Africa* were found to be *poor* and *dark*,
    whereas attribute words for a *software developer* were *geek* and *nerd* [[9]](https://aclanthology.org/2021.acl-long.416.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the second case for perpetuating bias in LLMs: the lack
    of diversity in the training dataset. As we’ve previously discussed, quantity
    isn’t quality. To holistically represent the views and values of different individuals
    or groups, the training dataset must be diverse and broadly representative of
    perspectives from distinct communities. In the paper, “On the Dangers of Stochastic
    Parrots: Can Language Models Be Too Big?,” the authors explore several factors
    where they determine that the voices of people aren’t equally represented in the
    training datasets for language models [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922).
    As we know, Reddit and Wikipedia are two widely used datasets for training LLMs.
    The authors discuss how 67% of Reddit users are men and 64% are between 18 and
    29 years old, while similarly, only 8.8% to 15% of Wikipedians are women or girls.
    They also discuss that the common practice of filtering out datasets, such as
    the Common Crawl dataset, further weakens the voices of underrepresented communities.
    For example, in the training for GPT-3, the Common Crawl dataset is filtered by
    finding documents similar to Reddit and Wikipedia datasets, which is then additionally
    filtered by removing any page that contains a list of 400 words related to sex,
    racial slurs, or white supremacy. The authors argue that while it may be an effective
    strategy for filtering out certain kinds of pornography and hate speech, it inadvertently
    also suppresses discourse for marginalized populations, such as LGBTQ people.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors also discuss the challenges with ever-changing social movements
    where views can either be overrepresented or not captured at all in online discourse,
    which ultimately is the data that LLMs are trained on. In a specific example,
    researchers discovered that the “intensified documentation” on Wikipedia of the
    Black Lives Matter (BLM) movement reinforces BLM’s claims about police violence
    being a systematic problem in the United States [[10]](https://dl.acm.org/doi/pdf/10.1145/2998181.2998232).
    Before the movement brought new attention to the problem, Wikipedia data on police
    violence, made up of isolated cases, might have told a different story. This is,
    of course, especially a concern when training data isn’t frequently updated, which
    is likely not practical given how time-intensive and computationally expensive
    LLMs are to train.
  prefs: []
  type: TYPE_NORMAL
- en: In a joint study from the University of Bath and Princeton University, researchers
    show why addressing bias in machine learning is a challenging problem [[6]](https://doi.org/10.1126/science.aal4230).
    First, they show that bias is identical to meaning, so it’s impossible to meaningfully
    use language without incorporating human bias. Second, they discuss how it’s equally
    impossible to algorithmically define bias because our societal understanding of
    it is constantly evolving and varies between cultures. Finally, they show how
    biases can also be a result of historical inequalities that may be important to
    represent in some contexts.
  prefs: []
  type: TYPE_NORMAL
- en: There have been efforts to debias word embeddings and language models, most
    commonly concerning gender. To reduce bias in word embeddings, you could change
    the representation of a gender-neutral word by removing their gender associations.
    For example, if we have the word *nurse*, which is more likely associated with
    *female*, it would be moved equally between *male* and *female* [[11]](https://arxiv.org/pdf/1607.06520.pdf).
    In 2022, a group of researchers surveyed five debiasing techniques for language
    models concerning gender, religious, and racial biases, where they determined
    that not only do current debiasing techniques not work as well for nongender biases,
    but they also result in a decrease in the ability to model language [[12]](https://arxiv.org/pdf/2110.08527.pdf).
    Although a noble effort, algorithmically eliminating bias from language models
    is extraordinarily difficult because it also removes meaning and information,
    giving the model an incomplete picture of our world and turning debiasing into
    “fairness through blindness” [[6]](https://doi.org/10.1126/science.aal4230).
  prefs: []
  type: TYPE_NORMAL
- en: As argued by Bender and Gebru et. al, a concrete path forward is to curate and
    document training datasets for language models [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922).
    As of now, most LLMs are trained on a proprietary mixture of datasets, with sources
    not provided to end users. Documentation is critical for understanding the data
    characteristics, mitigating some of these risks, and allowing for potential accountability.
    We *can* build representative and unbiased datasets by budgeting for dataset documentation
    and only collecting as much data as can be documented. Hugging Face, a company
    focused on building open source machine learning tools, has developed dataset
    cards that are a good starting point for dataset documentation, including details
    about the dataset contents, any potential biases within the dataset, and context
    for how the dataset should be used [[13]](https://huggingface.co/docs/hub/datasets-cards).
    Hugging Face also released a search tool for ROOTS, a 1.6 TB multilingual text
    corpus, which was used to train BLOOM, an LLM [[14]](https://arxiv.org/pdf/2302.14035.pdf).
    To encourage researchers to characterize large datasets, the tool allows you to
    search through the dataset for qualitative analysis of training data. Similarly,
    founded through Berkman Klein Center’s Assembly fellowship at Harvard, the Data
    Nutrition Project takes inspiration from nutritional labels on food to highlight
    the key ingredients in a dataset, such as metadata and demographic representation
    (see [https://datanutrition.org/](https://datanutrition.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, unlike AI, humans have context-specific memories and social examples
    to draw from that can be used to overcome racial and gender biases. Humans can
    fight their implicit biases and these biases, need not remain entrenched in our
    society forever.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because LLMs are trained on unfathomable amounts of data from a wide range of
    sources on the internet, they can sometimes contain personally identifiable information
    (PII), such as names, addresses, Social Security numbers, biometric data, sexual
    orientation, and so on, even if trained on public data. One potential risk is
    that the model could unintentionally “memorize” details from the data on which
    it’s trained; that is, sensitive information from the model could be reflected
    in its output. There are, naturally, additional concerns if a model trained on
    a proprietary dataset is made publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: A massive vulnerability of LLMs is that an adversary can perform a *training
    data extraction attack* in which malicious actors can query the model to recover
    sensitive and identifiable information. As with most security and privacy studies,
    it’s important to consider the risks and ethics of performing attacks for research
    purposes, so publicly available and published work in this space is often limited.
  prefs: []
  type: TYPE_NORMAL
- en: Google, in collaboration with OpenAI, Apple, Stanford, Northeastern University,
    and Berkeley, demonstrated their “attack” on GPT-2 to show that it’s possible
    to extract sensitive pieces of training data that the model has inadvertently
    “memorized.” Here, the attackers can query a language model to extract *verbatim
    information* from the training data. The researchers note that the training data
    extraction attacks have the most potential for harm when a model that is trained
    on a proprietary dataset is made publicly available, but they acknowledge that
    performing an attack for research purposes on such a dataset could also have harmful
    consequences. With this in mind, they chose GPT-2 because the training dataset
    collection process is documented and only uses public internet sources. They were
    able to extract hundreds of verbatim pieces of information that include PII (names,
    phone numbers, email addresses), instant messaging conversations, code, and universally
    unique identifiers (UUIDs). Most of these examples were memorized even though
    they appeared very infrequently, as little as in a single document, in the training
    dataset, and larger models were found to be more vulnerable to these attacks than
    smaller models [[15]](https://arxiv.org/pdf/2012.07805.pdf). A different study,
    “The Secret Sharer,” shows that unintended memorization is persistent and hard
    to avoid for LLMs [[16]](https://arxiv.org/pdf/1802.08232.pdf). They demonstrated
    an attack on the Enron Email Dataset (see [http://mng.bz/K9AZ](http://mng.bz/K9AZ)),
    which contains half a million emails sent between Enron Corporation employees.
    The dataset was made public and posted online by the Federal Energy Regulatory
    Commission during its investigation. The researchers used the Enron Email Dataset
    to train a language model and show that they are effortlessly able to extract
    credit card and Social Security numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward way to mitigate this problem is to make sure that
    models don’t train on any sensitive or PII data. This is extremely difficult to
    do in practice, however, and goes back to our earlier point of curating and documenting
    datasets for language models. Other solutions include *privacy-preserving* or
    *privacy-enhancing technologies* (PETs), which can help mitigate data privacy
    and security risks [[17]](https://royalsociety.org/-/media/policy/projects/privacy-enhancing-technologies/Protecting-privacy-in-practice.pdf).
    Some examples of PETs include methods for pseudonymization, obfuscation, sanitization,
    and data masking. An example of using these in practice is creating blocklists
    for possible sensitive sequences to filter out potentially private information
    from the training dataset. However, as demonstrated in “The Secret Sharer,” blocklisting
    is never a complete approach in security and won’t significantly reduce the effect
    of unintended memorization for any sequences that did appear. Differential privacy,
    an anonymization technique introduced in the early 2000s, is a popular PET that
    attempts to train via dataset without revealing details of any individual training
    samples. Here, the idea is to add statistical *noise* to obscure individual identities
    in a given dataset. But this technique also has its limitations because it won’t
    prevent memorization for content that isn’t repeated often in the dataset. In
    “Beyond Data: Reclaiming Human Rights at the Dawn of the Metaverse,” the author
    points out that PETs are not only highly technical, complex to use, expensive,
    and resource-intensive but also challenging for lawmakers and policymakers to
    audit or govern [[18]](https://books.google.com/books/about/Beyond_Data.xhtml?hl=&id=zJZuEAAAQBAJ).'
  prefs: []
  type: TYPE_NORMAL
- en: Privacy-preserving or privacy-enhancing technologies (PET) are umbrella terms
    used for approaches that can help mitigate privacy and security risks.
  prefs: []
  type: TYPE_NORMAL
- en: Given the limitations of current PET approaches, we hope that the efforts to
    raise awareness of this challenge will encourage researchers to develop new techniques
    to address this problem, as well as build on previous work to test unintended
    memorization from LLMs so we can respond to the problem appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs might be trained on a combination of open source or public datasets, datasets
    purchased from third-party vendors, datasets that companies collect themselves
    by scraping the web, or datasets that the companies create themselves by writing
    examples for the models to learn from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Autoregressive* models refer to the fact that future predictions are dependent
    on the model’s past data. All the models in the GPT family, as well as Google’s
    PaLM, are autoregressive models trained to predict the next token given some input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zero-shot* and *few-shot* refer to the number of examples that the model is
    given before being asked to perform a task. They are primary examples of the emergent
    abilities of LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs often encode and amplify stereotypical and derogatory associations; they
    also contain sensitive data, including personally identifiable information (PII).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A concrete path forward is to curate and document training datasets for language
    models, which is critical for understanding the data characteristics in order
    to mitigate risks and allow for potential accountability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An adversary can perform a *training data extraction attack* with an LLM in
    which malicious actors can query the model to recover sensitive and identifiable
    information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Privacy-preserving* or *privacy-enhancing technologies* (PETs) can help mitigate
    data privacy and security risks. PETs have several limitations, and we hope to
    see concentrated efforts of researchers in this area so there are techniques that
    LLM developers can easily adopt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
