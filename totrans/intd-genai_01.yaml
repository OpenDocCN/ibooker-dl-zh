- en: 2 Training large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 训练大型语言模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Explaining how LLMs are trained
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释LLMs是如何被训练的
- en: Introducing the emergent properties of LLMs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍LLMs的涌现特性
- en: Exploring the harms and vulnerabilities that come from training LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索训练LLMs带来的危害和风险
- en: For decades, the digital economy has run on the currency of data. The digital
    economy of collecting and trading information about who we are and what we do
    online is worth trillions of dollars, and as more of our daily activities have
    moved on to the internet, the mill has ever more grist to grind through. Large
    language models (LLMs) are inventions of the internet age, emulating human language
    by vacuuming up terabytes of text data found online.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，数字经济一直依赖于数据这一货币。收集和交易关于我们在网上是谁以及我们在做什么的信息的数字经济价值数万亿美元，随着我们越来越多的日常活动转移到互联网上，磨坊的磨盘上磨的谷物越来越多。大型语言模型（LLMs）是互联网时代的发明，通过收集在线发现的数以千计的文本数据来模拟人类语言。
- en: The process has yielded both predictable and unpredictable results. Notably,
    there are significant questions about both what is in the datasets used by LLMs
    and how to prevent the models from replicating some of the more objectionable
    text they hold in their training sets. With data collection at this scale, the
    collection of personal information and low-quality, spammy, or offensive content
    is expected, but how to address the problem is another challenge. LLMs at the
    scale we’re now seeing have exhibited a host of capabilities that don’t seem to
    be available to smaller language models. These properties make LLMs more attractive
    for a variety of uses and ensure that the race toward more and more data and bigger
    and bigger models won’t end anytime soon.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程产生了可预测和不可预测的结果。值得注意的是，关于LLMs使用的数据集中有什么以及如何防止模型复制他们在训练集中持有的某些更令人反感的文本，都存在重大问题。在如此规模的数据收集下，收集个人信息、低质量、垃圾邮件或攻击性内容是预期的，但如何解决这个问题是另一个挑战。我们现在看到的LLMs规模已经展现出了许多似乎不属于较小语言模型的能力。这些特性使LLMs在各种用途上更具吸引力，并确保了向更多数据和更大模型的竞赛不会很快结束。
- en: In this chapter, you’ll learn more about how LLMs are trained to understand
    what makes them unique compared to previous models and how these characteristics
    result in both new capabilities and potential vulnerabilities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解更多关于如何训练大型语言模型（LLMs）以理解它们与先前模型相比的独特之处，以及这些特性如何导致新的能力和潜在的风险。
- en: How are LLMs trained?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs是如何被训练的？
- en: In chapter 1, we introduced some of the concepts involved in training LLMs.
    We covered transformer architecture, a specific type of neural network used in
    LLMs, and talked about some of the sources of data that LLMs use. We also explained
    the self-supervised task they are trained to complete—generate the next most probable
    word or character, also known as token prediction. Here we’ll examine the training
    process in greater detail and discuss perhaps the most surprising and exciting
    aspect of LLMs—their emergent properties, that is, things they weren’t trained
    to do, but do well anyway.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们介绍了训练LLMs涉及的一些概念。我们涵盖了Transformer架构，这是一种在LLMs中使用的特定类型的神经网络，并讨论了LLMs使用的一些数据来源。我们还解释了他们被训练完成的自我监督任务——生成下一个最可能的单词或字符，也称为标记预测。在这里，我们将更详细地检查训练过程，并讨论LLMs最令人惊讶和激动人心的方面——它们的涌现特性，即他们没有被训练去做，但仍然做得很好的事情。
- en: The first step of creating an LLM, often called the *pre-training* step, is
    training on some token prediction task (for a generative model, autoregression
    or causal token prediction) with a gigantic corpus of data. It is called pre-training
    because even though this is a training phase, the knowledge encoded by the model
    during this phase is foundational to any subsequent natural language task. Then,
    the model is fine-tuned on one or many additional tasks, that is, trained with
    labeled data and a specific objective. Dialogue agents such as ChatGPT might be
    fine-tuned on conversational data; many generative models are fine-tuned on instruction
    datasets to improve their capability to follow instructions (e.g., “Write me a
    poem”); others might be fine-tuned for code generation. This process is pictured
    in figure 2.1, but it’s worthwhile to take a deeper look at each of these stages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个大型语言模型（LLM）的第一步，通常被称为**预训练**步骤，是在一个庞大的数据语料库上对某些标记预测任务（对于生成模型，是自回归或因果标记预测）进行训练。之所以称为预训练，是因为尽管这是一个训练阶段，但模型在此阶段编码的知识是任何后续自然语言任务的基础。然后，模型在单个或多个附加任务上进行微调，即使用标记数据和特定目标进行训练。例如，对话代理如ChatGPT可能会在对话数据上进行微调；许多生成模型会在指令数据集上进行微调，以提高其遵循指令的能力（例如，“为我写一首诗”）；其他模型可能会针对代码生成进行微调。这个过程在图2.1中有所展示，但深入探讨每个阶段都是值得的。
- en: '![](../../OEBPS/Images/CH02_F01_Dhamani.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F01_Dhamani.png)'
- en: Figure 2.1 The high-level training process for LLMs
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 LLMs的高级训练过程
- en: Exploring open web data collection
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索公开网络数据收集
- en: To model natural language and then generate language convincingly, LLMs need
    lots and lots of examples. Let’s consider all the implicit knowledge that goes
    into question-answering tasks. First, the model must have an accurate representation
    of both the question and the context (what the question is being asked about),
    which in turn means having a representation for each of the tokens in the question
    and context—analogous to knowing what the words themselves mean. The model must
    also be able to parse the question syntactically to identify what is being asked
    and then produce an answer, either from the context (the open-book case) or from
    its internal representation of external concepts (the closed-book case). Because
    LLMs have seen so much text from the internet, most would be able to answer a
    question like, “Who was the first president of the United States?” correctly without
    any provided context. More obscure information might result in an incorrect or
    made-up answer because the model wouldn’t have a high-probability response. Notably,
    if we ask ChatGPT, “Who was the first president?” without specifying that we are
    asking about the United States, ChatGPT responds, “The first president of the
    United States was George Washington.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟自然语言并生成令人信服的语言，LLM需要大量的例子。让我们考虑所有进入问答任务的问题。首先，模型必须对问题和上下文（问题所涉及的内容）有一个准确的表现，这反过来又意味着需要对问题和上下文中的每个标记有一个表现——类似于知道单词本身的意思。模型还必须能够从句法上解析问题，以确定所问的内容，然后产生一个答案，无论是从上下文（开放式书籍案例）还是从其对外部概念的内部分代（封闭式书籍案例）中产生。由于LLM已经看到了互联网上的大量文本，大多数LLM能够正确回答像“谁是美国的第一个总统？”这样的问题，而无需任何提供的上下文。更难以捉摸的信息可能会导致错误的或虚构的答案，因为模型不会有一个高概率的响应。值得注意的是，如果我们向ChatGPT提问“谁是第一个总统？”而没有指定我们是在询问美国，ChatGPT会回答，“美国的第一个总统是乔治·华盛顿。”
- en: LLMs use data from the open web, which refers to all public web pages on the
    internet, including sites such as Wikipedia and Reddit, but also possibly non-password-protected
    blogs, news aggregators, and non-private forums. Why does ChatGPT assume we’re
    asking about the United States? To be fair, the answer might be different if the
    request came from an IP address in another country, but the assumption also belies
    an indisputable fact about internet data—most of it is in English, and a disproportionate
    amount of it is from the United States and Western Europe. In chapter 1, we mentioned
    that Wikipedia is one of the classic data sources for LLMs. While the encyclopedia’s
    global geographic coverage continues to improve, there are more than 6.6 million
    articles in the English Wikipedia, whereas the next-highest total is 2.5 million
    articles in the French Wikipedia. The downstream effects of this are that the
    LLMs are better at understanding, generating, and completing tasks in English.
    They also better understand topics relevant to North America and Western Europe,
    and therefore serve these audiences better.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLM使用来自公开网络的数据，这指的是互联网上所有公开网页，包括维基百科和Reddit等网站，也可能包括非密码保护的博客、新闻聚合器和非私人论坛。为什么ChatGPT假设我们在询问美国？公平地说，如果请求来自另一个国家的IP地址，答案可能不同，但这种假设也掩盖了一个关于互联网数据的不可否认的事实——其中大部分是英语，而且不成比例的大量数据来自美国和西欧。在第1章中，我们提到维基百科是LLM的经典数据来源之一。虽然该百科的全球地理覆盖范围持续改善，但英语维基百科有超过660万篇文章，而下一个最高的总数是法语维基百科的250万篇文章。这种影响的下游效应是，LLM在理解、生成和完成英语任务方面表现得更好。它们也更好地理解与北美和西欧相关的话题，因此更好地服务于这些受众。
- en: To get a sense of other types of text datasets in use, we can look at open data
    repositories, such as that of the open source AI company Hugging Face (see [https://huggingface.co/datasets](https://huggingface.co/datasets)).
    Open data is available for anyone to download and use for their projects, although
    sometimes the type of permissible use is restricted by the data’s license; for
    example, a dataset provider might specify that the dataset should be used for
    academic or research purposes only, not in commercial applications. One dataset
    for language models consists of millions of Reddit posts (with non-English posts
    filtered out). Others include collections of news articles, reviews from sites
    such as Amazon and Rotten Tomatoes (review-aggregation website for movies and
    TV shows), or questions and answers from the community Q&A site Stack Exchange.
    Common Crawl is a nonprofit that maintains a massive repository of web page data
    and provides it for public use (see [https://commoncrawl.org/](https://commoncrawl.org/)).
    In short, anywhere that people are writing online is a potential data source.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解正在使用的其他类型文本数据集，我们可以查看公开数据存储库，例如开源AI公司Hugging Face的数据存储库（见[https://huggingface.co/datasets](https://huggingface.co/datasets)）。任何人都可以下载并使用这些公开数据为其项目服务，尽管有时数据的使用类型可能受到数据许可的限制；例如，数据集提供者可能指定数据集仅用于学术或研究目的，不得用于商业应用。一个用于语言模型的数据集包含数百万条Reddit帖子（过滤掉了非英语帖子）。其他数据集包括新闻文章集合、来自亚马逊和烂番茄（电影和电视剧的评论聚合网站）的评论，或来自社区问答网站Stack
    Exchange的问题和答案。Common Crawl是一个非营利组织，维护着一个庞大的网页数据存储库，并向公众提供这些数据（见[https://commoncrawl.org/](https://commoncrawl.org/)）。简而言之，任何人们在线写作的地方都是一个潜在的数据来源。
- en: Companies that develop LLMs might use a combination of open datasets such as
    those on Hugging Face, datasets that they purchase from third-party vendors, datasets
    that they collect themselves by scraping the web, or datasets that they create
    themselves by writing examples for the models to learn from. Although the initial
    training of the LLM might not require any manual intervention, as we’ll see, crowdsourcing
    and conversational collection are important in improving the model’s performance
    in specific domains, such as dialogue for chatbots.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 开发大型语言模型（LLM）的公司可能会使用多种数据集的组合，例如来自Hugging Face等公开数据集，从第三方供应商购买的数据集，通过爬取网络自行收集的数据集，或者通过为模型编写学习示例自行创建的数据集。尽管LLM的初始训练可能不需要任何人工干预，但正如我们将看到的，众包和对话收集对于提高模型在特定领域（如聊天机器人的对话）的性能至关重要。
- en: Demystifying autoregression and bidirectional token prediction
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 揭秘自回归和双向标记预测
- en: Some of the first LLMs, such as Google’s BERT, were focused much more heavily
    on natural language understanding, as compared to generative use cases such as
    chatbots. Because of this objective, BERT is known as a bidirectional model, meaning
    BERT was trained to predict the missing word (token) within a sentence and has
    access to both the left and right contexts (the bidirectional part). This is ideal
    for natural language understanding because the model picks up more information
    about the contexts a particular word is used in. However, if a model is used for
    text generation, it shouldn’t be trained on anything that comes after the missing
    token because it would only ever have access to the text that preceded it. This
    type of model is called *autoregressive*, because future predictions are dependent
    on the model’s past data. All the models in the GPT family, as well as Google’s
    Pathways Language Model (PaLM), are autoregressive.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些早期的LLMs，例如谷歌的BERT，相比聊天机器人等生成用例，更侧重于自然语言理解。正因为这个目标，BERT被称为双向模型，这意味着BERT被训练来预测句子中缺失的单词（标记）并能够访问左右两个上下文（双向部分）。这对于自然语言理解来说是非常理想的，因为模型能够获取更多关于特定单词使用上下文的信息。然而，如果一个模型用于文本生成，它不应该在缺失标记之后的内容上进行训练，因为它只能访问它之前的文本。这种类型的模型被称为**自回归**模型，因为未来的预测依赖于模型的历史数据。GPT家族中的所有模型，以及谷歌的Pathways语言模型（PaLM），都是自回归的。
- en: Autoregressive means that future predictions are dependent on the model’s past
    data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**自回归**意味着未来的预测依赖于模型的历史数据。'
- en: As an example, consider the sentence, “For their honeymoon, they flew to _____
    and had a romantic dinner in front of the Eiffel Tower.” The correct word for
    a model to predict here is “Paris”. In this case, the right context (what happens
    after the missing word) is especially informative, and a bidirectional model would
    very likely answer correctly. But when a model is asked to generate text, such
    as, “A good location for a romantic honeymoon is ______,” the task is structured
    such that the model’s completion is at the end of the context. Therefore, the
    model’s training should only use the left context (what comes before the missing
    word) to predict the missing tokens. The model learns by self-supervision, repeatedly
    guessing the final token in billions of examples from the text and adjusting its
    weights based on the correct token, until the model’s performance on guessing
    missing tokens in the training data is optimal. When we chat with ChatGPT, it
    doesn’t appear to look like a formal task to the user, but under the hood, the
    model is predicting what should come next after each message. When I type, “Hey!
    What’s up?” the logical and likeliest completions are to answer the question and
    return the greeting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这个句子，“为了他们的蜜月，他们飞往______并在埃菲尔铁塔前享用浪漫晚餐。”模型需要预测的正确单词是“巴黎”。在这种情况下，正确的上下文（缺失单词之后发生的事情）特别具有信息性，双向模型很可能给出正确的答案。但是，当模型被要求生成文本，例如，“一个浪漫蜜月的理想地点是______，”任务的结构是这样的，即模型的补全位于上下文的末尾。因此，模型的训练应该只使用左上下文（缺失单词之前的内容）来预测缺失的标记。模型通过自我监督学习，反复从文本中的数十亿个例子中猜测最终的标记，并根据正确的标记调整其权重，直到模型在训练数据中猜测缺失标记的性能达到最优。当我们与ChatGPT聊天时，这看起来对用户来说不是一个正式的任务，但在底层，模型正在预测每条消息之后应该出现的内容。当我输入“嘿！怎么了？”时，逻辑上最有可能的补全是回答问题并返回问候。
- en: Fine-tuning LLMs
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLMs）
- en: Once trained on the token completion task, a model can generate words, phrases,
    or complete sentences. At this stage the models are called foundation or base
    models because they provide the foundational knowledge, due to their complex representations
    of thousands of different words and concepts, for performing natural language
    processing (NLP) tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在标记补全任务上训练，模型可以生成单词、短语或完整的句子。在这个阶段，这些模型被称为基础或基模型，因为它们提供了基础的知识，这是由于它们对数千个不同单词和概念的复杂表示，从而能够执行自然语言处理（NLP）任务。
- en: Although these base models aren’t that impressive out of the box, they can be
    easily adapted to do well in specific tasks through *fine-tuning*, that is collecting
    labeled datasets that demonstrate the specific task or tasks the model needs to
    improve on. These tasks might be very narrow, such as a classification problem
    requiring specific domain expertise, or quite broad. Many commercial LLMs are
    fine-tuned on instruction-following data so that the models can better respond
    to inputs such as “Write a song,” or “Tell me a joke.” Other fine-tuning tasks
    are also common uses for LLMs, such as summarization and question answering. From
    a technical perspective, fine-tuning trains a neural network in a supervised fashion,
    but instead of starting from scratch, the neural network is initialized with the
    weights of the foundation model. Whereas training the foundation model takes weeks
    and uses large amounts of computing resources, fine-tuning can be done in minutes.
    The fine-tuned model uses the representations of the original but then adjusts
    its own weights and parameters to best fit the new data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些基础模型一开始并不那么令人印象深刻，但它们可以通过微调轻松适应以在特定任务上表现良好，即收集展示模型需要改进的特定任务或任务的标记数据集。这些任务可能非常狭窄，例如需要特定领域专业知识的分类问题，或者相当广泛。许多商业LLMs在遵循指令的数据上进行微调，以便模型能更好地响应“写一首歌”或“讲一个笑话”等输入。其他微调任务也是LLMs的常见用途，如摘要和问答。从技术角度来看，微调以监督方式训练神经网络，但不是从头开始，而是使用基础模型的权重初始化神经网络。而训练基础模型需要数周时间并使用大量计算资源，而微调可以在几分钟内完成。微调模型使用原始模型的表示，然后调整自己的权重和参数以最佳地适应新数据。
- en: 'The unexpected: Emergent properties of LLMs'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 意外的：LLMs的演化特性
- en: In some respects, LLMs are natural extensions of predecessor neural network
    models. Before the transformer architecture made it efficient to build larger
    and larger models, it was well-known that model size correlates with model performance
    on a range of common NLP tasks, and, in many cases, such performance improvements
    could be predicted based on empirically derived scaling laws. However, LLMs have
    also yielded behaviors, called emergent properties, that no one could have predicted
    via a scaling law. In a 2022 survey on the emergent abilities of LLMs, emergence
    is defined as “when quantitative changes in a system result in qualitative changes
    in behavior” [[1]](http://arxiv.org/abs/2206.07682). In other words, we might
    expect that for a particular task, a model with 100 billion parameters would achieve
    10% higher accuracy than a model with 100 million parameters. But the model with
    100 billion parameters—an LLM—can now do tasks that the smaller model can’t and
    in somewhat unpredictable and unexpected ways.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些方面，LLMs是前辈神经网络模型的自然扩展。在变压器架构使构建更大模型变得高效之前，众所周知，模型大小与模型在一系列常见NLP任务上的性能相关联，并且在许多情况下，这种性能提升可以根据经验推导的缩放定律进行预测。然而，LLMs还产生了被称为演化特性的行为，这些行为无法通过缩放定律进行预测。在2022年关于LLMs演化能力的一项调查中，演化被定义为“当系统中的数量变化导致行为在本质上发生变化时”
    [[1]](http://arxiv.org/abs/2206.07682)。换句话说，我们可能预期，对于特定任务，一个拥有1000亿参数的模型会比一个拥有1亿参数的模型实现10%更高的准确率。但是，拥有1000亿参数的模型——即LLM——现在可以执行较小模型无法执行的任务，并且以某种不可预测和出乎意料的方式。
- en: Emergent properties are abilities that LLMs begin to exhibit at very large model
    sizes with behaviors that are qualitatively different from those of smaller models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 演化特性是指LLMs在非常大的模型尺寸下开始展现的能力，这些能力与较小模型的行为在本质上有所不同。
- en: 'Quick study: Learning with few examples'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 快速学习：通过少量示例进行学习
- en: When talking about the emergent capabilities of LLMs, it’s useful to compare
    them to the capabilities derived from the process described in the previous section.
    In the standard case, the model is pre-trained and fine-tuned for one or many
    natural language abilities, such as translation or analogy completion. These abilities
    are part of the training pipeline and are considered predictable—not in exactly
    how the model will perform but in how the model improves as it’s trained.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈论LLMs的演化能力时，将其与上一节中描述的过程产生的能力进行比较是有用的。在标准情况下，模型是预训练并微调以具备一个或多个自然语言能力，如翻译或类比完成。这些能力是训练流程的一部分，被认为是可预测的——不是指模型将如何表现，而是指模型在训练过程中的改进。
- en: On the other hand, the primary examples of emergent abilities are zero-shot
    and few-shot learning. The terms *zero-shot* and *few-shot* refer to the number
    of examples that the model is given before being asked to perform a task. For
    instance, let’s say that a restaurateur wants to add visual indicators for vegetarian
    dishes on their restaurant’s menu. Using ChatGPT, they might write something like,
    “Please rewrite this menu and put an asterisk next to all dishes that do not contain
    any meat,” and then copy and paste the menu. This might seem like a trivial task
    for a human, but the model must first interpret the request, then classify each
    written menu item according to whether or not it contains meat, and finally produce
    the output in the corresponding format. The level of natural language understanding
    and generative ability required to complete such a task with no previous examples
    (we can safely assume that the model was never trained explicitly to do this)
    isn’t observed in previous language models, and yet LLMs can produce impressive
    results on many such zero-shot tasks, where the model has never seen the task
    before.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，涌现能力的首要例子是零样本和少样本学习。*零样本*和*少样本*这两个术语指的是在要求模型执行任务之前，模型所获得的示例数量。例如，假设一位餐馆老板想在他们的菜单上添加视觉指示，以标示素食菜肴。使用ChatGPT，他们可能会写一些像这样的事情：“请重新编写这个菜单，并在所有不含任何肉的菜肴旁边加上星号，”然后复制并粘贴菜单。这可能对人类来说似乎是一个微不足道的小任务，但模型必须首先解释请求，然后根据每一项书面菜单内容是否含有肉类进行分类，最后以相应的格式生成输出。完成这样一个任务所需的自然语言理解和生成能力，在没有先前示例的情况下（我们可以安全地假设模型从未被明确训练来完成这项任务），在先前的语言模型中并未观察到，然而，大型语言模型（LLMs）在许多这样的零样本任务上可以产生令人印象深刻的成果，在这些任务中，模型之前从未见过这个任务。
- en: Zero-shot or few-shot refers to the number of examples that the model is given
    before being asked to perform a task.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本或少样本指的是在要求模型执行任务之前，模型所获得的示例数量。
- en: 'In the few-shot case, the model is given a few examples of the task in the
    *prompt*, the text that the model takes as input to determine what output it should
    generate. In the previous zero-shot example, the user’s request constituted part
    or all of the model’s prompt (models are sometimes deployed with a base prompt,
    which might provide generic instructions on how to respond to inputs but isn’t
    relevant to this discussion). Another user might want the model to perform a slightly
    more complex task. Let’s say a freelance writer is working on three different
    pieces—one about dog breeding, one about exoplanets, and one about Pittsburgh—and
    wants to organize a list of articles by topic. In this case, they might write
    something like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本情况下，模型在*提示*中给出几个任务的示例，即模型作为输入文本来确定它应该生成什么输出。在之前的零样本示例中，用户的请求构成了模型提示的一部分或全部（模型有时会部署一个基础提示，这可能提供关于如何响应输入的通用指令，但与本次讨论无关）。另一个用户可能希望模型执行一个稍微复杂一些的任务。比如说，一个自由职业的作家正在撰写三篇不同的文章——一篇关于狗的繁殖，一篇关于系外行星，还有一篇关于匹兹堡——并希望按主题组织文章列表。在这种情况下，他们可能会写一些像这样的事情：
- en: Each of the following articles is related to one of “dog breeding,” “exoplanets,”
    or “Pittsburgh”. For each article, write the most likely related topic from those
    three topics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下每一篇文章都与“狗的繁殖”、“系外行星”或“匹兹堡”中的一个相关。对于每一篇文章，写出最可能的相关主题。
- en: 'This could be structured as a zero-shot task as well. However, it’s generally
    beneficial to model performance to provide a few examples, so if the response
    wasn’t exactly what the writer wanted, they might try to provide additional guidance:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以被结构化为一个零样本任务。然而，通常来说，提供几个示例对模型性能是有益的，所以如果响应并不完全符合作者的期望，他们可能会尝试提供额外的指导：
- en: 'Example: “The latest discovery of space telescopes”: Exoplanets; Example: “Why
    pugs have breathing problems”: Dog breeding; and so on.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：“最新太空望远镜的发现”：系外行星；示例：“为什么斗牛犬有呼吸问题”：狗的繁殖；等等。
- en: 'Figure 2.2 shows how zero-shot and few-shot prompts differ from fine-tuning
    a model for a task. If you’ve used an LLM to perform one of these tasks, you might
    have tried zero-shot and few-shot learning without even thinking about it or realizing
    it. This is one of the great strengths of LLMs: because the interface with these
    chatbots is simply natural language, we can often tweak the inputs to achieve
    the desired outputs in a much more intuitive way than we might with other models.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2展示了零样本和少量样本提示与为任务微调模型的不同之处。如果你使用LLM执行这些任务之一，你可能已经尝试了零样本和少量样本学习，甚至没有意识到或考虑过。这是LLM的巨大优势之一：因为这些聊天机器人的界面仅仅是自然语言，我们通常可以更直观地调整输入以实现所需的输出，这比我们可能使用其他模型的方式要直观得多。
- en: '![](../../OEBPS/Images/CH02_F02_Dhamani.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F02_Dhamani.png)'
- en: Figure 2.2 A comparison of fine-tuning, zero-shot learning, and one-shot learning
    on a machine translation task
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 在机器翻译任务中，微调、零样本学习和单样本学习的比较
- en: In addition to zero-shot and few-shot examples in the model’s prompts, other
    changes to the model’s prompt have uncovered additional emergent abilities. A
    technique called chain-of-thought prompting, or directing the model to break apart
    challenging problems into multiple steps, has been shown to improve model performance
    (in its simplest version, prefacing a prompt with “Let’s think step-by-step” has
    been shown to make the model generations more accurate in reasoning problems).
    People have also tested detailed instructions on zero-shot tasks, as well as asking
    the model about its level of confidence in its own response, each of which can
    improve responses in certain settings.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型提示中的零样本和少量样本示例之外，对模型提示的其他更改还揭示了额外的新兴能力。一种称为思维链提示的技术，或者指导模型将具有挑战性的问题分解成多个步骤，已被证明可以提高模型性能（在其最简单的版本中，在提示前加上“让我们一步步思考”已被证明可以使模型在推理问题上的生成更加准确）。人们还测试了零样本任务上的详细指令，以及询问模型对其自身响应的信心水平，这些都可以在某些设置中改善响应。
- en: In the previously mentioned study exploring the emergent abilities of LLMs,
    the authors examined the performance of LLMs of various sizes on few-shot tasks.
    In particular, the researchers looked for tasks where the performance of “small”
    LLMs was random, but then jumped sharply at the larger sizes. They found that
    language models’ ability to do addition, subtraction, and multiplication was emergent,
    with GPT-3 getting answers correct in almost no cases until the 13-billion-parameter
    model size; similarly, GPT-3 and other models were found to significantly improve
    their ability to answer questions about miscellaneous academic topics, including
    math, history, and law, after reaching about 70 billion or more parameters. Because
    these emergent abilities don’t follow the scaling law, it’s difficult to say with
    certainty whether larger sizes would promote even greater capabilities, at what
    size improvement would stop, or even how to reason about these tasks as compared
    to those where accuracy maps predictably to model size.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前提到的关于探索大型语言模型（LLM）新兴能力的研究中，作者们检查了不同大小LLM在少量样本任务上的表现。特别是，研究人员寻找的是“小型”LLM表现随机，但随后在更大规模上急剧提升的任务。他们发现，语言模型进行加法、减法和乘法的能力是新兴的，GPT-3在几乎没有任何情况下得到正确答案，直到13亿参数规模的模型；同样，GPT-3和其他模型在达到大约700亿或更多参数后，发现它们在回答关于各种学术主题的问题（包括数学、历史和法律）的能力显著提高。因为这些新兴能力不遵循规模定律，所以很难确定更大的规模是否会促进更大的能力，在什么规模下改进会停止，甚至如何与那些准确性可预测地映射到模型规模的任务相比来推理这些任务。
- en: Sparks of artificial general intelligence?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 人工通用智能的火花？
- en: According to an evaluation by a team at Microsoft, “beyond its mastery of language,
    GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision,
    medicine, law, psychology and more, without needing any special prompting” [[2]](http://arxiv.org/abs/2304.15004).
    These emergent abilities led them to provocatively title the paper, “Sparks of
    Artificial General Intelligence,” and write that “Given the depth and breadth
    of GPT-4’s capabilities, we believe it could be reasonably viewed as an early
    (yet still incomplete) version of an artificial general intelligence (AGI) system.”
    AGI has been the long-sought goal of many scientists in AI, and it’s understood
    to be intelligence that can learn as well as humans who have historically been
    much better at generalizing knowledge and adapting to unseen problems. The question
    of AGI, and whether any LLMs possess it, is outside the scope of this chapter,
    but we’ll discuss it and related questions in chapter 9.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据微软团队的一项评估，“除了对语言的精通之外，GPT-4还能够解决跨越数学、编码、视觉、医学、法律、心理学等多个领域的创新和困难任务，而无需任何特殊提示”
    [[2](http://arxiv.org/abs/2304.15004)]。这些涌现能力使他们大胆地将论文标题定为“人工智能的火花”，并写道：“鉴于GPT-4的能力深度和广度，我们认为它可以合理地被视为一个人工通用智能（AGI）系统的早期（尽管仍不完整）版本。”AGI一直是许多AI科学家长期追求的目标，它被理解为可以像人类一样学习，而人类在历史上一直擅长于概括知识和适应未见问题。关于AGI的问题，以及任何LLMs是否拥有它，超出了本章的范围，但我们将它在第9章中讨论和相关问题。
- en: Is emergence an illusion?
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 涌现是幻觉吗？
- en: Although several studies have documented evidence of emergent abilities, there
    isn’t yet a consensus about emergence within the machine learning community. A
    team of computer scientists at Stanford University argued that these so-called
    emergent abilities appear less because of some qualitative change in model behavior
    at certain scales and more because of the way that researchers are evaluating
    the models [[2]](http://arxiv.org/abs/2304.15004). In particular, the sharp increases
    in performance that characterize emergence in some tasks seem to be at least partially
    attributable to the choice of metric on the task, the amount of test data used
    for evaluation (because testing on less data will give a noisier estimate of model
    performance), and the number of large-scale models in the evaluation (because
    there are fewer large-scale models available than small-scale models). In other
    words, the authors don’t dispute the actual performance of the LLMs on any of
    these tasks, just the idea that the LLMs, in cases where emergent abilities were
    claimed, represented a fundamental change from previous versions. The emergence
    behavior depends on the performance metric selected, and while it’s not clear
    whether one metric is better than another, caution is warranted before we assume
    that *other* capabilities might readily emerge with more or different data and
    bigger models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有几项研究记录了涌现能力的证据，但在机器学习社区中关于涌现的共识尚未形成。斯坦福大学的一组计算机科学家认为，这些所谓的涌现能力之所以出现，更多的是因为模型在特定尺度上的行为发生了某些定性变化，而不是因为研究人员评估模型的方式
    [[2](http://arxiv.org/abs/2304.15004)]。特别是，某些任务中涌现的特征是性能的急剧增加，这似乎至少部分归因于任务中使用的度量标准的选择、用于评估的测试数据量（因为使用较少的数据会对模型性能的估计造成更多的噪声），以及评估中大规模模型的数量（因为可用的较大规模模型比小规模模型少）。换句话说，作者并不否认LLMs在这些任务上的实际性能，只是认为在声称涌现能力的情况下，LLMs与之前的版本相比代表了一种根本性的变化。涌现行为取决于选定的性能指标，而且虽然不清楚哪个指标更好，但在我们假设*其他*能力可能会随着更多或不同的数据以及更大的模型而轻易出现之前，我们应该保持谨慎。
- en: What’s in the training data?
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据中有什么？
- en: As we’ve previously discussed, LLMs are trained on *massive* amounts of noncurated
    data from the web. Just how much information have these LLMs been fed? Quite a
    lot. The general-purpose LLM, GPT-3, was trained on 45 terabytes (TB) of text
    data [[3]](https://arxiv.org/pdf/2005.14165.pdf), where 1 TB is generally estimated
    to contain 75 million pages [[4]](https://cloudnine.com/ediscoverydaily/electronic-discovery/ediscovery-best-practices-perspective-on-the-amount-of-data-contained-in-1-gigabyte/).
    When working with unfathomable amounts of noncurated and undocumented training
    data, no one is quite sure what the data includes, resulting in LLMs encoding
    and amplifying stereotypical and derogatory associations, as well as sometimes
    containing sensitive data, such as personally identifiable information (PII).
    In this section, we’ll talk more about the challenges that come with training
    language models on immeasurable amounts of text data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所讨论的，大型语言模型（LLMs）是在来自网络的*大量*非精选数据上训练的。这些LLMs到底被喂食了多少信息？相当多。通用LLM GPT-3是在45太字节（TB）的文本数据上训练的
    [[3]](https://arxiv.org/pdf/2005.14165.pdf)，其中1TB通常估计包含7500万页 [[4]](https://cloudnine.com/ediscoverydaily/electronic-discovery/ediscovery-best-practices-perspective-on-the-amount-of-data-contained-in-1-gigabyte/)。当与难以估量的非精选和未记录的训练数据一起工作时，没有人确切知道数据包含什么，这导致LLMs编码和放大了刻板和贬义的联系，有时甚至包含敏感数据，如个人可识别信息（PII）。在本节中，我们将更多地讨论在难以衡量的文本数据上训练语言模型所面临的挑战。
- en: Encoding bias
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码偏差
- en: Perpetuating harmful stereotypes and discriminatory language along the lines
    of gender, sexual orientation, race, ethnicity, religion, age, and disability
    status is a well-documented form of harm in LLMs [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922).
    Internet-based datasets encode bias and harmful stereotypes for different reasons.
    The first is that these associations are largely a reflection of the characteristics
    found in the training data. Here, as the LLM learns the characteristics and patterns
    of a language in order to generate human-like text, it also inherits human-like
    prejudices, historical injustice, and cultural associations that can be harmful
    and offensive. The second is the lack of diversity in training data. The dataset
    can be biased because some communities may be better represented than others,
    and the dataset may not be broadly representative of how different groups of people
    view the world. The third is that developing and changing social views can result
    in LLMs misrepresenting social movements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着性别、性取向、种族、民族、宗教、年龄和残疾状态等路线持续有害的刻板印象和歧视性语言是LLMs中已记录的一种伤害形式 [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)。基于互联网的数据集由于不同的原因编码了偏见和有害的刻板印象。首先，这些联系在很大程度上是训练数据中发现的特征的反映。在这里，当LLM为了生成类似人类的文本而学习语言的特性和模式时，它也继承了类似人类的偏见、历史不公和文化联系，这些可能是有害和冒犯性的。其次，训练数据缺乏多样性。数据集可能存在偏见，因为某些社区可能比其他社区有更好的代表性，而且数据集可能不能广泛地代表不同群体如何看待世界。第三，社会观点的发展和变化可能导致LLMs错误地代表社会运动。
- en: 'In chapter 1, we briefly discussed how word embeddings mirror the inequities
    that exist in society. In an early study of bias in word embeddings, the authors
    considered NLP applications that use word embeddings to determine this potential
    effect [[6]](https://doi.org/10.1126/science.aal4230). First, they looked at sentiment
    analysis, which classifies text as positive, negative, or neutral. The task was
    calculating a sentiment score for movie reviews, which can be helpful for marketing
    purposes. Their results showed that movie reviews containing European American
    names had more positive sentiment scores on average in comparison to those with
    African American names, even when the reviews were otherwise similar; that is,
    the sentiment scores exhibited racial bias for character and actor names in the
    movie reviews. Next, they looked at machine translation where they concluded that
    translations from many gender-neutral languages to English result in gender-stereotyped
    sentences. In their paper, they show how Google Translate converts Turkish sentences
    with genderless pronouns to English: *“O bir doktor. O bir hemşire.”* to “He is
    a doctor. She is a nurse.”'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们简要讨论了词嵌入如何反映社会中存在的差异。在一项关于词嵌入偏见的早期研究中，作者考虑了使用词嵌入来确定这种潜在影响的 NLP 应用 [[6]](https://doi.org/10.1126/science.aal4230)。首先，他们研究了情感分析，它将文本分类为积极、消极或中性。任务是计算电影评论的情感得分，这有助于营销目的。他们的结果显示，与非洲裔美国人名字相比，包含欧洲裔美国人名字的电影评论平均具有更积极的情感得分，即使评论在其他方面相似；也就是说，情感得分在电影评论中表现出对角色和演员名字的种族偏见。接下来，他们研究了机器翻译，他们得出结论，从许多性别中性的语言翻译成英语会导致性别刻板印象的句子。在他们的论文中，他们展示了
    Google 翻译如何将土耳其语中无性别代词的句子翻译成英语：“*O bir doktor. O bir hemşire.*” 到 “He is a doctor.
    She is a nurse.”
- en: Similarly, LLMs not only reinforce stereotypes but also amplify them. In a study
    exploring religious bias in language models, authors determined that OpenAI’s
    GPT-3 captures Muslim-violence bias, as well as anti-Semitic bias [[7]](https://arxiv.org/pdf/2101.05783.pdf).
    They show that prompts including the word “Muslim” yield text that maps to “terrorist”
    23% of the time, while “Jewish” maps to “money” 5% of the time. They further show
    that replacing “Muslim” in the prompt with other religious groups significantly
    reduces GPT-3 from including violence-related keywords and phrases.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，LLMs 不仅强化了刻板印象，还放大了它们。在一项探讨语言模型中宗教偏见的研究中，作者确定 OpenAI 的 GPT-3 捕获了穆斯林-暴力偏见以及反犹太偏见
    [[7]](https://arxiv.org/pdf/2101.05783.pdf)。他们表明，包含“穆斯林”一词的提示词中有 23% 的时间会映射到“恐怖分子”，而“犹太人”有
    5% 的时间会映射到“金钱”。他们进一步表明，将提示词中的“穆斯林”替换为其他宗教群体可以显著减少 GPT-3 包含与暴力相关的关键词和短语。
- en: Discriminatory gender, race, profession, and religion biases are also exaggerated
    in LLMs. In fictional stories generated by GPT-3, it was found that feminine characters
    were described as less powerful when compared to masculine characters, as well
    as more likely to be associated with family and appearance [[8]](https://aclanthology.org/2021.nuse-1.5.pdf).
    Other LLMs, such as BERT and GPT-2, also demonstrate strong stereotypical biases.
    For example, attribute words for *Africa* were found to be *poor* and *dark*,
    whereas attribute words for a *software developer* were *geek* and *nerd* [[9]](https://aclanthology.org/2021.acl-long.416.pdf).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）中，歧视性的性别、种族、职业和宗教偏见也被夸大了。在由 GPT-3 生成的虚构故事中，发现女性角色与男性角色相比，被描述为力量较弱，以及更可能与家庭和外表相关
    [[8]](https://aclanthology.org/2021.nuse-1.5.pdf)。其他 LLMs，如 BERT 和 GPT-2，也表现出强烈的刻板印象偏见。例如，对
    *非洲* 的属性词被发现是 *贫穷* 和 *黑暗*，而 *软件开发者* 的属性词则是 *极客* 和 *书呆子* [[9]](https://aclanthology.org/2021.acl-long.416.pdf)。
- en: 'Now, let’s look at the second case for perpetuating bias in LLMs: the lack
    of diversity in the training dataset. As we’ve previously discussed, quantity
    isn’t quality. To holistically represent the views and values of different individuals
    or groups, the training dataset must be diverse and broadly representative of
    perspectives from distinct communities. In the paper, “On the Dangers of Stochastic
    Parrots: Can Language Models Be Too Big?,” the authors explore several factors
    where they determine that the voices of people aren’t equally represented in the
    training datasets for language models [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922).
    As we know, Reddit and Wikipedia are two widely used datasets for training LLMs.
    The authors discuss how 67% of Reddit users are men and 64% are between 18 and
    29 years old, while similarly, only 8.8% to 15% of Wikipedians are women or girls.
    They also discuss that the common practice of filtering out datasets, such as
    the Common Crawl dataset, further weakens the voices of underrepresented communities.
    For example, in the training for GPT-3, the Common Crawl dataset is filtered by
    finding documents similar to Reddit and Wikipedia datasets, which is then additionally
    filtered by removing any page that contains a list of 400 words related to sex,
    racial slurs, or white supremacy. The authors argue that while it may be an effective
    strategy for filtering out certain kinds of pornography and hate speech, it inadvertently
    also suppresses discourse for marginalized populations, such as LGBTQ people.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看LLMs中持续存在偏见的第二个案例：训练数据集中缺乏多样性。正如我们之前讨论的，数量并不等于质量。为了全面代表不同个人或群体的观点和价值观，训练数据集必须多样化，并且广泛地代表来自不同社区的观点。在论文“关于随机鹦鹉的危险：语言模型可以太大吗？”中，作者们探讨了几个因素，他们认为在这些因素中，人们的声音在语言模型的训练数据集中没有得到平等的代表
    [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)。正如我们所知，Reddit和维基百科是训练LLMs的两个广泛使用的数据集。作者们讨论了Reddit用户中有67%是男性，其中64%年龄在18至29岁之间，而类似地，只有8.8%至15%的维基百科编辑是女性或女孩。他们还讨论了过滤数据集的常见做法，例如Common
    Crawl数据集，这进一步削弱了代表性不足的社区的声音。例如，在GPT-3的训练中，Common Crawl数据集通过寻找与Reddit和维基百科数据集相似的文档进行过滤，然后通过移除包含与性、种族诽谤或白人至上主义相关的400个单词列表的任何页面进行额外的过滤。作者们认为，虽然这可能是一种有效过滤某些类型色情和仇恨言论的策略，但它无意中压制了边缘化群体，如LGBTQ人群的言论。
- en: The authors also discuss the challenges with ever-changing social movements
    where views can either be overrepresented or not captured at all in online discourse,
    which ultimately is the data that LLMs are trained on. In a specific example,
    researchers discovered that the “intensified documentation” on Wikipedia of the
    Black Lives Matter (BLM) movement reinforces BLM’s claims about police violence
    being a systematic problem in the United States [[10]](https://dl.acm.org/doi/pdf/10.1145/2998181.2998232).
    Before the movement brought new attention to the problem, Wikipedia data on police
    violence, made up of isolated cases, might have told a different story. This is,
    of course, especially a concern when training data isn’t frequently updated, which
    is likely not practical given how time-intensive and computationally expensive
    LLMs are to train.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们还讨论了随着社会运动的不断变化所带来的挑战，在这些运动中，观点可能在线讨论中被过度代表或根本未被捕捉到，而这最终是LLMs训练所依赖的数据。在具体的一个例子中，研究人员发现，维基百科上对“黑人的命也是命”（BLM）运动的“加强文档”强化了BLM关于警察暴力是美国系统性问题的主张
    [[10]](https://dl.acm.org/doi/pdf/10.1145/2998181.2998232)。在运动将新的关注点引向这个问题之前，关于警察暴力的维基百科数据，由孤立案例组成，可能讲述了一个不同的故事。当然，当训练数据不经常更新时，这尤其令人担忧，考虑到LLMs的训练既耗时又计算量大，这很可能是不切实际的。
- en: In a joint study from the University of Bath and Princeton University, researchers
    show why addressing bias in machine learning is a challenging problem [[6]](https://doi.org/10.1126/science.aal4230).
    First, they show that bias is identical to meaning, so it’s impossible to meaningfully
    use language without incorporating human bias. Second, they discuss how it’s equally
    impossible to algorithmically define bias because our societal understanding of
    it is constantly evolving and varies between cultures. Finally, they show how
    biases can also be a result of historical inequalities that may be important to
    represent in some contexts.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在巴斯大学和普林斯顿大学的一项联合研究中，研究人员展示了为什么解决机器学习中的偏见是一个具有挑战性的问题 [[6](https://doi.org/10.1126/science.aal4230)]。首先，他们表明偏见与意义相同，因此不包含人类偏见就无法有意义地使用语言。其次，他们讨论了为什么算法上定义偏见同样是不可能的，因为我们对它的社会理解是不断演变的，并且在不同文化之间存在差异。最后，他们展示了偏见也可能是历史不平等的结果，在某些情况下这可能很重要地表示出来。
- en: There have been efforts to debias word embeddings and language models, most
    commonly concerning gender. To reduce bias in word embeddings, you could change
    the representation of a gender-neutral word by removing their gender associations.
    For example, if we have the word *nurse*, which is more likely associated with
    *female*, it would be moved equally between *male* and *female* [[11]](https://arxiv.org/pdf/1607.06520.pdf).
    In 2022, a group of researchers surveyed five debiasing techniques for language
    models concerning gender, religious, and racial biases, where they determined
    that not only do current debiasing techniques not work as well for nongender biases,
    but they also result in a decrease in the ability to model language [[12]](https://arxiv.org/pdf/2110.08527.pdf).
    Although a noble effort, algorithmically eliminating bias from language models
    is extraordinarily difficult because it also removes meaning and information,
    giving the model an incomplete picture of our world and turning debiasing into
    “fairness through blindness” [[6]](https://doi.org/10.1126/science.aal4230).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除词嵌入和语言模型中的偏见，人们已经做出了努力，最常见的是关于性别。为了减少词嵌入中的偏见，你可以通过去除它们的性别关联来改变中性词的表示。例如，如果我们有“护士”这个词，它更可能与“女性”相关联，那么它将在“男性”和“女性”之间被平等地移动
    [[11](https://arxiv.org/pdf/1607.06520.pdf)]。在2022年，一组研究人员调查了针对性别、宗教和种族偏见的五种语言模型去偏技术，他们确定，不仅当前的去偏技术对非性别偏见的效果不佳，而且它们还导致了对语言建模能力的下降
    [[12](https://arxiv.org/pdf/2110.08527.pdf)]。尽管这是一个崇高的努力，但算法上从语言模型中消除偏见是极其困难的，因为它也消除了意义和信息，给模型提供了一个不完整的世界图景，将去偏变成了“盲目中的公平”
    [[6](https://doi.org/10.1126/science.aal4230)]。
- en: As argued by Bender and Gebru et. al, a concrete path forward is to curate and
    document training datasets for language models [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922).
    As of now, most LLMs are trained on a proprietary mixture of datasets, with sources
    not provided to end users. Documentation is critical for understanding the data
    characteristics, mitigating some of these risks, and allowing for potential accountability.
    We *can* build representative and unbiased datasets by budgeting for dataset documentation
    and only collecting as much data as can be documented. Hugging Face, a company
    focused on building open source machine learning tools, has developed dataset
    cards that are a good starting point for dataset documentation, including details
    about the dataset contents, any potential biases within the dataset, and context
    for how the dataset should be used [[13]](https://huggingface.co/docs/hub/datasets-cards).
    Hugging Face also released a search tool for ROOTS, a 1.6 TB multilingual text
    corpus, which was used to train BLOOM, an LLM [[14]](https://arxiv.org/pdf/2302.14035.pdf).
    To encourage researchers to characterize large datasets, the tool allows you to
    search through the dataset for qualitative analysis of training data. Similarly,
    founded through Berkman Klein Center’s Assembly fellowship at Harvard, the Data
    Nutrition Project takes inspiration from nutritional labels on food to highlight
    the key ingredients in a dataset, such as metadata and demographic representation
    (see [https://datanutrition.org/](https://datanutrition.org/)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如Bender和Gebru等人所论证，一个具体的路径是整理和记录语言模型的训练数据集 [[5]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)。截至目前，大多数大型语言模型（LLM）都是基于未向最终用户提供的专有数据集混合进行训练。文档记录对于理解数据特征、减轻一些风险以及允许潜在的责任归属至关重要。我们可以通过为数据集文档编制预算，只收集可以记录的数据，来构建具有代表性且无偏见的数据库。专注于构建开源机器学习工具的公司Hugging
    Face已经开发了数据集卡片，这是数据集文档的一个良好起点，包括数据集内容、数据集中存在的任何潜在偏见以及数据集应如何使用的背景信息 [[13]](https://huggingface.co/docs/hub/datasets-cards)。Hugging
    Face还发布了一个用于ROOTS的搜索工具，ROOTS是一个1.6TB的多语言文本语料库，用于训练BLOOM这个LLM [[14]](https://arxiv.org/pdf/2302.14035.pdf)。为了鼓励研究人员对大型数据集进行特征描述，该工具允许您在数据集中进行定性分析以研究训练数据。同样，通过哈佛大学Berkman
    Klein中心的Assembly奖学金项目成立的Data Nutrition项目，从食品的营养标签中汲取灵感，突出数据集中的关键成分，如元数据和人口代表性（参见[https://datanutrition.org/](https://datanutrition.org/))）。
- en: Finally, unlike AI, humans have context-specific memories and social examples
    to draw from that can be used to overcome racial and gender biases. Humans can
    fight their implicit biases and these biases, need not remain entrenched in our
    society forever.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与AI不同，人类有特定情境的记忆和社会例子可以借鉴，这些可以用来克服种族和性别偏见。人类可以对抗他们的隐性偏见，这些偏见不需要永远根植于我们的社会中。
- en: Sensitive information
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 敏感信息
- en: Because LLMs are trained on unfathomable amounts of data from a wide range of
    sources on the internet, they can sometimes contain personally identifiable information
    (PII), such as names, addresses, Social Security numbers, biometric data, sexual
    orientation, and so on, even if trained on public data. One potential risk is
    that the model could unintentionally “memorize” details from the data on which
    it’s trained; that is, sensitive information from the model could be reflected
    in its output. There are, naturally, additional concerns if a model trained on
    a proprietary dataset is made publicly available.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM是在来自互联网上广泛来源的难以想象的大量数据上训练的，它们有时可能包含个人可识别信息（PII），如姓名、地址、社会保障号码、生物识别数据、性取向等，即使是在公共数据上训练也是如此。一个潜在的风险是模型可能无意中“记住”它所训练的数据中的细节；也就是说，模型中的敏感信息可能反映在其输出中。如果基于专有数据集训练的模型被公开，自然会有额外的担忧。
- en: A massive vulnerability of LLMs is that an adversary can perform a *training
    data extraction attack* in which malicious actors can query the model to recover
    sensitive and identifiable information. As with most security and privacy studies,
    it’s important to consider the risks and ethics of performing attacks for research
    purposes, so publicly available and published work in this space is often limited.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的一个巨大漏洞是，对手可以执行*训练数据提取攻击*，恶意行为者可以查询模型以恢复敏感和可识别的信息。与大多数安全和隐私研究一样，考虑进行攻击以进行研究的风险和伦理问题很重要，因此在这个领域公开可用和发表的工作通常有限。
- en: Google, in collaboration with OpenAI, Apple, Stanford, Northeastern University,
    and Berkeley, demonstrated their “attack” on GPT-2 to show that it’s possible
    to extract sensitive pieces of training data that the model has inadvertently
    “memorized.” Here, the attackers can query a language model to extract *verbatim
    information* from the training data. The researchers note that the training data
    extraction attacks have the most potential for harm when a model that is trained
    on a proprietary dataset is made publicly available, but they acknowledge that
    performing an attack for research purposes on such a dataset could also have harmful
    consequences. With this in mind, they chose GPT-2 because the training dataset
    collection process is documented and only uses public internet sources. They were
    able to extract hundreds of verbatim pieces of information that include PII (names,
    phone numbers, email addresses), instant messaging conversations, code, and universally
    unique identifiers (UUIDs). Most of these examples were memorized even though
    they appeared very infrequently, as little as in a single document, in the training
    dataset, and larger models were found to be more vulnerable to these attacks than
    smaller models [[15]](https://arxiv.org/pdf/2012.07805.pdf). A different study,
    “The Secret Sharer,” shows that unintended memorization is persistent and hard
    to avoid for LLMs [[16]](https://arxiv.org/pdf/1802.08232.pdf). They demonstrated
    an attack on the Enron Email Dataset (see [http://mng.bz/K9AZ](http://mng.bz/K9AZ)),
    which contains half a million emails sent between Enron Corporation employees.
    The dataset was made public and posted online by the Federal Energy Regulatory
    Commission during its investigation. The researchers used the Enron Email Dataset
    to train a language model and show that they are effortlessly able to extract
    credit card and Social Security numbers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Google 与 OpenAI、Apple、斯坦福大学、东北大学和加州大学伯克利分校合作，展示了他们对 GPT-2 的“攻击”，以表明有可能提取模型无意中“记住”的敏感训练数据。在这里，攻击者可以向语言模型查询，从训练数据中提取*原文信息*。研究人员指出，当基于专有数据集训练的模型被公开时，训练数据提取攻击具有最大的潜在危害，但他们承认，在这样一个数据集上进行研究目的的攻击也可能产生有害后果。考虑到这一点，他们选择了
    GPT-2，因为其训练数据集收集过程有记录，并且仅使用公共互联网资源。他们能够提取数百条原文信息，包括个人身份信息（姓名、电话号码、电子邮件地址）、即时通讯对话、代码和通用唯一标识符（UUID）。尽管这些例子在训练数据集中出现的频率很低，甚至只有一份文档中出现过，但大多数例子仍然被记住，而且发现更大的模型比小模型更容易受到这些攻击[[15]](https://arxiv.org/pdf/2012.07805.pdf)。另一项研究“秘密分享者”表明，对于大型语言模型（LLM）来说，无意中的记忆是持续的，难以避免[[16]](https://arxiv.org/pdf/1802.08232.pdf)。他们演示了对安然电子邮件数据集的攻击（见[http://mng.bz/K9AZ](http://mng.bz/K9AZ)），该数据集包含安然公司员工之间发送的五十万封电子邮件。该数据集在联邦能源监管委员会调查期间被公开并在线发布。研究人员使用安然电子邮件数据集来训练语言模型，并表明他们能够轻松地提取信用卡号和社会安全号码。
- en: 'The most straightforward way to mitigate this problem is to make sure that
    models don’t train on any sensitive or PII data. This is extremely difficult to
    do in practice, however, and goes back to our earlier point of curating and documenting
    datasets for language models. Other solutions include *privacy-preserving* or
    *privacy-enhancing technologies* (PETs), which can help mitigate data privacy
    and security risks [[17]](https://royalsociety.org/-/media/policy/projects/privacy-enhancing-technologies/Protecting-privacy-in-practice.pdf).
    Some examples of PETs include methods for pseudonymization, obfuscation, sanitization,
    and data masking. An example of using these in practice is creating blocklists
    for possible sensitive sequences to filter out potentially private information
    from the training dataset. However, as demonstrated in “The Secret Sharer,” blocklisting
    is never a complete approach in security and won’t significantly reduce the effect
    of unintended memorization for any sequences that did appear. Differential privacy,
    an anonymization technique introduced in the early 2000s, is a popular PET that
    attempts to train via dataset without revealing details of any individual training
    samples. Here, the idea is to add statistical *noise* to obscure individual identities
    in a given dataset. But this technique also has its limitations because it won’t
    prevent memorization for content that isn’t repeated often in the dataset. In
    “Beyond Data: Reclaiming Human Rights at the Dawn of the Metaverse,” the author
    points out that PETs are not only highly technical, complex to use, expensive,
    and resource-intensive but also challenging for lawmakers and policymakers to
    audit or govern [[18]](https://books.google.com/books/about/Beyond_Data.xhtml?hl=&id=zJZuEAAAQBAJ).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 减缓此类问题的最直接方法是在实践中确保模型不训练任何敏感或PII（个人身份信息）数据。然而，这极为困难，并回到了我们之前提到的为语言模型整理和记录数据集的问题。其他解决方案包括*隐私保护*或*隐私增强技术*（PETs），这些技术可以帮助缓解数据隐私和安全风险
    [[17]](https://royalsociety.org/-/media/policy/projects/privacy-enhancing-technologies/Protecting-privacy-in-practice.pdf)。PETs的例子包括匿名化、混淆、净化和数据屏蔽的方法。在实践中使用这些技术的例子是为可能的敏感序列创建黑名单，以从训练数据集中过滤出可能包含的个人信息。然而，正如“秘密分享者”所展示的，黑名单永远不是一种完整的方法，并且不会显著减少任何已出现序列的意外记忆效果。2000年代初引入的差分隐私是一种流行的PET，它试图通过数据集进行训练而不透露任何单个训练样本的细节。在这里，想法是为给定数据集中的个体身份添加统计*噪声*以掩盖其身份。但这项技术也有其局限性，因为它无法防止对在数据集中不常重复的内容的记忆。在《超越数据：元宇宙黎明时的人权恢复》一书中，作者指出，PETs不仅技术含量高、使用复杂、昂贵且资源密集，而且对立法者和政策制定者来说，审计或监管也具有挑战性
    [[18]](https://books.google.com/books/about/Beyond_Data.xhtml?hl=&id=zJZuEAAAQBAJ)。
- en: Privacy-preserving or privacy-enhancing technologies (PET) are umbrella terms
    used for approaches that can help mitigate privacy and security risks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护或隐私增强技术（PET）是用于描述可以帮助缓解隐私和安全风险的方法的通用术语。
- en: Given the limitations of current PET approaches, we hope that the efforts to
    raise awareness of this challenge will encourage researchers to develop new techniques
    to address this problem, as well as build on previous work to test unintended
    memorization from LLMs so we can respond to the problem appropriately.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到当前PET方法的局限性，我们希望提高对此挑战的认识将鼓励研究人员开发新技术来解决这个问题，并在此基础上测试LLMs（大型语言模型）的意外记忆，以便我们能够适当地应对这个问题。
- en: Summary
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs might be trained on a combination of open source or public datasets, datasets
    purchased from third-party vendors, datasets that companies collect themselves
    by scraping the web, or datasets that the companies create themselves by writing
    examples for the models to learn from.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可能是在开源或公共数据集、从第三方供应商购买的数据集、公司通过爬取网络自行收集的数据集，或公司通过为模型编写学习示例自行创建的数据集上训练的。
- en: '*Autoregressive* models refer to the fact that future predictions are dependent
    on the model’s past data. All the models in the GPT family, as well as Google’s
    PaLM, are autoregressive models trained to predict the next token given some input.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自回归*模型指的是未来预测依赖于模型的历史数据。GPT家族中的所有模型以及谷歌的PaLM都是自回归模型，这些模型被训练在给定一些输入的情况下预测下一个标记。'
- en: '*Zero-shot* and *few-shot* refer to the number of examples that the model is
    given before being asked to perform a task. They are primary examples of the emergent
    abilities of LLMs.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*零样本*和*少样本*指的是在要求模型执行任务之前，模型被给出的示例数量。它们是LLMs（大型语言模型）涌现能力的典型例子。'
- en: LLMs often encode and amplify stereotypical and derogatory associations; they
    also contain sensitive data, including personally identifiable information (PII).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）往往编码和放大刻板和贬义的联系；它们还包含敏感数据，包括个人可识别信息（PII）。
- en: A concrete path forward is to curate and document training datasets for language
    models, which is critical for understanding the data characteristics in order
    to mitigate risks and allow for potential accountability.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具体的途径是整理和记录语言模型的训练数据集，这对于理解数据特征以减轻风险和允许潜在的责任至关重要。
- en: An adversary can perform a *training data extraction attack* with an LLM in
    which malicious actors can query the model to recover sensitive and identifiable
    information.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击者可以使用LLM执行一种**训练数据提取攻击**，恶意行为者可以通过查询模型来恢复敏感和可识别的信息。
- en: '*Privacy-preserving* or *privacy-enhancing technologies* (PETs) can help mitigate
    data privacy and security risks. PETs have several limitations, and we hope to
    see concentrated efforts of researchers in this area so there are techniques that
    LLM developers can easily adopt.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私保护**或**增强隐私技术**（PETs）可以帮助缓解数据隐私和安全风险。PETs存在一些局限性，我们希望看到研究人员在这个领域的集中努力，以便有LLM开发者可以轻松采用的技巧。'
