- en: '11 Scaling up: Best practices for production deployment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Challenges and deployment options to consider for an application ready for production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production best practices covering scalability, latency, caching, and managed
    identities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability of LLM applications, with some practical examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMOps and how it compliments MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When organizations are ready to take their generative AI models from the realm
    of proof of concept (PoC) to the real world of production, they embark on a journey
    that requires careful consideration of key aspects. This chapter will discuss
    deployment and scaling options, sharing best practices for making generative AI
    solutions operational, reliable, performant, and secure.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and scaling generative AI models in a production setting is a complex
    task that requires meticulous consideration of various factors. While building
    a PoC can be a thrilling way to test an idea’s feasibility, taking it to production
    introduces a whole new realm of operational, technical, and business considerations.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on the key aspects developers must consider when deploying
    and scaling generative AI models in a production environment. We will discuss
    the operational criteria critical to monitoring the systems’ health, deployment
    options, and best practices for ensuring reliability, performance, and security.
  prefs: []
  type: TYPE_NORMAL
- en: We will also delve into the concepts of large language model operations (LLMOps)
    and machine learning operations (MLOps), which are essential and empowering for
    managing the lifecycle of generative AI models in production. Additionally, the
    chapter will underscore the importance of cost management and budgeting for models
    deployed in production and provide some enlightening case studies of successful
    deployment and scaling of generative AI models in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will experience a transformative journey of
    understanding the key considerations and best practices for deploying generative
    AI models to production. Let’s dive into this exciting world of knowledge by exploring
    some of the challenges most enterprises face when deploying a GenAI application
    to production.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Challenges for production deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI apps in an enterprise production environment face specific challenges
    that differ from those in conventional machine learning (ML). However, some of
    the challenges remain the same. For example, developers must deal with the complicated
    relationship of computational resource requirements, data quality standards, performance
    goals, the possibility of output variability, and the changing security situation
    around these powerful models.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary challenges in deploying generative AI models is their complexity.
    These models can be computationally intensive and require significant resources
    to train and deploy, even when factoring in today’s cloud-scale infrastructure
    and computing. Consequently, scaling the models to handle large volumes of requests
    or deploying them in resource-constrained environments can be difficult. Developers
    must carefully consider the hardware and software requirements of the models,
    as well as the infrastructure required to support them to ensure that they can
    be deployed and scaled effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge in deploying generative AI models is ensuring the quality
    and availability of data. A key aspect of the quality of data is also knowing
    the source of the data and whether it is an authoritative or authentic source,
    which is important. These models rely heavily on data quality and availability,
    and any problems with the data can significantly affect the models’ performance
    and accuracy. Developers must implement robust data validation and quality control
    processes and monitor the data sources and pipelines used to train and deploy
    the models to ensure the data is accurate, relevant, and current. This can be
    done by measuring accuracy with predictive performance metrics, relevance through
    task-specific evaluations, and currency by tracking data freshness. Enterprises
    should implement robust monitoring systems and document data lineage to maintain
    high data integrity standards. Chapter 12 covers evaluations and benchmarks in
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance and accuracy are also critical considerations when deploying
    generative AI models. Developers must carefully monitor the models’ performance
    and accuracy and implement regular testing and validation processes to ensure
    the models function as expected. In an ideal world, this requires a deep understanding
    of the models’ underlying algorithms and architectures and the ability to diagnose
    and resolve any problems that may arise. However, in a practical sense, most enterprises
    will have a cross-functional team of developers, data scientists, and business
    experts who will collectively help understand, guide, and model architecture and
    deployment considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability and availability are also key considerations in deploying generative
    AI models. These models must be reliable and available to meet the business’ needs,
    which requires careful consideration of factors such as redundancy, failover,
    and disaster recovery. Developers must implement robust monitoring and maintenance
    processes to ensure that the models function as expected and be prepared to respond
    quickly to any problems. Of course, most enterprises rely on the hyper-scaler
    they are using to provide much of this service. These services’ underlying reliability
    and availability are closely linked to those providers. With small language models
    (SLMs) also in the mix and being used with large language models (LLMs), the reliability
    and scale considerations are different, especially when considering edge deployments
    for SLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Security and compliance are also critical considerations. These models can process
    sensitive data, which must be protected from unauthorized access, theft, or misuse.
    Enterprises must ensure that the models comply with relevant regulations and standards,
    such as GDPR, HIPAA, or PCI-DSS, and implement robust security controls to protect
    the data and the models themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Companies must first know each regulation’s requirements to comply with these
    data protection regulations. This involves managing consent, securing sensitive
    information, and handling data breaches. They should track and control personal
    data used by LLMs, apply strong security measures, and include privacy in the
    system design from the beginning. Frequent compliance audits, employee training,
    and vendor management are important for maintaining standards. A good incident
    response plan for data breaches and careful record-keeping will help with compliance.
    Furthermore, using built-in compliance features of cloud services can assist in
    meeting these requirements. By keeping up with compliance standards and taking
    these steps, enterprises can use LLMs to match legal and regulatory obligations.
  prefs: []
  type: TYPE_NORMAL
- en: Cost management is another important consideration. Models can be expensive
    to deploy and maintain, particularly when it comes to computing, storage, and
    networking resources. Developers must carefully manage the costs associated with
    deploying and scaling the models and be prepared to make tradeoffs between cost
    and performance as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating existing systems and workflows is also critical in deploying generative
    AI models. These models often must be integrated with existing systems and workflows,
    which can be complex and time-consuming. Developers must ensure that the models
    are compatible with existing systems and can be easily integrated into existing
    workflows. They must also be prepared to work closely with other teams and stakeholders
    to ensure a smooth deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Human-in-the-loop considerations are another important factor. These models
    often require human intervention or oversight, particularly when they are used
    to make critical decisions or generate content that requires human review. Developers
    must ensure the models are designed with human-in-the-loop considerations and
    implement robust processes for managing and monitoring human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations are the final important factor in deploying generative
    AI models. These models can have significant ethical implications, particularly
    regarding bias, fairness, and transparency. Thus, developers must ensure that
    the models are designed and deployed ethically and must be prepared to address
    ethical concerns. Chapter 13 covers this topic in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'By understanding these challenges and considerations, developers can design
    and deploy generative AI models that are scalable, reliable, and secure and meet
    the business’ needs in a production environment. Several challenges and considerations
    must be addressed when deploying generative AI models in a production environment
    to ensure successful implementation. The following key points highlight these
    critical aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Complexity of generative AI models*—High computational requirements and significant
    resources are required for training and deployment. Consider hardware, software,
    and infrastructure for effective scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data quality and availability*—These are essential for model performance and
    accuracy. Implement robust data validation and quality control processes, and
    monitor data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model performance and accuracy*—Regular testing and validation are required.
    Cross-functional teams can aid in understanding and resolving problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reliability and availability*—Implement redundancy, failover, and disaster
    recovery. Use robust monitoring and maintenance processes. There is dependence
    on hyper-scalers for service reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security and compliance*—Protect sensitive data from unauthorized access.
    Ensure compliance with regulations such as GDPR, HIPAA, and PCI-DSS. Implement
    security controls, and manage data protection effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost management*—This involves careful management of computing, storage, and
    networking costs, balancing cost and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Integration with existing systems*—Ensure compatibility and smooth integration
    with current systems and workflows. Collaborate with other teams and stakeholders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human-in-the-loop considerations*—Design models with human oversight for critical
    decisions. Implement processes for managing human intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ethical considerations*—Address bias, fairness, and transparency problems.
    Ensure ethical design and deployment of models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.2 Deployment options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several options are available when deploying generative AI apps, with the best
    choice depending on factors such as model size and complexity, desired scalability
    and availability, and available infrastructure and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud deployment offers advantages such as scalability, diverse compute options,
    and managed services for easier deployment. However, consider potential ongoing
    costs, vendor lock-in, and data privacy concerns. On-premise deployment provides
    greater control, performance optimization, and data security, but it requires
    significant upfront investment and in-house expertise and may involve slower scaling.
    A hybrid approach combines both strengths, allowing sensitive data to remain on-premise,
    while using cloud scalability and introducing management complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the chosen deployment path, several core technologies facilitate
    the process. Containerization ensures consistent model execution across environments,
    while serverless functions are ideal for dynamic workloads. API gateways provide
    structured access for other applications to utilize models, and specialized GenAI
    platforms can streamline the deployment and management of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud deployment is popular due to its scalability and flexibility, particularly
    with providers such as Microsoft Azure, Amazon Web Services (AWS), and Google
    Cloud Platform (GCP). Depending on their needs, developers can choose from virtual
    machines, containers, or serverless functions. However, it’s crucial to carefully
    assess the required infrastructure and resources, including GPUs, memory, storage,
    and network bandwidth. Implementing load balancing and redundancy strategies ensures
    scalability and availability, while robust monitoring and automated testing are
    essential for maintaining performance and health.
  prefs: []
  type: TYPE_NORMAL
- en: By carefully considering these factors, developers can ensure reliable, scalable,
    and cost-effective deployment of generative AI Apps, regardless of the chosen
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Managed LLMs via API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the deployment options previously discussed, it’s important to
    note that some LLMs are only available via an API hosted online in a managed manner.
    This is often the case with cutting-edge models developed by AI research organizations
    or large tech companies. As we know, GenAI models require significant computational
    resources, making them difficult to run on-premise or in a hybrid manner. Table
    11.1 outlines some of the advantages of managed LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.1 Advantages of managed LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Advantages | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ease of use  | Managed LLMs via API are typically easy to use. Developers
    can send requests to the API and receive responses without worrying about the
    underlying infrastructure or model complexities.  |'
  prefs: []
  type: TYPE_TB
- en: '| Continuous updates  | The providers of these managed LLMs often continuously
    update and improve their models. An API allows you to take advantage of these
    improvements without manually updating your models.  |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability  | Managed LLMs via API can handle high volumes of requests and
    scale automatically based on demand, similar to other cloud-based services.  |'
  prefs: []
  type: TYPE_TB
- en: '| Model complexity  | LLMs are enormously complex ML models and can present
    several challenges for enterprises, particularly those without extensive experience
    in AI and ML. Managed services offload this complexity to the provider, exposing
    the inference via an API.  |'
  prefs: []
  type: TYPE_TB
- en: There are also some constraints and challenges to consider when using managed
    LLMs via an API, as outlined in table 11.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.2 Considerations with managed LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Considerations | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Cost  | The cost of using a managed LLM via API can vary significantly based
    on usage. While some providers offer free tiers, more extensive use can incur
    significant costs.  |'
  prefs: []
  type: TYPE_TB
- en: '| Dependency  | Using a managed LLM via API, you depend on the provider for
    the model and the infrastructure. If the provider experiences downtime or discontinues
    the service, this could affect your application.  |'
  prefs: []
  type: TYPE_TB
- en: '| Data privacy  | Data is sent to the provider’s servers for processing using
    a managed LLM via API, which can raise privacy concerns, especially regarding
    sensitive data.  |'
  prefs: []
  type: TYPE_TB
- en: '| Limited customization  | While managed LLMs via API offers ease of use, they
    typically offer limited customization options. You’re limited to the capabilities
    and configurations provided by the API and can’t modify the underlying model.  |'
  prefs: []
  type: TYPE_TB
- en: In summary, while managed LLMs via API offer several benefits, they also come
    with certain considerations. Whether they are the right option for your GenAI
    application depends on your needs and constraints. If you require a high level
    of customization, have strict data privacy requirements, or need to run your model
    offline, then an on-premise or hybrid deployment might be more suitable. However,
    a managed LLM via API could be a good choice if you value ease of use, continuous
    updates, and automatic scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4 Best practices for production deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use GenAI applications, a comprehensive approach is required that involves
    careful planning and execution to ensure scalability, reliability, and security.
    When using LLMs in your application, you need to think about aspects such as LLMOps,
    observability, and tooling to handle the lifecycle of your application effectively.
    In addition, you need to consider other aspects such as model serving and management,
    reliability and performance considerations, and security and compliance considerations.
    These areas are important to ensuring that the application does what it should
    and follows high reliability, security, and compliance standards.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn about many of these aspects, such as metrics
    for LLM inference, how to measure and understand latency for LLMs, scalability,
    inference options for LLMs, quotas and rate limits, and observability. It will
    provide you with a complete guide to help you scale the GenAI application in production.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.1 Metrics for LLM inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most important metrics from the production deployment perspective
    is related to LLM inference. This is the main area that we all work on and deal
    with when developing GenAI applications. As we have seen, LLMs produce text in
    two steps: the prompt, where the input tokens are processed at once, and decoding,
    where text is created one token at a time sequentially. Each created token is
    added to the input and used again by the model to create the next token. Generation
    ends when the LLM produces a special stop token or when a user-defined condition
    is satisfied (e.g., a maximum number of tokens has been produced).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding and managing key operational metrics related to LLM inference
    becomes critical. Many of these metrics are new and still too early for most users
    to be comfortable with, but the following four metrics are particularly important:
    time to the first token, time per output token, latency, and throughput. Table
    11.3 outlines the definition and importance of these operational criteria. Later
    in the chapter, you will see how to measure this on our LLM deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.3 LLM inference metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Metric | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Time to first token (TTFT)  | Measures the time it takes for the model to
    generate the first token after a user query. Lower TTFT means a more responsive
    user experience. TTFT is influenced by the time required to process the prompt
    and generate the first output token.  |'
  prefs: []
  type: TYPE_TB
- en: '| Time-per-output token (TPOT)  | Calculates the time required for the model
    to generate one token for a specific query. Lower TPOT means faster text generation.
    The model size, the hardware configuration, and the decoding algorithm influence
    TPOT.  |'
  prefs: []
  type: TYPE_TB
- en: '| Latency  | This metric measures the time it takes for data to move from its
    starting point to its destination. In the case of LLMs, it is the time for the
    model to generate a response to the user. The model and the tokens generated influence
    LLMs’ latency. Generally, most of the time is spent generating complete tokens,
    which are generated one at a time. The longer the generation, the higher the latency.  |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput  | Measures the amount of data that can be transferred in a unit
    of time. In this case, the number of output tokens per second on a deployment
    unit can be served across all requests.  |'
  prefs: []
  type: TYPE_TB
- en: '| Request per second (RPS)  | RPS measures the throughput of LLMs in production
    and indicates the number of requests an LLM can handle every second. This metric
    is crucial for understanding the scalability and efficiency of LLMs when deployed
    in real-world applications.  |'
  prefs: []
  type: TYPE_TB
- en: Note  RPS and throughput are often used interchangeably in the context of performance
    metrics, but they can have nuanced differences. In essence, while RPS is about
    the incoming load, throughput is about the server’s output or the successful handling
    of that load. A high throughput with a high RPS indicates a well-performing server,
    while a low throughput with a high RPS might suggest that the server is struggling
    to keep up with the demand.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.2 Latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Latency is a common metric used by almost everyone, but it is unclear and needs
    to be reexamined generative AI. The usual definition of latency does not fit well,
    as those APIs only gave back one result instead of multiple streaming responses.
    Because output generation depends greatly on input, GenAI has different latency
    points to consider. For instance, one latency is the first token latency; another
    is the full end-to-end latency after all the generation is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can’t rely on the second end-to-end latency alone, as we now know prompt
    size and output token count are the key influencing factors. The generation varies
    with the query (i.e., the prompt)—it is not a useful metric unless we compare
    similar tokens. For example, the following two require different amounts of computation
    and time, even when the input tokens are roughly the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Example 1*—Generate a three-verse poem on why dogs are amazing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Example 2*—Generate a three-page poem on why dogs are amazing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first example has 11 tokens, and the second one has 10 tokens when using
    the `cl100kbase` tokenizer (used by the newer GPT models). However, the generated
    tokens are very different. Also, as previously described, the time-per-output
    token (TPOT) does not consider the input prompt. The input prompt is also large
    for many tasks such as summarization because retrieval-augmented generation (RAG)
    is used for in-context information. Thus, using TPOT as a way of measuring latency
    is not precise.
  prefs: []
  type: TYPE_NORMAL
- en: The model size also affects resource usage; a smaller model is usually more
    efficient and uses fewer resources, while a larger model is more capable and powerful
    but takes much more time. Let’s use an example to show how to measure this.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows a simple method for measuring the latency of the
    Azure OpenAI Chat API. Unlike the previous examples, which use software development
    kits (SDK), this one uses the REST APIs, and hence, we have to construct the payload
    and call the POST methods. We choose the number of requests to simulate and have
    a main function that employs a `ThreadPoolExecutor` to send several API requests
    simultaneously. It passes the `call_api_and_measure_latency()` function to the
    executor for each simulated request, gathers the latencies of all the requests,
    computes the average latency, and displays it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.1 Measuring latency
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Setting Azure OpenAI Chat API endpoint and API key'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the payload, including the model details to use'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 We stream the response so we can start getting the response faster.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Function to call the Azure OpenAI Chat API and measure latency'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Start time used to calculate latency'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 End time used to calculate latency'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Number of requests to simulate'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Simulates concurrent API calls'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Calculates and print latency metrics'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 shows an example of the output executed with 50 iterations and an
    average latency of 11.35 seconds on a pay-as-you-go (PAYGO) instance. This is
    the round-trip call from the client to the service, not the latency of the service
    itself. This isn’t great, and for most production workloads, we need to look at
    the reserved capacity, which we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 Azure OpenAI latency example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As shown in figure 11.2, in this example, we can use Azure’s out-of-the-box
    features to get service metrics such as latency. Using the default metric options,
    we see an average latency on this PAYGO instance of 95.37 milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 Azure requests and latency average
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  The code we saw before is a basic example showing us how to measure latency
    and a view from a production perspective; it is not a good implementation for
    load testing latency, especially if one is not using PAYGO. A better approach
    is to use a script with OSS tools such as Apache JMeter ([https://jmeter.apache.org](https://jmeter.apache.org))
    or Locust ([https://locust.io](https://locust.io)).
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.3 Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main scaling options an enterprise should consider when deploying
    a production application that uses an LLM, such as Azure OpenAI, is provisioned
    throughput units (PTUs). PTUs for Azure OpenAI are units of model processing capacity
    that you can reserve and deploy for processing prompts and generating completions.
    They embody a normalized way of representing the throughput for your deployment,
    with each model–version pair requiring different amounts for deployment and throughput
    per PTU. The throughput per PTU can differ based on the model type and version,
    and it’s important to know this to scale your application well.
  prefs: []
  type: TYPE_NORMAL
- en: A PTU is essentially the same as a reserved instance that other Azure services
    have, but it is only a feature of Azure’s OpenAI service. When an application
    needs to scale and uses multiple AI services, the reserved instance capacity must
    be considered across all of those services, as there isn’t a universal service
    that reserves capacity for a specific application or subscription.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy a model in Azure OpenAI using PTUs, we must select the “provisioned-managed”
    deployment type and indicate how many PTUs are required for the workload, as shown
    in figure 11.3\. We also need to calculate the size of our specific workload shapes,
    which you can do with the Azure OpenAI Capacity calculator. This calculation helps
    determine the right number of PTUs for your deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 PTU deployment options on Azure OpenAI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In addition to PTUs, enterprises can utilize a PAYGO model, which uses tokens
    per minute (TPM) consumed on demand. This model can be combined with PTUs to optimize
    utilization and cost. Furthermore, API Management (APIM) can be used with Azure
    OpenAI to manage and implement policies for queuing, rate throttling, error handling,
    and usage quotas.
  prefs: []
  type: TYPE_NORMAL
- en: 'By running the same latency tests performed for PAYGO on the PTU instance with
    slight modifications, we get the following results across both when using GPT-4
    and the same model version. We randomly pick a prompt from a list to call and
    loop through 100 iterations in each case. An average of 2.9 seconds of end-to-end
    latency on PTUs is pretty decent compared to 6.3 seconds on PAYGO, which is not
    bad but not great:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code in listing 11.2 shows the difference. This function iterates over the
    two OpenAI clients and their corresponding models. A `ThreadPoolExecutor` with
    20 workers is created for each client–model pair, and tasks are submitted. Each
    task is a call to the `call_completion_api()` function (a wrapper around the Azure
    OpenAI completion API) with a randomly chosen input from the test inputs. It collects
    the latencies of all the tasks, calculates the median, average, minimum, and maximum
    latency, and prints these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.2 Measuring latency between PAYGO and PTU
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 11.4.4 PAYGO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The PAYGO model with TPM is a flexible payment method that lets you pay only
    for the resources you use. The method is especially helpful for applications that
    have changing usage patterns and do not need constant processing capacity. It
    is the standard for most customers and applications across most providers. TPM
    is the measure of the model’s processing power. When you send a request to the
    model, it uses a certain number of tokens based on the prompt and the response’s
    complexity and length. We are billed for each token consumed, so as the usage
    increases, you pay more, and if it decreases, you pay less.
  prefs: []
  type: TYPE_NORMAL
- en: Most cloud-based LLMs have a quota management feature that lets you assign rate
    limits to your deployments up to a global limit. Similarly, deployment and rate
    limits are associated with a model deployment. We can also assign a certain TPM
    to a specific deployment; when we do that, the available quota for that model
    will be reduced by that amount.
  prefs: []
  type: TYPE_NORMAL
- en: The PAYGO model is advantageous for scaling because it allows you to distribute
    TPM globally within a subscription and region, providing the flexibility to manage
    the allocation of rate limits across the deployments within your subscription.
    This model is ideal for applications with peak times of high usage followed by
    periods of low or no usage, as it ensures you only pay for what you use.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.5 Quotas and rate limits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quotas and rate limits are two mechanisms used in cloud services to manage and
    control resource usage. Quotas are the total amount of a resource that a user
    or service can consume over a specified period, such as a day or a month. They
    act as a cap on usage to prevent overconsumption of resources and ensure fair
    distribution among users.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, rate limits control the frequency of requests to a service. They
    are typically defined as the number of requests that can be made per second or
    minute. By limiting the rate at which users can make requests, rate limits help
    manage load and avoid overloading systems.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, quotas refer to the quantity of resources you can use, while rate
    limits refer to the frequency of access to those resources. Understanding both
    is crucial for efficient API management and avoiding service disruptions for enterprises.
    By adhering to rate limits, enterprises can ensure their applications do not send
    more requests than a service can handle at a given time, which helps maintain
    performance and stability. Meanwhile, by staying within quotas, they can control
    their costs and prevent unexpected overages.
  prefs: []
  type: TYPE_NORMAL
- en: Quotas for the OpenAI service, particularly for Azure OpenAI, are defined as
    limits on the resources or computational capacity a user or organization can consume.
    These quotas are typically measured in TPM and assigned on a per-region, per-model
    basis. The quotas ensure that the service can maintain consistent and predictable
    performance for all users.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprises should think about these quotas as a way to manage their usage and
    costs effectively. They must monitor their consumption to avoid exceeding these
    limits, which could lead to additional charges or service interruptions. It’s
    also important for enterprises to understand the rate limits associated with their
    deployments and plan accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if an enterprise has a quota of 240,000 TPM for a specific model
    in a region, it could create one deployment of 240K TPM, two of 120K TPM each,
    or multiple deployments adding up to less than 240K TPM in that region. For example,
    figure 11.4 outlines the quota setting for a specific Azure OpenAI endpoint and
    the various models deployed.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 Azure OpenAI model quota setting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: OpenAI has its system of quotas, but they are structured differently. OpenAI’s
    quotas are typically related to usage limits that are set based on the billing
    information provided by the user. Once billing information is entered, users have
    an approved usage limit of a set amount per month (the default is $100), which
    can automatically increase as usage on the platform grows. Users move from one
    usage tier to another, as shown in figure 11.5\. Users can review their current
    usage limit in the account settings under the limits page.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 OpenAI quota tiers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These quotas are designed to help manage and predict costs and prevent resource
    overuse. Enterprises should monitor their usage closely to ensure they stay within
    these limits and understand how these limits can scale with increased usage.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.6 Managing quota
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Managing quotas effectively is crucial for maintaining consistent and predictable
    application performance. Here are some best practices to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Understand your limits*. Familiarize yourself with the default quotas and
    limits that apply to the models, as each model and region can have different default
    quota limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monitor your usage*. Implement monitoring strategies to keep track of your
    usage against the assigned quotas. This will help you avoid unexpected throttling
    and ensure a good customer experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implement retry logic*. In your application, include retry logic to handle
    rate limit errors. This will allow your application to wait and retry the request
    after a brief pause rather than failing outright. A simple way to do this is to
    use the `Tenacity` library (an OSS library):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Avoid sharp workload changes*. Gradually increase your workload to prevent
    sudden spikes that could lead to throttling. Test different load increase patterns
    to find the most efficient approach for your application. Note that throttling
    intentionally slows down or limits the requests an app or service can handle over
    a certain period. The server or service provider usually enforces this to prevent
    system overloads, ensure fair usage, and maintain quality of service. As we know,
    throttling is a common practice in API management and cloud-based services to
    manage resources efficiently and protect the system from potential abuse or denial
    of service (DoS) attacks. It’s also used to prevent a single user or service from
    consuming all available resources and affecting the performance of other users
    or services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Manage TPM allocation*. Use the quota management feature to increase TPM on
    deployments with high traffic and reduce TPM on deployments with limited needs.
    This helps balance the load and optimize resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Request quota increases*. If you consistently exceed your quota limits, consider
    requesting an increase through the Azure portal or by contacting Microsoft support
    or your cloud provider for those not on Azure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Distribute requests evenly*. To avoid hitting the requests-per-minute (RPM)
    rate limit, distribute your requests evenly over time. Many cloud providers, including
    Azure OpenAI, evaluate incoming requests’ rates over a short period and may throttle
    if the RPM limit is surpassed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  With Azure OpenAI, you can combine PAYGO and PTUs to meet your workloads.
    This hybrid approach lets you use the flexibility of PAYGO for variable workloads,
    while having the reliability and consistency of PTUs for steady workloads. When
    you do this, PTUs are good for workloads with stable performance needs as they
    give you a fixed amount of throughput capacity that you reserve ahead of time,
    ensuring low latency variation. Furthermore, PAYGO is great for handling uncertain
    workloads where the usage can change. You’re charged based on the tokens used
    per minute, which means you pay more when your usage is high and less when it’s
    low.
  prefs: []
  type: TYPE_NORMAL
- en: By actively managing their quotas and rate limits, enterprises can ensure they
    have the necessary capacity for their applications, while controlling costs and
    maintaining service availability.
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.7 Observability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Observability for LLM applications refers to monitoring, logging, and tracing
    to ensure the application works as intended and fixes problems when they occur.
    Let’s examine each one in a little more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Monitoring*—Measure key performance indicators (KPIs) such as response times,
    throughput, error rates, and resource utilization. This data is essential for
    knowing the state of your application and making smart choices about scaling and
    optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logging*—Detailed logs should record requests and responses, including the
    input prompts and the model’s outputs. This information is priceless for debugging,
    understanding model behavior, and enhancing the user experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tracing*—Use tracing to track the route of requests through your application.
    This is especially important for applications with complex architectures or multiple
    models and services. Tracing helps locate bottlenecks and areas for optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we use MLflow, Traceloop, and Prompt flow to show
    you how to implement this. Let’s start with MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: MLFlow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MLflow is an open source platform that aims to manage the ML lifecycle, including
    experimentation, reproducibility, and deployment. It helps practitioners simplify
    their MLflow works with tools for tracking experiments, packaging code, and managing
    models. MLflow’s main components include tracking, model registry, and a server
    for deploying models, facilitating teamwork and innovation in ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow enhances the observability of LLMs by providing tools that streamline
    the deployment and monitoring process. It offers a unified interface for interacting
    with different LLM providers, simplifying the integration and management of models.
    MLflow’s platform-agnostic nature also facilitates seamless integrations and deployments
    across various cloud platforms, further aiding in the observability and management
    of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in listing 11.3, we use MLflow to achieve this. This basic console
    chat application uses Azure OpenAI and randomly uses a few prompts in the list
    `text_inputs`. We can set how many times to repeat this using multiple threads.
    When we call the chat completion API, we log various features to demonstrate how
    MLflow can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: We require that MLflow and Prometheus ([https://prometheus.io](https://prometheus.io))
    be installed and running at an endpoint to run this. In our case, we run this
    locally in a Docker container exposed at port 5000\. The docker-compose file is
    shown in the following listing. The book’s GitHub repository ([https://bit.ly/GenAIBook](https://bit.ly/GenAIBook))
    also has all the code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.3 docker-cmpose file for MLflow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by running the Docker container using the docker compose command,
    as shown: `docker compose up -d`. The `-d` parameter runs this as detached, which
    can be helpful and run in the background. As outlined in listing 11.4, we begin
    by specifying MLflow’s tracking URI (`http://localhost:5000`); this is the location
    where MLflow will store the data that we log and also assign a name for the experiment
    (`GenAI_ book`) so we can distinguish it from others. Of course, we are the sole
    users of this example since it runs locally. In addition, we need to install the
    following two dependencies for this to work: `– mlflow` and `colorama`. With conda,
    this can be installed using `conda install -c conda-forge mlflow colorama`, or
    with pip using `pip install mlflow colorama.`'
  prefs: []
  type: TYPE_NORMAL
- en: We measure features such as token count, prompts, conversation, and so forth.
    We also compute the time needed to receive a response and store it. We use the
    `mlflow.log_metrics()` function to store all these metrics. We also store the
    parameters used in the API request using the `mlflow.log_params()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.4 MLflow observability example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Logging this data allows you to compare different runs, examine your model’s
    performance, and see how parameter changes affect the output using the MLflow
    UI. Figure 11.6 shows an example of the information when we run multiple experiments
    and can contrast them.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 MLFlow experiments dashboard
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure 11.7 shows some metrics we have been monitoring: the `completion_tokens`
    and how they relate to the request latency when the `request_latency` is plotted.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 MLflow model metrics examples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 11.8 illustrates how we can also log some of the prompt details and the
    generated response, which is very useful for observability. Of course, this should
    be done carefully, depending on the privacy and legal implications of who can
    access this telemetry.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F08_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 MLflow prompt and response details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Traceloop and OpenLLMetry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traceloop ([https://www.traceloop.com/](https://www.traceloop.com/)) is an observability
    tool for monitoring LLM applications. It offers features such as real-time alerts
    and execution tracing to ensure quality deployment. OpenLLMetry, built on OpenTelemetry,
    is an open source extension maintained by Traceloop that enhances LLM observability.
    It integrates with Traceloop’s tools and adds LLM-specific monitoring capabilities,
    facilitating developers’ work with LLM observability, while aligning with OpenTelemetry
    standards.
  prefs: []
  type: TYPE_NORMAL
- en: OpenLLMetry extends OpenTelemetry’s functionality to cover generic operations
    such as database and API interactions and custom extensions for LLM-specific operations.
    This includes calls to LLM providers such as OpenAI or Anthropic and interactions
    with vector databases such as Chroma or Pinecone. In other words, OpenLLMetry
    offers a specialized toolkit for LLM applications, making it easier for developers
    to begin with observability in this domain, while still generating standard OpenTelemetry
    data that can be compatible with existing observability stacks.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating this with the existing application is quite simple. We need to install
    the Traceloop SDK (`pip` `install` `traceloop-sdk`). Next, we create a login and
    get an API key at [https://app.traceloop.com/](https://app.traceloop.com/). Initializing
    this is simple using `Traceloop.init()`, which instruments it automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.5 Using Traceloop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Traceloop also has multiple integration points into other systems and various
    LLM APIs. See [https://mng.bz/gAJx](https://mng.bz/gAJx) for more details. For
    our purposes, we’ll use the default dashboard for our example, as shown in figure
    11.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F09_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 Traceloop observability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Given that we can dig into various traces from an observability perspective,
    we get many details of the API calls (figure 11.10). In this example, we can see
    the system prompts, the user prompt, the completion, and other instrumentations,
    such as token usage. This can be a very powerful feature for many enterprise applications.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt flow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prompt flow is an open source set of tools and features from Microsoft. It improves
    the creation process of AI applications, especially those that use LLMs. It helps
    with the design, evaluation, and implementation stages of AI applications, providing
    a simple interface for developers to work with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F10_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 Traceloop observability example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prompt flow is a key feature for developers who want to use LLMs in enterprise
    applications, as it helps with both observability and LLMOps aspects. It lets
    developers build executable workflows that combine LLMs, prompts, and Python tools.
    This allows developers to find and fix errors and improve flows more easily, with
    the extra advantage of team collaboration features. Developers can create different
    prompt options, evaluate their effectiveness, and adjust the LLM’s performance
    as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt flow consists of four stages, as illustrated in figure 11.11\. The first
    stage, initialization, involves selecting a business use case, gathering a smaller
    dataset, and building a basic prompt and flow. Next, the experimentation stage
    requires testing and modifying the initial prompt until it reaches a good outcome.
    The third stage, evaluation and refinement, involves measuring the prompt’s quality
    and the flow’s performance on a larger dataset, with more adjustments and improvements
    made to achieve the desired output. Finally, the production stage involves launching
    the flow for production use, tracking usage, feedback, and any problems that may
    occur in a production setting.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F11_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 Prompt flow lifecycle
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prompt flow offers many benefits when an application moves from development
    to production. It helps the application work well with existing CI/CD pipelines
    and gives powerful version control and collaborative tools for scaling LLM applications.
    This complete environment allows developers to deploy LLM-powered applications
    with more confidence, supported by the ability to track and understand the model’s
    behavior in a live setting. Therefore, the prompt flow is a key part of the deployment
    strategy, ensuring that applications using LLMs are strong, dependable, and prepared
    for the production needs of the enterprise level. More details, including easy-start
    samples, can be found in Prompt flow’s GitHub repository at [https://github.com/microsoft/promptflow](https://github.com/microsoft/promptflow).
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Model serving involves deploying trained models to make predictions with
    new data. It’s a critical component for applications’ responsiveness and scalability.
    However, it demands significant investment in skills, computing resources across
    data centers, operational costs, and specialized hardware such as GPUs with InfiniBand
    connectivity. An open source software library such as vLLM could benefit organizations
    considering model serving. The efficient model hinges on scalable infrastructure,
    which can adjust resources for demand and ensure availability and cost-efficiency.
    Caching strategies and load balancing are key to reducing latency and evenly distributing
    requests. A solid update strategy, employing blue–green deployments, ensures smooth
    model transitions with minimal downtime. For more details on vLLM, see [https://www.vllm.ai/](https://www.vllm.ai/).
  prefs: []
  type: TYPE_NORMAL
- en: 11.4.8 Security and compliance considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Security and compliance are critical, especially when dealing with user data
    and potentially sensitive information. Adhering to best practices helps protect
    your users and ensures your application complies with relevant laws and regulations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data encryption*—Encrypt sensitive data at rest and in transit to protect
    against unauthorized access. Use secure protocols such as TLS for data in transit
    and utilize encryption features offered by your cloud provider for data at rest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Access control*—Implement strict access controls to ensure only authorized
    personnel can access production data and infrastructure. Use role-based access
    control (RBAC) and the principle of least privilege (PoLP) to minimize the risk
    of data breaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Compliance audits*—Regularly audit your application and its infrastructure
    for compliance with relevant regulations and standards, such as GDPR, HIPAA, or
    CCPA, depending on your application’s domain and geographical scope. This may
    involve conducting security assessments, vulnerability scanning, and compliance
    checks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Anomaly detection*—Deploy anomaly detection systems to monitor for unusual
    activity that could indicate a security breach or system misuse. This includes
    monitoring for abnormal usage patterns or unauthorized access attempts, allowing
    for rapid response to potential threats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure OpenAI Service offers many of these features as standard to meet enterprise
    readiness and compliance needs. As most enterprises demand, other cloud providers
    such as AWS and GCP have some versions of these controls.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5 GenAI operational considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Operational aspects of GenAI applications, particularly those utilizing LLMs
    such as GPT-4, are critical for ensuring smooth and efficient functioning of these
    systems. Understanding and managing key operational metrics such as tokens, latency,
    requests per second (RPS), and time to first byte (TTFB) are vital for optimizing
    performance, user experience, and cost. Let’s examine the definition, get a better
    understanding of the importance of these operational criteria, and explore how
    to measure and manage them effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.1 Reliability and performance considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Any production system, including the GenAI application, must be reliable and
    performant to meet the needs and expectations of your users. This means your system
    should be able to cope with different failures and scenarios. An API management
    or proxy system can assist you with many of these aspects, which we will discuss
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Monitoring tools*—Utilize monitoring tools and services to measure these operational
    metrics continuously. Tools such as Prometheus ([https://prometheus.io](https://prometheus.io))
    for metric collection and Grafana ([https://grafana.com](https://grafana.com))
    for visualization can provide real-time insights into your application’s performance.
    Cloud providers also offer native monitoring solutions that can be employed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance testing*—Regularly conduct performance testing to simulate various
    load conditions and measure how your application responds. Tools such as Apache
    JMeter ([https://jmeter.apache.org](https://jmeter.apache.org)) or Locust ([https://locust.io](https://locust.io))
    can simulate multiple users interacting with your application to assess its throughput
    and latency under stress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Optimization techniques*—Implementing effective optimization techniques is
    crucial for overall application performance, resource utilization, and user experience:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Token management*—Optimize the use of tokens by refining input prompts and
    responses. This can involve trimming unnecessary text, using more efficient encoding
    techniques, or customizing the model to produce shorter, more concise outputs
    without compromising quality.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Caching*—Implement caching strategies for frequently requested information
    to reduce latency and lower the computational load on your system. This is especially
    effective for static or rarely changing data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Load balancing and auto-scaling*—Use load balancers to distribute traffic
    evenly across your infrastructure, and implement auto-scaling to adjust resources
    dynamically based on demand. This helps maintain low latency and high RPS by ensuring
    your system can handle spikes in traffic without manual intervention.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost management*—Monitor and manage costs related to operational metrics,
    especially token usage, as this directly affects the cost of using LLM APIs. Implement
    quota systems or rate limiting if necessary to prevent unexpected spikes in usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By focusing on these operational aspects and continuously monitoring and optimizing
    based on real-world data, developers can ensure that their GenAI applications
    are functional but also efficient, scalable, and cost-effective. This holistic
    approach to operational management is crucial for the success of any application
    using the power of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5.2 Managed identities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Azure OpenAI has a key advantage over OpenAI or other LLM providers in terms
    of using managed identities for authentication. This method follows the best practices
    for enterprise production deployments, improving security and making credential
    management easier. Managed identities avoid the need to handle keys directly,
    lowering the chance of key exposure and simplifying the process of changing credentials.
    They also offer an automated way to authenticate services running on Azure with
    other Azure resources, using Azure Active Directory (AAD) for identity management
    (also known as Entra ID).
  prefs: []
  type: TYPE_NORMAL
- en: When using managed identities with Azure OpenAI, enterprises have a couple of
    authentication methods available—RBAC and Entra ID. The former allows for more
    complex security scenarios and involves assigning roles (e.g., user or contributor)
    to enable API calls without key-based authentication. Conversely, the latter is
    used to authenticate our OpenAI resource using a bearer token obtained through
    the Azure CLI. It requires a custom subdomain name and is suitable for applications
    running on Azure services such as VMs, function apps, and VM scale sets.
  prefs: []
  type: TYPE_NORMAL
- en: Managed identities offer several benefits over traditional key-based authentication
    methods, especially regarding security and management. Some of the key advantages
    are
  prefs: []
  type: TYPE_NORMAL
- en: '*No need to manage credentials*—Managed identities eliminate the need for developers
    to manage the secrets, credentials, certificates, and keys used to secure communication
    between services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automatic credential rotation*—System-assigned managed identities are tied
    to the lifecycle of the Azure resource, and Azure automatically handles the lifecycle
    of the credentials, including their rotation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Enhanced security*—Since credentials are not stored in the code, there’s a
    reduced risk of credential leaks. Managed identities also use AAD for authentication,
    which is more secure than storing and managing keys within your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Simplified access management*—Managed identities can be granted access to
    other Azure resources supporting Azure AD authentication, simplifying access management.
    Furthermore, user-assigned managed identities can be used by multiple resources,
    which can be particularly useful for complex environments and applications that
    need to scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These benefits contribute to a more secure and efficient environment for managing
    access to Azure resources, making managed identities a preferred choice for many
    enterprise scenarios. The following listing shows a simple example of implementing
    a managed identity using Azure OpenAI. Note that this might require installing
    the Azure Identity package, which can be done via pip`:` `pip` `install azure-identity`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.6 Using managed identities with Azure OpenAI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 11.5.3 Caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing caching when using OpenAI’s LLM in a production app is a strategic
    move to enhance performance and cost efficiency. Caching stores frequently requested
    data in a faster-access storage system, allowing for reduced latency, as repeated
    queries can be served swiftly. This improves user experience and minimizes operational
    costs by reducing the number of necessary API calls, often associated with fees.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, services typically impose rate limits to prevent excessive use, and
    caching helps us adhere to these limits while maintaining a responsive service.
    Regarding the best practices for caching with Redis, it’s crucial to design cache
    keys uniquely representing each request and its context. An effective invalidation
    strategy, such as setting a time-to-live (TTL) for keys, ensures the cache doesn’t
    serve outdated information.
  prefs: []
  type: TYPE_NORMAL
- en: The cache-aside pattern is a recommended approach where the application checks
    the cache first and, upon a miss, retrieves data from the source, updates the
    cache, and then returns the response. Monitoring your cache’s hit rates and performance
    metrics is essential to gauge its effectiveness and make necessary optimizations.
    It’s important to handle cache misses gracefully and ensure the application can
    operate correctly even when temporarily unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: We can illustrate how caching an LLM generation can benefit the application
    greatly in terms of cost and experience. However, we should not cache anything
    without a clear reason, hoping it will improve things, but consider it in the
    context of the use case and the related types of generations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our caching example, we will use Redis and build on that from our RAG implementation
    earlier in chapter 8\. Using the same Docker container, we will use the RedisVL
    library, a Python library designed for tasks like semantic search and real-time
    RAG pipelines. It provides an easy-to-use interface for vector-based searches
    and index management. RedisVL is built on the `redis-py` client and helps integrate
    Redis’ capabilities into AI-driven applications. We start by installing via pip:
    `pip` `install` `redisvl`.'
  prefs: []
  type: TYPE_NORMAL
- en: We continue by listing all the indexes in the Redis database, which only has
    one index, `posts`, from our RAG implementation earlier in chapter 8\.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rvl` `index` `listall` command to see all the indexes is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we initialize the cache, which is created if the cache does not exist.
    The cache initialization requires some parameters—the name (case sensitive), the
    prefix for the hash entries, the connection string (local in our case, as we are
    running it in Docker locally), and the distance threshold. The distance threshold
    can vary depending on the embedding code and the use case and can be changed on
    the fly.
  prefs: []
  type: TYPE_NORMAL
- en: Our function, `answer_question()`, takes a question and uses the `check()` method
    on the `llmcache` instance to search the question in the cache. If the cache has
    results, it gives back the response. If the cache is empty, it calls the `generate_response`
    function to get a response from the OpenAI client, which is then stored in the
    cache. Note that some of the code is skipped for simplicity. The following listing
    shows the whole thing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 11.7 Using Redis cache for OpenAI response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Index name'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Redis key prefix for hash entries'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Redis connection url string'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Semantic cache distance threshold'
  prefs: []
  type: TYPE_NORMAL
- en: When we run this, an example output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The TTL mechanism determines how long a piece of data should be stored in a
    cache before it’s considered stale and can be deleted. With Redis, once the TTL
    expires, the
  prefs: []
  type: TYPE_NORMAL
- en: 'cached data is automatically removed, ensuring that outdated information isn’t
    served to users. This helps maintain the freshness of the data being accessed
    by the application. This can be set as follows: `llmcache.set_ttl(5)` `# 5` `seconds`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH11_F12_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 Redis cache statistics for `GenAIBookCache`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can use the `rvl stats` command with the cache name as an argument to view
    the cache details. Figure 11.12 shows the output of this command: `rvl stats —i
    GenAIBookCache`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen the components we must consider when making a GenAI application
    scalable and operational. There is one more topic to cover: LLMOps and MLOps.
    These are not just for getting AI applications to work; they’re for doing so in
    a maintainable, ethical, and scalable way. This is why they are regarded as vital
    for any enterprise that wants to use AI technology well. Let’s explore them more
    closely.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.6 LLMOps and MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning operations (MLOps) apply DevOps principles and best practices
    to develop, deploy, and manage ML models and applications. MLOps aims to streamline
    the ML lifecycle, from data preparation and experimentation to model training
    and serving, while ensuring quality, reliability, and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps is a specialized domain within MLOps that focuses on the operational
    aspects of LLMs. LLMs are deep learning models that can generate natural language
    text and perform various natural language processing (NLP) tasks based on the
    input provided. Examples of LLMs include GPT-4, BERT, and similar advanced AI
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps introduces tools and best practices that help manage the lifecycle of
    LLMs and LLM-powered applications, such as prompt engineering, fine-tuning, deployment,
    monitoring, and governance. LLMOps also addresses the unique challenges and risks
    associated with LLMs, such as bias, hallucination, prompt injection, and ethical
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Both LLMOps and MLOps share some common goals and challenges, such as automating
    and orchestrating the ML pipeline; ensuring reproducibility, traceability, and
    versioning of data, code, models, and experiments; monitoring and optimizing the
    performance, availability, and resource utilization of models and applications
    in production; implementing security, privacy, and compliance measures to protect
    data and models from unauthorized access and misuse; and incorporating feedback
    loops and continuous improvement cycles to update and refine models and applications
    based on changing requirements and user behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, LLMOps and MLOps also have some distinct differences, and switching
    from MLOps to LLMOps is a paradigm shift—specifically in data, model complexity
    (including size), and model output in the context of generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data*—LLMs are pretrained on massive text datasets, such as the Common Crawl
    corpus, and can be adapted for specific use cases using prompt engineering and
    fine-tuning techniques. This reduces the need for extensive data collection and
    labeling and introduces the risk of data leakage and contamination from the pretraining
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Computational resources*—GenAI models, such as LLMs, are very large and complex,
    often consisting of billions of parameters and requiring specialized hardware
    and infrastructure to train and run, such as high-end GPUs, memory, and so forth.
    This poses significant challenges for model storage, distribution, inference,
    cost, and energy efficiency. This challenge is further amplified when we want
    to scale up to many users to handle incoming requests without compromising performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model generation*—LLMs are designed to generate coherent and contextually
    appropriate text rather than adhering to factual accuracy. This leads to various
    risks, such as bias amplification, hallucination, prompt injection, and ethical
    concerns. These risks require careful evaluation and mitigation strategies, such
    as responsible AI frameworks, human oversight, and explainability tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table 11.4 outlines key differences in the shift to LLMOps from MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.4 Differences between MLOps and LLMOps
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Area | Traditional MLOps | LLMOps |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Target audience  | ML engineers, data scientists  | Application developers,
    ML engineering, and data scientists  |'
  prefs: []
  type: TYPE_TB
- en: '| Components  | Model, data, inference environments, features  | LLMs, prompts,
    tokens, generations, APIs, embeddings, vector databases  |'
  prefs: []
  type: TYPE_TB
- en: '| Metrics  | Accuracy (F1 score, precision, recall, etc.)  | Quality (similarity),
    groundedness (accuracy), cost (tokens), latency, evaluations (Perplexity, BLEU,
    ROUGE, etc.)  |'
  prefs: []
  type: TYPE_TB
- en: '| Models  | Typically built from scratch  | Typically, prebuilt with inference
    via an API and multiple versions in production simultaneously  |'
  prefs: []
  type: TYPE_TB
- en: '| Ethical concerns  | Bias in training data  | Misuse and generation of harmful,
    fake, and biased output  |'
  prefs: []
  type: TYPE_TB
- en: Why LLMOps and MLOps?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLMOps and MLOps are key to the responsible and efficient deployment of LLMs
    and ML models, ensuring ethical and performance standards. They address problems
    such as slow development, inconsistent model quality, and high costs, while providing
    advantages such as speed, consistency, and risk management. LLMOps covers tools
    and practices for managing LLMs, including prompt engineering, fine-tuning, and
    governance, resulting in faster development, better quality, cost reduction, and
    risk control.
  prefs: []
  type: TYPE_NORMAL
- en: Given their complexity, effective management is critical for generative AI models’
    performance and cost efficiency. Important factors in LLMOps include model selection,
    deployment strategies, and version control. The right model size and configuration
    are essential, possibly customized to specific data. Options between cloud services
    and private infrastructure balance convenience and data security. Versioning and
    automated pipelines support smooth updates and rollbacks, enabling continuous
    integration and deployment. Adopting LLMOps ensures the successful, ethical use
    of generative AI, maximizing benefits and minimizing risks.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps and MLOps are crucial for the production deployment of AI applications.
    They provide the necessary infrastructure to ensure that AI applications are operational,
    sustainable, responsible, and capable of scaling according to user demand. For
    developers and technical professionals, these frameworks offer a way to maintain
    quality assurance, follow compliance and ethical standards, and cost-effectively
    manage AI applications. In an enterprise environment where reliability and scalability
    are vital, LLMOps and MLOps are essential for successfully integrating AI technology.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and telemetry systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While capable of delivering high-value business outcomes, powerful LLMs require
    careful monitoring and management to ensure optimal performance, accuracy, security,
    and user experience. Monitoring is an important part of LLMOps and MLOps, as it
    shows how well models and applications work in production. Continuous monitoring
    is vital for LLMOps, as for many production systems. It helps LLMOps teams solve
    problems quickly, ensuring the system is speedy and dependable. Monitoring covers
    performance metrics, such as response time, throughput, and resource utilization,
    enabling quick intervention if there are delays or performance declines. Telemetry
    tracking is crucial in this process, providing valuable insights into the model’s
    behavior and enabling continuous improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, ethical AI deployment must check for bias or harmful outputs. Using
    fairness-aware monitoring methods, LLMOps teams ensure that LLMs work ethically,
    minimizing unwanted biases and increasing user trust. Frequent model updates and
    maintenance, supported by automated pipelines, ensure that the LLM stays current
    with the latest developments and data trends, ensuring continued effectiveness
    and adaptability.
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 Checklist for production deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We covered many topics in this chapter. Before we end it, let’s summarize some
    of the advice into a simple checklist that can be handy as a reference guide when
    deploying applications to production. The following categories are the same as
    those described earlier in the chapter. Of course, as with most of this advice,
    this is incomplete and should be used as part of the wider set of responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Assess computational resources*—Determine your generative AI models’ hardware
    and software requirements and ensure the infrastructure can support them effectively.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quality and availability of data*—Implement robust data validation, quality
    control processes, and continuous monitoring to ensure data accuracy and relevance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model performance and reliability*—Set up regular testing and validation processes
    to monitor models’ performance. Plan for redundancy, failover, and disaster recovery
    to ensure high availability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security and compliance*—Apply encryption, access controls, and regular compliance
    audits. Ensure that your models adhere to regulations such as GDPR or HIPAA.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost management*—Closely monitor and manage the costs of deploying and maintaining
    your models. Be prepared to make tradeoffs between cost and performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System integration*—Ensure that the generative AI models can be easily integrated
    into existing systems and workflows.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human in the loop*—Design the models to include human oversight and intervention
    where necessary.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ethical considerations*—When deploying your models, address ethical implications,
    such as bias and fairness.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for production deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metrics for LLM inference*—Focus on key metrics such as time to first token
    (TTFT), time per output token (TPOT), latency, and throughput. Use tools such
    as MLflow to track these metrics.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Manage latency*—Understand different latency points, and measure them accurately.
    Consider the influence of prompt size and model size on latency.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scalability*—Utilize PTUs and PAYGO models to scale your application effectively.
    Use API management for queuing, rate throttling, and managing usage quotas.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quotas and rate limits*—Implement strategies to manage quotas and rate limits
    effectively, including understanding your limits, monitoring usage, and implementing
    retry logic.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Observability*—Use tools such as MLflow, Traceloop, and Prompt flow to monitor,
    log, and trace your application for improved performance and user experience.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security and compliance*—Encrypt data, control access, conduct compliance
    audits, and deploy anomaly detection systems.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMOps and MLOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adopt LLMOps and MLOps frameworks*—Ensure that your application follows best
    practices in LLMOps and MLOps for maintainable, ethical, and scalable AI solutions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monitoring and telemetry systems*—Use fairness-aware monitoring methods and
    telemetry tracking to ensure ethical AI deployment and continuous improvement
    of your models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI models are complex and resource intensive, requiring careful consideration
    of data quality, performance, security, cost, and ethical implications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For any production deployments, we must follow several best practices: monitor
    key metrics, optimize latency, ensure scalability, implement observability tools,
    prioritize security and compliance, and employ managed identities and caching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For observability, we implement monitoring, logging, and tracing tools such
    as MLflow, Traceloop, and Prompt flow to understand model behavior, diagnose problems,
    and improve user experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMOps is a specialized domain within MLOps that focuses on managing the unique
    challenges and risks of LLMs. Both share common goals such as automation, reproducibility,
    monitoring, and security but differ in data requirements, model complexity, and
    output characteristics. LLMOps addresses unique challenges such as bias, hallucination,
    and ethical concerns associated with LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
