<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span> </span> <span class="chapter-title-text">Counterfactuals and parallel worlds</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Motivating examples for counterfactual reasoning</li>
<li class="readable-text" id="p3">Turning counterfactual questions into symbolic form</li>
<li class="readable-text" id="p4">Building parallel world graphs for counterfactual reasoning</li>
<li class="readable-text" id="p5">Implementing the counterfactual inference algorithm</li>
<li class="readable-text" id="p6">Building counterfactual deep generative models of images</li>
</ul>
</div>
<div class="readable-text" id="p7">
<p>Marjani, a good friend of mine, once had to choose between two dating prospects at the same time. She had something of a mental score card for an ideal long-term match. She had good chemistry with one guy, but he didn’t rank well on the score card. In contrast, the second guy checked all the boxes, so she chose him. But after some time, despite him meeting all her criteria, she couldn’t muster any feelings for him. It was like a failed ritual summoning; the stars were perfectly aligned, but the summoned spirit never showed up. And so, as any of us would in that situation, she posed the <em>counterfactual question</em>: </p>
</div>
<div class="readable-text" id="p8">
<blockquote>
<div>
     I chose a partner based on my criteria and it’s not working out. Would it have worked out if I chose based on chemistry?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p9">
<p>Counterfactual queries like this describe hypothetical events that did not occur but could have occurred if something had been different. Counterfactuals are fundamental to how we define causality; if the answer to Marjani’s question is yes, it implies that choosing based on her score card <em>caused</em> her love life to be unsuccessful.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>Counterfactuals are core to the bread-and-butter question of causal effect inference, where we compare observed outcomes to “potential outcomes” that didn’t happen, like the outcome of Marjani’s love life if she chose a partner based on chemistry. More broadly, answering counterfactual questions is useful in learning policies for better decision-making. When some action leads to some outcome, and you ask how a different action might have led to a different outcome, a good answer can help you select better actions in the future. For example, after this experience, Marjani revised her score card to factor in chemistry when considering later romantic prospects.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>We’ll look at practical examples in this chapter, but I led with this love and romance example because it is universally relatable. It illustrates how fundamental counterfactual reasoning is to human cognition—our judgments about the world are fueled by our imagination of what could have been.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>Note that Marjani’s counterfactual reasoning involves a type of prediction. Like a Marvel superhero film, she is imagining a <em>parallel world</em> where she chose based on chemistry, and she’s <em>predicting</em> the outcome of her love life in that world. But statistical machine learning algorithms are better at making predictions than humans. That insight leads us to the prospect of building AI that automates human-like counterfactual reasoning with statistical machine learning tools.</p>
</div>
<div class="readable-text intended-text" id="p13">
<p>In this chapter, we’ll pursue that goal by learning to formalize counterfactual questions with probability. In the next chapter, we’ll implement a probabilistic counterfactual inference algorithm that can answer these questions. Let’s start by exploring some practical case studies that motivate algorithmic counterfactual reasoning.</p>
</div>
<div class="readable-text" id="p14">
<h2 class="readable-text-h2" id="sigil_toc_id_184"><span class="num-string">8.1</span> Motivating counterfactual reasoning</h2>
</div>
<div class="readable-text" id="p15">
<p>Here, I’ll introduce some case studies demonstrating the business value of answering counterfactual questions. I’ll then argue how they are useful for enhancing decision-making.</p>
</div>
<div class="readable-text" id="p16">
<h3 class="readable-text-h3" id="sigil_toc_id_185"><span class="num-string">8.1.1</span> Online gaming</h3>
</div>
<div class="readable-text" id="p17">
<p>Recall the online gaming example from chapter 7, where the amount of in-game purchases a player made was driven by their level of engagement in side-quests and whether they were in a guild. Suppose we observed an individual player who was highly engaged in side-quests and had many in-game purchases. A counterfactual question of interest might be, “What would their amount of in-game purchases be if their engagement was low?” </p>
</div>
<div class="readable-text" id="p18">
<h3 class="readable-text-h3" id="sigil_toc_id_186"><span class="num-string">8.1.2</span> The streaming wars</h3>
</div>
<div class="readable-text" id="p19">
<p>The intense competition amongst subscription streaming companies for a finite market of subscribers has been dubbed “the streaming wars.” Netflix is a dominant player with long experience in the space. It has learned to attract new subscribers by building blockbuster franchises from scratch, such as <em>House of Cards</em>, <em>Stranger Things,</em> and <em>Squid Game</em>.</p>
</div>
<div class="readable-text intended-text" id="p20">
<p>However, Netflix competes with Amazon, Apple, and Disney—companies with extremely deep pockets. They can compete with Netflix’s ability to build franchises from scratch by simply buying existing successful franchises (e.g., <em>Star Wars</em>) and making novel content within that franchise (e.g., <em>The Mandalorian</em>).</p>
</div>
<div class="readable-text intended-text" id="p21">
<p>Suppose that Disney is in talks to buy <em>James Bond</em>, the most valuable spy thriller franchise ever, and Netflix believes that a successful Bond deal may cause it to lose subscribers to Disney. Netflix hopes to prevent this by striking a deal with a famous showrunner to create a new spy-thriller franchise called <em>Dead Drop</em>. This new franchise would combine tried and true spy thriller tropes (e.g., gadgetry, exotic backdrops, car chases, over-the-top action sequences) with the complex characters, diverse representation, and emotionally compelling storylines characteristic of Netflix-produced shows. There is uncertainty about whether Netflix executives can close a deal with the candidate showrunner, as both parties would have to agree on creative control, budget, royalties, etc.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>Suppose the Bond deal succeeded, and Disney Plus now runs new series and films set in the “Bond-verse.” However, the <em>Dead Drop</em> deal fell through. Netflix then acquires data that identifies some subscribers who subsequently left Netflix, subscribed to Disney Plus, and went on to watch the new Bond content.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>A Netflix executive would be inclined to ask the following counterfactual: “Would those lost subscribers have stayed, had the <em>Dead Drop</em> deal succeeded?” Suppose the answer is “no,” because the Bond content was so strong an attraction that the <em>Dead Drop</em> deal outcome didn’t matter. In this case, the employees who failed to close the deal should not be blamed for losing subscribers. </p>
</div>
<div class="readable-text intended-text" id="p24">
<p>Or, suppose the <em>Dead Drop</em> deal succeeded, and Netflix subscribers can now watch the new <em>Dead Drop</em> franchise. “Would those subscribers who watch <em>Dead Drop</em> have left for the new Bond series on Disney, had the deal failed?” Again, if the answer is “no,” the employees who successfully closed the deal shouldn’t get credit for keeping all those subscribers. </p>
</div>
<div class="readable-text intended-text" id="p25">
<p>In both cases, answering these questions would help inform future deal-making decisions.</p>
</div>
<div class="readable-text" id="p26">
<h3 class="readable-text-h3" id="sigil_toc_id_187"><span class="num-string">8.1.3</span> Counterfactuals analysis of machine learning models</h3>
</div>
<div class="readable-text" id="p27">
<p>In this chapter, we are focusing on using a causal model to reason counterfactually about some data generating process. In machine learning, often the goal is a counterfactual analysis of a machine learning model itself; i.e., given some input features and some output predictions, how would the predictions have differed if the inputs were different? This counterfactual analysis supports explainable AI (XAI), AI fairness, and other tasks.</p>
</div>
<div class="readable-text" id="p28">
<h4 class="readable-text-h4 sigil_not_in_toc">Counterfactual analysis in classification</h4>
</div>
<div class="readable-text" id="p29">
<p>Consider the task of classification—a trained algorithm takes as input some set of features for a given example and produces a predicted class for that example. For example, given the details of a loan application, an algorithm classifies the application as “reject” or “approve.”</p>
</div>
<div class="readable-text intended-text" id="p30">
<p>Given a rejected application, a counterfactual question naturally arises: “Would the application have been approved if <em>some elements</em> of the application were different?” Often, the goal of the counterfactual analysis is to find a minimal change to the feature vector that corresponds to a change in the classification. Figure 8.1 illustrates this idea.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p31">
<img alt="figure" height="256" src="../Images/CH08_F01_Ness.png" width="620"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.1</span> What is the minimal change to the input feature that would have led to approval? In this case, the loan would have been approved if income were $20,000 higher.</h5>
</div>
<div class="readable-text" id="p32">
<p>Finding the minimal change that would have led to approval requires defining a distance metric in the feature space and then finding the feature value on the other side of the class boundary. In this example, the hypothetical condition “if the applicant had $20,000 more a year in income . . .” corresponds to the smallest change to the feature vector (in terms of distance on the decision surface) that would have led to approval. This type of analysis is useful for XAI; i.e., for understanding how features drive classification on a case-by-case basis.</p>
</div>
<div class="readable-text" id="p33">
<h4 class="readable-text-h4 sigil_not_in_toc">Counterfactual algorithmic recourse</h4>
</div>
<div class="readable-text" id="p34">
<p>Increasing salary by $20,000 is unrealistic for most loan applicants. That’s where counterfactual-based algorithmic recourse can be useful. <em>Algorithmic recourse</em> looks for the nearest hypothetical condition that would have led to a different classification. It operates under the constraint that the hypothetical condition was <em>achievable</em> or <em>actionable</em> by the applicant. Figure 8.2 shows how this works. </p>
</div>
<div class="readable-text intended-text" id="p35">
<p>In this example, the assumption is that increasing income by $5,000 <em>and</em> improving one’s credit score was achievable, according to some criteria (while increasing income by $20,000 was not).</p>
</div>
<div class="browsable-container figure-container" id="p36">
<img alt="figure" height="266" src="../Images/CH08_F02_Ness.png" width="742"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.2</span> In algorithmic recourse, we’re often interested in the nearest <em>actionable</em> feature vector on the other side of the decision boundary.<span class="aframe-location"/></h5>
</div>
<div class="readable-text" id="p37">
<p>Algorithmic recourse aims to give individuals subjected to machine learning–based decisions information that they can work with. If one fails an exam and asks why, an explanation of “because you are not a genius” is less useful than “because you didn’t review the practice exam,” even though both may be true. </p>
</div>
<div class="readable-text" id="p38">
<h4 class="readable-text-h4 sigil_not_in_toc">Counterfactual fairness</h4>
</div>
<div class="readable-text" id="p39">
<p>Counterfactual fairness analysis is a similar analysis that applies in cases where some of the input features correspond to attributes of a person. The idea is that certain attributes of an individual person should not, on ethical grounds, impact the classification. For example, it is unethical to use one’s ethnicity or gender in the decision to offer a loan. Even if such “protected attributes” are not explicitly coded into the input features, the classification algorithm may have learned proxies for protected attributes, such as the neighborhood where one lives, one’s social network, shopping habits, etc. It may make sense to have such features in the model, and it may not be obvious when those features behave as proxies for protected attributes. </p>
</div>
<div class="readable-text intended-text" id="p40">
<p>Figure 8.3 uses the loan algorithm example to illustrate how a counterfactual fairness analysis would ask counterfactual questions. In this case, the counterfactual question is “Would this person have been approved if they were of a different ethnicity?”<span class="aframe-location"/> The analyst would find features that are proxies for ethnicity and then see if a change to those proxies corresponding to a change in ethnicity would result in a classification of “approve.” Some techniques attempt to use this analysis during training to produce fairer algorithms.</p>
</div>
<div class="browsable-container figure-container" id="p41">
<img alt="figure" height="246" src="../Images/CH08_F03_Ness.png" width="615"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.3</span> For counterfactual fairness, suppose we want to test whether the algorithm has a bias against certain ethnicities. For a given feature vector, an ethnicity element, and a corresponding “reject” outcome, we test if the outcome would change if the ethnicity element changed.</h5>
</div>
<div class="readable-text intended-text" id="p42">
<p>While counterfactual fairness analysis is not enough to solve the broad problem of AI fairness, it is an essential element in the AI fairness toolkit.</p>
</div>
<div class="readable-text" id="p43">
<h3 class="readable-text-h3" id="sigil_toc_id_188"><span class="num-string">8.1.4</span> Regret and why do we care about what “would have happened?”</h3>
</div>
<div class="readable-text" id="p44">
<p>Traditional machine learning is usually forward-looking. Given data, you make a prediction, that prediction drives some decision to be made in the present, and that decision brings about some future cost or benefit. We want good predictions so we can get more future benefits. Imagine, for example, a machine learning algorithm that could accurately forecast the performance of a stock portfolio—that would obviously be quite valuable.</p>
</div>
<div class="readable-text intended-text" id="p45">
<p>Now, imagine a different algorithm that could accurately tell you how your portfolio would perform today if you had bought different stocks; that would certainly be less valuable than predicting the future. This contrast highlights a common criticism that modeling counterfactuals is backward-looking. For example, the counterfactual questions in our motivating case studies focus on decisions and outcomes that happened in the past. What’s done is done; getting the answers to such questions won’t change the past.</p>
</div>
<div class="readable-text intended-text" id="p46">
<p>But, first, not all counterfactuals are retrospective. In section 8.3 we’ll model questions like “What are the chances the subscriber would churn if you don’t send them a promotion and would not churn if you did send a promotion?” (“Churn” means to stop using a product or service within a certain time period.) That question has no past tense, does have business value, and is something we can model.</p>
</div>
<div class="readable-text intended-text" id="p47">
<p>Second, <em>retrospective</em> <em>counterfactuals help you understand how to make better decisions in the future</em>. Indeed, analyzing how your portfolio would have performed given different allocations—what investors call “backtesting”—is ideal for comparing various investment strategies. Similarly, the counterfactual insights from a failed <em>Dead Drop</em> deal might help Netflix executives make a deal with another famous showrunner.</p>
</div>
<div class="readable-text intended-text" id="p48">
<p>When we consider retrospective reasoning about things that <em>would have</em> or <em>could have</em> been, we arrive at the notion of <em>regret</em>. Regret is about retrospective counterfactual contrasts; given a choice, regret is a comparison between an outcome of the option you chose and an imagined counterfactual outcome of an option you rejected. In colloquial terms, regret is the bad feeling you get when the counterfactual outcome of an option you rejected is better than the option you chose. But cognitive science calls this <em>negative regret</em>; there is also <em>positive regret</em>, which is the good feeling you get when, upon comparing to imagined counterfactual outcomes, you realize you chose the better option (as in, “whew, I really dodged a bullet”).</p>
</div>
<div class="readable-text intended-text" id="p49">
<p>Regret can be useful for learning to make better decisions. Suppose you make a choice, you pay a cost (time, effort, resources, etc.), and it leads to an outcome. That gives you a baseline single point of data for learning. Now, suppose that, with the benefit of hindsight, you could imagine with 100% accuracy the outcome that would have occurred had you made a different choice. Now you have two comparable points of data for learning, and you only had to pay a cost for one of them.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>Usually your ability to imagine the counterfactual outcome of the rejected option is not 100% accurate. Even with the benefit of hindsight, there is still some uncertainty about the counterfactual outcome. But that’s no problem—we can model that uncertainty with probability. As long as hindsight knowledge provides you with some information about counterfactual outcomes, you can do better than the baseline of learning from a single point of data.</p>
</div>
<div class="readable-text intended-text" id="p51">
<p>In reinforcement learning and other automated decision-making, we often call our decision-making criteria “policies.” We can incorporate counterfactual analysis and regret in <em>evaluating</em> and <em>updating</em> <em>policies</em>.</p>
</div>
<div class="readable-text" id="p52">
<h3 class="readable-text-h3" id="sigil_toc_id_189"><span class="num-string">8.1.5</span> Reinforcement learning and automated decision-making</h3>
</div>
<div class="readable-text" id="p53">
<p>In automated decision-making, a “policy” is a function that takes in information about a decision problem and automatically selects some course of action. Reinforcement learning algorithms aim to find policies that optimize good outcomes over time.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>Automated counterfactual reasoning can credit good outcomes to the appropriate actions. In the investing example, we can imagine an algorithm that periodically backtests different portfolio allocation policies as more recent prices enter the data. Similarly, imagine we were writing a reinforcement learning (RL) algorithm to learn to play a game. We could have the algorithm use saved game instances to simulate how that game instance would have turned out differently if it had used a different policy. The algorithm can quantify the concept of regret by comparing those simulated outcomes to actual outcomes and using the results to learn a better policy. This would reduce the number of games the AI needed to learn a good policy, as well as enable it to learn from simulated conditions that don’t occur normally in the game. We’ll focus more on automated decision-making, bandits, and reinforcement learning in chapter 12.</p>
</div>
<div class="readable-text" id="p55">
<h3 class="readable-text-h3" id="sigil_toc_id_190"><span class="num-string">8.1.6</span> Steps to answering a counterfactual query</h3>
</div>
<div class="readable-text" id="p56">
<p>Across each of these applications, we can answer these counterfactual inference questions with the following workflow: </p>
</div>
<ol>
<li class="readable-text" id="p57"> <em>Pose the counterfactual question</em><em> </em>—Clearly articulate the counterfactual question(s) we want to pose in the simplest terms. </li>
<li class="readable-text" id="p58"> <em>Convert to a mathematical query</em><em> </em>—Convert the query to mathematical symbols so it is formal enough to apply mathematical or algorithmic analysis. </li>
<li class="readable-text" id="p59"> <em>Do inference</em><em> </em>—Run an inference algorithm that will generate an answer to the question. </li>
</ol>
<div class="readable-text" id="p60">
<p>In the following sections, we’ll focus on steps 1 and 2. In chapter 9, we’ll handle step 3 with an SCM-based algorithm for inferring the query we create in step 2. In chapter 10, we’ll see ways to do step 3 without an SCM but only data and a DAG.</p>
</div>
<div class="readable-text" id="p61">
<h2 class="readable-text-h2" id="sigil_toc_id_191"><span class="num-string">8.2</span> Symbolic representation of counterfactuals</h2>
</div>
<div class="readable-text" id="p62">
<p>In chapter 7, we saw the “counterfactual notation,” which uses subscripts to represent interventions. Now we are going to use this notation for counterfactual expressions. The trick is remembering, as we’ll see, that counterfactual queries are just a special type of <em>interventional</em> queries. We’ll see how interventional queries flow into counterfactual queries by revisiting our online gaming example.</p>
</div>
<div class="readable-text" id="p63">
<h3 class="readable-text-h3" id="sigil_toc_id_192"><span class="num-string">8.2.1</span> Hypothetical statements and questions</h3>
</div>
<div class="readable-text" id="p64">
<p>Consider our online gaming case study. When considering how much a player makes, we might say something like this:</p>
</div>
<div class="readable-text" id="p65">
<blockquote>
<div>
     The level of in-game purchases for a typical player would be more than $50.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p66">
<p>We’ll call this a <em>hypothetical statement.</em> In grammatical terms, I am using a <em>modal verb </em>(e.g., “would”, “could”, “should” in “would be more”) to intentionally mark <em>hypothetical language</em> rather than using <em>declarative language </em>(e.g., “is more” or “will be more”), which we use to make statements about objective facts.</p>
</div>
<div class="readable-text intended-text" id="p67">
<p>We want to formalize this statement in probability notation. For this statement, we’ll write <em>P</em><em> </em>(<em>I</em><em>  </em>&gt;<em>  </em>50)—recall that we used the random variable <em>I</em> to represent <em>In-Game Purchases</em>, <em>E</em> to represent <em>Side-Quest Engagement</em>, and <em>G</em> to represent <em>Guild Membership</em>.</p>
</div>
<div class="readable-text intended-text" id="p68">
<p>We’ll use hypothetical language in our open questions as well, like this:</p>
</div>
<div class="readable-text" id="p69">
<blockquote>
<div>
     What would be the amount of in-game purchases for a typical player?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p70">
<p>I am inquiring about the range of values the variable <em>I</em> could take, and I represent that with <em>P</em>(<em>I</em>).</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p71">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Declarative vs. hypothetical language and probability</h5>
</div>
<div class="readable-text" id="p72">
<p>Declarative language express certainty, as in “amount of in-game purchases is more than $50.” In contrast, hypothetical language is used for statements that convey conjecture, imagination, and supposition, as in “amount of in-game purchases <em>would be</em> more than $50.” </p>
</div>
<div class="readable-text" id="p73">
<p>Many of us learn to associate probability notation with declarative language, because of probability theory’s connection to propositional logic: <em>P</em>(<em>I</em>&gt;50) quantifies the probability that the declarative statement “amount of in-game purchases is more than $50” is true. But we are going to lean into the hypothetical language.</p>
</div>
<div class="readable-text" id="p74">
<p>Hypothetical language has an implicit lack of certainty—we are talking of things that <em>could be</em>, rather than things that <em>are</em>. Lack of certainty is equivalent to uncertainty, and the Bayesian philosophy we adopt in this book nudges us toward using probability to model uncertainty, so using hypothetical language will make it easier for us to formalize the question in probability notation. We’ll find this will help us formalize causal statements and questions.</p>
</div>
</div>
<div class="readable-text" id="p75">
<p>Note that the tense of the question or statement doesn’t matter when we map it to probabilistic notation. For example, we could have used this phrasing: </p>
</div>
<div class="readable-text" id="p76">
<blockquote>
<div>
     What would 
     <em>have been</em> the amount of in-game purchases for a typical player?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p77">
<p>Regardless of tense, we use the notation <em>P</em><em> </em>(<em>I</em><em>  </em>) to represent our uncertainty about a variable of interest in our question.</p>
</div>
<div class="readable-text" id="p78">
<h3 class="readable-text-h3" id="sigil_toc_id_193"><span class="num-string">8.2.2</span> Facts filter hypotheticals to a subpopulation</h3>
</div>
<div class="readable-text" id="p79">
<p>Suppose my statement was as follows:</p>
</div>
<div class="readable-text" id="p80">
<blockquote>
<div>
     The level of in-game purchases for a player with high side-quest engagement would be more than $50. 
     <em>P</em>( 
     <em>I</em>&gt;50| 
     <em>E</em>=“high”)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p81">
<p>Here, I am making a statement about a subset of players (those with high side-quest engagement) rather than all players. I’m doing the same when I ask this question:</p>
</div>
<div class="readable-text" id="p82">
<blockquote>
<div>
     What would be the level of in-game purchases for players with high side-quest engagement? 
     <em>P</em>( 
     <em>I</em>| 
     <em>E</em>=“high”)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p83">
<p>The fact that <em>Side-Quest Engagement</em> is high serves to filter the population of players down to those for whom that fact is true. As discussed in chapter 2, we use conditional probability to zoom in on a subpopulation. In this example, we use <em>P</em><em> </em>(<em>I</em><em>  </em>&gt;<em>  </em>50|<em>E</em>=“high”) for the statement, and <em>P</em><em> </em>(<em>I</em><em> </em>|<em>E</em>=“high”) for the question.</p>
</div>
<div class="readable-text intended-text" id="p84">
<p>I’ll use “factual conditions” to refer to facts, events, and evidence like <em>E</em><em> </em>=<em> </em>“high” that narrow down the target population. These factual conditions appear on the right side of “|” in the conditional probability notation <em>P</em><em> </em>(.|.). We might normally call them “conditions,” but I want to avoid confusion with “conditional hypothetical,” which I’ll introduce next.</p>
</div>
<div class="readable-text" id="p85">
<h3 class="readable-text-h3" id="sigil_toc_id_194"><span class="num-string">8.2.3</span> Conditional hypotheticals, interventions, and simulation</h3>
</div>
<div class="readable-text" id="p86">
<p>Now, suppose I made the following statement:</p>
</div>
<div class="readable-text" id="p87">
<blockquote>
<div>
     If a player’s side-quest engagement was high, they would spend more than $50 on in-game purchases. 
     <em>P</em> ( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub>&gt;50)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p88">
<p>We’ll call this a <em>conditional hypothetical</em> statement. We’ll call the “If side-quest engagement was high” part the <em>hypothetical condition</em>, and “they would spend more than $50 on in-game purchases” is the <em>hypothetical outcome</em>.</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>The hypothetical conditions in conditional hypothetical questions often follow a similar “what if” style of phrasing:</p>
</div>
<div class="readable-text" id="p90">
<blockquote>
<div>
<em>What</em> would be the amount of in-game purchases for a player 
     <em>if</em> their side-quest engagement was high? 
     <em>P</em> ( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub>)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p91">
<p>We will use the intervention notation (i.e., the subscripts in counterfactual notation) to represent these conditions. For the statement, we will use <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>&gt;50), and for the question, <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>). </p>
</div>
<div class="readable-text" id="p92">
<h4 class="readable-text-h4 sigil_not_in_toc">Imagination, conditions, and interventions</h4>
</div>
<div class="readable-text" id="p93">
<p>Using the ideal intervention to model hypothetical conditions in conditional hypothetical statements is a philosophical keystone of our causal modeling approach. The idea is that when we pose hypothetical conditionals, <em>we only attend to the causal consequences of the hypothetical conditional</em>.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p94">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Refresher: Ideal intervention</h5>
</div>
<div class="readable-text" id="p95">
<p>An ideal intervention is a change to the data generating process that does the following:</p>
</div>
<ol>
<li class="readable-text" id="p96"> Targets a fixed variable (e.g., X) </li>
<li class="readable-text" id="p97"> Sets that variable to a specific value (e.g., x) </li>
<li class="readable-text" id="p98"> In so doing, severs the causal influence of that variable’s parents </li>
</ol>
<div class="readable-text" id="p99">
<p>This definition generalizes to a <em>set</em> of variables.</p>
</div>
<div class="readable-text" id="p100">
<p>We sometimes write interventions with <em>do-notation</em>, as in do(<em>X</em>=<em>x</em>). In counterfactual notation, for a variable <em>Y</em>, we write <em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub> to indicate that the variable <em>Y</em> is under the influence of an intervention on <em>X</em>. In a DAG, we represent ideal intervention with graph surgery, meaning we cut the incoming edges to the target variable. In an SCM, we represent an ideal intervention by replacing the target variable’s assignment function with the intervention value. Causal libraries often implement these operations for us, often with a function or method called “do”. </p>
</div>
</div>
<div class="readable-text" id="p101">
<p>Let me illustrate by counterexample. Suppose we ask this:</p>
</div>
<div class="readable-text" id="p102">
<blockquote>
<div>
     What would be the amount of in-game purchases for a player if their side-quest engagement were high?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p103">
<p>Suppose we then modeled this with <em>P</em><em> </em>(<em>I</em><em> </em>|<em>E</em><em> </em>=<em> </em>“high”). Then inference on this query would use not just the causal impact that high engagement has on <em>In-Game Purchases</em> but also the non-causal association through the path <em>E</em> ← <em>G</em> → <em>I</em>; you can infer whether a player is in a guild from their level of <em>Side-Quest Engagement</em>, and <em>Guild Membership</em> also drives <em>In-Game Purchases</em>. But this question is not about <em>Guild Membership</em>; we’re just interested in how <em>Side-Quest Engagement</em> drives <em>In-Game Purchases</em>.</p>
</div>
<div class="readable-text intended-text" id="p104">
<p>“What if” hypotheticals use the ideal intervention because they attend only to the causal consequences of the condition. To illustrate, let’s rephrase the previous question to make that implied ideal intervention explicit: </p>
</div>
<div class="readable-text" id="p105">
<blockquote>
<div>
     What would be the amount of 
     <em>In-Game Purchases</em> for a player if their 
     <em>side-quest engagement</em>
<em>were </em>
<em>set to</em> high? 
     <em>P</em> ( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub>)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p106">
<p>The verb “set” connotes the action of intervening. Modeling hypothetical conditions with ideal interventions argues that the original phrasing and this phrasing mean the same thing (going forward, I’ll use the original phrasing).</p>
</div>
<div class="readable-text intended-text" id="p107">
<p>As humans, we answer “what if” questions like the preceding <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>) question (either the original or rephrased version) by imagining a world where the hypothetical condition is true and then imagining how the hypothetical scenario plays out as a consequence. The variables in our hypothetical condition may have their own causal drivers in the data-generating process (e.g., <em>Guild Membership</em> is a cause of <em>Side-Quest Engagement</em>), but we ignore those drivers because we are only interested in the consequences of the hypothetical condition. We isolate the variables in a hypothetical condition in our imaginations just as we would in an experiment. The ideal intervention is the right tool for setting a variable independently of its causes.</p>
</div>
<div class="readable-text" id="p108">
<h4 class="readable-text-h4 sigil_not_in_toc">Avoiding confusion between factual and hypothetical conditions</h4>
</div>
<div class="readable-text" id="p109">
<p>It is particularly easy to confuse “factual conditions” with “hypothetical conditions.” To reiterate in general terms, in the question “What would <em>Y</em> be if <em>X</em> were <em>x</em>”, <em>X</em> = <em>x</em> is a hypothetical condition and we use the notation <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>). In contrast, factual conditions serve to narrow down the population we are asking about. For example, in the question “What would <em>Y</em> be for cases where <em>X</em> is <em>x</em><em> </em>?” <em>X</em> = <em>x</em> is an actual condition used to filter down to cases where <em>X</em> = <em>x</em>. Here, we use notation <em>P</em><em> </em>(<em>Y</em><em>  </em>|<em>X</em><em> </em>=<em> </em><em>x</em><em> </em>).</p>
</div>
<div class="readable-text intended-text" id="p110">
<p>Keep in mind that we can combine factual and hypothetical conditions, as in the following question:</p>
</div>
<div class="readable-text" id="p111">
<blockquote>
<div>
     What would be the amount of in-game purchases for a player 
     <em>in a guild</em> if their side-quest engagement was high? 
     <em>P</em> ( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub>| 
     <em>G</em>= 
     <em>g</em>)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p112">
<p>Here, we are asking a conditional hypothetical on a subset of players who are guild members. This query is different from the following:</p>
</div>
<div class="readable-text" id="p113">
<blockquote>
<div>
     What would be the amount of in-game purchases for a player if their side-quest engagement was high 
     <em>and</em> they were in a guild? 
     <em>P</em> ( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub>, 
     <sub> </sub>
<sub><em>G</em></sub>
<sub>=“member”</sub>)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p114">
<p>That said, with the ambiguity of natural language, someone might ask the second question when what they really want is the answer to the first question. It is up to the modeler to dispel confusion, clarify meaning, and write down the correct notation.</p>
</div>
<div class="readable-text" id="p115">
<h3 class="readable-text-h3" id="sigil_toc_id_195"><span class="num-string">8.2.4</span> Counterfactual statements</h3>
</div>
<div class="readable-text" id="p116">
<p>In natural language, a <em>counterfactual statement</em> is a conditional hypothetical statement where there is some conflict between factual conditions and hypothetical conditions or outcomes. In other words, it is a conditional hypothetical statement that is “counter to the facts.”</p>
</div>
<div class="readable-text intended-text" id="p117">
<p>In everyday language, those conflicting factual conditions could be stated before the statement or implied by context. For our purposes, we’ll require counterfactual statements to state the conflicting factual conditions explicitly:</p>
</div>
<div class="readable-text" id="p118">
<blockquote>
<div>
<em>For a player with low side-quest engagement and an amount of in-game purchases less than $50,</em> if the player’s side-quest engagement were high, they would spend more than $50 on in-game purchases. 
     <em>P</em> ( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub>&gt;50| 
     <em>E</em>=“low”, 
     <em>I</em>  
     <span class="regular-symbol">£</span>50)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p119">
<p>As a question, we might ask:</p>
</div>
<div class="readable-text" id="p120">
<blockquote>
<div>
     What would be the amount of in-game purchases for a player 
     <em>with low side-quest engagement and in-game purchases less than $50 </em>if their side-quest engagement was high? 
     <em>P</em> ( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub> | 
     <em>E</em>=“low”, 
     <em>I</em>  
     <span class="regular-symbol">£</span>50)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p121">
<p>In both the statement and the question, the factual condition of low engagement conflicts with the hypothetical condition of high engagement. In the statement, the hypothetical outcome where in-game purchases is more than 50 conflicts with the factual condition where it is less than or equal to 50. Similarly, the question considers all possible hypothetical outcomes for in-game purchases, most of which conflict with the factual condition of being less than or equal to 50. We use counterfactual notation to write these queries just as we would other conditional hypotheticals..</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p122">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Overview of terminology in formalizing counterfactuals </h5>
</div>
<div class="readable-text" id="p123">
<p><em>Hypothetical language</em>—Used to express hypotheses, conjecture, supposition, and imagined possibilities. In English, it often involves “would” or “could” and contrasts with the declarative language. It is arguably easier to formalize causal statements and questions phrased in hypothetical language.</p>
</div>
<div class="readable-text" id="p124">
<p><em>Hypothetical statement</em>—A statement about the world phrased in hypothetical language, such as “<em>Y</em> would be <em>y,</em>” which we’d write in math as <em>P</em>(<em>Y</em>=<em>y</em>).</p>
</div>
<div class="readable-text" id="p125">
<p><em>Factual conditions</em>—Refer to facts, events, and evidence that narrow down the scope of what’s being talked about (the target population). Used as the conditions in conditional probability. For example, we’d write “Where <em>Z</em> is <em>z</em>, <em>Y</em> would be <em>y</em>” as <em>P</em>(<em>Y</em>=<em>y</em>|<em>Z</em>=<em>z</em>).</p>
</div>
<div class="readable-text" id="p126">
<p><em>Hypothetical conditions</em>—Conditions that frame a hypothetical scenario, as in “what if <em>X</em> were <em>x</em>?” or “If <em>X</em> were <em>x</em> …” We model hypothetical conditions with the ideal intervention and subscript <sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub> in counterfactual notation.</p>
</div>
<div class="readable-text" id="p127">
<p><em>Conditional hypothetical statement</em>—A hypothetical statement with hypothetical conditions, such as “If <em>X</em> were <em>x</em>, <em>Y</em> would be <em>y</em>,” which becomes <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>=<em>y</em>). We can add factual conditions like “Where <em>Z</em> is <em>z</em>, if <em>X</em> were <em>x</em>, <em>Y</em> would be <em>y</em>” becomes <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>=<em>y</em>|<em>Z</em>=<em>z</em>).</p>
</div>
<div class="readable-text" id="p128">
<p><em>Counterfactual statement</em>—A counterfactual statement is a conditional hypothetical statement where the variables in the factual conditions overlap with those in the hypothetical conditions or hypothetical outcomes. For example, in “Where <em>X</em> is <em>x</em>, if <em>X</em> were <em>x'</em>, <em>Y</em> would be <em>y</em>” (<em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em><sub>'</sub></em>=<em>y</em>|<em>X</em>=<em>x</em>)), the factual condition “Where <em>X</em> is <em>x</em>” overlaps with the hypothetical condition “if <em>X</em> were <em>x'</em>”. In “Where <em>Y</em> is <em>y</em>, if <em>X</em> were <em>x'</em>, <em>Y</em> would be <em>y'</em>” (<em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em><sub>'</sub></em>=<em>y'</em>|<em>Y</em>=<em>y</em>)), the factual condition “Where <em>Y</em> is <em>y</em>” overlaps with the hypothetical outcome “<em>Y</em> would be <em>y.</em>”</p>
</div>
<div class="readable-text" id="p129">
<p><em>Consistency rule</em>—You can drop a hypothetical condition in the subscript if a factual condition and a hypothetical condition overlap but don’t conflict. For example, <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>|<em>X</em>=<em>x</em>) = <em>P</em>(<em>Y</em>|<em>X</em>=<em>x</em>).</p>
</div>
<div class="readable-text" id="p130">
<p>Note that many texts will use the word “counterfactual” to describe formal causal queries that don’t necessarily condition on factual conditions, such as <em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub> or <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>=<em>y</em>) or <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>=1, <em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub><em><sub>'</sub></em>=0). I’m using “counterfactual statement” and other phrases above to describe common hypothetical and counterfactual natural language and to aid in the task of converting to formal counterfactual notation.</p>
</div>
</div>
<div class="readable-text" id="p131">
<p>Note that we can combine conflicting factual conditions with other non-conflicting factual conditions, such as being a member of the guild in this example:</p>
</div>
<div class="readable-text" id="p132">
<blockquote>
<div>
     What would be the amount of in-game purchases for a player 
     <em>in a guild</em> with low side-quest engagement and in-game purchases less than $50 if their side-quest engagement was high? 
     <em>P</em>( 
     <em>I</em>
<sub><em>E</em></sub>
<sub>=“high”</sub> | 
     <em>E</em>=“low”, 
     <em>I</em>
<span class="regular-symbol">£</span>50, 
     <em>G</em>=“member”)
    </div>
</blockquote>
</div>
<div class="readable-text" id="p133">
<p>Figure 8.4 diagrams the elements of a formalized counterfactual query.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p134">
<img alt="figure" height="146" src="../Images/CH08_F04_Ness.png" width="349"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.4</span> Elements of a conditional counterfactual hypothetical formalized in counterfactual notation</h5>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p135">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Formalizing counterfactuals with large language models</h5>
</div>
<div class="readable-text" id="p136">
<p>Formalizing a counterfactual question into counterfactual notation is an excellent task for a large language model (LLM). State-of-the-art LLMs perform quite well at benchmarks where a natural language query is converted to a symbolic query, such as an SQL statement, and formalizing a counterfactual question is an example of this task. We’ll look more at LLMs and causality in chapter 13, but for now you can experiment with prompting your favorite LLM to convert questions to counterfactual notation. </p>
</div>
</div>
<div class="readable-text" id="p137">
<h3 class="readable-text-h3" id="sigil_toc_id_196"><span class="num-string">8.2.5</span> The consistency rule</h3>
</div>
<div class="readable-text" id="p138">
<p>Consider the distribution <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em><em> </em>=<em> </em>“low”). Suppose that instead of the subscript <sub><em>E</em></sub><sub>=“high”</sub> we had <sub><em>E</em></sub><sub>=“low”</sub>, so the distribution is <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub> |<em>E</em><em> </em>=<em> </em>“low”). The <em>consistency rule</em> states that this distribution is equivalent to the simpler <em>P</em><em> </em>(<em>I</em><em> </em>|<em>E</em><em> </em>=<em> </em>“low”). More generally, <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>|<em>X</em><em> </em>=<em> </em><em>x</em>, <em>Z</em><em> </em>=<em> </em><em>z</em><em> </em>) = <em>P</em><em> </em>(<em>Y</em> |<em>X</em><em> </em>=<em> </em><em>x</em>, <em>Z</em><em> </em>=<em> </em><em>z</em><em> </em>) for any <em>z</em>.</p>
</div>
<div class="readable-text intended-text" id="p139">
<p>Intuitively, <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub>|<em>E</em>=“low”) corresponds to the rather odd question, “What would be the amount of in-game purchases for a player with low side-quest engagement<em> </em>if their side-quest engagement was low?” In this question, the factual condition and the hypothetical condition overlap but don’t conflict. The <em>consistency rule</em> says that, in this case, we drop the hypothetical condition, saying that this is equivalent to asking “What would be the amount of in-game purchases for a player with low side-quest engagement?”</p>
</div>
<div class="readable-text intended-text" id="p140">
<p>Now consider a version of this counterfactual where we observe an actual outcome for in-game purchases. Specifically, consider <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em><em> </em>=“low”, <em>I</em>=75). This is the corresponding counterfactual question:</p>
</div>
<div class="readable-text" id="p141">
<blockquote>
<div>
     What would be the amount of in-game purchases for a player 
     <em>with low side-quest engagement and in-game purchases equal to $75 </em>if their side-quest engagement was high?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p142">
<p>Now, instead, suppose we changed it to <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“low”</sub>|<em>E</em><em> </em>=<em> </em>“low”, <em>I</em><em> </em>=<em> </em>75). By the consistency rule, this collapses to <em>P</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em> </em>=“low”, <em>I</em><em> </em>=<em> </em>75): </p>
</div>
<div class="readable-text" id="p143">
<blockquote>
<div>
     What would be the amount of in-game purchases for a player 
     <em>with low side-quest engagement and in-game purchases equal to $75</em>?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p144">
<p>The answer, of course, is $75. If we ask about the distribution of <em>I</em> conditional on <em>I</em><em> </em>=<em> </em>75, then we have a distribution with all the probability value concentrated on 75.</p>
</div>
<div class="readable-text intended-text" id="p145">
<p>In counterfactual reasoning, we often want to know about hypothetical outcomes for the same variables we observe in the factual conditions. The consistency rule states that if the hypothetical conditions are the same as what actually happened, the hypothetical outcome must be the same as what actually happened.</p>
</div>
<div class="readable-text intended-text" id="p146">
<p>Recall that we use an intervention to model the hypothetical condition. The rule assures us that if a player had low <em>Side-Quest Engagement</em> and a certain amount of <em>In-Game Purchases</em>, they would have the exact same amount of <em>In-Game Purchases</em> if they were selected for an experiment that randomly selected them for the low <em>Side-Quest Engagement</em> group. That’s important if we expect our causal inferences to predict the outcomes of experiments.</p>
</div>
<div class="readable-text" id="p147">
<h3 class="readable-text-h3" id="sigil_toc_id_197"><span class="num-string">8.2.6</span> More examples</h3>
</div>
<div class="readable-text" id="p148">
<p>Table 8.1 presents several additional examples of mapping counterfactual questions to counterfactual notation.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p149">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 8.1</span> Examples of counterfactual notation</h5>
<table>
<thead>
<tr>
<th>
<div>
         Question 
       </div></th>
<th>
<div>
         Type 
       </div></th>
<th>
<div>
         Distribution in counterfactual notation 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  What would be the amount of in-game purchases for a typical player? <br/></td>
<td>  Hypothetical <br/></td>
<td> <em>P</em>( <em>I</em>) <br/></td>
</tr>
<tr>
<td>  What would be the amount of in-game purchases for a player with high side-quest engagement? <br/></td>
<td>  Hypothetical focused on highly engaged players <br/></td>
<td> <em>P</em>( <em>I</em>| <em>E</em>=“high”) <br/></td>
</tr>
<tr>
<td>  What would be the amount of in-game purchases for a player if they had high side-quest engagement? <br/></td>
<td>  Conditional hypothetical <br/></td>
<td> <em>P</em>( <em>I</em> <sub><em>E</em></sub> <sub>=“high”</sub>) <br/></td>
</tr>
<tr>
<td>  What would be the level of engagement <em>and</em> amount of in-game purchases if the player were a guild member? <br/></td>
<td>  Conditional hypothetical on two outcomes of interest <br/></td>
<td> <em>P</em>( <em>E</em> <sub><em>G</em></sub> <sub>=“member”</sub>, <em>I</em> <sub><em>G</em></sub> <sub>=“member”</sub>) <br/></td>
</tr>
<tr>
<td>  What would be the level of in-game purchases for a player if they had high side-quest engagement and they were not a guild member? <br/></td>
<td>  Conditional hypothetical with two hypothetical conditions <br/></td>
<td> <em>P</em>( <em>I</em> <sub><em>E</em></sub> <sub>=“high”, </sub> <sub><em>G</em></sub> <sub>=“nonmember”</sub>) <br/></td>
</tr>
<tr>
<td>  What would be the level of in-game purchases for a player in a guild if they had high side-quest engagement? <br/></td>
<td>  Conditional hypothetical focused on guild members <br/></td>
<td> <em>P</em>( <em>I</em> <sub><em>E</em></sub> <sub>=“high”</sub>| <em>G</em>=“member”) <br/></td>
</tr>
<tr>
<td>  For a player with low engagement, what would their level of in-game purchases be if their level of engagement was high? <br/></td>
<td>  Counterfactual. Factual condition conflicts with hypothetical condition. <br/></td>
<td> <em>P</em>( <em>I</em> <sub><em>E</em></sub> <sub>=“high”</sub>| <em>E</em>=“low”) <br/></td>
</tr>
<tr>
<td>  For a player who had at most $50 of in-game purchases, what would their level of in-game purchases be if their level of engagement was high? <br/></td>
<td>  Counterfactual. Factual condition (in-game purchases of <span class="regular-symbol">£</span>$50) conflicts with possible hypothetical outcomes (in-game purchases possibly &gt;$50). <br/></td>
<td> <em>P</em>( <em>I</em> <sub><em>E</em></sub> <sub>=“high”</sub>| <em>I</em> <span class="regular-symbol">£</span>50) <br/></td>
</tr>
<tr>
<td>  For a player who had low engagement and at most $50 of in-game purchases, what would their level of in-game purchases be if their level of engagement was high? <br/></td>
<td>  Counterfactual. Factual conditions conflict with a hypothetical condition and possible hypothetical outcomes. <br/></td>
<td> <em>P</em>( <em>I</em> <sub><em>E</em></sub> <sub>=“high”</sub>| <em>E</em>=“low”, <em>I</em> <span class="regular-symbol">£</span>50) <br/></td>
</tr>
<tr>
<td>  For a player in a guild who had low engagement, what would their level of in-game purchases be if their engagement were high and they weren’t a guild member? <br/></td>
<td>  Counterfactual. Factual conditions conflict with hypothetical conditions. <br/></td>
<td> <em>P</em>( <em>I</em> <sub><em>E</em></sub> <sub>=“high”, </sub> <sub><em>G</em></sub> <sub>=“nonmember”</sub>| <em>E</em>=“low”, <em>G</em>=“member”) <br/></td>
</tr>
<tr>
<td>  What would be the level of engagement if the player were a guild member? Moreover, what would be their level of in-game purchases if they were <em>not</em> a guild member? <br/></td>
<td>  Counterfactual. Involves two conflicting hypothetical conditions on two different outcomes. <br/></td>
<td> <em>P</em>( <em>E</em> <sub><em>G</em></sub> <sub>=“member”</sub>, <em>I</em> <sub><em>G</em></sub> <sub>=“nonmember”</sub>) <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p150">
<p>The last case in table 8.1 is a special case, more common in theory than practice, that does not involve a factual condition but has conflicting hypothetical conditions. </p>
</div>
<div class="readable-text intended-text" id="p151">
<p>Next, we’ll look at a particular class of counterfactuals that involve binary causes and outcomes.</p>
</div>
<div class="readable-text" id="p152">
<h2 class="readable-text-h2" id="sigil_toc_id_198"><span class="num-string">8.3</span> Binary counterfactuals</h2>
</div>
<div class="readable-text" id="p153">
<p>An important subclass of counterfactual query is one we’ll call <em>binary counterfactuals</em>. These are counterfactuals involving binary hypothetical conditions and outcome variables. Binary variables, especially binary causes, arise when we think in terms of observational and experimental studies, where we have “exposed” and “unexposed” groups, or “treatments” and “control” groups. But binary variables are also useful in reasoning about the occurrence of events; an event either happens or does not.</p>
</div>
<div class="readable-text intended-text" id="p154">
<p>Binary counterfactual queries deserve special mention because they are often simpler to think about, have simplifying mathematical properties that queries on nonbinary variables lack, and have several practical applications that we’ll cover in this section. Further, you can often word the question you want to answer in binary terms, such that you can convert nonbinary variables to binary variables when formalizing your query. To illustrate, in our online gaming example, suppose a player made $152.34 in online purchases, and we ask “Why did this player pay so much?” We are not interested in why they paid exactly that specific amount but why they paid such a high amount, where “such a high amount” is defined as, for example, more than $120. So our binary indicator variable is <em>X</em> = {1 if <em>I</em> <span class="regular-symbol">≥</span> 120 else 0}.</p>
</div>
<div class="readable-text" id="p155">
<h3 class="readable-text-h3" id="sigil_toc_id_199"><span class="num-string">8.3.1</span> Probabilities of causation</h3>
</div>
<div class="readable-text" id="p156">
<p>The probabilities of causation are an especially useful class of binary counterfactuals. Their utility lies in helping us answer “why” questions. They are foundational concepts in practical applications, including attribution in marketing, credit assignment in reinforcement learning, root cause analysis in engineering, and personalized medicine.</p>
</div>
<div class="readable-text intended-text" id="p157">
<p>Let’s demonstrate the usefulness of the probabilities of causation in the context of a churn attribution problem. In a subscription business model, churn is the rate at which your service loses subscribers, and it has a major impact on the value of a business or business unit. Typically, a company deploys a predictive algorithm that rates subscribers as having some degree of churn risk. The company wants to discourage subscribers with a high risk of churn from actually doing so. In our example, the company will send a promotion that will entice the subscriber to stay (not churn). The probabilities of causation can help us understand why a user would churn or stay.</p>
</div>
<div class="readable-text intended-text" id="p158">
<p>Given a binary (true/false) cause <em>X</em> and outcome <em>Y</em>, we’ll define the following probabilities of causation: probability of necessity, of sufficiency, of necessity <em>and</em> sufficiency, of enablement, and of disablement.</p>
</div>
<div class="readable-text" id="p159">
<h4 class="readable-text-h4 sigil_not_in_toc">Probability of necessity</h4>
</div>
<div class="readable-text" id="p160">
<p>For a binary cause <em>X</em> and binary outcome <em>Y</em>, the <em>probability of necessity</em> (PN) is the query <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><em> </em><sub>0</sub><em> </em>=<em> </em>0|<em>X</em><em> </em>=<em> </em>1, <em>Y</em><em> </em>=<em> </em>1). In plain language, the question underlying PN is “For cases where <em>X</em> happened, and <em>Y</em> happened, if <em>X</em> had not happened, would <em>Y</em> not have happened?” In other words, did <em>X</em> <em>need</em> to happen for <em>Y</em> to happen?</p>
</div>
<div class="readable-text intended-text" id="p161">
<p>Let’s consider our churn problem. Let <em>X</em> represent whether we sent a promotion and <em>Y</em> represent whether the user stayed (didn’t churn). In this example, <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><em> </em><sub>0</sub><em> </em>=<em> </em>0|<em>X</em><em> </em>=<em> </em>1, <em>Y</em><em> </em>=<em> </em>1) represents the query “For a subscriber who received the promotion and stayed, what are the chances they would have churned if they had not received the promotion?” In other words, was the promotional offer necessary to maintain the subscriber?</p>
</div>
<div class="readable-text" id="p162">
<h4 class="readable-text-h4 sigil_not_in_toc">Probability of sufficiency</h4>
</div>
<div class="readable-text" id="p163">
<p>The <em>probability of sufficiency</em> (PS) is <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><em> </em><sub>1</sub>=<em> </em>1|<em>X</em><em> </em>=<em> </em>0, <em>Y</em><em> </em>=<em> </em>0). A common plain language articulation of PS is “For cases where neither <em>X</em> nor <em>Y</em> happened, if <em>X</em> had happened, would <em>Y</em> have happened?” In other words, is <em>X</em> happening sufficient to cause <em>Y</em> to happen? For example, “for users who did not receive a promotion and didn’t stay (churned), would they have stayed had they received the promotion?” In other words, would a promotion have been enough (sufficient) to keep them?</p>
</div>
<div class="readable-text intended-text" id="p164">
<p>The plain language interpretation of sufficiency can be confusing. The factual conditions of the counterfactual query zoom in on cases where <em>X</em><em> </em>=<em> </em>0 and <em>Y</em><em> </em>=<em> </em>0 (cases where neither <em>X</em> nor <em>Y</em> happened). However, we’re often interested in looking at cases where <em>X</em>=1 and <em>Y</em>=1 and asking if <em>X</em> was sufficient by itself to cause <em>Y</em><em> </em>=<em> </em>1. In other words, given that <em>X</em> happened and <em>Y</em> happened, would <em>Y</em> still have happened even if the various other events that influenced <em>Y</em> had turned out different? But <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=1|<em>X</em><em> </em>=<em> </em>0, <em>Y</em><em> </em>=<em> </em>0) entails this interpretation without requiring us to enumerate all the “various other events that influenced <em>Y</em>” in the query. See the chapter notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for pointers to deeper research discussions on sufficiency.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p165">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Probabilities of causation and the law</h5>
</div>
<div class="readable-text" id="p166">
<p>The probabilities of causation are closely related to legal concepts in the law. It is helpful to know this relationship, since practical applications often intersect with the law, and many stakeholders we work with in practical settings have legal training.</p>
</div>
<ul>
<li class="readable-text" id="p167"> <em>But-for causation and the probability of necessity</em>—The but-for test is one test for determining causation in tort and criminal law. The way we phrase the probability of necessity is the probabilistic equivalent to the but-for test, rephrasing “if <em>X</em> had not happened, would <em>Y</em> not have happened?” as “but for <em>X</em> happening, would <em>Y</em> have happened?” </li>
<li class="readable-text" id="p168"> <em>Proximal causality and the probability of sufficiency</em>—In law, proximate cause refers to the primacy that a cause <em>X</em> had in the chain of events that directly brings about an outcome (e.g., injury or damage). There is indeed a connection with sufficiency, though not an equivalency. Proximal causality indeed considers whether a causal event was sufficient to cause the outcome, but legal theories of proximal cause often go beyond sufficiency to invoke moral judgments as well. </li>
</ul>
</div>
<div class="readable-text" id="p169">
<h4 class="readable-text-h4 sigil_not_in_toc">Probability of necessity and sufficiency</h4>
</div>
<div class="readable-text" id="p170">
<p>The <em>probability of necessity and sufficiency</em> (PNS) is <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=<em> </em>1, <em>Y</em><sub><em>X</em></sub><sub>=0</sub><em> </em>=<em> </em>0). In plain language, <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=<em> </em>1, <em>Y</em><sub><em>X</em></sub><sub>=0</sub>=<em> </em>0) reads, “<em>Y</em> would be 0 if <em>X</em> were 0 <em>and</em> <em>Y</em> would be 1 if <em>X</em> were 1.” For example, “What are the chances that a given user would churn if they didn’t receive a promotion and would stay if they did receive a promotion?” PNS decomposes as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p171">
<img alt="figure" height="22" src="../Images/ness-ch8-eqs-0x.png" width="514"/>
</div>
<div class="readable-text" id="p172">
<h4 class="readable-text-h4 sigil_not_in_toc">Probability of disablement and enablement</h4>
</div>
<div class="readable-text" id="p173">
<p>The <em>probabilities of disablement</em> (PD) and<em> enablement</em> (PE)<em> </em>are similar to PN and PS, except they do not condition on the cause <em>X</em>. </p>
</div>
<div class="readable-text intended-text" id="p174">
<p>PD is the query <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><em> </em><sub>0</sub>=<em> </em>0|<em>Y</em><em> </em>=<em> </em>1), meaning “For cases where <em>Y</em> happened, if <em>X</em> had not happened would <em>Y</em> not have happened?” For the churn problem, PD asks the question “What is the overall chance of churn if we don’t send promotions? exclusively in reference to the subpopulation of users who didn’t churn (regardless of whether they received a promotion). </p>
</div>
<div class="readable-text intended-text" id="p175">
<p>PE is the query <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=1|<em>Y</em><em> </em>=<em> </em>0), or “For cases where <em>Y</em> didn’t happen, if <em>X</em> had happened, would <em>Y</em> have happened?” In our churn problem, PE asks, “What is the overall chance of staying if we send promotions?” exclusively in reference to the subpopulation of users who churned (regardless of whether they received a promotion).</p>
</div>
<div class="readable-text intended-text" id="p176">
<p>The probabilities of causation can work as basic counterfactual primitives in advanced applications of counterfactual analysis. Next, I’ll give an example in the context of attribution.</p>
</div>
<div class="readable-text" id="p177">
<h3 class="readable-text-h3" id="sigil_toc_id_200"><span class="num-string">8.3.2</span> Probabilities of causation and attribution</h3>
</div>
<div class="readable-text" id="p178">
<p>The probabilities of causation are the core ingredients for methods that quantify why a given outcome happens. For example, suppose that a company’s network has a faulty server, such that accessing the server can cause the network to crash. Suppose the network crashes, and you’re tasked with analyzing the logs to find the root cause. You find that your colleague Lazlo has accessed the faulty server. Is Lazlo to blame?</p>
</div>
<div class="readable-text intended-text" id="p179">
<p>To answer that, you might quantify the chances that Lazlo was a sufficient cause of the crash; i.e., the chance that Lazlo accessing the server was enough to tip the domino that ultimately led to the network to crash. Second, what are the chances that Lazlo was a necessary cause? For example, perhaps Lazlo wasn’t a necessary cause because if he hadn’t accessed the server, someone else would have eventually.</p>
</div>
<div class="readable-text intended-text" id="p180">
<p>The probabilities of causation need to be combined with other elements to provide a complete view of attribution. One example is the concept of abnormality. The abnormality of a causal event describes whether that event, in some sense, violated expectations. For example, Lazlo might get more blame for crashing the network if it was highly unusual for employees to access that server. We can quantify the abnormality of a causal event with probability; if event <em>X</em>=1 was abnormal, then it was unlikely to have occurred, so we assign a low value to <em>P</em>(<em>X</em>=1). One attribution measure, called actual causal strength (ACS), combines abnormality with probabilities of causation as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p181">
<img alt="figure" height="22" src="../Images/ness-ch8-eqs-3x.png" width="421"/>
</div>
<div class="readable-text" id="p182">
<p>In other words, this approach views attribution as a trade-off between being an <em>abnormal</em> necessary cause and a <em>normal</em> sufficient cause.</p>
</div>
<div class="readable-text intended-text" id="p183">
<p>There is also a growing body of methods that combine attribution methods from the field of explainable AI (e.g., Shapley and SHAP values) with concepts of abnormality and causal concepts, such as the probabilities of causation. See the book notes at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for a list of references, including actual causal strength and explainable AI methods.</p>
</div>
<div class="readable-text" id="p184">
<h3 class="readable-text-h3" id="sigil_toc_id_201"><span class="num-string">8.3.3</span> Binary counterfactuals and uplift modeling</h3>
</div>
<div class="readable-text" id="p185">
<p>Statistical analysis of campaigns to influence human behavior is common in business, politics, and research. For instance, in our churn example, the goal of offering a promotion is to convince people not to churn. Similarly, businesses advertise to convince people to buy their products, and politicians reach out to voters to get them to vote or donate to a campaign.</p>
</div>
<div class="readable-text intended-text" id="p186">
<p>One of the challenges of campaigns to influence behavior is identifying who is likely to respond favorably to your attempt to influence so you only spend your limited resources influencing those people. John Wanamaker, a pioneer of the field of marketing, put it best:</p>
</div>
<div class="readable-text" id="p187">
<blockquote>
<div>
     Half the money I spend on advertising is wasted; the trouble is I don’t know which half.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p188">
<p><em>Uplift modeling </em>refers to a class of statistical techniques that seek to answer this question with data. However, a data scientist approaching this problem space for the first time will find various statistical approaches, varying in terminology, presumptive data types, modeling assumptions, and modeling approaches, leading to confusion. Binary counterfactuals are quite useful in understanding the problem at a high level and how various solutions succeed or fail at addressing it.</p>
</div>
<div class="readable-text" id="p189">
<h4 class="readable-text-h4 sigil_not_in_toc">Segmenting users into persuadables, sure things, lost causes, and sleeping dogs</h4>
</div>
<div class="readable-text" id="p190">
<p>In our churn example, we can assume there are two kinds of subscribers. For some subscribers, a promotion will influence their decision to churn. Others are non-responders, meaning people for whom the promotion will have no influence. We can break up the latter non-responders into two groups:</p>
</div>
<ul>
<li class="readable-text" id="p191"> <em>Lost causes</em><em> </em>—People who will churn regardless of whether they receive a promotion </li>
<li class="readable-text" id="p192"> <em>Sure things</em><em> </em>—People who will stay regardless of whether they receive a promotion </li>
</ul>
<div class="readable-text" id="p193">
<p>Of the people who do respond to the promotion, we have two groups:</p>
</div>
<ul>
<li class="readable-text" id="p194"> <em>Persuadables</em><em> </em>—Subscribers who could be persuaded by a promotion not to leave the service </li>
<li class="readable-text" id="p195"> <em>Sleeping dogs</em><em> </em>—Subscribers who would not churn if you didn’t send a promotion, and people who would churn if you did </li>
</ul>
<div class="readable-text" id="p196">
<p>Sleeping dogs are named for the expression “let sleeping dogs lie” (last they wake up and bite you). These people will do what you want if you leave them alone, but they’ll behave against your wishes if you don’t. Have you ever received a marketing email from a subscription service and thought, “These people send me too much spam! I’m going to cancel.” You were a “sleeping dog”—the company’s email was the kick that woke you up, and you bit them for it. Figure 8.5 shows how our subscribers break down into these four segments.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p197">
<img alt="figure" height="223" src="../Images/CH08_F05_Ness.png" width="475"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.5</span> In attempts to influence behavior, we break down the target population into these four segments. Given limited resources, we want to target our influence efforts on the persuadables and avoid the others, especially the sleeping dogs.</h5>
</div>
<div class="readable-text intended-text" id="p198">
<p>Promotions have a cost in terms of the additional value you give to the subscriber. You want to avoid spending that cost on subscribers who weren’t going to churn (sure things) and subscribers who were always going to churn (lost causes). And you definitely want to avoid spending that cost only to cause someone to churn (sleeping dogs). So, of these four groups, you want to send your promotions <em>only</em> to the persuadables. The task of statistical analysis is to segment our users into these four groups.</p>
</div>
<div class="readable-text intended-text" id="p199">
<p>This is where counterfactuals can help us; we can define each segment in probabilistic counterfactual terms:</p>
</div>
<ul>
<li class="readable-text" id="p200"> <em>Lost causes</em><em> </em>—People who probably would churn if we send a promotion and still churn if we did not send a promotion; i.e., <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=0, <em>Y</em><sub><em>X</em></sub><sub>=0</sub>=0) is high. </li>
<li class="readable-text" id="p201"> <em>Sure things</em><em> </em>—People who probably would stay if we send a promotion and stay if we did not send a promotion; i.e., <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=1, <em>Y</em><sub><em>X</em></sub><sub>=0</sub>=1) is high. </li>
<li class="readable-text" id="p202"> <em>Persuadables</em><em> </em>—People who probably would stay if we send a promotion and churn if we did not send a promotion; i.e., <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=1, <em>Y</em><sub><em>X</em></sub><sub>=0</sub>=0) is high. In other words, PNS is high. </li>
<li class="readable-text" id="p203"> <em>Sleeping dogs</em><em> </em>—People who probably would churn if we send a promotion and would stay if we did not send a promotion; i.e., <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>=0, <em>Y</em><sub><em>X</em></sub><sub>=0</sub>=1) is high. </li>
</ul>
<div class="readable-text" id="p204">
<p>You can see, in figure 8.6, how the population can be segmented. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p205">
<img alt="figure" height="224" src="../Images/CH08_F06_Ness.png" width="475"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.6</span> We can segment the population in counterfactual terms.</h5>
</div>
<div class="readable-text" id="p206">
<h4 class="readable-text-h4 sigil_not_in_toc">Using counterfactuals for segmentation</h4>
</div>
<div class="readable-text" id="p207">
<p>Each subscriber has some set of attributes (demographics, usage habits, content preferences, etc.). Our goal is to convert these attributes to predict whether a subscriber is a persuadable, sleeping dog, lost cause, or sure thing. </p>
</div>
<div class="readable-text intended-text" id="p208">
<p>Let <em>C</em> represent a set of subscriber attributes. Given a subscriber with attributes <em>C</em>=<em>c</em>, our causal query of interest is <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>, <em>Y</em><sub><em>X</em></sub><sub>=0</sub>|<em>C</em>=<em>c</em>). Various statistical segmentation methods seek to define <em>C</em> such that users fall into groups that have high probability for one of the four outcomes of <em>P</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>, <em>Y</em><sub><em>X</em></sub><sub>=0</sub>|<em>C</em>=<em>c</em>), but before we apply the stats, our first task will be to ensure we can estimate this query using sufficient assumptions and data. We’ll cover how to estimate counterfactuals with SCMs in chapter 9 and how to use identification with broader estimation techniques in chapter 10.</p>
</div>
<div class="readable-text intended-text" id="p209">
<p>Now that we’ve learned to pose our causal query and formalize it into math, let’s revisit the steps of making the counterfactual inference, in figure 8.7. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p210">
<img alt="figure" height="100" src="../Images/CH08_F07_Ness.png" width="845"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.7</span> The counterfactual inference workflow</h5>
</div>
<div class="readable-text" id="p211">
<p>In the next section, we’ll study the idea of possible worlds and parallel world graphs. These ideas are important to both identification (determining whether we can answer the question) and the inference algorithm.</p>
</div>
<div class="readable-text" id="p212">
<h2 class="readable-text-h2" id="sigil_toc_id_202"><span class="num-string">8.4</span> Possible worlds and parallel world graphs</h2>
</div>
<div class="readable-text" id="p213">
<p>In this section, I’ll introduce the notion of possible worlds and parallel world graphs, an extension of a causal DAG for an SCM that supports counterfactual reasoning across possible worlds.</p>
</div>
<div class="readable-text" id="p214">
<h3 class="readable-text-h3" id="sigil_toc_id_203"><span class="num-string">8.4.1</span> Potential outcomes in possible worlds</h3>
</div>
<div class="readable-text" id="p215">
<p>Counterfactual reasoning involves reasoning over <em>possible worlds</em>. A possible world is a way the world is or could be. The <em>actual world</em> is the possible world with the event outcomes we observed. All other possible worlds are <em>hypothetical worlds</em>.</p>
</div>
<div class="readable-text intended-text" id="p216">
<p>In terms of the data generating process (DGP), the actual world is how the DGP unrolled to produce our data. Other possible worlds are defined by all the ways the DGP could have produced different data.</p>
</div>
<div class="readable-text intended-text" id="p217">
<p><em>Potential outcomes</em> are a fundamental concept in causal effect inference. “Potential outcomes” refers to outcomes of the same variable across differing possible worlds. If you have a headache and take an aspirin, you might say there are two potential outcomes in two possible worlds: one where your headache gets better and one where it doesn’t.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p218">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Review of possible world terminology </h5>
</div>
<div class="readable-text" id="p219">
<p><em>Possible world</em>—A way the world is or could be</p>
</div>
<div class="readable-text" id="p220">
<p><em>Actual world</em>—A possible world with observed event outcomes</p>
</div>
<div class="readable-text" id="p221">
<p><em>Hypothetical world</em>—A possible world with no observed event outcomes</p>
</div>
<div class="readable-text" id="p222">
<p><em>Potential outcomes</em>—Outcomes of the same variable across differing possible worlds</p>
</div>
<div class="readable-text" id="p223">
<p><em>Parallel worlds</em>—A set of possible worlds being reasoned over, sharing both common and differing attributes</p>
</div>
<div class="readable-text" id="p224">
<p><em>Parallel world graph</em>—A graphical representation of parallel worlds used both for identifying counterfactual queries and in counterfactual inference algorithms</p>
</div>
</div>
<div class="readable-text" id="p225">
<h3 class="readable-text-h3" id="sigil_toc_id_204"><span class="num-string">8.4.2</span> The parallel world graph</h3>
</div>
<div class="readable-text" id="p226">
<p>A parallel world graph is a simple extension of a causal DAG that captures causality across possible worlds. Continuing with the online gaming example, suppose we are interested in the question, “For a player who had low engagement and less than $50 of in-game purchases, what would their level of in-game purchases be if their level of engagement was high?” I.e., <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em>=“low”, <em>I</em>&lt;50). For this counterfactual query, we can visualize both the actual and the hypothetical worlds in figure 8.8 <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p227">
<img alt="figure" height="305" src="../Images/CH08_F08_Ness.png" width="301"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.8</span> To answer the counterfactual query for the online gaming example, we start by duplicating the causal DAG across possible worlds.</h5>
</div>
<div class="readable-text intended-text" id="p228">
<p>We duplicate the causal DAG for the online gaming example across both possible worlds. Having one DAG for each world reflects that the causal structure of the DGP is the same in each world. But we’ll need to connect these DAGs in some way to reason <em>across</em> worlds.</p>
</div>
<div class="readable-text intended-text" id="p229">
<p>We’ll connect the two worlds using an SCM defined on the causal DAG. We’ll suppose that the original nodes of the DAG are the endogenous variables of the SCM and expand the DAG visualization by adding the exogenous variables. Further, the two causal DAGs will use the same exogenous nodes. We call the resulting graph a <em>parallel world graph</em> (or, for this typical case of two possible worlds, a “twin-world graph”). Figure 8.9 visualizes the parallel world graph.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p230">
<img alt="figure" height="357" src="../Images/CH08_F09_Ness.png" width="301"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.9</span> In the parallel world graph, we use the exogenous variables in an SCM to unite the duplicate causal DAGs across worlds. The result is a single SCM with duplicate endogenous variables.</h5>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p231">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Refresher: The structural causal model (SCM)</h5>
</div>
<div class="readable-text" id="p232">
<p>An SCM is a causal model with the following components:</p>
</div>
<ul>
<li class="readable-text" id="p233"> <em>Endogenous variables</em>—Endogenous variables are the variables we specifically want to model. </li>
<li class="readable-text" id="p234"> <em>Exogenous variables</em>—A set of exogenous variables. Exogenous variables are proxies for all the causes of our endogenous variables we don’t wish to model explicitly. In our formulation, we pair each endogenous variable <em>X</em> with a single exogenous variable <em>N</em><em><sub>X</sub></em> (there are more general formulations). </li>
<li class="readable-text" id="p235"> <em>Exogenous distributions</em>—To use the SCM as a generative model, we need a set of marginal probability distributions for each exogenous variable, such as <em>P</em>(<em>N</em><em><sub>X</sub></em>), which represents the modeler’s uncertainty about the values <em>N</em><em><sub>X</sub></em>. </li>
<li class="readable-text" id="p236"> <em>Functional assignments</em>—Each endogenous variable has a functional assignment that sets its value deterministically, given its parents. </li>
</ul>
<div class="readable-text" id="p237">
<p>For example, written as a generative model, an SCM for our online game model would look as follows.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p238">
<img alt="sidebar figure" height="206" src="../Images/ness-ch8-eqs-2x.png" width="159"/>
</div>
<div class="readable-text" id="p239">
<p>The assignment functions induce the causal DAG; each variable is a node, the exogenous variables are root nodes, and the inputs of a variable’s assignment function correspond to its parents in the DAG. The SCM is a particular case of a causal graphical model where endogenous variables are set by deterministic functions rather than sampled from causal Markov kernels.</p>
</div>
</div>
<div class="readable-text" id="p240">
<p>The result is a single SCM with one shared set of exogenous variables and duplicate sets of endogenous variables—one set for each possible world. Note that in an SCM, the endogenous variables are set deterministically, given the exogenous variables. So upon observing that <em>E</em><em> </em>=<em> </em>“low” and <em>I</em><em>  </em>&lt;<em> </em>50 in the actual world, we know that the hypothetical outcomes of <em>E</em> and <em>I</em> must be the same. Indeed, even though <em>Guild Membership</em> (<em>G</em><em> </em>) is a latent variable in the actual world, we know that whatever value <em>G</em> takes in the actual world must be the same as in the hypothetical world. In other words, our SCM upholds the consistency rule, as illustrated in figure 8.10. In figure 8.10, the <em>E</em> and <em>I</em> in the actual world are observed variables because we condition on them in the query <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em><em> </em>=<em> </em>“low”, <em>I</em> &lt; 50).</p>
</div>
<div class="browsable-container figure-container" id="p241">
<img alt="figure" height="400" src="../Images/CH08_F10_Ness.png" width="507"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.10</span> In an SCM, the endogenous variables are set deterministically, given the exogenous variables. In this model, the endogenous variables are duplicated across worlds. Therefore, upon observing low engagement and less than $50 of in-game purchases in the actual world, we know that those values must be the same in the hypothetical world unless we change something in the hypothetical world.</h5>
</div>
<div class="readable-text" id="p242">
<h3 class="readable-text-h3" id="sigil_toc_id_205"><span class="num-string">8.4.3</span> Applying the hypothetical condition via graph surgery<span class="aframe-location"/></h3>
</div>
<div class="readable-text" id="p243">
<p>The hypothetical world will, typically, differ from the actual world by the hypothetical condition. For example, in <em>P</em><em> </em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>|<em>E</em><em> </em>=<em> </em>“low”, <em>I</em> &lt; 50), “if engagement were high” (<sub><em>E</em></sub><sub>=“high”</sub>) differs from the factual condition “engagement was low” (<em>E</em><em> </em>=<em> </em>“low”). As we’ve discussed, we model the hypothetical condition with the ideal intervention—we intervene on <em>E</em>, setting it to “high” in the hypothetical world. We model the ideal intervention on the graph with graph surgery—we’ll remove incoming edges to the <em>E</em> variable in the hypothetical world as in figure 8.11.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p244">
<img alt="figure" height="452" src="../Images/CH08_F11_Ness.png" width="306"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.11</span> The ideal intervention and graph surgery represents the hypothetical condition in the hypothetical world. In this setting, the outcome for <em>I</em> in the hypothetical world can now take a different outcome than it has in the actual world because its parent <em>E</em> has a different outcome than it has in the actual world.</h5>
</div>
<div class="readable-text intended-text" id="p245">
<p>Now the outcome for <em>In-Game Purchases</em> (<em>I</em><em>  </em>) in the hypothetical world can take a different outcome than the actual world’s outcome of <em>I</em><em> </em>=<em> </em>50 because its causal parent <em>E</em> has different outcomes in each world.</p>
</div>
<div class="readable-text" id="p246">
<h3 class="readable-text-h3" id="sigil_toc_id_206"><span class="num-string">8.4.4</span> Reasoning across more than two possible worlds</h3>
</div>
<div class="readable-text" id="p247">
<p>The counterfactual notation and the parallel worlds graph formalism support counterfactual reasoning that extends across more than two possible worlds. To illustrate, let’s refer back to the Netflix example at the beginning of the chapter. Summarizing the story, the key variables in that narrative are as follows:</p>
</div>
<ul>
<li class="readable-text" id="p248"> Disney is trying to close a deal to buy the Bond franchise. Let <em>B</em> = “success” if the deal closes. Otherwise, <em>B </em>= “fail”. </li>
<li class="readable-text" id="p249"> Netflix is trying to close a deal to start a new spy franchise called <em>Dead Drop</em>. <em>D</em> = “success” if the <em>Dead Drop</em> deal closes and “fail” otherwise. If the Bond deal closes, it will affect the terms of this deal. Therefore, <em>B</em> causes <em>D</em>. </li>
<li class="readable-text" id="p250"> If the <em>Dead Drop</em> deal closes, it will affect engagement in spy-thriller-related content on Netflix. Let <em>E</em> = “high” if a subscriber’s engagement in Netflix’s spy-thriller content is high and “low” otherwise. </li>
<li class="readable-text" id="p251"> The outcome of the Bond deal and the <em>Dead Drop</em> deal will both affect the attrition of spy-thriller fans to Disney. Let <em>A</em> be the rate of attrition to Disney. </li>
</ul>
<div class="readable-text" id="p252">
<p>With this case study, the following multi-world counterfactual is plausible. Suppose the Bond deal was successful (<em>B</em> = “success”), but Netflix’s <em>Dead Drop</em> deal failed, and as a result, engagement was low (<em>E</em> = “low”) and Netflix attrition to Disney is 10 percent. Figure 8.12 illustrates this actual world outcome.</p>
</div>
<div class="readable-text intended-text" id="p253">
<p>As a Netflix executive, you start wondering about attribution. You assume that engagement would have been high if the Dead Drop deal had been successful. You ask the following counterfactual question:</p>
</div>
<div class="readable-text" id="p254">
<blockquote>
<div>
     Disney’s Bond deal succeeded, the Dead Drop deal failed, and as a result, Netflix’s spy thriller engagement was low, and attrition to Disney was 10%. I assume that had the Dead Drop deal been successful, engagement would have been high. In that case, how much attribution would there have been?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p255">
<p>We can implement this assumption with world 2 in the parallel world graph in figure 8.13. </p>
</div>
<div class="browsable-container figure-container" id="p256">
<img alt="figure" height="397" src="../Images/CH08_F12_Ness.png" width="283"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.12</span> A causal DAG representing the Netflix case study. The light gray nodes are observed outcomes in the actual world. The dark nodes are latent variables.</h5>
</div>
<div class="browsable-container figure-container" id="p257">
<img alt="figure" height="436" src="../Images/CH08_F13_Ness.png" width="507"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.13</span> The second possible world represents the assumption that if the <em>Dead Drop</em> deal was successful (via intervention <em>D=</em>“success”) engagement would have been high (<em>E</em><em><sub>D</sub></em><sub>=“success”</sub>=“high”).</h5>
</div>
<div class="readable-text" id="p258">
<p>Finally, you wonder what the level of Netflix attrition would be <em>if the Bond deal had failed</em>. But you wonder this based on your second-world assumption that engagement would be high if the <em>Dead Drop</em> deal had been successful. Since the Bond deal failing is a hypothetical condition that conflicts with the Bond deal success condition in the second world, you need a third world, as illustrated in figure 8.14.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p259">
<img alt="figure" height="496" src="../Images/CH08_F14_Ness.png" width="728"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.14</span> Given the actual outcomes in world 1, the hypothetical conditions and outcomes in world 2, you pose conditions in world 3 and reason about attrition in world 3.</h5>
</div>
<div class="readable-text" id="p260">
<p>In summary, this is the counterfactual question:</p>
</div>
<div class="readable-text" id="p261">
<blockquote>
<div>
     Disney’s Bond deal succeeded, the 
     <em>Dead Drop</em> deal failed, and as a result, Netflix’s spy thriller engagement was low, and attrition to Disney was 10%. I assume that had the 
     <em>Dead Drop</em> deal been successful, engagement would have been high. In that case, how much attribution would there have been if the Bond deal had failed?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p262">
<p>Note that the preceding reasoning is different from the following:</p>
</div>
<div class="readable-text" id="p263">
<blockquote>
<div>
     Disney’s Bond deal succeeded, the 
     <em>Dead Drop</em> deal failed, and as a result, Netflix’s spy thriller engagement was low, and attrition to Disney was 10%. I assume that had the 
     <em>Dead Drop</em> deal been successful 
     <em>and the Bond deal failed</em>, engagement would have been high. In that case, how much attribution would there have been?
    </div>
</blockquote>
</div>
<div class="readable-text" id="p264">
<p>Figure 8.15 illustrates the latter question.</p>
</div>
<div class="browsable-container figure-container" id="p265">
<img alt="figure" height="450" src="../Images/CH08_F15_Ness.png" width="514"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.15</span> In the case of assuming <em>E</em><em><sub>B</sub></em><sub>=“fail”,</sub><em><sub>D</sub></em><sub>=“success”</sub>, only two worlds are needed.</h5>
</div>
<div class="readable-text intended-text" id="p266">
<p>The latter question assumes engagement would be high if the Bond deal failed <em>and</em> the <em>Dead Drop</em> deal was successful (<em>E</em><sub><em>B</em></sub><sub>=“fail”, </sub><sub><em>D</em></sub><sub>=“success”</sub><sub>=</sub><sub>“</sub><sub>high”</sub>). In contrast, the former “three world” question assumes engagement would be high if both deals were successful. Then, in the third world, It allows for different possible levels of engagement in the hypothetical scenario where the Bond deal failed. For example, perhaps engagement would be high since Netflix would have its spy-thriller franchise and Disney wouldn't. Or perhaps, without a Bond reboot there would be less overall interest in spy-thrillers, resulting in low engagement in <em>Dead Drop</em>.</p>
</div>
<div class="readable-text" id="p267">
<h3 class="readable-text-h3" id="sigil_toc_id_207"><span class="num-string">8.4.5</span> Rule of thumb: Hypothetical worlds should be simpler<span class="aframe-location"/></h3>
</div>
<div class="readable-text" id="p268">
<p>Consider again the endogenous nodes in our online gaming example in figure 8.16. Notice that, in this example, the two worlds have the same sets of endogenous nodes, and the edges in the hypothetical world are a subset of the edges of those in the actual world. In other words, the possible world where we do intervene is simpler than the possible world where we condition on evidence.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p269">
<img alt="figure" height="429" src="../Images/CH08_F16_Ness.png" width="295"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.16</span> The graph representing the possible world with the hypothetical conditional is simpler than the graph representing the actual world.</h5>
</div>
<div class="readable-text intended-text" id="p270">
<p><strong> </strong>Similarly, in the three-world graph for the Netflix case study, world 3 is a subgraph of world 2, which is a subgraph of world 1. As an algorithmic rule of thumb, it is useful to have this descending ordering on possible worlds. This rule of thumb reduces the risk of algorithmic instability.</p>
</div>
<div class="readable-text intended-text" id="p271">
<p> That said, there are use cases for having more complicated hypothetical worlds. For example, a modeler could introduce new nodes as conditions in the hypothetical world. Or they could use stochastic interventions that randomly introduce new edges in the hypothetical world. Indeed, human counterfactual reasoning can be quite imaginative. Exploring such approaches could lead to interesting new algorithms for causal AI.</p>
</div>
<div class="readable-text intended-text" id="p272">
<p> In the next chapter, we’ll dive into using parallel world graphs in an algorithm for general counterfactual inference.</p>
</div>
<div class="readable-text" id="p273">
<h2 class="readable-text-h2" id="sigil_toc_id_208">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p274"> Counterfactual statements describe hypothetical events that potentially conflict with actual events. They are fundamental to defining causality. </li>
<li class="readable-text" id="p275"> Counterfactual reasoning supports learning policies for better decision-making. </li>
<li class="readable-text" id="p276"> Counterfactual reasoning involves reasoning over possible worlds. A <em>possible world </em>is a way the world is or could be. The <em>actual world </em>is a possible world with event outcomes we observed. Other possible worlds are hypothetical worlds. </li>
<li class="readable-text" id="p277"> In machine learning, often the goal is counterfactual analysis of a machine learning model itself. Here, we reason about how a prediction would have been different if elements of the input feature vector were different. </li>
<li class="readable-text" id="p278"> Counterfactual analysis in classification can help find the minimal change in features that would have led to a different classification. </li>
<li class="readable-text" id="p279"> Counterfactual analysis supports explainable AI by helping identify changes to features that would have changed the prediction outcome on a case-by-case basis. </li>
<li class="readable-text" id="p280"> Counterfactual analysis supports algorithmic recourse by identifying <em>actionable</em> changes to features that would change the prediction outcome. </li>
<li class="readable-text" id="p281"> Counterfactual analysis supports AI fairness by identifying features corresponding to protected attributes where changes to said features would change the prediction outcome. </li>
<li class="readable-text" id="p282"> “Potential outcomes” is a commonly used term that refers to outcomes for a given variable from across possible worlds. </li>
<li class="readable-text" id="p283"> We can use the ideal intervention and parallel world graphs to model hypothetical conditions in natural language counterfactual statements and questions. </li>
<li class="readable-text" id="p284"> Counterfactual notation helps represent hypothetical statements and questions in the language of probability. Probability can be used to quantify uncertainty about the truth of hypothetical statements and questions, including counterfactuals. </li>
<li class="readable-text" id="p285"> Using hypothetical language rather than declarative language helps with formalizing a counterfactual statement or question into counterfactual notation. Using hypothetical language implies imagined possibility, and thus uncertainty, which invites us to think about the probability of a hypothetical statement being true. </li>
<li class="readable-text" id="p286"> Binary counterfactual queries refer to queries on variables (hypothetical conditions and outcomes) that are binary. </li>
<li class="readable-text" id="p287"> The <em>probabilities of causation,</em> such as the <em>probability of necessity</em> (PN), <em>probability of sufficiency</em> (PS), and <em>probability of necessity and sufficiency</em> (PNS), are binary counterfactual queries that are useful as primitives in causal attribution methods and other types of advanced causal queries. </li>
<li class="readable-text" id="p288"> Binary counterfactual queries are also useful for distinguishing between “persuadables,” “sure things,” “lost causes,” and “sleeping dogs” in uplift modeling problems. </li>
<li class="readable-text" id="p289"> A parallel world graph is a simple extension of a causal DAG that captures causality across possible worlds. It represents an SCM over possible worlds that share a common set of exogenous variables and duplicate sets of endogenous variables. </li>
</ul>
</div></body></html>