<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Deploying to Test Environments"><div class="chapter" id="chapter_4_deploying_to_test_environments_1749354010445896">
      <h1><span class="label">Chapter 4. </span>Deploying to Test Environments</h1>
      <p>In <a data-type="xref" href="ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208">Chapter 3</a>, we explored the fundamentals of continuous integration, focusing on early steps in a CI/CD pipeline: mainly, building and pre-deployment testing<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="deploying to test environments" id="xi_testingdeployingtotestenvironments43348"/><a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="to test environments" data-secondary-sortas="test environments" id="xi_deploymenttotestenvironments43348"/>. We walked through an example pipeline triggered when a PR is opened, as shown in <a data-type="xref" href="#chapter_4_figure_1_1749354010437513">Figure 4-1</a>.</p>
      <figure><div id="chapter_4_figure_1_1749354010437513" class="figure">
        <img src="assets/ansd_0401.png" width="600" height="220"/>
        <h6><span class="label">Figure 4-1. </span>A CI pipeline</h6>
      </div></figure>
      <p>This pipeline built and packaged the code, conducted static code analysis, and executed early, fast tests including unit and lightweight integration tests, providing build and test feedback to the PR. These steps ensure that the code in the pull request is merge-ready, providing confidence that the merged code would function as intended and would not introduce any regressions. Assuming that the code changes in the PR prove ready, the developer can merge the PR. </p>
      <p>With our new code merged, the next step is getting ready for production by deploying into test environments and then running a battery of tests. AI and ML tools<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="deploying into test environments" id="id599"/> are being integrated into the deployment process. These tools help teams make better deployment decisions, identify potential issues proactively, and streamline the verification process. Rather than adding complexity, well-implemented AI actually reduces the cognitive load on developers while improving deployment reliability. </p>
      <p>Between the CI steps and the production release<a contenteditable="false" data-type="indexterm" data-primary="production deployment" data-secondary="to test environment to assess release viability" data-secondary-sortas="test environment to assess release viability" id="id600"/><a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" id="xi_CICDpipeline41057"/><a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="production deployment" id="id601"/>, we are primarily focused on testing. We want to learn if the release is ready for our users. If it is safe to release, we want to get it to our users quickly, to enhance the user experience and potentially drive increased customer engagement and loyalty. If our software has a problem, we need to detect and address it quickly. That dynamic is a barrier to releasing valuable updates, and the longer the time between the introduction of the defect and when it is brought back to the development team, the less likely the work will be fresh in the mind of the developers involved. They will have to spend more time and effort familiarizing themselves with those sections of code, making the remediation more expensive. If a developer is already deep into their next task, that task may be interrupted and become more expensive to complete as well.</p>
      <p>When the release is ready, we will deploy the release into one or more environments where we can test against running code. It is in these pre-production environments<a contenteditable="false" data-type="indexterm" data-primary="pre-production environments" data-secondary="continuous delivery and development (CD)" id="id602"/> that we bridge the gap between development and real-world usage, ensuring our software not only functions correctly but is ready for real-world scenarios. </p>
      <p><a data-type="xref" href="#chapter_4_figure_2_1749354010437547">Figure 4-2</a> gives a high-level depiction of our entire delivery process. Increasingly, AI is being embedded throughout this pipeline to strengthen testing and deployment decisions.</p>
      <figure><div id="chapter_4_figure_2_1749354010437547" class="figure">
        <img src="assets/ansd_0402.png" width="600" height="143"/>
        <h6><span class="label">Figure 4-2. </span>High-level delivery process</h6>
      </div></figure>
      <p>In this chapter we’ll look at steps to provision infrastructure, deploy to one or more pre-production environments, and test against the software. In addition, we will cover key best practices including:</p>
      <ul>
        <li>
          <p>Using IaC to create the lower environments that are consistent with production, but smaller</p>
        </li>
        <li>
          <p>Using “production-like” deployments to move your application consistently</p>
        </li>
        <li>
          <p>Connecting testing to the deployment pipeline</p>
        </li>
        <li>
          <p>Selecting where to apply AI to deployments and where to remain cautious</p>
        </li>
      </ul>
      <p>This stage in the pipeline is a pivotal one, where development and operations concerns intersect. By understanding and implementing these best practices, you’ll be well-equipped to determine the optimal number and types of test environments for your specific project requirements, regardless of project size or complexity. You will understand how to balance development velocity and operational stability, ensuring your software undergoes thorough testing and is release-ready.</p>
      <section data-type="sect1" data-pdf-bookmark="Establishing a Unified Deployment Process"><div class="sect1" id="chapter_4_establishing_a_unified_deployment_process_1749354010446024">
        <h1>Establishing a Unified Deployment Process</h1>
        <p>As we continue to navigate our delivery pipeline<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="CD deployment and test process" id="xi_CICDpipelineCDdeploymentandtestprocess43560"/> en route to production, we need to consider the deployment<a contenteditable="false" data-type="indexterm" data-primary="unified deployment process, establishing" id="xi_unifieddeploymentprocessestablishing435119"/><a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="establishing consistency in test environments" id="xi_deploymentestablishingconsistencyintestenvironments435119"/> steps and deployment environments that will be necessary. For a delivery process that is predictable and reliable, we need deployment steps and environments that are predictable and reliable.</p>
        <p>In this section, we’ll cover best practices to give us the predictability and reliability we’re after, from test to production. In <a data-type="xref" href="ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288">Chapter 8</a> we will cover production releases and production environments in greater detail.</p>
        <section data-type="sect2" data-pdf-bookmark="Deploy Consistently to Every Environment"><div class="sect2" id="chapter_4_deploy_consistently_to_every_environment_1749354010446089">
          <h2>Deploy Consistently to Every Environment</h2>
          <p>Automation<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="to deploy consistently" data-secondary-sortas="deploy consistently" id="xi_automationtodeployconsistently43924"/><a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="automation in" id="xi_deploymentautomationin43924"/> is the foundation of DevOps, and a key function of our delivery pipeline is to automate both the setup of our pre-production environments and the deployments to those environments. Just as we need to validate our software before releasing it into the wild, we need to have measures in place to validate how we deploy our software.</p>
          <p>We do this by consistently using the same methods to deploy to pre-production environments as we do to deploy to production. This consistency tests our deployment methods and minimizes the risk of unexpected issues when repeating these steps to deploy our software into production environments. </p>
          <p>The following best practices help provide the predictability we’re after.</p>
          <section data-type="sect3" data-pdf-bookmark="Use consistent tooling"><div class="sect3" id="chapter_4_use_consistent_tooling_1749354010446141">
            <h3>Use consistent tooling</h3>
            <p>It’s not unusual for developers to spin up their own lightweight deployment processes using simple tooling to deploy to test environments, while the operations teams focus on processes geared to production deployments using enterprise tooling. This inconsistency between processes leads to changes being communicated on an “as broken basis,” where developers will update their process and forget to notify operations until something breaks.</p>
            <p>This approach should be avoided, as it limits the effectiveness of testing in nonproduction environments and leads to duplicated effort in automation scripting. Instead, adopt a unified toolset for all deployments. </p>
            <p>One way to encourage consistency is to offer developers easy, premade template pipelines<a contenteditable="false" data-type="indexterm" data-primary="template pipelines" id="id603"/><a contenteditable="false" data-type="indexterm" data-primary="“paved roads”" data-primary-sortas="paved roads" id="id604"/> known as “golden pipelines” or “paved roads.” We will examine this in more detail in <a data-type="xref" href="ch10.html#chapter_10_a_platform_engineering_approach_to_modern_devops_1749354009763489">Chapter 10</a>. At a minimum, your developers and operations teams need to agree on a common set of tools for performing deployments. </p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Use consistent pipeline steps and deployment strategies"><div class="sect3" id="chapter_4_use_consistent_pipeline_steps_and_deployment_strat_1749354010446191">
            <h3>Use consistent pipeline steps and deployment strategies</h3>
            <p>Whether you’re using your CI/CD tool or custom deployment scripts, the sequence of actions should remain consistent across environments. Advanced deployment strategies like canary<a contenteditable="false" data-type="indexterm" data-primary="canary deployments" id="id605"/> or blue-green deployments<a contenteditable="false" data-type="indexterm" data-primary="blue-green deployments" id="id606"/> are typically selected based on derisking production deployments. If your production environment utilizes these strategies, replicate them in your pre-production environments. Similarly, if you were to use feature flags<a contenteditable="false" data-type="indexterm" data-primary="feature flags" data-secondary="in test environments" data-secondary-sortas="test environments" id="id607"/> to release individual features in production, use feature flags to roll out the features in test environments. This consistency minimizes the chance of introducing discrepancies or oversights during deployment. </p>
            <p>We’ll cover production deployment and these progressive deployment strategies more thoroughly in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch07.html#chapter_7_deploying_to_production_1749354011062634">7</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288">8</a>. For now, note that the steps and strategies you use should be replicated at every level. While a test environment may be smaller due to cost or resource constraints, deploy as if it were a production environment. For example, a rolling deployment in production might deploy two nodes at a time to 10 targets, while in a test environment you could deploy one node at a time to 3 targets. This approach ensures that your production deployment steps and strategy are thoroughly tested with each version deployed to the test environment.</p>
            <p>In <a data-type="xref" href="ch07.html#chapter_7_deploying_to_production_1749354011062634">Chapter 7</a>, we will examine in depth how AI techniques can verify that a deployment is not causing trouble in its new environment. Those same approaches should be used in lower environments to validate that they are working and protect our tests from being run against a faulty install.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Use parameterization for differences"><div class="sect3" id="chapter_4_use_parameterization_for_differences_1749354010446240">
            <h3>Use parameterization for differences</h3>
            <p>Inevitably, variations will exist between environments. Target names, service URLs, and passwords may differ. Instead of creating unique deployment scripts for each environment, leverage variables to accommodate these differences<a contenteditable="false" data-type="indexterm" data-primary="parameterization for deployment process" id="id608"/>. This allows you to maintain a single, adaptable script or pipeline that can be tailored to the specific environment at runtime.</p>
            <p>By being consistent in deployment, you’ll create a robust and reliable delivery pipeline that instills confidence in your team’s ability to release software seamlessly and efficiently.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Accelerate pipeline creation with AI"><div class="sect3" id="chapter_4_accelerate_pipeline_creation_with_ai_1749354010446287">
            <h3>Accelerate pipeline creation with AI</h3>
            <p>In <a data-type="xref" href="ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208">Chapter 3</a>, we discussed automatic pipeline creation. Templating<a contenteditable="false" data-type="indexterm" data-primary="pipelines" data-secondary="accelerating creation with AI" id="id609"/><a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="accelerating CI/CD pipeline creation" id="id610"/> remains a good pattern—you want your AI to leverage your organization’s templates and pull in the correct adjustments and variables for your project and team. Whether you or an AI are creating or maintaining pipelines, the less that pipeline code needs to be managed, the better.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Leverage Infrastructure as Code for deployment consistency"><div class="sect3" id="chapter_4_leverage_infrastructure_as_code_for_deployment_con_1749354010446337">
            <h3>Leverage Infrastructure as Code for deployment consistency</h3>
            <p>We want consistent and predictable environments to deliver our release to production. IaC<a contenteditable="false" data-type="indexterm" data-primary="Infrastructure as Code (IaC)" id="xi_InfrastructureasCodeIaC465124"/><a contenteditable="false" data-type="indexterm" data-primary="IaC (Infrastructure as Code)" id="xi_IaCInfrastructureasCode465124"/> gives us an approach to not only achieve consistency but also control our configuration with as much care and control as we do our code resources. At its core, IaC treats infrastructure configuration like software code.</p>
            <p>Engineers make changes to the IaC code locally and test them in their development environment. These changes are then committed to the VCS, just like application code. By managing our IaC, we leverage these features of our VCS and CI/CD pipelines.</p>
            <p>The as-code nature has made IaC a DevOps area that quickly benefited from large language models. AI coding assistants<a contenteditable="false" data-type="indexterm" data-primary="AI coding assistants" data-secondary="IAC" id="id611"/> generate and explain IaC code well, lowering the barriers to entry for developers and infrastructure professionals adopting new IaC languages. For major cloud providers with access to performance data from environments or DevOps platforms that combine cloud cost features with IaC management, future code generation tools will likely incorporate the following runtime optimizations based on live workloads:</p>
            <dl>
              <dt>Collaboration and code review</dt>
              <dd>
                <p>Version control<a contenteditable="false" data-type="indexterm" data-primary="version control, and IaC" id="id612"/><a contenteditable="false" data-type="indexterm" data-primary="collaboration and code review, IaC for" id="id613"/> enables multiple team members to work on files simultaneously and manage conflicts. We can define and enforce policies to require code reviews of our infrastructure configuration changes. </p>
              </dd>
              <dt>Branching and experimentation</dt>
              <dd>
                <p>Version control<a contenteditable="false" data-type="indexterm" data-primary="branching and experimentation, IaC for" id="id614"/> allows you to create branches for experimenting with different configurations without affecting the main production environment. </p>
              </dd>
              <dt>Traceability and auditability</dt>
              <dd>
                <p>A VCS<a contenteditable="false" data-type="indexterm" data-primary="traceability and auditability, IaC for" id="id615"/> provides a complete history of changes to your configuration settings. The commit messages and change history help you understand why your systems evolve, and audit trails are important in supporting compliance with security frameworks. </p>
              </dd>
              <dt>Rollback and recovery</dt>
              <dd>
                <p>If an infrastructure configuration<a contenteditable="false" data-type="indexterm" data-primary="rollbacks" data-secondary="IaC for" id="id616"/> change causes problems, you can quickly revert to a previous working version, minimizing downtime and impact on your systems. In addition, in the case of a catastrophic failure, you can use your version-controlled configurations to restore your systems to a known working state.</p>
              </dd>
              <dt>Automated testing</dt>
              <dd>
                <p>Delivery pipelines<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="of testing" data-secondary-sortas="testing" id="id617"/> can run automated tests on the IaC code, including syntax checks, security scans, and compliance tests. The changes are then applied to the staging environment for integration testing, and finally, they’re promoted to production, typically using a careful rollout strategy.</p>
              </dd>
              <dt>Security</dt>
              <dd>
                <p>Version control<a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="version control and IaC" id="id618"/><a contenteditable="false" data-type="indexterm" data-primary="version control, and IaC" id="id619"/> can help enforce security policies and controls around configuration changes, ensuring that only authorized personnel can make modifications.</p>
              </dd>
            </dl>
            <p>Consider a scenario all too familiar to many in the tech industry: an application works flawlessly in development and runs smoothly in staging, but descends into chaos when deployed to production. This discrepancy often stems from inconsistencies in infrastructure configurations across environments. With IaC configuration definitions you can ensure that every environment, from development to production, is provisioned identically. </p>
            <p>This methodical process ensures that your infrastructure evolves in a controlled, predictable manner. It eliminates the “worked in QA” problem by removing unexpected differences between environments. By treating your infrastructure with the same respect and rigor as your application code, you gain consistency, reliability, and agility.</p>
            <p>IaC offers several advantages beyond control and consistency. With a single command, you can spin up new environments that are exact replicas of your existing infrastructure. This not only makes your processes repeatable but also serves as accurate, living documentation. Because environments are easily created and destroyed, you can tear them down when not in use, saving resources and reducing costs, with the confidence that they can be recreated effortlessly.</p>
            <p>To implement IaC effectively, you need the right tools, and several popular options are available. Terraform<a contenteditable="false" data-type="indexterm" data-primary="Terraform" id="id620"/> and its more open fork, OpenTofu<a contenteditable="false" data-type="indexterm" data-primary="OpenTofu" id="id621"/>, use a cloud-agnostic approach. If you’re all in on a particular cloud provider, native tools like AWS CloudFormation<a contenteditable="false" data-type="indexterm" data-primary="AWS" data-secondary="CloudFormation" id="id622"/><a contenteditable="false" data-type="indexterm" data-primary="CloudFormation" id="id623"/> or Azure Resource Manager<a contenteditable="false" data-type="indexterm" data-primary="Azure Resource Manager" id="id624"/><a contenteditable="false" data-type="indexterm" data-primary="Microsoft Azure Resource Manager" id="id625"/> might be more appropriate<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_automationtodeployconsistently43924" id="id626"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentautomationin43924" id="id627"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_InfrastructureasCodeIaC465124" id="id628"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_IaCInfrastructureasCode465124" id="id629"/>. </p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Leverage Git Workflows with GitOps"><div class="sect2" id="chapter_4_leverage_git_workflows_with_gitops_1749354010446387">
          <h2>Leverage Git Workflows with GitOps</h2>
          <p>GitOps<a contenteditable="false" data-type="indexterm" data-primary="GitOps" data-secondary="deployment process with Git workflows" id="xi_GitOpsdeploymentprocesswithGitworkflows410220"/> is a newer and increasingly popular approach to deploying software that builds on the capabilities of code repositories. With a GitOps approach, you describe the desired state of infrastructure in version-controlled configuration. This description is declarative. GitOps tools include an agent that regularly reconciles the actual environment and the desired state described in Git-controlled configurations. Instead of running a script to directly deploy software, you instigate a software deployment by updating the configuration in your code repository. This approach and GitOps tools are typically used in Kubernetes<a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="container orchestrating with GitOps tools" id="id630"/><a contenteditable="false" data-type="indexterm" data-primary="orchestration, container" id="id631"/> environments to orchestrate containerized applications across clusters of machines.</p>
          <p>With this approach, you rely on your code repository to enforce security, provide governance, and implement your organization’s policies, such as requiring oversight through code reviews and approvals. Your updates are traceable and auditable. You can collaborate, experiment, and roll back the configuration updates used to deploy software. Once you make an update and merge it, the GitOps reconciliation agent does the rest, picking up the updates and implementing the changes to the target environment.</p>
          <p>The approach has gained traction because managing the intricate configurations describing complex orchestrated cloud systems is an application well suited to code repository capabilities. In addition, GitOps addresses the problem of environment drift<a contenteditable="false" data-type="indexterm" data-primary="environment drift" id="id632"/>; that is, the environment is changed operationally from the desired state. The reconciliation agent automatically detects and remediates, guarding against inconsistencies in environments.</p>
          <p>While a GitOps approach is powerful, deploying with a GitOps approach within a CI/CD delivery pipeline is more complicated than simply pushing your app updates with a script. With GitOps, your pipeline must automate the following steps:</p>
          <ol>
            <li>
              <p>Retrieve configuration from your code repository.</p>
            </li>
            <li>
              <p>Update the configuration to reference the latest version of your application.</p>
            </li>
            <li>
              <p>Merge the updated configuration back to Git. </p>
            </li>
          </ol>
          <p>GitOps reconciliation then takes it from there.</p>
          <p>You may also encounter complexities with applications that are geographically replicated across multiple clusters. Maintaining consistency and synchronizing across clusters can be difficult due to many GitOps reconcilers being optimized for deploying applications to a single cluster. You may need to balance the need for a single source of truth with the reality that certain configurations will need to be tailored for specific clusters. Commercial GitOps tools often provide orchestration and visibility in these more complicated scenarios, extending what open source provides.</p>
          <p>Despite these challenges, the benefits in terms of collaboration, traceability, and automated reconciliation make GitOps a compelling choice for organizations that extensively leverage Kubernetes<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_unifieddeploymentprocessestablishing435119" id="id633"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_GitOpsdeploymentprocesswithGitworkflows410220" id="id634"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentestablishingconsistencyintestenvironments435119" id="id635"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Continuous Delivery, Deployment, and Test in the CI/CD Pipeline"><div class="sect1" id="chapter_4_continuous_delivery_deployment_and_test_in_the_c_1749354010446443">
        <h1>Continuous Delivery, Deployment, <span class="keep-together">and Test in the CI/CD Pipeline</span></h1>
        <p>Now that we have an understanding of the importance of predictable and reliable deployment steps and environments, let’s return to our delivery pipeline. With our new code merged, we now want to deploy it into one or more environments where we can test against running code. <a data-type="xref" href="#chapter_4_figure_3_1749354010437567">Figure 4-3</a> shows an example pipeline. </p>
        <figure><div id="chapter_4_figure_3_1749354010437567" class="figure">
          <img src="assets/ansd_0403.png" width="600" height="283"/>
          <h6><span class="label">Figure 4-3. </span>Testing our code in pre-production environments</h6>
        </div></figure>
        <p>In this section, we’ll focus on the pipeline:</p>
        <dl>
          <dt>1. Code trigger</dt>
          <dd>
            <p>The pull request<a contenteditable="false" data-type="indexterm" data-primary="code trigger step, CI process" id="id636"/> is reviewed, approved, and merged into the main branch. In this pipeline, the PR merge triggers the pipeline.</p>
          </dd>
          <dt>2. Continuous integration</dt>
          <dd>
            <p>The pipeline repeats the continuous integration steps we reviewed in the last chapter, checking out, building, and executing continuous integration tests.</p>
          </dd>
          <dt>3. Provision infrastructure</dt>
          <dd>
            <p>The pipeline provisions<a contenteditable="false" data-type="indexterm" data-primary="provision infrastructure, CI/CD pipeline" id="id637"/> the pre-production environments required for testing.</p>
          </dd>
          <dt>4. Deploy to one or more pre-production environments</dt>
          <dd>
            <p>The pipeline deploys the app to one or more pre-production environments<a contenteditable="false" data-type="indexterm" data-primary="pre-production environments" data-secondary="continuous delivery and development (CD)" id="id638"/>.</p>
          </dd>
          <dt>5. Tests against the deployed app</dt>
          <dd>
            <p>The pipeline tests<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="pipeline testing against deployed app" id="id639"/> against the deployed software. Various types of tests can be run, depending on the type of software and the priorities of your organization. We’ll look at a number of different types of tests in the following section. The pipeline can be configured to run multiple types of tests in parallel or sequentially. Some tests can reuse the same pre-production environments, while others may necessitate pre-production environments tailored to the requirements of the tests. Generally, faster tests are prioritized over slower tests.</p>
          </dd>
          <dt>6. Deploy to production</dt>
          <dd>
            <p>The last step is to deploy, or promote, to the production<a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="decision to deploy to production" data-seealso="production deployment" id="id640"/> environment. Depending on your delivery process, the decision to deploy to production can be automated or require manual approval. We’ll look at promotion strategies and steps to deploy to production in <a data-type="xref" href="ch07.html#chapter_7_deploying_to_production_1749354011062634">Chapter 7</a>.</p>
          </dd>
        </dl>
        <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="chapter_4_continuous_delivery_versus_continuous_deployment_1749354010446494">
          <h1>Continuous Delivery Versus Continuous Deployment</h1>
          <p>The terms continuous delivery<a contenteditable="false" data-type="indexterm" data-primary="continuous delivery versus continuous deployment" id="id641"/> and continuous deployment are often used interchangeably. Continuous delivery is generally and loosely defined as a process that automates the software release up to the point of production deployment, requiring a manual approval before changes go live. Continuous deployment, on the other hand, fully automates the entire process, including deployment to production.</p>
          <p>The confusion arises because pipelines automate deployments to intermediate test environments. Some use “continuous delivery” to encompass these automated intermediate deployments, while others reserve it for processes that don’t deploy automatically to <em>any</em> environment. Similarly, “continuous deployment” is sometimes used broadly to describe any automated deployment, including to test environments.</p>
          <p>To avoid confusion, we prefer to use “continuous delivery” broadly, to refer to the process of frequent delivery of software to its users. Reducing the number of manual steps will tend to make this process more frequent. When we discuss deployment steps in a specific delivery process, we include details about the deployment environments (intermediate or production) and type (automated or manual).</p>
        </div></aside>
        <section data-type="sect2" data-pdf-bookmark="Types of Testing"><div class="sect2" id="chapter_4_types_of_testing_1749354010446547">
          <h2>Types of Testing</h2>
          <p>Test environments<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="types of" id="xi_testingtypesof416431"/> are crucial for running tests, but the choice of tests depends heavily on the type of application being developed, the intended users, software architecture, and budget and time constraints. For example, in general, testing priorities for a website will differ significantly from those of embedded software or a web API. Testing priorities will vary between software services in a highly regulated industry versus software that must be intuitive and compelling to a large retail user base. Your selection of tests and their frequency can substantially impact application quality, infrastructure costs, and overall delivery speed.</p>
          <p>AI-powered testing platforms<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="testing strategy optimization with ML" id="id642"/> increasingly use ML to optimize testing strategies. These platforms analyze historical test data, code changes, application architecture, and past deployment issues to intelligently select and prioritize tests. For example, AI-driven test selection tools identify the most impactful tests to execute for each code change, significantly speeding up test cycles. Vendors such as Harness, Tricentis SeaLights, and CloudBees Launchable are using AI and ML techniques to optimize test selection.</p>
          <p>Here are common types of tests that occur during this phase:</p>
          <dl>
            <dt>End-to-end or functional tests</dt>
            <dd>
              <p>These tests<a contenteditable="false" data-type="indexterm" data-primary="end-to-end or functional tests" id="id643"/> are the most straightforward test type, simulating real-world user scenarios and validating the entire application flow from start to finish, to determine if the software does what is expected. These tests may be automated or performed manually. Modern teams automate more. Selenium is a commonly used open source test automation framework that many commercial tools also build upon. ML has been present in these tools for quite some time, but we are increasingly seeing a shift toward an AI-first approach, which we’ll dive into shortly. </p>
            </dd>
            <dt>AI-powered testing</dt>
            <dd>
              <p>AI can automatically generate test cases<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="testing strategy optimization with ML" id="id644"/>, identify edge cases, and learn from previous test runs to focus on areas most likely to have issues. AI testing is likely to complement or be a part of your end-to-end (functional) test programs.</p>
            </dd>
            <dt>API tests</dt>
            <dd>
              <p>A form of functional testing<a contenteditable="false" data-type="indexterm" data-primary="functional testing" id="id645"/><a contenteditable="false" data-type="indexterm" data-primary="API tests" id="id646"/> is API testing, which validates that an API works as expected. In distributed systems, services interact over APIs, so ensuring that APIs are performing well is important. Common API testing frameworks include SoapUI, Postman, Insomnia, and Swagger. AI-enhanced API testing<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="enhancements for API testing" id="id647"/> goes beyond simple validation to intelligently explore API behaviors and edge cases. These systems can automatically generate API test scenarios by analyzing API documentation or actual usage patterns.</p>
            </dd>
            <dt>User experience tests</dt>
            <dd>
              <p>Developers, testers, and product managers may evaluate new features to make sure they are easy and intuitive to use<a contenteditable="false" data-type="indexterm" data-primary="user experience tests" id="id648"/>. While this may test the same systems as end-to-end testing, the focus is on assessing usability.</p>
            </dd>
            <dt>User acceptance tests</dt>
            <dd>
              <p>These tests are typically done as a final check<a contenteditable="false" data-type="indexterm" data-primary="user acceptance tests" id="id649"/> to ensure that the software meets the end user’s needs, that it meets the requirements, and that it functions as expected. User acceptance tests can include many other types of tests, from end-to-end to user experience and performance. These tests are done from the end user’s perspective with the purpose of providing a final and formal acceptance of the software release.</p>
            </dd>
            <dt class="pagebreak-before less_space">Accessibility tests</dt>
            <dd>
              <p>These tests ensure that our software is usable for people with disabilities<a contenteditable="false" data-type="indexterm" data-primary="accessibility" id="id650"/> such as visual, hearing, or cognitive impairments in order to serve our users and comply with legal, contractual, and regulatory requirements. Open source accessibility scanners include Lighthouse and Pa11Y. Companies, including accessiBe, are beginning to offer AI-augmented testing and remediation tooling as well.</p>
            </dd>
            <dt>Localization tests</dt>
            <dd>
              <p>Localization<a contenteditable="false" data-type="indexterm" data-primary="localization testing" id="id651"/> testing is important for software targeting a global audience. It involves a comprehensive assessment of the product’s linguistic accuracy, cultural appropriateness, and functional correctness within the specific target locale. This includes verifying translations, adapting visuals to cultural sensitivities, and ensuring the software functions correctly with local formats and regulations.</p>
            </dd>
            <dt>Performance tests</dt>
            <dd>
              <p>These tests<a contenteditable="false" data-type="indexterm" data-primary="performance tests" id="id652"/> simulate workloads to assess the speed, responsiveness, and stability of the application under different conditions. The tests help identify performance bottlenecks and ensure the application can handle expected traffic. This type of testing is critical for applications with seasonal peaks to ensure that the release can withstand peak demand. Apache JMeter, Gatling, and Grafana k6 are often used for performance testing. AI can leverage the data from performance testing to recommend resilience tests to run. These AI-powered performance testing systems can now detect performance anomalies with much greater accuracy than traditional threshold-based approaches. The systems establish baseline performance patterns and identify subtle deviations that might indicate looming issues. More advanced platforms can even pinpoint the specific components or code changes responsible for performance degradation by correlating test results with code changes and architecture maps.</p>
            </dd>
            <dt>Resilience tests</dt>
            <dd>
              <p>In modern distributed systems, a production system has many components<a contenteditable="false" data-type="indexterm" data-primary="resiliency" data-secondary="testing" id="id653"/>. The one certainty is that something is going to break somewhere. Resilience testing, also known as chaos testing, evaluates if the software can remain useful when services it relies on fail. We’ll return to resilience testing in <a data-type="xref" href="ch06.html#chapter_6_chaos_engineering_and_service_reliability_1749354010916149">Chapter 6</a>.</p>
            </dd>
            <dt>Security tests</dt>
            <dd>
              <p>These tests identify vulnerabilities and weaknesses<a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="testing" id="id654"/> in the application that could be exploited by attackers. They help ensure the security and integrity of the application. Dynamic application security testing (DAST)<a contenteditable="false" data-type="indexterm" data-primary="dynamic application security testing (DAST)" id="id655"/><a contenteditable="false" data-type="indexterm" data-primary="DAST (dynamic application security testing)" id="id656"/> is a specific type of security testing that automates penetration testing, inspecting your running application for security flaws. DAST attempts to attack your applications like a malicious user would. ZAP is a commonly used free tool, while commercial offerings from Veracode and Checkmarx are popular as well. We’ll return to security testing in <a data-type="xref" href="ch05.html#chapter_5_securing_applications_and_the_software_supply_chai_1749354010735711">Chapter 5</a>.</p>
            </dd>
          </dl>
          <p>While the test types outlined above are commonly used, it’s important to note that there is no one-size-fits-all approach to software testing, and terminology can vary across organizations. The specific tests you choose and how you categorize them will depend on your unique development process, application architecture, and risk tolerance<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_testingtypesof416431" id="id657"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Intent-Based Functional and End-to-End Testing"><div class="sect2" id="chapter_4_intent_based_functional_and_end_to_end_testing_1749354010446597">
          <h2>Intent-Based Functional and End-to-End Testing</h2>
          <p>Traditional approaches to automated functional and end-to-end testing<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="intent-based tests" id="id658"/><a contenteditable="false" data-type="indexterm" data-primary="intent-based tests" id="id659"/><a contenteditable="false" data-type="indexterm" data-primary="functional testing" id="id660"/><a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="testing" id="id661"/> often rely heavily on scripted tests or simplistic record-and-playback methods. While convenient initially, these tests quickly become brittle and difficult to maintain, breaking whenever minor UI changes occur. This fragility creates a high maintenance burden, slows down development, and frequently results in teams abandoning automated testing entirely or limiting its scope.</p>
          <p>An emerging AI-first approach to testing, known as intent-based testing, aims to overcome these challenges. Instead of explicitly scripting or manually recording each test step, teams express the intent of their test scenarios, describing the outcome they expect rather than the exact sequence of actions to achieve it. AI-native testing tools then dynamically generate and execute these tests by interacting with your application, much like a human user would.</p>
          <p>For example, instead of recording precise clicks and form inputs for an e-commerce checkout process, you could simply describe the goal: “Purchase a product using a credit card.” The AI would automatically determine the most appropriate paths through your application, interacting with buttons, forms, and workflows <span class="keep-together">intelligently.</span></p>
          <p>An important benefit is improved resilience<a contenteditable="false" data-type="indexterm" data-primary="resiliency" data-secondary="testing" id="id662"/> of the tests—addressing the challenge of UI-based tests being brittle. If the UI changes later, the AI adapts to the new layout or modified interactions, significantly reducing maintenance overhead. Test automation tools<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="of testing" data-secondary-sortas="testing" id="id663"/> have tried to automatically repair tests for many years, using techniques from tracking DOM objects to implementing ML. Shifting to understand the intent behind the test, and attempting to regenerate the entire script in response to a UI overhaul, brings a new level of recoverability. </p>
          <p>These tools may also help compensate for the shift from professional testers toward asking developers to own these tests. The tools can recommend additional tests and assertions related to the existing tests, which may help an optimistic developer remember to check for corner cases and bad user behavior.</p>
          <p>Advanced use cases for AI include migrating tests written in traditional tools such as Selenium<a contenteditable="false" data-type="indexterm" data-primary="Selenium" id="id664"/> and Playwright<a contenteditable="false" data-type="indexterm" data-primary="Playwright" id="id665"/> into intent-based testing tools, and generating and running not just individual tests but also entire test cases. </p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Traditional Testing Versus a Hollowing-Out-the-Middle Approach"><div class="sect2" id="chapter_4_traditional_testing_versus_a_hollowing_out_the_mid_1749354010446648">
          <h2>Traditional Testing Versus a Hollowing-Out-the-Middle Approach</h2>
          <p>In traditional software development, testing<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="traditional versus hollowing-out-the-middle" id="xi_testingtraditionalversushollowingoutthemiddle422258"/><a contenteditable="false" data-type="indexterm" data-primary="hollowing-out-the-middle approach to testing" id="xi_hollowingoutthemiddleapproachtotesting422258"/> is often compartmentalized with dedicated environments for each type. This ensures, for example, that manual user experience testing is never impacted by concurrent automated performance tests. However, this isolation comes at a cost: a proliferation of test environments is expensive and can be time-consuming to manage. When a single new release must clear numerous stages, the approach becomes increasingly unsustainable in the face of accelerating release cadences and growing application complexity.</p>
          <p>As you try to accelerate your release cadences and your application becomes more complex, it becomes increasingly unsustainable to test across many stages, with each stage requiring a new environment. <a data-type="xref" href="#chapter_4_figure_4_1749354010437583">Figure 4-4</a> illustrates this staged approach.</p>
          <figure><div id="chapter_4_figure_4_1749354010437583" class="figure">
            <img src="assets/ansd_0404.png" width="600" height="131"/>
            <h6><span class="label">Figure 4-4. </span>Traditional testing through several pre-production environments</h6>
          </div></figure>
          <p>On the other hand, a more modern testing approach challenges this model. This approach is sometimes referred to as “hollowing out the middle.” Instead of multiple, sequential test passes across multiple environments, there are fewer environments where tests run concurrently. This practice advocates shifting testing both to the left <em>and to the right</em>. </p>
          <p>We introduced shift-left security<a contenteditable="false" data-type="indexterm" data-primary="shift-left security" data-secondary="deployment to test environments" id="id666"/> in <a data-type="xref" href="ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208">Chapter 3</a>. By moving SAST, SCA, dependency scanning, and secrets detection into pre-deployment steps, our sample pipeline exemplified shift-left. We incorporated these crucial tests early such that passing them is a prerequisite to merging code. Unit and other early testing, completed as part of the merge workflow, also represent a shift-left approach. This helps catch issues sooner, reducing the need for extensive downstream testing.</p>
          <p>A shift-right<a contenteditable="false" data-type="indexterm" data-primary="shift-right security" id="id667"/><a contenteditable="false" data-type="indexterm" data-primary="production environment, shift-right testing approach" id="id668"/><a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="shift-right approach" id="id669"/> approach advocates executing some types of tests, traditionally late-cycle test types, against the new release in the live, production environment. Instead of provisioning and moving a release from one or more pre-production environments and using these isolated environments to test, we deploy the app straight to production and validate there. For example, load testing<a contenteditable="false" data-type="indexterm" data-primary="load testing, and shift-right approach" id="id670"/> can be difficult to execute well and the environments may need to be large. Deploying to a portion of prod, applying load to that targeted infrastructure, and measuring the impact using production observability tooling can be a viable alternative to traditional load testing. <a data-type="xref" href="#chapter_4_figure_5_1749354010437606">Figure 4-5</a> illustrates this approach.</p>
          <figure><div id="chapter_4_figure_5_1749354010437606" class="figure">
            <img src="assets/ansd_0405.png" width="600" height="131"/>
            <h6><span class="label">Figure 4-5. </span>A “hollow-out-the-middle” approach to testing </h6>
          </div></figure>
          <p>We can see that removing the need for pre-production environments that closely mirror production can save costs and maintenance toil, but how can extensive testing in a production environment be safe? Shift-right relies on new tools and production deployment practices. With advanced traffic management, observability tools, and containerization, many organizations have found that these tests can in fact be performed in the production environment with minimal side effects. Beyond significantly cutting infrastructure expenses, this approach has the advantage of yielding more accurate results. We’ll discuss these new tools and production deployment practices in <a data-type="xref" href="ch07.html#chapter_7_deploying_to_production_1749354011062634">Chapter 7</a>.</p>
          <p>Hollowing out the middle optimizes testing and is one modern strategy that organizations are taking to fuel faster delivery. By redesigning our approach to how we move our software between environments, we can similarly accelerate our delivery process. In <a data-type="xref" data-xrefstyle="select:nopage" href="#chapter_4_promotion_between_environments_1749354010446697">“Promotion Between Environments”</a>, we’ll look at how and why we should promote our releases between environments<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_testingtraditionalversushollowingoutthemiddle422258" id="id671"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_hollowingoutthemiddleapproachtotesting422258" id="id672"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipelineCDdeploymentandtestprocess43560" id="id673"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Promotion Between Environments"><div class="sect1" id="chapter_4_promotion_between_environments_1749354010446697">
        <h1>Promotion Between Environments</h1>
        <p>In the previous section, we looked at a typical delivery process<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="CD promotion between environments" id="xi_CICDpipelineCDpromotionbetweenenvironments424176"/> that required moving our software through multiple stages of testing,<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="promotion between environments" id="xi_testingpromotionbetweenenvironments4241145"/><a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="promotion between environments" id="xi_AIartificialintelligencesystemspromotionbetweenenvironments4241145"/><a contenteditable="false" data-type="indexterm" data-primary="promotion (of releases) between environments" id="xi_promotionofreleasesbetweenenvironments4241145"/> with each stage of testing conducted in a separate pre-production environment. In this process we want to promote our release as quickly and intelligently as possible, meaning our new version of software should advance to the next environment and stage without any undue delay. </p>
        <p>AI is beginning to play an increasing role in this promotion process, analyzing test results, performance data, and deployment history to make intelligent decisions about when and how to promote releases. These systems can evaluate multiple metrics simultaneously, detect subtle patterns that might indicate risks, and become increasingly accurate over time through ML.</p>
        <p>Ideally, our promotion process is simple: if the tests in one stage pass, our release is immediately promoted to the next environment, and that environment is ready and available for the next round of testing. The promotion decision is automatic and instant and simply based on whether the previous stage of testing passed. In practice, release promotion, even between test environments, becomes a bottleneck in many delivery processes. This can be attributed to several factors:</p>
        <dl>
          <dt>Promotion decision is by committee</dt>
          <dd>
            <p>Promotion decisioning is not automated and requires a group review and approval of test results.</p>
          </dd>
          <dt>Promotion relies on tedious manual steps</dt>
          <dd>
            <p>Manual intervention to trigger the next deployment creates bottlenecks.</p>
          </dd>
          <dt>Insufficient number of testing environments</dt>
          <dd>
            <p>If the next environment is occupied with testing another version, the new version must wait.</p>
          </dd>
        </dl>
        <p>In this section we’ll look at mitigations to address these issues. The practices we’ll introduce help us move our release from one pre-production environment to the next, and also apply to promoting our app into production. However, the final release into production has some special considerations, which will be addressed in more depth in <a data-type="xref" href="ch07.html#chapter_7_deploying_to_production_1749354011062634">Chapter 7</a>.</p>
        <section data-type="sect2" data-pdf-bookmark="From Decisions by Committee to Automated Decisions"><div class="sect2" id="chapter_4_from_decisions_by_committee_to_automated_decisions_1749354010446746">
          <h2>From Decisions by Committee to Automated Decisions</h2>
          <p>Human decision making<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="AI versus committee decision making" id="id674"/>, whether it’s a committee huddle or a trusted individual’s call, inevitably introduces delays in promoting your release from one stage to the next. Team members need to be alerted, then take time to analyze testing results before reaching a decision and taking action. While this might not always be labor-intensive, it undoubtedly slows things down.</p>
          <p>While traditional automation has relied on simple pass/fail criteria, AI systems offer more sophisticated decision-making capabilities. Modern AI promotion engines can evaluate hundreds of metrics simultaneously, looking beyond simple test results to analyze system behavior holistically. These systems might consider factors like performance trends, error types, user impact assessments, and even code change risk levels based on past deployment patterns. By weighting these factors appropriately, AI can make more nuanced decisions than traditional rule-based approaches.</p>
          <p>Our aim is to streamline this process by automating the decision to promote your release. We’ll revisit this topic in detail in <a data-type="xref" href="ch07.html#chapter_7_deploying_to_production_1749354011062634">Chapter 7</a>. </p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="From Manual to Automated Promotion"><div class="sect2" id="chapter_4_from_manual_to_automated_promotion_1749354010446792">
          <h2>From Manual to Automated Promotion</h2>
          <p>Once you’ve automated the decision-making process, the actual promotion of your build<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="promotion of build between environments" id="id675"/> becomes significantly easier. The key is to ensure that the deployment to the next environment is triggered immediately after the decision to proceed has been made, eliminating unnecessary wait times.</p>
          <p>How you implement this automation depends on your chosen continuous delivery tooling. Some tools offer end-to-end pipelines with simple, built-in triggers for seamless promotion between stages. Others allow you to call another pipeline or job as a step within your current pipeline, offering flexibility but potentially requiring more configuration. While the ease of implementation varies, achieving this level of automation is almost always possible.</p>
          <p>GitOps-style deployments, however, often present a unique challenge in this area, as we discussed in “Leverage Git Workflows with GitOps.” To execute the deployment, we need to automate the Git changes to the GitOps<a contenteditable="false" data-type="indexterm" data-primary="GitOps" data-secondary="automating Git changes" id="id676"/><a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="and GitOps" data-secondary-sortas="GitOps" id="id677"/> configurations instead of relying on manual updates. To do so, we will typically automate the pull request step and its approval directly within our CI/CD pipeline. We maintain Git as the source of truth that GitOps is known for while automating each step of our release promotion.</p>
          <p>For example, imagine a scenario where your pipeline has determined that a build is ready for promotion to the User Acceptance Testing (UAT) environment. When our pipeline is set up to generate the necessary pull request, trigger any required approvals, and (once approved) merge the changes into the main branch, our pipeline initiates the GitOps deployment to the UAT environment seamlessly.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Break the Environment Bottleneck"><div class="sect2" id="chapter_4_break_the_environment_bottleneck_1749354010446838">
          <h2>Break the Environment Bottleneck</h2>
          <p>A final challenge in automating promotion between stages and environments in your delivery process<a contenteditable="false" data-type="indexterm" data-primary="delivery process" data-secondary="breaking bottleneck in" id="id678"/> is determining the “right” number of environments that you’ll need. Having too many environments becomes a financial burden due to the cost of maintaining their underlying infrastructure, while having too few creates bottlenecks and delays in moving releases toward delivery, as the process waits on resources to become available.</p>
          <p>Ephemeral environments<a contenteditable="false" data-type="indexterm" data-primary="ephemeral environments" id="id679"/> present a common solution to this dilemma. This approach involves creating environments on demand when needed for testing and promptly dismantling them once tests are complete. In the pre-cloud era, environment creation was a laborious process, often taking days. Now, thanks to programmable cloud infrastructure, environments can be spun up and torn down in minutes.</p>
          <p>Infrastructure as Code Management (IaCM) tools<a contenteditable="false" data-type="indexterm" data-primary="Infrastructure as Code Management (IaCM) tools" id="id680"/><a contenteditable="false" data-type="indexterm" data-primary="IaCM (Infrastructure as Code Management) tools" id="id681"/><a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="IaCM" id="id682"/> simplify ephemeral environments. These specialized CI/CD platforms automate the provisioning, configuration, and deployment of infrastructure resources using code. Unlike traditional CI/CD tools focused on applications, IaCM tools manage the underlying infrastructure. With IaCM tools, you define your desired infrastructure state using declarative code templates, making configurations more manageable, maintainable, and version-controlled.</p>
          <p class="pagebreak-before">Ideally, to achieve our goal of “production-like” test environments, the same template should be used to create both pre-production test environments and production environments, with adjustments made only to variables. When your pipelines seamlessly integrate with IaCM tools, deploying to a “Test” stage automatically triggers the creation of a corresponding “Test” environment. Once this environment is provisioned and configured with necessary details like IP addresses, passwords, and other environment-specific variables, the deployment and testing processes can proceed. Upon completion, the IaCM tool efficiently dismantles the environment, freeing up resources.</p>
          <p>While this strategy offers significant benefits in terms of consistency, flexibility, and cost reduction, it’s important to note that the environment creation and teardown process can add a few minutes to the overall test cycle. Therefore, ephemeral environments might not be the ideal solution for pipelines targeting extremely rapid delivery cadences, such as those measured in minutes. However, for delivery cycles measured in hours, days, or weeks, ephemeral environments provide a powerful way to break bottlenecks, improve consistency, and optimize infrastructure costs<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_testingpromotionbetweenenvironments4241145" id="id683"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AIartificialintelligencesystemspromotionbetweenenvironments4241145" id="id684"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_promotionofreleasesbetweenenvironments4241145" id="id685"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipelineCDpromotionbetweenenvironments424176" id="id686"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="chapter_4_summary_1749354010446881">
        <h1>Summary</h1>
        <p>In this chapter we continued to navigate our delivery process, focusing on the continuous delivery steps that follow continuous integration. These are primarily testing steps, and we reviewed the types of tests that are important for validating all aspects of our software. We discussed the importance of reliable and predictable pre-production environments to testing and the best practices to give us these. By automating all aspects of promoting your release between testing stages, including promotion decision making, we can dramatically accelerate the delivery of our <span class="keep-together">software</span><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_testingdeployingtotestenvironments43348" id="id687"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CICDpipeline41057" id="id688"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymenttotestenvironments43348" id="id689"/>.</p>
        <p>After completing testing, there’s only one step left to get our latest software release into the hands of users: actually deploying to production. We’ll return to this step in <a data-type="xref" href="ch07.html#chapter_7_deploying_to_production_1749354011062634">Chapter 7</a>. Before we get there, we’ll take the next few chapters to discuss how we can fortify our releases to be more secure, more resilient, and more reliable.</p>
      </div></section>
    </div></section></div></div></body></html>