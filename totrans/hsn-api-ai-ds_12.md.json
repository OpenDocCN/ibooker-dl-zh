["```py\n.../chapter10 (main) $ mkdir airflow\n.../chapter10 (main) $ cd airflow\n.../airflow (main) $\n```", "```py\n.../airflow (main) $ curl -LfO \\\n'https://airflow.apache.org/docs/apache-airflow/2.10.0/docker-compose.yaml'\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 11342  100 11342    0     0   410k      0 --:--:-- --:--:-- --:--:--  410k\n```", "```py\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n```", "```py\n.../airflow (main) $ mkdir -p ./dags ./logs ./plugins ./config\n.../airflow (main) $ echo -e \"AIRFLOW_UID=$(id -u)\" > .env\n```", "```py\n.../airflow (main) $ touch docker-compose.override.yaml\n```", "```py\n#these are overrides to the default docker compose\nx-airflow-common:\n  &airflow-common\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false' ![1](assets/1.png)\n\nservices:\n  airflow-webserver:\n    <<: *airflow-common\n    command: webserver\n    environment:\n      <<: *airflow-common-env\n      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True' ![2](assets/2.png)\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    environment:\n      <<: *airflow-common-env\n      AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: '30' ![3](assets/3.png)\n```", "```py\n.../airflow (main) $ docker compose up airflow-init\n[+] Running 44/3\n  redis Pulled\n  postgres Pulled\n  airflow-init Pulled\n...\nairflow-init-1  | 2.10.0\nairflow-init-1 exited with code 0\n```", "```py\n.../airflow (main) $ docker compose up -d\n+] Running 7/7\n ✔ Container airflow-postgres-1           Healthy\n ✔ Container airflow-redis-1              Healthy\n ✔ Container airflow-airflow-init-1       Exited\n ✔ Container airflow-airflow-webserver-1  Started\n ✔ Container airflow-airflow-triggerer-1  Started\n ✔ Container airflow-airflow-scheduler-1  Started\n ✔ Container airflow-airflow-worker-1     Started\n```", "```py\n.../airflow (main) $ cd dags\n.../dags (main) $ sqlite3 analytics_database.db\nSQLite version 3.45.3 2024-04-15 13:34:05\nEnter \".help\" for usage hints.\nsqlite> CREATE TABLE player (\n    player_id INTEGER PRIMARY KEY,\n    gsis_id TEXT,\n    first_name TEXT,\n    last_name TEXT,\n    position TEXT,\n    last_changed_date DATE\n);\nsqlite> .exit\n```", "```py\n.../airflow (main) $ cd dags\n.../dags (main) $  touch recurring_player_api_insert_update_dag.py\n```", "```py\nimport datetime\nimport logging\nfrom airflow.decorators import dag ![1](assets/1.png)\nfrom airflow.providers.http.operators.http import HttpOperator ![2](assets/2.png)\nfrom airflow.operators.python import PythonOperator\nfrom shared_functions import upsert_player_data ![3](assets/3.png)\n\ndef health_check_response(response): ![4](assets/4.png)\n    logging.info(f\"Response status code: {response.status_code}\")\n    logging.info(f\"Response body: {response.text}\")\n    return response.status_code == 200 and response.json() == {\n        \"message\": \"API health check successful\"\n    }\n\ndef insert_update_player_data(**context): ![5](assets/5.png)\n\n    player_json = context[\"ti\"].xcom_pull(task_ids=\"api_player_query\") ![6](assets/6.png)\n\n    if player_json:\n        upsert_player_data(player_json) ![7](assets/7.png)\n    else:\n        logging.warning(\"No player data found.\")\n\n@dag(schedule_interval=None) ![8](assets/8.png)\ndef recurring_player_api_insert_update_dag():\n\n    api_health_check_task = HttpOperator(  ![9](assets/9.png)\n        task_id=\"check_api_health_check_endpoint\",\n        http_conn_id=\"sportsworldcentral_url\",\n        endpoint=\"/\",\n        method=\"GET\",\n        headers={\"Content-Type\": \"application/json\"},\n        response_check=health_check_response,\n    )\n\n    temp_min_last_change_date = \"2024-04-01\" ![10](assets/10.png)\n\n    api_player_query_task = HttpOperator( ![11](assets/11.png)\n       task_id=\"api_player_query\",\n       http_conn_id=\"sportsworldcentral_url\",\n       endpoint=(\n           f\"/v0/players/?skip=0&limit=100000&minimum_last_changed_date=\"\n           f\"{temp_min_last_change_date}\"\n       ),\n       method=\"GET\",\n       headers={\"Content-Type\": \"application/json\"},\n   )\n\n    player_sqlite_upsert_task = PythonOperator( ![12](assets/12.png)\n        task_id=\"player_sqlite_upsert\",\n        python_callable=insert_update_player_data,\n        provide_context=True,\n    )\n\n    # Run order of tasks\n    api_health_check_task >> api_player_query_task >> player_sqlite_upsert_task![13](assets/13.png)\n\n# Instantiate the DAG\ndag_instance = recurring_player_api_insert_update_dag() ![14](assets/14.png)\n```", "```py\n.../dags (main) $ touch shared_functions.py\n```", "```py\nimport logging\nimport json\nfrom airflow.hooks.base import BaseHook \n\ndef upsert_player_data(player_json):\n    import sqlite3\t\t\t![1](assets/1.png)\n    import pandas as pd\n\n# Fetch the connection object\n    database_conn_id = 'analytics_database'\n    connection = BaseHook.get_connection(database_conn_id) ![2](assets/2.png)\n\n    sqlite_db_path = connection.schema\n\n    if player_json:\n\n        player_data = json.loads(player_json)\n\n        # Use a context manager for the SQLite connection\n        with sqlite3.connect(sqlite_db_path) as conn:\n            cursor = conn.cursor()\t\t\t![3](assets/3.png)\n\n            # Insert each player record into the 'player' table\n            for player in player_data:\n                try:\n                    cursor.execute(\"\"\" ![4](assets/4.png)\n                        INSERT INTO player (\n                            player_id, gsis_id, first_name, last_name, \n                            position, last_changed_date\n                        ) \n                        VALUES (?, ?, ?, ?, ?, ?) \n                        ON CONFLICT(player_id) DO UPDATE ![5](assets/5.png)\n                        SET\n                            gsis_id = excluded.gsis_id,\n                            first_name = excluded.first_name,\n                            last_name = excluded.last_name,\n                            position = excluded.position,\n                            last_changed_date = excluded.last_changed_date\n                    \"\"\", (\n                       player['player_id'], player['gsis_id'],\n                       player['first_name'],\n                       player['last_name'],\n                       player['position'],\n                       player['last_changed_date']\n                   ))\n                except Exception as e:\n                   logging.error(\n                       f\"Failed to insert player {player['player_id']}: {e}\")\n                   raise\n\n    else:\n        logging.warning(\"No player data found.\")\n         raise ValueError(\n           \"No player data found. Task failed due to missing data.\")\n```", "```py\n.../dags (main) $ sqlite3 analytics_database.db\nSQLite version 3.45.3 2024-04-15 13:34:05\nEnter \".help\" for usage hints.\nsqlite> select count(*) from player;\n1018\n```"]