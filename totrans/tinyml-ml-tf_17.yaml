- en: Chapter 17\. Optimizing Model and Binary Size
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章。优化模型和二进制大小
- en: 'Whatever platform you choose, it’s likely that flash storage and RAM will be
    very limited. Most embedded systems have less than 1 MB of read-only storage in
    flash, and many have only tens of kilobytes. The same is true for memory: there’s
    seldom more than 512 KB of static RAM (SRAM) available, and on low-end devices
    that figure could be in the low single digits. The good news is that TensorFlow
    Lite for Microcontrollers is designed to work with as little as 20 KB of flash
    and 4 KB of SRAM, but you will need to design your application carefully and make
    engineering trade-offs to keep the footprint low. This chapter covers some of
    the approaches that you can use to monitor and control your memory and storage
    requirements.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪种平台，闪存存储和RAM都可能非常有限。大多数嵌入式系统的闪存只读存储器少于1 MB，许多只有几十KB。内存也是如此：很少有超过512 KB的静态RAM（SRAM）可用，而在低端设备上，这个数字可能只有几个KB。好消息是，TensorFlow
    Lite for Microcontrollers被设计为可以使用至少20 KB的闪存和4 KB的SRAM，但您需要仔细设计您的应用程序并做出工程权衡以保持占用空间较小。本章介绍了一些方法，您可以使用这些方法来监控和控制内存和存储需求。
- en: Understanding Your System’s Limits
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解系统的限制
- en: Most embedded systems have an architecture in which programs and other read-only
    data are stored in flash memory, which is written to only when new executables
    are uploaded. There’s usually also modifiable memory available, often using SRAM
    technology. This is the same technology used for caches on larger CPUs, and it
    gives fast access for low power consumption, but it’s limited in size. More advanced
    microcontrollers can offer a second tier of modifiable memory, using a more power-hungry
    but scalable technology like dynamic RAM (DRAM).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数嵌入式系统具有一种架构，其中程序和其他只读数据存储在闪存存储器中，仅在上传新可执行文件时才写入。通常还有可修改的内存可用，通常使用SRAM技术。这是用于较大CPU缓存的相同技术，它提供快速访问和低功耗，但尺寸有限。更先进的微控制器可以提供第二层可修改内存，使用更耗电但可扩展的技术，如动态RAM（DRAM）。
- en: You’ll need to understand what potential platforms offer and what the trade-offs
    are. For example, a chip that has a lot of secondary DRAM might be attractive
    for its flexibility, but if enabling that extra memory blows past your power budget,
    it might not be worth it. If you’re operating in the 1 mW-and-below power range
    that this book focuses on it’s usually not possible to use anything beyond SRAM,
    because larger memory approaches will consume too much energy. That means that
    the two key metrics you’ll need to consider are how much flash read-only storage
    is available and how much SRAM is available. These numbers should be listed in
    the description of any chip you’re looking at. Hopefully you won’t even need to
    dig as deeply as the datasheet [“Hardware Choice”](ch16.xhtml#hard_choice).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要了解潜在平台提供的内容以及权衡。例如，具有大量二级DRAM的芯片可能因其灵活性而具有吸引力，但如果启用额外的内存超出了您的功耗预算，那可能不值得。如果您正在操作本书关注的1
    mW及以下功率范围，通常不可能使用超出SRAM的任何东西，因为更大的内存方法将消耗太多能量。这意味着您需要考虑的两个关键指标是可用的闪存只读存储器量和可用的SRAM量。这些数字应列在您查看的任何芯片的描述中。希望您甚至不需要深入挖掘数据表[“硬件选择”](ch16.xhtml#hard_choice)。
- en: Estimating Memory Usage
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估算内存使用量
- en: When you have an idea of what your hardware options are, you need to develop
    an understanding of what resources your software will need and what trade offs
    you can make to control those requirements.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当您了解硬件选项时，您需要了解软件将需要的资源以及您可以做出的权衡来控制这些要求。
- en: Flash Usage
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 闪存使用量
- en: 'You can usually determine exactly how much room you’ll need in flash by compiling
    a complete executable, and then looking at the size of the resulting image. This
    can be confusing, because the first artifact that the linker produces is often
    an annotated version of the executable with debug symbols and section information,
    in a format like ELF (which we discuss in more detail in [“Measuring Code Size”](#measuring_code_size).
    The file you want to look at is the actual one that’s flashed to the device, often
    produced by a tool like `objcopy`. The simplest equation for gauging the amount
    of flash memory you need is the sum of the following factors:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，通过编译完整的可执行文件，然后查看生成图像的大小，您可以确定在闪存中需要多少空间。这可能会令人困惑，因为链接器生成的第一个工件通常是带有调试符号和部分信息的可执行文件的注释版本，格式类似于ELF（我们在[“测量代码大小”](#measuring_code_size)中更详细地讨论）。您要查看的文件是实际上刷入设备的文件，通常由`objcopy`等工具生成。用于估算所需闪存内存量的最简单方程式是以下因素之和：
- en: Operating system size
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统大小
- en: If you’re using any kind of real-time operating system (RTOS), you’ll need space
    in your executable to hold its code. This will usually be configurable depending
    on which features you’re using, and the simplest way to estimate the footprint
    is to build a sample “hello world” program with the features you need enabled.
    If you look at the image file size, this will give you a baseline for how large
    the OS program code is. Typical modules that can take up a lot of program space
    include USB, WiFi, Bluetooth, and cellular radio stacks, so ensure that these
    are enabled if you intend to use them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用任何类型的实时操作系统（RTOS），则需要在可执行文件中留出空间来保存其代码。这通常可以根据您使用的功能进行配置，并且估算占用空间的最简单方法是使用所需功能构建一个示例“hello
    world”程序。如果查看图像文件大小，这将为您提供OS程序代码有多大的基准。可能占用大量程序空间的典型模块包括USB、WiFi、蓝牙和蜂窝无线电堆栈，因此请确保启用它们，如果您打算使用它们。
- en: TensorFlow Lite for Microcontrollers code size
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite for Microcontrollers代码大小
- en: The ML framework needs space for the program logic to load and execute a neural
    network model, including the operator implementations that run the core arithmetic.
    Later in this chapter we discuss how to configure the framework to reduce the
    size for particular applications, but to get started just compile one of the standard
    unit tests (like [the `micro_speech` test](https://oreil.ly/7cafy)) that includes
    the framework and look at the resulting image size for an estimate.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架需要空间来加载和执行神经网络模型的程序逻辑，包括运行核心算术的操作实现。本章后面我们将讨论如何配置框架以减小特定应用程序的大小，但首先只需编译一个标准单元测试（比如[`micro_speech`测试](https://oreil.ly/7cafy)），其中包括框架，并查看估计的结果图像大小。
- en: Model data size
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型数据大小
- en: If you don’t yet have a model trained, you can get a good estimate of the amount
    of flash storage space it will need by counting its weights. For example, a fully
    connected layer will have a number of weights equal to the size of its input vector
    multiplied by the size of its output vector. For convolutional layers, it’s a
    bit more complex; you’ll need to multiply the width and height of the filter box
    by the number of input channels, and multiply this by the number of filters. You
    also need to add on storage space for any bias vectors associated with each layer.
    This can quickly become complex to calculate, so it can be easier just to create
    a candidate model in TensorFlow and then export it to a TensorFlow Lite file.
    This file will be directly mapped into flash, so its size will give you an exact
    figure for how much space it will take up. You can also look at the number of
    weights listed by [Keras’s `model.summary()` method](https://keras.io/models/about-keras-models).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有训练好的模型，可以通过计算其权重来估计它所需的闪存存储空间。例如，全连接层的权重数量等于其输入向量的大小乘以其输出向量的大小。对于卷积层，情况会更复杂一些；您需要将滤波框的宽度和高度乘以输入通道的数量，然后乘以滤波器的数量。您还需要为与每一层相关的任何偏置向量添加存储空间。这很快就会变得复杂，因此最简单的方法可能是在TensorFlow中创建一个候选模型，然后将其导出为TensorFlow
    Lite文件。该文件将直接映射到闪存中，因此其大小将为您提供占用多少空间的确切数字。您还可以查看[Keras的`model.summary()`方法](https://keras.io/models/about-keras-models)列出的权重数量。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We introduced quantization in [Chapter 4](ch04.xhtml#chapter_hello_world_training)
    and discussed it further in [Chapter 15](ch15.xhtml#optimizing_latency), but it’s
    worth a quick refresher in the context of model size. During training, weights
    are usually stored as floating-point values, taking up 4-bytes each in memory.
    Because space is such a constraint for mobile and embedded devices, TensorFlow
    Lite supports compressing those values down to a single byte in a process called
    *quantization*. It works by keeping track of the minimum and maximum values stored
    in a float array, and then converting all the values linearly to the closest of
    256 values equally spaced within that range. These codes are each stored in a
    byte, and arithmetic operations can be performed on them with a minimal loss of
    accuracy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](ch04.xhtml#chapter_hello_world_training)中介绍了量化，并在[第15章](ch15.xhtml#optimizing_latency)中进一步讨论了它，但在模型大小的背景下进行一个快速的复习是值得的。在训练期间，权重通常以浮点值存储，每个占用4个字节的内存。由于空间对于移动和嵌入式设备来说是一个限制，TensorFlow
    Lite支持将这些值压缩到一个字节中，这个过程称为*量化*。它通过跟踪存储在浮点数组中的最小值和最大值，然后将所有值线性转换为该范围内均匀间隔的256个值中最接近的一个。这些代码都存储在一个字节中，可以对它们进行算术运算而几乎不损失精度。
- en: Application code size
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序代码大小
- en: You’ll need code to access sensor data, preprocess it to prepare it for the
    neural network, and respond to the results. You might also need some other kinds
    of user interface and business logic outside of the machine learning module. This
    can be difficult to estimate, but you should at least try to understand whether
    you’ll need any external libraries (for example, for fast Fourier transforms)
    and calculate what their code space requirements will be.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要编写代码来访问传感器数据，对其进行预处理以准备神经网络，并响应结果。您可能还需要一些其他类型的用户界面和机器学习模块之外的业务逻辑。这可能很难估计，但您至少应该尝试了解是否需要任何外部库（例如用于快速傅立叶变换），并计算它们的代码空间需求。
- en: RAM Usage
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAM使用量
- en: 'Determining the amount of modifiable memory you’ll need can be more of a challenge
    than understanding the storage requirements, because the amount of RAM used varies
    over the life of your program. In a similar way to the process of estimating flash
    requirements, you’ll need to look at the different layers of your software to
    estimate the overall usage requirements:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 确定所需的可修改内存量可能比理解存储需求更具挑战性，因为程序的RAM使用量会随着程序的生命周期而变化。类似于估计闪存需求的过程，您需要查看软件的不同层以估计整体使用要求：
- en: Operating system size
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统大小
- en: Most [RTOSs (like FreeRTOS)](https://www.freertos.org/FAQMem.html) document
    how much RAM their different configuration options need, and you should be able
    to use this information to plan the required size. You will need to watch for
    modules that might require buffers—especially communication stacks like TCP/IP,
    WiFi, or Bluetooth. These will need to be added to any core OS requirements.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数[RTOS（如FreeRTOS）](https://www.freertos.org/FAQMem.html)记录了它们不同配置选项所需的RAM量，您应该能够使用这些信息来规划所需的大小。您需要注意可能需要缓冲区的模块，特别是通信堆栈如TCP/IP、WiFi或蓝牙。这些将需要添加到任何核心操作系统要求中。
- en: TensorFlow Lite for Microcontrollers RAM size
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 微控制器的TensorFlow Lite RAM大小
- en: The ML framework doesn’t have large memory needs for its core runtime and shouldn’t
    require more than a few kilobytes of space in SRAM for its data structures. These
    are allocated as part of the classes used for the interpreter, so whether your
    application code creates these as global or local objects will determine whether
    they’re on the stack or in general memory. We generally recommend creating them
    as global or `static` objects, because the lack of space will usually cause an
    error at linker time, whereas stack-allocated locals can cause a runtime crash
    that’s more difficult to understand.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ML框架的核心运行时不需要大量内存，并且其数据结构在SRAM中不应该需要超过几千字节的空间。这些分配为解释器使用的类的一部分，因此您的应用程序代码是将这些创建为全局或局部对象将决定它们是在堆栈上还是在一般内存中。我们通常建议将它们创建为全局或`static`对象，因为空间不足通常会导致链接时错误，而堆栈分配的局部变量可能会导致更难理解的运行时崩溃。
- en: Model memory size
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模型内存大小
- en: When a neural network is executed, the results of one layer are fed into subsequent
    operations and so must be kept around for some time. The lifetimes of these activation
    layers vary depending on their position in the graph, and the memory size needed
    for each is controlled by the shape of the array that a layer writes out. These
    variations mean that it’s necessary to calculate a plan over time to fit all these
    temporary buffers into as small an area of memory as possible. Currently this
    is done when the model is first loaded by the interpreter, so if the arena is
    not big enough, you’ll see an error on the console. If you see the difference
    between the available memory and what’s required in the error message and increase
    the arena by that amount, you should be able to run past that error.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络执行时，一个层的结果被馈送到后续操作中，因此必须保留一段时间。这些激活层的寿命因其在图中的位置而异，每个激活层所需的内存大小由层写出的数组的形状控制。这些变化意味着需要随时间计划以将所有这些临时缓冲区尽可能地放入内存的小区域中。目前，这是在解释器首次加载模型时完成的，因此如果竞技场不够大，您将在控制台上看到错误。如果您在错误消息中看到可用内存与所需内存之间的差异，并将竞技场增加该数量，您应该能够解决该错误。
- en: Application memory size
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序内存大小
- en: Like the program size, memory usage for your application logic can be difficult
    to calculate before it’s written. You can make some guesses about larger users
    of memory, though, such as buffers that you’ll need for storing incoming sample
    data, or areas of memory that libraries will need for preprocessing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与程序大小一样，应用程序逻辑的内存使用在编写之前可能很难计算。但是，您可以对内存的更大使用者进行一些猜测，例如您将需要用于存储传入样本数据的缓冲区，或者库将需要用于预处理的内存区域。
- en: Ballpark Figures for Model Accuracy and Size on Different Problems
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同问题上模型准确性和大小的大致数字
- en: It’s helpful to understand what the current state of the art is for different
    kinds of problems in order to help you plan for what you might be able to achieve
    for your application. Machine learning isn’t magic, and having a sense of its
    limitations will help you make smart trade-offs as you’re building your product.
    [Chapter 14](ch14.xhtml#designing_your_own_tinyml_applications), which examines
    the design process, is a good place to begin developing your intuition, but you’ll
    also need to think about how accuracy degrades as models are forced into tight
    resource constraints. To help with that, here are a few examples of architectures
    designed for embedded systems. If one of them is close to what you need to do,
    it might help you to envision what you could achieve at the end of your model
    creation process. Obviously your actual results it will depend a lot on your specific
    product and environment, so use these as guidelines for planning and don’t rely
    on being able to achieve exactly the same performance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 了解不同类型问题的当前技术水平将有助于您规划您的应用程序可能实现的目标。机器学习并非魔法，了解其局限性将有助于您在构建产品时做出明智的权衡。[第14章](ch14.xhtml#designing_your_own_tinyml_applications)探讨了设计过程，是开始培养直觉的好地方，但您还需要考虑随着模型被迫适应严格资源限制时准确性如何下降。为了帮助您，这里有一些为嵌入式系统设计的架构示例。如果其中一个接近您需要做的事情，可能会帮助您设想在模型创建过程结束时可能实现的结果。显然，您的实际结果将在很大程度上取决于您的具体产品和环境，因此请将这些作为规划的指导，并不要依赖于能够实现完全相同的性能。
- en: Speech Wake-Word Model
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音唤醒词模型
- en: 'The small (18 KB) model using 400,000 arithmetic operations that we covered
    earlier as a code sample is able to achieve 85% top-one accuracy (see [“Establish
    a Metric”](ch18.xhtml#establish_a_metric)) when distinguishing between four classes
    of sound: silence, unknown words, “yes,” and “no.” This is the training evaluation
    metric, which means it’s the result of presenting one-second clips and asking
    the model to do a one-shot classification of its input. In practice, you’d usually
    use the model on streaming audio, repeatedly predicting a result based on a one-second
    window that’s incrementally moving forward in time, so the actual accuracy in
    practical applications is lower than that figure might suggest. You should generally
    think about an audio model this size as a first-stage gatekeeper in a larger cascade
    of processing, so that its errors can be tolerated and dealt with by more complex
    models.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到的使用400,000次算术运算的小型（18 KB）模型作为代码示例，能够在区分四类声音时达到85%的一级准确性（参见[“建立度量”](ch18.xhtml#establish_a_metric)）。这是训练评估指标，这意味着通过呈现一秒钟的片段并要求模型对其输入进行一次分类来获得结果。在实践中，您通常会在流式音频上使用模型，根据逐渐向前移动的一秒钟窗口重复预测结果，因此在实际应用中的实际准确性低于该数字可能表明的准确性。您通常应该将这种大小的音频模型视为更大处理级联中的第一阶段门卫，以便更复杂的模型可以容忍和处理其错误。
- en: As a rule of thumb, you might need a model with 300 to 400 KB of weights and
    low-tens-of-millions of arithmetic operations to be able to detect a wake word
    with acceptable enough accuracy to use in a voice interface. Unfortunately you’ll
    also need a commercial-quality dataset to train on, given that there still aren’t
    enough open repositories of labeled speech data available, but hopefully that
    restriction will ease over time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个经验法则，您可能需要一个具有300到400 KB权重和数千万算术操作的模型，才能以足够可接受的准确性检测唤醒词，以在语音界面中使用。不幸的是，您还需要一个商业质量的数据集进行训练，因为目前仍然没有足够的开放标记语音数据库可用，但希望这种限制随着时间的推移会减轻。
- en: Accelerometer Predictive Maintenance Model
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加速度计预测性维护模型
- en: There are a wide range of different predictive maintenance problems, but one
    of the simpler cases is detecting a bearing failure in a motor. This often appears
    as distinctive shaking that can be spotted as patterns in accelerometer data.
    A reasonable model to spot these patterns might require only a few thousand weights,
    making it less than 10 KB in size, and a few hundred thousand arithmetic operations.
    You could expect better than 95% accuracy at classifying these events with such
    a model, and you can imagine scaling up the complexity of your model from there
    to handle more difficult problems (such as detecting failures on a machine with
    many moving parts or that’s traveling itself). Of course, the number of parameters
    and operations would scale up, as well.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种不同的预测性维护问题，但其中一个较简单的情况是检测电机轴承故障。这通常表现为加速度计数据中可以看到的明显震动模式。一个合理的模型来识别这些模式可能只需要几千个权重，使其大小不到10
    KB，并且数十万个算术操作。您可以期望使用这样的模型对这些事件进行分类的准确率超过95％，并且您可以想象从那里增加模型的复杂性来处理更困难的问题（例如检测具有许多移动部件或自行移动的机器上的故障）。当然，参数和操作的数量也会相应增加。
- en: Person Presence Detection
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人员存在检测
- en: Computer vision hasn’t been a common task on embedded platforms, so we’re still
    figuring out what applications make sense. One common request we’ve heard is the
    ability to detect when a person is nearby, to wake up a user interface or do other
    more power-hungry processing that it’s not possible to leave running all the time.
    We’ve tried to formally capture the requirements of this problem in the [Visual
    Wake Word Challenge](https://oreil.ly/E8GoU), and the results show that you can
    expect roughly 90% accuracy with binary classification of a small (96 × 96–pixel)
    monochrome image if you use a 250 KB model and around 60 million arithmetic operations.
    This is the baseline from using a scaled-down MobileNet v2 architecture (as described
    earlier in the book), so we hope to see the accuracy improve as more researchers
    tackle this specialized set of requirements, but it gives you a rough estimate
    of how well you might be able to do on visual problems within a microcontroller’s
    memory footprint. You might wonder how such a small model would do on the popular
    ImageNet–1,000 category problem—it’s hard to say exactly because the final fully
    connected layer for a thousand classes quickly takes up a hundred or more kilobytes
    (the number of parameters is the embedding input multiplied by the class count),
    but for a total size of around 500 KB, you could expect somewhere around 50% top-one
    accuracy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉在嵌入式平台上并不是常见的任务，因此我们仍在探索哪些应用是有意义的。我们听到的一个常见请求是能够检测到附近有人时，唤醒用户界面或执行其他更耗电的处理，这是不可能一直运行的。我们试图在[Visual
    Wake Word Challenge](https://oreil.ly/E8GoU)中正式捕捉这个问题的要求，结果显示，如果使用一个250 KB模型和大约6000万算术操作，您可以期望在一个小（96×96像素）单色图像的二进制分类中获得大约90%的准确性。这是使用缩减版MobileNet
    v2架构的基线（如本书中早期描述的），因此我们希望随着更多研究人员解决这一特殊需求集，准确性会提高，但它给出了您在微控制器内存占用中可能在视觉问题上表现如何的粗略估计。您可能会想知道这样一个小模型在流行的ImageNet-1000类别问题上会表现如何
    - 很难说确切的原因是最终的全连接层对于一千个类别很快就会占用一百多千字节（参数数量是嵌入输入乘以类别计数），但对于大约500 KB的总大小，您可以期望在top-one准确性方面达到大约50%。
- en: Model Choice
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择
- en: In terms of optimizing model and binary size, we highly recommend starting with
    an existing model. As we discuss in [Chapter 14](ch14.xhtml#designing_your_own_tinyml_applications),
    the most fruitful area to invest in is data gathering and improvement rather than
    tweaking architectures, and starting with a known model will let you focus on
    data improvements as early as possible. Machine learning software on embedded
    platforms is also still in its early stages, so using an existing model increases
    the chances that its ops are supported and well-optimized on the devices you care
    about. We’re hoping that the code samples accompanying this book will be good
    starting points for a lot of different applications—we chose them to cover as
    many different kinds of sensor input as we could—but if they don’t fit your use
    cases you might be able to search for some alternatives online. If you can’t find
    a size-optimized architecture that’s suitable, you can look into building your
    own from scratch in the training environment of TensorFlow, but as Chapters [Chapter 13](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers)
    and [Chapter 19](ch19.xhtml#ch19) discuss, it can be an involved process to successfully
    port that onto a microcontroller.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化模型和二进制大小方面，我们强烈建议从现有模型开始。正如我们在[第14章](ch14.xhtml#designing_your_own_tinyml_applications)中讨论的那样，投资最有价值的领域是数据收集和改进，而不是调整架构，从已知模型开始将让您尽早专注于数据改进。嵌入式平台上的机器学习软件也仍处于早期阶段，因此使用现有模型增加了其操作在您关心的设备上得到支持和优化的机会。我们希望本书附带的代码示例将成为许多不同应用的良好起点
    - 我们选择它们以涵盖尽可能多种不同类型的传感器输入，但如果它们不适合您的用例，您可能可以在线搜索一些替代方案。如果找不到适合的大小优化架构，您可以尝试在TensorFlow的训练环境中从头开始构建自己的架构，但正如[第13章](ch13.xhtml#chapter_tensorflow_lite_for_microcontrollers)和[第19章](ch19.xhtml#ch19)讨论的那样，成功将其移植到微控制器可能是一个复杂的过程。
- en: Reducing the Size of Your Executable
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减小可执行文件的大小
- en: 'Your model is likely to be one of the biggest consumers of read-only memory
    in a microcontroller application, but you also must think about how much space
    your compiled code takes. This constraint on code size is the reason that we can’t
    just use an unmodified version of TensorFlow Lite when targeting embedded platforms:
    it would take up many hundreds of kilobytes of flash memory. TensorFlow Lite for
    Microcontrollers can compile down to as little as 20 KB, but this can require
    you to make some changes to exclude the parts of the code that you don’t need
    for your application.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型可能是微控制器应用程序中只读内存的最大消耗者之一，但您还必须考虑编译代码占用了多少空间。代码大小的限制是我们在针对嵌入式平台时不能只使用未经修改的TensorFlow
    Lite的原因：它将占用数百KB的闪存内存。TensorFlow Lite for Microcontrollers可以缩减至至少20 KB，但这可能需要您进行一些更改，以排除您的应用程序不需要的代码部分。
- en: Measuring Code Size
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量代码大小
- en: Before you begin optimizing the size of your code, you need to know how big
    it is. This can be a little tricky on embedded platforms because the output of
    the building process is often a file that includes debugging and other information
    that’s not transferred onto the embedded device and so shouldn’t count toward
    the total size limit. On Arm and other modern toolchains this is often known as
    an Executable and Linking Format (ELF) file, whether or not it has an *.elf* suffix.
    If you’re on a Linux or macOS development machine, you can run the `file` command
    to investigate the output of your toolchain; it will show you whether a file is
    an ELF.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始优化代码大小之前，您需要知道它有多大。在嵌入式平台上，这可能有点棘手，因为构建过程的输出通常是一个文件，其中包含调试和其他信息，这些信息不会传输到嵌入式设备上，因此不应计入总大小限制。在Arm和其他现代工具链中，这通常被称为可执行和链接格式（ELF）文件，无论是否具有*.elf*后缀。如果您在Linux或macOS开发机器上，可以运行`file`命令来调查您的工具链的输出；它将向您显示文件是否为ELF。
- en: 'The better file to look at is what’s often known as the *bin*: the binary snapshot
    of the code that’s actually uploaded to the flash storage of an embedded device.
    This will usually be exactly the size of the read-only flash memory that will
    be used, so you can use it to understand what the usage actually is. You can find
    out its size by using a command line like `ls -l` or `dir` on the host, or even
    inspecting it in a GUI file viewer. Not all toolchains automatically show you
    this *bin* file, and it might not have any suffix, but it’s the file that you
    download and drag onto your device through USB on Mbed, and with the gcc toolchain
    you produce it by running something like `arm-none-eabi-objcopy app.elf app.bin
    -O binary`. It’s not helpful to look at the *.o* intermediates, or even the *.a*
    libraries that the build process produces, because they contain a lot of metadata
    that doesn’t make it into the final code footprint, and a lot of the code might
    be pruned as unused.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 查看的更好文件通常被称为*bin*：实际上传到嵌入式设备的闪存存储的代码二进制快照。这通常会完全等于将要使用的只读闪存内存的大小，因此您可以使用它来了解实际使用情况。您可以通过在主机上使用`ls
    -l`或`dir`之类的命令行，甚至在GUI文件查看器中检查它来找出其大小。并非所有工具链都会自动显示这个*bin*文件，它可能没有任何后缀，但它是您通过USB在Mbed上下载并拖放到设备上的文件，并且使用gcc工具链可以通过运行类似`arm-none-eabi-objcopy
    app.elf app.bin -O binary`来生成它。查看*.o*中间文件或甚至构建过程生成的*.a*库并不有用，因为它们包含了许多元数据，这些元数据不会出现在最终代码占用空间中，并且很多代码可能会被修剪为未使用。
- en: Because we expect you to compile your model into your executable as a C data
    array (since you can’t rely on a filesystem being present to load it from), the
    binary size you see for any program including the model will contain the model
    data. To understand how much space your actual code is taking, you’ll need to
    subtract this model size from the binary file length. The model size should usually
    be defined in the file that contains the C data array (like at the end of [*tiny_conv_micro_features_model_data.cc*](https://oreil.ly/Vknl2)),
    so you can subtract that from the binary file size to understand the real code
    footprint.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们期望您将模型编译为可执行文件中的C数据数组（因为您不能依赖存在文件系统来加载它），所以包括模型的任何程序的二进制大小将包含模型数据。要了解实际代码占用了多少空间，您需要从二进制文件长度中减去这个模型大小。模型大小通常应在包含C数据数组的文件中定义（比如在[*tiny_conv_micro_features_model_data.cc*](https://oreil.ly/Vknl2)的末尾），因此您可以从二进制文件大小中减去它以了解真实的代码占用空间。
- en: How Much Space Is Tensorflow Lite for Microcontrollers Taking?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tensorflow Lite for Microcontrollers占用了多少空间？
- en: When you know your entire application’s code footprint size, you might want
    to investigate how much space is being taken up by TensorFlow Lite. The simplest
    way to test this is by commenting out all your calls to the framework (including
    the creation of objects like `OpResolvers` and interpreters) and seeing how much
    smaller the binary becomes. You should expect at least a 20 to 30 KB decrease,
    so if you don’t see anything like that, you should double-check that you’ve caught
    all the references. This should work because the linker will strip out any code
    that you’re never calling, removing it from the footprint. This can be extended
    to other modules of your code, too—as long as you ensure there are no references—to
    help create a better understanding of where the space is going.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当您了解整个应用程序的代码占用空间大小时，您可能想要调查TensorFlow Lite占用了多少空间。测试这一点的最简单方法是注释掉所有对框架的调用（包括创建`OpResolvers`和解释器等对象），看看二进制文件变小了多少。您应该至少期望减少20到30
    KB，因此如果您没有看到类似的情况，您应该再次检查是否捕捉到了所有引用。这应该有效，因为链接器将剥离您从未调用的任何代码，将其从占用空间中删除。这也可以扩展到代码的其他模块，只要确保没有引用，以帮助更好地了解空间的去向。
- en: OpResolver
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpResolver
- en: TensorFlow Lite supports over a hundred operations, but it’s unlikely that you’ll
    need all of them within a single model. The individual implementations of each
    operation might take up only a few kilobytes, but the total quickly adds up with
    so many available. Luckily, there is a built-in mechanism to remove the code footprint
    of operations you don’t need.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite支持100多种操作，但在单个模型中不太可能需要所有这些操作。每个操作的单独实现可能只占用几千字节，但随着这么多可用的操作，总量很快就会增加。幸运的是，有一种内置机制可以去除你不需要的操作的代码占用空间。
- en: When TensorFlow Lite loads a model, it searches for implementations of each
    included op using the [`OpResolver` interface](https://oreil.ly/dfwOP). This is
    a class you pass into the interpreter to load a model, and it contains the logic
    to find the function pointers to an op’s implementation given the op definition.
    The reason this exists is so that you can control which implementations are actually
    linked in. For most of the sample code, you’ll see that we’re creating and passing
    in an instance of the [`AllOpsResolver` class](https://oreil.ly/tbzg6). As we
    discussed in [Chapter 5](ch05.xhtml#chapter_building_an_application), this implements
    the `OpResolver` interface, and as the name implies, it has an entry for every
    operation that’s supported in TensorFlow Lite for Microcontrollers. This is convenient
    for getting started, because it means that you can load any supported model without
    worrying about what operations it contains.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当TensorFlow Lite加载模型时，它会使用[`OpResolver`接口](https://oreil.ly/dfwOP)来搜索每个包含的操作的实现。这是一个你传递给解释器以加载模型的类，它包含了查找函数指针以获取操作实现的逻辑，给定操作定义。存在这个的原因是为了让你可以控制哪些实现实际上被链接进来。在大多数示例代码中，你会看到我们正在创建并传递一个[`AllOpsResolver`类的实例](https://oreil.ly/tbzg6)。正如我们在[第5章](ch05.xhtml#chapter_building_an_application)中讨论的那样，这实现了`OpResolver`接口，正如其名称所示，它为TensorFlow
    Lite for Microcontrollers中支持的每个操作都有一个条目。这对于入门很方便，因为这意味着你可以加载任何支持的模型，而不必担心它包含哪些操作。
- en: When you get to the point of worrying about code size, however, you’ll want
    to revisit this class. Instead of passing in an instance of `AllOpsResolver` in
    your application’s main loop, copy the *all_ops_resolver.cc* and *.h* files into
    your application and rename them to *my_app_resolver.cc* and *.h*, with the class
    renamed to `MyAppResolver`. Inside the constructor of your class, remove all the
    `AddBuiltin()` calls that apply to ops that you don’t use within your model. Unfortunately
    we know of an easy automatic way to create the list of operations a model uses,
    but the [Netron](https://oreil.ly/MKqF9) model viewer is a nice tool that can
    help with the process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你开始担心代码大小时，你会想要重新审视这个类。在你的应用程序主循环中，不要再传递`AllOpsResolver`的实例，而是将*all_ops_resolver.cc*和*.h*文件复制到你的应用程序中，并将它们重命名为*my_app_resolver.cc*和*.h*，类重命名为`MyAppResolver`。在你的类构造函数中，删除所有适用于你模型中不使用的操作的`AddBuiltin()`调用。不幸的是，我们不知道有一种简单的自动方式来创建模型使用的操作列表，但[Netron](https://oreil.ly/MKqF9)模型查看器是一个可以帮助这个过程的好工具。
- en: Make sure that you replace the `AllOpsResolver` instance you were passing into
    your interpreter with `MyAppResolver`. Now, as soon as you compile your app, you
    should see the size noticeably shrink. The reason behind this change is that most
    linkers automatically try to remove code that can’t be called (or *dead code*).
    By removing the references that were in `AllOpsResolver`, you allow the linker
    to determine that it can exclude all the op implementations that are no longer
    listed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你用`MyAppResolver`替换你传递给解释器的`AllOpsResolver`实例。现在，一旦编译你的应用程序，你应该会看到大小明显缩小。这个改变背后的原因是，大多数链接器会自动尝试删除不能被调用的代码（或*死代码*）。通过删除`AllOpsResolver`中的引用，你允许链接器确定可以排除所有不再列出的操作实现。
- en: 'If you use only a few ops, you don’t need to wrap registration in a new class,
    like we do with the large `AllOpsResolver`. Instead, you can create an instance
    of the `MicroMutableOpResolver` class and directly add the op registrations you
    need. `MicroMutableOpResolver` implements the `OpResolver` interface, but has
    additional methods that let you add ops to the list (which is why it’s named `Mutable`).
    This is the class that’s used to implement `AllOpsResolver`, and it’s a good base
    for any of your own resolver classes, too, but it can be simpler to call it directly.
    We use this approach in some of the examples, and you can see how it works in
    this snippet from the [`micro_speech` example](https://oreil.ly/gdZts):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只使用了少数操作，你不需要像我们使用大型`AllOpsResolver`那样将注册包装在一个新类中。相反，你可以创建一个`MicroMutableOpResolver`类的实例，并直接添加你需要的操作注册。`MicroMutableOpResolver`实现了`OpResolver`接口，但有额外的方法让你添加操作到列表中（这就是为什么它被命名为`Mutable`）。这是用来实现`AllOpsResolver`的类，也是你自己的解析器类的一个很好的基础，但直接调用它可能更简单。我们在一些示例中使用了这种方法，你可以在这个来自[`micro_speech`示例](https://oreil.ly/gdZts)的片段中看到它是如何工作的：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You might notice that we’re declaring the resolver object as `static`. This
    is because the interpreter can call into it at any time, so its lifetime needs
    to be at least as long as the object we created for the interpreter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到我们将解析器对象声明为`static`。这是因为解释器可以随时调用它，所以它的生命周期至少需要与我们为解释器创建的对象一样长。
- en: Understanding the Size of Individual Functions
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解单个函数的大小
- en: 'If you’re using the GCC toolchain, you can use tools like `nm` to get information
    on the size of functions and objects in object (*.o*) intermediate files. Here’s
    an example of building a binary and then inspecting the size of items in the compiled
    *audio_provider.cc* object file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用GCC工具链，你可以使用像`nm`这样的工具来获取目标（*.o*）中间文件中函数和对象的大小信息。这里有一个构建二进制文件然后检查编译后的*audio_provider.cc*对象文件中项目大小的示例：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see results that look something like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到类似以下的结果：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Many of these symbols are internal details or irrelevant, but the last few are
    recognizable as functions we define in *audio_provider.cc*, with their names mangled
    to match C++ linker conventions. The second column shows what their size is in
    hexadecimal. You can see here that the `InitAudioRecording()` function is `0x2c4`
    or 708 bytes, which could be quite significant on a small microcontroller, so
    if space were tight it would be worth investigating where the size inside the
    function is coming from.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些符号是内部细节或无关紧要的，但最后几个可以识别为我们在*audio_provider.cc*中定义的函数，它们的名称被搅乱以匹配C++链接器约定。第二列显示它们的大小是多少十六进制。您可以看到`InitAudioRecording()`函数的大小为`0x2c4`或708字节，这在小型微控制器上可能相当显著，因此如果空间紧张，值得调查函数内部大小的来源。
- en: 'The best way we’ve found to do this is to disassemble the functions with the
    source code intermingled. Luckily, the `objdump` tool lets us do this by using
    the `-S` flag—but unlike with `nm`, you can’t use the standard version that’s
    installed on your Linux or macOS desktop. Instead, you need to use one that came
    with your toolchain. This will usually be downloaded automatically if you’re using
    the TensorFlow Lite for Microcontrollers Makefile to build. It will exist somewhere
    like *tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin*. Here’s a command
    to run to see more about the functions inside *audio_provider.cc*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现的最佳方法是将源代码与反汇编函数混合在一起。幸运的是，`objdump`工具通过使用`-S`标志让我们可以做到这一点——但与`nm`不同，您不能使用安装在Linux或macOS桌面上的标准版本。相反，您需要使用随您的工具链一起提供的版本。如果您正在使用TensorFlow
    Lite for Microcontrollers的Makefile构建，通常会自动下载。它通常会存在于类似*tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin*的位置。以下是一个运行以查看*audio_provider.cc*内部函数更多信息的命令：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We won’t show all of the output, because it’s so long; instead, we present
    an abridged version showing only the function we were curious about:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会展示所有的输出，因为太长了；相反，我们只展示一个简化版本，只显示我们感兴趣的函数：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You don’t need to understand what the assembly is doing, but hopefully you can
    see where the space is going by seeing how the function size (the number on the
    far left of the disassembled lines; for example, hexadecimal *10* at the end of
    `InitAudioRecording()`) increases for each of the C++ source lines. What is revealed
    if you look at the entire function is that all of the hardware initialization
    code has been inlined within the `InitAudioRecording()` implementation, which
    explains why it’s so large.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要理解汇编在做什么，但希望您可以看到通过查看函数大小（反汇编行最左边的数字；例如，在`InitAudioRecording()`末尾的十六进制*10*）如何随着每个C++源代码行的增加而增加。如果查看整个函数，您会发现所有的硬件初始化代码都已内联在`InitAudioRecording()`实现中，这解释了为什么它如此庞大。
- en: Framework Constants
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架常量
- en: There are a few places in the library code where we use hardcoded sizes for
    arrays to avoid dynamic memory allocation. If RAM space becomes very tight, it’s
    worth experimenting to see whether you can reduce them for your application (or,
    for very complex use cases, you might even need to increase them). One of these
    arrays is [`TFLITE_REGISTRATIONS_MAX`](https://oreil.ly/hYTLi), which controls
    how many different operations can be registered. The default is 128, which is
    probably far too many for most applications—especially given that it creates an
    array of 128 `TfLiteRegistration` structs, which are at least 32 bytes each, requiring
    4 KB of RAM. You can also look at lesser offenders like [`kStackDataAllocatorSize`](https://oreil.ly/wIsPm)
    in `MicroInterpreter`, or try shrinking the size of the arena you pass into the
    constructor of your interpreter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在库代码中有一些地方我们使用硬编码的数组大小来避免动态内存分配。如果RAM空间非常紧张，值得尝试看看是否可以减少它们以适应您的应用程序（或者，对于非常复杂的用例，甚至可能需要增加它们）。其中一个数组是[`TFLITE_REGISTRATIONS_MAX`](https://oreil.ly/hYTLi)，它控制可以注册多少不同的操作。默认值为128，这对于大多数应用程序来说可能太多了——特别是考虑到它创建了一个包含128个`TfLiteRegistration`结构的数组，每个结构至少占用32字节，需要4
    KB的RAM。您还可以查看像`MicroInterpreter`中的[`kStackDataAllocatorSize`](https://oreil.ly/wIsPm)这样的较小的问题，或者尝试缩小您传递给解释器构造函数的arena的大小。
- en: Truly Tiny Models
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真正微小的模型
- en: A lot of the advice in this chapter is related to embedded systems that can
    afford to use 20 KB of code footprint on framework code to run machine learning,
    and aren’t trying to scrape by with less than 10 KB of RAM. If you have a device
    with extremely tight resource constraints—for example, just a couple of kilobytes
    of RAM or flash—you aren’t going to be able to use the same approach. For those
    environments, you will need to write custom code and hand-tune everything extremely
    carefully to reduce the size.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的许多建议都与能够承受使用20 KB框架代码占用的嵌入式系统有关，以运行机器学习，并且不试图仅使用不到10 KB的RAM。如果您的设备资源约束非常严格——例如，只有几千字节的RAM或闪存，您将无法使用相同的方法。对于这些环境，您需要编写自定义代码，并非常小心地调整每个细节以减小大小。
- en: We hope that TensorFlow Lite for Microcontrollers can still be useful in these
    situations, though. We recommend that you still train a model in TensorFlow, even
    if it’s tiny, and then use the export workflow to create a TensorFlow Lite model
    file from it. This can be a good starting point for extracting the weights, and
    you can use the existing framework code to verify the results of your custom version.
    The reference implementations of the ops you’re using should be good starting
    points for your own op code, too; they should be portable, understandable, and
    memory efficient, even if they’re not optimal for latency.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望TensorFlow Lite for Microcontrollers在这些情况下仍然有用。我们建议您仍然在TensorFlow中训练一个模型，即使它很小，然后使用导出工作流从中创建一个TensorFlow
    Lite模型文件。这可以作为提取权重的良好起点，并且您可以使用现有的框架代码来验证您自定义版本的结果。您正在使用的操作的参考实现也应该是您自己操作代码的良好起点；它们应该是可移植的、易于理解的，并且在内存效率方面表现良好，即使它们对延迟不是最佳的。
- en: Wrapping Up
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at some of the best techniques to shrink the amount
    of storage you need for your embedded machine learning project. This is likely
    to be one of the toughest constraints you’ll need to overcome, but when you have
    an application that’s small enough, fast enough, and doesn’t use too much energy,
    you’ve got a clear path to shipping your product. What remains is rooting out
    all of the inevitable gremlins that will cause your device to behave in unexpected
    ways. Debugging can be a frustrating process (we’ve heard it described as a murder
    mystery where you’re the detective, the victim, and the murderer), but it’s an
    essential skill to learn to get products out the door. [Chapter 18](ch18.xhtml#debugging)
    covers the basic techniques that can help you understand what’s happening in a
    machine learning system.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们看了一些最好的技术，来缩小嵌入式机器学习项目所需的存储量。这很可能是你需要克服的最艰难的限制之一，但当你拥有一个足够小、足够快、并且不消耗太多能量的应用程序时，你就有了一个明确的路径来推出你的产品。剩下的是排除所有不可避免的小精灵，它们会导致你的设备以意想不到的方式行为。调试可能是一个令人沮丧的过程（我们听说过它被描述为一场谋杀案，你是侦探、受害者和凶手），但这是一个必须学会的技能，以便将产品推向市场。[第18章](ch18.xhtml#debugging)介绍了可以帮助你理解机器学习系统发生了什么的基本技术。
