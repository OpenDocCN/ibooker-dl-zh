["```py\nimport torch\ninputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\n```", "```py\nquery = inputs[1]  #A \nattn_scores_2 = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\nprint(attn_scores_2)\n```", "```py\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n```", "```py\nres = 0.\n\nfor idx, element in enumerate(inputs[0]):\n res += inputs[0][idx] * query[idx]\nprint(res)\nprint(torch.dot(inputs[0], query))\n```", "```py\ntensor(0.9544)\ntensor(0.9544)\n```", "```py\nattn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\nprint(\"Attention weights:\", attn_weights_2_tmp)\nprint(\"Sum:\", attn_weights_2_tmp.sum())\n```", "```py\nAttention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\nSum: tensor(1.0000)\n```", "```py\ndef softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\nprint(\"Attention weights:\", attn_weights_2_naive)\nprint(\"Sum:\", attn_weights_2_naive.sum())\n```", "```py\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\n\n```", "```py\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\nprint(\"Attention weights:\", attn_weights_2)\nprint(\"Sum:\", attn_weights_2.sum())\n```", "```py\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\n```", "```py\nquery = inputs[1] # 2nd input token is the query\ncontext_vec_2 = torch.zeros(query.shape)\nfor i,x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i]*x_i\nprint(context_vec_2)\n```", "```py\ntensor([0.4419, 0.6515, 0.5683])\n```", "```py\nattn_scores = torch.empty(6, 6)\nfor i, x_i in enumerate(inputs):\n    for j, x_j in enumerate(inputs):\n        attn_scores[i, j] = torch.dot(x_i, x_j)\nprint(attn_scores)\n```", "```py\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n```", "```py\nattn_scores = inputs @ inputs.T\nprint(attn_scores)\n```", "```py\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n```", "```py\nattn_weights = torch.softmax(attn_scores, dim=1)\nprint(attn_weights)\n```", "```py\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n```", "```py\nrow_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nprint(\"Row 2 sum:\", row_2_sum)\nprint(\"All row sums:\", attn_weights.sum(dim=1))\n```", "```py\nRow 2 sum: 1.0\nAll row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n```", "```py\nall_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)\n```", "```py\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n```", "```py\nprint(\"Previous 2nd context vector:\", context_vec_2)\n```", "```py\nPrevious 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n```", "```py\nx_2 = inputs[1] #A\nd_in = inputs.shape[1] #B\nd_out = 2 #C\n```", "```py\ntorch.manual_seed(123)\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n```", "```py\nquery_2 = x_2 @ W_query \nkey_2 = x_2 @ W_key \nvalue_2 = x_2 @ W_value\nprint(query_2)\n```", "```py\ntensor([0.4306, 1.4551])\n```", "```py\nkeys = inputs @ W_key \nvalues = inputs @ W_value\nprint(\"keys.shape:\", keys.shape)\nprint(\"values.shape:\", values.shape)\n```", "```py\nkeys.shape: torch.Size([6, 2])\nvalues.shape: torch.Size([6, 2])\n```", "```py\nkeys_2 = keys[1] #A\nattn_score_22 = query_2.dot(keys_2)\nprint(attn_score_22)\n```", "```py\ntensor(1.8524)\n```", "```py\nattn_scores_2 = query_2 @ keys.T # All attention scores for given query\nprint(attn_scores_2)\n```", "```py\ntensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n```", "```py\nd_k = keys.shape[-1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\nprint(attn_weights_2)\n```", "```py\ntensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n```", "```py\ncontext_vec_2 = attn_weights_2 @ values\nprint(context_vec_2)\n```", "```py\ntensor([0.3061, 0.8210])\n```", "```py\nimport torch.nn as nn\nclass SelfAttention_v1(nn.Module):\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n\n    def forward(self, x):\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n        attn_scores = queries @ keys.T # omega\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1)\n        context_vec = attn_weights @ values\n        return context_vec\n```", "```py\ntorch.manual_seed(123)\nsa_v1 = SelfAttention_v1(d_in, d_out)\nprint(sa_v1(inputs))\n```", "```py\ntensor([[0.2996, 0.8053],\n        [0.3061, 0.8210],\n        [0.3058, 0.8203],\n        [0.2948, 0.7939],\n        [0.2927, 0.7891],\n        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n```", "```py\nclass SelfAttention_v2(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n        context_vec = attn_weights @ values\n        return context_vec\n```", "```py\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))\n```", "```py\ntensor([[-0.0739,  0.0713],\n        [-0.0748,  0.0703],\n        [-0.0749,  0.0702],\n        [-0.0760,  0.0685],\n        [-0.0763,  0.0679],\n        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n```", "```py\nqueries = sa_v2.W_query(inputs)  #A\nkeys = sa_v2.W_key(inputs) \nattn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\nprint(attn_weights)\n```", "```py\ntensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<SoftmaxBackward0>)\n```", "```py\ncontext_length = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(context_length, context_length))\nprint(mask_simple)\n```", "```py\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\n```", "```py\nmasked_simple = attn_weights*mask_simple\nprint(masked_simple)\n```", "```py\ntensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<MulBackward0>)\n\n```", "```py\nrow_sums = masked_simple.sum(dim=1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)\n```", "```py\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<DivBackward0>)\n```", "```py\nmask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)\n```", "```py\ntensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n       grad_fn=<MaskedFillBackward0>)\n\n```", "```py\nattn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\nprint(attn_weights)\n```", "```py\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<SoftmaxBackward0>)\n```", "```py\ntorch.manual_seed(123)\ndropout = torch.nn.Dropout(0.5) #A\nexample = torch.ones(6, 6) #B\nprint(dropout(example))\n```", "```py\ntensor([[2., 2., 0., 2., 2., 0.],\n        [0., 0., 0., 2., 0., 2.],\n        [2., 2., 2., 2., 0., 2.],\n        [0., 2., 2., 0., 0., 2.],\n        [0., 2., 0., 2., 0., 2.],\n        [0., 2., 2., 2., 2., 0.]])\n```", "```py\ntorch.manual_seed(123)\nprint(dropout(attn_weights))\n```", "```py\ntensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n       grad_fn=<MulBackward0>\n```", "```py\nbatch = torch.stack((inputs, inputs), dim=0)\nprint(batch.shape) #A \n```", "```py\ntorch.Size([2, 6, 3])\n```", "```py\nclass CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout)  #A\n        self.register_buffer(\n           'mask',\n           torch.triu(torch.ones(context_length, context_length),\n           diagonal=1)\n        )  #B\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape  #C \nNew batch dimension b\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.transpose(1, 2)  #C\n        attn_scores.masked_fill_(  #D\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        context_vec = attn_weights @ values\n        return context_vec\n```", "```py\ntorch.manual_seed(123)\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\ncontext_vecs = ca(batch)\nprint(\"context_vecs.shape:\", context_vecs.shape)\n```", "```py\ncontext_vecs.shape: torch.Size([2, 6, 2])\n```", "```py\nclass MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length,\n                 dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n             for _ in range(num_heads)]\n        )\n\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n```", "```py\ntorch.manual_seed(123)\ncontext_length = batch.shape[1] # This is the number of tokens\nd_in, d_out = 3, 2\nmha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)\n```", "```py\ntensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\n\n        [[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 4])\n```", "```py\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, \n                 context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads #A\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out) #B\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            'mask',\n             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        keys = self.W_key(x) #C\n        queries = self.W_query(x) #C\n        values = self.W_value(x) #C\n\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n\n        keys = keys.transpose(1, 2) #E\n        queries = queries.transpose(1, 2) #E\n        values = values.transpose(1, 2) #E\n\n        attn_scores = queries @ keys.transpose(2, 3)  #F \n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n\n        attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        context_vec = (attn_weights @ values).transpose(1, 2) #I\n        #J\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) #K\n        return context_vec\n```", "```py\na = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n                    [0.8993, 0.0390, 0.9268, 0.7388],\n                    [0.7179, 0.7058, 0.9156, 0.4340]],\n\n                   [[0.0772, 0.3565, 0.1479, 0.5331],\n                    [0.4066, 0.2318, 0.4545, 0.9737],\n                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n```", "```py\nprint(a @ a.transpose(2, 3))\n```", "```py\ntensor([[[[1.3208, 1.1631, 1.2879],\n          [1.1631, 2.2150, 1.8424],\n          [1.2879, 1.8424, 2.0402]],\n\n         [[0.4391, 0.7003, 0.5903],\n          [0.7003, 1.3737, 1.0620],\n          [0.5903, 1.0620, 0.9912]]]])\n```", "```py\nfirst_head = a[0, 0, :, :]\nfirst_res = first_head @ first_head.T\nprint(\"First head:\\n\", first_res)\n\nsecond_head = a[0, 1, :, :]\nsecond_res = second_head @ second_head.T\nprint(\"\\nSecond head:\\n\", second_res)\n```", "```py\nFirst head:\n tensor([[1.3208, 1.1631, 1.2879],\n        [1.1631, 2.2150, 1.8424],\n        [1.2879, 1.8424, 2.0402]])\n\nSecond head:\n tensor([[0.4391, 0.7003, 0.5903],\n        [0.7003, 1.3737, 1.0620],\n        [0.5903, 1.0620, 0.9912]])\n```", "```py\ntorch.manual_seed(123)\nbatch_size, context_length, d_in = batch.shape\nd_out = 2\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\ncontext_vecs = mha(batch)\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)\n```", "```py\ntensor([[[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]],\n\n        [[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n```"]