- en: Chapter 2\. Oh, to Be an AI Value Creator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we gave a set of imperatives that can instantly improve
    the odds of success for any AI journey. This all comes from our countless collective
    experiences, which range from thousands of customer engagements to TV shows such
    as *60 Minutes*, to the US White House, NATO, senior management, and even the
    Vatican! (The Vatican houses priceless artifacts in nitrogen vaults, and we—well,
    the broader IBM team—helped open those artifacts up to scholars to safely scale
    knowledge and history. While we can’t share with you the details of that deal,
    we’re confident in the afterlife.)
  prefs: []
  type: TYPE_NORMAL
- en: You also learned about the Netscape moment of today and how it’s a tsunami of
    change that will wash across your personal and professional shores. You now understand
    that just as electricity was once deemed magical even though it wasn’t, AI is
    not magic either. We nudged (OK, two-hand shoved) you into a +AI to AI+ mindset
    and gave you a rebooted for this moment AI Ladder to climb for AI success. Finally,
    we gave you some operational frameworks to classify AI budgets, pick use cases,
    and envision outcomes that either shift left or shift right your business.
  prefs: []
  type: TYPE_NORMAL
- en: We think [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974)
    and this chapter are important because they are both about defining the right
    details to pay attention to on your AI journeys. Why? Details will matter, details
    will differentiate, and details will earn (or keep) trust. We’ll use the history
    of the Statue of Liberty as an analogy of what you’re doing in the first part
    of this book. She stands tall and green in the iconic New York harbor. Her patina
    (the green chemical reaction to copper that occurs over the course of time) helps
    her stand strong against the elements—but it really must have been something for
    immigrants to see her copper glow on the horizon as they sailed into New York’s
    port, way back when. If you get a chance, take a moment to look at her hair. If
    you search for an up-close photo, you will see intricate braiding and precisely
    styled curls on the back of her head. It is perfect hair on top of a perfect statue.
    Interestingly enough, the Statue of Liberty was built 10 years before the first
    airplane. Her sculptor, Frédéric Auguste Bartholdi, had no reason to believe anyone
    would ever see her hair—yet the details mattered because sculpture was his craft,
    and his reputation depended on those details. What does this have to do with AI?
    The decisions you make over the next few years—and how you make them—may never
    be seen in isolation or explicitly, but the details of them will matter because
    they will stand for who you are and who your team, company, and you want to be.
    Remember that.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we want to introduce you to perhaps the most important AI
    destination you should have in your own personal navigation systems: *the AI Value
    Creator*. Remember, this part of the book is on the business side, so while we’ll
    give you some more insights into large language models (LLMs) with a technology
    point of view later on, we’ve got some more AI business-related stuff we want
    to ensure you think about so that you’ll have a bigger set of skills to draw from
    than those who don’t read this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AI Through the Years: The AI “Time Lapse” Section'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *AI* was first coined in 1956, and various generations of this technology
    (though none like this GenAI and agentic moment) have progressed and disappointed
    ever since. Some would say that AI has disappointed more than it’s delighted,
    which has caused “AI winters” from which AI has reemerged after some breakthroughs.
    If you look at the history of invention (take electricity, for example), it should
    come as no surprise that the path to AI breakthroughs has run through mass experimentation.
    While many AI experiments have failed, successful ones have had a substantial
    impact, and those successes have come from solving the problems that caused failures.
  prefs: []
  type: TYPE_NORMAL
- en: People have long been speculating about the possibility that machines would
    someday be able to think like a human, on their own. This has been going on since
    the late 1800s, but the idea really took root with Alan Turing’s 1950 seminal
    paper, “Computing Machinery and Intelligence.”^([1](ch02.html#id389)) Historians
    call Turing the father of AI because of this very paper. In it, he theorized that
    society could create computers that would play chess, described how those computers
    would surpass human players, and said we would make them proficient in natural
    language. He theorized that machines would eventually think.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of our careers at IBM, we’ve seen (and been a part of achieving)
    many of the milestones that Turing identified on the way to a “thinking” machine.
    These have included evolutions and variants of AI playing games like chess (with
    Deep Blue), *Jeopardy!*, and the board game Go, as well as debating systems. But
    Turing was just the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: If Turing’s paper was the spark, then the big bang came just six years later
    at Dartmouth College in its Summer Research Project on Artificial Intelligence
    workshop. There, a couple of young academics got together with a couple of senior
    scientists from Bell Labs and IBM and proposed an extended summer workshop with
    just a small handful of the top people in adjacent fields to intensively consider
    artificial intelligence. That’s where the term *AI* was first used, and it marks
    the point at which AI was established as a field of research.
  prefs: []
  type: TYPE_NORMAL
- en: In extensive detail, this team laid out many of the challenges that researchers
    have been working on ever since to develop machines that could think. Neural networks,
    self-directed learning, creativity, and more are all still relevant today.
  prefs: []
  type: TYPE_NORMAL
- en: For perspective, this was 1956, the same year the invention of the transistor
    won the Nobel Prize. Today, we can put over 100 billion transistors in a graphics
    processing unit (GPU) and provision legions of interconnected GPUs to provide
    the computing power needed for GenAI. Throughout the years, AI theories, techniques,
    and ideas have been developed in parallel with progress in hardware that have
    come together to dramatically reduce computing and storage costs. All of this
    is converging now to make AI very real and practical.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we want to make this critical point: it’s not just about powerful hardware
    and clever algorithms. *Maybe the most important ingredient of generative AI*—particularly
    when it comes to your business getting the most value from it—is *your* data.
    *You can’t talk about generative AI without talking about data*. This makes hardware,
    algorithms, and data the three legs of the AI stool.'
  prefs: []
  type: TYPE_NORMAL
- en: A Quick Bit on Foundation Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the GenAI world, you’ll often hear about how LLMs are powering GenAI. But
    what are they? At a basic level, LLMs are new ways of representing language in
    a high-dimensional space with a large number of parameters—representations created
    by training on massive quantities of text.
  prefs: []
  type: TYPE_NORMAL
- en: From this perspective, much of the history of computing has been about coming
    up with new ways to represent data and extract value from it. For a long time,
    we’ve put data in tables. For example, we put employees or customers in the rows
    of a database and put their attributes in the columns. This is great for things
    like online transaction processing (OLTP) or writing checks for payments to individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the world started representing data with graphs, and this helped us discover
    and appreciate relationships between data points like never before; for example,
    this person, business, or place was connected to these other people, businesses,
    or places. Data represented this way starts to reveal patterns. For example, companies
    use graphs to map a social network or spot anomalous purchases to help them detect
    credit card fraud. This technology is a combination of many data analysis approaches
    using various types of data repositories (a graph database is included here),
    and this is also how the People You May Know (PYMK) feature works on Facebook
    (as just one example).
  prefs: []
  type: TYPE_NORMAL
- en: Today, with LLMs, we’re taking lots of data that’s represented in neural networks
    that simulate (very loosely) an abstract version of brain cells. There are layers
    and layers of connections with millions, tens of billions, hundreds of billions,
    or even trillions of parameters—and suddenly, you can do some fascinating things.
    You can discover patterns so detailed that you can predict relationships with
    a lot more confidence. For example, you can predict that this word is most likely
    connected to this next word, and these two words are most likely followed by a
    specific third word—meaning you can build up, reassess, and predict again and
    again until something new is created or *generated*. Hence, the term *generative
    AI*.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s what GenAI is: the ability to look at data, discover relationships,
    and predict the likelihood of sequences with enough confidence to create or generate
    something that didn’t exist before. Text, images, videos, sounds, and really all
    types of data can be represented in a model.'
  prefs: []
  type: TYPE_NORMAL
- en: We could do a limited version of all of this before with deep learning, which
    was an AI milestone in its own right. With *deep learning*, we started representing
    a massive amount of data using very large neural networks with many layers, but
    training had to happen with annotated data that humans had to manually label;
    for example, looking at a picture and noting it as a “cat” and another picture
    as a “dog.” This is called *supervised learning*. So, what was the problem? As
    you can see in [Figure 2-1](#ch02_figure_1_1740182046144175), supervised learning
    is expensive, laborious, and time consuming, so only large institutions did that
    work and only for specific tasks. If you wanted AI to summarize and translate
    text, you needed to label two very large datasets...manually (more on this in
    a moment).
  prefs: []
  type: TYPE_NORMAL
- en: Around 2017, a new approach appeared that was powered by an architecture called
    *transformers* (we lightly detail these in [Chapter 9](ch09.html#ch09_generative_computing_a_new_style_of_computing_1740182052619664)).
    With this approach, AI could perform a new kind of frictionless learning called
    *self-supervised learning*, in which a language model could be trained on large
    amounts of unlabeled data by hiding certain sections of the text (words, sentences,
    etc.) and asking the model to fill in the blanks (the AI lingo for this is *masking*).
    For example, if we said, “May the force,” you’d likely guess that the next three
    words are “be with you” from *Star Wars*. Although an oversimplification, this
    amazing process, when done at scale, results in the powerful data representations
    that today we call LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screen shot of a computer  Description automatically generated](assets/aivc_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Comparing the activation energy of getting started with supervised
    learning versus self-supervised learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is where something truly magical happened. Researchers found that instead
    of building AI models that were only suited to narrow use cases and areas of expertise
    (for example, building and painstakingly curating one dataset for summarization
    and another for translation), they could have AI that was more broadly applicable.
    Basically, these LLMs could be trained on huge volumes of internet data (today’s
    most popular LLMs are really just highly compressed representations of everything
    on the internet—which is good and bad) and thus acquire a humanlike *set* of natural
    language capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-supervision at scale, combined with massive data and compute, gave the
    world AI that is generalizable and adaptable. We define these terms as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generalizable
  prefs: []
  type: TYPE_NORMAL
- en: This means the AI has the ability to perform well across a wide range of tasks
    and domains, often with little to no task-specific tuning. In other words, the
    same LLM that classifies the sentiment of a text document can extract people and
    places from text—an action referred to as named-entity recognition (NER)—and can
    translate, summarize, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptable
  prefs: []
  type: TYPE_NORMAL
- en: This means that the AI can not only do multiple tasks but can also handle different
    use cases it wasn’t originally trained for. AI that is adaptable is also *emergent*,
    meaning it has capabilities that it was not explicitly programmed to have and
    that arise unexpectedly; for example, an LLM can answer riddles or solve logic
    puzzles it has never been trained on simply by recognizing patterns. The bottom
    line is that being able to use the same model for multiple use cases and discovering
    new capabilities in them is a powerful tool (though you are still going to want
    to steer it to become an AI Value Creator; more on that in a bit).
  prefs: []
  type: TYPE_NORMAL
- en: Over the last decade, there’s been an explosion of applications for AI. (Our
    bet is that you’ve used many of them, even without knowing it. Have you used Siri
    or Alexa? Have you changed a gray sky to a sunny sky to create a picture-perfect
    moment? Have you used a translation app?) In that time, we’ve seen AI go from
    being a purely academic endeavor to being a major force that powers actions across
    a myriad of industries and affects the lives of billions each day.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, we’ve managed to build AI systems that can learn from thousands
    or millions of examples to help us better understand our world and find new solutions
    to difficult problems. These large-scale models have led to the development of
    systems that can understand us when we talk or write. These include the natural
    language processing (NLP) and natural language understanding (NLU) programs we
    use every day, from digital assistants to speech-to-text programs. Other systems,
    which are trained on things like the entire bodies of work of famous artists or
    every chemistry textbook in existence, have allowed us to build generative models
    that can create new works of art based on those artists’ styles or new compound
    formulation and docking combinations based on the history of chemical research.
  prefs: []
  type: TYPE_NORMAL
- en: While today many new AI systems are helping to solve all sorts of real-world
    problems, before GenAI, creating and deploying an AI for each new system using
    traditional methods required a considerable amount of time and resources. For
    each new application, you had to ensure that there was a large, well-labeled dataset
    for the specific task you wanted to tackle. If a dataset didn’t exist for that
    task, you had people taking hundreds or thousands of hours (perhaps more) to find
    and label appropriate images, text, or graphs for the training and validation
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: What does all this mean? You can take a large, pretrained LLM—if you’re using
    it for business, you’ll want to ensure you’re starting with a model that is trustworthy—and
    add *your* institutional knowledge to turbocharge the model to *excel at your
    specific use cases* with your specific data. (We get into the ills, wills, and
    thrills of this topic in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518).)
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you’re feeling a bit disheartened because you’re one of those businesses
    we talked about that spent enormous amounts of time collecting and labeling data
    for your AI projects, only to have them fail because you didn’t label enough data
    (that is how it went with traditional AI), fear not! That work is not throwaway
    in this GenAI world because that proprietary industry-specific data we just mentioned
    is what you’re going to use to tailor an LLM for your business needs. It’s what
    you need to do in order to become an AI Value Creator. In fact, you’re literally
    going to take those failed AI projects from two years ago and look like a hero
    when you bring forth to your bosses how you want to steer whatever LLM you land
    on for your business. How so? First, today’s LLMs don’t contain much enterprise
    data at all (about 1%), let alone your proprietary data. In [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974),
    we told you how your data is a competitive advantage, and now it’s time to put
    that data to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quite simply, when you bring together the data representations of an LLM and
    steer it with your labeled data (which now, you need much less of), you end up
    with something that is tailored to your business. Think of it this way: let’s
    assume you know Spanish, and today, you’re trying to learn French. On this journey,
    there is a lot of *foundational* knowledge you already have about how language
    works, like how to conjugate verbs. Just as it’s likely easier to learn French
    if you have Spanish as a foundation, as you’ll find out in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    there’s a new open source approach (called InstructLab) that makes it easier than
    ever to fold your data into your company’s private LLM and not share it with the
    world, and that’s bound to give some ooh la la to your final results.'
  prefs: []
  type: TYPE_NORMAL
- en: The current thinking is usually that you can apply LLMs (hence, their name)
    to language. But this should spark the question, what is a language? Signals in
    a piece of industrial equipment are talking to you, in their own language; there
    are programming languages, which consist of communication verbiage from humans
    to instruct machines; and there are the clicks of a user navigating a website,
    software code, chemistry, and diagrammatic representations of chemicals. We’ve
    even worked with a company using AI to model taste and smell. If you squint, *everything
    starts looking like a language*, and if it’s a language, it can be learned, deciphered,
    and understood.
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway is that AI can be specialized to do all kinds of things that boost
    productivity in any language. That means that AI can stretch horizontally across
    your business to HR processes, customer service, self-service, cybersecurity,
    code writing, application modernization, and so many other things that we’ll share
    with you in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425).
  prefs: []
  type: TYPE_NORMAL
- en: 'Going a Little Deeper: The Evolution of Large Language Models and Comparing
    Supervised Learning with Self-Supervised Learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models aren’t built the same way as traditional AI. They are
    trained using self-supervised learning, which means you don’t have to manually
    annotate a massive amount of data. Basically, you train a model by telling it
    to go read enormous amounts of data (for example, text) and when it’s done you
    end up with a large but versatile model with more humanlike language capabilities.
    AI uses mathematical models to represent the relationships in the data (like words)
    it ingests. If you give the model a few words in a prompt, it can mathematically
    predict the likelihood of words coming up in the sequence of the *Star Wars* phrase
    we shared in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: Two of the biggest things that excite us about GenAI are just how fast you can
    now build these same use cases for all the reasons summarized in [Figure 2-1](#ch02_figure_1_1740182046144175)
    and the fact that these models (as we noted in the previous section) are generalizable
    and adaptable. The best way to appreciate how GenAI flattens the time-to-value
    curve for AI projects is to go beyond labeling data and contrast GenAI with the
    traditional way in which AI use cases were brought into production.
  prefs: []
  type: TYPE_NORMAL
- en: Many of you who have been around AI for a while may feel that you’re seeing
    many use cases from the traditional AI era repeat themselves in this new GenAI
    era—and you’re right. That said, we’d be remiss if we didn’t note that while the
    initial set of GenAI use cases might be repeating themselves, there are new ones,
    and agentic AI brings plenty more. In the last decade, with the advent of deep
    learning, the world demonstrated (as a community) that you could bring incredible
    accuracy to specific tasks if you gathered enough data, labeled that data, trained
    models, and deployed them. This traditional methodology is what you see in [Figure 2-2](#ch02_figure_2_1740182046144214).
  prefs: []
  type: TYPE_NORMAL
- en: Notice in [Figure 2-2](#ch02_figure_2_1740182046144214) how each model is built
    for a specific AI use case. In this example, the use cases are summarization,
    tone analysis, and entity extraction. To build these models with the traditional
    approach to AI, your company would have created a separate team for each task,
    and each team would have built a separate model to anchor the task. All those
    teams would have gone through the same painstaking process of data selection and
    curation, labeling, model development, training, validation, and so on—perhaps
    even duplicating the same data!
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](assets/aivc_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. The traditional way to build AI, by assembling many data science
    teams and getting them to do as many projects as they can
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Different teams collecting data, curating it for their own use case, and going
    through the same steps other teams go through can only be described as long, hard,
    tedious, and expensive. In fact, we’d humbly suggest that how much your company
    could scale AI was really the answer to the questions: how many data science teams
    could you assemble, and how many projects could those teams carry out?'
  prefs: []
  type: TYPE_NORMAL
- en: Now contrast the new approach to AI (on the left side of [Figure 2-3](#ch02_figure_3_1740182046144239))
    with the traditional path to AI (on the right side of the figure). As you can
    see, instead of needing to build one AI model for each specific task (as in [Figure 2-2](#ch02_figure_2_1740182046144214)),
    you take an LLM that is likely trained by someone else (like IBM, Google, DeepSeek,
    OpenAI, or Anthropic; truth be told, few companies will build their own—rather,
    they will steer existing ones) and adapt it to many varied downstream tasks. Also,
    notice how a single LLM fuels the three use cases in [Figure 2-3](#ch02_figure_3_1740182046144239).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a model  Description automatically generated](assets/aivc_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. GenAI scales AI, reducing skill requirements, data, time, administration,
    and up-front costs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given the versatility of an LLM, companies can now use the same model to implement
    multiple business use cases. They could never really do that using traditional
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: We really want you to spend some time committing [Figure 2-3](#ch02_figure_3_1740182046144239)
    to memory because it illustrates why LLMs are becoming essential ingredients of
    the new AI workflow. Modern AI takes a very focused effort to create a *base model*
    (meaning a general-purpose LLM) and getting economies of scale from that investment.
    Creating an LLM on your own is quite a sophisticated endeavor, which is why we’re
    confident that most of you will choose one to start with and then steer it with
    your data to match your business and use case (which we tell you how to do later
    in this book).
  prefs: []
  type: TYPE_NORMAL
- en: We’re hoping you’ve gotten a good grasp of this methodology shift, because the
    next wave of AI looks to replace the task-specific models that have dominated
    the AI landscape to date with LLMs as their core. These models are trained on
    a broad set of data that can be used for different tasks, and what’s more, with
    their self-ideation to achieve defined goals, agentic AI will follow this path
    too.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the takeaway. What makes LLMs so versatile is that they, as their name
    suggests, can be the foundation of many AI and agentic applications. Using self-supervised
    learning and transfer learning, these AI models can apply information they have
    learned about in one situation to another situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to understand transfer learning is with a traditional computer
    vision example of AI being used to identify a cat. (Again, AI and cats seem to
    go hand-in-paw—it’s like some feline aficionado felt their deep learning needed
    some deep purring.) If you taught an AI how to identify a cat, that AI would start
    with shapes and edges and gradually build layers in its neural network to identify
    a cat. At its base levels, this AI would likely be able to detect triangles (combinations
    of edges). If you think about a cat, triangles form its ears and nose and other
    parts, and once the AI could find triangles, it could go on to discover other
    cat features as it used more and more layers in its neural network to ultimately
    define the object it sees as a cat. Now, imagine you wanted to identify a sailboat.
    An AI trained to identify sailboats would start at the same place: finding edges
    and shapes. So, you could take the levels of the AI that know what triangles look
    like and transfer it for boats, you could do the same thing for potentially thousands
    of layers—and now you understand transfer learning. Whether the AI was identifying
    a cat or a sailboat, that identification of a triangle would be critical.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of us can relate to the versatility of LLMs supporting multiple use cases
    in our everyday lives. For example, once you’ve learned how to drive a car, you’ve
    got some serious skills you can transfer to drive other cars. Sure, there are
    some nuances to get used to (like where to find the windshield wiper controls),
    and you could even run into major issues (try driving with a manual transmission
    if you’ve only ever driven an automatic), but there are still a bunch of base
    skills that transfer. Today, no one builds a convolutional neural network (CNN)
    or uses a vision transformer (ViT) for computer vision without some sort of transfer
    learning—it’s like the ultimate computer vision cheat code!
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway? It’s simple: instead of needing to build one AI model for each
    specific task, you can train one model and adapt it to many varied downstream
    tasks. This means that companies now have the opportunity to go from a modus operandi
    of *one task: one model* to *one model: many tasks*. For example, your IT support
    chatbot and your HR self-service initiatives can use the same base model as the
    new app that will write your marketing emails and summarize contract documents.'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 2-3](#ch02_figure_3_1740182046144239), there is still work
    to do! While the data engineering and labeling chores are now minuscule, you’re
    still going to want to use your data to steer the model toward your business domain
    and its brand, style, social norms, and so on. There are many ways to do this,
    using techniques such as prompt tuning, prompt engineering, fine-tuning with parameter-efficient
    fine-tuning (PEFT) methods, and InstructLab. You’ll learn more about this in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518),
    but all the preparatory work you must do before you put your data to work has
    greatly decreased because of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the eye opener here shouldn’t be the power of a model with billions
    or even trillions of parameters. Hopefully, it’s jumping off the page at you,
    but if isn’t: the productivity associated with LLMs means that businesses can
    finally scale their AI initiatives with *less* time, *less* data, *less* up-front
    money, and *less* administration. For example, in IBM’s own experience, it took
    7 years to support 12 languages using AI the traditional way—but once it adopted
    GenAI, the languages it supported jumped to 25 in just a year.'
  prefs: []
  type: TYPE_NORMAL
- en: AI Value Creation Should Be Your Destination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When oxygen, heat, and fuel combine, we get fire. It’s basic, it’s primal,
    and it’s the key that unlocked human progress. Think about it: fire provided light,
    heat, and protection, and our ancestors used it to move to new climates and eat
    new foods. Pottery, metallurgy, chemistry, rapid transportation, and many other
    technologies all started with fire.'
  prefs: []
  type: TYPE_NORMAL
- en: But imagine if fire had been proprietary? What if the knowledge of how to make
    fire hadn’t been shared, and what if there had been just a few keepers of the
    fire? Where would we be?
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember what we told you in the Preface: we’re in a lift, shift, rift, or
    cliff moment with GenAI, and especially with agents, it’s going to shape our society
    for generations to come. This section (and the rest of the book) is going to show
    you how to become your own AI fire starter, how to take control of your AI destiny,
    and why it’s so important to see yourself as an AI Value Creator and not just
    an AI User. Finally, we’ll detail why the future of AI needs an open innovation
    ecosystem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How Do You Consume AI: Be Ye a Value Creator or a Value User?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to using AI, there are three modes of consumption:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s baked into the software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You use someone else’s model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You use an AI platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI User: Shake (embed) and bake (into the product) the AI'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first way to consume AI is when it’s “baked into” off-the-shelf software.
    In this approach, a software vendor creates the AI, and you put it to use. (We’re
    going to assume you’re only working with real AI in products and not “fake and
    bake” AI, since everyone claims to have AI in their products these days.) Whether
    it’s a writing assistant (like Grammarly or Jasper) that can help you strike the
    right tone in your email, or image editing software (like Adobe Photoshop or Topaz
    Photo AI) that can automatically enhance the quality of your images and videos,
    with this consumption pattern, you, as an AI User, get access to some great functionality
    that can make you more productive. Who doesn’t want that?
  prefs: []
  type: TYPE_NORMAL
- en: But there’s a caveat! *You and everyone else get access to this same “magic,”*
    which means that while this form of AI might help you do your work faster and
    with better results (that’s a good thing), *it can (and will) do the same for
    anyone else* who invests time in getting skilled up in that software. In other
    words, these AI capabilities and productivity opportunities don’t become differentiators—*but*
    they do set a new, higher baseline for everyone, including your competition.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI User: Don’t fall when you make the service call (the even bigger *but*)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second model of AI consumption is when you prompt someone else’s model,
    either directly in a chat interface or through an API call. Quite simply, as you
    develop custom AI apps for your business, these apps can call out to another company’s
    GenAI service, use that company’s models, and get results. This also is a viable
    way of consuming AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'The truth is, just about every single one of us has been using GenAI this way,
    and that makes us all a bunch of AI Users. But think about being an AI User for
    a moment: you are mostly limited to simply prompting someone else’s AI model (not
    your model), you have no control over the model or the data used to train it,
    and you, in almost every case, have absolutely no idea what data was used to build
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how cleverly you use the model, you can *start* to differentiate
    how you put AI to work relative to your competitors. *But* there are still more
    caveats that you need to consider—*especially* if you’re trying to be an AI Value
    Creator.
  prefs: []
  type: TYPE_NORMAL
- en: The first consideration is that, like with our software example, those models
    and services you tap into are available to everyone, so are you really differentiated?
    Sure, perhaps you can prompt the same model better than someone else. *But* you’re
    still accessing the same model as everyone else.
  prefs: []
  type: TYPE_NORMAL
- en: There’s something else *you need to be even more concerned about* when your
    app makes that call and it goes off to work some magic—it’s connecting to something
    opaque (meaning you can’t see inside it). You don’t necessarily know what’s happening
    on the other end, what the AI model is doing with your data (learning from it,
    storing it, or just looking at it), or the provenance and governance of the data
    used to build the LLM the service is built on. Depending on the use case, this
    should make you somewhat nervous because your business is still accountable for
    the final outcomes (either socially or, more and more, by law—which we get into
    in [Chapter 5](ch05.html#ch05_live_die_buy_or_try_much_will_be_decided_by_ai_1740182048942635)).
    And if you’re talking about AI for business—as opposed to just personal use—we
    think that should make you nervous.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to give you something to think about as a second word of caution whenever
    you use someone else’s proprietary AI: what of the creation and accrual of value
    over the long term? In the past, we’ve seen a lot of value-extractive business
    models—if you’re on social media, you’re a part of one. Quite simply, we always
    tell people if you’re not paying for it, make no mistake about it, you’re more
    than likely the product being sold. But even if you’re paying for the service,
    indeed, you’ll get value from that service (or you wouldn’t be paying for it).
    *But* that other company is likely extracting value from your usage and from your
    data, accumulating more and more over time. It’s not our intent to call any of
    these companies by name in this book, but there is a plethora of examples of companies
    (including paid services) that benefit from your strategic data. Ironically, this
    is the very method with which those LLMs were made (scraping data on the internet,
    be that data copyrighted or not).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings up yet another question we want you to think about: if you’re an
    AI User making a call to someone else’s AI service, how much faster is their value
    growing than yours? (Hint: check out the stock price and valuation multiples of
    some of these companies we’re not naming.) Quite simply, there’s likely an imbalance
    in the relationship, and *that can have long term consequences* for your specific
    business, the overall economy, and the progress of technology.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A final *but*: Do we, as a society, really want to have just a few keepers
    of the AI “fire” upon which we are all dependent? Is that what’s best for your
    individual business and for your shareholders? We think no.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fire starter: Becoming an AI Value Creator'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The third model of AI consumption is the platform model, which is the most
    comprehensive. This is how you become your own AI fire starter, and when it comes
    to becoming an AI Value Creator, we want to be clear about something up front:
    it *does not mean* you’re doing it alone or reinventing AI from scratch. You’re
    not taking years and spending millions to build your own LLMs. Of course, you
    can do that with a platform, but that will be in a very small minority of cases.'
  prefs: []
  type: TYPE_NORMAL
- en: With an AI value creation platform, you have all the elements and ingredients
    (data, governance, and LLMs) in place to build your own AI solutions. You have
    access to a vast number of GenAI models (both open source and proprietary), or
    you can bring your own models into the platform. You have tools to improve and
    customize models to fold in your proprietary knowledge of your business without
    concerns about sharing some of your most valuable assets (your data). You can
    fine-tune the models, prompt-tune them, tailor them with InstructLab—whatever
    techniques we detail in [Chapter 8](ch08.html#ch08_using_your_data_as_a_differentiator_1740182052172518)
    you want to use to build your own tailored AI solutions. At its core, the AI Value
    Creator approach allows you to create and accrue value that is unique to your
    business. A great example of an AI Value Creator is L’Oréal, one of the world’s
    leading beauty companies. Imagine the corpus of formulation, material science,
    and preference data L’Oréal has accumulated as it nears its 120th birthday. In
    essence, L’Oréal possesses data that defines the language of makeup. It wants
    to be an AI Value Creator, so it set out to create a private AI model (in collaboration
    with IBM) to accelerate tasks like the formulation of new products, reformulation
    of existing cosmetics, and optimizations to scale-up production. If L’Oréal was
    just an AI User, it would give this data away, but instead it views its data as
    a competitive advantage and decided to put it to work to better equip L’Oréal’s
    4,000 researchers worldwide over the next several years. We think L’Oréal isn’t
    just applying AI to beauty—it’s giving it a makeover of its own. With data as
    rich as its foundations and as bold as its lipsticks, who knew AI could have such
    a great eye for color matching?
  prefs: []
  type: TYPE_NORMAL
- en: 'The path forward: How to create value with AI'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ultimately, we believe that most businesses should end up with a mix of all
    three models of AI consumption. You’ll use third-party software with AI embedded,
    and *sometimes* it will be totally appropriate to use someone else’s AI User to
    do something you’re trying to do. For example, perhaps you are a real estate agent
    and want a quick description of a kitchen for a new listing based on photos you’ve
    been handed. Unless you have some kind of proprietary description magic, this
    is likely a situation in which you might want to use some of the more famous models
    you’ve heard about without concern. But what if you’re classifying sentiment on
    a purchase based on thousands of sentiments you’ve gotten from three decades of
    selling houses? To fully realize the value of AI and differentiate yourself from
    competitors, you’ll want to use a platform approach (just like L’Oréal) to create
    value by using your own AI tuned to your business, and you’ll want to add the
    other AI consumption patterns where appropriate. Let’s go a bit deeper into AI
    value creation, starting with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that LLMs are large-scale, deep neural networks trained with lots of
    data and subsequently adapted to many downstream tasks. They might be broad, general
    models or narrower, deeper models, but the key is that they’re pretrained with
    the *expectation that you can further enhance them with your own proprietary data
    if you’re looking to become an AI Value Creator*. It’s just like when a new employee
    joins your business: they come in with some general skills as a foundation and
    the ability to learn. The more they learn about your business, the more they add
    institutional knowledge and expertise, and the more value they deliver (and likewise,
    the more hurtful it might be if they went to a competitor). The same is basically
    true of LLMs. You use your AI platform to tune them with your specific business
    data, proprietary knowledge, and expertise—and then they become more like experts
    about your business and more valuable to your business over time. You don’t want
    that sales employee you trained with insights into your accounts to start working
    for someone else, and AI Value Creators feel the same way about their data!'
  prefs: []
  type: TYPE_NORMAL
- en: And because AI Value Creators are in control of the platform, processes, and
    data, they accrue ever larger amounts of value over time. With some of the consumer
    AI on the market, we’ve already seen some of what happens when you surrender that
    control. You can get bad data that leads to bad outcomes, as well as confabulations
    or hallucinations. You could also get into some trouble for inadvertently using
    someone else’s rights-managed content (that’s what all the copyright lawsuits
    going on are about), and we’ve even seen proprietary or sensitive data being inadvertently
    leaked back into public spaces. These are just some of the reasons why, when it
    comes to AI for business, you need to know how your LLM was built, what data was
    used to train it, and the recipe used to put it all together. And this is also
    why you should prioritize exercising tight control over your sensitive data. *Strong
    AI governance is absolutely critical*.
  prefs: []
  type: TYPE_NORMAL
- en: Look before you leap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Yes, now is the time to jump into AI, but look before you leap, and ensure that
    you’re investing in a smart, safe, and sustainable approach in which your business
    and customers are the primary beneficiaries. We think this approach starts with
    an AI Value Creator persona using a trusted platform and expands from there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Planning Your AI Future: A Future with Many GenAI Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We think there is an AI myth out there right now, or at minimum, a basic misunderstanding.
    For the general public, GenAI has seemingly come out of nowhere. A lot of people
    think that there’s just a handful of consumer-oriented AI experiences out there
    and that one model is going to win (there will be “one model to rule them all,”
    in Tolkien-speak).
  prefs: []
  type: TYPE_NORMAL
- en: '*We don’t think that’s going to happen*. The future of AI is not about one
    model. It’s about many models (you’ll sometimes hear this referred to as *multimodel*),
    and it’s multimodal (can work on images, text, video, sound, and so on) too. Your
    business will be using multiple fine-tuned models to achieve the best results
    when you apply them to specific use cases. Some will be off-the-shelf, some will
    be steered with your data, some will be used to judge an AI’s output (they’re
    called *judge models*), some will be used as is to ensure safety, some will be
    used for tasks that require complex reasoning, and some will be used to power
    agents. That’s why the platform approach is so important—and it’s also why we
    introduced you to the Hugging Face community in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974).'
  prefs: []
  type: TYPE_NORMAL
- en: And as we’ve insinuated (well, we’ve outright told you, but we’re just being
    polite)—*bet on community* because the future of AI is less about proprietary
    models and more about being powered by open science and open source. Proprietary
    models will surely play a part, but so much of what is going to happen in the
    future will *not* (and should not) happen behind closed doors. It needs to (and
    will) play out in plain view, with full transparency and accountability in open
    source.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the energy around GenAI and agents in the open source community right
    now is phenomenal. There are distributed projects, university projects, and corporate
    efforts—all driving innovation and producing LLMs that you can tune and deploy
    for your use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Many people are saying, “Big tech AI is the problem.” *We disagree* (and not
    because we work for a big technology company). We’d rather you widened the aperture
    and said, “Proprietary and closed AI is a potentially serious problem.” That,
    we agree with. Why are we making this point? It’s because there are vendors big
    and small (we won’t mention them by name here...we’re not trying to pick a fight)
    that are closed and proprietary, and there are companies that are large (like
    IBM and Meta) and small (like Mistral AI and DeepSeek, among others) that are
    open.
  prefs: []
  type: TYPE_NORMAL
- en: For the good of society in the long term, we don’t want just one or a few winners—a
    few companies that can define what AI is and dictate how it’s used. From what
    we can see, we don’t think that’s going to happen—and that’s a good thing for
    you, your business, and society in general.
  prefs: []
  type: TYPE_NORMAL
- en: It’s Time to Demystify and Apply AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As sure as it’s been said that data is the “new oil,” many have dubbed AI the
    world’s “new electricity.” In addition to GenAI making today’s AI ubiquitous and
    increasingly accessible (thanks to the prompt), AI can (and *will*, if done right)
    enhance and alter the way business is conducted around the world. Today, AI can
    be used to enable predictions with supreme accuracy, and automate business processes
    and decision making. The impact is vast, ranging from frictionless customer experiences
    to intelligent products to more efficient services. In the end, the result will
    be economic impact for companies, countries, and societies.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be sure, organizations that drive mass experimentation in AI will win the
    next decade of market opportunity. To break down and help demystify AI, you need
    to consider two key elements of the category: *the componentry* and *the process*.
    In other words, you need to identify what’s behind it and how it can be adopted.'
  prefs: []
  type: TYPE_NORMAL
- en: The componentry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Much like how the development of the use of electricity was driven by basic
    components such as resistors, capacitors, diodes, and so on, the development of
    AI is being driven by modern software componentry that includes the components
    outlined in this section.
  prefs: []
  type: TYPE_NORMAL
- en: A unified, modern data fabric with an accompanying data-as-a-product point of
    view
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You’ve heard us already say it several times in this book: your AI needs an
    IA. Why? Because AI feeds on data, and therefore, your data must be prepared for
    AI. (This is why it’s first on our list.) This goes beyond garbage in, garbage
    out (GIGO), although that’s even more of an issue with AI since all AI does is
    find those numerical patterns we alluded to in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974).
    This will be a problem unless you think everything on the internet is real, there’s
    no fake news, and there isn’t hate, abuse, or profanity that goes on there. In
    other words, this is a problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A *data fabric* (when done right) covers all enterprise data with governed
    searchability and connectivity. It removes the complexity of connecting to data
    and understanding the details of the underlying technology using data intelligence.
    You’ll often hear us shout out, “Cloud is a capability, not a destination!” (Hybrid
    cloud is an approach pretty much settled on by all businesses.) In the same way,
    you use a data fabric to apply a parallel best thought process to your IA: the
    “data isn’t just in one place” mindset, which has benefits that are applicable
    everywhere.'
  prefs: []
  type: TYPE_NORMAL
- en: A data fabric acts as a logical representation of all data assets on any cloud
    (public, private, or on premises). It auto-organizes and auto-labels data across
    an enterprise (and outside the enterprise, if needed), no matter where it resides.
    It empowers shipping function to data, as opposed to data to function, and this
    optimizes compute cycles. In plain speak, that means it takes the operations you
    want to apply to data and sends them to where the data is, as opposed to getting
    all the data and pulling it into a single place to do the computations. In this
    big-data world, you can imagine how the latter won’t scale.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps most importantly, it provides a company’s employees with governed and
    seamless access to all available data through virtualization, from the firewall
    to the edge. When you think about data fabric, think *self-service*, *ease of
    access*,and *data protection.*
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, a data fabric transforms data utilization into a process of knitting
    together data across your business—and externally, if appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: A great IA strategy includes more than the things we just mentioned, but they
    are the levers to pull—and from there, tasks like collecting data, organizing
    it, governing it, infusing it into existing AI (and non-AI) business processes,
    and more all fall into place.
  prefs: []
  type: TYPE_NORMAL
- en: A development environment and an engine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A business needs a place to build, upskill, train, and run its AI models. Ideally,
    the componentry is integrated with your strategic decisions for data persistence
    (like a data lakehouse) and a governance framework—and it’s all integrated with
    shared metadata across the ecosystem. This approach also helps organizations come
    together on a common mission, language, and design process—from input to output.
    By the time you have both components in hand, your company’s data strategy will
    start to feel like magic. And while we’ve dismissed the magic myth, turbocharging
    a plan and having momentum at your back *will* feel amazing.
  prefs: []
  type: TYPE_NORMAL
- en: The modality of human features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A mechanism for bringing AI models “to life” involves connecting those models
    and applications to human features like voice, language, vision, and reasoning.
    GenAI, and especially agents, is included in a lot of frictionless customer experience
    discussions that typically land on the topic of chatbots. But the term *chatbot*
    often invokes visions of typing—and while that is one modality, a natural-sounding
    voice behind an interactive voice response (IVR) is a bot, too. We’ve all interacted
    with IVRs that don’t sound human at all, but with AI, you can bring real human
    sound *and* expression to theexperience. For example, try uploading something
    into Google’s NotebookLM and asking it to generate a podcast for you—impressive
    stuff! Using AI helps turbocharge IVRs with expressive voices that let you welcome
    your customers with humanlike speech, emotions, word emphasis, and interjections.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While we cover agentic AI in this book, we don’t specifically cover the impact
    of agents on the modality of interaction, specifically the user experience (UX).
    The designs of tomorrow will have to consider two kinds of users: humans and agents.
    The agent experience (AX) will be using APIs to compose workflows *but* now includes
    desktop interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: This capability is important because whether we realize it or not, as humans,
    we convey emotions in the words we speak. We may sound empathetic when apologizing
    to one another, uncertain when we don’t know the answer to something, and cheerful
    when we convey good news. This ability to convey emotion is what makes our voices
    human, and using AI to do this can ultimately reduce customer frustration when
    dealing with today’s phone experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'But here’s the big point we want you to understand (and why upskilling is such
    a hot topic): customized brand voices (even yours) can be generated in minutes,
    with no technical expertise required! Quite simply, expressive voices make customers
    feel like they are talking to a real human and not a robot, but your company will
    get the benefits of shifting left (deflecting) those costs from a live agent (who
    costs about $5 per interaction) to an AI-assisted agent (which costs about $0.25
    per interaction) for “the easy stuff.” This is such a great example of those problems
    we walk by every day that we can solve or make better with technology. If you
    own a support channel with an IVR and have no idea how easy it is to build out
    human-sounding natural interactions, you’re settling for a maze of “Press 1 to...,”
    where instead of finding the prize at the end, your clients find themselves yelling
    “Talk to someone!” into the void.'
  prefs: []
  type: TYPE_NORMAL
- en: This really leads to multimodal AI, where human features become more and more
    apparent in the AI. For example, Google’s Gemini, Apple’s FERRET, Meta’s Llama,
    DeepSeek’s Janus-Pro, IBM’s Granite, and various OpenAI models all allow you to
    include a picture in a prompt, and they’ll tell you what they see. Imagine that
    you’ve been sent a picture of a package at your door from a delivery service,
    and it came with an AI-generated description that might notify you, “The corner
    of this box is damaged.” Also imagine that same package came with a prefilled
    form to submit ifthe package’s contents are damaged once you get home and open
    it. If you open your package and all is fine, great, nothing to do. And if something
    is wrong with the shipped item, the return will be as frictionless as possible—this
    is agentic AI at work! We really want you to put yourself in the picture in this
    example. While it’s true no one wants something to go wrong (like getting shipped
    a damaged item, receiving the wrong item, or having to reset a password), the
    *bigger basic truth* is that when things do go wrong, you shouldn’t present your
    customers with friction (like transferring them three times, asking them to reauthenticate
    their identity, and all the stuff that could be summarized as the WTF moments
    we seem to live weekly these days). Ironically, studies show that truly good customer
    experiences are *not just* about a business getting it right. In fact, as a business,
    you’re probably allowed to get stuff wrong (depending on the use case—if your
    business is heart surgery and you get it wrong, then there may not be a customer
    to complain, but we’re sure some lawyers will). But keeping things frictionless
    is critical, and it buys your company customer patience, understanding, loyalty,
    and more when things don’t go as planned.
  prefs: []
  type: TYPE_NORMAL
- en: AI management and exploitation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This enables you to confidently insert AI into any application or business process,
    but to do that, you need to understand how the model was built, what data was
    used, how to improve a model’s impact, what has changed, drift, bias, and variance.
    This is where your models live for exploitation and enable lifecycle management
    of all AI. Lastly, this component offers proof of and explainability for decisions
    made by your AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of it this way: if we were to tell you the amount of data generated every
    minute in the world, that number would be out of date the moment we saved the
    first draft of this chapter. Every time we updated this chapter, it would be instantly
    out of date. Your models are not much different, and this is referred to as *drift*.
    You need to know that AI models can start to drift the moment they go into production.
    And if your data history (the data you used to train the model) doesn’t “rhyme”
    with the data of today, that model is really going to drift away from what it
    was intended to do (like pick an opportunity) and/or start to do bad things (like
    pick up bias).'
  prefs: []
  type: TYPE_NORMAL
- en: Agents and assistants for the masses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As you work AI into your business’s nervous system, classify the AI, and attach
    it to workflows (this is +AI), you should know that agents and assistants *really*
    help you deliver serious benefits to the business. We think agents and assistants
    are where you can really democratize AI in your company (in many cases, you will
    see them integrated). Yes, it’s important to have an AI platform that lets you
    collect, organize, and store data, build GenAI models, and govern them. Super
    important. But assistants and agents are the chassis to use the power of your
    models to lift the enterprise. For example, development teams can use Microsoft
    Copilot or a flavor of IBM’s watsonx Code Assistant to power up their development
    process. Perhaps you’re designing a frictionless experience for customers using
    watsonx Assistant or Kore.ai, or perhaps you’re even orchestrating workflows using
    Aisera or watsonx Orchestrate with its library of AI agents. All of these are
    examples of real AI boosting the productivity of people in your business. We think
    that’s a critical piece of any successful AI strategy because it gives detailed
    answers to the questions: who is going to use the AI and how is it going to help
    them? Depending on your job, you’d be well served to know the answers to these
    questions—or know to ask them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process: Cake ingredients without a recipe do not make a cake'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With these components in hand, more organizations will be able to unlock the
    value that lies within their data. But to fully leverage AI, you must also understand
    how to adopt and implement this technology. Here’s some quick advice on some fundamental
    steps to put AI for business to work (again, you’ll get more details as you read
    this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Identify the right business opportunities for AI'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The potential areas for adoption are vast: customer service, employee and company
    productivity, manufacturing defects, supply chain spending, and many more. Anything
    that can be easily described can be programmed, and once it’s programmed, AI will
    make it better. As you learned in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974)
    (and it will really come at you in [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425)),
    the opportunities are endless, *but* it’s important that you make all your efforts
    about business opportunities and outcomes, and *not* data science projects. During
    the Hadoop big-data frenzy, we saw too many clients invest massive amounts of
    budget and time into projects that didn’t deliver value to the business or weren’t
    consumable by the business. This is why GenAI is so different: it makes building
    use cases faster than ever before, and it’s consumable by the masses. Just remember,
    choose wisely.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Prepare the organization for AI'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Organizations will require greater capacity and expertise in many areas, from
    having the obvious data science teams all the way to having a broadened aperture
    on just what GenAI can do for your business (to avoid that whole “walking by problems
    every day that can be solved or made better with the technology” thing we keep
    talking about in this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'You’re going to need to do a massive upskilling around GenAI, LLMs, and agents.
    This effort isn’t about pop-quizzing your marketing copy editor on what least
    absolute shrinkage and selection operator (Lasso) regression is or what AUC stands
    for (area under the receiver operating characteristic [ROC] curve) and so on.
    Having a general base knowledge of the benefits and cautions around AI will be
    critical to getting AI to work for your company. We can’t stress this piece enough:
    you must have a plan to upskill all employees to distribute the benefits of AI
    across your company; and that’s why we dedicated a whole chapter to it—[Chapter 6](ch06.html#ch06_skills_that_thrill_1740182050334297).'
  prefs: []
  type: TYPE_NORMAL
- en: Why is this so important? Many of today’s repetitive and manual tasks will be
    automated (shifted left), which will evolve the role of many employees. *It’s
    rare that an entire role can be done by AI, and it’s also rare that no roles could
    be enhanced by AI.* All technology is useless without the talent to put it to
    use, so you must build a team of experts who will inspire and train others—but
    you must ensure that other employees’ skills are constantly evolving. After all,
    while technology years are typically akin to dog years (1 dog year equals 7 human
    years), GenAI and agents are progressing like mouse years (1 mouse year equals
    30 human years)—you need a plan to keep up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Select technology and partners'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While it’s unlikely a CEO would personally select a company’s GenAI technology
    stack (or stacks), the implication here is more of a cultural one. An organization
    should adopt many technologies and compare, contrast, and learn through that process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll give you a good tip that will save you a lot of pain: don’t fall into
    the common trap of thinking the cloud will be one place from one provider. Looking
    in the rearview mirror, it’s easy to see how that notion has been proven wrong.
    Now, we’re not saying you should have hundreds of AI vendors in your shop (they
    are popping up everywhere), but we are reminding you here that one AI model will
    not rule them all. Organizations should choose a handful of *trustworthy* partners
    that have both the skills and the technology to deliver AI. Also, we’ve italicized
    *trustworthy* here for a reason. We don’t need to get into the details here, but
    especially in tech, you’re likely familiar with good actors (upstanders) and bad
    actors (which are at best bystanders and at worst well-known malefactors). Again,
    we think trust will be the ultimate operating license, and we’ll let you think
    about who you trust from here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the day, we think most success comes from partnerships—be they
    personal or professional. Think about it: Batman partnered with Robin, Bert had
    Ernie, Sherlock was nothing without Watson, and even Snooki had The Situation.
    (That last bit is for anyone who still speaks the *Jersey Shore* parlance—we’re
    hoping there aren’t many of you left, and we’re even happier if you have no idea
    what we’re talking about.)'
  prefs: []
  type: TYPE_NORMAL
- en: Accept failures but do so in a safe manner
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Do you know that over 80% of traditional AI projects never made it to production?
    As you’ve read about in this chapter, GenAI should improve on those numbers because
    of the simplicity of getting it going, but you’re still going to encounter friction
    and failures (wrong completions, legislation, and so on). Perhaps you’ll try 40
    AI projects and 30 of them fail, but the 10 that work will more than compensate
    for the failures, *if* you pick the right use cases, which is why we wrote [Chapter 4](ch04.html#ch04_the_use_case_chapter_1740182047877425).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lots of people like to say, “Fail fast and fail forward.” This implies that
    teams should quickly recognize when stuff isn’t working, learn from their mistakes,
    and move on. We think that’s too shallow when it comes to GenAI (and especially
    agents) advice for many use cases. Think about it this way: would you tell your
    university kid (for whom you are footing the bill) the same thing? We highly doubt
    it. We’d propose thinking, “Fail fast, fail forward, and fail safe,” and advise
    your kid to do that instead. This is why we think governments shouldn’t necessarily
    regulate AI (the default position for many governments) but regulate the use cases
    for AI. We think the AI behind a criminal justice sentencing system (fail fast,
    fail forward, fail safe) should be held to a much higher account and have more
    regulatory oversight than an AI that recommends what TV series you should binge-watch
    next because you loved the *Young Sheldon* show (fail fast, fail forward—no one
    is truly going to get hurt from watching *The Real Housewives of New Jersey*...well,
    perhaps a few). This is exactly why in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974)
    we said the safest place to start is with an internal automation use case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The culture you create has to change too. It must be ready and willing to accept
    safe failures, learn from them, and move on to the next one. For those of you
    who are leading your company’s AI projects (again, some of which are bound to
    fail), we have this great piece of advice that we came up with by hybridizing
    quotes from Michael Hyatt and Forrest Gump: on your greatest days, you’re probably
    not as smart as you think you are, but on your worst days, you’re probably not
    as dumb as you think you are either.'
  prefs: []
  type: TYPE_NORMAL
- en: The Future of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the advances achieved in the last few years, the ambition of the 1950s
    has come full circle. Today’s models *do not* constitute true general intelligence
    (although reasoning models are getting us closer), but some of them can pass the
    *Turing test* (originally referred to as the *imitation game*), which is a test
    of a machine’s ability to exhibit intelligent behavior equivalent to or indistinguishable
    from that of a human.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what does this mean for all of us? Some people encounter GenAI and agents
    and think we’re at the dawn of a bright utopian age, while others think this is
    a prelude to dystopian misery. We take a moderate but positively slanted view:
    *a technology doesn’t have to be world ending to be world changing*. Like we said
    in [Chapter 1](ch01.html#ch01_ai_to_ai_generative_ai_and_the_netscape_moment_1740182043982974),
    we don’t think it should be a surprise to anyone that technological innovations
    can help and/or hurt us (social media is a great example of this). We want you
    to know that we think both optimism and anxiety are valid, and that society has
    questioned every major innovation milestone from the Industrial Revolution onward
    (and in many cases, gotten it wrong).'
  prefs: []
  type: TYPE_NORMAL
- en: AI isn’t just going to be about our digital world. It’s also about our physical
    world; and applied properly, imagine what AI can do for the pace of discovery
    and innovation. It’s not just makeup; imagine what it can do for new materials
    discovery for medicine, energy, climate, and all the other pressing challenges
    we face as a species—these are the same challenges of makeup, just described with
    a different “language.” And as quantum computing evolves, we’re bound to see a
    synergy of these innovations that we can use to tackle these problem domains and
    more. Finally, what of a new kind of computing around GenAI—we’ll save that for
    [Chapter 9](ch09.html#ch09_generative_computing_a_new_style_of_computing_1740182052619664).
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, our success and that of all humanity depends on how we and the rest
    of the world approach AI.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Get into It
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve covered a lot of topics (at a high level) so far in this book, but we’ve
    basically told you that you have to do a lot of non-techie work to be great at
    AI. Maybe that feels overwhelming. It’s not our intention to make you feel that
    way, but you do need to feel a bit unsettled to move faster—to move with intent
    so that you don’t miss out. The goal of the rest of this book is to *remove barriers
    to your participation, not construct them*.
  prefs: []
  type: TYPE_NORMAL
- en: Make no mistake about it, if you’re feeling a sense of urgency and fear about
    waiting too long and missing the moment, that’s OK. We can assure you that almost
    every other company is in the same situation, and lots of people are feeling the
    very same emotions that you are feeling right now. And trust us, we’ve heard many
    fishing stories of individuals and companies talking about their AI or how their
    products are built with AI, and like most fishing stories, many are exaggerated
    or untrue. We want to tell those people not to go telling fishing stories to those
    who know the real size of the fish, but we just smile and carry on with our day.
    That said, by reading this book, you’ll be in a position to do the same, and we’ll
    let you decide if you just smile or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can promise you (and your business) this: if you can show some restraint
    and not carelessly check the “I put AI in the business” box using fast and easy
    options (or be pressured to do so); if instead, you are thoughtful, deliberate,
    and strategic about using a platform that considers all the components you need
    (AI, data intelligence, data integration, and governance); and *most importantly,
    if you set your GPS to a destination of “AI Value Creator,”* then you’re going
    to be in a position to succeed over the long term. What’s more, like so many before
    you, your company won’t have to start over every time the winds of AI change direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Personally, we’re very excited about this new chapter in technology. We, all
    of us together, are going to use GenAI and agents to reshape not just our digital
    world but also our physical world. We’re going to use it to help tackle some of
    our toughest social, medical, and environmental problems, and more. We’ll do it
    through science, but also by empowering businesses—like the ones you work for
    and the one we work for—to do more faster and more responsibly. Whatever thing
    it is that your company does, AI is going to be a powerful new tool to help you
    do it better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re quite certain of this: *the AI Value Creators will be the ones who make
    the biggest impact*. They will take the amazing foundational technology that is
    GenAI and use it to build entirely new solutions and workflows. That’s why it’s
    our goal to make AI accessible to everyone and put it in your hands, which is
    what this book is all about.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch02.html#id389-marker)) Alan Turing, “Computing Machinery and Intelligence,”
    *Mind* 49, no. 236 (October 1950): 433‒460, [*https://doi.org/10.1093/mind/LIX.236.433*](https://doi.org/10.1093/mind/LIX.236.433).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.html#id395-marker)) Noise in training data is any kind of irrelevant
    or random information, errors, or variations that do not reflect the true underlying
    patterns or relationships in the data.
  prefs: []
  type: TYPE_NORMAL
