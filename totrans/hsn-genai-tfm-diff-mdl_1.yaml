- en: Chapter 2\. Building up to Stable Diffusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。逐步构建稳定扩散
- en: In the previous chapter, we introduced diffusion models and the underlying idea
    of iterative refinement. By the end of the chapter, we could generate images,
    but training the model was time-consuming and we had no control over the images
    that were generated. In this chapter, we’ll see how to go from this to text-conditioned
    models that can efficiently generate images based on text descriptions, with a
    model called Stable Diffusion (SD) as a case study. Before we get to SD, though,
    we’ll first look at how conditional models work and go over some of the innovations
    that lead up to the text-to-image models we have today.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了扩散模型和迭代细化的基本思想。在章节结束时，我们可以生成图像，但训练模型非常耗时，而且我们无法控制生成的图像。在本章中，我们将看到如何从这一点过渡到基于文本描述有效生成图像的条件化模型，以Stable
    Diffusion（SD）模型作为案例研究。不过，在我们深入研究SD之前，我们将首先看一下条件化模型的工作原理，并回顾一些导致我们今天拥有文本到图像模型的创新。
- en: 'Adding Control: Conditional Diffusion Models'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加控制：条件化扩散模型
- en: Before we deal with the problem of generating images from text descriptions
    (a very challenging task!), let’s focus on something slightly easier first. We’ll
    see how we can steer our models outputs towards specific types or *classes* of
    images. We can use a method called *conditioning*, where the idea is to ask the
    model to generate not just any image, but an image belonging to a pre-defined
    class.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们处理从文本描述生成图像的问题之前（这是一个非常具有挑战性的任务！），让我们先专注于一些稍微容易的事情。我们将看到如何引导我们模型的输出朝向特定类型或*类别*的图像。我们可以使用一种叫做*条件化*的方法，其思想是要求模型生成不仅仅是任何图像，而是属于预定义类别的图像。
- en: Model *conditioning* is a simple but effective idea. We’ll start from the same
    diffusion model we used in Chapter 3, with just a couple of changes. First, we’ll
    use a new dataset called Fashion MNIST instead of butterflies so that we can identify
    categories easily. Then, crucially, we’ll run two inputs through the model. Instead
    of just showing it how *real* images look like, we’ll also tell it the class every
    image belongs to. We hope the model will learn to associate images and labels,
    so it gets an idea about the distinctive features of sweaters, boots and the like.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 模型*条件化*是一个简单但有效的想法。我们将从第3章中使用的相同扩散模型开始，只做了一些改变。首先，我们将使用一个叫做Fashion MNIST的新数据集，而不是蝴蝶，这样我们就可以轻松识别类别。然后，至关重要的是，我们将两个输入通过模型。我们不仅向它展示*真实*图像是什么样子的，还告诉它每个图像属于哪个类别。我们希望模型能够学会将图像和标签关联起来，这样它就能了解毛衣、靴子等的独特特征。
- en: 'Note that we are not interested in solving a classification problem – we don’t
    want the model to tell us the class, given an input image –. We still want it
    to perform the same task as in Chapter 3, namely: *please, generate plausible
    images that look like they came from this dataset*. The only difference is that
    we are giving it additional information about those images. We’ll use the same
    loss function and training strategy, as it’s the same task as before.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不感兴趣解决分类问题 - 我们不希望模型在给定输入图像的情况下告诉我们类别。我们仍然希望它执行与第3章相同的任务，即：*请生成看起来像来自这个数据集的可信图像*。唯一的区别是我们给它提供了关于这些图像的额外信息。我们将使用相同的损失函数和训练策略，因为这与以前的任务相同。
- en: Preparing the Data
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据。
- en: We need a dataset with distinct groups of images. Datasets intended for computer
    vision classification tasks are ideal for this purpose. We could start with something
    like the ImageNet dataset, which contains millions of images across 1000 classes.
    However, training models on this dataset would take an extremely long time. When
    approaching a new problem, it’s often a good idea to start with a smaller dataset
    first, to make sure everything works as expected. This keeps the feedback loop
    short, so we can iterate quickly and make sure we’re on the right track.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个包含不同图像组的数据集。用于计算机视觉分类任务的数据集非常适合这个目的。我们可以从像ImageNet这样的数据集开始，该数据集包含了1000个类别的数百万张图像。然而，在这个数据集上训练模型需要非常长的时间。在解决新问题时，通常最好先从一个较小的数据集开始，以确保一切都按预期工作。这样可以保持反馈循环的短暂，这样我们可以快速迭代并确保我们走在正确的轨道上。
- en: 'For this example, we could choose MNIST as we did in Chapter 3\. To make things
    just a little bit different, we’ll choose Fashion MNIST instead. Fashion MNIST,
    developed and open-sourced by Zalando, is a replacement for MNIST that shares
    some of the same characteristics: a compact size, black & white images, and 10
    classes. The main difference is that instead of being digits, classes correspond
    to different types of clothing and the images contain more detail than simple
    handwritten digits.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们可以像在第3章中那样选择MNIST。为了稍微有些不同，我们将选择Fashion MNIST。Fashion MNIST是由Zalando开发并开源的，是MNIST的替代品，具有一些相同的特点：紧凑的大小，黑白图像和10个类别。主要区别在于，类别不是数字，而是对应于不同类型的服装，图像比简单的手写数字包含更多的细节。
- en: Let’s look at some examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些例子。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![image](assets/cell-7-output-1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-7-output-1.png)'
- en: 'So class *0* means t-shirt, *2* is a sweater and *9* means boot. Here’s a list
    of the 10 categories in Fashion MNIST: [*https://huggingface.co/datasets/fashion_mnist#data-fields*](https://huggingface.co/datasets/fashion_mnist#data-fields).
    We prepare our dataset and dataloader similarly to how we did it in Chapter 3,
    with the main difference that we’ll also include the class information as an input.
    Instead of resizing, in this case we’ll pad our image inputs (which have a size
    of *28 × 28* pixels) to *32 × 32*, as we did in Chapter 3.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以类别*0*表示T恤，*2*是毛衣，*9*表示靴子。以下是Fashion MNIST中的10个类别列表：[*https://huggingface.co/datasets/fashion_mnist#data-fields*](https://huggingface.co/datasets/fashion_mnist#data-fields)。我们准备我们的数据集和数据加载器，类似于我们在第3章中所做的，主要的区别是我们还将类别信息作为输入包含进去。在这种情况下，我们不是调整大小，而是将我们的图像输入（大小为*28×28*像素）填充到*32×32*，就像我们在第3章中所做的那样。
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Creating a Class-Conditioned Model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个类别条件化模型
- en: If we use the *UNet* model from the diffusers library, we can provide our own
    custom conditioning information. Here we create a similar model to the one we
    used in Chapter 3, but we add a *num_class_embeds* argument to the *UNet* constructor.
    This argument tells the model that we’d like to use class labels as additional
    conditioning. We’ll use 10, because we have 10 classes in Fashion MNIST.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用diffusers库中的*UNet*模型，我们可以提供自定义的条件信息。在这里，我们创建了一个类似于第3章中使用的模型，但我们在*UNet*构造函数中添加了一个*num_class_embeds*参数。该参数告诉模型我们想要使用类别标签作为额外的条件。我们将使用10，因为Fashion
    MNIST中有10个类别。
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To make predictions with this model, we must pass in the class labels as additional
    inputs to the *forward* method:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用该模型进行预测，我们必须将类别标签作为*forward*方法的附加输入传递：
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'You’ll notice we also pass something else to the model as conditioning: the
    timestep! That’s right, even the model from Chapter 3 can be considered a conditional
    diffusion model! We condition it on the timestep in the hopes that knowing how
    far we are in the diffusion process will help it generate more realistic images.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们还向模型传递了其他条件信息：时间步！没错，即使是第3章中的模型也可以被视为有条件的扩散模型！我们将其条件设置为时间步，希望知道我们在扩散过程中的进展有助于它生成更真实的图像。
- en: Internally, both the timestep and the class label are turned into **embeddings**
    that the model uses during its forward pass. At multiple stages throughout the
    UNet, these embeddings are projected onto a dimension that matches the number
    of channels in a given layer and are then added to the outputs of that layer.
    This means the conditioning information is fed to every block of the UNet, giving
    the model ample opportunity to learn how to use it effectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，时间步长和类别标签都被转换为模型在前向传递过程中使用的**嵌入**。在UNet的多个阶段，这些嵌入被投影到与给定层中通道数相匹配的维度，然后添加到该层的输出中。这意味着条件信息被馈送到UNet的每个块中，使模型有充分的机会学习如何有效地使用它。
- en: Training the Model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Adding noise works just as well on greyscale images as it did on the butterflies
    from Chapter 3.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 添加噪音对灰度图像的效果与第3章中的蝴蝶一样好。
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![image](assets/cell-10-output-1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-10-output-1.png)'
- en: Our training loop is also almost exactly the same as in Chapter 3 too, except
    that we now pass the class labels for conditioning. Note that this is just additional
    information for the model, but it doesn’t affect our loss function in any way.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练循环几乎与第3章中的相同，只是现在我们传递类别标签进行条件处理。请注意，这只是模型的附加信息，但并不以任何方式影响我们的损失函数。
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case we train for 25 epochs - full code can be found in the supplementary
    material.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们进行了25个时期的训练-完整的代码可以在补充材料中找到。
- en: Sampling
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样
- en: 'Now we have a model that expects two inputs when making predictions: the image
    and the class label. We can create samples by beginning with random noise and
    then iteratively denoising, passing in whatever class label we’d like to generate:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个模型，当进行预测时需要两个输入：图像和类别标签。我们可以通过从随机噪声开始，然后迭代去噪，传入我们想要生成的任何类别标签来创建样本：
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![image](assets/cell-15-output-2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-15-output-2.png)'
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![image](assets/cell-16-output-2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-16-output-2.png)'
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![image](assets/cell-17-output-2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-17-output-2.png)'
- en: 'As you can see, the generated images are far from perfect. They’d probably
    get much better if we explored the architecture and trained for longer. But it’s
    amazing that the model not only learnt the shapes of different types of clothing,
    but also realized that shape *9* looks different than shape *0*, just by sending
    this information alongside the training data. To put it in a slightly different
    way: the model is used to seeing the number *9* accompanying boots. When we ask
    it to generate an image and provide the *9*, it responds with a boot. We have
    successfully built a class-conditioned model capable of generating images **conditioned**
    on class labels from fasionMNIST!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，生成的图像远非完美。如果我们探索架构并进行更长时间的训练，它们可能会变得更好。但令人惊讶的是，该模型不仅学会了不同类型的服装形状，还意识到形状*9*看起来与形状*0*不同，只是通过在训练数据旁边发送这些信息。换句话说：该模型习惯于看到数字*9*伴随着靴子。当我们要求它生成一张图像并提供*9*时，它会回应一个靴子。我们成功地构建了一个能够生成**基于fashionMNIST类别标签**的图像的有条件模型！
- en: 'Improving Efficiency: Latent Diffusion'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高效率：潜在扩散
- en: Now that we can train a conditional model, all we need to do is scale it up
    and condition it on text instead of class labels, right? Well, not quite. As image
    size grows, so does the computational power required to work with those images.
    This is especially pronounced in an operation called self-attention, where the
    amount of operations grows quadratically with the number of inputs. A 128px square
    image has 4x as many pixels as a 64px square image, and so requires 16x (i.e. 
    <math alttext="4 squared"><msup><mn>4</mn> <mn>2</mn></msup></math> ) the memory
    and compute in a self-attention layer. This is a problem for anyone who’d like
    to generate high-resolution images!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练一个有条件的模型，我们所需要做的就是将其扩展并以文本而不是类别标签进行条件处理，对吗？嗯，并不完全是这样。随着图像尺寸的增加，处理这些图像所需的计算能力也会增加。这在一个称为自注意力的操作中尤为明显，其中操作的数量随着输入数量的增加呈二次增长。一个128像素的正方形图像有4倍于64像素正方形图像的像素数量，因此在自注意力层中需要16倍（即<math
    alttext="4 squared"><msup><mn>4</mn> <mn>2</mn></msup></math>）的内存和计算。这对于希望生成高分辨率图像的人来说是一个问题！
- en: '![image.png](assets/modelfigure_ldm.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![image.png](assets/modelfigure_ldm.png)'
- en: Figure 2-1\. The architecture introduced in the [Latent Diffusion Models paper](https://arxiv.org/abs/2112.10752).
    Note the VAE encoder and decoder on the left for translating between pixel space
    and latent space
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。[潜在扩散模型论文](https://arxiv.org/abs/2112.10752)中介绍的架构。请注意左侧的VAE编码器和解码器，用于在像素空间和潜在空间之间进行转换
- en: Latent diffusion tries to mitigate this issue by using a separate model called
    a Variational Auto-Encoder (VAE). As we saw in Chapter 2, VAEs can compress images
    to a smaller spatial dimension. The rationale behind this is that images tend
    to contain a large amount of redundant information - given enough training data,
    a VAE can hopefully learn to produce a much smaller representation of an input
    image and then reconstruct the image based on this small latent representation
    with a high degree of fidelity. The VAE used in SD takes in 3-channel images and
    produces a 4-channel latent representation with a reduction factor of 8 for each
    spatial dimension. That is, a 512px square input image will be compressed down
    to a 4x64x64 latent.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散试图通过使用称为变分自动编码器（VAE）的单独模型来缓解这个问题。正如我们在第2章中看到的，VAE可以将图像压缩到较小的空间维度。其背后的原理是图像往往包含大量冗余信息
    - 给定足够的训练数据，VAE有望学会以更小的表示产生输入图像，并且然后基于这个小的潜在表示重构图像，并且具有高度的保真度。SD中使用的VAE接收3通道图像，并产生一个4通道的潜在表示，每个空间维度的缩减因子为8。也就是说，一个512像素的正方形输入图像将被压缩到一个4x64x64的潜在表示。
- en: By applying the diffusion process on **these smaller latent representations**
    rather than on full-resolution images, we can get many of the benefits that would
    come from using smaller images (lower memory usage, fewer layers needed in the
    UNet, faster generation times…) and still decode the result back to a high-resolution
    image once we’re ready to view it. This innovation dramatically lowers the cost
    to train and run these models. The paper that introduced this idea ([High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    by Rombach et al) demonstrated the power of this technique by training models
    conditioned on segmentation maps, class labels and text. The impressive results
    led to further collaboration between the authors and partners such as RunwayML,
    LAION, and EleutherAI to train a more powerful version of the model, which later
    became Stable Diffusion.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在**这些较小的潜在表示**上应用扩散过程，而不是在全分辨率图像上应用，我们可以获得许多使用较小图像所带来的好处（更低的内存使用、UNet中需要更少的层、更快的生成时间...），并且一旦准备查看结果，仍然可以将结果解码回高分辨率图像。这种创新大大降低了训练和运行这些模型的成本。引入这一想法的论文（Rombach等人的《使用潜在扩散模型进行高分辨率图像合成》）通过训练模型展示了这一技术的威力，这些模型是基于分割地图、类标签和文本的。令人印象深刻的结果导致作者与RunwayML、LAION和EleutherAI等合作伙伴进一步合作，以训练模型的更强大版本，后来成为Stable
    Diffusion。
- en: 'Stable Diffusion: Components in Depth'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stable Diffusion：深入组件
- en: 'Stable Diffusion is a text-conditioned latent diffusion model. Thanks to its
    popularity, there are hundreds of websites and apps that let you use it to create
    images with no technical knowledge required. It’s also very well-supported by
    libraries like `diffusers`, which let us sample an image with SD using a user-friendly
    pipeline:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion是一个文本条件的潜在扩散模型。由于其受欢迎程度，有数百个网站和应用程序可以让您使用它来创建图像，而无需任何技术知识。它还得到了像`diffusers`这样的库的很好支持，这些库让我们使用用户友好的管道来使用SD对图像进行采样：
- en: '[PRE16]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![image](assets/cell-19-output-2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-19-output-2.png)'
- en: In this section we’ll explore all of the components that make this possible.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨使这一切成为可能的所有组件。
- en: The Text Encoder
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本编码器
- en: So how does Stable Diffusion understand text? Earlier on we showed how feeding
    additional information to the UNet allows us to have some additional control over
    the types of images generated. Given a noisy version of an image, the model is
    tasked with predicting the denoised version based on additional clues such as
    a class label. In the case of SD, the additional *clue* is the text prompt. At
    inference time, we can feed in the description of an image we’d like to see and
    some pure noise as a starting point, and the model does its best to *denoise*
    the random input into something that matches the caption.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那么Stable Diffusion如何理解文本呢？我们早些时候展示了如何向UNet提供额外信息，使我们能够对生成的图像类型有一些额外的控制。给定图像的嘈杂版本，模型的任务是根据额外的线索（例如类标签）来预测去噪版本。在SD的情况下，额外的*线索*是文本提示。在推断时，我们可以提供想要查看的图像的描述以及一些纯噪声作为起点，模型会尽力将随机输入*去噪*成与说明相匹配的东西。
- en: '![simplified_unet.png](assets/236d5a2e-1-simplified_unet.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![simplified_unet.png](assets/236d5a2e-1-simplified_unet.png)'
- en: Figure 2-2\. The text encoder turns an input string into text embeddings which
    are fed into the UNet along with the timestep and the noisy latents.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。文本编码器将输入字符串转换为文本嵌入，这些嵌入与时间步长和嘈杂的潜在一起输入UNet。
- en: For this to work, we need to create a numeric representation of the text that
    captures relevant information about what it describes. To do this, SD leverages
    a pre-trained transformer model based on CLIP, which was also introduced in Chapter
    2\. The text encoder is a transformer model that takes in a sequence of tokens
    and produces a 1024-dimensional vector for each token (0r 768-dimensional in the
    case of SD version 1 which we’re using for the demonstrations in this section).
    Instead of combining these vectors into a single representation, we keep them
    separate and use them as conditioning for the UNet. This allows the UNet to make
    use of the information in each token separately, rather than just the overall
    meaning of the entire prompt. Because we’re extracting these text embeddings from
    the internal representation of the CLIP model, they are often called the “encoder
    hidden states”. Figure 3 shows the text encoder architecture.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其工作，我们需要创建文本的数值表示，以捕获有关其描述的相关信息。为此，SD利用了基于CLIP的预训练变压器模型，这也是在第2章中介绍的。文本编码器是一个变压器模型，它接受一系列标记，并为每个标记产生一个1024维的向量（或者在我们用于本节演示的SD版本1中为768维）。我们不是将这些向量组合成单个表示，而是保持它们分开，并将它们用作UNet的条件。这使得UNet可以单独使用每个标记中的信息，而不仅仅是整个提示的整体含义。因为我们从CLIP模型的内部表示中提取这些文本嵌入，它们通常被称为“编码器隐藏状态”。图3显示了文本编码器的架构。
- en: '![text encoder](assets/text_encoder_noborder.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![text encoder](assets/text_encoder_noborder.png)'
- en: Figure 2-3\. Diagram showing the text encoding process which transforms the
    input prompt into a set of text embeddings (the encoder_hidden_states) which can
    then be fed in as conditioning to the UNet.
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3。显示文本编码过程的图表，将输入提示转换为一组文本嵌入（编码器隐藏状态），然后可以将其作为条件输入到UNet中。
- en: The first step to encode text is to follow a process called **tokenization**.
    This converts a sequence of characters into a sequence of numbers, where each
    number represents a group of various characters. Characters that are usually found
    together (like most common words) can be assigned a single token that represents
    the whole word or group. Long or complicated words, or words with many inflections,
    may be translated to multiple tokens, where each one usually represents a meaningful
    section of the word.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 编码文本的第一步是遵循一个称为**分词**的过程。这将把一系列字符转换成一系列数字，其中每个数字代表一组不同的字符。通常一起出现的字符（如大多数常见单词）可以被分配一个代表整个单词或组的单个标记。长或复杂的单词，或者有许多屈折的单词，可以被翻译成多个标记，其中每个标记通常代表单词的一个有意义的部分。
- en: There is no single “best” tokenizer; instead, each language model comes with
    its own one. Differences reside in the number of tokens supported, and on the
    tokenization strategy – do we use single characters, as we just described, or
    should we consider different primitive units. In the following example we see
    how the tokenization of a phrase works with Stable Diffusion’s tokenizer. Each
    word in our sentence is assigned a unique token number (for example, `photograph`
    happens to be `8853` in the tokenizer’s vocabulary). There are also additional
    tokens that are used to provide additional context, such as the point where the
    sentence ends.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 没有单一的“最佳”分词器；相反，每个语言模型都有自己的分词器。差异在于支持的标记数量，以及分词策略 - 我们使用单个字符，正如我们刚刚描述的，还是应该考虑不同的基本单元。在下面的示例中，我们看到了如何使用Stable
    Diffusion的分词器对短语进行分词。我们句子中的每个单词都被分配一个唯一的标记号（例如，`photograph`在分词器的词汇表中是`8853`）。还有其他额外的标记，用于提供额外的上下文，比如句子结束的地方。
- en: '[PRE18]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once the text is tokenized, we can pass it through the text encoder to get
    the final text embeddings that will be fed into the UNet:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本被分词，我们就可以通过文本编码器将其传递，以获得将被馈送到UNet中的最终文本嵌入：
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We’ll go into more detail about how a transformer model processes a string of
    tokens in the chapters focusing on transformer models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在关于变压器模型处理标记字符串的章节中详细介绍。
- en: Classifier-free guidance
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无分类器引导
- en: It turns out that even with all of the effort put into making the text conditioning
    as useful as possible, the model still tends to default to relying mostly on the
    noisy input image rather than the prompt when making its predictions. In a way,
    this makes sense - many captions are only loosely related to their associated
    images and so the model learns not to rely too heavily on the descriptions! However,
    this is undesirable when it comes time to generate new images - if the model doesn’t
    follow the prompt then we may get images out that don’t relate to our description
    at all.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，即使在使文本条件尽可能有用的所有努力中，模型在进行预测时仍倾向于主要依赖于嘈杂的输入图像而不是提示。在某种程度上，这是有道理的 - 许多标题只与其相关联的图像松散相关，因此模型学会不要过于依赖描述！然而，当生成新图像时，这是不可取的
    - 如果模型不遵循提示，那么我们可能得到与我们的描述毫不相关的图像。
- en: '![image.png](assets/cfg_example_0_1_2_10.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![image.png](assets/cfg_example_0_1_2_10.jpeg)'
- en: Figure 2-4\. Images generated from the prompt “An oil painting of a collie in
    a top hat” with CFG scale 0, 1, 2 and 10 (left to right)
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4。从提示“戴礼帽的牧羊犬的油画”生成的图像，CFG比例为0、1、2和10（从左到右）
- en: 'To fix this, we use a trick called Classifier-Free Guidance (CGF). During training,
    text conditioning is sometimes kept blank, forcing the model to learn to denoise
    images with no text information whatsoever (unconditional generation). Then at
    inference time, we make two separate predictions: one with the text prompt as
    conditioning and one without. We can then use the difference between these two
    predictions to create a final combined prediction that pushes even further in
    the direction indicated by the text-conditioned prediction according to some scaling
    factor (the guidance scale), hopefully resulting in an image that better matches
    the prompt. Figure 4 shows the outputs for a prompt at different guidance scales
    - as you can see, higher values result in images that better match the description.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们使用了一个叫做无分类器引导（CGF）的技巧。在训练期间，有时会保持文本条件为空，迫使模型学习无文本信息的图像去噪（无条件生成）。然后在推断时，我们进行两次单独的预测：一次是带有文本提示作为条件，一次是没有。然后我们可以使用这两次预测之间的差异来创建一个最终的组合预测，根据某个缩放因子（引导比例）进一步推动文本条件预测所指示的方向，希望得到更好地匹配提示的图像。图4显示了不同引导比例下提示的输出
    - 如您所见，较高的值会产生更好地匹配描述的图像。
- en: The VAE
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VAE
- en: The VAE is tasked with compressing images into a smaller latent representation
    and back again. The VAE used with Stable Diffusion is a truly impressive model.
    We won’t go into the training details here, but in addition to the usual reconstruction
    loss and KL divergence described in Chapter 2 they use an additional patch-based
    discriminator loss to help the model learn to output plausible details and textures.
    This adds a GAN-like component to training and helps to avoid the slightly blurry
    outputs that were typical in previous VAEs. Like the text encoder, the VAE is
    usually trained separately and used as a frozen component during the diffusion
    model training and sampling process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的任务是将图像压缩成较小的潜在表示，然后再次解压缩。与Stable Diffusion一起使用的VAE是一个真正令人印象深刻的模型。我们不会在这里详细介绍训练细节，但除了第2章中描述的常规重建损失和KL散度之外，它们还使用了一个额外的基于补丁的鉴别器损失，以帮助模型学习输出合理的细节和纹理。这为训练增加了类似GAN的组件，并有助于避免以前VAE中典型的略模糊的输出。与文本编码器一样，VAE通常是单独训练的，并在扩散模型的训练和采样过程中作为冻结组件使用。
- en: '![vae.drawio.png](assets/adc13a6c-1-vae.drawio.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![vae.drawio.png](assets/adc13a6c-1-vae.drawio.png)'
- en: Figure 2-5\. Encoding and decoding and image with the VAE
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 使用VAE对图像进行编码和解码
- en: 'Let’s load an image and see what it looks like after being compressed and decompressed
    by the VAE:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载一张图像，看看经过VAE压缩和解压后的样子：
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![image](assets/cell-23-output-1.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/cell-23-output-1.png)'
- en: '[PRE24]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Visualizing the low-resolution latent representation, we can that some of the
    rough structure of the input image is still visible in the different channels:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化低分辨率的潜在表示，我们可以看到输入图像的一些粗略结构仍然可见在不同的通道中：
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![image](assets/cell-25-output-1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/cell-25-output-1.png)'
- en: And decoding back to image space, we get an output image that is almost identical
    to the original. Can you spot the difference?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 解码回图像空间，我们得到一个几乎与原始图像相同的输出图像。你能发现区别吗？
- en: '[PRE27]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![image](assets/cell-26-output-1.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/cell-26-output-1.png)'
- en: When generating images from scratch, we create a random set of latents as the
    starting point. We iteratively refine these noisy latents to generate a sample,
    and then the VAE decoder is used to decode these final latents into an image we
    can view. The encoder is only used if we’d like to start the process from an existing
    image, something we’ll explore in chapter 5.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在从头开始生成图像时，我们创建一组随机潜变量作为起点。我们迭代地优化这些嘈杂的潜变量以生成一个样本，然后使用VAE解码器将这些最终潜变量解码成我们可以查看的图像。编码器仅在我们想要从现有图像开始该过程时使用，这是我们将在第5章中探讨的内容。
- en: The UNet
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UNet
- en: The UNet used in stable diffusion is somewhat similar to the one we used in
    chapter 3 for generating images. Instead of taking in a 3-channel image as the
    input we take in a 4-channel latent. The timestep embedding is fed in in the same
    way as the class conditioning was in the example at the start of this chapter.
    But this UNet also needs to accept the text embeddings as additional conditioning.
    Scattered throughout the UNet are cross-attention layers. Each spatial location
    in the UNet can *attend* to different tokens in the text conditioning, bringing
    in relevant information from the prompt. The diagram in Figure 7 shows how this
    text conditioning (as well as timestep-based conditioning) is fed in at different
    points.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散中使用的UNet与我们在第3章中用于生成图像的UNet有些相似。我们不是以3通道图像作为输入，而是以4通道潜变量作为输入。时间步嵌入以与本章开头示例中的类别条件相同的方式输入。但是这个UNet还需要接受文本嵌入作为额外的条件。UNet中散布着交叉注意力层。UNet中的每个空间位置都可以*关注*文本条件中的不同标记，从提示中带入相关信息。图7中的图表显示了文本条件（以及基于时间步的条件）是如何在不同点输入的。
- en: '![SD digram](assets/sd_unet_color.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![SD图表](assets/sd_unet_color.png)'
- en: Figure 2-6\. The Stable Diffusion UNet
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 稳定扩散 UNet
- en: The UNet for Stable Diffusion version 1 and 2 has around 860 million parameters.
    The more recent SD XL has even more, at around (details TBC), with most of the
    additional parameters being added at the lower-resolution stages via additional
    channels in the residual blocks (N vs 1280 in the original) and additional transformer
    blocks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散版本1和2的UNet大约有8.6亿个参数。更新更近期的SD XL拥有更多参数，大约为（详细信息待定），大部分额外参数是通过在残差块中添加额外通道（原始版本中的N对1280）和添加变压器块来增加低分辨率阶段的。
- en: 'NB: Stable Diffusion XL has not yet been publically released, so this section
    will be updated when more information is public.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注：稳定扩散XL尚未公开发布，因此当有更多信息公开时，本节将进行更新。
- en: 'Putting it All Together: Annotated Sampling Loop'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容放在一起：带注释的采样循环
- en: 'Now that we know what each of the components does, let’s put them together
    to generate an image without relying on the pipeline. Here are the settings we’ll
    use:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道每个组件的作用，让我们把它们放在一起，生成一张图像，而不依赖于管道。以下是我们将使用的设置：
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The first step is to encode the text prompt. Because we plan to do classifier-free
    guidance, we’ll actually create two sets of text embeddings: one with the prompt,
    and one representing an empty string. You can also encode a *negative prompt*
    in place of the empty string, or combine multiple prompts with different weightings,
    but this is the most common usage:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是对文本提示进行编码。因为我们计划进行无分类器的引导，实际上我们将创建两组文本嵌入：一组是提示，另一组代表空字符串。您还可以编码*负提示*来代替空字符串，或者结合多个提示以不同的权重，但这是最常见的用法：
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next we create our random initial latents and set up the scheduler to use the
    desired number of inference steps:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建我们的随机初始潜变量，并设置调度程序以使用所需数量的推断步骤：
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we loop through the sampling steps, getting the model prediction at each
    stage and using this to update the latents:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们循环进行采样步骤，获取每个阶段的模型预测，并使用这些来更新潜变量：
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice the classifier-free guidance step. Our final noise prediction is *noise_pred_uncond
    + guidance_scale * (noise_pred_text - noise_pred_uncond)*, pushing the prediction
    *away* from the unconditional prediction towards the prediction made based on
    the prompt. Try changing the guidance scale to see how this affects the output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意无分类器引导步骤。我们最终的噪声预测是*noise_pred_uncond + guidance_scale * (noise_pred_text
    - noise_pred_uncond)*，将预测*远离*无条件预测，朝向基于提示的预测。尝试更改引导比例，看看这如何影响输出。
- en: 'By the end of the loop the latents should hopefully now represent a plausible
    image that matches the prompt. The final step is to decode the latents into an
    image using the VAE so that we can see the result:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 循环结束时，潜变量应该能够表示一个符合提示的合理图像。最后一步是使用VAE将潜变量解码成图像，以便我们可以看到结果：
- en: '[PRE32]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![image](assets/cell-31-output-1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/cell-31-output-1.png)'
- en: If you explore the source code for the *StableDiffusionPipeline* you’ll see
    that the code above closely matches the **call** method used by the pipeline.
    Hopefully this annotated version shows that there is nothing too magical going
    on behind the scenes! Use this as a reference for when we encounter additional
    pipelines that add additional tricks to this foundation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您探索*StableDiffusionPipeline*的源代码，您会发现上面的代码与管道使用的**call**方法非常相似。希望这个带注释的版本显示了幕后并没有太神奇的事情发生！当我们遇到添加额外技巧的其他管道时，请将其用作参考。
- en: Training Data for Text-To-Image Models (TBD)
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到图像模型的训练数据（待定）
- en: 'NB: We may add a more in-depth section here with the history and technical
    details of how LAION came together, and some of the nuances and debate around
    training on public data scraped from the internet.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注：我们可能会在这里添加一个更深入的部分，介绍LAION是如何形成的历史和技术细节，以及一些关于在从互联网上抓取的公共数据上进行训练的细微差别和争论。
- en: Open Data, Open Models
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放数据，开放模型
- en: The LAION-5B dataset includes over 5 billion image-caption pairs scraped from
    the internet. This dataset was created by and for the open-source community, which
    saw the need for a publically-accessible dataset of this kind. Before the LAION
    initiative, only a handful of research labs at large companies had access to such
    data. These organizations kept the details of their private datasets to themselves,
    which made their results impossible to validate or replicate. By creating a publically
    available source of training data, LAION enabled a wave of smaller communities
    and organizations to train models and perform research that would otherwise have
    been impossible.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: LAION-5B数据集包括从互联网上抓取的50亿多个图像标题对。这个数据集是由开源社区创建的，他们认为有必要有这样一个公开可访问的数据集。在LAION计划之前，只有少数大公司的研究实验室才能访问这样的数据。这些组织将他们的私有数据集的细节保留给自己，这使得他们的结果无法验证或复制。通过创建一个公开可用的训练数据来源，LAION使得一波较小的社区和组织能够进行训练模型和进行研究，否则这是不可能的。
- en: '![image](assets/cell-32-output-1.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![image](assets/cell-32-output-1.png)'
- en: Figure 2-7\. “An explosion of artistic creativity” - Image generated by the
    authors using Stable Diffusion
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7。作者使用稳定扩散生成的“艺术创造力的爆发”
- en: Stable Diffusion was one such model, trained on a subset of LAION as part of
    a collaboration between the researchers who had invented latent diffusion models
    and an organization called Stability AI. Training a model like SD requires a significant
    amount of GPU time. Even with the freely-available LAION dataset, there aren’t
    many who could afford the investment. This is why the public release of the model
    weights and code was such a big deal - it marked the first time a powerful text-to-image
    model with similar capabilities to the best closed-source alternatives was available
    to all. Stable Diffusion’s public availability has made it the go-to choice for
    researchers and developers looking to explore this technology over the past year.
    Hundreds of papers build upon the base model, adding new capabilities or finding
    innovative ways to improve its speed and quality. And innumerable startups have
    found ways to integrate these rapidly-improving tools into their products, spawning
    an entire ecosystem of new applications.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散是这样一个模型，它是作为一个由发明了潜在扩散模型的研究人员和一个名为Stability AI的组织之间的合作的一部分，对LAION的子集进行训练的。像SD这样的模型需要大量的GPU时间来训练。即使有免费提供的LAION数据集，也没有多少人能够负担得起这样的投资。这就是为什么模型权重和代码的公开发布如此重要的原因——这标志着一个功能强大的文本到图像模型，具有类似于最好的闭源替代品的能力，首次对所有人都可用。稳定扩散的公开可用性使其成为过去一年来研究人员和开发人员探索这项技术的首选选择。数百篇论文都是基于基础模型，添加新的功能或找到改进其速度和质量的创新方法。无数初创公司已经找到了将这些快速改进的工具整合到其产品中的方法，从而产生了一个全新应用的生态系统。
- en: The months after the introduction of Stable Diffusion demonstrated the impact
    of sharing these technologies in the open. SD is not the best text-to-image model,
    but it IS the best model most of us had access to, so thousands of people have
    spent their time making it better and building upon that open foundation. We hope
    this example encourages others to follow suit and share their work with the open-source
    community in the future!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散推出后的几个月展示了在开放中分享这些技术的影响。SD并不是最好的文本到图像模型，但它是大多数人可以访问到的最好的模型，因此成千上万的人花费了时间使其变得更好，并在此开放基础上构建。我们希望这个例子鼓励其他人效仿，并在未来与开源社区分享他们的工作！
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we’ve seen how *conditioning* gives us new ways to control the
    images generated by diffusion models. We’ve seen how latent diffusion lets us
    train diffusion models more efficiently. We’ve seen how a text encoder can be
    used to condition a diffusion model on a text prompt, enabling powerful text-to-image
    capabilities. And we’ve explored how all of this comes together in the Stable
    Diffusion model by digging into the sampling loop and seeing how the different
    components work together. In the next chapter, we’ll show some of the many additional
    capabilities that can be added to diffusion models such as SD to take them beyond
    simple image generation. And later, in part 2 of the book, you’ll learn how to
    fine-tune SD to add new knowledge or capabilities to the model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到*条件*如何为我们提供了控制扩散模型生成图像的新方法。我们已经看到潜在扩散如何让我们更有效地训练扩散模型。我们已经看到文本编码器如何被用来在文本提示上对扩散模型进行条件设定，实现强大的文本到图像的能力。我们已经探讨了所有这些是如何在稳定扩散模型中结合在一起的，通过深入研究采样循环并看到不同组件如何一起工作。在下一章中，我们将展示可以添加到扩散模型（如SD）中的许多额外功能，使它们超越简单的图像生成。在本书的第二部分中，您将学习如何微调SD，以向模型添加新的知识或功能。
- en: About the Authors
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: '**Pedro Cuenca** is a machine learning engineer who works on diffusion software,
    models, and applications at Hugging Face.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Pedro Cuenca是一名机器学习工程师，他在Hugging Face公司负责扩散软件、模型和应用程序的工作。
- en: '**Apolinário Passos** is a machine learning art engineer at Hugging Face, working
    across teams on multiple machine learning for art and creativity use cases.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Apolinário Passos是Hugging Face公司的机器学习艺术工程师，他在多个机器学习艺术和创造性用例的团队之间工作。
- en: '**Omar Sanseviero** is a lead machine learning engineer at Hugging Face, where
    he works at the intersection of open source, community, and product. Previously,
    Omar worked at Google on Google Assistant and TensorFlow.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Omar Sanseviero是Hugging Face公司的首席机器学习工程师，他在开源、社区和产品的交叉领域工作。之前，Omar曾在谷歌的Google
    Assistant和TensorFlow工作。
- en: '**Jonathan Whitaker** is a data scientist and deep learning researcher focused
    on generative modeling. Besides his research and consulting work, his main focus
    is on sharing knowledge, which he does via the DataScienceCastnet YouTube channel
    and various free online resources he has created.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 乔纳森·惠特克（Jonathan Whitaker）是一位专注于生成建模的数据科学家和深度学习研究员。除了他的研究和咨询工作外，他的主要关注点是分享知识，他通过DataScienceCastnet
    YouTube频道和他创建的各种免费在线资源来实现。
