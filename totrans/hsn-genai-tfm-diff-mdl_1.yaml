- en: Chapter 2\. Building up to Stable Diffusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced diffusion models and the underlying idea
    of iterative refinement. By the end of the chapter, we could generate images,
    but training the model was time-consuming and we had no control over the images
    that were generated. In this chapter, we’ll see how to go from this to text-conditioned
    models that can efficiently generate images based on text descriptions, with a
    model called Stable Diffusion (SD) as a case study. Before we get to SD, though,
    we’ll first look at how conditional models work and go over some of the innovations
    that lead up to the text-to-image models we have today.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding Control: Conditional Diffusion Models'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we deal with the problem of generating images from text descriptions
    (a very challenging task!), let’s focus on something slightly easier first. We’ll
    see how we can steer our models outputs towards specific types or *classes* of
    images. We can use a method called *conditioning*, where the idea is to ask the
    model to generate not just any image, but an image belonging to a pre-defined
    class.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Model *conditioning* is a simple but effective idea. We’ll start from the same
    diffusion model we used in Chapter 3, with just a couple of changes. First, we’ll
    use a new dataset called Fashion MNIST instead of butterflies so that we can identify
    categories easily. Then, crucially, we’ll run two inputs through the model. Instead
    of just showing it how *real* images look like, we’ll also tell it the class every
    image belongs to. We hope the model will learn to associate images and labels,
    so it gets an idea about the distinctive features of sweaters, boots and the like.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we are not interested in solving a classification problem – we don’t
    want the model to tell us the class, given an input image –. We still want it
    to perform the same task as in Chapter 3, namely: *please, generate plausible
    images that look like they came from this dataset*. The only difference is that
    we are giving it additional information about those images. We’ll use the same
    loss function and training strategy, as it’s the same task as before.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a dataset with distinct groups of images. Datasets intended for computer
    vision classification tasks are ideal for this purpose. We could start with something
    like the ImageNet dataset, which contains millions of images across 1000 classes.
    However, training models on this dataset would take an extremely long time. When
    approaching a new problem, it’s often a good idea to start with a smaller dataset
    first, to make sure everything works as expected. This keeps the feedback loop
    short, so we can iterate quickly and make sure we’re on the right track.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we could choose MNIST as we did in Chapter 3\. To make things
    just a little bit different, we’ll choose Fashion MNIST instead. Fashion MNIST,
    developed and open-sourced by Zalando, is a replacement for MNIST that shares
    some of the same characteristics: a compact size, black & white images, and 10
    classes. The main difference is that instead of being digits, classes correspond
    to different types of clothing and the images contain more detail than simple
    handwritten digits.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![image](assets/cell-7-output-1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: 'So class *0* means t-shirt, *2* is a sweater and *9* means boot. Here’s a list
    of the 10 categories in Fashion MNIST: [*https://huggingface.co/datasets/fashion_mnist#data-fields*](https://huggingface.co/datasets/fashion_mnist#data-fields).
    We prepare our dataset and dataloader similarly to how we did it in Chapter 3,
    with the main difference that we’ll also include the class information as an input.
    Instead of resizing, in this case we’ll pad our image inputs (which have a size
    of *28 × 28* pixels) to *32 × 32*, as we did in Chapter 3.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Creating a Class-Conditioned Model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we use the *UNet* model from the diffusers library, we can provide our own
    custom conditioning information. Here we create a similar model to the one we
    used in Chapter 3, but we add a *num_class_embeds* argument to the *UNet* constructor.
    This argument tells the model that we’d like to use class labels as additional
    conditioning. We’ll use 10, because we have 10 classes in Fashion MNIST.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To make predictions with this model, we must pass in the class labels as additional
    inputs to the *forward* method:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You’ll notice we also pass something else to the model as conditioning: the
    timestep! That’s right, even the model from Chapter 3 can be considered a conditional
    diffusion model! We condition it on the timestep in the hopes that knowing how
    far we are in the diffusion process will help it generate more realistic images.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Internally, both the timestep and the class label are turned into **embeddings**
    that the model uses during its forward pass. At multiple stages throughout the
    UNet, these embeddings are projected onto a dimension that matches the number
    of channels in a given layer and are then added to the outputs of that layer.
    This means the conditioning information is fed to every block of the UNet, giving
    the model ample opportunity to learn how to use it effectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adding noise works just as well on greyscale images as it did on the butterflies
    from Chapter 3.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![image](assets/cell-10-output-1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Our training loop is also almost exactly the same as in Chapter 3 too, except
    that we now pass the class labels for conditioning. Note that this is just additional
    information for the model, but it doesn’t affect our loss function in any way.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case we train for 25 epochs - full code can be found in the supplementary
    material.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we have a model that expects two inputs when making predictions: the image
    and the class label. We can create samples by beginning with random noise and
    then iteratively denoising, passing in whatever class label we’d like to generate:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![image](assets/cell-15-output-2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![image](assets/cell-16-output-2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![image](assets/cell-17-output-2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the generated images are far from perfect. They’d probably
    get much better if we explored the architecture and trained for longer. But it’s
    amazing that the model not only learnt the shapes of different types of clothing,
    but also realized that shape *9* looks different than shape *0*, just by sending
    this information alongside the training data. To put it in a slightly different
    way: the model is used to seeing the number *9* accompanying boots. When we ask
    it to generate an image and provide the *9*, it responds with a boot. We have
    successfully built a class-conditioned model capable of generating images **conditioned**
    on class labels from fasionMNIST!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving Efficiency: Latent Diffusion'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can train a conditional model, all we need to do is scale it up
    and condition it on text instead of class labels, right? Well, not quite. As image
    size grows, so does the computational power required to work with those images.
    This is especially pronounced in an operation called self-attention, where the
    amount of operations grows quadratically with the number of inputs. A 128px square
    image has 4x as many pixels as a 64px square image, and so requires 16x (i.e. 
    <math alttext="4 squared"><msup><mn>4</mn> <mn>2</mn></msup></math> ) the memory
    and compute in a self-attention layer. This is a problem for anyone who’d like
    to generate high-resolution images!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![image.png](assets/modelfigure_ldm.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. The architecture introduced in the [Latent Diffusion Models paper](https://arxiv.org/abs/2112.10752).
    Note the VAE encoder and decoder on the left for translating between pixel space
    and latent space
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Latent diffusion tries to mitigate this issue by using a separate model called
    a Variational Auto-Encoder (VAE). As we saw in Chapter 2, VAEs can compress images
    to a smaller spatial dimension. The rationale behind this is that images tend
    to contain a large amount of redundant information - given enough training data,
    a VAE can hopefully learn to produce a much smaller representation of an input
    image and then reconstruct the image based on this small latent representation
    with a high degree of fidelity. The VAE used in SD takes in 3-channel images and
    produces a 4-channel latent representation with a reduction factor of 8 for each
    spatial dimension. That is, a 512px square input image will be compressed down
    to a 4x64x64 latent.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: By applying the diffusion process on **these smaller latent representations**
    rather than on full-resolution images, we can get many of the benefits that would
    come from using smaller images (lower memory usage, fewer layers needed in the
    UNet, faster generation times…) and still decode the result back to a high-resolution
    image once we’re ready to view it. This innovation dramatically lowers the cost
    to train and run these models. The paper that introduced this idea ([High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    by Rombach et al) demonstrated the power of this technique by training models
    conditioned on segmentation maps, class labels and text. The impressive results
    led to further collaboration between the authors and partners such as RunwayML,
    LAION, and EleutherAI to train a more powerful version of the model, which later
    became Stable Diffusion.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Stable Diffusion: Components in Depth'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stable Diffusion is a text-conditioned latent diffusion model. Thanks to its
    popularity, there are hundreds of websites and apps that let you use it to create
    images with no technical knowledge required. It’s also very well-supported by
    libraries like `diffusers`, which let us sample an image with SD using a user-friendly
    pipeline:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![image](assets/cell-19-output-2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: In this section we’ll explore all of the components that make this possible.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: The Text Encoder
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So how does Stable Diffusion understand text? Earlier on we showed how feeding
    additional information to the UNet allows us to have some additional control over
    the types of images generated. Given a noisy version of an image, the model is
    tasked with predicting the denoised version based on additional clues such as
    a class label. In the case of SD, the additional *clue* is the text prompt. At
    inference time, we can feed in the description of an image we’d like to see and
    some pure noise as a starting point, and the model does its best to *denoise*
    the random input into something that matches the caption.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![simplified_unet.png](assets/236d5a2e-1-simplified_unet.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. The text encoder turns an input string into text embeddings which
    are fed into the UNet along with the timestep and the noisy latents.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For this to work, we need to create a numeric representation of the text that
    captures relevant information about what it describes. To do this, SD leverages
    a pre-trained transformer model based on CLIP, which was also introduced in Chapter
    2\. The text encoder is a transformer model that takes in a sequence of tokens
    and produces a 1024-dimensional vector for each token (0r 768-dimensional in the
    case of SD version 1 which we’re using for the demonstrations in this section).
    Instead of combining these vectors into a single representation, we keep them
    separate and use them as conditioning for the UNet. This allows the UNet to make
    use of the information in each token separately, rather than just the overall
    meaning of the entire prompt. Because we’re extracting these text embeddings from
    the internal representation of the CLIP model, they are often called the “encoder
    hidden states”. Figure 3 shows the text encoder architecture.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![text encoder](assets/text_encoder_noborder.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Diagram showing the text encoding process which transforms the
    input prompt into a set of text embeddings (the encoder_hidden_states) which can
    then be fed in as conditioning to the UNet.
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first step to encode text is to follow a process called **tokenization**.
    This converts a sequence of characters into a sequence of numbers, where each
    number represents a group of various characters. Characters that are usually found
    together (like most common words) can be assigned a single token that represents
    the whole word or group. Long or complicated words, or words with many inflections,
    may be translated to multiple tokens, where each one usually represents a meaningful
    section of the word.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: There is no single “best” tokenizer; instead, each language model comes with
    its own one. Differences reside in the number of tokens supported, and on the
    tokenization strategy – do we use single characters, as we just described, or
    should we consider different primitive units. In the following example we see
    how the tokenization of a phrase works with Stable Diffusion’s tokenizer. Each
    word in our sentence is assigned a unique token number (for example, `photograph`
    happens to be `8853` in the tokenizer’s vocabulary). There are also additional
    tokens that are used to provide additional context, such as the point where the
    sentence ends.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once the text is tokenized, we can pass it through the text encoder to get
    the final text embeddings that will be fed into the UNet:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We’ll go into more detail about how a transformer model processes a string of
    tokens in the chapters focusing on transformer models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Classifier-free guidance
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It turns out that even with all of the effort put into making the text conditioning
    as useful as possible, the model still tends to default to relying mostly on the
    noisy input image rather than the prompt when making its predictions. In a way,
    this makes sense - many captions are only loosely related to their associated
    images and so the model learns not to rely too heavily on the descriptions! However,
    this is undesirable when it comes time to generate new images - if the model doesn’t
    follow the prompt then we may get images out that don’t relate to our description
    at all.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![image.png](assets/cfg_example_0_1_2_10.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Images generated from the prompt “An oil painting of a collie in
    a top hat” with CFG scale 0, 1, 2 and 10 (left to right)
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To fix this, we use a trick called Classifier-Free Guidance (CGF). During training,
    text conditioning is sometimes kept blank, forcing the model to learn to denoise
    images with no text information whatsoever (unconditional generation). Then at
    inference time, we make two separate predictions: one with the text prompt as
    conditioning and one without. We can then use the difference between these two
    predictions to create a final combined prediction that pushes even further in
    the direction indicated by the text-conditioned prediction according to some scaling
    factor (the guidance scale), hopefully resulting in an image that better matches
    the prompt. Figure 4 shows the outputs for a prompt at different guidance scales
    - as you can see, higher values result in images that better match the description.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The VAE
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The VAE is tasked with compressing images into a smaller latent representation
    and back again. The VAE used with Stable Diffusion is a truly impressive model.
    We won’t go into the training details here, but in addition to the usual reconstruction
    loss and KL divergence described in Chapter 2 they use an additional patch-based
    discriminator loss to help the model learn to output plausible details and textures.
    This adds a GAN-like component to training and helps to avoid the slightly blurry
    outputs that were typical in previous VAEs. Like the text encoder, the VAE is
    usually trained separately and used as a frozen component during the diffusion
    model training and sampling process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![vae.drawio.png](assets/adc13a6c-1-vae.drawio.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Encoding and decoding and image with the VAE
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s load an image and see what it looks like after being compressed and decompressed
    by the VAE:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![image](assets/cell-23-output-1.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: '[PRE24]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Visualizing the low-resolution latent representation, we can that some of the
    rough structure of the input image is still visible in the different channels:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![image](assets/cell-25-output-1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: And decoding back to image space, we get an output image that is almost identical
    to the original. Can you spot the difference?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![image](assets/cell-26-output-1.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: When generating images from scratch, we create a random set of latents as the
    starting point. We iteratively refine these noisy latents to generate a sample,
    and then the VAE decoder is used to decode these final latents into an image we
    can view. The encoder is only used if we’d like to start the process from an existing
    image, something we’ll explore in chapter 5.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The UNet
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UNet used in stable diffusion is somewhat similar to the one we used in
    chapter 3 for generating images. Instead of taking in a 3-channel image as the
    input we take in a 4-channel latent. The timestep embedding is fed in in the same
    way as the class conditioning was in the example at the start of this chapter.
    But this UNet also needs to accept the text embeddings as additional conditioning.
    Scattered throughout the UNet are cross-attention layers. Each spatial location
    in the UNet can *attend* to different tokens in the text conditioning, bringing
    in relevant information from the prompt. The diagram in Figure 7 shows how this
    text conditioning (as well as timestep-based conditioning) is fed in at different
    points.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![SD digram](assets/sd_unet_color.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. The Stable Diffusion UNet
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The UNet for Stable Diffusion version 1 and 2 has around 860 million parameters.
    The more recent SD XL has even more, at around (details TBC), with most of the
    additional parameters being added at the lower-resolution stages via additional
    channels in the residual blocks (N vs 1280 in the original) and additional transformer
    blocks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'NB: Stable Diffusion XL has not yet been publically released, so this section
    will be updated when more information is public.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it All Together: Annotated Sampling Loop'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know what each of the components does, let’s put them together
    to generate an image without relying on the pipeline. Here are the settings we’ll
    use:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The first step is to encode the text prompt. Because we plan to do classifier-free
    guidance, we’ll actually create two sets of text embeddings: one with the prompt,
    and one representing an empty string. You can also encode a *negative prompt*
    in place of the empty string, or combine multiple prompts with different weightings,
    but this is the most common usage:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next we create our random initial latents and set up the scheduler to use the
    desired number of inference steps:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we loop through the sampling steps, getting the model prediction at each
    stage and using this to update the latents:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice the classifier-free guidance step. Our final noise prediction is *noise_pred_uncond
    + guidance_scale * (noise_pred_text - noise_pred_uncond)*, pushing the prediction
    *away* from the unconditional prediction towards the prediction made based on
    the prompt. Try changing the guidance scale to see how this affects the output.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of the loop the latents should hopefully now represent a plausible
    image that matches the prompt. The final step is to decode the latents into an
    image using the VAE so that we can see the result:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![image](assets/cell-31-output-1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: If you explore the source code for the *StableDiffusionPipeline* you’ll see
    that the code above closely matches the **call** method used by the pipeline.
    Hopefully this annotated version shows that there is nothing too magical going
    on behind the scenes! Use this as a reference for when we encounter additional
    pipelines that add additional tricks to this foundation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Training Data for Text-To-Image Models (TBD)
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NB: We may add a more in-depth section here with the history and technical
    details of how LAION came together, and some of the nuances and debate around
    training on public data scraped from the internet.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Open Data, Open Models
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LAION-5B dataset includes over 5 billion image-caption pairs scraped from
    the internet. This dataset was created by and for the open-source community, which
    saw the need for a publically-accessible dataset of this kind. Before the LAION
    initiative, only a handful of research labs at large companies had access to such
    data. These organizations kept the details of their private datasets to themselves,
    which made their results impossible to validate or replicate. By creating a publically
    available source of training data, LAION enabled a wave of smaller communities
    and organizations to train models and perform research that would otherwise have
    been impossible.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![image](assets/cell-32-output-1.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. “An explosion of artistic creativity” - Image generated by the
    authors using Stable Diffusion
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Stable Diffusion was one such model, trained on a subset of LAION as part of
    a collaboration between the researchers who had invented latent diffusion models
    and an organization called Stability AI. Training a model like SD requires a significant
    amount of GPU time. Even with the freely-available LAION dataset, there aren’t
    many who could afford the investment. This is why the public release of the model
    weights and code was such a big deal - it marked the first time a powerful text-to-image
    model with similar capabilities to the best closed-source alternatives was available
    to all. Stable Diffusion’s public availability has made it the go-to choice for
    researchers and developers looking to explore this technology over the past year.
    Hundreds of papers build upon the base model, adding new capabilities or finding
    innovative ways to improve its speed and quality. And innumerable startups have
    found ways to integrate these rapidly-improving tools into their products, spawning
    an entire ecosystem of new applications.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The months after the introduction of Stable Diffusion demonstrated the impact
    of sharing these technologies in the open. SD is not the best text-to-image model,
    but it IS the best model most of us had access to, so thousands of people have
    spent their time making it better and building upon that open foundation. We hope
    this example encourages others to follow suit and share their work with the open-source
    community in the future!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we’ve seen how *conditioning* gives us new ways to control the
    images generated by diffusion models. We’ve seen how latent diffusion lets us
    train diffusion models more efficiently. We’ve seen how a text encoder can be
    used to condition a diffusion model on a text prompt, enabling powerful text-to-image
    capabilities. And we’ve explored how all of this comes together in the Stable
    Diffusion model by digging into the sampling loop and seeing how the different
    components work together. In the next chapter, we’ll show some of the many additional
    capabilities that can be added to diffusion models such as SD to take them beyond
    simple image generation. And later, in part 2 of the book, you’ll learn how to
    fine-tune SD to add new knowledge or capabilities to the model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: About the Authors
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pedro Cuenca** is a machine learning engineer who works on diffusion software,
    models, and applications at Hugging Face.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**Apolinário Passos** is a machine learning art engineer at Hugging Face, working
    across teams on multiple machine learning for art and creativity use cases.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**Omar Sanseviero** is a lead machine learning engineer at Hugging Face, where
    he works at the intersection of open source, community, and product. Previously,
    Omar worked at Google on Google Assistant and TensorFlow.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '**Jonathan Whitaker** is a data scientist and deep learning researcher focused
    on generative modeling. Besides his research and consulting work, his main focus
    is on sharing knowledge, which he does via the DataScienceCastnet YouTube channel
    and various free online resources he has created.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 乔纳森·惠特克（Jonathan Whitaker）是一位专注于生成建模的数据科学家和深度学习研究员。除了他的研究和咨询工作外，他的主要关注点是分享知识，他通过DataScienceCastnet
    YouTube频道和他创建的各种免费在线资源来实现。
