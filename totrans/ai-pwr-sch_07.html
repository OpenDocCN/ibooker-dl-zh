<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">6</span> </span> <span class="chapter-title-text">Using context to learn domain-specific language</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Classifying query intent</li>
<li class="readable-text" id="p3">Query-sense disambiguation</li>
<li class="readable-text" id="p4">Identifying key terminology from user signals</li>
<li class="readable-text" id="p5">Learning related phrases from user signals</li>
<li class="readable-text" id="p6">Learning misspellings and alternate term variations from user signals</li>
</ul>
</div>
<div class="readable-text" id="p7">
<p>In chapter 5, we demonstrated both how to generate and use a semantic knowledge graph (SKG) and how to extract entities, facts, and relationships explicitly into a knowledge graph. Both techniques rely on navigating either the linguistic connections between terms in a single document or the statistical co-occurrences of terms across multiple documents and contexts. You learned to use knowledge graphs to find related terms, and how those related terms can integrate into various query-rewriting strategies to increase recall or precision. </p>
</div>
<div class="readable-text intended-text" id="p8">
<p>In this chapter, we’ll dive deeper into understanding query intent and the nuances of using different contexts to interpret domain-specific terminology in queries. We’ll start by exploring query classification and then show how those classifications can be used to disambiguate queries with multiple potential meanings. Both approaches will extend our use of SKGs from the last chapter.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>While those SKG-based approaches are more effective at contextualizing and interpreting queries, they continue to rely on having high-quality documents that accurately represent your domain. As a result, their efficacy for interpreting user queries depends on how well the queries overlap with the content being searched.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>For example, if 75% of your users are searching for clothing, but most of your inventory is films and digital media, then when they search for the query <code>shorts</code> and all the results are videos with short run times (known as “digital shorts”), most of your users will be confused by the results. Given the data in your query logs, it would be better if “shorts” could map to other related terms more commonly found in your query signals, like “pants”, “clothing”, and “shirts”.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>It can be very beneficial to not only rely on the content of your documents to learn relationships between terms and phrases, but to also use your user-generated signals. In this chapter, we’ll demonstrate techniques to extract key phrases, learn related phrases, and identify common misspellings or alternative spellings based on user signals. By using both content-based context and behavioral context from real user interactions, your search engine will better understand domain-specific terminology and actual user intent.</p>
</div>
<div class="readable-text" id="p12">
<h2 class="readable-text-h2" id="sigil_toc_id_81"><span class="num-string">6.1</span> Classifying query intent</h2>
</div>
<div class="readable-text" id="p13">
<p>The goal or intent of a query usually matters more than the keywords. A search for <code>driver crashed</code> can mean two <em>very</em> different things in the context of news or travel content versus a computer technology context. Similarly, someone searching in e-commerce for a specific product name or product ID is probably searching for a very specific item with a high likelihood of wanting to purchase it. A general search like <code>kitchen appliances</code> could indicate the user just intends to browse available products to see what’s available. </p>
</div>
<div class="readable-text intended-text" id="p14">
<p>In both contexts, a query classifier can be effective at determining the general kind of query being issued. Depending on the domain, a query’s context could be automatically applied (e.g., filtering the category of documents), used to modify the relevance algorithm (automatically boosting specific products), or even used to drive a different user experience (skipping the results page and going directly to a specific product’s page). In this section, we’ll show how to use the SKG from chapter 5 as a classifier for incoming queries to build a query classifier.</p>
</div>
<div class="readable-text intended-text" id="p15">
<p>An SKG traversal does a <em>k</em>-nearest neighbor search at each level of the graph traversal. <em>K</em>-nearest neighbor is a type of classification that takes a data point (such as a query or term) and tries to find the top <em>k</em> most similar other data points in a vector space. If we have a field like <code>category</code> or <code>classification</code> on our documents, we can ask the SKG to “find the category with the highest relatedness to my starting node”. Since the starting node is typically a user’s query, an SKG can classify that query. </p>
</div>
<div class="readable-text intended-text" id="p16">
<p>We’ll continue to use the indexed Stack Exchange datasets as an SKG to be extended for query classification (in this section) and query-sense disambiguation (in section 6.2).</p>
</div>
<div class="readable-text intended-text" id="p17">
<p>Listing 6.1 shows a function that takes a user query and traverses the SKG to find semantically related categories to classify the query. Since we’ve indexed multiple different Stack Exchange categories (scifi, health, cooking, devops, etc.), we’ll use those categories as our classifications.</p>
</div>
<div class="browsable-container listing-container" id="p18">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.1</span> Query classification using the SKG</h5>
<div class="code-area-container">
<pre class="code-area">def print_query_classification(query, classification_field="category",
      classification_limit=5, keywords_field="body", min_occurrences=5):

  nodes_to_traverse = [{"field": keywords_field,<span class="aframe-location"/> #1
                        "values": [query]},  #1
                       {"field": classification_field,<span class="aframe-location"/> #2
                        "min_occurrences": min_occurrences, <span class="aframe-location"/> #3
                       <span class="aframe-location"/> "limit": classification_limit}]  #4

 <span class="aframe-location"/> traversal = skg.traverse(*nodes_to_traverse)  #5
  print_classifications(query, traversal) <span class="aframe-location"/> #6

skg = get_skg(get_engine().get_collection("stackexchange"))

print_query_classification("docker", classification_limit=3)
print_query_classification("airplane", classification_limit=1)
print_query_classification("airplane AND crash", classification_limit=2)
print_query_classification("vitamins", classification_limit=2)
print_query_classification("alien", classification_limit=1)
print_query_classification("passport", classification_limit=1)
print_query_classification("driver", classification_limit=2)
print_query_classification("driver AND taxi", classification_limit=2)
print_query_classification("driver AND install", classification_limit=2)</pre>
<div class="code-annotations-overlay-container">
     #1 The initial node of the graph based on a query matching against a field
     <br/>#2 The field from which we’ll find related classifications. In this case, we traverse to the category field.
     <br/>#3 Only classifications occurring in at least this number of documents will be returned.
     <br/>#4 Sets the number of classifications to return
     <br/>#5 Traverses the SKG to classify the query
     <br/>#6 Prints a query and its classifications
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p19">
<p>Example query classifications:</p>
</div>
<div class="browsable-container listing-container" id="p20">
<div class="code-area-container">
<pre class="code-area">Query: docker
  Classifications:
    devops  0.87978

Query: airplane
  Classifications:
    travel  0.33334

Query: airplane AND crash
  Classifications:
    scifi  0.02149
    travel  0.00475

Query: vitamins
  Classifications:
    health  0.48681
    cooking  0.09441

Query: alien
  Classifications:
    scifi  0.62541

Query: passport
  Classifications:
    travel  0.82883

Query: driver
  Classifications:
    travel  0.38996
    devops  0.08917

Query: driver AND taxi
  Classifications:
    travel  0.24184
    scifi  -0.13757

Query: driver AND install
  Classifications:
    devops  0.22277
    travel  -0.00675</pre>
</div>
</div>
<div class="readable-text" id="p21">
<p>This request uses the SKG to find the top <em>k</em> nearest neighbors based on a comparison of the semantic similarity between the query and each available classification (within the <code>category</code> field).</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>We see classification scores for each potential category for each query, with <code>airplane</code> and <code>passport</code> classified to “travel”, <code>vitamins</code> classified to “health” and “cooking”, and <code>alien</code> classified to “scifi”. When we refine the <code>airplane</code> query to a more specific query like <code>airplane AND crash</code>, however, we see that the classification changes from “travel” to “scifi”, because documents about airplane crashes are more likely to occur within “scifi” documents than “travel” documents.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>As another example, <code>driver</code> could have multiple meanings. It returns two potential classifications (“travel” or “devops”), with the “travel” category being the clear choice when no other context is provided. When additional context <em>is</em> provided, however, we can see that the query <code>driver AND taxi</code> gets appropriately classified to the “travel” category, while <code>driver AND install</code> gets appropriately classified to the “devops” category.</p>
</div>
<div class="readable-text intended-text" id="p24">
<p>This ability for the SKG to find semantic relationships between arbitrary combinations of terms makes it useful for on-the-fly classification of incoming queries. You can auto-apply the classifications as query filters or boosts, route queries to a context-specific algorithm or landing page, or automatically disambiguate query terms. We’ll explore using a two-level graph traversal in the next section to implement query-sense disambiguation. </p>
</div>
<div class="readable-text" id="p25">
<h2 class="readable-text-h2" id="sigil_toc_id_82"><span class="num-string">6.2</span> Query-sense disambiguation</h2>
</div>
<div class="readable-text" id="p26">
<p>When interpreting users’ intent from their queries, understanding exactly what they meant by each word is challenging. The problem of polysemy, or ambiguous terms, can significantly affect your search results. </p>
</div>
<div class="readable-text intended-text" id="p27">
<p>If someone searches for <code>server</code>, this could refer to someone who takes orders and waits on tables at a restaurant, or it could mean a computer that runs software on a network. Ideally, we want our search engine to be able to disambiguate each of these word senses and generate a unique list of related terms within each disambiguated context. Figure 6.1 demonstrates these two potential contexts for the word “server” and the kinds of related terms one might find within each context.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p28">
<img alt="figure" height="303" src="../Images/CH06_F01_Grainger.png" width="559"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.1</span> Differentiating multiple senses of the ambiguous term “server”</h5>
</div>
<div class="readable-text" id="p29">
<p>In section 6.1, we demonstrated how to use an SKG to automatically classify queries into a set of known categories. Given that we already know how to classify our queries, adding a second-level traversal can provide a contextualized list of related terms for each query classification.</p>
</div>
<div class="readable-text intended-text" id="p30">
<p>In other words, by traversing from query to classification and then to terms, we can generate a list of terms that describe a contextualized interpretation of the original query within each of the top classifications. The following listing shows a function that disambiguates a query this way utilizing an SKG.</p>
</div>
<div class="browsable-container listing-container" id="p31">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.2</span> Disambiguating query intent across different contexts</h5>
<div class="code-area-container">
<pre class="code-area">def print_query_disambigutaion(query,
      context_field="category", context_limit=5,
      keywords_field="body", keywords_limit=10, min_occurrences=5):

  nodes_to_traverse = [{"field": keywords_field, <span class="aframe-location"/> #1
                        "values": [query]}, #1
                      <span class="aframe-location"/> {"field": context_field,  #2
                        "min_occurrences": min_occurrences, #2
                        "limit": context_limit}, #2
                       {"field": keywords_field, <span class="aframe-location"/> #3
                        "min_occurrences": min_occurrences, #3
                        "limit": keywords_limit}] #3

  traversal = skg.traverse(*nodes_to_traverse)
  print_disambigutaions(query, traversal)</pre>
<div class="code-annotations-overlay-container">
     #1 The starting node of the graph traversal (the user’s query)
     <br/>#2 The first traversal returns the contexts for disambiguating the query.
     <br/>#3 The second traversal is from keywords related to both the query AND each related context.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p32">
<p>You can see from this listing that a context field (the <code>category</code> field by default) and a keywords field (the <code>body</code> field by default) are used as part of a two-level traversal. For any query that is passed in, we first find the most semantically related category and then the terms most semantically related to the original query within that category. </p>
</div>
<div class="readable-text intended-text" id="p33">
<p>The following listing demonstrates how to call this function, passing in three different queries containing ambiguous terms for which we want to find differentiated meanings.</p>
</div>
<div class="browsable-container listing-container" id="p34">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.3</span> Running query-sense disambiguation for several queries</h5>
<div class="code-area-container">
<pre class="code-area">print_query_disambigutaion("server")
print_query_disambigutaion("driver", context_limit=2)
print_query_disambigutaion("chef", context_limit=2)</pre>
</div>
</div>
<div class="readable-text" id="p35">
<p>The results of the queries in listing 6.3 can be found in tables 6.1–6.3, followed by the search-engine-specific SKG request used to disambiguate <code>chef</code> in listing 6.4. Each disambiguation context (<code>category</code> field) is scored relative to the query, and each discovered keyword (<code>body</code> field) is scored relative to both the query and the disambiguation context. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p36">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 6.1</span> Related terms lists contextualized by category for the query <code>server</code></h5>
<table>
<thead>
<tr>
<th>
<div>
         Query: server 
       </div></th>
<th/>
</tr>
</thead>
<tbody>
<tr>
<td> <code>Context: devops 0.83796</code> <br/> <code> Keywords:</code> <br/> <code> server 0.93698</code> <br/> <code> servers 0.76818</code> <br/> <code> docker 0.75955</code> <br/> <code> code 0.72832</code> <br/> <code> configuration 0.70686</code> <br/> <code> deploy 0.70634</code> <br/> <code> nginx 0.70366</code> <br/> <code> jenkins 0.69934</code> <br/> <code> git 0.68932</code> <br/> <code> ssh 0.6836</code> <br/></td>
<td> <code>Context: cooking -0.1574</code> <br/> <code> Keywords:</code> <br/> <code> server 0.66363</code> <br/> <code> restaurant 0.16482</code> <br/> <code> pie 0.12882</code> <br/> <code> served 0.12098</code> <br/> <code> restaurants 0.11679</code> <br/> <code> knife 0.10788</code> <br/> <code> pieces 0.10135</code> <br/> <code> serve 0.08934</code> <br/> <code> staff 0.0886</code> <br/> <code> dish 0.08553</code> <br/></td>
</tr>
<tr>
<td> <code>Context: travel -0.15959</code> <br/> <code> Keywords:</code> <br/> <code> server 0.81226</code> <br/> <code> tipping 0.54391</code> <br/> <code> vpn 0.45352</code> <br/> <code> tip 0.41117</code> <br/></td>
<td> <code>Context: scifi -0.28208</code> <br/> <code> Keywords:</code> <br/> <code> server 0.78173</code> <br/> <code> flynn's 0.53341</code> <br/> <code> computer 0.28075</code> <br/> <code> computers 0.2593</code> <br/></td>
</tr>
<tr>
<td> <code> servers 0.39053</code> <br/> <code> firewall 0.33092</code> <br/> <code> restaurant 0.21698</code> <br/> <code> tips 0.19524</code> <br/> <code> bill 0.18951</code> <br/> <code> cash 0.18485</code> <br/></td>
<td> <code> flynn 0.24963</code> <br/> <code> servers 0.24778</code> <br/> <code> grid 0.23889</code> <br/> <code> networking 0.2178</code> <br/> <code> shutdown 0.21121</code> <br/> <code> hacker 0.19444</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p37">
<p>Table 6.1 shows the top most semantically related categories for the query <code>server</code>, followed by the most semantically related keywords from the <code>body</code> field within each of those category contexts. Based on the data, we see that the category of “devops” is the most semantically related (positive score of <code>0.83796</code>), whereas the next three categories all contain negative scores (<code>-0.1574</code> for “cooking”, <code>-0.15959</code> for “travel”, and <code>-0.28208</code> for “scifi”). For the query <code>server</code>, the “devops” category is thus overwhelmingly the most likely category to be relevant. </p>
</div>
<div class="readable-text intended-text" id="p38">
<p>If we look at the different term lists that come back for each of the categories, we also see several distinct meanings arise. In the “devops” category, the meaning of the term “server” is focused on tools related to managing, building, and deploying code to a computer server. In the “scifi” category, the meaning revolves around computer grids being hacked and having their networks shut down. In the “travel” category, on the other hand, the overwhelming sense of the word “server” is related to some-one working in a restaurant, with terms like “tipping”, “restaurant”, and “bill” showing up.</p>
</div>
<div class="readable-text intended-text" id="p39">
<p>When implementing an intelligent search application using this data, if you know the user’s context is related to travel, it makes sense to use the specific meaning within the “travel” category. If the context is unknown, the best choice is usually either the most semantically related category or the most popular category among your users.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p40">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 6.2</span> Contextualized related terms lists by category for the query <code>driver</code></h5>
<table>
<thead>
<tr>
<th>
<div>
         Query: driver 
       </div></th>
<th/>
</tr>
</thead>
<tbody>
<tr>
<td> <code>Context: travel 0.38996</code> <br/> <code> Keywords:</code> <br/> <code> driver 0.93417</code> <br/> <code> drivers 0.76932</code> <br/> <code> taxi 0.71977</code> <br/> <code> car 0.65572</code> <br/> <code> license 0.61319</code> <br/> <code> driving 0.60849</code> <br/> <code> taxis 0.57708</code> <br/> <code> traffic 0.52823</code> <br/> <code> bus 0.52306</code> <br/> <code> driver's 0.51043</code> <br/></td>
<td> <code>Context: devops 0.08917</code> <br/> <code> Keywords:</code> <br/> <code> ipam 0.78219</code> <br/> <code> driver 0.77583</code> <br/> <code> aufs 0.73758</code> <br/> <code> overlayfs 0.73758</code> <br/> <code> container_name 0.73483</code> <br/> <code> overlay2 0.69079</code> <br/> <code> cgroup 0.68438</code> <br/> <code> docker 0.67529</code> <br/> <code> compose.yml 0.65012</code> <br/> <code> compose 0.55631</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p41">
<p>Table 6.2 demonstrates a query-sense disambiguation for the query <code>driver</code>. In this case, there are two related categories, with “travel” being the most semantically related (<code>0.38996</code>) versus “devops” (<code>0.08917</code>). We can see two very distinct meanings of “driver” appear within each of these contexts, with “driver” in the “travel” category being related to “taxi”, “car”, “license”, “driving”, and “bus”, whereas within the “devops” category “driver” is related to “ipam”, “aufs”, and “overlayfs”, which are all different kinds of computer-related drivers. </p>
</div>
<div class="readable-text intended-text" id="p42">
<p>If someone searches for <code>driver</code>, they usually do not intend to find documents about both meanings of the word in the search results. There are several ways to deal with multiple potential meanings for queried keywords, such as grouping results by meaning to highlight the differences, choosing only the most likely meaning, carefully interspersing different meanings within the search results to provide diversity, or providing alternative query suggestions for different contexts. An intentional choice here is usually much better than lazily lumping multiple different meanings together. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p43">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 6.3</span> Contextualized related terms lists by category for the query <code>chef</code></h5>
<table>
<thead>
<tr>
<th>
<div>
         Query: chef 
       </div></th>
<th/>
</tr>
</thead>
<tbody>
<tr>
<td> <code>Context: cooking 0.37731</code> <br/> <code> Keywords:</code> <br/> <code> chef 0.93239</code> <br/> <code> chefs 0.5151</code> <br/> <code> www.pamperedchef.com 0.41292</code> <br/> <code> kitchen 0.39127</code> <br/> <code> restaurant 0.38975</code> <br/> <code> cooking 0.38332</code> <br/> <code> chef's 0.37392</code> <br/> <code> professional 0.36688</code> <br/> <code> nakiri 0.36599</code> <br/> <code> pampered 0.34736</code> <br/></td>
<td> <code>Context: devops 0.34959</code> <br/> <code> Keywords:</code> <br/> <code> chef 0.87653</code> <br/> <code> puppet 0.79142</code> <br/> <code> docs.chef.io 0.7865</code> <br/> <code> ansible 0.73888</code> <br/> <code> www.chef.io 0.72073</code> <br/> <code> learn.chef.io 0.71902</code> <br/> <code> default.rb 0.70194</code> <br/> <code> configuration 0.68296</code> <br/> <code> inspec 0.65237</code> <br/> <code> cookbooks 0.61503</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p44">
<p>As a final example, table 6.3 demonstrates the query disambiguation for the query <code>chef</code>. The top two contexts both show reasonably positive relatedness scores, indicating that both meanings are likely interpretations. While the “cooking” context has a slightly higher score (<code>0.37731</code>) than the “devops” context (<code>0.34959</code>), it would still be important to consider the user’s context as far as possible when choosing between these two meanings. The meaning of <code>chef</code> within the “devops” context is related to the Chef configuration management software used to build and deploy servers (related terms include “puppet” and “ansible”), whereas within the “cooking” context it refers to a person who prepares food (“cooking”, “taste”, “restaurant”, “ingredients”). The Chef software borrows inspiration from the cooking domain as a metaphor for how to prepare and serve software, so it’s not surprising to see a term like “cookbooks” appear in the “devops” category. </p>
</div>
<div class="readable-text intended-text" id="p45">
<p>The search-engine-specific SKG request used to disambiguate a query can be seen by invoking the <code>print_disambigutaion_request</code> function. This can be useful for understanding and running the internal SKG request directly against your configured search engine or vector database. The Solr-specific SKG request syntax printed for this <code>chef</code> query-sense disambiguation function call is shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p46">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.4</span> Solr SKG disambiguation request for the query <code>chef</code></h5>
<div class="code-area-container">
<pre class="code-area">print_disambigutaion_request("chef", context_limit=2)</pre>
</div>
</div>
<div class="readable-text" id="p47">
<p>Result:</p>
</div>
<div class="browsable-container listing-container" id="p48">
<div class="code-area-container">
<pre class="code-area">{"limit": 0,
 "params": {"q": "*",
            "fore": "{!${defType} v=$q}",
            "back": "*",
            "defType": "edismax",
            "f0_0_query": "chef"},<span class="aframe-location"/> #1
 "facet": {
   "f0_0": {
     "type": "query",
     "query": "{!edismax qf=body v=$f0_0_query}", #1
     "field": "body",
     "sort": {"relatedness": "desc"},
     "facet": {"relatedness": {"type": "func",
                               "func": "relatedness($fore,$back)"},
       "f1_0": {
         "type": "terms",
         "field": "category",<span class="aframe-location"/> #2
         "mincount": 5, "limit": 2,
         "sort": {"relatedness": "desc"},
         "facet": {"relatedness": {"type": "func",
                                   "func": "relatedness($fore,$back)"},
           "f2_0": {
             "type": "terms", <span class="aframe-location"/> #3
             "field": "body", #3
             "mincount": 5, "limit": 10,
             "sort": {"relatedness": "desc"},
             "facet": {"relatedness":{"type": "func",
                                      "func": "relatedness($fore,$back)"}}
}}}}}}}</pre>
<div class="code-annotations-overlay-container">
     #1 The starting node is a query for chef.
     <br/>#2 The first SKG traversal finds terms from the category field most related to the starting node. These categories are the disambiguation contexts.
     <br/>#3 The final SKG traversal finds the terms from the body field related to the disambiguation context.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p49">
<p>This is the internal Solr SKG request used for disambiguating the query <code>chef</code> with a <code>context_limit</code> of <code>2</code>. The request will be specific to whichever search engine or vector database is configured, or it will fall back on Solr if the engine does not have SKG capabilities. See appendix B for instructions on changing your configured search engine. </p>
</div>
<div class="readable-text intended-text" id="p50">
<p>By combining query classification, term disambiguation, and query expansion, an SKG can power enhanced domain-specific and highly contextualized semantic search capabilities within your AI-powered search engine. We’ll dive into using these techniques further in chapter 7 when we apply them in a live semantic search application. </p>
</div>
<div class="readable-text" id="p51">
<h2 class="readable-text-h2" id="sigil_toc_id_83"><span class="num-string">6.3</span> Learning related phrases from query signals</h2>
</div>
<div class="readable-text" id="p52">
<p>Thus far, you’ve seen how to use your content as a knowledge graph to discover related terms, classify queries, and disambiguate terms. While these techniques are powerful, they are also entirely dependent upon the quality of your documents. Throughout the rest of this chapter, we’ll explore the other major source of knowledge about your domain—user signals (queries, clicks, and subsequent actions). Often, user signals can lead to similar, if not even more useful, insights than document content for interpreting queries. </p>
</div>
<div class="readable-text intended-text" id="p53">
<p>As a starting point for learning domain-specific terminology from real user behavior, let’s consider what your query logs represent. For every query to your search engine, a query log contains an identifier for the person running the search, the query that was run, and the timestamp of the query. This means that if a single user searches for multiple terms, you can group those searches together and also know in which order the terms were entered.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>While it’s not always true, one reasonable assumption is that if someone enters two different queries within a very short timespan, the second query is likely to be either a refinement of the first query or about a related topic. Figure 6.2 demonstrates a realistic sequence of searches you might find for a single user in your query logs.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p55">
<img alt="figure" height="262" src="../Images/CH06_F02_Grainger.png" width="789"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.2</span> A typical sequence of searches from query logs for a particular user</h5>
</div>
<div class="readable-text" id="p56">
<p>When looking at these queries, we intuitively understand that <code>iphond</code> is a misspelling of <code>iphone</code>, that <code>iphone accesories</code> is a misspelling of <code>iphone accessories</code>, and that <code>iphone</code>, <code>pink phone case</code>, and <code>pink iphone case</code> are all related queries. We’ll deal with the misspellings in a later section, but we can consider those to also be related terms for now.</p>
</div>
<div class="readable-text intended-text" id="p57">
<p>While it’s not wise to depend on a single user’s signals to deduce that two queries are related, similar query patterns across many users indicate likely relationships. As we demonstrated in section 5.4.5, queries can be expanded to include related terms to improve recall. In this section, we’ll explore techniques for learning related queries, first through mining query logs and then through cross-referencing product interaction logs.</p>
</div>
<div class="readable-text" id="p58">
<h3 class="readable-text-h3" id="sigil_toc_id_84"><span class="num-string">6.3.1</span> Mining query logs for related queries</h3>
</div>
<div class="readable-text" id="p59">
<p>Before we start mining user signals for related queries, let’s first convert our signals into a simpler format for processing. The following listing provides a transformation from our generic signal structure to a simple structure that maps each occurrence of a query term to the user who searched for that term. </p>
</div>
<div class="browsable-container listing-container" id="p60">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.5</span> Mapping signals into keyword, user pairs</h5>
<div class="code-area-container">
<pre class="code-area">signals_collection = engine.get_collection("signals")
create_view_from_collection(signals_collection, "signals")<span class="aframe-location"/> #1
query = """SELECT LOWER(searches.target) AS keyword, searches.user
           FROM signals AS searches <span class="aframe-location"/> #2
           WHERE searches.type='query'"""  #2
spark.sql(query).createOrReplaceTempView("user_searches")  #2
print_keyword_user_pairs()</pre>
<div class="code-annotations-overlay-container">
     #1 Loads all documents from the signals corpus into a Spark view
     <br/>#2 Selects keyword and user data from query signals
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p61">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p62">
<div class="code-area-container">
<pre class="code-area">Number of keyword user pairs: 725459

Keyword user pairs derived from signals:
User "u10" searched for "joy stick"
User "u10" searched for "xbox"
User "u10" searched for "xbox360"</pre>
</div>
</div>
<div class="readable-text" id="p63">
<p>You can see from this listing that over 725,000 queries are represented. Our goal is to find pairs of related queries based on how many users entered both queries. The more frequently two queries co-occur across different users’ query logs, the more related those queries are presumed to be.</p>
</div>
<div class="readable-text intended-text" id="p64">
<p>The next listing shows each query pair where both queries were searched by the same user, along with the number of users that searched for both queries (<code>users_cooc</code>).</p>
</div>
<div class="browsable-container listing-container" id="p65">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.6</span> Total occurrences and co-occurrences of queries</h5>
<div class="code-area-container">
<pre class="code-area">query = """SELECT k1.keyword AS keyword1, k2.keyword AS keyword2,
           COUNT(DISTINCT k1.user) users_cooc <span class="aframe-location"/> #1
           FROM user_searches k1
           JOIN user_searches k2 ON k1.user = k2.user
          <span class="aframe-location"/> WHERE k1.keyword &gt; k2.keyword  #2
           GROUP BY k1.keyword, k2.keyword""" <span class="aframe-location"/> #3
spark.sql(query).createOrReplaceTempView("keywords_users_cooc")
query = """SELECT keyword, COUNT(DISTINCT user) users_occ FROM
           user_searches GROUP BY keyword"""
spark.sql(query).createOrReplaceTempView("keywords_users_oc")
print_keyword_cooccurrences()</pre>
<div class="code-annotations-overlay-container">
     #1 Counts the number of users that searched for both k1 and k2
     <br/>#2 Limits keyword pairs to only one permutation to avoid duplicate pairs
     <br/>#3 Joins the user_searches view with itself on the user field to find all keyword pairs searched by the same user
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p66">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p67">
<div class="code-area-container">
<pre class="code-area">+-----------+---------+
|    keyword|users_occ|
+-----------+---------+
|     lcd tv|     8449|
|       ipad|     7749|
|hp touchpad|     7144|
|  iphone 4s|     4642|
|   touchpad|     4019|
|     laptop|     3625|
|    laptops|     3435|
|      beats|     3282|
|       ipod|     3164|
| ipod touch|     2992|
+-----------+---------+

Number of co-occurring keyword searches: 244876

+-------------+---------------+----------+
|     keyword1|       keyword2|users_cooc|
+-------------+---------------+----------+
|green lantern|captain america|        23|
|    iphone 4s|         iphone|        21|
|       laptop|      hp laptop|        20|
|         thor|captain america|        18|
|         bose|          beats|        17|
|    iphone 4s|       iphone 4|        17|
|   skullcandy|          beats|        17|
|      laptops|         laptop|        16|
|      macbook|            mac|        16|
|         thor|  green lantern|        16|
+-------------+---------------+----------+</pre>
</div>
</div>
<div class="readable-text" id="p68">
<p>In listing 6.6, the first query produces the most searched-for keywords, as seen in the results. While these may be the most popular queries, they aren’t necessarily the queries that co-occur the most often with other queries. The second query produces the total number of query pairs (244,876) where both queries were searched by the same user at least once. The final query ranks these query pairs by popularity. These top query pairs are highly related.</p>
</div>
<div class="readable-text intended-text" id="p69">
<p>Notice, however, that the top result only has <code>23</code> co-occurring users, which means the number of data points is sparse and will likely include more noise further down the list. In the next section, we’ll explore a technique to combine signals along a different axis (product interactions), which can help with this sparsity problem.</p>
</div>
<div class="readable-text intended-text" id="p70">
<p>While directly aggregating the number of searches into co-occurrences by users helps find the most popular query pairs, the popularity of searches isn’t the only metric useful for finding relatedness. The keywords “and” and “of” are highly co-occurring, as are “phones”, “movies”, “computers”, and “electronics”, because they are all general words that many people search. To additionally focus on the strength of the relationship between terms independent of their individual popularity, we can use a technique called <em>pointwise mutual information</em>. </p>
</div>
<div class="readable-text intended-text" id="p71">
<p><em>Pointwise mutual information</em> (PMI) is a measure of association between any two events. In the context of natural language processing, PMI predicts the likelihood of two words occurring together because they are related versus the likelihood of them occurring together by chance. Many formulas can be used to calculate and normalize PMI, but we’ll use a variation called PMI<sup>k</sup>, where <code>k = 2</code>, which does a better job than PMI at keeping scores consistent regardless of word frequencies.</p>
</div>
<div class="readable-text intended-text" id="p72">
<p>The formula for calculating PMI<sup>2</sup> is shown in figure 6.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p73">
<img alt="figure" height="67" src="../Images/grainger-ch6-eqs-0x.png" width="385"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.3</span> PMI<sup>2</sup> score</h5>
</div>
<div class="readable-text" id="p74">
<p>In our implementation, <code>k1</code> and <code>k2</code> represent two different keywords that we want to compare. <code>P(k1,k2)</code> represents how often the same user searches for both keywords, whereas <code>P(k1)</code> and <code>P(k2)</code> represent how often a user only searches for the first keyword or second keyword, respectively. Intuitively, if the keywords appear together more often than they would be expected to, based on their likelihood of randomly appearing together, then they will have a higher PMI<sup>2</sup> score. The higher the score, the more likely the terms are to be semantically related.</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>The following listing demonstrates the PMI<sup>2</sup> calculation on our co-occurring query pairs dataset.</p>
</div>
<div class="browsable-container listing-container" id="p76">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.7</span> PMI<sup>2</sup> calculation on user searches</h5>
<div class="code-area-container">
<pre class="code-area">query = """
SELECT k1.keyword AS k1, k2.keyword AS k2, k1_k2.users_cooc,
k1.users_occ AS n_users1, k2.users_occ AS n_users2,
LOG(POW(k1_k2.users_cooc, 2) / <span class="aframe-location"/> #1
    (k1.users_occ * k2.users_occ)) AS pmi2  #1
FROM keywords_users_cooc AS k1_k2
JOIN keywords_users_oc AS k1 ON k1_k2.keyword1 = k1.keyword
JOIN keywords_users_oc AS k2 ON k1_k2.keyword2 = k2.keyword"""
spark.sql(query).createOrReplaceTempView("user_related_keywords_pmi")

spark.sql("""SELECT k1, k2, users_cooc, n_users1,
                    n_users2, ROUND(pmi2, 3) AS pmi2
             FROM user_related_keywords_pmi
             WHERE users_cooc &gt; 5 ORDER BY pmi2 DESC, k1 ASC""").show(10)</pre>
<div class="code-annotations-overlay-container">
     #1 The PMI calculation
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p77">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p78">
<div class="code-area-container">
<pre class="code-area">+-----------------+--------------------+----------+--------+--------+------+
|               k1|                  k2|users_cooc|n_users1|n_users2|  pmi2|
+-----------------+--------------------+----------+--------+--------+------+
|  iphone 4s cases|      iphone 4 cases|        10|     158|     740|-7.064|
|     sony laptops|          hp laptops|         8|     209|     432|-7.252|
|otterbox iphone 4|            otterbox|         7|     122|     787| -7.58|
|    green lantern|     captain america|        23|     963|    1091|-7.594|
|          kenwood|              alpine|        13|     584|     717|-7.815|
|      sony laptop|         dell laptop|        10|     620|     451|-7.936|
|   wireless mouse|           godfather|         6|     407|     248|-7.939|
|       hp laptops|        dell laptops|         6|     432|     269| -8.08|
|      mp3 players|        dvd recorder|         6|     334|     365|-8.128|
|          quicken|portable dvd players|         6|     281|     434|-8.128|
+-----------------+--------------------+----------+--------+--------+------+</pre>
</div>
</div>
<div class="readable-text" id="p79">
<p>The results from listing 6.7 are sorted by PMI<sup>2</sup> score, and we set a minimum occurrences threshold at <code>&gt;5</code> to help remove noise. “hp laptops”, “dell laptops”, and “sony laptops” show up as related, as well as brands like “kenwood” and “alpine”. Notably, there is also noise in the pairs, like “wireless mouse” with “godfather” and “quicken” with “portable dvd players”. One caveat of using PMI is that a small number of occurrences together across a few users can lead to noise more easily than when using co-occurrence, which is based upon the assumption of terms commonly co-occurring.</p>
</div>
<div class="readable-text intended-text" id="p80">
<p>One way to blend the benefits of both the co-occurrence model and the PMI<sup>2</sup> models is to create a composite score. This will provide a blend of popularity and likelihood of occurrence, which should move query pairs that match on both scores to the top of the list. Listing 6.8 demonstrates one way to blend these two measures together. Specifically, we take a ranked list of all co-occurrence scores (<code>r1</code>) along with a ranked list of all PMI<sup>2</sup> scores (<code>r2</code>) and blend them together to generate a composite ranking score as shown in figure 6.4.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p81">
<img alt="figure" height="73" src="../Images/grainger-ch6-eqs-1x.png" width="474"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 6.4</span> Composite ranking score combining co-occurrence and PMI<sup>2</sup> ranking</h5>
</div>
<div class="readable-text" id="p82">
<p>The <code>comp_score</code>, or composite rank score, shown in figure 6.4 assigns a high score to query pairs (query <code>q1</code> and query <code>q2</code>) where their rank in the co-occurrence list (<code>r1</code>) and their rank in the PMI<sup>2</sup> list (<code>r2</code>) is high, and it assigns a lower rank as the terms move further down in the rank lists. The result is a blended ranking that considers both the popularity (co-occurrence) and the likelihood of the relatedness of queries regardless of their popularity (PMI<sup>2</sup>). The following listing shows how to calculate the <code>comp_score</code> based on the already-calculated co-occurrence and PMI<sup>2</sup> scores. </p>
</div>
<div class="browsable-container listing-container" id="p83">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.8</span> Calculating a composite score from co-occurrence and PMI</h5>
<div class="code-area-container">
<pre class="code-area">query = """
SELECT *, (r1 + r2 / (r1 * r2)) / 2 AS comp_score <span class="aframe-location"/> #1
FROM (
  SELECT *,
  RANK() OVER (PARTITION BY 1 <span class="aframe-location"/> #2
               ORDER BY users_cooc DESC) r1, #2
  RANK() OVER (PARTITION BY 1 <span class="aframe-location"/> #3
               ORDER BY pmi2 DESC) r2  #3
  FROM user_related_keywords_pmi)"""
spark.sql(query).createOrReplaceTempView("users_related_keywords_comp_score")

spark.sql("""SELECT k1, k2, users_cooc, ROUND(pmi2, 3) as pmi2,
             r1, r2, ROUND(comp_score, 3) as comp_score
             FROM users_related_keywords_comp_score
             ORDER BY comp_score ASC, pmi2 ASC""").show(20)</pre>
<div class="code-annotations-overlay-container">
     #1 The composite score calculation combines the sorted ranks of the PMI2 score and the co-occurrences.
     <br/>#2 Ranks the co-occurrence scores from best (highest co-occurrence) to worst (lowest co-occurrence)
     <br/>#3 Ranks the PMI2 scores from best (highest PMI2) to worst (lowest PMI2 score)
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p84">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p85">
<div class="code-area-container">
<pre class="code-area">+-------------+---------------+----------+-------+---+------+----------+
|           k1|             k2|users_cooc|   pmi2| r1|    r2|comp_score|
+-------------+---------------+----------+-------+---+------+----------+
|green lantern|captain america|        23| -7.594|  1|  8626|       1.0|
|    iphone 4s|         iphone|        21|-10.217|  2| 56156|      1.25|
|       laptop|      hp laptop|        20| -9.133|  3| 20383|     1.667|
|         thor|captain america|        18| -8.483|  4| 13190|     2.125|
|    iphone 4s|       iphone 4|        17|-10.076|  5| 51964|       2.6|
|         bose|          beats|        17|-10.074|  5| 51916|       2.6|
|   skullcandy|          beats|        17| -9.001|  5| 18792|       2.6|
|      laptops|         laptop|        16|-10.792|  8| 80240|     4.063|
|      macbook|            mac|        16| -9.891|  8| 45464|     4.063|
|         thor|  green lantern|        16| -8.594|  8| 14074|     4.063|
|   headphones|   beats by dre|        15| -9.989| 11| 49046|     5.545|
|  macbook pro|        macbook|        15| -9.737| 11| 39448|     5.545|
|  macbook air|        macbook|        15| -9.443| 11| 26943|     5.545|
|   ipod touch|           ipad|        13|-11.829| 14|200871|     7.036|
|       ipad 2|           ipad|        13|-11.765| 14|196829|     7.036|
|         nook|         kindle|        13| -9.662| 14| 36232|     7.036|
|  macbook pro|    macbook air|        13| -9.207| 14| 21301|     7.036|
|      kenwood|         alpine|        13| -7.815| 14|  9502|     7.036|
| beats by dre|          beats|        12|-10.814| 19| 82811|     9.526|
|      macbook|          apple|        12|-10.466| 19| 62087|     9.526|
+-------------+---------------+----------+-------+---+------+----------+</pre>
</div>
</div>
<div class="readable-text" id="p86">
<p>Overall, the composite rank score does a reasonable job of blending our co-occurrence and PMI<sup>2</sup> metrics to overcome the limitations of each. The top results shown in listing 6.8 all look reasonable. One problem we already noted in this section, however, is that the co-occurrence numbers are very sparse. Specifically, the highest co-occurrence of any query pairs, out of over 700,000 query signals, was <code>23</code> overlapping users for “green lantern” and “captain america”, as shown in listing 6.6.</p>
</div>
<div class="readable-text intended-text" id="p87">
<p>In the next section, we’ll show a way we can overcome this sparse data problem, where there is a lack of overlap between users for specific query pairs. We’ll accomplish this by aggregating many users together into a larger group with similar behaviors. Specifically, we’ll switch our focus to the products where user queries overlap, as opposed to focusing on the individual users issuing the overlapping queries. </p>
</div>
<div class="readable-text" id="p88">
<h3 class="readable-text-h3" id="sigil_toc_id_85"><span class="num-string">6.3.2</span> Finding related queries through product interactions</h3>
</div>
<div class="readable-text" id="p89">
<p>The technique used to find related terms in section 6.3.1 depends on many users searching for overlapping queries. As we saw, with over 700,000 query signals, the highest overlap of any query pair was <code>23</code> users. Because the data can be so sparse, it can often make sense to aggregate on something other than users. </p>
</div>
<div class="readable-text intended-text" id="p90">
<p>In this section, we’ll demonstrate how we can use the same technique (using co-occurrence and PMI<sup>2</sup>) but rolling up based on product click signals instead of users. Since you’ll hopefully have many more users than products, and since particular products are likely to be clicked in response to similar keywords, this technique helps overcome the data sparsity problem and generates higher overlaps between queries.</p>
</div>
<div class="readable-text intended-text" id="p91">
<p>The transformation in listing 6.9 combines separate query and click signals into single rows with three key columns: <code>keyword</code>, <code>user</code>, and <code>product</code>.</p>
</div>
<div class="browsable-container listing-container" id="p92">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.9</span> Mapping raw signals into keyword, user, product groupings</h5>
<div class="code-area-container">
<pre class="code-area">query = """SELECT LOWER(searches.target) AS keyword, searches.user AS user,
           clicks.target AS product FROM signals AS searches
           RIGHT JOIN signals AS clicks <span class="aframe-location"/> #1
           ON searches.query_id = clicks.query_id  #1
           WHERE searches.type = 'query'  #1
           AND clicks.type = 'click'"""  #1
spark.sql(query).createOrReplaceTempView("keyword_click_product")
print_signals_format()</pre>
<div class="code-annotations-overlay-container">
     #1 Utilizes click signals to produce keyword, user, and product groupings
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p93">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p94">
<div class="code-area-container">
<pre class="code-area">Original signals format:
+-------------------+-----------+----------------+-----------+-----+-------+
|                 id|   query_id|     signal_time|     target| type|   user|
+-------------------+-----------+----------------+-----------+-----+-------+
|000001e9-2e5a-4a...|u112607_0_1|2020-04-18 16:33|        amp|query|u112607|
|00001666-1748-47...|u396779_0_1|2019-10-16 10:22|Audio stand|query|u396779|
|000029d2-197d-4a...|u466396_0_1|2020-05-07 11:39|alarm clock|query|u466396|
+-------------------+-----------+----------------+-----------+-----+-------+

Simplified signals format:
+-------------+----+------------+
|      keyword|user|     product|
+-------------+----+------------+
|    joy stick| u10|097855018120|
|         xbox| u10|885370235876|
|virgin mobile|u100|799366521679|
+-------------+----+------------+</pre>
</div>
</div>
<div class="readable-text" id="p95">
<p>Using this data, we’ll now be able to determine the strength of the relationship between any two keywords based on their use across independent users searching for the same products. Listing 6.10 generates pairs of keywords to determine their potential relationship for all keyword pairs where both keywords were used in a query for the same document. The idea behind looking for overlapping queries for each user in section 6.3.1 was that each user is likely to search for related items. Each product is also likely to be searched for by related queries, though, so we can shift our mental model from “find how many users searched for both queries” to “find how many documents were found by both queries across all users”.</p>
</div>
<div class="readable-text intended-text" id="p96">
<p>The result of this transformation in listing 6.10 now includes the following columns:</p>
</div>
<ul>
<li class="readable-text" id="p97"> <code>k1</code>, <code>k2</code>—The two keywords that are potentially related because they both resulted in a click on the same product. </li>
<li class="readable-text" id="p98"> <code>n_users1</code>—The number of users who searched for <code>k1</code> that clicked on a product that was also clicked on after a search by some user for <code>k2</code>. </li>
<li class="readable-text" id="p99"> <code>n_users2</code>—The number of users who searched for <code>k2</code> that clicked on a product that was also clicked on after a search by some user for <code>k1</code>. </li>
<li class="readable-text" id="p100"> <code>users_cooc</code>—Represents the total number of users who searched for either <code>k1</code> or <code>k2</code> and visited a product visited by other searchers for <code>k1</code> or <code>k2</code>. Calculated as <code>n_users1</code> + <code>n_users2</code>. </li>
<li class="readable-text" id="p101"> <code>n_products</code>—The number of products that were clicked on by searchers for both <code>k1</code> and <code>k2</code>. </li>
</ul>
<div class="browsable-container listing-container" id="p102">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.10</span> Keyword pairs leading to the same product being clicked</h5>
<div class="code-area-container">
<pre class="code-area">query = """
SELECT k1.keyword AS k1, k2.keyword AS k2, SUM(p1) n_users1, sum(p2) n_users2,
SUM(p1 + p2) AS users_cooc, COUNT(1) n_products FROM (
  SELECT keyword, product, COUNT(1) AS p1 FROM keyword_click_product
  GROUP BY keyword, product) AS k1 JOIN (
  SELECT keyword, product, COUNT(1) AS p2 FROM keyword_click_product
  GROUP BY keyword, product) AS k2 ON k1.product = k2.product
WHERE k1.keyword &gt; k2.keyword GROUP BY k1.keyword, k2.keyword"""
spark.sql(query).createOrReplaceTempView("keyword_click_product_cooc")
print_keyword_pair_data()</pre>
</div>
</div>
<div class="readable-text" id="p103">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p104">
<div class="code-area-container">
<pre class="code-area">Number of co-occurring queries: 1579710

+--------------+-------------+--------+--------+----------+----------+
|            k1|           k2|n_users1|n_users2|users_cooc|n_products|
+--------------+-------------+--------+--------+----------+----------+
|       laptops|       laptop|    3251|    3345|      6596|       187|
|       tablets|       tablet|    1510|    1629|      3139|       155|
|        tablet|         ipad|    1468|    7067|      8535|       146|
|       tablets|         ipad|    1359|    7048|      8407|       132|
|       cameras|       camera|     637|     688|      1325|       116|
|          ipad|        apple|    6706|    1129|      7835|       111|
|      iphone 4|       iphone|    1313|    1754|      3067|       108|
|    headphones|  head phones|    1829|     492|      2321|       106|
|        ipad 2|         ipad|    2736|    6738|      9474|        98|
|     computers|     computer|     536|     392|       928|        98|
|iphone 4 cases|iphone 4 case|     648|     810|      1458|        95|
|       netbook|       laptop|    1017|    2887|      3904|        94|
|        laptop|    computers|    2794|     349|      3143|        94|
|       netbook|      laptops|    1018|    2781|      3799|        91|
|    headphones|    headphone|    1617|     367|      1984|        90|
|        laptop|           hp|    2078|     749|      2827|        89|
|        tablet|    computers|    1124|     449|      1573|        89|
|       laptops|    computers|    2734|     331|      3065|        88|
|           mac|        apple|    1668|    1218|      2886|        88|
|     tablet pc|       tablet|     296|    1408|      1704|        87|
+--------------+-------------+--------+--------+----------+----------+</pre>
</div>
</div>
<div class="readable-text" id="p105">
<p>The <code>users_cooc</code> and <code>n_products</code> calculations are two different ways to look at overall signal quality for how confident we are that any two terms <code>k1</code> and <code>k2</code> are related. The results are currently sorted by <code>n_products</code>, and you can see that the top of the list of relationships is quite clean. These keyword pairs represent multiple kinds of meaningful semantic relationships, including the following:</p>
</div>
<ul>
<li class="readable-text" id="p106"> <em>Spelling variations</em>—“laptops” ⇒ “laptop” ; “headphones” ⇒ “head phones” ; etc. </li>
<li class="readable-text" id="p107"> <em>Brand associations</em>—“tablet” ⇒ “ipad” ; “laptop” ⇒ “hp” ; “mac” ⇒ “apple” ; etc. </li>
<li class="readable-text" id="p108"> <em>Synonyms/alternate names</em>—“netbook” ⇒ “laptop” ; “tablet pc” ⇒ “tablet” </li>
<li class="readable-text" id="p109"> <em>Category expansion</em>—“ipad” ⇒ “tablet” ; “iphone 4” ⇒ “iphone” ; “tablet” ⇒ “computers” ; “laptops” ⇒ “computers” </li>
</ul>
<div class="readable-text" id="p110">
<p>You can write custom, domain-specific algorithms to identify some of these specific types of relationships, as we’ll do for spelling variations in section 6.5.</p>
</div>
<div class="readable-text intended-text" id="p111">
<p>It’s also possible to use <code>n_users1</code> and <code>n_users2</code> to identify which of the two queries is more popular. In the case of spelling variations, we see that <code>headphones</code> is used more commonly than <code>head phones</code> (1,829 versus 492 users) and is also more common than <code>headphone</code> (1,617 versus 367 users). Likewise, we see that <code>tablet</code> is much more common in usage than <code>tablet</code> <code>pc</code> (1,408 versus 296 users).</p>
</div>
<div class="readable-text intended-text" id="p112">
<p>While our current list of keyword pairs looks clean, it only represents the keyword pairs that both occurred together in searches that led to the same products. Determining the popularity of each keyword overall will provide a better sense of which specific keywords are the most important for our knowledge graph. The following listing calculates the most popular keywords from our query signals that resulted in at least one product click.</p>
</div>
<div class="browsable-container listing-container" id="p113">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.11</span> Computing keyword searches that resulted in clicks</h5>
<div class="code-area-container">
<pre class="code-area">query = """SELECT keyword, COUNT(1) AS n_users FROM keyword_click_product
           GROUP BY keyword"""
spark.sql(query).createOrReplaceTempView("keyword_click_product_oc")
print_keyword_popularity()</pre>
</div>
</div>
<div class="readable-text" id="p114">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p115">
<div class="code-area-container">
<pre class="code-area">Keyword searches that resulted in clicks: 13744

+------------+-------+
|     keyword|n_users|
+------------+-------+
|        ipad|   7554|
| hp touchpad|   4829|
|      lcd tv|   4606|
|   iphone 4s|   4585|
|      laptop|   3554|
|       beats|   3498|
|     laptops|   3369|
|        ipod|   2949|
|  ipod touch|   2931|
|      ipad 2|   2842|
|      kindle|   2833|
|    touchpad|   2785|
|   star wars|   2564|
|      iphone|   2430|
|beats by dre|   2328|
|     macbook|   2313|
|  headphones|   2270|
|        bose|   2071|
|         ps3|   2041|
|         mac|   1851|
+------------+-------+</pre>
</div>
</div>
<div class="readable-text" id="p116">
<p>This list is identical to the list from listing 6.6, but instead of showing the number of users who searched for a keyword, this list shows the number of users who searched for a keyword and also clicked on a product. We’ll use this as our master list of queries for the PMI<sup>2</sup> calculation.</p>
</div>
<div class="readable-text intended-text" id="p117">
<p>With our query pairs and query popularity now based on queries and product interactions, the rest of our calculations (PMI<sup>2</sup> and composite score) are the same as in section 6.3.1, so we’ll omit them here (they are included in the notebooks for you to run). After calculating the PMI<sup>2</sup> and composite scores, the following listing shows the final results of our product-interaction-based related terms calculations.</p>
</div>
<div class="browsable-container listing-container" id="p118">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.12</span> Related terms scoring based on product interactions</h5>
<div class="code-area-container">
<pre class="code-area">query = """SELECT k1, k2, n_users1, n_users2, ROUND(pmi2, 3) AS pmi2,
           ROUND(comp_score, 3) AS comp_score
           FROM product_related_keywords_comp_score
           ORDER BY comp_score ASC"""
dataframe = spark.sql(query)
print("Number of co-occurring queries:", dataframe.count(), "\n")
dataframe.show(20)</pre>
</div>
</div>
<div class="readable-text" id="p119">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p120">
<div class="code-area-container">
<pre class="code-area">Number of co-occurring queries: 1579710

+----------+-----------+--------+--------+-----+----------+
|        k1|         k2|n_users1|n_users2| pmi2|comp_score|
+----------+-----------+--------+--------+-----+----------+
|      ipad|hp touchpad|    7554|    4829|1.232|       1.0|
|    ipad 2|       ipad|    2842|    7554|1.431|      1.25|
|    tablet|       ipad|    1818|    7554|1.669|     1.667|
|  touchpad|       ipad|    2785|    7554|1.223|     2.125|
|   tablets|       ipad|    1627|    7554|1.749|       2.6|
|     ipad2|       ipad|    1254|    7554|1.903|     3.083|
|      ipad|      apple|    7554|    1814|  1.5|     3.571|
|  touchpad|hp touchpad|    2785|    4829|1.394|     4.063|
|      ipad|  hp tablet|    7554|    1421|1.594|     4.556|
|ipod touch|       ipad|    2931|    7554|0.863|      5.05|
|      ipad|      i pad|    7554|     612|2.415|     5.545|
|    kindle|       ipad|    2833|    7554|0.828|     6.042|
|    laptop|       ipad|    3554|    7554|0.593|     6.538|
|      ipad| apple ipad|    7554|     326|2.916|     7.036|
|    ipad 2|hp touchpad|    2842|    4829|1.181|     7.533|
|   laptops|     laptop|    3369|    3554| 1.29|     8.031|
|      ipad|         hp|    7554|    1125|1.534|     8.529|
|     ipads|       ipad|     254|    7554|3.015|     9.028|
|      ipad|  htc flyer|    7554|    1834|1.016|     9.526|
|      ipad|    i pad 2|    7554|     204| 3.18|    10.025|
+----------+-----------+--------+--------+-----+----------+</pre>
</div>
</div>
<div class="readable-text" id="p121">
<p>The results of listings 6.11 and 6.12 show the benefit of aggregating at a less granular level. By looking at all queries that led to a particular product being clicked on, the list of query pairs is now much larger than in section 6.3.1, where query pairs were aggregated by individual users. You can see that there are now 1,579,710 query pairs under consideration versus 244,876 (per listing 6.6) when aggregating by user.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>Further, you can see that the related queries include more fine-grained variations for top queries (<code>ipad</code>, <code>ipad 2</code>, <code>ipad2</code>, <code>i pad</code>, <code>ipads</code>, <code>i pad 2</code>). Having more granular variations like this will come in handy if you are combining this related term discovery with other algorithms, like misspelling detection, which we’ll cover in section 6.5.</p>
</div>
<div class="readable-text intended-text" id="p123">
<p>Between the SKG approach in the last chapter and query log mining in this chapter, you’ve now seen multiple techniques for discovering related phrases. Before we can apply the related phrases, however, we first need to be able to identify such known phrases in incoming queries. In the next section, we’ll cover how we can generate a list of known phrases from our query signals. </p>
</div>
<div class="readable-text" id="p124">
<h2 class="readable-text-h2" id="sigil_toc_id_86"><span class="num-string">6.4</span> Phrase detection from user signals</h2>
</div>
<div class="readable-text" id="p125">
<p>In section 5.3, we discussed several techniques for extracting arbitrary phrases and relationships from documents. While this can go a long way toward discovering all the relevant domain-specific phrases within your content, this approach suffers from two different problems:</p>
</div>
<ul>
<li class="readable-text" id="p126"> <em>It generates a lot of noise</em>—Not every noun phrase across your potentially massive set of documents is important, and the odds of identifying incorrect phrases (false positives) increase as your number of documents increases. </li>
<li class="readable-text" id="p127"> <em>It ignores what your users care about</em>—The real measure of user interest is communicated by what they search for. They may only be interested in a subset of your content or may be looking for things that aren’t even represented well within your content. </li>
</ul>
<div class="readable-text" id="p128">
<p>In this section, we’ll focus on how to identify important domain-specific phrases from your user signals.</p>
</div>
<div class="readable-text" id="p129">
<h3 class="readable-text-h3" id="sigil_toc_id_87"><span class="num-string">6.4.1</span> Treating queries as entities</h3>
</div>
<div class="readable-text" id="p130">
<p>The easiest way to extract entities from query logs is to treat the entire query as one entity. In use cases like our RetroTech e-commerce site, this works very well, as many of the queries are product names, categories, brand names, company names, or people’s names (actors, musicians, etc.). Given that context, most of the high-popularity queries end up being entities that can be used directly as phrases without needing any special parsing. </p>
</div>
<div class="readable-text intended-text" id="p131">
<p>Looking back at the output of listing 6.11, you’ll find the following most popular queries:</p>
</div>
<div class="browsable-container listing-container" id="p132">
<div class="code-area-container">
<pre class="code-area">+------------+-------+
|     keyword|n_users|
+------------+-------+
|        ipad|   7554|
| hp touchpad|   4829|
|      lcd tv|   4606|
|   iphone 4s|   4585|
|      laptop|   3554|
|        ... |   ... |
+------------+-------+</pre>
</div>
</div>
<div class="readable-text" id="p133">
<p>These are entities that belong in a known-entities list, with many of them being multiword phrases. In this case, the simplest method for extracting entities is also the most powerful—just use the queries as your entities list. The higher the frequency of each query across users, the more confident you can be about adding it to your entities list.</p>
</div>
<div class="readable-text intended-text" id="p134">
<p>One way to reduce potential false positives from noisy queries is to find phrases that overlap in both your documents and queries. Additionally, if you have different fields in your documents, like a product name or company, you can cross-reference your queries with those fields to assign a type to the entities found within your queries.</p>
</div>
<div class="readable-text intended-text" id="p135">
<p>Depending on the complexity of your queries, using the most common searches as your key entities may be the most straightforward way to achieve a high-quality entities list. </p>
</div>
<div class="readable-text" id="p136">
<h3 class="readable-text-h3" id="sigil_toc_id_88"><span class="num-string">6.4.2</span> Extracting entities from more complex queries</h3>
</div>
<div class="readable-text" id="p137">
<p>In some use cases, the queries may contain more noise (Boolean structure, advanced query operators, etc.) and therefore may not be directly usable as entities. In those cases, the best approach to extracting entities may be to reapply the entity extraction strategies from chapter 5, but on your query signals. </p>
</div>
<div class="readable-text intended-text" id="p138">
<p>Out of the box, a lexical search engine parses queries as individual keywords and looks them up in the inverted index. For example, a query for <code>new york city</code> will be automatically interpreted as the Boolean query <code>new AND york AND city</code> (or if you set the default operator to <code>OR</code>, then <code>new OR york OR city</code>). The relevance ranking algorithms will then score each keyword individually instead of understanding that certain words combine to make phrases that then take on a different meaning.</p>
</div>
<div class="readable-text intended-text" id="p139">
<p>Being able to identify and extract domain-specific phrases from queries can enable more accurate query interpretation and relevance. We already demonstrated one way to extract domain-specific phrases from documents in section 5.3, using the spaCy NLP library to do a dependency parse and extract out noun phrases. While queries are often too short to perform a true dependency parse, it’s still possible to apply some part of speech filtering on any discovered phrases in queries to omit non-noun phrases. If you need to split sections of queries apart, you can also tokenize the queries and remove query syntax (<code>AND</code>, <code>OR</code>, etc.) before looking for phrases to extract. Handling the specific query patterns for your application may require some domain-specific query parsing logic, but if your queries are largely single phrases or easily tokenizable into multiple phrases, your queries likely represent the best source of domain-specific phrases to extract and add to your knowledge graph. We’ll walk through code examples of phrase identification when parsing queries in section 7.4. </p>
</div>
<div class="readable-text" id="p140">
<h2 class="readable-text-h2" id="sigil_toc_id_89"><span class="num-string">6.5</span> Misspellings and alternative representations</h2>
</div>
<div class="readable-text" id="p141">
<p>We’ve covered detecting domain-specific phrases and finding related phrases, but there are two very important subcategories of related phrases that typically require special handling: misspellings and alternative spellings (also known as <em>alternative labels</em>). When entering queries, users will commonly misspell their keywords, and the general expectation is that an AI-powered search system will be able to understand and properly handle those misspellings. </p>
</div>
<div class="readable-text intended-text" id="p142">
<p>While general related phrases for “laptop” might be “computer”, “netbook”, or “tablet”, misspellings would look more like “latop”, “laptok”, or “lapptop”. <em>Alternative labels</em> are functionally no different than misspellings but occur when multiple valid variations for a phrase exist (such as “specialized” versus “specialised” or “cybersecurity” versus “cyber security”). In the case of both misspellings and alternative labels, the end goal is usually to normalize the less common variant into the more common, canonical form and then search for the canonical version. </p>
</div>
<div class="readable-text intended-text" id="p143">
<p>Spell-checking can be implemented in multiple ways. In this section, we’ll cover the out-of-the-box document-based spell-checking that is found in most search engines, and we’ll also show how user signals can be mined to fine-tune spelling corrections based upon real user interactions with your search engine.</p>
</div>
<div class="readable-text" id="p144">
<h3 class="readable-text-h3" id="sigil_toc_id_90"><span class="num-string">6.5.1</span> Learning spelling corrections from documents</h3>
</div>
<div class="readable-text" id="p145">
<p>Most search engines contain some spell-checking capabilities out of the box, based on the terms found within a collection’s documents. Apache Solr, for example, provides file-based, dictionary-based, and index-based spell-checking components. The file-based spell-checker requires assembling a list of terms that can be spell-corrected. The dictionary-based spell-checker can build a list of terms to be spell-corrected from fields in an index. The index-based spell-checker can use a field on the main index to spell-check directly without having to build a separate spell-checking index. Additionally, if someone has built a list of spelling corrections offline, you can use a synonym list to directly replace or expand any misspellings to their canonical form. </p>
</div>
<div class="readable-text intended-text" id="p146">
<p>Elasticsearch and OpenSearch have similar spellchecking capabilities, even allowing specific contexts to refine the scope of the spelling suggestions to a particular category or geographical location.</p>
</div>
<div class="readable-text intended-text" id="p147">
<p>While we encourage you to test out these out-of-the-box spell-checking algorithms, they all unfortunately suffer from a major problem: lack of user context. Specifically, anytime a keyword is searched that doesn’t appear a minimum number of times in the index, the spell-checking component begins looking at all terms in the index that are <em>off by the minimum number of characters</em>, and they then return the most prevalent keywords in the index that match the criteria. The following listing shows an example of where out-of-the-box index-based spell-checking configuration falls short.</p>
</div>
<div class="browsable-container listing-container" id="p148">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.13</span> Using out-of-the-box spelling corrections on documents</h5>
<div class="code-area-container">
<pre class="code-area">products_collection = engine.get_collection("products")
query = "moden"
results = engine.spell_check(products_collection, query)
print(results)</pre>
</div>
</div>
<div class="readable-text" id="p149">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p150">
<div class="code-area-container">
<pre class="code-area">{'modes': 421, 'model': 159, 'modern': 139, 'modem': 56, 'mode6': 9}</pre>
</div>
</div>
<div class="readable-text" id="p151">
<p>In listing 6.13, you can see a user query for <code>moden</code>. The spell-checker returns the suggested spelling corrections of “modes”, “model”, “modern”, and “modem”, plus one suggestion that only appears in a few documents, which we’ll ignore. Since our collection is tech products, it may be obvious which of these is likely the best spelling correction: it’s “modem”. In fact, it is unlikely that a user would intentionally search for “modes” or “model” as standalone queries, as those are both generic terms that would usually only make sense within a context containing other words.</p>
</div>
<div class="readable-text intended-text" id="p152">
<p>The content-based index has no way to distinguish easily that end users would be unlikely to search for “modern” or “model”. Thus, while content-based spell-checkers can work well in many cases, it is often more accurate to learn spelling corrections from users’ query behavior. </p>
</div>
<div class="readable-text" id="p153">
<h3 class="readable-text-h3" id="sigil_toc_id_91"><span class="num-string">6.5.2</span> Learning spelling corrections from user signals</h3>
</div>
<div class="readable-text" id="p154">
<p>Returning to our core thesis from section 6.3 that users tend to search for related queries until they find the expected results, it follows that a user who misspelled a particular query and received bad results would then try to correct their query. </p>
</div>
<div class="readable-text intended-text" id="p155">
<p>We already know how to find related phrases (discussed in section 6.3), but in this section we’ll cover how to specifically distinguish a misspelling based on user signals. This task largely comes down to two goals:</p>
</div>
<ol>
<li class="readable-text" id="p156"> Find terms with similar spellings. </li>
<li class="readable-text" id="p157"> Figure out which term is the correct spelling versus the misspelled variant. </li>
</ol>
<div class="readable-text" id="p158">
<p>For this task, we’ll rely solely on query signals. We’ll perform some up-front normalization to make the query analysis case-insensitive and filter duplicate queries to avoid signal spam. (We’ll discuss signal normalization in sections 8.2–8.3.) The following listing shows a query that grabs our normalized query signals.</p>
</div>
<div class="browsable-container listing-container" id="p159">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.14</span> Getting all queries searched by users</h5>
<div class="code-area-container">
<pre class="code-area">def get_search_queries():
  query = """SELECT searches.user AS user,
             LOWER(TRIM(searches.target)) As query <span class="aframe-location"/> #1
             FROM signals AS searches WHERE searches.type = 'query'
             GROUP BY searches.target, user""" <span class="aframe-location"/> #2
  return spark.sql(query).collect()</pre>
<div class="code-annotations-overlay-container">
     #1 Lowercasing the queries makes the query analysis ignore uppercase vs. lowercased variants.
     <br/>#2 Grouping by user prevents spam from a single user entering the same query many times.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p160">
<p>For the purposes of this section, we’re going to assume that the queries can contain multiple different keywords and that we want to treat each of these keywords as a potential spelling variant. This will allow individual terms to be found and substituted within a future query, as opposed to treating the entire query as a single phrase. It will also allow us to throw out certain terms that are likely to be noise, such as stop words or standalone numbers.</p>
</div>
<div class="readable-text intended-text" id="p161">
<p>The following listing demonstrates the process of tokenizing each query to generate a word list upon which we can do further analysis.</p>
</div>
<div class="browsable-container listing-container" id="p162">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.15</span> Finding words by tokenizing and filtering query terms</h5>
<div class="code-area-container">
<pre class="code-area">from nltk import tokenize, corpus<span class="aframe-location"/>, download  #1
download('stopwords') #1
stop_words = set( #1
  corpus.stopwords.words("english")) #1

def is_term_valid(term, minimum_length=4):<span class="aframe-location"/> #2
  return (term not in stop_words and
          len(term) &gt;= minimum_length and
          not term.isdigit())

def tokenize_query(query): <span class="aframe-location"/> #3
  return tokenize.RegexpTokenizer(r'\w+').tokenize(query) #3

def valid_keyword_occurrences(searches, tokenize=True):
  word_list = defaultdict(int)
  for search in searches:
    query = search["query"]
    terms = tokenize_query(query) if tokenize else [query]  #3
    for term in terms: <span class="aframe-location"/> #4
      if is_term_valid(term):  #4
        word_list[term] += 1  #4
  return word_list</pre>
<div class="code-annotations-overlay-container">
     #1 Defines stop words that shouldn’t be considered as misspellings or corrections utilizing the Natural Language Toolkit (nltk) 
     <br/>#2 Removes noisy terms including stop words, very short terms, and numbers
     <br/>#3 Splits the query on whitespace into individual terms if tokenizing
     <br/>#4 Aggregates the occurrences of valid keywords
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p163">
<p>Once the list of tokens has been cleaned up, the next step is to distinguish high-occurrence tokens from infrequently occurring tokens. Since misspellings will occur relatively infrequently and correct spellings will occur more frequently, we will use the relative number of occurrences to determine which version is most likely the canonical spelling and which variations are the misspellings.</p>
</div>
<div class="readable-text intended-text" id="p164">
<p>To ensure our spell correction list is as clean as possible, we’ll set some thresholds for popular terms and some for low-occurrence terms that are more likely misspellings. Because some collections may contain hundreds of documents and other collections could contain millions, we can’t just look at an absolute number for these thresholds, so we’ll use quantiles instead. The following listing shows the calculations for each of the quantiles between <code>0.1</code> and <code>0.9</code>.</p>
</div>
<div class="browsable-container listing-container" id="p165">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.16</span> Calculating quantiles to identify spelling candidates</h5>
<div class="code-area-container">
<pre class="code-area">def calculate_quantiles(word_list):
  quantiles_to_check = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
  quantile_values = numpy.quantile(numpy.array(list(word_list.values())),
                                   quantiles_to_check)
  return dict(zip(quantiles_to_check, quantile_values))

query_signals = get_search_queries()
word_list = valid_keyword_occurrences(query_signals, tokenize=True)
quantiles = calculate_quantiles(word_list)
display(quantiles)</pre>
</div>
</div>
<div class="readable-text" id="p166">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p167">
<div class="code-area-container">
<pre class="code-area">{0.1: 5.0,
 0.2: 6.0,
 0.3: 8.0,
 0.4: 12.0,
 0.5: 16.0,
 0.6: 25.0,
 0.7: 47.0,
 0.8: 142.20000000000027,
 0.9: 333.2000000000007}</pre>
</div>
</div>
<div class="readable-text" id="p168">
<p>Here we see that 80% of the terms are searched for <code>142.2</code> times or less. Likewise, only 20% of terms are searched for <code>6.0</code> times or less. Using the Pareto principle, let’s assume that most of our misspellings fall within the bottom-searched 20% of our terms and that the majority of our most important terms fall within the top 20% of queries searched. If you want higher precision (only generate spelling corrections for high-value terms and only if there’s a low probability of false positives), you can push these to the <code>0.1</code> quantile for misspellings and the <code>0.9</code> quantile for correctly spelled terms. You can also go the other direction to attempt to generate a larger misspelling list with a higher chance of false positives.</p>
</div>
<div class="readable-text intended-text" id="p169">
<p>In Listing 6.17, we’ll divide the terms into buckets, assigning low-frequency terms to the <code>misspellings</code> bucket and high-frequency terms to the <code>corrections</code> bucket. These buckets will be a starting point for finding high-quality spelling corrections when enough users search for both the misspelling candidate and the correction candidate.</p>
</div>
<div class="browsable-container listing-container" id="p170">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.17</span> Identifying spelling correction candidates</h5>
<div class="code-area-container">
<pre class="code-area">def create_spelling_candidates(word_list):
  quantiles = calculate_quantiles(word_list)
  misspellings = {"misspelling": [],
                  "misspell_counts": [],
                  "misspell_length": [],
                  "initial": []}
  corrections = {"correction": [],
                 "correction_counts": [],
                 "correction_length": [],
                 "initial": []}
  for key, value in word_list.items():
    if value &lt;= quantiles[0.2]: <span class="aframe-location"/> #1
      misspellings["misspelling"].append(key) #1
      misspellings["misspell_counts"].append(value) <span class="aframe-location"/> #2
      misspellings["misspell_length"].append(len(key)) <span class="aframe-location"/> #3
     <span class="aframe-location"/> misspellings["initial"].append(key[0])  #4
    if value &gt;= quantiles[0.8]:<span class="aframe-location"/> #5
      corrections["correction"].append(key) #5
      corrections["correction_counts"].append(value) #5
      corrections["correction_length"].append(len(key)) #5
      corrections["initial"].append(key[0])  #5
  return (pandas.DataFrame(misspellings), pandas.DataFrame(corrections))</pre>
<div class="code-annotations-overlay-container">
     #1 Terms at or below the 0.2 quantile are added to the misspellings list.
     <br/>#2 The number of searches is retained to keep track of popularity.
     <br/>#3 The length of the term will be used later to set thresholds for edit distance calculations.
     <br/>#4 The first letter of the term is stored to limit the scope of the misspellings checked.
     <br/>#5 The top 20% of terms have the same data stored but in the corrections list.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p171">
<p>To efficiently compare all of the <code>misspellings</code> and the <code>corrections</code> values, we first load them into dataframes in listing 6.17. You can imagine that <code>corrections</code> is a pristine list of the most popular searched terms, while the <code>misspellings</code> list should provide a good candidate list for less commonly searched terms that are more likely to be misspellings.</p>
</div>
<div class="readable-text intended-text" id="p172">
<p>When we compare misspelled candidates with correctly spelled candidates and decide how many character differences (or <em>edit distances</em>) are allowed, we need to consider the term length. The following listing shows a simple <code>good_match</code> function, which defines a general heuristic for how many edit distances a term match can be off by while still considering the misspelling a likely permutation of the correction candidate. </p>
</div>
<div class="browsable-container listing-container" id="p173">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.18</span> Finding proper spellings by lengths and edit distance</h5>
<div class="code-area-container">
<pre class="code-area">def good_match(word_length_1, word_length_2, edit_dist):
  min_length = min(word_length_1, word_length_2)
  return ((min_length &lt; 8 and edit_dist == 1) or
          (min_length &gt;= 8 and min_length &lt; 11 and edit_dist &lt;= 2) or
          (min_length &gt;= 11 and edit_dist == 3))</pre>
</div>
</div>
<div class="readable-text" id="p174">
<p>With our <code>misspellings</code> and <code>corrections</code> candidates loaded into dataframes and the <code>good_match</code> function defined, it’s time to generate our spelling correction list. Just like in section 6.5.1, where spelling corrections were generated from edit distances and counts of term occurrences within our collection of documents, listing 6.19 generates spelling corrections based on edit distances and term occurrences within our query logs.</p>
</div>
<div class="browsable-container listing-container" id="p175">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.19</span> Mapping misspellings to their correct spellings</h5>
<div class="code-area-container">
<pre class="code-area">from nltk import edit_distance

def calculate_spelling_corrections(word_list):
  (misspellings, corrections) = create_spelling_candidates(word_list)
  matches_candidates = pandas.merge(misspellings, <span class="aframe-location"/> #1
                       corrections, on="initial")  #1
  matches_candidates["edit_dist"] = matches_candidates.apply(
    lambda row: edit_distance(row.misspelling, <span class="aframe-location"/> #2
                      row.correction), axis=1)  #2
  matches_candidates["good_match"] = matches_candidates.apply(
    lambda row: good_match(row.misspell_length,<span class="aframe-location"/> #3
                           row.correction_length,  #3
                           row.edit_dist),axis=1)  #3
  cols = ["misspelling", "correction", "misspell_counts",
          "correction_counts", "edit_dist"]
  matches = matches_candidates[matches_candidates["good_match"]] \
    .drop(["initial", "good_match"],axis=1) \
    .groupby("misspelling").first().reset_index() \ <span class="aframe-location"/> #4
    .sort_values(by=["correction_counts", "misspelling",  #4
                     "misspell_counts"],  #4
                 ascending=[False, True, False])[cols] #4
  return matches

query_signals = get_search_queries()
word_list = valid_keyword_occurrences(query_signals, tokenize=True)
corrections = calculate_spelling_corrections(word_list)
display(corrections.head(20))<span class="aframe-location"/> #5</pre>
<div class="code-annotations-overlay-container">
     #1 Groups the misspelling and correction candidates on the first letter of the word
     <br/>#2 Calculates the edit distance between each misspelling and correction candidate
     <br/>#3 Applies the good_match function using the lengths of the terms and the edit distance
     <br/>#4 Aggregates all the misspellings by name
     <br/>#5 Gets the 20 most misspelled words
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p176">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p177">
<div class="code-area-container">
<pre class="code-area">misspelling   correction  misspell_counts correction_counts edit_dist
50   iphone3       iphone      6               16854             1
61   laptopa       laptop      6               14119             1
62   latop         laptop      5               14119             1
...
76   moden         modem       5               3590              1
77   modum         modem       6               3590              1
135  tosheba       toshiba     6               3432              1
34   gates         games       6               3239              1
84   phono         phone       5               3065              1</pre>
</div>
</div>
<div class="readable-text" id="p178">
<p>As you can see, we now have a relatively clean list of spelling corrections based on user signals. Our query of <code>moden</code> maps correctly to “modem”, as opposed to unlikely search terms like “model” and “modern”, which we saw in the document-based spelling correction in listing 6.13.</p>
</div>
<div class="readable-text intended-text" id="p179">
<p>There are numerous other ways that you could go about creating a spelling correction model. If you wanted to generate multiterm spelling corrections from documents, you could generate bigrams and trigrams to perform chained Bayesian analysis on probabilities of consecutive terms occurring. Likewise, to generate multiterm spelling corrections from query signals, you could remove the tokenization of queries by setting <code>tokenize</code> to <code>False</code> when calling <code>valid_keyword_occurrences</code>.</p>
</div>
<div class="browsable-container listing-container" id="p180">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 6.20</span> Finding multiterm spell corrections from full queries</h5>
<div class="code-area-container">
<pre class="code-area">query_signals = get_search_queries()
word_list = valid_keyword_occurrences(query_signals, tokenize=False)
corrections = calculate_spelling_corrections(word_list)
display(corrections.head(20))</pre>
</div>
</div>
<div class="readable-text" id="p181">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p182">
<div class="code-area-container">
<pre class="code-area">misspelling    correction   misspell_counts  correction_counts edit_dist
181 ipad.          ipad         6                7749              1
154 hp touchpad 32 hp touchpad  5                7144              3
155 hp toucpad     hp touchpad  6                7144              1
153 hp tochpad     hp touchpad  6                7144              1
190 iphone s4      iphone 4s    5                4642              2
193 iphone4 s      iphone 4s    5                4642              2
194 iphones 4s     iphone 4s    5                4642              1
412 touchpaf       touchpad     5                4019              1
406 tochpad        touchpad     6                4019              1
407 toichpad       touchpad     6                4019              1
229 latop          laptop       5                3625              1
228 laptopa        laptops      6                3435              1
237 loptops        laptops      5                3435              1
205 ipods touch    ipod touch   6                2992              1
204 ipod tuch      ipod touch   6                2992              1
165 i pod tuch     ipod touch   5                2992              2
173 ipad 2         ipad 2       6                2807              1
215 kimdle         kindle       5                2716              1
206 ipone          iphone       6                2599              1
192 iphone3        iphone       6                2599              1</pre>
</div>
</div>
<div class="readable-text" id="p183">
<p>You can see some of the common multiword misspellings and their corrections in listing 6.20, now that the queries are no longer being tokenized. Note that the single-term words are largely the same, but multiword queries have also been spell-checked. This is a great way to normalize product names, so that “iphone4 s”, “iphones 4s”, and “iphone s4” are all correctly mapped to the canonical “iphone 4s”. Note that in some cases this can be a lossy process, as “hp touchpad 32” maps to “hp touchpad”, and “iphone3” maps to “iphone”. Depending on your use case, you may find it beneficial to only spell-correct individual terms, or to include special handling in your <code>good_match</code> function for brand variations to ensure the spell-check code doesn’t mistakenly delete relevant query context. </p>
</div>
<div class="readable-text" id="p184">
<h2 class="readable-text-h2" id="sigil_toc_id_92"><span class="num-string">6.6</span> Pulling it all together</h2>
</div>
<div class="readable-text" id="p185">
<p>In this chapter, we dove deeper into understanding the context and meaning of domain-specific language. We showed how to use SKGs to classify queries and disambiguate terms that have different or nuanced meanings based on their context. We also explored how to mine relationships from user signals, which usually provides a better context for understanding your users than looking at your documents alone. We also showed how to extract phrases, misspellings, and alternative labels from query signals, enabling domain-specific terminology to be learned directly from users as opposed to only from documents.</p>
</div>
<div class="readable-text intended-text" id="p186">
<p>At this point, you should feel confident about learning domain-specific phrases and related phrases from documents or user signals, classifying queries to your available content, and disambiguating the meaning of terminology based on the query classification. These techniques are critical tools in your toolbox for interpreting query intent.</p>
</div>
<div class="readable-text intended-text" id="p187">
<p>Our goal isn’t just to assemble a large toolbox, though. Our goal is to use each of these tools where appropriate to build an end-to-end semantic search layer. This means we need to model known phrases into our knowledge graph, extract those phrases from incoming queries, handle misspellings, classify queries, disambiguate incoming terms, and ultimately generate a rewritten query for the search engine that uses each of our AI-powered search techniques. In the next chapter, we’ll show you how to assemble each of these techniques into a working semantic search system designed to best interpret and model query intent.</p>
</div>
<div class="readable-text" id="p188">
<h2 class="readable-text-h2" id="sigil_toc_id_93">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p189"> Classifying queries using a semantic knowledge graph (SKG) can help interpret query intent and improve query routing and filtering.  </li>
<li class="readable-text" id="p190"> Query-sense disambiguation can deliver a more contextual understanding of a user’s query, particularly for terms with significantly divergent meanings across different contexts. </li>
<li class="readable-text" id="p191"> In addition to learning from documents, domain-specific phrases and related phrases can also be learned from user signals. </li>
<li class="readable-text" id="p192"> Misspellings and spelling variations can be learned from both documents and user signals, with document-based approaches being more robust and user-signal-based approaches better representing user intent. </li>
</ul>
</div></body></html>