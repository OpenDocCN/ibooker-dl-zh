["```py\n#Importing the AsyncHtmlLoader\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n\n#This is the URL of the Wikipedia page on the 2023 Cricket World Cup\nurl=\"https://en.wikipedia.org/wiki/2023_Cricket_World_Cup\"\n\n#Instantiating the AsyncHtmlLoader\nloader = AsyncHtmlLoader (url)\n\n#Loading the extracted information\nhtml_data = loader.load()\n\nfrom langchain_community.document_transformers import Html2TextTransformer\n\n#Instantiate the Html2TextTransformer function\nhtml2text = Html2TextTransformer()\n\n#Call transform_documents\nhtml_data_transformed = html2text.transform_documents(html_data)\n```", "```py\n%pip install ragas== 0.2.13\n\n# Import necessary libraries\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom ragas.testset import TestsetGenerator\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# Instantiate the models\ngenerator_llm = \nLangchainLLMWrapper(\nChatOpenAI(model=\"gpt-4o-mini\")\n)\n\ngenerator_embeddings = \nLangchainEmbeddingsWrapper(\nOpenAIEmbeddings(model=\"text-embedding-3-small\")\n)\n\n# Create the TestsetGenerator\ngenerator = \nTestsetGenerator(\nllm=generator_llm, \nembedding_model=generator_embeddings\n)\n\n# Call the generator\ntestset = \ngenerator.generate_with_langchain_docs\n(\n          \thtml_data_transformed, \ntest_size=20, \n)\n```", "```py\n# Import FAISS class from vectorstore library\nfrom langchain_community.vectorstores import FAISS\n# Import OpenAIEmbeddings & ChatOpenAI from the library\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\n\ndef rag_function(query, db_path, index_name):    \n\n# Instantiate the embeddings object\n\nembeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# Load the database stored in the local directory\n\ndb=FAISS.load_local(\nfolder_path=db_path, \nindex_name=index_name, \nembeddings=embeddings,\nallow_dangerous_deserialization=True\n)\n\n# Ranking the chunks in descending order of similarity and selecting the top 2 queries\n\nretrieved_docs = db.similarity_search(query, k=2)\n\n# Keeping text of top 2 retrieved chunks\n\nretrieved_context=[ retrieved_docs[0].page_content \n+retrieved_docs[1].page_content]\n\n# Creating the prompt\n\naugmented_prompt=f\"\"\"\n\nGiven the context below, answer the question.\n\nQuestion: {query} \n\nContext : {retrieved_context}\n\nRemember to answer only based on the context \nprovided and not from any other source. \n\nIf the question cannot be answered based \non the provided context, say I don't know.\n\n\"\"\"\n\n# Instantiate the LLM\nllm = ChatOpenAI(\nmodel=\"gpt-4o-mini\",\ntemperature=0,\nmax_tokens=None,\ntimeout=None,\nmax_retries=2\n)\n\n# Create message to send to the LLM\n\nmessages=[(\"human\",augmented_prompt)]\n\n# Make the API call passing the message to the LLM\n\nresponse = llm.invoke(messages)\n\n# Extract the answer from the response object\n\nanswer=response.content\n\nreturn retrieved_context, answer\n```", "```py\n# Location of the stored vector index created by the indexing pipeline\ndb_path='../../Assets/Data'\n\n# User Question\nquery=\"Who won the 2023 cricket world cup?\"\n\n# Index Name\nindex_name=\"CWC_index\"\n\n# Calling the RAG function\nrag_function(query, db_path, index_name)\n```", "```py\n# Create Lists for Questions and Ground Truths from testset\nsample_queries = \ndataset.to_pandas()['user_input'].to_list()\n\nexpected_responses=\ndataset.to_pandas()['reference'].to_list()\n\n# Iterate through the testset to generate responses to questions\n\ndataset_to_eval=[]\n\nfor query, reference in zip(sample_queries,expected_responses):\n\n# Call the RAG function\nrag_context, rag_answer=rag_function(query,db_path,index_name)\n\n# Create a dictionary of question, answer, context, and ground truth\ndataset_to_eval.append(\n            {\n                    \"user_input\":query,\n                    \"retrieved_contexts\":relevant_docs,\n                    \"response\":response,\n                    \"reference\":reference\n            }\n                ) \n```", "```py\n# Import the EvaluationDataset library\nfrom ragas import EvaluationDataset\n\nevaluation_dataset = EvaluationDataset.from_list(dataset_to_eval)\n```", "```py\n#Import all the libraries\nfrom ragas import evaluate\n\nfrom ragas.metrics import (\n        LLMContextRecall, \nFaithfulness, \nFactualCorrectness, \nAnswerCorrectness, \nResponseRelevancy)\n\n#Set the judge LLM for evaluation\n\nevaluator_llm = \nLangchainLLMWrapper(\nChatOpenAI(model=\"gpt-4o-mini\")\n)\n\n# Calculate the metrics for the dataset \n\nresult = evaluate(\ndataset=evaluation_dataset,\nmetrics=[\nLLMContextRecall(), \nFaithfulness(), \nAnswerCorrectness(), \nResponseRelevancy(),\nFactualCorrectness()],\nllm=evaluator_llm)\n```"]