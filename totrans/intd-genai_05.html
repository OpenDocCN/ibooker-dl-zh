<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">6 <a id="idTextAnchor000"/>Accelerating productivity: Machine-augmented work</h1>
<p class="co-summary-head"><a id="idTextAnchor001"/>This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Using LLMs in professional and personal settings</li>
<li class="co-summary-bullet">Discussing the use and misuse of generative AI tools in education</li>
<li class="co-summary-bullet">Exploring methods to detect machine-generated content</li>
<li class="co-summary-bullet">Examining the overall economic effect of generative AI tools</li>
</ul>
<p class="body"><a id="marker-142"/>Everyone has, at some point in their life, experienced what in positive psychology is known as the concept of <i class="fm-italics">flow</i>: you’re deeply absorbed in what you’re working on and perhaps lose track of time because you’re so focused. And, most likely, you also experienced sudden interruptions, maybe the need to look something up or attend to something else, that break the flow. This frustration was top of mind for then GitHub CEO Nat Friedman when he announced the release of GitHub’s coding assistant, Copilot. “It helps you quickly discover alternative ways to solve problems, write tests, and explore new APIs without having to tediously tailor a search for answers on the internet,” Friedman wrote <a class="url" href="https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/">[1]</a>. Integrating into Microsoft’s code editor, <a id="idTextAnchor002"/>Visual Studio Code, was a crucial component: Copilot would plug directly into coders’ existing workflows.<a id="idIndexMarker000"/></p>
<p class="body">In programming and other fields, people are using large language models (LLMs) and other types of generative AI as a means of accelerating the work that they already do, whether that’s designing a curriculum or a workout plan. In this chapter, we investigate the current usage of LLMs in personal, professional, and educational settings. We also consider the possible shifts that this technology will cause in education and the economy. <a id="idTextAnchor003"/></p>
<h2 class="fm-head" id="heading_id_3">Using LLMs in the professional space</h2>
<p class="body">In the previous chapter, we discussed examples of occupational misuse of chatbots in such highly regulated industries as medicine, finance, and law. The focus of this section is the beneficial uses of chatbots across those professions and others. The consensus is that LLMs will be transformative, but what those effects will be remains unclear. Already, the use of LLMs is prompting existential questions about these professional fields. What does it mean to be a doctor? What does it mean to be a lawyer? Fundamentally, jobs have long been understood to imbue us with a sense of purpose—chatbots might cause professional identity crises by taking on some portion of these services. On the bright side, sectors such as medicine, law, and finance provide critical services in today’s society, and those services aren’t always accessible to people who need them. Although LLMs aren’t replacements for experienced people working in these fields, they might help to shoulder the load.<a id="idTextAnchor004"/><a id="idIndexMarker001"/><a id="marker-143"/></p>
<h3 class="fm-head1" id="heading_id_4">LLMs assisting doctors with administrative tasks</h3>
<p class="body">Primary care providers today often spend more time doing nonpatient-facing tasks than patient-facing tasks. Dr. James Barnett, a clinical associate professor at the University of Illinois College of Medicine Peoria, wrote about the “exhausting time burden” placed on medical practitioners, and quoted a physician colleague who said:<a id="idIndexMarker002"/></p>
<p class="fm-quote">Providing good medical care and taking care of patients is why I enjoy my career . . . With administrative overload, I find myself getting by with the minimal required care, compassion, and understanding of my patients. Satisfaction in my career suffers. <a class="url1" href="https://peoria.medicine.uic.edu/administrative-tasks-take-up-more-time-than-patient-care-for-many-pcps/">[2]</a></p>
<p class="body">Such administrative overload includes managing emails and phone calls, writing progress notes and charts, and interacting with health insurance providers over claims or appeals. One study showed that this nonpatient-facing work takes up about 60% of primary care providers’ time; another concluded that the real total is at least two-thirds <a class="url" href="https://peoria.medicine.uic.edu/administrative-tasks-take-up-more-time-than-patient-care-for-many-pcps/">[2]</a>. Given this reality in the United States and many other nations, it’s no wonder that early adopters have begun to see LLMs as a potential solution.</p>
<p class="body"><a id="marker-144"/>Dr. Richard Stern, a Dallas-based rheumatologist, asked GPT-4 to write a letter of appeal to an insurer that had denied coverage of off-label use of the drug Anakinra for a patient with persistent chronic inflammatory disease. Stern sent the letter produced by the LLM to the insurer, which then granted the request, sparing the patient $1,500 a month in out-of-pocket costs. Stern told the <i class="fm-italics">New York Times</i> that GPT-4 had made his time with patients significantly more productive and that his practice now use the model to compose email replies and responses to common questions from patients and to fill out paperwork. It’s not only the administrative work that doctors have started to lean on LLMs to do. Dr. Michael Pigone, the chair of the internal medicine department at Dell Medical School at the University of Texas at Austin, asked his team for a script that doctors could use to talk to patients with alcohol use disorder who had “not responded to behavioral interventions,” and were still drinking too much. “A week later, no one had done it,” said Pigone, but when he asked ChatGPT, the model immediately produced a usable script that hit all of the main talking points. Asked to rewrite it for patients with little medical knowledge, it then produced a more accessible version that began with, “If you think you drink too much alcohol, you’re not alone. Many people have this problem, but there are medicines that can help you feel better and have a happier, healthier life” <a class="url" href="https://www.nytimes.com/2023/06/12/health/doctors-chatgpt-artificial-intelligence.xhtml">[3]</a>.</p>
<p class="body">Using LLMs to write scripts for delivering messages to patients in a more empathetic manner is more controversial than things like summarizing patient notes because of its intrinsically interpersonal nature. In the same <i class="fm-italics">New York Times</i> report, a few medical professionals expressed umbrage at the idea of working doctors outsourcing empathy to an LLM, while others cautioned against confusing ChatGPT’s good bedside manner with good medical advice. A particularly striking anecdote details a doctor asking ChatGPT for the words he needs to comfort not a patient but a friend with an advanced form of terminal cancer. Dr. Gregory Moore, formerly a practicing physician in diagnostic radiology and neurology, then an executive leading health and life sciences at Microsoft, reported being blown away by the quality of ChatGPT’s responses, which offered empathy and encouragement without false hope. “I wish I would have had this when I was in training,” said Moore. “I have never seen or had a coach like this” <a class="url" href="https://www.nytimes.com/2023/06/12/health/doctors-chatgpt-artificial-intelligence.xhtml">[3]</a>. Anthropic AI’s LLM, Claude, generated the script in figure 6.1, following a prompt about talking to patients about quitting smoking.<a id="marker-145"/></p>
<div class="figure">
<p class="figure1"><a id="idTextAnchor005"/><img alt="" class="calibre2" height="679" src="../../OEBPS/Images/CH06_F01_Dhamani.png" width="600"/></p>
<p class="figurecaption">Figure 6.1 The beginning of a response written by Claude to a request for a script about smoking cessatio<a id="idTextAnchor006"/>n</p>
</div>
<h3 class="fm-head1" id="heading_id_5">LLMs for legal research, discovery, and documentation</h3>
<p class="body">The takeover of administrative work is far from limited to the medical field. According to a 2017 survey of 2,915 legal professionals in the United States, lawyers spend about half of their time on administrative tasks <a class="url" href="https://www.law.com/2017/09/26/what-do-lawyers-really-do-with-their-time/">[4]</a>. Most private lawyers and firms use the billable hours system, where employees assiduously track the time spent working on a particular case, often in six-minute intervals. Because the tasks involved with managing a legal practice that aren’t directly related to a case aren’t billable, legal firms are especially incentivized to automate this overhead. LLMs might be used to respond to client or potential client communications, as an example. But the real value that LLMs could unlock is in the very meat and potatoes of lawyering: discovery and legal research, and document drafting. <a id="idIndexMarker003"/><a id="marker-146"/></p>
<p class="body"><i class="fm-italics">Discovery</i> is “the formal process of exchanging information between the parties about the witnesses and evidence they’ll present at trial” <a class="url" href="https://www.americanbar.org/groups/public_education/resources/law_related_education_network/how_courts_work/discovery/">[5]</a>, and, depending on the lawsuit, can take months or years and involve the exchange of thousands of documents. E-discovery software applications, designed to help index these documents to locate salient information, have been a standard tool in legal practice for more than a decade. However, it typically relies on the user searching for a specific term, almost like a search engine for discovery materials. If prompted or fine-tuned for this task, LLMs could present brief summaries of documents, or even identify which materials support a particular argument.</p>
<p class="body">Another key component of practicing law is reading through case law and preceding decisions to draw comparisons and contrasts. Existing AI-based solutions already aim to find relevant decisions through techniques such as document embeddings and similarity (see chapter 1’s discussion of embeddings). Due to their rich internal representations, LLMs could do a better job of finding related cases and could also explain their similarities and differences, a feature that is well beyond the ability of non-LLM-based methods.</p>
<p class="body">Drafting documents is a more challenging but potentially transformative application of generative AI. Andrew Perlman, dean and professor at Suffolk University Law School, is the author of an article entitled “The Implications of ChatGPT for Legal Services and Society” published in Harvard Law School Center on the Legal Profession’s <i class="fm-italics">The Practice</i> magazine. In reality, however, he has a coauthor: as Perlman freely admits, ChatGPT did most of the writing <a class="url" href="https://clp.law.harvard.edu/article/the-implications-of-chatgpt-for-legal-services-and-society/">[6]</a>. Within the piece, Perlman includes examples of ChatGPT-written drafts of a legal complaint, a will, and contracts pertaining to the sale of real estate and a car. Each was generated with a separate prompt—the prompt for the car contract reads as follows:<a id="marker-147"/></p>
<p class="fm-quote">Create a contract for the sale of a 2018 Toyota Prius from Jane Smith to John Doe in Massachusetts for the sale price of $15,000. The contract should contain the usual representations and warranties of such a sale.</p>
<p class="body">Assessing the chatbot’s responses, Perlman calls the legal documents incomplete, but surprisingly sophisticated. While ChatGPT won’t be replacing top lawyers anytime soon, Perlman says that Bing Chat is “already operating at the level of a B/B+ law student, and it will only get better with time.” Like the doctors who used ChatGPT, however, he sees AI as a tool that will become essential in the legal profession:</p>
<p class="fm-quote">AI will not eliminate the need for lawyers, but it does portend the end of lawyering as we know it. Many clients, especially those facing complex issues, will still need lawyers to offer expertise, judgment, and counsel, but those lawyers will increasingly need AI tools to deliver those services efficiently and effectively. <a class="url1" href="https://clp.law.harvard.edu/article/the-implications-of-chatgpt-for-legal-services-and-society/">[6]</a></p>
<p class="body">Perlman also notes that 90% of low-income Americans and a majority of middle-income Americans receive “no meaningful assistance when facing important civil legal issues,” including child custody, eviction, foreclosure, and debt collection. If AI-powered tools could be safely used to explain in plain language what rights people were entitled to given their situation, as illustrated in figure 6.2, it could be an extremely powerful equalizer in these types of very common cases that aren’t typically legally complicated and have an enormous effect on people’s lives.<a id="idIndexMarker004"/><a id="idTextAnchor007"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="475" src="../../OEBPS/Images/CH06_F02_Dhamani.png" width="725"/></p>
<p class="figurecaption">Figure 6.2 Part of Bard’s response to a query from a renter about a dispute with their landlo<a id="idTextAnchor008"/>rd</p>
</div>
<h3 class="fm-head1" id="heading_id_6">LLMs augmenting financial investing and bank customer service</h3>
<p class="body">In the financial industry, the gauntlet has been thrown by Bloomberg, the business and finance data, news, and analytics company, with the release of BloombergGPT. According to the press release, BloombergGPT is a 50-billion-parameter LLM trained on “a wide range of financial data” <a class="url" href="https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/">[7]</a> and is designed for finance-related natural language processing tasks, presumably to help investment analysts process market news and information as quickly as possible. The effect of such a tool isn’t yet known, but in the world of high finance, any edge could potentially be worth billions.<a id="idIndexMarker005"/><a id="marker-148"/></p>
<p class="body">Banks have also long relied on chatbots for customer service, and an optimistic view is that LLMs could improve the quality of these interactions. According to the Consumer Financial Protection Bureau, 37% of the US population interacted with a bank’s chatbot in 2022, a staggering figure that is only projected to get larger, and all 10 of the largest banks in the country deploy chatbots on their websites. LLM-based chatbots could help address some of the existing problems, such as frustrating interactions where the bots don’t understand what the user wants or is trying to do. However, they also carry a greater risk of responding inappropriately, possibly by hallucinating about the bank’s offerings. Therefore, any financial usage should be vetted extremely thoroughly before deployment, especially given that incorrect responses may be a violation of consumer financial protection laws <a class="url" href="https://www.consumerfinance.gov/about-us/newsroom/cfpb-issue-spotlight-analyzes-artificial-intelligence-chatbots-in-banking/">[<span id="idTextAnchor009"/>8]</a>.</p>
<h3 class="fm-head1" id="heading_id_7">LLMs as collaborators in creativity</h3>
<p class="body">LLMs are most readily suited to generative tasks because of the probabilistic nature of their outputs—they can produce lots of different suitable responses, rather than a single “correct” answer. A Reddit thread surveying users on how they were using LLMs at their work included lots of descriptions of everyday tasks that people had successfully outsourced to chatbots <a class="url" href="https://www.reddit.com/r/ChatGPT/comments/12fhcec/how_are_you_using_chatgpt_at_work/">[9]</a>. Teachers have used them for creating lesson plans and teaching materials; social media marketers have used them to write short-form copy for networks such as Twitter and Instagram, and then expand the same key ideas into longer-form copy for blog posts. <a id="idIndexMarker006"/></p>
<p class="body">Naturally, then, LLMs are beginning to be used even more heavily in creative domains. Noah Brier, a serial entrepreneur in marketing and technology, launched BrXnd.ai to “explore the intersection between brands and AI” <a class="url" href="https://brxnd.ai/">[10]</a>. The organization’s inaugural event featured a competition billed as the first “ad Turing test,” where brand and advertising experts were tasked with identifying which of 10 posters advertising the same fictional energy drink were created by teams of marketing students and which were generated by AI <a class="url" href="https://www.contagious.com/news-and-views/experts-stumped-by-ad-turing-test">[11]</a>. A sample poster generated by AI is shown in figure 6.3<a id="idTextAnchor010"/>.<a id="marker-149"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="372" src="../../OEBPS/Images/CH06_F03_Dhamani.png" width="369"/></p>
<p class="figurecaption">Figure 6.3 An AI-generated advertisement, created with the prompt “poster for a new energy drink called Buzz” by the open source image-generation model Stable Diffusion</p>
</div>
<p class="body">The expert panel achieved an accuracy of 57%; the 300-person audience could only tell the difference between the human-created and machine-created ads with 53% accuracy, close to what we would expect from random guessing. Additionally, Brier submitted the ads generated with AI to System1, a marketing agency that measures people’s emotional response to ads at scale to predict their efficacy. The ads scored an average of 1.83 on the System1 rating scale, only slightly below the national average for a print advertisement, 1.9 <a class="url" href="https://system1group.com/work">[12]</a>. The teams that used AI were prevented from altering the model’s output in any way, and, in effect, the resulting ads were roughly indistinguishable from those conceived of, designed, and produced by humans. The models, of course, also generated their ads much more quickly, and could theoretically produce many different concepts within the same time that human teams took, for a lower cost.</p>
<p class="body">However, the competition between humans and AI is a false one, as one of the teams demonstrated: they admitted from the outset that although they had been assigned to use AI, they used the models to generate assets, and then put the final poster together themselves. Brier took their ad out of the Turing test event, but still scored its emotional response with System1, and the human-AI collaboration received a higher score (2.8) than any entry produced by humans or AI alone. In an interview with Contagious about the results, Brier said that while he doesn’t expect AI to replace human creativity, “It is the most amazing creative accelerant I’ve ever experienced” <a class="url" href="https://www.contagious.com/news-and-views/experts-stumped-by-ad-turing-test">[11]</a>. Although working with AI tools might not be for everyone, in the best case, humans and machines can function as collaborators, combining human imagination with AI’s ability to synthesize inputs and generate outputs rapidly.<a id="marker-150"/></p>
<p class="body">Counterintuitively, LLMs seem to be good at almost exactly the opposite things that we expect computers to be good at. Where typical machines produce responses deterministically and excel at math and logic, LLMs and the chatbots they power sometimes make mistakes in math, or make up facts entirely. On the other hand, LLMs excel at writing poetry and making conversation. There are many traits that we consider to be so interconnected with our concept of humanity that it once seemed impossible for machines to display them—empathy and creativity chief among them. Now, chatbots can produce responses that not only display these traits but also sometimes outperform humans as evaluated by other humans. This accomplishment shouldn’t be diminished, nor should it be overstated: the chatbots aren’t themselves empathetic, but they have learned to produce empathetic messages.</p>
<p class="body">For now, chatbots are best viewed as tools that make professionals more efficient and productive. They are valuable—and might soon become invaluable—but their work might be incomplete, or they might not pick up on the types of details that an experienced professional might. In other ways, though, they already far outperform humans, such as their ability to correlate vast amounts of data. More effective than either the AI or the human alone is the human-AI “team,” with the AI providing an initial analysis or first draft, and the human reviewing their work. Already, this ability and other skills have made chatbots valuable in all manner of workplaces. This might be uncomfortable for many people, but it could also be liberating, enabling professionals to have more control over how they spend their tim<a id="idTextAnchor011"/>e. <a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="marker-151"/></p>
<h2 class="fm-head" id="heading_id_8">LLMs as a programming sidekick</h2>
<p class="body">Perhaps unsurprisingly, many of the earliest adopters of LLMs are technologists and programmers. One of the most common practical applications of LLMs is as an aid in writing code. We’ve previously highlighted GitHub Copilot as the leading product in this space; Copilot is based on OpenAI’s Codex model, which has been fine-tuned for writing code on millions of GitHub repositories <a class="url" href="https://ghdocs-prod.azurewebsites.net/en/copilot">[13]</a>. Other code generation models include Amazon’s CodeWhisperer (see <a class="url" href="http://mng.bz/QPAe">http://mng.bz/QPAe</a>), Replit’s Ghostwriter (see <a class="url" href="http://mng.bz/XNvM">http://mng.bz/XNvM</a>), and the open source model StarCoder (see <a class="url" href="http://mng.bz/yQlE">http://mng.bz/yQlE</a>). In some ways, writing code is easier for a model than other types of generative tasks because there is a lot of structure and repeating patterns. In prose, people rarely use the same phrases more than once, but we expect to see functions called multiple times in code. These models are designed to be pair programmers and provide “autocomplete-style suggestions” as you code. You can specify the language and write a natural language description, as a comment or docstring (used to document a specific segment of code) of what you want a function to do. The model will then take a pass at implementing that function. While there are certainly failure modes, especially for complex functions, it often does a reasonable first attempt, making it much quicker to iterate.<a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/></p>
<p class="body">Generative models have been trained to interpret code, making it possible to use some LLMs as a computer terminal or command-line prompt or as toy databases. DiagramGPT, powered by GPT-4 and created by Eraser, a developer of brainstorming and diagram tools, is just one example of novel LLM-powered capabilities (see <a class="url" href="http://mng.bz/MBNm">http://mng.bz/MBNm</a>). It takes a schema, infrastructure definition, or code snippet as input and produces a diagram for the system described so that a person unfamiliar with the code or schema can easily visualize what’s going on.<a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="marker-152"/></p>
<p class="body">In keeping with the theme of using LLMs to take on rote tasks, another coding-related application that these models excel at is writing documentation. The usual privacy concerns apply—it’s inadvisable to paste proprietary code into an external application programming interface (API)—but for functions that aren’t sensitive, you can prompt Copilot or another LLM with the code and request that the model generate comments explaining the function, adding docstrings and type hints, and making other improvements that can make already-written code more readable. Figure 6.4. depicts an example of an AI-generated <a id="idTextAnchor012"/>docstring.<a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="405" src="../../OEBPS/Images/CH06_F04_Dhamani.png" width="645"/></p>
<p class="figurecaption">Figure 6.4 The docstring produced by ChatGPT correctly describes the function given and each input.</p>
</div>
<p class="body">Some LLMs that aren’t designed explicitly for pair programming can also be coding resources. For example, regular expressions (regexes) are famously tricky but powerful paradigms in programming. A regex defines some criteria for a string of text and then provides functionality for fast and efficient searches for bits of text that match the criteria. Different characters can signify what characters to look for, how many characters to expect, and which parts of the string to ignore. Regexes are often used to parse out things such as email addresses or phone numbers. To illustrate, a regex for extracting email addresses looks like this: /^([a-z0-9_\.-]+)@([\da-z\.-]+)\.([a-z\.]{2,63})$/. Recently, one of us needed a rather messy regex and asked GPT-4 to write it for us. Not only did GPT-4 produce the correct regex, but the model was able to explain its own answer, and what each symbol in the regex represented. ChatGPT’s generation for a simpler regex is shown in figure 6.5; other people have reported using ChatGPT to write Excel macros in a similar fa<a id="idTextAnchor013"/>shion <a class="url" href="https://www.adventuresincre.com/openai-gpt-3-excel-macro-real-estate-model/">[14]</a>.<a id="idIndexMarker019"/><a id="marker-153"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="374" src="../../OEBPS/Images/CH06_F05_Dhamani.png" width="672"/></p>
<p class="figurecaption">Figure 6.5 A partial response from ChatGPT when prompted to give a regex for social media handles. The full response gave regexes for Twitter, Instagram, Facebook, and LinkedIn usernames. We note here that the regex is correctly described, but Twitter handles actually range from 4 to 15 characters.</p>
</div>
<p class="body">Writing code is a collaborative endeavor, in that people have always shared, reused, and repurposed code. Consider Stack Exchange, mentioned in chapter 2 as a popular data source for training LLMs. Its flagship Q&amp;A website, Stack Overflow, is devoted to people asking each other questions about snippets of code—usually, question askers describe what they are trying to do, paste a few lines of code that reproduces the error that they are running into, and then wait for knowledgeable people to respond. The best answers on Stack Overflow provide not only the corrected code snippet but also detailed explanations about why the original poster’s attempt failed, perhaps due to concepts they misunderstood or quirks of particular programming languages. LLMs could serve functionally the same purpose as a community of millions of people and provide answers faster than the fastest of Stackers.<a id="idIndexMarker020"/><a id="idIndexMarker021"/></p>
<p class="body">In the world of LLM coding assistants, expertise still matters. Copilot can produce programs like a human because it was trained on human-written code. Just like human-generated code, though, its solutions might be inefficient or may fail to consider edge cases. LLMs are specialists in reproducing coding patterns and styles, but developers still need to exercise critical thinking around the composition and requirements of a given program. Knowledge of core concepts of computer science and best practices in software engineering may, if anything, become even more important, with LLMs capable of doing most boilerplate scripting. We expect that in the near term, the greatest utility will be derived from programmers leaning on LLMs such as Copilot to speed up their workflow and learn about specific syntaxes or libraries quickly, rather than from LLMs replacing progr<a id="idTextAnchor014"/>ammers entirely.<a id="marker-154"/></p>
<h2 class="fm-head" id="heading_id_9">LLMs in daily life</h2>
<p class="body">Although we’ve discussed at length the possible uses of generative models, the best method of uncovering applications is through experimentation. In addition to using LLMs to either speed up or replace parts of professional workflows, people have found all manners of ways to use the models for hobbies, projects, self-improvement, education, and entertainment. We expect that as users become familiar with these tools, and share their experiences, novel use cases will emerge as the design and capabilities of generative models continue to evolve. In this section, we’ll explore the ways that people are using generative models in th<a id="idTextAnchor015"/>eir daily lives.<a id="idIndexMarker022"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Communal prompting</p>
<p class="fm-sidebar-text">Online resources abound for sharing interesting or especially fruitful prompts for LLMs. The practice of structuring and refining prompts to elicit certain types of responses from LLMs is called <i class="fm-italics">prompt engineering</i>. PromptHero bills itself as the “#1 website for prompt engineering” and showcases millions of AI-generated images and texts along with the prompts that produced them (see <a class="url" href="https://prompthero.com/">https://prompthero.com/</a>). PromptHero and other websites like it address a real need: the prompts given to LLMs and image-generation models affect the outputs quite a bit and sometimes in unpredictable ways. Communities of users sharing their best prompts allow those users to iterate more quickly and get better results, especially while prompting continues to be both powerful and not well understood. <a id="idIndexMarker023"/></p>
</div>
<p class="body"><a id="marker-155"/>Stack Overflow is only one of a variety of services that people have suggested might be replaced entirely by LLMs. It’s also possible that these services will either integrate LLMs into their offerings or simply continue to exist as an alternative. We know that LLMs can perform translation, so Google Translate might not be needed as much on its own, but the language-learning app Duolingo has already launched an integration with GPT-4. With Duolingo Max, the LLM provides explanations for incorrect answers and lets users role-play their own scenarios, rather than simply participating in conversations written by Duolingo <a class="url" href="https://blog.duolingo.com/duolingo-max/">[15]</a>.</p>
<p class="body">One of the areas with the most potential to be affected by LLMs is education, including but not limited to language learning. While we’ll delve more deeply into the structural effect of generative AI on education in the next section, here we’ll also highlight how the applications we’ve discussed previously can be applied to self-teaching new concepts. In chapter 1, we compared the success of various LLMs at summarization and question-answering tasks. People interested in brushing up on history or following the latest developments in some scientific field or ongoing political conflict could ask LLMs to provide accessible summaries for them (see chapter 5, section Hallucinations for a discussion of retrieval). Students have successfully used LLMs to explain concepts as a means of exam prep: similar to Stack Exchange but more irreverent, the subreddit ELI5 (for “Explain like I’m five”) is filled with questions that posters want the answers to, including queries about machines, animals and nature, physics and the universe, and a whole bunch of assorted topics. As shown in figure 6.6, a student preparing for a physics exam might use the prompt “Explain string theory in simple terms” to grasp the basics of difficult concepts and could then ask follow-up questions on any aspects that they were struggling with (of course, it would be wise to double-check the responses with a cr<a id="idTextAnchor016"/>edible source).<a id="marker-156"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="530" src="../../OEBPS/Images/CH06_F06_Dhamani.png" width="645"/></p>
<p class="figurecaption">Figure 6.6 The partial response of ChatGPT to a prompt about string theory</p>
</div>
<p class="body">Today, most people look for information about products and services to buy primarily via search engines and sometimes review sites or large online retailers. When we’re looking for something to do, we might search for events in our local area this weekend, movies that are out in theaters, or shows that are popular on streaming services. When we’re about to make a large purchase—let’s say expensive cookware—there is often a fair amount of research involved: you might first Google for the best slow cooker, then click on a few options on Amazon, and read their reviews and ratings. Or, you might prefer to read reviews in home-related magazines or newspapers instead, and then buy the selected option from the retailer’s website. Although chatbots that don’t perform retrieval (web search) will be of limited value when it comes to new products, bots that do can synthesize this information as a sort of shopping assistant, like Bard’s response <a id="idTextAnchor017"/>in figure 6.7.<a id="marker-157"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="461" src="../../OEBPS/Images/CH06_F07_Dhamani.png" width="725"/></p>
<p class="figurecaption">Figure 6.7 Bard’s partial response to a query about possible grills to purchase.</p>
</div>
<p class="body">Indeed, this is the ultimate vision for virtual assistants, which began with natural-language assistants including Siri and Alexa. However, Siri, Alexa, and Google Assistant are (at least for now) “command-and-control systems,” meaning that they understand a finite list of requests, and can’t respond intelligently to requests outside of that list—they will simply decline to answer. LLM-powered chatbots, on the other hand, will by default respond to any question or request, if sometimes too confidently. That said, because of the relatively controlled manner in which the command-and-control systems operate, these assistants have already been connected to various other systems, whether to make adjustments in the home (turning off lights, changing thermostat settings) or to make purchases on Amazon.</p>
<p class="body">In AI, an <i class="fm-italics">agent</i> is a system that can pursue goals flexibly <a class="url" href="https://www.lesswrong.com/posts/dcoxvEhAfYcov2LA6/agentized-llms-will-change-the-alignment-landscape">[16]</a>. In effect, this means that the system must be able to interact with its environment and respond to changes in the environment. Capabilities such as changing the light settings or online shopping are examples of interacting with an environment—in this case, the real world. Siri and Alexa aren’t agents because they don’t adapt their goals, which is required to perform complex, multistep tasks. For example, let’s say that you ask Siri to recommend an outfit for you based on the weather forecast for your specific location as well as your agenda for the day. The assistant will be able to retrieve the weather but can’t execute the plan of retrieving the weather, reading your calendar, and then coming up with a reasonable suggestion for articles of clothing unless specifically programmed to do so. On the other hand, an LLM could break down the task into its component parts. If asked, it could likely respond with the steps required, and, if enabled to retrieve weather data and calendar information, could perform each step in a sequence. Accessing external data or using APIs to respond to a query is an example of agentic or agentized behavior in LLMs.<a id="idIndexMarker024"/><a id="marker-158"/></p>
<p class="fm-callout"><span class="fm-callout-head">Agent</span> is a system that can pursue goals flexibly, where the system must be able to interact with its environment and respond to changes in the environment.</p>
<p class="body">Agentizing LLMs is the next logical step in several existing commercial applications. For example, Expedia, a travel planning website, has an integration with ChatGPT that enables users to have open-ended conversations with the bot to get flight, hotel, and activity recommendations for their planned trips <a class="url" href="https://www.expediagroup.com/investors/news-and-events/financial-releases/news/news-details/2023/Chatgpt-Wrote-This-Press-Release--No-It-Didnt-But-It-Can-Now-Assist-With-Travel-Planning-In-The-Expedia-App/default.aspx">[17]</a>. The bot doesn’t actually book these recommendations, but all that would be required would be connecting the model to some sort of payment API. Of course, there are many valid reasons for not yet doing this; the bot might hallucinate flights that don’t exist or misunderstand users’ preferences. But it’s only a matter of time before applications like this become a reality. We’ll dig into more details about how agentized LLMs work in chapter 8.</p>
<p class="body">Already, there has been a flurry of activity in the open source world in agentizing LLMs. Projects such as LangChain (see <a class="url" href="http://mng.bz/a1WY">http://mng.bz/a1WY</a>) focus on the development of applications around LLMs that are both agentic, that is, interacting with an environment, and data-aware, meaning that they can access external data sources. Auto-GPT is an open source project that describes itself as “push[ing] the boundaries of what is possible with AI” by prompting GPT-4 to perform long-term planning for goal achievement. For now, this is still very challenging for AI, even GPT-4: Auto-GPT’s documentation reads in its section on limitations, “May not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!” <a class="url" href="https://github.com/Significant-Gravitas/Auto-GPT">[18]</a>. Despite the lofty ambitions of Auto-GPT, models of today tend to get stuck on intermediate steps or forget their previous work. The execution aspect isn’t there yet, but it seems imminent that more people will test it out on a limited number of tasks, and LLMs already have shown some utility in generating plans given specified objectives.</p>
<p class="body">For the more productivity-minded among us, chatbots might provide a structured plan for achieving goals, such as sticking to an exercise regimen or getting weekly chores done. Bryan X. Chen, the lead consumer technology writer for the <i class="fm-italics">New York Times</i>, explains that for best results, you should reference a particular self-help book with advice relevant to the task, to steer the chatbot in the right direction <a class="url" href="https://www.nytimes.com/2023/06/23/technology/ai-chatbot-life-coach.xhtml">[19]</a>. Chen uses the example of aiming to run a marathon. The prompt that he suggests reads:<a id="marker-159"/></p>
<p class="fm-quote">I want you to act as a life coach. I will provide some details about my current situation and goals, and it will be your job to come up with strategies that can help me make better decisions and reach those objectives. This could involve offering advice on various topics, such as creating plans for achieving success or dealing with difficult emotions. My first request is: My goal this fall is to run a marathon. Come up with a three-month plan using the principles of the book “Slow AF Run Club.”</p>
<p class="body">This prompt is descriptive, relies on a trusted source, and provides examples of the kinds of responses Chen is seeking. A simpler prompt, such as “Write me a marathon training plan,” will also yield results with ChatGPT, but they might not align as well with what Chen was looking for. Because the plan is generated by an LLM instead of posted on a website, the user could also ask for as many tweaks as they wanted until they were happy with the results. Theoretically, this might be used for achieving any type of goal.</p>
<p class="body">Finally, LLMs are of course used for all kinds of writing-related tasks. As generative models, they are well-suited for volleying ideas and brainstorming as you would a writing partner. LLMs will occasionally produce funny or creative texts, especially given an interesting prompt or one set to high temperature, but often—given the probabilistic generation of likely tokens—their generations are, well, predictable. This makes them also ideal candidates for the type of formulaic writing that many of us do daily, such as emails, meeting notes, and performance reviews.</p>
<p class="body">In section Professional Applications, we noted that evidence shows that even doctors spend a lot of time doing administrative tasks rather than interfacing directly with patients. The late anthropologist David Graeber documented the explosion of pointless paperwork, reports, and so-called “box-ticking” exercises in the past several decades in his best-selling book <i class="fm-italics">Bullshit Jobs</i>. Though Graeber has his own theories about why box-ticking jobs seem to abound in today’s economy, it’s true that despite an ever-present specter of a leisure-filled future brought about by technological progress, we haven’t made great strides in this direction. John Maynard Keynes predicted in 1930 that within a century, people would mostly be fighting against boredom rather than fatigue, and would perhaps work three or so hours a day just to feel productive.<a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="marker-160"/></p>
<p class="body">Needless to say, Keynes’ prediction hasn’t come to fruition. The hopeful outlook is that with LLMs, office workers everywhere could outsource some of the duller or more formulaic work to models and focus their energies on what interests them most. At the same time, there exists an interesting feedback cycle where the less high-quality, human-generated content is available, the more steadily models might degrade. A 2023 paper shows that a significant percentage of crowd workers who are responsible for labeling AI outputs are themselves using AI <a class="url" href="https://www.technologyreview.com/2023/06/22/1075405/the-people-paid-to-train-ai-are-outsourcing-their-work-to-ai/">[20]</a>. It’s hard to begrudge any of them for leaning on AI for their work, but if model-generated text becomes the norm on the internet, it could have big implications for future LLMs trained on internet data, as well as the experience of surfing the web. We would see comparatively less original content, whether that content is insightful cultural commentary or an innovative meme format. We may enter a phase wherein services such as coaching, creative copywriting, and personal training become premium experiences, with LLMs providing a lower-cost alternative. Ultimately, these tools are excellent resources, but, to date, there is no substitute for human experie<a id="idTextAnchor018"/>nce and ingenuity. <a id="idIndexMarker027"/></p>
<h2 class="fm-head" id="heading_id_10">Generative AI’s footprint on education</h2>
<p class="body">As with any “revolutionary” technology, ChatGPT caused some people’s jaws to drop and others’ brows to furrow. Its release was met with concern and criticism from some educators who feared that students could misuse the tool for cheating on assignments. Albeit prematurely, <i class="fm-italics">The Atlantic</i>, an American magazine, went so far as to say that it’s “The End of High School English” <a class="url" href="https://www.theatlantic.com/technology/archive/2022/12/openai-chatgpt-writing-high-school-english-essay/672412/">[21]</a> and “The College Essay Is Dead” <a class="url" href="https://www.theatlantic.com/technology/archive/2022/12/chatgpt-ai-writing-college-student-essays/672371/">[22]</a>. Ethan Mollick, a professor at the University of Pennsylvania’s Wharton School of Business, tweeted, “AI has basically ruined homework” <a class="url" href="https://twitter.com/emollick/status/1603762000815091714?s=20&amp;t=fVkX0l5OhVN2Pfp3Wfymow">[23]</a>. In a frenzy, schools started responding to these fears by blocking access to the chatbot. Citing “concerns about negative impacts on student learning, and concerns regarding the safety and accuracy of content,” the New York City education department blocked access to ChatGPT on all department devices and networks <a class="url" href="https://ny.chalkbeat.org/2023/1/3/23537987/nyc-schools-ban-chatgpt-writing-artificial-intelligence">[24]</a>. Meanwhile, Peter Wang, cofounder and CEO of Anaconda, tweeted, “I think we can basically re-invent the concept of education at scale. College as we know it will cease to exist” <a class="url" href="https://twitter.com/pwang/status/1599520310466080771">[25]</a>.<a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="marker-161"/></p>
<p class="body">For some educators, cheating is a practical fear—students are using ChatGPT to write essays and research papers, as well as solve math and science word problems, plagiarizing the AI-written work. Teachers and school administrators were caught off guard by the chatbot’s abilities, as they scrambled not only to catch students who were using the tool to cheat but also revamp their lesson plans accordingly. Some teachers are worried that students will never need to learn to write or be able to start an essay or paper from scratch. Adding to their concerns, the answers generated by ChatGPT and similar tools aren’t always accurate (see chapter 5, section Hallucinations). The chatbot tends to make up citations, include inaccurate facts, or repeatedly reference the same source—but the information can often be so specific and plausible-sounding that it can be an added step for teachers to carefully verify and corroborate the references and facts.</p>
<p class="body">While some teachers have prohibited the use of ChatGPT, others have embraced the tool. For one, the chatbot is hardly an A+ student—it’s an excellent synthesizer, but not a critical thinker <a class="url" href="http://www.brookings.edu/blog/education-plus-development/2023/01/09/chatgpt-educational-friend-or-foe/">[26]</a>. Secondly, with or without ChatGPT, stopping cheating entirely is likely an impossible task. ChatGPT is just another tool to aid in cheating, similar to ordering essays online from Kenyan workers <a class="url" href="https://www.bbc.com/news/blogs-trending-58465189">[27]</a> or copying answers on online exams from Chegg, an edtech company that provides homework help and other student services <a class="url" href="https://www.cnbc.com/2021/03/21/how-college-students-learned-new-ways-to-cheat-during-covid-.xhtml">[28]</a>. Finally, banning the use of ChatGPT will <i class="fm-italics">just not work</i>. Students can easily cheat the system by accessing the tool outside of class, on their personal devices, or perhaps by using a virtual private network (VPN) on school networks. Of course, teachers and school administrators who forbid using tools like ChatGPT will expect some students to use them anyway, so they will need to quickly find ways to detect content that is machine-generated, which as we’ve discussed previously (and we’ll further discuss in the next section), is a <i class="fm-italics">very</i> difficult problem. Tools to classify text as machine-generated, such as O<a id="idTextAnchor019"/>penAI’s classifier and GPTZero (see <a class="url" href="https://gptzero.me/">https://gptzero.me/</a>), are unreliable and limited in nature. If students edit the machine-generated text, then these tools can also be easily evaded. On the other hand, if teachers solely rely on such tools to catch cheating, then they may falsely identify the text as machine-generated, putting a student’s academic career in jeopardy. <a id="idIndexMarker030"/><a id="marker-162"/></p>
<p class="body">Not too long after ChatGPT’s public debut, a survey from Stanford University suggested that students were already using the tool to complete assignments and exams <a class="url" href="https://stanforddaily.com/2023/01/22/scores-of-stanford-students-used-chatgpt-on-final-exams-survey-suggests/">[29]</a>. As some colleges grapple with the emergence of ChatGPT, many have already incorporated the usage of generative AI tools in their academic integrity policies and provided guidance for teachers for incorporating AI tools in the classroom <a class="url" href="https://www.uvm.edu/wid/artificial-intelligence">[30]</a> <a class="url" href="https://ctl.wustl.edu/resources/chatgpt-and-ai-composition-tools/">[31]</a>. In the same vein, many educators advocating for ChatGPT in academia believe that, if used appropriately, it can be an effective teaching tool. Ditch That Textbook, a teaching blog, lists a multitude of ways that ChatGPT (or similar tools) can be used to enhance the learning experience for both the teacher and the student, some of which are shown in figure 6.8 (see <a class="url" href="https://ditchthattextbook.com/ai/">https://ditchthattextbook.com/ai/</a>). Teachers can use it to assist with lesson plan writing, perhaps even creating personalized learning experiences based on each student’s needs and abilities, or even asking for feedback on students’ work. Students could use it as a starting point for an assignment, evaluate the tool’s initial response, and then critically think about how to further revise it for improvement. It can be used to supplement in-person instruction by providing resources for students outside of the classroom, such as using it for after-hours tutoring to explain concepts or assistance for English language learners to improve their writing skills. ChatGPT can also be creatively incorporated into lesson plans, such as using it as a tool to hone their debating skills or asking students to grade the output from the chatbot. Similarly, educational technology (EdTech) start-ups have also used LLMs for teaching and learning purposes—a few examples include an AI tutor (see <a class="url" href="https://riiid.com/">https://riiid.com/</a>), a personalized learning platform (see <a class="url" href="https://www.alefeducation.com/">www.alefeducation.com/</a>), and a conversational virtual assistant for learning science (se<a id="idTextAnchor020"/>e <a class="url" href="https://www.cognii.com/">www.cognii.com/</a>). <a id="idIndexMarker031"/><a id="marker-163"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="302" src="../../OEBPS/Images/CH06_F08_Dhamani.png" width="471"/></p>
<p class="figurecaption">Figure 6.8 Examples of how to use chatbots in the classroom</p>
</div>
<p class="body">Sure, a tool like ChatGPT is certainly disruptive to a classroom setting, but the tool doesn’t need to be something that educators fear or are threatened by. Generative AI tools can be used for deeper, more engaged learning, especially because this is the world we live in now. Disruptive technology has always been met with excitement and fear—critics of the telephone feared that phones would disrupt face-to-face communication; with the invention of the television, some worried about the potential harm of creating a society of couch potatoes. Sam Altman, cofounder of OpenAI, responded to educators’ concerns about cheating in school by comparing the generative text to a calculator:</p>
<p class="fm-quote">We adapted to calculators and changed what we tested for in math class, I imagine. This is a more extreme version of that, no doubt, but also the benefits of it are more extreme, as well. <a class="url1" href="https://gizmodo.com/chatgpt-openai-ceo-sam-altman-schools-cheating-1850011314">[32]</a></p>
<p class="body">While Altman downplays legitimate concerns around misuse and limitations of the tool, educators do need to figure out a way to adjust to these tools instead of outright banning them.</p>
<p class="body"><a id="marker-164"/>Adjusting to new technology is rarely easy, and while tools such as ChatGPT are posed to change how we teach and learn, it’s not the end of formal education. In a <i class="fm-italics">VentureBeat</i> article, Andrew Ng, a globally recognized leader in AI, and Andrea Passerini, the CEO of Kira Learning, urged schools to teach AI and coding to prepare students for an AI-powered world <a class="url" href="https://venturebeat.com/ai/schools-should-teach-ai-to-every-child-according-to-andrew-ng-and-andrea-pasinetti/">[33]</a>. Similarly, Harvard University released the AI Pedagogy Project to help educators engage their students in discussions about the capabilities and limitations of AI systems (see <a class="url" href="https://aipedagogy.org/">https://aipedagogy.org/</a>). Whether we love them or fear them, we live in a world with generative AI tools, and students need to understand how to work alongside them. We need to teach them their strengths and weaknesses—how they can be used for productivity and creativity, but also how they can be misused and their risks. When used appropriately, ChatGPT and similar tools can augment the learning experience and help students navigate a world in which<a id="idTextAnchor021"/> AI works with humans. <a id="idIndexMarker032"/><a id="idIndexMarker033"/></p>
<h2 class="fm-head" id="heading_id_11">Detecting AI-generated text</h2>
<p class="body">In chapters 4 and 5, we discussed several efforts to detect machine-generated content, some of which are being used by educators to detect AI-plagiarized homework. While there have been promising results in developing detection methods, there are no silver bullet solutions—this is an extremely hard problem to solve, and it’s only getting harder with advancements in generative models. It’s also worth noting that this problem isn’t as well-posed as it seems. If we change one word, or maybe two words, in AI-generated text, then is the text still considered AI-generated? The ill-posed nature of this problem adds to the complexity of developing reliable detection methods. In this section, we’ll dive deeper into detection methods for AI-generated text. <a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="marker-165"/></p>
<p class="body">Traditional approaches to detecting machine-generated text involve statistical outlier detection methods, such as GLTR (discussed in chapter 5, section Adversarial Narratives). GLTR assumes that machine-generated text sticks to a limited subset of most probable words at each position in the sentence, whereas natural writing more frequently selects unpredictable words that make sense in that context <a class="url" href="https://arxiv.org/pdf/1906.04043.pdf">[34]</a>. This method makes use of fundamental statistical techniques, that is, distributional estimates, to distinguish machine-generated text from human-written text. Another statistical approach, DetectGPT, uses a probability curvature–based criterion to detect if the generated text is from an LLM <a class="url" href="https://arxiv.org/pdf/2301.11305.pdf">[35]</a>. Here, when a model generates a sentence, it calculates how likely, or probable, each word is to appear in a correct sentence. It makes the assumption that the model might think that minor edits to the sentence are less likely to be correct because they may not match well with what it was trained on, but the human-written text can be different in many ways.</p>
<p class="body">As discussed in the previous chapters, classifiers are commonly used to detect machine-generated text. Here, a classifier is an algorithm that is used to categorize data into distinct groups or classes. OpenAI released an “imperfect” classifier in January 2023 to distinguish between AI-generated and human-written text, which is a helpful example of how classifiers can be used in this context. They acknowledge that it’s impossible to reliably detect <i class="fm-italics">all</i> AI-generated text, but their classifier could be used to complement other methods for detecting AI-generated text rather than as a primary decision-making tool (though—as noted in chapter 4—the OpenAI classifier was taken down in five months after its release due to accuracy problems) <a class="url" href="https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text">[36]</a>. While good classifiers have their place in detecting machine-generated text, it’s important to recognize their limitations (as with any technical approaches for this task) and understand that it’s not a bulletproof way to identify AI-written text. It’s also worth noting that classifiers often overfit to a specific generator’s distribution. In other words, a classifier designed to detect text from GPT-4 will likely not perform as well when detecting text generated by other chatbots, such as Bard or Bing Chat. However, they have shown promise in informing mitigations for misuses of machine-generated text, especially in combination with other sociotechnical methods to determine the source of a piece of content.</p>
<p class="body"><a id="marker-166"/>Given the increasing difficulty of reliably detecting AI-generated content, researchers are exploring a novel technique known as <i class="fm-italics">watermarking</i>. Historically, watermarks have been used in images and videos to protect copyrighted content and prevent intellectual property theft. In an innovative approach, researchers have shown how to incorporate watermarking into text generated by LLMs to assist with the identification of AI-generated text <a class="url" href="https://arxiv.org/pdf/2301.10226.pdf">[37]</a>. Watermarking in text works by changing the pattern of words in the generated text, that is, altering the probabilities of certain special words to make them easier to detect later. Let’s visualize this concept (shown in figure 6.9)—imagine a list of words that make up the language model’s vocabulary, which is randomly divided in half into a “greenlist” and a “redlist.” Then, when an LLM, such as ChatGPT, generates text, it can insert a watermark by prompting the model to choose more greenlisted words than a human would be expected to use. So, the more words in the greenlist that are in a piece of content, the more likely it is that the content was generated by a machine. In comparison, text written by humans would likely be a more random mix of words. <a id="idIndexMarker036"/></p>
<p class="fm-callout"><span class="fm-callout-head">Watermarking</span> in text works by changing the pattern of words in the generated text, that is, altering the probabilities of certain special words to make the<a id="idTextAnchor022"/>m easier to detect later.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre2" height="688" src="../../OEBPS/Images/CH06_F09_Dhamani.png" width="725"/></p>
<p class="figurecaption">Figure 6.9 Illustration of how a chatbot could embed watermarking in the text it generates</p>
</div>
<p class="body"><a id="marker-167"/>These watermarks are meant to be invisible to the human eye—if someone tried to bypass the watermark by editing the text, they wouldn’t know which words to change. While this approach has shown more promise than statistical or classification techniques, it doesn’t come without limitations. First, this technique should ideally be implemented in the LLM from the very beginning. Making watermarking work for large models like ChatGPT without compromising the quality of the outputs is no easy feat.</p>
<p class="body">Next, for watermarking to truly be a successful detection technique, all the big players building generative language models need to unilaterally agree to incorporate it within their AI systems. This, in itself, may prove to be challenging, if not impossible (at least without government regulation). For the general public using watermark detection tools (e.g., educators trying to determine if the student’s essay is machine-generated), it could be quite cumbersome to check the text against multiple tools unless all the AI companies decide on an industry standard for watermark implementation, which again, is a far-fetched goal. However, open-sourcing the watermark implementation, or making it public, isn’t the answer either because anyone would be able to deduce the watermark pattern, then defeating their purpose. OpenAI has announced that they have been working on watermarking text <a class="url" href="https://scottaaronson.blog/?p=6823">[38]</a>, among other provenance techniques for detection. Perhaps the closest thing to an industry standard may be for organizations using OpenAI’s models to adopt their specific watermarking technique, which would place an immense amount of unregulated power and trust in the company.<a id="marker-168"/></p>
<p class="body">Outside of the successful adoption of an industry standard for watermarking, people will likely figure out how much text they need to change to bypass detection tools. Regrettably, that’s the problem with all detection tools—the tools <i class="fm-italics">can</i> make it easier to avoid detection. In other words, people could repeatedly modify machine-generated text and check it against a detection tool until it no longer classifies it as machine-generated. This suggests some concern with rolling out detection tools to the general public as adversaries could learn how to “fool” or bypass them. However, repeatedly modifying machine-generated text and checking it against detection tools can be a fairly time-consuming exercise to pass off the machine-generated text as one’s own, which may still deter the behavior.</p>
<p class="body">While we’ve discussed several notable technical solutions in this section, remember that there are no silver bullet solutions—given the complexity of this problem, there is no single solution that will reliably detect every piece of machine-generated content every single time. We probably won’t ever get to live in a world with a mythical detection tool that reliably detects machine-generated content. As we develop more novel techniques to detect content from machines, we’ll <i class="fm-italics">also</i> get better at generating more human-like content. This is why it’s necessary to adopt a comprehensive framework for detecting the misuse of AI-generated content that doesn’t solely rely on technical solutions, including AI education in schools and the workplace, so we can better understand its risks and limitations, and learn to use<a id="idTextAnchor023"/> them to enhance our lives.<a id="idIndexMarker037"/><a id="idIndexMarker038"/></p>
<h2 class="fm-head" id="heading_id_12">How LLMs affect jobs and the economy</h2>
<p class="body"><a id="marker-169"/>ChatGPT, and similar tools, will undoubtedly only get better and harder to detect, with billions of dollars being poured into the development of AI technology. As the general public becomes more and more aware that such tools are here to stay, many are speculating about how they will disrupt their day-to-day lives. In this chapter, we discussed how several occupations are using generative language models to make their jobs more efficient and productive, how people are using these tools in their everyday lives, and how the education industry was quickly disrupted with ChatGPT’s public release. Now, we’ll touch on the global economic effect and try to answer the question: What does this mean for all of us? <a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>
<p class="body">First, let’s discuss the optimistic view—generative AI tools are expected to make many workers more efficient and increase productivity while boosting the overall economy. Productivity, a key part of economic growth, has slowed down in the past two decades. A report from the Brookings Institution, <i class="fm-italics">Machines of Mind: The Case for an AI-Powered Productivity Boom</i>, argues that generative language models will provide a much-needed boost to productivity <a class="url" href="https://www.brookings.edu/research/machines-of-mind-the-case-for-an-ai-powered-productivity-boom/">[39]</a>. While we’ve discussed several limitations of LLMs, including bias and hallucinations, which require human oversight in the workplace, proponents of AI-driven productivity gains claim that “their economic value depends not on whether they are flawless, but on whether they can be used productively” <a class="url" href="https://www.brookings.edu/research/machines-of-mind-the-case-for-an-ai-powered-productivity-boom/">[39]</a>. In one scenario, the Brookings analysis illustrates a decade of productivity growth increases, leaving the economy 5% larger, and then compounding every year thereafter. Another report by Goldman Sachs suggests that generative AI could raise the global gross domestic product (GDP) by 7%, or 7 trillion dollars <a class="url" href="https://www.ft.com/content/50b15701-855a-4788-9a4b-5a0a9ee10561">[40]</a>. This is a significant effect for a single technology to have on the metrics that determine the long-term prosperity and wealth of our nations. <a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<p class="body"><a id="marker-170"/>In a 2023 report by OpenAI, OpenResearch, and the University of Pennsylvania, the authors noted that LLMs could affect 80% of the US workforce in some form <a class="url" href="https://arxiv.org/pdf/2303.10130.pdf">[41]</a>. Other reports stated GitHub CoPilot could help software engineers code twice as fast <a class="url" href="https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/">[42]</a>, writing tasks could also be completed twice as fast <a class="url" href="https://economics.mit.edu/sites/default/files/inline-files/Noy_Zhang_1.pdf">[43]</a>, economists could be 10%–20% more productive <a class="url" href="https://doi.org/10.3386/w30957">[44]</a>, and customer service workers 14% more productive. <a class="url" href="https://www.bloomberg.com/news/articles/2023-04-24/generative-ai-boosts-worker-productivity-14-new-study-finds">[45]</a> More notably, generative models can help a wide group of less-skilled workers upskill to be able to compete with people who have more credentials or experience. In a productivity study for using ChatGPT in professional writing tasks, such as marketing and HR, the authors demonstrate the inequality decrease between workers—that is, the less skilled workers get quantifiably better, while the more experienced workers get a little faster <a class="url" href="https://economics.mit.edu/sites/default/files/inline-files/Noy_Zhang_1.pdf">[46]</a>.</p>
<p class="body">On the other hand, generative AI tools could possibly do little to help overall economic growth. More pessimistically, they could be used to replace humans with machines, drive down wages, and exacerbate the inequality between wealth and income. In <i class="fm-italics">The Turing Trap: The Promise &amp; Peril of Human-Like Artificial Intelligence</i>, Erik Brynjolfsson argues that AI developers are too focused on imitating human intelligence instead of creating technology to give people new abilities <a class="url" href="https://www.amacad.org/publication/turing-trap-promise-peril-human-artificial-intelligence">[47]</a>. He believes that this pursuit of mimicking human-like capabilities, which replace humans with machines is the “single biggest explanation” for wealth inequality <a class="url" href="https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/">[48]</a>. In that vein, companies that design and develop these AI tools may potentially influence the effect on the economy. As discussed in chapter 1, the computational cost required to build and run LLMs creates a barrier of entry for anyone looking to compete in this space, leaving the power in the hands of the same big companies that already control so much of the technology world. There have, however, been strides made in the open source community to develop LLMs, BLOOM, Falcon, Stable LM, MPT-7B, Dolly, RedPajama, and OpenLLaMa. It’s also important to note that Meta’s LLaMa and Llama 2, which have been open sourced by the company, have greatly accelerated the development of these models. These efforts could help decentralize the power that is concentrated within a few big technology companies and help break their control over such technology in the future. <a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="marker-171"/></p>
<p class="body">Now, let’s get back to the question at the beginning of this section: What does this mean for all of us? For some, generative AI tools have been met with panic and concern that they may be out of a job soon. Goldman Sachs predicted that 300 million full-time jobs will be lost to AI <a class="url" href="https://www.forbes.com/sites/jackkelly/2023/03/31/goldman-sachs-predicts-300-million-jobs-will-be-lost-or-degraded-by-artificial-intelligence/">[49]</a>. But it’s important to keep the bigger picture in mind—this isn’t the first time that technology has disrupted our lives. Many experts think that this disruption could perhaps even create more new job opportunities than it would displace. In 2021, a report noted that 60% of the jobs done didn’t exist in 1940 <a class="url" href="https://www.bbc.com/worklife/article/20230515-workplace-ai-how-artificial-intelligence-will-transform-the-workday">[50]</a>. In other words, technology over the past 80 years created new industries and jobs, and we could potentially expect to see a similar movement with generative AI. Economists are also uncertain about the productivity boom and net benefits, as well as how jobs may be affected. Paul Krugman, professor emeritus at Princeton University, said, “Predictions about the economic impact of technology are notoriously unreliable,” asserting that LLMs should not affect economic projections in the next few years, or even the next decade. He further says “History suggests that large economic effects from A.I. will take longer to materialize than many people currently seem to expect” <a class="url" href="https://fortune.com/2023/04/03/nobel-laureate-paul-krugman-ai-chatgpt-economy/">[51]</a>. Regardless of when it takes place, we should expect an evolution, no<a id="idTextAnchor024"/>t a revolution, with generative AI.<a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
<h2 class="fm-head" id="heading_id_13">Summary<a id="marker-172"/></h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">People are already using generative AI tools to assist with both personal and professional tasks, especially to offload more administrative and repetitive work.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Coding assistants such as Copilot, CodeWhisperer, and Ghostwriter can be helpful throughout the software engineering workflow: from thinking through architectures to writing code to generating documentation and diagrams.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Prompts, follow-up questions, and feedback affect model results, and the best results seem to be produced by prompts that are detailed, instructive, and contain references or examples.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Some of the more powerful proposed applications of LLMs require the models to be agents, meaning that they will be able to interact with their environment and adapt accordingly.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Educators will need to adapt to a world in which generative AI tools exist by working alongside them in the classroom, as well as helping students learn about and navigate an AI-powered world.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Efforts to detect machine-generated text include statistical techniques, classifier-based detectors, and watermarking text.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Watermarking</i> in text works by changing the pattern of words in the generated text or prompting the model to choose certain special words to make them easier to detect later.</p>
</li>
<li class="fm-list-bullet">
<p class="list">There is no single technical solution to reliably detect every piece of machine-generated content every single time.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Economists are uncertain about the productivity boom and net benefits, as well as how jobs may be affected.</p>
</li>
<li class="fm-list-bullet">
<p class="list">With generative AI tools, we should expect an evolution, not a revolution.</p>
</li>
</ul>
</div></body></html>