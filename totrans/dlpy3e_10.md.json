["```py\n>>> import keras\n>>> model = keras.models.load_model(\n...     \"convnet_from_scratch_with_augmentation.keras\"\n... )\n>>> model.summary()\nModel: \"functional_3\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_3 (InputLayer)        │ (None, 180, 180, 3)      │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ rescaling_1 (Rescaling)           │ (None, 180, 180, 3)      │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_11 (Conv2D)                │ (None, 178, 178, 32)     │           896 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_6 (MaxPooling2D)    │ (None, 89, 89, 32)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_12 (Conv2D)                │ (None, 87, 87, 64)       │        18,496 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_7 (MaxPooling2D)    │ (None, 43, 43, 64)       │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_13 (Conv2D)                │ (None, 41, 41, 128)      │        73,856 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_8 (MaxPooling2D)    │ (None, 20, 20, 128)      │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_14 (Conv2D)                │ (None, 18, 18, 256)      │       295,168 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ max_pooling2d_9 (MaxPooling2D)    │ (None, 9, 9, 256)        │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ conv2d_15 (Conv2D)                │ (None, 7, 7, 512)        │     1,180,160 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ global_average_pooling2d_3        │ (None, 512)              │             0 │\n│ (GlobalAveragePooling2D)          │                          │               │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout (Dropout)                 │ (None, 512)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ (None, 1)                │           513 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 4,707,269 (17.96 MB)\n Trainable params: 1,569,089 (5.99 MB)\n Non-trainable params: 0 (0.00 B)\n Optimizer params: 3,138,180 (11.97 MB)\n```", "```py\nimport keras\nimport numpy as np\n\n# Downloads a test image\nimg_path = keras.utils.get_file(\n    fname=\"cat.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\"\n)\n\ndef get_img_array(img_path, target_size):\n    # Opens the image file and resizes it\n    img = keras.utils.load_img(img_path, target_size=target_size)\n    # Turns the image into a float32 NumPy array of shape (180, 180, 3)\n    array = keras.utils.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\" of a\n    # single sample. Its shape is now (1, 180, 180, 3).\n    array = np.expand_dims(array, axis=0)\n    return array\n\nimg_tensor = get_img_array(img_path, target_size=(180, 180)) \n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.axis(\"off\")\nplt.imshow(img_tensor[0].astype(\"uint8\"))\nplt.show() \n```", "```py\nfrom keras import layers\n\nlayer_outputs = []\nlayer_names = []\n# Extracts the outputs of all Conv2D and MaxPooling2D layers and put\n# them in a list\nfor layer in model.layers:\n    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n        layer_outputs.append(layer.output)\n        # Saves the layer names for later\n        layer_names.append(layer.name)\n# Creates a model that will return these outputs, given the model input\nactivation_model = keras.Model(inputs=model.input, outputs=layer_outputs) \n```", "```py\n# Returns a list of nine NumPy arrays — one array per layer activation\nactivations = activation_model.predict(img_tensor) \n```", "```py\n>>> first_layer_activation = activations[0]\n>>> print(first_layer_activation.shape)\n(1, 178, 178, 32)\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\") \n```", "```py\nimages_per_row = 16\n# Iterates over the activations (and the names of the corresponding\n# layers)\nfor layer_name, layer_activation in zip(layer_names, activations):\n    # The layer activation has shape (1, size, size, n_features).\n    n_features = layer_activation.shape[-1]\n    size = layer_activation.shape[1]\n    n_cols = n_features // images_per_row\n    # Prepares an empty grid for displaying all the channels in this\n    # activation\n    display_grid = np.zeros(\n        ((size + 1) * n_cols - 1, images_per_row * (size + 1) - 1)\n    )\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_index = col * images_per_row + row\n            # This is a single channel (or feature).\n            channel_image = layer_activation[0, :, :, channel_index].copy()\n            # Normalizes channel values within the [0, 255] range.\n            # All-zero channels are kept at zero.\n            if channel_image.sum() != 0:\n                channel_image -= channel_image.mean()\n                channel_image /= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n            # Places the channel matrix in the empty grid we prepared\n            display_grid[\n                col * (size + 1) : (col + 1) * size + col,\n                row * (size + 1) : (row + 1) * size + row,\n            ] = channel_image\n    # Displays the grid for the layer\n    scale = 1.0 / size\n    plt.figure(\n        figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0])\n    )\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.axis(\"off\")\n    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\") \n```", "```py\nimport keras_hub\n\n# Instantiates the feature extractor network from pretrained weights\nmodel = keras_hub.models.Backbone.from_preset(\n    \"xception_41_imagenet\",\n)\n# Loads the matching preprocessing to scale our input images\npreprocessor = keras_hub.layers.ImageConverter.from_preset(\n    \"xception_41_imagenet\",\n    image_size=(180, 180),\n) \n```", "```py\nfor layer in model.layers:\n    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n        print(layer.name) \n```", "```py\n# You could replace this with the name of any layer in the Xception\n# convolutional base.\nlayer_name = \"block3_sepconv1\"\n# This is the layer object we're interested in.\nlayer = model.get_layer(name=layer_name)\n# We use model.input and layer.output to create a model that, given an\n# input image, returns the output of our target layer.\nfeature_extractor = keras.Model(inputs=model.input, outputs=layer.output) \n```", "```py\nactivation = feature_extractor(preprocessor(img_tensor)) \n```", "```py\nfrom keras import ops\n\n# The loss function takes an image tensor and the index of the filter\n# we consider (an integer).\ndef compute_loss(image, filter_index):\n    activation = feature_extractor(image)\n    # We avoid border artifacts by only involving nonborder pixels in\n    # the loss: we discard the first 2 pixels along the sides of the\n    # activation.\n    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n    # Returns the mean of the activation values for the filter\n    return ops.mean(filter_activation) \n```", "```py\nimport tensorflow as tf\n\n@tf.function\ndef gradient_ascent_step(image, filter_index, learning_rate):\n    with tf.GradientTape() as tape:\n        # Explicitly watches the image tensor, since it isn't a\n        # TensorFlow Variable (only Variables are automatically watched\n        # in a gradient tape)\n        tape.watch(image)\n        # Computes the loss scalar, indicating how much the current\n        # image activates the filter\n        loss = compute_loss(image, filter_index)\n    # Computes the gradients of the loss with respect to the image\n    grads = tape.gradient(loss, image)\n    # Applies the \"gradient normalization trick\"\n    grads = ops.normalize(grads)\n    # Moves the image a little bit in a direction that activates our\n    # target filter more strongly\n    image += learning_rate * grads\n    # Returns the updated image, so we can run the step function in a\n    # loop\n    return image \n```", "```py\nimport torch\n\ndef gradient_ascent_step(image, filter_index, learning_rate):\n    # Creates a copy of \"image\" that we can get gradients for.\n    image = image.clone().detach().requires_grad_(True)\n    loss = compute_loss(image, filter_index)\n    loss.backward()\n    grads = image.grad\n    grads = ops.normalize(grads)\n    image = image + learning_rate * grads\n    return image \n```", "```py\nimport jax\n\ngrad_fn = jax.grad(compute_loss)\n\n@jax.jit\ndef gradient_ascent_step(image, filter_index, learning_rate):\n    grads = grad_fn(image, filter_index)\n    grads = ops.normalize(grads)\n    image += learning_rate * grads\n    return image \n```", "```py\nimg_width = 200\nimg_height = 200\n\ndef generate_filter_pattern(filter_index):\n    # The number of gradient ascent steps to apply\n    iterations = 30\n    # The amplitude of a single step\n    learning_rate = 10.0\n    image = keras.random.uniform(\n        # Initialize an image tensor with random values. (The Xception\n        # model expects input values in the [0, 1] range, so here we\n        # pick a range centered on 0.5.)\n        minval=0.4, maxval=0.6, shape=(1, img_width, img_height, 3)\n    )\n    # Repeatedly updates the values of the image tensor to maximize our\n    # loss function\n    for i in range(iterations):\n        image = gradient_ascent_step(image, filter_index, learning_rate)\n    return image[0] \n```", "```py\ndef deprocess_image(image):\n    # Normalizes image values within the [0, 255] range\n    image -= ops.mean(image)\n    image /= ops.std(image)\n    image *= 64\n    image += 128\n    image = ops.clip(image, 0, 255)\n    # Center crop to avoid border artifacts\n    image = image[25:-25, 25:-25, :]\n    image = ops.cast(image, dtype=\"uint8\")\n    return ops.convert_to_numpy(image) \n```", "```py\n>>> plt.axis(\"off\")\n>>> plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))\n```", "```py\n# Generates and saves visualizations for the first 64 filters in the\n# layer\nall_images = []\nfor filter_index in range(64):\n    print(f\"Processing filter {filter_index}\")\n    image = deprocess_image(generate_filter_pattern(filter_index))\n    all_images.append(image)\n\n# Prepares a blank canvas for us to paste filter visualizations\nmargin = 5\nn = 8\nbox_width = img_width - 25 * 2\nbox_height = img_height - 25 * 2\nfull_width = n * box_width + (n - 1) * margin\nfull_height = n * box_height + (n - 1) * margin\nstitched_filters = np.zeros((full_width, full_height, 3))\n\n# Fills the picture with our saved filters\nfor i in range(n):\n    for j in range(n):\n        image = all_images[i * n + j]\n        stitched_filters[\n            (box_width + margin) * i : (box_width + margin) * i + box_width,\n            (box_height + margin) * j : (box_height + margin) * j + box_height,\n            :,\n        ] = image\n\n# Saves the canvas to disk\nkeras.utils.save_img(f\"filters_for_layer_{layer_name}.png\", stitched_filters) \n```", "```py\n# Downloads the image and stores it locally under the path img_path\nimg_path = keras.utils.get_file(\n    fname=\"elephant.jpg\",\n    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\",\n)\n# Returns a Python Imaging Library (PIL) image\nimg = keras.utils.load_img(img_path)\nimg_array = np.expand_dims(img, axis=0) \n```", "```py\n>>> model = keras_hub.models.ImageClassifier.from_preset(\n...    \"xception_41_imagenet\",\n...    # We can configure the final activation of the classifier. Here,\n...    # we use a softmax activation so our outputs are probabilities.\n...    activation=\"softmax\",\n... )\n>>> preds = model.predict(img_array)\n>>> # ImageNet has 1,000 classes, so each prediction from our\n>>> # classifier has 1,000 entries.\n>>> preds.shape\n(1, 1000)\n>>> keras_hub.utils.decode_imagenet_predictions(preds)\n[[(\"African_elephant\", 0.90331),\n  (\"tusker\", 0.05487),\n  (\"Indian_elephant\", 0.01637),\n  (\"triceratops\", 0.00029),\n  (\"Mexican_hairless\", 0.00018)]]\n```", "```py\n>>> np.argmax(preds[0])\n386\n```", "```py\n# KerasHub tasks like ImageClassifier have a preprocessor layer.\nimg_array = model.preprocessor(img_array) \n```", "```py\nlast_conv_layer_name = \"block14_sepconv2_act\"\nlast_conv_layer = model.backbone.get_layer(last_conv_layer_name)\nlast_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output) \n```", "```py\nclassifier_input = last_conv_layer.output\nx = classifier_input\nfor layer_name in [\"pooler\", \"predictions\"]:\n    x = model.get_layer(layer_name)(x)\nclassifier_model = keras.Model(classifier_input, x) \n```", "```py\nimport tensorflow as tf\n\ndef get_top_class_gradients(img_array):\n    # Computes activations of the last conv layer and makes the tape\n    # watch it\n    last_conv_layer_output = last_conv_layer_model(img_array)\n    with tf.GradientTape() as tape:\n        tape.watch(last_conv_layer_output)\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = ops.argmax(preds[0])\n        # Retrieves the activation channel corresponding to the top\n        # predicted class\n        top_class_channel = preds[:, top_pred_index]\n\n    # Gets the gradient of the top predicted class with regard to the\n    # output feature map of the last convolutional layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n    return grads, last_conv_layer_output\n\ngrads, last_conv_layer_output = get_top_class_gradients(img_array)\ngrads = ops.convert_to_numpy(grads)\nlast_conv_layer_output = ops.convert_to_numpy(last_conv_layer_output) \n```", "```py\ndef get_top_class_gradients(img_array):\n    # Computes activations of the last conv layer\n    last_conv_layer_output = last_conv_layer_model(img_array)\n    # Creates a copy of last_conv_layer_output that we can get\n    # gradients for\n    last_conv_layer_output = (\n        last_conv_layer_output.clone().detach().requires_grad_(True)\n    )\n    # Retrieves the activation channel corresponding to the top\n    # predicted class\n    preds = classifier_model(last_conv_layer_output)\n    top_pred_index = ops.argmax(preds[0])\n    top_class_channel = preds[:, top_pred_index]\n    # Gets the gradient of the top predicted class with regard to the\n    # output feature map of the last convolutional layer\n    top_class_channel.backward()\n    grads = last_conv_layer_output.grad\n    return grads, last_conv_layer_output\n\ngrads, last_conv_layer_output = get_top_class_gradients(img_array)\ngrads = ops.convert_to_numpy(grads)\nlast_conv_layer_output = ops.convert_to_numpy(last_conv_layer_output) \n```", "```py\nimport jax\n\n# Defines a separate loss function\ndef loss_fn(last_conv_layer_output):\n    preds = classifier_model(last_conv_layer_output)\n    top_pred_index = ops.argmax(preds[0])\n    top_class_channel = preds[:, top_pred_index]\n    # Returns the activation value of the top-class channel\n    return top_class_channel[0]\n\n# Creates a gradient function\ngrad_fn = jax.grad(loss_fn)\n\ndef get_top_class_gradients(img_array):\n    last_conv_layer_output = last_conv_layer_model(img_array)\n    # Now  retrieving the gradient of the top-class channel is just a\n    # matter of calling the gradient function!\n    grads = grad_fn(last_conv_layer_output)\n    return grads, last_conv_layer_output\n\ngrads, last_conv_layer_output = get_top_class_gradients(img_array)\ngrads = ops.convert_to_numpy(grads)\nlast_conv_layer_output = ops.convert_to_numpy(last_conv_layer_output) \n```", "```py\n# This is a vector where each entry is the mean intensity of the\n# gradient for a given channel. It quantifies the importance of each\n# channel with regard to the top predicted class.\npooled_grads = np.mean(grads, axis=(0, 1, 2))\nlast_conv_layer_output = last_conv_layer_output[0].copy()\n# Multiplies each channel in the output of the last convolutional layer\n# by how important this channel is\nfor i in range(pooled_grads.shape[-1]):\n    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n# The channel-wise mean of the resulting feature map is our heatmap of\n# class activation.\nheatmap = np.mean(last_conv_layer_output, axis=-1) \n```", "```py\nheatmap = np.maximum(heatmap, 0)\nheatmap /= np.max(heatmap)\nplt.matshow(heatmap) \n```", "```py\nimport matplotlib.cm as cm\n\n# Loads the original image\nimg = keras.utils.load_img(img_path)\nimg = keras.utils.img_to_array(img)\n\n# Rescales the heatmap to the range 0–255\nheatmap = np.uint8(255 * heatmap)\n\n# Uses the \"jet\" colormap to recolorize the heatmap\njet = cm.get_cmap(\"jet\")\njet_colors = jet(np.arange(256))[:, :3]\njet_heatmap = jet_colors[heatmap]\n\n# Creates an image that contains the recolorized heatmap\njet_heatmap = keras.utils.array_to_img(jet_heatmap)\njet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\njet_heatmap = keras.utils.img_to_array(jet_heatmap)\n\n# Superimposes the heatmap and the original image, with the heatmap at\n# 40% opacity\nsuperimposed_img = jet_heatmap * 0.4 + img\nsuperimposed_img = keras.utils.array_to_img(superimposed_img)\n\n# Shows the superimposed image\nplt.imshow(superimposed_img) \n```"]