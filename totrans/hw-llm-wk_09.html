<html><head></head><body>
  <div class="readable-text" id="p1"> &#13;
   <h1 class=" readable-text-h1" id="references"><span class="title-text">References</span> </h1> &#13;
  </div> &#13;
  <div class="readable-text" id="p2"> &#13;
   <h2 class=" readable-text-h2" id="chapter-1">Chapter 1 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p3">Young, B. (2023). AI expert speculates on GPT-4 architecture. Weights &amp; Biases. <a href="https://api.wandb.ai/links/byyoung3/8zxbl12q">https://api.wandb.ai/links/byyoung3/8zxbl12q</a></li> &#13;
   <li class="readable-text" id="p4">Micikevicius, P. (2017). Mixed-precision training of deep neural networks. NVIDIA Developer. <a href="https://mng.bz/6eaA">https://mng.bz/6eaA</a></li> &#13;
   <li class="readable-text" id="p5">Accelerate AI development with Google Cloud TPUs. <a href="https://cloud.google.com/tpu">https://cloud.google.com/</a><a href="https://cloud.google.com/tpu">tpu</a></li> &#13;
   <li class="readable-text" id="p6">Metz, C. (2023, July 23). Researchers poke holes in safety controls of ChatGPT and other chatbots. <em>New York Times</em>.</li> &#13;
   <li class="readable-text" id="p7">Hu, K. (2023, February 2). ChatGPT sets record for fastest-growing user base—analyst note. Reuters. <a href="https://mng.bz/XxKv">https://mng.bz/XxKv</a></li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p8"> &#13;
   <h2 class=" readable-text-h2" id="chapter-2">Chapter 2 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p9">Friederici, A. D. (2011). The brain basis of language processing: From structure to function. <em>Physiology Review, 91,</em> 1357-1392. <a href="https://doi.org/10.1152/physrev.00006.2011">https://doi.org/10.1152/physrev</a><a href="https://doi.org/10.1152/physrev.00006.2011">.00006.2011</a> </li> &#13;
   <li class="readable-text" id="p10">Nation, P., and Waring, R. (1997). Vocabulary size, text coverage, and word lists. In: N. Schmitt and M. McCarthy, eds., <em>Vocabulary: Description, Acquisition, and Pedagogy</em> (pp. 6-19). Cambridge University Press. </li> &#13;
   <li class="readable-text" id="p11">Brown, T. B., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></li> &#13;
   <li class="readable-text" id="p12">Google/SentencePiece. <a href="https://github.com/google/sentencepiece">https://github.com/google/sentencepiece</a></li> &#13;
   <li class="readable-text" id="p13">Petrov, A., La Malfa, E., Torr, P. H. S., and Bibi, A. (2023). Language model tokenizers introduce unfairness between languages. <a href="https://arxiv.org/abs/2305.15425">https://arxiv.org/abs/2305.15425</a></li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p14"> &#13;
   <h2 class=" readable-text-h2" id="chapter-3">Chapter 3 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p15">Denk, T. (2019). Linear relationships in the transformer’s positional encoding. <a href="https://mng.bz/oKxd">https://mng.bz/oKxd</a></li> &#13;
   <li class="readable-text" id="p16">Raff, E. (2022). <em>Inside Deep Learning</em>. Manning.</li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p17"> &#13;
   <h2 class=" readable-text-h2" id="chapter-4">Chapter 4 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p18">Yong, E. (2012). Simulated brain scores top test marks. <em>Nature</em>. <a href="https://www.nature.com/articles/nature.2012.11914">https://www.nature</a><a href="https://www.nature.com/articles/nature.2012.11914">.com/articles/nature.2012.11914</a> </li> &#13;
   <li class="readable-text" id="p19">Forsyth, J. A., and Mongrut, S. (2022). Does duration of competitive advantage drive long-term returns in the stock market? <em>Revista Contabilidade &amp; Finanças, 33</em>(89), 329–342. <a href="https://doi.org/10.1590/1808-057x202113660">https://doi.org/10.1590/1808-057x202113660</a> </li> &#13;
   <li class="readable-text" id="p20">Lin, S., Hilton, J., and Evans, O. (2022). TruthfulQA: Measuring how models mimic human falsehoods. <a href="https://arxiv.org/abs/2109.07958">https://arxiv.org/abs/2109.07958</a></li> &#13;
   <li class="readable-text" id="p21">Parrish, A., Chen, A., Nangia, N., et al. (2022) BBQ: A hand-built bias benchmark for question answering. <a href="https://arxiv.org/abs/2110.08193">https://arxiv.org/abs/2110.08193</a>)</li> &#13;
   <li class="readable-text" id="p22">Chen, M., Tworek, J., Jun, H., et al. (2021). Evaluating large language models trained on code. <a href="https://arxiv.org/abs/2107.03374">https://arxiv.org/abs/2107.03374</a> </li> &#13;
   <li class="readable-text" id="p23">How can we draw a duck (in order to create a tikzducks package and store it in CTAN)? <a href="https://mng.bz/W2Jg">https://mng.bz/W2Jg</a></li> &#13;
   <li class="readable-text" id="p24">Sutton, R. (2019). The bitter lesson. <a href="https://mng.bz/EaJq">https://mng.bz/EaJq</a></li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p25"> &#13;
   <h2 class=" readable-text-h2" id="chapter-5">Chapter 5 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p26">Barber, R. G., Oza, A., Carlson, R., and Ramirez, R. (2023, October 18). Why scientists are reanimating spider corpses for research. NPR. <a href="https://mng.bz/lYgj">https://mng.bz/lYgj</a></li> &#13;
   <li class="readable-text" id="p27">OpenAI. Fine-tuning. <a href="https://mng.bz/dXnD">https://mng.bz/dXnD</a></li> &#13;
   <li class="readable-text" id="p28">Hugging Face. Fine-tune a pretrained model. <a href="https://huggingface.co/docs/transformers/training">https://huggingface.co/docs/</a><a href="https://huggingface.co/docs/transformers/training">transformers/training</a></li> &#13;
   <li class="readable-text" id="p29">Luo, Y, Yang, Z., Meng, F.,et al. (2025). An empirical study of catastrophic forgetting in large language models during continual fine-tuning. <a href="https://arxiv.org/abs/2308.08747">https://arxiv.org/abs/</a><a href="https://arxiv.org/abs/2308.08747">2308.08747</a></li> &#13;
   <li class="readable-text" id="p30">McCloskey, M., and Cohen, N. J. (1989). Catastrophic interference in connectionist networks. <em>Psychology of Learning and Motivation, 24,</em> 109-165. <a href="https://doi.org/10.1016/S0079-7421(08)60536-8">https://doi.org/</a><a href="https://doi.org/10.1016/S0079-7421(08)60536-8">10.1016/S0079-7421(08)60536-8</a></li> &#13;
   <li class="readable-text" id="p31">Belrose, N., Schneider-Joseph, D., Ravfogel, S., et al. (2023). LEACE: Perfect linear concept erasure in closed form. <a href="https://arxiv.org/abs/2306.03819">https://arxiv.org/abs/2306.03819</a></li> &#13;
   <li class="readable-text" id="p32">Phung, D. V., Thakur, A., Castricato, L., Tow, J., and Havrilla, A. (2025). Implementing RLHF: Learning to summarize with trlX. Weights &amp; Measures. <a href="https://mng.bz/rKzg">https://mng.bz/rKzg</a></li> &#13;
   <li class="readable-text" id="p33">Kolter, Z., and Madry, M. (n.d.). Adversarial robustness: Theory and practice. <a href="https://adversarial-ml-tutorial.org/">https://adversarial-ml-tutorial.org/</a></li> &#13;
   <li class="readable-text" id="p34">OpenAI. (2023, March 27). GPT-4 technical report. <a href="https://cdn.openai.com/papers/gpt-4.pdf">https://cdn.openai.com/</a><a href="https://cdn.openai.com/papers/gpt-4.pdf">papers/gpt-4.pdf</a></li> &#13;
   <li class="readable-text" id="p35">Chowdhery, A., Narang, S., Devlin, J., et al. (2022). PaLM: Scaling language modeling with pathways. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a></li> &#13;
   <li class="readable-text" id="p36">Liang, W., Izzo, Z., Zhang, Y., et al. (2024). Monitoring AI-modified content at scale: A case study on the impact of ChatGPT on AI conference peer reviews. <a href="https://arxiv.org/abs/2403.07183">https://arxiv.org/abs/2403.07183</a></li> &#13;
   <li class="readable-text" id="p37">Li, C., and Flanigan, J. (2023). Task contamination: Language models may not be few-shot anymore. <a href="https://arxiv.org/abs/2312.16337">https://arxiv.org/abs/2312.16337</a></li> &#13;
   <li class="readable-text" id="p38">Near, J. P., and Abuah, C. (2021). <em>Programming Differential Privacy</em>. <a href="https://programming-dp.com/">https://prog</a><a href="https://programming-dp.com/">ramming-dp.com/</a></li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p39"> &#13;
   <h2 class=" readable-text-h2" id="chapter-6">Chapter 6 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p40">Albergotti, R., and Matsakis, L. (2023, January 23). OpenAI has hired an army of contractors to make basic coding obsolete. Semafor. <a href="https://mng.bz/MDGQ">https://mng.bz/MDGQ</a></li> &#13;
   <li class="readable-text" id="p41">Introducing Code Llama, a state-of-the-art large language model for coding. (2023, August 24). Meta. <a href="https://mng.bz/av2j">https://mng.bz/av2j</a></li> &#13;
   <li class="readable-text" id="p42">von Werra, L., and Ben Allal, L. (2023, May 4). StarCoder: A state-of-the-art LLM for code. Hugging Face. <a href="https://huggingface.co/blog/starcoder">https://huggingface.co/blog/starcoder</a></li> &#13;
   <li class="readable-text" id="p43">Biderman, S., and Raff, E. (2022). Fooling MOSS detection with pretrained language models. <a href="https://arxiv.org/abs/2201.07406">https://arxiv.org/abs/2201.07406</a>. </li> &#13;
   <li class="readable-text" id="p44">Dyer, E., and Gur-Ari, G. (2020, June 30). Minerva: Solving quantitative reasoning problems with language models. Google Research. <a href="https://mng.bz/gane">https://mng.bz/gane</a>. </li> &#13;
   <li class="readable-text" id="p45">Azerbayev, Z., Schoelkopf, H., Paster, K., et al. (2023, October 16). Llemma: An open language model for mathematics. EleutherAI. <a href="https://blog.eleuther.ai/llemma/">https://blog.eleuther.ai/</a><a href="https://blog.eleuther.ai/llemma/">llemma/</a></li> &#13;
   <li class="readable-text" id="p46">Richardson, D. (1968). Some undecidable problems involving elementary functions of a real variable. <em>Journal of Symbolic Logic, 33,</em> 514–520.</li> &#13;
   <li class="readable-text" id="p47">Nogueira, R., Jiang, Z., and Lin, J. (2021). Investigating the limitations of transformers with simple arithmetic tasks. <a href="https://arxiv.org/abs/2102.13019v3">https://arxiv.org/abs/2102.13019v3</a></li> &#13;
   <li class="readable-text" id="p48">Golkar, S., Pettee, M., Eickenberg, M., et al. (2024). Investigating the limitations of transformers with simple arithmetic tasks. <a href="https://arxiv.org/abs/2310.02989">https://arxiv.org/abs/2310.02989</a></li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p49"> &#13;
   <h2 class=" readable-text-h2" id="chapter-7">Chapter 7 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p50">Romeo, R. R., Leonard, J. A., Robinson, S. T., et al. (2018). Beyond the 30-million-word gap: Children’s conversational exposure is associated with language-related brain function. <em>Psychological Science, 29,</em> 700–710. <a href="https://doi.org/10.1177/0956797617742725">https://doi.org/10.1177/</a><a href="https://doi.org/10.1177/0956797617742725">0956797617742725</a></li> &#13;
   <li class="readable-text" id="p51">Gilkerson, J., Richards, J. A., Warren, S. F., et al. (2017). Mapping the early language environment using all-day recordings and automated analysis. <em>American Journal of Speech-Language Pathology, 26,</em> 248-265. <a class="uri" href="https://doi.org/10.1044/2016_AJSLP-15-0169">https://doi.org/10.1044/2016_AJSLP-15-0169</a></li> &#13;
   <li class="readable-text" id="p52">Shumailov, I., Shumaylov, Z., Zhao, Y., et al. (2024). The curse of recursion: Training on generated data makes models forget. <a href="https://arxiv.org/abs/2305.17493">https://arxiv.org/abs/2305.17493</a></li> &#13;
   <li class="readable-text" id="p53">Stanovich K. E. (2009). <em>What Intelligence Tests Miss: The Psychology of Rational Thought</em>. Yale University Press.</li> &#13;
   <li class="readable-text" id="p54">Improving the realism of synthetic images. (2017, July 7). Apple Machine Learning Research. <a href="https://machinelearning.apple.com/research/gan">https://machinelearning.apple.com/research/gan</a> </li> &#13;
   <li class="readable-text" id="p55">Dai, D., Sun, Y., Dong, L., et al. (2023). Why can GPT learn in-context? Language models secretly perform gradient descent as meta-optimizers. In <em>Findings of the Association for Computational Linguistics: ACL 2023</em> (pp. 4005–4019). Association for Computational Linguistics.</li> &#13;
   <li class="readable-text" id="p56">Hiller, J. (2023, December 12). Microsoft targets nuclear to power AI operations. <em>Wall Street Journal</em>. <a href="https://mng.bz/pKe5">https://mng.bz/pKe5</a></li> &#13;
   <li class="readable-text" id="p57">Disavino, S. (2023, September 8). Texas power prices soar as grid passes reliability test in heat wave. Reuters. <a href="https://mng.bz/OB0K">https://mng.bz/OB0K</a></li> &#13;
   <li class="readable-text" id="p58">Emoji recently added, v15.1. (n.d.). Unicode. <a href="https://www.unicode.org/emoji/charts-15.1/emoji-released.html">https://www.unicode.org/emoji/</a><a href="https://www.unicode.org/emoji/charts-15.1/emoji-released.html">charts-15.1/emoji-released.html</a> </li> &#13;
   <li class="readable-text" id="p59">Wei, J., Wang, X., Schuurmans, D., et al. (2023). Chain-of-thought prompting elicits reasoning in large language models. <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a> </li> &#13;
   <li class="readable-text" id="p60">Wang, L., Xu, W., Lan, Y., et al. (2023). Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em> (Vol. 1: Long Papers, pp. 2609-2634). Association for Computational Linguistics.</li> &#13;
   <li class="readable-text" id="p61">Guan, L., Valmeekam, K., Sreedharan, S., and Kambhampati, S. (2023). Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. <a href="https://arxiv.org/abs/2305.14909">https://arxiv.org/abs/2305.14909</a></li> &#13;
   <li class="readable-text" id="p62">Bhargava, A. Y. (2015). <em>Grokking Algorithms: An illustrated Guide for Programmers and Other Curious People</em>. Manning Publications.</li> &#13;
   <li class="readable-text" id="p63">Merrill, W., and Sabharwal, S. (2024). The expressive power of transformers with chain of thought. In <em>International Conference on Learning Representations 2024</em>. <a href="https://openreview.net/forum?id=NjNGlPh8Wh">https://openreview.net/forum?id=NjNGlPh8Wh</a></li> &#13;
   <li class="readable-text" id="p64">Carlini, N. (2023, September 22). Playing chess with large language models. <a href="https://nicholas.carlini.com/writing/2023/chess-llm.html">https://nicholas.carlini.com/writing/2023/chess-llm.html</a> </li> &#13;
   <li class="readable-text" id="p65">Edwards B. (2022, November 7). New Go-playing trick defeats world-class Go AI—but loses to human amateurs. <em>Ars Technica</em>. <a href="https://mng.bz/dW6O">https://mng.bz/dW6O</a></li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p66"> &#13;
   <h2 class=" readable-text-h2" id="chapter-8">Chapter 8 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p67">Yagoda, M. (2024, February 23). Airline held liable for its chatbot giving passenger bad advice—what this means for travellers. BBC. <a href="https://mng.bz/xK7W">https://mng.bz/xK7W</a> </li> &#13;
   <li class="readable-text" id="p68">Notopoulos, K. (2023, December 18). A car dealership added an AI chatbot to its site: Then all hell broke loose. <a href="https://mng.bz/AQPz">https://mng.bz/AQPz</a></li> &#13;
   <li class="readable-text" id="p69">Suresh, H., Lao, N., and Liccardi, I. (2020). Misplaced trust: Measuring the interference of machine learning in human decision-making. In <em>Proceedings of the 12th ACM Conference on Web Science (WebSci ’20)</em> (pp. 315-324). Association for Computing Machinery. <a href="https://doi.org/10.1145/3394231.3397922">https://doi.org/10.1145/3394231.3397922</a></li> &#13;
  </ol> &#13;
  <div class="readable-text" id="p70"> &#13;
   <h2 class=" readable-text-h2" id="chapter-9">Chapter 9 </h2> &#13;
  </div> &#13;
  <ol> &#13;
   <li class="readable-text" id="p71">Hofmann, V., Kalluri, P. R., Jurafsky, D., and King, S. (2024). Dialect prejudice predicts AI decisions about people’s character, employability, and criminality. <a href="https://arxiv.org/abs/2403.00742">https://arxiv.org/abs/2403.00742</a> </li> &#13;
   <li class="readable-text" id="p72">Omiye, J. A., Lester, J. C., Spichak, S. et al. (2023). Large language models propagate race-based medicine. npj Digital Medicine, 6, 195. <a href="https://doi.org/10.1038/s41746-023-00939-z">https://doi.org/10.1038/</a><a href="https://doi.org/10.1038/s41746-023-00939-z">s41746-023-00939-z</a></li> &#13;
   <li class="readable-text" id="p73">Farm labor. (2025, January 8). Economic Research Service. <a href="https://www.ers.usda.gov/topics/farm-economy/farm-labor/">https://www.ers.usda</a><a href="https://www.ers.usda.gov/topics/farm-economy/farm-labor/">.gov/topics/farm-economy/farm-labor/</a> </li> &#13;
   <li class="readable-text" id="p74">Verma, P., and De Vync, G. (2023, June 2). ChatGPT took their jobs: Now they walk dogs and fix air conditioners. <em>The Washington Post</em>. <a href="https://mng.bz/RVYK">https://mng.bz/EwQd</a></li> &#13;
   <li class="readable-text" id="p75">Marr, B. (2024, April 18). The role of generative AI in video game development. <em>Forbes</em>. <a href="https://mng.bz/Pdpn">https://mng.bz/Pdpn</a></li> &#13;
   <li class="readable-text" id="p76">Lev-Ram, M. (2023, January 26). Casualties of Big Tech layoffs find other companies are clamoring to hire them. <em>Forbes</em>. <a href="https://mng.bz/JYXV">https://mng.bz/JYXV</a></li> &#13;
   <li class="readable-text" id="p77">Lohr, S. (2024, February 1). Generative A.I.’s biggest impact will be in banking and tech, report says. <em>New York Times</em>. <a href="https://mng.bz/wJ7P">https://mng.bz/wJ7P</a></li> &#13;
   <li class="readable-text" id="p78">Pethokoukis, J. (2016, June 16). What the story of ATMs and bank tellers reveals about the “rise of the robots’’ and jobs. American Enterprise Institute. <a href="https://mng.bz/qx7r">https://mng.bz/qx7r</a></li> &#13;
   <li class="readable-text" id="p79">Hunter, L. W., Bernhardt, A., Hughes, K. L., and Skuratowicz, E. (2001). It’s not just the ATMs: Technology, firm strategies, jobs, and earnings in retail banking. <em>ILR Review, 54</em>(2A), 402-424. <a href="https://doi.org/10.1177/001979390105400222">https://doi.org/10.1177/001979390105400222</a></li> &#13;
   <li class="readable-text" id="p80">Rosalsky, G. (2024, June 18). If AI is so good, why are there still so many jobs for translators? NPR. <a href="https://mng.bz/7pBv">https://mng.bz/7pBv</a></li> &#13;
   <li class="readable-text" id="p81">Marr, B. (2024, May 28). How generative AI will change the jobs of artists and designers. <em>Forbes</em>. <a href="https://mng.bz/mG7a">https://mng.bz/mG7a</a></li> &#13;
   <li class="readable-text" id="p82">Autor, D., Chin, C., Salomons, A., and Seegmiller, B. (2024). New frontiers: The origins and content of new work, 1940–2018. <em>The Quarterly Journal of Economics, 139,</em> 1399–1465. <a href="https://doi.org/10.1093/qje/qjae008">https://doi.org/10.1093/qje/qjae008</a></li> &#13;
   <li class="readable-text" id="p83">Dave, P. (2023, April 8). StackOverflow will charge AI giants for training data. <em>Wired</em>. <a href="https://mng.bz/5gDO">https://mng.bz/5gDO</a></li> &#13;
   <li class="readable-text" id="p84">Grimm, D. (2024, May 8). Stack Overflow bans users en masse for rebelling against OpenAI partnership—users banned for deleting answers to prevent them being used to train ChatGPT. Tom’s Hardware. <a href="https://mng.bz/nR75">https://mng.bz/nR75</a></li> &#13;
   <li class="readable-text" id="p85">Bishop, T. (2020, October 20). Expedia Group CEO on Google antitrust case: “Very pleased to see the government finally taking action.” Geek Wire. <a href="https://mng.bz/vK7p">https://mng.bz/vK7p</a></li> &#13;
   <li class="readable-text" id="p86">Siddiqui, T. (2023, June 29). Risks of artificial intelligence must be considered as the technology evolves: Geoffrey Hinton. University of Toronto. <a href="https://mng.bz/4aNR">https://mng.bz/4aNR</a></li> &#13;
   <li class="readable-text" id="p87">Bengio, Y. (2023, June 24). FAQ on catastrophic AI risks. <a href="https://mng.bz/QDO6">https://mng.bz/QDO6</a></li> &#13;
   <li class="readable-text" id="p88">Introducing Llama 3.1: Our most capable models to date. (2024, July 23). Meta. <a href="https://ai.meta.com/blog/meta-llama-3-1/">https://ai.meta.com/blog/meta-llama-3-1/</a> </li> &#13;
   <li class="readable-text" id="p89">Min, S., Gururangan, S., Wallace, E., et al. (2023). SILO language models: Isolating legal risk in a nonparametric datastore. <a href="https://arxiv.org/abs/2308.04430">https://arxiv.org/abs/2308.04430</a></li> &#13;
   <li class="readable-text" id="p90">Rivero, N. (2022, September 21). Low-background metal: Pure, unadulterated treasure. <em>Quartz</em>. <a href="https://mng.bz/eyXZ">https://mng.bz/eyXZ</a></li> &#13;
   <li class="readable-text" id="p91">Shumailov, I., Shumaylov, Z., Zhao, Y. et al. (2024). AI models collapse when trained on recursively generated data. <em>Nature, 631,</em> 755–759. <a href="https://doi.org/10.1038/s41586-024-07566-y">https://doi.org/10</a><a href="https://doi.org/10.1038/s41586-024-07566-y">.1038/s41586-024-07566-y</a></li> &#13;
   <li class="readable-text" id="p92">Coffey, L. (2024, February 9). Professors cautious of tools to detect AI-generated writing. Inside Higher Education. <a href="https://mng.bz/Xxj9">https://mng.bz/Xxj9</a></li> &#13;
   <li class="readable-text" id="p93">Has Stack Exchange’s traffic decreased since ChatGPT? (2023). Stack Exchange. <a href="https://mng.bz/yW7p">https://mng.bz/yW7p</a></li> &#13;
   <li class="readable-text" id="p94">Dhamani, N., and Engler, M. (2024). <em>Introduction to Generative AI</em>. Manning. <a href="https://www.manning.com/books/introduction-to-generative-ai">https://www.manning.com/books/introduction-to-generative-ai</a> </li> &#13;
  </ol>&#13;
 

  <div class="readable-text">
   <h1>index</h1>
  </div>
 </body></html>