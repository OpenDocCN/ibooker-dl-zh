["```py\nimport tensorflow as tf\n\nshakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\nfilepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\nwith open(filepath) as f:\n    shakespeare_text = f.read()\n```", "```py\n>>> print(shakespeare_text[:80])\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n```", "```py\ntext_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n                                                   standardize=\"lower\")\ntext_vec_layer.adapt([shakespeare_text])\nencoded = text_vec_layer([shakespeare_text])[0]\n```", "```py\nencoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\nn_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\ndataset_size = len(encoded)  # total number of chars = 1,115,394\n```", "```py\ndef to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\n    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n    ds = ds.batch(batch_size)\n    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n```", "```py\nlength = 100\ntf.random.set_seed(42)\ntrain_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n                       seed=42)\nvalid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\ntest_set = to_dataset(encoded[1_060_000:], length=length)\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n    tf.keras.layers.GRU(128, return_sequences=True),\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(\n    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\nhistory = model.fit(train_set, validation_data=valid_set, epochs=10,\n                    callbacks=[model_ckpt])\n```", "```py\nshakespeare_model = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n    model\n])\n```", "```py\n>>> y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n>>> y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n>>> text_vec_layer.get_vocabulary()[y_pred + 2]\n'e'\n```", "```py\n>>> log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n>>> tf.random.set_seed(42)\n>>> tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples\n<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>\n```", "```py\ndef next_char(text, temperature=1):\n    y_proba = shakespeare_model.predict([text])[0, -1:]\n    rescaled_logits = tf.math.log(y_proba) / temperature\n    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n    return text_vec_layer.get_vocabulary()[char_id + 2]\n```", "```py\ndef extend_text(text, n_chars=50, temperature=1):\n    for _ in range(n_chars):\n        text += next_char(text, temperature)\n    return text\n```", "```py\n>>> tf.random.set_seed(42)\n>>> print(extend_text(\"To be or not to be\", temperature=0.01))\nTo be or not to be the duke\nas it is a proper strange death,\nand the\n>>> print(extend_text(\"To be or not to be\", temperature=1))\nTo be or not to behold?\n\nsecond push:\ngremio, lord all, a sistermen,\n>>> print(extend_text(\"To be or not to be\", temperature=100))\nTo be or not to bef ,mt'&o3fpadm!$\nwh!nse?bws3est--vgerdjw?c-y-ewznq\n```", "```py\ndef to_dataset_for_stateful_rnn(sequence, length):\n    ds = tf.data.Dataset.from_tensor_slices(sequence)\n    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n\nstateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\nstateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],\n                                                 length)\nstateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n                              batch_input_shape=[1, None]),\n    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n])\n```", "```py\nclass ResetStatesCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()\n```", "```py\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])\n```", "```py\nimport tensorflow_datasets as tfds\n\nraw_train_set, raw_valid_set, raw_test_set = tfds.load(\n    name=\"imdb_reviews\",\n    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n    as_supervised=True\n)\ntf.random.set_seed(42)\ntrain_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\nvalid_set = raw_valid_set.batch(32).prefetch(1)\ntest_set = raw_test_set.batch(32).prefetch(1)\n```", "```py\n>>> for review, label in raw_train_set.take(4):\n...     print(review.numpy().decode(\"utf-8\"))\n...     print(\"Label:\", label.numpy())\n...\nThis was an absolutely terrible movie. Don't be lured in by Christopher [...]\nLabel: 0\nI have been known to fall asleep during films, but this is usually due to [...]\nLabel: 0\nMann photographs the Alberta Rocky Mountains in a superb fashion, and [...]\nLabel: 0\nThis is the kind of film for a snowy Sunday afternoon when the rest of the [...]\nLabel: 1\n```", "```py\nvocab_size = 1000\ntext_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\ntext_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))\n```", "```py\nembed_size = 128\ntf.random.set_seed(42)\nmodel = tf.keras.Sequential([\n    text_vec_layer,\n    tf.keras.layers.Embedding(vocab_size, embed_size),\n    tf.keras.layers.GRU(128),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nhistory = model.fit(train_set, validation_data=valid_set, epochs=2)\n```", "```py\ninputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\ntoken_ids = text_vec_layer(inputs)\nmask = tf.math.not_equal(token_ids, 0)\nZ = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\nZ = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n```", "```py\n>>> text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n...     max_tokens=vocab_size, ragged=True)\n...\n>>> text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n>>> text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])\n<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>\n```", "```py\n>>> text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])\n<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\narray([[ 86,  18,   0,   0,   0],\n [ 11,   7,   1, 116, 217]])>\n```", "```py\nimport os\nimport tensorflow_hub as hub\n\nos.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\nmodel = tf.keras.Sequential([\n    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n                   trainable=True, dtype=tf.string, input_shape=[]),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit(train_set, validation_data=valid_set, epochs=10)\n```", "```py\nurl = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\npath = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n                               extract=True)\ntext = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()\n```", "```py\nimport numpy as np\n\ntext = text.replace(\"\u00a1\", \"\").replace(\"\u00bf\", \"\")\npairs = [line.split(\"\\t\") for line in text.splitlines()]\nnp.random.shuffle(pairs)\nsentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists\n```", "```py\n>>> for i in range(3):\n...     print(sentences_en[i], \"=>\", sentences_es[i])\n...\nHow boring! => Qu\u00e9 aburrimiento!\nI love sports. => Adoro el deporte.\nWould you like to swap jobs? => Te gustar\u00eda que intercambiemos los trabajos?\n```", "```py\nvocab_size = 1000\nmax_length = 50\ntext_vec_layer_en = tf.keras.layers.TextVectorization(\n    vocab_size, output_sequence_length=max_length)\ntext_vec_layer_es = tf.keras.layers.TextVectorization(\n    vocab_size, output_sequence_length=max_length)\ntext_vec_layer_en.adapt(sentences_en)\ntext_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\n```", "```py\n>>> text_vec_layer_en.get_vocabulary()[:10]\n['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n>>> text_vec_layer_es.get_vocabulary()[:10]\n['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']\n```", "```py\nX_train = tf.constant(sentences_en[:100_000])\nX_valid = tf.constant(sentences_en[100_000:])\nX_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\nX_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\nY_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\nY_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])\n```", "```py\nencoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\ndecoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n```", "```py\nembed_size = 128\nencoder_input_ids = text_vec_layer_en(encoder_inputs)\ndecoder_input_ids = text_vec_layer_es(decoder_inputs)\nencoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n                                                    mask_zero=True)\ndecoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n                                                    mask_zero=True)\nencoder_embeddings = encoder_embedding_layer(encoder_input_ids)\ndecoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n```", "```py\nencoder = tf.keras.layers.LSTM(512, return_state=True)\nencoder_outputs, *encoder_state = encoder(encoder_embeddings)\n```", "```py\ndecoder = tf.keras.layers.LSTM(512, return_sequences=True)\ndecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n```", "```py\noutput_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\nY_proba = output_layer(decoder_outputs)\n```", "```py\nmodel = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))\n```", "```py\ndef translate(sentence_en):\n    translation = \"\"\n    for word_idx in range(max_length):\n        X = np.array([sentence_en])  # encoder input\n        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n        predicted_word_id = np.argmax(y_proba)\n        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n        if predicted_word == \"endofseq\":\n            break\n        translation += \" \" + predicted_word\n    return translation.strip()\n```", "```py\n>>> translate(\"I like soccer\")\n'me gusta el f\u00fatbol'\n```", "```py\n>>> translate(\"I like soccer and also going to the beach\")\n'me gusta el f\u00fatbol y a veces mismo al bus'\n```", "```py\nencoder = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(256, return_state=True))\n```", "```py\nencoder_outputs, *encoder_state = encoder(encoder_embeddings)\nencoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n```", "```py\nencoder = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))\n```", "```py\nattention_layer = tf.keras.layers.Attention()\nattention_outputs = attention_layer([decoder_outputs, encoder_outputs])\noutput_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\nY_proba = output_layer(attention_outputs)\n```", "```py\n>>> translate(\"I like soccer and also going to the beach\")\n'me gusta el f\u00fatbol y tambi\u00e9n ir a la playa'\n```", "```py\nmax_length = 50  # max length in the whole training set\nembed_size = 128\npos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\nbatch_max_len_enc = tf.shape(encoder_embeddings)[1]\nencoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\nbatch_max_len_dec = tf.shape(decoder_embeddings)[1]\ndecoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))\n```", "```py\nclass PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert embed_size % 2 == 0, \"embed_size must be even\"\n        p, i = np.meshgrid(np.arange(max_length),\n                           2 * np.arange(embed_size // 2))\n        pos_emb = np.empty((1, max_length, embed_size))\n        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n        self.supports_masking = True\n\n    def call(self, inputs):\n        batch_max_length = tf.shape(inputs)[1]\n        return inputs + self.pos_encodings[:, :batch_max_length]\n```", "```py\npos_embed_layer = PositionalEncoding(max_length, embed_size)\nencoder_in = pos_embed_layer(encoder_embeddings)\ndecoder_in = pos_embed_layer(decoder_embeddings)\n```", "```py\nN = 2  # instead of 6\nnum_heads = 8\ndropout_rate = 0.1\nn_units = 128  # for the first dense layer in each feedforward block\nencoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\nZ = encoder_in\nfor _ in range(N):\n    skip = Z\n    attn_layer = tf.keras.layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n    skip = Z\n    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n    Z = tf.keras.layers.Dense(embed_size)(Z)\n    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n```", "```py\ndecoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\ncausal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)\n```", "```py\nencoder_outputs = Z  # let's save the encoder's final outputs\nZ = decoder_in  # the decoder starts with its own inputs\nfor _ in range(N):\n    skip = Z\n    attn_layer = tf.keras.layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n    skip = Z\n    attn_layer = tf.keras.layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n    skip = Z\n    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n    Z = tf.keras.layers.Dense(embed_size)(Z)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n```", "```py\nY_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\nmodel = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))\n```", "```py\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\")  # many other tasks are available\nresult = classifier(\"The actors were very convincing\".)\n```", "```py\n>>> result\n[{'label': 'POSITIVE', 'score': 0.9998071789741516}]\n```", "```py\n>>> classifier([\"I am from India.\", \"I am from Iraq.\"])\n[{'label': 'POSITIVE', 'score': 0.9896161556243896},\n {'label': 'NEGATIVE', 'score': 0.9811071157455444}]\n```", "```py\n>>> model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n>>> classifier_mnli = pipeline(\"text-classification\", model=model_name)\n>>> classifier_mnli(\"She loves me. [SEP] She loves me not.\")\n[{'label': 'contradiction', 'score': 0.9790192246437073}]\n```", "```py\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n```", "```py\ntoken_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n                      padding=True, return_tensors=\"tf\")\n```", "```py\n>>> token_ids\n{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\narray([[ 101, 1045, 2066, 4715, 1012,  102, 2057, 2035, 2293, 4715,  999,\n 102,    0,    0,    0],\n [ 101, 3533, 2973, 2005, 1037, 2200, 2146, 2051, 1012,  102, 3533,\n 2003, 2214, 1012,  102]], dtype=int32)>,\n 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n```", "```py\n>>> outputs = model(token_ids)\n>>> outputs\nTFSequenceClassifierOutput(loss=None, logits=[<tf.Tensor: [...] numpy=\narray([[-2.1123817 ,  1.1786783 ,  1.4101017 ],\n [-0.01478387,  1.0962474 , -0.9919954 ]], dtype=float32)>], [...])\n```", "```py\n>>> Y_probas = tf.keras.activations.softmax(outputs.logits)\n>>> Y_probas\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.01619702, 0.43523544, 0.5485676 ],\n [0.08672056, 0.85204804, 0.06123142]], dtype=float32)>\n>>> Y_pred = tf.argmax(Y_probas, axis=1)\n>>> Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral\n<tf.Tensor: shape=(2,), dtype=int64, numpy=array([2, 1])>\n```", "```py\nsentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\nX_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\ny_train = tf.constant([0, 2])  # contradiction, neutral\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=2)\n```"]