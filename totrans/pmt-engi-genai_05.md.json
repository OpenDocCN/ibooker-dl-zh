["```py\n[-0.011904156766831875,\n -0.0323905423283577,\n 0.001950666424818337,\n...]\n```", "```py\n## Context\nMost relevant previous user messages:\n1\\. \"My name is Mike\".\n2\\. \"My dog's name is Hercules\".\n3\\. \"My coworker's name is James\".\n\n## Instructions\nPlease answer the user message using the context above.\nUser message: What is my name?\nAI message:\n```", "```py\nfrom openai import OpenAI\nclient = OpenAI()\n\n# Function to get the vector embedding for a given text\ndef get_vector_embeddings(text):\n    response = client.embeddings.create(\n        input=text,\n        model=\"text-embedding-ada-002\"\n    )\n    embeddings = [r.embedding for r in response.data]\n    return embeddings[0]\n\nget_vector_embeddings(\"Your text string goes here\")\n```", "```py\n[\n-0.006929283495992422,\n-0.005336422007530928,\n...\n-4.547132266452536e-05,\n-0.024047505110502243\n]\n```", "```py\nimport requests\nimport os\n\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\"\nhf_token = os.getenv(\"HF_TOKEN\")\n\napi_url = \"https://api-inference.huggingface.co/\"\napi_url += f\"pipeline/feature-extraction/{model_id}\"\nheaders = {\"Authorization\": f\"Bearer {hf_token}\"}\n\ndef query(texts):\n    response = requests.post(api_url, headers=headers,\n    json={\"inputs\": texts,\n    \"options\":{\"wait_for_model\":True}})\n    return response.json()\n\ntexts = [\"mickey mouse\",\n        \"cheese\",\n        \"trap\",\n        \"rat\",\n        \"ratatouille\"\n        \"bus\",\n        \"airplane\",\n        \"ship\"]\n\noutput = query(texts)\noutput\n```", "```py\n[[-0.03875632584095001, 0.04480459913611412,\n0.016051070764660835, -0.01789097487926483,\n-0.03518553078174591, -0.013002964667975903,\n0.14877274632453918, 0.048807501792907715,\n0.011848390102386475, -0.044042471796274185,\n...\n-0.026688814163208008, -0.0359361357986927,\n-0.03237859532237053, 0.008156519383192062,\n-0.10299170762300491, 0.0790356695652008,\n-0.008071334101259708, 0.11919838190078735,\n0.0005506130401045084, -0.03497892618179321]]\n```", "```py\nfrom gensim.models import Word2Vec\n\n# Sample data: list of sentences, where each sentence is\n# a list of words.\n# In a real-world scenario, you'd load and preprocess your\n# own corpus.\nsentences = [\n    [\"the\", \"cake\", \"is\", \"a\", \"lie\"],\n    [\"if\", \"you\", \"hear\", \"a\", \"turret\", \"sing\", \"you're\",\n    \"probably\", \"too\", \"close\"],\n    [\"why\", \"search\", \"for\", \"the\", \"end\", \"of\", \"a\",\n    \"rainbow\", \"when\", \"the\", \"cake\", \"is\", \"a\", \"lie?\"],\n    # ...\n    [\"there's\", \"no\", \"cake\", \"in\", \"space,\", \"just\", \"ask\",\n    \"wheatley\"],\n    [\"completing\", \"tests\", \"for\", \"cake\", \"is\", \"the\",\n    \"sweetest\", \"lie\"],\n    [\"I\", \"swapped\", \"the\", \"cake\", \"recipe\", \"with\", \"a\",\n    \"neurotoxin\", \"formula,\", \"hope\", \"that's\", \"fine\"],\n] + [\n    [\"the\", \"cake\", \"is\", \"a\", \"lie\"],\n    [\"the\", \"cake\", \"is\", \"definitely\", \"a\", \"lie\"],\n    [\"everyone\", \"knows\", \"that\", \"cake\", \"equals\", \"lie\"],\n    # ...\n] * 10  # repeat several times to emphasize\n\n# Train the word2vec model\nmodel =  Word2Vec(sentences, vector_size=100, window=5,\nmin_count=1, workers=4, seed=36)\n\n# Save the model\nmodel.save(\"custom_word2vec_model.model\")\n\n# To load the model later\n# loaded_model = word2vec.load(\n# \"custom_word2vec_model.model\")\n\n# Get vector for a word\nvector = model.wv['cake']\n\n# Find most similar words\nsimilar_words = model.wv.most_similar(\"cake\", topn=5)\nprint(\"Top five most similar words to 'cake': \", similar_words)\n\n# Directly query the similarity between \"cake\" and \"lie\"\ncake_lie_similarity = model.wv.similarity(\"cake\", \"lie\")\nprint(\"Similarity between 'cake' and 'lie': \",\ncake_lie_similarity)\n```", "```py\nTop 5 most similar words to 'cake':  [('lie',\n0.23420444130897522), ('test', 0.23205122351646423),\n('tests', 0.17178669571876526), ('GLaDOS',\n0.1536172330379486), ('got', 0.14605288207530975)]\nSimilarity between 'cake' and 'lie':  0.23420444\n```", "```py\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Convert sentences to a list of strings for TfidfVectorizer\ndocument_list = [' '.join(s) for s in sentences]\n\n# Compute TF-IDF representation\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(document_list)\n\n# Extract the position of the words \"cake\" and \"lie\" in\n# the feature matrix\ncake_idx = vectorizer.vocabulary_['cake']\nlie_idx = vectorizer.vocabulary_['lie']\n\n# Extract and reshape the vector for 'cake'\ncakevec = tfidf_matrix[:, cake_idx].toarray().reshape(1, -1)\n\n# Compute the cosine similarities\nsimilar_words = cosine_similarity(cakevec, tfidf_matrix.T).flatten()\n\n# Get the indices of the top 6 most similar words\n# (including 'cake')\ntop_indices = np.argsort(similar_words)[-6:-1][::-1]\n\n# Retrieve and print the top 5 most similar words to\n# 'cake' (excluding 'cake' itself)\nnames = []\nfor idx in top_indices:\n    names.append(vectorizer.get_feature_names_out()[idx])\nprint(\"Top five most similar words to 'cake': \", names)\n\n# Compute cosine similarity between \"cake\" and \"lie\"\nsimilarity = cosine_similarity(np.asarray(tfidf_matrix[:,\n    cake_idx].todense()), np.asarray(tfidf_matrix[:, lie_idx].todense()))\n# The result will be a matrix; we can take the average or\n# max similarity value\navg_similarity = similarity.mean()\nprint(\"Similarity between 'cake' and 'lie'\", avg_similarity)\n\n# Show the similarity between \"cake\" and \"elephant\"\nelephant_idx = vectorizer.vocabulary_['sing']\nsimilarity = cosine_similarity(np.asarray(tfidf_matrix[:,\n    cake_idx].todense()), np.asarray(tfidf_matrix[:,\n    elephant_idx].todense()))\navg_similarity = similarity.mean()\nprint(\"Similarity between 'cake' and 'sing'\",\n    avg_similarity)\n```", "```py\nTop 5 most similar words to 'cake':  ['lie', 'the', 'is',\n'you', 'definitely']\nSimilarity between 'cake' and 'lie' 0.8926458157227388\nSimilarity between 'cake' and 'sing' 0.010626735901461177\n```", "```py\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, # 100 tokens\n    chunk_overlap=20, #Â 20 tokens of overlap\n    )\n\ntext = \"\"\"\nWelcome to the \"Unicorn Enterprises: Where Magic Happens\"\nEmployee Handbook! We're thrilled to have you join our team\nof dreamers, doers, and unicorn enthusiasts. At Unicorn\nEnterprises, we believe that work should be as enchanting as\nit is productive. This handbook is your ticket to the\nmagical world of our company, where we'll outline the\nprinciples, policies, and practices that guide us on this\nextraordinary journey. So, fasten your seatbelts and get\nready to embark on an adventure like no other!\n\n...\n\nAs we conclude this handbook, remember that at Unicorn\nEnterprises, the pursuit of excellence is a never-ending\nquest. Our company's success depends on your passion,\ncreativity, and commitment to making the impossible\npossible. We encourage you to always embrace the magic\nwithin and outside of work, and to share your ideas and\ninnovations to keep our enchanted journey going. Thank you\nfor being a part of our mystical family, and together, we'll\ncontinue to create a world where magic and business thrive\nhand in hand!\n\"\"\"\n\nchunks = text_splitter.split_text(text=text)\nprint(chunks[0:3])\n```", "```py\n['Welcome to the \"Unicorn Enterprises: Where Magic Happens\"\nEmployee Handbook! We\\'re thrilled to have you join our team\nof dreamers, doers, and unicorn enthusiasts.',\n\"We're thrilled to have you join our team of dreamers,\ndoers, and unicorn enthusiasts. At Unicorn Enterprises, we\nbelieve that work should be as enchanting as it is\nproductive.\",\n ...\n\"Our company's success depends on your passion, creativity,\nand commitment to making the impossible possible. We\nencourage you to always embrace the magic within and outside\nof work, and to share your ideas and innovations to keep our\nenchanted journey going.\",\n\"We encourage you to always embrace the magic within and\noutside of work, and to share your ideas and innovations to\nkeep our enchanted journey going. Thank you for being a part\nof our mystical family, and together, we'll continue to\ncreate a world where magic and business thrive hand in\nhand!\"]\n```", "```py\nimport numpy as np\nimport faiss\n\n#  The get_vector_embeddings function is defined in a preceding example\nemb = [get_vector_embeddings(chunk) for chunk in chunks]\nvectors = np.array(emb)\n\n# Create a FAISS index\nindex = faiss.IndexFlatL2(vectors.shape[1])\nindex.add(vectors)\n\n# Function to perform a vector search\ndef vector_search(query_text, k=1):\n    query_vector = get_vector_embeddings(query_text)\n    distances, indices = index.search(\n        np.array([query_vector]), k)\n    return [(chunks[i], float(dist)) for dist,\n        i in zip(distances[0], indices[0])]\n\n# Example search\nuser_query = \"do we get free unicorn rides?\"\nsearch_results = vector_search(user_query)\nprint(f\"Search results for {user_query}:\", search_results)\n```", "```py\nSearch results for do we get free unicorn rides?: [(\"You'll\nenjoy a treasure chest of perks, including unlimited unicorn\nrides, a bottomless cauldron of coffee and potions, and\naccess to our company library filled with spellbinding\nbooks. We also offer competitive health and dental plans,\nensuring your physical well-being is as robust as your\nmagical spirit.\\n\\n**5: Continuous Learning and\nGrowth**\\n\\nAt Unicorn Enterprises, we believe in continuous\nlearning and growth.\", 0.3289167582988739)]\n```", "```py\n# Function to perform a vector search and then ask # GPT-3.5-turbo a question\ndef search_and_chat(user_query, k=1):\n  # Perform the vector search\n  search_results = vector_search(user_query, k)\n  print(f\"Search results: {search_results}\\n\\n\")\n\n  prompt_with_context = f\"\"\"Context:{search_results}\\\n Answer the question: {user_query}\"\"\"\n\n  # Create a list of messages for the chat\n  messages = [\n      {\"role\": \"system\", \"content\": \"\"\"Please answer the\n questions provided by the user. Use only the context\n provided to you to respond to the user, if you don't\n know the answer say \\\"I don't know\\\".\"\"\"},\n      {\"role\": \"user\", \"content\": prompt_with_context},\n  ]\n\n  # Get the model's response\n  response = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\", messages=messages)\n\n  # Print the assistant's reply\n  print(f\"\"\"Response:\n  {response.choices[0].message.content}\"\"\")\n\n# Example search and chat\nsearch_and_chat(\"What is Unicorn Enterprises' mission?\")\n```", "```py\nSearch results: [(\"\"\"As we conclude this handbook, remember that at\nUnicorn Enterprises, the pursuit of excellence is a never-ending\nquest. Our company's success depends on your passion,\ncreativity, and commitment to making the impossible\npossible. We encourage you to always embrace the magic\nwithin and outside of work, and to share your ideas and\ninnovations to keep our enchanted journey going. Thank you\",\n0.26446571946144104)]\n\nResponse:\nUnicorn Enterprises' mission is to pursue excellence in their\nwork by encouraging their employees to embrace the magic within\nand outside of work, share their ideas and innovations, and make\nthe impossible possible.\n```", "```py\n# Save the index to a file\nfaiss.write_index(index, \"data/my_index_file.index\")\n```", "```py\n# Load the index from a file\nindex = faiss.read_index(\"data/my_index_file.index\")\n```", "```py\n# Assuming index1 and index2 are two IndexFlatL2 indices\nindex1.add(index2.reconstruct_n(0, index2.ntotal))\n```", "```py\nfrom langchain_community.vectorstores.faiss import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# 1\\. Create the documents:\ndocuments = [\n    \"James Phoenix worked at JustUnderstandingData.\",\n    \"James Phoenix currently is 31 years old.\",\n    \"\"\"Data engineering is the designing and building systems for collecting,\n storing, and analyzing data at scale.\"\"\",\n]\n\n# 2\\. Create a vectorstore:\nvectorstore = FAISS.from_texts(texts=documents, embedding=OpenAIEmbeddings())\nretriever = vectorstore.as_retriever()\n\n# 3\\. Create a prompt:\ntemplate = \"\"\"Answer the question based only on the following context:\n---\nContext: {context} `---`\n`Question:` `{question}` ```", "```py\n```", "```py`` ```", "```py chain = (     {\"context\": retriever, \"question\": RunnablePassthrough()}     | prompt     | model     | StrOutputParser() ) ```", "```py chain.invoke(\"What is data engineering?\") # 'Data engineering is the process of designing and building systems for # collecting, storing, and analyzing data at scale.'  chain.invoke(\"Who is James Phoenix?\") # 'Based on the given context, James Phoenix is a 31-year-old individual who # worked at JustUnderstandingData.'  chain.invoke(\"What is the president of the US?\") # I don't know ```", "```py` ```", "```py`` ```", "```py from pinecone import Pinecone, ServerlessSpec import os  # Initialize connection (get API key at app.pinecone.io): os.environ[\"PINECONE_API_KEY\"] = \"insert-your-api-key-here\"  index_name = \"employee-handbook\" environment = \"us-west-2\" pc = Pinecone()  # This reads the PINECONE_API_KEY env var  # Check if index already exists: # (it shouldn't if this is first time) if index_name not in pc.list_indexes().names():     # if does not exist, create index     pc.create_index(         index_name,         # Using the same vector dimensions as text-embedding-ada-002         dimension=1536,         metric=\"cosine\",         spec=ServerlessSpec(cloud=\"aws\", region=environment),     )  # Connect to index: index = pc.Index(index_name)  # View index stats: index.describe_index_stats() ```", "```py {'dimension': 1536,  'index_fullness': 0.0,  'namespaces': {},  'total_vector_count': 0} ```", "```py from tqdm import tqdm # For printing a progress bar from time import sleep  # How many embeddings you create and insert at once batch_size = 10 retry_limit = 5  # maximum number of retries  for i in tqdm(range(0, len(chunks), batch_size)):     # Find end of batch     i_end = min(len(chunks), i+batch_size)     meta_batch = chunks[i:i_end]     # Get ids     ids_batch = [str(j) for j in range(i, i_end)]     # Get texts to encode     texts = [x for x in meta_batch]     # Create embeddings     # (try-except added to avoid RateLimitError)     done = False     try:         # Retrieve embeddings for the whole batch at once         embeds = []         for text in texts:             embedding = get_vector_embeddings(text)             embeds.append(embedding)         done = True     except:         retry_count = 0         while not done and retry_count < retry_limit:             try:                 for text in texts:                     embedding = get_vector_embeddings(text)                     embeds.append(embedding)                 done = True             except:                 sleep(5)                 retry_count += 1      if not done:         print(f\"\"\"Failed to get embeddings after         {retry_limit} retries.\"\"\")         continue      # Cleanup metadata     meta_batch = [{         'batch': i,         'text': x     } for x in meta_batch]     to_upsert = list(zip(ids_batch, embeds, meta_batch))      # Upsert to Pinecone     index.upsert(vectors=to_upsert) ```", "```py 100% 13/13 [00:53<00:00, 3.34s/it] ```", "```py # Retrieve from Pinecone user_query = \"do we get free unicorn rides?\"  def pinecone_vector_search(user_query, k):     xq = get_vector_embeddings(user_query)     res = index.query(vector=xq, top_k=k, include_metadata=True)     return res  pinecone_vector_search(user_query, k=1) ```", "```py {'matches':     [{'id': '15',     'metadata': {'batch': 10.0,     'text': \"You'll enjoy a treasure chest of perks, \"             'including unlimited unicorn rides, a '             'bottomless cauldron of coffee and potions, '             'and access to our company library filled '             'with spellbinding books. We also offer '             'competitive health and dental plans, '             'ensuring your physical well-being is as '             'robust as your magical spirit.\\n'             '\\n'             '**5: Continuous Learning and Growth**\\n'             '\\n'             'At Unicorn Enterprises, we believe in '             'continuous learning and growth.'},     'score': 0.835591,     'values': []},],  'namespace': ''} ```", "```py res = index.query(xq, filter={         \"batch\": {\"$eq\": 1}     }, top_k=1, include_metadata=True) ```", "```py from langchain_core.documents import Document from langchain_community.vectorstores.chroma import Chroma from langchain_openai import OpenAIEmbeddings import lark import getpass import os import warnings  # Disabling warnings: warnings.filterwarnings(\"ignore\") ```", "```py docs = [     Document(         page_content=\"A tale about a young wizard and his \\  journey in a magical school.\",         metadata={             \"title\": \"Harry Potter and the Philosopher's Stone\",             \"author\": \"J.K. Rowling\",             \"year_published\": 1997,             \"genre\": \"Fiction\",             \"isbn\": \"978-0747532699\",             \"publisher\": \"Bloomsbury\",             \"language\": \"English\",             \"page_count\": 223,             \"summary\": \"The first book in the Harry Potter \\  series where Harry discovers his magical \\  heritage.\",             \"rating\": 4.8,         },     ),     # ... More documents ... ] ```", "```py from langchain_openai.chat_models import ChatOpenAI from langchain.retrievers.self_query.base \\     import SelfQueryRetriever from langchain.chains.query_constructor.base \\     import AttributeInfo  # Create the embeddings and vectorstore: embeddings = OpenAIEmbeddings() vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())  # Basic Info basic_info = [     AttributeInfo(name=\"title\", description=\"The title of the book\",     type=\"string\"),     AttributeInfo(name=\"author\", description=\"The author of the book\",     type=\"string\"),     AttributeInfo(         name=\"year_published\",         description=\"The year the book was published\",         type=\"integer\",     ), ]  # Detailed Info detailed_info = [     AttributeInfo(         name=\"genre\", description=\"The genre of the book\",         type=\"string or list[string]\"     ),     AttributeInfo(         name=\"isbn\",         description=\"The International Standard Book Number for the book\",         type=\"string\",     ),     AttributeInfo(         name=\"publisher\",         description=\"The publishing house that published the book\",         type=\"string\",     ),     AttributeInfo(         name=\"language\",         description=\"The primary language the book is written in\",         type=\"string\",     ),     AttributeInfo(         name=\"page_count\", description=\"Number of pages in the book\",         type=\"integer\"     ), ]  # Analysis analysis = [     AttributeInfo(         name=\"summary\",         description=\"A brief summary or description of the book\",         type=\"string\",     ),     AttributeInfo(         name=\"rating\",         description=\"\"\"An average rating for the book (from reviews), ranging  from 1-5\"\"\",         type=\"float\",     ), ]  # Combining all lists into metadata_field_info metadata_field_info = basic_info + detailed_info + analysis ```", "```py document_content_description = \"Brief summary of a movie\" llm = ChatOpenAI(temperature=0) retriever = SelfQueryRetriever.from_llm(     llm, vectorstore, document_content_description, metadata_field_info )  # Looking for sci-fi books retriever.invoke(\"What are some sci-fi books?\") # [Document(page_content='''A futuristic society where firemen burn books to # maintain order.''', metadata={'author': 'Ray Bradbury', 'genre': '... # More documents..., truncated for brevity ```", "```py # Querying for a book by J.K. Rowling: retriever.invoke(     '''I want some books that are published by the  author J.K.Rowling''' ) # query=' ' filter=Comparison(comparator=<Comparator.EQ: # 'eq'>, attribute='author', value='J.K. Rowling') # limit=None # Documents [] omitted to save space ```", "```py retriever = SelfQueryRetriever.from_llm(     llm,     vectorstore,     document_content_description,     metadata_field_info,     enable_limit=True, )  retriever.get_relevant_documents(     query=\"Return 2 Fantasy books\", ) # query=' ' filter=Comparison( #    comparator=<Comparator.EQ: 'eq'>, attribute='genre', #   value='Fantasy') limit=2 # Documents [] omitted to save space ```", "```py` ```"]