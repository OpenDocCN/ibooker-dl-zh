- en: Chapter 1\. LLM Fundamentals with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [Preface](preface01.html#pr01_preface_1736545679069216) gave you a taste
    of the power of LLM prompting, where we saw firsthand the impact that different
    prompting techniques can have on what you get out of LLMs, especially when judiciously
    combined. The challenge in building good LLM applications is, in fact, in how
    to effectively construct the prompt sent to the model and process the model’s
    prediction to return an accurate output (see [Figure 1-1](#ch01_figure_1_1736545659763063)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lelc_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The challenge in making LLMs a useful part of your application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you can solve this problem, you are well on your way to building LLM applications,
    simple and complex alike. In this chapter, you’ll learn more about how LangChain’s
    building blocks map to LLM concepts and how, when combined effectively, they enable
    you to build LLM applications. But first, the sidebar [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    is a brief primer on why we think it useful to use LangChain to build LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Set Up with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the rest of the chapter, and the chapters to come, we recommend
    setting up LangChain on your computer first.
  prefs: []
  type: TYPE_NORMAL
- en: See the instructions in the [Preface](preface01.html#pr01_preface_1736545679069216)
    regarding setting up an OpenAI account and complete these if you haven’t yet.
    If you prefer using a different LLM provider, see [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    for alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Then head over to the [API Keys page](https://oreil.ly/BKrtV) on the OpenAI
    website (after logging in to your OpenAI account), create an API key, and save
    it—you’ll need it soon.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this book, we’ll show code examples in both Python and JavaScript (JS). LangChain
    offers the same functionality in both languages, so just pick the one you’re most
    comfortable with and follow the respective code snippets throughout the book (the
    code examples for each language are equivalent).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, some setup instructions for readers using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have Python installed. See the [instructions for your operating
    system](https://oreil.ly/20K9l).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Jupyter if you want to run the examples in a notebook environment. You
    can do this by running `pip install notebook` in your terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the LangChain library by running the following commands in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook by running this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You’re now ready to follow along with the Python code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the instructions for readers using JavaScript:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to run the examples as Node.js scripts, install Node by following
    the [instructions](https://oreil.ly/5gjiO).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the LangChain libraries by running the following commands in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Take each example, save it as a *.js* file and run it with `node ./file.js`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using LLMs in LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To recap, LLMs are the driving engine behind most generative AI applications.
    LangChain provides two simple interfaces to interact with any LLM API provider:'
  prefs: []
  type: TYPE_NORMAL
- en: Chat models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM interface simply takes a string prompt as input, sends the input to
    the model provider, and then returns the model prediction as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import LangChain’s OpenAI LLM wrapper to `invoke` a model prediction
    using a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice the parameter `model` passed to `OpenAI`. This is the most common parameter
    to configure when using an LLM or chat model, the underlying model to use, as
    most providers offer several models with different trade-offs in capability and
    cost (usually larger models are more capable, but also more expensive and slower).
    See [OpenAI’s overview](https://oreil.ly/dM886) of the models they offer.
  prefs: []
  type: TYPE_NORMAL
- en: Other useful parameters to configure include the following, offered by most
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This controls the sampling algorithm used to generate output. Lower values
    produce more predictable outputs (for example, 0.1), while higher values generate
    more creative, or unexpected, results (such as 0.9). Different tasks will need
    different values for this parameter. For instance, producing structured output
    usually benefits from a lower temperature, whereas creative writing tasks do better
    with a higher value:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: This limits the size (and cost) of the output. A lower value may cause the LLM
    to stop generating the output before getting to a natural end, so it may appear
    to have been truncated.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these, each provider exposes a different set of parameters. We recommend
    looking at the documentation for the one you choose. For an example, refer to
    [OpenAI’s platform](https://oreil.ly/5O1RW).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, the chat model interface enables back and forth conversations
    between the user and model. The reason why it’s a separate interface is because
    popular LLM providers like OpenAI differentiate messages sent to and from the
    model into *user*, *assistant*, and *system* roles (here *role* denotes the type
    of content the message contains):'
  prefs: []
  type: TYPE_NORMAL
- en: System role
  prefs: []
  type: TYPE_NORMAL
- en: Used for instructions the model should use to answer a user question
  prefs: []
  type: TYPE_NORMAL
- en: User role
  prefs: []
  type: TYPE_NORMAL
- en: Used for the user’s query and any other content produced by the user
  prefs: []
  type: TYPE_NORMAL
- en: Assistant role
  prefs: []
  type: TYPE_NORMAL
- en: Used for content generated by the model
  prefs: []
  type: TYPE_NORMAL
- en: 'The chat model’s interface makes it easier to configure and manage conversions
    in your AI chatbot application. Here’s an example utilizing LangChain’s ChatOpenAI
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of a single prompt string, chat models make use of different types
    of chat message interfaces associated with each role mentioned previously. These
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HumanMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message sent from the perspective of the human, with the user role
  prefs: []
  type: TYPE_NORMAL
- en: '`AIMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message sent from the perspective of the AI that the human is interacting
    with, with the assistant role
  prefs: []
  type: TYPE_NORMAL
- en: '`SystemMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message setting the instructions the AI should follow, with the system role
  prefs: []
  type: TYPE_NORMAL
- en: '`ChatMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message allowing for arbitrary setting of role
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s incorporate a `SystemMessage` instruction in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model obeyed the instruction provided in the `SystemMessage`
    even though it wasn’t present in the user’s question. This enables you to preconfigure
    your AI application to respond in a relatively predictable manner based on the
    user’s input.
  prefs: []
  type: TYPE_NORMAL
- en: Making LLM Prompts Reusable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section showed how the `prompt` instruction significantly influences
    the model’s output. Prompts help the model understand context and generate relevant
    answers to queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a detailed prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Although the prompt looks like a simple string, the challenge is figuring out
    what the text should contain and how it should vary based on the user’s input.
    In this example, the Context and Question values are hardcoded, but what if we
    wanted to pass these in dynamically?
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, LangChain provides prompt template interfaces that make it easy
    to construct prompts with dynamic inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaS**cript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This example takes the static prompt from the previous block and makes it dynamic.
    The `template` contains the structure of the final prompt alongside the definition
    of where the dynamic inputs will be inserted.
  prefs: []
  type: TYPE_NORMAL
- en: As such, the template can be used as a recipe to build multiple static, specific
    prompts. When you format the prompt with some specific values—in this case, `context`
    and `question`—you get a static prompt ready to be passed in to an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the `question` argument is passed dynamically via the `invoke`
    function. By default, LangChain prompts follow Python’s `f-string` syntax for
    defining dynamic parameters—any word surrounded by curly braces, such as `{question}`,
    are placeholders for values passed in at runtime. In the previous example, `{question}`
    was replaced by `“Which model providers offer LLMs?”`
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we’d feed this into an LLM OpenAI model using LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re looking to build an AI chat application, the `ChatPromptTemplate`
    can be used instead to provide dynamic inputs based on the role of the chat message:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how the prompt contains instructions in a `SystemMessage` and two instances
    of `HumanMessage` that contain dynamic `context` and `question` variables. You
    can still format the template in the same way and get back a static prompt that
    you can pass to a large language model for a prediction output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Getting Specific Formats out of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Plain text outputs are useful, but there may be use cases where you need the
    LLM to generate a *structured* output—that is, output in a machine-readable format,
    such as JSON, XML, CSV, or even in a programming language such as Python or JavaScript.
    This is very useful when you intend to hand that output off to some other piece
    of code, making an LLM play a part in your larger application.
  prefs: []
  type: TYPE_NORMAL
- en: JSON Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common format to generate with LLMs is JSON. JSON outputs can (for
    example) be sent over the wire to your frontend code or be saved to a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'When generating JSON, the first task is to define the schema you want the LLM
    to respect when producing the output. Then, you should include that schema in
    the prompt, along with the text you want to use as the source. Let’s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'So, first define a schema. In Python, this is easiest to do with Pydantic (a
    library used for validating data against schemas). In JS, this is easiest to do
    with Zod (an equivalent library). The method `with_structured_output` will use
    that schema for two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The schema will be converted to a `JSONSchema` object (a JSON format used to
    describe the shape [types, names, descriptions] of JSON data), which will be sent
    to the LLM. For each LLM, LangChain picks the best method to do this, usually
    function calling or prompting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The schema will also be used to validate the output returned by the LLM before
    returning it; this ensures the output produced respects the schema you passed
    in exactly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other Machine-Readable Formats with Output Parsers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also use an LLM or chat model to produce output in other formats, such
    as CSV or XML. This is where output parsers come in handy. *Output parsers* are
    classes that help you structure large language model responses. They serve two
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Providing format instructions
  prefs: []
  type: TYPE_NORMAL
- en: Output parsers can be used to inject some additional instructions in the prompt
    that will help guide the LLM to output text in the format it knows how to parse.
  prefs: []
  type: TYPE_NORMAL
- en: Validating and parsing output
  prefs: []
  type: TYPE_NORMAL
- en: The main function is to take the textual output of the LLM or chat model and
    render it to a more structured format, such as a list, XML, or other format. This
    can include removing extraneous information, correcting incomplete output, and
    validating the parsed values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how an output parser works:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: LangChain provides a variety of output parsers for various use cases, including
    CSV, XML, and more. We’ll see how to combine output parsers with models and prompts
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling the Many Pieces of an LLM Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key components you’ve learned about so far are essential building blocks
    of the LangChain framework. Which brings us to the critical question: How do you
    combine them effectively to build your LLM application?'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Runnable Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you may have noticed, all the code examples used so far utilize a similar
    interface and the `invoke()` method to generate outputs from the model (or prompt
    template, or output parser). All components have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a common interface with these methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invoke`: transforms a single input into an output'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch`: efficiently transforms multiple inputs into multiple outputs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream`: streams output from a single input as it’s produced'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There are built-in utilities for retries, fallbacks, schemas, and runtime configurability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Python, each of the three methods have `asyncio` equivalents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As such, all components behave the same way, and the interface learned for
    one of them applies to all:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, you see how the three main methods work:'
  prefs: []
  type: TYPE_NORMAL
- en: '`invoke()` takes a single input and returns a single output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch()` takes a list of outputs and returns a list of outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream()` takes a single input and returns an iterator of parts of the output
    as they become available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, where the underlying component doesn’t support iterative output,
    there will be a single part containing all output.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can combine these components in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Imperative
  prefs: []
  type: TYPE_NORMAL
- en: Call your components directly, for example, with `model.invoke(...)`
  prefs: []
  type: TYPE_NORMAL
- en: Declarative
  prefs: []
  type: TYPE_NORMAL
- en: Use LangChain Expression Language (LCEL), as covered in an upcoming section
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 1-1](#ch01_table_1_1736545659767905) summarizes their differences, and
    we’ll see each in action next.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-1\. The main differences between imperative and declarative composition.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Imperative | Declarative |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Syntax | All of Python or JavaScript | LCEL |'
  prefs: []
  type: TYPE_TB
- en: '| Parallel execution | Python: with threads or coroutinesJavaScript: with `Promise.all`
    | Automatic |'
  prefs: []
  type: TYPE_TB
- en: '| Streaming | With yield keyword | Automatic |'
  prefs: []
  type: TYPE_TB
- en: '| Async execution | With async functions | Automatic |'
  prefs: []
  type: TYPE_TB
- en: Imperative Composition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Imperative composition* is just a fancy name for writing the code you’re used
    to writing, composing these components into functions and classes. Here’s an example
    combining prompts, models, and output parsers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The preceding is a complete example of a chatbot, using a prompt and chat model.
    As you can see, it uses familiar Python syntax and supports any custom logic you
    might want to add in that function.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if you want to enable streaming or async support, you’d
    have to modify your function to support it. For example, streaming support can
    be added as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: So, either in JS or Python, you can enable streaming for your custom function
    by yielding the values you want to stream and then calling it with `stream`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For asynchronous execution, you’d rewrite your function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This one applies to Python only, as asynchronous execution is the only option
    in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Declarative Composition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LCEL is a *declarative language* for composing LangChain components. LangChain
    compiles LCEL compositions to an *optimized execution plan*, with automatic parallelization,
    streaming, tracing, and async support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the same example using LCEL:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Crucially, the last line is the same between the two examples—that is, you
    use the function and the LCEL sequence in the same way, with `invoke/stream/batch`.
    And in this version, you don’t need to do anything else to use streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'And, for Python only, it’s the same for using asynchronous methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned about the building blocks and key components
    necessary to build LLM applications using LangChain. LLM applications are essentially
    a chain consisting of the large language model to make predictions, the prompt
    instruction(s) to guide the model toward a desired output, and an optional output
    parser to transform the format of the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: All LangChain components share the same interface with `invoke`, `stream`, and
    `batch` methods to handle various inputs and outputs. They can either be combined
    and executed imperatively by calling them directly or declaratively using LCEL.
  prefs: []
  type: TYPE_NORMAL
- en: The imperative approach is useful if you intend to write a lot of custom logic,
    whereas the declarative approach is useful for simply assembling existing components
    with limited customization.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927),
    you’ll learn how to provide external data to your AI chatbot as *context* so that
    you can build an LLM application that enables you to “chat” with your data.
  prefs: []
  type: TYPE_NORMAL
