- en: Chapter 1\. LLM Fundamentals with LangChain
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章. 使用 LangChain 的 LLM 基础
- en: The [Preface](preface01.html#pr01_preface_1736545679069216) gave you a taste
    of the power of LLM prompting, where we saw firsthand the impact that different
    prompting techniques can have on what you get out of LLMs, especially when judiciously
    combined. The challenge in building good LLM applications is, in fact, in how
    to effectively construct the prompt sent to the model and process the model’s
    prediction to return an accurate output (see [Figure 1-1](#ch01_figure_1_1736545659763063)).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[前言](preface01.html#pr01_preface_1736545679069216)让你领略了 LLM 提示的力量，我们亲眼目睹了不同的提示技术如何影响从
    LLM 中获得的结果，尤其是在谨慎结合时。实际上，构建好的 LLM 应用程序的挑战在于如何有效地构建发送给模型的提示，并处理模型的预测以返回准确的结果（参见[图
    1-1](#ch01_figure_1_1736545659763063)）。'
- en: '![](assets/lelc_0101.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/lelc_0101.png)'
- en: Figure 1-1\. The challenge in making LLMs a useful part of your application
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1. 将 LLM 建成应用程序有用部分的挑战
- en: If you can solve this problem, you are well on your way to building LLM applications,
    simple and complex alike. In this chapter, you’ll learn more about how LangChain’s
    building blocks map to LLM concepts and how, when combined effectively, they enable
    you to build LLM applications. But first, the sidebar [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    is a brief primer on why we think it useful to use LangChain to build LLM applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够解决这个问题，那么你已经走上了构建简单和复杂 LLM 应用程序的道路。在本章中，你将了解 LangChain 的构建块如何映射到 LLM 概念，以及当它们有效结合时，如何使你能够构建
    LLM 应用程序。但首先，侧边栏[“为什么选择 LangChain？”](#ch01_why_langchain_1736545659776355)是对为什么我们认为使用
    LangChain 构建LLM 应用程序有用的简要介绍。
- en: Getting Set Up with LangChain
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LangChain 进行设置
- en: To follow along with the rest of the chapter, and the chapters to come, we recommend
    setting up LangChain on your computer first.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章的其余部分以及后续章节，我们建议首先在你的计算机上设置 LangChain。
- en: See the instructions in the [Preface](preface01.html#pr01_preface_1736545679069216)
    regarding setting up an OpenAI account and complete these if you haven’t yet.
    If you prefer using a different LLM provider, see [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    for alternatives.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅有关设置 OpenAI 账户的说明[前言](preface01.html#pr01_preface_1736545679069216)，如果你还没有完成这些操作，请完成它们。如果你更喜欢使用不同的
    LLM 提供商，请参阅[“为什么选择 LangChain？”](#ch01_why_langchain_1736545659776355)以获取替代方案。
- en: Then head over to the [API Keys page](https://oreil.ly/BKrtV) on the OpenAI
    website (after logging in to your OpenAI account), create an API key, and save
    it—you’ll need it soon.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，转到 OpenAI 网站的[API 密钥页面](https://oreil.ly/BKrtV)（登录你的 OpenAI 账户后），创建一个 API
    密钥，并保存它——你很快就会需要它。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this book, we’ll show code examples in both Python and JavaScript (JS). LangChain
    offers the same functionality in both languages, so just pick the one you’re most
    comfortable with and follow the respective code snippets throughout the book (the
    code examples for each language are equivalent).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将以 Python 和 JavaScript (JS) 两种语言展示代码示例。LangChain 在这两种语言中都提供相同的功能，所以只需选择你最熟悉的一种，并遵循全书中的相应代码片段（每种语言的代码示例是等效的）。
- en: 'First, some setup instructions for readers using Python:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为使用 Python 的读者提供一些设置说明：
- en: Ensure that you have Python installed. See the [instructions for your operating
    system](https://oreil.ly/20K9l).
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你已经安装了 Python。请参阅[你操作系统的说明](https://oreil.ly/20K9l)。
- en: Install Jupyter if you want to run the examples in a notebook environment. You
    can do this by running `pip install notebook` in your terminal.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想在笔记本环境中运行示例，请安装 Jupyter。你可以在终端中运行 `pip install notebook` 来完成此操作。
- en: 'Install the LangChain library by running the following commands in your terminal:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在终端中运行以下命令来安装 LangChain 库：
- en: '[PRE0]'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本节开头生成的 OpenAI API 密钥在你的终端环境中使其可用。你可以通过运行以下命令来完成此操作：
- en: '[PRE1]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要忘记将 `your-key` 替换为你之前生成的 API 密钥。
- en: 'Open a Jupyter notebook by running this command:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行此命令打开 Jupyter 笔记本：
- en: '[PRE2]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You’re now ready to follow along with the Python code examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好跟随 Python 代码示例了。
- en: 'Here are the instructions for readers using JavaScript:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是为使用 JavaScript 的读者提供的说明：
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本节开头生成的 OpenAI API 密钥在你的终端环境中使其可用。你可以通过运行以下命令来完成此操作：
- en: '[PRE3]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要忘记将 `your-key` 替换为你之前生成的 API 密钥。
- en: If you want to run the examples as Node.js scripts, install Node by following
    the [instructions](https://oreil.ly/5gjiO).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想将示例作为 Node.js 脚本运行，请按照 [说明](https://oreil.ly/5gjiO) 安装 Node。
- en: 'Install the LangChain libraries by running the following commands in your terminal:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的终端中运行以下命令来安装 LangChain 库：
- en: '[PRE4]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Take each example, save it as a *.js* file and run it with `node ./file.js`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个示例保存为 *.js* 文件，并使用 `node ./file.js` 运行它。
- en: Using LLMs in LangChain
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 LangChain 中使用 LLM
- en: 'To recap, LLMs are the driving engine behind most generative AI applications.
    LangChain provides two simple interfaces to interact with any LLM API provider:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，LLM 是大多数生成式 AI 应用程序背后的驱动引擎。LangChain 提供了两个简单的接口来与任何 LLM API 提供商交互：
- en: Chat models
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天模型
- en: LLMs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM
- en: The LLM interface simply takes a string prompt as input, sends the input to
    the model provider, and then returns the model prediction as output.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 接口简单地接受一个字符串提示作为输入，将输入发送给模型提供商，然后返回模型预测作为输出。
- en: 'Let’s import LangChain’s OpenAI LLM wrapper to `invoke` a model prediction
    using a simple prompt:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入 LangChain 的 OpenAI LLM 包装器来使用简单的提示 `invoke` 模型预测：
- en: '*Python*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*JavaScript*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*The output:*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出结果：*'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Notice the parameter `model` passed to `OpenAI`. This is the most common parameter
    to configure when using an LLM or chat model, the underlying model to use, as
    most providers offer several models with different trade-offs in capability and
    cost (usually larger models are more capable, but also more expensive and slower).
    See [OpenAI’s overview](https://oreil.ly/dM886) of the models they offer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意传递给 `OpenAI` 的参数 `model`。这是在使用大型语言模型或聊天模型时最常配置的参数，它指定了要使用的底层模型，因为大多数提供商都提供具有不同能力和成本折衷的多个模型（通常较大的模型功能更强，但成本更高且速度更慢）。请参阅他们提供的模型概述
    [OpenAI](https://oreil.ly/dM886)。
- en: Other useful parameters to configure include the following, offered by most
    providers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他有用的配置参数包括以下内容，大多数提供商都提供。
- en: '`temperature`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`temperature`'
- en: 'This controls the sampling algorithm used to generate output. Lower values
    produce more predictable outputs (for example, 0.1), while higher values generate
    more creative, or unexpected, results (such as 0.9). Different tasks will need
    different values for this parameter. For instance, producing structured output
    usually benefits from a lower temperature, whereas creative writing tasks do better
    with a higher value:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这控制了生成输出时使用的采样算法。较低的值会产生更可预测的输出（例如，0.1），而较高的值会产生更具创造性或意外的结果（例如，0.9）。不同的任务需要不同的参数值。例如，生成结构化输出通常从较低的温度中受益，而创意写作任务则更适合使用较高的值：
- en: '`max_tokens`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_tokens`'
- en: This limits the size (and cost) of the output. A lower value may cause the LLM
    to stop generating the output before getting to a natural end, so it may appear
    to have been truncated.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这限制了输出的大小（和成本）。较低的值可能导致 LLM 在达到自然结束之前停止生成输出，因此可能看起来被截断。
- en: Beyond these, each provider exposes a different set of parameters. We recommend
    looking at the documentation for the one you choose. For an example, refer to
    [OpenAI’s platform](https://oreil.ly/5O1RW).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，每个提供商都公开了一组不同的参数。我们建议查看您选择的提供商的文档。例如，请参阅 [OpenAI 的平台](https://oreil.ly/5O1RW)。
- en: 'Alternatively, the chat model interface enables back and forth conversations
    between the user and model. The reason why it’s a separate interface is because
    popular LLM providers like OpenAI differentiate messages sent to and from the
    model into *user*, *assistant*, and *system* roles (here *role* denotes the type
    of content the message contains):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，聊天模型界面允许用户和模型之间进行双向对话。之所以它是独立的界面，是因为流行的 LLM 提供商，如 OpenAI，将发送给和从模型的消息区分成 *用户*、*助手*
    和 *系统* 角色（这里的 *角色* 表示消息包含的内容类型）：
- en: System role
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 系统角色
- en: Used for instructions the model should use to answer a user question
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型回答用户问题时应使用的说明
- en: User role
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用户角色
- en: Used for the user’s query and any other content produced by the user
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用于用户的查询和用户产生的任何其他内容
- en: Assistant role
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 助理角色
- en: Used for content generated by the model
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型生成的内容
- en: 'The chat model’s interface makes it easier to configure and manage conversions
    in your AI chatbot application. Here’s an example utilizing LangChain’s ChatOpenAI
    model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模型界面使得在您的 AI 聊天机器人应用程序中配置和管理转换变得更加容易。以下是一个使用 LangChain 的 ChatOpenAI 模型的示例：
- en: '*Python*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*JavaScript*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*The output:*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出结果：*'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Instead of a single prompt string, chat models make use of different types
    of chat message interfaces associated with each role mentioned previously. These
    include the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个提示字符串不同，聊天模型利用与之前提到的每个角色相关联的不同类型的聊天消息接口。以下是一些包括的内容：
- en: '`HumanMessage`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`HumanMessage`'
- en: A message sent from the perspective of the human, with the user role
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类的角度发送的消息，具有用户角色
- en: '`AIMessage`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`AIMessage`'
- en: A message sent from the perspective of the AI that the human is interacting
    with, with the assistant role
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类与AI交互的角度发送的消息，具有助手角色
- en: '`SystemMessage`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`SystemMessage`'
- en: A message setting the instructions the AI should follow, with the system role
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 设置AI应遵循的指令的消息，具有系统角色
- en: '`ChatMessage`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChatMessage`'
- en: A message allowing for arbitrary setting of role
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 允许任意设置角色的消息
- en: 'Let’s incorporate a `SystemMessage` instruction in our example:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的例子中包含一个`SystemMessage`指令：
- en: '*Python*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*JavaScript*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*JavaScript*'
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*The output:*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, the model obeyed the instruction provided in the `SystemMessage`
    even though it wasn’t present in the user’s question. This enables you to preconfigure
    your AI application to respond in a relatively predictable manner based on the
    user’s input.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，即使`SystemMessage`中没有出现在用户的提问中，模型也遵循了提供的指令。这使得您可以根据用户的输入预先配置您的AI应用程序，以相对可预测的方式做出响应。
- en: Making LLM Prompts Reusable
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使LLM提示可重用
- en: The previous section showed how the `prompt` instruction significantly influences
    the model’s output. Prompts help the model understand context and generate relevant
    answers to queries.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节展示了`prompt`指令如何显著影响模型的输出。提示有助于模型理解上下文并生成针对查询的相关答案。
- en: 'Here is an example of a detailed prompt:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个详细提示的例子：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Although the prompt looks like a simple string, the challenge is figuring out
    what the text should contain and how it should vary based on the user’s input.
    In this example, the Context and Question values are hardcoded, but what if we
    wanted to pass these in dynamically?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提示看起来像是一个简单的字符串，但挑战在于确定文本应包含什么内容，以及它应如何根据用户的输入而变化。在这个例子中，上下文和问题值是硬编码的，但如果我们想动态地传递这些值怎么办呢？
- en: 'Fortunately, LangChain provides prompt template interfaces that make it easy
    to construct prompts with dynamic inputs:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，LangChain提供了提示模板接口，这使得构建具有动态输入的提示变得容易：
- en: '*Python*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python*'
- en: '[PRE15] `Answer: """``)`  `template``.``invoke``({`     `"context"``:` `"""The
    most recent advancements in NLP are being driven by Large`   `Language Models
    (LLMs). These models outperform their smaller`   `counterparts and have become
    invaluable for developers who are creating`   `applications with NLP capabilities.
    Developers can tap into these`   ``models through Hugging Face''s `transformers`
    library, or by utilizing``   `` OpenAI and Cohere''s offerings through the `openai`
    and `cohere` ``   `libraries, respectively."""``,`     `"question"``:` `"Which
    model providers offer LLMs?"` `})` [PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE15] `Answer: """``)`  `template``.``invoke``({`     `"context"``:` `"""The
    most recent advancements in NLP are being driven by Large`   `Language Models
    (LLMs). These models outperform their smaller`   `counterparts and have become
    invaluable for developers who are creating`   `applications with NLP capabilities.
    Developers can tap into these`   ``models through Hugging Face''s `transformers`
    library, or by utilizing``   `` OpenAI and Cohere''s offerings through the `openai`
    and `cohere` ``   `libraries, respectively."""``,`     `"question"``:` `"Which
    model providers offer LLMs?"` `})` [PRE16]'
- en: '[PRE17]` [PRE18] *JavaS**cript*    [PRE19]py    *The output:*    [PRE20]py    This
    example takes the static prompt from the previous block and makes it dynamic.
    The `template` contains the structure of the final prompt alongside the definition
    of where the dynamic inputs will be inserted.    As such, the template can be
    used as a recipe to build multiple static, specific prompts. When you format the
    prompt with some specific values—in this case, `context` and `question`—you get
    a static prompt ready to be passed in to an LLM.    As you can see, the `question`
    argument is passed dynamically via the `invoke` function. By default, LangChain
    prompts follow Python’s `f-string` syntax for defining dynamic parameters—any
    word surrounded by curly braces, such as `{question}`, are placeholders for values
    passed in at runtime. In the previous example, `{question}` was replaced by `“Which
    model providers offer LLMs?”`    Let’s see how we’d feed this into an LLM OpenAI
    model using LangChain:    *Python*    [PRE21]py `Answer: """``)`  `model` `=`
    `OpenAI``()`  ``# `prompt` and `completion` are the results of using template
    and model once``  `prompt` `=` `template``.``invoke``({`     `"context"``:` `"""The
    most recent advancements in NLP are being driven by Large`  `Language Models (LLMs).
    These models outperform their smaller`   `counterparts and have become invaluable
    for developers who are creating`   `applications with NLP capabilities. Developers
    can tap into these`   ``models through Hugging Face''s `transformers` library,
    or by utilizing``   `` OpenAI and Cohere''s offerings through the `openai` and
    `cohere` ``   `libraries, respectively."""``,`     `"question"``:` `"Which model
    providers offer LLMs?"` `})`  `completion` `=` `model``.``invoke``(``prompt``)`
    [PRE22]py   [PRE23]`py [PRE24] [PRE25] from langchain_openai import ChatOpenAI
    from langchain_core.pydantic_v1 import BaseModel  class AnswerWithJustification(BaseModel):     ''''''An
    answer to the user''s question along with justification for the   answer.''''''     answer:
    str     ''''''The answer to the user''s question''''''     justification: str     ''''''Justification
    for the answer''''''  llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0) structured_llm
    = llm.with_structured_output(AnswerWithJustification)  structured_llm.invoke("""What
    weighs more, a pound of bricks or a pound   of feathers""") [PRE26] import { ChatOpenAI
    } from ''@langchain/openai'' import { z } from "zod";  const answerSchema = z   .object({     answer:
    z.string().describe("The answer to the user''s question"),     justification:
    z.string().describe(`Justification for the   answer`),   })   .describe(`An answer
    to the user''s question along with justification for   the answer.`);  const model
    = new ChatOpenAI({   model: "gpt-3.5-turbo",   temperature: 0, }).withStructuredOutput(answerSchema)
    await model.invoke("What weighs more, a pound of bricks or a pound of feathers")
    [PRE27] {   answer: "They weigh the same",   justification: "Both a pound of bricks
    and a pound of feathers weigh one pound.      The weight is the same, but the
    volu"... 42 more characters } [PRE28] from langchain_core.output_parsers import
    CommaSeparatedListOutputParser parser = CommaSeparatedListOutputParser() items
    = parser.invoke("apple, banana, cherry") [PRE29] import { CommaSeparatedListOutputParser
    } from ''@langchain/core/output_parsers''  const parser = new CommaSeparatedListOutputParser()  await
    parser.invoke("apple, banana, cherry") [PRE30] [''apple'', ''banana'', ''cherry'']
    [PRE31] from langchain_openai.llms import ChatOpenAI  model = ChatOpenAI()  completion
    = model.invoke(''Hi there!'')  # Hi!  completions = model.batch([''Hi there!'',
    ''Bye!'']) # [''Hi!'', ''See you!'']  for token in model.stream(''Bye!''):     print(token)     #
    Good     # bye     # ! [PRE32] import { ChatOpenAI } from ''@langchain/openai''  const
    model = new ChatOpenAI()  const completion = await model.invoke(''Hi there!'')  //
    Hi!  const completions = await model.batch([''Hi there!'', ''Bye!'']) // [''Hi!'',
    ''See you!'']  for await (const token of await model.stream(''Bye!'')) {   console.log(token)   //
    Good   // bye   // ! } [PRE33] from langchain_openai.chat_models import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables
    import chain  # the building blocks  template = ChatPromptTemplate.from_messages([     (''system'',
    ''You are a helpful assistant.''),     (''human'', ''{question}''), ])  model
    = ChatOpenAI()  # combine them in a function # @chain decorator adds the same
    Runnable interface for any function you write  @chain def chatbot(values):     prompt
    = template.invoke(values)     return model.invoke(prompt)  # use it  chatbot.invoke({"question":
    "Which model providers offer LLMs?"}) [PRE34] import {ChatOpenAI} from ''@langchain/openai''
    import {ChatPromptTemplate} from ''@langchain/core/prompts'' import {RunnableLambda}
    from ''@langchain/core/runnables''  // the building blocks  const template = ChatPromptTemplate.fromMessages([   [''system'',
    ''You are a helpful assistant.''],   [''human'', ''{question}''], ])  const model
    = new ChatOpenAI()  // combine them in a function // RunnableLambda adds the same
    Runnable interface for any function you write  const chatbot = RunnableLambda.from(async
    values => {   const prompt = await template.invoke(values)   return await model.invoke(prompt)
    })  // use it  await chatbot.invoke({   "question": "Which model providers offer
    LLMs?" }) [PRE35] AIMessage(content="Hugging Face''s `transformers` library, OpenAI
    using the      `openai` library, and Cohere using the `cohere` library offer LLMs.")
    [PRE36] @chain def chatbot(values):     prompt = template.invoke(values)     for
    token in model.stream(prompt):         yield token  for part in chatbot.stream({     "question":
    "Which model providers offer LLMs?" }):     print(part) [PRE37] const chatbot
    = RunnableLambda.from(async function* (values) {   const prompt = await template.invoke(values)   for
    await (const token of await model.stream(prompt)) {     yield token   } })  for
    await (const token of await chatbot.stream({   "question": "Which model providers
    offer LLMs?" })) {   console.log(token) } [PRE38] AIMessageChunk(content="Hugging")
    AIMessageChunk(content=" Face''s") AIMessageChunk(content=" `transformers`") ...
    [PRE39] @chain async def chatbot(values):     prompt = await template.ainvoke(values)     return
    await model.ainvoke(prompt)  await chatbot.ainvoke({"question": "Which model providers
    offer LLMs?"}) # > AIMessage(content="""Hugging Face''s `transformers` library,
    OpenAI using     the `openai` library, and Cohere using the `cohere` library offer
    LLMs.""") [PRE40] from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompts
    import ChatPromptTemplate  # the building blocks  template = ChatPromptTemplate.from_messages([     (''system'',
    ''You are a helpful assistant.''),     (''human'', ''{question}''), ])  model
    = ChatOpenAI()  # combine them with the | operator  chatbot = template | model  #
    use it  chatbot.invoke({"question": "Which model providers offer LLMs?"}) [PRE41]
    import { ChatOpenAI } from ''@langchain/openai'' import { ChatPromptTemplate }
    from ''@langchain/core/prompts'' import { RunnableLambda } from ''@langchain/core/runnables''  //
    the building blocks  const template = ChatPromptTemplate.fromMessages([   [''system'',
    ''You are a helpful assistant.''],   [''human'', ''{question}''], ])  const model
    = new ChatOpenAI()  // combine them in a function  const chatbot = template.pipe(model)  //
    use it  await chatbot.invoke({   "question": "Which model providers offer LLMs?"
    }) [PRE42] AIMessage(content="Hugging Face''s `transformers` library, OpenAI using
    the      `openai` library, and Cohere using the `cohere` library offer LLMs.")
    [PRE43] chatbot = template | model  for part in chatbot.stream({     "question":
    "Which model providers offer LLMs?" }):     print(part)     # > AIMessageChunk(content="Hugging")     #
    > AIMessageChunk(content=" Face''s")     # > AIMessageChunk(content=" `transformers`")     #
    ... [PRE44] const chatbot = template.pipe(model)  for await (const token of await
    chatbot.stream({   "question": "Which model providers offer LLMs?" })) {   console.log(token)
    } [PRE45] chatbot = template | model  await chatbot.ainvoke({     "question":
    "Which model providers offer LLMs?" }) [PRE46]` `````'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE17]` [PRE18] *JavaScript*    [PRE19]py    *输出:*    [PRE20]py    本例从上一个块中获取静态提示并将其动态化。`template`
    包含最终提示的结构以及动态输入将被插入的位置的定义。    因此，模板可以用作构建多个静态、特定提示的配方。当你用一些特定值格式化提示——在这种情况下，`context`
    和 `question`——你将得到一个准备好的静态提示，可以传递给 LLM。    正如你所见，`question` 参数是通过 `invoke` 函数动态传递的。默认情况下，LangChain
    提示遵循 Python 的 `f-string` 语法来定义动态参数——任何被大括号包围的单词，如 `{question}`，都是运行时传入值的占位符。在先前的例子中，`{question}`
    被替换为 `“Which model providers offer LLMs?”`    让我们看看如何使用 LangChain 将其输入到 LLM OpenAI
    模型中：    *Python*    [PRE21]py `答案: """``)`  `model` `=` `OpenAI``()`  ``# `prompt`
    和 `completion` 是使用模板和模型一次的结果``  `prompt` `=` `template``.``invoke``({`     `"context"``:`
    `"""The most recent advancements in NLP are being driven by Large`  `Language
    Models (LLMs). These models outperform their smaller`   `counterparts and have
    become invaluable for developers who are creating`   ``applications with NLP capabilities.
    Developers can tap into these`   ``models through Hugging Face''s `transformers`
    library, or by utilizing``   `` OpenAI and Cohere''s offerings through the `openai`
    and `cohere` ``   `libraries, respectively."""``,`     `"question"``:` `"Which
    model providers offer LLMs?"` `})`  `completion` `=` `model``.``invoke``(``prompt``)`
    [PRE22]py   [PRE23]`py [PRE24] [PRE25] 从 langchain_openai 导入 ChatOpenAI 从 langchain_core.pydantic_v1
    导入 BaseModel  class AnswerWithJustification(BaseModel):     ''''''对用户问题的答案及其答案的依据''''''     answer:
    str     ''''''用户问题的答案''''''     justification: str     ''''''答案的依据''''''  llm
    = ChatOpenAI(model="gpt-3.5-turbo", temperature=0) structured_llm = llm.with_structured_output(AnswerWithJustification)  structured_llm.invoke("""What
    weighs more, a pound of bricks or a pound of feathers""") [PRE26] 导入 { ChatOpenAI
    } 从 ''@langchain/openai'' 导入 { z } 从 "zod"  const answerSchema = z   .object({     answer:
    z.string().describe("用户问题的答案"),     justification: z.string().describe(`答案的依据`),   })   .describe(`对用户问题的答案及其答案的依据.`);  const
    model = new ChatOpenAI({   model: "gpt-3.5-turbo",   temperature: 0, }).withStructuredOutput(answerSchema)
    await model.invoke("What weighs more, a pound of bricks or a pound of feathers")
    [PRE27] {   answer: "They weigh the same",   justification: "Both a pound of bricks
    and a pound of feathers weigh one pound.      The weight is the same, but the
    volu"... 42 more characters } [PRE28] 从 langchain_core.output_parsers 导入 CommaSeparatedListOutputParser
    parser = CommaSeparatedListOutputParser() items = parser.invoke("apple, banana,
    cherry") [PRE29] 从 ''@langchain/core/output_parsers'' 导入 { CommaSeparatedListOutputParser
    } const parser = new CommaSeparatedListOutputParser()  await parser.invoke("apple,
    banana, cherry") [PRE30] [''apple'', ''banana'', ''cherry''] [PRE31] 从 langchain_openai.llms
    导入 ChatOpenAI  model = ChatOpenAI()  completion = model.invoke(''Hi there!'')  #
    Hi!  completions = model.batch([''Hi there!'', ''Bye!'']) # [''Hi!'', ''See you!'']  for
    token in model.stream(''Bye!''):     print(token)     # Good     # bye     # !
    [PRE32] 从 ''@langchain/openai'' 导入 { ChatOpenAI } const model = new ChatOpenAI()  const
    completion = await model.invoke(''Hi there!'')  // Hi!  const completions = await
    model.batch([''Hi there!'', ''Bye!'']) // [''Hi!'', ''See you!'']  for await (const
    token of await model.stream(''Bye!'')) {   console.log(token)   // Good   // bye   //
    ! } [PRE33] 从 langchain_openai.chat_models 导入 ChatOpenAI 从 langchain_core.prompts
    导入 ChatPromptTemplate 从 langchain_core.runnables 导入 chain  # 构建块  template = ChatPromptTemplate.from_messages([     (''system'',
    ''You are a helpful assistant.''),     (''human'', ''{question}''), ])  model
    = ChatOpenAI()  # 结合它们到一个函数中  # @chain 装饰器为任何你写的函数添加相同的 Runnable 接口  @chain def
    chatbot(values):     prompt = template.invoke(values)     return model.invoke(prompt)  #
    使用它  chatbot.invoke({"question": "Which model providers offer LLMs?"}) [PRE34]
    从 ''@langchain/openai'' 导入 {ChatOpenAI} 从 ''@langchain/core/prompts'' 导入 {ChatPromptTemplate}
    从 ''@langchain/core/runnables'' 导入 {RunnableLambda}  // 构建块  const template =
    ChatPromptTemplate.fromMessages([   [''system'', ''You are a helpful assistant.''],   [''human'',
    ''{question}''], ])  const model = new ChatOpenAI()  // 结合它们到一个函数中  // RunnableLambda
    为任何你写的函数添加相同的 Runnable 接口  const chatbot = RunnableLambda.from(async values =>
    {   const prompt = await template.invoke(values)   return await model.invoke(prompt)
    })  // 使用它  await chatbot.invoke({   "question": "Which model providers offer
    LLMs?" }) [PRE35] AIMessage(content="Hugging Face''s `transformers` library, OpenAI
    using the      `openai` library, and Cohere using the `cohere` library offer LLMs.")
    [PRE36] @chain def chatbot(values):     prompt = template.invoke(values)     for
    token in model.stream(prompt):         yield token  for part in chatbot.stream({     "question":
    "Which model providers offer LLMs?" }):     print(part) [PRE37] const chatbot
    = RunnableLambda.from(async function* (values) {   const prompt = await template.invoke(values)   for
    await (const token of await model.stream(prompt)) {     yield token   } })  for
    await (const token of await chatbot.stream({   "question": "Which model providers
    offer LLMs?" })) {   console.log(token) } [PRE38] AIMessageChunk(content="Hugging")
    AIMessageChunk(content=" Face''s") AIMessageChunk(content=" `transformers`") ...
    [PRE39] @chain async def chatbot(values):     prompt = await template.ainvoke(values)     return
    await model.ainvoke(prompt)  await chatbot.ainvoke({"question": "Which model providers
    offer LLMs?"}) # > AIMessage(content="""Hugging Face''s `transformers` library,
    OpenAI using     the `openai` library, and Cohere using the `cohere` library offer
    LLMs.""") [PRE40] 从 langchain_openai.chat_models 导入 ChatOpenAI 从 langchain_core.prompts
    导入 ChatPromptTemplate  # 构建块  template = ChatPromptTemplate.from_messages([     (''system'',
    ''You are a helpful assistant.''),     (''human'', ''{question}''), ])  model
    = ChatOpenAI()  # 结合它们用 | 操作符  chatbot = template | model  # 使用它  chatbot.invoke({"question":
    "Which model providers offer LLMs?"}) [PRE41] 从 ''@langchain/openai'' 导入 { ChatOpenAI
    } 从 ''@langchain/core/prompts'' 导入 { ChatPromptTemplate } 从 ''@langchain/core/runnables''
    导入 { RunnableLambda }  // 构建块  const template = ChatPromptTemplate.fromMessages([   [''system'',
    ''You are a helpful assistant.''],   [''human'', ''{question}''], ])  const model
    = new ChatOpenAI()  // 结合它们到一个函数中  const chatbot = template.pipe(model)  // 使用它  await
    chatbot.invoke({   "question": "Which model providers offer LLMs?" }) [PRE42]
    AIMessage(content="Hugging Face''s `transformers` library, OpenAI using the      `openai`
    library, and Cohere using the `cohere` library offer LLMs.") [PRE43] chatbot =
    template | model  for part in chatbot.stream({     "question": "Which model providers
    offer LLMs?" }):     print(part)     # > AIMessageChunk(content="Hugging")     #
    > AIMessageChunk(content=" Face''s")     # > AIMessageChunk(content=" `transformers`")     #
    ... [PRE44] const chatbot = template.pipe(model)  for await (const token of await
    chatbot.stream({   "question": "Which model providers offer LLMs?" })) {   console.log(token)
    } [PRE45] chatbot = template | model  await chatbot.ainvoke({     "question":
    "Which model providers offer LLMs?" }) [PRE46]`'
