- en: Chapter 1\. LLM Fundamentals with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [Preface](preface01.html#pr01_preface_1736545679069216) gave you a taste
    of the power of LLM prompting, where we saw firsthand the impact that different
    prompting techniques can have on what you get out of LLMs, especially when judiciously
    combined. The challenge in building good LLM applications is, in fact, in how
    to effectively construct the prompt sent to the model and process the model’s
    prediction to return an accurate output (see [Figure 1-1](#ch01_figure_1_1736545659763063)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/lelc_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. The challenge in making LLMs a useful part of your application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you can solve this problem, you are well on your way to building LLM applications,
    simple and complex alike. In this chapter, you’ll learn more about how LangChain’s
    building blocks map to LLM concepts and how, when combined effectively, they enable
    you to build LLM applications. But first, the sidebar [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    is a brief primer on why we think it useful to use LangChain to build LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Set Up with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the rest of the chapter, and the chapters to come, we recommend
    setting up LangChain on your computer first.
  prefs: []
  type: TYPE_NORMAL
- en: See the instructions in the [Preface](preface01.html#pr01_preface_1736545679069216)
    regarding setting up an OpenAI account and complete these if you haven’t yet.
    If you prefer using a different LLM provider, see [“Why LangChain?”](#ch01_why_langchain_1736545659776355)
    for alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Then head over to the [API Keys page](https://oreil.ly/BKrtV) on the OpenAI
    website (after logging in to your OpenAI account), create an API key, and save
    it—you’ll need it soon.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this book, we’ll show code examples in both Python and JavaScript (JS). LangChain
    offers the same functionality in both languages, so just pick the one you’re most
    comfortable with and follow the respective code snippets throughout the book (the
    code examples for each language are equivalent).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, some setup instructions for readers using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have Python installed. See the [instructions for your operating
    system](https://oreil.ly/20K9l).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Jupyter if you want to run the examples in a notebook environment. You
    can do this by running `pip install notebook` in your terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the LangChain library by running the following commands in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook by running this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You’re now ready to follow along with the Python code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the instructions for readers using JavaScript:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the OpenAI API key you generated at the beginning of this section and
    make it available in your terminal environment. You can do this by running the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Don’t forget to replace `your-key` with the API key you generated previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to run the examples as Node.js scripts, install Node by following
    the [instructions](https://oreil.ly/5gjiO).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the LangChain libraries by running the following commands in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Take each example, save it as a *.js* file and run it with `node ./file.js`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using LLMs in LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To recap, LLMs are the driving engine behind most generative AI applications.
    LangChain provides two simple interfaces to interact with any LLM API provider:'
  prefs: []
  type: TYPE_NORMAL
- en: Chat models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM interface simply takes a string prompt as input, sends the input to
    the model provider, and then returns the model prediction as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import LangChain’s OpenAI LLM wrapper to `invoke` a model prediction
    using a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice the parameter `model` passed to `OpenAI`. This is the most common parameter
    to configure when using an LLM or chat model, the underlying model to use, as
    most providers offer several models with different trade-offs in capability and
    cost (usually larger models are more capable, but also more expensive and slower).
    See [OpenAI’s overview](https://oreil.ly/dM886) of the models they offer.
  prefs: []
  type: TYPE_NORMAL
- en: Other useful parameters to configure include the following, offered by most
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This controls the sampling algorithm used to generate output. Lower values
    produce more predictable outputs (for example, 0.1), while higher values generate
    more creative, or unexpected, results (such as 0.9). Different tasks will need
    different values for this parameter. For instance, producing structured output
    usually benefits from a lower temperature, whereas creative writing tasks do better
    with a higher value:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: This limits the size (and cost) of the output. A lower value may cause the LLM
    to stop generating the output before getting to a natural end, so it may appear
    to have been truncated.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these, each provider exposes a different set of parameters. We recommend
    looking at the documentation for the one you choose. For an example, refer to
    [OpenAI’s platform](https://oreil.ly/5O1RW).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, the chat model interface enables back and forth conversations
    between the user and model. The reason why it’s a separate interface is because
    popular LLM providers like OpenAI differentiate messages sent to and from the
    model into *user*, *assistant*, and *system* roles (here *role* denotes the type
    of content the message contains):'
  prefs: []
  type: TYPE_NORMAL
- en: System role
  prefs: []
  type: TYPE_NORMAL
- en: Used for instructions the model should use to answer a user question
  prefs: []
  type: TYPE_NORMAL
- en: User role
  prefs: []
  type: TYPE_NORMAL
- en: Used for the user’s query and any other content produced by the user
  prefs: []
  type: TYPE_NORMAL
- en: Assistant role
  prefs: []
  type: TYPE_NORMAL
- en: Used for content generated by the model
  prefs: []
  type: TYPE_NORMAL
- en: 'The chat model’s interface makes it easier to configure and manage conversions
    in your AI chatbot application. Here’s an example utilizing LangChain’s ChatOpenAI
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of a single prompt string, chat models make use of different types
    of chat message interfaces associated with each role mentioned previously. These
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HumanMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message sent from the perspective of the human, with the user role
  prefs: []
  type: TYPE_NORMAL
- en: '`AIMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message sent from the perspective of the AI that the human is interacting
    with, with the assistant role
  prefs: []
  type: TYPE_NORMAL
- en: '`SystemMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message setting the instructions the AI should follow, with the system role
  prefs: []
  type: TYPE_NORMAL
- en: '`ChatMessage`'
  prefs: []
  type: TYPE_NORMAL
- en: A message allowing for arbitrary setting of role
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s incorporate a `SystemMessage` instruction in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model obeyed the instruction provided in the `SystemMessage`
    even though it wasn’t present in the user’s question. This enables you to preconfigure
    your AI application to respond in a relatively predictable manner based on the
    user’s input.
  prefs: []
  type: TYPE_NORMAL
- en: Making LLM Prompts Reusable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section showed how the `prompt` instruction significantly influences
    the model’s output. Prompts help the model understand context and generate relevant
    answers to queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a detailed prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Although the prompt looks like a simple string, the challenge is figuring out
    what the text should contain and how it should vary based on the user’s input.
    In this example, the Context and Question values are hardcoded, but what if we
    wanted to pass these in dynamically?
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, LangChain provides prompt template interfaces that make it easy
    to construct prompts with dynamic inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15] `Answer: """``)`  `template``.``invoke``({`     `"context"``:` `"""The
    most recent advancements in NLP are being driven by Large`   `Language Models
    (LLMs). These models outperform their smaller`   `counterparts and have become
    invaluable for developers who are creating`   `applications with NLP capabilities.
    Developers can tap into these`   ``models through Hugging Face''s `transformers`
    library, or by utilizing``   `` OpenAI and Cohere''s offerings through the `openai`
    and `cohere` ``   `libraries, respectively."""``,`     `"question"``:` `"Which
    model providers offer LLMs?"` `})` [PRE16]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]` [PRE18] *JavaS**cript*    [PRE19]py    *The output:*    [PRE20]py    This
    example takes the static prompt from the previous block and makes it dynamic.
    The `template` contains the structure of the final prompt alongside the definition
    of where the dynamic inputs will be inserted.    As such, the template can be
    used as a recipe to build multiple static, specific prompts. When you format the
    prompt with some specific values—in this case, `context` and `question`—you get
    a static prompt ready to be passed in to an LLM.    As you can see, the `question`
    argument is passed dynamically via the `invoke` function. By default, LangChain
    prompts follow Python’s `f-string` syntax for defining dynamic parameters—any
    word surrounded by curly braces, such as `{question}`, are placeholders for values
    passed in at runtime. In the previous example, `{question}` was replaced by `“Which
    model providers offer LLMs?”`    Let’s see how we’d feed this into an LLM OpenAI
    model using LangChain:    *Python*    [PRE21]py `Answer: """``)`  `model` `=`
    `OpenAI``()`  ``# `prompt` and `completion` are the results of using template
    and model once``  `prompt` `=` `template``.``invoke``({`     `"context"``:` `"""The
    most recent advancements in NLP are being driven by Large`  `Language Models (LLMs).
    These models outperform their smaller`   `counterparts and have become invaluable
    for developers who are creating`   `applications with NLP capabilities. Developers
    can tap into these`   ``models through Hugging Face''s `transformers` library,
    or by utilizing``   `` OpenAI and Cohere''s offerings through the `openai` and
    `cohere` ``   `libraries, respectively."""``,`     `"question"``:` `"Which model
    providers offer LLMs?"` `})`  `completion` `=` `model``.``invoke``(``prompt``)`
    [PRE22]py   [PRE23]`py [PRE24] [PRE25] from langchain_openai import ChatOpenAI
    from langchain_core.pydantic_v1 import BaseModel  class AnswerWithJustification(BaseModel):     ''''''An
    answer to the user''s question along with justification for the   answer.''''''     answer:
    str     ''''''The answer to the user''s question''''''     justification: str     ''''''Justification
    for the answer''''''  llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0) structured_llm
    = llm.with_structured_output(AnswerWithJustification)  structured_llm.invoke("""What
    weighs more, a pound of bricks or a pound   of feathers""") [PRE26] import { ChatOpenAI
    } from ''@langchain/openai'' import { z } from "zod";  const answerSchema = z   .object({     answer:
    z.string().describe("The answer to the user''s question"),     justification:
    z.string().describe(`Justification for the   answer`),   })   .describe(`An answer
    to the user''s question along with justification for   the answer.`);  const model
    = new ChatOpenAI({   model: "gpt-3.5-turbo",   temperature: 0, }).withStructuredOutput(answerSchema)
    await model.invoke("What weighs more, a pound of bricks or a pound of feathers")
    [PRE27] {   answer: "They weigh the same",   justification: "Both a pound of bricks
    and a pound of feathers weigh one pound.      The weight is the same, but the
    volu"... 42 more characters } [PRE28] from langchain_core.output_parsers import
    CommaSeparatedListOutputParser parser = CommaSeparatedListOutputParser() items
    = parser.invoke("apple, banana, cherry") [PRE29] import { CommaSeparatedListOutputParser
    } from ''@langchain/core/output_parsers''  const parser = new CommaSeparatedListOutputParser()  await
    parser.invoke("apple, banana, cherry") [PRE30] [''apple'', ''banana'', ''cherry'']
    [PRE31] from langchain_openai.llms import ChatOpenAI  model = ChatOpenAI()  completion
    = model.invoke(''Hi there!'')  # Hi!  completions = model.batch([''Hi there!'',
    ''Bye!'']) # [''Hi!'', ''See you!'']  for token in model.stream(''Bye!''):     print(token)     #
    Good     # bye     # ! [PRE32] import { ChatOpenAI } from ''@langchain/openai''  const
    model = new ChatOpenAI()  const completion = await model.invoke(''Hi there!'')  //
    Hi!  const completions = await model.batch([''Hi there!'', ''Bye!'']) // [''Hi!'',
    ''See you!'']  for await (const token of await model.stream(''Bye!'')) {   console.log(token)   //
    Good   // bye   // ! } [PRE33] from langchain_openai.chat_models import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables
    import chain  # the building blocks  template = ChatPromptTemplate.from_messages([     (''system'',
    ''You are a helpful assistant.''),     (''human'', ''{question}''), ])  model
    = ChatOpenAI()  # combine them in a function # @chain decorator adds the same
    Runnable interface for any function you write  @chain def chatbot(values):     prompt
    = template.invoke(values)     return model.invoke(prompt)  # use it  chatbot.invoke({"question":
    "Which model providers offer LLMs?"}) [PRE34] import {ChatOpenAI} from ''@langchain/openai''
    import {ChatPromptTemplate} from ''@langchain/core/prompts'' import {RunnableLambda}
    from ''@langchain/core/runnables''  // the building blocks  const template = ChatPromptTemplate.fromMessages([   [''system'',
    ''You are a helpful assistant.''],   [''human'', ''{question}''], ])  const model
    = new ChatOpenAI()  // combine them in a function // RunnableLambda adds the same
    Runnable interface for any function you write  const chatbot = RunnableLambda.from(async
    values => {   const prompt = await template.invoke(values)   return await model.invoke(prompt)
    })  // use it  await chatbot.invoke({   "question": "Which model providers offer
    LLMs?" }) [PRE35] AIMessage(content="Hugging Face''s `transformers` library, OpenAI
    using the      `openai` library, and Cohere using the `cohere` library offer LLMs.")
    [PRE36] @chain def chatbot(values):     prompt = template.invoke(values)     for
    token in model.stream(prompt):         yield token  for part in chatbot.stream({     "question":
    "Which model providers offer LLMs?" }):     print(part) [PRE37] const chatbot
    = RunnableLambda.from(async function* (values) {   const prompt = await template.invoke(values)   for
    await (const token of await model.stream(prompt)) {     yield token   } })  for
    await (const token of await chatbot.stream({   "question": "Which model providers
    offer LLMs?" })) {   console.log(token) } [PRE38] AIMessageChunk(content="Hugging")
    AIMessageChunk(content=" Face''s") AIMessageChunk(content=" `transformers`") ...
    [PRE39] @chain async def chatbot(values):     prompt = await template.ainvoke(values)     return
    await model.ainvoke(prompt)  await chatbot.ainvoke({"question": "Which model providers
    offer LLMs?"}) # > AIMessage(content="""Hugging Face''s `transformers` library,
    OpenAI using     the `openai` library, and Cohere using the `cohere` library offer
    LLMs.""") [PRE40] from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompts
    import ChatPromptTemplate  # the building blocks  template = ChatPromptTemplate.from_messages([     (''system'',
    ''You are a helpful assistant.''),     (''human'', ''{question}''), ])  model
    = ChatOpenAI()  # combine them with the | operator  chatbot = template | model  #
    use it  chatbot.invoke({"question": "Which model providers offer LLMs?"}) [PRE41]
    import { ChatOpenAI } from ''@langchain/openai'' import { ChatPromptTemplate }
    from ''@langchain/core/prompts'' import { RunnableLambda } from ''@langchain/core/runnables''  //
    the building blocks  const template = ChatPromptTemplate.fromMessages([   [''system'',
    ''You are a helpful assistant.''],   [''human'', ''{question}''], ])  const model
    = new ChatOpenAI()  // combine them in a function  const chatbot = template.pipe(model)  //
    use it  await chatbot.invoke({   "question": "Which model providers offer LLMs?"
    }) [PRE42] AIMessage(content="Hugging Face''s `transformers` library, OpenAI using
    the      `openai` library, and Cohere using the `cohere` library offer LLMs.")
    [PRE43] chatbot = template | model  for part in chatbot.stream({     "question":
    "Which model providers offer LLMs?" }):     print(part)     # > AIMessageChunk(content="Hugging")     #
    > AIMessageChunk(content=" Face''s")     # > AIMessageChunk(content=" `transformers`")     #
    ... [PRE44] const chatbot = template.pipe(model)  for await (const token of await
    chatbot.stream({   "question": "Which model providers offer LLMs?" })) {   console.log(token)
    } [PRE45] chatbot = template | model  await chatbot.ainvoke({     "question":
    "Which model providers offer LLMs?" }) [PRE46]` `````'
  prefs: []
  type: TYPE_NORMAL
