- en: '12 Production, an ever-changing landscape: Things are just getting started'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A brief overview of LLMs in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of LLMs as a technology and several exciting fields of research into
    it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our closing remarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Web as I envisaged it, we have not seen it yet. The future is still so much
    bigger than the past.—Tim Berners-Lee (inventor of www)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Wow! We’ve really covered a lot of ground in this book. Is your head just about
    ready to explode? Because ours are, and we wrote the book. Writing this book has
    been no easy feat, as the industry has been constantly changing—and fast. Trying
    to stay on top of what’s happening with LLMs has been like trying to build a house
    on quicksand; you finish one level, and it seems to have already sunk before you
    can start the next. We know that portions of this book will inevitably become
    out of date, and that’s why we tried our best to stick to core concepts, the sturdy
    rocks in the sand, that will never change.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we wanted to take a step back and review some of the major
    takeaways we hope you will walk away with. We’ve spent a lot of time getting into
    the weeds and paying attention to details, so let’s reflect for a moment to see
    the whole picture and review what we’ve covered. After that, we’ll take a minute
    to discuss the future of the field and where we can expect to see some of the
    next major breakthroughs. Finally, we’ll leave you with our final thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 A thousand-foot view
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have gone over a lot of material in this book—from making a bag-of-words
    model to serving an LLM API on a Raspberry Pi. If you made it all the way through
    the whole book, that’s an accomplishment. Great work! We are not going to recap
    everything, but we wanted to take a second to see the forest from the trees, as
    it were. To summarize much of what we’ve covered, we can split most of the ideas
    into four distinct but very closely tied quadrants: Preparation, Training, Serving,
    and Developing. You can see these quadrants in figure 12.1\. You’ll notice that
    along with these sections, there’s a fifth one distinct from the others, which
    we labeled Undercurrents. These are elements that seem to affect all of the other
    quadrants to varying degrees and things you’ll have to worry about during each
    stage of an LLM product life cycle.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/12-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 LLM product life cycle. Here are all the key concepts discussed
    in the book, along with where they generally fit within the production environment.
    Undercurrents are important elements that show up in every part of the life cycle—for
    example, linguistics informs preparation, creates metrics in training and serving,
    and influences prompting and development.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Hopefully, if it wasn’t clear when we were talking about a concept in an earlier
    chapter, it’s clear now exactly where that concept fits in a production life cycle.
    You’ll notice that we’ve likely put some elements in places that your current
    production environment doesn’t reflect—for example, provisioning of the MLOps
    infrastructure doesn’t often actually happen within the preparation stage but
    is rather haphazardly thrown together the first time that serving needs to happen.
    We get it. But during preparation is where we feel it *should* happen. Take a
    moment to digest all that you’ve learned while reading this book, and consider
    how the pieces all come together.
  prefs: []
  type: TYPE_NORMAL
- en: Given this abstract and idealized version of a production life cycle, let’s
    move to the things not currently included there. What might we need to add to
    our development portion five years down the line, especially given how fast the
    field moves now?
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 The future of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we wrote this book, we made a conscious effort to focus on the foundational
    knowledge you will need to understand how LLMs work and how to deploy them to
    production. This information is crucial, as production looks very different for
    every single use case. Learning how to weigh the pros and cons of any decision
    requires that foundational knowledge if you have any hope of landing on the right
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Adjacent to this decision, we didn’t want this book to be all theory. We wanted
    it to be hands-on, with enough examples that you as a reader wouldn’t just know
    how things worked but would get a sense of how they feel—like getting a feel for
    how long it takes to load a 70B model onto a GPU, sensing what the experience
    will be like for your user if you run the model on an edge device, and feeling
    the soft glow of your computer monitor as you hide in a dark cave pouring over
    code and avoiding the warm sun on a nice spring day.
  prefs: []
  type: TYPE_NORMAL
- en: One of the hardest decisions we made when we wrote this book was deciding to
    focus on the here and now. We decided to focus on the best methods that we actually
    see people using in production today. This decision was hard because over the
    course of writing this book, there have been many mind-blowing research papers
    we’ve been convinced will “change everything.” However, for one reason or another,
    that research has yet to make it to production. In this section, we are going
    to change that restriction and talk about what’s up and coming regardless of the
    current state of the industry. But it’s not just research; public opinions, lawsuits,
    and political landscapes often shape the future of technology as well. We’ll be
    looking at where we see LLMs going in the next several years and mention some
    of the directions they could take.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Government and regulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the beginning of this book, we promised to show you how to create LLM products,
    not just demos. While we believe we have done just that, there’s one important
    detail we’ve been ignoring: the fact that products live in the real world. While
    demos just have to work in isolation, products have to work in general. Products
    are meant to be sold, and once there’s an exchange of currency, expectations are
    set, reputations are on the line, and ultimately, governments are going to get
    involved.'
  prefs: []
  type: TYPE_NORMAL
- en: While a team can’t build for future regulations that may never come, it’s important
    to be aware of the possible legal ramifications of the products you build. One
    lost lawsuit can set a precedent that brings a tidal wave of copycat lawsuits.
    Since products live in the real world, it is best that we pay attention to that
    world.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of us had the opportunity to participate in Utah’s legislative process
    for Utah’s SB-149 Artificial Intelligence Amendments bill. This bill is primarily
    concerned with introducing liability to actors using LLMs to skirt consumer protection
    laws in the state. At the moment, every legislative body is attempting to figure
    out where its jurisdiction starts and ends concerning AI and how to deal with
    the increased responsibility it has to protect citizens and corporations within
    its constituency. In Utah, the state government takes a very serious and business-first
    approach to AI and LLMs. Throughout the process and the bill itself, the legislature
    cannot create definitions that aren’t broken with “behold, a man” Diogenes-style
    examples, and we will need every bit of good faith to navigate the new world that
    LLMs bring to regulatory bodies. How do you define AI? The bill defines it as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “Artificial intelligence” means a machine-based system that makes predictions,
    recommendations, or decisions influencing real or virtual environments.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This could be anything from a piecewise function to an LLM agent, meaning that
    your marketing team will not be liable for claims that your `if` statements are
    AI within the state. That said, the bill contains a thorough and well-thought-out
    definition of a deceptive act by a supplier, along with the formulation of an
    AI analysis and research program to help the state assess risks and policy in
    a more long-term capacity, which seems novel and unique to Utah. The Utah state
    legislature was able to refine this bill by consulting with researchers, experts,
    c-level executives, and business owners within the state, and we’d encourage the
    reader to participate in creating worthwhile and meaningful regulations within
    your communities and governments. This is the only way to make sure that court
    systems are prepared to impose consequences where they are due in the long term.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the forefront of legal concerns is that of copyright infringement. LLMs trained
    on enough data can impersonate or copy the style of an author or creator or even
    straight-up word-for-word plagiarize. While this is exciting when considering
    building your own ghostwriter to help you in your creative process, it’s much
    less so when you realize a competitor could do the same.
  prefs: []
  type: TYPE_NORMAL
- en: Probably the biggest lawsuit to pay attention to is that of *The New York Times*
    v. OpenAI.[¹](#footnote-97) *The New York Times* is in the process of legal action
    against OpenAI, stating their chatbots were trained on the *Times*’ intellectual
    property without consent. It gives evidence that the chatbots are giving word-for-word
    responses identical to proprietary information found in articles a user would
    normally have to pay to see. As a result, there is the concern that fewer users
    will visit their site, reducing ad revenue. Essentially, they stole their data
    and are now using it as a competitor in the information space.
  prefs: []
  type: TYPE_NORMAL
- en: Bystanders to the fight worry that if the *Times* wins, it may significantly
    hamper the development of AI and cause the United States to lose its position
    as the leader in the global AI development race. The more AI companies are exposed
    to copyright liability, the greater risk and thus loss to competition, which means
    less innovation. Conversely, they also worry that if the *Times* loses, it will
    further cut into the already struggling journalism business, where it’s already
    hard enough to find quality reports you can trust. This, too, would severely hurt
    AI development, which is always starving for good clean data. It appears to be
    a lose–lose situation for the AI field.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of who wins or loses the lawsuit, it’s pretty clear that current
    copyright laws never took into consideration that robots would eventually copy
    us. We need new laws, and it’s unclear whether our lawmakers are technically capable
    enough to meet the challenge. So again, we’d encourage you to participate in the
    creation process of regulations within your own communities.
  prefs: []
  type: TYPE_NORMAL
- en: AI detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One area of concern that continues to break our hearts comes from the rise
    of “AI detection” products. Let us just state from the start: these products are
    all snake oils and shams. There’s no reliable way to determine whether a piece
    of text was written by a human or a bot. By this point in the book, we expect
    most readers to have come to this conclusion as well. The reason is simple: if
    we can reliably determine what is and isn’t generated text, we can create a new
    model to beat the detector. This is the whole point of adversarial machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There has been a running gag online that anything you read with the word “delve”
    in it must be written by an LLM (e.g., [https://mng.bz/o0nr](https://mng.bz/o0nr)).
    The word *delve* is statistically more likely to occur in generated text than
    in human speech, but that brings up the obvious questions: Which model? Which
    prompt? The human hubris to believe one can identify generated content simply
    by looking for particular words is laughable. But, of course, if people vainly
    believe this obvious falsehood, it’s no surprise they are willing to believe a
    more complex system or algorithm will be able to do it even better.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason it breaks our hearts, though, is because we’ve read story after story
    of students getting punished, given failing grades on papers, forced to drop out
    of classes, and given plagiarism marks on their transcripts. Now, we don’t know
    the details of every case, but as experts in the technology in question, we choose
    to believe the students more often than not.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that a paper marked by an “AI detection” system as having a high probability
    of being written by AI is put in the same category as plagiarism is also ridiculous.
    Now, we don’t condone cheating, but LLMs are a new tool. They help us with language
    the way calculators help us with math. We have figured out ways to teach and evaluate
    students’ progress without creating “calculator detection” systems. We can do
    it again.
  prefs: []
  type: TYPE_NORMAL
- en: Look, it’s not that it’s impossible to identify generated content. One investigation
    found that by simply searching for phrases like “As an AI language model” or “As
    of my last knowledge update,” they found hundreds of published papers in scientific
    journals written with the help of LLMs.[²](#footnote-98) Some phrases are obvious
    signs, but these are only identified due to the pure laziness of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: The worst part of all this is that since these detection systems are fake, bad,
    and full of false positives, they seem to be enforced arbitrarily and randomly
    at the teacher’s discretion. It’s hard to believe that a majority of papers aren’t
    flagged, so why is only a select group of students called out for it? It’s because
    these systems appear to have become a weapon of power and discrimination for teachers
    who will wield them to punish students they don’t like—not to mention the obvious
    hypocrisy since we could guess that some of these teachers are the same ones publishing
    papers with phrases like “As an AI language model” in them.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and ethics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This isn’t the first time we have spoken about bias and ethics found inside
    LLMs, but this time, let’s take a slightly deeper dive into what the discussion
    deserves. Let’s say a person is tied to some trolley tracks, you do nothing, and
    the trolley runs them over, ending their life. Are you responsible? This thought
    experiment, called “The Trolley Problem,” has been discussed ad nauseam; there’s
    even a video game (Trolley Problem Inc. from Read Graves) that poses dozens of
    variations based on published papers. We won’t even attempt to answer the question,
    but we will give you a brief rundown on how you might be able to decide the answer
    for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are way more than two ways you can analyze this, but we’ll only focus
    on two—the moral and the ethical—and we’ll reduce these because this isn’t a philosophy
    book. Morality here helps you determine fault based on a belief of what is good/not
    good. Ethics help us determine consequences within the practical framework of
    the legal system that exists within the societies we live in. If you are morally
    responsible for the death of the person on the tracks, you believe that it was
    ultimately your fault, that your actions are the cause of the disliving. This
    is different from ethical responsibility, which would mean that you deserve legal
    and societal consequences for that action. They can agree, but they don’t have
    to. Changing the context can help clarify the distinction: if you tell someone
    that a knife isn’t sharp and they cut themselves on it while checking, morally,
    it’s likely your fault they were in that situation, but ethically, you will avoid
    an attempted murder charge.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms create thousands of these situations where our morality and our ethics
    likely don’t agree. There’s an old example of moral and ethical responsibility
    in the Talmud that decides that a person is not a murderer if they push another
    person into water or fire and the pushed person fails to escape.[³](#footnote-99)
    Depending on your beliefs and the law you live under, Meta could be either morally
    or ethically at fault for genocide (not joking[⁴](#footnote-100)) in Myanmar.
    Meta didn’t even do the pushing into the fire in that scenario; their algorithm
    did. This is obviously a charged and brutal example, but LLMs create a very real
    scenario where ML practitioners need practical, consistent, and defensible frameworks
    of both morality and ethics, or they risk real tragedy under their watch. Obviously,
    we aren’t the arbiters of morality and aren’t going to judge you about where you
    find yourself there, but you should still consider the broader context of any
    system you create.
  prefs: []
  type: TYPE_NORMAL
- en: Laws are coming
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One thing we *can* be sure about is that regulation will come, and companies
    will be held responsible for what their AI agents do. Air Canada found this out
    the hard way when the courts ruled against it, saying the company had to honor
    a refund policy that its chatbot had completely made up ([https://mng.bz/pxvG](https://mng.bz/pxvG)).
    The bot gave incorrect information. It did link the customer to the correct refund
    policy; however, the courts rightly questioned “why customers should have to double-check
    information found in one part of its website on another part of its website.”
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen similar cases where users have used prompt engineering to trick Chevy’s
    LLM chatbot into selling a 2024 Tahoe for $1 ([https://mng.bz/XVmG](https://mng.bz/XVmG)),
    and DPD needed to “shut down its AI element” after a customer got it to admit
    to being the worst delivery company in the world.[⁵](#footnote-101) As we said
    earlier, it’s difficult to tell, even with existing legislation, what is ethically
    allowable for an LLM to do. Of course, it brings up the question of whether, if
    the chatbot was licensed and equipped to sell cars and did complete such a transaction,
    the customer’s bad faith interaction would actually matter, or whether a company
    would still be held ethically responsible for upholding such a transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Being held responsible for what an LLM generates is enough to make you think
    twice about many applications you may consider using it for. The higher the risk,
    the more time you should take to pause and consider potential legal ramifications.
    We highly recommend dialing in your prompt engineering system, setting up guard
    rails to keep your agent on task, and absolutely being sure to save your logs
    and keep your customer chat history.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 LLMs are getting bigger
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another thing we can be sure of is that we will continue to see models getting
    bigger and bigger for the near future. Since larger models continue to display
    emergent behavior, there’s no reason for companies to stop taking this approach
    when simply throwing money at the problem seems to generate more money. Not to
    mention, for companies that have invested the most, larger models are harder to
    replicate. As you’ve probably found, the best way for smaller companies to compete
    is to create smaller, specialized models. Ultimately, as long as we have large-enough
    training datasets to accommodate more parameters, we can expect to see more parameters
    stuffed into a model, but the question of whether we’ve ever had adequate data
    to demonstrate “general intelligence” (as in AGI) is as murky as ever.
  prefs: []
  type: TYPE_NORMAL
- en: Larger context windows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s not just larger models. We are really excited to see context lengths grow
    as well. When we started working on this book, they were a real limitation. It
    was rare to see models with context lengths greater than 10K tokens. ChatGPT only
    offered lengths up to 4,096 tokens at the time. A year later, and we see models
    like Gemini 1.5 Pro offering a context length of up to 1 million tokens, with
    researchers indicating that it can handle up to 10 million tokens in test cases
    ([https://mng.bz/YV4N](https://mng.bz/YV4N)). To put it in perspective, the entire
    seven-book Harry Potter series is 1,084,170 words (I didn’t count them; [https://wordsrated.com/harry-potter-stats/](https://wordsrated.com/harry-potter-stats/)),
    which would come out to roughly 1.5 million tokens depending on your tokenizer.
    At these lengths, it’s hard to believe there are any limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, there still are. These larger models with near infinite context windows
    generally have you paying per token. If the model doesn’t force your users to
    send smaller queries, your wallet will. Not to mention, if you are reading this
    book, you are likely more interested in smaller open source models you can deploy
    yourself, and many of these definitely still have limiting context sizes you have
    to work with. Don’t worry, though; right now and in the future, even smaller models
    will have million-sized context windows. There’s a lot of interesting research
    going into this area. If you are interested, we recommend you check out RoPE,[⁶](#footnote-102)
    YaRN,[⁷](#footnote-103) and Hyena.[⁸](#footnote-104)
  prefs: []
  type: TYPE_NORMAL
- en: The next attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of course, larger context windows are great, but they come at a cost. Remember,
    at the center of an LLM lies the attention algorithm, which is quadratic in complexity—meaning
    the more data we throw at it, the more compute we have to throw at it as well.
    One challenge driving the research community is finding the next attention algorithm
    that doesn’t suffer from this same problem. Can we build transformers with a new
    algorithm that is only linear in complexity? That is the billion-dollar question
    right now.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of competing innovations in this field, and we don’t even have
    time to discuss all of our absolute favorites. Two of those favorites are MAMBA,
    an alternative to transformers, and KAN, an alternative to multilayer perceptrons
    (MLPs). MAMBA, in particular, is an improvement on state space models (SSMs) incorporated
    into an attention-free neural network architecture.[⁹](#footnote-105) By itself,
    it isn’t all that impressive, as it took lots of hardware hacking to make it somewhat
    performant. However, later JAMBA came out, a MAMBA-style model that uses hybrid
    SSM-transformer layers and joint attention.[^(10)](#footnote-106) The hybrid approach
    appears to give us the best of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: So you can experience it for yourself, in listing 12.1, we will finetune and
    run inference on a JAMBA model. This model is a mixture-of-experts model with
    52B parameters, and the implementation will allow for 140K context lengths on
    an 80 GB GPU, which is much better performance than you’d get with an attention
    model alone. This example was adapted right from the Hugging Face model card,
    so the syntax should look very familiar compared to every other simple transformer
    implementation, and we are very grateful for the ease of trying out brand-new
    stuff.
  prefs: []
  type: TYPE_NORMAL
- en: For the training portion, unfortunately, the model is too big, even in half
    precision, to fit on a single 80 GB GPU, so you’ll have to use Accelerate to parallelize
    it between several GPUs to complete training. If you don’t have that compute just
    lying around, you can complete the imports up to the tokenizer and skip to after
    the training portion, changing very little. We aren’t doing anything fancy; the
    dataset we’ll use for training is just a bunch of famous quotes in English from
    various authors retrieved from Goodreads consisting of quote, author, and tags,
    so don’t feel like you are missing out if you decide to skip finetuning. We’ll
    start by loading the tokenizer, model, and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Finetuning and inferencing JAMBA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once all of those are in memory (you can stream the dataset if your hardware
    is limited), we’ll create training arguments and a LoRA config to help the finetuning
    work on even smaller hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, for the finale, similar to sklearn’s `model.fit()`, transformers’
    `trainer.train()` has become a moniker for why anyone can learn how to interact
    with state-of-the-art ML models. Once training completes (it took a little under
    an hour for us), we’ll save local versions of the tokenizer and the model and
    delete the model in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll reload the model, but in a memory-efficient way, to be used for
    inference. With an 80 GB GPU and loading in 8bit with this BitsandBytes config,
    you can now fit the model and a significant amount of data on a single GPU. Loading
    in 4bit allows that on any type of A100 or two 3090s, similar to a 70B parameter
    transformer. Using quantization to get it down to a 1-bit model, you can fit this
    model and a significant amount of data on a single 3090\. We’ll use the following
    8bit inference implementation and run inference on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We are blown away almost monthly at this point by the alternatives to various
    parts of LLM systems that pop up. Here, we’d like to draw your attention way back
    to where LLMs got their big break: “Attention Is All You Need.”[^(11)](#footnote-107)
    That paper showed that you could use dumb MLPs to get amazing results, using only
    attention to bridge the gap. We’re entering a new age where we aren’t focusing
    on just what we need but what we want for the best results. For example, we want
    subquadratic drop-in replacements for attention that match or beat flash attention
    for speed. We want attention-free transformers and millions-long context lengths
    with no “lost in the middle” problems. We want alternatives to dense MLPs with
    no drops in accuracy or learning speed. We are, bit by bit, getting all of these
    and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the boundaries of compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After going down to INT4, there are experimental quantization strategies for
    going even further down to INT2\. INT2 70B models still perform decently, much
    to many peoples’ surprise. Then there’s research suggesting we could potentially
    go even smaller to 1.58 bits per weight or even 0.68 using ternary and other smaller
    operators. Want to test it out? Llama3 70B already has 1-bit quantization implementations
    in GGUF, GPTQ, and AWQ formats, and it only takes up 16.6 GB of memory. Go nuts!
  prefs: []
  type: TYPE_NORMAL
- en: There’s another dimension to this, which doesn’t involve compressing models
    but instead decouples the idea of models being one piece and thinking of models
    as collections of layers and parameters again. Speculative decoding gives us yet
    another way of accessing large models quickly. Speculative decoding requires not
    just enough memory to load one large model but also another smaller model alongside
    it—think distillation models. An example often used in production these days is
    Whisper-Large-v3 and Distil-Whisper-Large-V3\. Whisper is a multimodal LLM that
    focuses on the speech-to-text problem, but speculative decoding will work with
    any two models that have the same architecture and different sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method allows us to sample larger models quicker (sometimes a straight
    2× speed boost) by computing several tokens in parallel and by an approximation
    “assistant” model that allows us to both complete a step and verify whether that
    step is easy or hard at the same time. The basic idea is this: use the smaller,
    faster Distil-Whisper model to generate guesses about the end result, and allow
    Whisper to evaluate those guesses in parallel, ignoring the ones that it would
    do the same thing on and correcting the ones that it would change. This allows
    for the speed of a smaller model with the accuracy of a larger one.'
  prefs: []
  type: TYPE_NORMAL
- en: In listing 12.2, we demonstrate speculative decoding on an English audio dataset.
    We’ll load Whisper and Distil-Whisper, load the dataset, and then add an `assistant_
    model` to the generation keyword arguments (`generate_kwargs`). You may ask, how
    does this system know that the assistant model is only meant to help with decoding,
    as the name suggests? Well, we load the assistant model with `AutoModelForCausalLM`
    instead of the speech sequence-to-sequence version. This way, the model will only
    help with the easier decoding steps in parallel with the larger one. With that
    done, we’re free to test.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Speculative decoding with Whisper
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In our testing, we observed about 42 seconds for Whisper-Large-V3 to get through
    all 73 examples with scaled dot product attention. With speculative decoding,
    that dropped to 18.7 seconds, with the exact same word error rate (WER). So there
    was an almost 2× speed increase with absolutely zero drop in accuracy. Yeah, pretty
    nuts.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we were wondering, “Why doesn’t everyone use this for everything
    all the time?” Here are the drawbacks to this method: first, it works best in
    smaller sequences. With LLMs, that’s under 128 tokens of generation or around
    20 seconds of audio processing. With the larger generations, the speed boost will
    be negligible. Beyond that, we don’t always have access to perfectly compatible
    pairs of large and small models, like BERT versus DistilBERT. The last reason
    is that very few people really know about it, even with its ease of implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, whether it’s sub-bit quantization, speculative decoding, or other
    advances, LLMs are pushing research into compression methodologies more than any
    other technology, and it’s interesting to watch as new techniques change the landscape.
    As these methods improve, we can push models to smaller and cheaper hardware,
    making the field even more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Multimodal spaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are so excited about the possibilities within multimodality. Going back to
    chapter 2, multimodality is one of the main features of language we haven’t seen
    as many solutions crop up for, and we’re seeing a shift toward actually attempting
    to solve phonetics. Audio isn’t the only modality that humans operate in, though.
    Accordingly, the push toward combining phonetics, semantics, and pragmatics and
    getting as much context within the same embedding space (for comparison) as the
    text is very strong. With this in mind, here are some points of interest in the
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first we want to draw attention to is ImageBind, a project showcasing that
    instead of trying to curtail a model into ingesting every type of data, we can
    instead squish every type of data into an embedding space the model would already
    be familiar with and be able to process. You can take a look at the official demo
    here: [https://imagebind.metademolab.com/](https://imagebind.metademolab.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'ImageBind builds off what multimodal projection models such as CLIP have already
    been showcasing for some time: the ability to create and process embeddings is
    the true power behind deterministic LLM systems. You can use these models for
    very fast searches, including searches that have been, up to this point, nigh
    impossible, like asking to find images of animals that make sounds similar to
    an uploaded audio clip.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OneLLM flips this logic the other way around, taking one model and one multimodal
    encoder to unify and embed eight modalities instead of the ImageBind example of
    using six different encoders to embed six modalities in the same dimension. It
    can be found here: [https://onellm.csuhan.com/](https://onellm.csuhan.com/). The
    big idea behind OneLLM is aligning the unified encoder using language, which offers
    a unique spin on multimodality that aligns the process of encoding rather than
    the result.'
  prefs: []
  type: TYPE_NORMAL
- en: We are extremely excited about the research happening in this area. This research
    is able to help bridge the gap between phonetics and pragmatics in the model ecosystem
    and allow for more human-like understanding and interaction, especially in the
    search field.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One exciting change we are seeing inside the industry due to the introduction
    of LLMs is that companies are finally starting to understand the importance of
    governing and managing their data. For some, it’s the drive to finetune their
    own LLMs and get in on the exciting race to deliver AI products. For others, it’s
    the fear of becoming obsolete, as the capabilities of these systems far surpass
    previous technologies; they are finding it’s only their data that provides any
    type of moat or protection from competition. And for everyone, it’s the worry
    they’ll make the same mistakes they’ve seen other companies make.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs aren’t just a driving factor; they are also helping teams label, tag, organize,
    and clean their data. Many companies had piles of data they didn’t know what to
    do with, but with LLM models like CLIP, captioning images has become a breeze.
    Some companies have found that simply creating embedding spaces of their text,
    images, audio, and video has allowed them to create meaningful structures for
    datasets previously unstructured. Structured data is much easier to operate around,
    opening doors for search, recommendations, and other insights.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect we see currently missing in the industry is valuable open source
    datasets, especially when it comes to evaluations. Many of the current benchmarks
    used to evaluate models rely on multiple-choice questions, but this is inefficient
    for anyone trying to create an LLM application. In the real world, when are your
    users going to ask your model questions in a multiple-choice format? Next to never.
    People ask freeform questions in conversations and when seeking help since they
    don’t know the answer themselves. However, these evaluation datasets have become
    benchmarks simply because they are easy for researchers to gather, compile, and
    evaluate for accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we believe another inevitability is the need for more language
    representation. The world is a tapestry of diverse languages and dialects, each
    carrying its unique cultural nuances and communicative subtleties. However, many
    languages remain underrepresented in existing datasets, leading to models that
    are biased toward more dominant languages. As technology becomes increasingly
    global, the inclusion of a wider range of languages is crucial. Adding multiple
    languages not only promotes inclusivity but also enhances the accuracy and applicability
    of language models in various international contexts, bridging communication gaps
    and fostering a more connected world. Imagine your startup didn’t need to pay
    anyone to get accurate information regarding entering China, Russia, or Saudi
    Arabia to expand your market.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.5 Solving hallucination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s a lot of evidence that LLMs have more information in them than they
    readily give out and even more evidence that people are generally either terrible
    or malicious at prompting. As a result, you’ll find that hallucinations are one
    of the largest roadblocks when trying to develop an application that consistently
    delivers results. This problem has frustrated many software engineering teams
    that are used to deterministic computer algorithms and rarely deal with nondeterministic
    systems. For many statisticians who are more familiar with these types of systems,
    hallucinations are seen as a feature, not a bug. Regardless of where you stand,
    there’s a lot of research going into the best ways to handle hallucinations, and
    this is an area of interest you should be watching.
  prefs: []
  type: TYPE_NORMAL
- en: Better prompt engineering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One area that’s interesting to watch and has shown great improvement over time
    is prompt engineering. One prompt engineering tool that helps reduce hallucinations
    is DSPy. We went over it briefly in chapter 7, but here we’ll give an example
    of how it works and why it can be a helpful step for solving hallucination in
    your LLMs. We’ve discussed the fact that LLMs are characteristically bad at math,
    even simple math, several times throughout the book, and we’ve also discussed
    why, but we haven’t really discussed solutions other than improving your tokenization.
    So in listing 12.3, we will show just how good you can coax an LLM to be at math
    with zero tokenization changes, zero finetuning, and no LoRAs or DoRAs, just optimizing
    your prompts to tell the model exactly how to answer the questions you’re asking.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll do this using the dspy-ai Python package and Llama3-8B-Instruct. We’ll
    start by loading and quantizing the model to fit on most GPUs and the Grade-School
    Math 8K dataset. We picked this dataset because it’s a collection of math problems
    that you, as a person who has graduated elementary (primary) school, likely don’t
    even need a calculator to solve. We’ll use 200 examples for our train and test
    (dev) sets, although we’d recommend you play with these numbers to find the best
    ratio for your use case without data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 DSPy for math
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our imports and loading ready, we’ll need to address the fact
    that we loaded Llama3 using transformers and not DSPy. DSPy expects to interact
    with models utilizing the OpenAI API, but we have a model loaded locally from
    Hugging Face, DSPy has recently added HFModel to their package, and it can now
    be easily imported, rather than needing the wrapper defined next. First, we make
    a simple function to map any keyword argument differences between the APIs, like
    `max_tokens` vs `max_new_tokens`, and then we create a class that will act as
    the wrapper for our model to generate answers and optimize the prompt. Once that’s
    ready, we’ll load DSPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets up the LM'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets up ΔSPY to use that LM'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are prepared with an LLM to take our math test, let’s test it.
    We’ll start by establishing a baseline. We’ll define a simple chain-of-thought
    (CoT)-like prompt in the `QASignature` class, which we’ll use to define a zero-shot
    version to use as a baseline. The prompt is likely pretty close to prompts you’ve
    seen before, so hopefully, this will be a very relevant demonstration of tasks
    you may be working on. For evaluation, we’re using DSPy’s `gsm8k_metric`, which
    we imported at the top to evaluate against, but you could always create your own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Δefines the QASignature and CoT'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets up the evaluator, which can be used multiple times'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Evaluates how the LLM does with no changes'
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With our simple zero-shot CoT prompt, Llama3 gets only 14.5% of the questions
    correct. This result might not seem very good, but it is actually quite a bit
    better than just running the model on the questions alone without any prompt,
    which only yields about 1% to 5% correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the baseline out of the way, let’s move on to the bread and butter of
    DSPy, optimizing the prompt to see where that gets us. There’s been some evolution
    in what people think of as a CoT prompt since the original paper came out. CoT
    has evolved in the industry to mean more than just adding “think step by step”
    in your prompt since this approach is seen more as just basic prompt engineering,
    whereas allowing the model to few-shot prompt itself to get a rationale for its
    ultimate output is considered the new CoT, and that’s how the DSPy framework uses
    those terms. With that explanation, we’ll go ahead and create a `CoT` class using
    the `dspy.ChainOfThought` function and then evaluate it like we did our `ZeroShot`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets up the optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Optimize the prompts'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Evaluates our “optimized_cot” program'
  prefs: []
  type: TYPE_NORMAL
- en: Look at that! If it doesn’t astonish you that the accuracy jumped from 14.5%
    to 74.5% by changing only the prompts—remember we haven’t done any finetuning
    or training—we don’t know what will. People are speculating whether the age of
    the prompt engineer is over, but we’d like to think that it’s just begun. That
    said, the age of “coming up with a clever string and doing no follow-up” has been
    over and shouldn’t have ever started. In this example, we used arbitrary boundaries,
    gave the sections of the dataset and the numbers absolutely no thought, and didn’t
    include any helpful tools or context for the model to access to improve. If we
    did, you’d see that after applying all the prompt engineering tricks in the book,
    it isn’t difficult to push the model’s abilities to staggering levels, even on
    things LLMs are characteristically bad at—like math.
  prefs: []
  type: TYPE_NORMAL
- en: Grounding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you are looking for ways to combat hallucinations, you’ll run into the term
    *grounding*. Grounding is when we give the LLM necessary context in the prompt.
    By giving it the information it needs, we are helping to provide a solid base
    for the generation to build off of, so it’s less likely to dream up visions out
    of thin air. If this sounds familiar, it should, as we have used one of the most
    common grounding techniques, RAG, several times in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The term *RAG* (retrieval augmented generation) is, at face value, synonymous
    with grounding since we are literally retrieving the appropriate context based
    on the prompt and then using it to augment the text generated from the LLM. However,
    RAG has become synonymous with using semantic search with a VectorDB for the retrieval
    portion. Technically, you could use any type of search algorithm or any type of
    database, but if you tell someone in the industry you have set up a RAG system,
    they will assume the former architecture.
  prefs: []
  type: TYPE_NORMAL
- en: With that clarification, RAG applications are most useful for answering simple
    questions. Consider the question, “What is Gal Gadot’s husband’s current job?”
    It’s really two questions in one, “Who is Gal Gadot’s husband?” and once we know
    that, “What does he do?” RAG alone is pretty terrible at solving these multistep
    questions, as a similarity vector search will likely return many articles about
    Gal Gadot and probably none about Jaron Varsano, her husband.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can enhance this approach in an important way that we haven’t touched on
    yet: using knowledge graphs. Knowledge graphs store information in a structure
    that captures relationships between entities. This structure consists of nodes
    that represent objects and edges that represent relationships. A graph database
    like NEO4J makes it easy to create and query knowledge graphs. And as it turns
    out, knowledge graphs are amazing at answering more complex multipart questions
    where you need to connect the dots between linked pieces of information. Why?
    Because they’ve already connected the dots for us.'
  prefs: []
  type: TYPE_NORMAL
- en: Many teams who have struggled to get value out of RAG have been able to see
    large improvements once they transitioned to a graph database from a vector one.
    This comes with two major hurdles, though. First, we can no longer simply embed
    our prompts and pull similar matches; we have the much harder task of coming up
    with a way to turn our prompts into queries our graph database will understand.
    While there are several methods to take this on, it’s just another NLP problem.
    Thankfully, as it turns out, LLMs are really good at this! Second, and probably
    the bigger problem, is that it is much harder to turn your documents into a knowledge
    graph. This is why vector databases have become so popular—the ease of turning
    your data into embeddings to search against. Turning your data into a knowledge
    graph will be a bit more work and take additional expertise, but it can really
    set you up for success down the road.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, few teams are willing to invest in the extra data engineering to
    prepare their data into a knowledge graph. Most companies are still looking for
    quick wins, building simple wrappers around LLM APIs. As the industry matures,
    we believe we’ll start to see organizations shift toward building knowledge graphs
    from their proprietary data to eke out better performance from their LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge editing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another promising field of research to combat hallucinations is *knowledge editing*.
    Knowledge editing is the process of efficiently adjusting specific behaviors.
    Optimally, this would look like surgery where we precisely go in and change the
    exact model weights that activate when we get incorrect responses, as can be seen
    in figure 12.2\. Knowledge editing can be used for many things, but it is often
    used to combat factual decay—the fact that, over time, facts change, like who
    the current Super Bowl winner is or the current president of any individual country.
    We could retrain or finetune the model, but these are often much heavier solutions
    that may change the model in unexpected ways when all we want to do is update
    a fact or two.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/12-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 Knowledge editing is a technique to essentially perform surgery
    on a model to directly insert, update, or erase information.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Knowledge editing is an interesting field of research that we unfortunately
    didn’t have the space to go into in this book. A host of algorithms and techniques
    have been created to do it, like ROME, MEND, and GRACE. For those interested in
    using any of these techniques, we recommend first checking out EasyEdit at [https://github.com/zjunlp/EasyEdit](https://github.com/zjunlp/EasyEdit).
    EasyEdit is a project that has implemented the most common knowledge editing techniques
    and provides a framework to utilize them easily. It includes examples, tutorials,
    and more to get you started.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.6 New hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with most popular technologies, LLMs have already created a fierce market
    of competition. While most companies are still competing on capabilities and features,
    there’s also a clear drive to make them faster and cheaper. We’ve discussed many
    of these methods you can employ, like quantization and compilation. One we expect
    to see more of is innovation around hardware.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, Sam Altman, CEO of OpenAI, has been trying to raise funds to the tune
    of $7 trillion dollars to invest in the semiconductor industry.[^(12)](#footnote-108)
    We’ve talked about the global GPU shortage before, but no one is as annoyed about
    it as some of the biggest players. The investment would go further than just meeting
    demand; it would also accelerate development and research into better chips like
    Application-Specifc Integrated Circuits (ASICs).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve talked about and have used GPUs a lot throughout this book, but GPUs weren’t
    designed for AI; they were designed for graphics. Of course, that fact didn’t
    stop NVIDIA from briefly becoming the world’s most valuable company.[^(13)](#footnote-109)
    ASICs are designed for specific tasks; an example would be Google’s TPUs or tensor
    processing units. ASICs designed to handle AI workloads are NPUs (neural processing
    units), and chances are, you’ve never heard of, or at least never seen, an NPU
    chip before. We point this out to show there’s still plenty of room for improvement,
    and it’s likely we will see a large array of new accelerators in the future, from
    better GPUs to NPUs and everything in between. For more info, take a look at Cerebras
    ([https://cerebras.ai/product-chip/](https://cerebras.ai/product-chip/)).
  prefs: []
  type: TYPE_NORMAL
- en: One of the authors of this book spent a good portion of his career working for
    Intel and Micron developing the now-discontinued memory technology known as 3D
    XPoint (3DxP). The details of 3DxP aren’t important for this discussion; what
    it offered, extremely fast and cheap memory, is. It was sold under the brand name
    Optane for several years and had even earned the moniker “The Fastest SSD Ever
    Made.”[^(14)](#footnote-110) This technology proved itself to be almost as fast
    as RAM but almost as cheap to produce as NAND flash memory and could be used to
    replace either.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a world where every processor conveniently had 500 GB or even 1 TB of
    memory space. Most of the limitations we’ve discussed so far would simply disappear.
    You could load entire LLMs the size of GPT-4 onto one GPU. You wouldn’t have to
    worry about parallelization or the underutilization problems that come with the
    extra overhead. Did I mention 3DxP was nonvolatile as well? Load your model once,
    and you’re done; you’d never need to reload it, even if you had to restart your
    server, which would make jobs like autoscaling so much easier.
  prefs: []
  type: TYPE_NORMAL
- en: 3DxP was a technology that had already proven itself in the market as capable,
    but it nonetheless suffered due to a perceived lack of demand. Consumers didn’t
    know what to do with this new layer in the memory hierarchy that it provided.
    Personally, with the arrival of LLMs, the authors see plenty of demand now for
    a technology like this. We’ll just have to wait and see whether the semiconductor
    industry decides to reinvest.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.7 Agents will become useful
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lastly, we believe LLM-based agents will eventually be more than just a novelty
    that works only in demos. Most agents we’ve seen have simply been feats of magic,
    or should I say smoke and mirrors, throwing a few prompt engineering tricks at
    the largest models. The fact that several of them work at all—even in a limited
    capacity—shines light on the possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen several companies chase after the holy grail, building agents to
    replace software engineers. In fact, you’ll see them try to build agents to replace
    doctors, sales associates, or managers. But just as many companies and AI experts
    used to promise we’d have self-driving cars in the near future, that near future
    keeps on eluding us. Don’t get me wrong: it’s not like we don’t have self-driving
    cars, but they are much more of an annoyance than anything, and they can only
    drive in select locations as rideshare vehicles. In a similar fashion, we aren’t
    too worried about agents replacing any occupation.'
  prefs: []
  type: TYPE_NORMAL
- en: What we are more interested in are small agents—agents trained and finetuned
    to do a specialized task but with greater flexibility to hold conversations. Many
    video game NPCs would benefit from this type of setup where they could not only
    use an LLM to hold random conversations and provide a more immersive experience
    but also to decide to take actions that would shape a unique story.
  prefs: []
  type: TYPE_NORMAL
- en: We are also likely to see them do smaller tasks well first. For example, LLMs
    can already read your email and summarize them for you, but a simple agent would
    go a step further and generate email responses for you. Maybe it wouldn’t actually
    send them, but simply provide you with the options, and all you’d have to do is
    pick the one you want, and then it would send it.
  prefs: []
  type: TYPE_NORMAL
- en: But mostly, we are excited to see LLM agents replace other bots. For example,
    who hasn’t uploaded their resume only to find they have to reenter all their information?
    Either because the resume extraction tool didn’t work well or it didn’t even exist.
    An LLM agent can not only read your resume and extract the information but also
    double-check its work and make sure it makes sense. Plus, we haven’t even mentioned
    the applicant tracking systems that automatically screen resumes based on keywords.
    These systems are often easily manipulated and terrible at separating the cream
    from the crop. An LLM agent has a much better chance of performing this task well.
    Of course, we care about ensuring fair hiring practices, but these systems are
    already automated and biased to some extent. A better model is an opportunity
    to reduce that non-useful bias.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, one way that models might make better agents is through the
    use of cache embeddings. It’s an interesting idea of something you can do with
    models that we haven’t really heard anyone talking about, other than Will Gaviro
    Rojas at a local Utah meetup. Caching embeddings allows you to cut down on repeating
    the same computations several times to complete several tasks in parallel. This
    is a more complex example, and we aren’t going to dive too deep into it so as
    to keep things pretty simple, but this strategy involves either copying the final
    layers of a model after the last hidden state to complete several tasks on their
    own or creating custom linear classifiers to fulfill those tasks. In listing 12.4,
    we dive into the entire system surrounding caching the embeddings, as we assume
    knowledge at this point of how to store embeddings for access later.
  prefs: []
  type: TYPE_NORMAL
- en: We start by loading Llama3-ChatQA in INT4 quantization with BitsandBytes to
    make sure it fits on smaller consumer GPUs, which should be familiar at the end
    of this book. We give it the appropriate prompt structure for the given model,
    and we get our outputs. Then we access the last hidden state or the embeddings
    with `outputs.last_ hidden_states` and show how we could either create copies
    of the relevant layers to put that hidden state through (provided they’re trained
    to handle this) or create a custom linear classifier in PyTorch that can be fully
    trained on any classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Caching embeddings for multiple smaller models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Traditional generation'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Embedding'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Finds the LM Head layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Custom Trainable classifier'
  prefs: []
  type: TYPE_NORMAL
- en: This decoupling of the idea of models as monoliths that connect to other systems
    is very engineering-friendly, allowing for one model to output hundreds of classifications
    around a single data point, thanks to embeddings. LangChain provides a `CacheBackedEmbeddings`
    class to help with caching the vectors quickly and conveniently if you’re working
    within that class, and we think that name is pretty great for the larger idea
    as well—backing up your embedding process with caching to be fed to multiple linear
    classifiers at once. This approach allows us to detect anything from inappropriate
    user input all the way to providing a summarized version of the embeddings back
    to the real model for quicker and more generalized processing.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Final thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We really hope you enjoyed this book and that you learned something new and
    useful. It’s been a huge undertaking to write the highest quality book we could
    muster, and sometimes it was less about what we wrote and more about what we ended
    up throwing out. Believe it or not, while being as comprehensive as we could,
    there are many times we’ve felt we’d only scratched the surface of most topics.
    Thank you for going on this journey with us.
  prefs: []
  type: TYPE_NORMAL
- en: We are so excited about where this industry is going. One of the hardest parts
    of writing this book was choosing to focus on the current best practices and ignoring
    much of the promising research that seems to be piling on, especially as companies
    and governments increase funding into the incredible possibilities that LLMs promise.
    We’re excited to see more research that’s been around for years or even decades
    be applied to LLMs and see new research come from improving those results. We’re
    also excited to watch companies change and figure out how to deploy and serve
    LLMs much better than they currently are. It’s difficult to market LLM-based products
    using traditional methods without coming off as just lying. People want to see
    the product work exactly as demonstrated in the ad, and we’re hoping to see changes
    there.
  prefs: []
  type: TYPE_NORMAL
- en: What an exciting time! There’s still so much more to learn and explore. Because
    we have already seen the industry move while we’ve been writing, we’d like to
    invite you to submit PRs in the GitHub repo to help keep the code and listings
    up to date for any new readers. While this is the end of the book, we hope it’s
    just the beginning of your journey into using LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are quickly challenging current laws and regulations and the interpretations
    thereof.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fear of LLMs being used for cheating has hurt many students with the introduction
    of AI detection systems that don’t work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are only getting bigger, and we will need solutions like better compression
    and the next attention algorithm to compensate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings are paving the way to multimodal solutions with interesting approaches
    like ImageBind and OneLLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is likely to be one of the largest bottlenecks and constraints to future
    improvements, largely starting with a lack of quality evaluation datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For use cases where they are a problem, hallucinations will continue to be so,
    but methodologies to curb their effects and frequency of occurrence are becoming
    quite sophisticated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs continue to suffer due to GPU shortages and will help drive research and
    innovation to develop more powerful computing systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM Agents don’t provide a pathway to AGI, but we will see them graduate from
    toys to tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) M. M. Grynbaum and R. Mac, “The Times Sues OpenAI
    and Microsoft Over A.I. Use of Copyrighted Work,” The New York Times, December
    27, 2023, [https://mng.bz/6Y0D](https://mng.bz/6Y0D).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) E. Maiberg, “Scientific journals are publishing papers
    with AI-generated text,” 404 Media, March 18, 2024, [https://mng.bz/n0og](https://mng.bz/n0og).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#footnote-source-3) Sanhedrin 76b:11, [https://mng.bz/vJaJ](https://mng.bz/vJaJ).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#footnote-source-4) “Myanmar army behind Facebook pages spewing hate
    speech: UN probe,” RFI, March 27, 2024, [https://mng.bz/mR0P](https://mng.bz/mR0P).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#footnote-source-5) A. Guzman, “Company disables AI after bot starts
    swearing at customer, calls itself the ‘worst delivery firm in the world,’” NY
    Post, January 20, 2024, [https://mng.bz/yoVq](https://mng.bz/yoVq).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#footnote-source-6) emozilla, “Dynamically Scaled RoPE further increases
    performance of long context LLaMA with zero fine-tuning,” Jun. 30, 2023, [https://mng.bz/M1pn](https://mng.bz/M1pn).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#footnote-source-7) B. Peng, J. Quesnelle, H. Fan, E. Shippole, N. Research,
    and Eleutherai, “YaRN: Efficient Context Window Extension of Large Language Models.”
    Available: [https://arxiv.org/pdf/2309.00071](https://arxiv.org/pdf/2309.00071)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#footnote-source-8) M. Poli et al., “Hyena Hierarchy: Towards Larger
    Convolutional Language Models,” Feb. 2023, doi: [https://doi.org/10.48550/arxiv.2302.10866](https://doi.org/10.48550/arxiv.2302.10866).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#footnote-source-9) A. Gu and T. Dao, “Mamba: Linear-Time Sequence Modeling
    with Selective State Spaces,” arXiv.org, Dec. 01, 2023, [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#footnote-source-10) [1]O. Lieber et al., “Jamba: A Hybrid Transformer-Mamba
    Language Model,” arXiv.org, Mar. 28, 2024, [https://arxiv.org/abs/2403.19887](https://arxiv.org/abs/2403.19887).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#footnote-source-11) Vaswani et al., Attention Is All You Need,” 2017,
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#footnote-source-12) K. H. and A. Fitch, “Sam Altman seeks trillions
    of dollars to reshape business of chips and AI,” Wall Street Journal, February
    8, 2024, [https://mng.bz/KDrK](https://mng.bz/KDrK).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#footnote-source-13) A. Pequeño IV, “Nvidia now world’s most valuable
    company—Topping Microsoft and Apple,” Forbes, June 18, 2024, [https://mng.bz/9ojl](https://mng.bz/9ojl).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#footnote-source-14) S. Webster, “Intel Optane SSD DC P5800X review:
    The fastest SSD ever made,” Tom’s Hardware, August 26, 2022, [https://mng.bz/j0Wx](https://mng.bz/j0Wx).'
  prefs: []
  type: TYPE_NORMAL
