- en: '12 Production, an ever-changing landscape: Things are just getting started'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 生产，一个不断变化的景观：一切才刚刚开始
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A brief overview of LLMs in production
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM在生产中的简要概述
- en: The future of LLMs as a technology and several exciting fields of research into
    it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs作为一项技术和对其进行的几个令人兴奋的研究领域
- en: Our closing remarks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的结束语
- en: The Web as I envisaged it, we have not seen it yet. The future is still so much
    bigger than the past.—Tim Berners-Lee (inventor of www)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如我所设想的网络，我们还没有看到它。未来仍然比过去大得多。——蒂姆·伯纳斯-李（www的发明者）
- en: Wow! We’ve really covered a lot of ground in this book. Is your head just about
    ready to explode? Because ours are, and we wrote the book. Writing this book has
    been no easy feat, as the industry has been constantly changing—and fast. Trying
    to stay on top of what’s happening with LLMs has been like trying to build a house
    on quicksand; you finish one level, and it seems to have already sunk before you
    can start the next. We know that portions of this book will inevitably become
    out of date, and that’s why we tried our best to stick to core concepts, the sturdy
    rocks in the sand, that will never change.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！在这本书中，我们确实覆盖了大量的内容。你的头脑是不是快要爆炸了？因为我们的确实是这样，我们写了这本书。写这本书并不容易，因为行业一直在不断变化——而且变化很快。试图跟上LLMs的发展就像在流沙上建造房屋；你完成了一层，似乎在你开始下一层之前它就已经沉下去了。我们知道这本书的部分内容不可避免地会过时，这就是为什么我们尽力坚持核心概念，这些概念就像沙子中的坚固岩石，永远不会改变。
- en: In this chapter, we wanted to take a step back and review some of the major
    takeaways we hope you will walk away with. We’ve spent a lot of time getting into
    the weeds and paying attention to details, so let’s reflect for a moment to see
    the whole picture and review what we’ve covered. After that, we’ll take a minute
    to discuss the future of the field and where we can expect to see some of the
    next major breakthroughs. Finally, we’ll leave you with our final thoughts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们想退后一步，回顾一些我们希望你能带走的主要收获。我们花了很多时间深入细节，所以让我们暂时反思一下，看看整个画面，回顾我们已经覆盖的内容。之后，我们将花一点时间讨论该领域的未来，以及我们可以期待看到的一些下一个重大突破。最后，我们将留下我们的最终想法。
- en: 12.1 A thousand-foot view
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 千米视角
- en: 'We have gone over a lot of material in this book—from making a bag-of-words
    model to serving an LLM API on a Raspberry Pi. If you made it all the way through
    the whole book, that’s an accomplishment. Great work! We are not going to recap
    everything, but we wanted to take a second to see the forest from the trees, as
    it were. To summarize much of what we’ve covered, we can split most of the ideas
    into four distinct but very closely tied quadrants: Preparation, Training, Serving,
    and Developing. You can see these quadrants in figure 12.1\. You’ll notice that
    along with these sections, there’s a fifth one distinct from the others, which
    we labeled Undercurrents. These are elements that seem to affect all of the other
    quadrants to varying degrees and things you’ll have to worry about during each
    stage of an LLM product life cycle.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们讨论了大量的内容——从制作词袋模型到在树莓派上部署LLM API。如果你读完了整本书，那是一项成就。干得好！我们不会回顾所有内容，但我们都想从树木中看到森林，总结一下我们所学到的很多东西。我们可以将大多数想法分为四个截然不同但非常紧密相关的象限：准备、训练、部署和开发。你可以在图12.1中看到这些象限。你会注意到，除了这些部分，还有一个与其他部分不同的第五个部分，我们将其标记为潜流。这些是似乎以不同程度影响所有其他象限的元素，以及你在LLM产品生命周期的每个阶段都必须关注的事情。
- en: '![figure](../Images/12-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/12-1.png)'
- en: Figure 12.1 LLM product life cycle. Here are all the key concepts discussed
    in the book, along with where they generally fit within the production environment.
    Undercurrents are important elements that show up in every part of the life cycle—for
    example, linguistics informs preparation, creates metrics in training and serving,
    and influences prompting and development.
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1 LLM产品生命周期。这里列出了书中讨论的所有关键概念，以及它们通常在生产环境中的位置。潜流是生命周期中每个部分的重要元素——例如，语言学在准备阶段提供信息，在训练和部署阶段创建指标，并影响提示和开发。
- en: Hopefully, if it wasn’t clear when we were talking about a concept in an earlier
    chapter, it’s clear now exactly where that concept fits in a production life cycle.
    You’ll notice that we’ve likely put some elements in places that your current
    production environment doesn’t reflect—for example, provisioning of the MLOps
    infrastructure doesn’t often actually happen within the preparation stage but
    is rather haphazardly thrown together the first time that serving needs to happen.
    We get it. But during preparation is where we feel it *should* happen. Take a
    moment to digest all that you’ve learned while reading this book, and consider
    how the pieces all come together.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 希望当我们之前在章节中讨论一个概念时，如果还没有讲清楚，现在应该很清楚这个概念在生产生命周期中的位置。你会注意到，我们可能将一些元素放在了你的当前生产环境并不反映的位置——例如，MLOps基础设施的配置通常并不发生在准备阶段，而是在第一次需要提供服务时随意拼凑。我们理解这一点。但在准备阶段，我们觉得它*应该*在那里。花点时间消化你在阅读这本书时所学到的所有内容，并考虑所有这些部分是如何结合在一起的。
- en: Given this abstract and idealized version of a production life cycle, let’s
    move to the things not currently included there. What might we need to add to
    our development portion five years down the line, especially given how fast the
    field moves now?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个抽象和理想化的生产生命周期版本中，让我们转向目前尚未包括其中的事物。五年后，我们可能需要在我们的开发部分添加什么，特别是在考虑到这个领域现在发展如此迅速的情况下？
- en: 12.2 The future of LLMs
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 LLMs的未来
- en: When we wrote this book, we made a conscious effort to focus on the foundational
    knowledge you will need to understand how LLMs work and how to deploy them to
    production. This information is crucial, as production looks very different for
    every single use case. Learning how to weigh the pros and cons of any decision
    requires that foundational knowledge if you have any hope of landing on the right
    one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们撰写这本书时，我们有意专注于你需要的基础知识，以便理解LLMs是如何工作的以及如何将它们部署到生产环境中。这些信息至关重要，因为每个用例的生产情况都大不相同。学习如何权衡任何决策的利弊，需要这些基础知识，这样你才有可能做出正确的选择。
- en: Adjacent to this decision, we didn’t want this book to be all theory. We wanted
    it to be hands-on, with enough examples that you as a reader wouldn’t just know
    how things worked but would get a sense of how they feel—like getting a feel for
    how long it takes to load a 70B model onto a GPU, sensing what the experience
    will be like for your user if you run the model on an edge device, and feeling
    the soft glow of your computer monitor as you hide in a dark cave pouring over
    code and avoiding the warm sun on a nice spring day.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与此决定相邻，我们不希望这本书只包含理论。我们希望它是实践性的，有足够的例子，让你作为读者不仅知道事物是如何工作的，而且能感受到它们的感觉——比如感受将一个70B模型加载到GPU上需要多长时间，如果你在边缘设备上运行该模型，你能感受到用户将会有怎样的体验，以及当你躲在黑暗的山洞里埋头于代码、避开春日温暖的阳光时，你能感受到电脑屏幕柔和的光芒。
- en: One of the hardest decisions we made when we wrote this book was deciding to
    focus on the here and now. We decided to focus on the best methods that we actually
    see people using in production today. This decision was hard because over the
    course of writing this book, there have been many mind-blowing research papers
    we’ve been convinced will “change everything.” However, for one reason or another,
    that research has yet to make it to production. In this section, we are going
    to change that restriction and talk about what’s up and coming regardless of the
    current state of the industry. But it’s not just research; public opinions, lawsuits,
    and political landscapes often shape the future of technology as well. We’ll be
    looking at where we see LLMs going in the next several years and mention some
    of the directions they could take.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写这本书的过程中，我们做出的最艰难的决定之一就是决定专注于当下。我们决定关注那些我们实际上看到人们在今天的生产环境中使用的最佳方法。这个决定之所以艰难，是因为在撰写这本书的过程中，我们遇到了许多令人震惊的研究论文，我们确信这些论文将“改变一切”。然而，由于种种原因，这些研究尚未进入生产阶段。在本节中，我们将打破这一限制，无论行业当前状态如何，都将讨论即将到来的趋势。但不仅仅是研究；公众舆论、诉讼和政治格局也常常塑造着技术的未来。我们将探讨在接下来的几年里，我们认为LLMs将走向何方，并提及它们可能采取的一些方向。
- en: 12.2.1 Government and regulation
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 政府和监管
- en: 'At the beginning of this book, we promised to show you how to create LLM products,
    not just demos. While we believe we have done just that, there’s one important
    detail we’ve been ignoring: the fact that products live in the real world. While
    demos just have to work in isolation, products have to work in general. Products
    are meant to be sold, and once there’s an exchange of currency, expectations are
    set, reputations are on the line, and ultimately, governments are going to get
    involved.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的开头，我们承诺向您展示如何创建LLM产品，而不仅仅是演示。虽然我们相信我们已经做到了这一点，但我们一直忽略了一个重要细节：产品存在于现实世界中。演示只需要在孤立的环境中工作，而产品必须在一般情况下工作。产品是为了出售的，一旦货币交换发生，就会设定期望，声誉就会受到考验，最终，政府将介入。
- en: While a team can’t build for future regulations that may never come, it’s important
    to be aware of the possible legal ramifications of the products you build. One
    lost lawsuit can set a precedent that brings a tidal wave of copycat lawsuits.
    Since products live in the real world, it is best that we pay attention to that
    world.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一个团队不能为可能永远不会到来的未来法规而建造，但了解您构建的产品可能产生的法律后果是很重要的。一场败诉的案件可以设定先例，引发模仿诉讼的浪潮。由于产品存在于现实世界中，我们最好关注那个世界。
- en: 'One of us had the opportunity to participate in Utah’s legislative process
    for Utah’s SB-149 Artificial Intelligence Amendments bill. This bill is primarily
    concerned with introducing liability to actors using LLMs to skirt consumer protection
    laws in the state. At the moment, every legislative body is attempting to figure
    out where its jurisdiction starts and ends concerning AI and how to deal with
    the increased responsibility it has to protect citizens and corporations within
    its constituency. In Utah, the state government takes a very serious and business-first
    approach to AI and LLMs. Throughout the process and the bill itself, the legislature
    cannot create definitions that aren’t broken with “behold, a man” Diogenes-style
    examples, and we will need every bit of good faith to navigate the new world that
    LLMs bring to regulatory bodies. How do you define AI? The bill defines it as
    follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中的一员有机会参与犹他州SB-149人工智能修正法案的立法过程。该法案主要关注引入对使用LLM规避州内消费者保护法律的行动者的责任。目前，每个立法机构都在试图弄清楚其在AI方面的管辖权从何开始到何结束，以及如何处理其对保护其选民中的公民和公司所承担的日益增加的责任。在犹他州，州政府对AI和LLM采取了非常严肃和以商业为先的方法。在整个过程中以及法案本身，立法机构不能创建不与“看哪，一个人”第欧根尼风格的例子相冲突的定义，我们将需要每一份善意来导航LLM为监管机构带来的新世界。你如何定义AI？法案如下定义：
- en: “Artificial intelligence” means a machine-based system that makes predictions,
    recommendations, or decisions influencing real or virtual environments.
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人工智能”是指一种基于机器的系统，它做出预测、推荐或决策，影响真实或虚拟环境。
- en: This could be anything from a piecewise function to an LLM agent, meaning that
    your marketing team will not be liable for claims that your `if` statements are
    AI within the state. That said, the bill contains a thorough and well-thought-out
    definition of a deceptive act by a supplier, along with the formulation of an
    AI analysis and research program to help the state assess risks and policy in
    a more long-term capacity, which seems novel and unique to Utah. The Utah state
    legislature was able to refine this bill by consulting with researchers, experts,
    c-level executives, and business owners within the state, and we’d encourage the
    reader to participate in creating worthwhile and meaningful regulations within
    your communities and governments. This is the only way to make sure that court
    systems are prepared to impose consequences where they are due in the long term.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括分段函数到LLM代理的任何东西，这意味着您的营销团队不会对您的`if`语句是处于状态中的AI的声明负责。话虽如此，该法案包含了对供应商欺诈行为的详尽和深思熟虑的定义，以及制定了一个AI分析和研究计划，以帮助州政府从更长期的角度评估风险和政策，这看起来对犹他州来说是新颖且独特的。犹他州立法机构通过与州内的研究人员、专家、C级高管和企业家进行咨询，能够完善这项法案，我们鼓励读者参与在您所在的社区和政府中制定有价值且有意义的法规。这是确保法院系统长期准备好在应受惩罚的地方实施惩罚的唯一方式。
- en: Copyright
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 版权
- en: At the forefront of legal concerns is that of copyright infringement. LLMs trained
    on enough data can impersonate or copy the style of an author or creator or even
    straight-up word-for-word plagiarize. While this is exciting when considering
    building your own ghostwriter to help you in your creative process, it’s much
    less so when you realize a competitor could do the same.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在法律担忧的前沿是版权侵权问题。在足够的数据上训练的LLM可以模仿或复制作者或创作者的风格，甚至直接一字不漏地剽窃。当考虑到构建自己的枪手以帮助你在创作过程中时，这很令人兴奋，但当你意识到竞争对手也能这样做时，这就不那么令人兴奋了。
- en: Probably the biggest lawsuit to pay attention to is that of *The New York Times*
    v. OpenAI.[¹](#footnote-97) *The New York Times* is in the process of legal action
    against OpenAI, stating their chatbots were trained on the *Times*’ intellectual
    property without consent. It gives evidence that the chatbots are giving word-for-word
    responses identical to proprietary information found in articles a user would
    normally have to pay to see. As a result, there is the concern that fewer users
    will visit their site, reducing ad revenue. Essentially, they stole their data
    and are now using it as a competitor in the information space.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要关注的最大诉讼是《纽约时报》诉OpenAI。[¹](#footnote-97)《纽约时报》正在对OpenAI提起法律诉讼，称其聊天机器人未经同意就在《时报》的知识产权上进行了训练。它提供了证据，表明聊天机器人给出的逐字逐句的回应与用户通常需要付费才能看到的专有信息相同。因此，人们担心用户访问其网站的人数会减少，从而减少广告收入。本质上，他们窃取了他们的数据，现在正在信息空间中作为竞争对手使用。
- en: Bystanders to the fight worry that if the *Times* wins, it may significantly
    hamper the development of AI and cause the United States to lose its position
    as the leader in the global AI development race. The more AI companies are exposed
    to copyright liability, the greater risk and thus loss to competition, which means
    less innovation. Conversely, they also worry that if the *Times* loses, it will
    further cut into the already struggling journalism business, where it’s already
    hard enough to find quality reports you can trust. This, too, would severely hurt
    AI development, which is always starving for good clean data. It appears to be
    a lose–lose situation for the AI field.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗这场斗争的旁观者担心，如果《时报》胜诉，可能会严重阻碍AI的发展，导致美国在全球AI发展竞赛中的领先地位受损。AI公司面临更大的版权责任风险，从而带来更大的竞争损失，这意味着更少的创新。相反，他们也担心，如果《时报》败诉，将进一步削弱已经陷入困境的新闻业，在那里，找到可以信赖的高质量报道已经很困难了。这对AI发展也是一个巨大的打击，AI发展总是渴望得到好的干净数据。这似乎是AI领域的一个双输局面。
- en: Regardless of who wins or loses the lawsuit, it’s pretty clear that current
    copyright laws never took into consideration that robots would eventually copy
    us. We need new laws, and it’s unclear whether our lawmakers are technically capable
    enough to meet the challenge. So again, we’d encourage you to participate in the
    creation process of regulations within your own communities.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 无论谁胜谁负，诉讼的结果都很明显，现行的版权法从未考虑过机器人最终会复制我们。我们需要新的法律，而且不清楚我们的立法者是否具备足够的技术能力来应对这一挑战。因此，我们再次鼓励你参与你所在社区内法规的制定过程。
- en: AI detection
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AI检测
- en: 'One area of concern that continues to break our hearts comes from the rise
    of “AI detection” products. Let us just state from the start: these products are
    all snake oils and shams. There’s no reliable way to determine whether a piece
    of text was written by a human or a bot. By this point in the book, we expect
    most readers to have come to this conclusion as well. The reason is simple: if
    we can reliably determine what is and isn’t generated text, we can create a new
    model to beat the detector. This is the whole point of adversarial machine learning.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个持续让我们心碎的担忧领域来自于“AI检测”产品的兴起。让我们一开始就明确：这些产品都是骗人的。没有可靠的方法来确定一段文本是由人类还是机器人所写。到这本书的这一部分，我们期望大多数读者也已经得出了这个结论。原因很简单：如果我们能够可靠地确定哪些是哪些不是生成文本，我们就可以创建一个新的模型来击败检测器。这正是对抗性机器学习的全部意义。
- en: 'There has been a running gag online that anything you read with the word “delve”
    in it must be written by an LLM (e.g., [https://mng.bz/o0nr](https://mng.bz/o0nr)).
    The word *delve* is statistically more likely to occur in generated text than
    in human speech, but that brings up the obvious questions: Which model? Which
    prompt? The human hubris to believe one can identify generated content simply
    by looking for particular words is laughable. But, of course, if people vainly
    believe this obvious falsehood, it’s no surprise they are willing to believe a
    more complex system or algorithm will be able to do it even better.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在网上有一个流行的玩笑，任何包含“深入挖掘”这个词的阅读内容都必须是由LLM撰写的（例如，[https://mng.bz/o0nr](https://mng.bz/o0nr)）。这个词*深入挖掘*在生成文本中比在人类语言中更可能出现，但这提出了明显的问题：哪个模型？哪个提示？人类自大的想法，认为仅通过寻找特定的单词就能识别生成内容，是可笑的。但当然，如果人们盲目地相信这种明显的错误，那么他们愿意相信一个更复杂或更先进的系统或算法能够做得更好，也就不足为奇了。
- en: The reason it breaks our hearts, though, is because we’ve read story after story
    of students getting punished, given failing grades on papers, forced to drop out
    of classes, and given plagiarism marks on their transcripts. Now, we don’t know
    the details of every case, but as experts in the technology in question, we choose
    to believe the students more often than not.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这让我们心碎的原因是因为我们读过一篇又一篇关于学生受到惩罚、论文被给予不及格分数、被迫退课以及在成绩单上被标记剽窃的故事。现在，我们不知道每个案例的细节，但作为相关技术的专家，我们更倾向于相信学生而非其他。
- en: The fact that a paper marked by an “AI detection” system as having a high probability
    of being written by AI is put in the same category as plagiarism is also ridiculous.
    Now, we don’t condone cheating, but LLMs are a new tool. They help us with language
    the way calculators help us with math. We have figured out ways to teach and evaluate
    students’ progress without creating “calculator detection” systems. We can do
    it again.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将被“AI检测”系统标记为有高概率由AI撰写的论文与剽窃归为同一类别也是荒谬的。现在，我们并不支持作弊，但大型语言模型（LLMs）是一种新工具。它们帮助我们处理语言，就像计算器帮助我们处理数学一样。我们已经找到了在不创建“计算器检测”系统的情况下教授和评估学生进步的方法。我们也可以再次做到这一点。
- en: Look, it’s not that it’s impossible to identify generated content. One investigation
    found that by simply searching for phrases like “As an AI language model” or “As
    of my last knowledge update,” they found hundreds of published papers in scientific
    journals written with the help of LLMs.[²](#footnote-98) Some phrases are obvious
    signs, but these are only identified due to the pure laziness of the authors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，识别生成内容并不是不可能的。一项调查发现，通过简单地搜索“作为人工智能语言模型”或“截至我最后一次知识更新”等短语，他们发现了数百篇在科学期刊上发表的、在LLMs帮助下撰写的论文。[²](#footnote-98)
    一些短语是明显的迹象，但这些只是由于作者们的纯粹懒惰而被识别。
- en: The worst part of all this is that since these detection systems are fake, bad,
    and full of false positives, they seem to be enforced arbitrarily and randomly
    at the teacher’s discretion. It’s hard to believe that a majority of papers aren’t
    flagged, so why is only a select group of students called out for it? It’s because
    these systems appear to have become a weapon of power and discrimination for teachers
    who will wield them to punish students they don’t like—not to mention the obvious
    hypocrisy since we could guess that some of these teachers are the same ones publishing
    papers with phrases like “As an AI language model” in them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些中最糟糕的部分是，由于这些检测系统是虚假的、糟糕的，并且充满了误报，它们似乎是由教师任意和随机地执行的。很难相信大多数论文没有被标记，那么为什么只有一小部分学生被点名批评呢？这是因为这些系统似乎已经变成了教师手中的权力和歧视武器，他们会利用这些系统来惩罚他们不喜欢的学生——更不用说这种明显的虚伪，因为我们猜测这些教师中的一些人可能就是那些在论文中使用“作为人工智能语言模型”等短语的人。
- en: Bias and ethics
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏见与伦理
- en: This isn’t the first time we have spoken about bias and ethics found inside
    LLMs, but this time, let’s take a slightly deeper dive into what the discussion
    deserves. Let’s say a person is tied to some trolley tracks, you do nothing, and
    the trolley runs them over, ending their life. Are you responsible? This thought
    experiment, called “The Trolley Problem,” has been discussed ad nauseam; there’s
    even a video game (Trolley Problem Inc. from Read Graves) that poses dozens of
    variations based on published papers. We won’t even attempt to answer the question,
    but we will give you a brief rundown on how you might be able to decide the answer
    for yourself.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们第一次讨论LLMs（大型语言模型）中发现的偏见和伦理问题，但这次，让我们更深入地探讨这次讨论应得的讨论。假设一个人被绑在轨道上，你什么也没做，电车撞上了他们，结束了他们的生命。你是否有责任？这个被称为“电车问题”的思想实验已经被讨论得淋漓尽致；甚至有一个基于已发表论文的电子游戏（Read
    Graves的Trolley Problem Inc.），提出了数十种变化。我们甚至不会尝试回答这个问题，但我们会简要介绍一下你如何自己决定答案。
- en: 'There are way more than two ways you can analyze this, but we’ll only focus
    on two—the moral and the ethical—and we’ll reduce these because this isn’t a philosophy
    book. Morality here helps you determine fault based on a belief of what is good/not
    good. Ethics help us determine consequences within the practical framework of
    the legal system that exists within the societies we live in. If you are morally
    responsible for the death of the person on the tracks, you believe that it was
    ultimately your fault, that your actions are the cause of the disliving. This
    is different from ethical responsibility, which would mean that you deserve legal
    and societal consequences for that action. They can agree, but they don’t have
    to. Changing the context can help clarify the distinction: if you tell someone
    that a knife isn’t sharp and they cut themselves on it while checking, morally,
    it’s likely your fault they were in that situation, but ethically, you will avoid
    an attempted murder charge.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分析这种问题的方法远不止两种，但我们只会关注其中两种——道德和伦理——并且我们会简化这些概念，因为这不是一本哲学书。在这里，道德帮助你根据对好/不好的信念来判断过错。伦理帮助我们确定在我们所生活的社会中的法律体系内的实际框架中的后果。如果你对轨道上的人的死亡负有道德责任，你相信这是你最终的责任，你的行为是导致他们死亡的原因。这与伦理责任不同，这意味着你因该行为应受到法律和社会的后果。他们可以同意，但不必如此。改变语境可以帮助阐明区别：如果你告诉某人一把刀不锋利，他们在检查时切到了自己，从道德上讲，他们陷入那种情况可能是你的责任，但从伦理上讲，你会避免被控企图谋杀。
- en: Algorithms create thousands of these situations where our morality and our ethics
    likely don’t agree. There’s an old example of moral and ethical responsibility
    in the Talmud that decides that a person is not a murderer if they push another
    person into water or fire and the pushed person fails to escape.[³](#footnote-99)
    Depending on your beliefs and the law you live under, Meta could be either morally
    or ethically at fault for genocide (not joking[⁴](#footnote-100)) in Myanmar.
    Meta didn’t even do the pushing into the fire in that scenario; their algorithm
    did. This is obviously a charged and brutal example, but LLMs create a very real
    scenario where ML practitioners need practical, consistent, and defensible frameworks
    of both morality and ethics, or they risk real tragedy under their watch. Obviously,
    we aren’t the arbiters of morality and aren’t going to judge you about where you
    find yourself there, but you should still consider the broader context of any
    system you create.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 算法创造了数千种这样的情况，在这些情况下，我们的道德和伦理可能并不一致。在塔木德中有一个关于道德和伦理责任的古老例子，它决定如果一个人把另一个人推入水中或火中，而被推的人未能逃脱，那么这个人不是杀人犯。[³](#footnote-99)
    根据你的信仰和你在的法律体系下，Meta在缅甸的种族灭绝（不是开玩笑[⁴](#footnote-100)）中可能是道德上或伦理上有过错的。在那种情况下，Meta甚至没有把人推入火中；是他们的算法做的。这显然是一个充满争议和残酷的例子，但LLMs创造了一个非常真实的情况，其中机器学习从业者需要实际、一致和可辩护的道德和伦理框架，否则他们可能会在他们监管下发生真正的悲剧。显然，我们不是道德的仲裁者，也不会评判你在那里的位置，但你仍然应该考虑你创建的任何系统的更广泛背景。
- en: Laws are coming
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 法律正在到来
- en: One thing we *can* be sure about is that regulation will come, and companies
    will be held responsible for what their AI agents do. Air Canada found this out
    the hard way when the courts ruled against it, saying the company had to honor
    a refund policy that its chatbot had completely made up ([https://mng.bz/pxvG](https://mng.bz/pxvG)).
    The bot gave incorrect information. It did link the customer to the correct refund
    policy; however, the courts rightly questioned “why customers should have to double-check
    information found in one part of its website on another part of its website.”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以肯定的一件事是，监管将会到来，公司将对它们的AI代理的行为负责。加拿大航空通过法院判决得知这一点，法院裁定该公司必须遵守其聊天机器人完全编造的退款政策([https://mng.bz/pxvG](https://mng.bz/pxvG))。该机器人提供了错误的信息。它确实将客户链接到了正确的退款政策；然而，法院正确地质疑了“为什么客户需要在网站的另一部分找到的信息在网站的其他部分再次进行双重检查。”
- en: We’ve seen similar cases where users have used prompt engineering to trick Chevy’s
    LLM chatbot into selling a 2024 Tahoe for $1 ([https://mng.bz/XVmG](https://mng.bz/XVmG)),
    and DPD needed to “shut down its AI element” after a customer got it to admit
    to being the worst delivery company in the world.[⁵](#footnote-101) As we said
    earlier, it’s difficult to tell, even with existing legislation, what is ethically
    allowable for an LLM to do. Of course, it brings up the question of whether, if
    the chatbot was licensed and equipped to sell cars and did complete such a transaction,
    the customer’s bad faith interaction would actually matter, or whether a company
    would still be held ethically responsible for upholding such a transaction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到过类似的案例，用户通过提示工程技巧欺骗了雪佛兰的LLM聊天机器人，以1美元的价格出售了一辆2024年的塔霍车型([https://mng.bz/XVmG](https://mng.bz/XVmG))，DPD在一位客户让它承认自己是世界上最差的快递公司后，不得不“关闭其AI元素”。[⁵](#footnote-101)正如我们之前所说，即使有现有的立法，也很难判断LLM在道德上可以做什么。当然，这也引发了一个问题：如果聊天机器人获得了销售汽车的许可并完成了这样的交易，客户的恶意互动是否真的重要，或者公司是否仍然在道德上对维护这样的交易负有责任。
- en: Being held responsible for what an LLM generates is enough to make you think
    twice about many applications you may consider using it for. The higher the risk,
    the more time you should take to pause and consider potential legal ramifications.
    We highly recommend dialing in your prompt engineering system, setting up guard
    rails to keep your agent on task, and absolutely being sure to save your logs
    and keep your customer chat history.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM生成的内容负责足以让你三思而后行，考虑你可能考虑使用它的许多应用。风险越高，你应该花更多的时间暂停并考虑潜在的法律后果。我们强烈建议调整你的提示工程系统，设置护栏以保持你的代理在任务上，并且绝对确保保存你的日志并保留客户聊天记录。
- en: 12.2.2 LLMs are getting bigger
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 LLMs正在变得更大
- en: Another thing we can be sure of is that we will continue to see models getting
    bigger and bigger for the near future. Since larger models continue to display
    emergent behavior, there’s no reason for companies to stop taking this approach
    when simply throwing money at the problem seems to generate more money. Not to
    mention, for companies that have invested the most, larger models are harder to
    replicate. As you’ve probably found, the best way for smaller companies to compete
    is to create smaller, specialized models. Ultimately, as long as we have large-enough
    training datasets to accommodate more parameters, we can expect to see more parameters
    stuffed into a model, but the question of whether we’ve ever had adequate data
    to demonstrate “general intelligence” (as in AGI) is as murky as ever.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件我们可以确定的事情是，在不久的将来，我们还将继续看到模型变得越来越庞大。由于更大的模型持续表现出涌现行为，公司没有理由停止采取这种方法，因为简单地投入资金似乎能带来更多的收益。更不用说，对于投入最多的公司来说，更大的模型更难复制。正如你可能发现的，小型公司竞争的最佳方式是创建更小、更专业的模型。最终，只要我们有足够大的训练数据集来容纳更多的参数，我们就可以期待看到更多的参数被塞入模型中，但关于我们是否曾经有过足够的数据来证明“通用智能”（如AGI）的问题，仍然像以往一样模糊不清。
- en: Larger context windows
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更大的上下文窗口
- en: It’s not just larger models. We are really excited to see context lengths grow
    as well. When we started working on this book, they were a real limitation. It
    was rare to see models with context lengths greater than 10K tokens. ChatGPT only
    offered lengths up to 4,096 tokens at the time. A year later, and we see models
    like Gemini 1.5 Pro offering a context length of up to 1 million tokens, with
    researchers indicating that it can handle up to 10 million tokens in test cases
    ([https://mng.bz/YV4N](https://mng.bz/YV4N)). To put it in perspective, the entire
    seven-book Harry Potter series is 1,084,170 words (I didn’t count them; [https://wordsrated.com/harry-potter-stats/](https://wordsrated.com/harry-potter-stats/)),
    which would come out to roughly 1.5 million tokens depending on your tokenizer.
    At these lengths, it’s hard to believe there are any limitations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅是大模型。我们非常兴奋地看到上下文长度也在增长。当我们开始编写这本书时，这是一个真正的限制。很少看到上下文长度超过10K令牌的模型。当时ChatGPT只提供最多4,096个令牌的长度。一年后，我们看到Gemini
    1.5 Pro这样的模型提供了最多1百万个令牌的上下文长度，研究人员指出，它在测试案例中可以处理多达1千万个令牌([https://mng.bz/YV4N](https://mng.bz/YV4N))。为了更直观地说明，整个七部《哈利·波特》系列共有1,084,170个单词（我没有数过；[https://wordsrated.com/harry-potter-stats/](https://wordsrated.com/harry-potter-stats/))，根据你的分词器，这大约相当于1.5百万个令牌。在这些长度下，很难相信有任何限制。
- en: Obviously, there still are. These larger models with near infinite context windows
    generally have you paying per token. If the model doesn’t force your users to
    send smaller queries, your wallet will. Not to mention, if you are reading this
    book, you are likely more interested in smaller open source models you can deploy
    yourself, and many of these definitely still have limiting context sizes you have
    to work with. Don’t worry, though; right now and in the future, even smaller models
    will have million-sized context windows. There’s a lot of interesting research
    going into this area. If you are interested, we recommend you check out RoPE,[⁶](#footnote-102)
    YaRN,[⁷](#footnote-103) and Hyena.[⁸](#footnote-104)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，挑战仍然存在。这些具有近乎无限上下文窗口的更大模型通常按令牌收费。如果模型不强迫用户发送更小的查询，那么用户的钱包就会受到影响。更不用说，如果你正在阅读这本书，你很可能对可以自己部署的小型开源模型更感兴趣，而这些模型中许多确实仍然有必须与之合作的限制性上下文大小。不过，不用担心；现在和将来，即使是更小的模型也将拥有百万级别的上下文窗口。这个领域正在进行许多有趣的研究。如果你感兴趣，我们建议你查看RoPE[⁶](#footnote-102)、YaRN[⁷](#footnote-103)和Hyena[⁸](#footnote-104)。
- en: The next attention
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下一个注意力
- en: Of course, larger context windows are great, but they come at a cost. Remember,
    at the center of an LLM lies the attention algorithm, which is quadratic in complexity—meaning
    the more data we throw at it, the more compute we have to throw at it as well.
    One challenge driving the research community is finding the next attention algorithm
    that doesn’t suffer from this same problem. Can we build transformers with a new
    algorithm that is only linear in complexity? That is the billion-dollar question
    right now.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，更大的上下文窗口是很好的，但它们也有代价。记住，在大型语言模型（LLM）的中心是注意力算法，其复杂度是二次的——这意味着我们投入的数据越多，我们就需要投入更多的计算资源。推动研究社区的一个挑战是找到下一个不遭受这种相同问题的注意力算法。我们能否构建一个仅具有线性复杂度的新算法的transformers？这正是现在的十亿美元问题。
- en: There are lots of competing innovations in this field, and we don’t even have
    time to discuss all of our absolute favorites. Two of those favorites are MAMBA,
    an alternative to transformers, and KAN, an alternative to multilayer perceptrons
    (MLPs). MAMBA, in particular, is an improvement on state space models (SSMs) incorporated
    into an attention-free neural network architecture.[⁹](#footnote-105) By itself,
    it isn’t all that impressive, as it took lots of hardware hacking to make it somewhat
    performant. However, later JAMBA came out, a MAMBA-style model that uses hybrid
    SSM-transformer layers and joint attention.[^(10)](#footnote-106) The hybrid approach
    appears to give us the best of both worlds.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域有许多竞争性的创新，我们甚至没有时间讨论我们所有绝对最喜欢的。其中两个最喜欢的分别是MAMBA，作为transformers的替代品，以及KAN，作为多层感知器（MLPs）的替代品。特别是MAMBA，它是对状态空间模型（SSMs）的改进，并将其融入了一个无注意力的神经网络架构中。[⁹](#footnote-105)
    单独来看，它并不那么令人印象深刻，因为它需要大量的硬件黑客技术才能使其具有一定的性能。然而，后来出现了JAMBA，这是一个MAMBA风格的模型，它使用了混合SSM-transformer层和联合注意力。[^(10)](#footnote-106)
    这种混合方法似乎为我们提供了两者的最佳结合。
- en: So you can experience it for yourself, in listing 12.1, we will finetune and
    run inference on a JAMBA model. This model is a mixture-of-experts model with
    52B parameters, and the implementation will allow for 140K context lengths on
    an 80 GB GPU, which is much better performance than you’d get with an attention
    model alone. This example was adapted right from the Hugging Face model card,
    so the syntax should look very familiar compared to every other simple transformer
    implementation, and we are very grateful for the ease of trying out brand-new
    stuff.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您亲身体验，在列表12.1中，我们将对JAMBA模型进行微调和运行推理。这个模型是一个专家混合模型，拥有520亿个参数，其实现将允许在80GB GPU上实现14万K的上下文长度，这比仅使用注意力模型要好得多。这个例子直接改编自Hugging
    Face模型卡片，所以与所有其他简单的transformer实现相比，语法应该非常熟悉，我们对尝试新事物如此容易感到非常感激。
- en: For the training portion, unfortunately, the model is too big, even in half
    precision, to fit on a single 80 GB GPU, so you’ll have to use Accelerate to parallelize
    it between several GPUs to complete training. If you don’t have that compute just
    lying around, you can complete the imports up to the tokenizer and skip to after
    the training portion, changing very little. We aren’t doing anything fancy; the
    dataset we’ll use for training is just a bunch of famous quotes in English from
    various authors retrieved from Goodreads consisting of quote, author, and tags,
    so don’t feel like you are missing out if you decide to skip finetuning. We’ll
    start by loading the tokenizer, model, and dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练部分，遗憾的是，即使在半精度下，模型也太大，无法适应单个80GB GPU，因此您必须使用Accelerate在多个GPU之间并行化以完成训练。如果您没有现成的计算资源，您可以完成到分词器的导入，然后跳过训练部分，改动非常小。我们并没有做什么特别的事情；我们将用于训练的数据集只是一些从Goodreads检索的著名作者的英文名言，包括名言、作者和标签，所以如果您决定跳过微调，请不要觉得自己错过了什么。我们将首先加载分词器、模型和数据集。
- en: Listing 12.1 Finetuning and inferencing JAMBA
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.1 JAMBA的微调和推理
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once all of those are in memory (you can stream the dataset if your hardware
    is limited), we’ll create training arguments and a LoRA config to help the finetuning
    work on even smaller hardware:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有这些都在内存中（如果您的硬件有限，您可以流式传输数据集），我们将创建训练参数和一个LoRA配置，以帮助微调在更小的硬件上工作：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And now, for the finale, similar to sklearn’s `model.fit()`, transformers’
    `trainer.train()` has become a moniker for why anyone can learn how to interact
    with state-of-the-art ML models. Once training completes (it took a little under
    an hour for us), we’ll save local versions of the tokenizer and the model and
    delete the model in memory:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了高潮部分，类似于sklearn的`model.fit()`，transformers的`trainer.train()`已经成为一个标志，表明任何人都可以学习如何与最先进的机器学习模型交互。一旦训练完成（对我们来说大约需要不到一个小时），我们将保存分词器和模型的本地版本，并删除内存中的模型：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll reload the model, but in a memory-efficient way, to be used for
    inference. With an 80 GB GPU and loading in 8bit with this BitsandBytes config,
    you can now fit the model and a significant amount of data on a single GPU. Loading
    in 4bit allows that on any type of A100 or two 3090s, similar to a 70B parameter
    transformer. Using quantization to get it down to a 1-bit model, you can fit this
    model and a significant amount of data on a single 3090\. We’ll use the following
    8bit inference implementation and run inference on it:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将以内存高效的方式重新加载模型，用于推理。在80GB GPU上，使用BitsandBytes配置以8位加载，您现在可以在单个GPU上拟合模型和大量数据。以4位加载允许在A100或两个3090上实现，类似于70B参数的transformer。使用量化将其降低到1位模型，您可以在单个3090上拟合这个模型和大量数据。我们将使用以下8位推理实现并在其上运行推理：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are blown away almost monthly at this point by the alternatives to various
    parts of LLM systems that pop up. Here, we’d like to draw your attention way back
    to where LLMs got their big break: “Attention Is All You Need.”[^(11)](#footnote-107)
    That paper showed that you could use dumb MLPs to get amazing results, using only
    attention to bridge the gap. We’re entering a new age where we aren’t focusing
    on just what we need but what we want for the best results. For example, we want
    subquadratic drop-in replacements for attention that match or beat flash attention
    for speed. We want attention-free transformers and millions-long context lengths
    with no “lost in the middle” problems. We want alternatives to dense MLPs with
    no drops in accuracy or learning speed. We are, bit by bit, getting all of these
    and more.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们几乎每个月都会被LLM系统各个部分的替代方案所震撼。在这里，我们想将您的注意力引回到LLM获得重大突破的地方：“注意力即一切”。[^(11)](#footnote-107)那篇论文表明，你可以使用简单的MLP获得惊人的结果，仅使用注意力来弥合差距。我们正进入一个新纪元，我们不再只关注我们需要什么，而是关注为了获得最佳结果我们想要什么。例如，我们想要低于二次方的注意力替代方案，以匹配或超越闪速注意力在速度上的表现。我们想要无注意力的transformer和数百万长度的上下文长度，没有“中间丢失”的问题。我们想要没有精度或学习速度下降的密集MLP的替代方案。我们正逐步获得所有这些以及更多。
- en: Pushing the boundaries of compression
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推进压缩的边界
- en: After going down to INT4, there are experimental quantization strategies for
    going even further down to INT2\. INT2 70B models still perform decently, much
    to many peoples’ surprise. Then there’s research suggesting we could potentially
    go even smaller to 1.58 bits per weight or even 0.68 using ternary and other smaller
    operators. Want to test it out? Llama3 70B already has 1-bit quantization implementations
    in GGUF, GPTQ, and AWQ formats, and it only takes up 16.6 GB of memory. Go nuts!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在降至INT4之后，有实验性的量化策略可以将模型进一步降至INT2。INT2 70B模型仍然表现良好，这让许多人感到惊讶。然后有研究表明，我们可能甚至可以进一步减小到每个权重1.58位或使用三进制和其他更小的算子达到0.68位。想试试吗？Llama3
    70B已经在GGUF、GPTQ和AWQ格式中实现了1位量化，它只占用16.6 GB的内存。尽情尝试吧！
- en: There’s another dimension to this, which doesn’t involve compressing models
    but instead decouples the idea of models being one piece and thinking of models
    as collections of layers and parameters again. Speculative decoding gives us yet
    another way of accessing large models quickly. Speculative decoding requires not
    just enough memory to load one large model but also another smaller model alongside
    it—think distillation models. An example often used in production these days is
    Whisper-Large-v3 and Distil-Whisper-Large-V3\. Whisper is a multimodal LLM that
    focuses on the speech-to-text problem, but speculative decoding will work with
    any two models that have the same architecture and different sizes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这还有另一个维度，它不涉及压缩模型，而是将模型是一个整体的想法与将模型视为层和参数集合的想法解耦。推测性解码为我们提供了快速访问大型模型的另一种方式。推测性解码不仅需要足够的内存来加载一个大型模型，还需要一个与之并行的较小模型——想想蒸馏模型。目前生产中常用的一个例子是Whisper-Large-v3和Distil-Whisper-Large-V3。Whisper是一个多模态LLM，专注于语音到文本问题，但推测性解码可以与任何具有相同架构但大小不同的两个模型一起工作。
- en: 'This method allows us to sample larger models quicker (sometimes a straight
    2× speed boost) by computing several tokens in parallel and by an approximation
    “assistant” model that allows us to both complete a step and verify whether that
    step is easy or hard at the same time. The basic idea is this: use the smaller,
    faster Distil-Whisper model to generate guesses about the end result, and allow
    Whisper to evaluate those guesses in parallel, ignoring the ones that it would
    do the same thing on and correcting the ones that it would change. This allows
    for the speed of a smaller model with the accuracy of a larger one.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使我们能够更快地（有时是直接的2倍速度提升）采样更大的模型，通过并行计算多个标记，并通过一个允许我们同时完成一个步骤并验证该步骤是简单还是困难的近似“助手”模型。基本思路是这样的：使用更小、更快的Distil-Whisper模型来生成关于最终结果的猜测，并允许Whisper并行评估这些猜测，忽略那些它将执行相同操作的情况，并纠正那些它将改变的情况。这允许我们以较小模型的速度和较大模型的准确性。
- en: In listing 12.2, we demonstrate speculative decoding on an English audio dataset.
    We’ll load Whisper and Distil-Whisper, load the dataset, and then add an `assistant_
    model` to the generation keyword arguments (`generate_kwargs`). You may ask, how
    does this system know that the assistant model is only meant to help with decoding,
    as the name suggests? Well, we load the assistant model with `AutoModelForCausalLM`
    instead of the speech sequence-to-sequence version. This way, the model will only
    help with the easier decoding steps in parallel with the larger one. With that
    done, we’re free to test.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 12.2 中，我们展示了在英语音频数据集上进行的推测性解码。我们将加载 Whisper 和 Distil-Whisper，加载数据集，然后向生成关键字参数（`generate_kwargs`）添加一个
    `assistant_model`。您可能会问，这个系统如何知道辅助模型只意味着帮助解码，正如其名称所暗示的那样？嗯，我们用 `AutoModelForCausalLM`
    而不是语音序列到序列版本加载辅助模型。这样，模型将只帮助并行于较大模型的大解码步骤。完成这些后，我们可以自由测试。
- en: Listing 12.2 Speculative decoding with Whisper
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.2 使用 Whisper 的推测性解码
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In our testing, we observed about 42 seconds for Whisper-Large-V3 to get through
    all 73 examples with scaled dot product attention. With speculative decoding,
    that dropped to 18.7 seconds, with the exact same word error rate (WER). So there
    was an almost 2× speed increase with absolutely zero drop in accuracy. Yeah, pretty
    nuts.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试中，我们观察到 Whisper-Large-V3 在使用缩放点积注意力机制的情况下，处理完所有 73 个示例大约需要 42 秒。使用推测性解码后，时间降至
    18.7 秒，但精确的词错误率（WER）保持不变。因此，速度提高了近 2 倍，而准确性没有丝毫下降。是的，相当疯狂。
- en: 'At this point, we were wondering, “Why doesn’t everyone use this for everything
    all the time?” Here are the drawbacks to this method: first, it works best in
    smaller sequences. With LLMs, that’s under 128 tokens of generation or around
    20 seconds of audio processing. With the larger generations, the speed boost will
    be negligible. Beyond that, we don’t always have access to perfectly compatible
    pairs of large and small models, like BERT versus DistilBERT. The last reason
    is that very few people really know about it, even with its ease of implementation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们想知道，“为什么每个人不总是用这个来做所有事情？”这种方法的一些缺点如下：首先，它在较短的序列中效果最好。对于 LLM 来说，这低于 128
    个生成标记或大约 20 秒的音频处理。对于更长的生成，速度提升将微不足道。除此之外，我们并不总是能够访问到完美兼容的大模型和小模型对，比如 BERT 与 DistilBERT。最后一个原因是，真正了解它的人非常少，尽管它的实现很简单。
- en: Ultimately, whether it’s sub-bit quantization, speculative decoding, or other
    advances, LLMs are pushing research into compression methodologies more than any
    other technology, and it’s interesting to watch as new techniques change the landscape.
    As these methods improve, we can push models to smaller and cheaper hardware,
    making the field even more accessible.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，无论是子比特量化、推测性解码还是其他进步，LLM 们比任何其他技术都更推动研究进入压缩方法，观察新技术如何改变格局是非常有趣的。随着这些方法的发展，我们可以将模型推向更小、更便宜的硬件，使该领域更加易于接触。
- en: 12.2.3 Multimodal spaces
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 多模态空间
- en: We are so excited about the possibilities within multimodality. Going back to
    chapter 2, multimodality is one of the main features of language we haven’t seen
    as many solutions crop up for, and we’re seeing a shift toward actually attempting
    to solve phonetics. Audio isn’t the only modality that humans operate in, though.
    Accordingly, the push toward combining phonetics, semantics, and pragmatics and
    getting as much context within the same embedding space (for comparison) as the
    text is very strong. With this in mind, here are some points of interest in the
    landscape.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对多模态的潜力感到非常兴奋。回到第 2 章，多模态是我们尚未看到许多解决方案出现的主要语言特征之一，我们正在看到向尝试解决语音学的转变。然而，人类操作的模式不仅仅是音频。因此，将语音学、语义学和语用学结合起来，在同一个嵌入空间（用于比较）中获得尽可能多的上下文（对于比较）的推动力非常强烈。考虑到这一点，以下是一些值得关注的领域点。
- en: 'The first we want to draw attention to is ImageBind, a project showcasing that
    instead of trying to curtail a model into ingesting every type of data, we can
    instead squish every type of data into an embedding space the model would already
    be familiar with and be able to process. You can take a look at the official demo
    here: [https://imagebind.metademolab.com/](https://imagebind.metademolab.com/).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先想引起注意的是 ImageBind 项目，该项目展示了我们不必试图将模型限制在摄入每种类型的数据，相反，我们可以将每种类型的数据压缩到一个模型已经熟悉并能处理的嵌入空间中。您可以在官方演示中查看：[https://imagebind.metademolab.com/](https://imagebind.metademolab.com/)。
- en: 'ImageBind builds off what multimodal projection models such as CLIP have already
    been showcasing for some time: the ability to create and process embeddings is
    the true power behind deterministic LLM systems. You can use these models for
    very fast searches, including searches that have been, up to this point, nigh
    impossible, like asking to find images of animals that make sounds similar to
    an uploaded audio clip.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ImageBind 建立在多模态投影模型（如 CLIP）已经展示了一段时间的能力之上：创建和处理嵌入的能力是确定性 LLM 系统背后的真正力量。您可以使用这些模型进行非常快速地搜索，包括之前几乎不可能完成的搜索，例如要求找到与上传音频剪辑声音相似的动物图片。
- en: 'OneLLM flips this logic the other way around, taking one model and one multimodal
    encoder to unify and embed eight modalities instead of the ImageBind example of
    using six different encoders to embed six modalities in the same dimension. It
    can be found here: [https://onellm.csuhan.com/](https://onellm.csuhan.com/). The
    big idea behind OneLLM is aligning the unified encoder using language, which offers
    a unique spin on multimodality that aligns the process of encoding rather than
    the result.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: OneLLM 将这种逻辑颠倒过来，使用一个模型和一个多模态编码器来统一和嵌入八个模态，而不是 ImageBind 示例中使用的六个不同编码器来在相同维度中嵌入六个模态。它可以在以下链接找到：[https://onellm.csuhan.com/](https://onellm.csuhan.com/)。OneLLM
    的核心思想是使用语言来对齐统一的编码器，这为多模态提供了一种独特的视角，它关注的是编码过程而不是结果。
- en: We are extremely excited about the research happening in this area. This research
    is able to help bridge the gap between phonetics and pragmatics in the model ecosystem
    and allow for more human-like understanding and interaction, especially in the
    search field.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这个领域的研究感到非常兴奋。这项研究能够帮助弥合模型生态系统中语音学和语用学之间的差距，并允许实现更类似人类的理解和交互，尤其是在搜索领域。
- en: 12.2.4 Datasets
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.4 数据集
- en: One exciting change we are seeing inside the industry due to the introduction
    of LLMs is that companies are finally starting to understand the importance of
    governing and managing their data. For some, it’s the drive to finetune their
    own LLMs and get in on the exciting race to deliver AI products. For others, it’s
    the fear of becoming obsolete, as the capabilities of these systems far surpass
    previous technologies; they are finding it’s only their data that provides any
    type of moat or protection from competition. And for everyone, it’s the worry
    they’ll make the same mistakes they’ve seen other companies make.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LLMs 的引入，我们在这个行业内看到的一个令人兴奋的变化是，公司终于开始理解管理和治理他们数据的重要性。对于一些人来说，这是推动他们微调自己的
    LLMs 并加入激动人心的 AI 产品交付竞赛的动力。对于另一些人来说，这是担心自己变得过时，因为这些系统的能力远远超过了以前的技术；他们发现，只有他们的数据才能提供任何类型的护城河或保护竞争。而对于所有人来说，他们担心会犯他们看到其他公司犯过的同样的错误。
- en: LLMs aren’t just a driving factor; they are also helping teams label, tag, organize,
    and clean their data. Many companies had piles of data they didn’t know what to
    do with, but with LLM models like CLIP, captioning images has become a breeze.
    Some companies have found that simply creating embedding spaces of their text,
    images, audio, and video has allowed them to create meaningful structures for
    datasets previously unstructured. Structured data is much easier to operate around,
    opening doors for search, recommendations, and other insights.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 不仅仅是推动因素；它们还在帮助团队标注、标记、组织和清理数据。许多公司堆积了大量的数据，却不知道如何处理，但有了 CLIP 等LLM 模型，图像字幕变得轻而易举。一些公司发现，仅仅创建他们的文本、图像、音频和视频的嵌入空间，就使他们能够为之前无结构的数据集创建有意义的结构。结构化数据更容易操作，为搜索、推荐和其他洞察打开了大门。
- en: One aspect we see currently missing in the industry is valuable open source
    datasets, especially when it comes to evaluations. Many of the current benchmarks
    used to evaluate models rely on multiple-choice questions, but this is inefficient
    for anyone trying to create an LLM application. In the real world, when are your
    users going to ask your model questions in a multiple-choice format? Next to never.
    People ask freeform questions in conversations and when seeking help since they
    don’t know the answer themselves. However, these evaluation datasets have become
    benchmarks simply because they are easy for researchers to gather, compile, and
    evaluate for accuracy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在行业中我们看到的一个缺失的方面是有价值的开源数据集，尤其是在评估方面。许多目前用于评估模型的基准测试依赖于多项选择题，但这对于试图创建一个LLM应用的人来说效率低下。在现实世界中，你的用户何时会以多项选择题的形式向你的模型提问？几乎永远不会。人们在对话和寻求帮助时提出开放式问题，因为他们自己也不知道答案。然而，这些评估数据集已经成为基准，仅仅是因为它们对研究人员来说很容易收集、汇编和评估准确性。
- en: In addition, we believe another inevitability is the need for more language
    representation. The world is a tapestry of diverse languages and dialects, each
    carrying its unique cultural nuances and communicative subtleties. However, many
    languages remain underrepresented in existing datasets, leading to models that
    are biased toward more dominant languages. As technology becomes increasingly
    global, the inclusion of a wider range of languages is crucial. Adding multiple
    languages not only promotes inclusivity but also enhances the accuracy and applicability
    of language models in various international contexts, bridging communication gaps
    and fostering a more connected world. Imagine your startup didn’t need to pay
    anyone to get accurate information regarding entering China, Russia, or Saudi
    Arabia to expand your market.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们相信另一个不可避免的需求是更多语言表示。世界是一个由多种语言和方言构成的织物，每种语言都承载着其独特的文化细微差别和交流微妙之处。然而，许多语言在现有数据集中代表性不足，导致模型偏向于更占主导地位的语言。随着技术的日益全球化，包括更广泛的语言至关重要。添加多种语言不仅促进了包容性，还增强了语言模型在不同国际环境中的准确性和适用性，弥合沟通差距，促进一个更加紧密相连的世界。想象一下，如果你的初创公司不需要支付任何人就能获得有关进入中国、俄罗斯或沙特阿拉伯以扩大市场的准确信息。
- en: 12.2.5 Solving hallucination
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.5 解决幻觉问题
- en: There’s a lot of evidence that LLMs have more information in them than they
    readily give out and even more evidence that people are generally either terrible
    or malicious at prompting. As a result, you’ll find that hallucinations are one
    of the largest roadblocks when trying to develop an application that consistently
    delivers results. This problem has frustrated many software engineering teams
    that are used to deterministic computer algorithms and rarely deal with nondeterministic
    systems. For many statisticians who are more familiar with these types of systems,
    hallucinations are seen as a feature, not a bug. Regardless of where you stand,
    there’s a lot of research going into the best ways to handle hallucinations, and
    this is an area of interest you should be watching.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量证据表明，LLM中包含的信息比它们愿意给出的要多，甚至有更多证据表明，人们在提示时通常要么很糟糕，要么恶意。因此，你会发现，幻觉是试图开发一个始终如一地提供结果的应用的最大的障碍之一。这个问题让许多习惯于确定性计算机算法且很少处理非确定性系统的软件工程团队感到沮丧。对于许多更熟悉这些类型系统的统计学家来说，幻觉被视为一个特性，而不是一个错误。无论你站在哪一方，都有大量研究投入到处理幻觉的最佳方法中，这是你应该关注的领域之一。
- en: Better prompt engineering
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更好的提示工程
- en: One area that’s interesting to watch and has shown great improvement over time
    is prompt engineering. One prompt engineering tool that helps reduce hallucinations
    is DSPy. We went over it briefly in chapter 7, but here we’ll give an example
    of how it works and why it can be a helpful step for solving hallucination in
    your LLMs. We’ve discussed the fact that LLMs are characteristically bad at math,
    even simple math, several times throughout the book, and we’ve also discussed
    why, but we haven’t really discussed solutions other than improving your tokenization.
    So in listing 12.3, we will show just how good you can coax an LLM to be at math
    with zero tokenization changes, zero finetuning, and no LoRAs or DoRAs, just optimizing
    your prompts to tell the model exactly how to answer the questions you’re asking.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣且随着时间的推移显示出巨大改进的领域是提示工程。一个有助于减少幻觉的提示工程工具是DSPy。我们在第7章中简要介绍了它，但在这里我们将给出一个如何工作的例子，以及为什么它可以是解决你LLMs中幻觉的有帮助的一步。我们在整本书中多次讨论了LLMs在数学方面特别糟糕的事实，甚至简单的数学，我们也讨论了原因，但我们并没有真正讨论除了改进你的分词之外的其他解决方案。所以，在列表12.3中，我们将展示如何通过零分词更改、零微调和没有LoRAs或DoRAs，仅仅优化你的提示来告诉模型如何回答你提出的问题。
- en: We’ll do this using the dspy-ai Python package and Llama3-8B-Instruct. We’ll
    start by loading and quantizing the model to fit on most GPUs and the Grade-School
    Math 8K dataset. We picked this dataset because it’s a collection of math problems
    that you, as a person who has graduated elementary (primary) school, likely don’t
    even need a calculator to solve. We’ll use 200 examples for our train and test
    (dev) sets, although we’d recommend you play with these numbers to find the best
    ratio for your use case without data leakage.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用dspy-ai Python包和Llama3-8B-Instruct来完成这项工作。我们将首先加载和量化模型，以便在大多数GPU和Grade-School
    Math 8K数据集上运行。我们选择这个数据集是因为它是一个数学问题集合，作为一个已经从小学毕业的人，你可能甚至不需要计算器就能解决这些问题。我们将为我们的训练集和测试集（开发集）使用200个示例，尽管我们建议你尝试这些数字，以找到最适合你用例的最佳比例，避免数据泄露。
- en: Listing 12.3 DSPy for math
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.3 DSPy for math
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have our imports and loading ready, we’ll need to address the fact
    that we loaded Llama3 using transformers and not DSPy. DSPy expects to interact
    with models utilizing the OpenAI API, but we have a model loaded locally from
    Hugging Face, DSPy has recently added HFModel to their package, and it can now
    be easily imported, rather than needing the wrapper defined next. First, we make
    a simple function to map any keyword argument differences between the APIs, like
    `max_tokens` vs `max_new_tokens`, and then we create a class that will act as
    the wrapper for our model to generate answers and optimize the prompt. Once that’s
    ready, we’ll load DSPy:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了导入和加载，我们需要解决的事实是我们使用transformers加载了Llama3，而不是DSPy。DSPy期望与使用OpenAI
    API的模型交互，但我们从Hugging Face加载了一个本地模型，DSPy最近为其包添加了HFModel，现在可以轻松导入，而不是需要定义包装器。首先，我们创建一个简单的函数来映射API之间的任何关键字参数差异，比如`max_tokens`与`max_new_tokens`，然后我们创建一个类，它将作为我们的模型生成答案和优化提示的包装器。一旦准备好了，我们将加载DSPy：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Sets up the LM'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 Sets up the LM'
- en: '#2 Sets up ΔSPY to use that LM'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 Sets up ΔSPY to use that LM'
- en: 'Now that we are prepared with an LLM to take our math test, let’s test it.
    We’ll start by establishing a baseline. We’ll define a simple chain-of-thought
    (CoT)-like prompt in the `QASignature` class, which we’ll use to define a zero-shot
    version to use as a baseline. The prompt is likely pretty close to prompts you’ve
    seen before, so hopefully, this will be a very relevant demonstration of tasks
    you may be working on. For evaluation, we’re using DSPy’s `gsm8k_metric`, which
    we imported at the top to evaluate against, but you could always create your own:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好使用一个大型语言模型（LLM）来参加我们的数学测试，让我们来测试一下。我们将首先建立一个基线。我们将在`QASignature`类中定义一个简单的思维链（CoT）样式的提示，我们将使用它来定义一个零样本版本，用作基线。这个提示可能非常接近你之前见过的提示，所以希望这将是一个非常相关的任务演示，你可能会在进行的任务。对于评估，我们使用DSPy的`gsm8k_metric`，我们在顶部导入以进行评估，但你始终可以创建自己的：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Δefines the QASignature and CoT'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 Δefines the QASignature and CoT'
- en: '#2 Sets up the evaluator, which can be used multiple times'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 Sets up the evaluator, which can be used multiple times'
- en: '#3 Evaluates how the LLM does with no changes'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 Evaluates how the LLM does with no changes'
- en: The output is
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With our simple zero-shot CoT prompt, Llama3 gets only 14.5% of the questions
    correct. This result might not seem very good, but it is actually quite a bit
    better than just running the model on the questions alone without any prompt,
    which only yields about 1% to 5% correct.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们简单的零样本CoT提示，Llama3只正确回答了14.5%的问题。这个结果可能看起来并不理想，但实际上它比仅仅在没有任何提示的情况下运行模型要强得多，后者正确率大约只有1%到5%。
- en: 'With the baseline out of the way, let’s move on to the bread and butter of
    DSPy, optimizing the prompt to see where that gets us. There’s been some evolution
    in what people think of as a CoT prompt since the original paper came out. CoT
    has evolved in the industry to mean more than just adding “think step by step”
    in your prompt since this approach is seen more as just basic prompt engineering,
    whereas allowing the model to few-shot prompt itself to get a rationale for its
    ultimate output is considered the new CoT, and that’s how the DSPy framework uses
    those terms. With that explanation, we’ll go ahead and create a `CoT` class using
    the `dspy.ChainOfThought` function and then evaluate it like we did our `ZeroShot`
    class:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决了基线问题之后，让我们继续探讨DSPy的核心内容，即优化提示以查看它能带我们走到哪里。自从原始论文发表以来，人们对CoT提示的看法已经发生了一些变化。在业界，CoT的含义已经超越了仅仅在提示中添加“逐步思考”这一基本提示工程方法，而允许模型通过少量提示自行获得其最终输出的理由被认为是新的CoT，这正是DSPy框架使用这些术语的方式。有了这个解释，我们将继续使用`dspy.ChainOfThought`函数创建一个`CoT`类，然后像评估我们的`ZeroShot`类一样评估它：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Sets up the optimizer'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 设置优化器'
- en: '#2 Optimize the prompts'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 优化提示'
- en: '#3 Evaluates our “optimized_cot” program'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 评估我们的“optimized_cot”程序'
- en: Look at that! If it doesn’t astonish you that the accuracy jumped from 14.5%
    to 74.5% by changing only the prompts—remember we haven’t done any finetuning
    or training—we don’t know what will. People are speculating whether the age of
    the prompt engineer is over, but we’d like to think that it’s just begun. That
    said, the age of “coming up with a clever string and doing no follow-up” has been
    over and shouldn’t have ever started. In this example, we used arbitrary boundaries,
    gave the sections of the dataset and the numbers absolutely no thought, and didn’t
    include any helpful tools or context for the model to access to improve. If we
    did, you’d see that after applying all the prompt engineering tricks in the book,
    it isn’t difficult to push the model’s abilities to staggering levels, even on
    things LLMs are characteristically bad at—like math.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 看看吧！如果仅仅通过改变提示，准确性就从14.5%跃升至74.5%，这不会让你感到惊讶——记住我们还没有进行任何微调或训练——我们不知道会发生什么。人们正在猜测提示工程师的时代是否已经结束，但我们认为它才刚刚开始。话虽如此，“提出一个巧妙的字符串而不进行后续跟进”的时代已经结束，而且根本就不应该开始。在这个例子中，我们使用了任意的边界，对数据集的各个部分和数字完全没有思考，并且没有包含任何有助于模型访问以改进的工具或上下文。如果我们这样做，你会发现，在应用了本书中的所有提示工程技巧之后，将模型的能力提升到令人震惊的水平并不困难，即使是在LLM通常表现不佳的领域——比如数学。
- en: Grounding
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Grounding
- en: If you are looking for ways to combat hallucinations, you’ll run into the term
    *grounding*. Grounding is when we give the LLM necessary context in the prompt.
    By giving it the information it needs, we are helping to provide a solid base
    for the generation to build off of, so it’s less likely to dream up visions out
    of thin air. If this sounds familiar, it should, as we have used one of the most
    common grounding techniques, RAG, several times in this book.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找对抗幻觉的方法，你可能会遇到“grounding”这个术语。Grounding是指我们在提示中为LLM提供必要的上下文。通过提供它所需的信息，我们正在帮助为生成内容提供一个坚实的基础，这样它就很少会凭空想象出幻象。如果这听起来很熟悉，那是因为我们在本书中已经多次使用了一种最常见的grounding技术，即RAG。
- en: The term *RAG* (retrieval augmented generation) is, at face value, synonymous
    with grounding since we are literally retrieving the appropriate context based
    on the prompt and then using it to augment the text generated from the LLM. However,
    RAG has become synonymous with using semantic search with a VectorDB for the retrieval
    portion. Technically, you could use any type of search algorithm or any type of
    database, but if you tell someone in the industry you have set up a RAG system,
    they will assume the former architecture.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*RAG*（检索增强生成）在字面上与grounding同义，因为我们实际上是根据提示检索适当的上下文，然后使用它来增强LLM生成的文本。然而，RAG已经与使用VectorDB进行语义检索的部分同义。技术上，你可以使用任何类型的搜索算法或任何类型的数据库，但如果你告诉业界人士你已设置了一个RAG系统，他们会假设前者架构。
- en: With that clarification, RAG applications are most useful for answering simple
    questions. Consider the question, “What is Gal Gadot’s husband’s current job?”
    It’s really two questions in one, “Who is Gal Gadot’s husband?” and once we know
    that, “What does he do?” RAG alone is pretty terrible at solving these multistep
    questions, as a similarity vector search will likely return many articles about
    Gal Gadot and probably none about Jaron Varsano, her husband.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个澄清，RAG应用在回答简单问题方面最为有用。考虑这样一个问题：“Gal Gadot的丈夫目前做什么工作？”这实际上包含两个问题，“Gal Gadot的丈夫是谁？”一旦我们知道答案，接下来就是“他做什么？”RAG单独解决这类多步骤问题相当糟糕，因为相似度向量搜索可能会返回许多关于Gal
    Gadot的文章，但可能没有关于她的丈夫Jaron Varsano的文章。
- en: 'We can enhance this approach in an important way that we haven’t touched on
    yet: using knowledge graphs. Knowledge graphs store information in a structure
    that captures relationships between entities. This structure consists of nodes
    that represent objects and edges that represent relationships. A graph database
    like NEO4J makes it easy to create and query knowledge graphs. And as it turns
    out, knowledge graphs are amazing at answering more complex multipart questions
    where you need to connect the dots between linked pieces of information. Why?
    Because they’ve already connected the dots for us.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一种我们尚未涉及的重要方式来增强这种方法：使用知识图谱。知识图谱以一种捕捉实体之间关系的结构存储信息。这种结构由代表对象的节点和代表关系的边组成。像NEO4J这样的图数据库使得创建和查询知识图谱变得容易。而且，事实证明，知识图谱在回答更复杂的多部分问题方面非常出色，在这些问题中，你需要连接信息片段之间的联系。为什么？因为它们已经为我们连接了这些点。
- en: Many teams who have struggled to get value out of RAG have been able to see
    large improvements once they transitioned to a graph database from a vector one.
    This comes with two major hurdles, though. First, we can no longer simply embed
    our prompts and pull similar matches; we have the much harder task of coming up
    with a way to turn our prompts into queries our graph database will understand.
    While there are several methods to take this on, it’s just another NLP problem.
    Thankfully, as it turns out, LLMs are really good at this! Second, and probably
    the bigger problem, is that it is much harder to turn your documents into a knowledge
    graph. This is why vector databases have become so popular—the ease of turning
    your data into embeddings to search against. Turning your data into a knowledge
    graph will be a bit more work and take additional expertise, but it can really
    set you up for success down the road.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在RAG上努力寻求价值但未能成功的团队，一旦从向量数据库过渡到图数据库，就能看到大幅度的改进。但这伴随着两个主要障碍。首先，我们不能再简单地嵌入我们的提示并拉取相似匹配；我们面临着一个更艰巨的任务，那就是想出一个方法，将我们的提示转换为图数据库能够理解的问题。虽然有几个方法可以解决这个问题，但这又是一个NLP问题。幸运的是，事实证明，LLM在这方面非常擅长！其次，可能更大的问题是，将你的文档转换为知识图谱要困难得多。这就是为什么向量数据库变得如此受欢迎——将你的数据转换为嵌入以进行搜索变得容易。将你的数据转换为知识图谱将需要更多的工作和额外的专业知识，但这确实可以为你的未来发展打下坚实的基础。
- en: Right now, few teams are willing to invest in the extra data engineering to
    prepare their data into a knowledge graph. Most companies are still looking for
    quick wins, building simple wrappers around LLM APIs. As the industry matures,
    we believe we’ll start to see organizations shift toward building knowledge graphs
    from their proprietary data to eke out better performance from their LLM applications.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，很少有团队愿意投资额外的数据工程，将他们的数据准备成知识图谱。大多数公司仍在寻找快速的成功，围绕LLM API构建简单的包装器。随着行业的成熟，我们相信我们将开始看到组织从他们的专有数据转向构建知识图谱，以从他们的LLM应用中获得更好的性能。
- en: Knowledge editing
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识编辑
- en: Another promising field of research to combat hallucinations is *knowledge editing*.
    Knowledge editing is the process of efficiently adjusting specific behaviors.
    Optimally, this would look like surgery where we precisely go in and change the
    exact model weights that activate when we get incorrect responses, as can be seen
    in figure 12.2\. Knowledge editing can be used for many things, but it is often
    used to combat factual decay—the fact that, over time, facts change, like who
    the current Super Bowl winner is or the current president of any individual country.
    We could retrain or finetune the model, but these are often much heavier solutions
    that may change the model in unexpected ways when all we want to do is update
    a fact or two.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有前景的研究领域，用于对抗幻觉，是*知识编辑*。知识编辑是高效调整特定行为的过程。理想情况下，这看起来就像手术，我们精确地进入并改变当我们得到错误响应时激活的确切模型权重，如图12.2所示。知识编辑可以用于许多事情，但它通常用于对抗事实退化——随着时间的推移，事实会发生变化，比如谁是当前超级碗的获胜者或任何个别国家的现任总统。我们可以重新训练或微调模型，但这些通常是更重的解决方案，可能会以意想不到的方式改变模型，而我们所想要的只是更新一些事实。
- en: '![figure](../Images/12-2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/12-2.png)'
- en: Figure 12.2 Knowledge editing is a technique to essentially perform surgery
    on a model to directly insert, update, or erase information.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2 知识编辑是一种技术，本质上是对模型进行手术，以直接插入、更新或删除信息。
- en: Knowledge editing is an interesting field of research that we unfortunately
    didn’t have the space to go into in this book. A host of algorithms and techniques
    have been created to do it, like ROME, MEND, and GRACE. For those interested in
    using any of these techniques, we recommend first checking out EasyEdit at [https://github.com/zjunlp/EasyEdit](https://github.com/zjunlp/EasyEdit).
    EasyEdit is a project that has implemented the most common knowledge editing techniques
    and provides a framework to utilize them easily. It includes examples, tutorials,
    and more to get you started.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 知识编辑是一个有趣的研究领域，遗憾的是，我们在这本书中没有足够的空间深入探讨。已经创建了许多算法和技术来实现它，如ROME、MEND和GRACE。对于那些有兴趣使用这些技术中任何一种的人，我们建议首先查看[https://github.com/zjunlp/EasyEdit](https://github.com/zjunlp/EasyEdit)上的EasyEdit。EasyEdit是一个实现了最常见知识编辑技术的项目，并提供了一个易于利用它们的框架。它包括示例、教程等，以帮助你开始。
- en: 12.2.6 New hardware
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.6 新硬件
- en: As with most popular technologies, LLMs have already created a fierce market
    of competition. While most companies are still competing on capabilities and features,
    there’s also a clear drive to make them faster and cheaper. We’ve discussed many
    of these methods you can employ, like quantization and compilation. One we expect
    to see more of is innovation around hardware.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就像大多数流行的技术一样，大型语言模型（LLMs）已经创造了一个激烈的市场竞争。虽然大多数公司仍在竞争功能和特性，但也有一些明确的动力使它们更快、更便宜。我们已经讨论了许多你可以采用的方法，比如量化编译。我们预计将看到更多围绕硬件的创新。
- en: In fact, Sam Altman, CEO of OpenAI, has been trying to raise funds to the tune
    of $7 trillion dollars to invest in the semiconductor industry.[^(12)](#footnote-108)
    We’ve talked about the global GPU shortage before, but no one is as annoyed about
    it as some of the biggest players. The investment would go further than just meeting
    demand; it would also accelerate development and research into better chips like
    Application-Specifc Integrated Circuits (ASICs).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，OpenAI的首席执行官Sam Altman一直在努力筹集高达7000亿美元的基金，用于投资半导体行业。[^(12)](#footnote-108)
    我们之前已经讨论过全球GPU短缺的问题，但没有人像一些最大的玩家那样对此感到烦恼。这笔投资将不仅仅是为了满足需求；它还将加速对像应用特定集成电路（ASICs）这样的更好芯片的开发和研究。
- en: We’ve talked about and have used GPUs a lot throughout this book, but GPUs weren’t
    designed for AI; they were designed for graphics. Of course, that fact didn’t
    stop NVIDIA from briefly becoming the world’s most valuable company.[^(13)](#footnote-109)
    ASICs are designed for specific tasks; an example would be Google’s TPUs or tensor
    processing units. ASICs designed to handle AI workloads are NPUs (neural processing
    units), and chances are, you’ve never heard of, or at least never seen, an NPU
    chip before. We point this out to show there’s still plenty of room for improvement,
    and it’s likely we will see a large array of new accelerators in the future, from
    better GPUs to NPUs and everything in between. For more info, take a look at Cerebras
    ([https://cerebras.ai/product-chip/](https://cerebras.ai/product-chip/)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中多次讨论并使用了 GPU，但 GPU 并不是为 AI 设计的；它是为图形设计的。当然，这个事实并没有阻止英伟达短暂地成为世界上最有价值的公司。[^(13)](#footnote-109)
    ASIC 是为特定任务设计的；一个例子是谷歌的 TPUs 或张量处理单元。专为处理 AI 工作负载设计的 ASIC 是 NPU（神经网络单元），而且可能性很大，你以前从未听说过，或者至少从未见过
    NPU 芯片。我们指出这一点是为了表明仍有很大的改进空间，我们很可能会在未来看到从更好的 GPU 到 NPU 以及介于两者之间的大量新加速器。更多信息，请参阅
    Cerebras ([https://cerebras.ai/product-chip/](https://cerebras.ai/product-chip/))。
- en: One of the authors of this book spent a good portion of his career working for
    Intel and Micron developing the now-discontinued memory technology known as 3D
    XPoint (3DxP). The details of 3DxP aren’t important for this discussion; what
    it offered, extremely fast and cheap memory, is. It was sold under the brand name
    Optane for several years and had even earned the moniker “The Fastest SSD Ever
    Made.”[^(14)](#footnote-110) This technology proved itself to be almost as fast
    as RAM but almost as cheap to produce as NAND flash memory and could be used to
    replace either.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的一位作者在英特尔和美光工作了一段时间，负责开发现在已停产的称为 3D XPoint（3DxP）的内存技术。3DxP 的细节对于这次讨论并不重要；它提供的，极快且便宜的内存，才是关键。它以
    Optane 品牌销售了几年，甚至赢得了“有史以来最快的 SSD”的美誉。[^(14)](#footnote-110) 这种技术证明其速度几乎与 RAM 相当，但生产成本几乎与
    NAND 闪存相当，并且可以用来替代任何一种。
- en: Imagine a world where every processor conveniently had 500 GB or even 1 TB of
    memory space. Most of the limitations we’ve discussed so far would simply disappear.
    You could load entire LLMs the size of GPT-4 onto one GPU. You wouldn’t have to
    worry about parallelization or the underutilization problems that come with the
    extra overhead. Did I mention 3DxP was nonvolatile as well? Load your model once,
    and you’re done; you’d never need to reload it, even if you had to restart your
    server, which would make jobs like autoscaling so much easier.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个世界，每个处理器都方便地拥有 500 GB 或甚至 1 TB 的内存空间。我们之前讨论的大多数限制都将简单地消失。你可以将整个 GPT-4 大小的
    LLM 加载到一个 GPU 上。你不必担心并行化或额外开销带来的利用率问题。我提到过 3DxP 也是非易失性的吗？加载一次模型，就完成了；即使你需要重新启动服务器，也不必重新加载它，这将使自动扩展等任务变得容易得多。
- en: 3DxP was a technology that had already proven itself in the market as capable,
    but it nonetheless suffered due to a perceived lack of demand. Consumers didn’t
    know what to do with this new layer in the memory hierarchy that it provided.
    Personally, with the arrival of LLMs, the authors see plenty of demand now for
    a technology like this. We’ll just have to wait and see whether the semiconductor
    industry decides to reinvest.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 3DxP 是一种已经在市场上证明了自己的技术，它能够胜任，但仍然因为人们认为需求不足而受到影响。消费者不知道如何利用它提供的内存层次结构中的这一新层。就个人而言，随着
    LLM 的到来，作者们现在看到了对这种技术的巨大需求。我们只需等待并观察半导体行业是否会决定重新投资。
- en: 12.2.7 Agents will become useful
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.7 代理将变得有用
- en: Lastly, we believe LLM-based agents will eventually be more than just a novelty
    that works only in demos. Most agents we’ve seen have simply been feats of magic,
    or should I say smoke and mirrors, throwing a few prompt engineering tricks at
    the largest models. The fact that several of them work at all—even in a limited
    capacity—shines light on the possibilities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们相信基于 LLM 的代理最终将不仅仅是一个只在演示中起作用的创新。我们看到的许多代理只是魔术般的壮举，或者说应该说是烟雾和镜子，只是在大模型上抛出一些提示工程技巧。它们中的几个甚至能正常工作——即使是在有限的范围内——这也揭示了可能性。
- en: 'We’ve seen several companies chase after the holy grail, building agents to
    replace software engineers. In fact, you’ll see them try to build agents to replace
    doctors, sales associates, or managers. But just as many companies and AI experts
    used to promise we’d have self-driving cars in the near future, that near future
    keeps on eluding us. Don’t get me wrong: it’s not like we don’t have self-driving
    cars, but they are much more of an annoyance than anything, and they can only
    drive in select locations as rideshare vehicles. In a similar fashion, we aren’t
    too worried about agents replacing any occupation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到几家公司在追逐圣杯，构建代理来取代软件工程师。实际上，你也会看到他们试图构建代理来取代医生、销售助理或经理。但就像许多公司和AI专家曾经承诺我们将在不久的将来拥有自动驾驶汽车一样，那个“不久的将来”一直在逃避我们。请别误会：并不是我们没有自动驾驶汽车，但它们更多的是一种烦恼，而且它们只能作为共享出行车辆在特定地点行驶。以类似的方式，我们并不太担心代理会取代任何职业。
- en: What we are more interested in are small agents—agents trained and finetuned
    to do a specialized task but with greater flexibility to hold conversations. Many
    video game NPCs would benefit from this type of setup where they could not only
    use an LLM to hold random conversations and provide a more immersive experience
    but also to decide to take actions that would shape a unique story.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更感兴趣的是小型代理——经过训练和微调以执行特定任务但具有更大灵活性进行对话的代理。许多电子游戏NPC将受益于这种设置，它们不仅可以使用LLM进行随机对话并提供更沉浸式的体验，还可以决定采取塑造独特故事的行为。
- en: We are also likely to see them do smaller tasks well first. For example, LLMs
    can already read your email and summarize them for you, but a simple agent would
    go a step further and generate email responses for you. Maybe it wouldn’t actually
    send them, but simply provide you with the options, and all you’d have to do is
    pick the one you want, and then it would send it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可能会看到它们首先做好小任务。例如，LLM已经可以阅读你的电子邮件并为你的总结，但一个简单的代理会更进一步，为你生成电子邮件回复。也许它实际上不会发送它们，但只是提供选项，而你只需选择你想要的，然后它会为你发送。
- en: But mostly, we are excited to see LLM agents replace other bots. For example,
    who hasn’t uploaded their resume only to find they have to reenter all their information?
    Either because the resume extraction tool didn’t work well or it didn’t even exist.
    An LLM agent can not only read your resume and extract the information but also
    double-check its work and make sure it makes sense. Plus, we haven’t even mentioned
    the applicant tracking systems that automatically screen resumes based on keywords.
    These systems are often easily manipulated and terrible at separating the cream
    from the crop. An LLM agent has a much better chance of performing this task well.
    Of course, we care about ensuring fair hiring practices, but these systems are
    already automated and biased to some extent. A better model is an opportunity
    to reduce that non-useful bias.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但主要的是，我们很兴奋地看到LLM代理取代其他机器人。例如，谁没有上传过简历，却发现他们不得不重新输入所有信息？要么是因为简历提取工具工作得不好，要么是因为它甚至不存在。LLM代理不仅能阅读你的简历并提取信息，还能双重检查其工作并确保其合理。此外，我们还没有提到那些根据关键词自动筛选简历的申请跟踪系统。这些系统往往很容易被操纵，并且很糟糕地无法区分出优秀者。LLM代理有更大的机会完成这项任务。当然，我们关心确保公平的招聘实践，但这些系统已经自动化，并且在某种程度上存在偏见。更好的模型是减少这种无益偏见的机会。
- en: With this in mind, one way that models might make better agents is through the
    use of cache embeddings. It’s an interesting idea of something you can do with
    models that we haven’t really heard anyone talking about, other than Will Gaviro
    Rojas at a local Utah meetup. Caching embeddings allows you to cut down on repeating
    the same computations several times to complete several tasks in parallel. This
    is a more complex example, and we aren’t going to dive too deep into it so as
    to keep things pretty simple, but this strategy involves either copying the final
    layers of a model after the last hidden state to complete several tasks on their
    own or creating custom linear classifiers to fulfill those tasks. In listing 12.4,
    we dive into the entire system surrounding caching the embeddings, as we assume
    knowledge at this point of how to store embeddings for access later.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，模型可能通过使用缓存嵌入来成为更好的代理。这是一个有趣的想法，你可以用模型做些事情，除了在当地犹他州的聚会中Will Gaviro Rojas之外，我们还没有听说任何人谈论过。缓存嵌入允许你减少重复计算多次以并行完成多个任务。这是一个更复杂的例子，我们不会深入探讨，以保持事情简单明了，但这个策略涉及在最后一个隐藏状态之后复制模型的最后几层来完成几个任务，或者创建自定义线性分类器来完成这些任务。在列表12.4中，我们深入探讨了围绕缓存嵌入的整个系统，因为我们假设此时已经了解了如何存储嵌入以便稍后访问。
- en: We start by loading Llama3-ChatQA in INT4 quantization with BitsandBytes to
    make sure it fits on smaller consumer GPUs, which should be familiar at the end
    of this book. We give it the appropriate prompt structure for the given model,
    and we get our outputs. Then we access the last hidden state or the embeddings
    with `outputs.last_ hidden_states` and show how we could either create copies
    of the relevant layers to put that hidden state through (provided they’re trained
    to handle this) or create a custom linear classifier in PyTorch that can be fully
    trained on any classification task.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用BitsandBytes在INT4量化中加载Llama3-ChatQA，以确保它适合较小的消费级GPU，这一点在本书的结尾应该会变得熟悉。我们为该模型提供了适当的提示结构，并得到了我们的输出。然后我们通过`outputs.last_hidden_states`访问最后一个隐藏状态或嵌入，并展示如何创建相关层的副本以通过该隐藏状态（如果它们被训练来处理这种情况）或者创建一个PyTorch中的自定义线性分类器，以便在任意分类任务上完全训练。
- en: Listing 12.4 Caching embeddings for multiple smaller models
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.4 缓存多个较小模型的嵌入
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Traditional generation'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 传统生成'
- en: '#2 Embedding'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 嵌入'
- en: '#3 Finds the LM Head layer'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 找到LM头部层'
- en: '#4 Custom Trainable classifier'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 自定义可训练分类器'
- en: This decoupling of the idea of models as monoliths that connect to other systems
    is very engineering-friendly, allowing for one model to output hundreds of classifications
    around a single data point, thanks to embeddings. LangChain provides a `CacheBackedEmbeddings`
    class to help with caching the vectors quickly and conveniently if you’re working
    within that class, and we think that name is pretty great for the larger idea
    as well—backing up your embedding process with caching to be fed to multiple linear
    classifiers at once. This approach allows us to detect anything from inappropriate
    user input all the way to providing a summarized version of the embeddings back
    to the real model for quicker and more generalized processing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将模型视为连接到其他系统的单一实体的想法的解耦非常符合工程实践，使得一个模型能够围绕一个单一数据点输出数百个分类，这要归功于嵌入。LangChain提供了一个`CacheBackedEmbeddings`类来帮助在类内部快速方便地缓存向量，我们认为这个名字对于更大的想法来说也非常棒——通过缓存来备份嵌入过程，以便一次性提供给多个线性分类器。这种方法使我们能够检测从不当的用户输入到为真实模型提供嵌入的摘要版本，以便更快、更通用地处理。
- en: 12.3 Final thoughts
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 最后的想法
- en: We really hope you enjoyed this book and that you learned something new and
    useful. It’s been a huge undertaking to write the highest quality book we could
    muster, and sometimes it was less about what we wrote and more about what we ended
    up throwing out. Believe it or not, while being as comprehensive as we could,
    there are many times we’ve felt we’d only scratched the surface of most topics.
    Thank you for going on this journey with us.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真心希望您喜欢这本书，并且学到了一些新的、有用的东西。编写一本最高质量的书是一项巨大的努力，有时它更多的是关于我们最终丢弃了什么，而不是我们写了什么。信不信由你，尽管我们尽可能全面，但很多时候我们感觉我们只是触及了大多数主题的表面。感谢您与我们一同踏上这段旅程。
- en: We are so excited about where this industry is going. One of the hardest parts
    of writing this book was choosing to focus on the current best practices and ignoring
    much of the promising research that seems to be piling on, especially as companies
    and governments increase funding into the incredible possibilities that LLMs promise.
    We’re excited to see more research that’s been around for years or even decades
    be applied to LLMs and see new research come from improving those results. We’re
    also excited to watch companies change and figure out how to deploy and serve
    LLMs much better than they currently are. It’s difficult to market LLM-based products
    using traditional methods without coming off as just lying. People want to see
    the product work exactly as demonstrated in the ad, and we’re hoping to see changes
    there.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这一行业的发展方向感到非常兴奋。撰写这本书最困难的部分之一是选择关注当前的最佳实践，而忽略了大量看似堆积如山的有希望的研究，尤其是在公司和政府增加对LLMs承诺的惊人可能性投资的情况下。我们期待看到多年或数十年的研究应用于LLMs，并看到新的研究从改进这些结果中产生。我们也期待看到公司发生变化，并找出如何比目前更好地部署和提供LLMs。在没有显得像是在撒谎的情况下，使用传统方法来营销基于LLMs的产品是困难的。人们希望看到产品的工作方式与广告中展示的完全一致，我们希望看到这方面的变化。
- en: What an exciting time! There’s still so much more to learn and explore. Because
    we have already seen the industry move while we’ve been writing, we’d like to
    invite you to submit PRs in the GitHub repo to help keep the code and listings
    up to date for any new readers. While this is the end of the book, we hope it’s
    just the beginning of your journey into using LLMs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个多么激动人心的时代！还有许多更多东西需要学习和探索。因为我们已经在写作过程中见证了行业的进步，所以我们想邀请您向GitHub仓库提交PR，以帮助保持代码和列表对新读者的更新。虽然这本书已经结束，但我们希望这只是您使用LLMs旅程的开始。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs are quickly challenging current laws and regulations and the interpretations
    thereof.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs正在迅速挑战当前的法律和法规及其解释。
- en: The fear of LLMs being used for cheating has hurt many students with the introduction
    of AI detection systems that don’t work.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs被用于作弊的恐惧伤害了许多学生，因为引入了不起作用的AI检测系统。
- en: LLMs are only getting bigger, and we will need solutions like better compression
    and the next attention algorithm to compensate.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs正在变得越来越大，我们将需要像更好的压缩和下一个注意力算法这样的解决方案来补偿。
- en: Embeddings are paving the way to multimodal solutions with interesting approaches
    like ImageBind and OneLLM.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式技术正在为多模态解决方案铺平道路，例如ImageBind和OneLLM等有趣的方法。
- en: Data is likely to be one of the largest bottlenecks and constraints to future
    improvements, largely starting with a lack of quality evaluation datasets.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据很可能是未来改进的最大瓶颈和约束，这很大程度上始于缺乏高质量的评估数据集。
- en: For use cases where they are a problem, hallucinations will continue to be so,
    but methodologies to curb their effects and frequency of occurrence are becoming
    quite sophisticated.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于它们成为问题的情况，幻觉将继续存在，但抑制其影响和发生频率的方法正在变得越来越复杂。
- en: LLMs continue to suffer due to GPU shortages and will help drive research and
    innovation to develop more powerful computing systems.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于GPU短缺，LLMs继续受到影响，并将有助于推动研究和创新，以开发更强大的计算系统。
- en: LLM Agents don’t provide a pathway to AGI, but we will see them graduate from
    toys to tools.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM代理并不提供通往AGI的途径，但我们将看到它们从玩具成长为工具。
- en: '[[1]](#footnote-source-1) M. M. Grynbaum and R. Mac, “The Times Sues OpenAI
    and Microsoft Over A.I. Use of Copyrighted Work,” The New York Times, December
    27, 2023, [https://mng.bz/6Y0D](https://mng.bz/6Y0D).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#footnote-source-1) M. M. Grynbaum和R. Mac，《时代》起诉OpenAI和微软侵犯版权作品的使用，《纽约时报》，2023年12月27日，[https://mng.bz/6Y0D](https://mng.bz/6Y0D)。'
- en: '[[2]](#footnote-source-2) E. Maiberg, “Scientific journals are publishing papers
    with AI-generated text,” 404 Media, March 18, 2024, [https://mng.bz/n0og](https://mng.bz/n0og).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#footnote-source-2) E. Maiberg，《科学期刊正在出版由AI生成的文本》，404 Media，2024年3月18日，[https://mng.bz/n0og](https://mng.bz/n0og)。'
- en: '[[3]](#footnote-source-3) Sanhedrin 76b:11, [https://mng.bz/vJaJ](https://mng.bz/vJaJ).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#footnote-source-3) Sanhedrin 76b:11，[https://mng.bz/vJaJ](https://mng.bz/vJaJ)。'
- en: '[[4]](#footnote-source-4) “Myanmar army behind Facebook pages spewing hate
    speech: UN probe,” RFI, March 27, 2024, [https://mng.bz/mR0P](https://mng.bz/mR0P).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[[4]](#footnote-source-4) “缅甸军队在Facebook页面上散布仇恨言论：联合国调查，”RFI，2024年3月27日，[https://mng.bz/mR0P](https://mng.bz/mR0P)。'
- en: '[[5]](#footnote-source-5) A. Guzman, “Company disables AI after bot starts
    swearing at customer, calls itself the ‘worst delivery firm in the world,’” NY
    Post, January 20, 2024, [https://mng.bz/yoVq](https://mng.bz/yoVq).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[[5]](#footnote-source-5) A. Guzman, “公司启用AI后机器人开始辱骂客户，自称‘世界上最差的快递公司’，”纽约邮报，2024年1月20日，[https://mng.bz/yoVq](https://mng.bz/yoVq).'
- en: '[[6]](#footnote-source-6) emozilla, “Dynamically Scaled RoPE further increases
    performance of long context LLaMA with zero fine-tuning,” Jun. 30, 2023, [https://mng.bz/M1pn](https://mng.bz/M1pn).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[[6]](#footnote-source-6) emozilla， “动态缩放RoPE进一步提高了长上下文LLaMA的性能，无需微调，”2023年6月30日，[https://mng.bz/M1pn](https://mng.bz/M1pn).'
- en: '[[7]](#footnote-source-7) B. Peng, J. Quesnelle, H. Fan, E. Shippole, N. Research,
    and Eleutherai, “YaRN: Efficient Context Window Extension of Large Language Models.”
    Available: [https://arxiv.org/pdf/2309.00071](https://arxiv.org/pdf/2309.00071)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7]](#footnote-source-7) B. Peng，J. Quesnelle，H. Fan，E. Shippole，N. Research，和Eleutherai，
    “YaRN：大型语言模型的效率上下文窗口扩展。” 可用：[https://arxiv.org/pdf/2309.00071](https://arxiv.org/pdf/2309.00071)'
- en: '[[8]](#footnote-source-8) M. Poli et al., “Hyena Hierarchy: Towards Larger
    Convolutional Language Models,” Feb. 2023, doi: [https://doi.org/10.48550/arxiv.2302.10866](https://doi.org/10.48550/arxiv.2302.10866).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[[8]](#footnote-source-8) M. Poli等， “鬣狗层次结构：向更大卷积语言模型迈进，”2023年2月，doi: [https://doi.org/10.48550/arxiv.2302.10866](https://doi.org/10.48550/arxiv.2302.10866).'
- en: '[[9]](#footnote-source-9) A. Gu and T. Dao, “Mamba: Linear-Time Sequence Modeling
    with Selective State Spaces,” arXiv.org, Dec. 01, 2023, [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[[9]](#footnote-source-9) A. Gu和T. Dao， “Mamba：使用选择性状态空间的线性时间序列建模，”arXiv.org，2023年12月1日，[https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752).'
- en: '[[10]](#footnote-source-10) [1]O. Lieber et al., “Jamba: A Hybrid Transformer-Mamba
    Language Model,” arXiv.org, Mar. 28, 2024, [https://arxiv.org/abs/2403.19887](https://arxiv.org/abs/2403.19887).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10]](#footnote-source-10) [1]O. Lieber等， “Jamba：混合Transformer-Mamba语言模型，”arXiv.org，2024年3月28日，[https://arxiv.org/abs/2403.19887](https://arxiv.org/abs/2403.19887).'
- en: '[[11]](#footnote-source-11) Vaswani et al., Attention Is All You Need,” 2017,
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[[11]](#footnote-source-11) Vaswani等， “注意力即一切所需，”2017年，[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
- en: '[[12]](#footnote-source-12) K. H. and A. Fitch, “Sam Altman seeks trillions
    of dollars to reshape business of chips and AI,” Wall Street Journal, February
    8, 2024, [https://mng.bz/KDrK](https://mng.bz/KDrK).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12]](#footnote-source-12) K. H.和A. Fitch， “萨姆·奥特曼寻求数千亿美元重塑芯片和AI业务，”华尔街日报，2024年2月8日，[https://mng.bz/KDrK](https://mng.bz/KDrK).'
- en: '[[13]](#footnote-source-13) A. Pequeño IV, “Nvidia now world’s most valuable
    company—Topping Microsoft and Apple,” Forbes, June 18, 2024, [https://mng.bz/9ojl](https://mng.bz/9ojl).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[[13]](#footnote-source-13) A. Pequeño IV, “英伟达成为全球最有价值的公司——超越微软和苹果，”福布斯，2024年6月18日，[https://mng.bz/9ojl](https://mng.bz/9ojl).'
- en: '[[14]](#footnote-source-14) S. Webster, “Intel Optane SSD DC P5800X review:
    The fastest SSD ever made,” Tom’s Hardware, August 26, 2022, [https://mng.bz/j0Wx](https://mng.bz/j0Wx).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14]](#footnote-source-14) S. Webster, “英特尔Optane SSD DC P5800X评测：制造过的最快固态硬盘，”Tom’s
    Hardware，2022年8月26日，[https://mng.bz/j0Wx](https://mng.bz/j0Wx).'
