- en: '12 Production, an ever-changing landscape: Things are just getting started'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 生产，一个不断变化的景观：一切才刚刚开始
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A brief overview of LLMs in production
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM在生产中的简要概述
- en: The future of LLMs as a technology and several exciting fields of research into
    it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs作为一项技术和对其进行的几个令人兴奋的研究领域
- en: Our closing remarks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的结束语
- en: The Web as I envisaged it, we have not seen it yet. The future is still so much
    bigger than the past.—Tim Berners-Lee (inventor of www)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如我所设想的网络，我们还没有看到它。未来仍然比过去大得多。——蒂姆·伯纳斯-李（www的发明者）
- en: Wow! We’ve really covered a lot of ground in this book. Is your head just about
    ready to explode? Because ours are, and we wrote the book. Writing this book has
    been no easy feat, as the industry has been constantly changing—and fast. Trying
    to stay on top of what’s happening with LLMs has been like trying to build a house
    on quicksand; you finish one level, and it seems to have already sunk before you
    can start the next. We know that portions of this book will inevitably become
    out of date, and that’s why we tried our best to stick to core concepts, the sturdy
    rocks in the sand, that will never change.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！在这本书中，我们确实覆盖了大量的内容。你的头脑是不是快要爆炸了？因为我们的确实是这样，我们写了这本书。写这本书并不容易，因为行业一直在不断变化——而且变化很快。试图跟上LLMs的发展就像在流沙上建造房屋；你完成了一层，似乎在你开始下一层之前它就已经沉下去了。我们知道这本书的部分内容不可避免地会过时，这就是为什么我们尽力坚持核心概念，这些概念就像沙子中的坚固岩石，永远不会改变。
- en: In this chapter, we wanted to take a step back and review some of the major
    takeaways we hope you will walk away with. We’ve spent a lot of time getting into
    the weeds and paying attention to details, so let’s reflect for a moment to see
    the whole picture and review what we’ve covered. After that, we’ll take a minute
    to discuss the future of the field and where we can expect to see some of the
    next major breakthroughs. Finally, we’ll leave you with our final thoughts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们想退后一步，回顾一些我们希望你能带走的主要收获。我们花了很多时间深入细节，所以让我们暂时反思一下，看看整个画面，回顾我们已经覆盖的内容。之后，我们将花一点时间讨论该领域的未来，以及我们可以期待看到的一些下一个重大突破。最后，我们将留下我们的最终想法。
- en: 12.1 A thousand-foot view
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 千米视角
- en: 'We have gone over a lot of material in this book—from making a bag-of-words
    model to serving an LLM API on a Raspberry Pi. If you made it all the way through
    the whole book, that’s an accomplishment. Great work! We are not going to recap
    everything, but we wanted to take a second to see the forest from the trees, as
    it were. To summarize much of what we’ve covered, we can split most of the ideas
    into four distinct but very closely tied quadrants: Preparation, Training, Serving,
    and Developing. You can see these quadrants in figure 12.1\. You’ll notice that
    along with these sections, there’s a fifth one distinct from the others, which
    we labeled Undercurrents. These are elements that seem to affect all of the other
    quadrants to varying degrees and things you’ll have to worry about during each
    stage of an LLM product life cycle.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们讨论了大量的内容——从制作词袋模型到在树莓派上部署LLM API。如果你读完了整本书，那是一项成就。干得好！我们不会回顾所有内容，但我们都想从树木中看到森林，总结一下我们所学到的很多东西。我们可以将大多数想法分为四个截然不同但非常紧密相关的象限：准备、训练、部署和开发。你可以在图12.1中看到这些象限。你会注意到，除了这些部分，还有一个与其他部分不同的第五个部分，我们将其标记为潜流。这些是似乎以不同程度影响所有其他象限的元素，以及你在LLM产品生命周期的每个阶段都必须关注的事情。
- en: '![figure](../Images/12-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/12-1.png)'
- en: Figure 12.1 LLM product life cycle. Here are all the key concepts discussed
    in the book, along with where they generally fit within the production environment.
    Undercurrents are important elements that show up in every part of the life cycle—for
    example, linguistics informs preparation, creates metrics in training and serving,
    and influences prompting and development.
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1 LLM产品生命周期。这里列出了书中讨论的所有关键概念，以及它们通常在生产环境中的位置。潜流是生命周期中每个部分的重要元素——例如，语言学在准备阶段提供信息，在训练和部署阶段创建指标，并影响提示和开发。
- en: Hopefully, if it wasn’t clear when we were talking about a concept in an earlier
    chapter, it’s clear now exactly where that concept fits in a production life cycle.
    You’ll notice that we’ve likely put some elements in places that your current
    production environment doesn’t reflect—for example, provisioning of the MLOps
    infrastructure doesn’t often actually happen within the preparation stage but
    is rather haphazardly thrown together the first time that serving needs to happen.
    We get it. But during preparation is where we feel it *should* happen. Take a
    moment to digest all that you’ve learned while reading this book, and consider
    how the pieces all come together.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 希望当我们之前在章节中讨论一个概念时，如果还没有讲清楚，现在应该很清楚这个概念在生产生命周期中的位置。你会注意到，我们可能将一些元素放在了你的当前生产环境并不反映的位置——例如，MLOps基础设施的配置通常并不发生在准备阶段，而是在第一次需要提供服务时随意拼凑。我们理解这一点。但在准备阶段，我们觉得它*应该*在那里。花点时间消化你在阅读这本书时所学到的所有内容，并考虑所有这些部分是如何结合在一起的。
- en: Given this abstract and idealized version of a production life cycle, let’s
    move to the things not currently included there. What might we need to add to
    our development portion five years down the line, especially given how fast the
    field moves now?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个抽象和理想化的生产生命周期版本中，让我们转向目前尚未包括其中的事物。五年后，我们可能需要在我们的开发部分添加什么，特别是在考虑到这个领域现在发展如此迅速的情况下？
- en: 12.2 The future of LLMs
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 LLMs的未来
- en: When we wrote this book, we made a conscious effort to focus on the foundational
    knowledge you will need to understand how LLMs work and how to deploy them to
    production. This information is crucial, as production looks very different for
    every single use case. Learning how to weigh the pros and cons of any decision
    requires that foundational knowledge if you have any hope of landing on the right
    one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们撰写这本书时，我们有意专注于你需要的基础知识，以便理解LLMs是如何工作的以及如何将它们部署到生产环境中。这些信息至关重要，因为每个用例的生产情况都大不相同。学习如何权衡任何决策的利弊，需要这些基础知识，这样你才有可能做出正确的选择。
- en: Adjacent to this decision, we didn’t want this book to be all theory. We wanted
    it to be hands-on, with enough examples that you as a reader wouldn’t just know
    how things worked but would get a sense of how they feel—like getting a feel for
    how long it takes to load a 70B model onto a GPU, sensing what the experience
    will be like for your user if you run the model on an edge device, and feeling
    the soft glow of your computer monitor as you hide in a dark cave pouring over
    code and avoiding the warm sun on a nice spring day.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与此决定相邻，我们不希望这本书只包含理论。我们希望它是实践性的，有足够的例子，让你作为读者不仅知道事物是如何工作的，而且能感受到它们的感觉——比如感受将一个70B模型加载到GPU上需要多长时间，如果你在边缘设备上运行该模型，你能感受到用户将会有怎样的体验，以及当你躲在黑暗的山洞里埋头于代码、避开春日温暖的阳光时，你能感受到电脑屏幕柔和的光芒。
- en: One of the hardest decisions we made when we wrote this book was deciding to
    focus on the here and now. We decided to focus on the best methods that we actually
    see people using in production today. This decision was hard because over the
    course of writing this book, there have been many mind-blowing research papers
    we’ve been convinced will “change everything.” However, for one reason or another,
    that research has yet to make it to production. In this section, we are going
    to change that restriction and talk about what’s up and coming regardless of the
    current state of the industry. But it’s not just research; public opinions, lawsuits,
    and political landscapes often shape the future of technology as well. We’ll be
    looking at where we see LLMs going in the next several years and mention some
    of the directions they could take.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写这本书的过程中，我们做出的最艰难的决定之一就是决定专注于当下。我们决定关注那些我们实际上看到人们在今天的生产环境中使用的最佳方法。这个决定之所以艰难，是因为在撰写这本书的过程中，我们遇到了许多令人震惊的研究论文，我们确信这些论文将“改变一切”。然而，由于种种原因，这些研究尚未进入生产阶段。在本节中，我们将打破这一限制，无论行业当前状态如何，都将讨论即将到来的趋势。但不仅仅是研究；公众舆论、诉讼和政治格局也常常塑造着技术的未来。我们将探讨在接下来的几年里，我们认为LLMs将走向何方，并提及它们可能采取的一些方向。
- en: 12.2.1 Government and regulation
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 政府和监管
- en: 'At the beginning of this book, we promised to show you how to create LLM products,
    not just demos. While we believe we have done just that, there’s one important
    detail we’ve been ignoring: the fact that products live in the real world. While
    demos just have to work in isolation, products have to work in general. Products
    are meant to be sold, and once there’s an exchange of currency, expectations are
    set, reputations are on the line, and ultimately, governments are going to get
    involved.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的开头，我们承诺向您展示如何创建LLM产品，而不仅仅是演示。虽然我们相信我们已经做到了这一点，但我们一直忽略了一个重要细节：产品存在于现实世界中。演示只需要在孤立的环境中工作，而产品必须在一般情况下工作。产品是为了出售的，一旦货币交换发生，就会设定期望，声誉就会受到考验，最终，政府将介入。
- en: While a team can’t build for future regulations that may never come, it’s important
    to be aware of the possible legal ramifications of the products you build. One
    lost lawsuit can set a precedent that brings a tidal wave of copycat lawsuits.
    Since products live in the real world, it is best that we pay attention to that
    world.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一个团队不能为可能永远不会到来的未来法规而建造，但了解您构建的产品可能产生的法律后果是很重要的。一场败诉的案件可以设定先例，引发模仿诉讼的浪潮。由于产品存在于现实世界中，我们最好关注那个世界。
- en: 'One of us had the opportunity to participate in Utah’s legislative process
    for Utah’s SB-149 Artificial Intelligence Amendments bill. This bill is primarily
    concerned with introducing liability to actors using LLMs to skirt consumer protection
    laws in the state. At the moment, every legislative body is attempting to figure
    out where its jurisdiction starts and ends concerning AI and how to deal with
    the increased responsibility it has to protect citizens and corporations within
    its constituency. In Utah, the state government takes a very serious and business-first
    approach to AI and LLMs. Throughout the process and the bill itself, the legislature
    cannot create definitions that aren’t broken with “behold, a man” Diogenes-style
    examples, and we will need every bit of good faith to navigate the new world that
    LLMs bring to regulatory bodies. How do you define AI? The bill defines it as
    follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中的一员有机会参与犹他州SB-149人工智能修正法案的立法过程。该法案主要关注引入对使用LLM规避州内消费者保护法律的行动者的责任。目前，每个立法机构都在试图弄清楚其在AI方面的管辖权从何开始到何结束，以及如何处理其对保护其选民中的公民和公司所承担的日益增加的责任。在犹他州，州政府对AI和LLM采取了非常严肃和以商业为先的方法。在整个过程中以及法案本身，立法机构不能创建不与“看哪，一个人”第欧根尼风格的例子相冲突的定义，我们将需要每一份善意来导航LLM为监管机构带来的新世界。你如何定义AI？法案如下定义：
- en: “Artificial intelligence” means a machine-based system that makes predictions,
    recommendations, or decisions influencing real or virtual environments.
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人工智能”是指一种基于机器的系统，它做出预测、推荐或决策，影响真实或虚拟环境。
- en: This could be anything from a piecewise function to an LLM agent, meaning that
    your marketing team will not be liable for claims that your `if` statements are
    AI within the state. That said, the bill contains a thorough and well-thought-out
    definition of a deceptive act by a supplier, along with the formulation of an
    AI analysis and research program to help the state assess risks and policy in
    a more long-term capacity, which seems novel and unique to Utah. The Utah state
    legislature was able to refine this bill by consulting with researchers, experts,
    c-level executives, and business owners within the state, and we’d encourage the
    reader to participate in creating worthwhile and meaningful regulations within
    your communities and governments. This is the only way to make sure that court
    systems are prepared to impose consequences where they are due in the long term.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括分段函数到LLM代理的任何东西，这意味着您的营销团队不会对您的`if`语句是处于状态中的AI的声明负责。话虽如此，该法案包含了对供应商欺诈行为的详尽和深思熟虑的定义，以及制定了一个AI分析和研究计划，以帮助州政府从更长期的角度评估风险和政策，这看起来对犹他州来说是新颖且独特的。犹他州立法机构通过与州内的研究人员、专家、C级高管和企业家进行咨询，能够完善这项法案，我们鼓励读者参与在您所在的社区和政府中制定有价值且有意义的法规。这是确保法院系统长期准备好在应受惩罚的地方实施惩罚的唯一方式。
- en: Copyright
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 版权
- en: At the forefront of legal concerns is that of copyright infringement. LLMs trained
    on enough data can impersonate or copy the style of an author or creator or even
    straight-up word-for-word plagiarize. While this is exciting when considering
    building your own ghostwriter to help you in your creative process, it’s much
    less so when you realize a competitor could do the same.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在法律担忧的前沿是版权侵权问题。在足够的数据上训练的LLM可以模仿或复制作者或创作者的风格，甚至直接一字不漏地剽窃。当考虑到构建自己的枪手以帮助你在创作过程中时，这很令人兴奋，但当你意识到竞争对手也能这样做时，这就不那么令人兴奋了。
- en: Probably the biggest lawsuit to pay attention to is that of *The New York Times*
    v. OpenAI.[¹](#footnote-97) *The New York Times* is in the process of legal action
    against OpenAI, stating their chatbots were trained on the *Times*’ intellectual
    property without consent. It gives evidence that the chatbots are giving word-for-word
    responses identical to proprietary information found in articles a user would
    normally have to pay to see. As a result, there is the concern that fewer users
    will visit their site, reducing ad revenue. Essentially, they stole their data
    and are now using it as a competitor in the information space.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要关注的最大诉讼是《纽约时报》诉OpenAI。[¹](#footnote-97)《纽约时报》正在对OpenAI提起法律诉讼，称其聊天机器人未经同意就在《时报》的知识产权上进行了训练。它提供了证据，表明聊天机器人给出的逐字逐句的回应与用户通常需要付费才能看到的专有信息相同。因此，人们担心用户访问其网站的人数会减少，从而减少广告收入。本质上，他们窃取了他们的数据，现在正在信息空间中作为竞争对手使用。
- en: Bystanders to the fight worry that if the *Times* wins, it may significantly
    hamper the development of AI and cause the United States to lose its position
    as the leader in the global AI development race. The more AI companies are exposed
    to copyright liability, the greater risk and thus loss to competition, which means
    less innovation. Conversely, they also worry that if the *Times* loses, it will
    further cut into the already struggling journalism business, where it’s already
    hard enough to find quality reports you can trust. This, too, would severely hurt
    AI development, which is always starving for good clean data. It appears to be
    a lose–lose situation for the AI field.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗这场斗争的旁观者担心，如果《时报》胜诉，可能会严重阻碍AI的发展，导致美国在全球AI发展竞赛中的领先地位受损。AI公司面临更大的版权责任风险，从而带来更大的竞争损失，这意味着更少的创新。相反，他们也担心，如果《时报》败诉，将进一步削弱已经陷入困境的新闻业，在那里，找到可以信赖的高质量报道已经很困难了。这对AI发展也是一个巨大的打击，AI发展总是渴望得到好的干净数据。这似乎是AI领域的一个双输局面。
- en: Regardless of who wins or loses the lawsuit, it’s pretty clear that current
    copyright laws never took into consideration that robots would eventually copy
    us. We need new laws, and it’s unclear whether our lawmakers are technically capable
    enough to meet the challenge. So again, we’d encourage you to participate in the
    creation process of regulations within your own communities.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 无论谁胜谁负，诉讼的结果都很明显，现行的版权法从未考虑过机器人最终会复制我们。我们需要新的法律，而且不清楚我们的立法者是否具备足够的技术能力来应对这一挑战。因此，我们再次鼓励你参与你所在社区内法规的制定过程。
- en: AI detection
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AI检测
- en: 'One area of concern that continues to break our hearts comes from the rise
    of “AI detection” products. Let us just state from the start: these products are
    all snake oils and shams. There’s no reliable way to determine whether a piece
    of text was written by a human or a bot. By this point in the book, we expect
    most readers to have come to this conclusion as well. The reason is simple: if
    we can reliably determine what is and isn’t generated text, we can create a new
    model to beat the detector. This is the whole point of adversarial machine learning.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个持续让我们心碎的担忧领域来自于“AI检测”产品的兴起。让我们一开始就明确：这些产品都是骗人的。没有可靠的方法来确定一段文本是由人类还是机器人所写。到这本书的这一部分，我们期望大多数读者也已经得出了这个结论。原因很简单：如果我们能够可靠地确定哪些是哪些不是生成文本，我们就可以创建一个新的模型来击败检测器。这正是对抗性机器学习的全部意义。
- en: 'There has been a running gag online that anything you read with the word “delve”
    in it must be written by an LLM (e.g., [https://mng.bz/o0nr](https://mng.bz/o0nr)).
    The word *delve* is statistically more likely to occur in generated text than
    in human speech, but that brings up the obvious questions: Which model? Which
    prompt? The human hubris to believe one can identify generated content simply
    by looking for particular words is laughable. But, of course, if people vainly
    believe this obvious falsehood, it’s no surprise they are willing to believe a
    more complex system or algorithm will be able to do it even better.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在网上有一个流行的玩笑，任何包含“深入挖掘”这个词的阅读内容都必须是由LLM撰写的（例如，[https://mng.bz/o0nr](https://mng.bz/o0nr)）。这个词*深入挖掘*在生成文本中比在人类语言中更可能出现，但这提出了明显的问题：哪个模型？哪个提示？人类自大的想法，认为仅通过寻找特定的单词就能识别生成内容，是可笑的。但当然，如果人们盲目地相信这种明显的错误，那么他们愿意相信一个更复杂或更先进的系统或算法能够做得更好，也就不足为奇了。
- en: The reason it breaks our hearts, though, is because we’ve read story after story
    of students getting punished, given failing grades on papers, forced to drop out
    of classes, and given plagiarism marks on their transcripts. Now, we don’t know
    the details of every case, but as experts in the technology in question, we choose
    to believe the students more often than not.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这让我们心碎的原因是因为我们读过一篇又一篇关于学生受到惩罚、论文被给予不及格分数、被迫退课以及在成绩单上被标记剽窃的故事。现在，我们不知道每个案例的细节，但作为相关技术的专家，我们更倾向于相信学生而非其他。
- en: The fact that a paper marked by an “AI detection” system as having a high probability
    of being written by AI is put in the same category as plagiarism is also ridiculous.
    Now, we don’t condone cheating, but LLMs are a new tool. They help us with language
    the way calculators help us with math. We have figured out ways to teach and evaluate
    students’ progress without creating “calculator detection” systems. We can do
    it again.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将被“AI检测”系统标记为有高概率由AI撰写的论文与剽窃归为同一类别也是荒谬的。现在，我们并不支持作弊，但大型语言模型（LLMs）是一种新工具。它们帮助我们处理语言，就像计算器帮助我们处理数学一样。我们已经找到了在不创建“计算器检测”系统的情况下教授和评估学生进步的方法。我们也可以再次做到这一点。
- en: Look, it’s not that it’s impossible to identify generated content. One investigation
    found that by simply searching for phrases like “As an AI language model” or “As
    of my last knowledge update,” they found hundreds of published papers in scientific
    journals written with the help of LLMs.[²](#footnote-98) Some phrases are obvious
    signs, but these are only identified due to the pure laziness of the authors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，识别生成内容并不是不可能的。一项调查发现，通过简单地搜索“作为人工智能语言模型”或“截至我最后一次知识更新”等短语，他们发现了数百篇在科学期刊上发表的、在LLMs帮助下撰写的论文。[²](#footnote-98)
    一些短语是明显的迹象，但这些只是由于作者们的纯粹懒惰而被识别。
- en: The worst part of all this is that since these detection systems are fake, bad,
    and full of false positives, they seem to be enforced arbitrarily and randomly
    at the teacher’s discretion. It’s hard to believe that a majority of papers aren’t
    flagged, so why is only a select group of students called out for it? It’s because
    these systems appear to have become a weapon of power and discrimination for teachers
    who will wield them to punish students they don’t like—not to mention the obvious
    hypocrisy since we could guess that some of these teachers are the same ones publishing
    papers with phrases like “As an AI language model” in them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些中最糟糕的部分是，由于这些检测系统是虚假的、糟糕的，并且充满了误报，它们似乎是由教师任意和随机地执行的。很难相信大多数论文没有被标记，那么为什么只有一小部分学生被点名批评呢？这是因为这些系统似乎已经变成了教师手中的权力和歧视武器，他们会利用这些系统来惩罚他们不喜欢的学生——更不用说这种明显的虚伪，因为我们猜测这些教师中的一些人可能就是那些在论文中使用“作为人工智能语言模型”等短语的人。
- en: Bias and ethics
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏见与伦理
- en: This isn’t the first time we have spoken about bias and ethics found inside
    LLMs, but this time, let’s take a slightly deeper dive into what the discussion
    deserves. Let’s say a person is tied to some trolley tracks, you do nothing, and
    the trolley runs them over, ending their life. Are you responsible? This thought
    experiment, called “The Trolley Problem,” has been discussed ad nauseam; there’s
    even a video game (Trolley Problem Inc. from Read Graves) that poses dozens of
    variations based on published papers. We won’t even attempt to answer the question,
    but we will give you a brief rundown on how you might be able to decide the answer
    for yourself.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们第一次讨论LLMs（大型语言模型）中发现的偏见和伦理问题，但这次，让我们更深入地探讨这次讨论应得的讨论。假设一个人被绑在轨道上，你什么也没做，电车撞上了他们，结束了他们的生命。你是否有责任？这个被称为“电车问题”的思想实验已经被讨论得淋漓尽致；甚至有一个基于已发表论文的电子游戏（Read
    Graves的Trolley Problem Inc.），提出了数十种变化。我们甚至不会尝试回答这个问题，但我们会简要介绍一下你如何自己决定答案。
- en: 'There are way more than two ways you can analyze this, but we’ll only focus
    on two—the moral and the ethical—and we’ll reduce these because this isn’t a philosophy
    book. Morality here helps you determine fault based on a belief of what is good/not
    good. Ethics help us determine consequences within the practical framework of
    the legal system that exists within the societies we live in. If you are morally
    responsible for the death of the person on the tracks, you believe that it was
    ultimately your fault, that your actions are the cause of the disliving. This
    is different from ethical responsibility, which would mean that you deserve legal
    and societal consequences for that action. They can agree, but they don’t have
    to. Changing the context can help clarify the distinction: if you tell someone
    that a knife isn’t sharp and they cut themselves on it while checking, morally,
    it’s likely your fault they were in that situation, but ethically, you will avoid
    an attempted murder charge.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分析这种问题的方法远不止两种，但我们只会关注其中两种——道德和伦理——并且我们会简化这些概念，因为这不是一本哲学书。在这里，道德帮助你根据对好/不好的信念来判断过错。伦理帮助我们确定在我们所生活的社会中的法律体系内的实际框架中的后果。如果你对轨道上的人的死亡负有道德责任，你相信这是你最终的责任，你的行为是导致他们死亡的原因。这与伦理责任不同，这意味着你因该行为应受到法律和社会的后果。他们可以同意，但不必如此。改变语境可以帮助阐明区别：如果你告诉某人一把刀不锋利，他们在检查时切到了自己，从道德上讲，他们陷入那种情况可能是你的责任，但从伦理上讲，你会避免被控企图谋杀。
- en: Algorithms create thousands of these situations where our morality and our ethics
    likely don’t agree. There’s an old example of moral and ethical responsibility
    in the Talmud that decides that a person is not a murderer if they push another
    person into water or fire and the pushed person fails to escape.[³](#footnote-99)
    Depending on your beliefs and the law you live under, Meta could be either morally
    or ethically at fault for genocide (not joking[⁴](#footnote-100)) in Myanmar.
    Meta didn’t even do the pushing into the fire in that scenario; their algorithm
    did. This is obviously a charged and brutal example, but LLMs create a very real
    scenario where ML practitioners need practical, consistent, and defensible frameworks
    of both morality and ethics, or they risk real tragedy under their watch. Obviously,
    we aren’t the arbiters of morality and aren’t going to judge you about where you
    find yourself there, but you should still consider the broader context of any
    system you create.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 算法创造了数千种这样的情况，在这些情况下，我们的道德和伦理可能并不一致。在塔木德中有一个关于道德和伦理责任的古老例子，它决定如果一个人把另一个人推入水中或火中，而被推的人未能逃脱，那么这个人不是杀人犯。[³](#footnote-99)
    根据你的信仰和你在的法律体系下，Meta在缅甸的种族灭绝（不是开玩笑[⁴](#footnote-100)）中可能是道德上或伦理上有过错的。在那种情况下，Meta甚至没有把人推入火中；是他们的算法做的。这显然是一个充满争议和残酷的例子，但LLMs创造了一个非常真实的情况，其中机器学习从业者需要实际、一致和可辩护的道德和伦理框架，否则他们可能会在他们监管下发生真正的悲剧。显然，我们不是道德的仲裁者，也不会评判你在那里的位置，但你仍然应该考虑你创建的任何系统的更广泛背景。
- en: Laws are coming
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 法律正在到来
- en: One thing we *can* be sure about is that regulation will come, and companies
    will be held responsible for what their AI agents do. Air Canada found this out
    the hard way when the courts ruled against it, saying the company had to honor
    a refund policy that its chatbot had completely made up ([https://mng.bz/pxvG](https://mng.bz/pxvG)).
    The bot gave incorrect information. It did link the customer to the correct refund
    policy; however, the courts rightly questioned “why customers should have to double-check
    information found in one part of its website on another part of its website.”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以肯定的一件事是，监管将会到来，公司将对它们的AI代理的行为负责。加拿大航空通过法院判决得知这一点，法院裁定该公司必须遵守其聊天机器人完全编造的退款政策([https://mng.bz/pxvG](https://mng.bz/pxvG))。该机器人提供了错误的信息。它确实将客户链接到了正确的退款政策；然而，法院正确地质疑了“为什么客户需要在网站的另一部分找到的信息在网站的其他部分再次进行双重检查。”
- en: We’ve seen similar cases where users have used prompt engineering to trick Chevy’s
    LLM chatbot into selling a 2024 Tahoe for $1 ([https://mng.bz/XVmG](https://mng.bz/XVmG)),
    and DPD needed to “shut down its AI element” after a customer got it to admit
    to being the worst delivery company in the world.[⁵](#footnote-101) As we said
    earlier, it’s difficult to tell, even with existing legislation, what is ethically
    allowable for an LLM to do. Of course, it brings up the question of whether, if
    the chatbot was licensed and equipped to sell cars and did complete such a transaction,
    the customer’s bad faith interaction would actually matter, or whether a company
    would still be held ethically responsible for upholding such a transaction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到过类似的案例，用户通过提示工程技巧欺骗了雪佛兰的LLM聊天机器人，以1美元的价格出售了一辆2024年的塔霍车型([https://mng.bz/XVmG](https://mng.bz/XVmG))，DPD在一位客户让它承认自己是世界上最差的快递公司后，不得不“关闭其AI元素”。[⁵](#footnote-101)正如我们之前所说，即使有现有的立法，也很难判断LLM在道德上可以做什么。当然，这也引发了一个问题：如果聊天机器人获得了销售汽车的许可并完成了这样的交易，客户的恶意互动是否真的重要，或者公司是否仍然在道德上对维护这样的交易负有责任。
- en: Being held responsible for what an LLM generates is enough to make you think
    twice about many applications you may consider using it for. The higher the risk,
    the more time you should take to pause and consider potential legal ramifications.
    We highly recommend dialing in your prompt engineering system, setting up guard
    rails to keep your agent on task, and absolutely being sure to save your logs
    and keep your customer chat history.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM生成的内容负责足以让你三思而后行，考虑你可能考虑使用它的许多应用。风险越高，你应该花更多的时间暂停并考虑潜在的法律后果。我们强烈建议调整你的提示工程系统，设置护栏以保持你的代理在任务上，并且绝对确保保存你的日志并保留客户聊天记录。
- en: 12.2.2 LLMs are getting bigger
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 LLMs正在变得更大
- en: Another thing we can be sure of is that we will continue to see models getting
    bigger and bigger for the near future. Since larger models continue to display
    emergent behavior, there’s no reason for companies to stop taking this approach
    when simply throwing money at the problem seems to generate more money. Not to
    mention, for companies that have invested the most, larger models are harder to
    replicate. As you’ve probably found, the best way for smaller companies to compete
    is to create smaller, specialized models. Ultimately, as long as we have large-enough
    training datasets to accommodate more parameters, we can expect to see more parameters
    stuffed into a model, but the question of whether we’ve ever had adequate data
    to demonstrate “general intelligence” (as in AGI) is as murky as ever.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件我们可以确定的事情是，在不久的将来，我们还将继续看到模型变得越来越庞大。由于更大的模型持续表现出涌现行为，公司没有理由停止采取这种方法，因为简单地投入资金似乎能带来更多的收益。更不用说，对于投入最多的公司来说，更大的模型更难复制。正如你可能发现的，小型公司竞争的最佳方式是创建更小、更专业的模型。最终，只要我们有足够大的训练数据集来容纳更多的参数，我们就可以期待看到更多的参数被塞入模型中，但关于我们是否曾经有过足够的数据来证明“通用智能”（如AGI）的问题，仍然像以往一样模糊不清。
- en: Larger context windows
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更大的上下文窗口
- en: It’s not just larger models. We are really excited to see context lengths grow
    as well. When we started working on this book, they were a real limitation. It
    was rare to see models with context lengths greater than 10K tokens. ChatGPT only
    offered lengths up to 4,096 tokens at the time. A year later, and we see models
    like Gemini 1.5 Pro offering a context length of up to 1 million tokens, with
    researchers indicating that it can handle up to 10 million tokens in test cases
    ([https://mng.bz/YV4N](https://mng.bz/YV4N)). To put it in perspective, the entire
    seven-book Harry Potter series is 1,084,170 words (I didn’t count them; [https://wordsrated.com/harry-potter-stats/](https://wordsrated.com/harry-potter-stats/)),
    which would come out to roughly 1.5 million tokens depending on your tokenizer.
    At these lengths, it’s hard to believe there are any limitations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅是大模型。我们非常兴奋地看到上下文长度也在增长。当我们开始编写这本书时，这是一个真正的限制。很少看到上下文长度超过10K令牌的模型。当时ChatGPT只提供最多4,096个令牌的长度。一年后，我们看到Gemini
    1.5 Pro这样的模型提供了最多1百万个令牌的上下文长度，研究人员指出，它在测试案例中可以处理多达1千万个令牌([https://mng.bz/YV4N](https://mng.bz/YV4N))。为了更直观地说明，整个七部《哈利·波特》系列共有1,084,170个单词（我没有数过；[https://wordsrated.com/harry-potter-stats/](https://wordsrated.com/harry-potter-stats/))，根据你的分词器，这大约相当于1.5百万个令牌。在这些长度下，很难相信有任何限制。
- en: Obviously, there still are. These larger models with near infinite context windows
    generally have you paying per token. If the model doesn’t force your users to
    send smaller queries, your wallet will. Not to mention, if you are reading this
    book, you are likely more interested in smaller open source models you can deploy
    yourself, and many of these definitely still have limiting context sizes you have
    to work with. Don’t worry, though; right now and in the future, even smaller models
    will have million-sized context windows. There’s a lot of interesting research
    going into this area. If you are interested, we recommend you check out RoPE,[⁶](#footnote-102)
    YaRN,[⁷](#footnote-103) and Hyena.[⁸](#footnote-104)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，挑战仍然存在。这些具有近乎无限上下文窗口的更大模型通常按令牌收费。如果模型不强迫用户发送更小的查询，那么用户的钱包就会受到影响。更不用说，如果你正在阅读这本书，你很可能对可以自己部署的小型开源模型更感兴趣，而这些模型中许多确实仍然有必须与之合作的限制性上下文大小。不过，不用担心；现在和将来，即使是更小的模型也将拥有百万级别的上下文窗口。这个领域正在进行许多有趣的研究。如果你感兴趣，我们建议你查看RoPE[⁶](#footnote-102)、YaRN[⁷](#footnote-103)和Hyena[⁸](#footnote-104)。
- en: The next attention
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下一个注意力
- en: Of course, larger context windows are great, but they come at a cost. Remember,
    at the center of an LLM lies the attention algorithm, which is quadratic in complexity—meaning
    the more data we throw at it, the more compute we have to throw at it as well.
    One challenge driving the research community is finding the next attention algorithm
    that doesn’t suffer from this same problem. Can we build transformers with a new
    algorithm that is only linear in complexity? That is the billion-dollar question
    right now.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，更大的上下文窗口是很好的，但它们也有代价。记住，在大型语言模型（LLM）的中心是注意力算法，其复杂度是二次的——这意味着我们投入的数据越多，我们就需要投入更多的计算资源。推动研究社区的一个挑战是找到下一个不遭受这种相同问题的注意力算法。我们能否构建一个仅具有线性复杂度的新算法的transformers？这正是现在的十亿美元问题。
- en: There are lots of competing innovations in this field, and we don’t even have
    time to discuss all of our absolute favorites. Two of those favorites are MAMBA,
    an alternative to transformers, and KAN, an alternative to multilayer perceptrons
    (MLPs). MAMBA, in particular, is an improvement on state space models (SSMs) incorporated
    into an attention-free neural network architecture.[⁹](#footnote-105) By itself,
    it isn’t all that impressive, as it took lots of hardware hacking to make it somewhat
    performant. However, later JAMBA came out, a MAMBA-style model that uses hybrid
    SSM-transformer layers and joint attention.[^(10)](#footnote-106) The hybrid approach
    appears to give us the best of both worlds.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域有许多竞争性的创新，我们甚至没有时间讨论我们所有绝对最喜欢的。其中两个最喜欢的分别是MAMBA，作为transformers的替代品，以及KAN，作为多层感知器（MLPs）的替代品。特别是MAMBA，它是对状态空间模型（SSMs）的改进，并将其融入了一个无注意力的神经网络架构中。[⁹](#footnote-105)
    单独来看，它并不那么令人印象深刻，因为它需要大量的硬件黑客技术才能使其具有一定的性能。然而，后来出现了JAMBA，这是一个MAMBA风格的模型，它使用了混合SSM-transformer层和联合注意力。[^(10)](#footnote-106)
    这种混合方法似乎为我们提供了两者的最佳结合。
- en: So you can experience it for yourself, in listing 12.1, we will finetune and
    run inference on a JAMBA model. This model is a mixture-of-experts model with
    52B parameters, and the implementation will allow for 140K context lengths on
    an 80 GB GPU, which is much better performance than you’d get with an attention
    model alone. This example was adapted right from the Hugging Face model card,
    so the syntax should look very familiar compared to every other simple transformer
    implementation, and we are very grateful for the ease of trying out brand-new
    stuff.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: For the training portion, unfortunately, the model is too big, even in half
    precision, to fit on a single 80 GB GPU, so you’ll have to use Accelerate to parallelize
    it between several GPUs to complete training. If you don’t have that compute just
    lying around, you can complete the imports up to the tokenizer and skip to after
    the training portion, changing very little. We aren’t doing anything fancy; the
    dataset we’ll use for training is just a bunch of famous quotes in English from
    various authors retrieved from Goodreads consisting of quote, author, and tags,
    so don’t feel like you are missing out if you decide to skip finetuning. We’ll
    start by loading the tokenizer, model, and dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Finetuning and inferencing JAMBA
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once all of those are in memory (you can stream the dataset if your hardware
    is limited), we’ll create training arguments and a LoRA config to help the finetuning
    work on even smaller hardware:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And now, for the finale, similar to sklearn’s `model.fit()`, transformers’
    `trainer.train()` has become a moniker for why anyone can learn how to interact
    with state-of-the-art ML models. Once training completes (it took a little under
    an hour for us), we’ll save local versions of the tokenizer and the model and
    delete the model in memory:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll reload the model, but in a memory-efficient way, to be used for
    inference. With an 80 GB GPU and loading in 8bit with this BitsandBytes config,
    you can now fit the model and a significant amount of data on a single GPU. Loading
    in 4bit allows that on any type of A100 or two 3090s, similar to a 70B parameter
    transformer. Using quantization to get it down to a 1-bit model, you can fit this
    model and a significant amount of data on a single 3090\. We’ll use the following
    8bit inference implementation and run inference on it:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are blown away almost monthly at this point by the alternatives to various
    parts of LLM systems that pop up. Here, we’d like to draw your attention way back
    to where LLMs got their big break: “Attention Is All You Need.”[^(11)](#footnote-107)
    That paper showed that you could use dumb MLPs to get amazing results, using only
    attention to bridge the gap. We’re entering a new age where we aren’t focusing
    on just what we need but what we want for the best results. For example, we want
    subquadratic drop-in replacements for attention that match or beat flash attention
    for speed. We want attention-free transformers and millions-long context lengths
    with no “lost in the middle” problems. We want alternatives to dense MLPs with
    no drops in accuracy or learning speed. We are, bit by bit, getting all of these
    and more.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the boundaries of compression
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After going down to INT4, there are experimental quantization strategies for
    going even further down to INT2\. INT2 70B models still perform decently, much
    to many peoples’ surprise. Then there’s research suggesting we could potentially
    go even smaller to 1.58 bits per weight or even 0.68 using ternary and other smaller
    operators. Want to test it out? Llama3 70B already has 1-bit quantization implementations
    in GGUF, GPTQ, and AWQ formats, and it only takes up 16.6 GB of memory. Go nuts!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: There’s another dimension to this, which doesn’t involve compressing models
    but instead decouples the idea of models being one piece and thinking of models
    as collections of layers and parameters again. Speculative decoding gives us yet
    another way of accessing large models quickly. Speculative decoding requires not
    just enough memory to load one large model but also another smaller model alongside
    it—think distillation models. An example often used in production these days is
    Whisper-Large-v3 and Distil-Whisper-Large-V3\. Whisper is a multimodal LLM that
    focuses on the speech-to-text problem, but speculative decoding will work with
    any two models that have the same architecture and different sizes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'This method allows us to sample larger models quicker (sometimes a straight
    2× speed boost) by computing several tokens in parallel and by an approximation
    “assistant” model that allows us to both complete a step and verify whether that
    step is easy or hard at the same time. The basic idea is this: use the smaller,
    faster Distil-Whisper model to generate guesses about the end result, and allow
    Whisper to evaluate those guesses in parallel, ignoring the ones that it would
    do the same thing on and correcting the ones that it would change. This allows
    for the speed of a smaller model with the accuracy of a larger one.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: In listing 12.2, we demonstrate speculative decoding on an English audio dataset.
    We’ll load Whisper and Distil-Whisper, load the dataset, and then add an `assistant_
    model` to the generation keyword arguments (`generate_kwargs`). You may ask, how
    does this system know that the assistant model is only meant to help with decoding,
    as the name suggests? Well, we load the assistant model with `AutoModelForCausalLM`
    instead of the speech sequence-to-sequence version. This way, the model will only
    help with the easier decoding steps in parallel with the larger one. With that
    done, we’re free to test.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 Speculative decoding with Whisper
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In our testing, we observed about 42 seconds for Whisper-Large-V3 to get through
    all 73 examples with scaled dot product attention. With speculative decoding,
    that dropped to 18.7 seconds, with the exact same word error rate (WER). So there
    was an almost 2× speed increase with absolutely zero drop in accuracy. Yeah, pretty
    nuts.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we were wondering, “Why doesn’t everyone use this for everything
    all the time?” Here are the drawbacks to this method: first, it works best in
    smaller sequences. With LLMs, that’s under 128 tokens of generation or around
    20 seconds of audio processing. With the larger generations, the speed boost will
    be negligible. Beyond that, we don’t always have access to perfectly compatible
    pairs of large and small models, like BERT versus DistilBERT. The last reason
    is that very few people really know about it, even with its ease of implementation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, whether it’s sub-bit quantization, speculative decoding, or other
    advances, LLMs are pushing research into compression methodologies more than any
    other technology, and it’s interesting to watch as new techniques change the landscape.
    As these methods improve, we can push models to smaller and cheaper hardware,
    making the field even more accessible.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Multimodal spaces
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are so excited about the possibilities within multimodality. Going back to
    chapter 2, multimodality is one of the main features of language we haven’t seen
    as many solutions crop up for, and we’re seeing a shift toward actually attempting
    to solve phonetics. Audio isn’t the only modality that humans operate in, though.
    Accordingly, the push toward combining phonetics, semantics, and pragmatics and
    getting as much context within the same embedding space (for comparison) as the
    text is very strong. With this in mind, here are some points of interest in the
    landscape.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'The first we want to draw attention to is ImageBind, a project showcasing that
    instead of trying to curtail a model into ingesting every type of data, we can
    instead squish every type of data into an embedding space the model would already
    be familiar with and be able to process. You can take a look at the official demo
    here: [https://imagebind.metademolab.com/](https://imagebind.metademolab.com/).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'ImageBind builds off what multimodal projection models such as CLIP have already
    been showcasing for some time: the ability to create and process embeddings is
    the true power behind deterministic LLM systems. You can use these models for
    very fast searches, including searches that have been, up to this point, nigh
    impossible, like asking to find images of animals that make sounds similar to
    an uploaded audio clip.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'OneLLM flips this logic the other way around, taking one model and one multimodal
    encoder to unify and embed eight modalities instead of the ImageBind example of
    using six different encoders to embed six modalities in the same dimension. It
    can be found here: [https://onellm.csuhan.com/](https://onellm.csuhan.com/). The
    big idea behind OneLLM is aligning the unified encoder using language, which offers
    a unique spin on multimodality that aligns the process of encoding rather than
    the result.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: We are extremely excited about the research happening in this area. This research
    is able to help bridge the gap between phonetics and pragmatics in the model ecosystem
    and allow for more human-like understanding and interaction, especially in the
    search field.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.4 Datasets
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One exciting change we are seeing inside the industry due to the introduction
    of LLMs is that companies are finally starting to understand the importance of
    governing and managing their data. For some, it’s the drive to finetune their
    own LLMs and get in on the exciting race to deliver AI products. For others, it’s
    the fear of becoming obsolete, as the capabilities of these systems far surpass
    previous technologies; they are finding it’s only their data that provides any
    type of moat or protection from competition. And for everyone, it’s the worry
    they’ll make the same mistakes they’ve seen other companies make.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: LLMs aren’t just a driving factor; they are also helping teams label, tag, organize,
    and clean their data. Many companies had piles of data they didn’t know what to
    do with, but with LLM models like CLIP, captioning images has become a breeze.
    Some companies have found that simply creating embedding spaces of their text,
    images, audio, and video has allowed them to create meaningful structures for
    datasets previously unstructured. Structured data is much easier to operate around,
    opening doors for search, recommendations, and other insights.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: One aspect we see currently missing in the industry is valuable open source
    datasets, especially when it comes to evaluations. Many of the current benchmarks
    used to evaluate models rely on multiple-choice questions, but this is inefficient
    for anyone trying to create an LLM application. In the real world, when are your
    users going to ask your model questions in a multiple-choice format? Next to never.
    People ask freeform questions in conversations and when seeking help since they
    don’t know the answer themselves. However, these evaluation datasets have become
    benchmarks simply because they are easy for researchers to gather, compile, and
    evaluate for accuracy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we believe another inevitability is the need for more language
    representation. The world is a tapestry of diverse languages and dialects, each
    carrying its unique cultural nuances and communicative subtleties. However, many
    languages remain underrepresented in existing datasets, leading to models that
    are biased toward more dominant languages. As technology becomes increasingly
    global, the inclusion of a wider range of languages is crucial. Adding multiple
    languages not only promotes inclusivity but also enhances the accuracy and applicability
    of language models in various international contexts, bridging communication gaps
    and fostering a more connected world. Imagine your startup didn’t need to pay
    anyone to get accurate information regarding entering China, Russia, or Saudi
    Arabia to expand your market.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.5 Solving hallucination
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s a lot of evidence that LLMs have more information in them than they
    readily give out and even more evidence that people are generally either terrible
    or malicious at prompting. As a result, you’ll find that hallucinations are one
    of the largest roadblocks when trying to develop an application that consistently
    delivers results. This problem has frustrated many software engineering teams
    that are used to deterministic computer algorithms and rarely deal with nondeterministic
    systems. For many statisticians who are more familiar with these types of systems,
    hallucinations are seen as a feature, not a bug. Regardless of where you stand,
    there’s a lot of research going into the best ways to handle hallucinations, and
    this is an area of interest you should be watching.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Better prompt engineering
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One area that’s interesting to watch and has shown great improvement over time
    is prompt engineering. One prompt engineering tool that helps reduce hallucinations
    is DSPy. We went over it briefly in chapter 7, but here we’ll give an example
    of how it works and why it can be a helpful step for solving hallucination in
    your LLMs. We’ve discussed the fact that LLMs are characteristically bad at math,
    even simple math, several times throughout the book, and we’ve also discussed
    why, but we haven’t really discussed solutions other than improving your tokenization.
    So in listing 12.3, we will show just how good you can coax an LLM to be at math
    with zero tokenization changes, zero finetuning, and no LoRAs or DoRAs, just optimizing
    your prompts to tell the model exactly how to answer the questions you’re asking.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: We’ll do this using the dspy-ai Python package and Llama3-8B-Instruct. We’ll
    start by loading and quantizing the model to fit on most GPUs and the Grade-School
    Math 8K dataset. We picked this dataset because it’s a collection of math problems
    that you, as a person who has graduated elementary (primary) school, likely don’t
    even need a calculator to solve. We’ll use 200 examples for our train and test
    (dev) sets, although we’d recommend you play with these numbers to find the best
    ratio for your use case without data leakage.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 DSPy for math
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have our imports and loading ready, we’ll need to address the fact
    that we loaded Llama3 using transformers and not DSPy. DSPy expects to interact
    with models utilizing the OpenAI API, but we have a model loaded locally from
    Hugging Face, DSPy has recently added HFModel to their package, and it can now
    be easily imported, rather than needing the wrapper defined next. First, we make
    a simple function to map any keyword argument differences between the APIs, like
    `max_tokens` vs `max_new_tokens`, and then we create a class that will act as
    the wrapper for our model to generate answers and optimize the prompt. Once that’s
    ready, we’ll load DSPy:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Sets up the LM'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets up ΔSPY to use that LM'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are prepared with an LLM to take our math test, let’s test it.
    We’ll start by establishing a baseline. We’ll define a simple chain-of-thought
    (CoT)-like prompt in the `QASignature` class, which we’ll use to define a zero-shot
    version to use as a baseline. The prompt is likely pretty close to prompts you’ve
    seen before, so hopefully, this will be a very relevant demonstration of tasks
    you may be working on. For evaluation, we’re using DSPy’s `gsm8k_metric`, which
    we imported at the top to evaluate against, but you could always create your own:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Δefines the QASignature and CoT'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets up the evaluator, which can be used multiple times'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Evaluates how the LLM does with no changes'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With our simple zero-shot CoT prompt, Llama3 gets only 14.5% of the questions
    correct. This result might not seem very good, but it is actually quite a bit
    better than just running the model on the questions alone without any prompt,
    which only yields about 1% to 5% correct.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'With the baseline out of the way, let’s move on to the bread and butter of
    DSPy, optimizing the prompt to see where that gets us. There’s been some evolution
    in what people think of as a CoT prompt since the original paper came out. CoT
    has evolved in the industry to mean more than just adding “think step by step”
    in your prompt since this approach is seen more as just basic prompt engineering,
    whereas allowing the model to few-shot prompt itself to get a rationale for its
    ultimate output is considered the new CoT, and that’s how the DSPy framework uses
    those terms. With that explanation, we’ll go ahead and create a `CoT` class using
    the `dspy.ChainOfThought` function and then evaluate it like we did our `ZeroShot`
    class:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Sets up the optimizer'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Optimize the prompts'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Evaluates our “optimized_cot” program'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Look at that! If it doesn’t astonish you that the accuracy jumped from 14.5%
    to 74.5% by changing only the prompts—remember we haven’t done any finetuning
    or training—we don’t know what will. People are speculating whether the age of
    the prompt engineer is over, but we’d like to think that it’s just begun. That
    said, the age of “coming up with a clever string and doing no follow-up” has been
    over and shouldn’t have ever started. In this example, we used arbitrary boundaries,
    gave the sections of the dataset and the numbers absolutely no thought, and didn’t
    include any helpful tools or context for the model to access to improve. If we
    did, you’d see that after applying all the prompt engineering tricks in the book,
    it isn’t difficult to push the model’s abilities to staggering levels, even on
    things LLMs are characteristically bad at—like math.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Grounding
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you are looking for ways to combat hallucinations, you’ll run into the term
    *grounding*. Grounding is when we give the LLM necessary context in the prompt.
    By giving it the information it needs, we are helping to provide a solid base
    for the generation to build off of, so it’s less likely to dream up visions out
    of thin air. If this sounds familiar, it should, as we have used one of the most
    common grounding techniques, RAG, several times in this book.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The term *RAG* (retrieval augmented generation) is, at face value, synonymous
    with grounding since we are literally retrieving the appropriate context based
    on the prompt and then using it to augment the text generated from the LLM. However,
    RAG has become synonymous with using semantic search with a VectorDB for the retrieval
    portion. Technically, you could use any type of search algorithm or any type of
    database, but if you tell someone in the industry you have set up a RAG system,
    they will assume the former architecture.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: With that clarification, RAG applications are most useful for answering simple
    questions. Consider the question, “What is Gal Gadot’s husband’s current job?”
    It’s really two questions in one, “Who is Gal Gadot’s husband?” and once we know
    that, “What does he do?” RAG alone is pretty terrible at solving these multistep
    questions, as a similarity vector search will likely return many articles about
    Gal Gadot and probably none about Jaron Varsano, her husband.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'We can enhance this approach in an important way that we haven’t touched on
    yet: using knowledge graphs. Knowledge graphs store information in a structure
    that captures relationships between entities. This structure consists of nodes
    that represent objects and edges that represent relationships. A graph database
    like NEO4J makes it easy to create and query knowledge graphs. And as it turns
    out, knowledge graphs are amazing at answering more complex multipart questions
    where you need to connect the dots between linked pieces of information. Why?
    Because they’ve already connected the dots for us.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Many teams who have struggled to get value out of RAG have been able to see
    large improvements once they transitioned to a graph database from a vector one.
    This comes with two major hurdles, though. First, we can no longer simply embed
    our prompts and pull similar matches; we have the much harder task of coming up
    with a way to turn our prompts into queries our graph database will understand.
    While there are several methods to take this on, it’s just another NLP problem.
    Thankfully, as it turns out, LLMs are really good at this! Second, and probably
    the bigger problem, is that it is much harder to turn your documents into a knowledge
    graph. This is why vector databases have become so popular—the ease of turning
    your data into embeddings to search against. Turning your data into a knowledge
    graph will be a bit more work and take additional expertise, but it can really
    set you up for success down the road.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Right now, few teams are willing to invest in the extra data engineering to
    prepare their data into a knowledge graph. Most companies are still looking for
    quick wins, building simple wrappers around LLM APIs. As the industry matures,
    we believe we’ll start to see organizations shift toward building knowledge graphs
    from their proprietary data to eke out better performance from their LLM applications.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge editing
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another promising field of research to combat hallucinations is *knowledge editing*.
    Knowledge editing is the process of efficiently adjusting specific behaviors.
    Optimally, this would look like surgery where we precisely go in and change the
    exact model weights that activate when we get incorrect responses, as can be seen
    in figure 12.2\. Knowledge editing can be used for many things, but it is often
    used to combat factual decay—the fact that, over time, facts change, like who
    the current Super Bowl winner is or the current president of any individual country.
    We could retrain or finetune the model, but these are often much heavier solutions
    that may change the model in unexpected ways when all we want to do is update
    a fact or two.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/12-2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 Knowledge editing is a technique to essentially perform surgery
    on a model to directly insert, update, or erase information.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Knowledge editing is an interesting field of research that we unfortunately
    didn’t have the space to go into in this book. A host of algorithms and techniques
    have been created to do it, like ROME, MEND, and GRACE. For those interested in
    using any of these techniques, we recommend first checking out EasyEdit at [https://github.com/zjunlp/EasyEdit](https://github.com/zjunlp/EasyEdit).
    EasyEdit is a project that has implemented the most common knowledge editing techniques
    and provides a framework to utilize them easily. It includes examples, tutorials,
    and more to get you started.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.6 New hardware
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with most popular technologies, LLMs have already created a fierce market
    of competition. While most companies are still competing on capabilities and features,
    there’s also a clear drive to make them faster and cheaper. We’ve discussed many
    of these methods you can employ, like quantization and compilation. One we expect
    to see more of is innovation around hardware.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: In fact, Sam Altman, CEO of OpenAI, has been trying to raise funds to the tune
    of $7 trillion dollars to invest in the semiconductor industry.[^(12)](#footnote-108)
    We’ve talked about the global GPU shortage before, but no one is as annoyed about
    it as some of the biggest players. The investment would go further than just meeting
    demand; it would also accelerate development and research into better chips like
    Application-Specifc Integrated Circuits (ASICs).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: We’ve talked about and have used GPUs a lot throughout this book, but GPUs weren’t
    designed for AI; they were designed for graphics. Of course, that fact didn’t
    stop NVIDIA from briefly becoming the world’s most valuable company.[^(13)](#footnote-109)
    ASICs are designed for specific tasks; an example would be Google’s TPUs or tensor
    processing units. ASICs designed to handle AI workloads are NPUs (neural processing
    units), and chances are, you’ve never heard of, or at least never seen, an NPU
    chip before. We point this out to show there’s still plenty of room for improvement,
    and it’s likely we will see a large array of new accelerators in the future, from
    better GPUs to NPUs and everything in between. For more info, take a look at Cerebras
    ([https://cerebras.ai/product-chip/](https://cerebras.ai/product-chip/)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: One of the authors of this book spent a good portion of his career working for
    Intel and Micron developing the now-discontinued memory technology known as 3D
    XPoint (3DxP). The details of 3DxP aren’t important for this discussion; what
    it offered, extremely fast and cheap memory, is. It was sold under the brand name
    Optane for several years and had even earned the moniker “The Fastest SSD Ever
    Made.”[^(14)](#footnote-110) This technology proved itself to be almost as fast
    as RAM but almost as cheap to produce as NAND flash memory and could be used to
    replace either.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a world where every processor conveniently had 500 GB or even 1 TB of
    memory space. Most of the limitations we’ve discussed so far would simply disappear.
    You could load entire LLMs the size of GPT-4 onto one GPU. You wouldn’t have to
    worry about parallelization or the underutilization problems that come with the
    extra overhead. Did I mention 3DxP was nonvolatile as well? Load your model once,
    and you’re done; you’d never need to reload it, even if you had to restart your
    server, which would make jobs like autoscaling so much easier.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 3DxP was a technology that had already proven itself in the market as capable,
    but it nonetheless suffered due to a perceived lack of demand. Consumers didn’t
    know what to do with this new layer in the memory hierarchy that it provided.
    Personally, with the arrival of LLMs, the authors see plenty of demand now for
    a technology like this. We’ll just have to wait and see whether the semiconductor
    industry decides to reinvest.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.7 Agents will become useful
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lastly, we believe LLM-based agents will eventually be more than just a novelty
    that works only in demos. Most agents we’ve seen have simply been feats of magic,
    or should I say smoke and mirrors, throwing a few prompt engineering tricks at
    the largest models. The fact that several of them work at all—even in a limited
    capacity—shines light on the possibilities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve seen several companies chase after the holy grail, building agents to
    replace software engineers. In fact, you’ll see them try to build agents to replace
    doctors, sales associates, or managers. But just as many companies and AI experts
    used to promise we’d have self-driving cars in the near future, that near future
    keeps on eluding us. Don’t get me wrong: it’s not like we don’t have self-driving
    cars, but they are much more of an annoyance than anything, and they can only
    drive in select locations as rideshare vehicles. In a similar fashion, we aren’t
    too worried about agents replacing any occupation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: What we are more interested in are small agents—agents trained and finetuned
    to do a specialized task but with greater flexibility to hold conversations. Many
    video game NPCs would benefit from this type of setup where they could not only
    use an LLM to hold random conversations and provide a more immersive experience
    but also to decide to take actions that would shape a unique story.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: We are also likely to see them do smaller tasks well first. For example, LLMs
    can already read your email and summarize them for you, but a simple agent would
    go a step further and generate email responses for you. Maybe it wouldn’t actually
    send them, but simply provide you with the options, and all you’d have to do is
    pick the one you want, and then it would send it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: But mostly, we are excited to see LLM agents replace other bots. For example,
    who hasn’t uploaded their resume only to find they have to reenter all their information?
    Either because the resume extraction tool didn’t work well or it didn’t even exist.
    An LLM agent can not only read your resume and extract the information but also
    double-check its work and make sure it makes sense. Plus, we haven’t even mentioned
    the applicant tracking systems that automatically screen resumes based on keywords.
    These systems are often easily manipulated and terrible at separating the cream
    from the crop. An LLM agent has a much better chance of performing this task well.
    Of course, we care about ensuring fair hiring practices, but these systems are
    already automated and biased to some extent. A better model is an opportunity
    to reduce that non-useful bias.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, one way that models might make better agents is through the
    use of cache embeddings. It’s an interesting idea of something you can do with
    models that we haven’t really heard anyone talking about, other than Will Gaviro
    Rojas at a local Utah meetup. Caching embeddings allows you to cut down on repeating
    the same computations several times to complete several tasks in parallel. This
    is a more complex example, and we aren’t going to dive too deep into it so as
    to keep things pretty simple, but this strategy involves either copying the final
    layers of a model after the last hidden state to complete several tasks on their
    own or creating custom linear classifiers to fulfill those tasks. In listing 12.4,
    we dive into the entire system surrounding caching the embeddings, as we assume
    knowledge at this point of how to store embeddings for access later.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: We start by loading Llama3-ChatQA in INT4 quantization with BitsandBytes to
    make sure it fits on smaller consumer GPUs, which should be familiar at the end
    of this book. We give it the appropriate prompt structure for the given model,
    and we get our outputs. Then we access the last hidden state or the embeddings
    with `outputs.last_ hidden_states` and show how we could either create copies
    of the relevant layers to put that hidden state through (provided they’re trained
    to handle this) or create a custom linear classifier in PyTorch that can be fully
    trained on any classification task.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Caching embeddings for multiple smaller models
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Traditional generation'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Embedding'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Finds the LM Head layer'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Custom Trainable classifier'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: This decoupling of the idea of models as monoliths that connect to other systems
    is very engineering-friendly, allowing for one model to output hundreds of classifications
    around a single data point, thanks to embeddings. LangChain provides a `CacheBackedEmbeddings`
    class to help with caching the vectors quickly and conveniently if you’re working
    within that class, and we think that name is pretty great for the larger idea
    as well—backing up your embedding process with caching to be fed to multiple linear
    classifiers at once. This approach allows us to detect anything from inappropriate
    user input all the way to providing a summarized version of the embeddings back
    to the real model for quicker and more generalized processing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Final thoughts
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We really hope you enjoyed this book and that you learned something new and
    useful. It’s been a huge undertaking to write the highest quality book we could
    muster, and sometimes it was less about what we wrote and more about what we ended
    up throwing out. Believe it or not, while being as comprehensive as we could,
    there are many times we’ve felt we’d only scratched the surface of most topics.
    Thank you for going on this journey with us.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: We are so excited about where this industry is going. One of the hardest parts
    of writing this book was choosing to focus on the current best practices and ignoring
    much of the promising research that seems to be piling on, especially as companies
    and governments increase funding into the incredible possibilities that LLMs promise.
    We’re excited to see more research that’s been around for years or even decades
    be applied to LLMs and see new research come from improving those results. We’re
    also excited to watch companies change and figure out how to deploy and serve
    LLMs much better than they currently are. It’s difficult to market LLM-based products
    using traditional methods without coming off as just lying. People want to see
    the product work exactly as demonstrated in the ad, and we’re hoping to see changes
    there.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: What an exciting time! There’s still so much more to learn and explore. Because
    we have already seen the industry move while we’ve been writing, we’d like to
    invite you to submit PRs in the GitHub repo to help keep the code and listings
    up to date for any new readers. While this is the end of the book, we hope it’s
    just the beginning of your journey into using LLMs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are quickly challenging current laws and regulations and the interpretations
    thereof.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fear of LLMs being used for cheating has hurt many students with the introduction
    of AI detection systems that don’t work.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are only getting bigger, and we will need solutions like better compression
    and the next attention algorithm to compensate.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings are paving the way to multimodal solutions with interesting approaches
    like ImageBind and OneLLM.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is likely to be one of the largest bottlenecks and constraints to future
    improvements, largely starting with a lack of quality evaluation datasets.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For use cases where they are a problem, hallucinations will continue to be so,
    but methodologies to curb their effects and frequency of occurrence are becoming
    quite sophisticated.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs continue to suffer due to GPU shortages and will help drive research and
    innovation to develop more powerful computing systems.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM Agents don’t provide a pathway to AGI, but we will see them graduate from
    toys to tools.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) M. M. Grynbaum and R. Mac, “The Times Sues OpenAI
    and Microsoft Over A.I. Use of Copyrighted Work,” The New York Times, December
    27, 2023, [https://mng.bz/6Y0D](https://mng.bz/6Y0D).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) E. Maiberg, “Scientific journals are publishing papers
    with AI-generated text,” 404 Media, March 18, 2024, [https://mng.bz/n0og](https://mng.bz/n0og).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#footnote-source-3) Sanhedrin 76b:11, [https://mng.bz/vJaJ](https://mng.bz/vJaJ).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[[4]](#footnote-source-4) “Myanmar army behind Facebook pages spewing hate
    speech: UN probe,” RFI, March 27, 2024, [https://mng.bz/mR0P](https://mng.bz/mR0P).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[[5]](#footnote-source-5) A. Guzman, “Company disables AI after bot starts
    swearing at customer, calls itself the ‘worst delivery firm in the world,’” NY
    Post, January 20, 2024, [https://mng.bz/yoVq](https://mng.bz/yoVq).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[[6]](#footnote-source-6) emozilla, “Dynamically Scaled RoPE further increases
    performance of long context LLaMA with zero fine-tuning,” Jun. 30, 2023, [https://mng.bz/M1pn](https://mng.bz/M1pn).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[[7]](#footnote-source-7) B. Peng, J. Quesnelle, H. Fan, E. Shippole, N. Research,
    and Eleutherai, “YaRN: Efficient Context Window Extension of Large Language Models.”
    Available: [https://arxiv.org/pdf/2309.00071](https://arxiv.org/pdf/2309.00071)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[[8]](#footnote-source-8) M. Poli et al., “Hyena Hierarchy: Towards Larger
    Convolutional Language Models,” Feb. 2023, doi: [https://doi.org/10.48550/arxiv.2302.10866](https://doi.org/10.48550/arxiv.2302.10866).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[[9]](#footnote-source-9) A. Gu and T. Dao, “Mamba: Linear-Time Sequence Modeling
    with Selective State Spaces,” arXiv.org, Dec. 01, 2023, [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[[10]](#footnote-source-10) [1]O. Lieber et al., “Jamba: A Hybrid Transformer-Mamba
    Language Model,” arXiv.org, Mar. 28, 2024, [https://arxiv.org/abs/2403.19887](https://arxiv.org/abs/2403.19887).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[[11]](#footnote-source-11) Vaswani et al., Attention Is All You Need,” 2017,
    [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[[12]](#footnote-source-12) K. H. and A. Fitch, “Sam Altman seeks trillions
    of dollars to reshape business of chips and AI,” Wall Street Journal, February
    8, 2024, [https://mng.bz/KDrK](https://mng.bz/KDrK).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[[13]](#footnote-source-13) A. Pequeño IV, “Nvidia now world’s most valuable
    company—Topping Microsoft and Apple,” Forbes, June 18, 2024, [https://mng.bz/9ojl](https://mng.bz/9ojl).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[[14]](#footnote-source-14) S. Webster, “Intel Optane SSD DC P5800X review:
    The fastest SSD ever made,” Tom’s Hardware, August 26, 2022, [https://mng.bz/j0Wx](https://mng.bz/j0Wx).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
