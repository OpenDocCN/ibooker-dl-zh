<html><head></head><body><section data-pdf-bookmark="Chapter 7. Toward Trustworthy General-Purpose AI and Generative AI" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_7_toward_trustworthy_general_purpose_ai_and_generati_1748539924538638">
<h1><span class="label">Chapter 7. </span>Toward Trustworthy General-Purpose AI and Generative AI</h1>

<p>The EU AI Act, which came into effect in August 2024, is the world’s first comprehensive legal framework for AI. Up to this point, this book has focused primarily on how the Act’s risk-based approach applies to predictive AI systems. However, the Act also includes provisions<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-type="indexterm" id="id608"/> for <em>general-purpose AI</em> (GPAI)—a relatively late addition spurred by the rapid rise of large language models (LLMs) such as GPT and BERT, along with the broader emergence of generative AI technologies during the period when the legislation was being drafted. This addition was intended to address a key regulatory challenge: how to govern AI models that are not tied to a specific task and can be adapted to a wide range of downstream uses.</p>

<p>The EU AI Act aims to balance innovation and risk mitigation by promoting research and development while safeguarding fundamental rights. As such, it does not apply to GPAI models developed and used solely for prototyping or for research and development purposes prior to being placed on the market or put into service. In general, the rules for GPAI models and systems are less stringent than those for high-risk AI systems—unless they are deemed to present a systemic risk<a contenteditable="false" data-primary="systemic risk" data-type="indexterm" id="id609"/><a contenteditable="false" data-primary="systemic risk" data-secondary="definition of" data-type="indexterm" id="id610"/> (defined as “a risk that is specific to the high-impact capabilities of general-purpose AI models, having a significant impact on the Union market due to their reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain”).</p>

<p>This chapter lays out the requirements for EU AI Act compliance for GPAI and generative AI. It also introduces the concept of<a contenteditable="false" data-primary="GenAIOps" data-type="indexterm" id="id611"/> GenAIOps—the application of MLOps principles to the development and deployment of generative AI applications. As in the previous chapters, the focus will be on how AI engineering principles and <span class="keep-together">practices</span> can be applied to meet the transparency obligations of the EU AI Act in this rapidly evolving domain.</p>

<section data-pdf-bookmark="The EU AI Act and Generative AI" data-type="sect1"><div class="sect1" id="chapter_7_the_eu_ai_act_and_generative_ai_1748539924538897">
<h1>The EU AI Act and Generative AI</h1>

<p>The notion of “general-purpose” AI (also known as “foundation models”<a contenteditable="false" data-primary="foundation models" data-seealso="general-purpose AI (GPAI) models" data-type="indexterm" id="id612"/>) was introduced into the EU AI Act relatively late in the negotiations between the European Parliament and Council. The rapid emergence and widespread adoption of generative AI systems like ChatGPT, DALL·E, and Midjourney during the legislative process revealed that the Act’s initial focus on AI systems with specific, identifiable risks stemming from clearly defined use cases was too narrow. General-purpose AI was added as a distinct category primarily to address concerns around quality control and copyright protection related to training data, while also allowing for the assessment of systemic risks, as elaborated in Chapter V of the Act. The regulations governing these models are relatively limited in scope, focusing mainly on requirements for documentation and transparency (Article 53)<a contenteditable="false" data-primary="Article 53: Obligations for Providers of General-Purpose AI Models" data-type="indexterm" id="id613"/> and obligations for cooperation with relevant authorities (Article 91).</p>

<p>Generative AI<a contenteditable="false" data-primary="generative AI (GenAI)" data-type="indexterm" id="id614"/>, or GenAI<a contenteditable="false" data-primary="EU AI Act" data-secondary="generative AI (GenAI)" data-type="indexterm" id="id615"/>, offers numerous potential benefits, including enhanced decision making, increased productivity, and the ability to generate novel content. GenAI technologies can be used in a wide range of domains, such as education, research, and customer service, to automate tasks and improve operational efficiency.</p>

<p>However, these technologies also pose significant risks. These include the potential for bias and discrimination, the generation of inaccurate or misleading information, and various form of misuse<a contenteditable="false" data-primary="generative AI (GenAI)" data-secondary="misuse of" data-type="indexterm" id="genai-misuse-1"/> (e.g., creating deceptive content, or deepfakes). Misuse tends to fall into two broad categories:<sup><a data-type="noteref" href="ch07.html#id616" id="id616-marker">1</a></sup></p>

<dl>
	<dt>Exploitation of GenAI capabilities</dt>
	<dd>
	<p>Tactics that leverage the features of GenAI models<a contenteditable="false" data-primary="generative AI (GenAI)" data-secondary="misuse of" data-tertiary="exploitation of GenAI capabilities" data-type="indexterm" id="id617"/> to create harmful outputs or support malicious activities. For example, AI robocalls can imitate real people and take actions on their behalf, and synthetic content can be generated to create fake social media accounts to promote a specific agenda or to fabricate identification documents.</p>
	</dd>
	<dt>Compromise of GenAI systems</dt>
	<dd>
	<p>Tactics that involve attacking or manipulating the GenAI systems, targeting model or data integrity vulnerabilities. Examples include prompt injection (manipulating model prompts to enable unintended or unauthorized outputs); compromising the privacy of training data to extract personal information; bypassing restrictions on the model’s safeguards (known as jailbreaking); and model extraction (reverse engineering to obtain details on the model’s architecture, parameters, or training data).</p>
	</dd>
</dl>

<p>The EU AI Act aims to mitigate these risks by imposing certain obligations on the providers of generative and general-purpose AI systems to ensure that they are developed and deployed responsibly<a contenteditable="false" data-primary="generative AI (GenAI)" data-secondary="misuse of" data-startref="genai-misuse-1" data-type="indexterm" id="id618"/>.</p>

<section data-pdf-bookmark="GPAI Systems and Transparency Obligations" data-type="sect2"><div class="sect2" id="chapter_7_gpai_systems_and_transparency_obligations_1748539924538975">
<h2>GPAI Systems and Transparency Obligations</h2>

<p>GPAI systems that interact directly with humans, including those that generate synthetic content, are subject to the following transparency obligations under Article 50 of the EU AI Act:</p>

<dl>
	<dt>Informing users of AI interaction</dt>
	<dd>
	<p>Providers must inform users when they are interacting with an AI system, unless it’s reasonably obvious or the system is used for law enforcement purposes (e.g., detecting, preventing, investigating, or prosecuting crimes). This applies to chatbots and content-generating tools.</p>
	</dd>
	<dt>Marking synthetic content</dt>
	<dd>
	<p>Providers of AI systems that generate synthetic content (audio, images, video, text) must clearly mark these outputs as artificially generated or manipulated. The labels should be machine-readable and easily detectable, signaling the content’s non-authentic nature. This requirement does not apply to assistive editing tools or systems that do not significantly alter the original input.</p>
	</dd>
	<dt>Disclosing deepfakes</dt>
	<dd>
	<p>Providers of AI systems that generate deepfakes (described by Article 3(60) as “AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful”) must clearly label these outputs as artificial. The labels should be machine-readable and easily detectable.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Regulating General-Purpose AI" data-type="sect2"><div class="sect2" id="chapter_7_regulating_general_purpose_ai_1748539924539041">
<h2>Regulating General-Purpose AI</h2>

<p>The EU AI Act introduces specific regulations for GPAI models and systems, focusing on transparency, documentation, and risk management. In this section, I’ll first define what constitutes a GPAI model or system under the Act and explain the criteria for determining whether a GPAI model presents systemic risk. I will then outline the obligations for providers of GPAI models and deployers of GPAI systems, including the increased obligations for providers of GPAI models deemed to pose systemic risks.</p>

<section data-pdf-bookmark="Definition and scope" data-type="sect3"><div class="sect3" id="chapter_7_definition_and_scope_1748539924539104">
<h3>Definition and scope</h3>

<p>The EU AI Act defines a <em>GPAI model</em> as<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="definition of" data-type="indexterm" id="id619"/> an AI model with significant generality that can perform a wide range of distinct tasks. These models are typically trained on large amounts of data, often using self-supervised learning techniques, and are designed to be integrated into various downstream systems or applications.</p>

<p>The Act defines a <em>GPAI system</em> as<a contenteditable="false" data-primary="general-purpose AI (GPAI) systems" data-seealso="general-purpose AI (GPAI) models" data-type="indexterm" id="id620"/> an AI system based on a GPAI model that can serve a variety of purposes, either as a standalone application or as a component integrated into other AI systems. As a reminder, the Act defines an AI system as “a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.”</p>

<p>It’s important to distinguish between the model and the system it powers. The Act regulates both providers of GPAI models and deployers who use or integrate these models into AI systems. The obligations for deployers of GPAI systems<a contenteditable="false" data-primary="general-purpose AI (GPAI) systems" data-secondary="obligations for deployers" data-type="indexterm" id="id621"/> are generally the same as for other AI systems, with specific oversight powers granted to the AI Office in the European Commission.</p>
</div></section>

<section data-pdf-bookmark="Systemic risk criteria" data-type="sect3"><div class="sect3" id="chapter_7_systemic_risk_criteria_1748539924539161">
<h3>Systemic risk criteria</h3>

<p>The EU AI Act establishes<a contenteditable="false" data-primary="systemic risk" data-secondary="criteria" data-type="indexterm" id="sys-risk-crit-1"/> a two-tiered regulatory approach for GPAI models:</p>

<dl>
	<dt>General obligations</dt>
	<dd>
	<p>All providers of GPAI models must meet baseline transparency and compatibility requirements.</p>
	</dd>
	<dt>Systemic risk</dt>
	<dd>
	<p>If a GPAI model is classified as having systemic risk, it triggers additional regulatory oversight and extra obligations. The criteria for this classification are outlined in <a data-type="xref" href="#chapter_7_table_1_1748539924522291">Table 7-1</a>.</p>
	</dd>
</dl>

<table class="striped" id="chapter_7_table_1_1748539924522291">
	<caption><span class="label">Table 7-1. </span>Criteria for systemic-risk GPAI models</caption>
	<thead>
		<tr>
			<th>Criterion</th>
			<th>Description of systemic risk</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>High-impact capabilities</p>
			</td>
			<td>
			<p>The model has significant potential to cause harm due to its broad range of functionalities. Risks include major accidents, disruption of critical infrastructure, threats to public health or safety, impacts on democratic processes, national or economic security risks, generation of illegal or discriminatory content, lowering barriers to development of chemical, biological, radiological, and nuclear (CBRN) weapons technology, or the ability of models to make copies of themselves or train other models autonomously.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>FLOPs threshold</p>
			</td>
			<td>
			<p>The model requires more than 10<sup>25</sup> floating-point operations per second (FLOPs) during training. This indicates a potential for high-impact capabilities.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Commission decision based on equivalent capabilities or impact</p>
			</td>
			<td>
			<p>Even if the FLOPs threshold is not met, the European Commission can classify a model as posing systemic risk if it demonstrates a potential for high-impact capabilities based on the assessment criteria in Annex XIII.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Additional factors considered by the Commission</p>
			</td>
			<td>
			<p>Additional factors that are taken into account include the number of parameters, quality and size of the training dataset (e.g., number of tokens), computational resources used for training (FLOPs, cost, time, energy consumption), input and output modalities, and degree of autonomy or scalability.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Contesting the presumption of systemic risk</p>
			</td>
			<td>
			<p>A provider can contest the presumption of systemic risk by presenting sufficiently substantiated arguments demonstrating that, even if the FLOPs criterion is met, the model does not actually present systemic risks due to its specific characteristics. General arguments are not sufficient.</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Obligations for providers of GPAI models" data-type="sect3"><div class="sect3" id="chapter_7_obligations_for_providers_of_gpai_models_1748539924539220">
<h3>Obligations for providers of GPAI models</h3>

<p>All providers of GPAI models<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers" data-type="indexterm" id="gpai-obl-prov-1"/>, regardless of their systemic risk classification, are subject to a set of general obligations under the EU AI Act. These obligations are intended to promote transparency, accountability, and responsible development of powerful AI models. There are simplified requirements for providers offering GPAI models under free and open licenses, provided that these models do not present systemic risk<a contenteditable="false" data-primary="systemic risk" data-secondary="criteria" data-startref="sys-risk-crit-1" data-type="indexterm" id="id622"/>. <a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers" data-tertiary="technical documentation" data-type="indexterm" id="id623"/>The core obligations (as outlined in <a href="https://oreil.ly/Xn8iw">Article 53</a>)<a contenteditable="false" data-primary="Article 53: Obligations for Providers of General-Purpose AI Models" data-type="indexterm" id="art-53-obl-2"/> include:</p>

<dl>
	<dt>Technical documentation</dt>
	<dd>
	<p>Providers must create and maintain comprehensive technical documentation including details about the model’s design specifications, training and testing processes, and evaluation results. This documentation must enable regulatory authorities to assess the model’s compliance with the EU AI Act and must be provided to the AI Office and national competent authorities upon request.</p>
	</dd>
	<dt>Documentation for downstream providers</dt>
	<dd>
	<p>Given that GPAI models<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers" data-tertiary="documentation for downstream providers" data-type="indexterm" id="id624"/> are often integrated into other AI systems, the EU AI Act requires providers to furnish downstream providers with documentation that enables them to understand the model’s capabilities, limitations, and known risks. For example, if the model is known to exhibit bias in certain contexts, this must be disclosed so that downstream providers can take appropriate mitigation steps in their AI systems.</p>
	</dd>
	<dt>Copyright compliance</dt>
	<dd>
	<p>GPAI models are trained on vast amounts of data, often including copyrighted material. Consequently, providers must<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers" data-tertiary="copyright compliance" data-type="indexterm" id="id625"/> implement a policy to comply with EU copyright law (<a contenteditable="false" data-primary="EU Copyright Directive" data-type="indexterm" id="id626"/>in particular, the <a href="https://oreil.ly/6l3Hq">EU Copyright Directive</a>), including measures to identify and respect any copyright restrictions on the data used to train the model and documentation of how such restrictions have been addressed.</p>
	</dd>
	<dt class="pagebreak-before">Training data summary</dt>
	<dd>
	<p>To promote transparency and accountability<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers" data-tertiary="training data summary" data-type="indexterm" id="id627"/>, providers must publish a detailed summary of the data used to train their GPAI models. This should provide information about the types and sources of data used, as well as any known limitations or biases in the data.</p>
	</dd>
	<dt>EU representative</dt>
	<dd>
	<p>Providers based outside the European Union<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers" data-tertiary="EU representative" data-type="indexterm" id="id628"/> must appoint an authorized representative within the EU. This representative acts as a point of contact for authorities and is responsible for ensuring that the provider complies with its obligations under the EU AI Act.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Additional obligations for providers of GPAI models with systemic risk " data-type="sect3"><div class="sect3" id="chapter_7_additional_obligations_for_providers_of_gpai_model_1748539924539277">
<h3>Additional obligations for providers of GPAI models with systemic risk </h3>

<p>GPAI models<a contenteditable="false" data-primary="Article 53: Obligations for Providers of General-Purpose AI Models" data-startref="art-53-obl-2" data-type="indexterm" id="id629"/> with systemic risk<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers (with systemic risk)" data-type="indexterm" id="id630"/> are subject to more stringent requirements due to their potential for causing significant harm. In addition to the general obligations outlined in the previous section, providers of these models must comply with the following obligations, specified<a contenteditable="false" data-primary="Article 55: Obligations for Providers of General-Purpose AI Models with Systemic Risk" data-type="indexterm" id="id631"/> in <a href="https://oreil.ly/2fYcc">Article 55</a>:</p>

<dl>
	<dt>Model evaluation</dt>
	<dd>
	<p>Conduct regular rigorous evaluations of the models using standardized protocols and tools. This includes adversarial testing to identify vulnerabilities and potential risks. The goal is to ensure that the model is robust, reliable, and does not pose unacceptable risks.</p>
	</dd>
	<dt>Risk mitigation</dt>
	<dd>
	<p>Proactively assess and mitigate potential systemic risks associated with use of the models. Identify potential harms related to fundamental rights, health and safety, or security, and implement documented measures to minimize those risks.</p>
	</dd>
	<dt>Incident reporting</dt>
	<dd>
	<p>Establish a system for tracking, documenting, and reporting serious incidents related to use of the models. Incidents must be reported to the AI Office and relevant national authorities, and appropriate corrective actions must be taken and documented.</p>
	</dd>
	<dt>Cybersecurity</dt>
	<dd>
	<p>Implement robust safeguards to protect the models and the infrastructure on which they operate. This is crucial to prevent unauthorized access, data breaches, and malicious use<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for providers" data-startref="gpai-obl-prov-1" data-type="indexterm" id="id632"/>.</p>
	</dd>
</dl>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Obligations for deployers of GPAI models" data-type="sect3"><div class="sect3" id="chapter_7_obligations_for_deployers_of_gpai_models_1748539924539333">
<h3 class="less_space">Obligations for deployers of GPAI models</h3>

<p>The EU AI Act applies to all deployers<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for deployers" data-type="indexterm" id="gpai-obl-depl-1"/> who operate an AI system under their own authority in a professional capacity. When deployers integrate GPAI models into their AI systems, they may incur additional responsibilities under the Act, particularly if the resulting systems are classified as high risk. These obligations include:</p>

<dl>
	<dt>Using the system as intended</dt>
	<dd>
	<p>The system must be used in accordance<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for deployers" data-tertiary="using the system as intended" data-type="indexterm" id="id633"/> with the provider’s instructions and for its intended purpose.</p>
	</dd>
	<dt>Human oversight</dt>
	<dd>
	<p>Appropriate human oversight mechanisms<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for deployers" data-tertiary="human oversight" data-type="indexterm" id="id634"/> must be established—for example, human review of the system’s outputs, the ability to intervene in the system’s operation, and clear assignment of responsibility for the system’s decisions.</p>
	</dd>
	<dt>Data quality</dt>
	<dd>
	<p>The input data provided to the system<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for deployers" data-tertiary="data quality" data-type="indexterm" id="id635"/> must be relevant, accurate, and representative. This is crucial to prevent biased or inaccurate outputs and to ensure that the system operates as intended.</p>
	</dd>
	<dt>Monitoring</dt>
	<dd>
	<p>System performance must be monitored<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for deployers" data-tertiary="monitoring" data-type="indexterm" id="id636"/> and any issues reported to the provider. This includes monitoring for unexpected outputs, performance degradation, or other indications that the system is not functioning correctly.</p>
	</dd>
	<dt>Recordkeeping</dt>
	<dd>
	<p>System logs must be maintained for the purposes<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for deployers" data-tertiary="recordkeeping" data-type="indexterm" id="id637"/> of incident investigation and demonstration of regulatory compliance.</p>
	</dd>
	<dt>Transparency</dt>
	<dd>
	<p>AI systems that generate or manipulate content<a contenteditable="false" data-primary="general-purpose AI (GPAI) models" data-secondary="obligations for deployers" data-tertiary="transparency" data-type="indexterm" id="id638"/> that is published to inform the public on matters of public interest must clearly disclose that the content is <span class="keep-together">synthetic</span>.</p>
	</dd>
</dl>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Predictive ML Versus GPAI" data-type="sect1"><div class="sect1" id="chapter_7_predictive_ml_versus_gpai_1748539924539393">
<h1>Predictive ML Versus GPAI</h1>

<p>Before we turn<a contenteditable="false" data-primary="predictive ML versus general-purpose AI" data-type="indexterm" id="pred-ml-v-gpai-1"/> our attention to operationalizing EU AI Act compliance for GPAI, let’s briefly reflect on the differences between predictive and generative AI to better understand where generative AI diverges and why the discipline of GenAIOps has emerged.</p>

<p>Machine learning is a broad field, but at its core, most models can be categorized into two distinct paradigms: predictive ML and GPAI (see Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#chapter_7_figure_1_1748539924514309">7-1</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#chapter_7_figure_2_1748539924514347">7-2</a> for concrete examples of each). While both rely on statistical learning, they differ in their goals, methodologies, and applications.</p>

<figure><div class="figure" id="chapter_7_figure_1_1748539924514309"><img src="assets/taie_0701.png"/>
<h6><span class="label">Figure 7-1. </span>A visualization of predictive ML—a discriminative model trained to predict whether a given image is painted by Vincent van Gogh. Image from the book <a class="orm:hideurl" href="https://oreil.ly/L5njI"><span class="plain">Generative Deep Learning</span>, 2nd edition</a>, by David Foster (O’Reilly). Used with permission.</h6>
</div></figure>

<figure><div class="figure" id="chapter_7_figure_2_1748539924514347"><img src="assets/taie_0702.png"/>
<h6><span class="label">Figure 7-2. </span>A visualization of generative AI—a generative model trained to generate realistic photos of horses. Image from the book <a class="orm:hideurl" href="https://oreil.ly/L5njI"><span class="plain">Generative Deep Learning</span>, 2nd edition</a>, by David Foster (O’Reilly). Used with permission.</h6>
</div></figure>

<p>At its core, predictive ML is about answering the question “What’s likely?” It focuses on estimation, forecasting, and pattern recognition. Predictive models analyze historical data to make informed forecasts or decisions. They learn patterns from structured datasets and aim to provide numerical outputs, classifications, or recommendations on input features.</p>

<p>GPAI, on the other hand, is an umbrella category that includes many generative <span class="keep-together">models—particularly</span> foundation models such as LLMs. Generative AI can therefore be thought of as a subset of GPAI. Most state-of-the-art generative models (such as GPT-4, DALL·E, and Stable Diffusion) are classified as GPAI under the EU AI Act. This includes models that are general-purpose (used across many tasks or systems) and models that are made available to others for downstream use or fine-tuning.</p>

<p>At its core, generative AI is about answering the question “What’s possible?” It moves beyond predictions to creating new content. Instead of mapping inputs to predefined outputs, generative models learn the statistical distribution of their training data and generate new, plausible data points that fall within that learned distribution.<a contenteditable="false" data-primary="foundation models" data-type="indexterm" id="found-mod-1"/></p>

<p><a data-type="xref" href="#chapter_7_table_2_1748539924522329">Table 7-2</a> provides a structured comparison of predictive ML and GPAI.</p>

<table class="striped" id="chapter_7_table_2_1748539924522329">
	<caption><span class="label">Table 7-2. </span>Predictive ML and GPAI—a side-by-side comparison</caption>
	<thead>
		<tr>
			<th>Feature</th>
			<th>Predictive ML</th>
			<th>GPAI</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Primary goal</p>
			</td>
			<td>
			<p>Solve a well-defined, narrow task (e.g., fraud detection, loan scoring)</p>
			</td>
			<td>
			<p>Provide broad capabilities that can be adapted to many tasks and use cases</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data input</p>
			</td>
			<td>
			<p>Domain-specific structured or unstructured data (e.g., tabular data, images)</p>
			</td>
			<td>
			<p>Massive and diverse cross-domain datasets (e.g., internet text, code, audio, images)</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Output</p>
			</td>
			<td>
			<p>Prediction, classification, regression, or recommendation for a specific task</p>
			</td>
			<td>
			<p>Versatile outputs: text, code, reasoning, vision tasks, language understanding, etc.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Downstream use</p>
			</td>
			<td>
			<p>Purpose-built systems with known users and applications</p>
			</td>
			<td>
			<p>Can be adapted and fine-tuned by third parties for diverse and evolving applications</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Examples</p>
			</td>
			<td>
			<p>Predictive maintenance models, fraud detection systems</p>
			</td>
			<td>
			<p>GPT-4, Gemini, LLaMA, Mistral, DALL·E</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Algorithms</p>
			</td>
			<td>
			<p>Decision trees, XGBoost, CNNs, RNNs</p>
			</td>
			<td>
			<p>Large-scale Transformer-based architectures, foundation models (e.g., LLMs, vision language models)</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Evaluation</p>
			</td>
			<td>
			<p>Task-specific metrics: accuracy, F1 score, AUC-ROC, MAE/RMSE</p>
			</td>
			<td>
			<p>Evaluated across a wide range of tasks using general benchmarks (e.g., MMLU, HELM), robustness, bias, toxicity</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="GenAIOps—Operationalizing EU AI Act Compliance for GPAI" data-type="sect1"><div class="sect1" id="chapter_7_genaiops_operationalizing_eu_ai_act_compliance_for_1748539924539463">
<h1>GenAIOps—Operationalizing EU AI Act Compliance <span class="keep-together">for GPAI</span></h1>

<p>GenAIOps<a contenteditable="false" data-primary="GenAIOps" data-type="indexterm" id="genai-ops-1"/> is the systematic<a contenteditable="false" data-primary="predictive ML versus general-purpose AI" data-startref="pred-ml-v-gpai-1" data-type="indexterm" id="id639"/> extension of DevOps and MLOps principles to meet the unique challenges of developing, deploying, and maintaining GPAI models and systems, including LLMs, image and video generators, and other foundation models. While traditional MLOps focuses on managing predictive models, GenAIOps must support the adaptation and governance of powerful generative models that may be used for a wide range of downstream tasks. This includes practices such as prompt engineering, retrieval-augmented generation (RAG)<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-type="indexterm" id="id640"/>, foundation model fine-tuning, and real-time monitoring for issues like hallucinations, bias, and safety risks. These needs introduce additional complexity in areas ranging from data governance to infrastructure, safety monitoring, and responsible use—especially in the context of compliance with regulatory frameworks like the EU AI Act.</p>

<p><a data-type="xref" href="#chapter_7_figure_3_1748539924514373">Figure 7-3</a> visualizes the operational stages for GPAI models and how they map to the CRISP-ML(Q) development phases.</p>

<figure><div class="figure" id="chapter_7_figure_3_1748539924514373"><img src="assets/taie_0703.png"/>
<h6><span class="label">Figure 7-3. </span>CRISP-ML(Q) framework for GPAI models and systems</h6>
</div></figure>

<section data-pdf-bookmark="Key Components" data-type="sect2"><div class="sect2" id="chapter_7_key_components_1748539924539524">
<h2>Key Components</h2>

<p>GenAIOps<a contenteditable="false" data-primary="GenAIOps" data-secondary="key components of" data-type="indexterm" id="genai-key-1"/> integrates automated pipelines, model versioning, and observability with inference optimization, safety guardrails, and compliance workflows to create a comprehensive framework for responsible GPAI deployment. This approach ensures that GPAI models operate with transparency and auditability in production environments while addressing concerns about misinformation, synthetic content attribution, and compliance with applicable regulatory requirements.</p>

<p>Key components of GenAIOps include the following (summarized in <a data-type="xref" href="#chapter_7_figure_4_1748539924514393">Figure 7-4</a>):</p>

<dl>
	<dt>Prompt engineering infrastructure</dt>
	<dd>
	<p>Tools<a contenteditable="false" data-primary="GenAIOps" data-secondary="prompt engineering infrastructure" data-type="indexterm" id="id641"/> for managing prompt creation, testing, versioning, and orchestration, in particular for complex reasoning chains and RAG systems.</p>
	</dd>
	<dt>Inference optimization</dt>
	<dd>
	<p>Techniques<a contenteditable="false" data-primary="GenAIOps" data-secondary="inference optimization" data-type="indexterm" id="id642"/> such as quantization, caching, and model distillation to reduce latency, cost, and resource usage in deployment environments.</p>
	</dd>
	<dt>Continuous evaluation</dt>
	<dd>
	<p>Real-time monitoring<a contenteditable="false" data-primary="GenAIOps" data-secondary="continuous evaluation" data-type="indexterm" id="id643"/> systems to detect hallucinations and performance drift, and mechanisms to collect human feedback for ongoing model refinement.</p>
	</dd>
	<dt>Safety and content controls</dt>
	<dd>
	<p>Automated content<a contenteditable="false" data-primary="GenAIOps" data-secondary="safety and content controls" data-type="indexterm" id="id644"/> filtering, watermarking, red-teaming protocols, and risk assessment workflows to ensure responsible and secure AI outputs.</p>
	</dd>
	<dt>Governance and compliance</dt>
	<dd>
	<p>Transparent documentation<a contenteditable="false" data-primary="GenAIOps" data-secondary="governance and compliance" data-type="indexterm" id="id645"/> (e.g., model cards), content provenance systems, and comprehensive audit trails to meet evolving regulatory requirements and ethical standards.</p>
	</dd>
	<dt>Foundation model adaptation</dt>
	<dd>
	<p>Fine-tuning workflows, reinforcement<a contenteditable="false" data-primary="GenAIOps" data-secondary="foundation model adaptation" data-type="indexterm" id="id646"/> learning from human feedback (RLHF), and parameter-efficient methods (e.g., LoRA) to customize models for specific downstream use cases<a contenteditable="false" data-primary="GenAIOps" data-secondary="key components of" data-startref="genai-key-1" data-type="indexterm" id="id647"/>.</p>
	</dd>
</dl>

<figure><div class="figure" id="chapter_7_figure_4_1748539924514393"><img src="assets/taie_0704.png"/>
<h6><span class="label">Figure 7-4. </span>Key components of GenAIOps</h6>
</div></figure>
</div></section>

<section data-pdf-bookmark="How GenAIOps Extends MLOps" data-type="sect2"><div class="sect2" id="chapter_7_how_genaiops_extends_mlops_1748539924539580">
<h2>How GenAIOps Extends MLOps</h2>

<p>The core goals of MLOps<a contenteditable="false" data-primary="GenAIOps" data-secondary="extending MLOps" data-type="indexterm" id="genai-mlops-1"/>, which is mainly focused on the lifecycle of predictive ML models, are reproducibility, scalability, and continuous delivery. GenAIOps retains these goals but adapts them to the specific needs of GPAI. For example, generative AI applications must operate on unstructured data at scale, incorporate prompt engineering and human-in-the-loop feedback mechanisms, and ensure that outputs are not only high quality but also safe, transparent, and appropriately labeled. <a data-type="xref" href="#chapter_7_table_3_1748539924522354">Table 7-3</a> outlines key differences between MLOps and GenAIOps across several areas, such as data management, model training and adaptation, model deployment and inference, monitoring, and ethics and regulations.</p>

<table class="striped" id="chapter_7_table_3_1748539924522354">
	<caption><span class="label">Table 7-3. </span>Key differences between MLOps and GenAIOps</caption>
	<thead>
		<tr>
			<th>Aspect</th>
			<th>MLOps (predictive ML)</th>
			<th>GenAIOps (GPAI)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Data management</p>
			</td>
			<td>
			<p>Focuses on curated, labeled datasets, feature engineering, and versioning of training data pipelines. Data is often structured and tabular or limited to task-specific corpora.</p>
			</td>
			<td>
			<p>Handles massive, unstructured datasets (text, images, audio). Embraces synthetic data generation for augmentation and requires robust data filtering to remove toxic or biased content from training corpora (to avoid amplifying harms). Uses embedding management (vector representations of data) in place of traditional feature stores to enable RAG, typically via vector databases.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Model training and adaptation</p>
			</td>
			<td>
			<p>Models are trained from scratch or with transfer learning, with hyperparameter tuning and model selection based on task-specific data. Models are typically smaller and trained to converge on labeled data.</p>
			</td>
			<td>
			<p>Starts from large, pretrained foundation models. Uses parameter-efficient fine-tuning (e.g., LoRA, adapter layers), prompt engineering, and few-shot learning instead of full retraining. Techniques like direct preference optimization (DPO) and RLHF are applied to align model behavior with human preferences. Experiment tracking must include prompt versions and chain configurations, not just model parameters.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Model deployment and inference</p>
			</td>
			<td>
			<p>Models are deployed as microservices or batch jobs behind REST APIs, using standard scaling, CI/CD, and version control. Inference usually involves a single-model prediction call (e.g., a classification API).</p>
			</td>
			<td>
			<p>Deployment requires specialized infrastructure (GPUs or TPUs, high memory usage). Focuses on real-time inference for generative outputs, often with streaming responses due to longer outputs (e.g., for chatbots). Frequently involves orchestrating multiple models and tools and managing prompt pipelines and session context. Optimization techniques such as model quantization, caching of outputs, and load balancing are applied to manage heavy inference loads.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Monitoring and continuous improvement</p>
			</td>
			<td>
			<p>Model performance (accuracy, latency) and data drift are monitored in production. User feedback may trigger periodic retraining or model updates. Rollback mechanisms minimize downtime in case of failures. When using third-party models, custom evaluation sets may be needed to detect behavioral changes between versions.</p>
			</td>
			<td>
			<p>Output quality and safety risks (hallucinations, toxicity, bias) are monitored, in addition to standard performance metrics. Requires continuous evaluation loops and human feedback to guide improvements.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Regulatory and ethical considerations</p>
			</td>
			<td>
			<p>Focuses on fairness, bias mitigation, explainability, and compliance in decision-making contexts (e.g., loan approvals). Models are accompanied by comprehensive documentation (e.g., model cards, bias audits).</p>
			</td>
			<td>
			<p>Faces new ethical and regulatory challenges: GPAI models can produce misinformation, harmful content, and deepfakes and can potentially be used for impersonation. Must support emerging requirements, such as labeling AI-generated content and ensuring transparency and traceability.</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="GenAIOps Tools, Workflows, and Frameworks to Support Transparency" data-type="sect2"><div class="sect2" id="chapter_7_genaiops_tools_workflows_and_frameworks_to_suppo_1748539924539638">
<h2>GenAIOps Tools, Workflows, and Frameworks to <span class="keep-together">Support Transparency</span></h2>

<p>GenAIOps builds on MLOps by adding capabilities like prompt management, chain-of-thought orchestration, synthetic data generation, and stringent output controls to address the unique demands of GPAI. It’s also essential for ensuring transparency in GPAI models and systems, as mandated by regulations like the EU AI Act. The transparency obligations outlined in Article 50 of the Act, which we discussed in detail in <a data-type="xref" href="ch06.html#chapter_6_ai_engineering_for_limited_risk_ai_systems_1748539923606988">Chapter 6</a>, apply to both providers and deployers of generative AI systems (as well as predictive ML systems). Meeting these obligations and effectively managing GenAI-related risks requires cross-disciplinary collaboration among data engineers, model developers, user experience designers, and ethics and compliance teams. The following GenAIOps tools and workflows can support this effort:</p>

<dl>
	<dt>Prompt and chain management</dt>
	<dd>
	<p>Frameworks like LangChain and NVIDIA NeMo<a contenteditable="false" data-primary="LangChain" data-type="indexterm" id="id648"/> can<a contenteditable="false" data-primary="NVIDIA NeMo" data-type="indexterm" id="id649"/> be used to orchestrate complex prompt workflows and ensure that required system prompts (including AI self-identification) are consistently included in user interactions. These tools also support prompt versioning, testing, and traceability.</p>
	</dd>
	<dt>Content watermarking and metadata</dt>
	<dd>
	<p>Libraries and APIs compliant with the C2PA (Content Provenance and Authenticity) standard<a contenteditable="false" data-primary="C2PA (Content Provenance and Authenticity)" data-type="indexterm" id="id650"/> are increasingly being integrated into image and video generation pipelines. For example, OpenAI’s DALL·E<a contenteditable="false" data-primary="DALL·E" data-type="indexterm" id="id651"/> API <a href="https://oreil.ly/djcGb">automatically adds C2PA metadata</a> to its outputs. There are also third-party services, such as Stability’s Stable Diffusion plug-ins or <a href="https://oreil.ly/PGrpq">Steg.AI</a>, <a contenteditable="false" data-primary="Steg.AI" data-type="indexterm" id="id652"/>that insert invisible watermarks into images for later verification. GenAIOps pipelines can call these services post-generation. For text-based content, research into statistical watermarking is also advancing.</p>
	</dd>
	<dt>Monitoring and detection services</dt>
	<dd>
	<p>To complement watermarks, GenAIOps can integrate<em> </em>deepfake detection models and content moderation AI into the monitoring stack. For instance, an enterprise might deploy a vision model that scans newly uploaded videos for signs of manipulation. If AI-generated media lacks the required disclosure, the detection system can flag it for review. Cloud providers are beginning to offer APIs for this purpose (e.g., Azure’s Video Authenticator<a contenteditable="false" data-primary="Azure Video Authenticator" data-type="indexterm" id="id653"/> and AWS’s Rekognition<a contenteditable="false" data-primary="AWS Rekognition" data-type="indexterm" id="id654"/>).</p>
	</dd>
	<dt>Guardrails and policy enforcement</dt>
	<dd>
	<p>Guardrail frameworks such as NVIDIA NeMo Guardrails or the open source Guardrails AI library can be used to enforce policy at runtime by intercepting model outputs before they reach the user. They can automatically append disclaimers or block content that should be labeled as synthetic but isn’t. For example, if a user prompts a GPAI model to generate a fake image of a person, a guardrail can attach a “Fake Image” label to the output. Similarly, an audible cue can be injected into audio content. These enforcement steps are configured directly within the inference pipeline.</p>
	</dd>
	<dt class="pagebreak-before">Logging and versioning</dt>
	<dd>
	<p>Prompts, responses, and content artifacts can be comprehensively logged using platforms like MLflow, Weights &amp; Biases, or Arize AI that support GenAIOps features. These logs provide a robust audit trail that can verify, for example, whether a watermarking function ran and what output it produced.</p>
	</dd>
</dl>

<p>In the remainder of this chapter, we’ll explore how the SMACTR framework introduced in the previous chapter can be integrated into the different phases of the CRISP-ML(Q) lifecycle, providing a practical foundation for operationalizing compliance in the development and deployment of GPAI<a contenteditable="false" data-primary="GenAIOps" data-secondary="extending MLOps" data-startref="genai-mlops-1" data-type="indexterm" id="id655"/> models and systems<a contenteditable="false" data-primary="GenAIOps" data-startref="genai-ops-1" data-type="indexterm" id="id656"/>.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Aligning AI Engineering with SMACTR and CRISP-ML(Q) for Transparency" data-type="sect1"><div class="sect1" id="chapter_7_aligning_ai_engineering_with_smactr_and_crisp_ml_q_1748539924539723">
<h1>Aligning AI Engineering with SMACTR and CRISP-ML(Q) <span class="keep-together">for Transparency</span></h1>

<p><a data-type="xref" href="ch06.html#chapter_6_ai_engineering_for_limited_risk_ai_systems_1748539923606988">Chapter 6</a> provided a detailed guide for AI engineers on achieving proactive compliance with the transparency requirements outlined in Article 50<a contenteditable="false" data-primary="Article 50" data-type="indexterm" id="id657"/> of the EU AI Act. This section applies the same approach to GPAI models and systems, incorporating GenAIOps principles. By integrating the SMACTR framework with the CRISP-ML(Q) methodology<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-type="indexterm" id="genaiops-integr-crisp-smactr-1"/>, teams can establish a robust, auditable process for responsible development and deployment of general-purpose AI.</p>

<section data-pdf-bookmark="Business and Data Understanding Phase" data-type="sect2"><div class="sect2" id="chapter_7_business_and_data_understanding_phase_1748539924539806">
<h2>Business and Data Understanding Phase</h2>

<p>GenAIOps is generally broken down into<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-tertiary="Business and Data Understanding Phase" data-type="indexterm" id="genai-ops-integr-bdup-1"/> three main operational phases (see <a data-type="xref" href="#chapter_7_figure_3_1748539924514373">Figure 7-3</a>). The initial phase encompasses tasks related to use case definition and data requirements planning that precede full-scale data engineering. It also includes early data engineering activities such as clarifying the application’s purpose and identifying data sources. For example, before building an LLM-powered system, teams must define the business problem (e.g., “customer support chatbot” versus “code generation assistant”) and assess what data and model capabilities are needed to solve it efficiently. This aligns closely with the business and data understanding phase of the CRISP-ML(Q) methodology.</p>

<p>The first step is translating business goals into ML/LLM objectives—that is, determining what you want the model to do, and what data or base models that will require. For instance, if the goal is to develop a Q&amp;A chatbot, GenAIOps teams must decide whether to use an existing Q&amp;A model or fine-tune a base LLM, and identify the relevant domain data. This reflects CRISP-ML(Q)’s emphasis that business objectives and data constraints should be considered jointly to avoid building “the right answers to the wrong questions.”</p>

<p>Early alignment on objectives, success criteria, and constraints shapes all subsequent work. CRISP-ML(Q) calls for success metrics to be defined at the business, ML, and system levels. GenAIOps expands this by requiring that user experience goals and ethical considerations also be addressed from the start—including expectations for model behavior, tone, and safety.</p>

<section data-pdf-bookmark="Implementation" data-type="sect3"><div class="sect3" id="chapter_7_implementation_1748539924539860">
<h3>Implementation</h3>

<p>Integrating CRISP-ML(Q) practices into GenAIOps at this stage means rigorously documenting requirements, constraints, and success criteria. In practice, the engineering team should convene domain experts, data scientists, and LLM engineers to define the scope of the GenAI application before development begins. The success criteria will go beyond classic metrics like accuracy or ROI to include measures of output quality (e.g., fluency, relevance) and safety (e.g., absence of harmful or biased content). An early feasibility assessment is crucial; if a required dataset is unavailable or the task is ill-suited to LLMs, that should be determined up front.</p>

<p>In GenAIOps, this phase often involves selecting an initial base model appropriate to the task. For example, the choice between a proprietary LLM like GPT-4 or a smaller open source model will depend on business needs and constraints such as cost, latency, and data privacy. Similar to algorithm selection in predictive ML projects, base model selection in GenAI is a key decision that should be guided by the problem scope and data requirements. For example, if the application involves sensitive or domain-specific content, a smaller fine-tuned model trained on proprietary data might be preferred over a general-purpose LLM. Documenting the decision and its rationale will help maintain clarity and transparency as the project progresses.</p>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_7_smactr_integration_1748539924539914">
<h3>SMACTR integration</h3>

<p>The Scoping and Mapping stages from the SMACTR framework are most relevant in this initial phase.</p>

<p>In the Scoping stage, the team conducts a preliminary risk assessment and an ethical review of the intended use cases. For a GenAI project, this involves asking how the application might affect users or stakeholders (e.g., “Could the chatbot give harmful advice or leak private data?”). Defining the ethical AI principles the project will follow is also part of this stage. Key outputs, such as a social impact assessment, help ensure the application’s design considers and takes steps to mitigate potential harms and aligns with the organization’s values and AI ethics guidelines.</p>

<p>In the Mapping stage, the focus shifts to identifying stakeholders and collaborators. In a GenAIOps context, this means mapping who needs to be involved—for example, data owners for access to data sources, compliance or legal experts if user data or intellectual property is involved, and end-user representatives. This stakeholder map promotes accountability and traceability by clarifying who is responsible for which inputs and decisions.</p>

<p>Incorporating Scoping and Mapping into the business and data understanding phase helps teams working on GenAI projects mitigate strategic risks before any data is collected or model work begins, reducing the chances of building a misaligned or unsafe product.</p>

<p><a data-type="xref" href="#chapter_7_table_4_1748539924522377">Table 7-4</a> provides an outline of key artifacts that should be produced during this phase.</p>

<table class="striped" id="chapter_7_table_4_1748539924522377">
	<caption><span class="label">Table 7-4. </span>Summary of artifacts produced during the business and data understanding phase</caption>
	<thead>
		<tr>
			<th>SMACTR stage</th>
			<th>Key artifacts</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Scoping</p>
			</td>
			<td>
			<ul>
				<li>Preliminary risk assessment</li>
				<li>Ethical review of intended use cases</li>
				<li>Definition of ethical AI principles for the project</li>
				<li>Statement of alignment with corporate values and AI ethics guidelines</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Mapping</p>
			</td>
			<td>
			<ul>
				<li>Identification of stakeholders and collaborators (e.g., data owners, compliance/legal experts, end users)</li>
				<li>Stakeholder map</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Data Preparation Phase" data-type="sect2"><div class="sect2" id="chapter_7_data_preparation_phase_1748539924539972">
<h2>Data Preparation Phase</h2>

<p>CRISP-ML(Q)’s <a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-startref="genai-ops-integr-bdup-1" data-tertiary="Business and Data Understanding Phase" data-type="indexterm" id="id658"/>data preparation phase<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-tertiary="Data Preparation Phase" data-type="indexterm" id="genai-integr-dpp-1"/>, in a GenAI context, includes steps such as gathering large-scale text data, cleaning and filtering it, and preparing it for model input (for example, performing tokenization and formatting). If the GenAI solution involves fine-tuning an LLM or training a domain-specific model, this phase will incorporate assembling the fine-tuning dataset and any prompt templates. It may also involve setting up a knowledge base for retrieval-augmented generation<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-type="indexterm" id="id659"/>, which is a unique data component in GenAIOps not typically present in traditional ML pipelines. The goal of the data preparation phase remains the same—to ensure that the data fed into the model is high quality, relevant, and appropriately prepared—whether it’s tabular data for classic ML or unstructured text and prompt data for LLMs.</p>

<p>Data quality directly affects model success. CRISP-ML(Q) emphasizes robust data handling by selecting the right data, cleaning it to remove noise, normalizing it, engineering features, and standardizing formats. GenAIOps extends MLOps to address the unique challenges of working with GenAI models. One major difference is that traditional feature engineering is unnecessary for LLMs. Instead, GenAIOps focuses on prompt engineering and data curation—for example, crafting prompt examples or structuring input/output pairs to teach the model a task. Another key difference is scale and diversity: GenAI applications often require large-scale data collection with an emphasis on diversity and representativeness to avoid bias or blind spots. If labeling is needed for fine-tuning or RLHF data, GenAIOps might use semi-automated techniques such as pre-labeling with another model or active learning.</p>

<section data-pdf-bookmark="Implementation" data-type="sect3"><div class="sect3" id="chapter_7_implementation_1748539924540027">
<h3>Implementation</h3>

<p>Data preparation involves implementing systematic data quality checks and creating thorough documentation. In practice, the engineering team should treat the text corpora or prompt datasets with the same discipline as curated ML datasets. Key activities include:</p>

<dl>
	<dt>Data selection and sourcing</dt>
	<dd>
	<p>Identify and collect data from multiple sources (e.g., internal documents, public datasets, web-scraped text) relevant to the use case. GenAIOps emphasizes automating this process and continuously ingesting new data if the application requires up-to-date information.</p>
	</dd>
	<dt>Data cleaning</dt>
	<dd>
	<p>Filter out problematic content, such as profanity, personally identifiable information, or offensive text, to prevent the model from learning undesirable patterns. Both MLOps and GenAIOps stress the importance of dataset heterogeneity to reduce bias and overfitting. Tools like <a href="https://oreil.ly/5wr6H">Data-Juicer</a> <a contenteditable="false" data-primary="Data-Juicer" data-type="indexterm" id="id660"/>can help with auditing dataset diversity.</p>
	</dd>
	<dt>Data structuring</dt>
	<dd>
	<p>In traditional ML, this involves tasks like normalizing values or formatting comma-separated values. In GenAI, it includes tokenizing raw text, splitting documents, and adding prompt prefixes or suffixes. It can also involve prompt engineering at the dataset level—that is, constructing input/output pairs that demonstrate the task for fine-tuning or evaluation.</p>
	</dd>
	<dt>Embeddings</dt>
	<dd>
	<p>GenAI data prep should account for dynamic data, meaning that if the application will retrieve documents at runtime (RAG), the team will need to set up an indexed vector database and pipelines to update it with new content. This is an extra operational consideration in GenAIOps to keep a knowledge base fresh.</p>
	</dd>
	<dt>Documentation</dt>
	<dd>
	<p>For GenAIOps, recording details like data sources, preprocessing steps, and known limitations of the dataset is crucial. This supports transparency (e.g., via datasheets for datasets) and helps ensure auditability if issues arise later.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_7_smactr_integration_1748539924540081">
<h3>SMACTR integration</h3>

<p>The Artifact Collection stage of SMACTR is particularly relevant during data preparation. As the team curates the dataset, they should also compile audit artifacts, including a comprehensive checklist to verify that all required documentation is in place and datasheets that capture data lineage, assumptions, etc. This implements quality assurance by design. Before modeling starts, auditors or internal QA reviewers should confirm that the dataset meets defined standards and that any potential biases or data limitations have been logged. For GenAI projects, this step helps surface issues like skewed representation in training data (say, underrepresentation of certain user demographics in a chatbot’s training data) so they can be addressed (e.g., by augmenting the dataset with more representative samples).</p>

<p>The insights from the Scoping stage also feed into data preparation. For instance, if privacy risks were identified, the data pipeline should incorporate appropriate mitigation steps such as data minimization or anonymization.</p>

<p>Finally, the team should begin planning for how data-related risks will be handled in testing. SMACTR’s Testing stage often draws on documented issues and failures, so any concerns that are identified during the data preparation phase should be noted (e.g., “Our dataset might not cover slang—possible failure mode for the chatbot”).</p>

<p><a data-type="xref" href="#chapter_7_table_5_1748539924522398">Table 7-5</a> summarizes the key artifacts that should be produced during the data preparation phase. By treating data as an auditable artifact, supported by checklists, datasheets, and bias analysis, GenAIOps can incorporate SMACTR’s accountability early in the pipeline.</p>

<table class="striped" id="chapter_7_table_5_1748539924522398">
	<caption><span class="label">Table 7-5. </span>Summary of key artifacts produced during the data preparation phase</caption>
	<thead>
		<tr>
			<th>SMACTR stage</th>
			<th>Output</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Artifact Collection</p>
			</td>
			<td>
			<ul>
				<li>Datasheets (data documentation) detailing data lineage, assumptions, etc.</li>
				<li>Log of identified biases or limitations in the dataset</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Scoping</p>
			</td>
			<td>
			<ul>
				<li>Documentation of privacy risks that might require additional data privacy controls during preparation or training</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testing (planning during data prep)</p>
			</td>
			<td>
			<ul>
				<li>Notes on potential data-related risks</li>
				<li>Preliminary plans for how these risks will be handled in testing</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Modeling Phase" data-type="sect2"><div class="sect2" id="chapter_7_modeling_phase_1748539924540137">
<h2>Modeling Phase</h2>

<p>In GenAIOps<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-startref="genai-integr-dpp-1" data-tertiary="Data Preparation Phase" data-type="indexterm" id="id661"/>, during the modeling phase<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-tertiary="Modeling Phase" data-type="indexterm" id="genai-integr-mp-1"/> the base model is chosen or built and adapted to the task. The base model selection and domain adaptation steps include prompt engineering, fine-tuning, or RAG. For a traditional ML project, modeling entails selecting algorithms, tuning hyperparameters, and training the model from scratch on prepared data. In a GenAI project, you’re usually starting with a preexisting model, such as a large pretrained transformer, so “modeling” is more about <em>adapting</em> that model: choosing which LLM to use as a starting point, deciding whether to fine-tune it or use it via prompting, and implementing those adaptations. For example, if building a custom chatbot, the team might take a base model like GPT, fine-tune it on their domain data, and craft a prompt strategy.</p>

<p>CRISP-ML(Q) underscores modeling practices such as ensuring reproducibility of experiments, trying multiple modeling techniques, and aligning the model choice with business objectives. In GenAI application development, the choice of model and the corresponding prompt or fine-tuning strategy must align with the use case <span class="keep-together">constraints</span> (latency, accuracy, etc.), and experiments should be tracked. A key difference in generative modeling is the iterative experimentation with prompts. In structured ML, once you choose an algorithm and features, training is relatively straightforward. But with LLMs, achieving the desired output often requires interactive prompt tuning and tweaking parameters such as temperature settings. This means the modeling phase in GenAIOps can be highly iterative and exploratory, often interleaving with evaluation. You might prompt the model, observe its output, adjust the prompt or use in-context few-shot examples, and repeat. Still, the underlying focus remains on optimizing the model’s behavior to meet predefined success <span class="keep-together">criteria</span>.</p>

<p>Another difference from traditional ML is the scale and tooling involved: training a classical ML model might involve a scikit-learn pipeline or custom code, whereas in GenAIOps, you might leverage specialized frameworks such as Transformers and parameter-efficient fine-tuning libraries and utilize distributed training or serving if fine-tuning a large model. In addition, the complexity of LLMs means that experiment tracking and versioning are even more crucial. Every prompt template or fine-tuned checkpoint is a variant that should be carefully managed (to avoid the “pipeline jungle” problem).</p>

<p>GenAIOps treats the model as an artifact to configure (select and tune) rather than invent from scratch, emphasizing configuration management and performance optimization. For example, model compression and GPU optimization are part of “modeling” in GenAIOps.</p>

<section data-pdf-bookmark="Implementation" data-type="sect3"><div class="sect3" id="chapter_7_implementation_1748539924540192">
<h3>Implementation</h3>

<p>Bringing CRISP-ML(Q)’s discipline into GenAI application development means adopting systematic experimentation and robust model management practices that support compliance with regulations such as the EU AI Act. Practically, this involves several key activities:</p>

<dl>
	<dt>Model selection and establishing a baseline</dt>
	<dd>
	<p>Just as a data scientist might test multiple algorithms, a GenAIOps team should evaluate different base LLMs—such as GPT-5, Mistral, or LLaMA-2—on a representative sample of tasks. The goal is to select the model that best balances performance with business and technical constraints, guided by the success criteria defined during the initial phase. The CRISP-ML(Q) approach emphasizes defining baseline model performance. In GenAI, that could mean using an off-the-shelf model to generate some outputs and evaluating those before customization.</p>
	</dd>
	<dt>Prompt and fine-tuning experimentation</dt>
	<dd>
	<p>If fine-tuning is planned, each training run should be treated as an experiment, with proper versioning of training data, code, and resulting model checkpoints. Similarly, prompt designs—like few-shot or chain-of-thought <span class="keep-together">designs—should</span> be versioned and documented. This will ensure that results can be traced to the exact models and configurations that produced them. Tools like Weights &amp; Biases or MLflow can be integrated into the GenAIOps pipeline to track prompt parameters and model versions.</p>
	</dd>
	<dt>Quality assurance techniques</dt>
	<dd>
	<p>CRISP-ML(Q) recommends integrating specific quality assurance practices during modeling, such as reproducibility checks and model stability assessments. For GenAI, this might include unit tests on prompts to verify that a fixed set of test inputs consistently produces the expected output formats, or static analysis to verify that fine-tuning has not degraded key model capabilities.</p>
	</dd>
	<dt>Addressing risks in model design</dt>
	<dd>
	<p class="fix_tracking">If earlier phases flagged certain risks, bias mitigation techniques should be <span class="keep-together">applied—for</span> example, embedding alignment, incorporating moderation models into the generation pipeline, or choosing a smaller, more controllable model over a larger, less interpretable one. In addition, GenAIOps often involves adding guardrails at the model interface—such as rules-based filters or rejection sampling—to ensure that outputs meet quality and compliance requirements. This aligns with CRISP-ML(Q)’s focus on mitigating risks during development.</p>
	</dd>
</dl>

<p>Throughout these steps, maintaining a “transparency trail” is encouraged by SMACTR. This means documenting design decisions such as why a specific model or prompt strategy was chosen, how configurations were set, and any known trade-offs. For example, you might want to create early drafts of model cards capturing the intended use, architecture, and limitations of the model.</p>

<p>By the end of the modeling phase, the engineering team should have not only a tuned model and pipeline ready for evaluation but also a comprehensive set of documentation and artifacts, guaranteeing that the modeling process is reproducible and well understood.</p>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_7_smactr_integration_1748539924540246">
<h3>SMACTR integration</h3>

<p>During the modeling phase, the key SMACTR stages to focus on are Artifact Collection and Testing (planning). The artifacts should include a design history file or similar documentation detailing the modeling decisions. This documentation might contain architecture diagrams (especially if the application includes multiple components, like an LLM and a vector database), records of hyperparameters or prompt scripts, and any ethical considerations incorporated into the model design (for instance, “we decided not to fine-tune on user chat logs containing sensitive data to preserve privacy”).</p>

<p class="fix_tracking">SMACTR recommends using an audit checklist to verify that essential model documentation, such as model cards and associated data documentation, is complete and accessible. By the end of the modeling phase, these artifacts should be largely finalized.</p>

<p>Although the Testing stage formally begins during the next phase (evaluation), preparation for testing should start during modeling. The team should prioritize which risks to test based on earlier risk mapping. For example, if a failure mode of concern is the model generating toxic language, the modeling phase might incorporate a toxicity classification head or plan for adversarial prompt testing. Essentially, SMACTR encourages baking in testability by ensuring the model includes hooks or supporting tools to facilitate the intensive testing to come.</p>

<p>To enforce accountability and ownership of quality, individual engineers or researchers should be assigned responsibility for various model components or experiments, tracing who fine-tuned which version, who designed which prompt set, etc.</p>

<p>By integrating SMACTR into the modeling phase, the GenAI team treats the model not as a black box but as a transparently developed component ready for rigorous audit and testing. <a data-type="xref" href="#chapter_7_table_6_1748539924522418">Table 7-6</a> summarizes the core artifacts produced during this phase.</p>

<table class="striped" id="chapter_7_table_6_1748539924522418">
	<caption><span class="label">Table 7-6. </span>Summary of key artifacts produced during the modeling phase</caption>
	<thead>
		<tr>
			<th>SMACTR stage</th>
			<th>Output</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Artifact Collection</p>
			</td>
			<td>
			<ul>
				<li>Design history file or documentation of modeling decisions (architecture diagrams, hyperparameters, prompt scripts, ethical considerations)</li>
				<li>Model documentation (model cards)</li>
				<li>Near-complete data documentation</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testing (planning during modeling)</p>
			</td>
			<td>
			<ul>
				<li>Prioritized risks to test based on earlier risk mapping</li>
				<li>Integration of testability features (e.g., toxicity classification head, adversarial prompt testing plans)</li>
				<li>Plans for intensive testing</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Mapping</p>
			</td>
			<td>
			<ul>
				<li>Identification of individuals responsible for model components or experiments (accountability)</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Evaluation Phase" data-type="sect2"><div class="sect2" id="chapter_7_evaluation_phase_1748539924540305">
<h2>Evaluation Phase</h2>

<p>This phase<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-startref="genai-integr-mp-1" data-tertiary="Modeling Phase" data-type="indexterm" id="id662"/> aligns directly with the model evaluation<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-tertiary="Evaluation Phase" data-type="indexterm" id="genai-integr-ep-1"/> stage in the GenAIOps lifecycle. After the model or prompt is developed, a thorough assessment of its performance against the previously defined success criteria should be conducted. In GenAIOps, evaluation is an ongoing, multifaceted process. It includes quantitative evaluation where applicable (e.g., accuracy on a benchmark or BLEU score for a translation task) as well as qualitative evaluation such as human judgment of output quality, user feedback loops, etc. It may also extend to assessing system behavior through techniques like red teaming or probing the model with adversarial or unusual inputs. In practice, the evaluation phase for GenAI often overlaps with deployment, involving shadow deployments or A/B testing with real users. Here, we’ll focus on the core pre-deployment evaluation steps.</p>

<p>CRISP-ML(Q) frames evaluation as verifying the model’s fitness for purpose by assessing its ability to meet the defined success criteria. Similarly, GenAIOps must determine whether the GenAI application meets business needs, quality standards, and thresholds for technical metrics. Both approaches involve testing on holdout datasets or scenarios.</p>

<p>Traditional ML evaluation typically uses well-defined metrics like accuracy, precision, root mean square error (RMSE), and static test datasets. GenAI evaluation is more complex, because generative outputs can’t be easily captured by a single scalar metric. Evaluation in GenAI remains an unsolved problem. For example, given the same inputs, the behavior of the model might change over time, and maintaining consistent outputs may require prompt adjustments. GenAI evaluation often involves human rating of outputs for correctness or preference, adversarial testing to expose failure modes or undesirable behavior, and specialized generative quality metrics such as BLEU or ROUGE scores or perplexity.</p>

<p>GenAI evaluation must also consider ethical and safety dimensions: a model might have great accuracy but fail due to biased or toxic outputs. Therefore, fairness and safety audits must be incorporated. In CRISP-ML(Q), evaluation is iterative; if the model fails to meet the defined criteria, you return to modeling or even data prep. But in GenAIOps, evaluation is often continuous due to nondeterministic outputs and evolving usage. Techniques like “golden sets”—carefully curated test prompts with expected answers for regression testing—are becoming standard.</p>

<p>CRISP-ML(Q) demands rigorous validation against objectives. GenAI expands the scope of evaluation to include open-ended output quality and ethical risk testing alongside traditional performance metrics.</p>

<section data-pdf-bookmark="Implementation" data-type="sect3"><div class="sect3" id="chapter_7_implementation_1748539924540362">
<h3>Implementation</h3>

<p>In the evaluation phase, the engineering team should implement a robust evaluation strategy incorporating:</p>

<dl>
	<dt>Benchmark testing</dt>
	<dd>
	<p>Evaluate the LLM on a dataset of questions or tasks with known answers. For instance, for a code generation model, maintain a suite of programming problems where correct outputs are known and measure success rate. This is analogous to a test set in traditional ML. The benchmarks should be versioned so that improvements and regressions are tracked over time.</p>
	</dd>
	<dt>Quality metrics</dt>
	<dd>
	<p class="fix_tracking">Define quantitative metrics appropriate to the use case. These might include BLEU or ROUGE scores for text similarity if a reference output is available, diversity metrics, or response latency. Incorporating user satisfaction scores or ratings into the evaluation loop is often useful, if the application can gather feedback.</p>
	</dd>
	<dt>Human and adversarial evaluation</dt>
	<dd>
	<p>Given the open-ended nature of generative models, humans should assess a sample of outputs for factors like correctness, coherence, and safety. Adversarial testing, which involves deliberately testing the model with challenging or sensitive prompts to observe its behavior, is also strongly encouraged during the Testing stage. For example, a tester might ask a chatbot inappropriate or policy-violating questions to see whether it responds with disallowed content. The results can be summarized in an ethical risk analysis chart, as suggested by SMACTR, rating failure modes by severity and likelihood.</p>
	</dd>
	<dt>Automated monitoring in evaluation</dt>
	<dd>
	<p>Use automated tools to evaluate outputs at scale. An emerging GenAIOps practice is using “watcher models” or classifiers to flag problematic content. For instance, toxicity detectors such as Llama Guard<a contenteditable="false" data-primary="Llama Gua" data-type="indexterm" id="id663"/> or Azure’s Prompt Shields can scan a large sample of outputs to estimate the percentage that might be harmful or inappropriate. These tools add a layer of quality assurance beyond simple <span class="keep-together">metrics</span>.</p>
	</dd>
	<dt>Comparison and validation</dt>
	<dd>
	<p>If multiple model versions or prompt variants were developed, evaluate them side by side. A/B testing can be done offline (with evaluators blind to which model produced which output) to choose the best version. This resembles ensemble/hyperparameter selection in CRISP-ML(Q), but in GenAI it may involve comparing two prompt templates.</p>
	</dd>
	<dt>Documentation</dt>
	<dd>
	<p>Documentation remains critical throughout evaluation. Teams should record what tests were run, the outcomes, and the decisions made based on those results. If the model fails certain tests, document whether the issues will be addressed or accepted with mitigations. For instance, if an LLM occasionally produces outputs that are slightly incorrect but harmless, the team might decide that’s acceptable for the business context; however, if it sometimes produces a privacy violation, that would trigger a model revision.</p>
	</dd>
</dl>

<p>The evaluation phase in GenAIOps is also where “go or no-go” decisions are made for deployment. The engineering team should define clear launch criteria, such as “fewer than 1% of outputs flagged as offensive” or “at least 85% of test questions answered correctly.” If the model fails to meet these thresholds, the process loops back to the modeling or even the data preparation phase to address the shortcomings through fine-tuning, prompt redesign, or data augmentation.</p>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_7_smactr_integration_1748539924540415">
<h3 class="less_space">SMACTR integration</h3>

<p>The Testing stage of SMACTR aligns directly with the evaluation phase. The framework recommends techniques such as adversarial testing to assess performance, and the production of artifacts such as an ethical risk analysis chart. In a GenAI context, this means paying special attention to verifying the model’s compliance with ethical and risk-related requirements identified during the Scoping stage. Teams should use risk assessment tools like Failure Mode and Effects Analysis (FMEA) to list potential failure modes, such as “model gives legal advice” or “model outputs biased language,” and implement tests to cover each identified risk.</p>

<p>The Testing stage also focuses on ethical compliance<em>.</em> To evaluate high-priority ethical risks or known issues arising from the training data or model design, teams may create and execute a checklist of ethical tests, including questions like “Does the model hallucinate facts?” or “Does it stereotype?”</p>

<p>This phase also lays the groundwork for the Reflection stage. Gather all evaluation results and insights to feed into a mitigation plan. For example, if testing reveals that the model struggles with a particular category of inputs, make a note of that for post-deployment monitoring or retraining.</p>

<p>Artifact Collection continues throughout this phase. Evaluation reports, risk charts, and test results should be saved as part of the transparency trail. The evaluation phase for GenAI applications effectively becomes an audit of the model’s readiness. As well as verifying accuracy and performance, you should also test for safety, fairness, and robustness, with documented evidence to show stakeholders such as regulators that due diligence was done before deployment. <a data-type="xref" href="#chapter_7_table_7_1748539924522437">Table 7-7</a> summarizes the key SMACTR artifacts for this stage.</p>

<table class="striped" id="chapter_7_table_7_1748539924522437">
	<caption><span class="label">Table 7-7. </span>Summary of key artifacts produced during the evaluation phase</caption>
	<thead>
		<tr>
			<th>SMACTR stage</th>
			<th>Output</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Testing</p>
			</td>
			<td>
			<ul>
				<li>Performance assessment using methods like adversarial testing</li>
				<li>Ethical risk analysis chart</li>
				<li>FMEA or similar risk assessment</li>
				<li>Tests for identified failure modes</li>
				<li>Checklist of ethical tests (e.g., hallucination, stereotyping)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Reflection</p>
			</td>
			<td>
			<ul>
				<li>Gathered results and insights from evaluation to inform a mitigation plan</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Artifact Collection</p>
			</td>
			<td>
			<ul>
				<li>Evaluation reports, risk charts, and test results</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Deployment Phase" data-type="sect2"><div class="sect2" id="chapter_7_deployment_phase_1748539924540472">
<h2>Deployment Phase</h2>

<p>The CRISP-ML(Q)<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-startref="genai-integr-ep-1" data-tertiary="Evaluation Phase" data-type="indexterm" id="id664"/> deployment phase<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-tertiary="Deployment Phase" data-type="indexterm" id="genai-integr-dp-1"/> focuses on delivering the model into production and making sure it meets business requirements in real-world operation. Other key activities include user acceptance testing and producing documentation. In <span class="keep-together">GenAIOps</span>, this phase covers multiple operational steps, such as integration and orchestration (CI/CD), security and reliability engineering, and the actual model deployment. Once a model is considered ready, it must be integrated into the application infrastructure, served to end users, and maintained with proper MLOps and GenAIOps practices.</p>

<p>As in traditional MLOps, deployment in GenAIOps involves pushing the model or pipeline to production by setting up API endpoints or embedding the model into an application. It also includes tasks like continuous delivery of prompt or model updates, implementing safety controls such as rate limiting, monitoring for misuse, and ensuring the system can scale and remain reliable under load.</p>

<p>Given the dynamic nature of GenAI applications, teams may frequently update prompts or swap in improved models, making a CI/CD pipeline essential. The security and reliability aspects of this stage reflect CRISP-ML(Q)’s quality assurance mindset. The main goal is to ensure that the deployed model doesn’t expose vulnerabilities (such as prompt injection) and remains robust.</p>

<section data-pdf-bookmark="Implementation" data-type="sect3"><div class="sect3" id="chapter_7_implementation_1748539924540527">
<h3>Implementation</h3>

<p>In practice, deploying a GenAI application with CRISP-ML(Q) principles means paying attention to engineering best practices and safety as the model goes live. Key considerations in the deployment phase include:</p>

<dl>
	<dt>CI/CD for prompts and models</dt>
	<dd>
	<p>Establish automated pipelines to move updated prompts or model versions through testing and into production. For example, similar to unit tests in software deployment, if a prompt template is updated to improve performance, a CI pipeline should run the evaluation suite and promote the change only if it passes. Likewise, infrastructure-as-code should be used to deploy model servers or services. This ensures reproducibility by creating a consistent environment and provides traceability regarding which version of the model or prompt is in use.</p>
	</dd>
	<dt>System integration and performance optimization</dt>
	<dd>
	<p>Unlike simple ML deployments, GenAI applications often have multiple components (frontend, backend, model APIs, databases, vector stores, etc.). Integration involves connecting these and making sure data flows correctly—for instance, a user query routes to the LLM, which might call a knowledge base before returning a response. Depending on nonfunctional requirements such as cost, speed, and scalability, you may need to optimize for latency and throughput. Because large models can be expensive or slow to run, you might want to apply techniques like model quantization, caching frequent responses, or using smaller distilled models for specific requests.</p>
	</dd>
	<dt>Monitoring setup</dt>
	<dd>
	<p>Although monitoring is the next formal phase in CRISP-ML(Q), the groundwork is laid during deployment. Implement logging for model inputs and outputs, set up dashboards and alerts for key metrics (error rates, response times, etc.), and track usage patterns. GenAIOps best practices recommend integrating observability from the moment a model is deployed. Tools like Prometheus and Grafana are commonly used for infrastructure monitoring, along with custom monitors for model behavior.</p>
	</dd>
	<dt>Safety mechanisms</dt>
	<dd>
	<p>For GenAI applications, guardrails such as content filters, user authentication and authorization, rate limiting, and fallback mechanisms should be put in place at deployment time. For instance, if the LLM is part of a user-facing app, you might integrate a moderation API or a simple rules engine to check model outputs. If an output violates policy (e.g., contains hate speech), the system can block or sanitize the response. You should also plan for failover: if the LLM service is unavailable, the system should default to a safe fallback such as a simpler response or a static message. These safety and reliability measures help ensure the live system is resilient and complies with the requirements of the EU AI Act.</p>
	</dd>
	<dt>Staged rollouts</dt>
	<dd>
	<p>Teams may choose to do a shadow deployment, where the model generates outputs in response to real user queries without the user seeing them, to collect performance data in production-like conditions. This is similar to a pilot or beta test in CRISP-ML(Q), providing assurance that the model works as expected before full launch.</p>
	</dd>
	<dt>Documentation and training</dt>
	<dd>
	<p>Ensure that the operations team (which may be the same as the development team) has clear documentation on how to roll back deployments, how to intervene if something goes wrong, and what the known issues are. In CRISP-ML(Q), deployment concludes with documentation being delivered to maintainers; in GenAIOps, this may take the form of runbooks or playbooks. For example, a playbook might say: “If monitoring shows a spike in slow responses, consider disabling complex features or scaling up compute resources.”</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_7_smactr_integration_1748539924540580">
<h3>SMACTR integration</h3>

<p>The SMACTR framework encourages a “deploy responsibly” mentality, treating deployment not as the end goal, but as a moment to verify that everything done so far aligns with ethical, quality, and safety standards and to set up mechanisms for ongoing vigilance once the model is in production.</p>

<p>In this phase, the Reflection stage of SMACTR should be activated. Before going live, the team should reflect on whether the development process met the initial ethical and risk-related objectives. Concretely, this means reviewing evaluation results and determining whether any remaining risks are unacceptable. For example, the Reflection stage might lead to a<em> </em>risk mitigation plan: “We observed that the model sometimes gives outdated information. Mitigation: Display a disclaimer or schedule regular retraining.”</p>

<p>SMACTR’s Reflection stage also calls for producing an algorithmic audit summary report, which is a summary of key findings, risk decisions, and ethical considerations throughout the project. For a GenAI deployment, this could be distilled into something like a model card plus a responsible AI deployment checklist that is filed for governance purposes.</p>

<p><a data-type="xref" href="#chapter_7_table_8_1748539924522457">Table 7-8</a> provides an overview of the artifacts that should be produced during the deployment phase, particularly as part of the SMACTR framework’s Reflection stage.</p>

<table id="chapter_7_table_8_1748539924522457">
	<caption><span class="label">Table 7-8. </span>Summary of key artifacts produced during the deployment phase</caption>
	<thead>
		<tr>
			<th>SMACTR stage</th>
			<th>Output</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Reflection</p>
			</td>
			<td>
			<ul>
				<li>Review of evaluation results</li>
				<li>Decision on acceptability of residual risks</li>
				<li>Risk mitigation plan (e.g., disclaimers, retraining schedules)</li>
				<li>Algorithmic audit summary report (or model card and responsible AI deployment checklist)</li>
				<li>Plan for periodic post-launch audits (internal reviews)</li>
				<li>Maintenance of traceability (model version, deployment date, evaluation)</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Model Monitoring and Maintenance Phase" data-type="sect2"><div class="sect2" id="chapter_7_model_monitoring_and_maintenance_phase_1748539924540642">
<h2>Model Monitoring and Maintenance Phase</h2>

<p>In GenAIOps<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-startref="genai-integr-dp-1" data-tertiary="Deployment Phase" data-type="indexterm" id="id665"/>, once an LLM-powered application<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-tertiary="Model Monitoring and Maintenance Phase" data-type="indexterm" id="genai-integr-mmmp-1"/> is deployed, teams must continuously monitor its performance, capture data on real-world use, and maintain or improve the model over time. This phase, often called <em>model operations</em>, is analogous to the CRISP-ML(Q) monitoring and maintenance phase.</p>

<p>Maintenance in a GenAI context can take several forms, such as fine-tuning a model on new data, switching to a new model version, updating prompts, refreshing the knowledge base in a RAG pipeline, or retraining if the domain has changed significantly. The monitoring and maintenance phase is specifically intended to address the risk of model degradation in evolving environments, where data drift, changing user needs, or concept drift may occur post-deployment.</p>

<p>In traditional ML, monitoring typically focuses on data drift, model drift, and core performance metrics—for instance, detecting if the distribution of the input data has changed enough to reduce accuracy and trigger retraining. Some of those concepts apply to GenAI as well: user queries may evolve over time, making the original model or prompt less relevant. However, GenAI introduces additional challenges, such as monitoring the content of outputs to detect inappropriate or factually incorrect answers and tracking user interaction patterns to detect potential misuse.</p>

<p>Feedback loops are especially critical in GenAI: models can be improved using techniques like RLHF or fine-tuning based on logged errors and failures. CRISP-ML(Q) emphasizes continuous quality control but focuses more on technical degradation and planned retraining, whereas GenAI maintenance involves active, sometimes real-time, management of model behavior.</p>

<p>Finally, integrating monitoring throughout the pipeline is essential. Without proper monitoring and maintenance, a model’s performance and utility will likely decay over time, and safety can quickly degrade. The dynamic, nondeterministic nature of GenAI applications makes this phase even more critical and complex.</p>

<section data-pdf-bookmark="Implementation" data-type="sect3"><div class="sect3" id="chapter_7_implementation_1748539924540708">
<h3>Implementation</h3>

<p>For a GenAI application, the monitoring and maintenance phase involves several ongoing practices, many of which parallel classic MLOps but are adapted for GenAI-specific challenges and metrics:</p>

<dl>
	<dt>Live performance monitoring</dt>
	<dd>
	<p>Track the model’s outputs in production, monitoring for quality and reliability. This may involve statistical indicators such as the percentage of queries answered successfully or user ratings (if available), as well as technical metrics like latency and error rates. In a GenAI chatbot, for instance, frequent rephrasing of questions by users might signal poor initial responses. Setting up alert thresholds is important. For example, if the rate of content flagging by a moderation filter doubles overnight, the team should be alerted to investigate potential model drift or misuse.</p>
	</dd>
	<dt>Data and usage drift</dt>
	<dd>
	<p>Unlike traditional ML, where input features can be directly compared, GenAI may require monitoring embeddings of user queries or clustering them to detect topic shifts. In a RAG-based system, for example, monitoring might reveal user interest in a new product that isn’t yet in the knowledge base.</p>
	</dd>
	<dt>Logging and audit</dt>
	<dd>
	<p>Maintain detailed logs of interactions, with appropriate privacy safeguards, to support retrospective analysis and auditing. These logs can feed into a feedback dataset. In addition, teams might periodically label a random sample of interactions to assess satisfaction or accuracy, helping to build an ongoing evaluation dataset to use for quality tracking.</p>
	</dd>
	<dt>Retraining or model updates</dt>
	<dd>
	<p>Decide on a schedule or criteria for when to retrain or update the model. In cases where the base model is provided via an API (like OpenAI’s GPT), retraining might not be feasible. However, teams can still fine-tune adapters and switch to newer model versions as they become available (e.g., upgrading from GPT-3.5 to GPT-4). Each update should pass through a condensed version of the evaluation and deployment lifecycle.</p>
	</dd>
	<dt>Prompt and system maintenance</dt>
	<dd>
	<p>Maintenance is also important for the surrounding system. Teams may refine prompt templates based on observed performance or introduce new safety rules in response to emerging failure cases. Because GenAI applications can be highly sensitive to prompt changes, treat these with the same caution as code changes—test before rollout and monitor performance closely.</p>
	</dd>
	<dt>User feedback and iteration</dt>
	<dd>
	<p>Provide channels for user feedback (thumbs up/down for responses, reporting issues, etc.), and establish a process for analyzing and acting on this feedback <span class="keep-together">regularly</span>.</p>
	</dd>
	<dt>Risk management</dt>
	<dd>
	<p>If new misuse patterns emerge—for example, if users find a new way to get the model to produce disallowed content—the team should respond quickly, updating filters, modifying prompts, or adjusting model behavior to block these <span class="keep-together">vulnerabilities</span>.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="SMACTR integration" data-type="sect3"><div class="sect3" id="chapter_7_smactr_integration_1748539924540770">
<h3>SMACTR integration</h3>

<p>Monitoring and maintenance corresponds to an ongoing cycle of Reflection and renewed Scoping in SMACTR terms. While the Reflection stage is originally framed as a pre-deployment audit, its core activity—evaluating outcomes against initial goals and risks—should continue periodically post-deployment.</p>

<p>For a GenAI application, this could mean scheduling regular internal audits or postmortems. For example, after a month of production use, the team might review questions such as:</p>

<ul>
	<li>
	<p>Are there new ethical concerns arising from how the model is being used?</p>
	</li>
	<li>
	<p>Did our mitigation plan hold up, or do we need new measures?</p>
	</li>
</ul>

<p>This continuous reflection can lead to updates to the risk mitigation plan and may even trigger a fresh round of Scoping if the application’s purpose or usage context evolves. For instance, if the system is used in a higher-stakes environment than originally intended, the team should revisit and rescope ethical and operational risks.</p>

<p>SMACTR also calls for documentation of incidents, meaning teams should maintain a history of design changes and keep a log of any significant failures and how they were addressed. This is analogous to treating the algorithmic use-related risk analysis and audit summary report as living documents, updated with real-world findings over time.</p>

<p>Additionally, the Mapping stage may need to be revisited if new stakeholders emerge. For example, customer support might get involved to handle user reports about the system, or legal teams might need to review compliance with regulations such as the EU AI Act. Maintenance processes should ensure that these stakeholders are identified and looped into governance and oversight activities.</p>

<p>Finally, monitoring effectively functions as continuous testing in production. The SMACTR framework emphasizes that monitoring and audits help identify emerging threats and act quickly. The engineering team should treat monitoring data not just as operational information but as audit evidence. Every anomaly should be investigated for root causes, every significant output error analyzed for potential broader impacts, and oversight responsibilities clearly assigned. This is especially important for risk mitigation in GenAI, where issues like model bias or inappropriate outputs can lead to reputational or legal consequences.</p>

<p>All notable outputs for the monitoring and maintenance phase for each SMACTR stage are outlined in <a data-type="xref" href="#chapter_7_table_9_1748539924522477">Table 7-9</a>.</p>

<table class="striped" id="chapter_7_table_9_1748539924522477">
	<caption><span class="label">Table 7-9. </span>Summary of key artifacts produced during the monitoring and maintenance phase</caption>
	<thead>
		<tr>
			<th>SMACTR stage</th>
			<th>Output</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Reflection (ongoing)</p>
			</td>
			<td>
			<ul>
				<li>Periodic internal audits or postmortems</li>
				<li>Evaluation of results relative to original goals and risks</li>
				<li>Updates to the risk mitigation plan (living document)</li>
				<li>Documentation of incidents (design history, failure logs)</li>
				<li>Maintenance of algorithmic use-related risk analysis and audit summary report (living documents)</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Scoping (renewed)</p>
			</td>
			<td>
			<ul>
				<li>Rescoping and reassessment of risks if the application’s scope expands or changes</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Mapping (revisited)</p>
			</td>
			<td>
			<ul>
				<li>Inclusion of new stakeholders (e.g., customer support, legal teams) and clarification of their oversight roles</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Testing (continuous)</p>
			</td>
			<td>
			<ul>
				<li>Monitoring as continuous testing in production</li>
				<li>Identification of emerging threats</li>
				<li>Investigation of anomalies and significant output errors</li>
				<li>Clear assignment of oversight responsibility</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="chapter_7_conclusion_1748539924540830">
<h1>Conclusion</h1>

<p>This<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-startref="genaiops-integr-crisp-smactr-1" data-type="indexterm" id="id666"/> chapter explored<a contenteditable="false" data-primary="GenAIOps" data-secondary="integration with CRISP-ML(Q) and SMACTR" data-startref="genai-integr-mmmp-1" data-tertiary="Model Monitoring and Maintenance Phase" data-type="indexterm" id="id667"/> the core aspects of the EU AI Act as they apply to general-purpose AI models and systems, along with the practical role of GenAIOps in supporting compliance and fostering trustworthy AI. As outlined here, the Act establishes a structured regulatory framework for GPAI, requiring documentation, disclosure, and risk mitigation measures to be embedded into providers’ workflows. GenAIOps extends traditional MLOps practices to meet the unique challenges of GPAI, enabling engineering teams to build systems that are compliant with regulatory requirements, resilient, scalable, and aligned with societal expectations. By integrating the SMACTR framework, organizations can further enhance CRISP-ML(Q) by embedding accountability mechanisms that address GPAI-specific risks such as hallucination and ethical misalignment.</p>

<p>Achieving compliance, however, is not a one-time task. It demands a proactive, ongoing approach in which transparency, accountability, and ethical reflection are integrated throughout the AI development lifecycle. From data preparation and model fine-tuning to deployment and continuous monitoring, teams must adopt structured, auditable processes to ensure their systems remain reliable and interpretable and meet ethical standards. This is particularly crucial for managing risks such as <span class="keep-together">bias, misinformation,</span> and unintended model behaviors. GenAIOps provides an operational framework that allows organizations to implement these safeguards systematically, bridging the gap between regulatory compliance and real-world AI development and deployment.</p>
</div></section>

<section data-pdf-bookmark="Final Words and Future of AI Policymaking" data-type="sect1"><div class="sect1" id="chapter_7_final_words_and_future_of_ai_policymaking_1748539924540894">
<h1>Final Words and Future of AI Policymaking</h1>

<p>As we conclude our journey through AI governance, engineering practices, and the EU AI Act, it’s important to reflect on what has been covered and look to the future. We began by exploring the foundational landscape of the EU AI Act’s risk-based approach and how it introduces crucial obligations, even for systems not classified as high risk. A key insight is that the Act requires transparency for all AI systems intended to interact directly with natural persons, regardless of their risk level (primarily through Article 50).</p>

<p>We then turned to the practical world of AI development and deployment, recognizing that compliance is not a post-deployment afterthought but a process that must be integrated throughout the AI system lifecycle. The adoption of structured methodologies such as CRISP-ML(Q), alongside ethical frameworks like SMACTR, emerges as a proactive catalyst for responsible and compliant development. By aligning the phases of CRISP-ML(Q), from business and data understanding to monitoring and maintenance, with SMACTR principles, organizations can systematically address ethical considerations, manage risks, and fulfill transparency requirements at every stage.</p>

<p>A recurring theme throughout this book has been the critical importance of documentation and robust metadata management. These are foundational for traceability, internal audits, demonstrating compliance, and fostering transparency. Key artifacts generated throughout the development process, such as the project scope document, ethical review report, stakeholder map, data quality assessment, and initial transparency requirements document, serve as evidence of due diligence and ethical consideration. Further documentation, such as data quality reports, feature documentation, evaluation reports, compliance verification records, deployment configurations, and monitoring logs, helps establish a comprehensive compliance trail.</p>

<p>The rapid emergence and widespread adoption of GPAI and generative AI models during the legislative process introduced new challenges, prompting the EU AI Act to incorporate specific provisions to address them. In this evolving landscape, the principles of MLOps—extended into GenAIOps—remain vital for managing these complex systems. They support transparency, logging, and version control to create auditable trails, enabling teams to operationalize compliance while maintaining flexibility and performance.</p>

<p>As you have seen, navigating the landscape of AI regulation requires more than just understanding the rules—it demands a fundamental shift in how AI systems are engineered and governed. Proactive compliance, embedded within engineering workflows and supported by comprehensive documentation, is the path forward.</p>

<p class="fix_tracking">The landscape of AI technologies is evolving at a remarkable pace. An example of this is the emergence of retrieval-augmented generation<a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-type="indexterm" id="id668"/> and agent-based architectures. To keep pace, we must continually learn about new technologies and stay informed about regulatory updates, best practices, and emerging methodologies for AI governance.</p>

<p class="fix_tracking">In the Preface, I quoted Fei-Fei Li, the founding codirector of the Stanford Institute for Human-Centered AI (HAI) and CEO and cofounder of World Labs, who <a href="https://oreil.ly/_PUIb">remarked</a>: “Now more than ever, AI needs a governance framework.” She laid out three fundamental principles for the future of AI policymaking<a contenteditable="false" data-primary="AI policymaking" data-type="indexterm" id="ai-pol-1"/>:</p>

<ol>
	<li>
	<p>AI engineering should <em>prioritize empirical validation over speculation</em>, ensuring models are rigorously tested with real-world data and transparent benchmarks. This means engineers should rely on scientifically grounded methods, avoiding hype and focusing on practical applications rather than speculative scenarios. Standardized frameworks like ISO/IEC AI guidelines, NIST’s AI Risk Management Framework, or SMACTR can be integrated into development workflows to promote reliability, fairness, and robustness.</p>
	</li>
	<li>
	<p>Engineers must <em>adopt risk-aware, pragmatic development practices, balancing innovation with responsible deployment</em> by integrating continuous monitoring and adherence to ethical standards. This includes incorporating risk assessment early in development, leveraging iterative prototyping to surface and address unintended consequences, and continuous post-deployment monitoring to detect bias or misuse. AI systems should be designed with built-in safeguards and governance mechanisms, especially in high-stakes areas like defense or healthcare.</p>
	</li>
	<li>
	<p>Lastly, <em>collaboration and open access</em> should be central, promoting open source contributions, knowledge sharing, and cross-sector partnerships to strengthen the AI ecosystem while maintaining accountability. Engineers can support this by sharing research findings, building reusable frameworks, and creating or contributing to tools that lower barriers to entry for startups and academic institutions.</p>
	</li>
</ol>

<p>Ultimately, building trustworthy  AI is both a technical challenge and a responsibility shared by the whole organization. As engineers, researchers, and practitioners, we shape not only what AI can do, but how it affects the world. By embedding ethics, transparency, and accountability into every stage of development, we can ensure that the systems we create are not only powerful, but worthy of the trust placed in them.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id616"><sup><a href="ch07.html#id616-marker">1</a></sup> For more information, see the paper <a href="https://oreil.ly/DDpPc">“Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data”</a> by Nahema Marchal et al. </p></div></div></section></body></html>