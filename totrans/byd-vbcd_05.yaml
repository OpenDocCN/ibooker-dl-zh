- en: 'Chapter 3\. The 70% Problem: AI-Assisted Workflows That Actually Work'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI-based coding tools are astonishingly good at certain tasks.^([1](ch03.html#id486))
    They excel at producing boilerplate, writing routine functions, and getting projects
    *most of the way* to completion. In fact, many developers find that an AI assistant
    can implement an initial solution that covers roughly 70% of the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Peter Yang perfectly captured what I’ve been observing in the field [in a post
    on X](https://oreil.ly/i9qwq):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Honest reflections from coding with AI so far as a non-engineer:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It can get you 70% of the way there, but that last 30% is frustrating. It keeps
    taking one step forward and two steps backward with new bugs, issues, etc.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If I knew how the code worked I could probably fix it myself. But since I don’t,
    I question if I’m actually learning that much.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nonengineers using AI for coding find themselves hitting a frustrating wall.
    They can get 70% of the way there surprisingly quickly, but that final 30% becomes
    an exercise in diminishing returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'This “70% problem” reveals something crucial about the current state of AI-assisted
    development. The initial progress feels magical: you can describe what you want,
    and AI tools like v0 or Bolt will generate a working prototype that looks impressive.
    But then reality sets in.'
  prefs: []
  type: TYPE_NORMAL
- en: The 70% is often the straightforward, patterned part of the work—the kind of
    code that follows well-trod paths or common frameworks. As one [*Hacker News*
    commenter observed](https://oreil.ly/Ff3Ts), AI is superb at handling the “accidental
    complexity” of software (the repetitive, mechanical stuff), while the “essential
    complexity”⁠—understanding and managing the inherent complexity of a problem—remains
    on human shoulders. In Fred Brooks’s classic terms, AI tackles the incidental
    but not the intrinsic difficulties of development.
  prefs: []
  type: TYPE_NORMAL
- en: Where do these tools struggle? Experienced engineers consistently report a “last
    mile” gap. AI can generate a plausible solution, but the final 30%—covering edge
    cases, refining the architecture, and ensuring maintainability—“needs serious
    human expertise.”
  prefs: []
  type: TYPE_NORMAL
- en: For example, an AI might give you a function that technically works for the
    basic scenario, but it won’t automatically account for unusual inputs, race conditions,
    performance constraints, or future requirements unless explicitly told. AI can
    get you most of the way there, but that final crucial 30% (edge cases, keeping
    things maintainable, and solid architecture) needs serious human expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, AI has a known tendency to generate convincing but incorrect output.
    It may introduce subtle bugs or “hallucinate” nonexistent functions and libraries.
    [Steve Yegge wryly likens](https://oreil.ly/hjv8f) today’s LLMs to “wildly productive
    junior developers”—incredibly fast and enthusiastic but “potentially whacked out
    on mind-altering drugs,” prone to concocting crazy or unworkable approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Yegge’s words](https://oreil.ly/yPMPO), an LLM can spew out code that looks
    polished at first glance, yet if a less-experienced developer naively says, “Looks
    good to me!” and runs with it, hilarity (or disaster) ensues in the following
    weeks. The AI doesn’t truly *understand* the problem; it stitches together patterns
    that *usually* make sense. Only a human can discern whether a seemingly fine solution
    hides long-term landmines. [Simon Willison echoed this](https://oreil.ly/sLzFY)
    after seeing an AI propose a bewitchingly clever design that *only a senior engineer
    with deep understanding of the problem* could recognize as flawed. The lesson:
    AI’s confidence far exceeds its reliability.'
  prefs: []
  type: TYPE_NORMAL
- en: Crucially, current AIs [do not create fundamentally new abstractions or strategies
    beyond their training data](https://oreil.ly/HkwVF). They won’t invent a novel
    algorithm or an innovative architecture for you—they remix what’s known. They
    also won’t take responsibility for decisions. As one engineer noted, “AIs don’t
    have ‘better ideas’ than what their training data contains. They don’t take responsibility
    for their work.”
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this means that creative and analytical thinking—deciding *what* to
    build, *how* to structure it, and *why*—firmly remains a human domain. In summary,
    AI is a force multiplier for developers, handling the repetitive 70% and giving
    us a “turbo boost” in productivity. But it is *not* a silver bullet that can replace
    human judgment. The remaining 30% of software engineering—the hard parts—still
    requires skills that only trained, thoughtful developers can bring. Those are
    the durable skills to focus on, and [Chapter 4](ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362)
    is dedicated to them. As one [discussion](https://oreil.ly/QXYsj) put it: “AI
    is a powerful tool, but it’s not a magic bullet.…Human judgment and good software
    engineering practices are still essential.”'
  prefs: []
  type: TYPE_NORMAL
- en: How Developers Are Actually Using AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ve observed two distinct patterns in how teams are leveraging AI for development.
    Let’s call them the “bootstrappers” and the “iterators.” Both are helping engineers
    (and even nontechnical users) reduce the gap from idea to execution (or MVP).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there are the *bootstrappers*, who are generally taking a new project
    from zero to MVP. Tools like Bolt, v0, and screenshot-to-code AI are revolutionizing
    how these teams bootstrap new projects. These teams typically:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a design or rough concept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use AI to generate a complete initial codebase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a working prototype in hours or days instead of weeks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on rapid validation and iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results can be impressive. I recently watched a solo developer use Bolt
    to turn a Figma design into a working web app in next to no time. It wasn’t production-ready,
    but it was good enough to get very initial user feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second camp, the *iterators*, uses tools like Cursor, Cline, Copilot, and
    Windsurf for their daily development workflow. This is less flashy but potentially
    more transformative. These developers are:'
  prefs: []
  type: TYPE_NORMAL
- en: Using AI for code completion and suggestions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging AI for complex refactoring tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating tests and documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using AI as a “pair programmer” for problem solving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But here’s the catch: while both approaches can dramatically accelerate development,
    they come with hidden costs that aren’t immediately obvious.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you watch a senior engineer work with AI tools like Cursor or Copilot,
    it looks like magic. They can scaffold entire features in minutes, complete with
    tests and documentation. But watch carefully, and you’ll notice something crucial:
    they’re not just accepting what the AI suggests. They’re constantly refactoring
    the generated code into smaller, focused modules. They’re adding comprehensive
    error handling and edge-case handling the AI missed, strengthening its type definitions
    and interfaces, and questioning its architectural decisions. In other words, they’re
    applying years of hard-won engineering wisdom to shape and constrain the AI’s
    output. The AI is accelerating their implementation, but their expertise is what
    keeps the code maintainable.'
  prefs: []
  type: TYPE_NORMAL
- en: Common Failure Patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Junior engineers often miss these crucial steps.  They accept the AI’s output
    more readily, leading to what I call “house of cards code”—it looks complete but
    collapses under real-world pressure.
  prefs: []
  type: TYPE_NORMAL
- en: Two steps back
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What typically happens next follows a predictable antipattern I call the “two
    steps back” pattern (shown in [Figure 3-1](#ch03_figure_1_1752630043194318)):'
  prefs: []
  type: TYPE_NORMAL
- en: You try to fix a small bug.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AI suggests a change that seems reasonable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This fix breaks something else.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You ask AI to fix the new issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This creates two more problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rinse and repeat.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/bevc_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. The “two steps back” antipattern.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This cycle is particularly painful for nonengineers because they lack the mental
    models to understand what’s actually going wrong. When an experienced developer
    encounters a bug, they can reason about potential causes and solutions based on
    years of pattern recognition. Without this background, you’re essentially playing
    whack-a-mole with code you don’t fully understand. This is the “knowledge paradox”
    I mentioned back in this book’s preface: senior engineers and developers use AI
    to accelerate what they already know how to do, while juniors try to use it to
    learn *what* to do.'
  prefs: []
  type: TYPE_NORMAL
- en: This cycle is particularly painful for nonengineers using AI in a “bootstrapper”
    pattern, because they lack the mental models needed to address these issues building
    their MVP. However, even experienced “iterators” can fall into this whack-a-mole
    trap if they overly rely on AI suggestions without deep validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a deeper issue here: the very thing that makes AI coding tools accessible
    to nonengineers—their ability to handle complexity on your behalf—can actually
    impede learning. When code just “appears” without you understanding the underlying
    principles, you don’t develop debugging skills. You miss learning fundamental
    patterns. You can’t reason about architectural decisions, and so you struggle
    to maintain and evolve the code. This creates a dependency where you need to keep
    going back to the AI model to fix issues rather than developing the expertise
    to handle them yourself.'
  prefs: []
  type: TYPE_NORMAL
- en: This dependency risk enters a new dimension with the emergence of autonomous
    AI coding agents—a topic I explore in depth in [Chapter 10](ch10.html#ch10_autonomous_background_coding_agents_1752630045087844).
     Unlike current tools that suggest code snippets, these agents represent a fundamental
    shift in how software can be developed. As I write this, we’re witnessing the
    early deployment of systems that can independently plan, execute, and iterate
    on entire development tasks with minimal human oversight.
  prefs: []
  type: TYPE_NORMAL
- en: This evolution from assistive to autonomous AI introduces profound questions
    about developer expertise and control. When an AI system can handle complete development
    workflows, from initial implementation through testing and deployment, the risk
    of skill atrophy becomes acute. Developers who rely heavily on these agents without
    maintaining their foundational knowledge may find themselves unable to effectively
    audit, guide, or intervene when the AI’s decisions diverge from intended outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge compounds when we consider how these autonomous systems make cascading
    decisions throughout a project. Each individual choice might appear reasonable
    in isolation, yet the cumulative effect could steer development in unintended
    directions. Without the expertise to recognize and correct these trajectory shifts
    early, teams risk building increasingly complex systems on foundations they don’t
    fully understand.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ll examine more thoroughly later, the advent of autonomous coding agents
    doesn’t diminish the importance of software engineering fundamentals—it amplifies
    it. The more powerful our AI tools become, the more critical it is that we maintain
    the expertise to remain architects of our systems rather than mere operators.
    Only through deep understanding of software principles can we ensure these remarkable
    tools enhance our capabilities rather than erode them.
  prefs: []
  type: TYPE_NORMAL
- en: The demo-quality trap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s becoming a pattern: teams use AI to rapidly build impressive demos. The
    happy path works beautifully. Investors and social networks are wowed. But when
    real users start clicking around? That’s when things fall apart.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve seen this firsthand: error messages that make no sense to normal users,
    edge cases that crash the application, confusing UI states that never got cleaned
    up, accessibility completely overlooked, and performance issues on slower devices.
    These aren’t just low-priority bugs—they’re the difference between software people
    tolerate and software people love.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating truly self-serve software—the kind where users never need to contact
    support—requires a different mindset, one that’s all about the lost art of polish.
    You need to be obsessing over error messages; testing on slow connections and
    with real, nontechnical users; making features discoverable; and handling every
    edge case gracefully. This kind of attention to detail (perhaps) can’t be AI-generated.
    It comes from empathy, experience, and caring deeply about craft.
  prefs: []
  type: TYPE_NORMAL
- en: 'What Actually Works: Practical Workflow Patterns'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive into coding in [Part II](part02.html#part02) of this book, we
    need to talk about modern development practices and how AI-assisted coding fits
    within a team workflow. Software development is more than writing code, after
    all—it’s a whole workflow that includes planning, collaboration, testing, deployment,
    and maintenance. And vibe coding isn’t a standalone novelty—it can be woven into
    agile methodologies and DevOps practices, augmenting the team’s productivity while
    preserving quality and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll explore how team members can collectively use vibe-coding
    tools without stepping on each other’s toes, how to balance AI suggestions with
    human insight, and how continuous integration/continuous delivery (CI/CD) pipelines
    can incorporate AI or adapt to AI-generated code. I’ll also touch on important
    considerations like version-control strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'After observing dozens of teams, here are three patterns I’ve seen work consistently
    in both solo and team workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: AI as first drafter
  prefs: []
  type: TYPE_NORMAL
- en: The AI model generates the initial code and developers then refine, refactor,
    and test it
  prefs: []
  type: TYPE_NORMAL
- en: AI as pair programmer
  prefs: []
  type: TYPE_NORMAL
- en: Developer and AI are in constant conversation, with tight feedback loops, frequent
    code review, and minimal context provided
  prefs: []
  type: TYPE_NORMAL
- en: AI as validator
  prefs: []
  type: TYPE_NORMAL
- en: Developers still write the initial code and then use AI to validate, test, and
    improve it (see [Figure 3-2](#ch03_figure_2_1752630043194352))
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bevc_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2\. AI validation workflow: developers write initial code; AI systems
    analyze for bugs and security issues, then suggest improvements; and developers
    review and apply recommended changes.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section, I’ll walk you through each pattern in turn, discussing workflows
    and tips for success.
  prefs: []
  type: TYPE_NORMAL
- en: AI as first drafter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to ensure everyone on the team is on the same page before you
    ask your AI model to draft any code. Communication is key so that developers don’t
    ask their AI assistants to do redundant tasks or generate conflicting implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In daily stand-ups (a staple of agile workflows), it’s worth discussing not
    just what you’re working on but also whether you plan to use AI for certain tasks.
    For example, two developers might be working on different features that both involve
    a utility function for date formatting. If both ask the AI to create a `formatDate`
    helper, you might end up with two similar functions. Coordinating up front (“I’ll
    generate a date utility we can both use”) can prevent duplication.
  prefs: []
  type: TYPE_NORMAL
- en: Teams that successfully integrate AI tools often start by agreeing on coding
    standards and prompting practices. For example, the team might decide on a consistent
    style (linting rules, project conventions) and even feed those guidelines into
    their AI tools (some assistants allow providing style preferences or example code
    to steer outputs). As [noted in Codacy’s blog](https://oreil.ly/FeEN_), by familiarizing
    the AI with the team’s coding standards, you get generated code that is more uniform
    and easier for everyone to work with. On a practical level, this could mean having
    a section in your project README for “AI Usage Tips,” where you note things like
    “We use functional components only” or “Prefer using Fetch API over Axios,” which
    developers can keep in mind when prompting AI.
  prefs: []
  type: TYPE_NORMAL
- en: Another practice is to use your tools’ *collaboration features*, if available.
    Some AI-assisted IDEs allow users to share their AI sessions or at least the prompts
    they use. If Developer A got a great result with a prompt for a complex component,
    sharing that prompt with Developer B (perhaps via the issue tracker or a team
    chat) can save time and ensure consistency.
  prefs: []
  type: TYPE_NORMAL
- en: As for using version control, the fundamentals remain—with a twist. Using Git
    (or another version control system) is nonnegotiable in modern development, and
    that doesn’t change with vibe coding. In fact, version control becomes even more
    crucial when AI is generating code rapidly. Commits act as the safety net to catch
    AI missteps; if an AI-generated change breaks something, you can revert to a previous
    commit.
  prefs: []
  type: TYPE_NORMAL
- en: One strategy is to commit more frequently when using AI assistance. Each time
    the AI produces a significant chunk of code (like generating a feature or doing
    some major refactoring) that you accept, consider making a commit with a clear
    message. Frequent commits ensure that if you need to bisect issues or undo a portion
    of AI-introduced code, the history is granular enough.
  prefs: []
  type: TYPE_NORMAL
- en: Also, try to isolate different AI-introduced changes. If you let the AI make
    many changes across different areas and commit them all together, it’s harder
    to disentangle if something goes wrong. For example, if you use an agent to optimize
    performance and it also tweaks some UI texts, commit those separately. (Your two
    commit messages might be “Optimize list rendering performance [AI-assisted]” and
    “Update UI copy for workout completion message [AI-assisted]”). Descriptive commit
    messages are important; some teams even tag commits that had heavy AI involvement,
    just for traceability. It’s not about blame but about understanding the origin
    of code—a commit tagged with “[AI]” might signal to a reviewer that the code could
    use an extra thorough review for edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, the team should treat AI usage as a normal part of the development
    conversation: share experiences, successful techniques, and warnings about what
    not to do (like “Copilot suggests using an outdated library for X, so be careful
    with that”).'
  prefs: []
  type: TYPE_NORMAL
- en: Review and refinement are crucial to this pattern. Developers should manually
    review and refactor the code for modularity, add comprehensive error handling,
    write thorough tests, and document key decisions as they refine the code. The
    next chapter goes into detail about these processes.
  prefs: []
  type: TYPE_NORMAL
- en: AI as pair programmer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditional pair programming involves two humans collaborating at one workstation.
    With the advent of AI, a hybrid approach has emerged: one human developer working
    alongside an AI assistant. This setup can be particularly effective, offering
    a blend of human intuition and machine efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: In a human-AI pairing, the developer interacts with the AI to generate code
    suggestions while also reviewing and refining the output. This dynamic allows
    the human to leverage the AI’s speed in handling repetitive tasks, such as writing
    boilerplate code or generating test cases, while maintaining oversight to ensure
    code quality and relevance.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, when integrating a new library, a developer might prompt the AI
    to draft the initial integration code. The developer then reviews the AI’s suggestions,
    cross-referencing with official documentation to verify accuracy. This process
    not only accelerates development but also facilitates knowledge acquisition, as
    the developer engages deeply with both the AI’s output and the library’s intricacies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compare this to traditional human-human pair programming:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Human-AI pairing* offers rapid code generation and can handle mundane tasks
    efficiently. It’s particularly beneficial for solo developers or when team resources
    are limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Human-human pairing* excels in complex problem-solving scenarios, where nuanced
    understanding and collaborative brainstorming are essential. It fosters shared
    ownership and collective code comprehension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both approaches have their merits, and your choice between them can be guided
    by the project’s complexity, resource availability, and the specific goals of
    the development process.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for AI pair programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To maximize the benefits of AI-assisted development, consider the following
    practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Initiate new AI sessions for distinct tasks
  prefs: []
  type: TYPE_NORMAL
- en: This helps maintain context clarity and ensures the AI’s suggestions are relevant
    to the specific task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Keep prompts focused and concise
  prefs: []
  type: TYPE_NORMAL
- en: Providing clear and specific instructions enhances the quality of the AI’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Review and commit changes frequently
  prefs: []
  type: TYPE_NORMAL
- en: Regularly integrating and testing AI-generated code helps catch issues early
    and maintains project momentum.
  prefs: []
  type: TYPE_NORMAL
- en: Maintain tight feedback loops
  prefs: []
  type: TYPE_NORMAL
- en: Continuously assess the AI’s contributions, providing corrections or refinements
    as needed to guide its learning and improve future suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: AI as validator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond code generation, AI can serve as a valuable validator, assisting in code
    review and quality assurance. AI tools can analyze code for potential bugs, security
    vulnerabilities, and adherence to best practices. For example, platforms like
    DeepCode and Snyk’s AI-powered code checker can identify issues such as missing
    input sanitization or insecure configurations, providing actionable insights directly
    within the development environment. Platforms such as Qodo and TestGPT can automatically
    generate test cases, ensuring broader coverage and reducing manual effort. And
    many AI tools can assist in monitoring application performance, detecting anomalies
    that might indicate underlying issues.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating AI validators into the development workflow, teams can enhance
    code quality, reduce the likelihood of defects, and ensure compliance with security
    standards. This proactive approach to validation complements human oversight,
    leading to more robust and reliable software. These tools enhance the efficiency
    and effectiveness of the quality assurance (QA) process by handling repetitive
    and time-consuming tasks, allowing human testers to focus on more complex and
    nuanced aspects of QA.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating AI into the development process, whether as a pair programmer
    or validator, offers opportunities to enhance productivity and code quality. By
    thoughtfully integrating these tools, developers can harness the strengths of
    both human and artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To maximize the benefits of both AI and human capabilities in QA, I recommend
    a few best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Use AI for initial assessments and preliminary scans to identify obvious issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritize human review for critical areas, such as complex functionalities,
    user experience, and AI limitations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foster an environment of continuous collaboration, where AI tools and human
    testers work in tandem, with ongoing feedback loops to improve both AI performance
    and human decision making.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Golden Rules of Vibe Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While vibe coding offers unprecedented speed and creative freedom in software
    development, its very flexibility demands a structured approach to ensure consistent
    quality and team cohesion. The rapid, intuitive nature of AI-assisted development
    can quickly lead to chaos without clear guidelines that balance creative exploration
    with engineering discipline.
  prefs: []
  type: TYPE_NORMAL
- en: These golden rules emerged from collective experience across teams who have
    successfully integrated vibe coding into their workflows. They represent hard-won
    insights about where AI excels, where it stumbles, and how human judgment remains
    essential throughout the process. Rather than constraining creativity, these principles
    create a framework within which teams can confidently experiment while maintaining
    the standards necessary for production-ready software.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules address three critical dimensions of vibe coding: the interaction
    between human and AI, the integration of AI-generated code into existing systems,
    and the cultivation of team practices that support sustainable AI-assisted development.
    By following these guidelines, teams can harness the transformative power of vibe
    coding while avoiding common pitfalls that lead to technical debt, security vulnerabilities,
    or unmaintainable codebases:'
  prefs: []
  type: TYPE_NORMAL
- en: Be specific and clear about what you want
  prefs: []
  type: TYPE_NORMAL
- en: Clearly articulate your requirements, tasks, and outcomes when interacting with
    AI. Precise prompts yield precise results.
  prefs: []
  type: TYPE_NORMAL
- en: Always validate AI output against your intent
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated code must always be checked against your original goal. Verify
    functionality, logic, and relevance before accepting.
  prefs: []
  type: TYPE_NORMAL
- en: Treat AI as a junior developer (with supervision)
  prefs: []
  type: TYPE_NORMAL
- en: Consider AI outputs as drafts that require your careful oversight. Provide feedback,
    refine, and ensure quality and correctness.
  prefs: []
  type: TYPE_NORMAL
- en: Use AI to expand your capabilities, not replace your thinking
  prefs: []
  type: TYPE_NORMAL
- en: Leverage AI to automate routine or complex tasks, but always remain actively
    engaged in problem solving and decision making.
  prefs: []
  type: TYPE_NORMAL
- en: Coordinate up front among the team before generating code
  prefs: []
  type: TYPE_NORMAL
- en: Align with your team on AI usage standards, code expectations, and practices
    before starting AI-driven development.
  prefs: []
  type: TYPE_NORMAL
- en: Treat AI usage as a normal part of the development conversation
  prefs: []
  type: TYPE_NORMAL
- en: Regularly discuss AI experiences, techniques, successes, and pitfalls with your
    team. Normalize AI as another tool for collective improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Isolate AI changes in Git by doing separate commits
  prefs: []
  type: TYPE_NORMAL
- en: Clearly identify and separate AI-generated changes within version control to
    simplify reviews, rollbacks, and tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that all code, whether human or AI-written, undergoes code review
  prefs: []
  type: TYPE_NORMAL
- en: Maintain consistent standards by subjecting all contributions to the same rigorous
    review processes, enhancing code quality and team understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t merge code you don’t understand
  prefs: []
  type: TYPE_NORMAL
- en: Never integrate AI-generated code unless you thoroughly comprehend its functionality
    and implications. Understanding is critical to maintainability and security.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize documentation, comments, and ADRs
  prefs: []
  type: TYPE_NORMAL
- en: Clearly document the rationale, functionality, and context for AI-generated
    code. Good documentation ensures long-term clarity and reduces future technical
    debt.
  prefs: []
  type: TYPE_NORMAL
- en: Share and reuse effective prompts
  prefs: []
  type: TYPE_NORMAL
- en: Document prompts that lead to high-quality AI outputs. Maintain a repository
    of proven prompts to streamline future interactions and enhance consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Regularly reflect and iterate
  prefs: []
  type: TYPE_NORMAL
- en: Periodically review and refine your AI development workflow. Use insights from
    past experiences to continuously enhance your team’s approach.
  prefs: []
  type: TYPE_NORMAL
- en: By adhering to these golden rules, your team can harness AI effectively, enhancing
    productivity while maintaining clarity, quality, and control.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The 70% problem defines the current state of AI-assisted development: these
    tools excel at generating boilerplate and routine functions but struggle with
    the final 30% that includes edge cases, architectural decisions, and production
    readiness. We’ve identified two main usage patterns—bootstrappers who rapidly
    build MVPs, and iterators who integrate AI into daily workflows—along with common
    failure patterns like the “two steps back” antipattern and the “demo-quality trap”
    where impressive prototypes fail under real-world pressure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three proven workflow patterns have emerged: AI as first drafter (generate
    then refine), AI as pair programmer (continuous collaboration), and AI as validator
    (human-written code with AI analysis). The golden rules of vibe coding provide
    essential guardrails, emphasizing clear communication, thorough validation, team
    coordination, and the nonnegotiable requirement to understand all code before
    merging it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Individual developers should choose one workflow pattern to experiment with
    systematically while implementing the golden rules in daily practice. Focus on
    developing the durable skills covered in [Chapter 4](ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362):
    system design, debugging, and architecture—rather than competing with AI on code
    generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Teams need to establish standards for AI usage, create shared repositories of
    effective prompts, and integrate AI considerations into existing agile practices.
    Regular knowledge sharing about successes and pitfalls will help teams avoid common
    traps while maximizing AI’s benefits.
  prefs: []
  type: TYPE_NORMAL
- en: As autonomous AI coding agents emerge, the human role will shift toward architectural
    oversight and strategic decision making. The next chapter explores how to maximize
    this irreplaceable human contribution, helping engineers at every level thrive
    as partners to increasingly capable AI systems rather than competitors.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch03.html#id486-marker)) This chapter is based on an essay originally
    published on my Substack newsletter. See Addy Osmani, [“The 70% Problem: Hard
    Truths About AI-Assisted Coding”](https://oreil.ly/aRKIJ), *Elevate with Addy
    Osmani*, December 4, 2024.'
  prefs: []
  type: TYPE_NORMAL
