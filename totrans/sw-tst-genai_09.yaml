- en: 8 Assisting exploratory testing with artificial intelligence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing exploratory testing charter creation using LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying opportunities for using LLMs in exploratory testing sessions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LLMs to support various activities during exploratory testing sessions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing exploratory testing session reports with LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we’ve explored how large language models (LLMs) can help us with a
    range of testing activities and artifacts that are algorithmic. Activities such
    as code and data generation have distinct syntax and formatting rules, and come
    with a certain degree of repeatability that works well with LLMs. But what about
    the more heuristic-based testing activities, such as exploratory testing? How
    can LLMs support us when we are executing testing ourselves? It’s important to
    repeat that LLMs cannot replace testing or testers, but by carefully observing
    what we’re doing during exploratory testing and knowledge of prompt engineering,
    we can selectively enhance our exploring in a way that doesn’t undermine the core
    value of exploratory testing. To do this, we’ll examine the following three aspects
    of exploratory testing and how LLMs can help: organizing exploratory testing with
    charters, performing exploratory testing, and reporting what we’ve discovered.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic and heuristic-based activities
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: When we refer to an activity as being heuristic-based, we’re specifying that
    the activity doesn’t have clear steps to carry out or is difficult to define explicitly,
    whereas algorithmic activities are more procedural in nature and can be defined
    in explicit terms. In the context of testing, test cases and scripts can be seen
    as algorithmic in nature, whereas exploratory testing is more heuristic because
    it relies on human ability to observe and analyze situations to determine next
    actions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Organizing exploratory testing with LLMs
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by focusing on how LLMs can help us identify charters for exploratory
    testing. Normally, when performing exploratory testing sessions, they are guided
    by a test charter such as:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: '| Explore flights booking with various providers to discover if all providers
    are shown in results |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: 'In this charter example, we’re following the charter template proposed by Elisabeth
    Hendrickson in her book *Explore It* (from Pragmatic Bookshelf):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s from these charters that we can frame what we want to focus our exploratory
    testing on and what to ignore. The idea is to have many different charters exploring
    features and products from different perspectives, or more specifically, different
    risks. When generating charters, we ideally want to be deriving them from risks,
    so that when we run exploratory testing based on charters, we know the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: What priority we should put our charters in (the higher the risk, the higher
    the priority)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What risks have been explored and what haven’t
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What value we are getting from each exploratory testing session
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从每次探索性测试会话中获得的价值
- en: To put this into a visual model, figure 8.1 demonstrates the relationship between
    risks, charters, and exploratory testing sessions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将其转化为视觉模型，图8.1 展示了风险、任务书和探索性测试会话之间的关系。
- en: The model shows that we first identify risks, which are then codified into charters,
    and it’s those charters that we can run multiple exploratory testing sessions
    on to see what we can learn.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型显示我们首先识别风险，然后将其编码为任务书，我们可以在这些任务书上运行多个探索性测试会话，以了解我们可以学到什么。
- en: '![](../../OEBPS/Images/CH08_F01_Winteringham2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F01_Winteringham2.png)'
- en: Figure 8.1 A visual model showing the relationship between risks, charters,
    and exploratory testing
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 显示风险、任务书和探索性测试之间关系的视觉模型
- en: Identifying risks and charters is predominantly a heuristic-based activity,
    as there is no clear pattern or procedure that can be followed to identify risks.
    It’s based on a tester and their team’s critical and lateral thinking skills.
    However, the structured nature of how we format risks into charters means there
    are times when LLMs can help augment our existing skills and increase our coverage.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 识别风险和任务书主要是一种基于启发式的方法，因为没有明确的模式或程序可以遵循来识别风险。它基于测试人员和他们的团队的关键和横向思维技能。然而，我们将风险格式化为任务书的结构化性质意味着有时LLMs可以帮助增强我们的现有技能并提高我们的覆盖率。
- en: 8.1.1 Augmenting identified risks with LLMs
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 使用LLMs增强已识别的风险
- en: Because identifying risks is a heuristic-based activity, it is subject to bias.
    This means that, at times, we can miss potential risks requiring our attention
    (for example, functional fixedness cognitive bias in which we are so focused on
    observing one event that we completely miss another). So, how do we prevent these
    gaps from appearing? Testers work hard to develop their skills to embrace and
    handle such biases and utilize testing heuristics to help us change our perspectives
    when identifying risks. But we can also use LLMs as an additional tool to help
    us consider different avenues and perhaps highlight potential risks we hadn’t
    considered.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于识别风险是一种基于启发式的方法，因此它容易受到偏见的影响。这意味着有时我们可能会错过需要我们注意的潜在风险（例如，功能固定性认知偏差，其中我们过于专注于观察一个事件，以至于完全错过了另一个）。那么，我们如何防止这些差距出现呢？测试人员努力提高他们的技能，以接受和处理这些偏见，并利用测试启发式方法来帮助我们改变在识别风险时的视角。但我们也可以使用LLMs作为额外的工具，帮助我们考虑不同的途径，并可能突出我们没有考虑到的潜在风险。
- en: 'To help demonstrate how they can help, let’s explore a sample user story that
    has been created by a team:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助展示它们如何帮助，让我们探索一个由团队创建的样本用户故事：
- en: '|  |    •   In order to manage my bookings as an administrator I want to be
    able to view a report of all my bookings   •   Acceptance Criteria:   −   Given
    I am logged in as an administrator, and I have multiple bookings made, when I
    load a report page then I am presented with my bookings in a monthly calendar
    view and the month selected is the current month   −   Given I am on the report
    page when I click on the navigation controls then I am able to move to different
    months in a year   −   Given I am on the report page when I click and drag across
    multiple dates then I am presented with a new booking form with the following
    fields&#124; firstname &#124; lastname &#124; room &#124; deposit paid &#124;  
    −   Given I have admin booking popup loaded when I complete the rest of the booking
    form then the report is reloaded and the new booking appears |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  |   •   为了作为管理员管理我的预订，我希望能够查看所有预订的报告   •   接受标准：   −   假设我已登录为管理员，并且我有多个预订，当我加载报告页面时，我可以看到我的预订以月历视图呈现，并且选中的月份是当前月份  
    −   假设我在报告页面上点击导航控件，那么我能够移动到一年中的不同月份   −   假设我在报告页面上点击并拖动多个日期，那么我会看到一个包含以下字段的新预订表单：姓名
    | 姓氏 | 房间 | 已支付押金 |   −   假设我在完成预订表单的其余部分时加载了管理员预订弹出窗口，那么报告将被重新加载，并且新的预订将出现 |'
- en: 'Using examples, the user story details how we would expect the feature to work
    from a user perspective. It’s this type of user story that we would typically
    come across in a planning session (with perhaps more or less detail), and it would
    be used as a jumping-off point to start discussing how it will be built and, equally
    important, what risks could affect the quality of our feature. Typically, this
    is where testers or quality-minded individuals would take time to ask questions
    and note down potential risks for further investigation. For example, a basic
    list of risks might contain the following items:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Bookings are not shown in the report view.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report view is hard to parse.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigation controls don’t work correctly.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admin bookings can’t be submitted.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a great start, but it feels insufficient. This list emulates situations
    in which we may find ourselves struggling to identify further risks, so at this
    time, we can opt to use a prompt like the following to help expand our analysis.
    We use delimiter tactic in instructions to distinguish different data sets being
    sent:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '| You are a professional software tester. You will be provided with a user
    story delimited by three hashes and a list of identified risks delimited by three
    back ticks. Review the user story and expand the list of risks. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: 'Next, we instruct the model to work out solutions before outputting them:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: '| Check that all generated risks are not duplicates of original risks and that
    they relate to the following quality characteristics: Usability and Accuracy.
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: 'Finally, we provide data sets in delimited sections:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: '| ###<User story added here>###[PRE1] |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: 'In this prompt, we’re providing three different sets of data to help guide
    the LLM:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '*Quality characteristics*—Quality characteristics help us define what quality
    means to our end users. For example, the quality characteristic of a trading platform
    might be quite different from the characteristics of a personal assistant app.
    Adding these to our prompt helps us make explicit which areas of risks we’re interested
    in and prevents an LLM from hallucinating risks that are irrelevant or not of
    interest.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User story*—We provide the user story to give the prompt context. Once again,
    the more description we can provide around what a feature is expected to do, the
    more we will maximize the relevance of the risks that the LLM will output.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Current risks*—Finally, providing existing risks prevents the LLM from simply
    outputting information we already have. By asking the LLM to work out solutions
    before outputting risks, we ensure that the risks being output are new for consideration.
    That doesn’t mean that its output is more useful but that it helps frame what
    we’ve been thinking about.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of this prompt is to have an LLM provide additional risks that could
    be added to our existing list. For example, sending the prompt to ChatGPT returned
    the following output:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: '| Usability Risks:User Confusion: Users may not easily understand how to access
    the report page, resulting in frustration and poor user experience.Complex Calendar
    View: The monthly calendar view may be overwhelming or confusing for administrators
    if it’s not designed intuitively, impacting usability.Navigation Controls: If
    the navigation controls for moving between months are not user-friendly or responsive,
    administrators might struggle to navigate, affecting usability.Booking Form Usability:
    The new booking form generated by clicking and dragging across dates should be
    user-friendly and intuitive. If it’s not, administrators might make mistakes while
    filling it out.Lack of Feedback: If there’s no feedback or confirmation message
    after submitting a new booking, administrators may be uncertain whether the booking
    was successful or not, affecting usability.Error Handling: Poor error handling,
    such as not handling edge cases or invalid input in the booking form, could lead
    to confusion and frustration for administrators.Accuracy Risks:Missing Bookings:
    Bookings not being displayed in the report view may result in incomplete or inaccurate
    reporting, impacting data accuracy.Data Integrity: If the system doesn’t properly
    validate and save the booking data entered by administrators, it may lead to inaccurate
    data in the reports.Incorrect Month Selection: If the default month displayed
    in the report is not the current month as specified, it would be an accuracy issue.Incorrect
    Navigation: If the navigation controls don’t function as expected, administrators
    may not be able to accurately navigate to the desired month.Missing Bookings in
    Report Refresh: If new bookings don’t appear in the report after completing the
    booking form, it would affect the accuracy of the displayed data.Failed Booking
    Submission: If admin bookings cannot be submitted correctly, it would result in
    incomplete or missing data in the system, affecting data accuracy. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: The first thing to observe is that ChatGPT has effectively categorized the proposed
    risks by quality characteristics. The second observation is how each risk comes
    with a short sentence to explain it further. This can help us contextualize the
    risks it has output or spark ideas around further risks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the list of proposed risks, some stand out as useful and relevant
    to our user story, such as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: User confusion
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing bookings in report refresh
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data integrity
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Further analysis shows that some risks displayed are almost a repetition of
    one another: for example, `Navigation` `controls` in the `usability` category
    and `Incorrect Navigation` in the `accuracy` category (note that these are not
    duplicates of our original risks). Although they are worded differently, the focus
    appears to be the same: navigation controls not working properly. The combination
    of new risks and repetition demonstrates that LLMs can be a useful tool for expanding
    our risk analysis, but they do not serve as a replacement for our ability to identify
    risks. So, after reading the list of risks from ChatGPT, we may choose to expand
    our list of risks to include'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Bookings are not shown in the report view.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report view is hard to parse.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigation controls don’t work correctly.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admin bookings can’t be submitted.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion over how to use the report controls and admin booking.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Errors are not handled correctly.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our final list, we have chosen to adopt and reframe some of the risks received
    from ChatGPT and have ignored others. If we are not satisfied, we can ask ChatGPT
    to return more risks to review. However, there is a tradeoff that if we request
    more risks, we face an increased chance of repetition creeping into ChatGPT’s
    responses. Once we’re happy with our list, we can begin to convert these into
    charters and return to an LLM for further assistance for ideas.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.1
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Pick a user story or list of requirements from the backlog of a project you
    are working on. Identify some initial risks and then compile a prompt that will
    suggest additional risks to consider. If you are unaware which quality characteristics
    to use in your prompt, pick one or two from this comprehensive list of characteristics
    from *The Test Eye*, available at [https://mng.bz/XV9Y](https://mng.bz/XV9Y).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Augmenting charter lists with LLMs
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have our risks identified, we can now convert them into test charters
    to guide our exploratory testing. For example, the following risk:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: '| Bookings are not shown in the report view |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: 'can be translated into:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: '| Explore how bookings are rendered in the report viewWith a collection of
    bookingsTo discover if all bookings are shown correctly |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: This charter gives us a bit more detail about what we want to focus on, what
    tools we are going to use, and what we’re hoping to discover. As mentioned before,
    the way charters are formed follows a clear structure that, just like with risks,
    we can feed into an LLM to expand the list of charters we could consider for future
    testing.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'So once again, we create a prompt similar to our previous risk-focused prompts.
    First, we provide initial instructions and delimiter tactic to clarify where charters
    will be found:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: '| You are a professional software tester. Review the exploratory testing charters
    delimited by three hashes and suggest additional charters. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: 'Then we instruct the model to work out solutions before outputting them:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: '| Check that all generated charters are not duplicates of original charters
    and that they relate to the following quality characteristics Usability and Accuracy,
    before outputting the additional charters. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: 'The following prompt provides data sets in delimited sections:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: '| ###Explore how bookings are rendered in the report viewWith a collection
    of bookingsTo discover if all bookings are shown correctlyExplore how bookings
    are rendered in the report viewWith a very large collection of bookingsTo discover
    if large amounts of bookings are easy to readExplore navigating report viewWith
    a collection of bookingsTo discover that the navigation controls work as expectedExplore
    submitting bookingWith the admin booking formTo discover if bookings can be submitted
    by an adminExplore using core features of the report viewUsing different personasTo
    discover if the report features are easy to understandExplore different error
    outputs in the admin formUsing malformed data and proxy toolsTo discover if errors
    are handled correctly### |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: 'The prompt is structured in a similar fashion to the one where we provided
    context to the prompt in the form of quality characteristics and the current list
    of identified charters. There is also an element of few-shot prompting occurring
    as each charter follows the same template. This means that when I send the prompt
    to ChatGPT, I get the following suggested charters:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: '| Explore the responsiveness of the report viewWith varying screen sizes and
    devicesTo discover if the report view adapts well to different screen sizesExplore
    search functionality in the report viewWith a variety of search criteriaTo discover
    if users can accurately find specific bookingsExplore the accuracy of date filtering
    in the report viewWith different date ranges and time zonesTo ensure that bookings
    within the specified date range are displayed correctlyExplore user permissions
    in the admin booking formWith different user roles (admin, regular user)To ensure
    that only admins can access and submit bookings in the admin formExplore the accuracy
    of booking data storageBy submitting bookings with various data formats and inputsTo
    verify that all data is stored accurately in the system |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
- en: 'Just as with other LLM outputs, there is a range in quality in what has been
    returned. Some charters stand out as suitable candidates for additional charters,
    for example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Explore the responsiveness of the report view
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With varying screen sizes and devices
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To discover if the report view adapts well to different screen sizes
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Others, like the following one, which is beyond the scope of adding a booking
    to a report, could be discarded:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Explore search functionality in the report view
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a variety of search criteria
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To discover if users can accurately find specific bookings
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, what this demonstrates is that LLMs can help us expand our options and
    identify new avenues to explore. But they are by no means perfect. And while tweaking
    prompts to make them more reliable can certainly help, we should always be selective
    in what to take from an LLM and what to ignore, which once again can be summarized
    using the area of effect model, as shown in figure 8.2.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Our ability to think creatively and use lateral and critical thinking skills
    to identify and organize our exploratory testing is essential. In this example,
    LLMs give us a way to shake things up when we’re out of ideas.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.2
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Take the risks from the previous activity and convert some of them into charters
    using the Explore, With, Using template. Add those into the prompt we’ve just
    explored to see what suggested prompts are returned.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F02_Winteringham2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Area of effect model demonstrating how LLMs can expand our analysis
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Using LLMs during exploratory testing
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen how we can work with LLMs to organize exploratory testing. Now let’s
    take a look at how to use LLMs as we perform exploratory testing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'What makes the use of LLMs in exploratory testing so interesting is the mix
    of activities that might occur during an exploratory testing session. There are
    technical elements to a session in which we need to rely on tools to manipulate
    a system in a specific way, and there are heuristic human-driven elements where
    we employ mental heuristics and oracles to make sense of what we’ve learned and
    use it to inspire further testing. Both aspects can be supported with the use
    of LLMs, so to help us better appreciate where to get the most value, let’s explore
    a use case of an exploratory testing session in which the following charter was
    used:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: '| Explore how bookings are rendered in the report viewWith a very large collection
    of bookingsTo discover if large amounts of bookings are easy to read |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: To help contextualize the session further, the goal of the session was to test
    the rendering of the report page, as shown in figure 8.3.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F03_Winteringham2.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 The report page under test
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The calendar in the report page renders all the bookings that exist for each
    room within the application, and our goal is to learn how it functions when there
    are many bookings, specifically learning how it might affect the rendering and
    usability of the calendar. This means there were challenges to overcome around
    understanding how data is sent to the calendar, how to create it quickly, and
    what types of testing aligned with the charter we might carry out.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Establishing an understanding
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first challenge is to understand how the report data arrived into the calendar,
    which means analyzing the `Report` API code to better understand it. To do this,
    I created the following prompt. We first set out the instruction to add code comments
    and give the system time to think:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: '| You are a Java Developer. Create code comments for the code delimited by
    three hashes. Check that the code has not been modified and that comments match
    each section of the code provided before outputting the created code. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: 'Next, we provide the code to comment using the delimiter tactic:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is designed to take the service layer code that existed in the `Report`
    API and code comment it for me. Through the use of code comments, I could learn
    more about how it worked and how it built the necessary report data sent to the
    calendar (something I discovered by using tools to monitor HTTP traffic being
    sent to the calendar). This type of prompt is helpful when we want to quickly
    build up an understanding of how a part of a system works, and it can be extremely
    useful for those who might not be confident coders.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'So, I built the prompt and then sent it to ChatGPT, which returned the following
    code that was code commented:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Due to the instruction `Check that the code has not been modified` inside the
    initial prompt, the original code that was provided is the same (although it’s
    important to check just in case hallucinations slip in and modify the code). In
    addition to code, we now have code comments, which gave me a bit more context
    and detail about how the code works. This in turn allowed me to model the data-retrieval
    process into a visualization similar to the one in figure 8.4.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The model is based on the information from the prompt to determine how the data
    is created and helped clarify how I needed to go about creating the required data.
    This is a great way to rapidly build understanding, but as always, it’s important
    to ratify that understanding and mitigate risks of LLM hallucinations. That’s
    why I visually modeled the process afterward so that the visualization can be
    easily shared with others to confirm whether my assumptions were correct or needed
    updating.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F04_Winteringham2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 A model representing how the report data was created
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Creating data requirements for a session
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a deeper understanding of how the report data was built, I could move on
    to the next challenge in the exploratory testing session—creating test data. I
    required a lot of bookings (thousands, in fact), which meant
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Creating test rooms to add bookings to
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating at least 1000 test bookings to assign to each test room
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do this, I once again used an LLM to help me generate SQL data I could add
    to `seed.sql` files that existed as part of the `room` and `booking` API projects.
    If I could get the correct SQL data in these seed files, then I could start the
    product up with all the data necessary for my testing.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve already explored data creation in chapter 6, so the first prompt for
    the `Room` API will feel familiar. To create the necessary room data, I built
    the following prompt. First, we use the initial instructions and the delimiter
    tactic to provide SQL data:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: '| You are a MySQL generator. Create a MySQL script to insert 10 new records
    that follow the structure of the MySQL statement delimited by three hashes. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: 'Then we provide additional details to handle the creation of array data:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: '|    *   If the keyword ARRAY is used, use the MySQL ARRAY function to create
    related data type |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: 'The instruction concludes with the time-to-think tactic:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: '| Check that each new entry matches the correct data types and uses valid MySQL
    before outputting it. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: 'Finally, we provide the SQL data that will be expanded into further test data:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is designed to take an existing `INSERT` statement and expand it
    into multiple `INSERT` statements that can be added to my `seed.sql` file. One
    specific instruction that was needed was the handling of String arrays within
    the `INSERT` statements. Before it was added, the `ARRAY` data type wasn’t created
    correctly, which led to odd data being created for features. However, with the
    addition of the following rule to the prompt
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: the LLM was able to create the necessary data. This is a good demonstration
    of a moment where an existing prompt (like the original data creation prompt from
    chapter 6) needs tweaking to suit specific requirements.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'With the prompt created, I sent it to ChatGPT which returned the following
    data (I’ve shortened it, but the original response gave me my ten required records):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The data generated isn’t the most inspiring, but given that my focus was on
    simply having some rooms added to the API, it was enough. Adding this information
    to the `seed.sql` file, with the comments stripped, I was able to rebuild the
    `Room` API with the necessary data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Next, I turned my attention to creating 1,000 bookings to be added to the booking
    API’s `seed.sql` file. However, this posed a new challenge. Asking an LLM to respond
    with 1,000 `INSERT` statements is a taxing activity for any LLM to do. Some LLMs
    are restricted to a certain size of response, and some are simply unable to respond
    with a token count size that I required. So, I had to approach the problem differently.
    Instead of prompting an LLM to create the necessary data, I asked it to create
    a script that would generate the `INSERT` statements for me. The initial instructions
    that use the delimiter tactic were
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: '| You are a NodeJS creator. Create a script to insert 1000 new records into
    a SQL file named `seed.sql` that follows the structure of the MySQL statement
    delimited by three hashes. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: 'These are additional instructions to ensure correct data is generated:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: '|    *   RoomId must be between 1 and 10   *   Checkin and Checkout dates must
    be between 2023-11-01 and 2023-11-30 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: 'The following code provides an SQL statement that the script will ultimately
    generate:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: In this prompt, I asked an LLM to transform a SQL statement into a NodeJS script
    that would output a `seed.sql` file with all the necessary data I need. That way,
    I could run the script once and then move the newly created `seed.sql` file into
    my booking API. The choice of language was purely a personal preference, but using
    the format tactic, we could quickly update this to have the LLM produce a script
    in a different language.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending this prompt to ChatGPT triggered the LLM to return the following script
    to use:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: '|'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Copying this script into a file and then running it, I got the `seed.sql` file
    that I needed. This approach meant I was still able to get the test data I required,
    but the prompt and response were much smaller than the large response I would
    have gotten back if I had directly asked for the SQL.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Once again, with the necessary data now in my booking `seed.sql` file, I was
    able to rebuild the `booking` API, start the application with all my required
    data, and begin exploring how it coped with such a massive data set.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Exploring and investigating bugs
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the test data in place, I was able to try out a collection of test ideas
    inspired by the charter for this session, including the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '*What happens when I load the page?* I discovered that I could see bookings
    in the calendar view and that additional bookings needed to be viewed by clicking
    the “View more” text for a specific day. I also learned that the page was very
    slow to load data when lots of bookings were in place.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What happens when I work with navigation controls?* Similar to previous observations,
    the calendar was slow to load when navigating between months. But I was able to
    navigate through the calendar despite the page slowness.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What if I wanted to view more bookings?* I discovered a bug around the pop-up
    that shows additional bookings for the day flow off the top of the page, so certain
    bookings could not be read. It was also slow to load the pop-up for days with
    considerable bookings.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Can I access the calendar using a keyboard?* I found that I was able to tab
    through the main view of the calendar successfully and that hitting Enter on a
    “View more” link opens up the pop-up with additional bookings. I also found bugs
    around tabbing into a pop-up in which I was unable to focus on bookings that overflowed
    out of the bounds of the page.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What each of these ideas has in common is they were identified using mental
    heuristics I have developed after years of exploratory testing. This concept of
    the use of unconscious heuristics, ones that have been internalized through experience,
    is explored in the article “Mind the Gap,” by Richard Bradshaw and Sarah Deery:
    [https://mng.bz/yoRJ](https://mng.bz/yoRJ). It talks about how we use both conscious
    and subconscious heuristics to guide our exploratory testing. In the context of
    my exploratory testing so far, I have relied on unconscious heuristics. However,
    as I came to the point in which my ideas were running dry, I turned to more explicit
    conscious heuristics to generate further test ideas—for example, using the test
    mnemonic PAOLO.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'PAOLO is a mnemonic, created by Maik Nog, that serves as a tool to trigger
    test ideas around the screen orientation and rendering. Each letter stands for
    a different aspect of orientation and rendering to consider, specifically:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '*Portrait*— How does it look in Portrait view?'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Audio*— Are there audio artifacts to consider?'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objects*— What other objects are in view that need considering?'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Landscape*—How does it look in the Landscape view?'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Overlay*—Are there overlays to consider?'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The value of mnemonics is that they help trigger different ideas for different
    individuals—which is a blessing and a curse as we are still somewhat reliant on
    our personal experiences and biases to generate new ideas. So, to help break out
    of my thinking, I created a new prompt using the PAOLO mnemonic to see what ideas
    I might get from an LLM. In the first prompt, we set instructions, use the delimiter
    tactic, and provide context:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: '| You are an exploratory tester. Using the list of criteria delimited by three
    hashes, suggest different test ideas for each list item to try out a report calendar
    that has many bookings. Format them by mentioning the list item first, followed
    by the suggestions. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: 'Then I use few-shot tactics to set out the format I want the responses in:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: '| For example:Navigation - Can the keyboard be used to navigate the report
    calendar. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: 'Next, I provide the mnemonic in the delimited section:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: '| ###   *   Portrait   *   Audio   *   Objects   *   Landscape   *   Overlay###
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: 'In this prompt, I need to provide some context around what I want test ideas
    generated for. Omitting these details would likely result in a fairly generic
    response or risk hallucinations. So, I provided details to improve the ideas I
    might receive. Then I provided the written-out mnemonic to frame the response
    and, if I wanted to, I could change the mnemonic to see what other ideas an LLM
    might return. Sending this prompt to ChatGPT returned a large collection of ideas,
    such as these notable ones:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: '| Portrait   −   UI Responsiveness: Verify how the report calendar displays
    in portrait mode across different screen sizes and resolutions.   −   Interaction
    Testing: Test the functionality of adding/editing/deleting bookings while in portrait
    mode.   −   Scrolling Behavior: Check if scrolling through the calendar in portrait
    mode is smooth and doesn’t cause any glitches or overlaps between bookings.  
    −   Portrait-Specific Features: Explore if there are any unique features or options
    specific to portrait mode and validate their functionality.   −   Text Legibility:
    Ensure that text and labels within the calendar remain clear and legible when
    the device is switched to portrait orientation.Landscape   −   Orientation Switch:
    Test the transition between portrait and landscape modes and ensure the calendar
    adjusts smoothly without losing data or causing display issues.   −   Bookings
    Layout: Verify the layout and arrangement of bookings in landscape mode, ensuring
    they remain organized and easily readable.   −   Multi-Day Bookings: Check how
    multi-day bookings display and behave when the device is in landscape orientation.  
    −   UI Consistency: Ensure that all features and functionalities available in
    portrait mode are also accessible and functional in landscape mode.   −   Rotation
    Speed: Test if the calendar responds quickly to changes in device orientation
    without lagging or delays. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: 'From this list, I hand-picked some of the ideas that stood out to me, specifically:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: '| How the report calendar displays in portrait mode across different screen
    sizes and resolutions. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: 'This triggered the idea of zooming in and out of the calendar to observe what
    would happen, resulting in the discovery of performance problems when zooming
    out and the calendar hiding the Show More Buttons link when zoomed in. The next
    prompt idea was:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: '| Test if the calendar responds quickly to changes in device orientation without
    lagging or delays. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: 'This triggered the question: What if I try out different screen sizes? Attempting
    to view the calendar with different mobile screen sizes found yet more performance
    problems as the page attempted to organize itself for the new view. However, the
    calendar ultimately did handle different views nicely. Finally, I chose the prompt
    idea:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: '| Ensure that text and labels within the calendar remain clear and legible
    when the device is switched to portrait orientation. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: I followed this suggestion more explicitly and discovered that, on mobile screen
    sizes, text is tiny and hard to read.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: There were more ideas I used and more that I could potentially explore, but
    as this was a session to demonstrate the value of LLMs in exploratory testing,
    I chose to end the session and collate my notes for future reporting.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Using LLMs to assist exploratory testing
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This use case demonstrates again how LLMs can be employed to fast-track us through
    specific technical needs, such as data generation, and act as a suggestion engine
    when we need new ideas. At no point was the LLM in charge of the exploratory testing.
    It was utilized to help me as I progressed—all of which can be defined, once again,
    using the area of effect model shown in figure 8.5.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The key skill is to be able to identify when an LLM could be of use to us in
    a way that doesn’t disrupt the flow of exploratory testing. This is where creating
    a library of prompts can become useful. By building a library of prompts, we begin
    to identify the pain points at which LLMs can help solidify our appreciation of
    when to use them. Then, once we’re in a session, we can quickly copy in a prompt
    of our choosing, generate the required response, and move on rapidly.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F05_Winteringham2.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 The area of effect model covering the roles in exploratory testing
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The key skill is to be able to identify when an LLM could be of use to us in
    a way that doesn’t disrupt the flow of exploratory testing. This is where creating
    a library of prompts can become useful. By building a library of prompts, we begin
    to identify the pain points at which LLMs can help solidify our appreciation of
    when to use them. Then, once we’re in a session, we can quickly copy in a prompt
    of our choosing, generate the required response, and move on rapidly.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.3
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Consider an exploratory testing session you have carried out in the past, or
    a situation in which you had to debug a complex problem. Write down different
    ways in which an LLM might have been of benefit in performing your exploratory
    testing or problem solving.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Summarizing testing notes with LLMs
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once an exploratory testing session is concluded, the results need to be reported
    to relevant individuals, which can be challenging, unlike the reporting of other
    testing activities. Given that our goal is to share what we’ve learned, it can
    be difficult to distill all the details of a session into a succinct report that
    shares details such as
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: How we approached our exploratory testing sessions
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What setup was required
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What testing we did
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we learned from our testing
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What bugs were discovered
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, we create notes as we explore to capture the aforementioned details.
    But to ensure that our note-taking doesn’t overwhelm or distract us from our exploring,
    we tend not to add too much detail. This works well if we are debriefing after
    a session. We can share what we’ve learned by using our notes as a trigger to
    discuss certain aspects of our testing. The problem is that if we are asked to
    share details about a session that was run in the past, our notes can go only
    so far as to prompt our memory.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'So, as an alternative, we can turn to LLMs to have them create a *testing story,*
    a type of report that tells the story of how some testing was carried out and
    what was learned during it. To do this, we can create the following prompt:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: '| You are an exploratory tester. Convert the exploratory testing notes delimited
    by three hashes and convert them into a test story that details what happened
    in the exploratory testing sessions.   *   Ensure that all bugs identified are
    listed at the end of the story for quick reference.###<Enter original testing
    notes>### |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: In this prompt, we’re asking the LLM to convert our testing notes into a testing
    story. In the case of my exploratory testing session, the notes were written in
    Markdown (see appendix C for the full notes) and would be copied into the delimited
    section found in the prompt (again, see appendix C for the full notes). The prompt
    also contains additional instructions to list the bugs found as a separate list,
    which might be useful when discussing them with our team. Copying my testing notes
    from Appendix C into the delimited section and sending the results to ChatGPT
    returned the following response.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: '| Test Story: Exploring Bookings Rendering in Report ViewIn a recent exploratory
    testing session, I delved into the rendering of bookings within the report view
    interface, focusing on the display of a large volume of bookings. The primary
    goal was to assess the readability and functionality when dealing with extensive
    booking data.Session OverviewI commenced the session by analyzing the code structures
    in the APIs associated with rooms and bookings to comprehend data generation for
    the report page. Here’s a breakdown of my exploration:Data Setup:   *   Explored
    generating test data for the report page, resulting in creating test data for
    both the room and booking APIs.   *   Encountered challenges in rebuilding the
    booking API due to test data integration issues, necessitating direct manipulation
    via the IDE.Investigation:   *   Loading Page Performance:   *   Observed significant
    slowness in loading the page when a substantial number of bookings were present.  
    *   Navigation Controls:   *   Noted slowness in the calendar view when navigating
    between busy and quiet months, although navigation remained functional.   *  
    Viewing Bookings:   *   Discovered issues with the popup displaying bookings,
    causing overflow and making certain bookings unreadable.   *   Encountered delays
    in loading the popup for days with numerous bookings.   *   Creating New Bookings:  
    *   Identified a delay issue when canceling the creation of a new booking, impacting
    the calendar component’s rendering time.   *   Keyboard Accessibility:   *   Successful
    tab navigation through the calendar but faced challenges with tabbing into and
    navigating within popups.Further Explorations:   *   Tested the calendar’s behavior
    under different scenarios, such as resizing, zooming, and device orientation changes,
    revealing multiple rendering and display issues across various screen sizes and
    orientations.Bugs Identified:   1.  Page loading significantly slows down with
    a high volume of bookings.   2.  Calendar navigation experiences slowness when
    transitioning between busy and quiet months.   3.  Popup for viewing bookings
    overflows and obstructs readability for certain bookings.   4.  Delay issue when
    canceling the creation of a new booking affects calendar rendering.   5.  Accessibility
    issues with keyboard navigation within popups.   6.  Slow rendering and content
    disappearance when resizing and zooming the calendar.   7.  Text readability problems
    on mobile devices due to small font size.   8.  Inability to perform click-and-drag
    actions for admin booking on mobile devices.These findings indicate several areas
    that require attention to enhance the performance, usability, and display of the
    booking system within the report view. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: The LLM has managed to successfully convert my exploratory testing notes, which
    are a mix of headers and short sentences and are somewhat structureless in form
    (the raw notes can be found in appendix C for comparison), into a much richer
    report. It starts with a summary of the exploratory testing as a whole and what
    my intention was before going into the session details.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Reviewing this report demonstrates a choice that can be made when using LLMs.
    The report shows heavy use of bullet points and numbered lists, a pattern that
    is quite common for ChatGPT. However, we could fix this by applying additional
    instructions or specific template structures to the prompt. For example, if we
    updated our prompt to use the Cornell method ([https://mng.bz/M1M2](https://mng.bz/M1M2)):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: '| You are an exploratory tester. Convert the exploratory testing notes delimited
    by three hashes into a report using the Cornell method of note taking.###<Enter
    original testing notes>### |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: 'We get a different type of report which has the title, cues, notes, and summary
    structure that can be found using the Cornell method:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: '| Report on Exploratory Testing: Bookings Rendering in Report ViewSummary:The
    exploratory testing aimed to assess the rendering of bookings in the report view,
    particularly focusing on handling large volumes of bookings. The examination involved
    investigating page loading times, navigation controls, creating new bookings,
    keyboard accessibility, responsiveness to screen size changes, device orientations,
    and text legibility in both portrait and landscape modes.Cue Column:Charter: Explore
    bookings’ rendering in the report view with a vast collection of bookings to determine
    readability.Session Notes:   •   Data Creation for APIs:   −   Analyzed code in
    APIs (room and booking) to understand data generation.   −   Strategies devised
    to generate test data for the room and booking APIs using SQL and scripts.   •  
    Investigations:   −   Loaded page: Noticed slow loading times for pages with numerous
    bookings.   −   Navigation Controls: Page sluggishness observed when transitioning
    from busy to quiet months.   −   Viewing More Bookings: Popup issues detected
    - overflow and slow loading.   −   Creating New Bookings: Lag experienced after
    canceling the booking action.   −   Keyboard Accessibility: Successful tab navigation,
    issues with tabbing into popups.   •   PAOLO Mnemonic:   −   Portrait Mode: Calendar
    responsiveness on different screen sizes and resolutions assessed.   −   Zooming
    In/Out: Issues identified with slow resizing and content hiding.   −   Device
    Orientation: Handling of screen resizing and orientation changes examined.   −  
    Text Legibility: Problems noted with tiny text on mobile screens.Notes Column:  
    •   Issues/Bugs:   −   Slow page loading with numerous bookings.   −   Sluggishness
    navigating between busy and quiet months.   −   Popup display issues - overflow,
    slow loading.   −   Lag after canceling a booking action.   −   Tabbing issues
    with popups, especially on overflowed content.   −   Slow resizing, content hiding
    on zooming in/out.   −   Messy initial layout on screen resizing.   −   Tiny text
    on mobile screens affecting readability.   −   Inability to click and drag on
    calendar for admin bookings in mobile view.Summary Column:   •   Key Findings:  
    −   Performance issues with page loading and navigation controls.   −   Popup
    display problems affecting user interaction.   −   Resizing and text legibility
    concerns on mobile devices.   −   Limited functionality for admin bookings on
    mobile view.Evaluation Column:   •   Implications:   −   Performance optimizations
    needed for smoother page loading.   −   Popup display improvements for better
    user experience.   −   Responsive design adjustments required for various screen
    sizes and orientations.   −   Text size adjustments for better readability on
    smaller screens.Action Column:   •   Action Items:   −   Optimize page loading
    times for better performance.   −   Prioritize fixing popup display issues for
    enhanced usability.   −   Implement responsive design enhancements for different
    screen sizes.   −   Adjust text sizes for improved readability on mobile devices.Reflection:The
    exploratory testing revealed critical issues related to performance, usability,
    and responsiveness in rendering bookings within the report view. Addressing these
    findings would significantly improve the overall user experience and functionality
    of the system. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: Experimentation is key here, but what this demonstrates is that LLMs have the
    potential to support us in ways that other, more traditional tools are unable
    to. Reporting sometimes gets short shrift in testing because rapidly sharing a
    wealth of knowledge can be difficult, which encourages teams to rely on more quantitative,
    test script-based reports that we normally see. However, with the use of LLMs,
    we can now structure and present reports that give a fuller picture of what we’ve
    learned during exploratory testing, sharing the rich information that hides behind
    the numbers of how many test scripts or sessions have been run.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.4
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Create a testing report based on notes you have captured in the past. Copy the
    notes into one of the prompts we’ve explored in this part of the chapter to see
    what sort of report is produced. Reflect on whether it’s of use to you or if the
    prompt needs tweaking to improve the reporting.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs can be used in the process of identifying risks and charters for exploratory
    testing.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When identifying risks, biases such as functional fixedness can make us miss
    potential risks to explore.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use LLMs to analyze risks we’ve already discovered and then propose additional
    risks for further consideration.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risks generated by an LLM will contain a mix of suggestions that may or may
    not be useful.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can carry out a similar process of requesting LLMs to suggest additional
    test charters.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of exploratory testing and LLMs, the goal is to use LLMs to shake
    up our thinking and offer new perspectives, not to accept them blindly as additional
    risks and charters.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to help support activities in exploratory testing sessions.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use LLMs to transform code and add code comments, which can help improve
    our understanding.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also use LLMs to create required test data by asking them to output test
    data for us or create scripts that we can use to generate our data.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining prompts with testing heuristics can generate suggested test ideas.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can determine which of the suggested test ideas are of use and discard the
    rest.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to convert testing notes into richer test reports, such as
    testing stories.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
