- en: 8 Assisting exploratory testing with artificial intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing exploratory testing charter creation using LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying opportunities for using LLMs in exploratory testing sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LLMs to support various activities during exploratory testing sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing exploratory testing session reports with LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we’ve explored how large language models (LLMs) can help us with a
    range of testing activities and artifacts that are algorithmic. Activities such
    as code and data generation have distinct syntax and formatting rules, and come
    with a certain degree of repeatability that works well with LLMs. But what about
    the more heuristic-based testing activities, such as exploratory testing? How
    can LLMs support us when we are executing testing ourselves? It’s important to
    repeat that LLMs cannot replace testing or testers, but by carefully observing
    what we’re doing during exploratory testing and knowledge of prompt engineering,
    we can selectively enhance our exploring in a way that doesn’t undermine the core
    value of exploratory testing. To do this, we’ll examine the following three aspects
    of exploratory testing and how LLMs can help: organizing exploratory testing with
    charters, performing exploratory testing, and reporting what we’ve discovered.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic and heuristic-based activities
  prefs: []
  type: TYPE_NORMAL
- en: When we refer to an activity as being heuristic-based, we’re specifying that
    the activity doesn’t have clear steps to carry out or is difficult to define explicitly,
    whereas algorithmic activities are more procedural in nature and can be defined
    in explicit terms. In the context of testing, test cases and scripts can be seen
    as algorithmic in nature, whereas exploratory testing is more heuristic because
    it relies on human ability to observe and analyze situations to determine next
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Organizing exploratory testing with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by focusing on how LLMs can help us identify charters for exploratory
    testing. Normally, when performing exploratory testing sessions, they are guided
    by a test charter such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Explore flights booking with various providers to discover if all providers
    are shown in results |'
  prefs: []
  type: TYPE_TB
- en: 'In this charter example, we’re following the charter template proposed by Elisabeth
    Hendrickson in her book *Explore It* (from Pragmatic Bookshelf):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s from these charters that we can frame what we want to focus our exploratory
    testing on and what to ignore. The idea is to have many different charters exploring
    features and products from different perspectives, or more specifically, different
    risks. When generating charters, we ideally want to be deriving them from risks,
    so that when we run exploratory testing based on charters, we know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What priority we should put our charters in (the higher the risk, the higher
    the priority)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What risks have been explored and what haven’t
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What value we are getting from each exploratory testing session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To put this into a visual model, figure 8.1 demonstrates the relationship between
    risks, charters, and exploratory testing sessions.
  prefs: []
  type: TYPE_NORMAL
- en: The model shows that we first identify risks, which are then codified into charters,
    and it’s those charters that we can run multiple exploratory testing sessions
    on to see what we can learn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 A visual model showing the relationship between risks, charters,
    and exploratory testing
  prefs: []
  type: TYPE_NORMAL
- en: Identifying risks and charters is predominantly a heuristic-based activity,
    as there is no clear pattern or procedure that can be followed to identify risks.
    It’s based on a tester and their team’s critical and lateral thinking skills.
    However, the structured nature of how we format risks into charters means there
    are times when LLMs can help augment our existing skills and increase our coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Augmenting identified risks with LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because identifying risks is a heuristic-based activity, it is subject to bias.
    This means that, at times, we can miss potential risks requiring our attention
    (for example, functional fixedness cognitive bias in which we are so focused on
    observing one event that we completely miss another). So, how do we prevent these
    gaps from appearing? Testers work hard to develop their skills to embrace and
    handle such biases and utilize testing heuristics to help us change our perspectives
    when identifying risks. But we can also use LLMs as an additional tool to help
    us consider different avenues and perhaps highlight potential risks we hadn’t
    considered.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help demonstrate how they can help, let’s explore a sample user story that
    has been created by a team:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |    •   In order to manage my bookings as an administrator I want to be
    able to view a report of all my bookings   •   Acceptance Criteria:   −   Given
    I am logged in as an administrator, and I have multiple bookings made, when I
    load a report page then I am presented with my bookings in a monthly calendar
    view and the month selected is the current month   −   Given I am on the report
    page when I click on the navigation controls then I am able to move to different
    months in a year   −   Given I am on the report page when I click and drag across
    multiple dates then I am presented with a new booking form with the following
    fields&#124; firstname &#124; lastname &#124; room &#124; deposit paid &#124;  
    −   Given I have admin booking popup loaded when I complete the rest of the booking
    form then the report is reloaded and the new booking appears |'
  prefs: []
  type: TYPE_TB
- en: 'Using examples, the user story details how we would expect the feature to work
    from a user perspective. It’s this type of user story that we would typically
    come across in a planning session (with perhaps more or less detail), and it would
    be used as a jumping-off point to start discussing how it will be built and, equally
    important, what risks could affect the quality of our feature. Typically, this
    is where testers or quality-minded individuals would take time to ask questions
    and note down potential risks for further investigation. For example, a basic
    list of risks might contain the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: Bookings are not shown in the report view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report view is hard to parse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigation controls don’t work correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admin bookings can’t be submitted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a great start, but it feels insufficient. This list emulates situations
    in which we may find ourselves struggling to identify further risks, so at this
    time, we can opt to use a prompt like the following to help expand our analysis.
    We use delimiter tactic in instructions to distinguish different data sets being
    sent:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a professional software tester. You will be provided with a user
    story delimited by three hashes and a list of identified risks delimited by three
    back ticks. Review the user story and expand the list of risks. |'
  prefs: []
  type: TYPE_TB
- en: 'Next, we instruct the model to work out solutions before outputting them:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Check that all generated risks are not duplicates of original risks and that
    they relate to the following quality characteristics: Usability and Accuracy.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, we provide data sets in delimited sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| ###<User story added here>###[PRE1] |'
  prefs: []
  type: TYPE_TB
- en: 'In this prompt, we’re providing three different sets of data to help guide
    the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Quality characteristics*—Quality characteristics help us define what quality
    means to our end users. For example, the quality characteristic of a trading platform
    might be quite different from the characteristics of a personal assistant app.
    Adding these to our prompt helps us make explicit which areas of risks we’re interested
    in and prevents an LLM from hallucinating risks that are irrelevant or not of
    interest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User story*—We provide the user story to give the prompt context. Once again,
    the more description we can provide around what a feature is expected to do, the
    more we will maximize the relevance of the risks that the LLM will output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Current risks*—Finally, providing existing risks prevents the LLM from simply
    outputting information we already have. By asking the LLM to work out solutions
    before outputting risks, we ensure that the risks being output are new for consideration.
    That doesn’t mean that its output is more useful but that it helps frame what
    we’ve been thinking about.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of this prompt is to have an LLM provide additional risks that could
    be added to our existing list. For example, sending the prompt to ChatGPT returned
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Usability Risks:User Confusion: Users may not easily understand how to access
    the report page, resulting in frustration and poor user experience.Complex Calendar
    View: The monthly calendar view may be overwhelming or confusing for administrators
    if it’s not designed intuitively, impacting usability.Navigation Controls: If
    the navigation controls for moving between months are not user-friendly or responsive,
    administrators might struggle to navigate, affecting usability.Booking Form Usability:
    The new booking form generated by clicking and dragging across dates should be
    user-friendly and intuitive. If it’s not, administrators might make mistakes while
    filling it out.Lack of Feedback: If there’s no feedback or confirmation message
    after submitting a new booking, administrators may be uncertain whether the booking
    was successful or not, affecting usability.Error Handling: Poor error handling,
    such as not handling edge cases or invalid input in the booking form, could lead
    to confusion and frustration for administrators.Accuracy Risks:Missing Bookings:
    Bookings not being displayed in the report view may result in incomplete or inaccurate
    reporting, impacting data accuracy.Data Integrity: If the system doesn’t properly
    validate and save the booking data entered by administrators, it may lead to inaccurate
    data in the reports.Incorrect Month Selection: If the default month displayed
    in the report is not the current month as specified, it would be an accuracy issue.Incorrect
    Navigation: If the navigation controls don’t function as expected, administrators
    may not be able to accurately navigate to the desired month.Missing Bookings in
    Report Refresh: If new bookings don’t appear in the report after completing the
    booking form, it would affect the accuracy of the displayed data.Failed Booking
    Submission: If admin bookings cannot be submitted correctly, it would result in
    incomplete or missing data in the system, affecting data accuracy. |'
  prefs: []
  type: TYPE_TB
- en: The first thing to observe is that ChatGPT has effectively categorized the proposed
    risks by quality characteristics. The second observation is how each risk comes
    with a short sentence to explain it further. This can help us contextualize the
    risks it has output or spark ideas around further risks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the list of proposed risks, some stand out as useful and relevant
    to our user story, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: User confusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing bookings in report refresh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Further analysis shows that some risks displayed are almost a repetition of
    one another: for example, `Navigation` `controls` in the `usability` category
    and `Incorrect Navigation` in the `accuracy` category (note that these are not
    duplicates of our original risks). Although they are worded differently, the focus
    appears to be the same: navigation controls not working properly. The combination
    of new risks and repetition demonstrates that LLMs can be a useful tool for expanding
    our risk analysis, but they do not serve as a replacement for our ability to identify
    risks. So, after reading the list of risks from ChatGPT, we may choose to expand
    our list of risks to include'
  prefs: []
  type: TYPE_NORMAL
- en: Bookings are not shown in the report view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report view is hard to parse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigation controls don’t work correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admin bookings can’t be submitted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion over how to use the report controls and admin booking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Errors are not handled correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our final list, we have chosen to adopt and reframe some of the risks received
    from ChatGPT and have ignored others. If we are not satisfied, we can ask ChatGPT
    to return more risks to review. However, there is a tradeoff that if we request
    more risks, we face an increased chance of repetition creeping into ChatGPT’s
    responses. Once we’re happy with our list, we can begin to convert these into
    charters and return to an LLM for further assistance for ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.1
  prefs: []
  type: TYPE_NORMAL
- en: Pick a user story or list of requirements from the backlog of a project you
    are working on. Identify some initial risks and then compile a prompt that will
    suggest additional risks to consider. If you are unaware which quality characteristics
    to use in your prompt, pick one or two from this comprehensive list of characteristics
    from *The Test Eye*, available at [https://mng.bz/XV9Y](https://mng.bz/XV9Y).
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Augmenting charter lists with LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have our risks identified, we can now convert them into test charters
    to guide our exploratory testing. For example, the following risk:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Bookings are not shown in the report view |'
  prefs: []
  type: TYPE_TB
- en: 'can be translated into:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Explore how bookings are rendered in the report viewWith a collection of
    bookingsTo discover if all bookings are shown correctly |'
  prefs: []
  type: TYPE_TB
- en: This charter gives us a bit more detail about what we want to focus on, what
    tools we are going to use, and what we’re hoping to discover. As mentioned before,
    the way charters are formed follows a clear structure that, just like with risks,
    we can feed into an LLM to expand the list of charters we could consider for future
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So once again, we create a prompt similar to our previous risk-focused prompts.
    First, we provide initial instructions and delimiter tactic to clarify where charters
    will be found:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a professional software tester. Review the exploratory testing charters
    delimited by three hashes and suggest additional charters. |'
  prefs: []
  type: TYPE_TB
- en: 'Then we instruct the model to work out solutions before outputting them:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Check that all generated charters are not duplicates of original charters
    and that they relate to the following quality characteristics Usability and Accuracy,
    before outputting the additional charters. |'
  prefs: []
  type: TYPE_TB
- en: 'The following prompt provides data sets in delimited sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| ###Explore how bookings are rendered in the report viewWith a collection
    of bookingsTo discover if all bookings are shown correctlyExplore how bookings
    are rendered in the report viewWith a very large collection of bookingsTo discover
    if large amounts of bookings are easy to readExplore navigating report viewWith
    a collection of bookingsTo discover that the navigation controls work as expectedExplore
    submitting bookingWith the admin booking formTo discover if bookings can be submitted
    by an adminExplore using core features of the report viewUsing different personasTo
    discover if the report features are easy to understandExplore different error
    outputs in the admin formUsing malformed data and proxy toolsTo discover if errors
    are handled correctly### |'
  prefs: []
  type: TYPE_TB
- en: 'The prompt is structured in a similar fashion to the one where we provided
    context to the prompt in the form of quality characteristics and the current list
    of identified charters. There is also an element of few-shot prompting occurring
    as each charter follows the same template. This means that when I send the prompt
    to ChatGPT, I get the following suggested charters:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Explore the responsiveness of the report viewWith varying screen sizes and
    devicesTo discover if the report view adapts well to different screen sizesExplore
    search functionality in the report viewWith a variety of search criteriaTo discover
    if users can accurately find specific bookingsExplore the accuracy of date filtering
    in the report viewWith different date ranges and time zonesTo ensure that bookings
    within the specified date range are displayed correctlyExplore user permissions
    in the admin booking formWith different user roles (admin, regular user)To ensure
    that only admins can access and submit bookings in the admin formExplore the accuracy
    of booking data storageBy submitting bookings with various data formats and inputsTo
    verify that all data is stored accurately in the system |'
  prefs: []
  type: TYPE_TB
- en: 'Just as with other LLM outputs, there is a range in quality in what has been
    returned. Some charters stand out as suitable candidates for additional charters,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the responsiveness of the report view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With varying screen sizes and devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To discover if the report view adapts well to different screen sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Others, like the following one, which is beyond the scope of adding a booking
    to a report, could be discarded:'
  prefs: []
  type: TYPE_NORMAL
- en: Explore search functionality in the report view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a variety of search criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To discover if users can accurately find specific bookings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, what this demonstrates is that LLMs can help us expand our options and
    identify new avenues to explore. But they are by no means perfect. And while tweaking
    prompts to make them more reliable can certainly help, we should always be selective
    in what to take from an LLM and what to ignore, which once again can be summarized
    using the area of effect model, as shown in figure 8.2.
  prefs: []
  type: TYPE_NORMAL
- en: Our ability to think creatively and use lateral and critical thinking skills
    to identify and organize our exploratory testing is essential. In this example,
    LLMs give us a way to shake things up when we’re out of ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.2
  prefs: []
  type: TYPE_NORMAL
- en: Take the risks from the previous activity and convert some of them into charters
    using the Explore, With, Using template. Add those into the prompt we’ve just
    explored to see what suggested prompts are returned.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F02_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Area of effect model demonstrating how LLMs can expand our analysis
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Using LLMs during exploratory testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen how we can work with LLMs to organize exploratory testing. Now let’s
    take a look at how to use LLMs as we perform exploratory testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'What makes the use of LLMs in exploratory testing so interesting is the mix
    of activities that might occur during an exploratory testing session. There are
    technical elements to a session in which we need to rely on tools to manipulate
    a system in a specific way, and there are heuristic human-driven elements where
    we employ mental heuristics and oracles to make sense of what we’ve learned and
    use it to inspire further testing. Both aspects can be supported with the use
    of LLMs, so to help us better appreciate where to get the most value, let’s explore
    a use case of an exploratory testing session in which the following charter was
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Explore how bookings are rendered in the report viewWith a very large collection
    of bookingsTo discover if large amounts of bookings are easy to read |'
  prefs: []
  type: TYPE_TB
- en: To help contextualize the session further, the goal of the session was to test
    the rendering of the report page, as shown in figure 8.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F03_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 The report page under test
  prefs: []
  type: TYPE_NORMAL
- en: The calendar in the report page renders all the bookings that exist for each
    room within the application, and our goal is to learn how it functions when there
    are many bookings, specifically learning how it might affect the rendering and
    usability of the calendar. This means there were challenges to overcome around
    understanding how data is sent to the calendar, how to create it quickly, and
    what types of testing aligned with the charter we might carry out.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Establishing an understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first challenge is to understand how the report data arrived into the calendar,
    which means analyzing the `Report` API code to better understand it. To do this,
    I created the following prompt. We first set out the instruction to add code comments
    and give the system time to think:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a Java Developer. Create code comments for the code delimited by
    three hashes. Check that the code has not been modified and that comments match
    each section of the code provided before outputting the created code. |'
  prefs: []
  type: TYPE_TB
- en: 'Next, we provide the code to comment using the delimiter tactic:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is designed to take the service layer code that existed in the `Report`
    API and code comment it for me. Through the use of code comments, I could learn
    more about how it worked and how it built the necessary report data sent to the
    calendar (something I discovered by using tools to monitor HTTP traffic being
    sent to the calendar). This type of prompt is helpful when we want to quickly
    build up an understanding of how a part of a system works, and it can be extremely
    useful for those who might not be confident coders.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, I built the prompt and then sent it to ChatGPT, which returned the following
    code that was code commented:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the instruction `Check that the code has not been modified` inside the
    initial prompt, the original code that was provided is the same (although it’s
    important to check just in case hallucinations slip in and modify the code). In
    addition to code, we now have code comments, which gave me a bit more context
    and detail about how the code works. This in turn allowed me to model the data-retrieval
    process into a visualization similar to the one in figure 8.4.
  prefs: []
  type: TYPE_NORMAL
- en: The model is based on the information from the prompt to determine how the data
    is created and helped clarify how I needed to go about creating the required data.
    This is a great way to rapidly build understanding, but as always, it’s important
    to ratify that understanding and mitigate risks of LLM hallucinations. That’s
    why I visually modeled the process afterward so that the visualization can be
    easily shared with others to confirm whether my assumptions were correct or needed
    updating.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F04_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 A model representing how the report data was created
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Creating data requirements for a session
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a deeper understanding of how the report data was built, I could move on
    to the next challenge in the exploratory testing session—creating test data. I
    required a lot of bookings (thousands, in fact), which meant
  prefs: []
  type: TYPE_NORMAL
- en: Creating test rooms to add bookings to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating at least 1000 test bookings to assign to each test room
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do this, I once again used an LLM to help me generate SQL data I could add
    to `seed.sql` files that existed as part of the `room` and `booking` API projects.
    If I could get the correct SQL data in these seed files, then I could start the
    product up with all the data necessary for my testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve already explored data creation in chapter 6, so the first prompt for
    the `Room` API will feel familiar. To create the necessary room data, I built
    the following prompt. First, we use the initial instructions and the delimiter
    tactic to provide SQL data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a MySQL generator. Create a MySQL script to insert 10 new records
    that follow the structure of the MySQL statement delimited by three hashes. |'
  prefs: []
  type: TYPE_TB
- en: 'Then we provide additional details to handle the creation of array data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   If the keyword ARRAY is used, use the MySQL ARRAY function to create
    related data type |'
  prefs: []
  type: TYPE_TB
- en: 'The instruction concludes with the time-to-think tactic:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Check that each new entry matches the correct data types and uses valid MySQL
    before outputting it. |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, we provide the SQL data that will be expanded into further test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is designed to take an existing `INSERT` statement and expand it
    into multiple `INSERT` statements that can be added to my `seed.sql` file. One
    specific instruction that was needed was the handling of String arrays within
    the `INSERT` statements. Before it was added, the `ARRAY` data type wasn’t created
    correctly, which led to odd data being created for features. However, with the
    addition of the following rule to the prompt
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: the LLM was able to create the necessary data. This is a good demonstration
    of a moment where an existing prompt (like the original data creation prompt from
    chapter 6) needs tweaking to suit specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the prompt created, I sent it to ChatGPT which returned the following
    data (I’ve shortened it, but the original response gave me my ten required records):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The data generated isn’t the most inspiring, but given that my focus was on
    simply having some rooms added to the API, it was enough. Adding this information
    to the `seed.sql` file, with the comments stripped, I was able to rebuild the
    `Room` API with the necessary data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I turned my attention to creating 1,000 bookings to be added to the booking
    API’s `seed.sql` file. However, this posed a new challenge. Asking an LLM to respond
    with 1,000 `INSERT` statements is a taxing activity for any LLM to do. Some LLMs
    are restricted to a certain size of response, and some are simply unable to respond
    with a token count size that I required. So, I had to approach the problem differently.
    Instead of prompting an LLM to create the necessary data, I asked it to create
    a script that would generate the `INSERT` statements for me. The initial instructions
    that use the delimiter tactic were
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a NodeJS creator. Create a script to insert 1000 new records into
    a SQL file named `seed.sql` that follows the structure of the MySQL statement
    delimited by three hashes. |'
  prefs: []
  type: TYPE_TB
- en: 'These are additional instructions to ensure correct data is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   RoomId must be between 1 and 10   *   Checkin and Checkout dates must
    be between 2023-11-01 and 2023-11-30 |'
  prefs: []
  type: TYPE_TB
- en: 'The following code provides an SQL statement that the script will ultimately
    generate:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: In this prompt, I asked an LLM to transform a SQL statement into a NodeJS script
    that would output a `seed.sql` file with all the necessary data I need. That way,
    I could run the script once and then move the newly created `seed.sql` file into
    my booking API. The choice of language was purely a personal preference, but using
    the format tactic, we could quickly update this to have the LLM produce a script
    in a different language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending this prompt to ChatGPT triggered the LLM to return the following script
    to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Copying this script into a file and then running it, I got the `seed.sql` file
    that I needed. This approach meant I was still able to get the test data I required,
    but the prompt and response were much smaller than the large response I would
    have gotten back if I had directly asked for the SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, with the necessary data now in my booking `seed.sql` file, I was
    able to rebuild the `booking` API, start the application with all my required
    data, and begin exploring how it coped with such a massive data set.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Exploring and investigating bugs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the test data in place, I was able to try out a collection of test ideas
    inspired by the charter for this session, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What happens when I load the page?* I discovered that I could see bookings
    in the calendar view and that additional bookings needed to be viewed by clicking
    the “View more” text for a specific day. I also learned that the page was very
    slow to load data when lots of bookings were in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What happens when I work with navigation controls?* Similar to previous observations,
    the calendar was slow to load when navigating between months. But I was able to
    navigate through the calendar despite the page slowness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What if I wanted to view more bookings?* I discovered a bug around the pop-up
    that shows additional bookings for the day flow off the top of the page, so certain
    bookings could not be read. It was also slow to load the pop-up for days with
    considerable bookings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Can I access the calendar using a keyboard?* I found that I was able to tab
    through the main view of the calendar successfully and that hitting Enter on a
    “View more” link opens up the pop-up with additional bookings. I also found bugs
    around tabbing into a pop-up in which I was unable to focus on bookings that overflowed
    out of the bounds of the page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What each of these ideas has in common is they were identified using mental
    heuristics I have developed after years of exploratory testing. This concept of
    the use of unconscious heuristics, ones that have been internalized through experience,
    is explored in the article “Mind the Gap,” by Richard Bradshaw and Sarah Deery:
    [https://mng.bz/yoRJ](https://mng.bz/yoRJ). It talks about how we use both conscious
    and subconscious heuristics to guide our exploratory testing. In the context of
    my exploratory testing so far, I have relied on unconscious heuristics. However,
    as I came to the point in which my ideas were running dry, I turned to more explicit
    conscious heuristics to generate further test ideas—for example, using the test
    mnemonic PAOLO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PAOLO is a mnemonic, created by Maik Nog, that serves as a tool to trigger
    test ideas around the screen orientation and rendering. Each letter stands for
    a different aspect of orientation and rendering to consider, specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Portrait*— How does it look in Portrait view?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Audio*— Are there audio artifacts to consider?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objects*— What other objects are in view that need considering?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Landscape*—How does it look in the Landscape view?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Overlay*—Are there overlays to consider?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The value of mnemonics is that they help trigger different ideas for different
    individuals—which is a blessing and a curse as we are still somewhat reliant on
    our personal experiences and biases to generate new ideas. So, to help break out
    of my thinking, I created a new prompt using the PAOLO mnemonic to see what ideas
    I might get from an LLM. In the first prompt, we set instructions, use the delimiter
    tactic, and provide context:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an exploratory tester. Using the list of criteria delimited by three
    hashes, suggest different test ideas for each list item to try out a report calendar
    that has many bookings. Format them by mentioning the list item first, followed
    by the suggestions. |'
  prefs: []
  type: TYPE_TB
- en: 'Then I use few-shot tactics to set out the format I want the responses in:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| For example:Navigation - Can the keyboard be used to navigate the report
    calendar. |'
  prefs: []
  type: TYPE_TB
- en: 'Next, I provide the mnemonic in the delimited section:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| ###   *   Portrait   *   Audio   *   Objects   *   Landscape   *   Overlay###
    |'
  prefs: []
  type: TYPE_TB
- en: 'In this prompt, I need to provide some context around what I want test ideas
    generated for. Omitting these details would likely result in a fairly generic
    response or risk hallucinations. So, I provided details to improve the ideas I
    might receive. Then I provided the written-out mnemonic to frame the response
    and, if I wanted to, I could change the mnemonic to see what other ideas an LLM
    might return. Sending this prompt to ChatGPT returned a large collection of ideas,
    such as these notable ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Portrait   −   UI Responsiveness: Verify how the report calendar displays
    in portrait mode across different screen sizes and resolutions.   −   Interaction
    Testing: Test the functionality of adding/editing/deleting bookings while in portrait
    mode.   −   Scrolling Behavior: Check if scrolling through the calendar in portrait
    mode is smooth and doesn’t cause any glitches or overlaps between bookings.  
    −   Portrait-Specific Features: Explore if there are any unique features or options
    specific to portrait mode and validate their functionality.   −   Text Legibility:
    Ensure that text and labels within the calendar remain clear and legible when
    the device is switched to portrait orientation.Landscape   −   Orientation Switch:
    Test the transition between portrait and landscape modes and ensure the calendar
    adjusts smoothly without losing data or causing display issues.   −   Bookings
    Layout: Verify the layout and arrangement of bookings in landscape mode, ensuring
    they remain organized and easily readable.   −   Multi-Day Bookings: Check how
    multi-day bookings display and behave when the device is in landscape orientation.  
    −   UI Consistency: Ensure that all features and functionalities available in
    portrait mode are also accessible and functional in landscape mode.   −   Rotation
    Speed: Test if the calendar responds quickly to changes in device orientation
    without lagging or delays. |'
  prefs: []
  type: TYPE_TB
- en: 'From this list, I hand-picked some of the ideas that stood out to me, specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| How the report calendar displays in portrait mode across different screen
    sizes and resolutions. |'
  prefs: []
  type: TYPE_TB
- en: 'This triggered the idea of zooming in and out of the calendar to observe what
    would happen, resulting in the discovery of performance problems when zooming
    out and the calendar hiding the Show More Buttons link when zoomed in. The next
    prompt idea was:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Test if the calendar responds quickly to changes in device orientation without
    lagging or delays. |'
  prefs: []
  type: TYPE_TB
- en: 'This triggered the question: What if I try out different screen sizes? Attempting
    to view the calendar with different mobile screen sizes found yet more performance
    problems as the page attempted to organize itself for the new view. However, the
    calendar ultimately did handle different views nicely. Finally, I chose the prompt
    idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Ensure that text and labels within the calendar remain clear and legible
    when the device is switched to portrait orientation. |'
  prefs: []
  type: TYPE_TB
- en: I followed this suggestion more explicitly and discovered that, on mobile screen
    sizes, text is tiny and hard to read.
  prefs: []
  type: TYPE_NORMAL
- en: There were more ideas I used and more that I could potentially explore, but
    as this was a session to demonstrate the value of LLMs in exploratory testing,
    I chose to end the session and collate my notes for future reporting.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Using LLMs to assist exploratory testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This use case demonstrates again how LLMs can be employed to fast-track us through
    specific technical needs, such as data generation, and act as a suggestion engine
    when we need new ideas. At no point was the LLM in charge of the exploratory testing.
    It was utilized to help me as I progressed—all of which can be defined, once again,
    using the area of effect model shown in figure 8.5.
  prefs: []
  type: TYPE_NORMAL
- en: The key skill is to be able to identify when an LLM could be of use to us in
    a way that doesn’t disrupt the flow of exploratory testing. This is where creating
    a library of prompts can become useful. By building a library of prompts, we begin
    to identify the pain points at which LLMs can help solidify our appreciation of
    when to use them. Then, once we’re in a session, we can quickly copy in a prompt
    of our choosing, generate the required response, and move on rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F05_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 The area of effect model covering the roles in exploratory testing
  prefs: []
  type: TYPE_NORMAL
- en: The key skill is to be able to identify when an LLM could be of use to us in
    a way that doesn’t disrupt the flow of exploratory testing. This is where creating
    a library of prompts can become useful. By building a library of prompts, we begin
    to identify the pain points at which LLMs can help solidify our appreciation of
    when to use them. Then, once we’re in a session, we can quickly copy in a prompt
    of our choosing, generate the required response, and move on rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.3
  prefs: []
  type: TYPE_NORMAL
- en: Consider an exploratory testing session you have carried out in the past, or
    a situation in which you had to debug a complex problem. Write down different
    ways in which an LLM might have been of benefit in performing your exploratory
    testing or problem solving.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Summarizing testing notes with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once an exploratory testing session is concluded, the results need to be reported
    to relevant individuals, which can be challenging, unlike the reporting of other
    testing activities. Given that our goal is to share what we’ve learned, it can
    be difficult to distill all the details of a session into a succinct report that
    shares details such as
  prefs: []
  type: TYPE_NORMAL
- en: How we approached our exploratory testing sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What setup was required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What testing we did
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we learned from our testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What bugs were discovered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, we create notes as we explore to capture the aforementioned details.
    But to ensure that our note-taking doesn’t overwhelm or distract us from our exploring,
    we tend not to add too much detail. This works well if we are debriefing after
    a session. We can share what we’ve learned by using our notes as a trigger to
    discuss certain aspects of our testing. The problem is that if we are asked to
    share details about a session that was run in the past, our notes can go only
    so far as to prompt our memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, as an alternative, we can turn to LLMs to have them create a *testing story,*
    a type of report that tells the story of how some testing was carried out and
    what was learned during it. To do this, we can create the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an exploratory tester. Convert the exploratory testing notes delimited
    by three hashes and convert them into a test story that details what happened
    in the exploratory testing sessions.   *   Ensure that all bugs identified are
    listed at the end of the story for quick reference.###<Enter original testing
    notes>### |'
  prefs: []
  type: TYPE_TB
- en: In this prompt, we’re asking the LLM to convert our testing notes into a testing
    story. In the case of my exploratory testing session, the notes were written in
    Markdown (see appendix C for the full notes) and would be copied into the delimited
    section found in the prompt (again, see appendix C for the full notes). The prompt
    also contains additional instructions to list the bugs found as a separate list,
    which might be useful when discussing them with our team. Copying my testing notes
    from Appendix C into the delimited section and sending the results to ChatGPT
    returned the following response.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Test Story: Exploring Bookings Rendering in Report ViewIn a recent exploratory
    testing session, I delved into the rendering of bookings within the report view
    interface, focusing on the display of a large volume of bookings. The primary
    goal was to assess the readability and functionality when dealing with extensive
    booking data.Session OverviewI commenced the session by analyzing the code structures
    in the APIs associated with rooms and bookings to comprehend data generation for
    the report page. Here’s a breakdown of my exploration:Data Setup:   *   Explored
    generating test data for the report page, resulting in creating test data for
    both the room and booking APIs.   *   Encountered challenges in rebuilding the
    booking API due to test data integration issues, necessitating direct manipulation
    via the IDE.Investigation:   *   Loading Page Performance:   *   Observed significant
    slowness in loading the page when a substantial number of bookings were present.  
    *   Navigation Controls:   *   Noted slowness in the calendar view when navigating
    between busy and quiet months, although navigation remained functional.   *  
    Viewing Bookings:   *   Discovered issues with the popup displaying bookings,
    causing overflow and making certain bookings unreadable.   *   Encountered delays
    in loading the popup for days with numerous bookings.   *   Creating New Bookings:  
    *   Identified a delay issue when canceling the creation of a new booking, impacting
    the calendar component’s rendering time.   *   Keyboard Accessibility:   *   Successful
    tab navigation through the calendar but faced challenges with tabbing into and
    navigating within popups.Further Explorations:   *   Tested the calendar’s behavior
    under different scenarios, such as resizing, zooming, and device orientation changes,
    revealing multiple rendering and display issues across various screen sizes and
    orientations.Bugs Identified:   1.  Page loading significantly slows down with
    a high volume of bookings.   2.  Calendar navigation experiences slowness when
    transitioning between busy and quiet months.   3.  Popup for viewing bookings
    overflows and obstructs readability for certain bookings.   4.  Delay issue when
    canceling the creation of a new booking affects calendar rendering.   5.  Accessibility
    issues with keyboard navigation within popups.   6.  Slow rendering and content
    disappearance when resizing and zooming the calendar.   7.  Text readability problems
    on mobile devices due to small font size.   8.  Inability to perform click-and-drag
    actions for admin booking on mobile devices.These findings indicate several areas
    that require attention to enhance the performance, usability, and display of the
    booking system within the report view. |'
  prefs: []
  type: TYPE_TB
- en: The LLM has managed to successfully convert my exploratory testing notes, which
    are a mix of headers and short sentences and are somewhat structureless in form
    (the raw notes can be found in appendix C for comparison), into a much richer
    report. It starts with a summary of the exploratory testing as a whole and what
    my intention was before going into the session details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reviewing this report demonstrates a choice that can be made when using LLMs.
    The report shows heavy use of bullet points and numbered lists, a pattern that
    is quite common for ChatGPT. However, we could fix this by applying additional
    instructions or specific template structures to the prompt. For example, if we
    updated our prompt to use the Cornell method ([https://mng.bz/M1M2](https://mng.bz/M1M2)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an exploratory tester. Convert the exploratory testing notes delimited
    by three hashes into a report using the Cornell method of note taking.###<Enter
    original testing notes>### |'
  prefs: []
  type: TYPE_TB
- en: 'We get a different type of report which has the title, cues, notes, and summary
    structure that can be found using the Cornell method:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Report on Exploratory Testing: Bookings Rendering in Report ViewSummary:The
    exploratory testing aimed to assess the rendering of bookings in the report view,
    particularly focusing on handling large volumes of bookings. The examination involved
    investigating page loading times, navigation controls, creating new bookings,
    keyboard accessibility, responsiveness to screen size changes, device orientations,
    and text legibility in both portrait and landscape modes.Cue Column:Charter: Explore
    bookings’ rendering in the report view with a vast collection of bookings to determine
    readability.Session Notes:   •   Data Creation for APIs:   −   Analyzed code in
    APIs (room and booking) to understand data generation.   −   Strategies devised
    to generate test data for the room and booking APIs using SQL and scripts.   •  
    Investigations:   −   Loaded page: Noticed slow loading times for pages with numerous
    bookings.   −   Navigation Controls: Page sluggishness observed when transitioning
    from busy to quiet months.   −   Viewing More Bookings: Popup issues detected
    - overflow and slow loading.   −   Creating New Bookings: Lag experienced after
    canceling the booking action.   −   Keyboard Accessibility: Successful tab navigation,
    issues with tabbing into popups.   •   PAOLO Mnemonic:   −   Portrait Mode: Calendar
    responsiveness on different screen sizes and resolutions assessed.   −   Zooming
    In/Out: Issues identified with slow resizing and content hiding.   −   Device
    Orientation: Handling of screen resizing and orientation changes examined.   −  
    Text Legibility: Problems noted with tiny text on mobile screens.Notes Column:  
    •   Issues/Bugs:   −   Slow page loading with numerous bookings.   −   Sluggishness
    navigating between busy and quiet months.   −   Popup display issues - overflow,
    slow loading.   −   Lag after canceling a booking action.   −   Tabbing issues
    with popups, especially on overflowed content.   −   Slow resizing, content hiding
    on zooming in/out.   −   Messy initial layout on screen resizing.   −   Tiny text
    on mobile screens affecting readability.   −   Inability to click and drag on
    calendar for admin bookings in mobile view.Summary Column:   •   Key Findings:  
    −   Performance issues with page loading and navigation controls.   −   Popup
    display problems affecting user interaction.   −   Resizing and text legibility
    concerns on mobile devices.   −   Limited functionality for admin bookings on
    mobile view.Evaluation Column:   •   Implications:   −   Performance optimizations
    needed for smoother page loading.   −   Popup display improvements for better
    user experience.   −   Responsive design adjustments required for various screen
    sizes and orientations.   −   Text size adjustments for better readability on
    smaller screens.Action Column:   •   Action Items:   −   Optimize page loading
    times for better performance.   −   Prioritize fixing popup display issues for
    enhanced usability.   −   Implement responsive design enhancements for different
    screen sizes.   −   Adjust text sizes for improved readability on mobile devices.Reflection:The
    exploratory testing revealed critical issues related to performance, usability,
    and responsiveness in rendering bookings within the report view. Addressing these
    findings would significantly improve the overall user experience and functionality
    of the system. |'
  prefs: []
  type: TYPE_TB
- en: Experimentation is key here, but what this demonstrates is that LLMs have the
    potential to support us in ways that other, more traditional tools are unable
    to. Reporting sometimes gets short shrift in testing because rapidly sharing a
    wealth of knowledge can be difficult, which encourages teams to rely on more quantitative,
    test script-based reports that we normally see. However, with the use of LLMs,
    we can now structure and present reports that give a fuller picture of what we’ve
    learned during exploratory testing, sharing the rich information that hides behind
    the numbers of how many test scripts or sessions have been run.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 8.4
  prefs: []
  type: TYPE_NORMAL
- en: Create a testing report based on notes you have captured in the past. Copy the
    notes into one of the prompts we’ve explored in this part of the chapter to see
    what sort of report is produced. Reflect on whether it’s of use to you or if the
    prompt needs tweaking to improve the reporting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs can be used in the process of identifying risks and charters for exploratory
    testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When identifying risks, biases such as functional fixedness can make us miss
    potential risks to explore.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use LLMs to analyze risks we’ve already discovered and then propose additional
    risks for further consideration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risks generated by an LLM will contain a mix of suggestions that may or may
    not be useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can carry out a similar process of requesting LLMs to suggest additional
    test charters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of exploratory testing and LLMs, the goal is to use LLMs to shake
    up our thinking and offer new perspectives, not to accept them blindly as additional
    risks and charters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to help support activities in exploratory testing sessions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use LLMs to transform code and add code comments, which can help improve
    our understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also use LLMs to create required test data by asking them to output test
    data for us or create scripts that we can use to generate our data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining prompts with testing heuristics can generate suggested test ideas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can determine which of the suggested test ideas are of use and discard the
    rest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to convert testing notes into richer test reports, such as
    testing stories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
