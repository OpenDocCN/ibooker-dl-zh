<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span> </span><span class="chapter-title-text">Storing our ACH files</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header"><span class="CharOverride-1">This chapter covers</span><span class="CharOverride-1"/></h3>
<ul>
<li class="readable-text" id="p2"><span class="CharOverride-2">Creating tables within our PostgreSQL database</span></li>
<li class="readable-text" id="p3"><span class="CharOverride-2">Designing a relational database capable of storing ACH files</span></li>
<li class="readable-text" id="p4"><span class="CharOverride-2">Using Python and Pydantic to validate ACH records and store them in our database</span></li>
<li class="readable-text" id="p5"><span class="CharOverride-2">Ensuring that our records are parsed and stored correctly by implementing unit testing with </span><span class="CharOverride-3">pytest</span></li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>In this sprint, we use another research spike to explore how to define our database. Databases store and persist our data across application instances, while providing a way to query and ensure the integrity of that data. Here, we examine how to store our ACH files in a database. After initial analysis, we expand our APIs to store an ACH file in the database. Continuing along that track, we expand our ACH parser to store the individual fields as well. Finally, we wrap up the chapter by examining how storing ACH data affects our unit and load tests.</p>
</div>
<div class="readable-text intended-text" id="p7">
<p>The introduction of a database is necessary in our project because an ACH file is a flat file. The current ACH system that Futuristic FinTech uses relies on flat files, and they can be challenging in many areas, including performance, querying, and data integrity. For instance, if a customer questions when a transaction was loaded, all the ACH files must be loaded and parsed to perform the search, which is time-consuming. Furthermore, keeping the parsed ACH files in memory becomes unfeasible given the number of records being dealt with.</p>
</div>
<div class="readable-text" id="p8">
<h2 class="readable-text-h2"><span class="num-string">5.1</span> Designing our database</h2>
</div>
<div class="readable-text" id="p9">
<p>When a user uploads files to our ACH dashboard, we obviously need the ability to save them, or our system will not be very useful. The current ACH dashboard for Futuristic FinTech does not use a relational database. Instead, once uploaded, the files are parsed and stored in flat files (i.e., unstructured text files), which makes more sophisticated functionality a chore to implement. The ACH dashboard we are replacing uses only the filesystem to store the files. To provide more advanced processing, we want our ACH dashboard to be backed by a relational database, and we work through the initial review and implementation of various database designs and concepts to support our dashboard. Often, we need to have these types of research stories included in our sprints to examine different ways we may go about when implementing a desired feature. </p>
</div>
<div class="readable-text intended-text" id="p10">
<p>There are at least a dozen different relational databases, and FinTech uses many of them. Our choice of database is often already determined by the database our company uses. We have seen Oracle, MySQL/MariaDB, and PostgreSQL used by FinTech—to name just a few. In our case, we have already set up an environment that enables Postgre­SQL to run in a Docker container, and we have seen how to build/initialize tables during startup and view our data through CloudBeaver. We can now start expanding our database to accommodate storing ACH files.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>Databases do much more than just storing our data—they can help ensure the reliability and consistency of the data and relationships, a concept known as <span class="Italics">referential integrity</span>. Referential integrity in our database is a fancy way to say that we will ensure our tables are appropriately related and the fields are correctly defined. For instance, recall that ACH is a fixed-length format, meaning the individual fields are fixed length. We may store the file ID modifier from the file header record as a <code>VARCHAR(1)</code> since it can only be a single uppercase character (or 0 through 9). Similarly, we may want to store the total debit entry amount in the file control (i.e., file trailer) record as <code>MONEY</code> or <code>NUMERIC(12,2)</code>. The <code>NUMERIC(12,2)</code> defines a field with a precision of 12 significant digits and a scale of 2, which is the number of decimal digits. Whether you’ll use <code>MONEY</code> or <code>NUMERIC</code> is up to you, but we favor the <code>NUMERIC(12,2)</code> representation as it closely resembles the field definition.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>Another aspect of referential integrity is preventing orphan records. Remember that there is a hierarchy of records in an ACH file, that is, file control <span class="CharOverride-4">→</span> batch header <span class="CharOverride-4">→ </span>entry records <span class="CharOverride-4">→</span> etc. For example, an orphan record may occur if we have not defined our database carefully and we were to delete a batch header record. Once we delete a batch header, all the entry and addenda records (as well as the batch control record) belonging to that batch are no longer valid, and we should delete them. Likewise, deleting the file control record should delete all records associated with that file. In our relational database, we can achieve this by creating a <code>FOREIGN</code> <code>KEY</code>—which references the other table entries—and using <code>ON DELETE</code> <code>CASCADE</code>.</p>
</div>
<div class="readable-text intended-text" id="p13">
<p>Our initial database will use the inherent benefits of a relational database by defining the following:</p>
</div>
<ul>
<li class="readable-text" id="p14"><em>Primary key</em><em>s</em>—Unique identifiers for each record in a table </li>
<li class="readable-text" id="p15"><em>Foreign key</em><em>s</em>—A link between two tables where a field (or fields) in one table refers to unique data (such as the primary key) in another table</li>
<li class="readable-text" id="p16"><em>Constraints on the field</em><em>s</em>—For example, <code>NOT</code> <code>NULL</code> (to ensure data is present), <code>UNIQUE</code> (to ensure data is unique across all rows), and <code>DEFAULT</code> (to assign a default value if none was provided)</li>
<li class="readable-text" id="p17"><em>Data integrit</em><em>y</em>—Attained by defining appropriate data types and sizes of fields</li>
</ul>
<div class="readable-text" id="p18">
<p>We’ll first look at storing ACH files with only Prearranged Payment and Deposit (PPD) data to keep things simpler. The PPD code is typically used for direct deposits and recurring payments such as payroll and pensions, so it is a widely used code that may frequently affect you (without your knowledge). To get an overview of what our database will look like, we again rely on PlantUML for a rendering of a proposed database structure.</p>
</div>
<div class="browsable-container listing-container" id="p19">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.1<span class="CharOverride-5"> </span>PlantUML definition of our database</h5>
<div class="code-area-container">
<pre class="code-area">@startuml #1

object ach_file_uploads { #2
    ach_files_id: UUID   #3
    filename: VARCHAR(255)  #3
    file_hash: VARCHAR(32)  #3
    credit_total: NUMERIC(12,2)  #3
    debit_total: NUMERIC(12,2) 
}

object ach_files {
    ach_files_id: UUID
    record_type: VARCHAR(1)
    record_id: UUID
    parsed: BOOLEAN    
    sequence: NUMERIC
    unparsed_record: VARCHAR(94)
}

object ach_file_header_records {
    record_id: UUID
    fields for record type
}

object ach_batch_header_records {
    record_id: UUID
    file_header_id: UUID
    fields for record type  
}

object ach_entry_detail_ppd_records {
    record_id: UUID
    batch_header_id: UUID
    fields for record type
}

object ach_addenda_ppd_records {
    record_id: UUID
    entry_detail_id: UUID
    fields for record type
}

object ach_batch_control_records {
    record_id: UUID
    batch_header_id: UUID
    fields for record type
}

object ach_file_control_records {
    record_id: UUID
    file_header_id: UUID
    fields for record type
}

ach_file_uploads::ach_files_id &lt;-- ach_files::ach_files_id  #4
 #4
ach_files::record_id &lt;-- ach_file_header_records::record_id #4
ach_files::record_id &lt;-- ach_batch_header_records::record_id #4
ach_files::record_id &lt;-- ach_entry_detail_ppd_records::record_id #4
ach_files::record_id &lt;-- ach_addenda_ppd_records::record_id #4
ach_files::record_id &lt;-- ach_batch_control_records::record_id #4
ach_files::record_id &lt;-- ach_file_control_records::record_id #4
 #4
ach_batch_header_records::file_header_id -&gt; <br/>ach_file_header_records::record_id #4
ach_entry_detail_ppd_records::batch_header_id -&gt; <br/>ach_batch_header_records::record_id #4
ach_addenda_ppd_records::entry_detail_id -&gt; <br/>ach_entry_detail_ppd_records::record_id #4
ach_batch_control_records::batch_header_id --&gt; <br/>ach_batch_header_records::record_id #4
ach_file_control_records::file_header_id --&gt; <br/>ach_file_header_records::record_id #4
 #4
@enduml  #4#5</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Begins a PlantUML definition</span>
<br/>#2 
     <span class="CharOverride-6">Defines a table for our diagram</span>
<br/>#3 
     <span class="CharOverride-6">Defines fields within the table for our diagram</span>
<br/>#4 
     <span class="CharOverride-6">Shows the relationships between keys in the table</span>
<br/>#5 
     <span class="CharOverride-6">Ends the PlantUML definiton</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p20">
<p>The preceding definition is presented in figure 5.1, which shows how we may define our fields and the relationships between our tables. This is not an exhaustive list of the fields in tables, but it gives us an idea of how our tables will be related. The arrows represent the foreign key constraints present in the table. For instance, we can see how <code>ach_files_id</code> in the <code>ach_files</code> table field is defined as a Universally Unique Identifier (UUID) and references the <code>ach_files_id</code> from <code>ach_file_uploads</code>.</p>
</div>
<div class="browsable-container figure-container" id="p21">
<img alt="A diagram of a computer  Description automatically generated" height="600" src="../Images/CH05_F01_Kardell.png" style="width: 100%; max-width: max-content;" width="870"/>
<h5 class="figure-container-h5"><span class="">Figure 5.1</span><span class=""> </span><span class="">Diagram showing relationships between our tables</span></h5>
</div>
<div class="readable-text" id="p22">
<p>Figure 5.1 also conveys our desire to accomplish the following goals:</p>
</div>
<ul>
<li class="readable-text" id="p23">Maintain the ordering of the records in our file</li>
<li class="readable-text" id="p24">Assume records will be unparsable and accommodate for that with unparsed records</li>
<li class="readable-text" id="p25">Maintain referential integrity by having parsed records refer to the unparsed ones</li>
</ul>
<div class="readable-text" id="p26">
<p>While the database structure seemingly meets those goals and the diagram gives us a visual guide to get started, there is always room for improvement. Regardless of whether this structure was provided to us by a subject matter expert (SME) or the interpretation of a database analyst (DBA), there may be opportunities to refine our work as we move through the project. With our diagram in hand, we should have an idea of how we want the database to look and can start working on it. However, we need to follow the general pattern of defining a test, building the table/fields, and finally building the API. When working with an SQL database, it is important to understand that companies will certainly have different approaches to managing their data. Some companies may extract the SQL portion away from the developer, either by using an object-relational mapper (ORM) such as SQLAlchemy or by rolling their own. An ORM helps simplify code by abstracting the database and providing benefits such as database agnostic, optimizations, and improved productivity. </p>
</div>
<div class="readable-text intended-text" id="p27">
<p>Other companies may require you to write the SQL yourself because they like the level of control afforded by direct SQL. ORMs may make complex queries difficult or inefficient. Furthermore, troubleshooting queries and performance problems may also be more difficult to track down. First, we show here how to use straight SQL commands as we have been doing and then move to SQLAlchemy so that you can get acquainted with both approaches. There are always several factors that will be added, regardless of whether we use one approach over the other or combine them. Usually, the existing software will dictate our approach, so be careful not to stray too far from existing software standards initially as this could create a maintenance nightmare.</p>
</div>
<div class="readable-text" id="p28">
<h2 class="readable-text-h2"><span class="num-string">5.2</span> Using SQL directly</h2>
</div>
<div class="readable-text" id="p29">
<p>Chapter 3 explored parsing an ACH file and creating accompanying unit tests. We also created a simple API that accessed a database and returned results. So, we have all the pieces we need to store an ACH file in a database. Now we only have to put them together. In our first approach, we update our parser to store the ACH file in the database. We make the following assumptions:</p>
</div>
<ul>
<li class="readable-text" id="p30">The database is always up and running.</li>
<li class="readable-text" id="p31">We do not have any data that we want to preserve.</li>
<li class="readable-text" id="p32">We are running our code from an IDE and not inside Do<span class="Hyperlink">ck</span>er.</li>
</ul>
<div class="readable-text" id="p33">
<p>In other words, we are just working on the process of being able to parse and store the records in the database. We build out the previous <code>AchFileProcessor</code> to allow it to store ACH files. The next listing shows adding the necessary code to obtain a connection to the database. Since we are in a research sprint, we have hardcoded the database connection parameters such as username and password. Later, when we are certain that this is the right approach, we can start to abstract some of these hardcoded values so we have a more flexible configuration by using <code>environment</code> variables or secret management tools such as AWS Secrets Manager (<a href="https://aws.amazon.com/secrets-manager/"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/aws.amazon.com/secrets-manager/</span></a>) or HashiCorp Vault (<a href="https://www.hashicorp.com/products/vault"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/www.hashicorp.com/products/vault</span></a>).</p>
</div>
<div class="browsable-container listing-container" id="p34">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.2<span class="CharOverride-5"> </span>Adding database fields</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">class AchFileProcessor:
    records = []
    exceptions = []
    last_trace_number = None
    expected_record_types = ["1"]
<strong>    POSTGRES_USER = "someuser"    </strong>#1
<strong>    POSTGRES_PASSWORD = "supersecret"  </strong>
<strong>    DATABASE_URL = f"dbname={POSTGRES_USER} user={POSTGRES_USER}</strong>
<span class="CharOverride-8">➥</span><strong>     password={POSTGRES_PASSWORD} host=postgres port=5432" </strong>#2
…    
<strong>    def get_db():      </strong>#3
<strong>        conn = psycopg.connect(self.DATABASE_URL)</strong>
<strong>        return conn</strong></pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Temporarily hardcodes our username and password</span>
<br/>#2 
     <span class="CharOverride-6">A hardcoded host and port, but we should consider parametrizing those as well.</span>
<br/>#3 
     <span class="CharOverride-6">A new function to return a connection to the database</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p35">
<p>That should provide us with the ability to connect to the database; however, we will need to use this code. Before we begin parsing the ACH file, we want to simply store the unparsed records in the database in what we may want to consider an extract<br/>–load–transform (ELT) approach rather than directly parsing the records through an extract–transform–load (ETL) approach.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p36">
<h5 class="callout-container-h5 readable-text-h5">ELT vs. ETL</h5>
</div>
<div class="readable-text" id="p37">
<p>There are two approaches to handling data when processing it. They are usually discussed with regard to data warehousing and business intelligence, but ACH processing has unique challenges. ETL (extract–transform–load) is a traditional approach and may cross our mind first when processing data. For instance, we know that we want to parse each of these ACH records into their respective fields and store them in a database. However, dealing with data that may not be formatted correctly all the time is one of the challenges in ACH processing. With an ETL approach, this invalid data may cause processing to halt completely.</p>
</div>
<div class="readable-text" id="p38">
<p>In an ELT (extract–load–transform) approach, the data is loaded and then transformed once it is going to be used. Typically, we see ELT when dealing with very large data and using data warehouses such as Snowflake or Redshift, where there is enough processing power to perform transformations on request.</p>
</div>
<div class="readable-text" id="p39">
<p>So, why do we care about these approaches? Often, financial institutions will allow some leeway with the data and will not always reject a file if an error is considered recoverable. These conditions can vary from one financial institution to another and with their customers. For example, a count of records on a control file may be updated if incorrect rather than just rejecting the file or batch. Or, invalid data in a field may just be updated to spaces before loading it. While we can rely on logging and exceptions to keep track of changes (and we should be logging any such changes), we still want to keep a record of the original file so that a banker has the opportunity to review the data.</p>
</div>
</div>
<div class="readable-text" id="p40">
<h3 class="readable-text-h3"><span class="num-string">5.2.1</span> Adding records to the ach_files table</h3>
</div>
<div class="readable-text" id="p41">
<p>We will take the approach of loading our ACH file unaltered into the database before parsing out any records. Figure 5.2 illustrates a basic flow for this section.</p>
</div>
<div class="browsable-container figure-container" id="p42">
<img alt="" height="88" src="../Images/CH05_F02_Kardell.png" style="width: 100%; max-width: max-content;" width="881"/>
<h5 class="figure-container-h5"><span class="">Figure 5.2</span><span class=""> </span><span class="">Flowchart for section 5.2</span></h5>
</div>
<div class="readable-text" id="p43">
<p>Listing 5.3 shows an <code>ach_files</code> table. So, before parsing any records, let’s just add all our records to that database table.</p>
</div>
<div class="browsable-container listing-container" id="p44">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.3 Simple<span class="CharOverride-9"> </span><code>ach_files</code> table</h5>
<div class="code-area-container">
<pre class="code-area">-- Create the uuid extension
CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; #1

-- Create the ach_files table
CREATE TABLE ach_files (
    ach_files_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(), #2
    file_name VARCHAR(255) NOT NULL, 
    unparsed_record VARCHAR(94) NOT NULL, #3
    sequence_number INTEGER NOT NULL #4
);</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Allows PostgreSQL </span>
<span class="CharOverride-6">to create UUIDs</span>
<br/>#2 
     <span class="CharOverride-6">Primary UUID key</span>
<br/>#3 
     <span class="CharOverride-6">The unparsed ACH record stored as is</span>
<br/>#4 
     <span class="CharOverride-6">A sequence number used to maintain ordering when retrieving data</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p45">
<p>With this code, running <code>docker-compose</code> <code>down</code>, <code>docker-compose</code> <code>build</code>, and <code>docker-compose</code> <code>up</code> should allow our table to be created, and now we just need to update the code to write out some records!</p>
</div>
<div class="readable-text intended-text" id="p46">
<p>Before we write out any records, we must ensure that the basics are working. So, we simply add the following code at the beginning of the existing parse routine. This code merely gets a connection to the database, obtains a cursor that can be used to execute SQL statements, and then closes both the cursor and connection.</p>
</div>
<div class="browsable-container listing-container" id="p47">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.4<span class="CharOverride-5"> </span>Testing the connection</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">    def parse(self, filename) -&gt; [List, List]:

<strong>        conn = self.get_db()</strong> #1
<strong>        cursor = conn.cursor()</strong> #2
<strong>        cursor.close() </strong>#3
<strong>        conn.close() </strong>#4

        with open(filename, "r") as file:
            lines = file.readlines()</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Calls our new get_db() function to connect to the database</span>
<br/>#2 
     <span class="CharOverride-6">Creates a cursor used to execute SQL commands</span>
<br/>#3 
     <span class="CharOverride-6">Closes the cursor</span>
<br/>#4 
     <span class="CharOverride-6">Closes the connection</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p48">
<p>If we were to run our code at this point, we may encounter several problems. First, our hardcoded <code>DATABASE_URL</code> <code>string</code> has a host of <code>postgres</code> and a port of <code>5432</code>. We are working within an IDE and not within Docker, so <code>postgres</code> is not the correct name to use. Indeed, we should see an error <code>psycopg.OperationalError:</code> <code>connection</code> <code>is bad:</code> <code>Unknown</code> <code>host</code> if we were to run the program. Instead, we want to use <code>localhost</code> because we are calling from the system hosting Docker. </p>
</div>
<div class="readable-text intended-text" id="p49">
<p>In addition, we need to expose the port for our container. Our docker-compose.yml should look like the one in listing 5.5. Without exposing the port, we would see an error similar to <code>psycopg.OperationalError:</code> <code>connection</code> <code>failed:</code> <code>:1),</code> <code>port</code> <code>5432</code> <code>failed:</code> <code>could</code> <code>not</code> <code>receive</code> <code>data</code> <code>from</code> <code>server:</code> <code>Socket</code> <code>is</code> <code>not</code> <code>connected</code>, which would hopefully tip us off that there is a problem with our port. Remember that these types of problems will always be popping up in our development, and we just need to retrace our steps to see what we missed.</p>
</div>
<div class="browsable-container listing-container" id="p50">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.5<span class="CharOverride-5"> </span>Updated docker-compose.yml file</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">  postgres:
    build: 
      context: ./db
      dockerfile: Dockerfile
<strong>    ports:</strong>
<strong>      - 5432:5432 </strong>#1
    env_file:
      - ./.sql_server.conf</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Exposes the standard PostgreSQL port</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p51">
<p>Once we have a basic connection working, we can start writing out the records. We could use a method similar to the one in listing 5.4 where we handle opening and closing the connections manually; however, having to remember to close the connections can be error-prone as we may forget to close them (we have seen files not being closed in a production environment for years only to spring up when moving to a new version of software or another vendor). We will use Python’s<code> with </code>statement as it automatically handles closing of various resources when a particular block of code is exited. In fact, we have already used it when reading our file, so we can simply expand on that.</p>
</div>
<div class="browsable-container listing-container" id="p52">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.6<span class="CharOverride-5"> </span>Updated parse function</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">    def parse(self, filename) -&gt; [List, List]:

        with open(filename, "r") as file, <strong>self.get_db() as conn: </strong>#1
            lines = file.readlines()
            <strong>sequence_number = 0</strong>  #2

            for line in lines:
                <strong>sequence_number += 1 </strong>#3
                line = line.replace("\n", "")

                <strong>with conn.cursor() as cursor:</strong> #4
<strong>                    cursor.execute(f"INSERT INTO ach_files</strong> #4
<span class="CharOverride-8">➥</span><strong> (file_name, unparsed_record, sequence_number)</strong> #4
<span class="CharOverride-8">➥</span><strong> VALUES (%s, %s, %s)", (filename, line, sequence_number)) </strong> #4
<strong>                    conn.commit() </strong></pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Creates a connection as part of the existing with statement</span>
<br/>#2 
     <span class="CharOverride-6">Initializes our </span>
<span class="CharOverride-6">sequence_number to zero</span>
<br/>#3 
     <span class="CharOverride-6">Increments the sequence_number for each record</span>
<br/>#4 
     <span class="CharOverride-6">Creates a cursor using the with statement, inserts, and commits the record</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p53">
<p>We can rerun our unit test <code>test_parsing_ach_file.py</code>, which will run our sample file through our code, and then check CloudBeaver to verify the records have been added. This is a good start: we can store the records in our database, and it should not be much of a stretch to use a similar approach to go into our individual parsing functions and store the data there.</p>
</div>
<div class="readable-text intended-text" id="p54">
<p>One thing that we need to do is update our unit test to pull the data from the database instead of relying on data that is being returned, because our goal is to store all the data in the database and not return anything other than a status. For now, let’s take a look at updating the <code>pytest</code> to get the record count from the database.</p>
</div>
<div class="browsable-container listing-container" id="p55">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.7<span class="CharOverride-5"> </span>Updated <code>pytest</code></h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">import os
import psycopg
import pytest

from ach_processor.AchFileProcessor import AchFileProcessor

<strong>POSTGRES_USER = "someuser"                   </strong>#1
<strong>POSTGRES_PASSWORD = "supersecret"            </strong> #1
 #1
<strong>DATABASE_URL = f"dbname={POSTGRES_USER} user={POSTGRES_USER} </strong> #1
<span class="CharOverride-8">➥</span><strong>password={POSTGRES_PASSWORD} host=localhost port=5432" </strong> #1
 #1
<strong>def get_db():                             </strong> #1
<strong>    conn = psycopg.connect(DATABASE_URL)  </strong> #1
<strong>    return conn                           </strong> #1

def test_record_count():
    dir_path = os.path.dirname(os.path.realpath(__file__))
    file_path = os.path.join(dir_path, "data", "sample.ach")
    expected_result = 41
    parser = AchFileProcessor()
    records, exceptions = parser.parse(file_path)
<strong>    with get_db() as conn, conn.cursor() as cursor:         </strong>#2
<strong>        cursor.execute("SELECT COUNT(*) FROM ach_files")    </strong>
<strong>        record_count = cursor.fetchone()[0]</strong>
    assert (
        <strong>record_count</strong> == expected_result
    ), f"Expected {expected_result}, but got {<strong>record_count</strong>}"</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">We pull the code from </span>
<span class="CharOverride-6">the AchFileProcessor that connects to the database. This is a temporary solution and we will eventually need to refactor the code again when more tests need to connect to the database.</span>
<br/>#2 
     <span class="CharOverride-6">Simple query to fetch the count of records stored in ach_files</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p56">
<p>This test should work when Docker is first brought up, but subsequent tests will fail because the records are being added. So, our second iteration fails with </p>
</div>
<div class="browsable-container listing-container" id="p57">
<div class="code-area-container">
<pre class="code-area">Expected :41
Actual   :82</pre>
</div>
</div>
<div class="readable-text" id="p58">
<p>Our the third iteration fails with</p>
</div>
<div class="browsable-container listing-container" id="p59">
<div class="code-area-container">
<pre class="code-area">Expected :41
Actual   :123</pre>
</div>
</div>
<div class="readable-text" id="p60">
<p>We need a way to clear the database before each test. Ideally, we want to have a database that only exists for the length of the test, but first, let’s see how we may clear the table after each test. We can use a <code>pytest.fixture</code> that will perform any setup/teardown for our individual tests.</p>
</div>
<div class="browsable-container listing-container" id="p61">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.8<span class="CharOverride-9"> </span><code>pytest.fixture </code>to setup and teardown our unit test</h5>
<div class="code-area-container">
<pre class="code-area">@pytest.fixture
def setup_teardown_method():
    print("\nsetup test\n")       #1
    yield                         #2
    print("\nteardown test\n")    #3
    with get_db() as conn, conn.cursor() as cursor: #4
        cursor.execute("TRUNCATE ach_files")        
…
def test_record_count(setup_teardown_method):       #5</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Anything preceding yield will execute before the test.</span>
<br/>#2 
     <span class="CharOverride-6">yield to allow the test to execute</span>
<br/>#3 
     <span class="CharOverride-6">Anything following yield will execute after the test.</span>
<br/>#4 
     <span class="CharOverride-6">Gets a connection and cursor, then trunctuates the table to clear it</span>
<br/>#5 
     <span class="CharOverride-6">Includes our fixture </span>
<span class="CharOverride-6">in the unit test</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p62">
<p>With this code, our test will pass repeatedly because the table is cleared after each run. We have worked through our problem of having the database retain records between runs. Of course, we need to be careful. If our unit tests point to a database that is not a development server, we run the risk of wiping out needed data. For this reason, we may want to look at other options such as an in-memory database, mocking, or <code>pytest-postgresql</code>.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p63">
<h5 class="callout-container-h5 readable-text-h5">Dealing with databases</h5>
</div>
<div class="readable-text" id="p64">
<p>A common way to work with databases for unit testing is to use an in-memory database such as SQLite or H2. This allows us to run a database that exists entirely in memory and therefore can be isolated to our unit testing. The benefits are usually quick execution and the isolation of data. The drawback is that many times there may be functionality that does not exist between our production database and these in-memory databases, which can lead to problems when trying to create a unit test. For instance, SQLite has five data types to define data, while PostgreSQL has over 40 types. This is not to say that one is inherently better than the other—we just highlight the challenges we may face if our tables use the unsupported data types. We may end up fighting unnecessary battles to get our unit tests to run. That is why we should have additional tools and techniques we can use.</p>
</div>
<div class="readable-text" id="p65">
<p>Mocking with a tool such as pgmock can also remove the need for a database in your test. In our scenario, we are actually testing whether the data has made it to the database, so mocking does not really provide a viable solution but something to look into later.</p>
</div>
<div class="readable-text" id="p66">
<p><code>Pytest-postgresql</code> is a package that helps us manage <code>postgresql</code> connections for a <code>pytest</code>, which offers the best of both worlds by allowing us to connect to a test database or create/clear/modify tables as part of the test. </p>
</div>
<div class="readable-text" id="p67">
<p>As our project progresses, we will find that managing our data for tests and keeping the tests isolated becomes increasingly harder. In chapter 10, we eventually start incorporating the Testcontainers package to isolate our tests. This approach will also be beneficial as the infrastructure for the project matures and we begin running our tests as part of a build pipeline.</p>
</div>
</div>
<div class="readable-text" id="p68">
<p>The ability to store data is necessary in most applications, but as we have seen, it also adds more complexity to our design and coding. In this section, we started small by ensuring that we could connect to the database and by storing our unparsed records, which helped minimize the number of code changes required. As we move forward, the complexity of our application will gradually increase. As we work through parsing and storing our ACH files, we should also keep in mind that we will be retrieving and aggregating data from the database for our ACH dashboard. A way to easily store our ACH files may lend itself to one database structure, while the ACH dashboard may be better served by an alternative structure. Our job is to strike an acceptable balance between those two goals.</p>
</div>
<div class="readable-text" id="p69">
<h2 class="readable-text-h2"><span class="num-string">5.3</span> Storing the file header record</h2>
</div>
<div class="readable-text" id="p70">
<p>The ACH file header record should be the first ACH record we encounter in an ACH file. So, it makes sense that this is the first record we should explore by adding it to the database. We first show how we may approach it using ChatGPT and then how we go through a complete example with GitHub Copilot installed in our IDE.</p>
</div>
<div class="readable-text" id="p71">
<h3 class="readable-text-h3"><span class="num-string">5.3.1</span> Using generative AI</h3>
</div>
<div class="readable-text" id="p72">
<p>Generative AI can help with a lot of boilerplate code that can get repetitive after a while. Depending on your experience level, that boilerplate code may be new to you, and it may be beneficial to go through the process a few times. Once it becomes tedious, that may be a great sign we can start to lean on generative AI tools. For instance, we may prompt ChatGPT with the following general prompt:</p>
</div>
<div class="readable-text prompt" id="p73">
<p><strong class="prompt-head-image"><img alt="image" height="32" src="../Images/Prompt-Icon.png" width="33"/></strong> Please create a Postgres table to store a parsed Nacha File Header Record.</p>
</div>
<div class="readable-text" id="p74">
<p>We then received the <code>CREATE</code> <code>TABLE</code> statement from ChatGPT that does a great job incorporating <code>CHAR</code>, <code>VARCHAR</code>, and <code>NOT</code> <code>NULL</code>.</p>
</div>
<div class="browsable-container listing-container" id="p75">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.9<span class="CharOverride-5"> </span>File header generated by ChatGPT 40</h5>
<div class="code-area-container">
<pre class="code-area">CREATE TABLE nacha_file_header (
    id SERIAL PRIMARY KEY,
    record_type_code CHAR(1) NOT NULL,
    priority_code CHAR(2) NOT NULL,
    immediate_destination CHAR(10) NOT NULL,
    immediate_origin CHAR(10) NOT NULL,
    file_creation_date CHAR(6) NOT NULL,
    file_creation_time CHAR(4),  #1
    file_id_modifier CHAR(1) NOT NULL,
    record_size CHAR(3) NOT NULL,
    blocking_factor CHAR(2) NOT NULL,
    format_code CHAR(1) NOT NULL,
    immediate_destination_name VARCHAR(23), #2
    immediate_origin_name VARCHAR(23),  #3
    reference_code VARCHAR(8) 
);</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">The file_creation_time is an optional field, so it could be NULL.</span>
<br/>#2 
     <span class="CharOverride-6">These fields are also optional in the Nacha standard. Notice how ChatGPT used VARCHAR instead of CHAR since these fields may be padded with spaces.</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p76">
<p>From personal experience, we prefer to use <code>VARCHAR</code> for most fields to avoid unnecessary padding. We have not encountered any meaningful performance effects from using <code>CHAR</code> versus <code>VARCHAR</code>. Storing ACH records may be one of the areas where using a <code>CHAR</code> could make sense since the fixed-length fields will not have any unnecessary space. <code>CHAR</code> can often be misused when declared too large, and any unused space is padded.</p>
</div>
<div class="readable-text intended-text" id="p77">
<p>Out of curiosity, we asked ChatGPT if <code>CHAR</code> or <code>VARCHAR</code> was more performant in a <code>Postgres</code> database. After making a nice comparison between the two, it updated its example (without us asking it to) to use <code>VARCHAR</code> instead of <code>CHAR</code>! We were okay with this since our preference is to use <code>VARCHAR</code>.</p>
</div>
<div class="readable-text" id="p78">
<h3 class="readable-text-h3"><span class="num-string">5.3.2</span> Full example</h3>
</div>
<div class="readable-text" id="p79">
<p>ChatGPT can be of great help if we have a good idea of what we want to accomplish or if we don’t mind spending time configuring the prompt. Otherwise, we may want to work through the process aided by Copilot. Figure 5.3 shows the flow we will be using in this section. Here, we update the unit test at the end of our process because this is a relatively short development cycle. If we do not spend too much time coding before testing, we should be fine with working on the unit test after we do a little coding.</p>
</div>
<div class="browsable-container figure-container" id="p80">
<img alt="" height="84" src="../Images/CH05_F03_Kardell.png" style="width: 100%; max-width: max-content;" width="788"/>
<h5 class="figure-container-h5"><span class="">Figure 5.3</span><span class=""> </span><span class="">Flow and associated code listings for this section</span></h5>
</div>
<div class="readable-text" id="p81">
<p>Using our knowledge of storing the records, we should be able to store a parsed record in the database, and once that is complete, the other record formats should fall into place. Recall that we returned the parsed record as a dictionary, as shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p82">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.10<span class="CharOverride-5"> </span>A dictionary with a parsed file header record</h5>
<div class="code-area-container">
<pre class="code-area">        return {
            "record_type_code": line[0],   #1
            "priority_code": line[1:3],    
            "immediate_destination": line[3:13].strip(), #2
            "immediate_origin": line[13:23].strip(), 
            "file_creation_date": line[23:29],
            "file_creation_time": line[29:33],
            "file_id_modifier": line[33],
            "record_size": line[34:37],
            "blocking_factor": line[37:39],
            "format_code": line[39],
            "immediate_destination_name": line[40:63].strip(),
            "immediate_origin_name": line[63:86].strip(),
            "reference_code": line[86:94].strip(),
        }</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">We use hardcoded values for the offsets because the offsets will not be changing, and we want the ability to quickly refer to fields in question when problems arise.</span>
<br/>#2 
     <span class="CharOverride-6">We strip off extra spaces from fields as necessary.</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p83">
<p>Now we want to take that record and store it in a database instead of simply returning it. We could keep this method of parsing and call it from another routine that was also interested in storing the data in a database. Another approach we may choose is to create dedicated parser classes or utility methods to handle parsing. Any of these approaches would help keep the code clean and reusable. For the sake of simplicity, we are going to take this routine and simply store the data in a database. </p>
</div>
<div class="readable-text intended-text" id="p84">
<p>First, we want to create a table for storing ACH file headers in the database, and the next listing shows the sample table. At this point, we are going to keep it simple and only supply the fields we need to store the data without referencing any foreign keys or other basic constraints such as <code>field</code> <code>length</code> and <code>NOT</code> <code>NULL</code>. With Copilot installed, many of these field names were automatically populated, which saved us some time and effort.</p>
</div>
<div class="browsable-container listing-container" id="p85">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.11<span class="CharOverride-5"> </span>Table to store ACH file headers</h5>
<div class="code-area-container">
<pre class="code-area">CREATE TABLE ach_file_headers (
    ach_file_headers_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    record_type_code VARCHAR(1) NOT NULL,
    priority_code VARCHAR(2) NOT NULL,
    immediate_destination VARCHAR(10) NOT NULL,
    immediate_origin VARCHAR(10) NOT NULL,
    file_creation_date VARCHAR(6) NOT NULL,
    file_creation_time VARCHAR(4),
    file_id_modifier VARCHAR(1) NOT NULL,
    record_size VARCHAR(3) NOT NULL,
    blocking_factor VARCHAR(2) NOT NULL,
    format_code VARCHAR(1) NOT NULL,
    immediate_destination_name VARCHAR(23),
    immediate_origin_name VARCHAR(23),
    reference_code VARCHAR(8)
);</pre>
</div>
</div>
<div class="readable-text" id="p86">
<p>Once we have the table, we can move on to updating our code. We chose to pass the connection object (previously created in listing 5.4) into our routine. We could have also stored the connection object as part of our class, but passing it in as a parameter will make it easier to unit test our routine. Different situations may require alternative approaches, so by no means is this the only way to accomplish our task.</p>
</div>
<div class="browsable-container listing-container" id="p87">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.12<span class="CharOverride-5"> </span>Passing the connection object</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">match record_type:
    case "1":
        result = self._parse_file_header(<strong>conn</strong>, line)
    case "5":</pre>
</div>
</div>
<div class="readable-text" id="p88">
<p>Now that we have the connection object, we can update the parsing routine to store the data.</p>
</div>
<div class="browsable-container listing-container" id="p89">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.13<span class="CharOverride-5"> </span>Updating the <code>_parse_file_header</code> routine</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">def _parse_file_header(self, <strong>conn: Connection[tuple[Any, ...]]</strong>,
<span class="CharOverride-8">➥</span> line: str) -&gt; Dict[str, str]: #1
    self.expected_record_types = ["5"]

    file_header = {
    … #2
    }

    conn.execute(f"INSERT INTO ach_file_headers (ach_file_headers_id, " #3
<span class="CharOverride-8">➥</span>                     f"record_type_code, priority_code, 
<span class="CharOverride-8">➥</span> immediate_destination, immediate_origin," 
<span class="CharOverride-8">➥</span>                     f"file_creation_date, file_creation_time, 
<span class="CharOverride-8">➥</span>file_id_modifier, record_size," 
                       f"blocking_factor, format_code,
<span class="CharOverride-8">➥</span>immediate_destination_name," 
                       f"immediate_origin_name, reference_code) " 
                       f"VALUES (DEFAULT, %(record_type_code)s, #4
%(priority_code)s, %(immediate_destination)s, "
                       f"%(immediate_origin)s, %(file_creation_date)s,
<span class="CharOverride-8">➥</span>%(file_creation_time)s, "
                       f"%(file_id_modifier)s, %(record_size)s,
<span class="CharOverride-8">➥</span>%(blocking_factor)s, %(format_code)s, "
                       f"%(immediate_destination_name)s,
<span class="CharOverride-8">➥</span>%(immediate_origin_name)s, %(reference_code)s)" 
<span class="CharOverride-8">➥</span>                           , file_header) #5

        return file_header</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">We add a connection parameter.</span>
<br/>#2 
     <span class="CharOverride-6">The return statement becomes a variable named file_header.</span>
<br/>#3 
     <span class="CharOverride-6">We can execute directly on the connection to insert records.</span>
<br/>#4 
     <span class="CharOverride-6">We used named variables as placeholders.</span>
<br/>#5 
     <span class="CharOverride-6">The file_header variable passes our values in.</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p90">
<p>We should now be able to insert our file header records into the database. Next, we can go back and update the unit test.</p>
</div>
<div class="browsable-container listing-container" id="p91">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.14<span class="CharOverride-5"> </span>Updated unit test example</h5>
<div class="code-area-container">
<pre class="code-area">import pytest
import psycopg
from psycopg.rows import dict_row
from typing import Dict
from ach_processor.AchFileProcessor import AchFileProcessor

<span>POSTGRES_USER = "someuser"</span>
<span>POSTGRES_PASSWORD = "supersecret"</span>

<span>DATABASE_URL = f"dbname={POSTGRES_USER} user={POSTGRES_USER} </span>
<span class="CharOverride-8">➥</span><span> password={POSTGRES_PASSWORD} host=localhost port=5432"</span>

@pytest.fixture
def setup_teardown_method():
    print("\nsetup test\n")
    yield
    print("\nteardown test\n")
    with get_db() as conn:
        conn.execute("TRUNCATE ach_file_headers")

def get_db(row_factory = None): #1
    conn = psycopg.connect(DATABASE_URL, row_factory=row_factory) #2
    return conn

def test_parse_file_header(setup_teardown_method):
    sample_header = "101 267084131 6910001340402200830A094101DEST NAME
<span class="CharOverride-8">➥</span>              ORIGIN NAME            XXXXXXXX"

    expected_result: Dict[str:str] = {
… #3
    }

    parser = AchFileProcessor()
    with get_db() as conn:
        result = parser._parse_file_header(conn, sample_header) #4

    with get_db(dict_row) as conn: #5
        actual_result = conn.execute("SELECT * FROM 
<span class="CharOverride-8">➥</span> ach_file_headers").fetchone() #6
        del actual_result["ach_file_headers_id"] #7

    assert result == expected_result, f"Expected {expected_result},
<span class="CharOverride-8">➥</span> but got {result}"
    assert actual_result == expected_result,
<span class="CharOverride-8">➥</span> f"Expected {expected_result}, but got {actual_result}" #8</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Creates another get_db function that takes a row_factory parameter</span>
<br/>#2 
     <span class="CharOverride-6">Uses the new parameter in the connect method</span>
<br/>#3 
     <span class="CharOverride-6">Expected result is </span>
<span class="CharOverride-6">the same as before.</span>
<br/>#4 
     <span class="CharOverride-6">Leaves the current parse result in place</span>
<br/>#5 
     <span class="CharOverride-6">Gets another connection specifying the result should be returned as a dictionary</span>
<br/>#6 
     <span class="CharOverride-6">Returns the result; actual_result will be a dictionary.</span>
<br/>#7 
     <span class="CharOverride-6">Removes ach_file_headers_id from the returned result</span>
<br/>#8 
     <span class="CharOverride-6">Compares the two results</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p92">
<p>Here, we duplicated some of our code, such as the <code>get_db</code> method with a new parameter. It is important that as we move through the code, we keep an eye on this type of duplication and consider pulling out these methods into utility or helper classes whenever possible. The IDEs by JetBrains (and others) will often point to duplicated code and provide automated options to extract code into functions.</p>
</div>
<div class="readable-text intended-text" id="p93">
<p>We also left the original result comparison in place because the method is still returning the records. As we continue to improve the project with additional functionality, we will likely remove this in favor of other return information (such as whether the record was parsed). We should now understand how the parsed ACH records can be stored in the database. For the ACH dashboard, to provide a meaningful interface with aggregated ACH data, we need to be able to store all the parsed ACH information in the database. In the next sections, we will explore how the code evolves as we work with it, not only expanding the features, but also addressing some nonfunctional requirements such as maintainability and testability.</p>
</div>
<div class="readable-text" id="p94">
<h2 class="readable-text-h2"><span class="num-string">5.4</span> Storing the rest of our ACH records</h2>
</div>
<div class="readable-text" id="p95">
<p>We have now created a database and stored data in two separate tables, so we should have enough of a framework to store the data for the remaining records in the database. The process is the same for all the remaining tables:</p>
</div>
<ul>
<li class="readable-text" id="p96">Create the table and restart Docker.</li>
<li class="readable-text" id="p97">Update the parse routine.</li>
<li class="readable-text" id="p98">Update the unit test and verify your results.</li>
</ul>
<div class="readable-text" id="p99">
<p>If embracing the test-driven development approach, we can swap the updating of the parse routine and the unit test. Either way, we should be working in short feedback cycles, allowing any errors to shake out early in the process and not after we have implemented a dozen tables. We also find opportunities for cleaning up the code and making it better overall.</p>
</div>
<div class="readable-text intended-text" id="p100">
<p>Given the presumed structure of our database, this might be the perfect place to add a few tables. Check whether the database structure makes sense—is there anything that should be changed? </p>
</div>
<div class="readable-text intended-text" id="p101">
<p>The following section will go over some of the lessons learned and insights gained from working through the rest of the tables that had to be added.</p>
</div>
<div class="readable-text" id="p102">
<h3 class="readable-text-h3"><span class="num-string">5.4.1</span> Storing ACH files challenge: Lessons learned</h3>
</div>
<div class="readable-text" id="p103">
<p>While adding the additional tables, we should have made some observations about the code and taken opportunities to make improvements to our code. Were there any that stood out? It is important to note and resolve possible pain points such as duplicated code, inefficient processing, confusing logic, and so forth. Sometimes, we can really get a sense of accomplishment by cleaning up code to be more straightforward and maintainable. We will go through some of the problems we encountered when adding additional database tables.</p>
</div>
<div class="readable-text intended-text" id="p104">
<p>With our use of <code>psycopg3</code> (which is confusingly defined as <code>psycopg</code>) instead of <code>psycopg2</code>, we found that our generative AI tools tended not to take advantage of some newer enhancements. For instance, GitHub Copilot insisted on declaring <code>cursor</code> methods at first but seemed to learn our preference for using the connection, and after a while, it stopped offering them. This makes sense as Copilot is supposed to learn and adapt to our style and coding preferences. In recent releases of these tools, we have also seen the inclusion of retrieval-augmented generation (RAG), which helps large language models (LLMs) such as ChatGPT stay current with up-to-date information. Of course, we will have to see how they perform over the long term since a lot of the training data would not have used the newer features.</p>
</div>
<div class="readable-text intended-text" id="p105">
<p>When creating tables, GitHub Copilot did a good job naming the columns, and they matched up with the documented field names in most instances, with only a few minor exceptions. This was helpful since we had already created the field names in our Python code to match the NACHA standards, and because we were able to use named parameters in our SQL queries, it was a breeze to transition from returning records to writing out to the database. Another feature that we were continually impressed with (although it did not always work) was the ability of Copilot to assist in writing our unit tests. We had created unit tests for each record that needed to be parsed, and when we broke it down into a dictionary for comparison purposes, Copilot lent a helpful hand and populated the individual fields with our test data. Even though it was off by a character or two, in some cases, it was definitely helpful overall.</p>
</div>
<div class="readable-text intended-text" id="p106">
<p>As we went through and built our unit tests, we did eventually create a <code>SqlUtils</code> class to handle a few of the repetitive tasks. We started by moving the database parameters such as the user, password, and the URL to the routine. Then later, we expanded this to handle taking a parameter of a row factory so that we could return a dictionary of the values. We also created a routine to truncate the table so that repetitive tests had clean tables to work with. Therefore, our assertions would not fail when we expected a single record but retrieved multiple.</p>
</div>
<div class="readable-text intended-text" id="p107">
<p>Similar to removing duplication of code with the <code>SqlUtils</code> class, we also removed hard coding of the table names throughout the unit tests by creating a variable to hold the table name and using Python f-strings to create the SQL commands where necessary (but not for parameter passing). It is important to note that we still used parameterized queries whenever possible, even though we consider this internal tooling and would not expect malicious code to be entered. We did, however, consider the possibility of checking the passed table names against the database information schema to ensure they were valid. However, that seemed a bit overkill for internal tooling.</p>
</div>
<div class="readable-text intended-text" id="p108">
<p>We were bitten several times by neglecting to put the setup/teardown <code>pytest.fixture</code> into the unit test methods as we were coding. This often led to errors when repeatedly running our tests since the database was not always in a clean state. It happened often enough that we considered creating a class hierarchy that would incorporate the clearing of the table so that we would save ourselves from ourselves. However, it felt too early in our process to add that, so we stayed away from it for the moment.</p>
</div>
<div class="readable-text" id="p109">
<h2 class="readable-text-h2"><span class="num-string">5.5</span> Storing exceptions</h2>
</div>
<div class="readable-text" id="p110">
<p>We should now have the basics of working with Python and PostgreSQL. Having successfully stored our sample.ach file in the database should boost our confidence. However, we should have also noted that our exceptions are not being stored in the database yet. We will want to keep track of those as well. It is common for files to be rejected for various reasons, and ACH processors need to be able to determine whether the file can be fixed manually or a new one should be requested from the originating party. Figure 5.4 shows the flow for this section.</p>
</div>
<div class="browsable-container figure-container" id="p111">
<img alt="" height="85" src="../Images/CH05_F04_Kardell.png" style="width: 100%; max-width: max-content;" width="788"/>
<h5 class="figure-container-h5"><span class="">Figure 5.4</span><span class=""> </span><span class="">Flow and associated code listings for this section</span></h5>
</div>
<div class="readable-text" id="p112">
<p>The first order of business is to create the table. We start with a straightforward record that only contains the error description. As our project expands, we may find that we need to break exceptions out by record types, add references to specific records (to help maintain the integrity of the database should records be removed or updated), or implement several other improvements that will become more obvious as we enhance the project. However, those concerns are beyond the current scope of what we need to accomplish, so we will cross that bridge when we come to it in chapter 8.</p>
</div>
<div class="browsable-container listing-container" id="p113">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.15<span class="CharOverride-5"> </span>Simple exceptions table</h5>
<div class="code-area-container">
<pre class="code-area">CREATE TABLE ach_exceptions (
    ach_exceptions_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    exception_description VARCHAR(255) NOT NULL
);</pre>
</div>
</div>
<div class="readable-text" id="p114">
<p>With this table created, we can restart Docker and add a method to our AchFile­Processor.py to insert the record. </p>
</div>
<div class="browsable-container listing-container" id="p115">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.16<span class="CharOverride-5"> </span>Simple method to write the table</h5>
<div class="code-area-container">
<pre class="code-area">    def _add_exception(self, conn: Connection[tuple[Any, ...]],
<span class="CharOverride-8">➥</span> exception: str) -&gt; None: #1
        conn.execute(f"INSERT INTO ach_exceptions#2
<span class="CharOverride-8">➥</span> (ach_exceptions_id, exception_description) "  #2
                     f"VALUES (DEFAULT, %(exception)s)", #2
<span class="CharOverride-8">➥</span> {"exception": exception} )   #2
        return</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Simple method passing a connection and an exception string</span>
<br/>#2 
     <span class="CharOverride-6">Inserts the string into the table</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p116">
<p>With this task completed, we can replace the array we used to hold our exceptions with a single call. We also need to update the various test cases that covered these exceptions, such as an incorrect addenda indicator, invalid record lengths, and ensuring the trace numbers are ascending. Updating these unit tests provides additional opportunities for creating maintainable code.</p>
</div>
<div class="readable-text intended-text" id="p117">
<p>We removed the passing of our exceptions back to the calling routine, so code such as <code>records,</code> <code>exceptions</code> <code>=</code> <code>parser.parse(file_path)</code> becomes simply <code>records</code> <code>=</code> <code>parser.parse(file_path)</code>. However, this change made us immediately retrieve the exceptions because our unit tests were validating the number of exceptions and exception message text. We chose to add another method to <code>SqlUtils</code> to handle this.</p>
</div>
<div class="browsable-container listing-container" id="p118">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.17<span class="CharOverride-5"> </span>Method to retrieve ACH exceptions</h5>
<div class="code-area-container">
<pre class="code-area">    def get_exceptions() -&gt; list:
        with SqlUtils.get_db() as conn:  #1
            exceptions = conn.execute(f"SELECT exception_description
<span class="CharOverride-8">➥</span> FROM ach_exceptions").fetchall() #2
        flattened_list = [item[0] for item in exceptions] #3
        return flattened_list  </pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Used with keyword to get </span>
<span class="CharOverride-6">a database connection</span>
<br/>#2 
     <span class="CharOverride-6">Grabs all exceptions in a single execute command</span>
<br/>#3 
     <span class="CharOverride-6">Returns a list of exceptions instead of tuples</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p119">
<p>With the helper function in place, we can now return the exceptions with <code>exceptions =</code> <code>SqlUtils.get_exceptions()</code>, and the existing logic for the unit test should work without any modifications.</p>
</div>
<div class="readable-text intended-text" id="p120">
<p>As a result of storing our exceptions in a table, we now have multiple tables that need to be truncated in our unit tests. We could continue to call the <code>SqlUtils.truncate()</code> method with the new table. At this point, each test has a maximum of only two tables. However, we would prefer a way to clear all our tables because that will ensure our database is empty for each test. Listing 5.17 showed a truncate-all method that clears our tables. Obviously, this approach should be used with care as we are now truncating all the tables in the database. We could also have dropped and recreated the database; however, this gives us more control since each table is accessed individually. We have worked with projects that have expanded on this type of approach to examine tables for unexpected data, such as determining whether data has unexpectedly been written out to other tables. Of course, our needs will vary from one project to another, and it may be that you do not need or want to truncate the data using this method.</p>
</div>
<div class="browsable-container listing-container" id="p121">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.18<span class="CharOverride-5"> </span>SQL to truncate all tables</h5>
<div class="code-area-container">
<pre class="code-area">DO $$         #1
   DECLARE        #2
      r RECORD;   
BEGIN             #3
   FOR r IN SELECT table_name FROM information_schema.tables
<span class="CharOverride-8">➥</span> WHERE #D  table_schema = 'public' #4
   LOOP                  #4
      EXECUTE 'ALTER TABLE ' || quote_ident(r.table_name) #4
<span class="CharOverride-8">➥</span> || ' DISABLE TRIGGER ALL';  #4
   END LOOP;  #4

   EXECUTE (  #5
      SELECT 'TRUNCATE TABLE ' || string_agg( #5
<span class="CharOverride-8">➥</span>quote_ident(table_name), ', ') || ' CASCADE'   #5
        FROM information_schema.tables #5
       WHERE table_schema = 'public'   #5
   );  #5

   FOR r IN SELECT table_name FROM information_schema.tables
<span class="CharOverride-8">➥</span> WHERE table_schema = 'public' #6
   LOOP #6
      EXECUTE 'ALTER TABLE ' || quote_ident(r.table_name) || #6
<span class="CharOverride-8">➥</span> ' ENABLE TRIGGER ALL';  #6
   END LOOP;  #6
END $$; #7</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Creates an anonymous block of code</span>
<br/>#2 
     <span class="CharOverride-6">Declares a variable of type RECORD, a placeholder for a row that has no predefined structure</span>
<br/>#3 
     <span class="CharOverride-6">Signifies the start </span>
<span class="CharOverride-6">of a new transaction</span>
<br/>#4 
     <span class="CharOverride-6">Disables any triggers on the table</span>
<br/>#5 
     <span class="CharOverride-6">Gathers a list of tables and truncates them</span>
<br/>#6 
     <span class="CharOverride-6">Enables the </span>
<span class="CharOverride-6">triggers again</span>
<br/>#7 
     <span class="CharOverride-6">Commits the current transaction</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p122">
<p>At this point, we should have arrived at a similar point to where we were with our running code from chapter 3. Recall we are able to perform some basic parsing on some simple ACH files, with the major change being that we are now storing the results of our hard work into a database. We can take a moment to congratulate ourselves, but only for a moment, because while we have the ability to parse the file, we do not have a way for a user to load a file. The next sections look at expanding our API to upload an ACH file and see the cascading effect it has on our database.</p>
</div>
<div class="readable-text" id="p123">
<h2 class="readable-text-h2"><span class="num-string">5.6</span> Uploading an ACH file</h2>
</div>
<div class="readable-text" id="p124">
<p>We may consider this our most important change yet. Of course, being able to parse a file is necessary, but it is often allowing interaction with the user that feels the most rewarding for many developers. This is possibly true because being able to work and interact with our project just feels like an accomplishment. And we feel that unit testing gives us much the same rewarding experience, which is why we like testing so much! Figure 5.5 shows the flow for this section.</p>
</div>
<div class="browsable-container figure-container" id="p125">
<img alt="" height="76" src="../Images/CH05_F05_Kardell.png" style="width: 100%; max-width: max-content;" width="863"/>
<h5 class="figure-container-h5"><span class="">Figure 5.5</span><span class=""> </span><span class="">Flow and associated code listings for this section</span></h5>
</div>
<div class="readable-text" id="p126">
<p>We have previously built a basic API using hardcoded values. We will be taking that code and adding the ability to upload an ACH file. From there, we’ll be expanding the APIs to retrieve the data from our database instead of the hardcoded values. In the original ACH dashboard, one of the problems was a lack of control in processing ACH files. </p>
</div>
<div class="readable-text intended-text" id="p127">
<p>Before creating a new API, let’s just ensure we can unit-test one of our existing endpoints with a hardcoded value. The next listing shows what that looks like. Copilot was able to produce the majority of this code as we typed, so it is just a matter of making sure it does what we intended.</p>
</div>
<div class="browsable-container listing-container" id="p128">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.19<span class="CharOverride-5"> </span>A unit test for FastAPI</h5>
<div class="code-area-container">
<pre class="code-area">from fastapi.testclient import TestClient #1

from app.main import app  #2

client = TestClient(app) #3

def test_read_files(): #4
    <span>response = client.get(</span><span>"</span><span>/api/v1/files</span><span>"</span><span>) </span>#5
<span>    assert response.status_code == 200  </span>#6
<span>    assert response.json() == [{</span><span>"</span><span>file</span><span>"</span><span>: </span><span>"</span><span>File_1</span><span>"</span><span>},</span> #6
<span class="CharOverride-8">➥</span> {<span>"</span>file<span>"</span>: <span>"</span>File_2<span>"</span>}] </pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Imports the TestClient needed for unit testing</span>
<br/>#2 
     <span class="CharOverride-6">Imports our application</span>
<br/>#3 
     <span class="CharOverride-6">Defines a client</span>
<br/>#4 
     <span class="CharOverride-6">Defines a method for testing</span>
<br/>#5 
     <span class="CharOverride-6">Calls our endpoint and saves the response</span>
<br/>#6 
     <span class="CharOverride-6">Assert statements validate the response</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p129">
<p>So, that was pretty easy. Now, we would like to work on being able to <code>POST</code> an ACH file to the backend, get a response that it has been uploaded, and then begin parsing the file. Why not parse the file and then return a response to the user? The processing of our ACH file may take some time, especially when we consider that it will eventually need to interact with other systems to execute tasks such as verifying accounts/balances, OFAC validation, fraud analysis, and any other tasks an ACH processor may want to perform. Rather than have a user wait, we can verify the file is uploaded and start a task to perform the parsing.</p>
</div>
<div class="readable-text intended-text" id="p130">
<p>We already have an endpoint for posting a file, so we can add a unit test and then upload the file. Since the file is going to be uploaded using an <code>HTTP</code> <code>POST</code> request, it will be encoded using <code>multipart-formdata</code>. This means that we need the package <code>python-multipart</code> because this is a requirement for parsing requests encoded with that format. If we do not have it installed, we will receive a friendly reminder (in the form of an error).</p>
</div>
<div class="browsable-container listing-container" id="p131">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.20<span class="CharOverride-9"> </span><code>python-multipart</code> not installed</h5>
<div class="code-area-container">
<pre class="code-area">E   RuntimeError: Form data requires "python-multipart" to be installed. 
E   You can install "python-multipart" with: 
E   
E   pip install python-multipart</pre>
</div>
</div>
<div class="readable-text" id="p132">
<p>Creating a test for uploading should look something like the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p133">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.21<span class="CharOverride-5"> </span>Unit test for uploading a file</h5>
<div class="code-area-container">
<pre class="code-area">def test_upload_file():
    with open("data/sample.ach", "rb") as test_file:  #1
        <span>response = client.post("/api/v1/files",  </span>#2
<span>                   </span>files={"file": test_file})    
    assert response.status_code == 201  #3</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Opens the sample file</span>
<br/>#2 
     <span class="CharOverride-6">Uses the client </span>
<span class="CharOverride-6">to POST the file</span>
<br/>#3 
     <span class="CharOverride-6">Ensures we receive </span>
<span class="CharOverride-6">a 20</span>
<span class="CharOverride-11">1</span>
<span class="CharOverride-6"> status</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p134">
<p>We will also need to update the endpoint to receive the file. There is no need to return anything as we are just interested in the status code.</p>
</div>
<div class="browsable-container listing-container" id="p135">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.22<span class="CharOverride-5"> </span>Updated endpoint to receive the file</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">from fastapi import APIRouter, Request, status,
<span class="CharOverride-8">➥</span> <strong>File, UploadFile </strong>#1
@router.post("", status_code=status.HTTP_201_CREATED)
async def create_file(file: UploadFile = File(...)): #2
    return None  #3</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Additional imports </span>
<span class="CharOverride-6">are needed.</span>
<br/>#2 
     <span class="CharOverride-6">Defines the file </span>
<span class="CharOverride-6">as UploadFile</span>
<br/>#3 
     <span class="CharOverride-6">Returns nothing; only the status code as necessary</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p136">
<p>Now we should have a successful unit test, and the real work can begin. Uploading a file manually or through some automatic process should be the driving force behind all our ACH tables. We should store some file information such as filename, time uploaded, a file hash (for tracking of duplicate files), and whatever other information we may decide is needed. The UUID of this record should then be included in any child tables (all the previous tables we have just created). If we had more experience or worked through the problem in a different order (perhaps uploading a file first), we may have avoided having more rework, but we would have also had to introduce more database concepts to start with. A benefit of this approach is the actual rework required to incorporate the changes. Often, developers become paralyzed because of the fear of change. Large complex systems sometimes seem like a house of cards where one wrong move can cause everything to come tumbling down. Having good unit test coverage and confidence in the tests can go a long way to alleviate these fears. The fear of changing and improving working software eventually leads to code rot.</p>
</div>
<div class="readable-text intended-text" id="p137">
<p>We will repurpose our <code>ach_files</code> table to be our main table that contains the upload information and rename <code>ach_files</code> to <code>ach_records</code> so that its only job is to store the unparsed ach records. The updated table definitions are in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p138">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.23<span class="CharOverride-5"> </span>Updated table listings for <code>ach_files</code> and <code>ach_records</code></h5>
<div class="code-area-container">
<pre class="code-area">CREATE TABLE ach_files (         #1
    ach_files_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(), 
    file_name VARCHAR(255) NOT NULL, 
    file_hash VARCHAR(32) NOT NULL, 
    created_at TIMESTAMP NOT NULL DEFAULT NOW(), 
); #2

CREATE TABLE ach_records (  #3
    ach_records_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ach_files_id UUID NOT NULL REFERENCES ach_files(ach_files_id)
<span class="CharOverride-8">➥</span> ON DELETE CASCADE ON UPDATE CASCADE, #4
    <del>file_name VARCHAR(255) NOT NULL,</del>  #5
    unparsed_record VARCHAR(94) NOT NULL,
    sequence_number INTEGER NOT NULL
);</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">The ach_files table is duplicated and repurposed to store the upload details.</span>
<br/>#2 
     <span class="CharOverride-6">The ach_files table is duplicated and repurposed to store the upload details.</span>
<br/>#3 
     <span class="CharOverride-6">The former ach_files will become ach_records.</span>
<br/>#4 
     <span class="CharOverride-6">Creates a foreign key named ach_files_id that references the ach_files_id from ach_files</span>
<br/>#5 
     <span class="CharOverride-6">Removes the file_name as it is now stored in the ach_files table</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p139">
<p>The new table <code>ach_records</code> uses the keywords <code>REFERENCES</code>, <code>ON</code> <code>DELETE</code>, and <code>ON</code> <code>UPDATE</code> to create a foreign key to the <code>ach_files</code> table. This feature allows the database to maintain its referential integrity. For instance, when we delete an ACH file out of <code>ach_files</code>, we do not want to go into every table and delete associated data. Instead, we want to define a relationship between the tables where all associated data is removed if we were to delete the <code>ach_file</code>. Once we have completed updating our tables, we can see this in action. This will also affect our tests. Once we have implemented referential integrity, we will need to ensure the <code>FOREIGN</code> <code>KEY</code> constraints are maintained. </p>
</div>
<div class="readable-text intended-text" id="p140">
<p>For example, if we want to have a unit test write out records to the <code>ach_records</code> table. we need a valid <code>ach_files_id</code> (it must exist in the <code>ach_files</code> table). For this reason, we will likely be looking at expanding the <code>SqlUtils</code> class we had developed earlier to set up some generic records and make this easier. Maintaining referential integrity can mean extra work on our side when setting up for testing, but it is worth implementing. We have worked in systems with the benefit of referential integrity and have seen programs crash or loop because of incomplete relationships. Usually, a lack of referential integrity is apparent in a system where developers have written various utility programs to scan and fix the data (which we have had to do ourselves on more than one occasion).</p>
</div>
<div class="readable-text intended-text" id="p141">
<p>Additionally, we want to store the MD5 (or whichever algorithm you prefer) hash of the ACH file so we to identify duplicate files. First, we can get the hash from the command line using <code>Get-FileHash</code> <code>-Path</code> <code>".\sample.ach"</code> <code>-Algorithm</code> <code>MD5</code> <code>|</code> <code>Select-Object</code> <code>-ExpandProperty</code>, which in our case prints out <code>18F3C08227C3A60D418B3772213A94E3</code>. We’ll keep this info handy because we’ll be storing the hash as computed with Python in the database and expect it to be the same.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>Interestingly enough, when coding the function, Copilot prompted us to use string interpolation for the SQL query, <code>f"INSERT</code> <code>INTO</code> <code>ach_files</code> <code>(file_name,</code> <code>md5_hash) VALUES</code> <code>('{file.filename}',</code> <code>'{md5_hash}'),</code> but we continue with our standard parameterized query because we want to maintain secure coding practices. Queries that use string interpolation are ripe for SQL injection (<a href="https://mng.bz/QDNj"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/QDNj</span></a>). Our code for the <code>/api/v1/files</code> endpoint is shown in the next listing.</p>
</div>
<div class="browsable-container listing-container" id="p143">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.24<span class="CharOverride-5"> </span>Updated endpoint to upload ACH files</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">@router.post("", status_code=status.HTTP_201_CREATED)
async def create_file(file: UploadFile = File(...)):
    <strong>contents = await file.read()  </strong>#1
<strong>    md5_hash  = hashlib.md5(contents).hexdigest() </strong>#2
<strong>    with DbUtils.get_db_connection() as conn: </strong>#3
<strong>        conn.execute( </strong>#4
<strong>            f"""  </strong> #4
<strong>            INSERT INTO ach_files (file_name, file_hash) </strong> #4
<strong>            VALUES (%s, %s) </strong> #4
<strong>            """, (file.filename, md5_hash) </strong> #4
<strong>        ) </strong> #4
    return None #4</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Reads the uploaded file</span>
<br/>#2 
     <span class="CharOverride-6">Uses hashlib to create a hash of the file</span>
<br/>#3 
     <span class="CharOverride-6">Uses our newly created DbUtils to return a connection</span>
<br/>#4 
     <span class="CharOverride-6">Simple SQL to INSERT a record into ach_files</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p144">
<p>We created a simple <code>DbUtils</code> class to return a database connection. We imagine it will be used in multiple places in our project and thus extracted it into its own file.</p>
</div>
<div class="browsable-container listing-container" id="p145">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.25<span class="CharOverride-9"> </span><code>DbUtils</code> for creating a database connection</h5>
<div class="code-area-container">
<pre class="code-area"><span>import psycopg</span>

<span>POSTGRES_USER = "someuser"     </span>#1
<span>POSTGRES_PASSWORD = "supersecret" </span> #1
 #1
<span>DATABASE_URL = f"dbname={POSTGRES_USER} user={POSTGRES_USER}</span> #1
<span class="CharOverride-8">➥</span><span>  password={POSTGRES_PASSWORD} host=localhost port=5432" </span> #1

def get_db_connection(row_factory = None):  #2
    conn = psycopg.connect(DATABASE_URL,  #2
<span class="CharOverride-8">➥</span>row_factory=row_factory)  #2
    return conn  </pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Hardcoded values to be removed later</span>
<br/>#2 
     <span class="CharOverride-6">Creates and returns a connection</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p146">
<p>With these changes, we should be able to run the unit test and then head over to our CloudBeaver UI and see the newly added record and manually compare the record hash. However, manually checking the record is no fun, so as a final piece, we should update our unit test to incorporate the same <code>SqlUtils</code>, as we have seen previously, and ensure that we have at least one record in our table. The following listing shows the updated unit test.</p>
</div>
<div class="browsable-container listing-container" id="p147">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.26<span class="CharOverride-5"> </span>Updated unit test for our upload file</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">…
<strong>from tests.SqlUtils import SqlUtils </strong>#1
<strong>import pytest</strong>
…
<strong>@pytest.fixture   </strong>#2
<strong>def setup_teardown_method(): </strong> #2
<strong>    yield</strong> #2
<strong>    SqlUtils.truncate_all() </strong> #2
…
def test_upload_file(<strong>setup_teardown_method</strong>): #3
    with open("data/sample.ach", "rb") as test_file:
        response = client.post("/api/v1/files", files={"file": test_file})
    assert response.status_code == 201
<strong>    assert SqlUtils.get_row_count_of_1('ach_files') </strong>#4</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Required imports</span>
<br/>#2 
     <span class="CharOverride-6">Defines our fixture to ensure all tables are truncated</span>
<br/>#3 
     <span class="CharOverride-6">Includes the fixture </span>
<span class="CharOverride-6">in our unit test</span>
<br/>#4 
     <span class="CharOverride-6">Ensures there is one row in the ach_files table</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p148">
<p>Being able to upload a file successfully may not seem like a big deal, but we must remember that all other functionality of our ACH dashboard is based on this one piece. Functionality such as error checking and recovery, crediting/debiting accounts, fraud analysis, and much more is available to us now that we can upload ACH files. After completing this task, we can expand on the database structure.</p>
</div>
<div class="readable-text" id="p149">
<h2 class="readable-text-h2"><span class="num-string">5.7</span> Storing records with integrity</h2>
</div>
<div class="readable-text" id="p150">
<p>As mentioned before, we want our database to have referential integrity, which can simplify navigation in database tools, and more importantly, have the ability to prevent dangling records. We have seen that databases without referential integrity require maintenance programs that run periodically to clean up those dangling records, as well as infinite loops and program crashes resulting from incomplete data. A great benefit of using relational databases is their support for data integrity. We highly recommend keeping it in mind when designing databases.</p>
</div>
<div class="readable-text intended-text" id="p151">
<p>In our examples so far, we have defined a <code>PRIMARY</code> <code>KEY</code> and how to update the <code>ach_records</code> table. In listing 5.23, we added our first <code>FOREIGN</code> <code>KEY</code> by using the <code>REFERENCES</code> keyword, which starts us on the path of maintaining referential integrity. We continue to update the remaining tables and code with foreign keys and review the effects of using them on our tables, code, and unit tests.</p>
</div>
<div class="readable-text intended-text" id="p152">
<p>If we try running our unit tests at this point, we will see errors referring to an <code>UndefinedColumn</code> since we updated our table definitions in listing 5.23. This is not a bad thing. The fact that we have built unit tests allows us to refactor our code with confidence. We start with diving into our <code>test_record_count</code> unit test (from listing 5.7) to resolve our error of <code>psycopg.errors.UndefinedColumn:</code> <code>column</code> <code>"unparsed_record" of</code> <code>relation</code> <code>"ach_files"</code> <code>does</code> <code>not</code> <code>exist</code>. The offending code is shown in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p153">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.27<span class="CharOverride-5"> </span>Invalid column names</h5>
<div class="code-area-container">
<pre class="code-area">conn.execute("""
                INSERT INTO ach_files
[CA] (file_name, unparsed_record, sequence_number) 
                VALUES (%s, %s, %s)
                """, (os.path.basename(filename), line, sequence_number))</pre>
</div>
</div>
<div class="readable-text" id="p154">
<p>Depending on your IDE and setup, this statement may already be flagged. In PyCharm, we have defined a connection to the database, so the fields <code>unparsed_record</code> and <code>sequence_number</code> are flagged with an <code>Unable</code> <code>to</code> <code>resolve</code> <code>column</code> error. If your IDE does not have that capability, then the stack trace also shows the line number. Given this information, we notice that the <code>INSERT</code> statement is pointing to the wrong table as we had renamed <code>ach_files</code> to <code>ach_records</code>. Changing that and rerunning the test gives a new error <code>psycopg.errors.NotNullViolation:</code> <code>null</code> <code>value</code> <code>in</code> <code>column "ach_files_id"</code> <code>of</code> <code>relation</code> <code>"ach_records"</code> <code>violates</code> <code>not-null</code> <code>constraint</code>. We defined <code>ach_files_id</code> as a foreign key, and it cannot be null—in fact, it must point to a valid record in the <code>ach_files</code> table. Recall that our API will create this record when a file is uploaded. So, it is plausible that the <code>AchFileProcessor</code> will now need to be called with both the filename being parsed and a valid <code>ach_files_id</code>. We can update the unit test so that the parse routine is called with this ID. The <code>parser.parse(file_path)</code> will need to become <code>parser.parse(ach_files_id,</code> <code>file_path)</code>, but it is not enough to define <code>ach_files_id</code>. We need a way to create a valid <code>ach_files_id</code> because we have to maintain database integrity. We could drop the constraints during testing, which may be an option if our unit tests were primarily concerned with functionality. However, in this case, we would like to maintain the constraint. So, we will need to create a function to insert the record, then take a step back and consider our path forward as there may be a few different routes we want to take.</p>
</div>
<div class="readable-text intended-text" id="p155">
<p>Remember that we already coded an <code>INSERT</code> statement to insert the <code>ach_files</code> record into our API. We could duplicate that query in our unit test and then grab that inserted value. That would work, but we will have duplicated code, and we want to try to avoid that whenever possible. We could add a routine to <code>SqlUtils</code> to handle this task for us, which other methods would have access to because other unit tests may also need the functionality. However, <code>SqlUtils</code> is meant to assist us in our tests only, and we have already seen that we will need this functionality elsewhere. Perhaps we should create a utility class for the <code>AchFileProcessor</code> to insert our desired record, which would also allow us to refactor the existing SQL queries out of the parsing routines. There is no definitive answer. Depending on the project, our needs, programming style, and experience, we may see other alternatives or have preferences on how to approach this problem. In this case, we believe that using Pydantic is the best way to move forward.</p>
</div>
<div class="readable-text" id="p156">
<h2 class="readable-text-h2"><span class="num-string">5.8</span> Using Pydantic</h2>
</div>
<div class="readable-text" id="p157">
<p>In the previous section, we ran into an problem of how to handle inserting a record to make our unit test work. While there are several approaches we could take, Pydantic will help further refactoring and expansion of our project. We had introduced Pydantic previously to help document our APIs in chapter 4, so we know that it already has some benefits. Another benefit is that we will be able to define our tables and fields in a way that allows developers not to have to remember which fields are present. In other words, we start abstracting the SQL from the parsing logic. Let’s start applying that with our <code>ach_files</code> table so that we can create a record and return a valid record ID, which is all we really wanted to do to get around our first unit test problem.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p158">
<h5 class="callout-container-h5 readable-text-h5">Why is there no object-relational model framework?</h5>
</div>
<div class="readable-text" id="p159">
<p>An object-relational model (ORM) has many benefits and is widely used in many different industries. We are holding off on incorporating an ORM such as SQLAlchemy for now because we want to ensure the reader is exposed directly to SQL in case those skills need brushing up on. </p>
</div>
<div class="readable-text" id="p160">
<p>Later, we will show how to include an ORM into the project, so hang in there. Of course, if you are familiar with ORMs, you can dive right in and start using them.</p>
</div>
</div>
<div class="readable-text" id="p161">
<p>We first create a Pydantic model for our table, as shown in the following listing. This simple model provides a basic layout for modeling the data that’ll be written to our <code>ach_files</code> table.</p>
</div>
<div class="browsable-container listing-container" id="p162">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.28<span class="CharOverride-5"> </span>Our Pydantic model for <code>ach_files</code></h5>
<div class="code-area-container">
<pre class="code-area">from typing import Optional           #1
from datetime import datetime          #1
from pydantic import BaseModel, UUID4 #1


class AchFileSchema(BaseModel):  #2
    id: Optional[UUID4] = None   #3
    file_name: str                #3
    file_hash: str                #3
    created_at: Optional[datetime] = None #3</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Necessary import statements</span>
<br/>#2 
     <span class="CharOverride-6">The class extends the Pydantic BaseModel.</span>
<br/>#3 
     <span class="CharOverride-6">Our field definitions; note the Optional keyword for fields (such as ID) that our database will supply.</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p163">
<p>Next, we can go ahead and define a class that will provide some basic Create, Read, Update, and Delete (CRUD) methods to handle working with the database table. The following listing shows the <code>AchFileSql</code> class we will create to wrap the logic for dealing with our <code>ach_files</code> database table.</p>
</div>
<div class="browsable-container listing-container" id="p164">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.29<span class="CharOverride-9"> </span><code>AchFileSql</code> class</h5>
<div class="code-area-container">
<pre class="code-area">from typing import Optional  #1
from uuid import UUID    #1
from psycopg.rows import dict_row #1
from ach_processor.database import DbUtils #1
from ach_processor.schemas.AchFileSchema  #1
<span class="CharOverride-8">➥</span>import AchFileSchema #1


class AchFileSql:
    def insert_record(self, ach_file: AchFileSchema)
<span class="CharOverride-8">➥</span> -&gt; UUID: #2
        with DbUtils.get_db_connection() as conn:
            result = conn.execute(
                """
                INSERT INTO ach_files(ach_files_id,
<span class="CharOverride-8">➥</span> file_name, file_hash, created_at)
                               VALUES 
<span class="CharOverride-8">➥</span>(DEFAULT, %(file_name)s, %(file_hash)s, DEFAULT)
                               RETURNING ach_files_id #3
                                """, ach_file.model_dump()) #4

        return result.fetchone()[0]

    def get_record(self, ach_file_id: UUID)
<span class="CharOverride-8">➥</span> -&gt; AchFileSchema: #5
        with DbUtils.get_db_connection(row_factory=class_row(AchFileSchema))
<span class="CharOverride-8">➥</span> as conn: #6
            result = conn.execute(
                """
                SELECT * FROM ach_files WHERE ach_files_id = %s
                """, [ach_file_id.hex])

            record = result.fetchone()

            if not record: #7
                raise KeyError(f"Record with id #7
<span class="CharOverride-8">➥</span> {ach_file_id} not found")  #7

            return record</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Required imports </span>
<span class="CharOverride-6">for the class</span>
<br/>#2 
     <span class="CharOverride-6">Creates a function to insert the record and return the UUID</span>
<br/>#3 
     <span class="CharOverride-6">Uses the RETURNING keyword to return the ID of the newly inserted record</span>
<br/>#4 
     <span class="CharOverride-6">Uses </span>
<span class="CharOverride-6">model_dump </span>
<span class="CharOverride-6">to create a </span>
<span class="CharOverride-6">dictionary that references </span>
<span class="CharOverride-6">the fields</span>
<br/>#5 
     <span class="CharOverride-6">Creates a function to return a specified record</span>
<br/>#6 
     <span class="CharOverride-6">By using the row_factory of class_row, we can return the record directly.</span>
<br/>#7 
     <span class="CharOverride-6">If nothing is found, raises an error</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p165">
<p>Finally, we create a unit test to verify our newly created classes. We create this unit test as a class just to show another way to help organize our tests. We also introduce the <code>autouse</code> option on our <code>pytest</code> fixtures so we do not have to include them in every method.</p>
</div>
<div class="browsable-container listing-container" id="p166">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.30<span class="CharOverride-5"> </span>Unit testing our new classes</h5>
<div class="code-area-container">
<pre class="code-area">import pytest  #1
from tests.SqlUtils import SqlUtils #1
from ach_processor.database.AchFileSql import AchFileSql #1
from ach_processor.schemas.AchFileSchema #1
<span class="CharOverride-8">➥</span> import AchFileSchema #1


class TestAchFileSql:
    @pytest.fixture(autouse=True)   #2
    def setup_teardown_method(self):
        print("\nsetup test\n")
        yield
        SqlUtils.truncate_all()  #3

    def test_insert_record(self):
        ach_file_record = AchFileSchema(
            file_name="sample.ach",
            file_hash="1234567890"
        )
        sql = AchFileSql()
        ach_file_id = sql.insert_record(ach_file_record)  #4
        retrieved_record = sql.get_record(ach_file_id)    #5
        assert SqlUtils.get_row_count_of_1("ach_files")
<span class="CharOverride-8">➥</span> is True, "Expected 1 record" #6
        assert retrieved_record.file_name == ach_file_record.file_name, #7
<span class="CharOverride-8">➥</span> f"Expected {ach_file_record.file_name}, but got <br/>{retrieved_record.file_name}"  #7</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Required imports</span>
<br/>#2 
     <span class="CharOverride-6">Our fixture now </span>
<span class="CharOverride-6">uses autouse.</span>
<br/>#3 
     <span class="CharOverride-6">Truncates the </span>
<span class="CharOverride-6">tables when finished</span>
<br/>#4 
     <span class="CharOverride-6">Inserts the record</span>
<br/>#5 
     <span class="CharOverride-6">Immediately </span>
<span class="CharOverride-6">returns the record</span>
<br/>#6 
     <span class="CharOverride-6">Assert statements validate our results</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p167">
<p>This is a great start to taking our project to the next level. You may be wondering why we did not write our test first in a test-driven development manner. Sometimes, it is easier to take a test-later approach, especially when demonstrating new concepts. Again, it is not so about writing the test but about working in a short cycle. So, as soon as we got something to test, we started testing it.</p>
</div>
<div class="readable-text intended-text" id="p168">
<p>Recall that with Pydantic, we get field validation and the ability to document our APIs. We will be looking at that later, but for now, we should continue refactoring our code to take advantage of Pydantic. Once we have the code refactored using Pydantic and our unit tests passing, we can move along to handling APIs.</p>
</div>
<div class="readable-text" id="p169">
<h2 class="readable-text-h2"><span class="num-string">5.9</span> Lessons learned</h2>
</div>
<div class="readable-text" id="p170">
<p>The process of including Pydantic and separating our code required us to refactor not only the ach_file_processor.py but our unit tests as well. Refactoring allowed us to improve our code in both areas and obtain code that is cleaner and easier to test. Unfortunately, we also ran into some problems when we finished creating a database structure that followed the original specification from figure 5.1. Did you notice any problems with the structure before refactoring the code?</p>
</div>
<div class="readable-text intended-text" id="p171">
<p>Inserting foreign keys into that database structure revealed a problem with maintaining data integrity in the <code>ach_files</code> table. While deleting a record from those child tables would behave correctly, the <code>ach_files</code> table did not get rid of all the records we desired. For instance, if we were to delete a batch, we would expect the associated type 6–8 records to be removed as well, and this cannot happen in the current structure.</p>
</div>
<div class="readable-text intended-text" id="p172">
<p>This situation is not uncommon. Often, when given specs for a project—depending on the experience of the project stakeholders and the time allowed for researching areas of the project—it may not be possible to flush out all the design details. In a scaled Agile context, we may consider this project an enabler—more specifically, an <span class="Italics">exploration enabler.</span> Exploration enablers help explore prospective solutions and include activities such as research and prototyping. If these are not completed sufficiently before we got a project, we may run into problems we had here. There are several alternatives to handle the situation, and the right answer will likely depend on the reevaluation of project requirements.</p>
</div>
<div class="readable-text intended-text" id="p173">
<p>To recap the project requirements, we wanted to find a solution that</p>
</div>
<ul>
<li class="readable-text" id="p174">Provided both ETL and ELT options when processing a database.</li>
<li class="readable-text" id="p175">Provided data integrity over the entire file. As the ACH file is a hierarchy, there are multiple scenarios that require additional records to be cleaned up when a file, batch, or entry are deleted.</li>
</ul>
<div class="readable-text" id="p176">
<p>Let’s propose a few different options we may need to present to the business stakeholders.</p>
</div>
<div class="readable-text intended-text" id="p177">
<p>First, we could ditch parsing the individual records into the database and just store the unparsed records. This would certainly simplify the database design as we only have one table to deal with. Of course, this approach limits the usefulness of having a relational database and would require additional application code to handle the data integrity we mentioned earlier. For instance, if the user wanted to delete a batch, we would have to ensure that all the records were deleted by the application code, which would expand and complicate our application code.</p>
</div>
<div class="readable-text intended-text" id="p178">
<p>Second, we could ditch storing unparsed records in the table. This could potentially be a good solution if we determine we do not need the unparsed records. Of course, it also means that our file needs to conform to the database constraints, and the business has already required the unparsed records to be retained in the event of invalid data (such as non-numeric data in a numeric field) that may cause a parsed record to be rejected from the database. This seems to be a hard requirement to get around.</p>
</div>
<div class="readable-text intended-text" id="p179">
<p>Third, we could investigate setting up a <span class="Italics">database trigger</span> to delete the records. Database triggers are code that can be automatically executed when certain events occur in the database. We may be able to create triggers in the parsed record tables that execute when a record is deleted to also eliminate the associated unparsed record. That does not sound like much fun though.</p>
</div>
<div class="readable-text intended-text" id="p180">
<p>We opted for another route to address this problem—reorganizing the table into the structure where we used an unparsed record table for each type. This required substantial refactoring of the tables and associated application code, but for us, it made the most sense if we wanted to maintain the requirements for keeping unparsed and parsed records. The updated database diagram is shown in figure 5.6, along with a reference to the original layout from figure 5.1.</p>
</div>
<div class="readable-text intended-text" id="p181">
<p>This structure gave us the data integrity we were originally looking for, but we now have to find a way to view all the unparsed records. To accomplish this, we create a database view. A <span class="Italics">database view</span> is a virtual table that is the result of a stored query. By using a view, we can save ourselves and others who need to use our database the tedious task of tying the unparsed data together. The next listing shows the created database view. </p>
</div>
<div class="browsable-container listing-container" id="p182">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.31<span class="CharOverride-5"> </span>Creating a database view</h5>
<div class="code-area-container">
<pre class="code-area">CREATE VIEW combined_ach_records AS
SELECT 
    r1.ach_records_type_1_id AS primary_key, 
    r1.unparsed_record, 
    r1.sequence_number,
    r1.ach_files_id
FROM 
    ach_records_type_1 AS r1

UNION ALL                 #1

SELECT 
    r5.ach_records_type_5_id, 
    r5.unparsed_record, 
    r5.sequence_number,
    r1_r5.ach_files_id
FROM 
    ach_records_type_5 AS r5
JOIN ach_records_type_1 AS r1_r5 
    USING (ach_records_type_1_id) #2
 #3

…
UNION ALL

SELECT 
       r9.ach_records_type_9_id, 
       r9.unparsed_record, 
       r9.sequence_number,
       r1_r9.ach_files_id
  FROM 
       ach_records_type_9 AS r9
  JOIN ach_records_type_1 AS r1_r9 
 USING (ach_records_type_1_id)</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">UNION ALL will combine the results from the successive SELECT statements.</span>
<br/>#2 
     <span class="CharOverride-6">The USING statements allow for a more concise syntax instead of ON table.field =.table.field. It can be used in PostgreSQL when the joining fields share the same name among tables.</span>
<br/>
</div>
</div>
</div>
<div class="browsable-container figure-container" id="p183">
<img alt="" height="435" src="../Images/CH05_F06_Kardell.png" style="width: 100%; max-width: max-content;" width="859"/>
<h5 class="figure-container-h5"><span class="">Figure 5.6</span><span class=""> </span><span class="">Updated table diagram</span></h5>
</div>
<div class="readable-text" id="p184">
<p>This code allows us to view the entire ACH file as it was uploaded to the system, without any manipulation from our processing. Having a way to view the raw file allows us to present users with options to view the entire ACH file contents, as well as exception records to help them troubleshoot any problems. </p>
</div>
<div class="readable-text" id="p185">
<h2 class="readable-text-h2"><span class="num-string">5.10</span> Coding changes</h2>
</div>
<div class="readable-text" id="p186">
<p>Now that we have settled on this database structure, it was a matter of writing any additional unit tests and getting the existing ones updated to run correctly. Let’s look at some of the changes made to support this structure.</p>
</div>
<div class="readable-text" id="p187">
<h3 class="readable-text-h3"><span class="num-string">5.10.1</span> Creating Pydantic schema for the unparsed ACH records</h3>
</div>
<div class="readable-text" id="p188">
<p>All the unparsed records share the <code>unparsed_record</code> and sequence number fields. Therefore, this is a good opportunity to create a class structure that will inherit those fields so that we do not have to type them every time. We create an <code>AchRecordBaseSchema</code>.</p>
</div>
<div class="browsable-container listing-container" id="p189">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.32<span class="CharOverride-5"> </span>Base schema for our ACH records</h5>
<div class="code-area-container">
<pre class="code-area">from abc import ABC  #1
from pydantic import BaseModel #2

class AchRecordBaseSchema(ABC, BaseModel):  #3
    unparsed_record: str    #4
    sequence_number: int  </pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Importing ABC allows us to create an abstract class.</span>
<br/>#2 
     <span class="CharOverride-6">The BaseModel is </span>
<span class="CharOverride-6">required for Pydantic.</span>
<br/>#3 
     <span class="CharOverride-6">Our class inherits from both ABC and BaseModel.</span>
<br/>#4 
     <span class="CharOverride-6">Fields that will be present </span>
<span class="CharOverride-6">in all ACH record classes</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p190">
<p>With that schema, we can define each record type, as shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p191">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.33<span class="CharOverride-5"> </span>ACH record schema</h5>
<div class="code-area-container">
<pre class="code-area">from typing import Optional #1
from pydantic import UUID4 #1
 #1
from ach_processor.schemas.ach_record.ach_record_base_schema import AchRecordBaseSchema #1
 #1
class AchRecordType1Schema(AchRecordBaseSchema): #2
    ach_records_type_1_id: Optional[UUID4] = None #3
    ach_files_id: UUID4 #4</pre>
<div class="code-annotations-overlay-container">
     #1 
     <span class="CharOverride-6">Required </span>
<span class="CharOverride-6">imports</span>
<br/>#2 
     <span class="CharOverride-6">This is a Pydantic class by virtue of being a subclass of AchRecordBaseSchema, which was a subclass of BaseModel.</span>
<br/>#3 
     <span class="CharOverride-6">We have an ID field that is marked as optional since it will be assigned by the database.</span>
<br/>#4 
     <span class="CharOverride-6">The ach_files_id field is passed in and is not optional as it is a foreign key to the file that was uploaded.</span>
<br/>
</div>
</div>
</div>
<div class="readable-text" id="p192">
<h3 class="readable-text-h3"><span class="num-string">5.10.2</span> Creating Pydantic schema for the parsed ACH records</h3>
</div>
<div class="readable-text" id="p193">
<p>The Pydantic definitions for each of the parsed records are less interesting at this point as we just inherit from the Pydantic <code>BaseModel</code> record and define the necessary fields. We will expand on the fields for constraints, validation, and documentation later. For now, we just keep them simple.</p>
</div>
<div class="browsable-container listing-container" id="p194">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.34<span class="CharOverride-5"> </span>Pydantic schema for an ACH batch header record</h5>
<div class="code-area-container">
<pre class="code-area">from pydantic import BaseModel, UUID4


class AchBatchHeaderSchema(BaseModel):
    ach_records_type_5_id: UUID4
    record_type_code: str
    service_class_code: str
    company_name: str
    company_discretionary_data: str
    company_identification: str
    standard_entry_class_code: str
    company_entry_description: str
    company_descriptive_date: str
    effective_entry_date: str
    settlement_date: str
    originator_status_code: str
    originating_dfi_identification: str
    batch_number: str</pre>
</div>
</div>
<div class="readable-text" id="p195">
<h3 class="readable-text-h3"><span class="num-string">5.10.3</span> Unit test changes</h3>
</div>
<div class="readable-text" id="p196">
<p>As part of code refactoring, we ensured that our tests were cleaned up as well (listing 5.35). First, we updated our <code>setup_teardown_method</code> to set <code>autouse</code> to <code>true</code> and then ensure the <code>SqlUtils.truncate_all</code> method was executed first. We may have previously chosen to clear the tables after the tests were run, which is a good practice that cleans up any data from our test. However, it also has the unfortunate side effect of cleaning up the data when tests fail, which is not very helpful when we want to examine the database after a test. To make debugging and troubleshooting easier, we decided to clear the data before the test. This also ensures the database is ready as we do not have to rely on a previous test to clean up after itself. Adding the <code>autouse</code> parameters means we no longer need to pass the fixture to our tests. We also used <code>truncate_all</code> instead of a specific table since we now have multiple tables being used.</p>
</div>
<div class="browsable-container listing-container" id="p197">
<h5 class="listing-container-h5 browsable-container-h5">Listing 5.35<span class="CharOverride-5"> </span>Updated <code>pytest</code> fixture</h5>
<div class="code-area-container">
<pre class="code-area">@pytest.fixture(autouse=True)
def setup_teardown_method():
    SqlUtils.truncate_all()
    yield</pre>
</div>
</div>
<div class="readable-text" id="p198">
<h2 class="readable-text-h2"><span class="num-string">5.11</span> Design and different approaches</h2>
</div>
<div class="readable-text" id="p199">
<p>It was a fair amount of rework to add foreign keys to the database, so we may be thinking about ways to minimize that work or why we did not just add them in the beginning. As you saw, we dove into parsing the ACH file, which we accomplished, and then moved on to supporting some additional functionality that required a lot of rework. This was partly because we wanted to show some ACH basics before getting into overall functionality and additional database knowledge.</p>
</div>
<div class="readable-text intended-text" id="p200">
<p>However, let’s consider whether we approached the problem from a functionality standpoint and whether the user was going to need these files uploaded. Had we then started with the <code>ach_files</code> table and associated endpoint, we could have incorporated foreign keys from the beginning. Assuming you had knowledge and experience working with ACH files and APIs, it would certainly have been a valid approach. Then we could have proceeded in much the same fashion, with the exception that we may have had a better-designed database from the start.</p>
</div>
<div class="readable-text intended-text" id="p201">
<p>This just goes to show that initial design can be important as well as the approach to the problem you take. We spoke earlier about enabler stories, or what is also known as research spikes. They reduce risk and help us understand requirements and otherwise gain a better understanding of the tasks that will need to be performed. We cannot stress the importance of these types of stories, especially when working with large complex systems. We can always expect some rework to be required when we get into a project, either due to unidentified requirements or changes in scope that could not be accounted for. Hopefully, research spikes help minimize those instances, but it can often be difficult for a business to devote time to something they do not immediately see a benefit from. This will obviously become a problem because problems that are flushed out by enabler stories will not be identified and will go unnoticed in PI planning. The approach we may end up taking may require rework that could have been identified earlier, and therefore, our story points could have been allocated correctly.</p>
</div>
<div class="readable-text intended-text" id="p202">
<p>We have seen numerous examples where a project is seemingly complete, only to have a subject matter expert point out something that was missed in a system demo and requires rework. It is important to remember that we will have to deal with these types of situations at times as it may be often tempting to take an easier way out. In such situations, we will need to judge the effects of the change on the project timeline, amount of rework, risk involved, technical debt incurred, and so forth. For instance, the business may have decided that we did not need referential integrity of the database and that it would be faster to write a program that could be manually run to check for dangling records/missing relationships.</p>
</div>
<div class="readable-text" id="p203">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p204">This chapter shows how to create a database that could store our unaltered records, as well as a parsed version of the record that will be beneficial when necessary to provide details on the loaded ACH files.</li>
<li class="readable-text" id="p205">We saw the implications of adding referential integrity to our database late in the process and the need to rework both our code and unit tests. When a feature requires rework to be done correctly, it can often be put on the back burner. </li>
<li class="readable-text" id="p206">Despite the extra work to implement these types of features, it is important to champion them both to the team members and management so they are not left by the wayside.</li>
<li class="readable-text" id="p207">Defining databases is crucial for data storage, querying, and integrity in applications.</li>
<li class="readable-text" id="p208">ACH files face challenges with performance, querying, and data integrity when handled as flat files.</li>
<li class="readable-text" id="p209">Relational databases offer advantages, such as primary and foreign keys, constraints, and data integrity.</li>
<li class="readable-text" id="p210">Implementing referential integrity prevents orphan records and ensures database consistency.</li>
<li class="readable-text" id="p211">A research spike (enabler story) is beneficial when evaluating database design and various implementation approaches.</li>
<li class="readable-text" id="p212">ELT and ETL offer different benefits for processing ACH files and handling errors.</li>
<li class="readable-text" id="p213">Pydantic helps in modeling database tables, abstracting SQL, and enhancing documentation and validation.</li>
<li class="readable-text" id="p214">Uploading files and integrating APIs are foundational for expanding functionality in an ACH system.</li>
<li class="readable-text" id="p215">Data and referential integrity are critical for relational databases to prevent errors and improve reliability.</li>
<li class="readable-text" id="p216">Continuous testing, refactoring, and revisiting initial design choices help maintain and improve database performance and structure.</li>
</ul>
</div></body></html>