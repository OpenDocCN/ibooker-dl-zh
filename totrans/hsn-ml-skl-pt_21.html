<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 19. Reinforcement Learning"><div class="chapter" id="rl_chapter">
<h1><span class="label">Chapter 19. </span>Reinforcement Learning</h1>


<p><em>Reinforcement learning</em> (RL)<a data-type="indexterm" data-primary="reinforcement learning (RL)" id="xi_reinforcementlearningRL19530_1"/> is one of the most exciting fields of machine learning today, and also one of the oldest. It has been around since the 1950s, producing many interesting applications over the years,⁠<sup><a data-type="noteref" id="id4170-marker" href="ch19.html#id4170">1</a></sup> particularly in games (e.g., <em>TD-Gammon</em>, a backgammon-playing program) and in machine control, but seldom making the headline news. However, a revolution took place in 2013, when researchers from a British startup called DeepMind<a data-type="indexterm" data-primary="DeepMind" data-secondary="and reinforcement learning" data-secondary-sortas="reinforcement learning" id="xi_DeepMindandreinforcementlearning195597_1"/>⁠<sup><a data-type="noteref" id="id4171-marker" href="ch19.html#id4171">2</a></sup> demonstrated a system that could learn to play just about any Atari game from scratch,⁠<sup><a data-type="noteref" id="id4172-marker" href="ch19.html#id4172">3</a></sup> eventually outperforming humans⁠<sup><a data-type="noteref" id="id4173-marker" href="ch19.html#id4173">4</a></sup> in most of them, using only raw pixels as inputs and without any prior knowledge of the rules of the games.⁠<sup><a data-type="noteref" id="id4174-marker" href="ch19.html#id4174">5</a></sup>  This was the first of a series of amazing feats:</p>

<ul>
<li>
<p>In 2016, DeepMind’s AlphaGo<a data-type="indexterm" data-primary="AlphaGo" id="id4175"/> beat Lee Sedol, a legendary professional player of the game of Go; and in 2017, it beat Ke Jie, the world champion. No program had ever come close to beating a master of this game, let alone the very best.</p>
</li>
<li>
<p>In 2020, DeepMind released AlphaFold<a data-type="indexterm" data-primary="AlphaFold" id="id4176"/>, which can predict the 3D shape of proteins with unprecedented accuracy. This is a game changer in biology, chemistry, and medicine. In fact, Demis Hassabis (founder and CEO) and John Jumper (director) were awarded the Nobel Prize in Chemistry for AlphaFold.</p>
</li>
<li>
<p>In 2022, DeepMind released AlphaCode<a data-type="indexterm" data-primary="AlphaCode" id="id4177"/>, which can generate code at a competitive programming level.</p>
</li>
<li>
<p>In 2023, DeepMind released GNoME<a data-type="indexterm" data-primary="GNoME" id="id4178"/> which can predict new crystal structures, including hundreds of thousands of predicted stable materials.</p>
</li>
</ul>

<p>So how did DeepMind researchers achieve all of this? Well, they applied the power of deep learning to the field of reinforcement learning, and it worked beyond their wildest dreams: <em>deep reinforcement learning</em><a data-type="indexterm" data-primary="deep reinforcement learning" id="id4179"/><a data-type="indexterm" data-primary="deep learning" id="id4180"/> was born. Today, although DeepMind continues to lead the way, many other organizations have joined in, and the whole field is boiling with new ideas, with a wide range of applications.<a data-type="indexterm" data-startref="xi_DeepMindandreinforcementlearning195597_1" id="id4181"/></p>

<p>In this chapter I will first explain what reinforcement learning is and what it’s good at, then present three of the most important families of techniques in deep reinforcement learning: policy gradients, deep Q-networks (including a discussion of Markov decision processes), and lastly, actor-critic methods, including the popular PPO, which we will use to beat an Atari game. So let’s get started!</p>






<section data-type="sect1" data-pdf-bookmark="What Is Reinforcement Learning?"><div class="sect1" id="id354">
<h1>What Is Reinforcement Learning?</h1>

<p>In<a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="agents, actions, and rewards" id="xi_reinforcementlearningRLagentsactionsandrewards19173_1"/> reinforcement learning, a software <em>agent</em><a data-type="indexterm" data-primary="agents, reinforcement learning" id="xi_agentsreinforcementlearning191746_1"/> makes <em>observations</em> and takes <em>actions</em><a data-type="indexterm" data-primary="actions, in reinforcement learning" id="id4182"/> within an <em>environment</em>,<a data-type="indexterm" data-primary="environments, reinforcement learning" id="id4183"/> and in return it receives <em>rewards</em> from the environment. Its objective is to learn to act in a way that will maximize its expected rewards<a data-type="indexterm" data-primary="rewards, reinforcement learning" id="xi_rewardsreinforcementlearning1917252_1"/> over time. If you don’t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term “reward” is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.</p>

<p>This is quite a broad setting that can apply to a wide variety of tasks. Here are a few examples (see <a data-type="xref" href="#rl_examples_diagram">Figure 19-1</a>):</p>

<ul>
<li>
<p>The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of <em>sensors</em>,<a data-type="indexterm" data-primary="sensors" id="id4184"/> such as cameras and touch sensors, and its actions consist of sending signals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time or goes in the wrong direction.</p>
</li>
<li>
<p>The agent can be the program controlling <em>Ms. Pac-Man</em>. In this case, the environment is a simulation of the Atari game, the actions are the nine possible joystick positions (upper left, down, center, and so on), the observations are screenshots, and the rewards are just the game points.</p>
</li>
<li>
<p>Similarly, the agent can be the program playing a board game such as Go. It only gets a reward if it wins.</p>
</li>
<li>
<p>The agent does not have to control a physically (or virtually) moving thing. For example, it can be a smart thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs.</p>
</li>
<li>
<p>The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses.</p>
</li>
</ul>

<p>Note that there may not be any positive rewards at all; for example, the agent may move around in a maze, getting a negative reward at every time step, so it had better find the exit as quickly as possible! There are many other examples of tasks to which reinforcement learning is well suited, such as self-driving cars, recommender systems, placing ads on a web page, or controlling where an image classification system should focus its attention.<a data-type="indexterm" data-startref="xi_agentsreinforcementlearning191746_1" id="id4185"/><a data-type="indexterm" data-startref="xi_reinforcementlearningRLagentsactionsandrewards19173_1" id="id4186"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="examples of" id="id4187"/><a data-type="indexterm" data-startref="xi_rewardsreinforcementlearning1917252_1" id="id4188"/></p>

<figure class="width-85"><div id="rl_examples_diagram" class="figure">
<img src="assets/hmls_1901.png" alt="Reinforcement learning examples include a Mars rover, the _Ms. Pac-Man_ game, a Go board, a smart thermostat, and a digital stock trading interface." width="1413" height="1153"/>
<h6><span class="label">Figure 19-1. </span>Reinforcement learning examples: (a) robotics, (b) <em>Ms. Pac-Man</em>, (c) Go player, (d) thermostat, (e) automatic trader⁠<sup><a data-type="noteref" id="id4189-marker" href="ch19.html#id4189">6</a></sup></h6>
</div></figure>

<p>Let’s now turn to one large family of RL algorithms: <em>policy gradients</em>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Policy Gradients"><div class="sect1" id="id355">
<h1>Policy Gradients</h1>

<p>The<a data-type="indexterm" data-primary="policy and policy gradients" id="xi_policyandpolicygradients19364_1"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="policy gradients" data-see="policy and policy gradients" id="id4190"/> algorithm a software agent uses to determine its actions is called its <em>policy</em>.<a data-type="indexterm" data-primary="gradients" data-secondary="PG algorithm" data-see="policy and policy gradients" id="id4191"/><a data-type="indexterm" data-primary="policy, algorithm" id="id4192"/> The policy can be any algorithm you can think of, such as a neural network taking observations as inputs and outputting the action to take (see <a data-type="xref" href="#rl_with_nn_policy_diagram">Figure 19-2</a>).</p>

<figure class="width-85"><div id="rl_with_nn_policy_diagram" class="figure">
<img src="assets/hmls_1902.png" alt="Diagram illustrating reinforcement learning with a robot agent using a neural network policy to interact with an environment, showing the flow of actions, rewards, and observations." width="951" height="383"/>
<h6><span class="label">Figure 19-2. </span>Reinforcement learning using a neural network policy</h6>
</div></figure>

<p>The policy does not even have to be deterministic. In fact, in some cases it does not even have to observe the environment, as long as it can get rewards! For example, consider a blind robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability <em>p</em> every second, or randomly rotate left or right with probability 1 – <em>p</em>. The rotation angle would be a random angle between –<em>r</em> and +<em>r</em>. Since this policy involves some randomness, it is called a <em>stochastic policy</em><a data-type="indexterm" data-primary="stochastic policy" id="id4193"/>. The robot will have an erratic trajectory, which guarantees that it will eventually get to any place it can reach and pick up all the dust. The question is, how much dust will it pick up in 30 minutes?</p>

<p>How would you train such a robot? There are just two <em>policy parameters</em><a data-type="indexterm" data-primary="policy parameters" id="id4194"/> you can tweak: the probability <em>p</em> and the angle range <em>r</em>. One possible learning algorithm could be to try out many different values for these parameters, and pick the combination that performs best (see <a data-type="xref" href="#policy_search_diagram">Figure 19-3</a>). This is an example of <em>policy search</em>,<a data-type="indexterm" data-primary="policy search" id="id4195"/> in this case using a brute-force approach. When the <em>policy space</em><a data-type="indexterm" data-primary="policy space" id="id4196"/> is too large (which is generally the case), finding a good set of parameters this way is like searching for a needle in a gigantic haystack.</p>

<p class="pagebreak-before">Another way to explore the policy space is to use <em>genetic algorithms</em>.<a data-type="indexterm" data-primary="genetic algorithms, policy space" id="id4197"/> For example, you could randomly create a first generation of 100 policies and try them out, then “kill” the 80 worst policies⁠<sup><a data-type="noteref" id="id4198-marker" href="ch19.html#id4198">7</a></sup> and make the 20 survivors produce 4 offspring each. An offspring is a copy of its parent⁠<sup><a data-type="noteref" id="id4199-marker" href="ch19.html#id4199">8</a></sup> plus some random variation. The surviving policies plus their offspring together constitute the second generation. You can continue to iterate through generations this way until you find a good policy.⁠<sup><a data-type="noteref" id="id4200-marker" href="ch19.html#id4200">9</a></sup></p>

<figure class="width-85"><div id="policy_search_diagram" class="figure">
<img src="assets/hmls_1903.png" alt="Diagram illustrating four points in the policy space on the left and their corresponding agent behaviors on the right, demonstrating the exploration of policy space in reinforcement learning." width="1167" height="564"/>
<h6><span class="label">Figure 19-3. </span>Four points in the policy space (left) and the agent’s corresponding behavior (right)</h6>
</div></figure>

<p>Yet another approach is to use optimization techniques by evaluating the gradients of the rewards with regard to the policy parameters, then tweaking these parameters by following the gradients toward higher rewards.⁠<sup><a data-type="noteref" id="id4201-marker" href="ch19.html#id4201">10</a></sup> Algorithms that follow this strategy are known as <em>policy gradient</em> (PG) algorithms. But before we can implement them, we first need to create an environment for the agent to live in⁠—so it’s time to introduce the Gymnasium library.<a data-type="indexterm" data-startref="xi_policyandpolicygradients19364_1" id="id4202"/></p>








<section data-type="sect2" data-pdf-bookmark="Introduction to the Gymnasium Library"><div class="sect2" id="id356">
<h2>Introduction to the Gymnasium Library</h2>

<p>One<a data-type="indexterm" data-primary="Gymnasium library" id="xi_Gymnasiumlibrary19554_1"/><a data-type="indexterm" data-primary="policy and policy gradients" data-secondary="Gymnasium library" id="xi_policyandpolicygradientsGymnasiumlibrary19554_1"/> of the challenges of reinforcement learning is that in order to train an agent, you first need to have a working environment<a data-type="indexterm" data-primary="environments, reinforcement learning" id="xi_environmentsreinforcementlearning1955129_1"/><a data-type="indexterm" data-primary="simulated environments" id="xi_simulatedenvironments1955129_1"/>. If you want to program an agent that will learn to play an Atari game, you will need an Atari game simulator. If you want to program a walking robot, then the environment is the real world, and you can directly train your robot in that environment. However, this has its limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed up time either—adding more computing power won’t make the robot move any faster—and it’s generally too expensive to train 1,000 robots in parallel. In short, training is hard and slow in the real world, so you generally need a <em>simulated environment</em> at least for bootstrap training. For example, you might use a library like <a href="https://pybullet.org">PyBullet</a> or <a href="https://mujoco.org">MuJoCo</a> for 3D physics simulation.</p>

<p>The <a href="https://gymnasium.farama.org">Gymnasium library</a> is an open source toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physics simulations, and so on), that you can use to train agents, compare them, or develop new RL algorithms. It’s the successor of OpenAI Gym, and is now maintained by a community of researchers and developers.</p>

<p>Gymnasium is preinstalled on Colab, along with the Arcade Learning Environment (ALE) library<a data-type="indexterm" data-primary="ALE (Arcade Learning Environment) library" id="id4203"/><a data-type="indexterm" data-primary="Arcade Learning Environment (ALE) library" id="id4204"/> <code translate="no">ale_py</code>, which is an emulator for Atari 2600 games and is required for all the Atari environments, as well as the Box2D library, required for several environments with 2D physics. If you are coding on your own machine instead of Colab, and you followed the installation instructions at <a href="https://homl.info/install-p" class="bare"><em class="hyperlink">https://homl.info/install-p</em></a>, then you should be good to go.</p>

<p>Let’s start by importing Gymnasium and making an environment:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">gymnasium</code> <code class="k">as</code> <code class="nn">gym</code>

<code class="n">env</code> <code class="o">=</code> <code class="n">gym</code><code class="o">.</code><code class="n">make</code><code class="p">(</code><code class="s2">"CartPole-v1"</code><code class="p">,</code> <code class="n">render_mode</code><code class="o">=</code><code class="s2">"rgb_array"</code><code class="p">,</code> <code class="n">max_episode_steps</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code></pre>

<p>Here, we’ve created a CartPole environment<a data-type="indexterm" data-primary="CartPole environment" data-seealso="Deep Q-Learning; policy and policy gradients" id="id4205"/> (version 1). This is a 2D simulation in which a cart can be accelerated left or right in order to balance a pole placed on top of it (see <a data-type="xref" href="#cart_pole_diagram">Figure 19-4</a>)—a classic control task. I’ll explain <code translate="no">render_mode</code> and <code translate="no">max_episode_steps</code> shortly.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The <code translate="no">gym.envs.registry</code> dictionary contains the names and specifications of all the available environments. You can print a nice list with <code translate="no">gym.pprint_registry()</code>. The Atari environments will only be available once we start the ALE emulator.</p>
</div>

<figure class="width-70"><div id="cart_pole_diagram" class="figure">
<img src="assets/hmls_1904.png" alt="Diagram illustrating the CartPole environment with labeled vectors showing the cart's position, velocity, pole angle, and angular velocity." width="906" height="542"/>
<h6><span class="label">Figure 19-4. </span>The CartPole environment</h6>
</div></figure>

<p>After the environment is created, you must initialize it using the <code translate="no">reset()</code> method, optionally specifying a random seed. This returns the first observation<a data-type="indexterm" data-primary="observations" id="xi_observations1979157_1"/>. Observations depend on the type of environment. For the CartPole environment, each observation is a NumPy array containing four floats representing the cart’s horizontal position (<code translate="no">0.0</code> = center), its velocity (positive means right), the angle of the pole (<code translate="no">0.0</code> = vertical), and its angular velocity (positive means clockwise). The <code translate="no">reset()</code> method also returns a dictionary that may contain extra environment-specific information. This can be useful for debugging and sometimes for training. For example, in many Atari environments, it contains the number of lives left. However, in the CartPole environment, this dictionary is empty:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code>
<code class="go">array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">info</code>
<code class="go">{}</code></pre>

<p>Let’s call the <code translate="no">render()</code> method to render this environment as an image. Since we set <code translate="no">render_mode="rgb_array"</code> when creating the environment, the image will be returned as a NumPy array (you can then use Matplotlib’s <code translate="no">imshow()</code> function to display this image):</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">img</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">render</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">img</code><code class="o">.</code><code class="n">shape</code>  <code class="c1"># height, width, channels (3 = Red, Green, Blue)</code>
<code class="go">(400, 600, 3)</code></pre>

<p>Now let’s ask the environment what actions are possible:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code>
<code class="go">Discrete(2)</code></pre>

<p><code translate="no">Discrete(2)</code> means that the possible actions are integers 0 and 1, which represent accelerating left or right. Other environments may have additional discrete actions, or other kinds of actions (e.g., continuous). Since the pole is leaning toward the right (<code translate="no">obs[2] &gt; 0</code>), let’s accelerate the cart toward the right:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">action</code> <code class="o">=</code> <code class="mi">1</code>  <code class="c1"># accelerate right</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code>
<code class="go">array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">,</code> <code class="n">info</code>
<code class="go">(1.0, False, False, {})</code></pre>

<p>The <code translate="no">step()</code> method executes the desired action and returns five values:</p>
<dl>
<dt><code translate="no">obs</code></dt>
<dd>
<p>This is the new observation. The cart is now moving toward the right (<code translate="no">obs[1] &gt; 0</code>). The pole is still tilted toward the right (<code translate="no">obs[2] &gt; 0</code>), but its angular velocity is now negative (<code translate="no">obs[3] &lt; 0</code>), so it will likely be tilted toward the left after the next step.<a data-type="indexterm" data-startref="xi_observations1979157_1" id="id4206"/></p>
</dd>
<dt><code translate="no">reward</code></dt>
<dd>
<p>In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep the episode running for as long as possible. An <em>episode</em><a data-type="indexterm" data-primary="episodes" id="id4207"/> is one run of the environment until the game is over or interrupted.</p>
</dd>
<dt><code translate="no">done</code></dt>
<dd>
<p>This value will be <code translate="no">True</code> when the episode is over. This will happen when the pole tilts too much, or goes off the screen. After that, the environment must be reset before it can be used again.</p>
</dd>
<dt><code translate="no">truncated</code></dt>
<dd>
<p>This value will be <code translate="no">True</code> when an episode is interrupted early, typically by an environment wrapper that imposes a maximum number of steps per episode (see Gymnasium’s documentation for more details on environment wrappers). By default, the environment specification for CartPole sets the maximum number of steps to 500, but we changed this to 1,000 when we created the environment. Some RL algorithms treat truncated episodes<a data-type="indexterm" data-primary="truncated episodes" id="id4208"/> differently from episodes finished normally (i.e., when <code translate="no">done</code> is <code translate="no">True</code>), but in this chapter we will treat them identically.</p>
</dd>
<dt><code translate="no">info</code></dt>
<dd>
<p>This environment-specific dictionary may provide extra information, just like the one returned by the <code translate="no">reset()</code> method.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>Once you have finished using an environment—possibly after many episodes—you should call its <code translate="no">close()</code> method to free resources.</p>
</div>

<p>Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the left and accelerates right when the pole is leaning toward the right. We will run this policy to see the average rewards it gets over 500 episodes:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">basic_policy</code><code class="p">(</code><code class="n">obs</code><code class="p">):</code>
    <code class="n">angle</code> <code class="o">=</code> <code class="n">obs</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code>
    <code class="k">return</code> <code class="mi">0</code> <code class="k">if</code> <code class="n">angle</code> <code class="o">&lt;</code> <code class="mi">0</code> <code class="k">else</code> <code class="mi">1</code>  <code class="c1"># go left if leaning left, otherwise go right</code>

<code class="n">totals</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">episode</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">500</code><code class="p">):</code>
    <code class="n">total_rewards</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">obs</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="n">episode</code><code class="p">)</code>
    <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>  <code class="c1"># no risk of infinite loop: will be truncated after 1000 steps</code>
        <code class="n">action</code> <code class="o">=</code> <code class="n">basic_policy</code><code class="p">(</code><code class="n">obs</code><code class="p">)</code>
        <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
        <code class="n">total_rewards</code> <code class="o">+=</code> <code class="n">reward</code>
        <code class="k">if</code> <code class="n">done</code> <code class="ow">or</code> <code class="n">truncated</code><code class="p">:</code>
            <code class="k">break</code>

    <code class="n">totals</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">total_rewards</code><code class="p">)</code></pre>

<p>This code is self-explanatory. Let’s look at the result:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">totals</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">totals</code><code class="p">),</code> <code class="nb">min</code><code class="p">(</code><code class="n">totals</code><code class="p">),</code> <code class="nb">max</code><code class="p">(</code><code class="n">totals</code><code class="p">)</code>
<code class="go">(np.float64(41.698), np.float64(8.389445512070509), 24.0, 63.0)</code></pre>

<p>Even with 500 tries, this policy never managed to keep the pole upright for more than 63 consecutive steps. Not great. If you look at the simulation in this chapter’s notebook, you will see that the cart oscillates left and right more and more strongly until the pole tilts too much. A neural network can do better!<a data-type="indexterm" data-startref="xi_Gymnasiumlibrary19554_1" id="id4209"/><a data-type="indexterm" data-startref="xi_policyandpolicygradientsGymnasiumlibrary19554_1" id="id4210"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Neural Network Policies"><div class="sect2" id="id357">
<h2>Neural Network Policies</h2>

<p>Let’s<a data-type="indexterm" data-primary="artificial neural networks (ANNs)" data-secondary="reinforcement learning policies" id="xi_artificialneuralnetworksANNsreinforcementlearningpolicies191636_1"/><a data-type="indexterm" data-primary="policy and policy gradients" data-secondary="neural network policies" id="xi_policyandpolicygradientsneuralnetworkpolicies191636_1"/> create a neural network policy. This neural network will take an observation as input, and it will output the action to be executed, just like the policy we hardcoded earlier. More precisely, it will estimate a probability for each action<a data-type="indexterm" data-primary="actions, in reinforcement learning" id="xi_actionsinreinforcementlearning19163245_1"/>, then it will select an action randomly, according to the estimated probabilities (see <a data-type="xref" href="#neural_network_policy_diagram">Figure 19-5</a>). In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron. It will output the probability <em>p</em> of action 1 (right), and of course the probability of action 0 (left) will be 1 – <em>p</em>. For example, if it outputs 0.7, then we will pick action 1 with 70% probability, or action 0 with 30% probability (this is a <em>Bernoulli distribution</em> with <em>p</em> = 0.7).</p>

<figure class="width-55"><div id="neural_network_policy_diagram" class="figure">
<img src="assets/hmls_1905.png" alt="Diagram illustrating a neural network policy where observations are processed through hidden layers to generate a probability for action, leading to random action sampling." width="780" height="915"/>
<h6><span class="label">Figure 19-5. </span>Neural network policy</h6>
</div></figure>

<p>You may wonder why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score. This approach lets the agent find the right balance between <em>exploring</em> new actions and <em>exploiting</em> the actions that are known to work well. Here’s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing, so you randomly pick one. If it turns out to be good, you can increase the probability that you’ll order it next time, but you shouldn’t increase that probability up to 100%, or you will never try the other dishes, some of which may be even better than the one you tried. This <em>exploration</em>/<em>exploitation dilemma</em><a data-type="indexterm" data-primary="exploration/exploitation dilemma, reinforcement learning" id="id4211"/> is central in reinforcement learning.</p>

<p>Also note that in this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment’s full state. If there were some hidden state, then you might need to consider past actions and observations as well. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is when the observations are noisy; in that case, you generally want to use the past few observations to estimate the most likely current state. The CartPole problem is thus as simple as can be; the observations are noise-free, and they contain the environment’s full state.</p>

<p>Let’s use PyTorch to implement a basic neural network policy for CartPole:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="k">class</code> <code class="nc">PolicyNetwork</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">net</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">state</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">net</code><code class="p">(</code><code class="n">state</code><code class="p">)</code></pre>

<p>Our policy network is a tiny MLP, since it’s a fairly simple task. The number of inputs is the size of the environment’s state: in the case of CartPole, it is just the size of a single observation, which is four. We have just one hidden layer with five units (no need for more in this case). Finally, we want to output a single probability, so we have a single output neuron. If there were more than two possible actions, there would be one output neuron per action instead. For performance and numerical stability, we don’t add a sigmoid function at the end, so the network will actually output logits rather than probabilities.</p>

<p>Next let’s define a function that will use this policy network to choose an action:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">choose_action</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">obs</code><code class="p">):</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">as_tensor</code><code class="p">(</code><code class="n">obs</code><code class="p">)</code>
    <code class="n">logit</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
    <code class="n">dist</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">distributions</code><code class="o">.</code><code class="n">Bernoulli</code><code class="p">(</code><code class="n">logits</code><code class="o">=</code><code class="n">logit</code><code class="p">)</code>
    <code class="n">action</code> <code class="o">=</code> <code class="n">dist</code><code class="o">.</code><code class="n">sample</code><code class="p">()</code>
    <code class="n">log_prob</code> <code class="o">=</code> <code class="n">dist</code><code class="o">.</code><code class="n">log_prob</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
    <code class="k">return</code> <code class="nb">int</code><code class="p">(</code><code class="n">action</code><code class="o">.</code><code class="n">item</code><code class="p">()),</code> <code class="n">log_prob</code></pre>

<p>The function takes a single observation, converts it to a tensor, and passes it to the policy network to get the logit for action 1 (right). It then creates a <code translate="no">Bernoulli</code> probability distribution with this logit, and it samples an action from it: this distribution will output 1 (right) with probability <em>p</em> = exp(logit) / (1 + exp(logit)), and 0 (left) with probability 1 – <em>p</em>. If there were more than two possible actions, you would use a <code translate="no">Categorical</code> distribution instead. Lastly, we compute the log probability of the sampled action (i.e., either log(<em>p</em>) or log(1 – <em>p</em>)): this log probability will be needed later for training.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If the action space is continuous, you can use a Gaussian distribution instead of a Bernoulli or categorical distribution. Instead of predicting logits, the policy network must predict the mean and standard deviation (or the log of the standard deviation) of the distribution. The log of the standard deviation is often clipped to ensure the distribution is neither too wide nor too narrow.</p>
</div>

<p>OK, we now have a neural network policy that can take an environment state (in this case, a single observation) and choose an action.<a data-type="indexterm" data-startref="xi_actionsinreinforcementlearning19163245_1" id="id4212"/> But how do we train it?<a data-type="indexterm" data-startref="xi_artificialneuralnetworksANNsreinforcementlearningpolicies191636_1" id="id4213"/><a data-type="indexterm" data-startref="xi_environmentsreinforcementlearning1955129_1" id="id4214"/><a data-type="indexterm" data-startref="xi_policyandpolicygradientsneuralnetworkpolicies191636_1" id="id4215"/><a data-type="indexterm" data-startref="xi_simulatedenvironments1955129_1" id="id4216"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Evaluating Actions: The Credit Assignment Problem"><div class="sect2" id="id358">
<h2>Evaluating Actions: The Credit Assignment Problem</h2>

<p>If<a data-type="indexterm" data-primary="credit assignment problem" id="xi_creditassignmentproblem192123_1"/><a data-type="indexterm" data-primary="policy and policy gradients" data-secondary="credit assignment problem" id="xi_policyandpolicygradientscreditassignmentproblem192123_1"/> we knew what the best action was at each step, we could train the neural network as usual by minimizing the cross-entropy between the estimated probability distribution and the target probability distribution. It would just be regular supervised learning. However, in reinforcement learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed. For example, if the agent manages to balance the pole for a total of 100 steps, how can it know which of the 100 actions it took were good, and which of them were bad? All it knows is that the pole fell after the last action, but surely this last action is not entirely responsible. This is called the <em>credit assignment problem</em>: when the agent gets a reward (or a penalty), it is hard for it to know which actions should get credited (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well; will it understand what it is being rewarded for?</p>

<p>To simplify credit assignment, a common strategy is to evaluate an action based on the sum of all the rewards that come after it, applying a <em>discount factor, _γ</em> (gamma), at each step<a data-type="indexterm" data-primary="discount factor γ in reward system" id="id4217"/><a data-type="indexterm" data-primary="γ (gamma) discount factor" id="id4218"/><a data-type="indexterm" data-primary="gamma (γ) discount factor" id="id4219"/>. This sum of discounted rewards<a data-type="indexterm" data-primary="discounted rewards" id="id4220"/> is called the action’s <em>return</em>. Consider the example in <a data-type="xref" href="#discounted_rewards_diagram">Figure 19-6</a>. If an agent decides to go right three times in a row and gets +10 reward after the first step, 0 after the second step, and finally –50 after the third step, then assuming we use a discount factor <em>γ</em> = 0.8, the first action will have a return of 10 + <em>γ</em> × 0 + <em>γ</em><sup>2</sup> × (–50) = –22.</p>

<figure class="width-65"><div id="discounted_rewards_diagram" class="figure">
<img src="assets/hmls_1906.png" alt="Diagram showing a robot earning sequential rewards of +10, 0, and -50 as it moves right, with calculated discounted returns of -22 and -40, highlighting the effect of an 80% discount factor." width="867" height="626"/>
<h6><span class="label">Figure 19-6. </span>Computing an action’s return: the sum of discounted future rewards</h6>
</div></figure>

<p>The following function computes the returns, given the rewards and the discount factor:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">compute_returns</code><code class="p">(</code><code class="n">rewards</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">):</code>
    <code class="n">returns</code> <code class="o">=</code> <code class="n">rewards</code><code class="p">[:]</code>  <code class="c1"># copy the rewards</code>
    <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">returns</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">):</code>
        <code class="n">returns</code><code class="p">[</code><code class="n">step</code> <code class="o">-</code> <code class="mi">1</code><code class="p">]</code> <code class="o">+=</code> <code class="n">returns</code><code class="p">[</code><code class="n">step</code><code class="p">]</code> <code class="o">*</code> <code class="n">discount_factor</code>

    <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">returns</code><code class="p">)</code></pre>

<p>This function produces the expected result:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">compute_returns</code><code class="p">([</code><code class="mi">10</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">50</code><code class="p">],</code> <code class="n">discount_factor</code><code class="o">=</code><code class="mf">0.8</code><code class="p">)</code>
<code class="go">tensor([-22., -40., -50.])</code></pre>

<p>If the discount factor is close to 0, then future rewards won’t count for much compared to immediate rewards. Conversely, if the discount factor is close to 1, then rewards far into the future will count almost as much as immediate rewards. Typical discount factors vary from 0.9 to 0.99. With a discount factor of 0.95, rewards 13 steps into the future count roughly for half as much as immediate rewards (since 0.95<sup>13</sup> ≈ 0.5), while with a discount factor of 0.99, rewards 69 steps into the future count for half as much as immediate rewards. In the CartPole environment, actions have fairly short-term effects, so choosing a low discount factor of 0.95 seems reasonable, and it will help with credit assignment, making training faster and more stable. However, if the discount factor is set too low, then the agent will learn a suboptimal strategy, focusing too much on short-term gains.</p>

<p>Now that we have a way to evaluate each action, we are ready to train our first agent using policy gradients. Let’s see how.<a data-type="indexterm" data-startref="xi_creditassignmentproblem192123_1" id="id4221"/><a data-type="indexterm" data-startref="xi_policyandpolicygradientscreditassignmentproblem192123_1" id="id4222"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Solving the CartPole Using Policy Gradients"><div class="sect2" id="id359">
<h2>Solving the CartPole Using Policy Gradients</h2>

<p>As<a data-type="indexterm" data-primary="policy and policy gradients" data-secondary="CartPole" id="xi_policyandpolicygradientsCartPole192453_1"/> discussed earlier, policy gradient algorithms optimize the parameters of a policy by following the gradients toward higher rewards. One popular PG algorithm, called <em>REINFORCE</em> (or <em>Monte Carlo PG</em>),<a data-type="indexterm" data-primary="REINFORCE algorithms" id="xi_REINFORCEalgorithms19245203_1"/> was <a href="https://homl.info/132">introduced back in 1992 by Ronald Williams</a>.⁠<sup><a data-type="noteref" id="id4223-marker" href="ch19.html#id4223">11</a></sup> It has many variants, with various tweaks, but the general principle is this:</p>
<ol>
<li>
<p>First, let the neural network policy play the game for an episode, and record the rewards and estimated log probabilities.</p>
</li>
<li>
<p>Then compute each action’s return, using the function defined in the previous section.</p>
</li>
<li>
<p>If an action’s return is positive, it means that the action was probably good, and you want to make this action even more likely to be chosen in the future. Conversely, if an action’s return is negative, you want to make this action <em>less</em> likely. To achieve this, you can minimize the REINFORCE loss defined in <a data-type="xref" href="#reinforce_equation">Equation 19-1</a>: this will maximize the expected discounted rewards.</p>
<div id="reinforce_equation" data-type="equation">
<h5><span class="label">Equation 19-1. </span>REINFORCE loss</h5>
<math display="block">
  <mrow>
    <mi>ℒ</mi>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">θ</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mo>-</mo>
    <munder><mo>∑</mo> <mi>t</mi> </munder>
    <mrow>
      <mo form="prefix">log</mo>
      <msub><mi>π</mi> <mi mathvariant="bold">θ</mi> </msub>
      <mrow>
        <mo>(</mo>
        <msub><mi>a</mi> <mi>t</mi> </msub>
        <mo>|</mo>
        <msub><mi>s</mi> <mi>t</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mo>·</mo>
      <msub><mi>r</mi> <mi>t</mi> </msub>
    </mrow>
  </mrow>
</math>
</div>
</li>

</ol>

<p>In this equation, <em>π</em><sub><strong>θ</strong></sub>(<em>a</em><sub><em>t</em></sub>|<em>s</em><sub><em>t</em></sub>) is the policy network’s estimated probability for action <em>a</em><sub><em>t</em></sub>, given state <em>s</em><sub><em>t</em></sub> (where <em>t</em> is the time step), and <em>r</em><sub><em>t</em></sub> is the observed return of this action; <strong>θ</strong> represents the model parameters.</p>

<p>Let’s use PyTorch<a data-type="indexterm" data-primary="PyTorch" data-secondary="REINFORCE algorithm" id="xi_PyTorchREINFORCEalgorithm1926218_1"/> to implement this algorithm. First, we need a function to let the policy network play an episode, and record the rewards and log probabilities:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">run_episode</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
    <code class="n">log_probs</code><code class="p">,</code> <code class="n">rewards</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>
    <code class="n">obs</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>  <code class="c1"># the environment will truncate the episode if it is too long</code>
        <code class="n">action</code><code class="p">,</code> <code class="n">log_prob</code> <code class="o">=</code> <code class="n">choose_action</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">obs</code><code class="p">)</code>
        <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">,</code> <code class="n">_info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
        <code class="n">log_probs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">log_prob</code><code class="p">)</code>
        <code class="n">rewards</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">reward</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">done</code> <code class="ow">or</code> <code class="n">truncated</code><code class="p">:</code>
            <code class="k">return</code> <code class="n">log_probs</code><code class="p">,</code> <code class="n">rewards</code></pre>

<p>The function first resets the environment to start a new episode. For reproducibility, we pass a seed to the <code translate="no">reset()</code><a data-type="indexterm" data-primary="Gymnasium library" id="id4224"/> method. Then comes the game loop: at each iteration, we pass the current environment state (i.e., the last observation) to the <code translate="no">choose_action()</code> method we defined earlier. It returns the chosen action and its log probability. We then call the environment’s <code translate="no">step()</code> method to execute the action. This returns a new observation (a NumPy array), a reward, two booleans indicating whether the game is over or truncated, and an info dict (which we can safely ignore in the case of CartPole). We record the log probabilities and rewards in two lists, which we return when the episode is over.</p>

<p>We can finally write the training function:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_reinforce</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">n_episodes</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">episode</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_episodes</code><code class="p">):</code>
        <code class="n">seed</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="o">**</code><code class="mi">32</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">())</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
        <code class="n">log_probs</code><code class="p">,</code> <code class="n">rewards</code> <code class="o">=</code> <code class="n">run_episode</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
        <code class="n">returns</code> <code class="o">=</code> <code class="n">compute_returns</code><code class="p">(</code><code class="n">rewards</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">)</code>
        <code class="n">std_returns</code> <code class="o">=</code> <code class="p">(</code><code class="n">returns</code> <code class="o">-</code> <code class="n">returns</code><code class="o">.</code><code class="n">mean</code><code class="p">())</code> <code class="o">/</code> <code class="p">(</code><code class="n">returns</code><code class="o">.</code><code class="n">std</code><code class="p">()</code> <code class="o">+</code> <code class="mf">1e-7</code><code class="p">)</code>
        <code class="n">losses</code> <code class="o">=</code> <code class="p">[</code><code class="o">-</code><code class="n">logp</code> <code class="o">*</code> <code class="n">rt</code> <code class="k">for</code> <code class="n">logp</code><code class="p">,</code> <code class="n">rt</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">log_probs</code><code class="p">,</code> <code class="n">std_returns</code><code class="p">)]</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">cat</code><code class="p">(</code><code class="n">losses</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\r</code><code class="s2">Episode </code><code class="si">{</code><code class="n">episode</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">, Reward: </code><code class="si">{</code><code class="nb">sum</code><code class="p">(</code><code class="n">rewards</code><code class="p">)</code><code class="si">:</code><code class="s2">.2f</code><code class="si">}</code><code class="s2">"</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">" "</code><code class="p">)</code></pre>

<p>That’s nice and short, isn’t it? At each training iteration, the function runs an episode and gets the log probabilities and rewards.⁠<sup><a data-type="noteref" id="id4225-marker" href="ch19.html#id4225">12</a></sup> Then it computes the return for each action. Next, it standardizes the returns (i.e., it subtracts the mean return and divides by the standard deviation, plus a small value to avoid division by zero). This standardization step is optional but it’s a common and recommended tweak to the REINFORCE algorithm, as it stabilizes training. Next, the function computes the REINFORCE loss using <a data-type="xref" href="#reinforce_equation">Equation 19-1</a>, and it performs an optimizer step to minimize the loss.</p>

<p>That’s it, we’re ready to build and train a policy network!</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">PolicyNetwork</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.06</code><code class="p">)</code>
<code class="n">train_reinforce</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">n_episodes</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">discount_factor</code><code class="o">=</code><code class="mf">0.95</code><code class="p">)</code></pre>

<p>Training will take less than a minute. If you run an episode using this policy network, you will see that it perfectly balances the pole. Success!</p>

<p>The simple policy gradients algorithm we just trained solved the CartPole task, but it would not scale well to larger and more complex tasks. Indeed, it is highly <em>sample inefficient</em>,<a data-type="indexterm" data-primary="sample inefficiency" id="id4226"/> meaning it needs to explore the game for a very long time before it can make significant progress. This is because its return estimates are extremely noisy, especially when good actions are mixed with bad ones. However, it is the foundation of more powerful algorithms, such as <em>actor-critic</em> algorithms<a data-type="indexterm" data-primary="actor-critic algorithms" id="id4227"/> (which we will discuss at the end of this chapter).<a data-type="indexterm" data-startref="xi_PyTorchREINFORCEalgorithm1926218_1" id="id4228"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>Researchers try to find algorithms that work well even when the agent<a data-type="indexterm" data-primary="agents, reinforcement learning" id="id4229"/> initially knows nothing about the environment. However, unless you are writing a paper, you should not hesitate to inject prior knowledge into the agent, as it will speed up training dramatically. For example, since you know that the pole should be as vertical as possible, you could add negative rewards proportional to the pole’s angle. This will make the rewards<a data-type="indexterm" data-primary="rewards, reinforcement learning" id="id4230"/> much less sparse and speed up training. Also, if you already have a reasonably good policy (e.g., hardcoded), you may want to train the neural network to imitate it before using policy gradients to improve it.</p>
</div>

<p>Moreover, the REINFORCE algorithm is quite unstable: the agent may improve for a while during training, then forget everything catastrophically, learn again, forget, learn, etc. It’s a roller coaster. This is in large part because the training samples are not independent and identically distributed (IID); indeed, the training samples consist of whatever states the agent is capable of reaching right now. As the agent progresses, it explores different parts of the environment, and it can forget everything about other parts. For example, once it learns to properly hold the pole upright, it will no longer see nonvertical poles, and it will totally forget how to handle them. And this issue gets much worse with more complex environments.<a data-type="indexterm" data-startref="xi_policyandpolicygradientsCartPole192453_1" id="id4231"/><a data-type="indexterm" data-startref="xi_REINFORCEalgorithms19245203_1" id="id4232"/></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Reinforcement learning is notoriously difficult, largely because of the training instabilities and the huge sensitivity to the choice of hyperparameter<a data-type="indexterm" data-primary="hyperparameters" data-secondary="PG algorithm" id="id4233"/> values and random seeds.⁠<sup><a data-type="noteref" id="id4234-marker" href="ch19.html#id4234">13</a></sup> As the researcher Andrej Karpathy put it, “[Supervised learning] wants to work. […​] RL must be forced to work”. You will need time, patience, perseverance, and perhaps a bit of luck, too. This is a major reason RL is not as widely adopted as regular deep learning.</p>
</div>

<p>We will now look at another popular family of algorithms: <em>value-based methods</em>.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Value-Based Methods"><div class="sect1" id="id360">
<h1>Value-Based Methods</h1>

<p>Whereas<a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="value-based methods" data-see="value-based methods" id="id4235"/><a data-type="indexterm" data-primary="value-based methods" data-seealso="Q-learning algorithm" id="id4236"/> PG algorithms directly try to optimize the policy to increase rewards, value-based methods are less direct: the agent learns to estimate the value of each state (i.e., the expected return), or the value of each action in a given state, then it uses this knowledge to decide how to act. To understand these algorithms, we must first discuss <em>Markov decision processes</em> (MDPs).</p>








<section data-type="sect2" data-pdf-bookmark="Markov Decision Processes"><div class="sect2" id="id361">
<h2>Markov Decision Processes</h2>

<p>In<a data-type="indexterm" data-primary="Markov decision processes (MDPs)" id="xi_MarkovdecisionprocessesMDPs193263_1"/><a data-type="indexterm" data-primary="MDPs (Markov decision processes)" id="xi_MDPsMarkovdecisionprocesses193263_1"/> the early 20th century, the mathematician Andrey Markov studied stochastic processes with no memory, called <em>Markov chains</em>.<a data-type="indexterm" data-primary="Markov chains" id="id4237"/> Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state <em>s</em> to a state <em>s</em>′ is fixed, and it depends only on the pair (<em>s</em>, <em>s</em>′), not on past states. This is why we say that the system has no memory.</p>

<p class="pagebreak-before"><a data-type="xref" href="#markov_chain_diagram">Figure 19-7</a> shows an example of a Markov chain with four states.</p>

<figure class="width-50"><div id="markov_chain_diagram" class="figure">
<img src="assets/hmls_1907.png" alt="Diagram of a Markov chain with four states, illustrating transitions and probabilities between states including a terminal state." width="761" height="371"/>
<h6><span class="label">Figure 19-7. </span>Example of a Markov chain</h6>
</div></figure>

<p>Suppose that the process starts in state <em>s</em><sub>0</sub>, and there is a 70% chance that it will remain in that state at the next step. Eventually it is bound to leave that state and never come back, because no other state points back to <em>s</em><sub>0</sub>. If it goes to state <em>s</em><sub>1</sub>, it will then most likely go to state <em>s</em><sub>2</sub> (90% probability), then immediately back to state <em>s</em><sub>1</sub> (with 100% probability). It may alternate a number of times between these two states, but eventually it will fall into state <em>s</em><sub>3</sub> and remain there forever, since there’s no way out: this is called a <em>terminal state</em>.<a data-type="indexterm" data-primary="terminal state, Markov chain" id="id4238"/> Markov chains can have very different dynamics, and they are frequently used in thermodynamics, chemistry, statistics, and much more.</p>

<p>Markov decision processes were first described in the 1950s by <a href="https://homl.info/133">Richard Bellman</a>.⁠<sup><a data-type="noteref" id="id4239-marker" href="ch19.html#id4239">14</a></sup> They resemble Markov chains, but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), and the agent’s goal is to find a policy that will maximize its cumulative reward over time.</p>

<p>For example, the MDP represented in <a data-type="xref" href="#mdp_diagram">Figure 19-8</a> has three states (represented by circles) and up to three possible discrete actions at each step (represented by diamonds).</p>

<figure><div id="mdp_diagram" class="figure">
<img src="assets/hmls_1908.png" alt="Diagram of a Markov decision process with three states (circles) and discrete actions (diamonds), showing transitions and associated rewards or penalties." width="1211" height="517"/>
<h6><span class="label">Figure 19-8. </span>Example of a Markov decision process</h6>
</div></figure>

<p>If it starts in state <em>s</em><sub>0</sub>, the agent can choose among actions <em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, or <em>a</em><sub>2</sub>. If it chooses action <em>a</em><sub>1</sub>, it just remains in state <em>s</em><sub>0</sub> with certainty and without any reward. It can thus decide to stay there forever if it wants to. But if it chooses action <em>a</em><sub>0</sub>, it has a 70% probability of gaining a reward of +10 and remaining in state <em>s</em><sub>0</sub>. It can then try again and again to gain as much reward as possible, but at one point it is going to end up instead in state <em>s</em><sub>1</sub>. In state <em>s</em><sub>1</sub> it has only two possible actions: <em>a</em><sub>0</sub> or <em>a</em><sub>2</sub>. It can choose to stay put by repeatedly choosing action <em>a</em><sub>0</sub>, or it can choose to move on to state <em>s</em><sub>2</sub> and get a negative reward of –50 (ouch). In state <em>s</em><sub>2</sub> it has no choice but to take action <em>a</em><sub>1</sub>, which will most likely lead it back to state <em>s</em><sub>0</sub>, gaining a reward of +40 on the way. You get the picture. By looking at this MDP, can you guess which strategy will gain the most reward over time? In state <em>s</em><sub>0</sub> it is clear that action <em>a</em><sub>0</sub> is the best option, and in state <em>s</em><sub>2</sub> the agent has no choice but to take action <em>a</em><sub>1</sub>, but in state <em>s</em><sub>1</sub> it is not obvious whether the agent should stay put (<em>a</em><sub>0</sub>) or go through the fire (<em>a</em><sub>2</sub>).</p>

<p>Bellman found a way to estimate the <em>optimal state value</em> <a data-type="indexterm" data-primary="optimal state value, MDP" id="id4240"/>of any state <em>s</em>, denoted <em>V</em>*(<em>s</em>), which is the sum of all discounted future rewards the agent can expect on average starting from state <em>s</em>, assuming it acts optimally. He showed that if the agent acts optimally, then the <em>Bellman optimality equation</em><a data-type="indexterm" data-primary="Bellman optimality equation" id="id4241"/> applies (see <a data-type="xref" href="#bellman_optimality_equation">Equation 19-2</a>). This recursive equation says that if the agent acts optimally, then the optimal value of the current state is equal to the reward it will get on average after taking one optimal action, plus the expected optimal value of all possible next states that this action can lead to.</p>
<div id="bellman_optimality_equation" data-type="equation">
<h5><span class="label">Equation 19-2. </span>Bellman optimality equation</h5>
<math alttext="upper V Superscript asterisk Baseline left-parenthesis s right-parenthesis equals max Underscript a Endscripts sigma-summation Underscript s Superscript prime Baseline Endscripts upper T left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis left-bracket upper R left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis plus gamma dot upper V Superscript asterisk Baseline left-parenthesis s Superscript prime Baseline right-parenthesis right-bracket for all s" display="block">
  <mrow>
    <msup><mi>V</mi> <mo>*</mo> </msup>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <mi>a</mi> </munder>
    <munder><mo>∑</mo> <msup><mi>s</mi> <mo>'</mo> </msup> </munder>
    <mi>T</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <msup><mi>s</mi> <mo>'</mo> </msup>
      <mo>)</mo>
    </mrow>
    <mrow>
      <mo>[</mo>
      <mi>R</mi>
      <mrow>
        <mo>(</mo>
        <mi>s</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <mi>a</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo>+</mo>
      <mi>γ</mi>
      <mo>·</mo>
      <msup><mi>V</mi> <mo>*</mo> </msup>
      <mrow>
        <mo>(</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo>]</mo>
    </mrow>
    <mspace width="1.em"/>
    <mtext>for</mtext>
    <mspace width="4.pt"/>
    <mtext>all</mtext>
    <mspace width="4.pt"/>
    <mi>s</mi>
  </mrow>
</math>
</div>

<p class="pagebreak-before">In this equation:</p>

<ul>
<li>
<p><em>T</em>(<em>s</em>, <em>a</em>, <em>s</em>′) is the transition probability from state <em>s</em> to state <em>s</em>′, given that the agent chose action <em>a</em>. For example, in <a data-type="xref" href="#mdp_diagram">Figure 19-8</a>, <em>T</em>(<em>s</em><sub>2</sub>, <em>a</em><sub>1</sub>, <em>s</em><sub>0</sub>) = 0.8. Note that <math alttext="sigma-summation Underscript s Superscript prime Baseline Endscripts upper T left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis equals 1">
  <mstyle scriptlevel="0" displaystyle="false">
    <mrow>
      <msub><mo>∑</mo> <msup><mi>s</mi> <mo>'</mo> </msup> </msub>
      <mi>T</mi>
      <mrow>
        <mo>(</mo>
        <mi>s</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <mi>a</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
  </mstyle>
</math>.</p>
</li>
<li>
<p><em>R</em>(<em>s</em>, <em>a</em>, <em>s</em>′) is the reward that the agent gets when it goes from state <em>s</em> to state <em>s</em>′, given that the agent chose action <em>a</em>. For example, in <a data-type="xref" href="#mdp_diagram">Figure 19-8</a>, <em>R</em>(<em>s</em><sub>2</sub>, <em>a</em><sub>1</sub>, <em>s</em><sub>0</sub>) = +40.</p>
</li>
<li>
<p><em>γ</em> is the discount factor.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In the Bellman equation and the rest of this chapter, an optimal policy is one that maximizes the expected sum of <em>discounted</em> future rewards: this means that it depends on the discount factor <em>γ</em>. However, in real-world tasks we’re generally more interested in the expected sum of rewards per episode, without any discount (in fact, that’s usually how we evaluate agents). To approach this goal, we usually choose a discount factor close to 1 (but not too close or else training becomes slow and unstable).</p>
</div>

<p>This equation leads directly to an algorithm that can precisely estimate the optimal state value of every possible state: first initialize all the state value estimates to zero, and then iteratively update them using the <em>value iteration</em> algorithm (see <a data-type="xref" href="#value_iteration_equation">Equation 19-3</a>). A remarkable result is that, given enough time, these estimates are guaranteed to converge to the optimal state values, corresponding to the optimal <span class="keep-together">policy</span>.</p>
<div id="value_iteration_equation" data-type="equation">
<h5><span class="label">Equation 19-3. </span>Value iteration algorithm</h5>
<math alttext="upper V Subscript k plus 1 Baseline left-parenthesis s right-parenthesis left-arrow max Underscript a Endscripts sigma-summation Underscript s Superscript prime Baseline Endscripts upper T left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis left-bracket upper R left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis plus gamma dot upper V Subscript k Baseline left-parenthesis s Superscript prime Baseline right-parenthesis right-bracket for all s" display="block">
  <mrow>
    <msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>←</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <mi>a</mi> </munder>
    <munder><mo>∑</mo> <msup><mi>s</mi> <mo>'</mo> </msup> </munder>
    <mi>T</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <msup><mi>s</mi> <mo>'</mo> </msup>
      <mo>)</mo>
    </mrow>
    <mrow>
      <mo>[</mo>
      <mi>R</mi>
      <mrow>
        <mo>(</mo>
        <mi>s</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <mi>a</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo>+</mo>
      <mi>γ</mi>
      <mo>·</mo>
      <msub><mi>V</mi> <mi>k</mi> </msub>
      <mrow>
        <mo>(</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo>]</mo>
    </mrow>
    <mspace width="1.em"/>
    <mtext>for</mtext>
    <mspace width="4.pt"/>
    <mtext>all</mtext>
    <mspace width="4.pt"/>
    <mi>s</mi>
  </mrow>
</math>
</div>

<p>In this equation, <em>V</em><sub><em>k</em></sub>(<em>s</em>) is the estimated value of state <em>s</em> at the <em>k</em><sup>th</sup> iteration of the algorithm.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This algorithm is an example of <em>dynamic programming</em><a data-type="indexterm" data-primary="dynamic programming" id="id4242"/>, which breaks down a complex problem into tractable subproblems that can be tackled iteratively.</p>
</div>

<p class="pagebreak-before">Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not give us the optimal policy for the agent. Luckily, Bellman found a very similar algorithm to estimate the optimal <em>state-action values</em>, generally called <em>Q-values</em> (quality values). The optimal Q-value of the state-action pair (<em>s</em>, <em>a</em>), denoted <em>Q</em>*(<em>s</em>, <em>a</em>), is the sum of discounted future rewards the agent can expect on average starting from state <em>s</em> if it chooses action <em>a</em>, but before it sees the outcome of this action, assuming it acts optimally after that action.</p>

<p>Let’s look at how it works. Once again, you start by initializing all the Q-value estimates to zero, then you update them using the <em>Q-value iteration</em> algorithm<a data-type="indexterm" data-primary="Q-Value Iteration algorithm" id="xi_QValueIterationalgorithm19381162_1"/><a data-type="indexterm" data-primary="Q-Values" id="xi_QValues19381162_1"/><a data-type="indexterm" data-primary="state-action values (Q-Values)" id="xi_stateactionvaluesQValues19381162_1"/> (see <a data-type="xref" href="#q_value_iteration_equation">Equation 19-4</a>).</p>
<div id="q_value_iteration_equation" data-type="equation">
<h5><span class="label">Equation 19-4. </span>Q-value iteration algorithm</h5>
<math alttext="upper Q Subscript k plus 1 Baseline left-parenthesis s comma a right-parenthesis left-arrow sigma-summation Underscript s Superscript prime Baseline Endscripts upper T left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis left-bracket upper R left-parenthesis s comma a comma s Superscript prime Baseline right-parenthesis plus gamma dot max Underscript a Superscript prime Baseline Endscripts upper Q Subscript k Baseline left-parenthesis s prime comma a Superscript prime Baseline right-parenthesis right-bracket for all left-parenthesis s comma a right-parenthesis" display="block">
  <mrow>
    <msub><mi>Q</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mo>←</mo>
    <munder><mo>∑</mo> <msup><mi>s</mi> <mo>'</mo> </msup> </munder>
    <mi>T</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <msup><mi>s</mi> <mo>'</mo> </msup>
      <mo>)</mo>
    </mrow>
    <mrow>
      <mo>[</mo>
      <mi>R</mi>
      <mrow>
        <mo>(</mo>
        <mi>s</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <mi>a</mi>
        <mo lspace="0%" rspace="0%">,</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo>+</mo>
      <mi>γ</mi>
      <mo>·</mo>
      <munder><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>'</mo> </msup> </munder>
      <mspace width="0.166667em"/>
      <msub><mi>Q</mi> <mi>k</mi> </msub>
      <mrow>
        <mo>(</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo lspace="0%" rspace="0%">,</mo>
        <msup><mi>a</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo>]</mo>
    </mrow>
    <mspace width="1.em"/>
    <mtext>for</mtext>
    <mspace width="4.pt"/>
    <mtext>all</mtext>
    <mspace width="4.pt"/>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>Once you have the optimal Q-values, defining the optimal policy, denoted <em>π</em><sup>*</sup>(<em>s</em>), is trivial: when the agent is in state <em>s</em>, it should choose the action with the highest Q-value for that state. The fancy math notation for this is <math alttext="pi Superscript asterisk Baseline left-parenthesis s right-parenthesis equals argmax Underscript a Endscripts upper Q Superscript asterisk Baseline left-parenthesis s comma a right-parenthesis">
  <mrow>
    <msup><mi>π</mi> <mo>*</mo> </msup>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mtext>argmax</mtext> <mi>a</mi></munder>
    <mspace width="0.166667em"/>
    <mspace width="0.166667em"/>
    <msup><mi>Q</mi> <mo>*</mo> </msup>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>.</p>

<p>Let’s apply this algorithm to the MDP represented in <a data-type="xref" href="#mdp_diagram">Figure 19-8</a>. First, we need to define the MDP:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">transition_probabilities</code> <code class="o">=</code> <code class="p">[</code>  <code class="c1"># shape=[s, a, s']</code>
    <code class="p">[[</code><code class="mf">0.7</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">]],</code>
    <code class="p">[[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="kc">None</code><code class="p">,</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">]],</code>
    <code class="p">[</code><code class="kc">None</code><code class="p">,</code> <code class="p">[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">],</code> <code class="kc">None</code><code class="p">]</code>
<code class="p">]</code>
<code class="n">rewards</code> <code class="o">=</code> <code class="p">[</code>  <code class="c1"># shape=[s, a, s']</code>
    <code class="p">[[</code><code class="o">+</code><code class="mi">10</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]],</code>
    <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">50</code><code class="p">]],</code>
    <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="o">+</code><code class="mi">40</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>
<code class="p">]</code>
<code class="n">possible_actions</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">]]</code></pre>

<p>For example, to know the transition probability of going from <em>s</em><sub>2</sub> to <em>s</em><sub>0</sub> after playing action <em>a</em><sub>1</sub>, we will look up <code translate="no">transition_probabilities[2][1][0]</code> (which is 0.8). Similarly, to get the corresponding reward, we will look up <code translate="no">rewards[2][1][0]</code> (which is +40). And to get the list of possible actions in <em>s</em><sub>2</sub>, we will look up 
<span class="keep-together"><code translate="no">possible_actions[2]</code></span> (in this case, only action <em>a</em><sub>1</sub> is possible). Next, we must 
<span class="keep-together">initialize</span> all the Q-values to zero (except for the impossible actions, for which we set the Q-values to –∞):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">Q_values</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">full</code><code class="p">((</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">inf</code><code class="p">)</code>  <code class="c1"># -np.inf for impossible actions</code>
<code class="k">for</code> <code class="n">state</code><code class="p">,</code> <code class="n">actions</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">possible_actions</code><code class="p">):</code>
    <code class="n">Q_values</code><code class="p">[</code><code class="n">state</code><code class="p">,</code> <code class="n">actions</code><code class="p">]</code> <code class="o">=</code> <code class="mf">0.0</code>  <code class="c1"># for all possible actions</code></pre>

<p>Now let’s run the Q-value iteration algorithm. It applies <a data-type="xref" href="#q_value_iteration_equation">Equation 19-4</a> repeatedly, to all Q-values, for every state and every possible action:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">gamma</code> <code class="o">=</code> <code class="mf">0.90</code>  <code class="c1"># the discount factor</code>

<code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">50</code><code class="p">):</code>
    <code class="n">Q_prev</code> <code class="o">=</code> <code class="n">Q_values</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="n">possible_actions</code><code class="p">[</code><code class="n">s</code><code class="p">]:</code>
            <code class="n">Q_values</code><code class="p">[</code><code class="n">s</code><code class="p">,</code> <code class="n">a</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">([</code>
                    <code class="n">transition_probabilities</code><code class="p">[</code><code class="n">s</code><code class="p">][</code><code class="n">a</code><code class="p">][</code><code class="n">sp</code><code class="p">]</code>
                    <code class="o">*</code> <code class="p">(</code><code class="n">rewards</code><code class="p">[</code><code class="n">s</code><code class="p">][</code><code class="n">a</code><code class="p">][</code><code class="n">sp</code><code class="p">]</code> <code class="o">+</code> <code class="n">gamma</code> <code class="o">*</code> <code class="n">Q_prev</code><code class="p">[</code><code class="n">sp</code><code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">())</code>
                <code class="k">for</code> <code class="n">sp</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">)])</code></pre>

<p>That’s it! The resulting Q-values look like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">Q_values</code>
<code class="go">array([[18.91891892, 17.02702702, 13.62162162],</code>
<code class="go">       [ 0.        ,        -inf, -4.87971488],</code>
<code class="go">       [       -inf, 50.13365013,        -inf]])</code></pre>

<p>For example, when the agent is in state <em>s</em><sub>0</sub> and it chooses action <em>a</em><sub>1</sub>, the expected sum of discounted future rewards is approximately 17.0.</p>

<p>For each state, we can find the action that has the highest Q-value:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">Q_values</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># optimal action for each state</code>
<code class="go">array([0, 0, 1])</code></pre>

<p>This gives us the optimal policy for this MDP when using a discount factor of 0.90: in state <em>s</em><sub>0</sub> choose action <em>a</em><sub>0</sub>, in state <em>s</em><sub>1</sub> choose action <em>a</em><sub>0</sub> (i.e., stay put), and in state <em>s</em><sub>2</sub> choose action <em>a</em><sub>1</sub> (the only possible action). Interestingly, if we increase the discount factor to 0.95, the optimal policy changes: in state <em>s</em><sub>1</sub> the best action becomes <em>a</em><sub>2</sub> (go through the fire!). This makes sense because the more you value future rewards, the more you are willing to put up with some pain now for the promise of future bliss.<a data-type="indexterm" data-startref="xi_MarkovdecisionprocessesMDPs193263_1" id="id4243"/><a data-type="indexterm" data-startref="xi_MDPsMarkovdecisionprocesses193263_1" id="id4244"/><a data-type="indexterm" data-startref="xi_QValueIterationalgorithm19381162_1" id="id4245"/><a data-type="indexterm" data-startref="xi_QValues19381162_1" id="id4246"/><a data-type="indexterm" data-startref="xi_stateactionvaluesQValues19381162_1" id="id4247"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Temporal Difference Learning"><div class="sect2" id="id362">
<h2>Temporal Difference Learning</h2>

<p>Reinforcement<a data-type="indexterm" data-primary="temporal difference (TD) learning" id="xi_temporaldifferenceTDlearning1945914_1"/> learning problems with discrete actions can often be modeled as Markov decision processes, but the agent<a data-type="indexterm" data-primary="agents, reinforcement learning" id="id4248"/> initially has no idea what the transition probabilities are (it does not know <em>T</em>(<em>s</em>, <em>a</em>, <em>s</em>′)), and it does not know what the rewards are going to be either (it does not know <em>R</em>(<em>s</em>, <em>a</em>, <em>s</em>′)). It must experience each state and each transition at least once to know the rewards, and it must experience them multiple times if it is to have a reasonable estimate of the transition probabilities.</p>

<p>The <em>temporal difference (TD) learning</em> algorithm is very similar to the Q-value iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an <em>exploration policy</em><a data-type="indexterm" data-primary="exploration policies" id="id4249"/>—for example, a purely random policy—to explore the MDP, and as it progresses, the TD learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see <a data-type="xref" href="#td_learning_equation">Equation 19-5</a>).</p>
<div id="td_learning_equation" data-type="equation">
<h5><span class="label">Equation 19-5. </span>TD learning algorithm</h5>
<math alttext="StartLayout 1st Row 1st Column upper V Subscript k plus 1 Baseline left-parenthesis s right-parenthesis 2nd Column left-arrow left-parenthesis 1 minus alpha right-parenthesis upper V Subscript k Baseline left-parenthesis s right-parenthesis plus alpha left-parenthesis r plus gamma dot upper V Subscript k Baseline left-parenthesis s prime right-parenthesis right-parenthesis 2nd Row 1st Column or comma 2nd Column equivalently colon 3rd Row 1st Column upper V Subscript k plus 1 Baseline left-parenthesis s right-parenthesis 2nd Column left-arrow upper V Subscript k Baseline left-parenthesis s right-parenthesis plus alpha dot delta Subscript k Baseline left-parenthesis s comma r comma s Superscript prime Baseline right-parenthesis 4th Row 1st Column with 2nd Column delta Subscript k Baseline left-parenthesis s comma r comma s Superscript prime Baseline right-parenthesis equals r plus gamma dot upper V Subscript k Baseline left-parenthesis s prime right-parenthesis minus upper V Subscript k Baseline left-parenthesis s right-parenthesis EndLayout" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>←</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <mi>α</mi>
            <mo>)</mo>
          </mrow>
          <msub><mi>V</mi> <mi>k</mi> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s</mi>
            <mo>)</mo>
          </mrow>
          <mo>+</mo>
          <mi>α</mi>
          <mfenced separators="" open="(" close=")">
            <mi>r</mi>
            <mo>+</mo>
            <mi>γ</mi>
            <mo>·</mo>
            <msub><mi>V</mi> <mi>k</mi> </msub>
            <mrow>
              <mo>(</mo>
              <msup><mi>s</mi> <mo>'</mo> </msup>
              <mo>)</mo>
            </mrow>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mtext>or</mtext>
          <mo lspace="0%" rspace="0%">,</mo>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mtext>equivalently:</mtext>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>←</mo>
          <msub><mi>V</mi> <mi>k</mi> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s</mi>
            <mo>)</mo>
          </mrow>
          <mo>+</mo>
          <mi>α</mi>
          <mo>·</mo>
          <msub><mi>δ</mi> <mi>k</mi> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s</mi>
            <mo lspace="0%" rspace="0%">,</mo>
            <mi>r</mi>
            <mo lspace="0%" rspace="0%">,</mo>
            <msup><mi>s</mi> <mo>'</mo> </msup>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mtext>with</mtext>
          <mspace width="4.pt"/>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msub><mi>δ</mi> <mi>k</mi> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s</mi>
            <mo lspace="0%" rspace="0%">,</mo>
            <mi>r</mi>
            <mo lspace="0%" rspace="0%">,</mo>
            <msup><mi>s</mi> <mo>'</mo> </msup>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mi>r</mi>
          <mo>+</mo>
          <mi>γ</mi>
          <mo>·</mo>
          <msub><mi>V</mi> <mi>k</mi> </msub>
          <mrow>
            <mo>(</mo>
            <msup><mi>s</mi> <mo>'</mo> </msup>
            <mo>)</mo>
          </mrow>
          <mo>-</mo>
          <msub><mi>V</mi> <mi>k</mi> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>α</em> is the learning rate (e.g., 0.01).</p>
</li>
<li>
<p><em>r</em> + <em>γ</em> · <em>V</em><sub><em>k</em></sub>(<em>s</em>′) is called the <em>TD target</em>.</p>
</li>
<li>
<p><em>δ</em><sub><em>k</em></sub>(<em>s</em>, <em>r</em>, <em>s</em>′) is called the <em>TD error</em>.</p>
</li>
</ul>

<p>A more concise way of writing the first form of this equation is to use the notation <math alttext="a left-arrow Underscript alpha Endscripts b">
  <mrow>
    <mi>a</mi>
    <munder><mo>←</mo> <mi>α</mi> </munder>
    <mi>b</mi>
  </mrow>
</math>, which means <em>a</em><sub><em>k</em>+1</sub> ← (1 – <em>α</em>) · <em>a</em><sub><em>k</em></sub> + <em>α</em>  ·<em>b</em><sub><em>k</em></sub>. So the first line of <a data-type="xref" href="#td_learning_equation">Equation 19-5</a> can be rewritten like this: <math alttext="upper V left-parenthesis s right-parenthesis left-arrow Underscript alpha Endscripts r plus gamma dot upper V left-parenthesis s prime right-parenthesis">
  <mrow>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <munder><mo>←</mo> <mi>α</mi> </munder>
    <mi>r</mi>
    <mo>+</mo>
    <mi>γ</mi>
    <mo>·</mo>
    <mi>V</mi>
    <mrow>
      <mo>(</mo>
      <msup><mi>s</mi> <mo>'</mo> </msup>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>TD learning has many similarities with stochastic gradient descent<a data-type="indexterm" data-primary="stochastic gradient descent (SGD)" data-secondary="and TD learning" data-secondary-sortas="TD" id="id4250"/>, including the fact that it handles one sample at a time. Moreover, just like SGD, it can only truly converge if you gradually reduce the learning rate; otherwise, it will keep bouncing around the optimum Q-values.</p>
</div>

<p>For each state <em>s</em>, this algorithm keeps track of a running average of the immediate rewards the agent gets upon leaving that state, plus the rewards it expects to get later, assuming it acts optimally.<a data-type="indexterm" data-startref="xi_temporaldifferenceTDlearning1945914_1" id="id4251"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Q-Learning"><div class="sect2" id="id363">
<h2>Q-Learning</h2>

<p>Similarly<a data-type="indexterm" data-primary="Q-learning algorithm" id="xi_Qlearningalgorithm1949010_1"/>, the Q-learning algorithm is an adaptation of the Q-value iteration algorithm to the situation where the transition probabilities and the rewards are initially unknown (see <a data-type="xref" href="#q_learning_equation">Equation 19-6</a>). Q-learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-values. Once it has <span class="keep-together">accurate</span> Q-value estimates (or close enough), then the optimal policy is to choose the action that has the highest Q-value (i.e., the greedy policy).</p>
<div id="q_learning_equation" data-type="equation" class="less_space pagebreak-before">
<h5><span class="label">Equation 19-6. </span>Q-learning algorithm</h5>
<math alttext="upper Q left-parenthesis s comma a right-parenthesis left-arrow Underscript alpha Endscripts r plus gamma dot max Underscript a Superscript prime Baseline Endscripts upper Q left-parenthesis s prime comma a prime right-parenthesis" display="block">
  <mrow>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mspace width="0.166667em"/>
    <munder><mo>←</mo> <mi>α</mi> </munder>
    <mspace width="0.166667em"/>
    <mi>r</mi>
    <mo>+</mo>
    <mi>γ</mi>
    <mo>·</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>'</mo> </msup> </munder>
    <mspace width="0.166667em"/>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <msup><mi>s</mi> <mo>'</mo> </msup>
      <mo lspace="0%" rspace="0%">,</mo>
      <msup><mi>a</mi> <mo>'</mo> </msup>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>For each state-action pair (<em>s</em>, <em>a</em>), this algorithm keeps track of a running average of the rewards <em>r</em> the agent gets upon leaving the state <em>s</em> with action <em>a</em>, plus the sum of discounted future rewards it expects to get. To estimate this sum, we take the maximum of the Q-value estimates for the next state <em>s</em>′, since we assume that the target policy will act optimally from then on.</p>

<p>Let’s implement the Q-learning algorithm<a data-type="indexterm" data-primary="Q-learning algorithm" data-secondary="implementation" id="xi_Qlearningalgorithmimplementation1950341_1"/>. First, we will need to make an agent explore the environment. For this, we need a step function so that the agent can execute one action and get the resulting state and reward:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">):</code>
    <code class="n">probas</code> <code class="o">=</code> <code class="n">transition_probabilities</code><code class="p">[</code><code class="n">state</code><code class="p">][</code><code class="n">action</code><code class="p">]</code>
    <code class="n">next_state</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="n">p</code><code class="o">=</code><code class="n">probas</code><code class="p">)</code>
    <code class="n">reward</code> <code class="o">=</code> <code class="n">rewards</code><code class="p">[</code><code class="n">state</code><code class="p">][</code><code class="n">action</code><code class="p">][</code><code class="n">next_state</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code></pre>

<p>Now let’s implement the agent’s exploration policy. Since the state space is pretty small, a simple random policy will be sufficient. If we run the algorithm for long enough, the agent will visit every state many times, and it will also try every possible action many times:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">exploration_policy</code><code class="p">(</code><code class="n">state</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">possible_actions</code><code class="p">[</code><code class="n">state</code><code class="p">])</code></pre>

<p>Next, after we initialize the Q-values just like earlier, we are ready to run the Q-learning algorithm with learning rate decay (using power scheduling, introduced in <a data-type="xref" href="ch11.html#deep_chapter">Chapter 11</a>):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">alpha0</code> <code class="o">=</code> <code class="mf">0.05</code>  <code class="c1"># initial learning rate</code>
<code class="n">decay</code> <code class="o">=</code> <code class="mf">0.005</code>  <code class="c1"># learning rate decay</code>
<code class="n">gamma</code> <code class="o">=</code> <code class="mf">0.90</code>  <code class="c1"># discount factor</code>
<code class="n">state</code> <code class="o">=</code> <code class="mi">0</code>  <code class="c1"># initial state</code>

<code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10_000</code><code class="p">):</code>
    <code class="n">action</code> <code class="o">=</code> <code class="n">exploration_policy</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
    <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code> <code class="o">=</code> <code class="n">step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">)</code>
    <code class="n">next_value</code> <code class="o">=</code> <code class="n">Q_values</code><code class="p">[</code><code class="n">next_state</code><code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>  <code class="c1"># greedy policy at the next step</code>
    <code class="n">alpha</code> <code class="o">=</code> <code class="n">alpha0</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">iteration</code> <code class="o">*</code> <code class="n">decay</code><code class="p">)</code>
    <code class="n">Q_values</code><code class="p">[</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">]</code> <code class="o">*=</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">alpha</code>
    <code class="n">Q_values</code><code class="p">[</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">]</code> <code class="o">+=</code> <code class="n">alpha</code> <code class="o">*</code> <code class="p">(</code><code class="n">reward</code> <code class="o">+</code> <code class="n">gamma</code> <code class="o">*</code> <code class="n">next_value</code><code class="p">)</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">next_state</code></pre>

<p>This algorithm will converge to the optimal Q-values, but it will take many iterations, and possibly quite a lot of hyperparameter tuning. As you can see in <a data-type="xref" href="#q_value_plot">Figure 19-9</a>, the Q-value iteration algorithm (left) converges very quickly, in fewer than 20 iterations, while the Q-learning algorithm (right) takes about 8,000 iterations to converge. Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!</p>

<figure><div id="q_value_plot" class="figure">
<img src="assets/hmls_1909.png" alt="Comparison of learning curves showing Q-value iteration converging in under 20 iterations and Q-learning taking around 8,000 iterations." width="2862" height="1061"/>
<h6><span class="label">Figure 19-9. </span>Learning curve of the Q-value iteration algorithm versus the Q-learning algorithm</h6>
</div></figure>

<p>The Q-learning algorithm is called an <em>off-policy</em> algorithm<a data-type="indexterm" data-primary="off-policy algorithm" id="id4252"/> because the policy being trained is not necessarily the one used during training. For example, in the code we just ran, the policy being executed (the exploration policy) was completely random, while the policy being trained was never used. After training, the optimal policy corresponds to systematically choosing the action with the highest Q-value. Conversely, the REINFORCE algorithm is <em>on-policy</em>:<a data-type="indexterm" data-primary="on-policy algorithm" id="id4253"/> it explores the world using the policy being trained. It is somewhat surprising that Q-learning is capable of learning the optimal policy by just watching an agent act randomly. Imagine learning to play golf when your teacher is a blindfolded monkey. Can we do better?<a data-type="indexterm" data-startref="xi_Qlearningalgorithmimplementation1950341_1" id="id4254"/></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Exploration Policies"><div class="sect2" id="id364">
<h2>Exploration Policies</h2>

<p>Of<a data-type="indexterm" data-primary="Q-learning algorithm" data-secondary="exploration policies" id="id4255"/> course, Q-learning can work only if the exploration policy explores the MDP thoroughly enough. Although a purely random policy is guaranteed to eventually visit every state and every transition many times, it may take an extremely long time to do so. Therefore, a better option is to use the <em>ε-greedy policy</em> (ε is epsilon): at each step it acts randomly with probability <em>ε</em>, or greedily with probability 1–<em>ε</em> (i.e., choosing the action with the highest Q-value). The advantage of the <em>ε</em>-greedy policy<a data-type="indexterm" data-primary="ε-greedy policy" id="id4256"/> (compared to a completely random policy) is that it will spend more and more time exploring the interesting parts of the environment, as the Q-value estimates get better and better, while still spending some time visiting unknown regions of the MDP. It is quite common to start with a high value for <em>ε</em> (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05).</p>

<p>Alternatively, rather than relying only on chance for exploration, another approach is to encourage the exploration policy to try actions that it has not tried much before. This can be implemented as a bonus added to the Q-value estimates, as shown in <a data-type="xref" href="#exploration_function_equation">Equation 19-7</a>.</p>
<div id="exploration_function_equation" data-type="equation">
<h5><span class="label">Equation 19-7. </span>Q-learning using an exploration function</h5>
<math alttext="upper Q left-parenthesis s comma a right-parenthesis left-arrow Underscript alpha Endscripts r plus gamma dot max Underscript a Superscript prime Baseline Endscripts f left-parenthesis upper Q left-parenthesis s prime comma a Superscript prime Baseline right-parenthesis comma upper N left-parenthesis s prime comma a prime right-parenthesis right-parenthesis" display="block">
  <mrow>
    <mi>Q</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mspace width="0.166667em"/>
    <munder><mo>←</mo> <mi>α</mi> </munder>
    <mspace width="0.166667em"/>
    <mi>r</mi>
    <mo>+</mo>
    <mi>γ</mi>
    <mo>·</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>'</mo> </msup> </munder>
    <mspace width="0.166667em"/>
    <mi>f</mi>
    <mfenced separators="" open="(" close=")">
      <mi>Q</mi>
      <mrow>
        <mo>(</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo lspace="0%" rspace="0%">,</mo>
        <msup><mi>a</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>N</mi>
      <mrow>
        <mo>(</mo>
        <msup><mi>s</mi> <mo>'</mo> </msup>
        <mo lspace="0%" rspace="0%">,</mo>
        <msup><mi>a</mi> <mo>'</mo> </msup>
        <mo>)</mo>
      </mrow>
    </mfenced>
  </mrow>
</math>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>N</em>(<em>s</em>′, <em>a</em>′) counts the number of times the action <em>a</em>′ was chosen in state <em>s</em>′.</p>
</li>
<li>
<p><em>f</em>(<em>Q</em>, <em>N</em>) is an <em>exploration function</em>, such as <em>f</em>(<em>Q</em>, <em>N</em>) = <em>Q</em> + <em>κ</em>/(1 + <em>N</em>), where <em>κ</em> (kappa) is a curiosity hyperparameter that measures how much the agent is attracted to the unknown.</p>
</li>
</ul>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Approximate Q-Learning and Deep Q-Learning"><div class="sect2" id="id365">
<h2>Approximate Q-Learning and Deep Q-Learning</h2>

<p>The<a data-type="indexterm" data-primary="approximate Q-learning" id="id4257"/><a data-type="indexterm" data-primary="Q-learning algorithm" data-secondary="approximate Q-learning" id="id4258"/> main problem with Q-learning is that it does not scale well to large (or even medium) MDPs with many states and actions. For example, suppose you wanted to use Q-learning to train an agent to play <em>Ms. Pac-Man</em> (see <a data-type="xref" href="#rl_examples_diagram">Figure 19-1</a>). There are about 240 pellets that Ms. Pac-Man can eat, each of which can be present or absent (i.e., already eaten). So, the number of possible pellet states is about 2<sup>240</sup> ≈ 10<sup>73</sup>. And if you add all the possible combinations of positions for all the ghosts and Ms. Pac-Man, the number of possible states becomes larger than the number of atoms in our galaxy, so there’s absolutely no way you can keep track of an estimate for every single Q-value.</p>

<p>The solution is to find a function <em>Q</em><sub><strong>θ</strong></sub>(<em>s</em>, <em>a</em>) that approximates the Q-value of any state-action pair (<em>s</em>, <em>a</em>), where the vector <strong>θ</strong> parameterizes the function. This is called <em>approximate Q-learning</em>. For years it was recommended to use linear combinations of handcrafted features extracted from the state (e.g., the distances of the closest ghosts, their directions, and so on) to estimate Q-values, but in 2013, <a href="https://homl.info/dqn">DeepMind</a> showed that using deep neural networks can work much better, especially for complex problems, and it does not require any feature engineering. A DNN that is used to estimate Q-values is called a <em>deep Q-network</em> (DQN), and using a DQN for approximate Q-learning is called <em>deep Q-learning</em>.</p>

<p>Now, how can we train a DQN? Well, consider the approximate Q-value computed by the DQN for a given state-action pair (<em>s</em>, <em>a</em>). Thanks to Bellman, we know we want this approximate Q-value to be as close as possible to the reward <em>r</em> that we actually observe after playing action <em>a</em> in state <em>s</em>, plus the discounted value of playing optimally from then on. To estimate this sum of future discounted rewards, we can just execute the DQN on the next state <em>s</em>′, for all possible actions <em>a</em>′. We get an approximate future Q-value for each possible action. We then pick the highest (since we assume we will be playing optimally) and discount it, and this gives us an estimate of the sum of future discounted rewards. By summing the reward <em>r</em> and the future discounted value estimate, we get a target Q-value <em>y</em>(<em>s</em>, <em>a</em>) for the state-action pair (<em>s</em>, <em>a</em>), as shown in <a data-type="xref" href="#target_q_value_equation">Equation 19-8</a>.</p>
<div id="target_q_value_equation" data-type="equation">
<h5><span class="label">Equation 19-8. </span>Target Q-value</h5>
<math display="block">
  <mrow>
    <mi>y</mi>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo lspace="0%" rspace="0%">,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>r</mi>
    <mo>+</mo>
    <mi>γ</mi>
    <mo>·</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <msup><mi>a</mi> <mo>'</mo> </msup> </munder>
    <mspace width="0.166667em"/>
    <msub><mi>Q</mi> <mi mathvariant="bold">θ</mi> </msub>
    <mrow>
      <mo>(</mo>
      <msup><mi>s</mi> <mo>'</mo> </msup>
      <mo lspace="0%" rspace="0%">,</mo>
      <msup><mi>a</mi> <mo>'</mo> </msup>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>With this target Q-value, we can run a training step using any gradient descent algorithm. In general, we try to minimize the squared error between the estimated Q-value <em>Q</em><sub><strong>θ</strong></sub>(<em>s</em>, <em>a</em>) and the target Q-value <em>y</em>(<em>s</em>, <em>a</em>), or the Huber loss to reduce the algorithm’s sensitivity to large errors. And that’s the deep Q-learning algorithm! Let’s see how to implement it to solve the CartPole environment.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Implementing Deep Q-Learning"><div class="sect2" id="id366">
<h2>Implementing Deep Q-Learning</h2>

<p>The<a data-type="indexterm" data-primary="deep Q-learning" id="xi_deepQlearning195874_1"/><a data-type="indexterm" data-primary="Q-learning algorithm" data-secondary="deep Q-learning" id="xi_QlearningalgorithmdeepQlearning195874_1"/> first thing we need is a deep Q-network. In theory, we need a neural net that takes a state-action pair as input, and outputs an approximate Q-value. However, in practice it’s much more efficient to use a neural net that takes only a state as input, and outputs one approximate Q-value for each possible action. To solve the CartPole environment, we do not need a very complicated neural net; a couple of hidden layers will do:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DQN</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">net</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">32</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
                                 <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">32</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
                                 <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">state</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">net</code><code class="p">(</code><code class="n">state</code><code class="p">)</code></pre>

<p>Our DQN is very similar to our earlier policy network, except it outputs a Q-value for each action instead of logits. Now let’s define a function to choose an action based on this DQN:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">choose_dqn_action</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">obs</code><code class="p">,</code> <code class="n">epsilon</code><code class="o">=</code><code class="mf">0.0</code><code class="p">):</code>
        <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">rand</code><code class="p">(())</code> <code class="o">&lt;</code> <code class="n">epsilon</code><code class="p">:</code>  <code class="c1"># epsilon greedy policy</code>
            <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">())</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="n">state</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">as_tensor</code><code class="p">(</code><code class="n">obs</code><code class="p">)</code>
            <code class="n">Q_values</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
            <code class="k">return</code> <code class="n">Q_values</code><code class="o">.</code><code class="n">argmax</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>  <code class="c1"># optimal according to the DQN</code></pre>

<p>This function takes an environment state (a single observation) and passes it to the neural net to predict the Q-values, then it simply returns the action with the largest predicted Q-value (<code translate="no">argmax()</code>). To ensure that the agent explores the environment, we use an <em>ε</em>-greedy policy, <a data-type="indexterm" data-primary="ε-greedy policy" id="id4259"/>meaning we choose a random action with probability <em>ε</em>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>DQNs generally don’t work with continuous action spaces, unless you can discretize the space (which only works if it’s tiny) or combine them with policy gradients. This is because the DQN agent must find the action with the highest Q-value at each step. In a continuous action space, this requires running an optimization algorithm on the Q-value function at each step, which is not 
<span class="keep-together">practical.</span></p>
</div>

<p>Instead of training the DQN based only on the latest experiences, we will store all experiences in a <em>replay buffer</em><a data-type="indexterm" data-primary="replay buffer" id="xi_replaybuffer19619117_1"/> (or <em>replay memory</em>), and we will sample a random training batch from it at each training iteration. This helps reduce the correlations between the experiences in a training batch, which stabilizes training by making the data distribution more consistent. Each experience will be represented as a tuple with six elements: a state <em>s</em>, the action <em>a</em> that the agent took, the resulting reward <em>r</em>, the next state <em>s′</em> it reached, a boolean indicating whether the episode ended at that point (<code translate="no">done</code>), and finally another boolean indicating whether the episode was truncated at that point. We will also need a function to sample a random batch of experiences from the replay buffer. It will return a tuple containing six tensors, one for each field:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">sample_experiences</code><code class="p">(</code><code class="n">replay_buffer</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">):</code>
    <code class="n">indices</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">replay_buffer</code><code class="p">),</code> <code class="n">size</code><code class="o">=</code><code class="p">[</code><code class="n">batch_size</code><code class="p">])</code>
    <code class="n">batch</code> <code class="o">=</code> <code class="p">[</code><code class="n">replay_buffer</code><code class="p">[</code><code class="n">index</code><code class="p">]</code> <code class="k">for</code> <code class="n">index</code> <code class="ow">in</code> <code class="n">indices</code><code class="o">.</code><code class="n">tolist</code><code class="p">()]</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">to_tensor</code><code class="p">([</code><code class="n">exp</code><code class="p">[</code><code class="n">index</code><code class="p">]</code> <code class="k">for</code> <code class="n">exp</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">])</code> <code class="k">for</code> <code class="n">index</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">6</code><code class="p">)]</code>

<code class="k">def</code> <code class="nf">to_tensor</code><code class="p">(</code><code class="n">data</code><code class="p">):</code>
    <code class="n">array</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">stack</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
    <code class="n">dtype</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">float32</code> <code class="k">if</code> <code class="n">array</code><code class="o">.</code><code class="n">dtype</code> <code class="o">==</code> <code class="n">np</code><code class="o">.</code><code class="n">float64</code> <code class="k">else</code> <code class="kc">None</code>
    <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">as_tensor</code><code class="p">(</code><code class="n">array</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">dtype</code><code class="p">)</code></pre>

<p>The <code translate="no">sample_experiences()</code> function takes a replay buffer and a batch size, and it randomly samples the desired number of experience tuples from the buffer. Then, for each of the six fields in the experience tuples, it extracts that field from each experience in the batch, and converts that list to a tensor using the <code translate="no">to_tensor()</code> function. Lastly, it returns the list of six tensors. The tensors all have shape [<code translate="no">batch size</code>] except for the observation tensors, which have shape [<code translate="no">batch size</code>, 4].</p>

<p>The <code translate="no">to_tensor()</code> function takes a Python list containing observations (i.e., 64-bit NumPy arrays of shape [<code translate="no">4</code>]), or actions (integers), or rewards (floats), or booleans (done or truncated), and it returns a tensor of the appropriate PyTorch type. Note that the 64-bit NumPy arrays containing the observations are converted to 32-bit tensors.</p>

<p>The replay buffer can be any data structure that supports appending and indexing, and can limit the size to avoid blowing up memory during training. For simplicity, we will use a Python <em>deque</em>,<a data-type="indexterm" data-primary="deque" id="id4260"/> from the standard <code translate="no">collections</code> package. This is a double-ended queue, in which elements can be efficiently appended or popped (i.e., removed) on both ends. If you set a size limit, and that limit is reached, appending an element to one end of the queue automatically pops an item from the other side. This means that each new experience replaces the oldest experience, which is exactly what we want.<a data-type="indexterm" data-startref="xi_replaybuffer19619117_1" id="id4261"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>Appending and popping items on the ends of a deque is very fast, but random access can be slow when the queue gets very long (e.g., 100,000 items or more). If you need a very large replay buffer, you should use a circular buffer instead (see the notebook for an implementation), or check out <a href="https://homl.info/reverb">DeepMind’s Reverb library</a>.</p>
</div>

<p>Let’s also create a function that will play a full episode using our DQN, and store the resulting experiences in the replay buffer. We’ll run in eval mode with <code translate="no">torch.no_grad()</code> since we don’t need gradients for now. For logging purposes, we’ll also make the function sum up all the rewards in the episode and return the result:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">play_and_record_episode</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">replay_buffer</code><code class="p">,</code> <code class="n">epsilon</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
    <code class="n">obs</code><code class="p">,</code> <code class="n">_info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="n">total_rewards</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
            <code class="n">action</code> <code class="o">=</code> <code class="n">choose_dqn_action</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">obs</code><code class="p">,</code> <code class="n">epsilon</code><code class="p">)</code>
            <code class="n">next_obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">,</code> <code class="n">_info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
            <code class="n">experience</code> <code class="o">=</code> <code class="p">(</code><code class="n">obs</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">next_obs</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">)</code>
            <code class="n">replay_buffer</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">experience</code><code class="p">)</code>
            <code class="n">total_rewards</code> <code class="o">+=</code> <code class="n">reward</code>
            <code class="k">if</code> <code class="n">done</code> <code class="ow">or</code> <code class="n">truncated</code><code class="p">:</code>
                <code class="k">return</code> <code class="n">total_rewards</code>
            <code class="n">obs</code> <code class="o">=</code> <code class="n">next_obs</code></pre>

<p>Next, let’s create a function that will sample a batch of experiences from the replay buffer and train the DQN by performing a single gradient descent step on this batch:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">dqn_training_step</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">replay_buffer</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code>
                      <code class="n">discount_factor</code><code class="p">):</code>
    <code class="n">experiences</code> <code class="o">=</code> <code class="n">sample_experiences</code><code class="p">(</code><code class="n">replay_buffer</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">)</code>
    <code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code> <code class="o">=</code> <code class="n">experiences</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">inference_mode</code><code class="p">():</code>
        <code class="n">next_Q_value</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">next_state</code><code class="p">)</code>

    <code class="n">max_next_Q_value</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">next_Q_value</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">running</code> <code class="o">=</code> <code class="p">(</code><code class="o">~</code><code class="p">(</code><code class="n">done</code> <code class="o">|</code> <code class="n">truncated</code><code class="p">))</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>  <code class="c1"># 0 if s' is over, 1 if running</code>
    <code class="n">target_Q_value</code> <code class="o">=</code> <code class="n">reward</code> <code class="o">+</code> <code class="n">running</code> <code class="o">*</code> <code class="n">discount_factor</code> <code class="o">*</code> <code class="n">max_next_Q_value</code>
    <code class="n">all_Q_values</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
    <code class="n">Q_value</code> <code class="o">=</code> <code class="n">all_Q_values</code><code class="o">.</code><code class="n">gather</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">action</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">Q_value</code><code class="p">,</code> <code class="n">target_Q_value</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The <code translate="no">torch.inference_mode()</code><a data-type="indexterm" data-primary="torch" data-secondary="inference_mode()" id="id4262"/> context is like <code translate="no">torch.no_grad()</code>, plus models run in eval mode within the context, and new tensors cannot be used in backpropagation.</p>
</div>

<p>Here’s what’s happening in this code:</p>

<ul>
<li>
<p>The function starts by sampling a batch of experiences from the replay buffer.</p>
</li>
<li>
<p>Then it uses the DQN to compute the target Q-value for each experience in the batch. For this, the code implements <a data-type="xref" href="#target_q_value_equation">Equation 19-8</a>: the DQN is used in inference mode to evaluate all the Q-values for the next state <em>s’</em>, then we keep only the max Q-value since we assume that the agent will play optimally from now on, and we multiply this max Q-value with the discount factor. If the episode was over (done or truncated), then the discounted max Q-value is multiplied by zero since we cannot expect any more rewards. Otherwise, it’s multiplied by 1 (i.e., unchanged). Lastly, we add the experience’s reward. All of this is performed simultaneously for all experiences in the batch.</p>
</li>
<li>
<p>Next, the function uses the model again (in training mode this time) to compute all the Q-values for the current state <em>s</em>, and it uses the <code translate="no">gather()</code> method to extract just the Q-value that corresponds to the action that was actually chosen. Again, this is done simultaneously for all experiences in the batch.</p>
</li>
<li>
<p>Lastly, we compute the loss, which is typically the MSE between the target Q-values and the predicted Q-values, and we perform an optimizer step to minimize the loss.</p>
</li>
</ul>

<p>Phew! That was the hardest part. Now we can write the main training function and run it:</p>

<pre translate="no" data-type="programlisting" data-code-language="python" class="widows15"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">deque</code>

<code class="k">def</code> <code class="nf">train_dqn</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">replay_buffer</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">n_episodes</code><code class="o">=</code><code class="mi">800</code><code class="p">,</code>
              <code class="n">warmup</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">discount_factor</code><code class="o">=</code><code class="mf">0.95</code><code class="p">):</code>
    <code class="n">totals</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">episode</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_episodes</code><code class="p">):</code>
        <code class="n">epsilon</code> <code class="o">=</code> <code class="nb">max</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">episode</code> <code class="o">/</code> <code class="mi">500</code><code class="p">,</code> <code class="mf">0.01</code><code class="p">)</code>
        <code class="n">seed</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="o">**</code><code class="mi">32</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">())</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
        <code class="n">total_rewards</code> <code class="o">=</code> <code class="n">play_and_record_episode</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">replay_buffer</code><code class="p">,</code>
                                                <code class="n">epsilon</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\r</code><code class="s2">Episode: </code><code class="si">{</code><code class="n">episode</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">, Rewards: </code><code class="si">{</code><code class="n">total_rewards</code><code class="si">}</code><code class="s2">"</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">" "</code><code class="p">)</code>
        <code class="n">totals</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">total_rewards</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">episode</code> <code class="o">&gt;=</code> <code class="n">warmup</code><code class="p">:</code>
            <code class="n">dqn_training_step</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">replay_buffer</code><code class="p">,</code>
                              <code class="n">batch_size</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">totals</code>

<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">dqn</code> <code class="o">=</code> <code class="n">DQN</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">dqn</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.03</code><code class="p">)</code>
<code class="n">mse</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">replay_buffer</code> <code class="o">=</code> <code class="n">deque</code><code class="p">(</code><code class="n">maxlen</code><code class="o">=</code><code class="mi">100_000</code><code class="p">)</code>
<code class="n">totals</code> <code class="o">=</code> <code class="n">train_dqn</code><code class="p">(</code><code class="n">dqn</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">replay_buffer</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">mse</code><code class="p">)</code></pre>

<p>The training algorithm runs for 800 episodes. At each training iteration, we make the DQN play one full episode using the <code translate="no">play_and_record_episode()</code> function, then we run one training step using the <code translate="no">dqn_training_step()</code> function. Note that we only start training after several warmup episodes to ensure that the replay buffer contains plenty of experiences. We also linearly decrease the epsilon value for the ε-greedy policy from 1.0 down to 0.01 after 500 episodes (then it remains at 0.01). This way, the agent’s behavior will gradually become less random, focusing more on exploitation and less on exploration. The function also records the total rewards for each episode, and returns these totals; they are plotted in <a data-type="xref" href="#dqn_rewards_plot">Figure 19-10</a>.</p>

<figure><div id="dqn_rewards_plot" class="figure">
<img src="assets/hmls_1910.png" alt="Graph showing the sum of rewards per episode during deep Q-learning training, illustrating increasing performance with noticeable improvement after 500 episodes." width="2209" height="1074"/>
<h6><span class="label">Figure 19-10. </span>Learning curve of the deep Q-learning algorithm</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why not plot the loss? Well, it’s a poor indicator of the model’s performance, so it’s preferable to plot the total rewards for each episode. Indeed, the loss might go down while the agent performs worse (e.g., if the agent gets stuck in one small region of the environment and the DQN starts overfitting it). Conversely, the loss could go up while the agent performs better (e.g., if the DQN was underestimating the target Q-values and it starts correctly increasing them).</p>
</div>

<p>The good news is that the algorithm worked: the trained agent perfectly balances the pole on the cart and reaches the maximum total reward of 1,000. The bad news is that the training is completely unstable. In fact, it’s even less stable than REINFORCE. I had to tweak the hyperparameters quite a bit before stumbling upon this successful training run. As you can see, the agent managed to reach a reward of 200 points after roughly 200 episodes, which isn’t bad, but soon after it forgot everything and performed terribly until episode ~550, when it quickly cracked the problem.</p>

<p>So why is this DQN implementation unstable? Could it be the data distribution? Well, the replay buffer is quite large, so the data distribution is certainly much more stable than with the REINFORCE algorithm. So what’s happening? Well, in this basic deep Q-learning implementation, the model is used both to make predictions and to set its own targets. This can lead to a situation analogous to a dog chasing its own tail. This feedback loop can make the network unstable: it can diverge, oscillate, freeze, and so on. Luckily, there are ways to improve this; let’s see how.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="DQN Improvements"><div class="sect2" id="id367">
<h2>DQN Improvements</h2>

<p>In their 2013 paper, DeepMind<a data-type="indexterm" data-primary="DeepMind" data-secondary="and reinforcement learning" data-secondary-sortas="reinforcement learning" id="id4263"/> researchers proposed a way to stabilize DQN training by using two DQNs instead of one: the first is the <em>online model</em>,<a data-type="indexterm" data-primary="online model, DQN" id="id4264"/> which learns at each step and is used to move the agent around, and the other is the <em>target model</em><a data-type="indexterm" data-primary="target model, DQN" id="id4265"/> used only to define the targets. The target model is just a clone of the online model, and its weights are copied from the online model at regular intervals (e.g., every 10,000 steps in their Atari models). This makes the Q-value targets much more stable, so the feedback loop is damped, and its effects are much less severe. They combined this major improvement with several other tweaks: a very large replay buffer, a tiny learning rate, a very long training time (50 million steps), a very slowly decreasing epsilon (over 1 million steps), and a powerful neural net (a CNN).</p>

<p>Then, in a <a href="https://homl.info/doubledqn">2015 paper</a>,⁠<sup><a data-type="noteref" id="id4266-marker" href="ch19.html#id4266">15</a></sup> DeepMind researchers tweaked their DQN algorithm again, increasing its performance and somewhat stabilizing training. They called this variant <em>double DQN</em>.<a data-type="indexterm" data-primary="double DQN" id="id4267"/> The update was based on the 
<span class="keep-together">observation</span> that the target network is prone to overestimating Q-values. Indeed, suppose all actions are equally good: the Q-values estimated by the target model should be identical, but since they are approximations, some may be slightly greater than others by pure chance. The target model will always select the largest Q-value, which will be slightly greater than the mean Q-value, most likely overestimating the true Q-value (a bit like counting the height of the tallest random wave when measuring the depth of a pool). To fix this, the researchers proposed using the online model instead of the target model when selecting the best action for the next state, and using the target model only to estimate the Q-value of this best action.</p>

<p>Another important improvement was the introduction of <em>prioritized experience replay</em> (PER)<a data-type="indexterm" data-primary="PER (prioritized experience replay)" id="id4268"/><a data-type="indexterm" data-primary="prioritized experience replay (PER)" id="id4269"/>, which was proposed in a <a href="https://homl.info/prioreplay">2015 paper</a>⁠<sup><a data-type="noteref" id="id4270-marker" href="ch19.html#id4270">16</a></sup> by DeepMind researchers (once again!). Instead of sampling experiences <em>uniformly</em> from the replay buffer, why not sample important experiences more frequently?</p>

<p>More specifically, experiences are considered “important” if they are likely to lead to fast learning progress. But how can we estimate this? One reasonable approach is to measure the magnitude of the TD error <em>δ</em> = <em>r</em> + <em>γ</em>·<em>V</em>(<em>s</em>′) – <em>V</em>(<em>s</em>). A large TD<a data-type="indexterm" data-primary="temporal difference (TD) learning" id="id4271"/> error 
<span class="keep-together">indicates</span> that a transition (<em>s</em>, <em>a</em>, <em>s</em>′) is very surprising and thus probably worth learning from.⁠<sup><a data-type="noteref" id="id4272-marker" href="ch19.html#id4272">17</a></sup> When an experience is recorded in the replay buffer, its priority is set to a very large value to ensure that it gets sampled at least once. However, once it is sampled (and every time it is sampled), the TD error <em>δ</em> is computed, and this experience’s priority is set to <em>p</em> = |<em>δ</em>| (plus a small constant to ensure that every experience has a nonzero probability of being sampled). The probability <em>P</em> of sampling an experience with priority <em>p</em> is proportional to <em>p</em><sup><em>ζ</em></sup>, where <em>ζ</em> (zeta) is a hyperparameter<a data-type="indexterm" data-primary="hyperparameters" data-secondary="importance sampling (ζ)" id="id4273"/><a data-type="indexterm" data-primary="ζ (zeta) hyperparameter" id="id4274"/><a data-type="indexterm" data-primary="zeta (ζ) hyperparameter" id="id4275"/> that controls how greedy we want importance sampling<a data-type="indexterm" data-primary="importance sampling (IS)" id="id4276"/><a data-type="indexterm" data-primary="IS (importance sampling)" id="id4277"/> to be: when <em>ζ</em> = 0, we just get uniform sampling, and when <em>ζ</em> = 1, we get full-blown importance sampling. In the paper, the authors used <em>ζ</em> = 0.6, but the optimal value will depend on the task.</p>

<p>There’s one catch though: since the samples will be biased toward important experiences, we must compensate for this bias during training by downweighting the experiences according to their importance, or the model will just overfit the important experiences. To be clear, we want important experiences to be sampled more often, but this also means we must give them a lower weight during training. To do this, we define each experience’s training weight<a data-type="indexterm" data-primary="weights" data-secondary="in prioritized experience replay" data-secondary-sortas="prioritized experience replay" id="id4278"/> as <em>w</em> = (<em>n</em> <em>P</em>)<sup>–<em>β</em></sup>, where <em>n</em> is the number of experiences in the replay buffer, and <em>β</em> is a hyperparameter that controls how much we want to compensate for the importance sampling bias (0 means not at all, while 1 means entirely). In the paper, the authors used <em>β</em> = 0.4 at the beginning of training and linearly increased it to <em>β</em> = 1 by the end of training. Again, the optimal value will depend on the task, but if you increase one, you will usually want to increase the other as well.</p>

<p>One last noteworthy DQN variant is the <em>dueling DQN</em><a data-type="indexterm" data-primary="DDQN (Dueling DQN)" id="id4279"/><a data-type="indexterm" data-primary="Dueling DQN (DDQN)" id="id4280"/> algorithm (DDQN, not to be confused with double DQN, although both techniques can easily be combined). It was introduced in yet another <a href="https://homl.info/ddqn">2015 paper</a>⁠<sup><a data-type="noteref" id="id4281-marker" href="ch19.html#id4281">18</a></sup> by DeepMind researchers. To understand how it works, we must first note that the Q-value of a state-action pair (<em>s</em>, <em>a</em>) can be expressed as <em>Q</em>(<em>s</em>, <em>a</em>) = <em>V</em>(<em>s</em>) + <em>A</em>(<em>s</em>, <em>a</em>), where <em>V</em>(<em>s</em>) is the value of state <em>s</em>, and <em>A</em>(<em>s</em>, <em>a</em>) is the <em>advantage</em> of taking the action <em>a</em> in state <em>s</em>, compared to all other possible actions in that state. Moreover, the value of a state is equal to the Q-value of the best action <em>a</em><sup>*</sup> for that state (since we assume the optimal policy will pick the best action), so <em>V</em>(<em>s</em>) = <em>Q</em>(<em>s</em>, <em>a</em><sup>*</sup>), which implies that <em>A</em>(<em>s</em>, <em>a</em><sup>*</sup>) = 0. In a dueling DQN, the model estimates both the value of the state and the advantage of each possible action. Since the best action should have an advantage of 0, the model subtracts the maximum predicted <span class="keep-together">advantage</span> from all predicted advantages. The rest of the algorithm is just the same as earlier.</p>

<p>These techniques can be combined in various ways, as DeepMind<a data-type="indexterm" data-primary="DeepMind" data-secondary="and reinforcement learning" data-secondary-sortas="reinforcement learning" id="id4282"/> demonstrated in a <a href="https://homl.info/rainbow">2017 paper</a>:⁠<sup><a data-type="noteref" id="id4283-marker" href="ch19.html#id4283">19</a></sup> the paper’s authors combined six different techniques into an agent called <em>Rainbow</em><a data-type="indexterm" data-primary="Rainbow agent" id="id4284"/>, which largely outperformed the state of the art.<a data-type="indexterm" data-startref="xi_Qlearningalgorithm1949010_1" id="id4285"/></p>

<p>Speaking of combining different methods, why not combine policy gradients with value-based methods to get the best of both worlds? This is the core idea behind actor-critic algorithms. Let’s discuss them now.<a data-type="indexterm" data-startref="xi_deepQlearning195874_1" id="id4286"/><a data-type="indexterm" data-startref="xi_QlearningalgorithmdeepQlearning195874_1" id="id4287"/></p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Actor-Critic Algorithms"><div class="sect1" id="id368">
<h1>Actor-Critic Algorithms</h1>

<p>Actor-critics<a data-type="indexterm" data-primary="actor-critic algorithms" id="xi_actorcriticalgorithms1975214_1"/><a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="actor-critic algorithms" id="xi_reinforcementlearningRLactorcriticalgorithms1975214_1"/> are a family of RL algorithms that combine policy gradients with value-based methods. An actor-critic is composed of a policy (the actor) and a value network (the critic), which are trained simultaneously. The actor relies on the critic to estimate the value (or advantage) of actions or states, guiding its policy updates. Since the critic can use a large replay buffer, it stabilizes training and increases data efficiency. It’s a bit like an athlete (the actor) learning with the help of a coach (the critic).</p>

<p>Moreover, actor-critic methods support stochastic policies and continuous action spaces, just like policy gradients. So we do get the best of both worlds.</p>

<p class="pagebreak-before">Let’s implement a basic actor-critic:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ActorCritic</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">body</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">32</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
                                  <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">32</code><code class="p">),</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">())</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">actor_head</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># outputs action logits</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">critic_head</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># outputs state values</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">state</code><code class="p">):</code>
        <code class="n">features</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">body</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">actor_head</code><code class="p">(</code><code class="n">features</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">critic_head</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>

<p>In the constructor, we build the actor and critic networks. In this implementation, they share the same lower layers (called the <em>body</em>)<a data-type="indexterm" data-primary="body, actor-critic" id="id4288"/>. This is common practice, as it reduces the total number of parameters and thereby increases data efficiency, but it also makes training a bit less stable since it couples the actor and critic more closely (another dog chasing its tail situation). The actor network takes a batch of environment states and outputs an action logit for each state (that’s the logit for action 1, just like for REINFORCE). The critic network estimates the value of each given state. The <code translate="no">forward()</code> method takes a batch of states and runs them through both networks (with a shared body), and returns the action logits and state values.</p>

<p>Now let’s write a function to choose an action. It’s identical to the <code translate="no">choose_action()</code> function we wrote earlier for the REINFORCE policy network, except that it also returns the state value estimated by the critic network. This will be needed for training:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">choose_action_and_evaluate</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">obs</code><code class="p">):</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">as_tensor</code><code class="p">(</code><code class="n">obs</code><code class="p">)</code>
    <code class="n">logit</code><code class="p">,</code> <code class="n">state_value</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
    <code class="n">dist</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">distributions</code><code class="o">.</code><code class="n">Bernoulli</code><code class="p">(</code><code class="n">logits</code><code class="o">=</code><code class="n">logit</code><code class="p">)</code>
    <code class="n">action</code> <code class="o">=</code> <code class="n">dist</code><code class="o">.</code><code class="n">sample</code><code class="p">()</code>
    <code class="n">log_prob</code> <code class="o">=</code> <code class="n">dist</code><code class="o">.</code><code class="n">log_prob</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
    <code class="k">return</code> <code class="nb">int</code><code class="p">(</code><code class="n">action</code><code class="o">.</code><code class="n">item</code><code class="p">()),</code> <code class="n">log_prob</code><code class="p">,</code> <code class="n">state_value</code></pre>

<p>Great! Now let’s see how to train our actor-critic. We’ll start by defining a function that will perform one training step:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">ac_training_step</code><code class="p">(</code><code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">state_value</code><code class="p">,</code> <code class="n">target_value</code><code class="p">,</code> <code class="n">log_prob</code><code class="p">,</code>
                     <code class="n">critic_weight</code><code class="p">):</code>
    <code class="n">td_error</code> <code class="o">=</code> <code class="n">target_value</code> <code class="o">-</code> <code class="n">state_value</code>
    <code class="n">actor_loss</code> <code class="o">=</code> <code class="o">-</code><code class="n">log_prob</code> <code class="o">*</code> <code class="n">td_error</code><code class="o">.</code><code class="n">detach</code><code class="p">()</code>
    <code class="n">critic_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">state_value</code><code class="p">,</code> <code class="n">target_value</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">actor_loss</code> <code class="o">+</code> <code class="n">critic_weight</code> <code class="o">*</code> <code class="n">critic_loss</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code></pre>

<p>First<a data-type="indexterm" data-primary="critic" data-see="actor-critic algorithms" id="id4289"/>, we compute the TD error, which is the difference between the target value <em>y</em> = <em>r</em> + <em>γ</em>V(<em>s’</em>) and the state value V(<em>s</em>). The actor’s loss is the same as in REINFORCE, except that we multiply the log probability by the TD error instead of the (standardized) return. In other words, we encourage actions that performed better than the value network expected. As for the critic’s loss, it encourages the critic’s value estimates V(<em>s</em>) to match the target values <em>y</em> (e.g., using the MSE). Lastly, the overall loss is a weighted sum of the actor’s loss and the critic’s loss. To stabilize training, it’s generally a good idea to give less weight to the critic’s loss. Then we perform an optimizer step to minimize the loss. Oh, and note that we call <code translate="no">td_error.detach()</code> because we don’t want gradient descent to affect the critic network via the actor’s loss.</p>

<p>We’ll also need a function to compute the target value:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">get_target_value</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">next_obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">):</code>
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">inference_mode</code><code class="p">():</code>
        <code class="n">_</code><code class="p">,</code> <code class="n">_</code><code class="p">,</code> <code class="n">next_state_value</code> <code class="o">=</code> <code class="n">choose_action_and_evaluate</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">next_obs</code><code class="p">)</code>

    <code class="n">running</code> <code class="o">=</code> <code class="mf">0.0</code> <code class="k">if</code> <code class="p">(</code><code class="n">done</code> <code class="ow">or</code> <code class="n">truncated</code><code class="p">)</code> <code class="k">else</code> <code class="mf">1.0</code>
    <code class="n">target_value</code> <code class="o">=</code> <code class="n">reward</code> <code class="o">+</code> <code class="n">running</code> <code class="o">*</code> <code class="n">discount_factor</code> <code class="o">*</code> <code class="n">next_state_value</code>
    <code class="k">return</code> <code class="n">target_value</code></pre>

<p>This code first evaluates V(<em>s’</em>) using the <code translate="no">choose_action_and_evaluate()</code> function (we ignore the chosen action and its log probability). We run this in inference mode because we are computing the target: we don’t want gradient descent to affect it. Next, we simply evaluate the target <em>y</em> = <em>r</em> + <em>γ</em>V(<em>s’</em>). If the episode is over, then <em>y</em> = <em>r</em>.</p>

<p>With that, we have all we need to write a function that will run a whole episode and train the actor-critic at each step (we also compute the total rewards and return it when the episode is over):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">run_episode_and_train</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">,</code>
                          <code class="n">critic_weight</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
    <code class="n">obs</code><code class="p">,</code> <code class="n">_info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
    <code class="n">total_rewards</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
        <code class="n">action</code><code class="p">,</code> <code class="n">log_prob</code><code class="p">,</code> <code class="n">state_value</code> <code class="o">=</code> <code class="n">choose_action_and_evaluate</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">obs</code><code class="p">)</code>
        <code class="n">next_obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">truncated</code><code class="p">,</code> <code class="n">_info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
        <code class="n">target_value</code> <code class="o">=</code> <code class="n">get_target_value</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">next_obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code>
                                        <code class="n">truncated</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">)</code>
        <code class="n">ac_training_step</code><code class="p">(</code><code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">state_value</code><code class="p">,</code> <code class="n">target_value</code><code class="p">,</code>
                         <code class="n">log_prob</code><code class="p">,</code> <code class="n">critic_weight</code><code class="p">)</code>
        <code class="n">total_rewards</code> <code class="o">+=</code> <code class="n">reward</code>
        <code class="k">if</code> <code class="n">done</code> <code class="ow">or</code> <code class="n">truncated</code><code class="p">:</code>
            <code class="k">return</code> <code class="n">total_rewards</code>
        <code class="n">obs</code> <code class="o">=</code> <code class="n">next_obs</code></pre>

<p>And lastly, we can write our main training function, which just calls the <code>run_​epi⁠sode_and_train()</code> function many times and returns the total rewards for each 
<span class="keep-together">episode:</span></p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_actor_critic</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code> <code class="n">n_episodes</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code>
                       <code class="n">discount_factor</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">critic_weight</code><code class="o">=</code><code class="mf">0.3</code><code class="p">):</code>
    <code class="n">totals</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">episode</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_episodes</code><code class="p">):</code>
        <code class="n">seed</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="o">**</code><code class="mi">32</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">())</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>
        <code class="n">total_rewards</code> <code class="o">=</code> <code class="n">run_episode_and_train</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">env</code><code class="p">,</code>
                                              <code class="n">discount_factor</code><code class="p">,</code> <code class="n">critic_weight</code><code class="p">,</code>
                                              <code class="n">seed</code><code class="o">=</code><code class="n">seed</code><code class="p">)</code>
        <code class="n">totals</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">total_rewards</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"</code><code class="se">\r</code><code class="s2">Episode: </code><code class="si">{</code><code class="n">episode</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">, Rewards: </code><code class="si">{</code><code class="n">total_rewards</code><code class="si">}</code><code class="s2">"</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">" "</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">totals</code></pre>

<p>Let’s run it!</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">ac_model</code> <code class="o">=</code> <code class="n">ActorCritic</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">NAdam</code><code class="p">(</code><code class="n">ac_model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">1.1e-3</code><code class="p">)</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">totals</code> <code class="o">=</code> <code class="n">train_actor_critic</code><code class="p">(</code><code class="n">ac_model</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">,</code> <code class="n">criterion</code><code class="p">,</code> <code class="n">env</code><code class="p">)</code></pre>

<p>And it works! We get a very stable CartPole that collects the maximum rewards. That said, this implementation is still very sensitive to the choice of hyperparameters and random seeds, and training is still very unstable. Luckily, researchers have come up with various techniques that can stabilize the actor-critic. Here are some of the most popular:</p>
<dl>
<dt><a href="https://homl.info/a3c"><em>Asynchronous advantage actor-critic (A3C)</em></a>⁠<sup><a data-type="noteref" id="id4290-marker" href="ch19.html#id4290">20</a></sup></dt>
<dd>
<p>This<a data-type="indexterm" data-primary="Asynchronous Advantage Actor-Critic (A3C)" id="id4291"/><a data-type="indexterm" data-primary="A3C (Asynchronous Advantage Actor-Critic)" id="id4292"/> is an important actor-critic variant introduced by DeepMind<a data-type="indexterm" data-primary="DeepMind" data-secondary="and reinforcement learning" data-secondary-sortas="reinforcement learning" id="id4293"/> researchers in 2016 where multiple agents learn in parallel, exploring different copies of the environment. At regular intervals, but asynchronously (hence the name), each agent pushes some weight updates to a master network, then it pulls the latest weights from that network. Each agent thus contributes to improving the master network and benefits from what the other agents have learned. Moreover, instead of estimating the state values, or even the Q-values, the critic estimates the <em>advantage</em> of each action (hence the second A in the name), just like in the Dueling DQN.</p>
</dd>
<dt><a href="https://homl.info/a2c"><em>Advantage actor-critic (A2C)</em></a></dt>
<dd>
<p>A2C<a data-type="indexterm" data-primary="Advantage Actor-Critic (A2C)" id="id4294"/><a data-type="indexterm" data-primary="A2C (Advantage Actor-Critic)" id="id4295"/> is a variant of the A3C algorithm that removes the asynchronicity. All model updates are synchronous, so gradient updates are performed over larger batches, which allows the model to better utilize the power of the GPU.</p>
</dd>
<dt><a href="https://homl.info/sac"><em>Soft actor-critic (SAC)</em></a>⁠<sup><a data-type="noteref" id="id4296-marker" href="ch19.html#id4296">21</a></sup></dt>
<dd>
<p>SAC<a data-type="indexterm" data-primary="SAC (Soft Actor-Critic)" id="id4297"/><a data-type="indexterm" data-primary="Soft Actor-Critic (SAC)" id="id4298"/> is an actor-critic variant proposed in 2018 by Tuomas Haarnoja and other UC Berkeley researchers. It learns not only rewards, but also how to maximize the entropy of its actions. In other words, it tries to be as unpredictable as possible while still getting as many rewards as possible. This encourages the agent to explore the environment, which speeds up training and makes it less likely to repeatedly execute the same action when the critic produces imperfect estimates. This algorithm<a data-type="indexterm" data-primary="PPO (proximal policy optimization)" id="xi_PPOproximalpolicyoptimization19878495_1"/><a data-type="indexterm" data-primary="proximal policy optimization (PPO)" id="xi_proximalpolicyoptimizationPPO19878495_1"/><a data-type="indexterm" data-primary="policy and policy gradients" data-secondary="PPO" id="xi_policyandpolicygradientsPPO19878495_1"/> has demonstrated an amazing sample efficiency (contrary to all the previous algorithms, which learn very slowly).</p>
</dd>
<dt><a href="https://homl.info/ppo"><em>Proximal policy optimization (PPO)</em></a>⁠<sup><a data-type="noteref" id="id4299-marker" href="ch19.html#id4299">22</a></sup></dt>
<dd>
<p>This algorithm by John Schulman and other OpenAI researchers is based on A2C, but it clips the loss function to avoid excessively large weight updates (which often lead to training instabilities). PPO is a simplification of the previous <a href="https://homl.info/trpo"><em>trust region policy optimization</em> (TRPO) algorithm</a>,⁠<sup><a data-type="noteref" id="id4300-marker" href="ch19.html#id4300">23</a></sup> <a data-type="indexterm" data-primary="trust region policy optimization (TRPO)" id="id4301"/> also by OpenAI. OpenAI made the news in April 2019 with its AI called OpenAI Five, based on the PPO algorithm, which defeated the world champions at the multiplayer game <em>Dota 2</em>.</p>
</dd>
</dl>

<p>The last two algorithms, SAC and PPO, are among the most widely used RL algorithms today, and several libraries provide easy to use and highly optimized implementations. For example, let’s use the popular Stable-Baselines3 library to train a PPO agent on the <em>Breakout</em> Atari game.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Which RL algorithm should you use? PPO is a great general-purpose RL algorithm—a good bet if you’re not sure. SAC is the most sample efficient for continuous action tasks, making it ideal for robotics. DQN remains strong for discrete tasks such as Atari games or board games.<a data-type="indexterm" data-startref="xi_actorcriticalgorithms1975214_1" id="id4302"/><a data-type="indexterm" data-startref="xi_reinforcementlearningRLactorcriticalgorithms1975214_1" id="id4303"/></p>
</div>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Mastering Atari Breakout Using the &#10;Stable-Baselines3 PPO Implementation"><div class="sect1" id="id369">
<h1>Mastering Atari Breakout Using the 
<span class="keep-together">Stable-Baselines3 PPO Implementation</span></h1>

<p>Since<a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="Stable-Baselines3" id="xi_reinforcementlearningRLStableBaselines3198876_1"/><a data-type="indexterm" data-primary="SB3 (Stable-Baselines3)" id="xi_SB3StableBaselines3198876_1"/><a data-type="indexterm" data-primary="Stable-Baselines3 (SB3)" id="xi_StableBaselines3SB3198876_1"/> Stable-Baselines3 (SB3) is not installed by default on Colab, we must first run <code translate="no">%pip install -q stable_baselines3</code>, which will take a couple of minutes. However, if you are running the code on your own machine and you followed the <a href="https://homl.info/install-p">installation instructions</a>, then it’s already installed.</p>

<p>Next, we must create an ALE interface: it will run the Atari 2600 emulator and allow Gymnasium to interface with it (the Atari games will appear in the list of available environments):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">ale_py</code>

<code class="n">ale</code> <code class="o">=</code> <code class="n">ale_py</code><code class="o">.</code><code class="n">ALEInterface</code><code class="p">()</code></pre>

<p>Atari games were stored on read-only memory (ROM) cartridges. These ROMs can now be downloaded freely and used for research and educational purposes. On Colab, they are preinstalled, and if you followed the installation instructions to run the code locally, then they are also preinstalled.</p>

<p>Now that we have SB3, the ALE interface, and the ROMs, we are ready to create the <em>Breakout</em> environment. But instead of creating it using Gymnasium directly, we will use SB3’s <code translate="no">make_atari_env()</code> function: it creates a wrapper environment containing multiple <em>Breakout</em> environments that will run in parallel. Each observation from the wrapper environment will contain one observation for each <em>Breakout</em> environment. Similarly, the wrapper environment’s <code translate="no">step()</code> function will take an array containing one action for each <em>Breakout</em> environment. Lastly, the wrapper environment will take care of preprocessing the images, converting them from 210 × 160 RGB images to 84 × 84 grayscale images. Very convenient! So let’s create an SB3 environment containing four <em>Breakout</em> environments, and reset it to get an observation:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">stable_baselines3.common.env_util</code> <code class="kn">import</code> <code class="n">make_atari_env</code>

<code class="n">envs</code> <code class="o">=</code> <code class="n">make_atari_env</code><code class="p">(</code><code class="s2">"BreakoutNoFrameskip-v4"</code><code class="p">,</code> <code class="n">n_envs</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="n">obs</code> <code class="o">=</code> <code class="n">envs</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>  <code class="c1"># a 4 × 84 × 84 × 1 NumPy array (note: no info dict)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The <code translate="no">env.get_images()</code> method returns the original images, before preprocessing (see <a data-type="xref" href="#atari_preprocessing_plot">Figure 19-11</a>).</p>
</div>

<figure><div id="atari_preprocessing_plot" class="figure">
<img src="assets/hmls_1911.png" alt="Comparison of an original Breakout game frame in color with a preprocessed grayscale version, illustrating the change in dimensions and color reduction for efficiency." width="1174" height="664"/>
<h6><span class="label">Figure 19-11. </span>A <em>Breakout</em> frame before (left) and after (right) preprocessing</h6>
</div></figure>

<p>The ALE interface runs at 60 frames per second, which is quite fast, so consecutive frames look very similar, which wastes computation. To avoid this, the default <em>Breakout</em> environment repeats each action four times and returns only the final observation; this is called <em>frame skipping</em>.<a data-type="indexterm" data-primary="frame skipping" id="id4304"/> However, instead of skipping the frames, it’s preferable to stack them into a single four-channel image and use that as the observation. For this, we must first avoid frame skipping: this is why we used the <code translate="no">BreakoutNoFrameskip-v4</code> environment rather than <code translate="no">Breakout-v4</code>.⁠<sup><a data-type="noteref" id="id4305-marker" href="ch19.html#id4305">24</a></sup> Then, we must wrap the environment in a <code translate="no">VecFrameStack</code>; this wrapper environment will repeat each action several times (four in our case) and stack the resulting frames along the channel dimension (i.e., the last one):</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">stable_baselines3.common.vec_env</code> <code class="kn">import</code> <code class="n">VecFrameStack</code>

<code class="n">envs_stacked</code> <code class="o">=</code> <code class="n">VecFrameStack</code><code class="p">(</code><code class="n">envs</code><code class="p">,</code> <code class="n">n_stack</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="n">obs</code> <code class="o">=</code> <code class="n">envs_stacked</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>  <code class="c1"># returns a 4 × 84 × 84 × 4 NumPy array</code></pre>

<p>Now let’s create a PPO model with some good hyperparameters:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">stable_baselines3</code> <code class="kn">import</code> <code class="n">PPO</code>

<code class="n">ppo_model</code> <code class="o">=</code> <code class="n">PPO</code><code class="p">(</code><code class="s2">"CnnPolicy"</code><code class="p">,</code> <code class="n">envs_stacked</code><code class="p">,</code> <code class="n">device</code><code class="o">=</code><code class="n">device</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">2.5e-4</code><code class="p">,</code>
                <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code> <code class="n">n_steps</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code> <code class="n">n_epochs</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">clip_range</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
                <code class="n">vf_coef</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">ent_coef</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.99</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>

<p class="pagebreak-before">That’s a lot of arguments! Let’s see what they do:</p>

<ul>
<li>
<p>The first argument is the policy network. Since we specified <code translate="no">CnnPolicy</code>, SB3 will build a good CNN<a data-type="indexterm" data-primary="convolutional neural networks (CNNs)" data-secondary="Stable-Baselines3" id="id4306"/> for us, based on the chosen algorithm (PPO in this case) and the observation space. If you’re curious, take a look at <code translate="no">ppo_model.policy</code> to see the CNN’s architecture: it’s a deep CNN with an actor head (for the action logits) and a critic head (for the state values). There are four possible actions: left, right, fire (to launch the ball), and no-op (do nothing). If you prefer to use a custom neural net, you must create a subclass of the <code translate="no">ActorCriticPolicy</code> class, located in the <code translate="no">stable_baselines3.common.policies</code> module. See <a href="https://homl.info/sb3">SB3’s documentation</a> for more details.</p>
</li>
<li>
<p><code translate="no">env</code>, <code translate="no">device</code>, <code translate="no">learning_rate</code>, and <code translate="no">batch_size</code> are self-explanatory.</p>
</li>
<li>
<p><code translate="no">n_steps</code> is the number of environment steps to run (per environment) before each policy update.</p>
</li>
<li>
<p><code translate="no">n_epochs</code> is the number of training steps to run on each batch during 
<span class="keep-together">optimization.</span></p>
</li>
<li>
<p><code translate="no">clip_range</code> limits the magnitude of the policy updates to avoid large changes that might cause catastrophic forgetting.</p>
</li>
<li>
<p><code translate="no">vf_coef</code> is the weight of the value function loss in the total loss (similar to our actor-critic’s <code translate="no">critic_weight</code> hyperparameter).</p>
</li>
<li>
<p><code translate="no">ent_coef</code> is the weight of the entropy term that encourages exploration.</p>
</li>
<li>
<p><code translate="no">gamma</code> is the discount rate.</p>
</li>
<li>
<p><code translate="no">verbose</code> is the logging verbosity (0 = silent, 1 = info, 2 = debug).</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>For new tasks, the default <code translate="no">PPO</code> hyperparameters are a good place to start. If learning is too slow, try more parallel environments first; then consider using a higher learning rate or a larger clip range. You can also shrink <code translate="no">n_steps</code> or <code translate="no">batch_size</code>, but this risks noisier gradients. If learning is unstable, try lowering the learning rate or clip range, and use larger rollouts (i.e., <code translate="no">n_steps</code>) or batch sizes. Use <code translate="no">gamma</code> near 0.95 for short-horizon tasks, and 0.995 to 0.999 for long-horizon ones. Lastly, increase <code translate="no">ent_coef</code> if you want to encourage more exploration.</p>
</div>

<p>And now let’s start training. The following code will train the model for 30 million steps. This will take many hours and will probably be too long for a Colab session (unless you get a paid subscription), so the notebook also includes code to download the trained model if you prefer. Whenever you train a model for a long time, it’s important to save checkpoints at regular intervals (e.g., every 100,000 calls to the <code translate="no">step()</code> method) to avoid having to start from scratch in case of a crash or a power outage. For this, we can create a checkpoint callback and pass it to the <code translate="no">learn()</code> method:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">stable_baselines3.common.callbacks</code> <code class="kn">import</code> <code class="n">CheckpointCallback</code>

<code class="n">cb</code> <code class="o">=</code> <code class="n">CheckpointCallback</code><code class="p">(</code><code class="n">save_freq</code><code class="o">=</code><code class="mi">100_000</code><code class="p">,</code> <code class="n">save_path</code><code class="o">=</code><code class="s2">"my_ppo_breakout.ckpt"</code><code class="p">)</code>
<code class="n">ppo_model</code><code class="o">.</code><code class="n">learn</code><code class="p">(</code><code class="n">total_timesteps</code><code class="o">=</code><code class="mi">30_000_000</code><code class="p">,</code> <code class="n">progress_bar</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">callback</code><code class="o">=</code><code class="n">cb</code><code class="p">)</code>
<code class="n">ppo_model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"my_ppo_breakout"</code><code class="p">)</code>  <code class="c1"># save the final model</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code translate="no">save_freq</code> argument counts calls to the <code translate="no">step()</code> method. Since there are 4 environments running in parallel, 50,000 calls correspond to 200,000 total time steps.</p>
</div>

<p>To see the progress during training, one option is to load the latest checkpoint 
<span class="keep-together">in another</span> notebook, and try it out. A simpler option is to use TensorBoard<a data-type="indexterm" data-primary="TensorBoard" id="id4307"/> to visualize the learning curves, especially the mean reward per episode. For this, you must first activate the TensorBoard extension in Colab or Jupyter by running <code translate="no">%load_ext</code> <code translate="no">tensorboard</code> (this is done at the start of this chapter’s notebook). Next, you must start the TensorBoard server, point it to a log directory, and choose the TCP port it will listen on. The following “magic” command (i.e., starting with a %) will do that and also open up the TensorBoard client interface directly inside Colab or Jupyter:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">tensorboard_logdir</code> <code class="o">=</code> <code class="s2">"my_ppo_breakout_tensorboard"</code>  <code class="c1"># path to the log directory</code>
<code class="o">%</code><code class="n">tensorboard</code> <code class="o">--</code><code class="n">logdir</code><code class="o">=</code><code class="p">{</code><code class="n">tensorboard_logdir</code><code class="p">}</code> <code class="o">--</code><code class="n">port</code> <code class="mi">6006</code></pre>

<p>Next, you must tell the PPO model where to save its TensorBoard logs. This is done when creating the model:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">ppo_model</code> <code class="o">=</code> <code class="n">PPO</code><code class="p">(</code><code class="s2">"CnnPolicy"</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">],</code> <code class="n">tensorboard_log</code><code class="o">=</code><code class="n">tensorboard_logdir</code><code class="p">)</code></pre>

<p>And that’s it. Once you start training, you will see the learning curves change every 30 seconds or so in the TensorBoard interface (or click the refresh button). The most important metric to track is the <code translate="no">rollout/ep_rew_mean</code>, which is the mean reward per episode: it should slowly ramp up, even though it will sometimes go down a bit. After 1 million total steps it will typically reach around 20; that’s not a very good agent. But if you let training run for 10 million steps, it should reach human level. And after 50 million steps, it will generally be superhuman.</p>

<p>Congratulations, you know how to train a superhuman AI! You can try it out like this:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="n">ppo_model</code> <code class="o">=</code> <code class="n">PPO</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"my_ppo_agent_breakout"</code><code class="p">)</code>  <code class="c1"># or load the best checkpoint</code>
<code class="n">eval_env</code> <code class="o">=</code> <code class="n">make_atari_env</code><code class="p">(</code><code class="s2">"BreakoutNoFrameskip-v4"</code><code class="p">,</code> <code class="n">n_envs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">eval_stacked</code> <code class="o">=</code> <code class="n">VecFrameStack</code><code class="p">(</code><code class="n">eval_env</code><code class="p">,</code> <code class="n">n_stack</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="n">frames</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">obs</code> <code class="o">=</code> <code class="n">eval_stacked</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
<code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5000</code><code class="p">):</code>  <code class="c1"># some limit in case the agent never loses</code>
    <code class="n">frames</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">eval_stacked</code><code class="o">.</code><code class="n">render</code><code class="p">())</code>
    <code class="n">action</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">ppo_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">obs</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code> <code class="c1"># for reproducibility</code>
    <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">eval_stacked</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">done</code><code class="p">[</code><code class="mi">0</code><code class="p">]:</code>  <code class="c1"># note: there's no `truncated`</code>
        <code class="k">break</code>

<code class="n">eval_stacked</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre>

<p>This will capture all the frames during one episode. You can render them as an animation using Matplotlib (see the notebook for an example). If you trained the agent for long enough (or used the pretrained model), you will see that the agent plays pretty well, and even found the strategy of digging tunnels on the sides and sending the ball through them: that’s one of the best strategies in this game<a data-type="indexterm" data-startref="xi_PPOproximalpolicyoptimization19878495_1" id="id4308"/><a data-type="indexterm" data-startref="xi_proximalpolicyoptimizationPPO19878495_1" id="id4309"/><a data-type="indexterm" data-startref="xi_policyandpolicygradientsPPO19878495_1" id="id4310"/>!<a data-type="indexterm" data-startref="xi_reinforcementlearningRLStableBaselines3198876_1" id="id4311"/><a data-type="indexterm" data-startref="xi_SB3StableBaselines3198876_1" id="id4312"/><a data-type="indexterm" data-startref="xi_StableBaselines3SB3198876_1" id="id4313"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Overview of Some Popular RL Algorithms"><div class="sect1" id="id370">
<h1>Overview of Some Popular RL Algorithms</h1>

<p>Before<a data-type="indexterm" data-primary="reinforcement learning (RL)" data-secondary="popular algorithms" id="xi_reinforcementlearningRLpopularalgorithms1910127_1"/> we close this chapter, let’s take a brief look at a few other popular algorithms:</p>
<dl>
<dt><a href="https://homl.info/alphago"><em>AlphaGo</em></a>⁠<sup><a data-type="noteref" id="id4314-marker" href="ch19.html#id4314">25</a></sup></dt>
<dd>
<p>AlphaGo<a data-type="indexterm" data-primary="AlphaGo" id="xi_AlphaGo1910158_1"/> uses a variant of <em>Monte Carlo tree search</em> (MCTS)<a data-type="indexterm" data-primary="MCTS (Monte Carlo tree search)" id="id4315"/><a data-type="indexterm" data-primary="Monte Carlo Tree Search (MCTS)" id="id4316"/> based on deep neural networks to beat human champions at the game of Go. MCTS was invented in 1949 by Nicholas Metropolis and Stanislaw Ulam. It selects the best move after running many simulations, repeatedly exploring the search tree starting from the current position, and spending more time on the most promising branches. When it reaches a node that it hasn’t visited before, it plays randomly until the game ends, and updates its estimates for each visited node (excluding the random moves), increasing or decreasing each estimate, depending on the final outcome.</p>

<p>AlphaGo is based on the same principle, but it uses a policy network to select moves, rather than playing randomly. This policy net is trained using policy gradients. The original algorithm involved three additional neural networks, and was more complicated, but it was simplified in the <a href="https://homl.info/alphagozero">AlphaGo Zero paper</a>,⁠<sup><a data-type="noteref" id="id4317-marker" href="ch19.html#id4317">26</a></sup> which uses a single neural network to both select moves and evaluate game states. The <a href="https://homl.info/alphazero">AlphaZero paper</a>⁠<sup><a data-type="noteref" id="id4318-marker" href="ch19.html#id4318">27</a></sup> generalized this algorithm, making it 
<span class="keep-together">capable</span> of tackling not only the game of Go, but also chess and shogi (Japanese chess). Lastly, the <a href="https://homl.info/muzero">MuZero paper</a>⁠<sup><a data-type="noteref" id="id4319-marker" href="ch19.html#id4319">28</a></sup> continued to improve upon this algorithm, outperforming the previous iterations even though the agent starts out without even knowing the rules of the game!</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The rules of the game of Go were hardcoded into AlphaGo. In contrast, MuZero gradually learns a model of the environment: given a state <em>s</em> and an action <em>a</em>, it learns to predict the reward <em>r</em> and the probability of reaching state <em>s’</em>. Having a model of the environment (hardcoded or learned) allows these algorithms to plan ahead (in this case using MCTS). For this reason, both of these algorithms belong to the broad class of <em>model-based RL</em> algorithms. In contrast, policy gradients, value-based methods, and actor-critic methods are all <em>model-free RL</em> algorithms: they have a policy model and/or a value model, but not an <em>environment</em> model.<a data-type="indexterm" data-startref="xi_AlphaGo1910158_1" id="id4320"/></p>
</div>
<dl>
<dt><a href="https://homl.info/curiosity"><em>Curiosity-based exploration</em></a>⁠<sup><a data-type="noteref" id="id4321-marker" href="ch19.html#id4321">29</a></sup></dt>
<dd>
<p>A<a data-type="indexterm" data-primary="curiosity-based exploration" id="id4322"/> recurring problem in RL is the sparsity of the rewards, which makes learning very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have proposed an exciting way to tackle this issue: why not ignore the rewards and just make the agent extremely curious to explore the environment? The rewards thus become intrinsic to the agent, rather than coming from the environment. Similarly, stimulating curiosity in a child is more likely to give good results than purely rewarding the child for getting good grades.</p>

<p>How does this work? The agent continuously tries to predict the outcome of its actions, and it seeks situations where the outcome does not match its predictions. In other words, it wants to be surprised. If the outcome is predictable (boring), it goes elsewhere. However, if the outcome is unpredictable but the agent notices that it has no control over it, it also gets bored after a while. With only curiosity, the authors succeeded in training an agent at many video games: even though the agent gets no penalty for losing, it finds it boring to lose because the game starts over, so it learns to avoid it.</p>
</dd>
<dt>Open-ended learning (OEL)</dt>
<dd>
<p>The objective of OEL<a data-type="indexterm" data-primary="OEL (open-ended learning)" id="id4323"/><a data-type="indexterm" data-primary="open-ended learning (OEL)" id="id4324"/> is to train agents capable of endlessly learning new and interesting tasks, typically generated procedurally. We’re not there yet, but there has been some amazing progress over the last few years. For example, a <a href="https://homl.info/poet">2019 paper</a>⁠<sup><a data-type="noteref" id="id4325-marker" href="ch19.html#id4325">30</a></sup> by a team of researchers from Uber AI introduced the <em>POET algorithm</em>, which generates multiple simulated 2D environments with bumps and holes, and trains one agent per environment. The agent’s goal is to walk as fast as possible while avoiding the obstacles.</p>

<p>The algorithm starts out with simple environments, but they gradually get harder over time: this is called <em>curriculum learning</em><a data-type="indexterm" data-primary="curriculum learning" id="id4326"/>. Moreover, although each agent is only trained within one environment, it must regularly compete against other agents, across all environments. In each environment, the winner is copied over and replaces the agent that was there before. This way, knowledge is regularly transferred across environments, and the most adaptable agents are selected.</p>

<p>In the end, the agents are much better walkers than agents trained on a single task, and they can tackle much harder environments. Of course, this principle can be applied to other environments and tasks as well. If you’re interested in OEL, make sure to check out the <a href="https://homl.info/epoet">Enhanced POET paper</a>,<a data-type="indexterm" data-primary="POET algorithm" id="id4327"/>⁠<sup><a data-type="noteref" id="id4328-marker" href="ch19.html#id4328">31</a></sup> as well as DeepMind’s <a href="https://homl.info/oel2021">2021 paper</a>⁠<sup><a data-type="noteref" id="id4329-marker" href="ch19.html#id4329">32</a></sup> on this topic.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>If you’d like to learn more about reinforcement learning, check out the book <a class="orm:hideurl" href="https://homl.info/rlbook"><em>Reinforcement Learning</em></a> by Phil Winder (O’Reilly).</p>
</div>

<p>We covered many topics in this chapter. We learned about policy gradient methods; we implemented the REINFORCE algorithm to solve the CartPole problem using Gymnasium; we explored Markov chains and Markov decision processes, which led us to value-based methods; and we implemented a deep Q-Learning model. Then we discussed actor-critic methods, and we used the Stable-Baselines3 library to implement a PPO model that beat the Atari game <em>Breakout</em>. Lastly, we took a peek at some of the other areas of RL, including model-based RL and more. Reinforcement learning is a huge and exciting field, with new ideas and algorithms popping out every day, so I hope this chapter sparked your curiosity. There is a whole world to explore!<a data-type="indexterm" data-startref="xi_reinforcementlearningRLpopularalgorithms1910127_1" id="id4330"/></p>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Exercises"><div class="sect1" id="id741">
<h1>Exercises</h1>
<ol>
<li>
<p>How would you define reinforcement learning? How is it different from regular supervised or unsupervised learning?</p>
</li>
<li>
<p>Can you think of three possible applications of RL that were not mentioned in this chapter? For each of them, what is the environment? What is the agent? What are some possible actions? What are the rewards?</p>
</li>
<li>
<p>What is the discount factor? Can the optimal policy change if you modify the discount factor?</p>
</li>
<li>
<p>How do you measure the performance of a reinforcement learning agent?</p>
</li>
<li>
<p>What is the credit assignment problem? When does it occur? How can you alleviate it?</p>
</li>
<li>
<p>What is the point of using a replay buffer?</p>
</li>
<li>
<p>What is an off-policy RL algorithm? What are the benefits?</p>
</li>
<li>
<p>What is a model-based RL algorithm? Can you give some examples?</p>
</li>
<li>
<p>Use policy gradients to solve Gymnasium’s LunarLander-v2 environment.</p>
</li>
<li>
<p>Solve the BipedalWalker-v3 environment using the RL algorithm of your choice.</p>
</li>
<li>
<p>If you have about $100 to spare, you can purchase a Raspberry Pi 3 plus some cheap robotics components, install PyTorch on the Pi, and go wild! Start with simple goals, like making the robot turn around to find the brightest angle (if it has a light sensor) or the closest object (if it has a sonar sensor), and move in that direction. Then you can start using deep learning. For example, if the robot has a camera, you can try to implement an object detection algorithm so it detects people and moves toward them. You can also try to use RL to make the agent learn on its own how to use the motors to achieve that goal. Have fun!</p>
</li>

</ol>

<p>Solutions to these exercises are available at the end of this chapter’s notebook, at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Thank You!"><div class="sect1" id="id371">
<h1>Thank You!</h1>

<p>Before we close the last chapter of this book, I would like to thank you for reading it up to the last paragraph. I truly hope that you had as much pleasure reading this book as I had writing it, and that it will be useful for your projects, big or small.</p>

<p>If you find errors, please send feedback. More generally, I would love to know what you think, so please don’t hesitate to contact me via O’Reilly, or through the <em>ageron/handson-mlp</em> GitHub project.</p>

<p>Going forward, my best advice to you is to practice and practice: try going through all the exercises (if you have not done so already), play with the Jupyter notebooks, join Kaggle.com or some other ML community, watch ML courses, read papers, attend conferences, and meet experts. It also helps tremendously to have a concrete project to work on, whether it is for work or fun (ideally for both), so if there’s anything you have always dreamt of building, give it a shot! Work incrementally; don’t shoot for the moon right away, but stay focused on your project and build it piece by piece. It will require patience and perseverance, but when you have a walking robot, or a working chatbot, or whatever else you fancy to build, it will be immensely rewarding.<a data-type="indexterm" data-startref="xi_reinforcementlearningRL19530_1" id="id4331"/></p>

<p>My greatest hope is that this book will inspire you to build a wonderful ML application that will benefit all of us! What will it be?</p>
<p class="byline">—<em>Aurélien Geron</em></p>
<p class="byline cont"><em>October 22, 2025</em></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id4170"><sup><a href="ch19.html#id4170-marker">1</a></sup> For more details, be sure to check out Richard Sutton and Andrew Barto’s book on RL, <em>Reinforcement Learning: An Introduction</em> (MIT Press).</p><p data-type="footnote" id="id4171"><sup><a href="ch19.html#id4171-marker">2</a></sup> DeepMind was bought by Google for over $500 million in 2014.</p><p data-type="footnote" id="id4172"><sup><a href="ch19.html#id4172-marker">3</a></sup> Volodymyr Mnih et al., “Playing Atari with Deep Reinforcement Learning”, arXiv preprint arXiv:1312.5602 (2013), <a href="https://homl.info/dqn" class="bare"><em class="hyperlink">https://homl.info/dqn</em></a>.</p><p data-type="footnote" id="id4173"><sup><a href="ch19.html#id4173-marker">4</a></sup> Volodymyr Mnih et al., “Human-Level Control Through Deep Reinforcement Learning”, <em>Nature</em> 518 (2015): 529–533, <a href="https://homl.info/dqn2" class="bare"><em class="hyperlink">https://homl.info/dqn2</em></a>.</p><p data-type="footnote" id="id4174"><sup><a href="ch19.html#id4174-marker">5</a></sup> Check out the videos of DeepMind’s system learning to play <em>Space Invaders</em>, <em>Breakout</em>, and other video games at <a href="https://homl.info/dqn3" class="bare"><em class="hyperlink">https://homl.info/dqn3</em></a>.</p><p data-type="footnote" id="id4189"><sup><a href="ch19.html#id4189-marker">6</a></sup> Images (a), (d), and (e) are in the public domain. Image (b) is a screenshot from the <em>Ms. Pac-Man</em> game, copyright Atari (fair use in this chapter). Image (c) is reproduced from Wikipedia; it was created by user Stevertigo and released under <a href="https://oreil.ly/O2fAq">Creative Commons BY-SA 2.0</a>.</p><p data-type="footnote" id="id4198"><sup><a href="ch19.html#id4198-marker">7</a></sup> It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the “gene pool”.</p><p data-type="footnote" id="id4199"><sup><a href="ch19.html#id4199-marker">8</a></sup> If there is a single parent, this is called <em>asexual reproduction</em>. With two (or more) parents, it is called <em>sexual reproduction</em>. An offspring’s genome (in this case a set of policy parameters) is randomly composed of parts of its parents’ genomes.</p><p data-type="footnote" id="id4200"><sup><a href="ch19.html#id4200-marker">9</a></sup> One interesting example of a genetic algorithm used for reinforcement learning is the <a href="https://homl.info/neat"><em>NeuroEvolution of Augmenting Topologies</em></a> (NEAT) algorithm. Also check out <a href="https://homl.info/epo"><em>evolutionary policy optimization</em></a> (EPO), proposed in 2025, where a master agent learns stably and efficiently from the experiences of a population of agents.</p><p data-type="footnote" id="id4201"><sup><a href="ch19.html#id4201-marker">10</a></sup> This is called <em>gradient ascent</em>.<a data-type="indexterm" data-primary="gradient ascent" id="id4332"/><a data-type="indexterm" data-primary="PG algorithm" data-see="policy and policy gradients" id="id4333"/> It’s just like gradient descent, but in the opposite direction: maximizing instead of minimizing.</p><p data-type="footnote" id="id4223"><sup><a href="ch19.html#id4223-marker">11</a></sup> Ronald J. Williams, “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Leaning”, <em>Machine Learning</em> 8 (1992): 229–256.</p><p data-type="footnote" id="id4225"><sup><a href="ch19.html#id4225-marker">12</a></sup> We generate a new random seed for each episode using <code translate="no">torch.randint()</code><a data-type="indexterm" data-primary="torch" data-secondary="randint()" id="id4334"/>. This ensures that each episode is different, yet the whole training process is reproducible if we set PyTorch’s random seed before calling <code translate="no">train_reinforce()</code>.</p><p data-type="footnote" id="id4234"><sup><a href="ch19.html#id4234-marker">13</a></sup> A great <a href="https://homl.info/rlhard">2018 post</a> by Alex Irpan nicely lays out RL’s biggest difficulties and <span class="keep-together">limitations</span>.</p><p data-type="footnote" id="id4239"><sup><a href="ch19.html#id4239-marker">14</a></sup> Richard Bellman, “A Markovian Decision Process”, <em>Journal of Mathematics and Mechanics</em> 6, no. 5 (1957): 679–684.</p><p data-type="footnote" id="id4266"><sup><a href="ch19.html#id4266-marker">15</a></sup> Hado van Hasselt et al., “Deep Reinforcement Learning with Double Q-Learning”, <em>Proceedings of the 30th AAAI Conference on Artificial Intelligence</em> (2015): 2094–2100.</p><p data-type="footnote" id="id4270"><sup><a href="ch19.html#id4270-marker">16</a></sup> Tom Schaul et al., “Prioritized Experience Replay”, arXiv preprint arXiv:1511.05952 (2015).</p><p data-type="footnote" id="id4272"><sup><a href="ch19.html#id4272-marker">17</a></sup> It could also just be that the rewards are noisy, in which case there are better methods for estimating an experience’s importance (see “Prioritized Experience Replay” for some examples).</p><p data-type="footnote" id="id4281"><sup><a href="ch19.html#id4281-marker">18</a></sup> Ziyu Wang et al., “Dueling Network Architectures for Deep Reinforcement Learning”, arXiv preprint arXiv:1511.06581 (2015).</p><p data-type="footnote" id="id4283"><sup><a href="ch19.html#id4283-marker">19</a></sup> Matteo Hessel et al., “Rainbow: Combining Improvements in Deep Reinforcement Learning”, arXiv preprint arXiv:1710.02298 (2017): 3215–3222.</p><p data-type="footnote" id="id4290"><sup><a href="ch19.html#id4290-marker">20</a></sup> Volodymyr Mnih et al., “Asynchronous Methods for Deep Reinforcement Learning”, <em>Proceedings of the 33rd International Conference on Machine Learning</em> (2016): 1928–1937.</p><p data-type="footnote" id="id4296"><sup><a href="ch19.html#id4296-marker">21</a></sup> Tuomas Haarnoja et al., “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor”, <em>Proceedings of the 35th International Conference on Machine Learning</em> (2018): 1856–1865.</p><p data-type="footnote" id="id4299"><sup><a href="ch19.html#id4299-marker">22</a></sup> John Schulman et al., “Proximal Policy Optimization Algorithms”, arXiv preprint arXiv:1707.06347 (2017).</p><p data-type="footnote" id="id4300"><sup><a href="ch19.html#id4300-marker">23</a></sup> John Schulman et al., “Trust Region Policy Optimization”, <em>Proceedings of the 32nd International Conference on Machine Learning</em> (2015): 1889–1897.</p><p data-type="footnote" id="id4305"><sup><a href="ch19.html#id4305-marker">24</a></sup> The “v4” suffix is the version number; it’s unrelated to frame skipping or the number of parallel environments.</p><p data-type="footnote" id="id4314"><sup><a href="ch19.html#id4314-marker">25</a></sup> David Silver et al., “Mastering the Game of Go with Deep Neural Networks and Tree Search”, <em>Nature</em> 529 (2016): 484–489.</p><p data-type="footnote" id="id4317"><sup><a href="ch19.html#id4317-marker">26</a></sup> David Silver et al., “Mastering the Game of Go Without Human Knowledge”, <em>Nature</em> 550 (2017): 354–359.</p><p data-type="footnote" id="id4318"><sup><a href="ch19.html#id4318-marker">27</a></sup> David Silver et al., “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm”, arXiv preprint arXiv:1712.01815.</p><p data-type="footnote" id="id4319"><sup><a href="ch19.html#id4319-marker">28</a></sup> Julian Schrittwieser et al., “Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model”, arXiv preprint arXiv:1911.08265 (2019).</p><p data-type="footnote" id="id4321"><sup><a href="ch19.html#id4321-marker">29</a></sup> Deepak Pathak et al., “Curiosity-Driven Exploration by Self-Supervised Prediction”, <em>Proceedings of the 34th International Conference on Machine Learning</em> (2017): 2778–2787.</p><p data-type="footnote" id="id4325"><sup><a href="ch19.html#id4325-marker">30</a></sup> Rui Wang et al., “Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions”, arXiv preprint arXiv:1901.01753 (2019).</p><p data-type="footnote" id="id4328"><sup><a href="ch19.html#id4328-marker">31</a></sup> Rui Wang et al., “Enhanced POET: Open-Ended Reinforcement Learning Through Unbounded Invention of Learning Challenges and Their Solutions”, arXiv preprint arXiv:2003.08536 (2020).</p><p data-type="footnote" id="id4329"><sup><a href="ch19.html#id4329-marker">32</a></sup> Open-Ended Learning Team et al., “Open-Ended Learning Leads to Generally Capable Agents”, arXiv preprint arXiv:2107.12808 (2021).</p></div></div></section></div></div></body></html>