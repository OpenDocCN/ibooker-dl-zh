<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__intro"> <span class="chapter-title-numbering"><span class="num-string">1</span></span> <span class="title-text"> Analyzing data with large language models</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">An introduction to language models</li> 
    <li id="p3">Data analysis with language models</li> 
    <li id="p4">Using language models efficiently</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>Language models are powerful neural networks that can be used for various data-processing tasks. This chapter introduces language models and shows how and why to use them for data analysis.</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <h2 class=" readable-text-h2" id="what-can-language-models-do"><span class="num-string browsable-reference-id">1.1</span> What can language models do?</h2> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>We will start this section with a little poem and an associated picture (figure <a href="#fig__ai4data">1.1</a>) connecting the two main topics of this book, data analysis and large language models:</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p><em>In the silent hum of the server’s light,<br/> Data flows through the veins of night.<br/> Rows and columns, a structured sea,<br/> With stories hidden, waiting to be free.</em></p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p><em>Each number sings of pasts untold,<br/> Trends and truths in patterns bold.<br/> And here arrives a curious friend,<br/> A language model, eager to comprehend.</em></p> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p><em>It listens close, with circuits keen,<br/> To turn raw facts into insight unseen.<br/> From scatter plots to sentences clear,<br/> Data’s language is all it can hear.</em></p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p><em>The figures dance, the texts reply,<br/> As code meets meaning under AI’s eye.<br/> They merge their worlds, a seamless blend,<br/> Where logic and language have no end.</em></p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p><em>For in this bond, both deep and wide,<br/> Data’s essence finds a guide.<br/> And in the neural net’s embrace,<br/> Data analysis gains a poetic grace.</em></p> 
  </div> 
  <div class="browsable-container figure-container" id="p13">  
   <img alt="figure" src="../Images/CH01_F01_Trummer.png" width="900" height="900"/> 
   <h5 class=" figure-container-h5" id="fig__ai4data"><span class="num-string">Figure <span class="browsable-reference-id">1.1</span></span> Illustration by GPT-4o, connecting the topics “data analysis” and “large language models”</h5>
  </div> 
  <div class="readable-text" id="p14"> 
   <p>The poem and the picture were generated by GPT-4o (“o” for “omni”), a language model by OpenAI that processes multimodal data, based solely on the instructions “Write a poem connecting data analysis and large language models!” followed by “Now draw a corresponding picture!” Both the picture and the poem seem to relate to the requested topics. Although the poem may not win any literature awards, its text is coherent, it is structured as we would expect from a poem, and it rhymes! Perhaps most importantly, all it took to generate the poem and the picture were short instructions expressed in natural language. Whereas prior machine learning methods relied on large amounts of task-specific training data, this requirement is now obsolete. And, of course, the task is specific enough to convince us that the language model is not copying existing solutions from the web and generates original content instead.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>Writing poems and generating pictures are only two of many possible use cases (albeit possibly the most entertaining ones). Models like GPT-4o can solve various tasks, such as summarizing text documents, writing program code, and answering questions about pictures. In this book, you will learn how to use language models to accomplish a plethora of data-analysis tasks ranging from extracting information from large collections of text documents to writing code for data analysis. After reading this book, you will be able to quickly build data-analysis pipelines that are based on language models and extract useful insights from a variety of data formats.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p16"> 
    <h5 class=" callout-container-h5 readable-text-h5">What does GPT stand for?</h5> 
   </div> 
   <div class="readable-text" id="p17"> 
    <p> <em>GPT</em> stands for <em>Generative Pretrained Transformer</em>.</p> 
   </div> 
   <div class="readable-text" id="p18"> 
    <p><em>Generative</em>: GPT is a large neural network that generates content (e.g., text or code) in response to input text. This fact distinguishes it from other neural networks that, for example, can only classify input text into a fixed set of predefined categories.</p> 
   </div> 
   <div class="readable-text" id="p19"> 
    <p><em>Pretrained</em>: GPT is pretrained on large amounts of data, solving generic tasks such as predicting the next word in text. Typically, the pretraining task is different from the tasks it is primarily used for. However, pretraining helps it learn more specialized tasks faster.</p> 
   </div> 
   <div class="readable-text" id="p20"> 
    <p>The <em>Transformer</em> is a new neural network architecture that is particularly useful for learning tasks that involve variable-length input or output (such as text documents). It is currently the dominant architecture for generative AI approaches.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p21"> 
   <h2 class=" readable-text-h2" id="sec__bookscope"><span class="num-string browsable-reference-id">1.2</span> What you will learn</h2> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>This book is about using language models for data analysis. We can categorize data-analysis tasks by the type of data we’re analyzing and by the type of analysis. This book covers a wide range of data types and analysis tasks.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>We focus on <em>multimodal</em> data analysis: that is, we use language models to analyze various types of data. More precisely, we cover the following data types in this book:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p24"><em>Text</em>—Think of emails, newspaper articles, and comments on a web forum. Text data is ubiquitous and contains valuable information. In this book, we will see how to use language models to automatically classify text documents based on their content, how to extract specific pieces of information from text, and how to group text documents about related topics.</li> 
   <li class="readable-text" id="p25"><em>Images</em>—A picture is worth a thousand words, as they say. Images help us to understand complex concepts, capture fond memories of our last holiday, and illustrate current events. Language models can easily extract information from pictures. For instance, we will use language models to answer arbitrary questions about images or identify people who appear in pictures based on a database of profiles.</li> 
   <li class="readable-text" id="p26"><em>Videos</em>—A large percentage of the data on the web is video data. Even on your smartphone, video data is probably taking up a significant part of your phone’s total storage capacity. In this book, we will see that language models can be applied to analyze videos as well: for instance, to generate suitable video titles based on the video content.</li> 
   <li class="readable-text" id="p27"><em>Audio</em>—To many people, speech is the most natural form of communication. Audio recordings capture speeches and conversations and complement videos. In this book, we will see how to transcribe audio recordings, how to translate spoken language into other languages, and how to build a query interface that answers spoken questions about data.</li> 
   <li class="readable-text" id="p28"><em>Tables</em>—Imagine a data set containing information about customers. It is natural to represent that data as a table, featuring columns for the customer’s address, phone number, and credit card information, while different rows store information about different customers. In this book, we will see how to use language models to write code that performs complex operations on such tabular data.</li> 
   <li class="readable-text" id="p29"><em>Graphs</em>—From social networks to metro networks, many data sets are conveniently represented as graphs, modeling entities (such as people or metro stations) and their connections (representing friendships or metro connections). We will see how we can use language models to generate code that analyzes large graphs in various ways.</li> 
  </ul> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p30"> 
    <h5 class=" callout-container-h5 readable-text-h5">Structured vs. unstructured data</h5> 
   </div> 
   <div class="readable-text" id="p31"> 
    <p> Data types are often categorized into two groups: <em>structured</em> and <em>unstructured data</em>. Structured data has a structure that facilitates efficient data processing via specialized tools. Examples of structured data include tables and graph data. For such data, we typically use the language model as an interface to specialized data-processing tools. Unstructured data, including text, images, videos, and audio files, does not have a structure that can be easily exploited for efficient processing. So, for unstructured data, we typically need to use the language model directly on the data.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>For most of this book, we will use OpenAI models via OpenAI’s Python library. Toward the end of the book, we will also discuss language models from other providers. As libraries from different providers tend to offer similar functionality, getting used to other models shouldn’t take long.</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>Typically, using language models incurs monetary fees proportional to the amount of data being processed. The fees depend on the language model used, the model configuration, and the way in which the input to the language model is formulated. In this book, not only will you learn to solve various data-analysis tasks via language models, but we will discuss how to do so with minimal costs.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <h2 class=" readable-text-h2" id="how-to-use-language-models"><span class="num-string browsable-reference-id">1.3</span> How to use language models</h2> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>State-of-the-art language models are used via a method called <em>prompting</em>. We discuss prompting next, followed by the interfaces we can use for prompting.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <h3 class=" readable-text-h3" id="prompting"><span class="num-string browsable-reference-id">1.3.1</span> Prompting</h3> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>Until a few years ago, machine learning models were trained for one specific task. For instance, we might have a model trained to classify the text of a review as either “positive” (i.e., the review author is satisfied) or “negative” (i.e., the author is dissatisfied). To use that model, we only need the review text as input. There’s no need to describe the task (classifying the review) as part of the input because the model has been specialized to do that task and that task only.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>This has changed in recent years with the emergence of large language models such as GPT. Such models are no longer trained for specific tasks. Instead, they are intended to serve as universal task solvers that can, in principle, solve any task the user desires. When using such a model, it is up to the user to describe to the model in precise terms what the model should do.</p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>The prompt is the input to the language model. The prompt can contain multimodal data: for example, text and images. At a minimum, to get the language model to solve a specific task, the prompt should contain a text instructing the model on what to do. Beyond those instructions, the prompt should contain all relevant context. For instance, if the instructions ask the model to determine whether a car is visible in a picture, the prompt must also contain the picture. The instructions in the prompt should be specific and clarify, for instance, the expected output format. For example, if we want the model to output “1” if a car is present and “0” otherwise, enabling us to easily add the numbers generated by the model to count cars, we need to explicitly clarify that in the prompt (otherwise, the model might answer “Yes, there is a car in the picture,” which makes it harder to count in the post-processing stage). Besides instructions and context, the prompt may contain examples to help the language model understand the task.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p40"> 
    <h5 class=" callout-container-h5 readable-text-h5">Few-shot vs. zero-shot learning</h5> 
   </div> 
   <div class="readable-text" id="p41"> 
    <p> We can help the language model better understand a task by providing examples as part of the prompt. Those examples are similar to the task we want the model to solve and specify the input and desired output. This approach is sometimes called <em>few-shot learning</em>, as the model learns the task based on a few samples. On the other hand, we can use <em>zero-shot learning</em>, meaning the model learns the task without any (zero) samples based only on the task description.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p42"> 
   <h3 class=" readable-text-h3" id="sub__ExamplePrompt"><span class="num-string browsable-reference-id">1.3.2</span> Example prompt</h3> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>Let’s illustrate prompts with an example. A classical use case for language models is analyzing product reviews to determine the sentiment underlying the review: whether the review is positive (i.e., the customer recommends the product) or negative (i.e., the customer is unhappy with the product). Assume that we have a review to classify as positive or negative. If we have a specialized model trained for review classification for the specific product category we’re interested in, all it takes is to send our review to that model. As the model is specialized to the target problem, it already “knows” what to do with the input and the required output format. However, because we use large language models, we have to provide a bit more context along with the review.</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>Our prompt should contain all relevant information for the model, describing the task to solve and all context. In the example scenario, we probably want to include the following pieces of information:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p45"><em>Review text</em>—The text of the review we want to classify.</li> 
   <li class="readable-text" id="p46"><em>Task description</em>—A description of the task to solve.</li> 
   <li class="readable-text" id="p47"><em>Output formats</em>—What is the required output format?</li> 
   <li class="readable-text" id="p48"><em>Relevant context</em>—For example, are we reviewing laptops or lawn mowers?</li> 
  </ul> 
  <div class="readable-text" id="p49"> 
   <p>Optionally, we can include a few example reviews with their associated correct classification. This may help the model classify reviews more accurately.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>The following prompt includes all the relevant pieces of information for an example review.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p51"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__ReviewPrompt"><span class="num-string">Listing <span class="browsable-reference-id">1.1</span></span> Prompt for classifying a laptop review</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">We are considering product reviews for laptops.  #1
For each review, output "satisfied" or "dissatisfied", 
depending on whether the customer is satisfied 
with the product or not.  #2
Examples:
This is a great laptop! I recommend everyone to buy it! 
satisfied  #3
This laptop did not work. I had to return it.
dissatisfied  #4
The screen is too small and it takes too long to start.  #5</pre> 
    <div class="code-annotations-overlay-container">
     #1 Context
     <br/>#2 Task description and output format
     <br/>#3 First example
     <br/>#4 Second example
     <br/>#5 Review
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>This prompt starts with a description of relevant context (<strong class="cueball">1</strong>). Customers are reviewing laptops, so, for example, if they label items as “heavy,” that’s probably a bad sign (unlike analyzing reviews for, let’s say, steamrollers). The task description (<strong class="cueball">2</strong>) tells the model what to do with the reviews and specifies the desired output format (output “satisfied” or “dissatisfied”) as well. Next, we have a list of examples. Strictly speaking, adding examples in the prompt may not be necessary for this simple task. However, adding examples in the prompt can sometimes increase the accuracy of the output. Here, we add two example reviews (<strong class="cueball">3</strong> and <strong class="cueball">4</strong>), together with the desired output for those reviews. Finally, we add the review (<strong class="cueball">5</strong>) that we want the model to classify. Given the preceding prompt, state-of-the-art language models are likely to output “dissatisfied” when sent this prompt as input. That, of course, is indeed the desired output.</p> 
  </div> 
  <div class="readable-text" id="p53"> 
   <h3 class=" readable-text-h3" id="interfaces"><span class="num-string browsable-reference-id">1.3.3</span> Interfaces</h3> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>So how can we send prompts to a language model? Providers such as OpenAI typically offer web interfaces, enabling users to send single prompts to their language models. In chapter 2, we will use OpenAI’s web interface to send prompts instructing the model to analyze text or to write code for data processing.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>The web interface works well as long as we send only a few prompts. However, analyzing a large collection of text documents would require sending many prompts (one per text document). Clearly, we don’t want to enter thousands of prompts by hand. This is where OpenAI’s Python library comes in handy. Using this library enables us to send prompts to OpenAI’s models directly from Python and to process the model’s answer in Python. This enables us to automate data loading, prompt generation, and any kind of post-processing we need to do on the model’s answers. It also allows us to integrate language models with other useful tools: for example, to use the language model to write code for data processing and immediately execute that code using other tools.</p> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>We will review OpenAI’s Python library in chapter 3. We will use this library throughout most of this book. Other providers of language models, including Google, Anthropic, and Cohere, offer similar Python libraries to send prompts to their language models. We will discuss those libraries in more detail in chapter 8.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h2 class=" readable-text-h2" id="using-language-models-for-data-analysis"><span class="num-string browsable-reference-id">1.4</span> Using language models for data analysis</h2> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>So how do we use language models specifically for data analysis? This book considers two possibilities. First, we can use the language model <em>directly</em> on the data. This means the language model receives the data we want to analyze as part of the prompt (along with instructions on which analysis to perform). Second, we can use the language model <em>indirectly</em> to analyze data. Here, the language model does not directly “see” the data: that is, we do not include the data in its entirety in the prompt. Instead, we use the language model to write code for data processing, executed in specialized data-processing tools. Which approach to use depends on the data properties and the task. Let’s have a closer look at both methods.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <h3 class=" readable-text-h3" id="using-language-models-directly-on-data"><span class="num-string browsable-reference-id">1.4.1</span> Using language models directly on data</h3> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>The most natural approach to analyzing data with language models is to put the data directly into the prompt. This is what we did in section <a href="#sub__ExamplePrompt">1.3.2</a>: to analyze a review, we include the review text in the prompt, along with instructions on what to do with the text. We can use the same approach for other types of data besides text. For example, when using multimodal models such as GPT-4o, we can simply include the pictures to analyze, together with analysis instructions, in the prompt.</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>Typically, we do not want to analyze a single picture or review but a whole collection of them. For instance, assume that we want to classify an entire collection of reviews, determining for each of them whether the review is positive or negative. In such cases, we generally take the following approach, implemented in Python using OpenAI’s Python library (or an equivalent library allowing users to send prompts to other providers’ models). We load the reviews to classify and generate one prompt for each review. Then, we send those prompts to the language model, extract the classification result from the answer generated by the model for each review, and save the results in a file on disk.</p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>In this scenario, we want to solve the same task (review classification) for multiple text documents (i.e., reviews). As you can imagine, the prompts for different reviews should therefore bear some similarity. Although the text of the review to classify changes each time, the task description and other parts of the prompt remain the same.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>To generate prompts in Python, we use a <em>prompt template</em>. A prompt template specifies a prompt associated with a specific task to solve. In our example, we would use a prompt template to classify reviews as positive or negative. A prompt template contains placeholders to represent parts of the prompt that change depending on the input data. Considering our prompt template for review classification, we should probably include a placeholder for the review text. Then, when generating prompts in Python, we replace that placeholder with the text of the current review to classify.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>For instance, we can use the following prompt template to classify reviews.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p65"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="lb_listing_headingprompt-template-for-classifying-laptop-reviews"><span class="num-string">Listing <span class="browsable-reference-id">1.2</span></span> Prompt template for classifying laptop reviews<span id="code__ReviewTemplate"/></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">We are considering product reviews for laptops.  #1
For each review, output "satisfied" or "dissatisfied", 
depending on whether the customer is satisfied 
with the product or not.  #2
Examples:
This is a great laptop! I recommend everyone to buy it! 
satisfied  #3
This laptop did not work. I had to return it.
dissatisfied  #4
[ReviewText]  #5</pre> 
    <div class="code-annotations-overlay-container">
     #1 Context
     <br/>#2 Task description and output format
     <br/>#3 First example
     <br/>#4 Second example
     <br/>#5 Placeholder for review text
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>This prompt template generalizes the prompt we saw for classifying one specific review (have a look at listing <a href="#code__ReviewPrompt">1.1</a> in section <a href="#sub__ExamplePrompt">1.3.2</a>). Again, we provide context (the fact that we’re classifying laptop reviews) (<strong class="cueball">1</strong>) and instructions describing the task to solve, as well as the output format (<strong class="cueball">2</strong>). We also provide a few example reviews with associated classification results (<strong class="cueball">3</strong> and <strong class="cueball">4</strong>). Although the review to classify changes, depending on the input, we do not need to change the example reviews. Those reviews merely illustrate what task the language model needs to solve. Finally (<strong class="cueball">5</strong>), we have a placeholder for the review text. When iterating over different reviews, we generate a prompt for each of them by substituting the review text for this placeholder.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>The example prompt template has only a single placeholder. In general, several parts of the prompt may change depending on the input data. If so, we introduce placeholders for each of those parts and substitute all of them to generate prompts.</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>Figure <a href="#fig__directDataAnalysis">1.2</a> summarizes how we use prompt templates when analyzing data directly with language models. For each data item (e.g., a review to classify), we substitute for placeholders in the prompt template to generate a prompt (we can also say that we <em>instantiate</em> a prompt). We then send this prompt to the language model to solve the data-analysis task we’re interested in.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p69">  
   <img alt="figure" src="../Images/CH01_F02_Trummer.png" width="761" height="452"/> 
   <h5 class=" figure-container-h5" id="fig__directDataAnalysis"><span class="num-string">Figure <span class="browsable-reference-id">1.2</span></span> Using language models directly for data analysis. A prompt template describes the analysis task. It contains placeholders that are replaced with data to analyze. After substituting for the placeholders, the resulting prompt is submitted to the language model to produce output.</h5>
  </div> 
  <div class="readable-text" id="p70"> 
   <h3 class=" readable-text-h3" id="sub__indirectAnalysis"><span class="num-string browsable-reference-id">1.4.2</span> Data analysis via external tools</h3> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Putting data directly into the prompt is not always the most efficient approach. For some types of data, specialized tools are available that process certain operations on that data very efficiently. In those cases, it is often more efficient to use the language model to write code for data processing (rather than analyzing the data directly). The code generated by the language model can then be executed by the specialized tool.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>We will apply this approach to structured data. For structured data such as data tables and graphs, specialized data-processing tools are available that support a wide range of analysis operations. Those operations, such as filtering and aggregating data, can be performed very efficiently on structured data. Even if it was possible to perform the same operations reliably with language models (which is not the case), we would not want to do it because the fees we pay to providers like OpenAI are proportional to the size of the input data. Processing large structured data sets (such as tables with millions of rows) using language models is prohibitively expensive. In the following chapters, we discuss the following types of tools for structured data processing:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p73"><em>Relational database management system</em>—Stores and processes relational data: that is, collections of data tables. Most relational database management systems support <em>SQL</em>, the Structured Query Language. We will use language models to translate questions about data to queries in SQL.</li> 
   <li class="readable-text" id="p74"><em>Graph data management system</em>—Handles graph data representing entities and the relationships between them. Different graph data management systems support different query languages. In chapter 5, we see how to use language models to translate questions about data into queries in the <em>Cypher</em> language, supported by the Neo4j graph data management system.</li> 
  </ul> 
  <div class="readable-text" id="p75"> 
   <p>For instance, let’s assume we want to enable lay users ompt template for translating questto analyze a relational database: that is, a collection of data tables. Perhaps a table contains the results of a survey, and we want to let users aggregate answers from different groups of respondents. The survey results are stored in a relational database management system (the most suitable type of tool for this data type). Using language models, we can enable users to ask questions about the data in natural language (that is, in plain English). The language model takes care of translating those questions into formal queries. More precisely, given that the data is stored in a relational database management system, we want to translate those questions into SQL queries.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>Again, we introduce a prompt template for the task we’re interested in. Here, we’re interested in text-to-SQL translation, meaning we want to use the language model to translate questions in natural language to SQL queries. Although the task (text-to-SQL translation) and the data (the database containing survey results) remain fixed, the user’s questions will change over time. Therefore, we introduce a placeholder for the user question in our prompt template. In principle, the following prompt template should enable us to translate questions on our survey data into SQL queries.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p77"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="lb_listing_headingprompt-template-for-translating-questions-to-sql"><span class="num-string">Listing <span class="browsable-reference-id">1.3</span></span> Prompt template for translating questions to SQL<span id="code__SQLtemplate"/></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">Database:  #1
The database contains the results of a survey, stored
in a table called "SurveyResults" with the following
columns: ...
Question:  #2
[Question]
Translate the question to SQL!  #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Description of database
     <br/>#2 Question to translate
     <br/>#3 Task description
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>First the prompt describes the structure of our data (<strong class="cueball">1</strong>). This is required to enable the system to write correct queries (e.g., queries that refer to the correct names of tables and columns in those tables). The description in the example template is abbreviated. We will see how to accurately describe the structure of a relational database in later chapters. Next, the prompt template contains the question to translate (<strong class="cueball">2</strong>). This is a placeholder to enable users to ask different questions using the same prompt template. Finally, the prompt template contains a (concise) task description (<strong class="cueball">3</strong>): we want to translate questions to SQL queries!</p> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>Figure <a href="#fig__textToSQL">1.3</a> summarizes the process for text-to-SQL translation. Given a corresponding prompt template, we substitute the user question for the placeholder, translate the question to an SQL query via the language model, and finally execute the query in a relational database management system. The query result is shown to the user.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/CH01_F03_Trummer.png" width="820" height="590"/> 
   <h5 class=" figure-container-h5" id="fig__textToSQL"><span class="num-string">Figure <span class="browsable-reference-id">1.3</span></span> Using language models indirectly to build a natural language interface for tabular data. The prompt template contains placeholders for questions about data. After substituting for placeholders, the resulting prompt is used as input for the language model. The model translates the question into an SQL query that is executed via a relational database management system.</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <h2 class=" readable-text-h2" id="minimizing-costs"><span class="num-string browsable-reference-id">1.5</span> Minimizing costs</h2> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>When processing data with language models, we typically pay fees to a model provider. The larger the amount of data we process, the higher the fees. Before analyzing large amounts of data, we want to make sure we’re not overpaying. For instance, using larger language models (the neural network implementing the language model has more “neurons,” so to speak) is often more expensive, but for complex tasks, it may pay off with higher-quality results. But if the large model is not needed to solve our current task well, we should save the money and use a smaller model. Fortunately, there are quite a few ways in which we can optimize the tradeoff between processing costs and result quality. We discuss the different options next. All of them are covered in more detail in later book chapters.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <h3 class=" readable-text-h3" id="picking-the-best-model"><span class="num-string browsable-reference-id">1.5.1</span> Picking the best model</h3> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>OpenAI offers many different versions of the GPT model, ranging from relatively small models to giant models like GPT-4. At the time of writing, using GPT-4 is over 100 times more expensive, per input token, than using the cheapest version.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p85"> 
    <h5 class=" callout-container-h5 readable-text-h5">What are tokens?</h5> 
   </div> 
   <div class="readable-text" id="p86"> 
    <p> The processing fees for language models like GPT-4 are proportional to the number of tokens read and generated by the model. A <em>token</em> is the atomic unit at which the language model represents text internally. Typically, one token corresponds to approximately four characters.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>Given those price differences, it is clearly a good idea to think hard about which specific model satisfies our needs. For instance, for a simple task like review classification, we probably don’t need to use OpenAI’s most expensive model. But if we want to use the model to write complex code for data processing, using the most expensive version may be worth it.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>Of course, we don’t need to restrict ourselves to models offered by OpenAI. Language models are offered by many providers, including Google, Anthropic, and Cohere. In principle, we might even choose to host our own model, using models that are publicly available: for example, on the Hugging Face platform. Some of those models are generic (similar to OpenAI’s GPT models), whereas others are trained for more specific tasks. If we happen to be interested in tasks for which specialized models exist, we may want to use one of them. We discuss models from other providers in more detail in chapter 8.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Picking the right model for your needs is not an easy task. As a first step, you might want to look at benchmarks such as Stanford’s Holistic Evaluation of Language Models (HELM, <a href="https://crfm.stanford.edu/helm/">https://crfm.stanford.edu/helm/</a>; see figure <a href="#fig__helm">1.4</a>). This benchmark compares the quality of results produced by different language models on different types of tasks. Ultimately, you may have to try a few models on your task and a data sample to ensure that you choose the optimal one. In chapter 9, we will see how to benchmark different models systematically for an example task.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p90">  
   <img alt="figure" src="../Images/CH01_F04_Trummer.png" width="1100" height="588"/> 
   <h5 class=" figure-container-h5" id="fig__helm"><span class="num-string">Figure <span class="browsable-reference-id">1.4</span></span> Holistic Evaluation of Language Models (HELM): comparing language models offered by different providers according to various metrics</h5>
  </div> 
  <div class="readable-text" id="p91"> 
   <h3 class=" readable-text-h3" id="optimally-configuring-models"><span class="num-string browsable-reference-id">1.5.2</span> Optimally configuring models</h3> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>The OpenAI Python library offers a variety of tuning parameters to influence model behavior. For instance, we can influence the probability that certain words appear in the output of a model. This can be useful, for instance, when classifying reviews. If the output of the model should be one of only a few possible choices (such as “positive” and “negative”), it makes sense to restrict possible outputs to those choices. That way, we avoid cases in which the model generates output that does not correspond to any of the class names. To take another example, we can fine-tune the criteria used to decide when the model stops generating output. For instance, if we know that the output should consist of a single token (e.g., the name of a class when classifying reviews), we can explicitly limit the output length to a single token. This prevents the model from generating more output than necessary (saving us money in the process, as costs depend on the amount of output generated).</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>We will discuss those and many other tuning parameters in more detail in chapter 3. In chapter 9, we will see how to use those tuning parameters to get better performance from our language models.</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Another option to configure models is to fine-tune them. This means, essentially, that we’re creating our own variant of an existing model. By training the model with a small amount of task-specific training data, we get a model that potentially performs better at our task than the vanilla version. For instance, if we want to classify reviews, we might train the model with a few hundred example reviews and associated classification results. This may enable us to use a much smaller and cheaper model, fine-tuned for our specific task, that performs as well on this task as a much larger model that has not been fine-tuned.</p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Of course, fine-tuning also costs money, and it may not be immediately clear whether it is worth it for a specific task. We discuss fine-tuning and the associated tradeoffs in more detail in chapter 9.</p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <h3 class=" readable-text-h3" id="prompt-engineering"><span class="num-string browsable-reference-id">1.5.3</span> Prompt engineering</h3> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>The prompt template can significantly affect the quality of the results produced by the language model. A good prompt template clearly specifies the task to solve and provides all relevant context. We will see how to map various tasks to suitable prompt templates throughout the following chapters, covering a variety of data types. After working through those examples, you should be able to design your own prompt templates for novel tasks, following the same principles.</p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>Similar to the model choice, it can be hard to pick the best prompt template for a given task without doing any testing. In chapter 9, we will test prompt templates in an example scenario and illustrate how different prompt templates lead to different outcomes. In some cases, investing a little time in finding the best prompt template may enable you to get satisfactory performance with fairly cheap models (whereas working with the unoptimized prompt template may make a more expensive model necessary).</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p99"> 
    <h5 class=" callout-container-h5 readable-text-h5">Where to get prompt templates</h5> 
   </div> 
   <div class="readable-text" id="p100"> 
    <p> Finding a good prompt template for a new task may take some time. If you do not want to spend that time, have somebody else do it for you! More precisely, you can find platforms on the web that enable users to buy and sell prompt templates. One of them is PromptBase (<a href="https://promptbase.com">https://promptbase.com</a>). Say you want to translate English questions into SQL queries. By entering corresponding keywords, you will find not one but multiple alternative prompt templates on that platform. If the prompt template seems like a good match based on the associated description, you can buy it and use it for your data-analysis needs.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p101"> 
   <h2 class=" readable-text-h2" id="advanced-software-frameworks-and-agents"><span class="num-string browsable-reference-id">1.6</span> Advanced software frameworks and agents</h2> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>Throughout most of this book, we will use OpenAI’s Python library and similar libraries from other providers. For instance, these libraries enable you to send prompts to language models and receive the models’ answers. Although they are entirely sufficient for many use cases, you may want to consider more advanced software frameworks when developing complex applications that are based on language models.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>In this book, we discuss two advanced software frameworks for working with language models: LangChain (<a href="https://langchain.com">https://langchain.com</a>) and LlamaIndex (<a href="http://www.llamaindex.ai">www.llamain</a><a href="http://www.llamaindex.ai">dex.ai</a>). Both make it easier to develop Python applications for data analysis with language models.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Besides many other features, these frameworks make it easy to create agents that use language models. This approach is useful for complex data-analysis tasks requiring, for instance, combining data from multiple sources. For most of this book, we solve data-analysis tasks with a single invocation of the language model, whether it is analyzing a text document or translating a question about data to a formal query. If the task requires multiple steps, such as performing preprocessing before calling the language model or post-processing on the model’s answer, we must hard-code the corresponding processing logic.</p> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>This approach works as long as we can reliably predict the sequence of steps required for data processing. However, in some cases, it can be difficult to predict which steps are required. For instance, we may get questions from users that refer either to a text document or to a relational database. So, depending on the question, we need to either write an SQL query or extract information from text documents. Or perhaps we might need information from both the text and the relational database, extracting information related to the question from the text and then using the information we obtain to formulate an SQL query.</p> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>In such cases, it is not possible to hard-code all possible sequences of steps in advance. Instead, we want to design an approach that is flexible enough to decide independently what step is required next. This can be done using agents and language models. With this approach, the language model is used to decompose complex analysis tasks into subproblems. Furthermore, the language model may choose to invoke <em>tools</em>: arbitrary functions whose interfaces are described in natural language. Such tools can, for instance, encapsulate the invocation of an SQL query on a relational database. After invoking a corresponding tool, the language model is given access to the invocation result (e.g., the query result) and can use that result to plan the next steps. We will see how to use agents to solve complex data-analysis tasks where it is unclear, a priori, which data sources and processing methods are required to solve them.</p> 
  </div> 
  <div class="readable-text" id="p107"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p108">Language models can solve novel tasks without specialized training.</li> 
   <li class="readable-text" id="p109">The prompt is the input to the language model.</li> 
   <li class="readable-text" id="p110">Prompts may combine text with other types of data, such as images.</li> 
   <li class="readable-text" id="p111">A prompt contains a task description, context, and (optionally) examples.</li> 
   <li class="readable-text" id="p112">Language models can analyze certain types of data directly.</li> 
   <li class="readable-text" id="p113">When analyzing data directly, the data must appear in the prompt.</li> 
   <li class="readable-text" id="p114">Prompt templates contain placeholders: for example, to represent data items.</li> 
   <li class="readable-text" id="p115">By substituting for placeholders in a prompt template, we obtain a prompt.</li> 
   <li class="readable-text" id="p116">Language models can also help to analyze data via external tools.</li> 
   <li class="readable-text" id="p117">Language models can instruct other tools on how to process data.</li> 
   <li class="readable-text" id="p118">Models are available in many different sizes with significant cost differences.</li> 
   <li class="readable-text" id="p119">Models can be configured using various configuration parameters.</li> 
   <li class="readable-text" id="p120">LangChain and LlamaIndex help to develop complex applications.</li> 
   <li class="readable-text" id="p121">Agents use language models to solve complex problems.</li> 
  </ul>
 </div></div></body></html>