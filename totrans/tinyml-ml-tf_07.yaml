- en: 'Chapter 7\. Wake-Word Detection: Building an Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TinyML might be a new phenomenon, but its most widespread application is perhaps
    already at work in your home, in your car, or even in your pocket. Can you guess
    what it is?
  prefs: []
  type: TYPE_NORMAL
- en: The past few years have seen the rise of digital assistants. These products
    provide a voice user interface (UI) designed to give instant access to information
    without the need for a screen or keyboard. Between Google Assistant, Apple’s Siri,
    and Amazon Alexa, these digital assistants are nearly ubiquitous. Some variant
    is built into almost every mobile phone, from flagship models to voice-first devices
    designed for emerging markets. They’re also in smart speakers, computers, and
    vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, the heavy lifting of speech recognition, natural language processing,
    and generating responses to users’ queries is done in the cloud, on powerful servers
    running large ML models. When a user asks a question, it’s sent to the server
    as a stream of audio. The server figures out what it means, looks up any required
    information, and sends the appropriate response back.
  prefs: []
  type: TYPE_NORMAL
- en: But part of an assistants’ appeal is that they’re always on, ready to help you
    out. By saying “Hey Google,” or “Alexa,” you can wake up your assistant and tell
    it what you need without ever having to press a button. This means they must be
    listening for your voice 24/7, whether you’re sitting in your living room, driving
    down the freeway, or in the great outdoors with a phone in your hand.
  prefs: []
  type: TYPE_NORMAL
- en: Although it’s easy to do speech recognition on a server, it’s just not feasible
    to send a constant stream of audio from a device to a data center. From a privacy
    perspective, sending every second of audio captured to a remote server would be
    an absolute disaster. Even if that were somehow okay, it would require vast amounts
    of bandwidth and chew through mobile data plans in hours. In addition, network
    communication uses energy, and sending a constant stream of data would quickly
    drain the device’s battery. What’s more, with every request going to a server
    and back, the assistant would feel laggy and slow to respond.
  prefs: []
  type: TYPE_NORMAL
- en: The only audio the assistant really needs is what immediately follows the wake
    word (e.g., “Hey Google”). What if we could detect that word without sending data,
    but start streaming when we heard it? We’d protect user privacy, save battery
    life and bandwidth, and wake up the assistant without waiting for the network.
  prefs: []
  type: TYPE_NORMAL
- en: And this is where TinyML comes in. We can train a tiny model that listens for
    a wake word, and run it on a low-powered chip. If we embed this in a phone, it
    can listen for wake words all the time. When it hears the magic word, it informs
    the phone’s operating system (OS), which can begin to capture audio and send it
    to the server.
  prefs: []
  type: TYPE_NORMAL
- en: Wake-word detection is the perfect application for TinyML. It’s ideally suited
    to delivering privacy, efficiency, speed, and offline inference. This approach,
    in which a tiny, efficient model “wakes up” a larger, more resource-hungry model,
    is called *cascading*.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we examine how we can use a pretrained speech detection model
    to provide always-on wake-word detection using a tiny microcontroller. In [Chapter 8](ch08.xhtml#chapter_training_micro_speech),
    we’ll explore how the model is trained, and how to create our own.
  prefs: []
  type: TYPE_NORMAL
- en: What We’re Building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re going to build an embedded application that uses an 18 KB model, trained
    on a dataset of speech commands, to classify spoken audio. The model is trained
    to recognize the words “yes” and “no,” and is also capable of distinguishing between
    unknown words and silence or background noise.
  prefs: []
  type: TYPE_NORMAL
- en: Our application will listen to its surroundings with a microphone and indicate
    when it has detected a word by lighting an LED or displaying data on a screen,
    depending on the capabilities of the device. Understanding this code will give
    you the ability to control any electronics project with voice commands.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like with [Chapter 5](ch05.xhtml#chapter_building_an_application), the source
    code for this application is available in the [TensorFlow GitHub repository](https://oreil.ly/Bql0J).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll follow a similar pattern to [Chapter 5](ch05.xhtml#chapter_building_an_application),
    walking through the tests, then the application code, followed by the logic that
    makes the sample work on various devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide instructions for deploying the application to the following devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/6qlMD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ST Microelectronics STM32F746G Discovery kit](https://oreil.ly/cvm4J)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorFlow Lite regularly adds support for new devices, so if the device you’d
    like to use isn’t listed here, check the example’s [*README.md*](https://oreil.ly/OE3Pn).
    You can also check there for updated deployment instructions if you run into trouble
    following these steps.
  prefs: []
  type: TYPE_NORMAL
- en: This is a significantly more complex application than the “hello world” example,
    so let’s begin by walking through its structure.
  prefs: []
  type: TYPE_NORMAL
- en: Application Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the previous few chapters, you’ve learned that a machine learning application
    does the following sequence of things:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtains an input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocesses the input to extract features suitable to feed into a model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Runs inference on the processed input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Postprocesses the model’s output to make sense of it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses the resulting information to make things happen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The “hello world” example followed these steps in a very straightforward manner.
    It took a single floating-point number as input, generated by a simple counter.
    Its output was another floating-point number that we used directly to control
    visual output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our wake-word application will be more complicated for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes audio data as an input. As you’ll see, this requires heavy processing
    before it can be fed into a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its model is a classifier, outputting class probabilities. We’ll need to parse
    and make sense of this output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s designed to perform inference continually, on live data. We’ll need to
    write code to make sense of a stream of inferences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is larger and more complex. We’ll be pushing our hardware to the limits
    of its capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because much of this complexity results from the model we’ll be using, let’s
    learn a little more about it.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Our Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, the model we use in this chapter is trained to recognize
    the words “yes” and “no,” and is also capable of distinguishing between unknown
    words and silence or background noise.
  prefs: []
  type: TYPE_NORMAL
- en: The model was trained on a dataset called the [Speech Commands dataset](https://oreil.ly/qtOSI).
    This consists of 65,000 one-second-long utterances of 30 short words, crowdsourced
    online.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the dataset contains 30 different words, the model was trained to
    distinguish between only four categories: the words “yes” and “no,” “unknown”
    words (meaning the other 28 words in the dataset), and silence.'
  prefs: []
  type: TYPE_NORMAL
- en: The model takes in one second’s worth of data at a time. It outputs four probability
    scores, one for each of these four classes, predicting how likely it is that the
    data represented one of them.
  prefs: []
  type: TYPE_NORMAL
- en: However, the model doesn’t take in raw audio sample data. Instead, it works
    with *spectrograms*, which are two-dimensional arrays that are made up of slices
    of frequency information, each taken from a different time window.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-1](#spectrogram_yes) is a visual representation of a spectrogram
    generated from a one-second audio clip of someone saying “yes.” [Figure 7-2](#spectrogram_no)
    shows the same thing for the word “no.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spectrogram for ''yes''](Images/timl_0822.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Spectrogram for “yes”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Spectrogram for ''no''](Images/timl_0823.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Spectrogram for “no”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By isolating the frequency information during preprocessing, we make the model’s
    life easier. During training, it doesn’t need to learn how to interpret raw audio
    data; instead, it gets to work with a higher-layer abstraction that distills the
    most useful information.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at how the spectrogram is generated later in this chapter. For now,
    we just need to know that the model takes a spectrogram as input. Because a spectrogram
    is a two-dimensional array, we feed it into the model as a 2D tensor.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a type of neural network architecture that is specifically designed
    to work well with multidimensional tensors in which information is contained in
    the relationships between groups of adjacent values. It’s called a *convolutional
    neural network* (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: The most common example of this type of data is images, for which a group of
    adjacent pixels might represent a shape, pattern, or texture. During training,
    a CNN is able to identify these features and learn what they represent.
  prefs: []
  type: TYPE_NORMAL
- en: It can learn how simple image features (like lines or edges) fit together into
    more complex features (like an eye or an ear), and in turn how those features
    might be combined to form an input image, such as a photo of a human face. This
    means that a CNN can learn to distinguish between different classes of input image,
    such as between a photo of a person and a photo of a dog.
  prefs: []
  type: TYPE_NORMAL
- en: Although they’re often applied to images, which are 2D grids of pixels, CNNs
    can be used with any multidimensional vector input. It turns out they’re very
    well suited to working with spectrogram data.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.xhtml#chapter_training_micro_speech), we’ll look at how
    this model was trained. Until then, let’s get back to discussing the architecture
    of our application.
  prefs: []
  type: TYPE_NORMAL
- en: All the Moving Parts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, our wake-word application is a more complicated than the
    “hello world” example. [Figure 7-3](#application_architecture) shows the components
    that comprise it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the components of our wake word application](Images/timl_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. The components of our wake-word application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s investigate what each of these pieces do:'
  prefs: []
  type: TYPE_NORMAL
- en: Main loop
  prefs: []
  type: TYPE_NORMAL
- en: Like the “hello world” example, our application runs in a continuous loop. All
    of the subsequent processes are contained within it, and they execute continually,
    as fast as the microcontroller can run them, which is multiple times per second.
  prefs: []
  type: TYPE_NORMAL
- en: Audio provider
  prefs: []
  type: TYPE_NORMAL
- en: The audio provider captures raw audio data from the microphone. Because the
    methods for capturing audio vary from device to device, this component can be
    overridden and customized.
  prefs: []
  type: TYPE_NORMAL
- en: Feature provider
  prefs: []
  type: TYPE_NORMAL
- en: The feature provider converts raw audio data into the spectrogram format that
    our model requires. It does so on a rolling basis as part of the main loop, providing
    the interpreter with a sequence of overlapping one-second windows.
  prefs: []
  type: TYPE_NORMAL
- en: TF Lite interpreter
  prefs: []
  type: TYPE_NORMAL
- en: The interpreter runs the TensorFlow Lite model, transforming the input spectrogram
    into a set of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs: []
  type: TYPE_NORMAL
- en: The model is included as a data array and run by the interpreter. The array
    is located in [*tiny_conv_micro_features_model_data.cc*](https://oreil.ly/XIUz9).
  prefs: []
  type: TYPE_NORMAL
- en: Command recognizer
  prefs: []
  type: TYPE_NORMAL
- en: Because inference is run multiple times per second, the `RecognizeCommands`
    class aggregates the results and determines whether, on average, a known word
    was heard.
  prefs: []
  type: TYPE_NORMAL
- en: Command responder
  prefs: []
  type: TYPE_NORMAL
- en: If a command was heard, the command responder uses the device’s output capabilities
    to let the user know. Depending on the device, this could mean flashing an LED
    or showing data on an LCD display. It can be overridden for different device types.
  prefs: []
  type: TYPE_NORMAL
- en: The example’s files on GitHub contain tests for each of these components. We’ll
    walk through them next to learn how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Walking Through the Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in [Chapter 5](ch05.xhtml#chapter_building_an_application), we can use tests
    to learn how the application works. We’ve already covered a lot of C++ and TensorFlow
    Lite basics, so we won’t need to explain every single line. Instead, let’s focus
    on the most important parts of each test and explain what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll explore the following tests, which you can find in the [GitHub repository](https://oreil.ly/YiSbu):'
  prefs: []
  type: TYPE_NORMAL
- en: '[*micro_speech_test.cc*](https://oreil.ly/FiBEN)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to run inference on spectrogram data and interpret the results
  prefs: []
  type: TYPE_NORMAL
- en: '[*audio_provider_test.cc*](https://oreil.ly/bQOKd)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to use the audio provider
  prefs: []
  type: TYPE_NORMAL
- en: '[*feature_provider_mock_test.cc*](https://oreil.ly/V9rK8)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to use the feature provider, using a *mock* (fake) implementation
    of the audio provider to pass in fake data
  prefs: []
  type: TYPE_NORMAL
- en: '[*recognize_commands_test.cc*](https://oreil.ly/P9pCG)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to interpret the model’s output to decide whether a command was found
  prefs: []
  type: TYPE_NORMAL
- en: '[*command_responder_test.cc*](https://oreil.ly/OqftF)'
  prefs: []
  type: TYPE_NORMAL
- en: Shows how to call the command responder to trigger an output
  prefs: []
  type: TYPE_NORMAL
- en: There are many more tests in the example, but exploring these few will give
    us an understanding of the key moving parts.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The test *micro_speech_test.cc* follows the same basic flow we’re familiar
    with from the “hello world” example: we load the model, set up the interpreter,
    and allocate tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: However, there’s a notable difference. In the “hello world” example, we used
    the `AllOpsResolver` to pull in all of the deep learning operations that might
    be necessary to run the model. This is a reliable approach, but it’s wasteful
    because a given model probably doesn’t use all of the dozens of available operations.
    When deployed to a device, these unnecessary operations will take up valuable
    memory, so it’s best if we include only those we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we first define the ops that our model will need, at the top of
    the test file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up logging and load our model, as normal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After our model is loaded, we declare a `MicroMutableOpResolver` and use its
    method `AddBuiltin()` to add the ops we listed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You’re probably wondering how we know which ops to include for a given model.
    One way is to try running the model using a `MicroMutableOpResolver`, but without
    calling `AddBuiltin()` at all. Inference will fail, and the accompanying error
    messages will inform us which ops are missing and need to be added.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `MicroMutableOpResolver` is defined in [*tensorflow/lite/micro/micro_mutable_op_resolver.h*](https://oreil.ly/TGVZz),
    which you’ll need to add to your `include` statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the `MicroMutableOpResolver` is set up, we just carry on as usual, setting
    up our interpreter and its working memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In our “hello world” application we allocated only 2 * 1,024 bytes for the `tensor_arena`,
    given that the model was so small. Our speech model is a lot bigger, and it deals
    with more complex input and output, so it needs more space (10 1,024). This was
    determined by trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we check the input tensor size. However, it’s a little different this
    time around:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Because we’re dealing with a spectrogram as our input, the input tensor has
    more dimensions—four, in total. The first dimension is just a wrapper containing
    a single element. The second and third represent the “rows” and “columns” of our
    spectrogram, which happens to have 49 rows and 40 columns. The fourth, innermost
    dimension of the input tensor, which has size 1, holds each individual “pixel”
    of the spectrogram. We’ll look more at the spectrogram’s structure later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we grab a sample spectrogram for a “yes,” stored in the constant `g_yes_micro_f2e59fea_nohash_1_data`.
    The constant is defined in the file [*micro_features/yes_micro_features_data.cc*](https://oreil.ly/rVn8O),
    which was included by this test. The spectrogram exists as a 1D array, and we
    just iterate through it to copy it into the input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After the input has been assigned, we run inference and inspect the output
    tensor’s size and shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Our output has two dimensions. The first is just a wrapper. The second has four
    elements. This is the structure that holds the probabilities that each of our
    four classes (silence, unknown, “yes,” and “no”) were matched.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chunk of code checks whether the probabilities were as expected. A
    given element of the output tensor always represents a certain class, so we know
    which index to check for each one. The order is defined during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We passed in a “yes” spectrogram, so we expect that the variable `yes_score`
    contains a higher probability than `silence_score`, `unknown_score`, and `no_score`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we’re satisfied with “yes,” we do the same thing with a “no” spectrogram.
    First, we copy in an input and run inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After inference is done, we confirm that “no” achieved the highest score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: And we’re done!
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this test, issue the following command from the root of the TensorFlow
    repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, let’s look at the source of all our audio data: the audio provider.'
  prefs: []
  type: TYPE_NORMAL
- en: The Audio Provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The audio provider is what connects a device’s microphone hardware to our code.
    Every device has a different mechanism for capturing audio. As a result, [*audio_provider.h*](https://oreil.ly/89FGG)
    defines an interface for requesting audio data, and developers can write their
    own implementations for any platforms that they want to support.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The example includes audio provider implementations for Arduino, STM32F746G,
    SparkFun Edge, and macOS. If you’d like this example to support a new device,
    you can read the existing implementations to learn how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core part of the audio provider is a function named `GetAudioSamples()`,
    defined in *audio_provider.h*. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As described in *audio_provider.h*, the function is expected to return an array
    of 16-bit pulse code modulated (PCM) audio data. This is a very common format
    for digital audio.
  prefs: []
  type: TYPE_NORMAL
- en: The function is called with an `ErrorReporter` instance, a start time (`start_ms`),
    a duration (`duration_ms`), and two pointers.
  prefs: []
  type: TYPE_NORMAL
- en: These pointers are a mechanism for `GetAudioSamples()` to provide data. The
    caller declares variables of the appropriate type and then passes pointers to
    them when it calls the function. Inside the function’s implementation, the pointers
    are dereferenced and the variables’ values are set.
  prefs: []
  type: TYPE_NORMAL
- en: The first pointer, `audio_samples_size`, will receive the total number of 16-bit
    samples in the audio data. The second pointer, `audio_samples`, will receive an
    array containing the audio data itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'By looking at the tests, we can see this in action. There are two tests in
    [*audio_provider_test.cc*](https://oreil.ly/9XgFg), but we need to look only at
    the first to learn how to use the audio provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The test shows how `GetAudioSamples()` is called with some values and some pointers.
    The test confirms that the pointers are assigned correctly after the function
    is called.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll notice the use of some constants, `kFeatureSliceDurationMs` and `kMaxAudioSampleSize`.
    These are values that were chosen when the model was trained, and you can find
    them in [*micro_features/micro_model_settings.h*](https://oreil.ly/WLuug).
  prefs: []
  type: TYPE_NORMAL
- en: The default implementation of *audio_provider.cc* just returns an empty array.
    To prove that it’s the right size, the test simply loops through it for the expected
    number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to `GetAudioSamples()`, the audio provider contains a function called
    `LatestAudioTimestamp()`. This is intended to return the time that audio data
    was last captured, in milliseconds. This information is needed by the feature
    provider to determine what audio data to fetch.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the audio provider tests, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The audio provider is used by the feature provider as a source of fresh audio
    samples, so let’s take a look at that next.
  prefs: []
  type: TYPE_NORMAL
- en: The Feature Provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The feature provider converts raw audio, obtained from the audio provider, into
    spectrograms that can be fed into our model. It is called during the main loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its interface is defined in [*feature_provider.h*](https://oreil.ly/59uTO),
    and looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To see how it’s used, we can take a look at the tests in [*feature_provider_mock_test.cc*](https://oreil.ly/N3YPu).
  prefs: []
  type: TYPE_NORMAL
- en: For there to be audio data for the feature provider to work with, these tests
    use a special fake version of the audio provider, known as a mock, that is set
    up to provide audio data. It is defined in [*audio_provider_mock.cc*](https://oreil.ly/aQSP8).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The mock audio provider is substituted for the real thing in the build instructions
    for the test, which you can find in [*Makefile.inc*](https://oreil.ly/51m0b) under
    `FEATURE_PROVIDER_MOCK_TEST_SRCS`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file *feature_provider_mock_test.cc* contains two tests. Here’s the first
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a `FeatureProvider`, we call its constructor, passing in `feature_size`
    and `feature_data` arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first argument indicates how many total data elements should be in the spectrogram.
    The second argument is a pointer to an array that we want to be populated with
    the spectrogram data.
  prefs: []
  type: TYPE_NORMAL
- en: The number of elements in the spectrogram was decided when the model was trained
    and is defined as `kFeatureElementCount` in [*micro_features/micro_model_settings.h*](https://oreil.ly/FdUCq).
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain features for the past second of audio, `feature_provider.PopulateFeatureData()`
    is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We supply an `ErrorReporter` instance, an integer representing the last time
    this method was called (`last_time_in_ms`), the current time (`time_in_ms`), and
    a pointer to an integer that will be updated with how many new *feature slices*
    we receive (`how_many_new_slices`). A slice is just one row of the spectrogram,
    representing a chunk of time.
  prefs: []
  type: TYPE_NORMAL
- en: Because we always want the last second of audio, the feature provider will compare
    when it was last called (`last_time_in_ms`) with the current time (`time_in_ms`),
    create spectrogram data from the audio captured during that time, and then update
    the `feature_data` array to add any additional slices and drop any that are older
    than one second.
  prefs: []
  type: TYPE_NORMAL
- en: When `PopulateFeatureData()` runs, it will request audio from the mock audio
    provider. The mock will give it audio representing a “yes,” and the feature provider
    will process it and provide the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'After calling `PopulateFeatureData()`, we check whether its result is what
    we expect. We compare the data it generated to a known spectrogram that is correct
    for the “yes” input given by the mock audio provider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The mock audio provider can provide audio for a “yes” or a “no” depending on
    which start and end times are passed into it. The second test in *feature_provider_mock_test.cc*
    does exactly the same thing as the first, but for audio representing “no.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the tests, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How the feature provider converts audio to a spectrogram
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The feature provider is implemented in [*feature_provider.cc*](https://oreil.ly/xzLzE).
    Let’s talk through how it works.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve discussed, its job is to populate an array that represents a spectrogram
    of one second of audio. It’s designed to be called in a loop, so to avoid unnecessary
    work, it will generate new features only for the time between now and when it
    was last called. If it were called less than a second ago, it would keep some
    of its previous output and generate only the missing parts.
  prefs: []
  type: TYPE_NORMAL
- en: In our code, each spectrogram is represented as a 2D array, with 40 columns
    and 49 rows, where each row represents a 30-millisecond (ms) sample of audio split
    into 43 frequency buckets.
  prefs: []
  type: TYPE_NORMAL
- en: To create each row, we run a 30-ms slice of audio input through a *fast Fourier
    transform* (FFT) algorithm. This technique analyzes the frequency distribution
    of audio in the sample and creates an array of 256 frequency buckets, each with
    a value from 0 to 255\. These are averaged together into groups of six, leaving
    us with 43 buckets.
  prefs: []
  type: TYPE_NORMAL
- en: The code that does this is in the file [*micro_features/micro_features_generator.cc*](https://oreil.ly/HVU2G),
    and is called by the feature provider.
  prefs: []
  type: TYPE_NORMAL
- en: To build the entire 2D array, we combine the results of running the FFT on 49
    consecutive 30-ms slices of audio, with each slice overlapping the last by 10
    ms. [Figure 7-4](#spectrogram_generation) shows how this happens.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how the 30-ms sample window is moved forward by 20 ms each time
    until it has covered the full one-second sample. The resulting spectrogram is
    ready to pass into our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can understand how this process happens in *feature_provider.cc*. First,
    it determines which slices it actually needs to generate based on the time `PopulateFeatureData()`
    was last called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Diagram of audio samples being processed](Images/timl_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Diagram of audio samples being processed
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If it hasn’t run before, or it ran more than one second ago, it will generate
    the maximum number of slices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The resulting number is written to `how_many_new_slices`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it calculates how many of any existing slices it should keep, and shifts
    data in the array around to make room for any new ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re a seasoned C++ author, you might wonder why we don’t use standard
    libraries to do things like copying data around. The reason is that we’re trying
    to avoid unnecessary dependencies, in an effort to keep our binary size small.
    Because embedded platforms have very little memory, a smaller application binary
    means that we have space for a larger and more accurate deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After moving data around, it begins a loop that iterates once for each new
    slice that it needs. In this loop, it first requests audio for that slice from
    the audio provider using `GetAudioSamples()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: To complete the loop iteration, it passes that data into `GenerateMicroFeatures()`,
    defined in *micro_features/micro_features_generator.h*. This is the function that
    performs the FFT and returns the audio frequency information.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also passes a pointer, `new_slice_data`, which points at the memory location
    where the new data should be written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After this process has happened for each slice, we have an entire second’s worth
    of up-to-date spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The function that generates the FFT is `GenerateMicroFeatures()`. If you’re
    interested, you can read its definition in [*micro_features/micro_features_generator.cc*](https://oreil.ly/L0juB).
  prefs: []
  type: TYPE_NORMAL
- en: If you’re building your own application that uses spectrograms, you can reuse
    this code as is. You’ll need to use the same code to pre-process data into spectrograms
    when training your model.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a spectrogram, we can run inference on it using the model. After
    this happens, we need to interpret the results. That task belongs to the class
    we explore next, `RecognizeCommands`.
  prefs: []
  type: TYPE_NORMAL
- en: The Command Recognizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After our model outputs a set of probabilities that a known word was spoken
    in the last second of audio, it’s the job of the `RecognizeCommands` class to
    determine whether this indicates a successful detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'It seems like this would be simple: if the probability in a given category
    is more than a certain threshold, the word was spoken. However, in the real world,
    things become a bit more complicated.'
  prefs: []
  type: TYPE_NORMAL
- en: As we established earlier, we’re running multiple inferences per second, each
    on a one-second window of data. This means that we’ll run inference on any given
    word multiple times, in multiple windows.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 7-5](#noted_negative), you can see a waveform of the word “noted”
    being spoken, surrounded by a box representing a one-second window being captured.
  prefs: []
  type: TYPE_NORMAL
- en: '![The word ''noted'' being captured in our window](Images/timl_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. The word “noted” being captured in our window
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our model is trained to detect the word “no,” and it understands that the word
    “noted” is not the same thing. If we run inference on this one-second window,
    it will (hopefully) output a low probability for the word “no.” However, what
    if the window came slightly earlier in the audio stream, as in [Figure 7-6](#noted_positive)?
  prefs: []
  type: TYPE_NORMAL
- en: '![Part of the word ''noted'' being captured in our window](Images/timl_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Part of the word “noted” being captured in our window
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case, the only part of the word “noted” that appears within the window
    is its first syllable. Because the first syllable of “noted” sounds like “no,”
    it’s likely that the model will interpret this as having a high probability of
    being a “no.”
  prefs: []
  type: TYPE_NORMAL
- en: This problem, along with others, means that we can’t rely on a single inference
    to tell us whether a word was spoken. This is where `RecognizeCommands` comes
    in!
  prefs: []
  type: TYPE_NORMAL
- en: The recognizer calculates the average score for each word over the past few
    inferences, and decides whether it’s high enough to count as a detection. To do
    this, we feed it each inference result as they roll in.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see its interface in [*recognize_commands.h*](https://oreil.ly/5W3Ea),
    partially reproduced here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The class `RecognizeCommands` is defined, along with a constructor that defines
    default values for a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: The length of the averaging window (`average_window_duration_ms`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum average score that counts as a detection (`detection_threshold`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount of time we’ll wait after hearing a command before recognizing a second
    one (`suppression_ms`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum number of inferences required in the window for a result to count
    (`3`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class has one method, `ProcessLatestResults()`. It accepts a pointer to
    a `TfLiteTensor` containing the model’s output (`latest_results`), and it must
    be called with the current time (`current_time_ms`).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, it takes three pointers that it uses for output. First, it gives
    us the name of any word that was detected (`found_command`). It also provides
    the average score of the command (`score`) and whether the command is new or has
    been heard in previous inferences within a certain timespan (`is_new_command`).
  prefs: []
  type: TYPE_NORMAL
- en: Averaging the results of multiple inferences is a useful and common technique
    when dealing with time-series data. In the next few pages, we’ll walk through
    the code in [*recognize_commands.cc*](https://oreil.ly/lAh-0) and learn a bit
    about how it works. You don’t need to understand every line, but it’s helpful
    to get some insight into what might be a helpful tool in your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we make sure the input tensor is the right shape and type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we check `current_time_ms` to verify that it is after the most recent
    result in our averaging window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we add the latest result to a list of results we’ll be averaging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If there are fewer results in our averaging window than the minimum number
    (defined by `minimum_count_`, which is `3` by default), we can’t provide a valid
    average. In this case, we set the output pointers to indicate that `found_command`
    is the most recent top command, that the score is 0, and that the command is not
    a new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, we continue by averaging all of the scores in the window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have enough information to identify which category is our winner. Establishing
    this is a simple process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The final piece of logic determines whether the result was a valid detection.
    To do this, it ensures that its score is above the detection threshold (200 by
    default), and that it didn’t happen too quickly after the last valid detection,
    which can be an indication of a faulty result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: If the result was valid, `is_new_command` is set to `true`. This is what the
    caller can use to determine whether a word was genuinely detected.
  prefs: []
  type: TYPE_NORMAL
- en: The tests (in [*recognize_commands_test.cc*](https://oreil.ly/rOkMb)) exercise
    various different combinations of inputs and results that are stored in the averaging
    window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through one of the tests, `RecognizeCommandsTestBasic`, which demonstrates
    how `RecognizeCommands` is used. First, we just create an instance of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a tensor containing some fake inference results, which will
    be used by `ProcessLatestResults()` to decide whether a command was heard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set up some variables that will be set with the output of `ProcessLatestResults()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call `ProcessLatestResults()`, providing pointers to these variables
    along with the tensor containing the results. We assert that the function will
    return `kTfLiteOk`, indicating that the input was processed successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The other tests in the file perform some more exhaustive checks to make sure
    the function is performing correctly. You can read through them to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run all of the tests, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As soon as we’ve determined whether a command was detected, it’s time to share
    our results with the world (or at least our on-board LEDs). The command responder
    is what makes this happen.
  prefs: []
  type: TYPE_NORMAL
- en: The Command Responder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final piece in our puzzle, the command responder, is what produces an output
    to let us know that a word was detected.
  prefs: []
  type: TYPE_NORMAL
- en: The command responder is designed to be overridden for each type of device.
    We explore the device-specific implementations later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let’s look at its very simple reference implementation, which just
    logs detection results as text. You can find it in the file [*command_responder.cc*](https://oreil.ly/kMjg2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s it! The file implements just one function: `RespondToCommand()`. As
    parameters, it expects an `error_reporter`, the current time (`current_time`),
    the command that was last detected (`found_command`), the score it received (`score`),
    and whether the command was newly heard (`is_new_command`).'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that in our program’s main loop, this function will be
    called every time inference is performed, even if a command was not detected.
    This means that we should check `is_new_command` to determine whether anything
    needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test for this function, in [*command_responder_test.cc*](https://oreil.ly/loLZo),
    is equally simple. It just calls the function, given that there’s no way for it
    to test that it generates the correct output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this test, enter this in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! We’ve walked through all of the components of the application.
    Now, let’s see how they come together in the program itself.
  prefs: []
  type: TYPE_NORMAL
- en: Listening for Wake Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the following code in [*main_functions.cc*](https://oreil.ly/n2eD1),
    which defines the `setup()` and `loop()` functions that are the core of our program.
    Let’s read through it together!
  prefs: []
  type: TYPE_NORMAL
- en: Because you’re now a seasoned TensorFlow Lite expert, a lot of this code will
    look familiar to you. So let’s try to focus on the new bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we list the ops that we want to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up our global variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we declare a `FeatureProvider` and a `RecognizeCommands` in addition
    to the usual TensorFlow suspects. We also declare a variable named `g_previous_time`,
    which keeps track of the most recent time we received new audio samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, in the `setup()` function, we load the model, set up our interpreter,
    add ops, and allocate tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'After allocating tensors, we check that the input tensor is the correct shape
    and type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next comes the interesting stuff. First, we instantiate a `FeatureProvider`,
    pointing it at our input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create a `RecognizeCommands` instance and initialize our `previous_time`
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Up next, it’s time for our `loop()` function. Like in the previous example,
    this function will be called over and over again, indefinitely. In the loop, we
    first use the feature provider to create a spectrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: If there’s no new data since the last iteration, we don’t bother running inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have our input, we just invoke the interpreter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The model’s output tensor is now filled with the probabilities for each category.
    To interpret them, we use our `RecognizeCommands` instance. We obtain a pointer
    to the output tensor, then set up a few variables to receive the `ProcessLatestResults()`
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call the command responder’s `RespondToCommand()` method so that
    it can notify users if a word was detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! The call to `RespondToCommand()` is the final thing in our loop.
    Everything from feature generation onward will repeat endlessly, checking the
    audio for known words and producing some output if one is confirmed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `setup()` and `loop()` functions are called by our `main()` function, defined
    in *main.cc*, which begins the loop when the application starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Running Our Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The example contains an audio provider compatible with macOS. If you have access
    to a Mac, you can run the example on your development machine. First, use the
    following command to build it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'After the build completes, you can run the example with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: You might see a pop-up asking for microphone access. If so, grant it, and the
    program will start.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try saying “yes” and “no.” You should see output that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The number after each detected word is its score. By default, the command recognizer
    component considers matches as valid only if their score is more than 200, so
    all of the scores you see will be at least 200.
  prefs: []
  type: TYPE_NORMAL
- en: The number after the score is the number of milliseconds since the program was
    started.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t see any output, make sure your Mac’s internal microphone is selected
    in the Mac’s Sound menu and that its input volume is turned up high enough.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve established that the program works on a Mac. Now, let’s get it running
    on some embedded hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Microcontrollers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we deploy the code to three different devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arduino Nano 33 BLE Sense](https://oreil.ly/ztU5E)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SparkFun Edge](https://oreil.ly/-hoL-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ST Microelectronics STM32F746G Discovery kit](https://oreil.ly/cvm4J)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each one, we’ll walk through the build and deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: Because every device has its own mechanism for capturing audio, there’s a separate
    implementation of *audio_provider.cc* for each one. The same is true for output,
    so each has a variant of *command_responder.cc*, too.
  prefs: []
  type: TYPE_NORMAL
- en: The *audio_provider.cc* implementations are complex and device-specific, and
    not directly related to machine learning. Consequently, we won’t walk through
    them in this chapter. However, there’s a walkthrough of the Arduino variant in
    [Appendix B](app02.xhtml#appendix_arduino_audio). If you need to capture audio
    in your own project, you’re welcome to reuse these implementations in your own
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside deployment instructions, we’re also going to walk through the *command_responder.cc*
    implementation for each device. First up, it’s time for Arduino.
  prefs: []
  type: TYPE_NORMAL
- en: Arduino
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As of this writing, the only Arduino board with a built-in microphone is the
    [Arduino Nano 33 BLE Sense](https://oreil.ly/hjOzL), so that’s what we’ll be using
    for this section. If you’re using a different Arduino board and attaching your
    own microphone, you’ll need to implement your own *audio_provider.cc*.
  prefs: []
  type: TYPE_NORMAL
- en: The Arduino Nano 33 BLE Sense also has a built-in LED, which is what we use
    to indicate that a word has been recognized.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-7](#arduino_nano_sense_led_2) shows a picture of the board with its
    LED highlighted.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image of the Arduino Nano 33 BLE Sense board with the LED highlighted](Images/timl_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. The Arduino Nano 33 BLE Sense board with the LED highlighted
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now let’s look at how we use this LED to indicate that a word has been detected.
  prefs: []
  type: TYPE_NORMAL
- en: Responding to commands on Arduino
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every Arduino board has a built-in LED, and there’s a convenient constant called
    `LED_BUILTIN` that we can use to obtain its pin number, which varies across boards.
    To keep this code portable, we’ll constrain ourselves to using this single LED
    for output.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what we’re going to do. To show that inference is running, we’ll flash
    the LED by toggling it on or off with each inference. However, when we hear the
    word “yes,” we’ll switch on the LED for a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: What about the word “no”? Well, because this is just a demonstration, we won’t
    worry about it too much. We do, however, log all of the detected commands to the
    serial port, so we can connect to the device and see every match.
  prefs: []
  type: TYPE_NORMAL
- en: 'The replacement command responder for Arduino is located in [*arduino/command_responder.cc*](https://oreil.ly/URkYi).
    Let’s walk through its source. First, we include the command responder header
    file and the Arduino platform’s library header file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we begin our function implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Our next step is to place the built-in LED’s pin into output mode so that we
    can switch it on and off. We do this inside an `if` statement that runs only once,
    thanks to a `static bool` called `is_initialized`. Remember, `static` variables
    preserve their state between function calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up another couple of `static` variables to keep track of the last
    time a “yes” was detected, and the number of inferences that have been performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the fun stuff. If the `is_new_command` argument is `true`, we know
    we’ve heard something, so we log it with the `ErrorReporter` instance. But if
    it’s a “yes” we heard—which we determine by checking the first character of the
    `found_command` character array—we store the current time and switch on the LED:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we implement the behavior that switches off the LED after a few seconds—three,
    to be precise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'When the LED is switched off, we also set `last_yet_time` to `0`, so we won’t
    enter this `if` statement until the next time a “yes” is heard. The `return` statement
    is important: it’s what prevents any further output code from running if we recently
    heard a “yes,” so the LED stays solidly lit.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, our implementation will switch on the LED for around three seconds when
    a “yes” is heard. The next part will toggle the LED on and off with each inference—except
    for while we’re in “yes” mode, when we’re prevented from reaching this point by
    the aforementioned `return` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the final chunk of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: By incrementing the `count` variable for each inference, we keep track of the
    total number of inferences that we’ve performed. Inside the `if` conditional,
    we use the `&` operator to do a binary AND operation with the `count` variable
    and the number `1`.
  prefs: []
  type: TYPE_NORMAL
- en: By performing an AND on `count` with `1`, we filter out all of `count`’s bits
    except the smallest. If the smallest bit is a `0`, meaning `count` is an odd number,
    the result will be a `0`. In a C++ `if statement`, this evaluates to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, the result will be a `1`, indicating an even number. Because a `1`
    evaluates to `true`, our LED will switch on with even values and off with odd
    values. This is what makes it toggle.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! We’ve now implemented our command responder for Arduino. Let’s
    get it running so that we can see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To deploy this example, here’s what we’ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: An Arduino Nano 33 BLE Sense board
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A micro-USB cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Arduino IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/7VozJ) for the latest
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The projects in this book are available as example code in the TensorFlow Lite
    Arduino library. If you haven’t already installed the library, open the Arduino
    IDE and select Manage Libraries from the Tools menu. In the window that appears,
    search for and install the library named *Arduino_TensorFlowLite*. You should
    be able to use the latest version, but if you run into issues, the version that
    was tested with this book is 1.14-ALPHA.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also install the library from a *.zip* file, which you can either [download](https://oreil.ly/blgB8)
    from the TensorFlow Lite team or generate yourself using the TensorFlow Lite for
    Microcontrollers Makefile. If you’d prefer to do the latter, see [Appendix A](app01.xhtml#appendix_arduino_library_zip).
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve installed the library, the `micro_speech` example will show up
    in the File menu under Examples→Arduino_TensorFlowLite, as shown in [Figure 7-8](#arduino_examples_micro_speech).
  prefs: []
  type: TYPE_NORMAL
- en: Click “micro_speech” to load the example. It will appear as a new window, with
    a tab for each of the source files. The file in the first tab, *micro_speech*,
    is equivalent to the *main_functions.cc* we walked through earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Examples'' menu](Images/timl_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. The Examples menu
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[“Running the Example”](ch06.xhtml#hello_world_running_the_example) already
    explained the structure of the Arduino example, so we won’t cover it again here.'
  prefs: []
  type: TYPE_NORMAL
- en: To run the example, plug in your Arduino device via USB. Make sure the correct
    device type is selected from the Board drop-down list in the Tools menu, as shown
    in [Figure 7-9](#arduino_board_dropdown_2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Board'' dropdown](Images/timl_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. The Board drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your device’s name doesn’t appear in the list, you’ll need to install its
    support package. To do this, click Boards Manager. In the window that appears,
    search for your device, and then install the latest version of the corresponding
    support package. Next, make sure the device’s port is selected in the Port drop-down
    list, also in the Tools menu, as demonstrated in [Figure 7-10](#arduino_port_dropdown_2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the ''Port'' dropdown](Images/timl_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. The Port drop-down list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, in the Arduino window, click the upload button (highlighted in white
    in [Figure 7-11](#arduino_upload_button_2)) to compile and upload the code to
    your Arduino device.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the upload button, which has an arrow icon](Images/timl_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-11\. The upload button, a right-facing arrow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After the upload has successfully completed you should see the LED on your Arduino
    board begin to flash.
  prefs: []
  type: TYPE_NORMAL
- en: To test the program, try saying “yes.” When it detects a “yes,” the LED will
    remain lit solidly for around three seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you can’t get the program to recognize your “yes,” try saying it a few times
    in a row.
  prefs: []
  type: TYPE_NORMAL
- en: You can also see the results of inference via the Arduino Serial Monitor. To
    do this, open the Serial Monitor from the Tools menu. Now, try saying “yes,” “no,”
    and other words. You should see something like [Figure 7-12](#micro_speech_serial_monitor).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of the Arduino IDE''s Serial Monitor](Images/timl_0712.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-12\. The Serial Monitor displaying some matches
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model we’re using is small and imperfect, and you’ll probably notice that
    it’s better at detecting “yes” than “no.” This is an example of how optimizing
    for a tiny model size can result in issues with accuracy. We cover this topic
    in [Chapter 8](ch08.xhtml#chapter_training_micro_speech).
  prefs: []
  type: TYPE_NORMAL
- en: Making your own changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve deployed the application, try playing around with the code!
    You can edit the source files in the Arduino IDE. When you save, you’ll be prompted
    to re-save the example in a new location. After you’ve made your changes, you
    can click the upload button in the Arduino IDE to build and deploy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few ideas you could try:'
  prefs: []
  type: TYPE_NORMAL
- en: Switch the example to light the LED when “no” is spoken, instead of “yes,”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the application respond to a specific sequence of “yes” and “no” commands,
    like a secret code phrase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the “yes” and “no” commands to control other components, like additional
    LEDs or servos.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkFun Edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SparkFun Edge has both a microphone and a row of four colored LEDs—red,
    blue, green, and yellow—which will make displaying results easy. [Figure 7-13](#sparkfun_edge_leds_2)
    shows the SparkFun Edge with its LEDs highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: '![Photo of the SparkFun Edge highlighting its four LEDs](Images/timl_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-13\. The SparkFun Edge’s four LEDs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Responding to commands on SparkFun Edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make it clear that our program is running, let’s toggle the blue LED on and
    off with each inference. We’ll switch on the yellow LED when a “yes” is heard,
    the red LED when a “no” is heard, and the green LED when an unknown command is
    heard.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command responder for SparkFun Edge is implemented in [*sparkfun_edge/command_responder.cc*](https://oreil.ly/i-3eJ).
    The file begins with some includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The *command_responder.h* include is this file’s corresponding header. *am_bsp.h*
    is the Ambiq Apollo3 SDK, which you saw in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the function definition, the first thing we do is set up the pins connected
    to the LEDs as outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We call the `am_hal_gpio_pinconfig()` function from the Apollo3 SDK to set all
    four LED pins to output mode, represented by the constant `g_AM_HAL_GPIO_OUTPUT_12`.
    We use the `is_initialized` `static` variable to ensure that we do this only once!
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the code that will toggle the blue LED on and off. We do this using
    a `count` variable, in the same way as in the Arduino implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the `am_hal_gpio_output_set()` and `am_hal_gpio_output_clear()`
    functions to switch the blue LED’s pin either on or off.
  prefs: []
  type: TYPE_NORMAL
- en: By incrementing the `count` variable at each inference, we keep track of the
    total number of inferences we’ve performed. Inside the `if` conditional, we use
    the `&` operator to do a binary AND operation with the `count` variable and the
    number `1`.
  prefs: []
  type: TYPE_NORMAL
- en: By performing an AND on `count` with `1`, we filter out all of `count`’s bits
    except the smallest. If the smallest bit is a `0`, meaning `count` is an odd number,
    the result will be a `0`. In a C++ `if statement`, this evaluates to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, the result will be a `1`, indicating an even number. Because a `1`
    evaluates to `true`, our LED will switch on with even values and off with odd
    values. This is what makes it toggle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we light the appropriate LED depending on which word was just heard.
    By default, we clear all of the LEDs, so if a word was not recently heard the
    LEDs will all be unlit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use some simple `if` statements to switch on the appropriate LED depending
    on which command was heard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: As we saw earlier, `is_new_command` is `true` only if `RespondToCommand()` was
    called with a genuinely new command, so if a new command wasn’t heard the LEDs
    will remain off. Otherwise, we use the `am_hal_gpio_output_set()` function to
    switch on the appropriate LED.
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve now walked through how our example code lights up LEDs on the SparkFun
    Edge. Next, let’s get the example up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/U3Cgo) for the latest
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build and deploy our code, we’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A SparkFun Edge board
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A USB programmer (we recommend the SparkFun Serial Basic Breakout, which is
    available in [micro-B USB](https://oreil.ly/2GMNf) and [USB-C](https://oreil.ly/lp39T)
    variants)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A matching USB cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3 and some dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.xhtml#ch_6) shows how to confirm whether you have the correct
    version of Python installed. If you already did this, great. If not, it’s worth
    flipping back to [“Running the Example”](ch06.xhtml#running_hello_world_sparkfun_edge)
    to take a look.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your terminal, clone the TensorFlow repository and then change into its
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’re going to build the binary and run some commands that get it ready
    for downloading to the device. To avoid some typing, you can copy and paste these
    commands from [*README.md*](https://oreil.ly/xY-Rj).
  prefs: []
  type: TYPE_NORMAL
- en: Build the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following command downloads all of the required dependencies and then compiles
    a binary for the SparkFun Edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The binary is created as a *.bin* file, in the following location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether the file exists, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: If you run that command, you should see `Binary was successfully created` printed
    to the console. If you see `Binary is missing`, there was a problem with the build
    process. If so, it’s likely that there are some clues to what went wrong in the
    output of the `make` command.
  prefs: []
  type: TYPE_NORMAL
- en: Sign the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The binary must be signed with cryptographic keys to be deployed to the device.
    Let’s now run some commands that will sign the binary so it can be flashed to
    the SparkFun Edge. The scripts used here come from the Ambiq SDK, which is downloaded
    when the Makefile is run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following command to set up some dummy cryptographic keys that you
    can use for development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run the following command to create a signed binary. Substitute `python3`
    with `python` if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates the file *main_nonsecure_ota.bin*. Now run this command to create
    a final version of the file that can be used to flash your device with the script
    you will use in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: You should now have a file called *main_nonsecure_wire.bin* in the directory
    where you ran the commands. This is the file you’ll be flashing to the device.
  prefs: []
  type: TYPE_NORMAL
- en: Flash the binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The SparkFun Edge stores the program it is currently running in its 1 megabyte
    of flash memory. If you want the board to run a new program, you need to send
    it to the board, which will store it in flash memory, overwriting any program
    that was previously saved.
  prefs: []
  type: TYPE_NORMAL
- en: Attach the programmer to the board
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To download new programs to the board, you’ll use the SparkFun USB-C Serial
    Basic serial programmer. This device allows your computer to communicate with
    the microcontroller via USB.
  prefs: []
  type: TYPE_NORMAL
- en: 'To attach this device to your board, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: On the side of the SparkFun Edge, locate the six-pin header.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plug the SparkFun USB-C Serial Basic into these pins, ensuring the pins labeled
    BLK and GRN on each device are lined up correctly, as illustrated in [Figure 7-14](#sparkfun_edge_serial_basic_2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A photo showing how the SparkFun Edge and USB-C Serial Basic should be connected](Images/timl_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-14\. Connecting the SparkFun Edge and USB-C Serial Basic (courtesy
    of SparkFun)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Attach the programmer to your computer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You connect the board to your computer via USB. To program the board, you need
    to find out the name that your computer gives the device. The best way of doing
    this is to list all the computer’s devices before and after attaching it, and
    look to see which device is new.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some people have reported issues with their operating system’s default drivers
    for the programmer, so we strongly recommend installing the [driver](https://oreil.ly/kohTX)
    before you continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before attaching the device via USB, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output a list of attached devices that looks something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, connect the programmer to your computer’s USB port and run the command
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an extra item in the output, as shown in the example that follows.
    Your new item might have a different name. This new item is the name of the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This name will be used to refer to the device. However, it can change depending
    on which USB port the programmer is attached to, so if you disconnect the board
    from your computer and then reattach it, you might need to look up its name again.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some users have reported two devices appearing in the list. If you see two devices,
    the correct one to use begins with the letters “wch”; for example, “/dev/wchusbserial-14410.”
  prefs: []
  type: TYPE_NORMAL
- en: 'After you’ve identified the device name, put it in a shell variable for later
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: This is a variable that you can use when running commands that require the device
    name, later in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Run the script to flash your board
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To flash the board, you must put it into a special “bootloader” state that prepares
    it to receive the new binary. You’ll then run a script to send the binary to the
    board.
  prefs: []
  type: TYPE_NORMAL
- en: 'First create an environment variable to specify the baud rate, which is the
    speed at which data will be sent to the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now paste the command that follows into your terminal—but *do not press Enter
    yet*! The `${DEVICENAME}` and `${BAUD_RATE}` in the command will be replaced with
    the values you set in the previous sections. Remember to substitute `python3`
    with `python` if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you’ll reset the board into its bootloader state and flash the board.
    On the board, locate the buttons marked `RST` and `14`, as shown in [Figure 7-15](#sparkfun_edge_buttons_2).
    Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that your board is connected to the programmer and the entire thing is
    connected to your computer via USB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the board, press and hold the button marked `14`. *Continue holding it*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While still holding the button marked `14`, press the button marked `RST` to
    reset the board.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press Enter on your computer to run the script. *Continue holding button `14`.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should now see something like the following appearing on your screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '![A photo showing the SparkFun Edge''s buttons](Images/timl_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-15\. The SparkFun Edge’s buttons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Keep holding button `14` until you see `Sending Data Packet of length 8180`.
    You can release the button after seeing this (but it’s okay if you keep holding
    it). The program will continue to print lines on the terminal. Eventually, you’ll
    see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: This indicates a successful flashing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the program output ends with an error, check whether `Sending Reset Command.`
    was printed. If so, flashing was likely successful despite the error. Otherwise,
    flashing might have failed. Try running through these steps again (you can skip
    over setting the environment variables).
  prefs: []
  type: TYPE_NORMAL
- en: Testing the program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make sure the program is running, press the `RST` button. You should now
    see the blue LED flashing.
  prefs: []
  type: TYPE_NORMAL
- en: To test the program, try saying “yes.” When it detects a “yes,” the orange LED
    will flash. The model is also trained to recognize “no,” and when unknown words
    are spoken. The red LED should flash for “no,” and the green for unknown.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you can’t get the program to recognize your “yes,” try saying it a few times
    in a row: “yes, yes, yes.”'
  prefs: []
  type: TYPE_NORMAL
- en: The model we’re using is small and imperfect, and you’ll probably notice that
    it’s better at detecting “yes” than “no,” which it often recognizes as “unknown.”
    This is an example of how optimizing for a tiny model size can result in issues
    with accuracy. We cover this topic in [Chapter 8](ch08.xhtml#chapter_training_micro_speech).
  prefs: []
  type: TYPE_NORMAL
- en: Viewing debug data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The program will also log successful recognitions to the serial port. To view
    this data, we can monitor the board’s serial port output using a baud rate of
    115200\. On macOS and Linux, the following command should work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'You should initially see output that looks something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Try issuing some commands by saying “yes” or “no.” You should see the board
    printing debug information for each command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: To stop viewing the debug output with `screen`, press Ctrl-A immediately followed
    by the K key, and then press the Y key.
  prefs: []
  type: TYPE_NORMAL
- en: Making your own changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve deployed the basic application, try playing around and making
    some changes. You can find the application’s code in the *tensorflow/lite/micro/examples/micro_speech*
    folder. Just edit and save and then repeat the preceding instructions to deploy
    your modified code to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things that you could try:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RespondToCommand()`’s `score` argument shows the prediction score. Use the
    LEDs as a meter to show the strength of the match.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the application respond to a specific sequence of “yes” and “no” commands,
    like a secret code phrase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the “yes” and “no” commands to control other components, like additional
    LEDs or servos.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ST Microelectronics STM32F746G Discovery Kit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because the STM32F746G comes with a fancy LCD display, we can use this to show
    off whichever wake words are detected, as depicted in [Figure 7-16](#stm_micro_speech).
  prefs: []
  type: TYPE_NORMAL
- en: '![STM32F746G displaying a ''no''](Images/timl_0716.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-16\. STM32F746G displaying a “no”
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Responding to commands on STM32F746G
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The STM32F746G’s LCD driver gives us methods that we can use to write text
    to the display. In this exercise, we’ll use these to show one of the following
    messages, depending on which command was heard:'
  prefs: []
  type: TYPE_NORMAL
- en: “Heard yes!”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Heard no :(”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Heard unknown”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Heard silence”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll also set the background color differently depending on which command was
    heard.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we include some header files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The first, *command_responder.h*, just declares the interface for this file.
    The second, *LCD_DISCO_F74NG.h*, gives us an interface to control the device’s
    LCD display. You can read more about it on the [Mbed site](https://oreil.ly/6oirs).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we instantiate an `LCD_DISCO_F746NG` object, which holds the methods
    we use to control the LCD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next few lines, the `RespondToCommand()` function is declared, and we
    check whether it has been called with a new command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: When we know this is a new command, we use the `error_reporter` to log it to
    the serial port.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use a big `if` statement to determine what happens when each command
    is found. First comes “yes”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: We use `lcd.Clear()` to both clear any previous content from the screen and
    set a new background color, like a fresh coat of paint. The color `0xFF0F9D58`
    is a nice, rich green.
  prefs: []
  type: TYPE_NORMAL
- en: On our green background, we use `lcd.DisplayStringAt()` to draw some text. The
    first argument specifies an *x* coordinate, the second specifies a *y*. To position
    our text roughly in the middle of the display, we use a helper function, `LINE()`,
    to determine the *y* coordinate that would correspond to the fifth line of text
    on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: The third argument is the string of text we’ll be displaying, and the fourth
    argument determines the alignment of the text; here, we use the constant `CENTER_MODE`
    to specify that the text is center-aligned.
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue the `if` statement to cover the remaining three possibilities,
    “no,” “unknown,” and “silence” (which is captured by the `else` block):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! Because the LCD library gives us such easy high-level control
    over the display, it doesn’t take much code to output our results. Let’s deploy
    the example to see this all in action.
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we can use the Mbed toolchain to deploy our application to the device.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s always a chance that the build process might have changed since this
    book was written, so check [*README.md*](https://oreil.ly/1INIO) for the latest
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we’ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An STM32F746G Discovery kit board
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mini-USB cable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Arm Mbed CLI (follow the [Mbed setup guide](https://oreil.ly/tR57j))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3 and `pip`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like the Arduino IDE, Mbed requires source files to be structured in a certain
    way. The TensorFlow Lite for Microcontrollers Makefile knows how to do this for
    us and can generate a directory suitable for Mbed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the creation of a new directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: This directory contains all of the example’s dependencies structured in the
    correct way for Mbed to be able to build it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, change into the directory so that you can run some commands within it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Next, you’ll use Mbed to download the dependencies and build the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, use the following command to inform Mbed that the current directory
    is the root of an Mbed project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, instruct Mbed to download the dependencies and prepare to build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Mbed builds the project using C++98\. However, TensorFlow Lite
    requires C++11\. Run the following Python snippet to modify the Mbed configuration
    files so that it uses C++11\. You can just type or paste it into the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, run the following command to compile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in a binary at the following path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the nice things about the STM32F746G board is that deployment is really
    easy. To deploy, just plug in your STM board and copy the file to it. On macOS,
    you can do this by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Alternately, just find the `DIS_F746NG` volume in your file browser and drag
    the file over.
  prefs: []
  type: TYPE_NORMAL
- en: Copying the file initiates the flashing process.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When this is complete, try saying “yes.” You should see the appropriate text
    appear on the display and the background color change.
  prefs: []
  type: TYPE_NORMAL
- en: If you can’t get the program to recognize your “yes,” try saying it a few times
    in a row, like “yes, yes, yes.”
  prefs: []
  type: TYPE_NORMAL
- en: The model we’re using is small and imperfect, and you’ll probably notice that
    it’s better at detecting “yes” than “no,” which it often recognizes as “unknown.”
    This is an example of how optimizing for a tiny model size can result in issues
    with accuracy. We cover this topic in [Chapter 8](ch08.xhtml#chapter_training_micro_speech).
  prefs: []
  type: TYPE_NORMAL
- en: Viewing debug data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The program also logs successful recognitions to the serial port. To view the
    output, establish a serial connection to the board using a baud rate of 9600.
  prefs: []
  type: TYPE_NORMAL
- en: 'On macOS and Linux, the device should be listed when you issue the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'It will look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'After you’ve identified the device, use the following command to connect to
    it, replacing <`*/dev/tty.devicename*`> with the name of your device as it appears
    in */dev*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Try issuing some commands by saying “yes” or “no.” You should see the board
    printing debug information for each command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: To stop viewing the debug output with `screen`, press Ctrl-A, immediately followed
    by the K key, and then press the Y key.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re not sure how to make a serial connection on your platform, you could
    try [CoolTerm](https://oreil.ly/FP7gK), which works on Windows, macOS, and Linux.
    The board should show up in CoolTerm’s Port drop-down list. Make sure you set
    the baud rate to 9600.
  prefs: []
  type: TYPE_NORMAL
- en: Making your own changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you’ve deployed the application, it could be fun to play around and
    make some changes. You can find the application’s code in the *tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed*
    folder. Just edit and save and then repeat the preceding instructions to deploy
    your modified code to the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things you could try:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RespondToCommand()`’s `score` argument shows the prediction score. Create
    a visual indicator of the score on the LCD display.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the application respond to a specific sequence of “yes” and “no” commands,
    like a secret code phrase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the “yes” and “no” commands to control other components, like additional
    LEDs or servos.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application code we’ve walked through has been mostly concerned with capturing
    data from the hardware and then extracting features that are suitable for inference.
    The part that actually feeds data into the model and runs inference is relatively
    small, and it’s very similar to the example covered in Chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: This is fairly typical of machine learning projects. The model is already trained,
    thus our job is just to keep it fed with the appropriate sort of data. As an embedded
    developer working with TensorFlow Lite, you’ll be spending most of your programming
    time on capturing sensor data, processing it into features, and responding to
    the output of your model. The inference part itself is quick and easy.
  prefs: []
  type: TYPE_NORMAL
- en: But the embedded application is only part of the package—the really fun part
    is the model. In [Chapter 8](ch08.xhtml#chapter_training_micro_speech), you’ll
    learn how to train your own speech model to listen for different words. You’ll
    also learn more about how it works.
  prefs: []
  type: TYPE_NORMAL
