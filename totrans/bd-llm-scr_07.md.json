["```py\npip install torch\n```", "```py\npip install torch==2.4.0\n```", "```py\nimport torch\ntorch.__version__\n```", "```py\n'2.4.0'\n```", "```py\nimport torch\ntorch.cuda.is_available()\n```", "```py\nTrue\n```", "```py\nprint(torch.backends.mps.is_available())\n```", "```py\nimport torch\n\ntensor0d = torch.tensor(1)     #1\n\ntensor1d = torch.tensor([1, 2, 3])    #2\n\ntensor2d = torch.tensor([[1, 2], \n                         [3, 4]])     #3\n\ntensor3d = torch.tensor([[[1, 2], [3, 4]], \n                         [[5, 6], [7, 8]]])    #4\n```", "```py\ntensor1d = torch.tensor([1, 2, 3])\nprint(tensor1d.dtype)\n```", "```py\ntorch.int64\n```", "```py\nfloatvec = torch.tensor([1.0, 2.0, 3.0])\nprint(floatvec.dtype)\n```", "```py\ntorch.float32\n```", "```py\nfloatvec = tensor1d.to(torch.float32)\nprint(floatvec.dtype)\n```", "```py\ntorch.float32\n```", "```py\ntensor2d = torch.tensor([[1, 2, 3], \n                         [4, 5, 6]])\nprint(tensor2d)\n```", "```py\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n```", "```py\nprint(tensor2d.shape)\n```", "```py\ntorch.Size([2, 3])\n```", "```py\nprint(tensor2d.reshape(3, 2))\n```", "```py\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n```", "```py\nprint(tensor2d.view(3, 2))\n```", "```py\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n```", "```py\nprint(tensor2d.T)\n```", "```py\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n```", "```py\nprint(tensor2d.matmul(tensor2d.T))\n```", "```py\ntensor([[14, 32],\n        [32, 77]])\n```", "```py\nprint(tensor2d @ tensor2d.T)\n```", "```py\ntensor([[14, 32],\n        [32, 77]])\n```", "```py\nimport torch.nn.functional as F     #1\n\ny = torch.tensor([1.0])          #2\nx1 = torch.tensor([1.1])    #3\nw1 = torch.tensor([2.2])    #4\nb = torch.tensor([0.0])            #5\nz = x1 * w1 + b                 #6\na = torch.sigmoid(z)               #7\nloss = F.binary_cross_entropy(a, y)\n```", "```py\nimport torch.nn.functional as F\nfrom torch.autograd import grad\n\ny = torch.tensor([1.0])\nx1 = torch.tensor([1.1])\nw1 = torch.tensor([2.2], requires_grad=True)\nb = torch.tensor([0.0], requires_grad=True)\n\nz = x1 * w1 + b \na = torch.sigmoid(z)\n\nloss = F.binary_cross_entropy(a, y)\n\ngrad_L_w1 = grad(loss, w1, retain_graph=True)   #1\ngrad_L_b = grad(loss, b, retain_graph=True)\n```", "```py\nprint(grad_L_w1)\nprint(grad_L_b)\n```", "```py\n(tensor([-0.0898]),)\n(tensor([-0.0817]),)\n```", "```py\nloss.backward()\nprint(w1.grad)\nprint(b.grad)\n```", "```py\n(tensor([-0.0898]),)\n(tensor([-0.0817]),)\n```", "```py\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self, num_inputs, num_outputs):    #1\n        super().__init__()\n\n        self.layers = torch.nn.Sequential(\n\n            # 1st hidden layer\n            torch.nn.Linear(num_inputs, 30),    #2\n            torch.nn.ReLU(),               #3\n\n            # 2nd hidden layer\n            torch.nn.Linear(30, 20),    #4\n            torch.nn.ReLU(),\n\n            # output layer\n            torch.nn.Linear(20, num_outputs),\n        )\n\n    def forward(self, x):\n        logits = self.layers(x)\n        return logits           #5\n```", "```py\nmodel = NeuralNetwork(50, 3)\n```", "```py\nprint(model)\n```", "```py\nNeuralNetwork(\n  (layers): Sequential(\n    (0): Linear(in_features=50, out_features=30, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=30, out_features=20, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=20, out_features=3, bias=True)\n  )\n)\n```", "```py\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"Total number of trainable model parameters:\", num_params)\n```", "```py\nTotal number of trainable model parameters: 2213\n```", "```py\nprint(model.layers[0].weight)\n```", "```py\nParameter containing:\ntensor([[ 0.1174, -0.1350, -0.1227,  ...,  0.0275, -0.0520, -0.0192],\n        [-0.0169,  0.1265,  0.0255,  ..., -0.1247,  0.1191, -0.0698],\n        [-0.0973, -0.0974, -0.0739,  ..., -0.0068, -0.0892,  0.1070],\n        ...,\n        [-0.0681,  0.1058, -0.0315,  ..., -0.1081, -0.0290, -0.1374],\n        [-0.0159,  0.0587, -0.0916,  ..., -0.1153,  0.0700,  0.0770],\n        [-0.1019,  0.1345, -0.0176,  ...,  0.0114, -0.0559, -0.0088]],\n       requires_grad=True)\n```", "```py\nprint(model.layers[0].weight.shape)\n```", "```py\ntorch.Size([30, 50])\n```", "```py\ntorch.manual_seed(123)\nmodel = NeuralNetwork(50, 3)\nprint(model.layers[0].weight)\n```", "```py\nParameter containing:\ntensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n        ...,\n        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n       requires_grad=True)\n```", "```py\ntorch.manual_seed(123)\nX = torch.rand((1, 50))\nout = model(X)\nprint(out)\n```", "```py\ntensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n```", "```py\nwith torch.no_grad():\n    out = model(X)\nprint(out)\n```", "```py\ntensor([[-0.1262,  0.1080, -0.1792]])\n```", "```py\nwith torch.no_grad():\n    out = torch.softmax(model(X), dim=1)\nprint(out)\n```", "```py\ntensor([[0.3113, 0.3934, 0.2952]]))\n```", "```py\nX_train = torch.tensor([\n    [-1.2, 3.1],\n    [-0.9, 2.9],\n    [-0.5, 2.6],\n    [2.3, -1.1],\n    [2.7, -1.5]\n])\ny_train = torch.tensor([0, 0, 0, 1, 1])\n\nX_test = torch.tensor([\n    [-0.8, 2.8],\n    [2.6, -1.6],\n])\ny_test = torch.tensor([0, 1])\n```", "```py\nfrom torch.utils.data import Dataset\n\nclass ToyDataset(Dataset):\n    def __init__(self, X, y):\n        self.features = X\n        self.labels = y\n\n    def __getitem__(self, index):        #1\n        one_x = self.features[index]     #1\n        one_y = self.labels[index]       #1\n        return one_x, one_y              #1\n\n    def __len__(self):\n        return self.labels.shape[0]      #2\n\ntrain_ds = ToyDataset(X_train, y_train)\ntest_ds = ToyDataset(X_test, y_test)\n```", "```py\nprint(len(train_ds))\n```", "```py\n5\n```", "```py\nfrom torch.utils.data import DataLoader\n\ntorch.manual_seed(123)\n\ntrain_loader = DataLoader(\n    dataset=train_ds,     #1\n    batch_size=2,\n    shuffle=True,          #2\n    num_workers=0     #3\n)\n\ntest_loader = DataLoader(\n    dataset=test_ds,\n    batch_size=2,\n    shuffle=False,     #4\n    num_workers=0\n)\n```", "```py\nfor idx, (x, y) in enumerate(train_loader):\n    print(f\"Batch {idx+1}:\", x, y)\n```", "```py\nBatch 1: tensor([[-1.2000,  3.1000],\n                 [-0.5000,  2.6000]]) tensor([0, 0])\nBatch 2: tensor([[ 2.3000, -1.1000],\n                 [-0.9000,  2.9000]]) tensor([1, 0])\nBatch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n```", "```py\ntrain_loader = DataLoader(\n    dataset=train_ds,\n    batch_size=2,\n    shuffle=True,\n    num_workers=0,\n    drop_last=True\n)\n```", "```py\nfor idx, (x, y) in enumerate(train_loader):\n    print(f\"Batch {idx+1}:\", x, y)\n```", "```py\nBatch 1: tensor([[-0.9000,  2.9000],\n        [ 2.3000, -1.1000]]) tensor([0, 1])\nBatch 2: tensor([[ 2.7000, -1.5000],\n        [-0.5000,  2.6000]]) tensor([1, 0])\n```", "```py\nimport torch.nn.functional as F\n\ntorch.manual_seed(123)\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)    #1\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=0.5\n)            #2\n\nnum_epochs = 3\nfor epoch in range(num_epochs): \n\n    model.train()\n    for batch_idx, (features, labels) in enumerate(train_loader):\n        logits = model(features)\n\n        loss = F.cross_entropy(logits, labels)\n\n        optimizer.zero_grad()            #3\n        loss.backward()         #4\n        optimizer.step()        #5\n\n        ### LOGGING\n        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n              f\" | Train Loss: {loss:.2f}\")\n\n    model.eval()\n    # Insert optional model evaluation code\n```", "```py\nEpoch: 001/003 | Batch 000/002 | Train Loss: 0.75\nEpoch: 001/003 | Batch 001/002 | Train Loss: 0.65\nEpoch: 002/003 | Batch 000/002 | Train Loss: 0.44\nEpoch: 002/003 | Batch 001/002 | Trainl Loss: 0.13\nEpoch: 003/003 | Batch 000/002 | Train Loss: 0.03\nEpoch: 003/003 | Batch 001/002 | Train Loss: 0.00\n```", "```py\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(X_train)\nprint(outputs)\n```", "```py\ntensor([[ 2.8569, -4.1618],\n        [ 2.5382, -3.7548],\n        [ 2.0944, -3.1820],\n        [-1.4814,  1.4816],\n        [-1.7176,  1.7342]])\n```", "```py\ntorch.set_printoptions(sci_mode=False)\nprobas = torch.softmax(outputs, dim=1)\nprint(probas)\n```", "```py\ntensor([[    0.9991,     0.0009],\n        [    0.9982,     0.0018],\n        [    0.9949,     0.0051],\n        [    0.0491,     0.9509],\n        [    0.0307,     0.9693]])\n```", "```py\npredictions = torch.argmax(probas, dim=1)\nprint(predictions)\n```", "```py\ntensor([0, 0, 0, 1, 1])\n```", "```py\npredictions = torch.argmax(outputs, dim=1)\nprint(predictions)\n```", "```py\ntensor([0, 0, 0, 1, 1])\n```", "```py\npredictions == y_train\n```", "```py\ntensor([True, True, True, True, True])\n```", "```py\ntorch.sum(predictions == y_train)\n```", "```py\n5\n```", "```py\ndef compute_accuracy(model, dataloader):\n\n    model = model.eval()\n    correct = 0.0\n    total_examples = 0\n\n    for idx, (features, labels) in enumerate(dataloader):\n\n        with torch.no_grad():\n            logits = model(features)\n\n        predictions = torch.argmax(logits, dim=1)\n        compare = labels == predictions       #1\n        correct += torch.sum(compare)      #2\n        total_examples += len(compare)\n\n    return (correct / total_examples).item()    #3\n```", "```py\nprint(compute_accuracy(model, train_loader))\n```", "```py\n1.0\n```", "```py\nprint(compute_accuracy(model, test_loader))\n```", "```py\n1.0\n```", "```py\ntorch.save(model.state_dict(), \"model.pth\")\n```", "```py\nmodel = NeuralNetwork(2, 2) \nmodel.load_state_dict(torch.load(\"model.pth\"))\n```", "```py\nprint(torch.cuda.is_available())\n```", "```py\nTrue\n```", "```py\ntensor_1 = torch.tensor([1., 2., 3.])\ntensor_2 = torch.tensor([4., 5., 6.])\nprint(tensor_1 + tensor_2)\n```", "```py\ntensor([5., 7., 9.])\n```", "```py\ntensor_1 = tensor_1.to(\"cuda\")\ntensor_2 = tensor_2.to(\"cuda\")\nprint(tensor_1 + tensor_2)\n```", "```py\ntensor([5., 7., 9.], device='cuda:0')\n```", "```py\ntensor_1 = tensor_1.to(\"cpu\")\nprint(tensor_1 + tensor_2)\n```", "```py\nRuntimeError      Traceback (most recent call last)\n<ipython-input-7-4ff3c4d20fc3> in <cell line: 2>()\n      1 tensor_1 = tensor_1.to(\"cpu\")\n----> 2 print(tensor_1 + tensor_2)\nRuntimeError: Expected all tensors to be on the same device, but found at\nleast two devices, cuda:0 and cpu!\n```", "```py\ntorch.manual_seed(123)\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)\n\ndevice = torch.device(\"cuda\")      #1\nmodel = model.to(device)          #2\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n\nnum_epochs = 3\n\nfor epoch in range(num_epochs):\n\n    model.train()\n    for batch_idx, (features, labels) in enumerate(train_loader):\n        features, labels = features.to(device), labels.to(device)   #3\n        logits = model(features)\n        loss = F.cross_entropy(logits, labels) # Loss function\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        ### LOGGING\n        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n              f\" | Train/Val Loss: {loss:.2f}\")\n\n    model.eval()\n    # Insert optional model evaluation code\n```", "```py\nEpoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\nEpoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\nEpoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\nEpoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\nEpoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\nEpoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n```", "```py\ndevice = torch.device(\n    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n)\n```", "```py\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\n```", "```py\ndef ddp_setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"    #1\n    os.environ[\"MASTER_PORT\"] = \"12345\"      #2\n    init_process_group(\n        backend=\"nccl\",              #3\n        rank=rank,                         #4\n        world_size=world_size            #5\n    )\n    torch.cuda.set_device(rank)        #6\n\ndef prepare_dataset():\n    # insert dataset preparation code \n    train_loader = DataLoader(\n        dataset=train_ds,\n        batch_size=2,\n        shuffle=False,             #7\n        pin_memory=True,           #8\n        drop_last=True,\n        sampler=DistributedSampler(train_ds)    #9\n    )    \n    return train_loader, test_loader\n\ndef main(rank, world_size, num_epochs):       #10\n    ddp_setup(rank, world_size)\n    train_loader, test_loader = prepare_dataset()\n    model = NeuralNetwork(num_inputs=2, num_outputs=2)\n    model.to(rank)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n    model = DDP(model, device_ids=[rank])\n    for epoch in range(num_epochs):\n        train_loader.sampler.set_epoch(epoch)\n        model.train()\n        for features, labels in train_loader:\n            features, labels = features.to(rank), labels.to(rank)      #11\n            # insert model prediction and backpropagation code \n            print(f\"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n                  f\" | Batchsize {labels.shape[0]:03d}\"\n                  f\" | Train/Val Loss: {loss:.2f}\")\n\n    model.eval()\n    train_acc = compute_accuracy(model, train_loader, device=rank)\n    print(f\"[GPU{rank}] Training accuracy\", train_acc)\n    test_acc = compute_accuracy(model, test_loader, device=rank)\n    print(f\"[GPU{rank}] Test accuracy\", test_acc)\n    destroy_process_group()                      #12\n\nif __name__ == \"__main__\":\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n    torch.manual_seed(123)\n    num_epochs = 3\n    world_size = torch.cuda.device_count()\n    mp.spawn(main, args=(world_size, num_epochs), nprocs=world_size)  #13\n```", "```py\nCUDA_VISIBLE_DEVICES=0 python some_script.py\n```", "```py\nCUDA_VISIBLE_DEVICES=0,2 python some_script.py\n```", "```py\npython ch02-DDP-script.py\n```", "```py\nPyTorch version: 2.2.1+cu117\nCUDA available: True\nNumber of GPUs available: 1\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.62\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.32\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.11\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.07\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.02\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.03\n[GPU0] Training accuracy 1.0\n[GPU0] Test accuracy 1.0\n```", "```py\nPyTorch version: 2.2.1+cu117\nCUDA available: True\nNumber of GPUs available: 2\n[GPU1] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.60\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.59\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.16\n[GPU1] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.17\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\n[GPU1] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\n[GPU1] Training accuracy 1.0\n[GPU0] Training accuracy 1.0\n[GPU1] Test accuracy 1.0\n[GPU0] Test accuracy 1.0\n```", "```py\nif rank == 0:                  #1\n    print(\"Test accuracy: \", accuracy)\n```"]