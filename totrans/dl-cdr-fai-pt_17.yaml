- en: Chapter 13\. Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.xhtml#chapter_mnist_basics), we learned how to create a
    neural network recognizing images. We were able to achieve a bit over 98% accuracy
    at distinguishing 3s from 7s—but we also saw that fastai’s built-in classes were
    able to get close to 100%. Let’s start trying to close the gap.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will begin by digging into what convolutions are and building
    a CNN from scratch. We will then study a range of techniques to improve training
    stability and learn all the tweaks the library usually applies for us to get great
    results.
  prefs: []
  type: TYPE_NORMAL
- en: The Magic of Convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most powerful tools that machine learning practitioners have at their
    disposal is *feature engineering*. A *feature* is a transformation of the data
    that is designed to make it easier to model. For instance, the `add_datepart`
    function that we used for our tabular dataset preprocessing in [Chapter 9](ch09.xhtml#chapter_tabular)
    added date features to the Bulldozers dataset. What kinds of features might we
    be able to create from images?
  prefs: []
  type: TYPE_NORMAL
- en: 'Jargon: Feature Engineering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating new transformations of the input data in order to make it easier to
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of an image, a feature is a visually distinctive attribute. For
    example, the number 7 is characterized by a horizontal edge near the top of the
    digit, and a top-right to bottom-left diagonal edge underneath that. On the other
    hand, the number 3 is characterized by a diagonal edge in one direction at the
    top left and bottom right of the digit, the opposite diagonal at the bottom left
    and top right, horizontal edges at the middle, top, and bottom, and so forth.
    So what if we could extract information about where the edges occur in each image,
    and then use that information as our features, instead of raw pixels?
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that finding the edges in an image is a very common task in computer
    vision and is surprisingly straightforward. To do it, we use something called
    a *convolution*. A convolution requires nothing more than multiplication and addition—two
    operations that are responsible for the vast majority of work that we will see
    in every single deep learning model in this book!
  prefs: []
  type: TYPE_NORMAL
- en: A convolution applies a *kernel* across an image. A kernel is a little matrix,
    such as the 3×3 matrix in the top right of [Figure 13-1](#basic_conv).
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying a kernel to one location](Images/dlcf_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. Applying a kernel to one location
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The 7×7 grid to the left is the *image* we’re going to apply the kernel to.
    The convolution operation multiplies each element of the kernel by each element
    of a 3×3 block of the image. The results of these multiplications are then added
    together. The diagram in [Figure 13-1](#basic_conv) shows an example of applying
    a kernel to a single location in the image, the 3×3 block around cell 18.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do this with code. First, we create a little 3×3 matrix like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re going to call this our kernel (because that’s what fancy computer vision
    researchers call these). And we’ll need an image, of course:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we’re going to take the top 3×3-pixel square of our image, and multiply
    each of those values by each item in our kernel. Then we’ll add them up, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Not very interesting so far—all the pixels in the top-left corner are white.
    But let’s pick a couple of more interesting spots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Top section of a digit](Images/dlcf_13in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There’s a top edge at cell 5,7\. Let’s repeat our calculation there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There’s a right edge at cell 8,18\. What does that give us?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this little calculation is returning a high number where the
    3×3-pixel square represents a top edge (i.e., where there are low values at the
    top of the square and high values immediately underneath). That’s because the
    `-1` values in our kernel have little impact in that case, but the `1` values
    have a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look a tiny bit at the math. The filter will take any window of size 3×3
    in our images, and if we name the pixel values like this
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row 1st Column a Baseline 1 2nd Column a Baseline
    2 3rd Column a Baseline 3 2nd Row 1st Column a Baseline 4 2nd Column a Baseline
    5 3rd Column a Baseline 6 3rd Row 1st Column a Baseline 7 2nd Column a Baseline
    8 3rd Column a Baseline 9 EndLayout" display="block"><mtable><mtr><mtd><mrow><mi>a</mi>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mi>a</mi> <mn>2</mn></mrow></mtd> <mtd><mrow><mi>a</mi>
    <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mi>a</mi> <mn>4</mn></mrow></mtd>
    <mtd><mrow><mi>a</mi> <mn>5</mn></mrow></mtd> <mtd><mrow><mi>a</mi> <mn>6</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mi>a</mi> <mn>7</mn></mrow></mtd> <mtd><mrow><mi>a</mi> <mn>8</mn></mrow></mtd>
    <mtd><mrow><mi>a</mi> <mn>9</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: it will return <math alttext="a Baseline 1 plus a Baseline 2 plus a Baseline
    3 minus a Baseline 7 minus a Baseline 8 minus a Baseline 9"><mrow><mi>a</mi> <mn>1</mn>
    <mo>+</mo> <mi>a</mi> <mn>2</mn> <mo>+</mo> <mi>a</mi> <mn>3</mn> <mo>-</mo> <mi>a</mi>
    <mn>7</mn> <mo>-</mo> <mi>a</mi> <mn>8</mn> <mo>-</mo> <mi>a</mi> <mn>9</mn></mrow></math>
    . If we are in a part of the image where <math alttext="a Baseline 1"><mrow><mi>a</mi>
    <mn>1</mn></mrow></math> , <math alttext="a Baseline 2"><mrow><mi>a</mi> <mn>2</mn></mrow></math>
    , and <math alttext="a Baseline 3"><mrow><mi>a</mi> <mn>3</mn></mrow></math> add
    up to the same as <math alttext="a Baseline 7"><mrow><mi>a</mi> <mn>7</mn></mrow></math>
    , <math alttext="a Baseline 8"><mrow><mi>a</mi> <mn>8</mn></mrow></math> , and
    <math alttext="a Baseline 9"><mrow><mi>a</mi> <mn>9</mn></mrow></math> , then
    the terms will cancel each other out and we will get 0\. However, if <math alttext="a
    Baseline 1"><mrow><mi>a</mi> <mn>1</mn></mrow></math> is greater than <math alttext="a
    Baseline 7"><mrow><mi>a</mi> <mn>7</mn></mrow></math> , <math alttext="a Baseline
    2"><mrow><mi>a</mi> <mn>2</mn></mrow></math> is greater than <math alttext="a
    Baseline 8"><mrow><mi>a</mi> <mn>8</mn></mrow></math> , and <math alttext="a Baseline
    3"><mrow><mi>a</mi> <mn>3</mn></mrow></math> is greater than <math alttext="a
    Baseline 9"><mrow><mi>a</mi> <mn>9</mn></mrow></math> , we will get a bigger number
    as a result. So this filter detects horizontal edges—more precisely, edges where
    we go from bright parts of the image at the top to darker parts at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Changing our filter to have the row of `1`s at the top and the `–1`s at the
    bottom would detect horizontal edges that go from dark to light. Putting the `1`s
    and `–1`s in columns versus rows would give us filters that detect vertical edges.
    Each set of weights will produce a different kind of outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a function to do this for one location, and check that it matches
    our result from before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: But note that we can’t apply it to the corner (e.g., location 0,0), since there
    isn’t a complete 3×3 square there.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping a Convolutional Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can map `apply_kernel()` across the coordinate grid. That is, we’ll be taking
    our 3×3 kernel and applying it to each 3×3 section of our image. For instance,
    [Figure 13-2](#nopad_conv) shows the positions a 3×3 kernel can be applied to
    in the first row of a 5×5 image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying a kernel across a grid](Images/dlcf_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. Applying a kernel across a grid
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To get a grid of coordinates, we can use a *nested list comprehension*, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Nested List Comprehensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nested list comprehensions are used a lot in Python, so if you haven’t seen
    them before, take a few minutes to make sure you understand what’s happening here,
    and experiment with writing your own nested list comprehensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the result of applying our kernel over a coordinate grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking good! Our top edges are black, and bottom edges are white (since they
    are the *opposite* of top edges). Now that our image contains negative numbers
    too, `matplotlib` has automatically changed our colors so that white is the smallest
    number in the image, black the highest, and zeros appear as gray.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can try the same thing for left edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in04.png)'
  prefs: []
  type: TYPE_IMG
- en: As we mentioned before, a convolution is the operation of applying such a kernel
    over a grid. Vincent Dumoulin and Francesco Visin’s paper [“A Guide to Convolution
    Arithmetic for Deep Learning”](https://oreil.ly/les1R) has many great diagrams
    showing how image kernels can be applied. [Figure 13-3](#three_ex_four_conv) is
    an example from the paper showing (at the bottom) a light blue 4×4 image with
    a dark blue 3×3 kernel being applied, creating a 2×2 green output activation map
    at the top.
  prefs: []
  type: TYPE_NORMAL
- en: '![Result of applying a 3x3 kernel to a 4x4 image](Images/dlcf_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent
    Dumoulin and Francesco Visin)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Look at the shape of the result. If the original image has a height of `h` and
    a width of `w`, how many 3×3 windows can we find? As you can see from the example,
    there are `h-2` by `w-2` windows, so the image we get as a result has a height
    of `h-2` and a width of `w-2`.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t implement this convolution function from scratch, but use PyTorch’s
    implementation instead (it is way faster than anything we could do in Python).
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Convolution is such an important and widely used operation that PyTorch has
    it built in. It’s called `F.conv2d` (recall that `F` is a fastai import from `torch.nn.functional`,
    as recommended by PyTorch). PyTorch docs tell us that it includes these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input`'
  prefs: []
  type: TYPE_NORMAL
- en: input tensor of shape `(minibatch, in_channels, iH, iW)`
  prefs: []
  type: TYPE_NORMAL
- en: '`weight`'
  prefs: []
  type: TYPE_NORMAL
- en: filters of shape `(out_channels, in_channels, kH, kW)`
  prefs: []
  type: TYPE_NORMAL
- en: Here `iH,iW` is the height and width of the image (i.e., `28,28`), and `kH,kW`
    is the height and width of our kernel (`3,3`). But apparently PyTorch is expecting
    rank-4 tensors for both these arguments, whereas currently we have only rank-2
    tensors (i.e., matrices, or arrays with two axes).
  prefs: []
  type: TYPE_NORMAL
- en: The reason for these extra axes is that PyTorch has a few tricks up its sleeve.
    The first trick is that PyTorch can apply a convolution to multiple images at
    the same time. That means we can call it on every item in a batch at once!
  prefs: []
  type: TYPE_NORMAL
- en: 'The second trick is that PyTorch can apply multiple kernels at the same time.
    So let’s create the diagonal-edge kernels too, and then stack all four of our
    edge kernels into a single tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To test this, we’ll need a `DataLoader` and a sample mini-batch. Let’s use
    the data block API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, fastai puts data on the GPU when using data blocks. Let’s move
    it to the CPU for our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'One batch contains 64 images, each of 1 channel, with 28×28 pixels. `F.conv2d`
    can handle multichannel (color) images too. A *channel* is a single basic color
    in an image—for regular full-color images, there are three channels, red, green,
    and blue. PyTorch represents an image as a rank-3 tensor, with these dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll see how to handle more than one channel later in this chapter. Kernels
    passed to `F.conv2d` need to be rank-4 tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`edge_kernels` is currently missing one of these: we need to tell PyTorch that
    the number of input channels in the kernel is one, which we can do by inserting
    an axis of size one (this is known as a *unit axis*) in the first location, where
    the PyTorch docs show `in_channels` is expected. To insert a unit axis into a
    tensor, we use the `unsqueeze` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is now the correct shape for `edge_kernels`. Let’s pass this all to `conv2d`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shape shows we have 64 images in the mini-batch, 4 kernels, and
    26×26 edge maps (we started with 28×28 images, but lost one pixel from each side
    as discussed earlier). We can see we get the same results as when we did this
    manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in05.png)'
  prefs: []
  type: TYPE_IMG
- en: The most important trick that PyTorch has up its sleeve is that it can use the
    GPU to do all this work in parallel—applying multiple kernels to multiple images,
    across multiple channels. Doing lots of work in parallel is critical to getting
    GPUs to work efficiently; if we did each of these operations one at a time, we’d
    often run hundreds of times slower (and if we used our manual convolution loop
    from the previous section, we’d be millions of times slower!). Therefore, to become
    a strong deep learning practitioner, one skill to practice is giving your GPU
    plenty of work to do at a time.
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice to not lose those two pixels on each axis. The way we do that
    is to add *padding*, which is simply additional pixels added around the outside
    of our image. Most commonly, pixels of zeros are added.
  prefs: []
  type: TYPE_NORMAL
- en: Strides and Padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With appropriate padding, we can ensure that the output activation map is the
    same size as the original image, which can make things a lot simpler when we construct
    our architectures. [Figure 13-4](#pad_conv) shows how adding padding allows us
    to apply the kernel in the image corners.
  prefs: []
  type: TYPE_NORMAL
- en: '![A convolution with padding](Images/dlcf_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. A convolution with padding
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6
    activation map, as we can see in [Figure 13-5](#four_by_five_conv).
  prefs: []
  type: TYPE_NORMAL
- en: '![4x4 kernel with 5x5 input and 2 pixels of padding](Images/dlcf_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy
    of Vincent Dumoulin and Francesco Visin)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we add a kernel of size `ks` by `ks` (with `ks` an odd number), the necessary
    padding on each side to keep the same shape is `ks//2`. An even number for `ks`
    would require a different amount of padding on the top/bottom and left/right,
    but in practice we almost never use an even filter size.
  prefs: []
  type: TYPE_NORMAL
- en: So far, when we have applied the kernel to the grid, we have moved it one pixel
    over at a time. But we can jump further; for instance, we could move over two
    pixels after each kernel application, as in [Figure 13-6](#three_by_five_conv).
    This is known as a *stride-2* convolution. The most common kernel size in practice
    is 3×3, and the most common padding is 1\. As you’ll see, stride-2 convolutions
    are useful for decreasing the size of our outputs, and stride-1 convolutions are
    useful for adding layers without changing the output size.
  prefs: []
  type: TYPE_NORMAL
- en: '![3x3 kernel with 5x5 input, stride 2 convolution, and 1 pixel of padding](Images/dlcf_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel
    of padding (courtesy of Vincent Dumoulin and Francesco Visin)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In an image of size `h` by `w`, using a padding of 1 and a stride of 2 will
    give us a result of size `(h+1)//2` by `(w+1)//2`. The general formula for each
    dimension is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: where `pad` is the padding, `ks` is the size of our kernel, and `stride` is
    the stride.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at how the pixel values of the result of our convolutions
    are computed.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Convolution Equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To explain the math behind convolutions, fast.ai student Matt Kleinsmith came
    up with the very clever idea of showing [CNNs from different viewpoints](https://oreil.ly/wZuBs).
    In fact, it’s so clever, and so helpful, we’re going to show it here too!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s our 3×3-pixel image, with each pixel labeled with a letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The image](Images/dlcf_13in06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And here’s our kernel, with each weight labeled with a Greek letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The kernel](Images/dlcf_13in07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the filter fits in the image four times, we have four results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The activations](Images/dlcf_13in08.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 13-7](#apply_kernel) shows how we applied the kernel to each section
    of the image to yield each result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying the kernel](Images/dlcf_1307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-7\. Applying the kernel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The equation view is in [Figure 13-8](#eq_view).
  prefs: []
  type: TYPE_NORMAL
- en: '![The equation](Images/dlcf_1308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-8\. The equation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the bias term, *b*, is the same for each section of the image. You
    can consider the bias as part of the filter, just as the weights (α, β, γ, δ)
    are part of the filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an interesting insight—a convolution can be represented as a special
    kind of matrix multiplication, as illustrated in [Figure 13-9](#conv_matmul).
    The weight matrix is just like the ones from traditional neural networks. However,
    this weight matrix has two special properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The zeros shown in gray are untrainable. This means that they’ll stay zero throughout
    the optimization process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some of the weights are equal, and while they are trainable (i.e., changeable),
    they must remain equal. These are called *shared weights*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The zeros correspond to the pixels that the filter can’t touch. Each row of
    the weight matrix corresponds to one application of the filter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution as matrix multiplication](Images/dlcf_1309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-9\. Convolution as matrix multiplication
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we understand what convolutions are, let’s use them to build a neural
    net.
  prefs: []
  type: TYPE_NORMAL
- en: Our First Convolutional Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no reason to believe that some particular edge filters are the most
    useful kernels for image recognition. Furthermore, we’ve seen that in later layers,
    convolutional kernels become complex transformations of features from lower levels,
    but we don’t have a good idea of how to manually construct these.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, it would be best to learn the values of the kernels. We already know
    how to do this—SGD! In effect, the model will learn the features that are useful
    for classification. When we use convolutions instead of (or in addition to) regular
    linear layers, we create a *convolutional neural network* (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s go back to the basic neural network we had in [Chapter 4](ch04.xhtml#chapter_mnist_basics).
    It was defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can view a model’s definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We now want to create a similar architecture to this linear model, but using
    convolutional layers instead of linear. `nn.Conv2d` is the module equivalent of
    `F.conv2d`. It’s more convenient than `F.conv2d` when creating an architecture,
    because it creates the weight matrix for us automatically when we instantiate
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a possible architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: One thing to note here is that we didn’t need to specify `28*28` as the input
    size. That’s because a linear layer needs a weight in the weight matrix for every
    pixel, so it needs to know how many pixels there are, but a convolution is applied
    over each pixel automatically. The weights depend only on the number of input
    and output channels and the kernel size, as we saw in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about what the output shape is going to be; then let’s try it and see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This is not something we can use to do classification, since we need a single
    output activation per image, not a 28×28 map of activations. One way to deal with
    this is to use enough stride-2 convolutions such that the final layer is size
    1\. After one stride-2 convolution, the size will be 14×14; after two, it will
    be 7×7; then 4×4, 2×2, and finally size 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try that now. First, we’ll define a function with the basic parameters
    we’ll use in each convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Refactoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refactoring parts of your neural networks like this makes it much less likely
    you’ll get errors due to inconsistencies in your architectures, and makes it more
    obvious to the reader which parts of your layers are actually changing.
  prefs: []
  type: TYPE_NORMAL
- en: When we use a stride-2 convolution, we often increase the number of features
    at the same time. This is because we’re decreasing the number of activations in
    the activation map by a factor of 4; we don’t want to decrease the capacity of
    a layer by too much at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jargon: Channels and Features'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These two terms are largely used interchangeably and refer to the size of the
    second axis of a weight matrix, which is the number of activations per grid cell
    after a convolution. *Features* is never used to refer to the input data, but
    *channels* can refer to either the input data (generally, channels are colors)
    or activations inside the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we can build a simple CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Jeremy Says
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I like to add comments like the ones here after each convolution to show how
    large the activation map will be after each layer. These comments assume that
    the input size is 28×28.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the network outputs two activations, which map to the two possible levels
    in our labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our `Learner`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To see exactly what’s going on in the model, we can use `summary`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note that the output of the final `Conv2d` layer is `64x2x1x1`. We need to remove
    those extra `1x1` axes; that’s what `Flatten` does. It’s basically the same as
    PyTorch’s `squeeze` method, but as a module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if this trains! Since this is a deeper network than we’ve built from
    scratch before, we’ll use a lower learning rate and more epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.072684 | 0.045110 | 0.990186 | 00:05 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.022580 | 0.030775 | 0.990186 | 00:05 |'
  prefs: []
  type: TYPE_TB
- en: Success! It’s getting closer to the `resnet18` result we had, although it’s
    not quite there yet, and it’s taking more epochs, and we’re needing to use a lower
    learning rate. We still have a few more tricks to learn, but we’re getting closer
    and closer to being able to create a modern CNN from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Convolution Arithmetic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can see from the summary that we have an input of size `64x1x28x28`. The
    axes are `batch,channel,height,width`. This is often represented as `NCHW` (where
    `N` refers to batch size). TensorFlow, on the other hand, uses `NHWC` axis order.
    Here is the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'So we have 1 input channel, 4 output channels, and a 3×3 kernel. Let’s check
    the weights of the first convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary shows we have 40 parameters, and `4*1*3*3` is 36\. What are the
    other four parameters? Let’s see what the bias contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this information to clarify our statement in the previous section:
    “When we use a stride-2 convolution, we often increase the number of features
    because we’re decreasing the number of activations in the activation map by a
    factor of 4; we don’t want to decrease the capacity of a layer by too much at
    a time.”'
  prefs: []
  type: TYPE_NORMAL
- en: There is one bias for each channel. (Sometimes channels are called *features*
    or *filters* when they are not input channels.) The output shape is `64x4x14x14`,
    and this will therefore become the input shape to the next layer. The next layer,
    according to `summary`, has 296 parameters. Let’s ignore the batch axis to keep
    things simple. So, for each of `14*14=196` locations, we are multiplying `296-8=288`
    weights (ignoring the bias for simplicity), so that’s `196*288=56_448` multiplications
    at this layer. The next layer will have `7*7*(1168-16)=56_448` multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: What happened here is that our stride-2 convolution halved the *grid size* from
    `14x14` to `7x7`, and we doubled the *number of filters* from 8 to 16, resulting
    in no overall change in the amount of computation. If we left the number of channels
    the same in each stride-2 layer, the amount of computation being done in the net
    would get less and less as it gets deeper. But we know that the deeper layers
    have to compute semantically rich features (such as eyes or fur), so we wouldn’t
    expect that doing *less* computation would make sense.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of this is based on receptive fields.
  prefs: []
  type: TYPE_NORMAL
- en: Receptive Fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *receptive field* is the area of an image that is involved in the calculation
    of a layer. On the [book’s website](https://book.fast.ai), you’ll find an Excel
    spreadsheet called *conv-example.xlsx* that shows the calculation of two stride-2
    convolutional layers using an MNIST digit. Each layer has a single kernel. [Figure 13-10](#preced1)
    shows what we see if we click one of the cells in the *conv2* section, which shows
    the output of the second convolutional layer, and click *trace precedents*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Immediate precedents of conv2 layer](Images/dlcf_1310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-10\. Immediate precedents of Conv2 layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, the cell with the green border is the cell we clicked, and the blue highlighted
    cells are its *precedents*—the cells used to calculate its value. These cells
    are the corresponding 3×3 area of cells from the input layer (on the left), and
    the cells from the filter (on the right). Let’s now click *trace precedents* again,
    to see what cells are used to calculate these inputs. [Figure 13-11](#preced2)
    shows what happens.
  prefs: []
  type: TYPE_NORMAL
- en: '![Secondary precedents of conv2 layer](Images/dlcf_1311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-11\. Secondary precedents of Conv2 layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example, we have just two convolutional layers, each of stride 2, so
    this is now tracing right back to the input image. We can see that a 7×7 area
    of cells in the input layer is used to calculate the single green cell in the
    Conv2 layer. This 7×7 area is the *receptive field* in the input of the green
    activation in Conv2\. We can also see that a second filter kernel is needed now,
    since we have two layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you see from this example, the deeper we are in the network (specifically,
    the more stride-2 convs we have before a layer), the larger the receptive field
    for an activation in that layer is. A large receptive field means that a large
    amount of the input image is used to calculate each activation in that layer.
    We now know that in the deeper layers of the network, we have semantically rich
    features, corresponding to larger receptive fields. Therefore, we’d expect that
    we’d need more weights for each of our features to handle this increasing complexity.
    This is another way of saying the same thing we mentioned in the previous section:
    when we introduce a stride-2 conv in our network, we should also increase the
    number of channels.'
  prefs: []
  type: TYPE_NORMAL
- en: When writing this particular chapter, we had a lot of questions we needed answers
    for, to be able to explain CNNs to you as best we could. Believe it or not, we
    found most of the answers on Twitter. We’re going to take a quick break to talk
    to you about that now, before we move on to color images.
  prefs: []
  type: TYPE_NORMAL
- en: A Note About Twitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are not, to say the least, big users of social networks in general. But our
    goal in writing this book is to help you become the best deep learning practitioner
    you can, and we would be remiss not to mention how important Twitter has been
    in our own deep learning journeys.
  prefs: []
  type: TYPE_NORMAL
- en: 'You see, there’s another part of Twitter, far away from Donald Trump and the
    Kardashians, where deep learning researchers and practitioners talk shop every
    day. As we were writing this section, Jeremy wanted to double-check that what
    we were saying about stride-2 convolutions was accurate, so he asked on Twitter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![twitter 1](Images/dlcf_13in09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A few minutes later, this answer popped up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![twitter 2](Images/dlcf_13in10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Christian Szegedy is the first author of [Inception](https://oreil.ly/hGE_Y),
    the 2014 ImageNet winner, and source of many key insights used in modern neural
    networks. Two hours later, this appeared:'
  prefs: []
  type: TYPE_NORMAL
- en: '![twitter 3](Images/dlcf_13in11.png)'
  prefs: []
  type: TYPE_IMG
- en: Do you recognize that name? You saw it in [Chapter 2](ch02.xhtml#chapter_production),
    when we were talking about the Turing Award winners who established the foundations
    of deep learning today!
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy also asked on Twitter for help checking that our description of label
    smoothing in [Chapter 7](ch07.xhtml#chapter_sizing_and_tta) was accurate, and
    got a response again directly from Christian Szegedy (label smoothing was originally
    introduced in the Inception paper):'
  prefs: []
  type: TYPE_NORMAL
- en: '![twitter 4](Images/dlcf_13in12.png)'
  prefs: []
  type: TYPE_IMG
- en: Many of the top people in deep learning today are Twitter regulars, and are
    very open about interacting with the wider community. One good way to get started
    is to look at a list of Jeremy’s [recent Twitter likes](https://oreil.ly/sqOI7),
    or [Sylvain’s](https://oreil.ly/VWYHY). That way, you can see a list of Twitter
    users whom we think have interesting and useful things to say.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter is the main way we both stay up to date with interesting papers, software
    releases, and other deep learning news. For making connections with the deep learning
    community, we recommend getting involved both in the [fast.ai forums](https://forums.fast.ai)
    and on Twitter.
  prefs: []
  type: TYPE_NORMAL
- en: That said, let’s get back to the meat of this chapter. Up until now, we have
    shown you examples of pictures in only black and white, with one value per pixel.
    In practice, most colored images have three values per pixel to define their color.
    We’ll look at working with color images next.
  prefs: []
  type: TYPE_NORMAL
- en: Color Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A color picture is a rank-3 tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first axis contains the channels red, green, and blue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in14.png)'
  prefs: []
  type: TYPE_IMG
- en: We saw what the convolution operation was for one filter on one channel of the
    image (our examples were done on a square). A convolutional layer will take an
    image with a certain number of channels (three for the first layer for regular
    RGB color images) and output an image with a different number of channels. As
    with our hidden size that represented the numbers of neurons in a linear layer,
    we can decide to have as many filters as we want, and each will be able to specialize
    (some to detect horizontal edges, others to detect vertical edges, and so forth)
    to give something like the examples we studied in [Chapter 2](ch02.xhtml#chapter_production).
  prefs: []
  type: TYPE_NORMAL
- en: In one sliding window, we have a certain number of channels and we need as many
    filters (we don’t use the same kernel for all the channels). So our kernel doesn’t
    have a size of 3×3, but `ch_in` (for channels in) by 3×3\. On each channel, we
    multiply the elements of our window by the elements of the corresponding filter,
    and then sum the results (as we saw before) and sum over all the filters. In the
    example given in [Figure 13-12](#rgbconv), the result of our conv layer on that
    window is red + green + blue.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution over an RGB image](Images/dlcf_1312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-12\. Convolution over an RGB image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, in order to apply a convolution to a color picture, we require a kernel
    tensor with a size that matches the first axis. At each location, the corresponding
    parts of the kernel and the image patch are multiplied together.
  prefs: []
  type: TYPE_NORMAL
- en: These are then all added together to produce a single number for each grid location
    for each output feature, as shown in [Figure 13-13](#rgbconv2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding the RGB filters](Images/dlcf_1313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-13\. Adding the RGB filters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Then we have `ch_out` filters like this, so in the end, the result of our convolutional
    layer will be a batch of images with `ch_out` channels and a height and width
    given by the formula outlined earlier. This give us `ch_out` tensors of size `ch_in
    x ks x ks` that we represent in one big tensor of four dimensions. In PyTorch,
    the order of the dimensions for those weights is `ch_out x ch_in x ks x ks`.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we may want to have a bias for each filter. In the preceding example,
    the final result for our convolutional layer would be <math alttext="y Subscript
    upper R Baseline plus y Subscript upper G Baseline plus y Subscript upper B Baseline
    plus b"><mrow><msub><mi>y</mi> <mi>R</mi></msub> <mo>+</mo> <msub><mi>y</mi> <mi>G</mi></msub>
    <mo>+</mo> <msub><mi>y</mi> <mi>B</mi></msub> <mo>+</mo> <mi>b</mi></mrow></math>
    in that case. As in a linear layer, there are as many biases as we have kernels,
    so the bias is a vector of size `ch_out`.
  prefs: []
  type: TYPE_NORMAL
- en: No special mechanisms are required when setting up a CNN for training with color
    images. Just make sure your first layer has three inputs.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of ways of processing color images. For instance, you can change
    them to black and white, change from RGB to HSV (hue, saturation, and value) color
    space, and so forth. In general, it turns out experimentally that changing the
    encoding of colors won’t make any difference to your model results, as long as
    you don’t lose information in the transformation. So, transforming to black and
    white is a bad idea, since it removes the color information entirely (and this
    can be critical; for instance, a pet breed may have a distinctive color); but
    converting to HSV generally won’t make any difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you know what those pictures in [Chapter 1](ch01.xhtml#chapter_intro) of
    “what a neural net learns” from the [Zeiler and Fergus paper](https://oreil.ly/Y6dzZ)
    mean! As a reminder, this is their picture of some of the layer 1 weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Layer 1 kernels found by Zeiler and Fergus](Images/dlcf_13in15.png)'
  prefs: []
  type: TYPE_IMG
- en: This is taking the three slices of the convolutional kernel, for each output
    feature, and displaying them as images. We can see that even though the creators
    of the neural net never explicitly created kernels to find edges, for instance,
    the neural net automatically discovered these features using SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see how we can train these CNNs, and show you all the techniques fastai
    uses under the hood for efficient training.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Training Stability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we are so good at recognizing 3s from 7s, let’s move on to something
    harder—recognizing all 10 digits. That means we’ll need to use `MNIST` instead
    of `MNIST_SAMPLE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is in two folders named *training* and *testing*, so we have to tell
    `GrandparentSplitter` about that (it defaults to `train` and `valid`). We do that
    in the `get_dls` function, which we define to make it easy to change our batch
    size later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember, it’s always a good idea to look at your data before you use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in16.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have our data ready, we can train a simple model on it.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, we built a model based on a `conv` function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s start with a basic CNN as a baseline. We’ll use the same as one as earlier,
    but with one tweak: we’ll use more activations. Since we have more numbers to
    differentiate, we’ll likely need to learn more filters.'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, we generally want to double the number of filters each time
    we have a stride-2 layer. One way to increase the number of filters throughout
    our network is to double the number of activations in the first layer—then every
    layer after that will end up twice as big as in the previous version as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'But this creates a subtle problem. Consider the kernel that is being applied
    to each pixel. By default, we use a 3×3-pixel kernel. Therefore, there are a total
    of 3 × 3 = 9 pixels that the kernel is being applied to at each location. Previously,
    our first layer had four output filters. So four values were being computed from
    nine pixels at each location. Think about what happens if we double this output
    to eight filters. Then when we apply our kernel, we will be using nine pixels
    to calculate eight numbers. That means it isn’t really learning much at all: the
    output size is almost the same as the input size. Neural networks will create
    useful features only if they’re forced to do so—that is, if the number of outputs
    from an operation is significantly smaller than the number of inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To fix this, we can use a larger kernel in the first layer. If we use a kernel
    of 5×5 pixels, 25 pixels are being used at each kernel application. Creating eight
    filters from this will mean the neural net will have to find some useful features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'As you’ll see in a moment, we can look inside our models while they’re training
    in order to try to find ways to make them train better. To do this, we use the
    `ActivationStats` callback, which records the mean, standard deviation, and histogram
    of activations of every trainable layer (as we’ve seen, callbacks are used to
    add behavior to the training loop; we’ll explore how they work in [Chapter 16](ch16.xhtml#chapter_accel_sgd)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to train quickly, so that means training at a high learning rate. Let’s
    see how we go at 0.06:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2.307071 | 2.305865 | 0.113500 | 00:16 |'
  prefs: []
  type: TYPE_TB
- en: This didn’t train at all well! Let’s find out why.
  prefs: []
  type: TYPE_NORMAL
- en: One handy feature of the callbacks passed to `Learner` is that they are made
    available automatically, with the same name as the callback class, except in `camel_case`.
    So, our `ActivationStats` callback can be accessed through `activation_stats`.
    I’m sure you remember `learn.recorder`…can you guess how that is implemented?
    That’s right, it’s a callback called `Recorder`!
  prefs: []
  type: TYPE_NORMAL
- en: '`ActivationStats` includes some handy utilities for plotting the activations
    during training. `plot_layer_stats(*idx*)` plots the mean and standard deviation
    of the activations of layer number *`idx`*, along with the percentage of activations
    near zero. Here’s the first layer’s plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally our model should have a consistent, or at least smooth, mean and
    standard deviation of layer activations during training. Activations near zero
    are particularly problematic, because it means we have computation in the model
    that’s doing nothing at all (since multiplying by zero gives zero). When you have
    some zeros in one layer, they will therefore generally carry over to the next
    layer…which will then create more zeros. Here’s the penultimate layer of our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in18.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the problems get worse toward the end of the network, as the instability
    and zero activations compound over layers. Let’s look at what we can do to make
    training more stable.
  prefs: []
  type: TYPE_NORMAL
- en: Increase Batch Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to make training more stable is to increase the batch size. Larger
    batches have gradients that are more accurate, since they’re calculated from more
    data. On the downside, though, a larger batch size means fewer batches per epoch,
    which means fewer opportunities for your model to update weights. Let’s see if
    a batch size of 512 helps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2.309385 | 2.302744 | 0.113500 | 00:08 |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s see what the penultimate layer looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in19.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, we’ve got most of our activations near zero. Let’s see what else we can
    do to improve training stability.
  prefs: []
  type: TYPE_NORMAL
- en: 1cycle Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our initial weights are not well suited to the task we’re trying to solve.
    Therefore, it is dangerous to begin training with a high learning rate: we may
    very well make the training diverge instantly, as we’ve seen. We probably don’t
    want to end training with a high learning rate either, so that we don’t skip over
    a minimum. But we want to train at a high learning rate for the rest of the training
    period, because we’ll be able to train more quickly that way. Therefore, we should
    change the learning rate during training, from low, to high, and then back to
    low again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Leslie Smith (yes, the same guy who invented the learning rate finder!) developed
    this idea in his article [“Super-Convergence: Very Fast Training of Neural Networks
    Using Large Learning Rates”](https://oreil.ly/EB8NU). He designed a schedule for
    learning rate separated into two phases: one where the learning rate grows from
    the minimum value to the maximum value (*warmup*), and one where it decreases
    back to the minimum value (*annealing*). Smith called this combination of approaches
    *1cycle training*.'
  prefs: []
  type: TYPE_NORMAL
- en: '1cycle training allows us to use a much higher maximum learning rate than other
    types of training, which gives two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: By training with higher learning rates, we train faster—a phenomenon Smith calls
    *super-convergence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By training with higher learning rates, we overfit less because we skip over
    the sharp local minima to end up in a smoother (and therefore more generalizable)
    part of the loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second point is an interesting and subtle one; it is based on the observation
    that a model that generalizes well is one whose loss would not change very much
    if you changed the input by a small amount. If a model trains at a large learning
    rate for quite a while, and can find a good loss when doing so, it must have found
    an area that also generalizes well, because it is jumping around a lot from batch
    to batch (that is basically the definition of a high learning rate). The problem
    is that, as we have discussed, just jumping to a high learning rate is more likely
    to result in diverging losses, rather than seeing your losses improve. So we don’t
    jump straight to a high learning rate. Instead, we start at a low learning rate,
    where our losses do not diverge, and we allow the optimizer to gradually find
    smoother and smoother areas of our parameters by gradually going to higher and
    higher learning rates.
  prefs: []
  type: TYPE_NORMAL
- en: Then, once we have found a nice smooth area for our parameters, we want to find
    the very best part of that area, which means we have to bring our learning rates
    down again. This is why 1cycle training has a gradual learning rate warmup, and
    a gradual learning rate cooldown. Many researchers have found that in practice
    this approach leads to more accurate models and trains more quickly. That is why
    it is the approach that is used by default for `fine_tune` in fastai.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 16](ch16.xhtml#chapter_accel_sgd), we’ll learn all about *momentum*
    in SGD. Briefly, momentum is a technique whereby the optimizer takes a step not
    only in the direction of the gradients, but also that continues in the direction
    of previous steps. Leslie Smith introduced the idea of *cyclical momentum* in
    [“A Disciplined Approach to Neural Network Hyper-Parameters: Part 1”](https://oreil.ly/oL7GT).
    It suggests that the momentum varies in the opposite direction of the learning
    rate: when we are at high learning rates, we use less momentum, and we use more
    again in the annealing phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use 1cycle training in fastai by calling `fit_one_cycle`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.210838 | 0.084827 | 0.974300 | 00:08 |'
  prefs: []
  type: TYPE_TB
- en: We’re finally making some progress! It’s giving us a reasonable accuracy now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view the learning rate and momentum throughout training by calling `plot_sched`
    on `learn.recorder`. `learn.recorder` (as the name suggests) records everything
    that happens during training, including losses, metrics, and hyperparameters such
    as learning rate and momentum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Smith’s original 1cycle paper used a linear warmup and linear annealing. As
    you can see, we adapted the approach in fastai by combining it with another popular
    approach: cosine annealing. `fit_one_cycle` provides the following parameters
    you can adjust:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lr_max`'
  prefs: []
  type: TYPE_NORMAL
- en: The highest learning rate that will be used (this can also be a list of learning
    rates for each layer group, or a Python `slice` object containing the first and
    last layer group learning rates)
  prefs: []
  type: TYPE_NORMAL
- en: '`div`'
  prefs: []
  type: TYPE_NORMAL
- en: How much to divide `lr_max` by to get the starting learning rate
  prefs: []
  type: TYPE_NORMAL
- en: '`div_final`'
  prefs: []
  type: TYPE_NORMAL
- en: How much to divide `lr_max` by to get the ending learning rate
  prefs: []
  type: TYPE_NORMAL
- en: '`pct_start`'
  prefs: []
  type: TYPE_NORMAL
- en: What percentage of the batches to use for the warmup
  prefs: []
  type: TYPE_NORMAL
- en: '`moms`'
  prefs: []
  type: TYPE_NORMAL
- en: A tuple `(*mom1*,*mom2*,*mom3*)`, where *`mom1`* is the initial momentum, *`mom2`*
    is the minimum momentum, and *`mom3`* is the final momentum
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at our layer stats again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The percentage of nonzero weights is getting much better, although it’s still
    quite high. We can see even more about what’s going on in our training by using
    `color_dim`, passing it a layer index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in22.png)'
  prefs: []
  type: TYPE_IMG
- en: '`color_dim` was developed by fast.ai in conjunction with a student, Stefano
    Giomo. Giomo, who refers to the idea as the *colorful dimension*, provides an
    [in-depth explanation](https://oreil.ly/bPXGw) of the history and details behind
    the method. The basic idea is to create a histogram of the activations of a layer,
    which we would hope would follow a smooth pattern such as the normal distribution
    ([Figure 13-14](#colorful_dist)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram in colorful dimension](Images/dlcf_1314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-14\. Histogram in colorful dimension (courtesy of Stefano Giomo)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To create `color_dim`, we take the histogram shown on the left here and convert
    it into just the colored representation shown at the bottom. Then we flip it on
    its side, as shown on the right. We found that the distribution is clearer if
    we take the log of the histogram values. Then, Giomo describes:'
  prefs: []
  type: TYPE_NORMAL
- en: The final plot for each layer is made by stacking the histogram of the activations
    from each batch along the horizontal axis. So each vertical slice in the visualisation
    represents the histogram of activations for a single batch. The color intensity
    corresponds to the height of the histogram; in other words, the number of activations
    in each histogram bin.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 13-15](#colorful_summ) shows how this all fits together.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summary of the colorful dimension](Images/dlcf_1315.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-15\. Summary of the colorful dimension (courtesy of Stefano Giomo)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This illustrates why log(*f*) is more colorful than *f* when *f* follows a normal
    distribution, because taking a log changes the Gaussian curve in a quadratic,
    which isn’t as narrow.
  prefs: []
  type: TYPE_NORMAL
- en: 'So with that in mind, let’s take another look at the result for the penultimate
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in23.png)'
  prefs: []
  type: TYPE_IMG
- en: This shows a classic picture of “bad training.” We start with nearly all activations
    at zero—that’s what we see at the far left, with all the dark blue. The bright
    yellow at the bottom represents the near-zero activations. Then, over the first
    few batches, we see the number of nonzero activations exponentially increasing.
    But it goes too far and collapses! We see the dark blue return, and the bottom
    becomes bright yellow again. It almost looks like training restarts from scratch.
    Then we see the activations increase again and collapse again. After repeating
    this a few times, eventually we see a spread of activations throughout the range.
  prefs: []
  type: TYPE_NORMAL
- en: It’s much better if training can be smooth from the start. The cycles of exponential
    increase and then collapse tend to result in a lot of near-zero activations, resulting
    in slow training and poor final results. One way to solve this problem is to use
    batch normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To fix the slow training and poor final results we ended up with in the previous
    section, we need to fix the initial large percentage of near-zero activations,
    and then try to maintain a good distribution of activations throughout training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sergey Ioffe and Christian Szegedy presented a solution to this problem in
    the 2015 paper [“Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift”](https://oreil.ly/MTZJL). In the abstract, they describe
    just the problem that we’ve seen:'
  prefs: []
  type: TYPE_NORMAL
- en: Training Deep Neural Networks is complicated by the fact that the distribution
    of each layer’s inputs changes during training, as the parameters of the previous
    layers change. This slows down the training by requiring lower learning rates
    and careful parameter initialization…We refer to this phenomenon as internal covariate
    shift, and address the problem by normalizing layer inputs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Their solution, they say is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Making normalization a part of the model architecture and performing the normalization
    for each training mini-batch. Batch Normalization allows us to use much higher
    learning rates and be less careful about initialization.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The paper caused great excitement as soon as it was released, because it included
    the chart in [Figure 13-16](#batchnorm), which clearly demonstrated that batch
    normalization could train a model that was even more accurate than the current
    state of the art (the *Inception* architecture) and around 5× faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Impact of batch normalization](Images/dlcf_1316.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-16\. Impact of batch normalization (courtesy of Sergey Ioffe and Christian
    Szegedy)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Batch normalization (often called *batchnorm*) works by taking an average of
    the mean and standard deviations of the activations of a layer and using those
    to normalize the activations. However, this can cause problems because the network
    might want some activations to be really high in order to make accurate predictions.
    So they also added two learnable parameters (meaning they will be updated in the
    SGD step), usually called `gamma` and `beta`. After normalizing the activations
    to get some new activation vector `y`, a batchnorm layer returns `gamma*y + beta`.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s why our activations can have any mean or variance, independent from
    the mean and standard deviation of the results of the previous layer. Those statistics
    are learned separately, making training easier on our model. The behavior is different
    during training and validation: during training we use the mean and standard deviation
    of the batch to normalize the data, while during validation we instead use a running
    mean of the statistics calculated during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add a batchnorm layer to `conv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'and fit our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.130036 | 0.055021 | 0.986400 | 00:10 |'
  prefs: []
  type: TYPE_TB
- en: 'That’s a great result! Let’s take a look at `color_dim`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/dlcf_13in24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is just what we hope to see: a smooth development of activations, with
    no “crashes.” Batchnorm has really delivered on its promise here! In fact, batchnorm
    has been so successful that we see it (or something very similar) in nearly all
    modern neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting observation about models containing batch normalization layers
    is that they tend to generalize better than models that don’t contain them. Although
    we haven’t as yet seen a rigorous analysis of what’s going on here, most researchers
    believe that the reason is that batch normalization adds some extra randomness
    to the training process. Each mini-batch will have a somewhat different mean and
    standard deviation than other mini-batches. Therefore, the activations will be
    normalized by different values each time. In order for the model to make accurate
    predictions, it will have to learn to become robust to these variations. In general,
    adding additional randomization to the training process often helps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since things are going so well, let’s train for a few more epochs and see how
    it goes. In fact, let’s *increase* the learning rate, since the abstract of the
    batchnorm paper claimed we should be able to “train at much higher learning rates”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.191731 | 0.121738 | 0.960900 | 00:11 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.083739 | 0.055808 | 0.981800 | 00:10 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.053161 | 0.044485 | 0.987100 | 00:10 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.034433 | 0.030233 | 0.990200 | 00:10 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.017646 | 0.025407 | 0.991200 | 00:10 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.183244 | 0.084025 | 0.975800 | 00:13 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.080774 | 0.067060 | 0.978800 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.050215 | 0.062595 | 0.981300 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.030020 | 0.030315 | 0.990700 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.015131 | 0.025148 | 0.992100 | 00:12 |'
  prefs: []
  type: TYPE_TB
- en: At this point, I think it’s fair to say we know how to recognize digits! It’s
    time to move on to something harder…
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve seen that convolutions are just a type of matrix multiplication, with
    two constraints on the weight matrix: some elements are always zero, and some
    elements are tied (forced to always have the same value). In [Chapter 1](ch01.xhtml#chapter_intro),
    we saw the eight requirements from the 1986 book *Parallel Distributed Processing*;
    one of them was “A pattern of connectivity among units.” That’s exactly what these
    constraints do: they enforce a certain pattern of connectivity.'
  prefs: []
  type: TYPE_NORMAL
- en: These constraints allow us to use far fewer parameters in our model, without
    sacrificing the ability to represent complex visual features. That means we can
    train deeper models faster, with less overfitting. Although the universal approximation
    theorem shows that it should be *possible* to represent anything in a fully connected
    network in one hidden layer, we’ve seen now that in *practice* we can train much
    better models by being thoughtful about network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions are by far the most common pattern of connectivity we see in neural
    nets (along with regular linear layers, which we refer to as *fully connected*),
    but it’s likely that many more will be discovered.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve also seen how to interpret the activations of layers in the network to
    see whether training is going well or not, and how batchnorm helps regularize
    the training and makes it smoother. In the next chapter, we will use both of those
    layers to build the most popular architecture in computer vision: a residual network.'
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a feature?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write out the convolutional kernel matrix for a top edge detector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write out the mathematical operation applied by a 3×3 kernel to a single pixel
    in an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the value of a convolutional kernel applied to a 3×3 matrix of zeros?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is padding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is stride?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a nested list comprehension to complete any task that you choose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the shapes of the `input` and `weight` parameters to PyTorch’s 2D convolution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a channel?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the relationship between a convolution and a matrix multiplication?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a convolutional neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the benefit of refactoring parts of your neural network definition?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `Flatten`? Where does it need to be included in the MNIST CNN? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does NCHW mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does the third layer of the MNIST CNN have `7*7*(1168-16)` multiplications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a receptive field?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the size of the receptive field of an activation after two stride-2
    convolutions? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run *conv-example.xlsx* yourself and experiment with *trace precedents*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have a look at Jeremy or Sylvain’s list of recent Twitter “likes,” and see if
    you find any interesting resources or ideas there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is a color image represented as a tensor?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a convolution work with a color input?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What method can we use to see that data in `DataLoaders`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we double the number of filters after each stride-2 conv?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we use a larger kernel in the first conv with MNIST (with `simple_cnn`)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What information does `ActivationStats` save for each layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we access a learner’s callback after training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the three statistics plotted by `plot_layer_stats`? What does the x-axis
    represent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are activations near zero problematic?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the upsides and downsides of training with a larger batch size?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why should we avoid using a high learning rate at the start of training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is 1cycle training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the benefits of training with a high learning rate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we want to use a low learning rate at the end of training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is cyclical momentum?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What callback tracks hyperparameter values during training (along with other
    information)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does one column of pixels in the `color_dim` plot represent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does “bad training” look like in `color_dim`? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What trainable parameters does a batch normalization layer contain?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What statistics are used to normalize in batch normalization during training?
    How about during validation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do models with batch normalization layers generalize better?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What features other than edge detectors have been used in computer vision (especially
    before deep learning became popular)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Other normalization layers are available in PyTorch. Try them out and see what
    works best. Learn about why other normalization layers have been developed and
    how they differ from batch normalization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try moving the activation function after the batch normalization layer in `conv`.
    Does it make a difference? See what you can find out about what order is recommended
    and why.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
