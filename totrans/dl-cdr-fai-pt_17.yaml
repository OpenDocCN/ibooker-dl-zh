- en: Chapter 13\. Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。卷积神经网络
- en: In [Chapter 4](ch04.xhtml#chapter_mnist_basics), we learned how to create a
    neural network recognizing images. We were able to achieve a bit over 98% accuracy
    at distinguishing 3s from 7s—but we also saw that fastai’s built-in classes were
    able to get close to 100%. Let’s start trying to close the gap.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#chapter_mnist_basics)中，我们学习了如何创建一个识别图像的神经网络。我们能够在区分3和7方面达到98%以上的准确率，但我们也看到fastai内置的类能够接近100%。让我们开始尝试缩小这个差距。
- en: In this chapter, we will begin by digging into what convolutions are and building
    a CNN from scratch. We will then study a range of techniques to improve training
    stability and learn all the tweaks the library usually applies for us to get great
    results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先深入研究卷积是什么，并从头开始构建一个CNN。然后，我们将研究一系列技术来改善训练稳定性，并学习库通常为我们应用的所有调整，以获得出色的结果。
- en: The Magic of Convolutions
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积的魔力
- en: One of the most powerful tools that machine learning practitioners have at their
    disposal is *feature engineering*. A *feature* is a transformation of the data
    that is designed to make it easier to model. For instance, the `add_datepart`
    function that we used for our tabular dataset preprocessing in [Chapter 9](ch09.xhtml#chapter_tabular)
    added date features to the Bulldozers dataset. What kinds of features might we
    be able to create from images?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习从业者手中最强大的工具之一是*特征工程*。*特征*是数据的一种转换，旨在使其更容易建模。例如，我们在[第9章](ch09.xhtml#chapter_tabular)中用于我们表格数据集预处理的`add_datepart`函数向Bulldozers数据集添加了日期特征。我们能够从图像中创建哪些特征呢？
- en: 'Jargon: Feature Engineering'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：特征工程
- en: Creating new transformations of the input data in order to make it easier to
    model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 创建输入数据的新转换，以使其更容易建模。
- en: In the context of an image, a feature is a visually distinctive attribute. For
    example, the number 7 is characterized by a horizontal edge near the top of the
    digit, and a top-right to bottom-left diagonal edge underneath that. On the other
    hand, the number 3 is characterized by a diagonal edge in one direction at the
    top left and bottom right of the digit, the opposite diagonal at the bottom left
    and top right, horizontal edges at the middle, top, and bottom, and so forth.
    So what if we could extract information about where the edges occur in each image,
    and then use that information as our features, instead of raw pixels?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像的背景下，特征是一种视觉上独特的属性。例如，数字7的特征是在数字的顶部附近有一个水平边缘，以及在其下方有一个从右上到左下的对角边缘。另一方面，数字3的特征是在数字的左上角和右下角有一个方向的对角边缘，在左下角和右上角有相反的对角边缘，在中间、顶部和底部有水平边缘等等。那么，如果我们能够提取关于每个图像中边缘出现位置的信息，然后将该信息用作我们的特征，而不是原始像素呢？
- en: It turns out that finding the edges in an image is a very common task in computer
    vision and is surprisingly straightforward. To do it, we use something called
    a *convolution*. A convolution requires nothing more than multiplication and addition—two
    operations that are responsible for the vast majority of work that we will see
    in every single deep learning model in this book!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，在图像中找到边缘是计算机视觉中非常常见的任务，而且非常简单。为了做到这一点，我们使用一种称为*卷积*的东西。卷积只需要乘法和加法——这两种操作是我们将在本书中看到的每个深度学习模型中绝大部分工作的原因！
- en: A convolution applies a *kernel* across an image. A kernel is a little matrix,
    such as the 3×3 matrix in the top right of [Figure 13-1](#basic_conv).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积将一个*卷积核*应用于图像。卷积核是一个小矩阵，例如[图13-1](#basic_conv)右上角的3×3矩阵。
- en: '![Applying a kernel to one location](Images/dlcf_1301.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: 应用卷积到一个位置
- en: Figure 13-1\. Applying a kernel to one location
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-1。将卷积应用到一个位置
- en: The 7×7 grid to the left is the *image* we’re going to apply the kernel to.
    The convolution operation multiplies each element of the kernel by each element
    of a 3×3 block of the image. The results of these multiplications are then added
    together. The diagram in [Figure 13-1](#basic_conv) shows an example of applying
    a kernel to a single location in the image, the 3×3 block around cell 18.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的7×7网格是我们将应用卷积核的*图像*。卷积操作将卷积核的每个元素与图像的一个3×3块的每个元素相乘。然后将这些乘积的结果相加。[图13-1](#basic_conv)中的图示显示了将卷积核应用于图像中单个位置的示例，即围绕18单元格的3×3块。
- en: 'Let’s do this with code. First, we create a little 3×3 matrix like so:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用代码来做这个。首先，我们创建一个小的3×3矩阵如下：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We’re going to call this our kernel (because that’s what fancy computer vision
    researchers call these). And we’ll need an image, of course:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将称之为卷积核（因为这是时髦的计算机视觉研究人员称呼的）。当然，我们还需要一张图片：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](Images/dlcf_13in01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in01.png)'
- en: 'Now we’re going to take the top 3×3-pixel square of our image, and multiply
    each of those values by each item in our kernel. Then we’ll add them up, like
    so:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将取图像的顶部3×3像素正方形，并将这些值中的每一个与我们的卷积核中的每个项目相乘。然后我们将它们加在一起，就像这样：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Not very interesting so far—all the pixels in the top-left corner are white.
    But let’s pick a couple of more interesting spots:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止并不是很有趣——左上角的所有像素都是白色的。但让我们选择一些更有趣的地方：
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Top section of a digit](Images/dlcf_13in02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![数字的顶部部分](Images/dlcf_13in02.png)'
- en: 'There’s a top edge at cell 5,7\. Let’s repeat our calculation there:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在5,7单元格处有一个顶边。让我们在那里重复我们的计算：
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There’s a right edge at cell 8,18\. What does that give us?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在8,18单元格处有一个右边缘。这给我们带来了什么？
- en: '[PRE10]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, this little calculation is returning a high number where the
    3×3-pixel square represents a top edge (i.e., where there are low values at the
    top of the square and high values immediately underneath). That’s because the
    `-1` values in our kernel have little impact in that case, but the `1` values
    have a lot.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这个小计算返回了一个高数字，其中3×3像素的正方形代表顶边（即，在正方形顶部有低值，紧接着是高值）。这是因为我们的卷积核中的`-1`值在这种情况下影响很小，但`1`值影响很大。
- en: Let’s look a tiny bit at the math. The filter will take any window of size 3×3
    in our images, and if we name the pixel values like this
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row 1st Column a Baseline 1 2nd Column a Baseline
    2 3rd Column a Baseline 3 2nd Row 1st Column a Baseline 4 2nd Column a Baseline
    5 3rd Column a Baseline 6 3rd Row 1st Column a Baseline 7 2nd Column a Baseline
    8 3rd Column a Baseline 9 EndLayout" display="block"><mtable><mtr><mtd><mrow><mi>a</mi>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mi>a</mi> <mn>2</mn></mrow></mtd> <mtd><mrow><mi>a</mi>
    <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mi>a</mi> <mn>4</mn></mrow></mtd>
    <mtd><mrow><mi>a</mi> <mn>5</mn></mrow></mtd> <mtd><mrow><mi>a</mi> <mn>6</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mi>a</mi> <mn>7</mn></mrow></mtd> <mtd><mrow><mi>a</mi> <mn>8</mn></mrow></mtd>
    <mtd><mrow><mi>a</mi> <mn>9</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: it will return <math alttext="a Baseline 1 plus a Baseline 2 plus a Baseline
    3 minus a Baseline 7 minus a Baseline 8 minus a Baseline 9"><mrow><mi>a</mi> <mn>1</mn>
    <mo>+</mo> <mi>a</mi> <mn>2</mn> <mo>+</mo> <mi>a</mi> <mn>3</mn> <mo>-</mo> <mi>a</mi>
    <mn>7</mn> <mo>-</mo> <mi>a</mi> <mn>8</mn> <mo>-</mo> <mi>a</mi> <mn>9</mn></mrow></math>
    . If we are in a part of the image where <math alttext="a Baseline 1"><mrow><mi>a</mi>
    <mn>1</mn></mrow></math> , <math alttext="a Baseline 2"><mrow><mi>a</mi> <mn>2</mn></mrow></math>
    , and <math alttext="a Baseline 3"><mrow><mi>a</mi> <mn>3</mn></mrow></math> add
    up to the same as <math alttext="a Baseline 7"><mrow><mi>a</mi> <mn>7</mn></mrow></math>
    , <math alttext="a Baseline 8"><mrow><mi>a</mi> <mn>8</mn></mrow></math> , and
    <math alttext="a Baseline 9"><mrow><mi>a</mi> <mn>9</mn></mrow></math> , then
    the terms will cancel each other out and we will get 0\. However, if <math alttext="a
    Baseline 1"><mrow><mi>a</mi> <mn>1</mn></mrow></math> is greater than <math alttext="a
    Baseline 7"><mrow><mi>a</mi> <mn>7</mn></mrow></math> , <math alttext="a Baseline
    2"><mrow><mi>a</mi> <mn>2</mn></mrow></math> is greater than <math alttext="a
    Baseline 8"><mrow><mi>a</mi> <mn>8</mn></mrow></math> , and <math alttext="a Baseline
    3"><mrow><mi>a</mi> <mn>3</mn></mrow></math> is greater than <math alttext="a
    Baseline 9"><mrow><mi>a</mi> <mn>9</mn></mrow></math> , we will get a bigger number
    as a result. So this filter detects horizontal edges—more precisely, edges where
    we go from bright parts of the image at the top to darker parts at the bottom.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Changing our filter to have the row of `1`s at the top and the `–1`s at the
    bottom would detect horizontal edges that go from dark to light. Putting the `1`s
    and `–1`s in columns versus rows would give us filters that detect vertical edges.
    Each set of weights will produce a different kind of outcome.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a function to do this for one location, and check that it matches
    our result from before:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: But note that we can’t apply it to the corner (e.g., location 0,0), since there
    isn’t a complete 3×3 square there.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Mapping a Convolutional Kernel
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can map `apply_kernel()` across the coordinate grid. That is, we’ll be taking
    our 3×3 kernel and applying it to each 3×3 section of our image. For instance,
    [Figure 13-2](#nopad_conv) shows the positions a 3×3 kernel can be applied to
    in the first row of a 5×5 image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying a kernel across a grid](Images/dlcf_1302.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. Applying a kernel across a grid
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To get a grid of coordinates, we can use a *nested list comprehension*, like
    so:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Nested List Comprehensions
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nested list comprehensions are used a lot in Python, so if you haven’t seen
    them before, take a few minutes to make sure you understand what’s happening here,
    and experiment with writing your own nested list comprehensions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the result of applying our kernel over a coordinate grid:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](Images/dlcf_13in03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Looking good! Our top edges are black, and bottom edges are white (since they
    are the *opposite* of top edges). Now that our image contains negative numbers
    too, `matplotlib` has automatically changed our colors so that white is the smallest
    number in the image, black the highest, and zeros appear as gray.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！我们的顶部边缘是黑色的，底部边缘是白色的（因为它们是顶部边缘的*相反*）。现在我们的图像中也包含负数，`matplotlib`已自动更改了我们的颜色，使得白色是图像中最小的数字，黑色是最高的，零显示为灰色。
- en: 'We can try the same thing for left edges:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以尝试同样的方法来处理左边缘：
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](Images/dlcf_13in04.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in04.png)'
- en: As we mentioned before, a convolution is the operation of applying such a kernel
    over a grid. Vincent Dumoulin and Francesco Visin’s paper [“A Guide to Convolution
    Arithmetic for Deep Learning”](https://oreil.ly/les1R) has many great diagrams
    showing how image kernels can be applied. [Figure 13-3](#three_ex_four_conv) is
    an example from the paper showing (at the bottom) a light blue 4×4 image with
    a dark blue 3×3 kernel being applied, creating a 2×2 green output activation map
    at the top.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，卷积是将这样的内核应用于网格的操作。Vincent Dumoulin和Francesco Visin的论文[“深度学习卷积算术指南”](https://oreil.ly/les1R)中有许多出色的图表，展示了如何应用图像内核。[图13-3](#three_ex_four_conv)是论文中的一个示例，显示了（底部）一个浅蓝色的4×4图像，应用了一个深蓝色的3×3内核，创建了一个顶部的2×2绿色输出激活图。
- en: '![Result of applying a 3x3 kernel to a 4x4 image](Images/dlcf_1303.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![将3x3内核应用于4x4图像的结果](Images/dlcf_1303.png)'
- en: Figure 13-3\. Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent
    Dumoulin and Francesco Visin)
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3。将3×3内核应用于4×4图像的结果（由Vincent Dumoulin和Francesco Visin提供）
- en: Look at the shape of the result. If the original image has a height of `h` and
    a width of `w`, how many 3×3 windows can we find? As you can see from the example,
    there are `h-2` by `w-2` windows, so the image we get as a result has a height
    of `h-2` and a width of `w-2`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下结果的形状。如果原始图像的高度为`h`，宽度为`w`，我们可以找到多少个3×3窗口？正如您从示例中看到的，有`h-2`乘以`w-2`个窗口，因此我们得到的结果图像的高度为`h-2`，宽度为`w-2`。
- en: We won’t implement this convolution function from scratch, but use PyTorch’s
    implementation instead (it is way faster than anything we could do in Python).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会从头开始实现这个卷积函数，而是使用PyTorch的实现（它比我们在Python中能做的任何事情都要快）。
- en: Convolutions in PyTorch
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch中的卷积
- en: 'Convolution is such an important and widely used operation that PyTorch has
    it built in. It’s called `F.conv2d` (recall that `F` is a fastai import from `torch.nn.functional`,
    as recommended by PyTorch). PyTorch docs tell us that it includes these parameters:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一个如此重要且广泛使用的操作，PyTorch已经内置了它。它被称为`F.conv2d`（回想一下，`F`是从`torch.nn.functional`中导入的fastai，正如PyTorch建议的）。PyTorch文档告诉我们它包括这些参数：
- en: '`input`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`input`'
- en: input tensor of shape `(minibatch, in_channels, iH, iW)`
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(minibatch, in_channels, iH, iW)`的输入张量
- en: '`weight`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`weight`'
- en: filters of shape `(out_channels, in_channels, kH, kW)`
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(out_channels, in_channels, kH, kW)`的滤波器
- en: Here `iH,iW` is the height and width of the image (i.e., `28,28`), and `kH,kW`
    is the height and width of our kernel (`3,3`). But apparently PyTorch is expecting
    rank-4 tensors for both these arguments, whereas currently we have only rank-2
    tensors (i.e., matrices, or arrays with two axes).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`iH，iW`是图像的高度和宽度（即`28,28`），`kH，kW`是我们内核的高度和宽度（`3,3`）。但显然PyTorch期望这两个参数都是秩为4的张量，而当前我们只有秩为2的张量（即矩阵，或具有两个轴的数组）。
- en: The reason for these extra axes is that PyTorch has a few tricks up its sleeve.
    The first trick is that PyTorch can apply a convolution to multiple images at
    the same time. That means we can call it on every item in a batch at once!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些额外轴的原因是PyTorch有一些技巧。第一个技巧是PyTorch可以同时将卷积应用于多个图像。这意味着我们可以一次在批次中的每个项目上调用它！
- en: 'The second trick is that PyTorch can apply multiple kernels at the same time.
    So let’s create the diagonal-edge kernels too, and then stack all four of our
    edge kernels into a single tensor:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个技巧是PyTorch可以同时应用多个内核。因此，让我们也创建对角边缘内核，然后将我们的四个边缘内核堆叠成一个单个张量：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To test this, we’ll need a `DataLoader` and a sample mini-batch. Let’s use
    the data block API:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个，我们需要一个`DataLoader`和一个样本小批量。让我们使用数据块API：
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By default, fastai puts data on the GPU when using data blocks. Let’s move
    it to the CPU for our examples:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，fastai在使用数据块时会将数据放在GPU上。让我们将其移动到CPU用于我们的示例：
- en: '[PRE23]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'One batch contains 64 images, each of 1 channel, with 28×28 pixels. `F.conv2d`
    can handle multichannel (color) images too. A *channel* is a single basic color
    in an image—for regular full-color images, there are three channels, red, green,
    and blue. PyTorch represents an image as a rank-3 tensor, with these dimensions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一个批次包含64张图片，每张图片有1个通道，每个通道有28×28个像素。`F.conv2d`也可以处理多通道（彩色）图像。*通道*是图像中的单个基本颜色——对于常规全彩图像，有三个通道，红色、绿色和蓝色。PyTorch将图像表示为一个秩为3的张量，具有以下维度：
- en: '[PRE24]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We’ll see how to handle more than one channel later in this chapter. Kernels
    passed to `F.conv2d` need to be rank-4 tensors:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到如何处理多个通道。传递给`F.conv2d`的内核需要是秩为4的张量：
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`edge_kernels` is currently missing one of these: we need to tell PyTorch that
    the number of input channels in the kernel is one, which we can do by inserting
    an axis of size one (this is known as a *unit axis*) in the first location, where
    the PyTorch docs show `in_channels` is expected. To insert a unit axis into a
    tensor, we use the `unsqueeze` method:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`edge_kernels`目前缺少其中一个：我们需要告诉PyTorch内核中的输入通道数是1，我们可以通过在第一个位置插入一个大小为1的轴来实现（这称为*单位轴*），PyTorch文档显示`in_channels`应该是预期的。要在张量中插入一个单位轴，我们使用`unsqueeze`方法：'
- en: '[PRE26]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is now the correct shape for `edge_kernels`. Let’s pass this all to `conv2d`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是`edge_kernels`的正确形状。让我们将所有这些传递给`conv2d`：
- en: '[PRE28]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output shape shows we have 64 images in the mini-batch, 4 kernels, and
    26×26 edge maps (we started with 28×28 images, but lost one pixel from each side
    as discussed earlier). We can see we get the same results as when we did this
    manually:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状显示我们有64个图像在小批量中，4个内核，以及26×26的边缘映射（我们从前面讨论中开始是28×28的图像，但每边丢失一个像素）。我们可以看到我们得到了与手动操作时相同的结果：
- en: '[PRE31]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](Images/dlcf_13in05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in05.png)'
- en: The most important trick that PyTorch has up its sleeve is that it can use the
    GPU to do all this work in parallel—applying multiple kernels to multiple images,
    across multiple channels. Doing lots of work in parallel is critical to getting
    GPUs to work efficiently; if we did each of these operations one at a time, we’d
    often run hundreds of times slower (and if we used our manual convolution loop
    from the previous section, we’d be millions of times slower!). Therefore, to become
    a strong deep learning practitioner, one skill to practice is giving your GPU
    plenty of work to do at a time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch最重要的技巧是它可以使用GPU并行地完成所有这些工作-将多个核应用于多个图像，跨多个通道。并行进行大量工作对于使GPU高效工作至关重要；如果我们一次执行每个操作，通常会慢几百倍（如果我们使用前一节中的手动卷积循环，将慢数百万倍！）。因此，要成为一名优秀的深度学习从业者，一个需要练习的技能是让GPU一次处理大量工作。
- en: It would be nice to not lose those two pixels on each axis. The way we do that
    is to add *padding*, which is simply additional pixels added around the outside
    of our image. Most commonly, pixels of zeros are added.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在每个轴上丢失这两个像素会很好。我们这样做的方法是添加*填充*，简单地在图像周围添加额外的像素。最常见的是添加零像素。
- en: Strides and Padding
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步幅和填充
- en: With appropriate padding, we can ensure that the output activation map is the
    same size as the original image, which can make things a lot simpler when we construct
    our architectures. [Figure 13-4](#pad_conv) shows how adding padding allows us
    to apply the kernel in the image corners.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当的填充，我们可以确保输出激活图与原始图像的大小相同，这在构建架构时可以使事情变得简单得多。[图13-4](#pad_conv)显示了添加填充如何允许我们在图像角落应用核。
- en: '![A convolution with padding](Images/dlcf_1304.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![带填充的卷积](Images/dlcf_1304.png)'
- en: Figure 13-4\. A convolution with padding
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-4。带填充的卷积
- en: With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6
    activation map, as we can see in [Figure 13-5](#four_by_five_conv).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用5×5输入，4×4核和2像素填充，我们最终得到一个6×6的激活图，如我们在[图13-5](#four_by_five_conv)中所看到的。
- en: '![4x4 kernel with 5x5 input and 2 pixels of padding](Images/dlcf_1305.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![4x4核与5x5输入和2像素填充](Images/dlcf_1305.png)'
- en: Figure 13-5\. A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy
    of Vincent Dumoulin and Francesco Visin)
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-5。一个4×4的核与5×5的输入和2像素的填充（由Vincent Dumoulin和Francesco Visin提供）
- en: If we add a kernel of size `ks` by `ks` (with `ks` an odd number), the necessary
    padding on each side to keep the same shape is `ks//2`. An even number for `ks`
    would require a different amount of padding on the top/bottom and left/right,
    but in practice we almost never use an even filter size.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们添加一个大小为`ks`乘以`ks`的核（其中`ks`是一个奇数），为了保持相同的形状，每一侧所需的填充是`ks//2`。对于`ks`的偶数，需要在上/下和左/右两侧填充不同数量，但实际上我们几乎从不使用偶数滤波器大小。
- en: So far, when we have applied the kernel to the grid, we have moved it one pixel
    over at a time. But we can jump further; for instance, we could move over two
    pixels after each kernel application, as in [Figure 13-6](#three_by_five_conv).
    This is known as a *stride-2* convolution. The most common kernel size in practice
    is 3×3, and the most common padding is 1\. As you’ll see, stride-2 convolutions
    are useful for decreasing the size of our outputs, and stride-1 convolutions are
    useful for adding layers without changing the output size.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，当我们将核应用于网格时，我们每次将其移动一个像素。但我们可以跳得更远；例如，我们可以在每次核应用后移动两个像素，就像[图13-6](#three_by_five_conv)中所示。这被称为*步幅-2*卷积。实践中最常见的核大小是3×3，最常见的填充是1。正如您将看到的，步幅-2卷积对于减小输出大小很有用，而步幅-1卷积对于添加层而不改变输出大小也很有用。
- en: '![3x3 kernel with 5x5 input, stride 2 convolution, and 1 pixel of padding](Images/dlcf_1306.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![3x3核与5x5输入，步幅2卷积和1像素填充](Images/dlcf_1306.png)'
- en: Figure 13-6\. A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel
    of padding (courtesy of Vincent Dumoulin and Francesco Visin)
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-6。一个3×3的核与5×5的输入，步幅2卷积和1像素填充（由Vincent Dumoulin和Francesco Visin提供）
- en: In an image of size `h` by `w`, using a padding of 1 and a stride of 2 will
    give us a result of size `(h+1)//2` by `(w+1)//2`. The general formula for each
    dimension is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在大小为`h`乘以`w`的图像中，使用填充1和步幅2将给出大小为`(h+1)//2`乘以`(w+1)//2`的结果。每个维度的一般公式是
- en: '[PRE32]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: where `pad` is the padding, `ks` is the size of our kernel, and `stride` is
    the stride.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`pad`是填充，`ks`是我们核的大小，`stride`是步幅。
- en: Let’s now take a look at how the pixel values of the result of our convolutions
    are computed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何计算我们卷积结果的像素值。
- en: Understanding the Convolution Equations
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解卷积方程
- en: To explain the math behind convolutions, fast.ai student Matt Kleinsmith came
    up with the very clever idea of showing [CNNs from different viewpoints](https://oreil.ly/wZuBs).
    In fact, it’s so clever, and so helpful, we’re going to show it here too!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释卷积背后的数学，fast.ai学生Matt Kleinsmith提出了一个非常聪明的想法，展示了[不同视角的CNNs](https://oreil.ly/wZuBs)。事实上，这个想法非常聪明，非常有帮助，我们也会在这里展示！
- en: 'Here’s our 3×3-pixel image, with each pixel labeled with a letter:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的3×3像素图像，每个像素都用字母标记：
- en: '![The image](Images/dlcf_13in06.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/dlcf_13in06.png)'
- en: 'And here’s our kernel, with each weight labeled with a Greek letter:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的核，每个权重都用希腊字母标记：
- en: '![The kernel](Images/dlcf_13in07.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![核](Images/dlcf_13in07.png)'
- en: 'Since the filter fits in the image four times, we have four results:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于滤波器适合图像四次，我们有四个结果：
- en: '![The activations](Images/dlcf_13in08.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![激活](Images/dlcf_13in08.png)'
- en: '[Figure 13-7](#apply_kernel) shows how we applied the kernel to each section
    of the image to yield each result.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-7](#apply_kernel)显示了我们如何将核应用于图像的每个部分以产生每个结果。'
- en: '![Applying the kernel](Images/dlcf_1307.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![应用核](Images/dlcf_1307.png)'
- en: Figure 13-7\. Applying the kernel
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-7。应用核
- en: The equation view is in [Figure 13-8](#eq_view).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 方程视图在[图13-8](#eq_view)中。
- en: '![The equation](Images/dlcf_1308.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![方程](Images/dlcf_1308.png)'
- en: Figure 13-8\. The equation
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-8。方程
- en: Notice that the bias term, *b*, is the same for each section of the image. You
    can consider the bias as part of the filter, just as the weights (α, β, γ, δ)
    are part of the filter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，偏置项*b*对于图像的每个部分都是相同的。您可以将偏置视为滤波器的一部分，就像权重（α、β、γ、δ）是滤波器的一部分一样。
- en: 'Here’s an interesting insight—a convolution can be represented as a special
    kind of matrix multiplication, as illustrated in [Figure 13-9](#conv_matmul).
    The weight matrix is just like the ones from traditional neural networks. However,
    this weight matrix has two special properties:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的见解——卷积可以被表示为一种特殊类型的矩阵乘法，如[图13-9](#conv_matmul)所示。权重矩阵就像传统神经网络中的那些一样。但是，这个权重矩阵具有两个特殊属性：
- en: The zeros shown in gray are untrainable. This means that they’ll stay zero throughout
    the optimization process.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 灰色显示的零是不可训练的。这意味着它们在优化过程中将保持为零。
- en: Some of the weights are equal, and while they are trainable (i.e., changeable),
    they must remain equal. These are called *shared weights*.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些权重是相等的，虽然它们是可训练的（即可更改的），但它们必须保持相等。这些被称为*共享权重*。
- en: The zeros correspond to the pixels that the filter can’t touch. Each row of
    the weight matrix corresponds to one application of the filter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 零对应于滤波器无法触及的像素。权重矩阵的每一行对应于滤波器的一次应用。
- en: '![Convolution as matrix multiplication](Images/dlcf_1309.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![卷积作为矩阵乘法](Images/dlcf_1309.png)'
- en: Figure 13-9\. Convolution as matrix multiplication
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-9。卷积作为矩阵乘法
- en: Now that we understand what convolutions are, let’s use them to build a neural
    net.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了卷积是什么，让我们使用它们来构建一个神经网络。
- en: Our First Convolutional Neural Network
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个卷积神经网络
- en: There is no reason to believe that some particular edge filters are the most
    useful kernels for image recognition. Furthermore, we’ve seen that in later layers,
    convolutional kernels become complex transformations of features from lower levels,
    but we don’t have a good idea of how to manually construct these.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由相信某些特定的边缘滤波器是图像识别最有用的卷积核。此外，我们已经看到在后续层中，卷积核变成了来自较低层特征的复杂转换，但我们不知道如何手动构建这些转换。
- en: Instead, it would be best to learn the values of the kernels. We already know
    how to do this—SGD! In effect, the model will learn the features that are useful
    for classification. When we use convolutions instead of (or in addition to) regular
    linear layers, we create a *convolutional neural network* (CNN).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，最好学习卷积核的值。我们已经知道如何做到这一点——SGD！实际上，模型将学习对分类有用的特征。当我们使用卷积而不是（或者除了）常规线性层时，我们创建了一个*卷积神经网络*（CNN）。
- en: Creating the CNN
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建CNN
- en: 'Let’s go back to the basic neural network we had in [Chapter 4](ch04.xhtml#chapter_mnist_basics).
    It was defined like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到[第4章](ch04.xhtml#chapter_mnist_basics)中的基本神经网络。它的定义如下：
- en: '[PRE33]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can view a model’s definition:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看模型的定义：
- en: '[PRE34]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We now want to create a similar architecture to this linear model, but using
    convolutional layers instead of linear. `nn.Conv2d` is the module equivalent of
    `F.conv2d`. It’s more convenient than `F.conv2d` when creating an architecture,
    because it creates the weight matrix for us automatically when we instantiate
    it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要创建一个类似于这个线性模型的架构，但是使用卷积层而不是线性层。`nn.Conv2d`是`F.conv2d`的模块等效物。在创建架构时，它比`F.conv2d`更方便，因为在实例化时会自动为我们创建权重矩阵。
- en: 'Here’s a possible architecture:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可能的架构：
- en: '[PRE36]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: One thing to note here is that we didn’t need to specify `28*28` as the input
    size. That’s because a linear layer needs a weight in the weight matrix for every
    pixel, so it needs to know how many pixels there are, but a convolution is applied
    over each pixel automatically. The weights depend only on the number of input
    and output channels and the kernel size, as we saw in the previous section.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，我们不需要指定`28*28`作为输入大小。这是因为线性层需要在权重矩阵中为每个像素设置一个权重，因此它需要知道有多少像素，但卷积会自动应用于每个像素。权重仅取决于输入和输出通道的数量以及核大小，正如我们在前一节中看到的。
- en: 'Think about what the output shape is going to be; then let’s try it and see:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想输出形状会是什么；然后让我们尝试一下：
- en: '[PRE37]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This is not something we can use to do classification, since we need a single
    output activation per image, not a 28×28 map of activations. One way to deal with
    this is to use enough stride-2 convolutions such that the final layer is size
    1\. After one stride-2 convolution, the size will be 14×14; after two, it will
    be 7×7; then 4×4, 2×2, and finally size 1.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们可以用来进行分类的东西，因为我们需要每个图像一个单独的输出激活，而不是一个28×28的激活图。处理这个问题的一种方法是使用足够多的步幅为2的卷积，使得最终层的大小为1。经过一次步幅为2的卷积后，大小将为14×14；经过两次后，将为7×7；然后是4×4，2×2，最终大小为1。
- en: 'Let’s try that now. First, we’ll define a function with the basic parameters
    we’ll use in each convolution:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一下。首先，我们将定义一个函数，其中包含我们在每个卷积中将使用的基本参数：
- en: '[PRE39]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Refactoring
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重构
- en: Refactoring parts of your neural networks like this makes it much less likely
    you’ll get errors due to inconsistencies in your architectures, and makes it more
    obvious to the reader which parts of your layers are actually changing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 重构神经网络的部分，可以减少由于架构不一致而导致的错误，也可以更明显地向读者展示哪些层的部分实际上在改变。
- en: When we use a stride-2 convolution, we often increase the number of features
    at the same time. This is because we’re decreasing the number of activations in
    the activation map by a factor of 4; we don’t want to decrease the capacity of
    a layer by too much at a time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用步幅为2的卷积时，通常会同时增加特征的数量。这是因为我们通过将激活图中的激活数量减少4倍来减少层的容量，我们不希望一次过多地减少层的容量。
- en: 'Jargon: Channels and Features'
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：通道和特征
- en: These two terms are largely used interchangeably and refer to the size of the
    second axis of a weight matrix, which is the number of activations per grid cell
    after a convolution. *Features* is never used to refer to the input data, but
    *channels* can refer to either the input data (generally, channels are colors)
    or activations inside the network.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个术语通常可以互换使用，指的是权重矩阵的第二轴的大小，即卷积后每个网格单元的激活数量。*特征*从不用于指代输入数据，但*通道*可以指代输入数据(通常是颜色)或网络内部的激活。
- en: 'Here is how we can build a simple CNN:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何构建一个简单的CNN：
- en: '[PRE40]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Jeremy Says
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jeremy说
- en: I like to add comments like the ones here after each convolution to show how
    large the activation map will be after each layer. These comments assume that
    the input size is 28×28.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢在每个卷积后添加类似这里的注释，以显示每个层后激活图的大小。这些注释假定输入大小为28×28。
- en: 'Now the network outputs two activations, which map to the two possible levels
    in our labels:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在网络输出两个激活，这对应于我们标签中的两个可能级别：
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can now create our `Learner`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建我们的`Learner`：
- en: '[PRE43]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To see exactly what’s going on in the model, we can use `summary`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看模型中发生的情况，我们可以使用`summary`：
- en: '[PRE44]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note that the output of the final `Conv2d` layer is `64x2x1x1`. We need to remove
    those extra `1x1` axes; that’s what `Flatten` does. It’s basically the same as
    PyTorch’s `squeeze` method, but as a module.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最终的`Conv2d`层的输出是`64x2x1x1`。我们需要去除那些额外的`1x1`轴；这就是`Flatten`所做的。这基本上与PyTorch的`squeeze`方法相同，但作为一个模块。
- en: 'Let’s see if this trains! Since this is a deeper network than we’ve built from
    scratch before, we’ll use a lower learning rate and more epochs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是否训练！由于这是我们从头开始构建的比以前更深的网络，我们将使用更低的学习率和更多的时代：
- en: '[PRE46]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 时代 | 训练损失 | 验证损失 | 准确性 | 时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.072684 | 0.045110 | 0.990186 | 00:05 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.072684 | 0.045110 | 0.990186 | 00:05 |'
- en: '| 1 | 0.022580 | 0.030775 | 0.990186 | 00:05 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.022580 | 0.030775 | 0.990186 | 00:05 |'
- en: Success! It’s getting closer to the `resnet18` result we had, although it’s
    not quite there yet, and it’s taking more epochs, and we’re needing to use a lower
    learning rate. We still have a few more tricks to learn, but we’re getting closer
    and closer to being able to create a modern CNN from scratch.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！它越来越接近我们之前的`resnet18`结果，尽管还不完全达到，而且需要更多的时代，我们需要使用更低的学习率。我们还有一些技巧要学习，但我们越来越接近能够从头开始创建现代CNN。
- en: Understanding Convolution Arithmetic
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解卷积算术
- en: 'We can see from the summary that we have an input of size `64x1x28x28`. The
    axes are `batch,channel,height,width`. This is often represented as `NCHW` (where
    `N` refers to batch size). TensorFlow, on the other hand, uses `NHWC` axis order.
    Here is the first layer:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从总结中看到，我们有一个大小为`64x1x28x28`的输入。轴是`批次、通道、高度、宽度`。这通常表示为`NCHW`(其中`N`是批次大小)。另一方面，TensorFlow使用`NHWC`轴顺序。这是第一层：
- en: '[PRE47]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'So we have 1 input channel, 4 output channels, and a 3×3 kernel. Let’s check
    the weights of the first convolution:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有1个输入通道，4个输出通道和一个3×3的内核。让我们检查第一个卷积的权重：
- en: '[PRE49]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The summary shows we have 40 parameters, and `4*1*3*3` is 36\. What are the
    other four parameters? Let’s see what the bias contains:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 总结显示我们有40个参数，`4*1*3*3`是36。其他四个参数是什么？让我们看看偏差包含什么：
- en: '[PRE51]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We can now use this information to clarify our statement in the previous section:
    “When we use a stride-2 convolution, we often increase the number of features
    because we’re decreasing the number of activations in the activation map by a
    factor of 4; we don’t want to decrease the capacity of a layer by too much at
    a time.”'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以利用这些信息来澄清我们在上一节中的陈述：“当我们使用步幅为2的卷积时，我们经常增加特征的数量，因为我们通过4的因子减少了激活图中的激活数量；我们不希望一次性太多地减少层的容量。”
- en: There is one bias for each channel. (Sometimes channels are called *features*
    or *filters* when they are not input channels.) The output shape is `64x4x14x14`,
    and this will therefore become the input shape to the next layer. The next layer,
    according to `summary`, has 296 parameters. Let’s ignore the batch axis to keep
    things simple. So, for each of `14*14=196` locations, we are multiplying `296-8=288`
    weights (ignoring the bias for simplicity), so that’s `196*288=56_448` multiplications
    at this layer. The next layer will have `7*7*(1168-16)=56_448` multiplications.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每个通道都有一个偏差。(有时通道被称为*特征*或*滤波器*，当它们不是输入通道时。) 输出形状是`64x4x14x14`，因此这将成为下一层的输入形状。根据`summary`，下一层有296个参数。让我们忽略批次轴，保持简单。因此，对于`14*14=196`个位置，我们正在乘以`296-8=288`个权重(为简单起见忽略偏差)，因此在这一层有`196*288=56,448`次乘法。下一层将有`7*7*(1168-16)=56,448`次乘法。
- en: What happened here is that our stride-2 convolution halved the *grid size* from
    `14x14` to `7x7`, and we doubled the *number of filters* from 8 to 16, resulting
    in no overall change in the amount of computation. If we left the number of channels
    the same in each stride-2 layer, the amount of computation being done in the net
    would get less and less as it gets deeper. But we know that the deeper layers
    have to compute semantically rich features (such as eyes or fur), so we wouldn’t
    expect that doing *less* computation would make sense.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况是，我们的步幅为2的卷积将*网格大小*从`14x14`减半到`7x7`，并且我们将*滤波器数量*从8增加到16，导致总体计算量没有变化。如果我们在每个步幅为2的层中保持通道数量不变，那么网络中所做的计算量会随着深度增加而减少。但我们知道，更深层次必须计算语义丰富的特征(如眼睛或毛发)，因此我们不会期望*减少*计算是有意义的。
- en: Another way to think of this is based on receptive fields.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考这个问题的方式是基于感受野。
- en: Receptive Fields
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感受野
- en: The *receptive field* is the area of an image that is involved in the calculation
    of a layer. On the [book’s website](https://book.fast.ai), you’ll find an Excel
    spreadsheet called *conv-example.xlsx* that shows the calculation of two stride-2
    convolutional layers using an MNIST digit. Each layer has a single kernel. [Figure 13-10](#preced1)
    shows what we see if we click one of the cells in the *conv2* section, which shows
    the output of the second convolutional layer, and click *trace precedents*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*接受域*是参与层计算的图像区域。在[书籍网站](https://book.fast.ai)上，您会找到一个名为*conv-example.xlsx*的Excel电子表格，展示了使用MNIST数字计算两个步幅为2的卷积层的过程。每个层都有一个单独的核。[图13-10](#preced1)展示了如果我们点击*conv2*部分中的一个单元格，显示第二个卷积层的输出，并点击*trace
    precedents*时看到的内容。'
- en: '![Immediate precedents of conv2 layer](Images/dlcf_1310.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![conv2层的直接前置](Images/dlcf_1310.png)'
- en: Figure 13-10\. Immediate precedents of Conv2 layer
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-10. Conv2层的直接前置
- en: Here, the cell with the green border is the cell we clicked, and the blue highlighted
    cells are its *precedents*—the cells used to calculate its value. These cells
    are the corresponding 3×3 area of cells from the input layer (on the left), and
    the cells from the filter (on the right). Let’s now click *trace precedents* again,
    to see what cells are used to calculate these inputs. [Figure 13-11](#preced2)
    shows what happens.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，有绿色边框的单元格是我们点击的单元格，蓝色高亮显示的单元格是它的*前置*——用于计算其值的单元格。这些单元格是输入层（左侧）的对应3×3区域单元格和滤波器（右侧）的单元格。现在让我们再次点击*trace
    precedents*，看看用于计算这些输入的单元格。[图13-11](#preced2)展示了发生了什么。
- en: '![Secondary precedents of conv2 layer](Images/dlcf_1311.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![conv2层的次要前置](Images/dlcf_1311.png)'
- en: Figure 13-11\. Secondary precedents of Conv2 layer
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-11. Conv2层的次要前置
- en: In this example, we have just two convolutional layers, each of stride 2, so
    this is now tracing right back to the input image. We can see that a 7×7 area
    of cells in the input layer is used to calculate the single green cell in the
    Conv2 layer. This 7×7 area is the *receptive field* in the input of the green
    activation in Conv2\. We can also see that a second filter kernel is needed now,
    since we have two layers.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们只有两个步幅为2的卷积层，因此现在追溯到了输入图像。我们可以看到输入层中的一个7×7区域单元格用于计算Conv2层中的单个绿色单元格。这个7×7区域是Conv2中绿色激活的输入的*接受域*。我们还可以看到现在需要第二个滤波器核，因为我们有两个层。
- en: 'As you see from this example, the deeper we are in the network (specifically,
    the more stride-2 convs we have before a layer), the larger the receptive field
    for an activation in that layer is. A large receptive field means that a large
    amount of the input image is used to calculate each activation in that layer.
    We now know that in the deeper layers of the network, we have semantically rich
    features, corresponding to larger receptive fields. Therefore, we’d expect that
    we’d need more weights for each of our features to handle this increasing complexity.
    This is another way of saying the same thing we mentioned in the previous section:
    when we introduce a stride-2 conv in our network, we should also increase the
    number of channels.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中可以看出，我们在网络中越深（特别是在一个层之前有更多步幅为2的卷积层时），该层中激活的接受域就越大。一个大的接受域意味着输入图像的大部分被用来计算该层中每个激活。我们现在知道，在网络的深层，我们有语义丰富的特征，对应着更大的接受域。因此，我们期望我们需要更多的权重来处理这种不断增加的复杂性。这是另一种说法，与我们在前一节提到的相同：当我们在网络中引入步幅为2的卷积时，我们也应该增加通道数。
- en: When writing this particular chapter, we had a lot of questions we needed answers
    for, to be able to explain CNNs to you as best we could. Believe it or not, we
    found most of the answers on Twitter. We’re going to take a quick break to talk
    to you about that now, before we move on to color images.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写这一特定章节时，我们有很多问题需要回答，以便尽可能好地向您解释CNN。信不信由你，我们在Twitter上找到了大部分答案。在我们继续讨论彩色图像之前，我们将快速休息一下，与您谈谈这个问题。
- en: A Note About Twitter
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于Twitter的一点说明
- en: We are not, to say the least, big users of social networks in general. But our
    goal in writing this book is to help you become the best deep learning practitioner
    you can, and we would be remiss not to mention how important Twitter has been
    in our own deep learning journeys.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们并不是社交网络的重度用户。但我们写这本书的目标是帮助您成为最优秀的深度学习从业者，我们不提及Twitter在我们自己的深度学习之旅中有多么重要是不合适的。
- en: 'You see, there’s another part of Twitter, far away from Donald Trump and the
    Kardashians, where deep learning researchers and practitioners talk shop every
    day. As we were writing this section, Jeremy wanted to double-check that what
    we were saying about stride-2 convolutions was accurate, so he asked on Twitter:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您看，Twitter还有另一部分，远离唐纳德·特朗普和卡戴珊家族，深度学习研究人员和从业者每天都在这里交流。在我们撰写这一部分时，Jeremy想要再次确认我们关于步幅为2的卷积的说法是否准确，所以他在Twitter上提问：
- en: '![twitter 1](Images/dlcf_13in09.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![twitter 1](Images/dlcf_13in09.png)'
- en: 'A few minutes later, this answer popped up:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，这个答案出现了：
- en: '![twitter 2](Images/dlcf_13in10.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![twitter 2](Images/dlcf_13in10.png)'
- en: 'Christian Szegedy is the first author of [Inception](https://oreil.ly/hGE_Y),
    the 2014 ImageNet winner, and source of many key insights used in modern neural
    networks. Two hours later, this appeared:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Christian Szegedy是[Inception](https://oreil.ly/hGE_Y)的第一作者，这是2014年ImageNet的获奖作品，也是现代神经网络中许多关键见解的来源。两小时后，这个出现了：
- en: '![twitter 3](Images/dlcf_13in11.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![twitter 3](Images/dlcf_13in11.png)'
- en: Do you recognize that name? You saw it in [Chapter 2](ch02.xhtml#chapter_production),
    when we were talking about the Turing Award winners who established the foundations
    of deep learning today!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你认识那个名字吗？您在[第2章](ch02.xhtml#chapter_production)中看到过，当时我们在谈论今天建立深度学习基础的图灵奖获得者！
- en: 'Jeremy also asked on Twitter for help checking that our description of label
    smoothing in [Chapter 7](ch07.xhtml#chapter_sizing_and_tta) was accurate, and
    got a response again directly from Christian Szegedy (label smoothing was originally
    introduced in the Inception paper):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![twitter 4](Images/dlcf_13in12.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: Many of the top people in deep learning today are Twitter regulars, and are
    very open about interacting with the wider community. One good way to get started
    is to look at a list of Jeremy’s [recent Twitter likes](https://oreil.ly/sqOI7),
    or [Sylvain’s](https://oreil.ly/VWYHY). That way, you can see a list of Twitter
    users whom we think have interesting and useful things to say.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Twitter is the main way we both stay up to date with interesting papers, software
    releases, and other deep learning news. For making connections with the deep learning
    community, we recommend getting involved both in the [fast.ai forums](https://forums.fast.ai)
    and on Twitter.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: That said, let’s get back to the meat of this chapter. Up until now, we have
    shown you examples of pictures in only black and white, with one value per pixel.
    In practice, most colored images have three values per pixel to define their color.
    We’ll look at working with color images next.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Color Images
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A color picture is a rank-3 tensor:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](Images/dlcf_13in13.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'The first axis contains the channels red, green, and blue:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![](Images/dlcf_13in14.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: We saw what the convolution operation was for one filter on one channel of the
    image (our examples were done on a square). A convolutional layer will take an
    image with a certain number of channels (three for the first layer for regular
    RGB color images) and output an image with a different number of channels. As
    with our hidden size that represented the numbers of neurons in a linear layer,
    we can decide to have as many filters as we want, and each will be able to specialize
    (some to detect horizontal edges, others to detect vertical edges, and so forth)
    to give something like the examples we studied in [Chapter 2](ch02.xhtml#chapter_production).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: In one sliding window, we have a certain number of channels and we need as many
    filters (we don’t use the same kernel for all the channels). So our kernel doesn’t
    have a size of 3×3, but `ch_in` (for channels in) by 3×3\. On each channel, we
    multiply the elements of our window by the elements of the corresponding filter,
    and then sum the results (as we saw before) and sum over all the filters. In the
    example given in [Figure 13-12](#rgbconv), the result of our conv layer on that
    window is red + green + blue.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution over an RGB image](Images/dlcf_1312.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Figure 13-12\. Convolution over an RGB image
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, in order to apply a convolution to a color picture, we require a kernel
    tensor with a size that matches the first axis. At each location, the corresponding
    parts of the kernel and the image patch are multiplied together.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: These are then all added together to produce a single number for each grid location
    for each output feature, as shown in [Figure 13-13](#rgbconv2).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding the RGB filters](Images/dlcf_1313.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 13-13\. Adding the RGB filters
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Then we have `ch_out` filters like this, so in the end, the result of our convolutional
    layer will be a batch of images with `ch_out` channels and a height and width
    given by the formula outlined earlier. This give us `ch_out` tensors of size `ch_in
    x ks x ks` that we represent in one big tensor of four dimensions. In PyTorch,
    the order of the dimensions for those weights is `ch_out x ch_in x ks x ks`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we may want to have a bias for each filter. In the preceding example,
    the final result for our convolutional layer would be <math alttext="y Subscript
    upper R Baseline plus y Subscript upper G Baseline plus y Subscript upper B Baseline
    plus b"><mrow><msub><mi>y</mi> <mi>R</mi></msub> <mo>+</mo> <msub><mi>y</mi> <mi>G</mi></msub>
    <mo>+</mo> <msub><mi>y</mi> <mi>B</mi></msub> <mo>+</mo> <mi>b</mi></mrow></math>
    in that case. As in a linear layer, there are as many biases as we have kernels,
    so the bias is a vector of size `ch_out`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: No special mechanisms are required when setting up a CNN for training with color
    images. Just make sure your first layer has three inputs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of ways of processing color images. For instance, you can change
    them to black and white, change from RGB to HSV (hue, saturation, and value) color
    space, and so forth. In general, it turns out experimentally that changing the
    encoding of colors won’t make any difference to your model results, as long as
    you don’t lose information in the transformation. So, transforming to black and
    white is a bad idea, since it removes the color information entirely (and this
    can be critical; for instance, a pet breed may have a distinctive color); but
    converting to HSV generally won’t make any difference.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you know what those pictures in [Chapter 1](ch01.xhtml#chapter_intro) of
    “what a neural net learns” from the [Zeiler and Fergus paper](https://oreil.ly/Y6dzZ)
    mean! As a reminder, this is their picture of some of the layer 1 weights:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Layer 1 kernels found by Zeiler and Fergus](Images/dlcf_13in15.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: This is taking the three slices of the convolutional kernel, for each output
    feature, and displaying them as images. We can see that even though the creators
    of the neural net never explicitly created kernels to find edges, for instance,
    the neural net automatically discovered these features using SGD.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s see how we can train these CNNs, and show you all the techniques fastai
    uses under the hood for efficient training.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Improving Training Stability
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we are so good at recognizing 3s from 7s, let’s move on to something
    harder—recognizing all 10 digits. That means we’ll need to use `MNIST` instead
    of `MNIST_SAMPLE`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The data is in two folders named *training* and *testing*, so we have to tell
    `GrandparentSplitter` about that (it defaults to `train` and `valid`). We do that
    in the `get_dls` function, which we define to make it easy to change our batch
    size later:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Remember, it’s always a good idea to look at your data before you use it:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](Images/dlcf_13in16.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Now that we have our data ready, we can train a simple model on it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Baseline
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, we built a model based on a `conv` function like this:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let’s start with a basic CNN as a baseline. We’ll use the same as one as earlier,
    but with one tweak: we’ll use more activations. Since we have more numbers to
    differentiate, we’ll likely need to learn more filters.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, we generally want to double the number of filters each time
    we have a stride-2 layer. One way to increase the number of filters throughout
    our network is to double the number of activations in the first layer—then every
    layer after that will end up twice as big as in the previous version as well.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'But this creates a subtle problem. Consider the kernel that is being applied
    to each pixel. By default, we use a 3×3-pixel kernel. Therefore, there are a total
    of 3 × 3 = 9 pixels that the kernel is being applied to at each location. Previously,
    our first layer had four output filters. So four values were being computed from
    nine pixels at each location. Think about what happens if we double this output
    to eight filters. Then when we apply our kernel, we will be using nine pixels
    to calculate eight numbers. That means it isn’t really learning much at all: the
    output size is almost the same as the input size. Neural networks will create
    useful features only if they’re forced to do so—that is, if the number of outputs
    from an operation is significantly smaller than the number of inputs.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'To fix this, we can use a larger kernel in the first layer. If we use a kernel
    of 5×5 pixels, 25 pixels are being used at each kernel application. Creating eight
    filters from this will mean the neural net will have to find some useful features:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'As you’ll see in a moment, we can look inside our models while they’re training
    in order to try to find ways to make them train better. To do this, we use the
    `ActivationStats` callback, which records the mean, standard deviation, and histogram
    of activations of every trainable layer (as we’ve seen, callbacks are used to
    add behavior to the training loop; we’ll explore how they work in [Chapter 16](ch16.xhtml#chapter_accel_sgd)):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We want to train quickly, so that means training at a high learning rate. Let’s
    see how we go at 0.06:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2.307071 | 2.305865 | 0.113500 | 00:16 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: This didn’t train at all well! Let’s find out why.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: One handy feature of the callbacks passed to `Learner` is that they are made
    available automatically, with the same name as the callback class, except in `camel_case`.
    So, our `ActivationStats` callback can be accessed through `activation_stats`.
    I’m sure you remember `learn.recorder`…can you guess how that is implemented?
    That’s right, it’s a callback called `Recorder`!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '`ActivationStats` includes some handy utilities for plotting the activations
    during training. `plot_layer_stats(*idx*)` plots the mean and standard deviation
    of the activations of layer number *`idx`*, along with the percentage of activations
    near zero. Here’s the first layer’s plot:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](Images/dlcf_13in17.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Generally our model should have a consistent, or at least smooth, mean and
    standard deviation of layer activations during training. Activations near zero
    are particularly problematic, because it means we have computation in the model
    that’s doing nothing at all (since multiplying by zero gives zero). When you have
    some zeros in one layer, they will therefore generally carry over to the next
    layer…which will then create more zeros. Here’s the penultimate layer of our network:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](Images/dlcf_13in18.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: As expected, the problems get worse toward the end of the network, as the instability
    and zero activations compound over layers. Let’s look at what we can do to make
    training more stable.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Increase Batch Size
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to make training more stable is to increase the batch size. Larger
    batches have gradients that are more accurate, since they’re calculated from more
    data. On the downside, though, a larger batch size means fewer batches per epoch,
    which means fewer opportunities for your model to update weights. Let’s see if
    a batch size of 512 helps:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2.309385 | 2.302744 | 0.113500 | 00:08 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: 'Let’s see what the penultimate layer looks like:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '![](Images/dlcf_13in19.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: Again, we’ve got most of our activations near zero. Let’s see what else we can
    do to improve training stability.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 1cycle Training
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our initial weights are not well suited to the task we’re trying to solve.
    Therefore, it is dangerous to begin training with a high learning rate: we may
    very well make the training diverge instantly, as we’ve seen. We probably don’t
    want to end training with a high learning rate either, so that we don’t skip over
    a minimum. But we want to train at a high learning rate for the rest of the training
    period, because we’ll be able to train more quickly that way. Therefore, we should
    change the learning rate during training, from low, to high, and then back to
    low again.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Leslie Smith (yes, the same guy who invented the learning rate finder!) developed
    this idea in his article [“Super-Convergence: Very Fast Training of Neural Networks
    Using Large Learning Rates”](https://oreil.ly/EB8NU). He designed a schedule for
    learning rate separated into two phases: one where the learning rate grows from
    the minimum value to the maximum value (*warmup*), and one where it decreases
    back to the minimum value (*annealing*). Smith called this combination of approaches
    *1cycle training*.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '1cycle training allows us to use a much higher maximum learning rate than other
    types of training, which gives two benefits:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: By training with higher learning rates, we train faster—a phenomenon Smith calls
    *super-convergence*.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By training with higher learning rates, we overfit less because we skip over
    the sharp local minima to end up in a smoother (and therefore more generalizable)
    part of the loss.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second point is an interesting and subtle one; it is based on the observation
    that a model that generalizes well is one whose loss would not change very much
    if you changed the input by a small amount. If a model trains at a large learning
    rate for quite a while, and can find a good loss when doing so, it must have found
    an area that also generalizes well, because it is jumping around a lot from batch
    to batch (that is basically the definition of a high learning rate). The problem
    is that, as we have discussed, just jumping to a high learning rate is more likely
    to result in diverging losses, rather than seeing your losses improve. So we don’t
    jump straight to a high learning rate. Instead, we start at a low learning rate,
    where our losses do not diverge, and we allow the optimizer to gradually find
    smoother and smoother areas of our parameters by gradually going to higher and
    higher learning rates.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Then, once we have found a nice smooth area for our parameters, we want to find
    the very best part of that area, which means we have to bring our learning rates
    down again. This is why 1cycle training has a gradual learning rate warmup, and
    a gradual learning rate cooldown. Many researchers have found that in practice
    this approach leads to more accurate models and trains more quickly. That is why
    it is the approach that is used by default for `fine_tune` in fastai.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 16](ch16.xhtml#chapter_accel_sgd), we’ll learn all about *momentum*
    in SGD. Briefly, momentum is a technique whereby the optimizer takes a step not
    only in the direction of the gradients, but also that continues in the direction
    of previous steps. Leslie Smith introduced the idea of *cyclical momentum* in
    [“A Disciplined Approach to Neural Network Hyper-Parameters: Part 1”](https://oreil.ly/oL7GT).
    It suggests that the momentum varies in the opposite direction of the learning
    rate: when we are at high learning rates, we use less momentum, and we use more
    again in the annealing phase.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use 1cycle training in fastai by calling `fit_one_cycle`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.210838 | 0.084827 | 0.974300 | 00:08 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: We’re finally making some progress! It’s giving us a reasonable accuracy now.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view the learning rate and momentum throughout training by calling `plot_sched`
    on `learn.recorder`. `learn.recorder` (as the name suggests) records everything
    that happens during training, including losses, metrics, and hyperparameters such
    as learning rate and momentum:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在`learn.recorder`上调用`plot_sched`来查看训练过程中的学习率和动量。`learn.recorder`（顾名思义）记录了训练过程中发生的一切，包括损失、指标和超参数，如学习率和动量：
- en: '[PRE74]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '![](Images/dlcf_13in20.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in20.png)'
- en: 'Smith’s original 1cycle paper used a linear warmup and linear annealing. As
    you can see, we adapted the approach in fastai by combining it with another popular
    approach: cosine annealing. `fit_one_cycle` provides the following parameters
    you can adjust:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Smith的原始1cycle论文使用了线性热身和线性退火。正如您所看到的，我们通过将其与另一种流行方法——余弦退火相结合，在fastai中改进了这种方法。`fit_one_cycle`提供了以下您可以调整的参数：
- en: '`lr_max`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`lr_max`'
- en: The highest learning rate that will be used (this can also be a list of learning
    rates for each layer group, or a Python `slice` object containing the first and
    last layer group learning rates)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用的最高学习率（这也可以是每个层组的学习率列表，或包含第一个和最后一个层组学习率的Python `slice`对象）
- en: '`div`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`div`'
- en: How much to divide `lr_max` by to get the starting learning rate
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 将`lr_max`除以多少以获得起始学习率
- en: '`div_final`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`div_final`'
- en: How much to divide `lr_max` by to get the ending learning rate
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 将`lr_max`除以多少以获得结束学习率
- en: '`pct_start`'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`pct_start`'
- en: What percentage of the batches to use for the warmup
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 用于热身的批次百分比
- en: '`moms`'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '`moms`'
- en: A tuple `(*mom1*,*mom2*,*mom3*)`, where *`mom1`* is the initial momentum, *`mom2`*
    is the minimum momentum, and *`mom3`* is the final momentum
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一个元组`(*mom1*,*mom2*,*mom3*)`，其中*`mom1`*是初始动量，*`mom2`*是最小动量，*`mom3`*是最终动量
- en: 'Let’s take a look at our layer stats again:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次查看我们的层统计数据：
- en: '[PRE75]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '![](Images/dlcf_13in21.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in21.png)'
- en: 'The percentage of nonzero weights is getting much better, although it’s still
    quite high. We can see even more about what’s going on in our training by using
    `color_dim`, passing it a layer index:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 非零权重的百分比正在得到很大的改善，尽管仍然相当高。通过使用`color_dim`并传递一个层索引，我们可以更多地了解我们的训练情况：
- en: '[PRE76]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![](Images/dlcf_13in22.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in22.png)'
- en: '`color_dim` was developed by fast.ai in conjunction with a student, Stefano
    Giomo. Giomo, who refers to the idea as the *colorful dimension*, provides an
    [in-depth explanation](https://oreil.ly/bPXGw) of the history and details behind
    the method. The basic idea is to create a histogram of the activations of a layer,
    which we would hope would follow a smooth pattern such as the normal distribution
    ([Figure 13-14](#colorful_dist)).'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '`color_dim`是由fast.ai与学生Stefano Giomo共同开发的。Giomo将这个想法称为*丰富多彩维度*，并提供了一个[深入解释](https://oreil.ly/bPXGw)这种方法背后的历史和细节。基本思想是创建一个层的激活直方图，我们希望它会遵循一个平滑的模式，如正态分布（[图13-14](#colorful_dist)）。'
- en: '![Histogram in colorful dimension](Images/dlcf_1314.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![丰富多彩维度的直方图](Images/dlcf_1314.png)'
- en: Figure 13-14\. Histogram in colorful dimension (courtesy of Stefano Giomo)
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-14。丰富多彩维度的直方图（由Stefano Giomo提供）
- en: 'To create `color_dim`, we take the histogram shown on the left here and convert
    it into just the colored representation shown at the bottom. Then we flip it on
    its side, as shown on the right. We found that the distribution is clearer if
    we take the log of the histogram values. Then, Giomo describes:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建`color_dim`，我们将左侧显示的直方图转换为底部显示的彩色表示。然后，我们将其翻转，如右侧所示。我们发现，如果我们取直方图值的对数，分布会更清晰。然后，Giomo描述：
- en: The final plot for each layer is made by stacking the histogram of the activations
    from each batch along the horizontal axis. So each vertical slice in the visualisation
    represents the histogram of activations for a single batch. The color intensity
    corresponds to the height of the histogram; in other words, the number of activations
    in each histogram bin.
  id: totrans-327
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个层的最终图是通过将每批次的激活直方图沿水平轴堆叠而成的。因此，可视化中的每个垂直切片代表单个批次的激活直方图。颜色强度对应直方图的高度；换句话说，每个直方图柱中的激活数量。
- en: '[Figure 13-15](#colorful_summ) shows how this all fits together.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-15](#colorful_summ)展示了这一切是如何结合在一起的。'
- en: '![Summary of the colorful dimension](Images/dlcf_1315.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![丰富多彩维度的总结](Images/dlcf_1315.png)'
- en: Figure 13-15\. Summary of the colorful dimension (courtesy of Stefano Giomo)
  id: totrans-330
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-15。丰富多彩维度的总结（由Stefano Giomo提供）
- en: This illustrates why log(*f*) is more colorful than *f* when *f* follows a normal
    distribution, because taking a log changes the Gaussian curve in a quadratic,
    which isn’t as narrow.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了为什么当*f*遵循正态分布时，log(*f*)比*f*更丰富多彩，因为取对数会将高斯曲线变成二次曲线，这样不会那么狭窄。
- en: 'So with that in mind, let’s take another look at the result for the penultimate
    layer:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们再次看看倒数第二层的结果：
- en: '[PRE77]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '![](Images/dlcf_13in23.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in23.png)'
- en: This shows a classic picture of “bad training.” We start with nearly all activations
    at zero—that’s what we see at the far left, with all the dark blue. The bright
    yellow at the bottom represents the near-zero activations. Then, over the first
    few batches, we see the number of nonzero activations exponentially increasing.
    But it goes too far and collapses! We see the dark blue return, and the bottom
    becomes bright yellow again. It almost looks like training restarts from scratch.
    Then we see the activations increase again and collapse again. After repeating
    this a few times, eventually we see a spread of activations throughout the range.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了一个经典的“糟糕训练”图片。我们从几乎所有激活都为零开始——这是我们在最左边看到的，所有的深蓝色。底部的明黄色代表接近零的激活。然后，在最初的几批中，我们看到非零激活数量呈指数增长。但它走得太远并崩溃了！我们看到深蓝色回来了，底部再次变成明黄色。它几乎看起来像是训练重新从头开始。然后我们看到激活再次增加并再次崩溃。重复几次后，最终我们看到激活在整个范围内分布。
- en: It’s much better if training can be smooth from the start. The cycles of exponential
    increase and then collapse tend to result in a lot of near-zero activations, resulting
    in slow training and poor final results. One way to solve this problem is to use
    batch normalization.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练一开始就能平稳进行会更好。指数增长然后崩溃的周期往往会导致大量接近零的激活，从而导致训练缓慢且最终结果不佳。解决这个问题的一种方法是使用批量归一化。
- en: Batch Normalization
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化
- en: To fix the slow training and poor final results we ended up with in the previous
    section, we need to fix the initial large percentage of near-zero activations,
    and then try to maintain a good distribution of activations throughout training.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决前一节中出现的训练缓慢和最终结果不佳的问题，我们需要解决初始大比例接近零的激活，并尝试在整个训练过程中保持良好的激活分布。
- en: 'Sergey Ioffe and Christian Szegedy presented a solution to this problem in
    the 2015 paper [“Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift”](https://oreil.ly/MTZJL). In the abstract, they describe
    just the problem that we’ve seen:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sergey Ioffe和Christian Szegedy在2015年的论文[“Batch Normalization: Accelerating
    Deep Network Training by Reducing Internal Covariate Shift”](https://oreil.ly/MTZJL)中提出了这个问题的解决方案。在摘要中，他们描述了我们所见过的问题：'
- en: Training Deep Neural Networks is complicated by the fact that the distribution
    of each layer’s inputs changes during training, as the parameters of the previous
    layers change. This slows down the training by requiring lower learning rates
    and careful parameter initialization…We refer to this phenomenon as internal covariate
    shift, and address the problem by normalizing layer inputs.
  id: totrans-340
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练深度神经网络的复杂性在于每一层输入的分布在训练过程中会发生变化，因为前一层的参数发生变化。这需要降低学习率和谨慎的参数初始化，从而减慢训练速度...我们将这种现象称为内部协变量转移，并通过对层输入进行归一化来解决这个问题。
- en: 'Their solution, they say is as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 他们说他们的解决方案如下：
- en: Making normalization a part of the model architecture and performing the normalization
    for each training mini-batch. Batch Normalization allows us to use much higher
    learning rates and be less careful about initialization.
  id: totrans-342
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将归一化作为模型架构的一部分，并对每个训练小批量进行归一化。批量归一化使我们能够使用更高的学习率，并且对初始化要求不那么严格。
- en: The paper caused great excitement as soon as it was released, because it included
    the chart in [Figure 13-16](#batchnorm), which clearly demonstrated that batch
    normalization could train a model that was even more accurate than the current
    state of the art (the *Inception* architecture) and around 5× faster.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文一经发布就引起了极大的兴奋，因为它包含了[图13-16](#batchnorm)中的图表，清楚地表明批量归一化可以训练出比当前最先进技术（*Inception*架构）更准确且速度快约5倍的模型。
- en: '![Impact of batch normalization](Images/dlcf_1316.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![批量归一化的影响](Images/dlcf_1316.png)'
- en: Figure 13-16\. Impact of batch normalization (courtesy of Sergey Ioffe and Christian
    Szegedy)
  id: totrans-345
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-16\. 批量归一化的影响（由Sergey Ioffe和Christian Szegedy提供）
- en: Batch normalization (often called *batchnorm*) works by taking an average of
    the mean and standard deviations of the activations of a layer and using those
    to normalize the activations. However, this can cause problems because the network
    might want some activations to be really high in order to make accurate predictions.
    So they also added two learnable parameters (meaning they will be updated in the
    SGD step), usually called `gamma` and `beta`. After normalizing the activations
    to get some new activation vector `y`, a batchnorm layer returns `gamma*y + beta`.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化（通常称为*batchnorm*）通过取层激活的均值和标准差的平均值来归一化激活。然而，这可能会导致问题，因为网络可能希望某些激活非常高才能进行准确的预测。因此，他们还添加了两个可学习参数（意味着它们将在SGD步骤中更新），通常称为`gamma`和`beta`。在将激活归一化以获得一些新的激活向量`y`之后，批量归一化层返回`gamma*y
    + beta`。
- en: 'That’s why our activations can have any mean or variance, independent from
    the mean and standard deviation of the results of the previous layer. Those statistics
    are learned separately, making training easier on our model. The behavior is different
    during training and validation: during training we use the mean and standard deviation
    of the batch to normalize the data, while during validation we instead use a running
    mean of the statistics calculated during training.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们的激活可以具有任何均值或方差，独立于前一层结果的均值和标准差。这些统计数据是分开学习的，使得我们的模型训练更容易。在训练和验证期间的行为是不同的：在训练期间，我们使用批次的均值和标准差来归一化数据，而在验证期间，我们使用训练期间计算的统计数据的运行均值。
- en: 'Let’s add a batchnorm layer to `conv`:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`conv`中添加一个批量归一化层：
- en: '[PRE78]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'and fit our model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 并适应我们的模型：
- en: '[PRE79]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.130036 | 0.055021 | 0.986400 | 00:10 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.130036 | 0.055021 | 0.986400 | 00:10 |'
- en: 'That’s a great result! Let’s take a look at `color_dim`:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的结果！让我们看看`color_dim`：
- en: '[PRE80]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '![](Images/dlcf_13in24.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlcf_13in24.png)'
- en: 'This is just what we hope to see: a smooth development of activations, with
    no “crashes.” Batchnorm has really delivered on its promise here! In fact, batchnorm
    has been so successful that we see it (or something very similar) in nearly all
    modern neural networks.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们希望看到的：激活的平稳发展，没有“崩溃”。Batchnorm在这里真的兑现了承诺！事实上，批量归一化非常成功，我们几乎可以在所有现代神经网络中看到它（或类似的东西）。
- en: An interesting observation about models containing batch normalization layers
    is that they tend to generalize better than models that don’t contain them. Although
    we haven’t as yet seen a rigorous analysis of what’s going on here, most researchers
    believe that the reason is that batch normalization adds some extra randomness
    to the training process. Each mini-batch will have a somewhat different mean and
    standard deviation than other mini-batches. Therefore, the activations will be
    normalized by different values each time. In order for the model to make accurate
    predictions, it will have to learn to become robust to these variations. In general,
    adding additional randomization to the training process often helps.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Since things are going so well, let’s train for a few more epochs and see how
    it goes. In fact, let’s *increase* the learning rate, since the abstract of the
    batchnorm paper claimed we should be able to “train at much higher learning rates”:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.191731 | 0.121738 | 0.960900 | 00:11 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.083739 | 0.055808 | 0.981800 | 00:10 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.053161 | 0.044485 | 0.987100 | 00:10 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.034433 | 0.030233 | 0.990200 | 00:10 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.017646 | 0.025407 | 0.991200 | 00:10 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '[PRE82]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.183244 | 0.084025 | 0.975800 | 00:13 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.080774 | 0.067060 | 0.978800 | 00:12 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.050215 | 0.062595 | 0.981300 | 00:12 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.030020 | 0.030315 | 0.990700 | 00:12 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.015131 | 0.025148 | 0.992100 | 00:12 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: At this point, I think it’s fair to say we know how to recognize digits! It’s
    time to move on to something harder…
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve seen that convolutions are just a type of matrix multiplication, with
    two constraints on the weight matrix: some elements are always zero, and some
    elements are tied (forced to always have the same value). In [Chapter 1](ch01.xhtml#chapter_intro),
    we saw the eight requirements from the 1986 book *Parallel Distributed Processing*;
    one of them was “A pattern of connectivity among units.” That’s exactly what these
    constraints do: they enforce a certain pattern of connectivity.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: These constraints allow us to use far fewer parameters in our model, without
    sacrificing the ability to represent complex visual features. That means we can
    train deeper models faster, with less overfitting. Although the universal approximation
    theorem shows that it should be *possible* to represent anything in a fully connected
    network in one hidden layer, we’ve seen now that in *practice* we can train much
    better models by being thoughtful about network architecture.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions are by far the most common pattern of connectivity we see in neural
    nets (along with regular linear layers, which we refer to as *fully connected*),
    but it’s likely that many more will be discovered.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve also seen how to interpret the activations of layers in the network to
    see whether training is going well or not, and how batchnorm helps regularize
    the training and makes it smoother. In the next chapter, we will use both of those
    layers to build the most popular architecture in computer vision: a residual network.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Questionnaire
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a feature?
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write out the convolutional kernel matrix for a top edge detector.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write out the mathematical operation applied by a 3×3 kernel to a single pixel
    in an image.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the value of a convolutional kernel applied to a 3×3 matrix of zeros?
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is padding?
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is stride?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a nested list comprehension to complete any task that you choose.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the shapes of the `input` and `weight` parameters to PyTorch’s 2D convolution?
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a channel?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the relationship between a convolution and a matrix multiplication?
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a convolutional neural network?
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the benefit of refactoring parts of your neural network definition?
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `Flatten`? Where does it need to be included in the MNIST CNN? Why?
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does NCHW mean?
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does the third layer of the MNIST CNN have `7*7*(1168-16)` multiplications?
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a receptive field?
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the size of the receptive field of an activation after two stride-2
    convolutions? Why?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run *conv-example.xlsx* yourself and experiment with *trace precedents*.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have a look at Jeremy or Sylvain’s list of recent Twitter “likes,” and see if
    you find any interesting resources or ideas there.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is a color image represented as a tensor?
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a convolution work with a color input?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What method can we use to see that data in `DataLoaders`?
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we double the number of filters after each stride-2 conv?
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we use a larger kernel in the first conv with MNIST (with `simple_cnn`)?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What information does `ActivationStats` save for each layer?
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we access a learner’s callback after training?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the three statistics plotted by `plot_layer_stats`? What does the x-axis
    represent?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are activations near zero problematic?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the upsides and downsides of training with a larger batch size?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why should we avoid using a high learning rate at the start of training?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is 1cycle training?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the benefits of training with a high learning rate?
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we want to use a low learning rate at the end of training?
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is cyclical momentum?
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What callback tracks hyperparameter values during training (along with other
    information)?
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does one column of pixels in the `color_dim` plot represent?
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does “bad training” look like in `color_dim`? Why?
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What trainable parameters does a batch normalization layer contain?
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What statistics are used to normalize in batch normalization during training?
    How about during validation?
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do models with batch normalization layers generalize better?
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further Research
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What features other than edge detectors have been used in computer vision (especially
    before deep learning became popular)?
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Other normalization layers are available in PyTorch. Try them out and see what
    works best. Learn about why other normalization layers have been developed and
    how they differ from batch normalization.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try moving the activation function after the batch normalization layer in `conv`.
    Does it make a difference? See what you can find out about what order is recommended
    and why.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
