- en: 2 Introduction to large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An overview of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key use cases powered by LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundational models and their effect on AI development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New architecture concepts for LLMs, such as prompts, prompt engineering, embeddings,
    tokens, model parameters, context window, and emergent behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of small language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of open source and commercial LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models (LLMs) are generative AI models that can understand and
    generate human-like text based on a given input. LLMs are the foundation of many
    natural language processing (NLP) tasks, such as search, speech-to-text, sentiment
    analysis, text summarization, and more. In addition, they are general-purpose
    language models that are pretrained and can be fine-tuned for specific tasks and
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores the fascinating world of LLMs and their transformative
    effect on artificial intelligence (AI). As a significant advancement in AI, LLMs
    have demonstrated remarkable capabilities in understanding and generating human-like
    text, thus enabling numerous applications across various industries. Here, we
    dive into the critical use cases of LLMs, the different types of LLMs, and the
    concept of foundational models that has revolutionized AI development.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter discusses essential LLM concepts, such as prompts, prompt engineering,
    embeddings, tokens, model parameters, context windows, transformer architecture,
    and emergent behavior. Finally, we compare open source and commercial LLMs, highlighting
    their advantages and disadvantages. By the end of this chapter, you will have
    a comprehensive understanding of LLMs and their implications for AI applications
    and research. LLMs are built on foundational models; therefore, we will start
    by outlining what these models are before discussing LLMs in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Overview of foundational models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced by Stanford researchers in 2021, foundational models have substantially
    transformed the construction of AI systems. They diverge from task-specific models,
    shifting to broader, more adaptable models trained on large data volumes. These
    models can excel in diverse natural language tasks, such as machine translation
    and question answering, as they learn general language representations from extensive
    text and code datasets. These representations can then be used to perform various
    tasks, even tasks they were not explicitly trained on, as shown in figure 2.1.
  prefs: []
  type: TYPE_NORMAL
- en: In more technical terms, foundational models utilize established machine learning
    techniques such as self-supervised learning and transfer learning, enabling them
    to apply acquired knowledge across various tasks. Developed by means of deep learning,
    these models employ multilayered artificial neural networks to comprehend complex
    data patterns; hence, their proficiency with unstructured data such as images,
    audio, and text. This also extends to 3D signals—data representing 3D attributes
    that capture spatial dimensions and depth, such as 3D point clouds from LiDAR
    sensors, 3D medical imaging such as CT scans, or 3D models used in computer graphics
    and simulations. These can be utilized to make predictions based on 3D data for
    tasks such as object recognition, scene understanding, and navigation in robotics
    and autonomous vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Transfer learning is a machine learning technique in which a model developed
    for one task is reused as a starting point for a similar task. Instead of starting
    from scratch, we use the knowledge from the previous task to perform better on
    the new one. It’s like using knowledge from a previous job to excel at a new but
    related job.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI and foundational models are closely interlinked. As outlined,
    foundational models, trained on massive datasets, can be adapted to perform various
    tasks; this property makes them particularly suitable for generative AI and allows
    for creating new content. The broad knowledge base of these models allows for
    effective transfer learning, which can be used to generate new, contextually appropriate
    content across diverse domains. They represent a unified approach, where a single
    model can generate various outputs, offering state-of-the-art performance owing
    to their extensive training. Without foundational models as the backbone, there
    would be no generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F01_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 Foundational model overview
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here are some examples of the common foundation models:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT (Generative Pre-trained Transformer) Family is an NLP family of models developed
    by OpenAI. It is a large language model trained on a massive dataset of text and
    code, which makes it capable of generating text, translating languages, writing
    creative content, and answering your questions informatively. GPT-4, the latest
    version at the time of this writing, is also a multimodal model—it can manage
    both language and images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codex is a large language model trained specifically on code that is used to
    help with code generation. It supports over a dozen programming languages, including
    some of the more commonly used, such as C#, Java, Python, JavaScript, SQL, Go,
    PHP, and Shell, among others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claude is an LLM built by a startup called Anthropic. Like OpenAI’s ChatGPT,
    it predicts the next token in a sequence when given a certain prompt and can generate
    text, write code, summarize, and reason.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT (Bidirectional Encoder Representations from Transformers) is an NLP model
    developed by Google. It is a bidirectional model, meaning it can process text
    in both directions, from left to right and right to left. This feature makes it
    better at understanding the context of words and phrases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PaLM (Pathway Language Model) and its successor PaLM2 are large multimodal language
    models developed by Google. The multimodal model can process text, code, and images
    simultaneously, making it capable of performing a wider range of tasks across
    those modalities compared to traditional language models operating only in one
    modality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemini is Google’s latest AI model, capable of understanding text, images, videos,
    and audio. It’s a multimodal model described as being able to complete complex
    tasks in math, physics, and other areas, as well as understanding and generating
    high-quality code in various programming languages. Gemini was built from the
    ground up to be multimodal, meaning it can generalize and seamlessly understand,
    operate across, and combine different types of information. It’s also the new
    umbrella name for all of Google’s AI tools, replacing Google Bard and Duet AI,
    and is considered a successor to the PaLM model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a foundational model is trained, it can be adapted to a wide range of downstream
    tasks by fine-tuning its parameters. Fine-tuning involves adjusting the model’s
    parameters to optimize the model for a specific task. It can be done using a small
    amount of labeled data. By fine-tuning these models for specific tasks or domains,
    we use their general understanding of language and supplement it with task-specific
    knowledge. The benefits of this approach include time and resource efficiency,
    coupled with remarkable versatility. We can also adapt a model via Prompt engineering,
    which we’ll discuss later in this chapter. Now that we know more about foundational
    models, let’s explore LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Overview of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs represent a significant advancement in AI. They are trained on a vast amount
    of text data, such as books, articles, and websites, to learn patterns in human
    language. They are also hard to develop and maintain, as they require lots of
    data, computing, and engineering resources. OpenAI’s ChatGPT is an example of
    an LLM—it generates human-like text by predicting the probability of a word considering
    the words already used in the text.
  prefs: []
  type: TYPE_NORMAL
- en: The model learns to generate coherent and contextually relevant sentences by
    adjusting its internal parameters to minimize the difference between its predictions
    and the actual outcomes in the training data. When generating text, the model
    chooses the word with the highest probability as its subsequent output and then
    repeats the process for the next word.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are foundational models adapted for natural language processing and language
    generation tasks. These LLMs are general-purpose and can handle tasks without
    task-specific training data. As briefly described in the previous chapter, given
    the right prompt, they can answer questions, write essays, summarize texts, translate
    languages, and even generate code. LLMs can be applied to many applications across
    different industries, as outlined in chapter 1—from summarization to classification,
    Q&A chatbots, content generation, data analysis, entity extraction, and more.
    Before we get into more details of LLMs, let us look at the Transformer architecture,
    which makes these foundational models possible.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Transformer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers are the bedrock of foundational models and are responsible for
    their remarkable language understanding capabilities. The Transformer model was
    first introduced in the paper “Attention Is All You Need” by Vaswani et al. in
    2017 [1]. Since then, Transformer-based models have become state-of-the-art for
    many tasks. GPT and BERT are examples of Transformer-based models, and the “T”
    in GPT stands for Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: At their core, Transformers use a mechanism known as attention (specifically
    self-attention), which allows the model to consider the entire context of a sentence,
    considering all words simultaneously rather than processing the sentence word
    by word. This approach is more efficient and can improve the results of many NLP
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The strength of this approach is that it captures dependencies regardless of
    their position in the text, which is an essential factor in language understanding.
    This is key for tasks such as machine translation and text summarization, where
    the meaning of a sentence can depend on terms that are several words apart.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers can parallelize their computations, which makes them much faster
    to train than other types of neural networks. This mechanism enables the model
    to pay attention to the most relevant parts of the task input.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of generative AI, a transformer model would take an input (such
    as a prompt) and generate an output (such as the next word or the completion of
    the sentence) by weighing the importance of each part of the input in generating
    the output. For example, in the sentence “The cat sat on the...,” a Transformer
    model would likely give much weight to the word “cat” when determining that the
    likely next word might be “mat.” These models exhibit generative properties by
    predicting the next item in a sequence—the next word in a sentence or the next
    note in a melody. We explore this more in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models are usually very large, requiring significant computational
    resources to train and use. Using a car analogy, think of Transformer models as
    supercharged engines that need much power to run but do amazing things. Think
    of them as the next step after models such as ResNET 50, which is used for recognizing
    images. While ResNET 50 is like a car with 50 gears, OpenAI’s GPT-3 is like a
    megatruck with 96 gears and extra features. Because of their advanced capabilities,
    these models are a top pick for creating intelligent AI outputs.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs use transformers, which are composed of an encoder and a decoder. The encoder
    processes the input text (i.e., the prompt) and generates a sequence of hidden
    states that represent the meaning of the input text. The decoder uses these hidden
    states to generate the output text. These encoders and decoders form one layer,
    similar to a mini-brain. Multiple layers can be stacked one upon another. As outlined
    earlier, GPT3 is a decoder-only model with 96 layers.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Training cutoff
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of foundational models, the training cutoff refers to the point
    at which the model’s training ends, that is, the time until the data used to train
    the model was collected. In the case of AI models developed by OpenAI, such as
    GPT-3 or GPT-4, the training cutoff is when the model was last trained on new
    data.
  prefs: []
  type: TYPE_NORMAL
- en: This cutoff is important because after this point, the model is not aware of
    any events, advancements, new concepts, or changes in language usage. For example,
    the training data cutoff for the GPT-3.5 Turbo was in September 2021, GPT-4 Turbo
    in April 2023, and GPT-4o in October 2023, meaning the model does not know about
    real-world events or advancements in various fields beyond that point.
  prefs: []
  type: TYPE_NORMAL
- en: The key point is that while these models can generate text based on the data
    they were trained on, they do not learn or update their knowledge after the training
    cutoff. They cannot access or retrieve real-time information from the internet
    or any external database. Their responses are generated purely based on patterns
    they have learned during their training period.
  prefs: []
  type: TYPE_NORMAL
- en: Note  The recent announcement that the premium versions of ChatGPT will have
    access to the internet via the Bing plugin doesn’t mean that the model has more
    up-to-date information. This uses a pattern called RAG (retrieval-augmented generation),
    which will be covered later in chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Types of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in table 2.1, there are three categories of LLMs. When we talk about
    LLMs, having the context is crucial, and it might not be evident in some cases.
    This is of great importance, as the paths we can go down when using the models
    aren’t interchangeable, and picking the right type depends on the use case one
    tries to solve. Furthermore, there is also a dependency on how effectively one
    can adapt the models to specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 Types of LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| LLM Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Base LLM  | These are the original models, pretrained on a massive corpus
    of text data, and they can generate text based on the patterns they learned during
    this training. Some also call these raw language models or even refer to them
    as foundational models; they can be used out of the box to generate text. They
    learn powerful and general representations but lack specific expertise in a particular
    task. GPT-3’s DaVinci model is an example of a base LLM.  |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction-based LLM  | This involves using a base LLM and providing explicit
    instructions in prompt input. In many examples we saw in the previous chapter,
    we instructed the model to follow instructions, such as “Translate the following
    text to French:” or “Summarize this article:” Sometimes, these models are also
    called instruction-tuned LLMs.  |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned LLM  | Fine-tuning involves taking the base LLM and training it
    further on a task it might perform poorly at, often in a specific domain. An example
    would be training the model on medical literature if we want it to understand
    medical topics or training it on customer service interactions if we want it to
    respond to customer inquiries for a specific industry. Fine- tuning can help make
    the model more accurate or helpful to particular tasks or domains, but it requires
    additional data and training time.  |'
  prefs: []
  type: TYPE_TB
- en: 'It’s worth noting that all these methods have their pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: Base LLMs are versatile and can handle many tasks without additional training.
    However, they might not be as accurate or reliable as you’d like for specific
    tasks or domains, especially in an enterprise setting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruction-based usage can be very effective for some tasks, but it requires
    careful prompt crafting and doesn’t fundamentally change the model’s capabilities.
    This is where many of the prompt engineering techniques and best practices apply.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning can yield excellent results for specific tasks or domains. However,
    it requires additional resources and comes with the risk of overfitting the training
    data, which could limit the model’s ability to generalize to new examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, one can take approaches (zero-shot, few-shot, and transfer learning)
    to adapt the LLM further for a specific task to make it perform better and be
    more robust in those specific domains. To some extent, the type of LLM implemented
    also dictates which approach is better suited for the need. Each approach has
    its strengths and weaknesses, and the best one depends on the specific task, data
    available, and resources at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Zero-shot learning*—A model can generalize to a task without seeing examples
    of that task during training. For example, we could ask a model trained only in
    English to translate from English to German, even if it hasn’t seen any specific
    examples of the German language during training. It can do this zero-shot translation
    using semantic similarity, word embeddings, and machine learning. Using these
    techniques, we can measure how similar two words are even in different languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Few-shot learning*—This involves showing the model examples of the task we
    want to perform and then asking the model to do the same task with a new example.
    Thus, if we want a model to identify the sentiment of a tweet, we might show it
    a few examples of tweets and associated sentiment and then ask it to predict the
    sentiment of a new tweet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transfer learning*—This involves training a model on one task and then using
    what it learned for a different but related task. For example, although LLMs have
    been trained in language and not specific customer support tickets, they can be
    used to categorize customer support tickets into different categories, such as
    billing, technical issues, or general inquiries. This can help streamline the
    customer support process and ensure a speedy resolution, with higher customer
    satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.6 Small language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Small language models (SLMs) are an emerging trend that has excited many enterprises.
    They are scaled-down versions of larger language models designed to provide many
    benefits of their larger counterparts while being more resource efficient and
    accessible. They differ from LLMs (e.g., GPT-4) in several ways, primarily in
    size and complexity, computational resources, training and operational costs,
    and performance quality. Techniques such as knowledge distillation and transfer
    learning empower smaller models to excel in analysis, translation, and summarization
    with faster training. In some cases, they can also match or outperform the larger
    LLMs, making them a game changer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding size and complexity, SLMs are notably smaller and have fewer parameters
    than large models such as GPT-4\. This difference in scale is substantial: a small
    model may have millions to a few billion parameters, in contrast to the tens or
    hundreds of billions of parameters in large models. This reduction in size and
    complexity makes SLMs fundamentally different in how they process and generate
    language.'
  prefs: []
  type: TYPE_NORMAL
- en: From a computational resource perspective, SLMs’ smaller size requires less
    computational power. This reduced requirement makes them more suitable for applications
    with limited processing capabilities or situations where real-time responses are
    crucial. The lesser demand for computational resources also means that SLMs can
    be deployed in a wider range of environments, including on-edge devices or systems
    with lower processing capacities.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding training and operational costs, SLMs are generally less expensive
    to train and operate. This cost-effectiveness stems from their reduced complexity
    and the smaller amount of data needed to train them. Consequently, SLMs become
    more accessible to individuals and organizations with limited budgets, democratizing
    access to advanced language-processing technologies.
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to performance and quality, while capable of handling
    a wide range of language tasks effectively, SLMs generally do not match the performance
    level of larger models. This is particularly evident in more complex tasks that
    require a broad understanding of context or specialized knowledge. Large models,
    with their greater depth and breadth of data and understanding, are better equipped
    to handle such complexities. In contrast, SLMs might struggle with these challenges
    due to their inherent limitations in size and training.
  prefs: []
  type: TYPE_NORMAL
- en: Strategic data selection and a new, innovative training approach are the two
    key reasons why SLMs such as the Phi series have been so successful. Strategic
    data selection prioritizes high quality over quantity and involves using textbook-quality
    data, which includes synthetic datasets and meticulously curated web data [2].
    Data is selected to provide a robust foundation of common-sense reasoning and
    general knowledge. This strategic approach to data selection is crucial for the
    model’s exceptional performance across a spectrum of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the innovative training approach includes scaling up from smaller
    models such as Phi-1.5 and embedding its knowledge in Phi-2 [3]. This method accelerates
    training convergence and enhances benchmark scores, challenging conventional scaling
    laws and demonstrating that remarkable capabilities can be achieved even with
    smaller-scale language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'SLMs are still early in the lifecycle but have been increasingly considered
    for production use in enterprises. However, their readiness largely depends on
    the specific requirements and application context. Here are some factors to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Task complexity*—SLMs are suitable for simpler, more defined tasks. However,
    a larger model might be necessary for optimal performance if the enterprise application
    involves complex language understanding or generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resource constraints*—SLMs are an excellent choice for businesses with limited
    computational resources or those needing to deploy models on edge devices due
    to their lower resource requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost efficiency*—Operating SLMs is generally more cost-effective than operating
    larger models in terms of computational resources and energy consumption. This
    can be a significant advantage for enterprises looking to minimize operational
    costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Speed and responsiveness*—SLMs can offer faster response times, which are
    beneficial for applications where real-time interaction is critical, such as in
    customer service chatbots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*On-premise*—For applications that cannot be deployed or connected to the cloud
    for regulatory or policy reasons, SLMs can be an option, as they can be more easily
    deployed on-premises or in a private cloud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few examples of SLMs available today include
  prefs: []
  type: TYPE_NORMAL
- en: '*Phi-3*—A family of small language models, which is a Phi-2 successor, recently
    introduced by Microsoft. They are noted for outperforming other models of similar
    or even larger sizes across various benchmarks and come in three sizes: mini (3.8B),
    small (7B), and medium (14B).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Phi-2*—A 2.7 billion parameter model from Microsoft that demonstrates state-of-the-art
    performance on reasoning and language-understanding tasks, which can outperform
    models 25x its size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Orca 2*—A 7-billion- or 13-billion-parameter model from Microsoft that learns
    various reasoning techniques and solution strategies from a more capable teacher
    model 5\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gemini Nano*—A 122-million-parameter model from Google, part of the Gemini
    series, designed for efficient inference and deployment on edge devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DistilBERT*—A smaller version of BERT that retains 97% of its language understanding
    capabilities while being 40% smaller and 60% faster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GPT-Neo*—A smaller version of the GPT architecture (125M and 1.3B), part of
    the GPT-Neo series created by EleutherAI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These SLMs are particularly useful when deploying a large model is infeasible
    due to cost, speed, or computational requirements. They balance performance and
    efficiency, making advanced NLP capabilities more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: While SLMs might not be suitable for every enterprise application, especially
    those requiring deep understanding or complex language generation, they are ready
    for production in many scenarios, particularly where efficiency, speed, and cost
    are key considerations. Enterprises should evaluate their specific needs and constraints
    to determine if an SLM is the right choice for their application.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Open source vs. commercial LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today’s commercial models provide top-notch performance in terms of AI quality
    and wide-ranging abilities. However, since the release of ChatGPT, there has been
    a significant shift toward open source models. Many of these open source initiatives
    focus on developing smaller foundational models, asserting they can achieve nearly
    the same quality levels without significant loss. Figure 2.2 [4] illustrates these
    lineages and how they have exploded quickly.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F02_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2 Timeline of LLMs with 10+B parameters: A Survey of LLMs'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 2.7.1 Commercial LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Currently, there are several commercial LLMs, and they will print and have
    started making traction. Almost all of them follow the OpenAI paradigm and are
    exposed via an API we use. Although still startups, many have serious funding,
    and their founders have deep research backgrounds:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI is an AI research lab that develops and publishes cutting-edge AI models,
    such as the GPT series. It certainly has the most mindshare today. It has several
    foundational models, such as GPT-4, DALL.E, and ChatGPT, and is the most mature
    in this group, with serious backing and ownership by Microsoft.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure OpenAI and OpenAI offer access to powerful language models but differ
    in their nature and implementation. The primary distinction is that Azure OpenAI
    is a managed service, whereas OpenAI is not. Microsoft handles the underlying
    infrastructure and maintenance for Azure OpenAI, making it a valuable option for
    businesses lacking the resources to manage their OpenAI deployment. Furthermore,
    Azure OpenAI packages the raw OpenAI models into developer-friendly services that
    developers can seamlessly integrate into their applications. These services run
    on Azure, ensuring high scalability, reliability, and global availability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anthropic is a startup founded by ex-OpenAI engineers that has released Claude,
    an LLM that can generate text and code. Their key differentiator is implementing
    the LLM using constitutional AI [5]. Constitutional AI uses reinforcement learning
    (RL) and traditional supervised learning and claims to produce less harmful outputs.
    As of this publication, Anthropic was backed by both Google and Amazon. Claude
    3, the latest family of models, has three versions: Haiku (small-sized), Sonnet
    (medium-sized), and Opus (large-sized) models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemini is Google’s latest GenAI model, available as part of the Google Cloud
    offering in the recently launched Google AI Studio product. At the time of this
    writing, Google is opening up API access to the models in private preview.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohere AI, a startup originating from a Transformer paper (“Attention is all
    you need”), has an LLM and other products such as Neural Search and Embed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.7.2 Open source LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A new crop of OSS LLMs is coming up, and some of these will compete with ChatGPT.
    As figure 2.1 shows, there are too many to mention, but a few noteworthy ones
    are listed in table 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.2 Open source LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Company | Open source LLM | Parameter size |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Meta  | Llama LLM is one of the models that has inspired many other OSS models.
    It comes in many sizes (7B, 13B, 33B, and 65B), and while smaller than GPT-3,
    it can be matched across many tasks. Meta shared the models with researchers (and
    they were also leaked separately online), inspiring many others to use that as
    a jumping-off point.  | Various (7B–65B)  |'
  prefs: []
  type: TYPE_TB
- en: '| Databricks  | Databricks recently released v2 of Dolly, which they label
    the “world’s first truly open instruction-tuned LLM.” It is released under the
    CCA 3.0 license, allowing anyone to use, extend, and modify it, including for
    commercial purposes.  | 12B  |'
  prefs: []
  type: TYPE_TB
- en: '| Alpaca  | Standford University’s Alpaca, an instruction model based on Llama,
    claims to match the GPT-3.5 Turbo performance in some tasks.  | 7B  |'
  prefs: []
  type: TYPE_TB
- en: '| FreedomGPT  | This is an OSS conversational agent based on Alpaca. They claim
    to offer 100% uncensored and private conversations.  | Not disclosed  |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna  | Academic researchers from multiple institutions (UC Berkeley, CMU,
    Stanford, UC San Diego, and MBZUAI) released Vicuna, a fine-tuned version of Llama
    that matches the GPT4 performance across many tasks.  | 13B  |'
  prefs: []
  type: TYPE_TB
- en: '| Koala  | Berkley AI Research released Koala, a fine-tuned version of Llama
    using internet dialogues.  | 13B  |'
  prefs: []
  type: TYPE_TB
- en: '| ChatLLaMa  | Technically, it’s not a model but tooling for models. Nebuly
    AI released ChatLLaMa, a library that can create ChatGPT-like conversational assistance
    using your data.  | 7B  |'
  prefs: []
  type: TYPE_TB
- en: '| ColossalChat  | UC Berkeley’s ColossalAI project released ColossalChat, a
    ChatGPT-like model that includes complete RLHF pipelines based on Llama.  | 7B  |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon  | The Technology Innovation Institute (TII) in the United Arab Emirates
    released a family of LLMs called the Falcon LLM model. At the time, Falcon was
    the biggest OSS LLM ever released and was at the top of the OSS LLM Leaderboard.
    More recently, a more powerful 180B parameters model is again at the top of the
    leaderboard.  | Various (1B–180B)  |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral  | Mistral AI, a French startup, has developed a range of models.
    Some are open source models licensed under Apache 2.0, a permissive license allowing
    unrestricted use in any context. As mentioned in the previous chapter, they also
    have commercial models.  | Various (7B – 141B)  |'
  prefs: []
  type: TYPE_TB
- en: OpenAI vs. Azure OpenAI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Azure OpenAI and OpenAI are both services that provide access to OpenAI’s powerful
    language models, but they have some key differences. OpenAI caters more to smalland
    medium-business individual developers and startups. In contrast, Azure OpenAI
    is intended for enterprises that need additional security and availability in
    different parts of the world and that have regulatory needs.
  prefs: []
  type: TYPE_NORMAL
- en: Azure OpenAI offers additional enterprise-ready features, such as data privacy,
    customer-managed keys, encryption at rest, private networking, regional availability,
    and responsible AI content filtering. These features can be important for businesses
    that need to comply with specific security or regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the APIs between the two are similar but not the same. However,
    the underlying models are the same, and Azure OpenAI has a deployment that incorporates
    these additional features that most enterprises require.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Key concepts of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes the architecture of a typical LLM implementation. Figure
    2.3 shows the abstract structure of a common LLM implementation at a high level;
    it follows this process whenever we use an LLM such as OpenAI’s GPT.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F03_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 Conceptual architecture of an LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The model starts with the input text—the prompt. It is first converted into
    a sequence of tokens using tokenization. Each token is then converted into a numerical
    vector via a process called embedding, which acts as the encoder input.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder processes the input sequence and generates a sequence of hidden
    states. These hidden states are then fed into the decoder with a start token.
    The decoder generates the output sequence one token at a time by predicting the
    next token based on the previous tokens and hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the layers have processed the information, the model predicts the next
    token in the learned sequence. This outcome is converted back to the text, and
    we see the response. This process runs in an iterative loop and occurs for each
    new token generated, thus creating a coherent text output. The final text that
    the model generates is an emergent property of this layered, iterative process.
    The final output sequence is also called a completion.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine each of these LLM aspects in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.1 Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A prompt is how we “talk” to these models. A prompt is just text describing
    the task we are trying to achieve using natural language. The output of these
    models is also text. The ability to express our intention in this manner (natural
    language) instead of conforming to the input restrictions of a machine makes prompts
    powerful. Crafting or designing the text in the prompt is akin to programming
    the model and creating a new paradigm called prompt engineering, which will be
    covered later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompts allow us to capture a broader set of intents and emotions using the
    same paradigm. In the following two examples (generated by AI), a prompt asks
    for a Haiku outlining how great LLMs are: one haiku is happy, and the other is
    sad. Getting this level of emotion and expressiveness on the same task was impossible.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Write a happy haiku on how great LLMs are.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**AI-generated response'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are great
  prefs: []
  type: TYPE_NORMAL
- en: They can chat and generate
  prefs: []
  type: TYPE_NORMAL
- en: Creative and smart
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Write a sad haiku on how great LLMs are.'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**AI-generated response'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are great
  prefs: []
  type: TYPE_NORMAL
- en: But they cannot feel or love
  prefs: []
  type: TYPE_NORMAL
- en: Only simulate
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  LLMs do not understand text like humans do. They are statistical models
    that learn patterns in data and use those patterns to generate output. Despite
    their impressive output, these models don’t comprehend the content generated in
    the same sense as we do—including beliefs, desires, consciousness, and right and
    wrong. They are just extremely good at predicting the next word in a sequence
    of text based on patterns they’ve seen millions of times.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.2 Tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tokens are the basic units of text that an LLM uses to process both the request
    and the response, that is, to understand and generate text. Tokenization is the
    process of converting text into a sequence of smaller units called tokens. When
    using LLMs, we use tokens to converse with these models, which is one of the most
    fundamental elements of understanding LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens are the new currency when incorporating LLMs into your application or
    solutions. They directly correlate with the cost of running models, both in terms
    of money and of the experience with latency and throughput. The more tokens, the
    more processing the model must do. This means more computational resources are
    required for the model, which means lower performance and higher latency.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs convert the text into tokens before processing. Depending on the tokenization
    algorithm, they can be individual characters, words, sub-words, or even larger
    linguistic units. A rough rule of thumb is that one token is approximately four
    characters or 0.75 words for English text. For most LLMs today, the token size
    that they support includes both the input prompt and the response.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate this through an example. Figure 2.4 shows how the sentence
    “I have a white dog named Champ” gets tokenized (using OpenAI’s tokenizer in this
    case). Each block represents a different token. In this example, we use eight
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F04_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 Tokenizer example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs generate text by predicting the next word or symbol (token) most likely
    to follow a given sequence of words or symbols (tokens) they use as input, that
    is, the prompt. We show a visual representation of this in figure 2.5, where the
    list of tokens on the right shows the highest probability of tokens following
    the prompt “The dog sat on.” We can influence some of this probability of tokens
    using a few parameters we will see later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F05_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 LLM—next token predictor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Suppose we have a sequence of tokens with a length of n. Utilizing these n tokens
    as the context, we generate the subsequent token, n + 1\. This newly predicted
    token is then appended to the original sequence of tokens, thereby expanding the
    context. Consequently, the expanded context window for generating token n + 2
    becomes n + (n + 1). This process is repeated in a continuous loop until a predetermined
    stop condition, such as a specific sequence or a size limit for the tokens, is
    reached.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have a sentence, “Hawaiian pizza is my favorite,” the probability
    distribution of the next word we see is shown in figure 2.6\. The most likely
    word is “type,” finishing the sentence “Hawaiian pizza is my favorite type.”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F06_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 Next token probability distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you run this example again, you will get a probability different from the
    one shown here. This is because most AI is nondeterministic, specifically in the
    case of LLMs. Simultaneously, it might predict one token, and it is probably being
    looked at across all the possible tokens that the model has learned in the training
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: We also use two examples that outline how one token changes the distribution
    dramatically (changing one word from “the” to “a”). Figure 2.7 shows that the
    most probable next token is “mat” at 41% probability. We also see a list of the
    other tokens and their probabilistic distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F07_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 Example 1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, changing one token from “the” to “a” dramatically changes the next
    distribution set, with the mat jumping up 30 points to a probability of nearly
    75%, as shown in figure 2.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F08_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 Example 2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some settings related to LLMs are important and can change how the model behaves
    and generates text. These settings are the model configurations and can be changed
    via an API, GUI, or both. We cover model configurations in more detail later in
    the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.3 Counting tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many developers will probably be new to tracking tokens when using LLM, especially
    in an enterprise setting. However, counting tokens is important for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Memory limitations*—LLMs can process a maximum number of tokens in a single
    pass. This is due to the memory limitations of their architecture, often defined
    by their context window (another concept we discuss later in this chapter). For
    example, OpenAI’s latest GPT-4o model has a content window of 128K, and Google’s
    latest Gemini 1.5 Pro has a context window of 1M tokens. GPT3.5-Turbo, another
    OpenAI model, has two models supporting 8K and 16K token lengths. There is research
    ongoing to see how to solve this, such as LongNet [6] from Microsoft Research,
    which shows how to scale to 1B context windows. It is important to point out that
    this is still an active research area and has not been productized yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost*—When thinking about cost, there are two dimensions: the computational
    costs in terms of latency, memory, and the overall experience, and the actual
    cost in terms of money. For each call, the computational resources required for
    processing tokens directly correlate to the tokens’ length. As the token length
    increases, it requires more processing time, leading to more computational requirements
    (specifically memory and GPUs) and higher latency. This also means increased costs
    for using the LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AI quality*—The quality of a model’s output depends on the number of tokens
    it is asked to generate or process. If the text is too short, the model might
    not have enough context to provide a good answer. Conversely, if the text is too
    long, the model might lose coherence in its response. We will touch on the notion
    of good versus poor as part of prompt engineering later in chapter 6\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For many enterprises, cost and performance are key factors in deciding whether
    to use tokens. Generally speaking, smaller models are more cost-effective and
    efficient than bigger ones.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 shows a simple way to calculate the number of tokens. In this example,
    we use an open source library called `tiktoken`, released by OpenAI. This tokenizer
    library implements a byte-pair encoding (BPE) algorithm. These tokenizers are
    designed with their respective LLMs, ensuring efficient tokenization and optimal
    performance during pretraining and fine-tuning processes. If you use one of the
    OpenAI models, you must use this tokenizer; many other transformer models also
    use it. If needed, you can install the `tiktoken` library using `pip` `install`
    `tiktoken`
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Counting tokens for GPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The encoding specifies how the text is converted into tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code, as expected, gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note  Byte-pair encoding (BPE) is a compression algorithm widely used in NLP
    tasks, such as text classification, text generation, and machine translation.
    One of the BPE advantages is that it is reversible and lossless, so we can get
    the original text. BPE works on any text that the tokenizer’s training data hasn’t
    seen, and it compresses the text, resulting in shorter token sequences than the
    original text. BPE also helps generalize repeating patterns in a language and
    provides a better understanding of grammar. For example, the gerund -ing form
    is quite common in English (swimming, running, debugging, etc.). BPE will split
    it into different tokens, so “swim” and “-ing” in swimming become two tokens and
    generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: If we are not sure of the name of the encoding to use, instead of the function
    `get_ encoding()`, we can use the `encoding_for_model()`function. This takes the
    name of the model we want to use and utilizes the corresponding encoding, such
    as `encoding =` `tiktoken.encoding_for_model('gpt-4').` For OpenAI, table 2.3
    shows different supported encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.3 OpenAI encodings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Encoding | OpenAI model |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `cl100k_base`  | gpt-4, gpt-3.5-turbo, gpt-35-turbo, text-embedding-ada-002  |'
  prefs: []
  type: TYPE_TB
- en: '| `p50k_base`  | Codex models, text-davinci-002, text-davinci-003  |'
  prefs: []
  type: TYPE_TB
- en: '| `r50k_base`  | GPT-3 models (davinci, curie, babage, ada)  |'
  prefs: []
  type: TYPE_TB
- en: Listing 2.2 shows how to use different encodings and how to get the original
    text from the tokens. We should understand this as a basic construct for now,
    but it is useful for more advanced use cases such as caching and chunking text—aspects
    that we cover later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Tokens
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the `tiktoken` library we have been using in the examples, there
    are a few other popular tokenizers. Remember that each tokenizer is designed for
    the corresponding LLM and cannot be interchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '*WordPiece*—Used by the BERT model from Google, it splits text into smaller
    units based on the most frequent word pieces, allowing for efficient representation
    of rare or out-of-vocabulary words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SentencePiece*—Meta’s RoBERTa model (Robustly Optimized BERT) uses the model.
    It combines WordPiece and BPE approaches into a single language-agnostic framework,
    allowing for more flexibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T5 tokenizer*—Based on SentencePiece, it is used by Google’s T5 model (Text-to-Text
    Transfer Transformer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*XLM tokenizer*—This is used in Meta’s XLM (Cross-lingual Language Model) and
    implements a BPE method with learned embeddings (BPEmb). It is designed to handle
    multilingual text and support cross-lingual transfer learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.8.4 Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Embeddings are powerful machine-learning tools for large inputs representing
    words. They capture semantic similarities in a vector space (i.e., a collection
    of vectors, as shown in figure 2.9), allowing us to determine if two text chunks
    represent the same meaning. By providing a similarity score, embeddings can help
    us better understand the relationships between different pieces of text.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind embeddings is that words with similar meanings should have similar
    vector representations, as measured by their distances. Vectors with smaller distances
    between them suggest they are highly related, and those with longer distances
    suggest low relatedness. There are a few ways to measure similarities; we will
    cover these later in chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: These vectors are learned during training and are used to capture the meaning
    of words or phrases. AI algorithms can easily utilize these vectors of floating-point
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F09_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 Embeddings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For example, the word “cat” might be represented by a vector as [0.2, 0.3, -0.1],
    while the word “dog” might be represented as [0.4, 0.1, 0.2]. These vectors can
    then be used as input to machine learning models for tasks such as text classification,
    sentiment analysis, and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are learned when the model is trained on a large corpus of text data.
    The idea is to capture the meaning of words or phrases based on their context
    in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the task, there are several algorithms for creating embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: Similarity embeddings are good at capturing semantic similarity between two
    or more pieces of text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text search embeddings measure whether long documents are relevant to a short
    query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code search embeddings are useful for embedding code snippets and natural language
    search queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note  Embeddings created by one method cannot be understood by another. In other
    words, if you create an embedding using OpenAI’s API, embeddings of another provider
    will not understand the vectors created, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 shows how to get an embedding (from OpenAI in this example). We
    define a function called `get_embedding()` that takes a string for which we need
    to create embeddings as a parameter. The function uses OpenAI’s API to generate
    an embedding for the input text using the `text-embedding-ada-002` model. The
    embedding is returned as a list of floating-point numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Getting an embedding in OpenAI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The vector space resulting from the embedding isn’t a one-to-one mapping to
    the tokens but can be a lot more. The output of the previous examples is shown
    next. For brevity, we only show the first five items in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 2.8.5 Model configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most LLMs expose some configuration settings to the user, allowing one to tweak
    how the model operates and its behavior to some extent. While a few parameters
    would change depending on the model implementation, the three key configurations
    are temperature, top probability (`top_p`), and max response. Note that some implementations
    might have a different name but mean the same thing. The OpenAI implementation
    of GPT calls the maximum response as max tokens. Let us explore these in a little
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Max response
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The parameter known as max response essentially defines the upper limit for
    the text length that the model generates. This means that once the model hits
    this predetermined length, it halts text generation, regardless of whether it
    is mid-word or mid-sentence. It’s crucial to grasp this configuration because
    there is a size limit to the tokens most models can process. Increasing this size
    corresponds to heightened computational demands, leading to increased latency
    and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When generating text, as with any foundational model, inherent randomness yields
    a different output each time we call the model. Temperature is one of the most
    important settings for controlling the degree of the model’s randomness. Typically,
    this is a value from 0 to 1, with 0 representing a more accurate and predictable
    output. In contrast, setting a 1 makes the output more diverse and random, as
    shown in figure 2.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F10_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 Temperature settings and their effect on probability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Top probability (top_p)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The top probability (`top_p`) parameter (also known as nucleus sampling) is
    a setting in language model APIs that steers the randomness of the text-generation
    process. This parameter allows one to fine-tune the balance between creativity
    and reliability in the text that the model generates. It defines a threshold probability;
    only words with probabilities above this threshold are considered when the model
    generates text. When a language model generates text, it predicts the probability
    of each word being the next in the sequence. The `top_p` parameter helps truncate
    this probability distribution to enhance the quality of the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for output generation, setting `top_p` to a lower value (e.g.,
    0.3), the model will only consider the top 30% most probable words for the next
    word in the sequence, as shown in figure 2.11\. This makes the text more predictable
    and less varied. However, if we set `top_p` to a higher value (e.g., 0.9), the
    model will consider a much broader range of words, including those that are less
    likely. This can lead to a more diverse and potentially interesting generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F11_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 Example showing how top-p works
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s show how these settings can be programmatically used. The following code
    snippet shows an example of how to do these configurations with OpenAI. Most of
    these settings are used sparingly, are optional, and would fall to default values.
    Only some options, such as `max_tokens` and `temperature`, are used in almost
    every use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Given that the API is stateless, these settings can differ between different
    instances and API calls of the same instance, depending on the business scenario
    one tries to achieve. Furthermore, there are no globally optimal values for these
    settings, which depend on the task. In general, if you want to have a balanced
    output and not have the model hallucinate much, a setting of 0.7 or 0.8 is good
    for temperature. Table 2.4 outlines configuration settings to control the behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.4 LLM configuration settings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Configuration | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Max tokens  | This sets a limit on the number of tokens per model response.
    Depending on the model, a maximum limit is shared between the prompt (including
    system message, examples, message history, and user query) and the model’s response.  |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature  | This controls randomness. Lowering the temperature means the
    model produces more repetitive and deterministic responses. Increasing the temperature
    results in more unexpected or creative responses. Try adjusting the temperature
    or `top_p` but not both. Typically, as sequences get longer, the model naturally
    becomes more confident in its predictions, and one can use a much higher temperature
    for long prompts without going off-topic. Conversely, using a high-temperature
    setting on short prompts can lead to unstable outputs.  |'
  prefs: []
  type: TYPE_TB
- en: '| Top probability ( `top_p`)  | This is a probability threshold that, like
    temperature, controls randomness but uses a different method. Lowering `top_p`
    narrows the model’s token selection to the most likely tokens and ignores the
    long tail of less likely tokens. Increasing `top_p` will allow the model to choose
    from high- and low-likelihood tokens. Try adjusting either temperature or `top_p,`
    but not both.  |'
  prefs: []
  type: TYPE_TB
- en: '| Stop sequences  | This makes the model end its response at a desired point.
    The model response will end before the specified sequence so that it won’t contain
    the stop-sequence text.  |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency penalty  | This reduces the chance of repeating a token proportionally
    based on how often it has appeared in the text. This decreases the likelihood
    of repeating the same text in response.  |'
  prefs: []
  type: TYPE_TB
- en: '| Presence penalty  | This reduces the chance of repeating any token that has
    appeared in the text so far. This increases the likelihood of introducing new
    topics in a response.  |'
  prefs: []
  type: TYPE_TB
- en: 2.8.6 Context window
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A context window is a relatively new and very important concept. It refers to
    the range of tokens or words surrounding a particular word or token that an LLM
    considers when making predictions. The context window helps the model understand
    the dependencies and relationships between the words, enabling it to generate
    more accurate and coherent predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when predicting the next word in a sentence, the context window
    might include several words preceding the target word. The context window size
    may vary depending on the model and its architecture. In LLMs, context windows
    can be quite large, allowing the model to capture long-range dependencies and
    intricate semantic relationships among the words. These longer-context windows
    can help get better output for tasks such as text generation, translation, and
    summarization.
  prefs: []
  type: TYPE_NORMAL
- en: The current LLM architecture limits the context window size to several thousand
    tokens. Although some of the newer models support up to a million tokens, the
    context window is still a critical focal point, mainly because the global nature
    of the attention mechanism imposes computational costs that are quadratic in context
    length. In other words, the bigger the context windows, the more the computation
    cost is proportional to the square of the input data size. While having a larger
    context window might seem good, it is important to understand that it has both
    positive and negative implications. With larger context windows, the performance
    of the model is much slower in terms of both understanding the request and the
    generation, with a higher latency. While we might feel like a longer context would
    be better, in general, use a smaller window if that would suffice for the task
    at hand—it would have a better performance than the larger one. Some of the pros
    are
  prefs: []
  type: TYPE_NORMAL
- en: '*Improved comprehension of context*—A longer context window allows the model
    to capture long-range dependencies and intricate semantic relationships among
    words, resulting in better predictions and more coherent text generation. However,
    this comes at a considerable cost and should be used judiciously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Better performance at complex tasks*—With a more extensive context window,
    language models can perform better at tasks that require a better comprehension
    of the broader context, such as machine translation, summarization, and sentiment
    analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Increased computational requirements*—A longer context window requires more
    memory and computational power to process and store the additional information,
    which can result in longer training and inference times and require more powerful
    hardware or distributed computing solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Potential for overfittin*g—As the context window increases, the model becomes
    more complex and prone to overfitting, especially if the training data is limited.
    Overfitting occurs when the model learns to perform exceptionally well on the
    training data but struggles to generalize new and unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Difficulty handling very long sequences*—Although longer context windows can
    improve performance, they may also introduce challenges when processing extremely
    long sequences. Some models may struggle to maintain the necessary information
    over such long distances, leading to a degradation in performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Diminishing returns*—While increasing the context window size can improve
    performance, there may be a point of diminishing returns where further increases
    yield only marginal improvements. It’s essential to balance the context window
    size with the computational cost and model complexity to achieve optimal performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The context window as a concept is quite important for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Captures dependencies*—A context window allows the model to understand relationships
    between words, phrases, or sentences within a text. This helps the model grasp
    the overall meaning and context of the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Improved predictions*—This is probably what most of us see when using LLMs,
    where the context window enables the model to generate more accurate and coherent
    suggestions based on the preceding text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Provides context for better understanding*—By considering the context window,
    LLMs can better understand the context at play and, as a result, the syntactical
    structure and semantic relationships in the text; this allows for more accurate
    language understanding and generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.8.7 Prompt engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt engineering is a relatively new field that involves curating or designing
    prompts to elicit desired responses or behaviors from a machine learning model,
    particularly LLMs. It is a powerful technique that can improve the performance
    of language models on various tasks. Prompt engineering is an emerging field that
    requires creativity and attention to detail.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering can be seen as both an art and a science. It involves careful
    selection and phrasing of input commands to help guide the AI in producing the
    desired output. These input commands can be as simple as selecting the right words,
    phrases, and formats to guide the model in generating high-quality and relevant
    texts for a specific task.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, to get a detailed answer, you might use a prompt such as “Explain
    in detail …,” or to get a quick summary, you might use “Summarize in a few bullets
    … ” Similarly, to make the AI write in the style of a particular author, you might
    use a prompt like “Write a passage as if you were P. G. Wodehouse.”
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering requires understanding the AI model’s capabilities, underlying
    training data, and how it responds to different kinds of input. Effective prompt
    engineering can significantly improve the usefulness of AI models in various tasks.
    Note that this section is just an introduction to prompt engineering as a fundamental
    concept; we will cover prompt engineering in more depth later in chapter 6.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.8 Model adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs are pretrained and general-purpose, and sometimes they must be fine-tuned.
    They are trained on a large corpus of text data and can be used as a starting
    point for training on a smaller dataset for a specific task. Fine-tuning the base
    LLM on the smaller dataset can improve its performance for that specific task.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is taking a pretrained model and training it further on a new task
    or dataset. The pretrained model is used as a starting point, and the weights
    of the model are adjusted during training to improve its performance on the new
    task. Fine-tuning is often used in transfer learning, where a model trained on
    one task is adapted to another related task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of fine-tuning LLMs include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Text classification*—Fine-tuning an LLM on a specific text classification
    task, such as sentiment analysis or spam detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Question answering*—Fine-tuning an LLM on a specific question-answering task,
    such as answering questions about a specific topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Language generation*—Fine-tuning an LLM on a specific language generation
    task, such as generating summaries or translations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning an LLM such as GPT-3.5 Turbo can be a powerful way to customize
    the model for specific tasks, but it can also be very expensive and should be
    one of the last options. In some cases, fine-tuning can also lead to catastrophic
    forgetting. This occurs when the model is fine-tuned on a new dataset, causing
    it to forget the knowledge it had learned from its original training data, resulting
    in the fine-tuned model losing its reasoning skills. We will cover model adaptation
    in more detail in chapter 9, including any pitfalls of fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.9 Emergent behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concept of emergent behavior defines the significance of foundation models
    and LLMs. Emergent behavior implies unexpected behaviors exhibited by LLMs when
    interacting with their environment, specifically when trained with large amounts
    of data. LLMs are not explicitly trained to have these abilities but learn them
    by observing natural language. The emergence of the behavior is implicitly induced
    rather than explicitly constructed; it is both the source of scientific excitement
    and anxiety about unanticipated consequences.
  prefs: []
  type: TYPE_NORMAL
- en: As figure 2.12 shows, a model’s performance on multiple natural language benchmarks
    (e.g., question answering) is no better than random chance until they reach a
    certain scale, measured in this example by training computation in FLOPs. The
    model’s performance sharply increases at this point, which is an example of emergent
    abilities. This helps us understand that emergent properties of LLMs are not present
    in smaller models. Furthermore, these abilities show up only at a scale when the
    model’s size reaches a certain threshold. Emergent properties cannot be predicted
    by extrapolating the performance of smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH02_F12_Bahree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 Model scaling as measured by training computation requirements in
    FLOP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let us see an example using code, as the exact logic and nature of what a piece
    of code is doing cannot be predetermined or programmed for an LLM. It needs to
    rely on the emergent behavior property of foundational models to understand, analyze,
    and comprehend the code.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have C code that reverses a string, as shown in the following listing.
    This uses some of C’s more advanced features, such as pointers, XOR operators,
    and macros. In our example, we use GPT-4 to convert this C function to Python.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Function written in C to reverse a string
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first step would be to ask our LLM to convert this code to Python using
    a prompt similar to the next one. We are omitting the C function (listing 2.4)
    again for brevity. The code is part of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Convert this C function to Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Listing 2.5 shows the output from our model with the code converted to Python.
    It does a good job of understanding what the C function is doing and outlines
    Python’s different constraints and properties in the response. Note that we did
    not give it any hints as to what the function is doing. It deciphered the `while`
    loop itself, showing an example of emergent behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**This function will return the reversed
    string rather than modify the original string because Python strings are immutable.
    In other words, once a string is created in Python, it cannot be changed. Hence,
    the function `inplace_reverse` is a bit of a misnomer in this Python context.
    You could consider renaming it to something like `reverse_string`.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Converted Python function for string reversal
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can simplify this further by asking the model the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Prompt.png)**Can you further simplify and optimize the
    Python code?'
  prefs: []
  type: TYPE_NORMAL
- en: '**![image](../Images/Response.png)**Sure! One of the simplest ways to reverse
    a string in Python uses slicing. Here’s a function that will return the reverse
    of a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `[::-1]` is a slice that starts at the end of the string and moves backward.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you want to emulate the same logic as the C program, that is, reversing
    a string in place using two pointers, here is a more Pythonic way of achieving
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this version, we convert the string to a list, reverse the list in place
    using the built-in `reverse` function, and then join the list back into a string.
    The `reverse` function in Python uses a similar two-pointer approach, but it’s
    all under the hood, so the code looks much simpler.
  prefs: []
  type: TYPE_NORMAL
- en: The point the response calls out on a similar two-pointer approach demonstrates
    the emergent property’s power. We never explicitly called this out anywhere in
    the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we looked at the core concepts that make LLMs work and what
    we need to be mindful of when implementing these new generative AI techniques
    in the enterprise. We cover new concepts such as prompts, prompt engineering,
    embeddings, tokens, model parameters, context windows, transformer architecture,
    and emergent behavior. Now that we have a broad understanding of LLMs and their
    implications in the next chapter, let us put this into action and work through
    an API to generate text, both as completions where we provide instructions via
    a prompt and as chat where we can have a dialogue, and in that, generate text,
    in the context of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) represent a major advancement in AI. They are trained
    on vast amounts of text data to learn patterns in human language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are general-purpose and can handle tasks without task-specific training
    data, such as answering questions, writing essays, summarizing texts, translating
    languages, and generating code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key LLM use cases include summarization, classification, Q&A/chatbots, content
    generation, data analysis, translation and localization, process automation, research
    and development, sentiment analysis, and entity extraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of LLMs include base, instruction-based, and fine-tuned LLM. Each has
    pros and cons and is powered by foundational models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foundational models are large AI models trained on vast quantities of data at
    a massive scale, resulting in models that can be adapted to a wide range of downstream
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some key LLM concepts include prompts, prompt engineering, embeddings, tokens,
    model parameters, context windows, transformer architecture, and emergent behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source and commercial LLMs have advantages and disadvantages, with commercial
    models typically offering state-of-the-art performance and open source models
    providing more flexibility for customization and integration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small language models (SLMs) are a new emerging trend of lightweight generative
    AI models that produce text, summarize documents, translate languages, and answer
    questions. In some cases, they offer capabilities similar to those of larger models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
