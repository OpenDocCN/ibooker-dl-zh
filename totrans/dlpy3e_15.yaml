- en: Language models and the Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer](https://deeplearningwithpython.io/chapters/chapter15_language-models-and-the-transformer)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With the basics of text preprocessing and modeling covered in the previous chapter,
    this chapter will tackle some more involved language problems such as machine
    translation. We will build up a solid intuition for the Transformer model that
    powers products like ChatGPT and has helped trigger a wave of investment in natural
    language processing (NLP).
  prefs: []
  type: TYPE_NORMAL
- en: The language model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to convert text data to numeric inputs,
    and we used this numeric representation to classify movie reviews. However, text
    classification is, in many ways, a uniquely simple problem. We only need to output
    a single floating-point number for binary classification and, at worst, *N* numbers
    for *N*-way classification.
  prefs: []
  type: TYPE_NORMAL
- en: What about other text-based tasks like question answering or translation? For
    many real-world problems, we are interested in a model that can generate a text
    output for a given input. Just like we needed tokenizers and embeddings to help
    us handle text on the *way in* to a model, we must build up some techniques before
    we can produce text on the *way out*.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to start from scratch here; we can continue to use the idea of
    an integer sequence as a natural numeric representation for text. In the previous
    chapter, we covered *tokenizing* a string, where we split inputs into tokens and
    map each token to an int. We can *detokenize* a sequence by proceeding in reverse
    — map ints back to string tokens and join them together. With this approach, our
    problem becomes building a model that can predict an integer sequence of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest option to consider might be to train a direct classifier over the
    space of all possible output integer sequences, but some back-of-the-envelope
    math will quickly show this is intractable. With a vocabulary of 20,000 words,
    there are 20,000 ^ 4, or 160 quadrillion possible 4-word sequences, and fewer
    atoms in the universe than possible 20-word sequences. Attempting to represent
    every output sequence as a unique classifier output would overwhelm compute resources
    no matter how we design our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A practical approach for making such a prediction problem feasible is to build
    a model that only predicts a single token output at a time. A *language model*
    is a model that, in its simplest form, learns a straightforward but deep probability
    distribution: `p(token|past tokens)`. Given a sequence of all tokens observed
    up to a point, a language model will attempt to output a probability distribution
    over all possible tokens that could come next. A 20,000-word vocabulary means
    the model needs only predict 20,000 outputs, but by *repeatedly* predicting the
    next token, we will have built a model that can generate a long sequence of text.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make this more concrete by building a simple language model that predicts
    the next character in a sequence of characters. We will train a small model that
    can output Shakespeare-like text.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Shakespeare language model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To begin, we can download a collection of some of Shakespeare’s plays and sonnets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.1](#listing-15-1): Downloading an abbreviated collection of Shakespeare’s
    work'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at some of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To build a *language model* from this input, we will need to massage our source
    text. First, we will split our data into equal-length chunks that we can batch
    and use for model training, much as we did for weather measurements in the timeseries
    chapter. Because we will be using a character-level tokenizer here, we can do
    this chunking directly on the string input. A 100-character string will map to
    a 100-integer sequence.
  prefs: []
  type: TYPE_NORMAL
- en: We will also split each input into two separate feature and label sequences,
    with each label sequence simply being the input sequence offset by a single character.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.2](#listing-15-2): Splitting text into chunks for language model
    training'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an `(x, y)` input sample. Our label at each position in the sequence
    is the next character in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To map this input to a sequence of integers, we can again use the `TextVectorization`
    layer we saw in the last chapter. To learn a character-level vocabulary instead
    of a word-level vocabulary, we can change our `split` argument. Rather than the
    default `"whitespace"` splitting, we instead split by `"character"`. We will do
    no standardization here — to keep things simple, we will preserve case and pass
    punctuation through unaltered.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.3](#listing-15-3): Learning a character-level vocabulary with the
    `TextVectorization` layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We need only 67 characters to handle the full source text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can apply our tokenization layer to our input text. And finally, we
    can shuffle, batch, and cache our dataset so we don’t need to recompute it every
    epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With that, we are ready to start modeling.
  prefs: []
  type: TYPE_NORMAL
- en: To build our simple language model, we want to predict the probability of a
    character given all past characters. Of all the modeling possibilities we have
    seen so far in this book, an RNN is the most natural fit, as the recurrent state
    of each cell allows the model to propagate information about past characters when
    predicting the label of the current character. We can also use an `Embedding`,
    as we saw in the previous chapter, to embed each input character as a unique 256-dimensional
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: We will use only a single recurrent layer to keep this model small and easy
    to train. Any recurrent layer would do here, but to keep things simple, we will
    use a `GRU`, which is fast and has a simpler internal state than an `LSTM`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.4](#listing-15-4): Building a miniature language model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at our model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This model outputs a softmax probability for every possible character in our
    vocabulary, and we will `compile()` it with a crossentropy loss. Note that our
    model is still training on a classification problem, it’s just that we will make
    one classification prediction for every token in our sequence. For our batch of
    64 samples with 100 characters each, we will predict 6,400 individual labels.
    Loss and accuracy metrics reported by Keras during training will be averaged first
    across each sequence and, second, across each batch.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go ahead and train our language model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.5](#listing-15-5): Training a miniature language model'
  prefs: []
  type: TYPE_NORMAL
- en: After 20 epochs, our model can eventually predict the next character in our
    input sequences around 70% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Shakespeare
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have trained a model that can predict the next *individual* tokens
    with some accuracy, we would like to use it to extrapolate an entire predicted
    sequence. We can do this by calling the model in a loop, where the model’s predicted
    output at one time step becomes the model’s input at the next time step. A model
    built for this kind of feedback loop is sometimes called an *autoregressive* model.
  prefs: []
  type: TYPE_NORMAL
- en: To run such a loop, we need to perform a slight surgery on the model we just
    trained. During training, our model handled only a fixed sequence length of 100
    tokens, and the `GRU` cell’s state was handled implicitly when calling the layer.
    During generation, we would like to predict a single output token at a time and
    explicitly output the state of the `GRU`’s cell. We need to propagate that state,
    which contains all information the model has encoded about past input characters,
    the next time we call the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make a model that handles a single input character at a time and allows
    explicitly passing the RNN state. Because this model will have the same computational
    structure, with slightly modified inputs and outputs, we can assign weights from
    one model to another.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.6](#listing-15-6): Modifying the language model for autoregressive
    inference'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we can call the model to predict an output sequence in a loop. Before
    we do, we will make explicit lookup tables so we switch from characters to integers
    and choose a *prompt* — a snippet of text we will feed as input to the model before
    we begin predicting new tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To begin generation, we first need to “prime” the internal state of the GRU
    with our prompt. To do this, we will feed the prompt into the model one token
    at a time. This will compute the exact RNN state the model would see if this prompt
    had been encountered during training.
  prefs: []
  type: TYPE_NORMAL
- en: When we feed the very last character of the prompt into the model, our state
    output will capture information about the entire prompt sequence. We can save
    the final output prediction to later select the first character of our generated
    response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.7](#listing-15-7): Using a fixed prompt to compute a language model’s
    starting state'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to let the model predict a new output sequence. In a loop,
    up to a desired length, we will continually select the most likely next character
    predicted by the model, feed that to the model, and persist the new RNN state.
    In this way, we can predict an entire sequence, a token at time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.8](#listing-15-8): Predicting with the language model a token at
    a time'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s convert our output integer sequence to a string to see what the model
    predicted. To *detokenize* our input, we simply map all token IDs to strings and
    join them together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We have yet to produce the next great tragedy, but this is not terrible for
    two minutes of training on a minimal dataset. The goal of this toy example is
    to show the power of the language model setup. We trained the model on the narrow
    problem of guessing a single character at a time but still use it for a much broader
    problem, generating an open-ended, Shakespearean-like text response.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to notice that this training setup only works because a recurrent
    neural network only passes information forward in the sequence. If you’d like,
    try replacing the `GRU` layer with a `Bidirectional(GRU(...))`. The training accuracy
    will zoom to above 99% immediately, and generation will stop working entirely.
    During training, our model sees the entire sequence each train step. If we “cheat”
    by letting information from the next token in the sequence affect the current
    token’s prediction, we’ve made our problem trivially easy.
  prefs: []
  type: TYPE_NORMAL
- en: This *language modeling* setup is fundamental to countless problems in the text
    domain. It is also somewhat unique compared to other modeling problems we have
    seen so far in this book. We cannot simply call `model.predict()` to get the desired
    output. There is an entire loop, and a nontrivial amount of logic, that exists
    only at inference time! The looping of state in the RNN cell happens for both
    training and inference, but at no point during training do we feed a model’s predicted
    labels back into itself as input.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take the language model idea and extend it to tackle an important problem
    — machine translation. Translation belongs to a class of modeling problems often
    called *sequence-to-sequence* modeling (or *seq2seq* if you are trying to save
    keystrokes). We seek to build a model that can take in a source text as a fixed
    input sequence and generate the translated text sequence as a result. Question
    answering is another classic sequence-to-sequence problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general template behind sequence-to-sequence models is described in figure
    15.1\. During training, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: An encoder model turns the source sequence into an intermediate representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder is trained using the language modeling setup we saw previously. It
    will recursively predict the next token in the target sequence by looking at all
    previous target tokens *and* our encoder’s representation of the source sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During inference, we don’t have access to the target sequence — we’re trying
    to predict it from scratch. We will generate it one token at a time, just as we
    did with our Shakespeare generator:'
  prefs: []
  type: TYPE_NORMAL
- en: We obtain the encoded source sequence from the encoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder starts by looking at the encoded source sequence as well as an initial
    “seed” token (such as the string `"[start]"`) and uses them to predict the first
    real token in the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predicted sequence so far is fed back into the decoder, in a loop, until
    it generates a stop token (such as the string `"[end]"`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9a9039d4015f58ca855e14986b7f53df.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.1](#figure-15-1): Sequence-to-sequence learning: the source sequence
    is processed by the encoder and is then sent to the decoder. The decoder looks
    at the target sequence so far and predicts the target sequence offset by one step
    in the future. During inference, we generate one target token at a time and feed
    it back into the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build a sequence-to-sequence translation model.
  prefs: []
  type: TYPE_NORMAL
- en: English-to-Spanish translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll be working with an English-to-Spanish translation dataset. Let’s download
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The text file contains one example per line: an English sentence, followed
    by a tab character, followed by the corresponding Spanish sentence. Let’s parse
    this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `text_pairs` look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s shuffle them and split them into the usual training, validation, and
    test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s prepare two separate `TextVectorization` layers: one for English
    and one for Spanish. We’re going to need to customize the way strings are preprocessed:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to preserve the `"[start]"` and `"[end]"` tokens that we’ve inserted.
    By default, the characters `[` and `]` would be stripped, but we want to keep
    them around so we can distinguish the word `"start"` from the start token `"[start]"`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Punctuation is different from language to language! In the Spanish `TextVectorization`
    layer, if we’re going to strip punctuation characters, we need to also strip the
    character `¿`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that for a non-toy translation model, we would treat punctuation characters
    as separate tokens rather than stripping them since we would want to be able to
    generate correctly punctuated sentences. In our case, for simplicity, we’ll get
    rid of all punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.9](#listing-15-9): Learning token vocabularies for English and
    Spanish text'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can turn our data into a `tf.data` pipeline. We want it to return
    a tuple `(inputs, target, sample_weights)` where `inputs` is a dict with two keys,
    `"english"` (the tokenized English sentence) and `"spanish"` (the tokenized Spanish
    sentence), and `target` is the Spanish sentence offset by one step ahead. `sample_weights`
    here will be used to tell Keras which labels to use when calculating our loss
    and metrics. Our output translations are not all equal in length, and some of
    our label sequences will be padded with zeros. We only care about predictions
    for non-zero labels that represent actual translated text.
  prefs: []
  type: TYPE_NORMAL
- en: This matches the “off by one” label set up in the generation model we just built,
    with the addition of the fixed encoder inputs, which will be handled separately
    in our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.10](#listing-15-10): Tokenizing and preparing the Translation data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what our dataset outputs look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The data is now ready — time to build some models.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence learning with RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we try the twin encoder/decoder setup we previously mentioned, let’s
    think through simpler options. The easiest, naive way to use RNNs to turn one
    sequence into another is to keep the output of the RNN at each time step and predict
    an output token from it. In Keras, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: However, there is a critical issue with this approach. Due to the step-by-step
    nature of RNNs, the model will only look at tokens `0...N` in the source sequence
    to predict token `N` in the target sequence. Consider translating the sentence,
    “I will bring the bag to you.” In Spanish, that would be “Te traeré la bolsa,”
    where “Te,” the first word of the translation, corresponds to “you” in the English
    source text. There’s simply no way to output the first word of the translation
    without seeing the last word of the source English text!
  prefs: []
  type: TYPE_NORMAL
- en: If you’re a human translator, you’d start by reading the entire source sentence
    before beginning to translate it. This is especially important if you’re dealing
    with languages with wildly different word ordering. And that’s precisely what
    standard sequence-to-sequence models do. In a proper sequence-to-sequence setup
    (see figure 15.2), you would first use an encoder RNN to turn the entire source
    sequence into a single representation of the source text. This could be the last
    output of the RNN or, alternatively, its final internal state vectors. We can
    use this representation as the initial state of a decoder RNN in the language
    model setup instead of an initial state of zeros, which we used in our Shakespeare
    generator. This decoder learns to predict the next word of the Spanish translation
    given the current word of the translation, with all information about the English
    sequence coming from that initial RNN state.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed3f977b6d8a998c69ff33ac7cad5bc2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.2](#figure-15-2): A sequence-to-sequence RNN: an RNN encoder is
    used to produce a vector that encodes the entire source sequence, which is used
    as the initial state for an RNN decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement this in Keras, with GRU-based encoders and decoders. We can
    start with just the encoder. Since we will not actually be predicting tokens in
    the encoder sequence, we don’t have to worry about “cheating” by letting the model
    pass information from the end of the sequence to positions at the beginning. In
    fact, this is a good idea, as we want a rich representation of the source sequence.
    We can achieve this with a `Bidirectional` layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.11](#listing-15-11): Building a sequence-to-sequence encoder'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s add the decoder — a simple `GRU` layer that takes as its initial
    state the encoded source sentence. On top of it, we add a `Dense` layer that produces
    a probability distribution over the Spanish vocabulary for each output step. Here,
    we want to predict the next tokens based only on what came before, so a `Bidirectional`
    RNN would break training by making the loss function trivially easy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.12](#listing-15-12): Building a sequence-to-sequence decoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the seq2seq model in full:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model and data are both ready. We can now begin training our translation
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We picked accuracy as a crude way to monitor validation set performance during
    training. We get to 65% accuracy: on average, the model correctly predicts the
    next word in the Spanish sentence 65% of the time. However, in practice, next-token
    accuracy isn’t a great metric for machine translation models, in particular because
    it makes the assumption that the correct target tokens from `0` to `N` are already
    known when predicting token `N + 1`. In reality, during inference, you’re generating
    the target sentence from scratch, and you can’t rely on previously generated tokens
    being 100% correct. When working on a real-world machine translation system, metrics
    must be more carefully designed. There are standard metrics, such as a BLEU score,
    that measure the similarity of the machine-translated text to a set of high-quality
    reference translations and can tolerate slightly misaligned sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: At last, let’s use our model for inference. We’ll pick a few sentences in the
    test set and check how our model translates them. We’ll start from the seed token
    `"[start]"` and feed it into the decoder model, together with the encoded English
    source sentence. We’ll retrieve a next-token prediction, and we’ll re-inject it
    into the decoder repeatedly, sampling one new target token at each iteration,
    until we get to `"[end]"` or reach the maximum sentence length.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.13](#listing-15-13): Generating translations with a seq2seq RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact translations will vary from run to run, as the final model weights
    will depend on the random initializations of our weights and the random shuffling
    of our input data. Here’s what we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Our model works decently well for a toy model, although it still makes many
    basic mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this inference setup, while very simple, is inefficient, since we
    reprocess the entire source sentence and the entire generated target sentence
    every time we sample a new word. In a practical application, you’d want to be
    careful not to recompute any state that has not changed. All we really need to
    predict a new token in the decoder is the current token and the previous RNN state,
    which we could cache before each loop iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways this toy model could be improved. We could use a deep stack
    of recurrent layers for both the encoder and the decoder, we could try other RNN
    layers like `LSTM`, and so on. Beyond such tweaks, however, the RNN approach to
    sequence-to-sequence learning has a few fundamental limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: The source sequence representation has to be held entirely in the encoder state
    vector, which significantly limits the size and complexity of the sentences you
    can translate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs have trouble dealing with very long sequences since they tend to progressively
    forget about the past — by the time you’ve reached the 100th token in either sequence,
    little information remains about the start of the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks dominated sequence-to-sequence learning in the mid-2010s.
    Google Translate circa 2017 was powered by a stack of seven large `LSTM` layers
    in a setup similar to what we just created. However, these limitations of RNNs
    eventually led to researchers developing a new style of sequence model, called
    the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2017, Vaswani et al. introduced the Transformer architecture in the seminal
    paper “Attention Is All You Need.”^([[1]](#footnote-1)) The authors were working
    on translation systems like the one we just built, and the critical discovery
    is right in the title. As it turned out, a simple mechanism called *attention*
    can be used to construct powerful sequence models that don’t feature recurrent
    layers at all. The idea of attention was not new and had been used in NLP systems
    for a couple of years when they published. But the idea that attention was so
    useful it could be the *only* mechanism used to pass information across a sequence
    was quite surprising at the time.
  prefs: []
  type: TYPE_NORMAL
- en: This finding unleashed nothing short of a revolution in natural language processing
    — and beyond. Attention has fast become one of the most influential ideas in deep
    learning. In this section, you’ll get an in-depth explanation of how it works
    and why it has proven so effective for sequence modeling. We’ll then use attention
    to rebuild our English-to-Spanish translation model.
  prefs: []
  type: TYPE_NORMAL
- en: So, with all that as build-up, what exactly is attention? And how does it offer
    a replacement for the recurrent neural networks we have used so far?
  prefs: []
  type: TYPE_NORMAL
- en: Attention was actually developed as a way to augment an RNN model like the one
    we just built. Researchers noticed that while RNNs excelled at modeling dependencies
    in a local neighborhood, they struggled with recall as sequences got longer. Say
    you were building a system to answer questions about a source document. If the
    document length got too long, RNN results would get plain bad, a far cry from
    human performance.
  prefs: []
  type: TYPE_NORMAL
- en: As a thought experiment, imagine using this book to build a weather prediction
    model. If you had enough time, you might read the entire book cover to cover,
    but when you actually implemented your model, you would pay special attention
    to just the timeseries chapters. Even within a chapter, you might find specific
    code samples and explanations you would refer to often. On the other hand, you
    would not be particularly worried about the details of image convolutions as you
    worked on your code. The overall word count of this book is well over 100,000,
    far beyond any sequence length we have tackled, but humans can be *selective*
    and *contextual* in how we pull information from text.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs, on the other hand, lack any mechanism to refer back to a previous section
    of a sequence directly. All information must, by design, be passed through an
    RNN cell’s internal state in a loop, through *every* position in a sequence. It’s
    a bit like finishing this book, closing it, and trying to implement that weather
    prediction model entirely from memory. The idea with attention is to build a mechanism
    by which a neural network can give more weight to some part of a sequence and
    less weight to others contextually, depending on the current input being processed
    (figure 15.3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3129120895f1bf9c96b397c648057a6f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.3](#figure-15-3): The general concept of attention in deep learning:
    input features get assigned attention scores, which can be used to inform the
    next representation of the input.'
  prefs: []
  type: TYPE_NORMAL
- en: Dot-product attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s revisit our translation RNN and try to add the notion of selective attention.
    Consider predicting just a single token. After passing the `source` and `target`
    sequences through our `GRU` layers, we will have a vector representing the target
    token we are about to predict and a sequence of vectors representing each word
    in the source text.
  prefs: []
  type: TYPE_NORMAL
- en: With attention, our goal is to give the model a way to *score* every single
    vector in our source sequence based on its *relevance* to the current word we
    are trying to predict (figure 15.4). If the vector representation of a source
    token has a high score, we consider it particularly important; if not, we care
    less about it. For now, let’s assume we have this function `score(target_vector,
    source_vector)`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00096f30f6b13d63d087c6f71161bab2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.4](#figure-15-4): Attention assigns a relevance score to each vector
    in a source for each vector in a target sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For attention to work well, we want to avoid passing information about important
    tokens through a loop potentially as long as our combined source and target sequence
    length — this is where RNNs start to fail. A simple way to do this is to take
    a weighted sum of all the source vectors based on this score we will compute.
    It would also be convenient if the sum of all attention scores for a given target
    were 1, as this would give our weighted sum a predictable magnitude. We can achieve
    this by running the scores through a `softmax` function — something like this,
    in NumPy pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: But how should we compute this relevance score? When researchers first worked
    with attention, this question was a big topic of inquiry. It turns out that one
    of the most straightforward approaches is best. We can use a dot-product as a
    simple measure of the distance between target and source vectors. If the source
    and target vectors are close together, we assume that means the source token is
    relevant to our prediction. At the end of this chapter, we will examine why this
    assumption makes intuitive sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s update our pseudocode. We can make our snippet more complete by handling
    the entire target sequence at once — it will be equivalent to running our previous
    snippet in a loop for each token in the target sequence. When both `target` and
    `source` are sequences, the attention scores will be a matrix. Each row represents
    how much a target word will value a source word in the weighted sum (see figure
    15.5). We will use the Einsum notation as a convenient way to write the dot-product
    and weighted sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5a894dfe04e69c356b3b2f38c25b9e54.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.5](#figure-15-5): When both target and source are sequences, attention
    scores are a 2D matrix. Each row shows the attention scores for the word we are
    trying to predict (in green).'
  prefs: []
  type: TYPE_NORMAL
- en: We can make the *hypothesis space* of this attention mechanism much richer if
    we give the model parameters to control the attention score. If we project both
    source and target vectors with `Dense` layers, the model can find a good shared
    space where source vectors are close to target vectors if they help the overall
    prediction quality. Similarly, we should allow the model to project the source
    vectors into an entirely separate space before they are combined and once again
    after the summation.
  prefs: []
  type: TYPE_NORMAL
- en: We can also adopt a slightly different naming for inputs that has become standard
    in the field. What we just wrote is roughly summarized as `sum(score(target, source)
    * source)`. We will write this equivalently with different input names as `sum(score(query,
    key) * value)`. This three-argument version is more general — in rare cases, you
    might not want to use the same vector to score your source inputs as you use to
    sum your source inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The terminology comes from search engines and recommender systems. Imagine a
    search tool to look up photos in a database — the “query” is your search term,
    the “keys” are photo tags you use to match with the query, and finally, the “values”
    are the photos themselves (figure 15.6). The attention mechanism we are building
    is roughly analogous to this sort of lookup.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06f4a4bfcfd1563e8cf33f6be461e375.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.6](#figure-15-6): Retrieving images from a database: the *query*
    is compared to a set of *keys*, and the match scores are used to rank *values*
    (images).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s update our pseudocode, so we have a parameterized attention using our
    new terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This block is a perfectly functional attention mechanism! We just wrote a function
    that will allow the model to pull information from anywhere in the source sequence,
    contextually, depending on the target word we are decoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The “Attention is all you need” authors made two more changes to our mechanism
    through trial and error. The first is a simple scaling factor. When input vectors
    get long, the dot-product scores can get quite large, which can affect the stability
    of our softmax gradients. The fix is simple: we can scale down our softmax scores
    slightly. Scaling by the square root of the vector length works well for any vector
    size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other has to do with the expressivity of the attention mechanism. The softmax
    sum we are doing is powerful — it allows a direct connection across distant parts
    of a sequence. But the summation is also blunt: if the model tries to attend to
    too many tokens at once, the interesting features of individual source tokens
    will get “washed out” in the combined representation. A simple trick that works
    well is to do this attention operation several times for the same sequence, with
    several different attention *heads* running the same computation with different
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: By projecting the query and key differently, one head might learn to match the
    subject of the source sentence, while another head might attend to punctuation.
    This multi-headed attention avoids the limitation of needing to combine the entire
    source sequence with a single softmax sum (figure 15.7).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/304293112b9495ea5eb0e67c73ae1138.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.7](#figure-15-7): Multi-headed attention allows each target word
    to attend to different parts of the source sequence in separate partitions of
    the eventual output vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in practice, you would want to write this code as a reusable layer.
    Here, Keras has you covered. We can recreate our previous code with the `MultiHeadAttention`
    layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Transformer encoder block
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One way to use the `MultiHeadAttention` layer would be to add it to our existing
    RNN translation model. We could pass the sequence output from our encoder and
    decoder into an attention layer and use its output to update our target sequence
    before prediction. Attention would allow the model to handle long-range dependencies
    in text that the `GRU` layer will struggle with. This does, in fact, improve an
    RNN model’s capabilities and is how attention was first used in the mid-2010s.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the authors of “Attention is all you need” realized you could go further
    and use attention as a general mechanism for handling all sequence data in a model.
    Although so far we have only looked at attention as a way to handle information
    passing between two sequences, you could also use attention as a way to let a
    sequence attend to itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This is called *self-attention*, and it is quite powerful. With self-attention,
    each token can attend to every token in its own sequence, including itself, allowing
    the model to learn a representation of the word in context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example sentence: “The train left the station on time.” Now, consider
    one word in the sentence: “station.” What kind of station are we talking about?
    Could it be a radio station? Maybe the International Space Station? With self-attention,
    the model could learn to give a high attention score to the pair of “station”
    and “train,” summing the vector used to represent “train” into the representation
    of the word “station.”'
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention gives the model an effective way to go from representing a word
    in a vacuum to representing a word conditioned on all other tokens that appear
    in the sequence. This sounds a lot like what an RNN is supposed to do. Can we
    just go ahead and replace our RNN layers with `MultiHeadAttention`?
  prefs: []
  type: TYPE_NORMAL
- en: Almost! But not quite; we still need an essential ingredient for any deep neural
    network — a nonlinear activation function. The `MultiHeadAttention` layer combines
    linear projections of every element in a source sequence, but that’s it. In a
    sense, it’s a very expressive pooling operation. Consider, in the extreme case,
    a token length of one. In this case, the attention score matrix is always a single
    one, and the entire layer boils down to a linear projection of the source sequence,
    with no nonlinearities. You could stack 100 attention layers together and still
    be able to simplify the entire computation to a single matrix multiplication!
    That’s a real problem with the expressiveness of our model.
  prefs: []
  type: TYPE_NORMAL
- en: At some point, all recurrent cells pass the input vector for each token through
    a dense projection and apply an activation function; we need a plan for something
    similar. The authors of “Attention is all you need” decided to add this back in
    the simplest way possible — stacking a feedforward network of two dense layers
    with an activation in the middle. Attention passes information across the sequence,
    and the feedforward network updates the representation of individual sequence
    items.
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to start building a Transformer model. Let’s start by replacing
    the encoder of our translation model. We will use self-attention to pass information
    along the source sequence of English words. We will also add in two things we
    learned to be particularly important when building ConvNets back in chapter 9,
    *normalization* and _residual connections.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.14](#listing-15-14): A Transformer encoder block'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll note that the normalization layers we’re using here aren’t `BatchNormalization`
    layers like those we’ve used in image models. That’s because `BatchNormalization`
    doesn’t work well for sequence data. Instead, we’re using the `LayerNormalization`
    layer, which normalizes each sequence independently from other sequences in the
    batch — like this, in NumPy-like pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare to `BatchNormalization` (during training):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: While `BatchNormalization` collects information from many samples to obtain
    accurate statistics for the feature means and variances, `LayerNormalization`
    pools data within each sequence separately, which is more appropriate for sequence
    data.
  prefs: []
  type: TYPE_NORMAL
- en: We also pass a new input to the `MultiHeadAttention` layer called `attention_mask`.
    This Boolean tensor input will be broadcast to the same shape as our attention
    scores `(batch_size, target_length, source_length)`. When set, it will zero the
    attention score in specific locations, stopping the source tokens at these locations
    from being used in the attention calculation. We will use this to prevent any
    token in the sequence from attending to padding tokens, which contain no information.
    Our encoder layer takes a `source_mask` input that will mark all the non-padding
    tokens in our inputs and upranks it to shape `(batch_size, 1, source_length)`
    to use as an `attention_mask`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the input and outputs of this layer have the same shape, so encoder
    blocks can be stacked on top of each other, building a progressively more expressive
    representation of the input English sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer decoder block
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next up is the decoder block. This layer will be almost identical to the encoder
    block, except we want the decoder to use the encoder output sequence as an input.
    To do this, we can use attention twice. We first apply a self-attention layer
    like our encoder, which allows each position in the target sequence to use information
    from other target positions. We then add another `MultiHeadAttention` layer, which
    receives both the source and target sequence as input. We will call this attention
    layer *cross-attention* as it brings information across the encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.15](#listing-15-15): A Transformer decoder block'
  prefs: []
  type: TYPE_NORMAL
- en: Our decoder layer takes in both a `target` and `source`. Like with the `TransformerEncoder`,
    we take in a `source_mask` that marks the location of all padding in the source
    input (`True` for non-padding, `False` for padding) and use it as an `attention_mask`
    for the cross-attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: For the decoder’s self-attention layer, we need a different type of attention
    mask. Recall that when we built our RNN decoder, we avoided using a `Bidirectional`
    RNN. If we had used one, the model would be able to cheat by seeing the label
    it was trying to predict as a feature! Attention is inherently bidirectional;
    in self-attention, any token position in the target sequence can attend to any
    other position. Without special care, our model will learn to pass the next token
    in the sequence as the current label and will have no ability to generate novel
    translations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can achieve one-directional information flow with a special “causal” attention
    mask. Let’s say we pass an attention mask with ones in the lower-triangular section
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Each row `i` can be read as a mask for attention for the target token at position
    `i`. In the first row, the first token can only attend to itself. In the second
    row, the second token can attend to both the first and second tokens, and so forth.
    This gives us the same effect as our RNN layer, where information can only propagate
    forward in the sequence, not backward. In Keras, you can specify this lower-triangular
    mask simply by passing `use_casual_mask` to the `MultiHeadAttention` layer when
    calling it. Figure 15.8 shows a visual representation of the layers in both the
    encoder and decoder layers, when stacked into a Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e1ad371ac9fa40b33c1c0ac497e8fe9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 15.8](#figure-15-8): A visual representation of the computations for
    both `TransformerEncoder` and `TransformerDecoder` blocks'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence learning with a Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try putting this all together. We will use the same basic setup as our
    RNN model, replacing the `GRU` layers with our `TransformerEncoder` and `TransformerDecoder`.
    We will use `256` as the embedding size throughout the model, except in the feedforward
    block. In the feedforward block, we scale up the embedding size to `2048` before
    nonlinearity and scale back to the model’s hidden size afterward. This large intermediate
    dimension works well in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.16](#listing-15-16): Building a Transformer model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the summary of our Transformer model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model has almost exactly the same structure as the `GRU` translation model
    we trained earlier, with attention now substituting for recurrent layers as the
    mechanism to pass information across the sequence. Let’s try training the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we get to about 58% accuracy: on average, the model correctly
    predicts the next word in the Spanish sentence 58% of the time. Something is off
    here. Training is worse than the RNN model by 7 percentage points. Either this
    Transformer architecture is not what it was hyped up to be, or we missed something
    in our implementation. Can you spot what it is?'
  prefs: []
  type: TYPE_NORMAL
- en: This section is ostensibly about sequence models. In the previous chapter, we
    saw how vital word order could be to meaning. And yet, the Transformer we just
    built wasn’t a sequence model at all. Did you notice? It’s composed of dense layers
    that process sequence tokens independently of each other and an attention layer
    that looks at the tokens as a set. You could change the order of the tokens in
    a sequence, and you’d get identical pairwise attention scores and the same context-aware
    representations. If you were to rearrange every word in every English source sentence
    completely, the model wouldn’t notice, and you’d still get the same accuracy.
    Attention is a set-processing mechanism, focused on the relationships between
    pairs of sequence elements — it’s blind to whether these elements occur at the
    beginning, at the end, or in the middle of a sequence. So why do we say that Transformer
    is a sequence model? And how could it possibly be suitable for machine translation
    if it doesn’t look at word order?
  prefs: []
  type: TYPE_NORMAL
- en: For RNNs, we relied on the layer’s *computation* to be order aware. In the case
    of the Transformer, we instead inject positional information directly into our
    embedded sequence itself. This is called a *positional embedding.* Let’s take
    a look.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding positional information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea behind a positional embedding is very simple: to give the model access
    to word order information, we will add the word’s position in the sentence to
    each word embedding. Our input word embeddings will have two components: the usual
    word vector, which represents the word independently of any specific context,
    and a position vector, which represents the position of the word in the current
    sentence. Hopefully, the model will then figure out how to best use this additional
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward scheme to add position information would be concatenating
    each word’s position to its embedding vector. You’d add a “position” axis to the
    vector and fill it with `0` for the first word in the sequence, `1` for the second,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: However, that may not be ideal because the positions can potentially be very
    large integers, which will disrupt the range of values in the embedding vector.
    As you know, neural networks don’t like very large input values or discrete input
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The “Attention is all you need” authors used an interesting trick to encode
    word positions: they added to the word embeddings a vector containing values in
    the range `[-1, 1]` that varied cyclically depending on the position (they used
    cosine functions to achieve this). This trick offers a way to uniquely characterize
    any integer in a large range via a vector of small values. It’s clever, but it
    turns out we can do something simpler and more effective: we’ll learn positional
    embedding vectors the same way we learn to embed word indices. We’ll then add
    our positional embeddings to the corresponding word embeddings to obtain a position-aware
    word embedding. This is called a *positional embedding*. Let’s implement it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.17](#listing-15-17): A learned position embedding layer'
  prefs: []
  type: TYPE_NORMAL
- en: We would use this `PositionalEmbedding` layer just like a regular `Embedding`
    layer. Let’s see it in action as we try training our Transformer for a second
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.18](#listing-15-18): Building a Transformer model with positional
    embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the positional embedding now added to our model, let’s try training again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: With positional information back in the model, things went much better. We achieved
    a 67% accuracy when guessing the next word. It’s a noticeable improvement from
    the `GRU` model, and that’s all the more impressive when you consider that this
    model has half the parameters of the GRU counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: There’s one other important thing to notice about this training run. Training
    is noticeably faster than the RNN — each epoch takes about a third of the time.
    This would be true even if we matched parameter count with the RNN model, and
    it is a side effect of getting rid of the looped state passing of our `GRU` layers.
    With attention, there is no looping computation to handle during training, meaning
    that on a GPU or TPU, we can handle the entire attention computation in one go.
    This makes the `Transformer` quicker to train on accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s rerun generation with our newly trained `Transformer`. We can use the
    same code as we did for our RNN sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.19](#listing-15-19): Generating translations with a Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the generation code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Subjectively, the Transformer performs significantly better than the GRU-based
    translation model. It’s still a toy model, but it’s a better toy model.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer is a powerful architecture that has laid the basis for an explosion
    of interest in text-processing models. It’s also fairly complex, as deep learning
    models go. After seeing all of these implementation details, one might reasonably
    protest that this all seems quite arbitrary. There are so many small details to
    take on faith. How could we possibly know this choice and configuration of layers
    is optimal?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is simple — it’s not. Over the years, a number of improvements have
    been proposed to the Transformer architecture by making changes to attention,
    normalization, and positional embeddings. Many new models in research today are
    replacing attention altogether with something less computationally complex as
    sequence lengths get very long. Eventually, perhaps by the time you are reading
    this book, something will have supplanted the Transformer as the dominant architecture
    used for language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot we can learn from the Transformer that will stand the test of
    time. At the end of this chapter, we will discuss what makes the Transformer so
    effective. But it’s worth remembering that, as a whole, the field of machine learning
    moves empirically. Attention grew out of an attempt to augment RNNs, and after
    years of guessing and checking by a ton of people, it gave rise to the Transformer.
    There is little reason to think this process is done playing out.
  prefs: []
  type: TYPE_NORMAL
- en: Classification with a pretrained Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After “Attention is all you need,” people started to notice how far Transformer
    training could scale, especially compared to models that had come before. As we
    just mentioned, one big plus was that the model is faster to train than RNNs.
    No more loops during training, which is always good when working with a GPU or
    TPU.
  prefs: []
  type: TYPE_NORMAL
- en: It is also a very data hungry model architecture. We actually got a little taste
    of this in the last section. While our RNN translation model plateaued in validation
    performance after 5 or so epochs, the Transformer model was still improving its
    validation score after 30 epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: These observations prompted many to try scaling up the Transformer with more
    data, layers, and parameters — with great results. This caused a distinctive shift
    in the field toward large pretrained models that can cost millions to train but
    perform noticeably better on a wide range of problems in the text domain.
  prefs: []
  type: TYPE_NORMAL
- en: For our last code example in the text section, we will revisit our IMDb text-classification
    problem, this time with a pretrained Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining a Transformer encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the first pretrained Transformers to become popular in NLP was called
    BERT, short for Bidirectional Encoder Representations from Transformers^([[2]](#footnote-2)).
    The paper and model were released a year after “Attention Is All You Need.” The
    model structure was exactly the same as the encoder part of the translation Transformer
    we just built. This encoder model is *bidirectional* in that every position in
    the sequence can attend to positions in front of and behind it. This means it’s
    a good model for computing a rich representation of input text, but not a model
    meant to run generation in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: BERT was trained in sizes between 100 million and 300 million parameters, much
    bigger than the 14 million parameter Transformer we just trained. This meant the
    model needed a lot of training data to perform well. To achieve this, the authors
    used a riff on the classic language modeling setup called *masked language modeling*.
    To pretrain the model, we take a sequence of text and replace about 15% of the
    tokens with a special `[MASK]` token. The model will attempt to predict the original
    masked token values during training. While the classic language model, sometimes
    called a *causal language model*, attempts to predict `p(token|past tokens)`,
    the masked language model attempts to predict `p(token|surrounding tokens)`.
  prefs: []
  type: TYPE_NORMAL
- en: This training setup is unsupervised. You don’t need any labels about the text
    you feed in; for any text sequence, you can easily choose some random tokens and
    mask them out. That made it easy for the authors to find a large amount of text
    data needed to train models of this size. For the most part, they pulled from
    Wikipedia as a source.
  prefs: []
  type: TYPE_NORMAL
- en: Using pretrained word embeddings was already common practice when BERT was released
    — we saw this ourselves in the last chapter. But pretraining an entire Transformer
    gave something much more powerful — the ability to compute a word embedding for
    a word in the *context* of the words around it. And the Transformer allowed doing
    this at a scale and quality that were unheard of at the time.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of BERT took this model, pretrained on a huge amount of text, and
    specialized it to achieve state-of-the-art results on several NLP benchmarks at
    the time. This marked a distinctive shift in the field toward using very large,
    pretrained models, often with only a small amount of fine-tuning. Let’s try this
    out.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a pretrained Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using BERT here, let’s use a follow-up model called RoBERTa^([[3]](#footnote-3)),
    short for Robustly Optimized BERT. RoBERTa made some minor simplifications to
    BERT’s architecture, but most notably used more training data to improve performance.
    BERT used 16 GB of English language text, mainly from Wikipedia. The RoBERTa authors
    used 160 GB of text from all over the web. It’s estimated that RoBERTa cost a
    few hundred thousand dollars to train at the time. Because of this extra training
    data, the model performs noticeably better for an equivalent overall parameter
    count.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a pretrained model we will need a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A matching tokenizer* — Used with the pretrained model itself. Any text must
    be tokenized in the same way as during pretraining. If the words of our IMDb reviews
    map to different token indices than they would have during pretraining, we cannot
    use the learned representations of each token in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A matching model architecture* — To use the pretrained model, we need to recreate
    the math used internally by the model for pretraining exactly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The pretrained weights* — These weights were created by training the model
    for about a day on 1,024 GPUs and billions of input words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recreating the tokenizer and architecture code ourselves would not be too hard.
    The model internals almost exactly match the `TransformerEncoder` we built previously.
    However, matching a model implementation is a time-consuming process, and as we
    have done earlier in this book, we can instead use the KerasHub library to access
    pretrained model implementations for Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use KerasHub to load a RoBERTa tokenizer and model. We can use the special
    constructor `from_preset()` to load a pretrained model’s weights, configuration,
    and tokenizer assets from disk. We will load RoBERTa’s base model, which is the
    smallest of the few pretrained checkpoints released with the RoBERTa paper.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.20](#listing-15-20): Loading the RoBERTa pretrained model with
    KerasHub'
  prefs: []
  type: TYPE_NORMAL
- en: The `Tokenizer` maps from text to sequences of integers, as we would expect.
    Remember the `SubWordTokenizer` we built in the last chapter? RoBERTa’s tokenizer
    is almost the same as that tokenizer, with minor tweaks to handle Unicode characters
    from any language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the size of RoBERTa’s pretraining dataset, subword tokenization is a
    must. Using a character-level tokenizer would make input sequences way too long,
    making the model much more expensive to train. Using a word-level tokenizer would
    require a massive vocabulary to attempt to cover all the distinct words in the
    millions of documents of text used from across the web. Getting good coverage
    of words would blow up our vocabulary size and make the `Embedding` layer at the
    front of the Transformer unworkably large. Using a subword tokenizer allows the
    model to handle any word with only a 50,000-term vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: What is this `Backbone` we just loaded? We saw in chapter 8 that a `backbone`
    is a term often used in computer vision for a network that maps from input images
    to a latent space — basically a vision model without a head for making predictions.
    In KerasHub, a backbone refers to any pretrained model that is not yet specialized
    for a task. The model we just loaded takes in an input sequence and embeds it
    to an output sequence with shape `(batch_size, sequence_length, 768)`, but it’s
    not set up for a particular loss function. You could use it for any number of
    downstream tasks — classifying sentences, identifying text spans with certain
    information, identifying parts of speech, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will attach a classification head to this backbone that specializes
    it for our IMDb review classification fine-tuning. You can think of this as attaching
    different heads to a screwdriver: a Phillips head for one task, a flat head for
    another.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at our backbone. We loaded the *smallest* variant of RoBERTa
    here, but it still has 124 million parameters, which is the biggest model we have
    used in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: RoBERTa uses 12 Transformer encoder layers stacked on top of each other. That’s
    a big step up from our translation model!
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing IMDb movie reviews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can reuse the IMDb loading code we used in chapter 14 unchanged. This will
    download the movie review data to a `train_dir` and `test_dir` and split a validation
    dataset into a `val_dir`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: After loading, we once again have a training set of 20,000 movie reviews and
    a validation set of 5,000 movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before fine-tuning our classification model, we must tokenize our movie reviews
    with the RoBERTa tokenizer we loaded. During pretraining, RoBERTa used a specific
    form of “packing” tokens into a sequence, similar to what we did for our translation
    model. Each sequence would begin with an `<s>` token, end with an `</s>` token,
    and be followed by any number of `<pad>` tokens like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to match the token ordering used for pretraining as closely as
    possible; the model will train more quickly and accurately if we do. KerasHub
    provides a layer for this type of token packing called the `StartEndPacker`. The
    layer appends start, end, and padding tokens, trimming long sequences to a given
    sequence length if necessary. Let’s use it along with our tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.21](#listing-15-21): Preprocessing IMDb movie reviews with RoBERTa’s
    tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at a single preprocessed batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: With our inputs preprocessed, we are ready to start fine-tuning our model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a pretrained Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we fine-tune our backbone to predict movie reviews, we need to update
    it so it outputs a binary classification label. The backbone outputs an entire
    sequence with shape `(batch_size, sequence_length, 768)`, where each 768-dimensional
    vector represents an input word in the context of its surrounding words. Before
    predicting a label, we must condense this sequence to a single vector per sample.
  prefs: []
  type: TYPE_NORMAL
- en: One option would be to do mean pooling or max pooling across the whole sequence,
    computing an average of all token vectors. What works slightly better is simply
    using the first token’s representation as the pooled value. This is due to the
    nature of the attention in our model — the first position in the final encoder
    layer will be able to attend to all other positions in the sequence and pull information
    from them. So rather than pooling information with something coarse, like taking
    an average, attention allows us to pool information *contextually* across the
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now add a classification head to our backbone. We will also add one final
    `Dense` projection with a nonlinearity before generating an output prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.22](#listing-15-22): Extending the base RoBERTa model for classification'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are ready to fine-tune and evaluate the model on the IMDb dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 15.23](#listing-15-23): Training the RoBERTa classification model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s evaluate the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: In just a single epoch of training, our model reached 93%, a noticeable improvement
    from the 90% ceiling we hit in our last chapter. Of course, this is a far more
    expensive model to use than the simple bigram classifier we built previously,
    but there are clear benefits to using such a large model. And this is all with
    the smaller size of the RoBERTa model. Using the larger 300 million parameter
    model, we could achieve an accuracy of over 95%.
  prefs: []
  type: TYPE_NORMAL
- en: What makes the Transformer effective?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2013, at Google, Tomas Mikolov and his colleagues noticed something remarkable.
    They were building a pretrained embedding called “Word2Vec,” similar to the Continuous
    Bag of Words (CBOW) embedding we built in the last chapter. Much like our CBOW
    model, their training objective sought to turn correlation relationships between
    words into distance relationships in the embedding space: a vector was associated
    with each word in a vocabulary, and the vectors were optimized so that the dot-product
    (cosine proximity) between vectors representing frequently co-occurring words
    would be closer to 1, while the dot-product between vectors representing rarely
    co-occurring words would be closer to 0.'
  prefs: []
  type: TYPE_NORMAL
- en: They found that the resulting embedding space did much more than capture semantic
    similarity. It featured some form of emergent learning — a sort of “word arithmetic.”
    A vector existed in the space that you could add to many male nouns to obtain
    a point that would land close to its female equivalent, as in `V(king) - V(man)
    + V(woman) = V(queen)`, a gender vector. This was quite surprising; the model
    had not been trained for this in any explicit way. There seemed to be dozens of
    such magic vectors — a plural vector, a vector to go from wild animals’ names
    to their closest pet equivalent, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Fast-forward about 10 years — we are now in the age of large, pretrained Transformer
    models. On the surface, these models couldn’t seem any further from the primitive
    Word2Vec model. A Transformer can generate perfectly fluent language — a feat
    Word2Vec was entirely incapable of. As we will see in the next chapter, such models
    can seem knowledgeable about virtually any topic. And yet, they actually have
    a lot in common with good old Word2Vec.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both models seek to embed tokens (words or subwords) in a vector space. Both
    rely on the same fundamental principle to learn this space: tokens that appear
    together end up close in the embedding space. The distance function used to compare
    tokens is cosine distance in both cases. Even the dimensionality of the embedding
    space is similar: a vector with somewhere between 1,000 and 10,000 dimensions
    to represent each word.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you might interject: A Transformer is trained to predict missing
    words in a sequence, not to group tokens in an embedding space. How does the language
    model loss function relate to Word2Vec’s objective of maximizing the dot-product
    between co-occurring tokens? The answer is the attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention is, by far, the most critical component in the Transformer architecture.
    It’s a mechanism for learning a new token embedding space by linearly recombining
    token embeddings from some prior space, in weighted combinations that give greater
    importance to tokens that are already “closer” to each other (i.e., that have
    a higher dot-product). It will tend to pull together the vectors of already close
    tokens, resulting over time in a space where token correlation relationships turn
    into embedding proximity relationships (in terms of cosine distance). Transformers
    work by learning a series of incrementally refined embedding spaces, each based
    on recombining elements from the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention provides Transformers with two crucial properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding spaces they learn are semantically continuous — that is, moving
    a bit in an embedding space only changes the human-facing meaning of the corresponding
    tokens by a bit. The Word2Vec space also exhibited this property.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embedding spaces they learn are semantically interpolative — that is, taking
    the intermediate point between two points in an embedding space produces a point
    representing the “intermediate meaning” between the corresponding tokens. This
    comes from the fact that each new embedding space is built by interpolating between
    vectors from the previous space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not entirely unlike the way the brain learns. The key learning principle
    in the brain is Hebbian learning — in short, “neurons that fire together, wire
    together.” Correlation relationships between neural firing events (which may represent
    actions or perceptual inputs) are turned into proximity relationships in the brain
    network, just like the Transformer and Word2Vec turn correlation relationships
    into vector proximity relationships. Both are maps of a space of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, there are significant differences between Word2Vec and the Transformer.
    Word2Vec was not designed for generative text sampling. A Transformer can get
    far bigger and can encode vastly more complex transformations. The thing is, Word2Vec
    is very much a toy model: it is to today’s language models as a logistic regression
    on MNIST pixels is to state-of-the-art computer vision models. The fundamental
    principles are mostly the same, but the toy model lacks any meaningful representation
    power. Word2Vec wasn’t even a deep neural network — it had a shallow, single-layer
    architecture. Meanwhile, today’s Transformer models have the highest representation
    power of any model anyone has ever trained — they feature dozens of stacked attention
    and feedforward layers, and their parameter count ranges in the billions.'
  prefs: []
  type: TYPE_NORMAL
- en: Like with Word2Vec, the Transformer learns useful semantic functions as a by-product
    of organizing tokens into a vector space. But thanks to this increased representation
    power and a much more refined autoregressive optimization objective, we’re no
    longer confined to linear transformations like a gender vector or a plural vector.
    Transformers can store arbitrarily complex vector functions — so complex, in fact,
    that it would be more accurate to refer to them as vector programs rather than
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec enabled you to do basic things like `plural(cat) → cats` or `male_to_female(king)
    → queen`. Meanwhile, a large Transformer model can do pure magic — things like
    `write_this_in_style_of_shakespeare("...your poem...") → "...new poem..."`. And
    a single model can contain millions of such programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see a Transformer as analogous to a database: it stores information
    you can retrieve via the tokens you pass in. But there are two important differences
    between a Transformer and a database.'
  prefs: []
  type: TYPE_NORMAL
- en: The first difference is that a Transformer is a continuous, interpolative kind
    of database. Instead of being stored as a set of discrete entries, your data is
    stored as a vector space — a curve. You can move around on the curve (it’s semantically
    continuous, as we discussed) to explore nearby, related points. And you can interpolate
    on the curve between different data points to find their in-between. This means
    that you can retrieve a lot more from your database than you put into it — although
    not all of it will be accurate or meaningful. Interpolation can lead to generalization,
    but it can also lead to hallucinations — a significant problem facing the generative
    language models trained today.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second difference is that a Transformer doesn’t just contain data. For
    models like RoBERTa, trained on hundreds of thousands of documents scraped from
    the internet, there is a lot of data: facts, places, people, dates, things, and
    relationships. But it’s also — perhaps primarily — a database of programs.'
  prefs: []
  type: TYPE_NORMAL
- en: They’re different from the kind of programs you’re used to dealing with, mind
    you. These are not like Python programs — series of symbolic statements processing
    data step by step. Instead, these vector programs are highly nonlinear functions
    that map the latent embedding space unto itself, analogous to Word2Vec’s magic
    vectors, but far more complex.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will push Transformer models to an entirely new scale.
    Models will use billions of parameters and train on trillions of words. Output
    from these models can often feel like magic — like an intelligent operator sitting
    inside our model and pulling the strings. But it’s important to remember that
    these models are fundamentally interpolative — thanks to attention, they learn
    an interpolative embedding space for a significant chunk of all text written in
    the English language. Wandering this embedding space can lead to interesting,
    unexpected generalizations, but it cannot synthesize something fundamentally new
    with anything close to genuine, human-level intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A *language model* is a model that learns a specific probability distribution
    — `p(token|past tokens)`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language models have broad applications, but the most important is that you
    can generate text by calling them in a loop, where the output token at one time
    step becomes the input token in the next.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A *masked language model* learns a related probability distribution `p(tokens|surrounding
    tokens)` and can be helpful for classifying text and individual tokens.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A *sequence-to-sequence language model* learns to predict the next token given
    both past tokens in a target sequence and an entirely separate, fixed source sequence.
    Sequence-to-sequence models are useful for problems like translation and question
    answering.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A sequence-to-sequence model usually has two separate components. An *encoder*
    computes a representation of the source sequence, and a *decoder* takes this representation
    as input and predicts the next token in a target sequence based on past tokens.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention* is a mechanism that allows a model to pull information from anywhere
    in a sequence selectively based on the context of the token currently being processed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention avoids the problems RNNs have with long-range dependencies in text.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention works by taking the dot-product of two vectors to compute an attention
    score. Vectors near each other in an embedding space will be summed together in
    the attention mechanism.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Transformer* is a sequence modeling architecture that uses attention as
    the only mechanism to pass information across a sequence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Transformer works by stacking blocks of alternating attention and two-layer
    feedforward networks.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Transformer can scale to many parameters and lots of training data while
    still improving accuracy in the language modeling problem.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike RNNs, the Transformer involves no sequence-length loops at training time,
    making the model much easier to train in parallel across many machines.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A Transformer encoder uses bidirectional attention to build a rich representation
    of a sequence.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A Transformer decoder uses causal attention to predict the next word in a language
    model setup.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vaswani et al., “Attention Is All You Need,” Proceedings of the 31st International
    Conference on Neural Information Processing Systems (2017), [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762).
    [[↩]](#footnote-link-1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding,” Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (2019), [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).
    [[↩]](#footnote-link-2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach” (2019),
    [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692). [[↩]](#footnote-link-3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
