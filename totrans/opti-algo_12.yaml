- en: 9 Particle swarm optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 粒子群优化
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing swarm intelligence
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍群体智能
- en: Understanding the continuous particle swarm optimization algorithm
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解连续粒子群优化算法
- en: Understanding binary particle swarm optimization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解二进制粒子群优化
- en: Understanding permutation-based particle swarm optimization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于排列的粒子群优化
- en: Adapting particle swarm optimization for a better trade-off between exploration
    and exploitation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应粒子群优化以实现探索和利用之间的更好权衡
- en: Solving continuous and discrete problems using particle swarm optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用粒子群优化解决连续和离散问题
- en: In the treasure-hunting mission I introduced in chapter 2, suppose you want
    to collaborate and share information with your friends instead of doing the treasure-
    hunting alone. However, you do not want to follow a competitive approach in which
    you only keep better-performing hunters and recruit new hunters to replace poorer-performing
    ones, like in the genetic algorithm (GA) explained in the previous chapters. You
    want to adopt a more cooperative approach and keep all the hunters, without replacing
    any, but you want to give more weight to the better-performing hunters and try
    to emulate their success. This scenario uses *swarm intelligence* and corresponds
    to population-based optimization algorithms such as *particle swarm optimization*
    (PSO), *ant colony optimization* (ACO), and *artificial bee colony* (ABC) algorithms,
    which will be explained in this fourth part of the book.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中我介绍的寻宝任务中，假设你想要与你的朋友合作并共享信息，而不是独自进行寻宝。然而，你不想采取一种竞争性的方法，在这种方法中，你只保留表现更好的猎人，并招募新的猎人替换表现较差的猎人，就像在前几章中解释的遗传算法（GA）那样。你想要采取一种更合作的方法，保留所有猎人，不进行任何替换，但你希望给予表现更好的猎人更多的权重，并尝试模仿他们的成功。这种场景使用了*群体智能*，对应于基于群体的优化算法，如*粒子群优化*（PSO）、*蚁群优化*（ACO）和*人工蜂群*（ABC）算法，这些算法将在本书的第四部分中解释。
- en: In this chapter, we’ll focus on different variants of PSO algorithms and apply
    them to solve continuous and discrete optimization problems. These variants include
    continuous PSO, binary PSO, permutation-based PSO, and adaptive PSO. Function
    optimization, the traveling salesman problem, neural network training, trilateration,
    coffee shop planning, and the doctor scheduling problem are discussed in this
    chapter and its supplementary exercises included in appendix C. The next chapter
    will cover the ACO and ABC algorithms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注PSO算法的不同变体，并将它们应用于解决连续和离散优化问题。这些变体包括连续PSO、二进制PSO、基于排列的PSO和自适应PSO。本章讨论了函数优化、旅行商问题、神经网络训练、三角测量、咖啡馆规划以及医生排班问题，并在附录C中包含了补充练习。下一章将介绍ACO和ABC算法。
- en: 9.1 Introducing swarm intelligence
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 介绍群体智能
- en: 'Life on this planet is full of astonishing examples of collective behavior.
    Individual species depend upon one another for sustenance, often forming surprising
    alliances to achieve a common goal: the continuance of the species. The majority
    of living things also display amazing altruism in order to protect and provide
    the best care for their offspring, comparable to any form of sacrifice shown by
    human beings. They can cooperatively perform complex tasks such as foraging for
    food, dividing up labor, constructing nests, brood sorting, protecting, herding,
    schooling, and flocking, to name just a few. These complex collective behaviors
    emerge from individual interactions between spatially distributed and simple entities
    without a centralized controller or coordinator and without a script or access
    to global information. Various cooperation patterns, communication mechanisms,
    and adaptation strategies are employed to enable such complex collective behaviors.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个星球上，集体行为充满了令人惊叹的例子。各种物种为了生存相互依赖，常常形成令人惊讶的联盟以实现共同的目标：物种的延续。大多数生物也表现出惊人的利他主义，为了保护和为它们的后代提供最佳护理，这与人类所表现出的任何形式的牺牲相当。它们可以合作完成复杂的任务，如觅食、分配劳动、建造巢穴、孵化排序、保护、放牧、鱼群和鸟群，仅举几例。这些复杂的集体行为是从空间分布的简单实体之间的个体互动中产生的，没有中央控制器或协调者，也没有脚本或访问全局信息。各种合作模式、通信机制和适应策略被采用，以实现这种复杂的集体行为。
- en: Swarm intelligence
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 群体智能
- en: Swarm intelligence is a subfield of artificial intelligence that explores how
    large groups of relatively simple and spatially distributed agents can interact
    with each other and with their environment in a decentralized and self-organized
    manner to collectively achieve complex goals.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 群体智能是人工智能的一个子领域，它探索了大量的相对简单且空间分布的智能体如何以去中心化和自我组织的方式相互以及与它们的环境互动，以集体实现复杂目标。
- en: Several efficient population-based algorithms have been designed to exploit
    the power of collective intelligence by mimicking the collective behaviors observed
    in nature to solve complex optimization problems. Table 9.1 provides a non-exhaustive
    list of swarm intelligence algorithms and their sources of inspiration from unicellular
    and multicellular living organisms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 已经设计了几种高效的基于群体的算法，通过模仿自然界中观察到的集体行为来利用集体智慧的力量，以解决复杂的优化问题。表9.1提供了一个非详尽的群体智能算法及其灵感来源的单细胞和多细胞生物列表。
- en: Table 9.1 Examples of swarm intelligence algorithms and their sources of inspiration
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 群体智能算法及其灵感来源的示例
- en: '| Organisms | Class | Source of inspiration | Algorithms |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 生物体 | 类别 | 灵感来源 | 算法 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Unicellular organisms | Bacteria | Bacterial swarm foraging | Bacterial foraging
    optimization algorithm (BFO)Bacterial swarming algorithm (BSA) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 单细胞生物 | 细菌 | 细菌集群觅食 | 细菌觅食优化算法 (BFO) 细菌集群算法 (BSA) |'
- en: '| Multicellular organisms | Bird/fish | Bird flocking and fish schooling |
    Particle swarm optimization (PSO) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 多细胞生物 | 鸟/鱼 | 鸟群飞行和鱼群游动 | 粒子群优化 (PSO) |'
- en: '| Ant | Ant foraging behaviors | Ant colony optimization (ACO) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 蚂蚁 | 蚂蚁的觅食行为 | 蚂蚁群优化 (ACO) |'
- en: '| Bees | Foraging behavior of honeybees | Artificial bee colony (ABC) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 蜜蜂 | 蜜蜂的觅食行为 | 人工蜂群 (ABC) |'
- en: '| Bats | Echolocation behavior of bats | Bat algorithm (BA) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 蝙蝠 | 蝙蝠的回声定位行为 | 蝙蝠算法 (BA) |'
- en: '| Fireflies | Flashing behavior of fireflies | Firefly algorithm (FA) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 萤火虫 | 萤火虫的闪烁行为 | 萤火虫算法 (FA) |'
- en: '| Butterflies | Foraging behavior of butterflies | Butterfly optimization algorithm
    (BOA) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 蝴蝶 | 蝴蝶的觅食行为 | 蝴蝶优化算法 (BOA) |'
- en: '| Dragonflies | Static and dynamic swarming behaviors of dragonflies | Dragonfly
    algorithm (DA) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 蜻蜓 | 蜻蜓的静态和动态集群行为 | 蜻蜓算法 (DA) |'
- en: '| Spiders | Cooperative behavior of the social spiders | Social spider optimization
    (SSO) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 蜘蛛 | 社会性蜘蛛的协作行为 | 社会性蜘蛛优化 (SSO) |'
- en: '| Krill | Herding behavior of krill | Krill herd (KH) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 食藻类动物 | 食藻类动物的群居行为 | 食藻类动物群 (KH) |'
- en: '| Frogs | Frog cooperative search for food | Shuffled frog leaping algorithm
    (SFLA) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 青蛙 | 青蛙的觅食合作 | 混洗青蛙跳跃算法 (SFLA) |'
- en: '| Fish | Gregarious behavior of fish | Fish school search (FSS) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 鱼 | 鱼的群居行为 | 鱼群搜索 (FSS) |'
- en: '| Dolphins | Dolphins’ behavior in detecting, chasing after, and preying on
    swarms of sardines | Dolphin partner optimization (DPO)Dolphin swarm optimization
    algorithm (DSOA) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 海豚 | 海豚在检测、追逐和捕食沙丁鱼群的行为 | 海豚伙伴优化 (DPO) 海豚群优化算法 (DSOA) |'
- en: '| Cats | Resting and tracing behaviors of cats | Cat swarm optimization (CSO)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 猫 | 猫的休息和追踪行为 | 猫群优化 (CSO) |'
- en: '| Monkeys | Search for food | Monkey search algorithm (MSA) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 猴子 | 寻找食物 | 猴子搜索算法 (MSA) |'
- en: '| Lions | Solitary and cooperative behaviors of lions | Lion optimization algorithm
    (LOA) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 狮子 | 狮子的独居和合作行为 | 狮子优化算法 (LOA) |'
- en: '| Cuckoos | Reproduction strategy of cuckoos | Cuckoo search (CS)Cuckoo optimization
    algorithm (COA) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 鹰科鸟类 | 鹰科鸟类的繁殖策略 | 鹰科搜索 (CS) 鹰科优化算法 (COA) |'
- en: '| Wolves | Leadership hierarchy and hunting mechanism of gray wolves | Wolf
    search algorithm (WSA)Grey wolf optimizer (GWO) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 狼 | 灰狼的领导层级和狩猎机制 | 狼搜索算法 (WSA) 灰狼优化器 (GWO) |'
- en: For example, bacteria, which are single-celled organisms, possess underlying
    social intelligence allowing them to cooperate to solve challenges. Bacteria develop
    intricate communication capabilities like chemotactic signaling to cooperatively
    self-organize into highly structured colonies with elevated environmental adaptability.
    Bacterial chemotaxis is the process by which bacterial cells migrate through concentration
    gradients of chemical attractants and repellents. The E. coli bacterium uses this
    bacterial chemotaxis during the foraging process. This collective behavior provides
    the basis for optimization algorithms such as the bacterial foraging optimization
    algorithm (BFO) and bacterial swarming algorithm (BSA).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，细菌，作为单细胞生物，拥有潜在的社会智慧，这使得它们能够合作解决挑战。细菌发展出复杂的通讯能力，如趋化性信号，以合作的方式自我组织成高度结构化的群体，并提高环境适应性。细菌的趋化性是指细菌细胞通过化学吸引剂和排斥剂的浓度梯度迁移的过程。大肠杆菌在觅食过程中就利用了这种细菌趋化性。这种集体行为为优化算法如细菌觅食优化算法（BFO）和细菌集群算法（BSA）提供了基础。
- en: 'Ethology, the study of animal behavior, is the main source of inspiration for
    swarm intelligence algorithms such as particle swarm optimization (PSO), ant colony
    optimization (ACO), artificial bee colony (ABC), bat algorithm (BA), firefly algorithm
    (FA), and social spider optimization (SSO). For example, honeybees are highly
    cooperative social insects that cooperatively construct hives in which about 30,000
    bees can live and work together. They differentiate their work: some make wax,
    some make honey, some make bee-bread, some shape and mold combs, and some bring
    water to the cells and mingle it with the honey. Young bees engage in out-of-door
    work while the elder bees do the indoor work. During the foraging process, rather
    than expending energy searching in all directions, honeybee colonies use individual
    foragers to reduce the cost/ benefit ratio. Additionally, colonies concentrate
    their foraging efforts on the most lucrative patches and disregard those of lesser
    quality. It has been observed that when a colony’s food resources are scarce,
    foragers recruit more nestmates to food sources they have found, and changes in
    their dance patterns upon returning to the hive facilitate this increased recruitment.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 生态学，动物行为的研究，是群体智能算法如粒子群优化（PSO）、蚁群优化（ACO）、人工蜂群（ABC）、蝙蝠算法（BA）、萤火虫算法（FA）和社会蜘蛛优化（SSO）的主要灵感来源。例如，蜜蜂是一种高度合作的社交昆虫，它们合作建造蜂巢，大约有30,000只蜜蜂可以生活在其中并共同工作。它们分工明确：有的制作蜂蜡，有的制作蜂蜜，有的制作蜂粮，有的塑造和塑造蜂巢，有的将水带到蜂房并与蜂蜜混合。年轻的蜜蜂从事户外工作，而年老的蜜蜂则从事室内工作。在觅食过程中，蜜蜂群体不是在所有方向上消耗能量搜索，而是使用单个觅食者来降低成本/收益比。此外，群体将觅食努力集中在最有利可图的区域，并忽视那些质量较差的区域。观察到，当群体的食物资源稀缺时，觅食者会招募更多的巢居者到它们找到的食物来源，它们在返回蜂巢时舞蹈模式的改变有助于这种增加的招募。
- en: The fundamental components of swarm intelligence algorithms typically involve
    numerous decentralized processing agents that operate without central supervision.
    These agents communicate with neighboring agents and adapt their behavior based
    on received information. Furthermore, the majority of the research carried out
    on swarm intelligence algorithms is primarily based on experimental observations
    of collective behavior exhibited by living organisms. These observations are translated
    into models, which are then tested through simulations in order to derive the
    metaheuristics that form the basis of swarm intelligence algorithms, as illustrated
    in figure 9.1\. This experimental approach enables researchers to gain a deeper
    understanding of the complex interactions between individual agents and how they
    give rise to collective behavior. By simulating these interactions and testing
    various scenarios, researchers can refine the algorithms and improve their effectiveness.
    As an example of such experimental research, you can watch the “The Waggle Dance
    of the Honeybee” video of the experiment conducted by Georgia Tech College of
    Computing to understand how honeybees communicate the location of new food sources
    (www.youtube.com/watch?v=bFDGPgXtK-U).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 群智能算法的基本组件通常涉及大量无需中央监督的分布式处理代理。这些代理与邻近代理进行通信，并根据接收到的信息调整其行为。此外，对群智能算法进行的绝大多数研究主要基于对生物体集体行为的实验观察。这些观察结果被转化为模型，然后通过模拟进行测试，以推导出构成群智能算法基础的元启发式算法，如图9.1所示。这种实验方法使研究人员能够更深入地了解个体代理之间的复杂相互作用以及它们如何产生集体行为。通过模拟这些相互作用并测试各种场景，研究人员可以改进算法并提高其有效性。例如，您可以通过观看乔治亚理工学院计算机学院进行的“蜜蜂摇摆舞”实验视频来了解蜜蜂如何传达新食物源的位置（www.youtube.com/watch?v=bFDGPgXtK-U）。
- en: '![](../Images/CH09_F01_Khamis.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F01_Khamis.png)'
- en: Figure 9.1 Derivation process for swarm intelligence algorithms
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 群智能算法的推导过程
- en: 'Algorithm 9.1 shows the common steps in a swarm intelligence algorithm. The
    algorithm starts by initializing the algorithm parameters, such as the number
    of individuals in the swarm, the maximum number of iterations, and the termination
    criteria. A swarm of initial candidate solutions is then sampled (the different
    sampling methods were explained in section 7.1). The algorithm then iterates over
    all the individuals in the swarm, performing the following operations: finding
    the best so far, finding the best neighbor, and updating the individual.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 算法9.1展示了群智能算法中的常见步骤。算法首先初始化算法参数，例如群体中个体的数量、最大迭代次数和终止条件。然后从初始候选解群体中采样（不同的采样方法在第7.1节中已解释）。算法随后遍历群体中的所有个体，执行以下操作：找到迄今为止的最佳解、找到最佳邻居以及更新个体。
- en: Algorithm 9.1 Swarm intelligence algorithm
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 算法9.1 群智能算法
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The individual and the neighbor are evaluated using the defined objective/fitness
    function. The neighborhood structure and update mechanism depend on the algorithm
    being used. This loop over all the individuals is repeated until the termination
    criterion is met, which could be a maximum number of iterations or reaching a
    satisfactory fitness level. At this point, the algorithm stops and returns the
    best solution found during the optimization process. In the following sections,
    we’ll dive deep into the PSO algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用定义的目标/适应度函数对个体及其邻居进行评估。邻域结构和更新机制取决于所使用的算法。这个遍历所有个体的循环会重复进行，直到满足终止条件，这可能是一个最大迭代次数或达到令人满意的适应度水平。在此阶段，算法停止并返回优化过程中找到的最佳解。在接下来的章节中，我们将深入探讨PSO算法。
- en: 9.2 Continuous PSO
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 连续PSO
- en: Particle swarm optimization (PSO) is a population-based stochastic optimization
    technique developed by Russell Eberhart and James Kennedy in 1995\. Since then,
    PSO has gained popularity and has been applied to various real-world applications
    in different domains. This algorithm is inspired by the conduct of social organisms
    such as birds, fish, ants, termites, wasps, and bees. PSO emulates the actions
    of these creatures, with each member of the swarm being referred to as a *particle*,
    akin to a bird in a flock, a fish in a school, or a bee in a colony. Eberhart
    and Kennedy opted to use the term “particle” to refer to an individual or candidate
    solution in the context of optimization, as they believed it was more suitable
    for describing the particle’s velocity and acceleration.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 粒子群优化（PSO）是一种由Russell Eberhart和James Kennedy于1995年开发的基于群体的随机优化技术。从那时起，PSO已经变得流行，并被应用于不同领域的各种现实世界应用。该算法受到鸟类、鱼类、蚂蚁、白蚁、黄蜂和蜜蜂等社会生物行为的启发。PSO模仿这些生物的行为，群体中的每个成员被称为*粒子*，类似于鸟群中的鸟、鱼群中的鱼或蜂群中的蜜蜂。Eberhart和Kennedy选择使用“粒子”一词来指代优化中的个体或候选解，因为他们认为这更适合描述粒子的速度和加速度。
- en: Bird flocking
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 鸟群行为
- en: 'Bird flocking is a behavior controlled by three simple rules, as illustrated
    in the following figure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 鸟群行为是由三个简单规则控制的行为，如下面的图所示：
- en: '*Separation*—Prevent getting too close to nearby birds to avoid overcrowding.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分离*—避免靠近附近的鸟，以防止过度拥挤。'
- en: '*Alignment*—Adjust the heading to correspond to the average direction of neighboring
    birds.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对齐*—调整航向以对应邻近鸟类的平均方向。'
- en: '*Coherence*—Move toward the average position of neighboring birds.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一致性*—向邻近鸟类的平均位置移动。'
- en: '![](../Images/CH09_F01_UN01_Khamis.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F01_UN01_Khamis.png)'
- en: 'Bird flocking rules: separation, alignment, and coherence'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 鸟群行为规则：分离、对齐和一致性
- en: When birds—spatially distributed agents interacting with each other and with
    their environment in a decentralized and self-organized manner without access
    to global information—apply these three simple rules, the outcome is the emergent
    behavior of bird flocking.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当鸟类——作为在去中心化和自我组织的方式下相互作用的分布式代理，与它们的环境相互作用，但没有访问全局信息——应用这三个简单规则时，结果是鸟群行为的涌现行为。
- en: 'The particles (candidate solutions) move, or fly, through the feasible search
    space by following the current best particles. Thus, PSO is guided by a straightforward
    principle: emulate the success of neighboring individuals. Each particle in the
    swarm operates in a decentralized manner by utilizing both its own intelligence
    and the collective intelligence of the group. Therefore, if one particle uncovers
    a favorable route to food, the remaining members of the swarm can immediately
    adopt the same path.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 粒子（候选解）通过跟随当前最佳粒子在可行搜索空间中移动或飞行。因此，PSO遵循一个简单的原则：模仿邻近个体的成功。群体中的每个粒子以去中心化的方式运作，利用自身的智能和群体的集体智能。因此，如果一个粒子发现了一条通往食物的有利路径，群体中的其他成员可以立即采用相同的路径。
- en: The PSO algorithm
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: PSO算法
- en: “This [PSO] algorithm belongs ideologically to that philosophical school that
    allows wisdom to emerge rather than trying to impose it, that emulates nature
    rather than trying to control it, and that seeks to make things simpler rather
    than more complex.” J. Kennedy and R. Eberhart, inventors of PSO [1].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: “这个[PSO]算法在意识形态上属于那个允许智慧自然涌现而不是试图强加智慧、模仿自然而不是试图控制自然、寻求使事物更简单而不是更复杂的哲学学派。” J.
    Kennedy 和 R. Eberhart，PSO的发明者 [1]。
- en: Figure 9.2 shows the PSO flowchart. We start by initializing the algorithm parameters
    and creating an initial swarm of particles. These particles represent the candidate
    solutions. Each particle in the search space holds the current position *x^i*
    and current velocity *v^i*. The fitness of each particle is then evaluated based
    on the fitness/objective function to be optimized. The best position each particle
    has achieved so far is known as the personal best or *pbest*. The best position
    achieved by the particles in its neighborhood is known as *nbest*. If the neighborhood
    is restricted to a few particles, the best is called the local best, *lbest*.
    If the neighborhood is the whole swarm, the best achieved by the whole swarm is
    called the global best, *gbest*. We’ll discuss neighborhood structures in PSO
    further in section 9.2.3.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 展示了 PSO 流程图。我们首先初始化算法参数并创建一个初始粒子群。这些粒子代表候选解。搜索空间中的每个粒子都持有当前位置 *x^i* 和当前速度
    *v^i*。每个粒子迄今为止达到的最佳位置称为个人最佳或 *pbest*。粒子在其邻域中达到的最佳位置称为 *nbest*。如果邻域限制为少数几个粒子，则最佳称为局部最佳，*lbest*。如果邻域是整个群体，则整个群体达到的最佳称为全局最佳，*gbest*。我们将在
    9.2.3 节中进一步讨论 PSO 的邻域结构。
- en: '![](../Images/CH09_F02_Khamis.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F02_Khamis.png)'
- en: Figure 9.2 The PSO algorithm
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 PSO 算法
- en: After evaluating the fitness of each particle, PSO updates each particle’s personal
    best position if the current fitness is superior, identifies the global best position
    based on the best fitness in the entire swarm, and adjusts particle velocities
    and positions using a combination of personal and global information. These steps
    guide the swarm toward optimal or near-optimal solutions by balancing individual
    and collective learning, promoting exploration and exploitation in the search
    space. The process iterates until termination criteria are met.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估每个粒子的适应度后，PSO（粒子群优化）会更新每个粒子的个人最佳位置，如果当前适应度更高，然后根据整个群体中最佳适应度确定全局最佳位置，并使用个人和全局信息的组合来调整粒子的速度和位置。这些步骤通过平衡个体和集体学习，促进搜索空间中的探索和利用，引导群体向最优或近似最优解发展。这个过程会迭代进行，直到满足终止条件。
- en: 9.2.1 Motion equations
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 运动方程
- en: 'The velocity (*v*) and position (*x*) of each particle are updated using the
    following equations:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个粒子的速度 (*v*) 和位置 (*x*) 使用以下方程进行更新：
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F02_Khamis-EQ01.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F02_Khamis-EQ01.png)'
- en: '| 9.1 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 9.1 |'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F02_Khamis-EQ02.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F02_Khamis-EQ02.png)'
- en: '| 9.2 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 9.2 |'
- en: where
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*k* is the iteration number.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 是迭代次数。'
- en: '*i* and *d* are the particle number and the dimension. For example, dimension
    = 1 in the case of a univariate optimization problem with a single decision variable,
    dimension = 2 in the case of a bivariate problem, etc.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i* 和 *d* 是粒子编号和维度。例如，在只有一个决策变量的单变量优化问题中，维度 = 1；在双变量问题中，维度 = 2 等。'
- en: '*ω* is the inertia weight.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ω* 是惯性权重。'
- en: '*c*1, *c*2 are the acceleration coefficients.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c*1, *c*2 是加速度系数。'
- en: '*r*1, *r*2 are random numbers between 0 and 1 and are generated in each iteration
    for each dimension, and not for each particle.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r*1, *r*2 是介于 0 和 1 之间的随机数，并在每个迭代中为每个维度生成，而不是为每个粒子生成。'
- en: '*pbest* is the best position achieved by the particle.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pbest* 是粒子达到的最佳位置。'
- en: '*gbest* is the best position achieved by the whole swarm. *gbest* should be
    replaced by *nbest or lbest* if you are dividing the swarm into multiple neighborhoods.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gbest* 是整个群体达到的最佳位置。如果您将群体划分为多个邻域，则应将 *gbest* 替换为 *nbest* 或 *lbest*。'
- en: As you can see in these two equations, we start by updating the velocity *v*[(]*[k]*
    [+ 1)]*^(id)*. The position is then updated to *x*[(]*[k]* [+ 1)]*^(id)* by taking
    the current position *x[k]^(id)* and adding to it the new displacement *v*[(]*[k]*
    [+ 1)]*^(id)* × *timestamp* where *timestamp* = 1, which represents a single iteration.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如这两个方程所示，我们首先更新速度 *v*[(]*[k]* [+ 1)]*^(id)*。然后，通过将当前位置 *x[k]^(id)* 与新的位移 *v*[(]*[k]*
    [+ 1)]*^(id)* × *timestamp* 相加，更新位置到 *x*[(]*[k]* [+ 1)]*^(id)*，其中 *timestamp*
    = 1，这代表单次迭代。
- en: 'To understand these motion update equations, let’s visualize these equations
    using vectors in the 2D Cartesian coordinate system, as shown in figure 9.3\.
    As you can see, the velocity update equation consists of three primary components,
    each contributing to the movement of particles in the search space:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些运动更新方程，让我们使用二维笛卡尔坐标系中的向量来可视化这些方程，如图 9.3 所示。正如您所看到的，速度更新方程由三个主要组成部分组成，每个部分都贡献于搜索空间中粒子的运动：
- en: '*Inertia component*—The first part of the velocity update equation represents
    the influence of a particle’s inertia, taking into account that a particle (like
    a fish in a school or a bird in a flock) cannot abruptly change its direction.
    As you will see later, this inertia component is crucial, as it allows the algorithm
    to be more adaptive and helps maintain a balance between exploration and exploitation.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*惯性成分*—速度更新方程的第一部分代表了粒子的惯性影响，考虑到粒子（如鱼群中的鱼或鸟群中的鸟）不能突然改变方向。您将在后面看到，这个惯性成分至关重要，因为它使算法更具适应性，并有助于在探索和利用之间保持平衡。'
- en: '*Cognitive component*—The second part of the equation, referred to as the cognitive
    component, represents the particle’s attraction toward its personal best position,
    or individual proximity (*i*-proximity). This component reflects the degree of
    trust a particle places in its own past experiences, without considering the experiences
    of its neighbors or the swarm as a whole. The cognitive component encourages particles
    to explore the areas around their personal best positions, allowing them to fine-tune
    their search in promising regions.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*认知成分*—方程的第二部分被称为认知成分，它代表了粒子对其个人最佳位置的吸引力，或个体邻近性（*i*-邻近性）。这个成分反映了粒子对其自身过去经验的信任程度，不考虑其邻居或整个群体的经验。认知成分鼓励粒子探索其个人最佳位置周围的区域，使他们能够在有希望的区域内微调搜索。'
- en: '*Social component*—The third part of the velocity update equation is the social
    component, which represents the particle’s attraction to the swarm’s collective
    knowledge or group proximity (*g*-proximity). This component takes into account
    the experiences of neighboring particles and the swarm as a whole, guiding the
    particles toward the global best position found so far. The social component fosters
    collaboration among particles, helping them converge toward an optimal solution
    more effectively.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*社会成分*—速度更新方程的第三部分是社会成分，它代表了粒子对群体集体知识或群体邻近性的吸引力（*g*-邻近性）。这个成分考虑了邻近粒子和整个群体的经验，引导粒子向迄今为止找到的全局最佳位置移动。社会成分促进了粒子之间的协作，帮助他们更有效地收敛到最佳解。'
- en: '![](../Images/CH09_F03_Khamis.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F03_Khamis.png)'
- en: Figure 9.3 Visualizing the motion equation for a particle in the swarm
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 展示了群体中粒子的运动方程
- en: To better understand the meaning of each component, imagine a group of friends
    visiting a large amusement park for the first time. Their goal is to visit the
    most thrilling rides in the park as efficiently as possible. The friends can be
    thought of as particles in the PSO algorithm, with each person’s enjoyment of
    the rides serving as the objective function to optimize. Each person has a preferred
    way of exploring the available rides, like walking through certain parts of the
    park or trying specific rides like roller coasters or water slides. This is similar
    to the inertia component in PSO, where particles maintain their current velocity
    and direction, ensuring they don’t change their exploration pattern too abruptly.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解每个成分的含义，想象一群朋友第一次参观一个大型游乐园。他们的目标是尽可能高效地参观公园中最刺激的游乐设施。这些朋友可以被看作是PSO算法中的粒子，每个人对游乐设施的享受作为要优化的目标函数。每个人都有自己探索可用游乐设施的首选方式，比如穿过公园的某些部分或尝试特定的游乐设施，如过山车或水上滑梯。这类似于PSO中的惯性成分，其中粒子保持其当前的速度和方向，确保他们不会过于突然地改变探索模式。
- en: Each friend relies on their own personal experiences to find the most thrilling
    rides. For instance, one friend might have had a great time on a roller coaster
    earlier in the day. They’re more likely to return to their favorite one or want
    to find more similar rides, knowing it was a good choice. They trust their judgment
    and focus on exploring the areas around the roller coaster, seeking out rides
    that they think they’ll enjoy based on their personal experience. This is the
    cognitive component, where particles in PSO are attracted to their personal best
    positions, following their past experiences and individual preferences.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个朋友都依靠自己的个人经验来寻找最刺激的游乐设施。例如，一个朋友可能在当天早些时候在过山车上玩得很开心。他们更有可能回到他们最喜欢的游乐设施，或者想要找到更多类似的游乐设施，因为他们知道这是一个好选择。他们信任自己的判断，并专注于探索过山车周围的区域，寻找他们认为会喜欢的游乐设施，基于他们的个人经验。这就是认知成分，在PSO中，粒子被吸引到它们的个人最佳位置，遵循它们过去的经验和个人偏好。
- en: The friends then collaborate to find the most thrilling ride based on their
    shared experiences. Imagine one of the friends has just ridden the most exciting
    roller coaster and can’t wait to tell the others about it. As they share their
    excitement, the group collectively becomes more attracted to that ride, influencing
    their individual choices. This is the social component, where particles in PSO
    are influenced by the global best position or the collective knowledge of the
    swarm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，朋友们根据他们的共同经历合作寻找最刺激的游乐设施。想象一下，其中一位朋友刚刚乘坐了最刺激的过山车，迫不及待地想告诉其他人。当他们分享他们的兴奋时，整个群体对那个游乐设施的兴趣会集体增加，影响他们的个人选择。这是社会成分，其中PSO中的粒子受到全局最佳位置或群体集体知识的
    影响。
- en: The following subsections dive into more detail about the different PSO parameters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的子节将更详细地介绍不同的PSO参数。
- en: 9.2.2 Fitness update
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 适应度更新
- en: 'After moving, each particle updates its own personal best using the following
    equation, assuming a minimization problem:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 移动后，每个粒子使用以下方程更新其个人最佳值，假设是一个最小化问题：
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F03_Khamis-EQ03.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F03_Khamis-EQ03.png)'
- en: '| 9.3 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 9.3 |'
- en: 'After that, each neighborhood updates its best as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，每个邻域按照以下方式更新其最佳值：
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F03_Khamis-EQ04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F03_Khamis-EQ04.png)'
- en: '| 9.4 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 9.4 |'
- en: The neighborhood best (*nbest*) is the same as the global best (*gbest*) if
    the neighborhood is the whole swarm.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果邻域是整个群体，则邻域最佳 (*nbest*) 与全局最佳 (*gbest*) 相同。
- en: 'PSO has two main variants based on how particles’ positions and velocities
    are updated—synchronous and asynchronous PSO:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: PSO根据粒子位置和速度的更新方式主要有两种变体——同步和异步PSO：
- en: '*Synchronous PSO (S-PSO)*—All particles in the swarm update their positions
    and velocities simultaneously in a global manner. The local and global best are
    then updated. This synchronous approach ensures that all particles have access
    to the same global best position when updating their velocities and positions,
    promoting global exploration.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同步PSO (S-PSO)*——群体中的所有粒子以全局方式同时更新其位置和速度。然后更新局部和全局最佳值。这种同步方法确保了在更新速度和位置时，所有粒子都能访问相同的全局最佳位置，从而促进全局探索。'
- en: '*Asynchronous PSO (A-PSO)*—Particles are updated based on the current state
    of the swarm. This asynchronous approach allows the particles to update their
    positions and velocities based on the most recent information available.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异步PSO (A-PSO)*——粒子根据群体的当前状态进行更新。这种异步方法允许粒子根据最新的可用信息更新其位置和速度。'
- en: Figure 9.4 shows the difference between S-PSO and A-PSO. As you’ll notice, in
    A-PSO, the neighborhood best update is moved into the particle’s update loop.
    This allows particles to evaluate their fitness and update their positions and
    velocities independently and asynchronously.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4显示了S-PSO和A-PSO之间的差异。你会注意到，在A-PSO中，邻域最佳更新被移动到粒子的更新循环中。这允许粒子独立和异步地评估其适应度并更新其位置和速度。
- en: '![](../Images/CH09_F04_Khamis.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F04_Khamis.png)'
- en: Figure 9.4 Synchronous and asynchronous PSO
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 同步和异步PSO
- en: Although both synchronous and asynchronous PSO strategies can be employed to
    handle optimization problems, the asynchronous version is generally more effective,
    as it allows particles to take advantage of the most recent neighbor information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管同步和异步PSO策略都可以用于处理优化问题，但异步版本通常更有效，因为它允许粒子利用最新的邻域信息。
- en: 9.2.3 Initialization
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 初始化
- en: 'PSO initialization includes initializing the particle position, velocity, and
    personal best, and initializing the algorithm’s parameters:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PSO初始化包括初始化粒子位置、速度和个人最佳值，以及初始化算法的参数：
- en: '*Particle position initialization*—Particle positions represent candidate solutions
    to the problem, and they can be sampled using different sampling methods, as explained
    in section 7.1\. For example, the initial positions of the particles can be randomly
    assigned within the defined feasible search space.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*粒子位置初始化*——粒子位置代表问题的候选解，它们可以使用不同的采样方法进行采样，如第7.1节所述。例如，粒子的初始位置可以在定义的可行搜索空间内随机分配。'
- en: '*Particle velocity initialization*—The velocities of the particles can be set
    to zero or small values initially. Initializing them with small velocities ensures
    that the particles’ updates are gradual, preventing them from moving too far away
    from their starting positions. In contrast, large initial velocities may lead
    to significant updates, which can potentially cause divergence and hinder the
    convergence of the algorithm.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*粒子速度初始化*——粒子的速度可以最初设置为零或小值。用小速度初始化它们确保粒子的更新是渐进的，防止它们远离起始位置。相比之下，大的初始速度可能导致显著的更新，这可能会引起发散并阻碍算法的收敛。'
- en: '*Personal best position initialization*—The personal best position of each
    particle, which represents the best solution found by the particle so far, should
    be initialized to its initial position. This allows the particles to begin their
    search with their starting points as a reference and to update their personal
    bests as they discover better solutions.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*个人最佳位置初始化*——每个粒子的个人最佳位置，代表粒子迄今为止找到的最佳解，应初始化为其初始位置。这允许粒子以它们的起始点作为参考开始搜索，并在发现更好的解决方案时更新它们的个人最佳位置。'
- en: 'As shown in equation 9.1, PSO has three primary parameters that play a critical
    role in controlling the search algorithm’s behavior: the inertia weight (*ω*)
    and the acceleration coefficients (*c*1, *c*2). These parameters influence the
    balance between exploration and exploitation within the optimization process:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程9.1所示，PSO有三个主要参数，这些参数在控制搜索算法行为方面起着关键作用：惯性权重（*ω*）和加速度系数（*c*1，*c*2）。这些参数影响优化过程中探索与利用之间的平衡：
- en: '*Inertia weight*—Large values of *ω* encourage exploration, while small values
    promote exploitation, allowing the cognitive and social components to exert greater
    control. A widely adopted value for *ω* is 0.792.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*惯性权重*——大的*ω*值鼓励探索，而小的值促进利用，允许认知和社会组件发挥更大的控制作用。*ω*的一个广泛采用值是0.792。'
- en: '*Acceleration coefficients*—Setting *c*1 to 0 reduces the PSO algorithm to
    a *social-only* or *selfless PSO* model. In this case, particles are solely attracted
    to the group best and ignore their personal bests. This leads to an emphasis on
    global exploration based on the swarm’s collective knowledge. Setting *c*2 to
    0 results in a *cognition-only model*, where particles act as independent hill
    climbers, relying only on their personal bests. In this scenario, the particles
    do not consider the experiences of other swarm members, focusing on local exploitation
    based on their individual experiences. In many applications, *c*1 and *c*2 are
    set to 1.49\. Although there is no theoretical justification for this specific
    value, it has been empirically found to work well in various optimization problems.
    Generally, the sum of *c*1 and *c*2 should be less than or equal to 4 to maintain
    the algorithm’s stability and convergence properties.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加速度系数*——将*c*1设置为0将PSO算法简化为*仅社交*或*无私的PSO*模型。在这种情况下，粒子仅被吸引到群体最佳位置，而忽略它们个人的最佳位置。这导致基于群体集体知识的全局探索得到强调。将*c*2设置为0将导致*仅认知*模型，其中粒子作为独立的爬山者行动，仅依靠它们个人的最佳位置。在这种情况下，粒子不考虑其他群体成员的经验，专注于基于它们个人经验的局部利用。在许多应用中，*c*1和*c*2被设置为1.49。尽管没有理论上的依据来支持这个特定值，但经验上发现它在各种优化问题中表现良好。一般来说，*c*1和*c*2的和应小于或等于4，以保持算法的稳定性和收敛特性。'
- en: 'Other parameters to consider include swarm size and neighborhood size. There
    is no one-size-fits-all solution, as the optimal values depend on the specific
    problem being solved. However, some best practices and guidelines can help inform
    your choices:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的其他参数包括群体大小和邻域大小。没有一种适合所有情况的解决方案，因为最佳值取决于要解决的问题的具体情况。然而，一些最佳实践和指南可以帮助你做出选择：
- en: '*Swarm size*—A large swarm size can promote global exploration and prevent
    premature convergence, but at the cost of increased computational effort. A small
    swarm size can lead to faster convergence and reduced computational effort but
    may increase the risk of premature convergence. For many problems, a swarm size
    between 20 and 100 particles has been found to yield good results. It is advisable
    to conduct experiments with different swarm sizes to determine the best trade-off
    between exploration, exploitation, and computational complexity for the problem
    at hand.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*群体大小*—较大的群体大小可以促进全局探索并防止过早收敛，但代价是增加了计算工作量。较小的群体大小可以导致更快收敛和减少计算工作量，但可能会增加过早收敛的风险。对于许多问题，20到100个粒子的群体大小已被发现能产生良好的结果。建议进行不同群体大小的实验，以确定针对特定问题的探索、利用和计算复杂度之间的最佳权衡。'
- en: '*Neighborhood size*—A large neighborhood size can encourage global exploration
    and information sharing among particles but may reduce the ability to exploit
    local optima. A small neighborhood size can promote local exploitation and convergence
    speed but may limit global exploration. You can use different neighborhood structures,
    as you’ll see in the next subsection.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*邻域大小*—较大的邻域大小可以鼓励全局探索和粒子之间的信息共享，但可能会降低利用局部最优的能力。较小的邻域大小可以促进局部利用和收敛速度，但可能会限制全局探索。你可以使用不同的邻域结构，如下一小节所示。'
- en: Generally speaking, selecting the best algorithm parameters requires experimentation
    and fine-tuning based on the specific problem you are trying to solve. It is often
    beneficial to perform a sensitivity analysis or use a parameter-tuning technique
    to find the optimal parameter values for your problem. We’ll look at this in more
    detail in section 9.5.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，选择最佳算法参数需要根据你试图解决的问题进行实验和微调。进行敏感性分析或使用参数调整技术以找到问题的最佳参数值通常是很有益的。我们将在9.5节中更详细地探讨这一点。
- en: 9.2.4 Neighborhoods
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 邻域
- en: In the PSO algorithm, particles within a specific vicinity engage in mutual
    communication by sharing details about each other’s success within that local
    region. Subsequently, all particles gravitate toward a position that is considered
    to be an improvement based on a key performance indicator. The efficacy of the
    PSO algorithm is heavily reliant on the social network structure it employs. Choosing
    an appropriate neighborhood topology plays a crucial role in ensuring the algorithm’s
    convergence and in preventing it from becoming trapped in local minima.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在PSO算法中，特定邻域内的粒子通过共享该局部区域内彼此的成功细节进行相互通信。随后，所有粒子都会向一个被认为基于关键性能指标有所改进的位置聚集。PSO算法的有效性高度依赖于所采用的社会网络结构。选择合适的邻域拓扑在确保算法收敛和防止其陷入局部最优中起着至关重要的作用。
- en: 'Some of the prevalent neighborhood topologies utilized in PSO include the star
    social structure, the ring topology, the Von Neumann model, and the wheel topology:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在PSO中使用的常见邻域拓扑包括星型社交结构、环形拓扑、冯·诺伊曼模型和轮形拓扑：
- en: '*The star social structure, also known as the global best (gbest) PSO*—This
    is a neighborhood topology where all particles are connected, as shown in figure
    9.5a. This structure allows access to global information within the swarm, with
    the result that each particle is drawn toward the optimal solution discovered
    by the entire swarm. The *gbest* PSO has been demonstrated to converge more rapidly
    than other network structures. However, it is more prone to becoming ensnared
    in local minima without fully exploring the search space. This topology excels
    when applied to unimodal problems, as it allows for efficient and effective optimization
    in such cases.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*星型社交结构，也称为全局最优（gbest）PSO*—这是一个所有粒子都相互连接的邻域拓扑，如图9.5a所示。这种结构允许群体内访问全局信息，结果是每个粒子都会被整个群体发现的最佳解所吸引。gbest
    PSO已被证明比其他网络结构收敛得更快。然而，它更容易在没有完全探索搜索空间的情况下陷入局部最优。当应用于单峰问题时，这种拓扑特别出色，因为它允许在这种情况下进行高效有效的优化。'
- en: '*Ring topology, also known as the local best (lbest) PSO*—Following this topology,
    a particle interacts exclusively with its immediately adjacent neighbors (figure
    9.5b). Each particle endeavors to emulate its most successful neighbor by gravitating
    toward the optimal solution discovered within the local vicinity. Although convergence
    occurs at a slower pace than with the star structure, the ring topology explores
    a more extensive portion of the search space. This topology is recommended for
    multimodal problems.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*环形拓扑，也称为局部最优（lbest）PSO*—遵循此拓扑，粒子仅与其直接相邻的邻居相互作用（如图9.5b所示）。每个粒子都试图通过向局部范围内发现的最佳解靠近来模仿其最成功的邻居。尽管收敛速度比星型结构慢，但环形拓扑探索了更广泛的搜索空间。这种拓扑建议用于多模态问题。'
- en: '*Von Neumann model*—In this topology, particles are arranged in a grid-like
    structure or square topology where each particle is connected with four other
    particles (the neighbors above, below, and to the right and left), as shown in
    figure 9.5c.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*冯·诺伊曼模型*—在这个拓扑中，粒子以网格状结构或正方形拓扑排列，每个粒子与四个其他粒子（上方、下方、右侧和左侧的邻居）相连，如图9.5c所示。'
- en: '*Wheel topology*—In this topology, the particles are isolated from each other,
    and one particle is randomly selected as the focal point or hub for all information
    flow, as illustrated in figure 9.5d.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轮形拓扑*—在这个拓扑中，粒子彼此隔离，随机选择一个粒子作为所有信息流的焦点或中心，如图9.5d所示。'
- en: The choice of neighborhood topology depends on the problem’s characteristics
    and the desired balance between exploration and exploitation. Experiment with
    different topologies to find the best fit for your problem.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域拓扑的选择取决于问题的特征以及探索和利用之间所需达到的平衡。尝试不同的拓扑结构以找到最适合您问题的最佳匹配。
- en: '![](../Images/CH09_F05_Khamis.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F05_Khamis.png)'
- en: 'Figure 9.5 PSO neighborhood topologies: a) the star social structure, b) the
    ring topology, c) the Von Neumann model, and d) the wheel topology'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 PSO邻域拓扑：a) 星型社会结构，b) 环形拓扑，c) 冯·诺伊曼模型，和d) 轮形拓扑
- en: 'Let’s now look at how to solve a continuous optimization problem using PSO.
    The Michalewicz function is a nonconvex mathematical function commonly used as
    a test problem for optimization algorithms. This function is given by the following
    formula:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何使用PSO解决连续优化问题。Michalewicz函数是一种非凸数学函数，通常用作优化算法的测试问题。该函数由以下公式给出：
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F05_Khamis-EQ05.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F05_Khamis-EQ05.png)'
- en: '| 9.5 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 9.5 |'
- en: where *d* is the dimension of the problem and *m* is a constant (usually *m*
    = 10). This function has *d* local minima. For *d* = 2, the minimum value is –1.8013
    at (2.20, 1.57).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*d*是问题的维度，*m*是一个常数（通常*m* = 10）。该函数有*d*个局部最小值。对于*d* = 2，最小值是-1.8013，在(2.20,
    1.57)处。
- en: Let’s start by defining the Michalewicz function as shown in listing 9.1\. This
    function can accept size-1 arrays with single or multiple rows. If the input position
    is a size-1 array with a single row, we reshape it to a 2D array with a single
    row using the `reshape()` function. A *size-1 array*, also known as a *singleton
    array*, is an array data structure that contains only one element. This reshaping
    enables uniform handling of both single-row size-1 arrays and arrays with multiple
    rows. This is evident in the implementation of the PSO solver, where the function
    addresses solving one solution at a time. Additionally, the function seamlessly
    manages arrays with multiple rows, a scenario encountered when simultaneously
    evaluating multiple solutions. This aspect will be further elaborated upon later
    in the context of pymoo and PySwarms solvers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义Michalewicz函数开始，如列表9.1所示。该函数可以接受大小为1的数组，具有单行或多行。如果输入位置是大小为1的单行数组，我们使用`reshape()`函数将其重塑为单行的二维数组。*大小为1的数组*，也称为*单元素数组*，是一种只包含一个元素的数据结构。这种重塑使得可以统一处理单行大小为1的数组和多行数组。这在PSO求解器的实现中很明显，其中该函数一次解决一个解。此外，该函数无缝地管理多行数组，这在同时评估多个解时遇到。这一方面将在pymoo和PySwarms求解器的上下文中进一步阐述。
- en: Listing 9.1 Solving the Michalewicz function using PSO
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 使用PSO求解Michalewicz函数
- en: '[PRE1]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Reshape to a 2D array with a single row if the position is a size-1 array.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ① 如果位置是大小为1的数组，则将其重塑为单行的二维数组。
- en: ② The Michalewicz formula
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ② Michalewicz公式
- en: 'Let’s now create a PSO solver from scratch. As a continuation of listing 9.1,
    we’ll start by defining a particle class with position, velocity, and personal
    best value as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在从头开始创建一个PSO求解器。作为9.1列表的延续，我们首先定义一个具有位置、速度和个人最佳值的粒子类，如下所示：
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The fitness function to be minimized is the Michalewicz function in this example:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要最小化的适应度函数是本例中的Michalewicz函数：
- en: '[PRE3]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now define the velocity update function following equation 9.1\. The
    function takes three arguments—`particle`, which is an object representing the
    current particle; `gbest_position`, which is the global best position found by
    the swarm so far; and `options`, which is a dictionary containing the algorithm
    parameters (specifically, the inertia weight `w` and the cognitive and social
    acceleration coefficients `c1` and `c2`):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义速度更新函数，根据方程9.1。该函数接受三个参数——`particle`，它是一个表示当前粒子的对象；`gbest_position`，它是迄今为止群体找到的全局最佳位置；以及`options`，它是一个包含算法参数的字典（特别是惯性权重`w`和认知及社会加速度系数`c1`和`c2`）：
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The function computes the three components of the new velocity: the inertia
    component, the cognitive component, and the social component, as per equation
    9.1\. It returns the updated velocity as the sum of the three components.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 函数根据方程9.1计算新速度的三个分量：惯性分量、认知分量和社会分量。它返回三个分量的总和作为更新后的速度。
- en: 'We can now define the PSO solver function. This function takes four parameters
    as inputs—`swarm_size`, which is the size of the particle swarm; `iterations`,
    which is the maximum number of iterations to run the algorithm for; `bounds`,
    which is a list of tuples defining the lower and upper bounds of the search space
    for each dimension of the input vector; and `options`, which is a dictionary containing
    the algorithm parameters (such as the inertia weight and cognitive and social
    acceleration coefficients):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义PSO求解器函数。此函数接受四个参数作为输入——`swarm_size`，它是粒子群的大小；`iterations`，它是运行算法的最大迭代次数；`bounds`，它是一个元组列表，定义了输入向量每个维度的搜索空间的上下边界；以及`options`，它是一个包含算法参数的字典（例如惯性权重和认知及社会加速度系数）：
- en: '[PRE5]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Initialize a random swarm.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化一个随机群体。
- en: ② Initialize the global best.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化全局最佳。
- en: ③ Update the velocity and position.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 更新速度和位置。
- en: ④ Apply the bounds.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 应用边界。
- en: ⑤ Update the personal best (pbest).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 更新个人最佳（pbest）。
- en: ⑥ Update the global best (gbest).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 更新全局最佳（gbest）。
- en: ⑦ Update the position.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 更新位置。
- en: ⑧ Return the global best position and corresponding value.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 返回全局最佳位置和相应的值。
- en: The function first initializes the particle swarm by randomly generating the
    initial positions and velocities for each particle within the bounds defined by
    `bounds`. It then evaluates the fitness function for each particle and updates
    its personal best position and value accordingly. The function then enters a loop,
    where it updates the velocity and position of each particle using the `update_velocity`
    function, which takes the global best position found so far as input. The function
    also applies bounds to the particle position and updates its personal best position
    and value. The function then updates the global best position and value based
    on the star topology. Other topologies, such as ring, Von Neumann, and wheel,
    can be found in the complete code of listing 9.1, available in the book’s GitHub
    repository. Finally, the function returns the global best position and value found
    by the algorithm.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先通过在`bounds`定义的边界内随机生成每个粒子的初始位置和速度来初始化粒子群。然后，它评估每个粒子的适应度函数，并相应地更新其个人最佳位置和值。然后，函数进入一个循环，在该循环中使用`update_velocity`函数更新每个粒子的速度和位置，该函数接受迄今为止找到的全局最佳位置作为输入。该函数还应用边界到粒子位置，并更新其个人最佳位置和值。然后，根据星型拓扑更新全局最佳位置和值。其他拓扑，如环形、冯·诺伊曼和轮形，可以在9.1列表的完整代码中找到，该代码可在本书的GitHub存储库中找到。最后，函数返回算法找到的全局最佳位置和值。
- en: 'We can now use this PSO solver to minimize the Michalewicz function after we
    set up the problem and algorithm parameters as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置问题和算法参数后，我们现在可以使用此PSO求解器最小化Michalewicz函数，如下所示：
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① PSO parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ① PSO参数
- en: ② Dimension and domain of the Michalewicz function for each variable
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ② Michalewicz函数的维度和域
- en: ③ Use the implemented PSO solver to solve the problem.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用已实现的PSO求解器解决问题。
- en: 'You can print the optimal solution and minimum value of the function after
    running PSO:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在运行PSO后打印出最优解和函数的最小值：
- en: '[PRE7]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output would be as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Compared to genetic algorithms, there are fewer Python libraries available
    for PSO. Pymoo provides a PSO implementation for continuous problems. As a continuation
    of listing 9.1, pymoo PSO can be used to solve the same problem as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与遗传算法相比，Python 库中可用的 PSO 更少。Pymoo 为连续问题提供了一个 PSO 实现。作为列表 9.1 的延续，pymoo PSO 可以如下用于解决相同的问题：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Define the problem.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义问题。
- en: ② The 2D Michalewicz function
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ② 2D Michalewicz 函数
- en: ③ Set the lower and upper bounds.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置下限和上限。
- en: ④ Evaluate the objective function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 评估目标函数。
- en: ⑤ Create a problem instance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建问题实例。
- en: ⑥ Define the solver with the parameters.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用参数定义求解器。
- en: ⑦ Apply PSO to solve the problem.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 将 PSO 应用于解决问题。
- en: ⑧ Print the optimal solution and minimum value of the function after running
    PSO.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 运行 PSO 后打印最优解和函数的最小值。
- en: 'This code produces the following output:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码产生以下输出：
- en: '[PRE10]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'PySwarms is another open source optimization library for Python that implements
    different variants of PSO. PySwarms can be used as follows to handle the problem:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: PySwarms 是另一个用于 Python 的开源优化库，它实现了 PSO 的不同变体。PySwarms 可以如下使用来处理问题：
- en: '[PRE11]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Import the PSO solver from pyswarms.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从 pyswarms 导入 PSO 求解器。
- en: ② Dimension of the Michalewicz function
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ② Michalewicz 函数的维度
- en: ③ Create bounds for the search space.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 为搜索空间创建边界。
- en: ④ Set up the optimizer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 设置优化器。
- en: ⑤ Create an instance of the optimizer.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建优化器的实例。
- en: ⑥ Optimize the Michalewicz function
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 优化 Michalewicz 函数
- en: ⑦ Print the optimal solution and minimum value of the function after running
    PSO.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 运行 PSO 后打印最优解和函数的最小值。
- en: 'This code produces the following output:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码产生以下输出：
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The pyswarms.single package implements various techniques in continuous single-
    objective optimization. From this module, we used the global-best PSO (*gbest*
    PSO) algorithm in the previous code. You can experiment by replacing this solver
    with local-best.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: pyswarms.single 包实现了连续单目标优化中的各种技术。从该模块中，我们使用了之前代码中的全局最优 PSO（*gbest* PSO）算法。你可以通过将此求解器替换为局部最优来实验。
- en: Figure 9.6 shows the 3D landscape and 2D contours of the Michalewicz function,
    the optimal solution, and the solutions obtained by PSO, PSO Pymoo, and PSO PySwarms.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 显示了 Michalewicz 函数的 3D 地形和 2D 等高线，最优解以及 PSO、PSO Pymoo 和 PSO PySwarms 获得的解。
- en: '![](../Images/CH09_F06_Khamis.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F06_Khamis.png)'
- en: Figure 9.6 3D and 2D plots of the Michalewicz function. The three solutions
    are all very close to the optimal solution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 Michalewicz 函数的 3D 和 2D 图。三个解都非常接近最优解。
- en: The three versions of PSO provide comparable results. However, PSO PySwarms
    and PSO pymoo are more stable, as they provide more consistent results each time
    you run the code. The PySwarms library is more comprehensive than pymoo for PSO,
    as it provides implementations of different variants and topologies of PSO, including
    discrete PSO, which will be explained in the next two sections.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 三个版本的 PSO 提供了可比的结果。然而，PSO PySwarms 和 PSO pymoo 更稳定，因为它们每次运行代码时都提供更一致的结果。与 pymoo
    相比，PySwarms 库在 PSO 方面更全面，因为它提供了不同变体和拓扑结构的 PSO 实现，包括离散 PSO，这将在下一两节中解释。
- en: 9.3 Binary PSO
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 二进制 PSO
- en: PSO was originally developed for problems that involve continuous-valued variables.
    However, many real-world problems are discrete or combinatorial in nature, such
    as TSP, task allocation, scheduling, assignment problems, and feature selection,
    among others. These types of problems involve searching through a finite set of
    possible solutions, rather than searching through a continuous space. To handle
    these discrete problems, PSO variants have been developed, such as binary PSO
    and permutation-based PSO.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: PSO 最初是为涉及连续值变量的问题开发的。然而，许多现实世界的问题在本质上是非连续的或组合的，例如 TSP、任务分配、调度、分配问题和特征选择等。这类问题涉及在有限的可能解集中搜索，而不是在连续空间中搜索。为了处理这些离散问题，已经开发了
    PSO 的变体，如二进制 PSO 和基于排列的 PSO。
- en: In *binary PSO* (BPSO), each particle represents a position in the binary space,
    where each element is either 0 or 1\. The binary sequence is updated bit by bit
    based on its current value, the fitness-based value of that particular bit within
    the particle, and the best value of the same bit observed so far among its neighboring
    particles. This approach enables the search to be conducted in a binary space
    rather than a continuous space, which is well suited for problems where the variables
    are binary.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *二进制粒子群优化*（BPSO）中，每个粒子代表二进制空间中的一个位置，其中每个元素是 0 或 1。二进制序列根据其当前值、粒子中特定位的基于适应度的值以及迄今为止观察到的相邻粒子中相同位的最佳值逐位更新。这种方法使得搜索可以在二进制空间而不是连续空间中进行，这对于变量为二进制的实际问题非常适合。
- en: 'In BPSO, the velocity is defined in terms of the probability of the bit changing.
    To restrict the values of the velocity elements to the range of [0,1], the sigmoid
    function is used:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BPSO 中，速度是根据位变化的概率定义的。为了将速度元素的值限制在 [0,1] 范围内，使用了 S 形函数：
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F06_Khamis-EQ06.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F06_Khamis-EQ06.png)'
- en: '| 9.6 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 9.6 |'
- en: The position update equation then becomes
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 位置更新方程因此变为
- en: '|'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F06_Khamis-EQ07.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F06_Khamis-EQ07.png)'
- en: '| 9.7 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 9.7 |'
- en: where *r* is a randomly generated number in [0, 1]. Figure 9.7 shows the sigmoid
    function and the probability of the updated position to be 1\. For example, if
    *v* = 0.3, this means that the probability that the updated position will be 1
    is 30%, and the probability that it will be 0 is 70%.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *r* 是在 [0, 1] 范围内随机生成的数字。图 9.7 展示了 S 形函数和更新位置为 1 的概率。例如，如果 *v* = 0.3，这意味着更新位置为
    1 的概率是 30%，而为 0 的概率是 70%。
- en: '![](../Images/CH09_F07_Khamis.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F07_Khamis.png)'
- en: Figure 9.7 Position and velocity notations in binary PSO (BPSO). Each particle
    represents a position in the binary space. Velocity is defined in terms of the
    probability of the bit changing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 二进制粒子群优化（BPSO）中的位置和速度表示。每个粒子代表二进制空间中的一个位置。速度是根据位变化的概率定义的。
- en: 'As you’ll notice, the velocity components will remain as real-valued numbers
    using the original equation, but these values are then passed through the sigmoid
    function before updating the position vector. The following equations are the
    velocity update equations in BPSO:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所注意到的，速度分量将保持为使用原始方程中的实数值，但在更新位置向量之前，这些值将通过 S 形函数。以下方程是 BPSO 中的速度更新方程：
- en: '|'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F07_Khamis-EQ08.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F07_Khamis-EQ08.png)'
- en: '| 9.8 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 9.8 |'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F07_Khamis-EQ09.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F07_Khamis-EQ09.png)'
- en: '| 9.9 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 9.9 |'
- en: 'The positions are updated according to the following equation:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 位置更新根据以下方程进行：
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F07_Khamis-EQ10.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F07_Khamis-EQ10.png)'
- en: '| 9.10 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 9.10 |'
- en: where
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*ϕ*[1] and *ϕ*[2] represent different random numbers drawn from uniform distributions.
    Sometimes these parameters are chosen from a uniform distribution 0–2, such that
    the sum of their two limits is 4.0.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ϕ*[1] 和 *ϕ*[2] 代表从均匀分布中抽取的不同随机数。有时这些参数是从 0–2 的均匀分布中选择的，使得它们的两个极限之和为 4.0。'
- en: '*v^i^d[k]* [+1] is the probability that an individual *i* will choose 1 for
    the bit at the *d^(th)* site in the bit string.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*v^i^d[k]* [+1] 是个体 *i* 在二进制字符串的第 *d^(th)* 位选择 1 的概率。'
- en: '*x[k]^(id)* is the current state of string *i* at bit *d*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x[k]^(id)* 是字符串 *i* 在位 *d* 的当前状态。'
- en: '*v[k]^(id)* is a measure of the string’s current probability to choose 1.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*v[k]^(id)* 是字符串当前选择 1 的概率的度量。'
- en: '*pbest[k]^(id)* is the best state found so far for bit *d* of individual *i*
    (i.e., a 1 or a 0).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pbest[k]^(id)* 是个体 *i* 中位 *d* 的迄今为止找到的最佳状态（即，1 或 0）。'
- en: '*gbest[k]^d* is 1 or 0 depending on what the value of bit *d* is in the best
    neighbor to date.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gbest[k]^d* 取决于迄今为止最佳邻居中位 *d* 的值是 1 还是 0。'
- en: BPSO example
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: BPSO 示例
- en: 'To illustrate how BPSO works, suppose we have a population of five binary particles,
    where each particle consists of 6 bits. Let’s assume the particles are represented
    by the following binary strings: 101101, 110001, 011110, 100010, and 001011\.
    We want to update particle 4 (represented by the binary string 100010) at bit
    3 (which has a current value of 0). The current propensity (velocity) of this
    bit to be 1 is assumed to be 0.23\. Additionally, we assume that the best value
    of this particle found so far is 101110, while the best value found by the entire
    population is 101111\. Let’s also assume that ϕ[1] = 1.5 and ϕ[2] = 1.9. Using
    equations 9.8 and 9.9, we can get the updated velocity of bit 3 in particle 4
    as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明BPSO的工作原理，假设我们有一个由五个二进制粒子组成的种群，其中每个粒子由6位组成。让我们假设粒子由以下二进制字符串表示：101101，110001，011110，100010，和001011。我们想要更新粒子4（由二进制字符串100010表示）在位3（当前值为0）的值。假设这个位变为1的当前倾向（速度）是0.23。此外，我们假设这个粒子迄今为止找到的最佳值是101110，而整个种群找到的最佳值是101111。让我们还假设ϕ[1]
    = 1.5和ϕ[2] = 1.9。使用方程9.8和9.9，我们可以得到粒子4中位3的更新速度如下：
- en: 'Particle 4: 100010, *v[k]*^(43) = 0.23, *x[k]*^(43) = 0, *pbest[k]*^(43) =
    1, *gbest[k]*³ = 1, ϕ[1] = 1.5, ϕ[2] = 1.9'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 粒子4：100010，*v[k]*^(43) = 0.23，*x[k]*^(43) = 0，*pbest[k]*^(43) = 1，*gbest[k]*³
    = 1，ϕ[1] = 1.5，ϕ[2] = 1.9
- en: '*v[k]* [+ 1]^(43) = 0.23 + 1.5(1–0) + 1.9(1–0) = 3.63'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*v[k]* [+ 1]^(43) = 0.23 + 1.5(1–0) + 1.9(1–0) = 3.63'
- en: '*sig*(*v[k]* [+ 1]^(43)) = *sig*(3.63) = 1/(1 + *e*^(–3.63)) = 0.974'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*sig*(*v[k]* [+ 1]^(43)) = *sig*(3.63) = 1/(1 + *e*^(–3.63)) = 0.974'
- en: 'Generate a random number *r*^(43) = 0.6, and update the position using equation
    9.10 as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个随机数 *r*^(43) = 0.6，并使用方程9.10更新位置如下：
- en: '*x[k]* [+ 1]^(43) = 1 as *sig*(*v[k]* [+ 1]^(43)) > *r*^(43)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*x[k]* [+ 1]^(43) = 1 因为 *sig*(*v[k]* [+ 1]^(43)) > *r*^(43)'
- en: 'Updated particle 4: 100110'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 更新粒子4：100110
- en: For more information about BPSO, see Kennedy and Eberhart’s article “A discrete
    binary version of the particle swarm algorithm” [2].
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于BPSO的信息，请参阅Kennedy和Eberhart的文章“粒子群算法的离散二进制版本” [2]。
- en: 9.4 Permutation-based PSO
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 基于排列的PSO
- en: Numerous efforts have been undertaken to employ PSO in solving permutation problems.
    The challenge of adapting PSO to tackle these problems arises from the fact that
    the notions of velocity and direction are not inherently applicable to permutation
    problems. To overcome this obstacle, arithmetic operations like addition and multiplication
    need to be redefined.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 已经进行了许多努力来将PSO应用于解决排列问题。将PSO适应这些问题的挑战源于速度和方向的概念本身不适用于排列问题。为了克服这个障碍，需要重新定义像加法和乘法这样的算术运算。
- en: 'In M. Clerc’s 2004 article, “Discrete particle swarm optimization, illustrated
    by the traveling salesman problem” [3], PSO was applied to solve the TSP. The
    position of a particle was the solution to a problem (the permutation of cities).
    The velocity of a particle was defined as the set of swaps to be performed on
    a particle. As you have seen, the right side of equation 9.1 contains three arithmetic
    operations: multiplication, subtraction, and addition. These operations are redefined
    for the new search space as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在M. Clerc的2004年文章“离散粒子群优化，以旅行商问题为例” [3]中，PSO被应用于解决TSP问题。粒子的位置是问题的解（城市的排列）。粒子的速度被定义为对粒子执行的一组交换。正如你所看到的，方程9.1的右侧包含三个算术运算：乘法、减法和加法。这些运算在新的搜索空间中被重新定义为以下内容：
- en: '*Multiplication*—The velocity vector constrains a number of swaps between cities.
    Multiplying this vector by a constant *c*, results in another velocity vector
    with a different length, depending on the value of the constant. If *c* = 0, the
    length of the velocity vector (i.e., the included number of swaps) is set to 0\.
    This means that no swap will be performed. If *c* < 1, the velocity is truncated.
    If *c* > 1, the velocity is augmented as illustrated in figure 9.8\. Augmentation
    means adding a swap taken from the top of the current velocity vector to the end
    of the new velocity vector.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*乘法*—速度向量限制城市之间的交换次数。将这个向量乘以一个常数 *c*，结果得到另一个具有不同长度的速度向量，这取决于常数的值。如果 *c* = 0，则速度向量的长度（即包含的交换次数）设置为0。这意味着不会执行任何交换。如果
    *c* < 1，则速度被截断。如果 *c* > 1，则速度如图9.8所示增加。增加意味着将来自当前速度向量顶部的交换添加到新速度向量的末尾。'
- en: '![](../Images/CH09_F08_Khamis.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F08_Khamis.png)'
- en: Figure 9.8 Redefined multiplication for permutation-based PSO
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 基于排列的PSO重新定义的乘法
- en: '*Subtraction*—Subtracting two positions should produce a velocity. This operation
    produces the sequence of swaps that could transform one position to the other.
    For example, let’s consider an 8-city TSP. A candidate solution for this TSP is
    represented by a permutation such as [2, 4, 6, 1, 5, 3, 8, 7]. Figure 9.9 shows
    how a new velocity vector is produced by subtracting two positions.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*减法*—两个位置相减应产生一个速度。此操作产生将一个位置转换为另一个位置的交换序列。例如，让我们考虑一个8城市TSP问题。该TSP的一个候选解由排列表示，例如[2,
    4, 6, 1, 5, 3, 8, 7]。图9.9展示了如何通过减去两个位置来生成一个新的速度向量。'
- en: '![](../Images/CH09_F09_Khamis.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F09_Khamis.png)'
- en: Figure 9.9 Redefined subtraction operation for permutation-based PSO
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9基于排列的PSO重新定义的减法操作
- en: '*Addition*—The operation is performed by applying the sequence of swaps defined
    by the velocity to the position vector. Figure 9.10 shows how a new position (i.e.,
    a new candidate solution) is generated by adding the velocity swap vector to the
    current position.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加法*—该操作通过应用由速度定义的交换序列到位置向量来执行。图9.10展示了如何通过将速度交换向量加到当前位置来生成一个新的位置（即一个新的候选解）。'
- en: '![](../Images/CH09_F10_Khamis.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F10_Khamis.png)'
- en: Figure 9.10 Redefined addition operation for permutation-based PSO
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10基于排列的PSO重新定义的加法操作
- en: These redefined arithmetic operations allow us to update the velocity and position
    of PSO particles.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这些重新定义的算术运算使我们能够更新PSO粒子的速度和位置。
- en: 9.5 Adaptive PSO
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 自适应PSO
- en: The inertia, cognitive, and social components are the primary PSO parameters
    that can be used to achieve an equilibrium between exploration and exploitation
    during the optimization process. These three factors significantly influence the
    behavior of the algorithm, as discussed in the following subsections.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性、认知和社会成分是PSO的主要参数，可以在优化过程中实现探索和开发的平衡。这三个因素显著影响算法的行为，如以下小节所述。
- en: 9.5.1 Inertia weight
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 惯性权重
- en: The inertia parameter represents the tendency of a particle to maintain its
    current trajectory. By adjusting the inertia value, the algorithm can balance
    its focus on searching the solution space broadly (exploration) or homing in on
    the best solutions found thus far (exploitation). Large values of *ω* promote
    exploration, and small values promote exploitation, as illustrated in figure 9.11\.
    Excessively small values may hinder the swarm’s exploration capabilities. As the
    value of *ω* decreases, the influence of the cognitive and social components on
    position updates becomes more dominant.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性参数表示粒子保持其当前轨迹的趋势。通过调整惯性值，算法可以在广泛搜索解空间（探索）和专注于迄今为止找到的最佳解（开发）之间取得平衡。较大的*ω*值促进探索，而较小的值促进开发，如图9.11所示。过小的值可能会阻碍群体的探索能力。随着*ω*值的减小，认知和社会成分对位置更新的影响变得更加突出。
- en: '![](../Images/CH09_F11_Khamis.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F11_Khamis.png)'
- en: Figure 9.11 Effect of PSO parameters on the search behavior. Large inertia promotes
    exploration, and small values promote exploitation. *c*1 > *c*2 results in excessive
    wandering of individuals through the search space. In contrast, *c*2 > *c*1 may
    lead particles to rush prematurely toward a local optimum.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 PSO参数对搜索行为的影响。较大的惯性促进探索，而较小的值促进开发。*c*1 > *c*2会导致个体在搜索空间中过度徘徊。相比之下，*c*2
    > *c*1可能会导致粒子过早地向局部最优解冲去。
- en: When *ω* > 1, particle velocities tend to escalate over time, accelerating toward
    the maximum velocity (provided that velocity clamping is utilized), ultimately
    causing the swarm to diverge. In this scenario, particles struggle to alter their
    direction to return to promising regions. On the other hand, when *ω* < 1, particles
    may gradually decelerate until their velocities approach 0, depending on the acceleration
    coefficients’ values. Velocity clamping can be considered by setting a maximum
    (and minimum) limit for the velocity. If the calculated velocity for a particle
    exceeds this limit, it is set to the maximum (or minimum) value. This prevents
    particles from wandering too far off in the problem space or getting stuck in
    a specific region in the search space.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *ω* > 1 时，粒子速度倾向于随时间增加，加速向最大速度（假设使用了速度钳位），最终导致群体发散。在这种情况下，粒子难以改变方向返回有希望的领域。另一方面，当
    *ω* < 1 时，粒子可能会逐渐减速，直到它们的速度接近 0，这取决于加速度系数的值。可以通过设置速度的最大（和最小）限制来考虑速度钳位。如果粒子的计算速度超过此限制，则将其设置为最大（或最小）值。这防止粒子在问题空间中走得太远或在搜索空间的特定区域卡住。
- en: 'The following methods can be used to update the inertia weight:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下方法来更新惯性权重：
- en: '*Random selection (RS)*—This involves selecting a different inertia weight
    in each iteration. The weight can be chosen from a distribution with a mean and
    standard deviation of your choice, but it’s important to ensure that the swarm
    still converges despite the randomness. The following formula can be used:'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机选择 (RS)*—这涉及到在每次迭代中选择不同的惯性权重。权重可以从具有您选择的平均值和标准差的分布中选择，但重要的是要确保尽管存在随机性，群体仍然收敛。以下公式可以用来：'
- en: '|'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F11_Khamis-EQ11.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F11_Khamis-EQ11.png)'
- en: '| 9.11 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 9.11 |'
- en: where *rand*(.) is a uniformly distributed random number within the range [0,1].
    Therefore, the mean value of the inertia weight is 0.75.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *rand*(.) 是 [0,1] 范围内均匀分布的随机数。因此，惯性权重的平均值是 0.75。
- en: '*Linear time varying (LTV)*—This involves gradually decreasing the value of
    *𝜔* from a starting high value of *𝜔[max]* to a final low value of *𝜔[min]* following
    this equation:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性时变 (LTV)*—这涉及到逐渐减小 *𝜔* 的值，从起始的高值 *𝜔[max]* 下降到最终的低值 *𝜔[min]*，遵循以下方程：'
- en: '|'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F11_Khamis-EQ12.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F11_Khamis-EQ12.png)'
- en: '| 9.12 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 9.12 |'
- en: where *𝑡[max]* is the number of iterations, *t* is the current iteration, and
    *𝜔[t]* is the value of the inertia weight in the *t*^(th) iteration. Typically,
    the convention is to set *𝜔[max]* and *𝜔[min]* to 0.9 and 0.4 respectively.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *𝑡[max]* 是迭代次数，*t* 是当前迭代，*𝜔[t]* 是第 *t* 次迭代的惯性权重值。通常，约定将 *𝜔[max]* 和 *𝜔[min]*
    设置为 0.9 和 0.4。
- en: '*Nonlinear time varying (NLTV)*—This approach also involves decreasing the
    inertia weight from an initial high value, but this decrement can be nonlinear,
    as shown in the following equation:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*非线性时变 (NLTV)*—这种方法也涉及到从初始高值减小惯性权重，但这种减小可以是非线性的，如下方程所示：'
- en: '|'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F11_Khamis-EQ13.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F11_Khamis-EQ13.png)'
- en: '| 9.13 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 9.13 |'
- en: where *𝜔[𝑡]*[=0] = 0.9 is the initial choice of *𝜔*. By allowing more time to
    fall off toward the lower end of the dynamic range, NLTV can enhance local search
    or exploitation.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *𝜔[𝑡]*[=0] = 0.9 是 *𝜔* 的初始选择。通过允许更多时间下降到动态范围的较低端，NLTV 可以增强局部搜索或开发。
- en: Figure 9.12 shows these three update methods.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 展示了这三种更新方法。
- en: '![](../Images/CH09_F12_Khamis.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F12_Khamis.png)'
- en: Figure 9.12 Different inertia weight update methods
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 不同惯性权重更新方法
- en: As you can see, in random selection, a different inertia weight is randomly
    selected in each iteration. The mean value of the inertia weight is 0.75\. LTV
    linearly decreases the inertia weight. In NLTV, the inertia weight decrement is
    more gradual than in LTV. In summary, the inertia weight plays a crucial role
    in the convergence speed and solution quality of the PSO algorithm. A high inertia
    weight promotes exploration, while a low inertia weight encourages exploitation.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在随机选择中，每次迭代都会随机选择不同的惯性权重。惯性权重的平均值是 0.75。LTV 线性减小惯性权重。在 NLTV 中，惯性权重的减小比
    LTV 更渐进。总之，惯性权重在 PSO 算法的收敛速度和解决方案质量中起着至关重要的作用。高惯性权重促进探索，而低惯性权重鼓励开发。
- en: 9.5.2 Cognitive and social components
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 认知和社会成分
- en: The cognitive component *c*1 is a parameter associated with a particle’s individual
    learning capability, where the particle is influenced by its own experiences.
    The social component *c*2 is a parameter linked to the collective learning capability
    of all particles within the swarm. It represents the degree to which a particle
    is influenced by the best solutions found by its neighbors. If *c*1 > *c*2, the
    algorithm will show exploratory behavior, and if *c*2 > *c*1, the algorithm will
    tend to exploit the local search space, as illustrated in figure 9.11\. Setting
    *c*1 = 0 reduces the velocity model to a social-only model or selfless model (the
    particles are all attracted to *nbest*). On the other hand, setting *c*2 = 0 reduces
    it to a cognition-only model (particles are independent, as in the case of the
    hill climbing algorithm).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 认知组件*c*1是与粒子个体学习能力相关的参数，其中粒子受其自身经验的影响。社会组件*c*2是与群体中所有粒子的集体学习能力相关的参数。它表示粒子受其邻居找到的最佳解决方案影响的程度。如果*c*1
    > *c*2，算法将表现出探索行为，如果*c*2 > *c*1，算法将倾向于利用局部搜索空间，如图9.11所示。将*c*1 = 0将速度模型简化为仅社会模型或无私模型（粒子都被吸引到*nbest*）。另一方面，将*c*2
    = 0将其简化为仅认知模型（粒子是独立的，如爬山算法的情况）。
- en: 'Typically, *c*1 and *c*2 are kept constant in PSO. Empirically, the sum of
    *c*1 and *c*2 should be less than or equal to 4, and any significant deviations
    from this may result in divergent behavior. In adaptive PSO, it is advisable to
    gradually decrease the value of *c*1 over time and concurrently increase the value
    of *c*2 using linear formulas [4], as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*c*1和*c*2在PSO中保持不变。经验上，*c*1和*c*2的和应小于或等于4，任何与此有显著差异的情况可能导致发散行为。在自适应PSO中，建议随着时间的推移逐渐降低*c*1的值，并使用线性公式[4]同时增加*c*2的值，如下所示：
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F12_Khamis-EQ14.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F12_Khamis-EQ14.png)'
- en: '| 9.14 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 9.14 |'
- en: '|'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F12_Khamis-EQ15.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F12_Khamis-EQ15.png)'
- en: '| 9.15 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 9.15 |'
- en: where *t* is the iteration index, *c*1*[max]* and *c*2*[max]* are the maximum
    cognitive and social parameters respectively, *c*1*[min]* and *c*2*[min]* are
    the minimum cognitive and social parameters respectively, and *t[max]* is the
    maximum iteration.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*t*是迭代索引，*c*1*[max]*和*c*2*[max]*分别是最大认知和社会参数，*c*1*[min]*和*c*2*[min]*分别是最小认知和社会参数，*t[max]*是最大迭代次数。
- en: Figure 9.13 shows the linearly changing *c*1 and *c*2. As you can see, we start
    with *c*1 > *c*2 to favor exploration. As the search progresses, *c*2 starts to
    be higher than *c*1 in order to favor exploitation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13显示了线性变化的*c*1和*c*2。如图所示，我们开始时*c*1 > *c*2，以利于探索。随着搜索的进行，*c*2开始高于*c*1，以利于利用。
- en: '![](../Images/CH09_F13_Khamis.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F13_Khamis.png)'
- en: Figure 9.13 Cognitive and social acceleration coefficient updates. c1>c2 results
    in more exploration, while c2>c1 may lead to more exploitation.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13认知和社会加速系数更新。c1>c2导致更多的探索，而c2>c1可能导致更多的利用。
- en: Let’s now see how we can use PSO to handle continuous and discrete optimization
    problems.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用PSO处理连续和离散优化问题。
- en: 9.6 Solving the traveling salesman problem
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6解决旅行商问题
- en: In the previous chapter, you saw how to solve the TSP for 20 major cities in
    the United States, starting from New York City, using a genetic algorithm. Let’s
    now solve the same problem using PSO, as shown in the next listing. We’ll start
    by defining the latitude and longitude for the twenty US cities and computing
    the inter-city distances between them.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您看到了如何使用遗传算法从纽约市开始解决美国20个主要城市的TSP问题。现在，让我们使用PSO解决相同的问题，如下所示。我们首先定义二十个美国城市的纬度和经度，并计算它们之间的城市距离。
- en: Listing 9.2 Solving TSP using PSO
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2使用PSO解决TSP
- en: '[PRE13]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Define the latitude and longitude for twenty major US cities.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ①为二十个主要美国城市定义纬度和经度。
- en: ② Create a haversine distance matrix based on the latitude and longitude coordinates.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ②根据纬度和经度坐标创建哈夫曼距离矩阵。
- en: ③ Convert the distance dictionary into a dataframe with distances as values
    and city names as headers.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ③将距离字典转换为具有距离值和城市名称作为标题的数据框。
- en: 'Next, we can count the number of cities and set up the integer bounds of the
    decision variables, which represent the order in which the cities are visited.
    The first function, `tsp_distance`, takes two arguments: `position` and `distance`.
    `position` is a 1D array that represents the order in which the cities are visited.
    `distance` is a 2D array that contains the distances between all pairs of cities.
    The function first defines `tour` as a permutation of the indices that represent
    the order of visiting the cities. It then calculates the total distance of the
    tour by summing the distances between adjacent cities as well as the distance
    between the last city in the tour and the starting city.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以计算城市的数量并设置决策变量的整数边界，这些变量代表访问城市的顺序。第一个函数`tsp_distance`接受两个参数：`position`和`distance`。`position`是一个一维数组，表示访问城市的顺序。`distance`是一个二维数组，包含所有城市对之间的距离。该函数首先将`tour`定义为表示访问城市顺序的索引排列。然后，通过计算相邻城市之间的距离以及路线中最后一个城市与起始城市之间的距离来计算路线的总距离。
- en: 'The second function, `tsp_cost`, takes two arguments: `x` and `distance`. `x`
    is a 2D array that contains the decision variables for the TSP problem, with each
    row representing a different particle in the swarm. `distance` is a 2D array that
    contains the distances between all pairs of cities. The function calculates the
    cost of each particle by calling the `tsp_distance` function on each row of `x`
    and returns a list of the costs:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数`tsp_cost`接受两个参数：`x`和`distance`。`x`是一个二维数组，包含TSP问题的决策变量，其中每一行代表群体中的不同粒子。`distance`是一个二维数组，包含所有城市对之间的距离。该函数通过在`x`的每一行上调用`tsp_distance`函数来计算每个粒子的成本，并返回一个成本列表：
- en: '[PRE14]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Define the TSP problem as a permutation optimization problem with integer
    bounds.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将旅行商问题（TSP）定义为具有整数边界的排列优化问题。
- en: ② Define the TSP distance function
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义TSP距离函数
- en: ③ Convert the permutation to a TSP tour.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将排列转换为TSP路线。
- en: ④ Compute the total distance of the tour from New York City as the first city,
    and add the distance from the last city back to New York City.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 从纽约市作为第一个城市开始计算路线的总距离，并添加最后一个城市返回纽约市的距离。
- en: ⑤ Compute and return the cost of each particle in the swarm.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算并返回每个粒子在群体中的成本。
- en: 'As a continuation of listing 9.2, the following code sets the parameters for
    the PSO optimizer. `options` is a dictionary that contains the values for the
    inertia weight (`w`), cognitive (`c1`) and social (`c2`) acceleration coefficients,
    number of neighbors to consider (`k`), and the p-value for the Minkowski distance
    (`p`). `n_particles` represents the number of particles used in the optimization,
    and `dimensions` represents the number of decision variables, which is equal to
    the number of cities in the TSP problem. The best solution found by the optimizer
    is converted to a TSP tour by sorting the indices of the solution in ascending
    order and using them to index the `city_names` list in the same order. This creates
    a list of city names in the order that they are visited in the best tour. We then
    print the best route and its length:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 作为9.2列表的延续，以下代码设置了PSO优化器的参数。`options`是一个字典，包含惯性权重（`w`）、认知（`c1`）和社会（`c2`）加速度系数、要考虑的邻居数量（`k`）以及Minkowski距离的p值。`n_particles`代表优化中使用的粒子数量，`dimensions`代表决策变量的数量，等于TSP问题中的城市数量。优化器找到的最佳解通过按升序排序解的索引，并使用它们以相同的顺序索引`city_names`列表来转换为TSP路线。这创建了一个按最佳路线访问顺序排列的城市名称列表。然后，我们打印最佳路线及其长度：
- en: '[PRE15]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Set up the PSO parameters.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置PSO参数。
- en: ② Instantiate the PSO optimizer.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ② 实例化PSO优化器。
- en: ③ Solve the problem.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 解决问题。
- en: ④ Convert the best solution to a TSP tour.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将最佳解转换为TSP路线。
- en: ⑤ Print the best route and its length.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印最佳路线及其长度。
- en: 'Listing 9.2 produces the following output:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2生成以下输出：
- en: '[PRE16]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Figure 9.14 shows the obtained route. The complete version of listing 9.2 is
    available in the book’s GitHub repo, and it shows the steps for visualizing the
    route as a NetworkX graph.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14显示了获得的路线。9.2列表的完整版本可在本书的GitHub仓库中找到，它展示了将路线可视化为NetworkX图的过程。
- en: '![](../Images/CH09_F14_Khamis.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F14_Khamis.png)'
- en: Figure 9.14 PSO solution for the 20-city TSP
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14展示了20城市TSP的PSO解决方案
- en: Feel free to adjust the code according to your needs by modifying elements such
    as the problem data, the initial city, or the parameters of the algorithm.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的需要调整代码，例如修改问题数据、初始城市或算法参数。
- en: 9.7 Neural network training using PSO
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 使用PSO进行神经网络训练
- en: Machine learning (ML) is a subfield of artificial intelligence (AI) that endows
    an artificial system or process with the ability to learn from experience and
    observation without being explicitly programmed. Many ML approaches have been
    and are still being proposed, and more details about ML will be provided in chapter
    11\. For now, let’s consider neural networks, which are one of the most used and
    successful statistical ML approaches. The artificial neural network (ANN or NN)
    approach is inspired by the biological brain and can be considered a highly simplified
    computational model, as NN is very far from matching a brain’s complexity. NN
    is at the heart of deep learning models that nowadays form the basis of many successful
    applications that touch everybody’s life, such as text, audio, image, and video
    generation, voice assistants, and recommendation engines, to name just a few.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是人工智能（AI）的一个子领域，它赋予人工系统或过程从经验观察中学习的能力，而不需要明确编程。许多机器学习方法已经被提出，并且仍在被提出，更多关于机器学习的细节将在第11章中提供。现在，让我们考虑神经网络，这是最常用且最成功的统计机器学习方法之一。人工神经网络（ANN或NN）方法受生物大脑的启发，可以被认为是一个高度简化的计算模型，因为神经网络与大脑的复杂性相去甚远。神经网络是深度学习模型的核心，如今它是许多成功应用的基础，这些应用触及到每个人的生活，如文本、音频、图像和视频生成、语音助手和推荐引擎等，仅举几例。
- en: The human brain
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑
- en: Aristotle (384-322 BC) wrote, “Of all the animals, man has the largest brain
    in proportion to his size.” The human brain is composed of an average of 86 billon
    interconnected nerve cells or *neurons*. Each biological neuron is connected to
    several thousand other neurons. It is extremely energy efficient, as it can perform
    the equivalent of an exaflop (a billion billion mathematical operations per second)
    with just 20 watts of power.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 亚里士多德（公元前384-322年）写道：“在所有动物中，人类的大脑相对于其体型来说最大。”人类大脑由平均86亿个相互连接的神经细胞或神经元组成。每个生物神经元连接到几千个其他神经元。它非常节能，因为它只需20瓦的功率就能完成相当于一个艾弗洛普（每秒十亿亿次数学运算）的工作。
- en: For simplicity, consider ML as glorified curve fitting, which intends to find
    a mapping function between independent and dependent variables. For example, suppose
    a vision-based object recognition model takes as input a digital image taken by
    the front camera of a vehicle—the output would be recognized objects, such as
    cars, pedestrians, cyclists, lanes, traffic lights, etc. In fact, ML shares the
    same ingredients as curve fitting in terms of model, scoring criteria, and search
    strategy. However, ML approaches, such as NNs, are a way to create functions that
    no human could write. They tend to create nonlinear, nonmonotonic, nonpolynomial,
    and even noncontinuous functions that approximate the relationship between independent
    and dependent variables in a data set.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们可以将机器学习视为一种美化的曲线拟合，其目的是在自变量和因变量之间找到一个映射函数。例如，假设一个基于视觉的对象识别模型以车辆前摄像头拍摄的数字图像作为输入——输出将是识别出的对象，如汽车、行人、骑自行车的人、车道、交通灯等。实际上，在模型、评分标准和搜索策略方面，机器学习与曲线拟合有相同的成分。然而，机器学习方法，如神经网络，是一种创建人类无法编写的函数的方法。它们倾向于创建非线性、非单调、非多项式，甚至非连续的函数，这些函数近似地表示数据集中自变量和因变量之间的关系。
- en: 'An NN is a massively parallel adaptive network of simple nonlinear computing
    elements called neurons that are arranged in input, hidden, and output layers.
    Each node, or artificial neuron, connects to another and has an associated weight
    and threshold allowing the node to simulate a neuron firing. Each individual node
    has its own linear regression model, composed of input data, a bias, a threshold,
    and an output, as illustrated in figure 9.15\. A neuron *k* can be described with
    the following equation:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个由简单非线性计算元素（称为神经元）组成的巨大并行自适应网络，这些神经元排列在输入、隐藏和输出层中。每个节点，或人工神经元，都与另一个节点相连，并具有相关的权重和阈值，允许节点模拟神经元放电。每个单独的节点都有自己的线性回归模型，由输入数据、偏差、阈值和输出组成，如图9.15所示。一个神经元
    *k* 可以用以下方程来描述：
- en: '|'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F14_Khamis-EQ16.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![方程](../Images/CH09_F14_Khamis-EQ16.png)'
- en: '| 9.16 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 9.16 |'
- en: Its output is
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出是
- en: '|'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../Images/CH09_F14_Khamis-EQ17.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F14_Khamis-EQ17.png)'
- en: '| 9.17 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 9.17 |'
- en: where *x[i]* are the inputs, *ω[ki]* are the weights, *b* is the bias term that
    defines the ability to fire in the absence of external input to the node, and
    *φ* is the activation function. This activation function makes the neuron fire
    the output when the input *z[k]* reaches a threshold *θ[k]*. There are different
    forms of activation functions (aka squashing functions) such as sign, step, tanh,
    arctan, s-shaped sigmoid (aka logistic), softmax, radial basis function, and rectified
    linear unit (ReLU).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x[i]* 是输入，*ω[ki]* 是权重，*b* 是偏差项，它定义了在没有外部输入到节点的情况下节点能够激活的能力，而 *φ* 是激活函数。这个激活函数使得当输入
    *z[k]* 达到阈值 *θ[k]* 时，神经元会输出。激活函数有不同的形式（也称为压缩函数），例如符号、阶跃、tanh、反正切、S形sigmoid（也称为逻辑回归）、softmax、径向基函数和修正线性单元（ReLU）。
- en: 'As in the case of curve fitting, a scoring criterion or cost function is used
    to estimate deviation between the estimated values and the actual values. In this
    context, training an NN is fundamentally an optimization problem. The goal of
    training is to find the optimal parameters (weights and biases) that minimize
    the difference between the network’s output and the expected output. This difference
    is often quantified using a loss or cost function, such as these:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 就像曲线拟合的情况一样，使用评分标准或损失函数来估计估计值和实际值之间的偏差。在这种情况下，训练神经网络本质上是一个优化问题。训练的目标是找到最优参数（权重和偏差），以最小化网络输出和预期输出之间的差异。这种差异通常使用损失或成本函数来量化，例如这些：
- en: '*Mean squared error (MSE)*—MSE is often used in regression problems. It calculates
    the square of the difference between the predicted and actual values and then
    averages these across the dataset. This function heavily penalizes large errors.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*均方误差 (MSE)*—MSE 常用于回归问题。它计算预测值和实际值之间的差的平方，然后在整个数据集上平均这些值。这个函数对大误差进行重罚。'
- en: '*Cross-entropy loss*—Cross-entropy loss is typically used for binary and multiclass
    classification problems. It measures the dissimilarity between the predicted probability
    distribution and the actual distribution. In other words, it compares the model’s
    confidence in its prediction with the actual outcome.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交叉熵损失*—交叉熵损失通常用于二进制和多类分类问题。它衡量预测概率分布和实际分布之间的不相似性。换句话说，它比较模型对其预测的信心与实际结果。'
- en: '*Negative log likelihood (NLL)*—NLL is another loss function in multiclass
    classification. If *y* is the true label and *p*(*y*) is the predicted probability
    of that label, the negative log likelihood is defined as –log(*p*(*y*)). The log
    function transforms the probabilities, which range between 0 and 1, to a scale
    that ranges from positive infinity to 0\. When the predicted probability for the
    correct class is high (close to 1), the log value is closer to 0, but as the predicted
    probability for the correct class decreases, the log value increases toward infinity.
    Negating the log value thus gives a quantity that is minimized when the predicted
    probability for the correct class is maximized.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*负对数似然 (NLL)*—NLL 是多类分类中的另一个损失函数。如果 *y* 是真实标签，而 *p*(*y*) 是该标签的预测概率，则负对数似然定义为
    –log(*p*(*y*)）。对数函数将介于0和1之间的概率转换为介于正无穷大到0的尺度。当预测概率对于正确类别较高（接近1）时，对数值更接近0，但随着预测概率对于正确类别的降低，对数值增加到无穷大。因此，取对数值的相反数给出一个量，当预测概率对于正确类别最大化时，该量被最小化。'
- en: '![](../Images/CH09_F15_Khamis.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F15_Khamis.png)'
- en: Figure 9.15 Neural network node demonstration
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 神经网络节点演示
- en: 'Training an NN involves the following steps:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及以下步骤：
- en: '*Initialization*—Before training starts, the weights and biases in the network
    are typically initialized with small random numbers.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*初始化*—在训练开始之前，网络中的权重和偏差通常使用小的随机数进行初始化。'
- en: '*Feedforward*—In this stage, the input is passed through the network to produce
    an output. This output is generated by performing computations on the inputs using
    the initial or current weights, bias, and activation function transformation.
    The output of one layer becomes the input to the next layer until the final output
    is produced.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前馈*—在这个阶段，输入通过网络产生输出。这个输出是通过使用初始或当前权重、偏差和激活函数转换对输入进行计算而生成的。一个层的输出成为下一层的输入，直到产生最终输出。'
- en: '*Error calculation*—After the feedforward stage, the output is compared with
    the desired output to calculate the error using a loss function. This function
    quantifies how far the network’s predictions are from the actual values.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*误差计算*—在正向传播阶段之后，输出与期望输出进行比较，使用损失函数计算误差。这个函数量化了网络的预测值与实际值之间的距离。'
- en: '*Backpropagation*—The calculated error is then propagated back through the
    network, starting from the output layer and moving back toward the input layer.
    This process computes the gradient or derivative of the loss function with respect
    to the weights and biases in the network.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向传播*—计算出的误差随后通过网络反向传播，从输出层开始，移动到输入层。这个过程计算了网络中损失函数相对于权重和偏差的梯度或导数。'
- en: '*Weight adjustment*—In this final stage, the weights of the network are updated
    in an effort to reduce the error. This is typically done using a technique called
    *gradient descent*. The weights are adjusted in the direction that most decreases
    the error, as determined by the gradients calculated during backpropagation.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*权重调整*—在这个最终阶段，网络权重被更新以减少误差。这通常使用称为*梯度下降*的技术来完成。权重调整的方向是减少误差最多的方向，这是通过反向传播期间计算的梯度确定的。'
- en: By repeating these steps for many iterations (or epochs), the network gradually
    learns to produce outputs that are closer to the desired ones, thus “learning”
    from the input data.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多次迭代（或称为epoch）重复这些步骤，网络逐渐学会产生更接近期望的输出，从而“学习”输入数据。
- en: Now that you have a basic understanding of NNs, let’s train a simple NN using
    PSO following a supervised learning approach. During supervised training, the
    NN learns by initially processing a labeled dataset. By training on a labeled
    dataset, the network can subsequently predict labels for a new, unlabeled data
    set during the inferencing stage, after training.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对神经网络有了基本了解，让我们使用PSO（粒子群优化）按照监督学习的方法训练一个简单的神经网络。在监督训练过程中，神经网络通过最初处理标记的数据集来学习。通过在标记的数据集上训练，网络可以在训练后的推理阶段预测新、未标记数据集的标签。
- en: 'For this example, we will use the Penguins dataset. This is a popular dataset
    in the data science community, containing information on the size, sex, and species
    of penguins. The dataset consists of 344 observations collected from three islands
    in the Palmer Archipelago, Antarctica. It includes the following seven variables:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用企鹅数据集。这是数据科学社区中一个流行的数据集，包含有关企鹅大小、性别和种类的信息。该数据集由来自南极洲帕默群岛三个岛屿的344个观测值组成。它包括以下七个变量：
- en: '`species`—The species of penguin (Adelie, Chinstrap, or Gentoo)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`species`—企鹅的种类（阿德利企鹅、帝企鹅或金图企鹅）'
- en: '`island`—The island where the penguin was observed (Biscoe, Dream, or Torgersen)'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`island`—观察到企鹅的岛屿（比斯科、梦想或托格森）'
- en: '`bill_length_mm`—The length of the penguin’s bill in millimeters'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bill_length_mm`—企鹅喙的长度（毫米）'
- en: '`bill_depth_mm`—The depth of the penguin’s bill in millimeters'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bill_depth_mm`—企鹅喙的深度（毫米）'
- en: '`flipper_length_mm`—The length of the penguin’s flipper in millimeters'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flipper_length_mm`—企鹅鳍的长度（毫米）'
- en: '`body_mass_g`—The mass of the penguin’s body in grams'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`body_mass_g`—企鹅身体的重量（克）'
- en: '`sex`—The sex of the penguin (male or female)'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sex`—企鹅的性别（雄性或雌性）'
- en: 'Our simple NN, described in the PySwarms use cases, has the following characteristics:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在PySwarms用例中描述的简单神经网络具有以下特征：
- en: '*Input layer size*—4'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入层大小*—4'
- en: '*Hidden layer size*—10 (activation function: tanh(*x*)). The hyperbolic tangent
    activation function (aka Tanh, tanh, or TanH) maps input values to be between
    –1 and 1, and it’s used to introduce nonlinearity in NNs. Remember that a sigmoid
    function maps input values to be between 0 and 1\. The tanh function is centered
    at 0, which helps mitigate the vanishing gradient problem, compared to the sigmoid
    function. However, both tanh and sigmoid activations suffer from the vanishing
    gradient problem to some degree. Alternatives like rectified linear unit (ReLU)
    and its variants are often preferred.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐藏层大小*—10（激活函数：tanh(*x*)）。双曲正切激活函数（又称Tanh、tanh或TanH）将输入值映射到-1和1之间，并用于在神经网络中引入非线性。记住，sigmoid函数将输入值映射到0和1之间。tanh函数以0为中心，这有助于减轻梯度消失问题，与sigmoid函数相比。然而，tanh和sigmoid激活都存在一定程度的梯度消失问题。像ReLU及其变体这样的替代方案通常更受欢迎。'
- en: '*Output layer size*—3 (activation function: softmax(*x*)). Softmax is a generalization
    of the sigmoid function. This function takes as input the *logits* that represent
    unnormalized outputs of the last layer of the network before they are transformed
    into probabilities by applying a softmax function. These logits can be interpreted
    as a measure of the “evidence” that a certain input belongs to a particular class.
    The higher the logit value for a particular class, the more likely it is that
    the input belongs to that class.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出层大小*—3（激活函数：softmax(*x*)）。Softmax是sigmoid函数的推广。该函数接受作为输入的*logits*，这些*logits*代表网络最后一层在应用softmax函数将其转换为概率之前的未归一化输出。这些logits可以解释为衡量“证据”的指标，表明某个输入属于特定类。特定类的logits值越高，输入属于该类的可能性就越大。'
- en: The following listing shows the steps for training this simple NN using PSO.
    We start by importing the libraries we’ll need and reading the penguin dataset.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了使用PSO训练这个简单神经网络（NN）的步骤。我们首先导入所需的库并读取penguin数据集。
- en: Listing 9.3 Neural network training using PSO
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 使用PSO进行神经网络训练
- en: '[PRE17]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Required for loading the dataset
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ① 用于加载数据集
- en: ② Required for target label encoding
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ② 用于目标标签编码
- en: ③ Required for dimensionality reduction
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 用于降维
- en: ④ Load the Penguins dataset.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 加载Penguins数据集。
- en: ⑤ Show the dataset rows and columns.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 显示数据集的行和列。
- en: This produces the output shown in figure 9.16.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生如图9.16所示的输出。
- en: '![](../Images/CH09_F16_Khamis.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F16_Khamis.png)'
- en: Figure 9.16 Penguins dataset
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 鹦鹉螺数据集
- en: 'As a continuation of listing 9.3, we can visualize this dataset using the seaborn
    library as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 作为9.3列表的延续，我们可以使用seaborn库如下可视化这个数据集：
- en: '[PRE18]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output is shown in figure 9.17.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如图9.17所示。
- en: '![](../Images/CH09_F17_Khamis.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F17_Khamis.png)'
- en: Figure 9.17 Bill length vs. body mass by species in the Penguins dataset
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 鹦鹉螺数据集中不同物种的喙长与体重
- en: 'Next, we define a `logits_function` to take in a vector `p` of parameters for
    the NN and return the logits (pre-activation values) for the final layer of the
    network. As illustrated in figure 9.18, this function starts by extracting the
    weights and biases for the first and second layers of the network from the parameter
    vector *p* using indexing and reshaping operations. Then, the function performs
    forward propagation by computing the pre-activation value *z*¹ in the first layer
    as the dot product of the input data *X* and the first set of weights *W*¹, plus
    the bias term *b*¹. It then applies the tanh activation function to *z*¹ to obtain
    the activation value *a*¹ in the first layer. Finally, the function computes the
    pre-activation value for the second layer by taking the dot product of *a*¹ and
    the second set of weights *W*² and adding the bias term *b*². The resulting values
    are returned as the logits from the final layer of the network:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`logits_function`，它接受一个参数向量`p`，用于神经网络，并返回网络的最后一层的logits（预激活值）。如图9.18所示，该函数首先使用索引和重塑操作从参数向量*p*中提取网络第一层和第二层的权重和偏差。然后，该函数通过计算第一层的预激活值*z*¹（输入数据*X*和第一组权重*W*¹的点积加上偏差项*b*¹）来执行前向传播。然后，它将tanh激活函数应用于*z*¹以获得第一层的激活值*a*¹。最后，该函数通过将*a*¹与第二组权重*W*²的点积加上偏差项*b*²来计算第二层的预激活值。这些结果作为网络的最后一层的logits返回：
- en: '[PRE19]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Extracting the weights of the first laye
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ① 提取第一层的权重。
- en: ② Extracting the weights of the second layer
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ② 提取第二层的权重。
- en: ③ Extracting the biases of the second layer
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 提取第二层的偏差
- en: ④ Calculate the pre-activation value in the first layer .
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算第一层的预激活值。
- en: ⑤ Calculate and return the logits.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算并返回logits。
- en: '![](../Images/CH09_F18_Khamis.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F18_Khamis.png)'
- en: Figure 9.18 NN layers
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 神经网络层
- en: 'Next, we define the `forward_prop` function to perform a forward pass through
    an NN with two layers. This computes the softmax probabilities and negative log
    likelihood loss for the output, given a set of parameters `params`. The function
    first calls the `logits_function` to obtain the logits for the final layer of
    the network, given the parameters `params`. Then the function applies the softmax
    function to the logits using the `np.exp` function and normalizes the resulting
    values by dividing by the sum of the exponentiated logits for each sample, using
    the `np.sum` function with the `axis=1` argument. This gives a probability distribution
    over the classes for each sample. The function then computes the negative log
    likelihood loss by taking the negative log of the probability of the correct class
    for each sample, which is obtained by indexing the `probs` array using the `y`
    variable, which contains the true class labels. The `np.sum` function is used
    to compute the sum of these negative log probabilities across all samples, and
    the result is divided by the total number of samples to obtain the average loss
    per sample. Finally, the function returns the computed loss:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义`forward_prop`函数，通过具有两层NN进行正向传播。此函数计算给定一组参数`params`的输出softmax概率和负对数似然损失。函数首先调用`logits_function`来获取网络最后一层的logits，给定参数`params`。然后，该函数使用`np.exp`函数对logits应用softmax函数，并使用带有`axis=1`参数的`np.sum`函数通过将每个样本的指数化logits的总和除以这些值来归一化结果。这给出了每个样本的类别概率分布。然后，该函数通过使用包含真实类别标签的`y`变量索引`probs`数组来计算每个样本正确类别的概率的负对数，该变量包含真实类别标签。使用`np.sum`函数计算这些负对数概率的总和，并将结果除以样本总数以获得每个样本的平均损失。最后，该函数返回计算出的损失：
- en: '[PRE20]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① Obtain the logits for the softmax.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取softmax的logits。
- en: ② Apply softmax to calculate the probability distribution over the classes.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ② 应用softmax计算类别的概率分布。
- en: ③ Compute the negative log likelihood.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 计算负对数似然。
- en: ④ Compute and return the loss.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算并返回损失。
- en: 'To perform forward propagation over the whole swarm of particles, we define
    the following `particle_loss()` function. This function computes the loss for
    each particle in a PSO swarm, given its position in the search space. It is worth
    noting that each position represents the NN parameters (w1,b1,w2,b2) with `dimension`
    calculated as follows:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在整个粒子群上执行正向传播，我们定义以下`particle_loss()`函数。此函数计算PSO群中每个粒子的损失，给定其在搜索空间中的位置。值得注意的是，每个位置代表NN参数（w1,b1,w2,b2），其`dimension`计算如下：
- en: '[PRE21]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'An example of a candidate setting of NN parameters (i.e., a *position* in PSO
    terminology) is given here:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一个NN参数候选设置（即PSO术语中的*位置*）的例子如下所示：
- en: '[PRE22]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The PSO algorithm can then use these loss values to update the positions of
    the particles and search for the optimal set of parameters for the NN.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，PSO算法可以使用这些损失值来更新粒子的位置，并搜索NN的最佳参数集。
- en: '[PRE23]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Determine the number of particles.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ① 确定粒子数量。
- en: ② Compute and return the loss for each particle.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算并返回每个粒子的损失。
- en: 'The last function we need is `predict`, which uses the NN parameters corresponding
    to the positions of particles in a PSO swarm to predict the class labels for each
    sample in the dataset. This function first calls `logits_function` to obtain the
    logits for the final layer of the network, given the positions `pos` of the particles
    in the search space. Then the function computes the predicted class labels by
    taking the `argmax` of the logits across the columns (i.e., along the second axis
    or `axis=1`), using the `np.argmax` function. This gives the index of the class
    with the highest probability for each sample. Finally, the function returns the
    predicted class labels as a numpy array `y_pred`:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的最后一个函数是`predict`，它使用PSO群中粒子的位置对应的NN参数来预测数据集中每个样本的类别标签。此函数首先调用`logits_function`来获取网络最后一层的logits，给定搜索空间中粒子的位置`pos`。然后，该函数通过使用`np.argmax`函数计算logits的`argmax`值（即沿第二轴或`axis=1`），来计算预测的类别标签。这给出了每个样本具有最高概率的类别的索引。最后，该函数将预测的类别标签作为numpy数组`y_pred`返回：
- en: '[PRE24]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ① Obtain logits for the final layer of the network.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取网络最后一层的logits。
- en: ② Compute and return the predicted class labels.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ② 计算并返回预测的类别标签。
- en: 'We can now train the NN using different PSOs available in PySwarms. The code
    starts by setting up several training samples, inputs, and the number of hidden
    layers and outputs. The dimensions are then computed based on the number of inputs,
    hidden nodes, and output classes. Three variants of PSO are defined: `globalBest`,
    `localBest`, and `binaryPSO`. The PSO hyperparameters are set using a dictionary
    called `options`. These hyperparameters include the inertia weight `w`, the cognitive
    parameter `c1`, the social parameter `c2`, the number of neighbors to be considered
    `k`, and the Minkowski distance parameter `p` (`p=1` is the sum-of-absolute values
    [or the L1 distance], while `p=2` is the Euclidean [or L2] distance):'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用PySwarms中可用的不同PSO来训练神经网络。代码首先设置几个训练样本、输入以及隐藏层和输出的数量。然后根据输入数量、隐藏节点和输出类别计算维度。定义了三种PSO变体：`globalBest`、`localBest`和`binaryPSO`。使用名为`options`的字典设置PSO超参数。这些超参数包括惯性权重`w`、认知参数`c1`、社会参数`c2`、要考虑的邻居数量`k`和Minkowski距离参数`p`（`p=1`是绝对值之和[或L1距离]，而`p=2`是欧几里得[或L2]距离）：
- en: '[PRE25]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Get the feature vector
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取特征向量
- en: ② Set up the number of training samples, inputs, hidden layers, and outputs.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置训练样本数量、输入、隐藏层和输出。
- en: ③ Set up the dimensions of the problem.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置问题的维度。
- en: ④ Define the variants of PSO.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义PSO的变体。
- en: ⑤ Set up the PSO hyperparameters
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置PSO超参数
- en: ⑥ Train the NN using different variants of PSO, and print the best accuracy.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用不同的PSO变体训练神经网络，并打印最佳准确率。
- en: The code then trains an NN, using each variant of PSO in turn, by creating an
    instance of the PSO optimizer and calling the `optimize` method, passing in the
    loss function and the number of iterations to run, `iters`. The best loss and
    particle position found by the optimizer are stored in `cost` and `pos` respectively.
    The code then prints the variant of PSO used along with the best accuracy that
    was obtained by using the corresponding particle position to make a prediction
    and comparing it to the true class label `y`.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 然后代码通过创建PSO优化器实例并调用`optimize`方法，传入损失函数和要运行的迭代次数`iters`，依次使用PSO的每个变体来训练一个神经网络。优化器找到的最佳损失和粒子位置分别存储在`cost`和`pos`中。然后代码打印出所使用的PSO变体以及使用相应的粒子位置进行预测并与之比较真实类别标签`y`所获得的最佳准确率。
- en: 'Running the complete listing produces the following output:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完整的列表生成以下输出：
- en: '[PRE26]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, `globalBest` PSO is the most efficient PSO variant for training
    this NN. Binary PSO does not match with the continuous nature of the NN parameters.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`globalBest` PSO是训练此神经网络最有效的PSO变体。二进制PSO与神经网络参数的连续性不匹配。
- en: You can experiment with the code by changing the problem and algorithm parameters.
    For example, you could use a reduced feature set such as `bill_length_mm` and
    `flipper_length_mm` instead of the four features used in this code. You could
    also change the algorithm parameters and apply velocity clamping. `velocity clamp`
    is a parameter enabled in PySwarms to set the limits for velocity clamping. It’s
    a tuple of size 2 where the first entry is the minimum velocity and the second
    entry is the maximum velocity.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过改变问题和算法参数来实验代码。例如，你可以使用如`bill_length_mm`和`flipper_length_mm`这样的减少特征集，而不是本代码中使用的四个特征。你也可以更改算法参数并应用速度限制。`速度限制`是PySwarms中启用的一个参数，用于设置速度限制的界限。它是一个大小为2的元组，其中第一个条目是最小速度，第二个条目是最大速度。
- en: In the next chapter, you will be introduced to ant colony optimization (ACO)
    and artificial bee colony (ABC) as other effective optimization algorithms inspired
    by swarm intelligence.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解到蚁群优化（ACO）和人工蜂群（ABC）作为其他受群体智能启发的有效优化算法。
- en: Summary
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: PSO employs a stochastic approach that utilizes the collective intelligence
    and movement of a swarm of particles. It is based on the idea of social interaction,
    which allows for efficient problem-solving.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PSO采用一种随机方法，利用粒子群体的集体智慧和运动。它基于社会互动的理念，允许高效地解决问题。
- en: The fundamental principle of PSO is to guide the swarm toward the best position
    in the search space while also remembering each particle’s own best-known position,
    as well as the global best-known position of the swarm.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PSO的基本原理是在引导群体向搜索空间中的最佳位置移动的同时，记住每个粒子的已知最佳位置，以及群体的全局已知最佳位置。
- en: 'PSO is guided by a straightforward principle: emulate the success of neighboring
    individuals.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PSO遵循一个简单的原则：模仿邻近个体的成功。
- en: Although PSO was initially designed for solving problems with continuous variables,
    many real-world problems involve discrete or combinatorial variables. In these
    problems, the search space is finite, and the algorithm needs to search through
    a set of discrete solutions. To address these types of problems, different variants
    of PSO have been developed, such as binary PSO (BPSO) and permutation-based PSO.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然PSO最初是为解决具有连续变量的问题而设计的，但许多现实世界的问题涉及离散或组合变量。在这些问题中，搜索空间是有限的，算法需要搜索一系列离散解。为了解决这类问题，已经开发出PSO的不同变体，例如二进制PSO（BPSO）和基于排列的PSO。
- en: By carefully tuning inertia weight and cognitive and social acceleration coefficients,
    PSO can effectively balance exploration and exploitation.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过仔细调整惯性权重、认知和社会加速系数，PSO可以有效地平衡探索和利用。
