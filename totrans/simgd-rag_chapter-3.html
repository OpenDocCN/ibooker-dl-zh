<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>chapter-3</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css" />
 </head>
 <body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span> </span><span class="chapter-title-text">Indexing pipeline: Creating a knowledge base for RAG</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header"><span class="CharOverride-2">This chapter covers</span></h3> 
   <ul> 
    <li class="readable-text" id="p2"><span class="CharOverride-3">Data loading</span></li> 
    <li class="readable-text" id="p3"><span class="CharOverride-3">Text splitting or chunking</span></li> 
    <li class="readable-text" id="p4"><span class="CharOverride-3">Converting text to embeddings</span></li> 
    <li class="readable-text" id="p5"><span class="CharOverride-3">Storing embeddings in vector databases</span></li> 
    <li class="readable-text" id="p6"><span class="CharOverride-3">Examples in Python using LangChain</span></li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In chapter 2, we discussed the main components of retrieval-augmented generation (RAG) systems. You may recall that the indexing pipeline creates the knowledge base or the non-parametric memory of RAG applications. An indexing pipeline needs to be set up before the real-time user interaction with the large language model (LLM) can begin.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>This chapter elaborates on the four components of the indexing pipeline. We begin by discussing data loading, which involves connecting to the source, extracting files, and parsing text. At this stage, we introduce a framework called LangChain, which has become increasingly popular in the LLM app developer community. Next, we elaborate on the need for data splitting or chunking and discuss chunking strategies. Embeddings is an important design pattern in the world of AI and ML. We explore embeddings in detail and how they are relevant in the RAG context. Finally, we look at a new storage technique called vector storage and the databases that facilitate it. </p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>By the end of this chapter, you should have a solid understanding of how a knowledge base, or the non-parametric memory of a RAG application, is created. We also embellish this chapter with snippets of Python code, so those of you who are so inclined can try a hands-on development of the indexing pipeline.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>By the end of this chapter, you should</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p11">Know how to extract data from sources.</li> 
   <li class="readable-text" id="p12">Get a deeper understanding of text-chunking strategies.</li> 
   <li class="readable-text" id="p13">Learn what embeddings are and how they are used.</li> 
   <li class="readable-text" id="p14">Gain knowledge of vector storage and vector databases.</li> 
   <li class="readable-text" id="p15">Have an end-to-end knowledge of setting up the indexing pipeline.</li> 
  </ul> 
  <div class="readable-text" id="p16"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.1</span> Data loading</h2> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>This section focuses on the first stage of the indexing pipeline. You will read about data loaders, metadata information, and data transformers.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>The first step toward building a knowledge base (or non-parametric memory) of a RAG system is to source data from its original location. This data may be in the form of Word documents, PDF files, CSV, HTML, and similar. Furthermore, the data may be stored in file, block, or object stores, in data lakes, data warehouses, or even in third-party sources that can be accessed via the open internet. This process of sourcing data from its original location is called <em>data loading</em>. Loading documents from a list of sources may turn out to be a complicated process. Therefore, it is advisable to document all the sources and the file formats in advance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>Before going too deep, let’s begin with a simple example. If you recall, in chapter 1, we used Wikipedia as a source of information about the 2023 Cricket World Cup. At that time, we copied the opening paragraph of the article and pasted it in the ChatGPT prompt window. Instead of doing it manually, we will now <em>connect</em> to Wikipedia and <em>extract</em> the data programmatically, using a very popular framework called LangChain. The code in this chapter and the book can be run on Python notebooks and is available in the GitHub repository of this book (<a href="https://mng.bz/a9DJ"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/a9DJ</span></a>).</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p20"> 
   <p><span class="print-book-callout-head">Note</span> LangChain is an open source framework developed by Harrison Chase and launched in October 2022. It was written in Python and JavaScript and designed for building applications using LLMs. Apart from being suitable for RAG, LangChain is also suitable for building application use cases such as chatbots, document summarizers, synthetic data generation, and more. Over time, LangChain has built integrations with LLM providers such as OpenAI, Anthropic, and Hugging Face; a variety of vector store providers; cloud storage systems such as AWS, Google, Azure, and SQL and NoSQL databases; and APIs for news, weather, and similar. Although LangChain has received some criticism, it is still a good starting point for developers.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p21"> 
    <h5 class=" callout-container-h5 readable-text-h5">Installing LangChain </h5> 
   </div> 
   <div class="readable-text" id="p22"> 
    <p>To install LangChain (we’ll use the version 0.3.19 in this chapter) using <code>pip</code>, run</p> 
   </div> 
   <div class="browsable-container listing-container" id="p23"> 
    <div class="code-area-container"> 
     <pre class="code-area">%pip install langchain==0.3.19</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p24"> 
    <p>The <code>langchain-community</code> package contains third-party integrations. It is automatically installed by LangChain, but in case it does not work, you can also install it separately using <code>pip</code>:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p25"> 
    <div class="code-area-container"> 
     <pre class="code-area">%pip install langchain-community</pre>  
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>Now that you have installed LangChain, we will use it to connect to Wikipedia and extract data from the page about the 2023 Cricket World Cup. For this task, we will use the <code>AsyncHtmlLoader</code> function from the <code>document_loaders</code><em> </em>library in the <code>langchain-community</code> package. To run <code>AsyncHtmlLoader</code>, we will have to install another Python package called bs4:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p27"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong>#Installing bs4 package</strong>
%pip install bs4==0.0.2 --quiet

<strong>#Importing the AsyncHtmlLoader</strong>
from langchain_community.document_loaders import AsyncHtmlLoader

<strong>#This is the URL of the Wikipedia page on the 2023 Cricket World Cup</strong>
url=&quot;https://en.wikipedia.org/wiki/2023_Cricket_World_Cup&quot;

<strong>#Invoking the AsyncHtmlLoader</strong>
loader = AsyncHtmlLoader (url)

<strong>#Loading the extracted information</strong>
html_data = loader.load()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>The <code>data</code> variable in the code now stores the information from the Wikipedia page. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p29"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(data)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Here is the output (A large section of the text is replaced with periods to save space.)</p> 
  </div> 
  <div class="browsable-container listing-container" id="p31"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;[<strong>Document</strong>(<strong>page_content</strong>='&lt;!DOCTYPE html&gt;\n&lt;html class=&quot;client-nojs vector-feature-language-in-header-enabled………………………………………………………………………………………………….of In the knockout stage, India and Australia beat New Zealand and South Africa respectively to advance to the final, played on 19 November at &lt;a href=&quot;/wiki/Narendra_Modi_Stadium&quot; title=&quot;Narendra Modi Stadium&quot;&gt;Narendra Modi Stadium&lt;/a&gt;. Australia won by 6 wickets, winning their sixth Cricket World Cup title………………………………………………… &quot;datePublished&quot;:&quot;2013-06-29T19:20:08Z&quot;,&quot;dateModified&quot;:&quot;2024-05-01T05:16:34Z&quot;,&quot;image&quot;:&quot;https:\\/\\/upload.wikimedia.org\\/wikipedia\\/en\\/e\\/eb\\/2023_CWC_Logo.svg&quot;,&quot;headline&quot;:&quot;13th edition of the premier international cricket competition&quot;}&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;', <strong>metadata</strong>={'source': 'https://en.wikipedia.org/wiki/2023_Cricket_World_Cup', 'title': '2023 Cricket World Cup - Wikipedia', 'language': 'en'})]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>The variable <code>data</code> is a list of documents that includes two elements: <code>page_content</code> and <code>metadata</code>. <code>page_content</code> contains the text sourced from the URL. You will notice that the text along with the relevant information also has newline characters (<code>\n</code>) and other HTML tags; however, <code>metadata</code> contains another important data aspect. </p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>Metadata is information about the data (e.g., type, origin, and purpose). This can include a data summary; the way the data was created; who created it and why; when it was created; and the size, quality, and condition of the data. Metadata information comes in extremely handy in the retrieval stage. Also, it can be used to resolve conflicting information that can arise due to chronology or origin. In the previous example, while extracting the data from the URL, Wikipedia has already provided the source, title, and language in the metadata information. For many data sources, you will have to add metadata.</p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>Often, a <em>cleaning</em> of the source data is required. The data in our example has a lot of new line characters and HTML tags, which requires a certain level of cleanup. We will attempt to clean up the webpage data that we extracted using the <code>Html2Text&shy;Transformer</code> function from the <code>document_transformers</code> library in the <code>langchain-community</code> package. For <code>Html2TextTransformer</code><em>, </em>we will also have to install another package called <code>html2text</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p35"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong>#Install html2text</strong>
%pip install html2text==2024.2.26 –quiet

<strong>#Import Html2TextTransformer</strong>
from langchain_community.document_transformers import Html2TextTransformer

<strong>#Assign the Html2TextTransformer function</strong>
html2text = Html2TextTransformer()

<strong>#Call transform_documents</strong>
html_data_transformed = html2text.transform_documents(data)

print(html_data_transformed[0].page_content)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>The output of the <code>page_content</code> is now free of any HTML tags and contains only the text from the webpage:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p37"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;Jump to content  Main menu  Main menu  move to sidebar hide Navigation    * Main page   * Contents   * Current events   * Random article   * About Wikipedia   * Contact us   * Donate  Contribute………….In the knockout stage, India and Australia beat New Zealand and South Africa respectively to advance to the final, played on 19 November at Narendra Modi Stadium. Australia won by 6 wickets, winning their sixth Cricket World Cup title…… * This page was last edited on 1 May 2024, at 05:16 (UTC).   * Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia&reg; is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.    * Privacy policy   * About Wikipedia   * Disclaimers   * Contact Wikipedia   * Code of Conduct * Developers   * Statistics   * Cookie statement   * Mobile view    *</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>The text is more coherent now since we have removed the HTML part of the data. There can be further cleanup, such as removing special characters and other unnecessary information. Data cleaning also removes duplication. Yet another step to include in the data-loading stage can be masking of sensitive information such as PII (Personally Identifiable Information) or company secrets. In some cases, a fact check may also be required.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>The source for our data was Wikipedia (more precisely, a web address pointing to a Wikipedia page), and the format was HTML. The source can also be other storage locations such as AWS S3, SQL/NoSQL databases, Google Drive, GitHub, even WhatsApp, YouTube, and other social media sites. Likewise, the data formats can be .doc, .pdf, .csv, .ppt, .eml, and the like. Most of the time, you will be able to use frameworks such as LangChain that have integrations for the sources and the formats already built in. Sometimes, you may have to build custom connectors and loaders.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>Although data loading may seem simple (after all, it’s just connecting to a source and extracting data), the nuances of adding metadata, document transformation, masking, and similar add complexity to this step. Advanced planning of the sources, a review of the formats, and curation of metadata information are advised for best results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>We have now taken the first step toward building our RAG system. The data-loading process can be further broken down into four sub-steps, as shown in figure 3.1:</p> 
  </div> 
  <div class="browsable-container figure-container " id="p42">  
   <img src="../Images/CH03_F01_Kimothi.png" alt="A diagram of a data conversion process

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.1</span><span class=""> </span><span class="">Four sub-steps of the data-loading component of the indexing pipeline</span></h5>
  </div> 
  <ol> 
   <li class="readable-text" id="p43">Connect to the source of data.</li> 
   <li class="readable-text" id="p44">Extract text from the file.</li> 
   <li class="readable-text" id="p45">Review and update metadata information.</li> 
   <li class="readable-text" id="p46">Clean or transform the data.</li> 
  </ol> 
  <div class="readable-text" id="p47"> 
   <p>We have now obtained data from the source and cleaned it to an extent. This Wikipedia page that we have loaded has more than 8,000 words, alone. Imagine the number of words if we had multiple documents. For efficient management of information, we employ something called data splitting, which will be discussed in the next section.</p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.2</span> Data splitting (chunking)</h2> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Breaking down long pieces of text to manageable segments is called <em>data splitting</em> or <em>chunking</em>. This section discusses why chunking is necessary and the different chunking strategies. We also use functions from LangChain to illustrate a few examples. </p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.2.1</span> Advantages of chunking</h3> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>In the previous section, we loaded the data from a URL (a Wikipedia page) and extracted the text. It was a long piece of text of approximately 8,000 words. When it comes to overcoming the major limitations of using long pieces of text in LLM applications, chunking offers the following three advantages:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p52"><em>Context window of LLM</em><em>s</em>—Due to the inherent nature of the technology, the number of tokens (loosely, words) LLMs can work with at a time is limited. This includes both the number of tokens in the prompt (or the input) and in the completion (or the output). The limit on the total number of tokens that an LLM can process in one go is called “the context window size.” If we pass an input that is longer than the context window size, the LLM chooses to ignore all text beyond the size. It is thus very important to be careful with the amount of text being passed to the LLM. In our example, a text of 50,000 words will not work well with LLMs that have a smaller context window. The way to address this problem is to break the text down into smaller chunks.</li> 
   <li class="readable-text" id="p53"><em>Lost-in-the-middle proble</em><em>m</em>—Even in those LLMs that have a long context window (e.g., Claude 3 by Anthropic has a context window of up to 200,000 tokens), a problem with accurately reading the information has been observed. It has been noticed that accuracy declines dramatically if the relevant information is somewhere in the middle of the prompt. This problem can be addressed by passing only the relevant information to the LLM instead of the entire document.</li> 
   <li class="readable-text" id="p54"><em>Ease of searc</em><em>h</em>—This is not a problem with the LLM per se, but it has been observed that large chunks of text are harder to search over. When we use a retriever (recall the generation pipeline introduced in chapter 2), it is more efficient to search over smaller pieces of text.</li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p55"> 
   <p><span class="print-book-callout-head">DEFINITION</span> Tokens are the fundamental semantic units used in natural language processing (NLP) tasks. Tokens can be assumed to be words, but sometimes, a single word can be made up of more than one token. OpenAI suggests one token to be made of four characters or 0.75 words. Tokens are important as most proprietary LLMs are priced based on token usage.</p> 
  </div> 
  <div class="readable-text" id="p56"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.2.2</span> Chunking process</h3> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>The chunking process can be divided into three steps, as illustrated in figure 3.2: </p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p58">Divide the longer text into compact, meaningful units (e.g., sentences or paragraphs).</li> 
   <li class="readable-text" id="p59">Merge the smaller units into larger chunks until a specific size is achieved. After that, this chunk is treated as an independent segment of text.</li> 
   <li class="readable-text" id="p60">When creating a new chunk, include a part of the previous chunk at the start of the new chunk. This overlap is necessary to maintain contextual continuity. </li> 
  </ol> 
  <div class="readable-text" id="p61"> 
   <p>This process is also known as “small to big” chunking. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p62">  
   <img src="../Images/CH03_F02_Kimothi.png" alt="Diagram of a diagram of a large chunking unit

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.2</span><span class=""> </span><span class="">Data-chunking process</span></h5>
  </div> 
  <div class="readable-text" id="p63"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.2.3</span> Chunking methods</h3> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>While splitting documents into chunks might sound like a simple concept, multiple methods can be employed to execute chunking. The following two aspects vary across the chunking methodologies:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p65">The manner of text splitting</li> 
   <li class="readable-text" id="p66">Measuring of the chunk size</li> 
  </ul> 
  <div class="readable-text" id="p67"> 
   <h4 class=" readable-text-h4">Fixed-Size Chunking</h4> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>A very common approach is to predetermine the size of the chunk and the amount of overlap between the chunks. The following two methods fall under the <em>fixed-size chunking</em> category:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p69"><em>Split by characte</em><em>r</em>—Here, we specify a certain character, such as a newline character <code>\n</code> or a special character <code>*</code>, to determine how the text should be split. The text is split into a unit whenever this character is encountered. The chunk size is measured in the number of characters. We must choose the chunk size or the number of characters we need in each chunk. We can also choose the number of characters we need to overlap between two chunks. We will look at an example and demonstrate this method using <code>CharacterTextSplitter</code> from <code>langchain.text_splitters</code>. For this, we will take the same document that we loaded and transformed in the previous section from Wikipedia and store it in the variable <code>html_data_transformed.</code> </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p70"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong>#import libraries</strong>
from langchain.text_splitters import CharacterTextSplitter
<strong>#Set the CharacterTextSplitter parameters</strong>
text_splitter = CharacterTextSplitter(
    separator=&quot;\n&quot;,    <strong>#The character that should be used to split</strong>
    chunk_size=1000,   <strong>#Number of characters in each chunk</strong>
    chunk_overlap=200, <strong>#Number of overlapping characters between chunks</strong>
)

<strong>#Create Chunks</strong>
chunks=
text_splitter.create_documents(
[html_data_transformed[0].page_content]
)

<strong>#Show the number of chunks created</strong>
print(f&quot;The number of chunks created : {len(chunks)}&quot;)

&gt;&gt;The number of chunks created: 67</pre>  
   </div> 
  </div> 
  <div class="readable-text list-body-item" id="p71"> 
   <p>This method created 64 chunks. But what about the overlap? Let’s check two chunks at random, say, chunks 4 and 5. We will compare the last 200 characters <br />of chunk 4 with the first 200 characters of chunk 5:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p72"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">chunks[4].page_content[-200:]

&gt;&gt; 'on was to be played from 9 February to 26 March\<strong>n2023.[3][4] In July 2020 it was announced that due to the disruption of the\nqualification schedule by the COVID-19 pandemic, the start of the tournament</strong>'

chunks[5].page_content[:200]

&gt;&gt; <strong>'</strong><strong>2023.[3][4] In July 2020 it was announced that due to the disruption of the\nqualification schedule by the COVID-19 pandemic, the start of the tournament</strong>\nwould be delayed to October.[5][6] The ICC rele'</pre>  
   </div> 
  </div> 
  <div class="readable-text list-body-item" id="p73"> 
   <p>Comparing the two outputs, we can observe that there is an overlap between the two consecutive chunks.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p74"> 
   <p>Splitting by character is a simple and effective way to create chunks. It is the first chunking method that anyone should try. However, sometimes, it may not be feasible to create chunks within the specified length. This is because the sequential occurrence of the character on which the text needs to be split is far apart. To address this problem, a recursive approach is employed.</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p75"><em>Recursively split by character—</em>This method is quite like the split by character but instead of specifying a single character for splitting, we specify a list of characters. The approach initially tries creating chunks based on the first character. In case it is not able to create a chunk of the specified size using the first character, it then uses the next character to further break down chunks to the required size. This method ensures that chunks are largely created within the specified size. This method is recommended for generic texts. You can use <code>RecursiveCharacter&shy;TextSplitter</code> from LangChain to use this method. The only difference in <code>RecursiveCharacterTextSplitter</code><em> </em>is that instead of passing a single character in the separator parameter<code> separator=</code><code>&quot;</code><code>\n</code><code>&quot;</code>, we will need to pass a list <code>separators= [</code><code>&quot;</code><code>\n\n</code><code>&quot;</code><code>,</code><code>&quot;</code><code>\n</code><code>&quot;</code><code>, </code><code>&quot;</code><code>.</code><code>&quot;</code><code>, </code><code>&quot;</code> <code>&quot;</code><code>]</code>.</li> 
  </ul> 
  <div class="readable-text" id="p76"> 
   <p>Another perspective to consider with fixed-sized chunking is the use of tokens. As shown at the beginning of this section, tokens are the fundamental units of NLP. They can be understood loosely as a proxy for words. All LLMs process text in the form of tokens. So, it would also make sense to use tokens to determine the size of the chunks. This method is called the <em>split by token method</em>. Here, the splitting still happens based on a character, but the size of the chunk and the overlap are determined by the number of tokens instead of the number of characters.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p77"> 
   <p><span class="print-book-callout-head">NOTE</span> Tokenizers are used to create tokens from a piece of text. Tokens are slightly different from words. A phrase such as “I’d like that!” has three words; however, in NLP, this text may be parsed as five tokens, that is, “I”, “‘d”, “like”, “that”, “!”. Different LLMs use different methods for creating tokens. OpenAI uses a tokenizer called tiktoken for GPT3.5 and GPT4 models;<strong> </strong>Llama2 by Meta uses LLamaTokenizer, available in the transformers library by Hugging Face. You can also explore other tokenizers on Hugging Face. NLTK and spaCy are some other popular libraries that can be used as tokenizers.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>To use the split by token method, you can use specific methods within the <code>Recursive&shy;CharacterTextSplitter</code><span class="_Bold-Italics"> </span>and<span class="_Bold-Italics"> </span><code>CharacterTextSplitter</code> classes, such as <code>Recursive&shy;CharacterTextSplitter.from_tiktoken_encoder</code> <code>(encoding=</code><code>&quot;</code><code>cl100k_base</code><code>&quot;</code><code>, chunk_size=100,</code> <code>chunk_overlap=10)</code> for creating chunks of 100 tokens with an overlap of 10 tokens using OpenAI’s tiktoken tokenizer or <code>CharacterTextSplitter.from_</code><code>huggingface_tokenizer(tokenizer,</code> <code>chunk_size=100,</code> <code>chunk_overlap=10)</code> for creating the same sized chunk using another tokenizer from Hugging Face.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>The limitation of fixed-size chunking is that it doesn’t consider the semantic integrity of the text. In other words, the meaning of the text is ignored. It works best in scenarios where data is inherently uniform, such as genetic sequences and service manuals, or uniformly structured reports such as survey responses.</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <h4 class=" readable-text-h4">Specialized chunking</h4> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Chunking aims to keep meaningful data together. If we are dealing with data in the form of HTML, Markdown, JSON, or even computer code, it makes more sense to split the data based on the structure rather than a fixed size. Another approach to chunking is to consider the format of the extracted and loaded data. A markdown file, for example, is organized by headers, a code written in a programming language such as Python or Java is organized by classes and functions, and likewise, HTML is organized in headers and sections. For such formats, a specialized chunking approach can be employed. LangChain offers classes such as <code>MarkdownHeaderTextSplitter</code>, <code>HTMLHeader&shy;TextSplitter</code>, and <code>RecursiveJsonSplitter</code>, among others, for these formats.  </p> 
  </div> 
  <div class="readable-text intended-text" id="p82"> 
   <p>Here is a simple example of a code that splits an HTML document using <code>HTML&shy;SectionSplitter</code><em>. </em>We are using the same Wikipedia article to source the HTML page. We first split the input data based on the sections. Sections in HTML are tagged as <code>&lt;h1&gt;</code>, <code>&lt;h2&gt;</code>, <code>&lt;table&gt;</code>, and so on. It can be assumed that a well-structured HTML document will have similar information. This helps us in creating chunks that have similar information. To use the <code>HTMLSectionSplitter</code><em> </em>library, we must install another Python package called <code>lxml</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong>#Installing lxml</strong>
%pip install lxml==5.3.1 --quiet

<strong># Import the HTMLHeaderTextSplitter library</strong>
from langchain_text_splitters import HTMLSectionSplitter

<strong># Set URL as the Wikipedia page link</strong>
url=&quot;https://en.wikipedia.org/wiki/2023_Cricket_World_Cup&quot;

loader = AsyncHtmlLoader (url)

html_data = loader.load()

<strong># Specify the header tags on which splits should be made</strong>
sections_to_split_on=[
    (&quot;h1&quot;, &quot;Header 1&quot;),
    (&quot;h2&quot;, &quot;Header 2&quot;),
    (&quot;table &quot;, &quot;Table&quot;),
    (&quot;p&quot;, &quot;Paragraph&quot;)
]

<strong># Create the HTMLHeaderTextSplitter function</strong>
splitter = HTMLSectionSplitter(sections_to_split_on)

<strong># Create splits in text obtained from the URL</strong>
Split_content = splitter.split_text(html_data[0].page_content)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>The advantage of specialized chunking is that chunk sizes are no longer limited by a fixed width. This feature helps in preserving the inherent structure of the data. Because the size of the chunks changes depending on the structure, this method is also sometimes called <em>adaptive chunking</em>. Specialized chunking works well in structured scenarios such as customer reviews or patient records where data can be of different lengths but should ideally be in the same chunk.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>In the previous example, let’s see how many chunks have been created:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p86"> 
   <div class="code-area-container"> 
    <pre class="code-area">len(split_content)

&gt;&gt; 231</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>This method has given us 231 chunks from the URL. Chunking methods do not have to be exclusive. We can further chunk these 231 chunks using a fixed-size chunking method such as <code>RecursiveCharacterTextSplitter</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p88"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
separators=[&quot;\n\n&quot;,&quot;\n&quot;,&quot;.&quot;]
chunk_size=1000, chunk_overlap=100, 
)

final_chunks = text_splitter.split_documents(split_content)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Let’s look at how many chunks were created by this combination of techniques:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p90"> 
   <div class="code-area-container"> 
    <pre class="code-area">len(chunks)

&gt;&gt; 285</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>A total of 285 chunks were created by splitting the HTML data from the URL first, using a specialized chunking method followed by a fixed size method. This gave us more chunks than using the fixed size method alone that we saw in the previous section (“split by character” gave us 67 chunks).</p> 
  </div> 
  <div class="readable-text intended-text" id="p92"> 
   <p>You may be wondering about the advantages of having more chunks and the optimal number. Unfortunately, there’s no straightforward answer to that. Having many chunks (consequently smaller-sized chunks) means that the information in the chunks is precise. This is advantageous when it comes to providing the LLM with accurate information. In contrast, by chunking into small sizes, you may lose the overall themes, ideas, and coherence of the larger document. The task here is to strike a balance. We will discuss more chunking strategies after we take a cursory look at a novel method that considers the meaning of the text to perform chunking and aims to create chunks that are super-contextual.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h4 class=" readable-text-h4">Semantic chunking</h4> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>This idea, proposed by Greg Kamradt, questions two aspects of the previous chunking methods.</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p95">Why should we have a predefined fixed size of chunks?</li> 
   <li class="readable-text" id="p96">Why don’t chunking methods take into consideration the actual meaning of content?</li> 
  </ul> 
  <div class="readable-text" id="p97"> 
   <p>To address these problems, a method that looks at semantic similarity (or similarity in the meaning) between sentences is called semantic chunking. It first creates groups of three sentences and then merges groups that are similar in meaning. To find out the similarity in meaning, this method uses embeddings. (We will discuss embeddings in the next section.) This is still an experimental chunking technique. In LangChain, you can use the class <code>SemanticChunker</code> from the <code>langchain_experimental.text_splitter</code> library. See figure 3.3 for examples of different chunking methods.  </p> 
  </div> 
  <div class="browsable-container figure-container " id="p98">  
   <img src="../Images/CH03_F03_Kimothi.png" alt="Several different types of diagrams

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.3</span><span class=""> </span><span class="">Chunking methods</span></h5>
  </div> 
  <div class="readable-text" id="p99"> 
   <p>As the LLM and the generative AI space are evolving fast, chunking methods are also becoming more sophisticated. Simple chunking methods predetermine the size of the chunks and a split by characters. A slightly more nuanced technique is to split the data by tokens. Specialized methods are more suitable for different data formats. Experimental techniques such as semantic chunking and agentic chunking are spearheading the advancements in the chunking space. Now, let’s consider the important question of how to select a chunking method.</p> 
  </div> 
  <div class="readable-text" id="p100"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.2.4</span> Choosing a chunking strategy</h3> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>We have seen that there are many chunking methods available. Which chunking method to use (i.e., whether to use a single method or multiple methods) is a question that comes up during the creation of the indexing pipeline. There are no guidelines or rules to answer this question. However, certain features of the application that you’re developing can guide you toward an effective strategy.</p> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h4 class=" readable-text-h4">Nature of the content </h4> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>The type of data that you’re dealing with can be a guide for the chunking strategy. If your application uses data in a specific format such as code or HTML, a specialized chunking method is recommended. Not only that, whether you’re working with long documents such as whitepapers and reports or short-form content such as social media posts, tweets, and so on, can also guide the chunk size and overlap limits. If you’re using a diverse set of information sources, then you might have to use different methods for different sources.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <h4 class=" readable-text-h4">Expected length and complexity of user query</h4> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>The nature of the query that your RAG system is likely to receive also determines the chunking strategy. If your system expects a short and straightforward query, then the size of your chunks should be different when compared to a long and complex query. Matching long queries to short chunks may prove inefficient in certain cases. Similarly, short queries matching large chunks may yield partially irrelevant <br />results.</p> 
  </div> 
  <div class="readable-text" id="p106"> 
   <h4 class=" readable-text-h4">Application and use case requirements</h4> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>The nature of the use case you’re addressing may also determine the optimal chunking strategy. For a direct question-answering system, shorter chunks are likely used for precise results, while for summarization tasks, longer chunks may make more sense. If the results of your system need to serve as an input to another downstream application, that may also influence the choice of the chunking strategy.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <h4 class=" readable-text-h4">Embeddings model</h4> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>We are going to discuss embeddings in the next section. For now, you can make a note that certain embeddings models perform better with chunks of specific sizes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>We have discussed chunking at length in this section. From understanding the need and advantages of chunking to different chunking methods and the choice of chunking strategies, you are now equipped to load data from different sources and split them into optimal sizes. Remember, chunking is not an overcomplicated task, and most chunking methods will work. You will, however, have to evaluate and improve your chunking strategy depending on the results you observe.</p> 
  </div> 
  <div class="readable-text intended-text" id="p111"> 
   <p>Now that data has been split into manageable sizes, we need to store it so that it can be fetched later to be used in the generation pipeline. We need to ensure that these chunks can be effectively searched over to match the user query. Turns out that one data pattern is the most efficient for such tasks. This pattern is called “embeddings.” Let’s explore embeddings and their use in RAG systems in the next <br />section.</p> 
  </div> 
  <div class="readable-text" id="p112"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.3</span> Data conversion (embeddings)</h2> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>Computers, at their very core, do mathematical calculations. Mathematical calculations are done on numbers. Therefore, for a computer to process any kind of nonnumeric data such as text or image, it must be first converted into a numerical form. </p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.3.1</span> What are embeddings?</h3> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p><em>Embeddings</em> is a design pattern that is extremely helpful in the fields of data science, machine learning, and AI. Embeddings are vector representations of data. As a general definition, embeddings are data that has been transformed into <em>n</em>-dimensional matrixes. The word <em>embedding</em> is a vector representation of words. I explain embeddings by using three words as an example: <em>dog, bark,</em> and <em>fly</em>.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p116"> 
   <p><span class="print-book-callout-head">NOTE</span> In physics and mathematics, the vector is an object that has a magnitude and a direction, like an arrow in space. The length of the arrow is the magnitude of the quantity and the direction that the arrow points to is the direction of the quantity. Examples of such quantities in physics are velocity, force, acceleration, and so forth. In computer science and machine learning, the idea of a vector is an abstract representation of data, and the representation is an array or list of numbers. These numbers represent the data features or attributes. In NLP, a vector can represent a document, a sentence, or even a word. The length of the array or list is the number of dimensions in the vector. A 2D vector will have two numbers, a 3D vector will have three numbers, and an <em>n</em>-dimensional vector will have <em>n</em> numbers. </p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>Let’s understand embeddings by assigning a number to the three words: Dog = 1, Bark = 2 and Fly = 6, as shown in figure 3.4. We chose these numbers because the word <em>dog</em> is closer to the word <em>bark</em> and farther from the word <em>fly</em>. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p118">  
   <img src="../Images/CH03_F04_Kimothi.png" alt="A blue line with black text

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.4</span><span class=""> </span><span class="">Words in a unidimensional vector</span> </h5>
  </div> 
  <div class="readable-text" id="p119"> 
   <p>Unidimensional vectors are not great representations because we can’t plot unrelated words accurately. In our example, we can plot that the words <em>fly</em> and <em>bark</em>, which are verbs, are far from each other, and bark is closer to a dog because dogs can bark. But how do we plot words such as <em>love</em> or <em>re</em><em>d</em>? To accurately represent all the words, we need to increase the number of dimensions. See figure 3.5.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p120">  
   <img src="../Images/CH03_F05_Kimothi.png" alt="A blue line with white text

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.5</span><span class=""> </span><span class="">Words in a 2D vector space</span></h5>
  </div> 
  <div class="readable-text" id="p121"> 
   <p>The goal of an embedding model is to convert words (or sentences/paragraphs) into <em>n</em>-dimensional vectors so that the words (or sentences/paragraphs) that are like each other in meaning lie close to each other in the vector space. See figure 3.6.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p122">  
   <img src="../Images/CH03_F06_Kimothi.png" alt="A diagram of a model

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.6</span><span class=""> </span><span class="">The process of embedding transforms data (such as text) into vectors and compresses the input information, which results in an embedding space specific to the training data.</span></h5>
  </div> 
  <div class="readable-text" id="p123"> 
   <p>An embeddings model can be trained on a corpus of preprocessed text data using an embedding algorithm such as Word2Vec, GloVe, FastText, or BERT:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p124"><em>Word2Ve</em><em>c</em>—Word2Vec is a shallow-neural-network-based model for learning word embeddings, developed by researchers at Google. It is one of the earliest embedding techniques.</li> 
   <li class="readable-text" id="p125"><em>GloV</em><em>e</em>—Global Vectors for Word Representations is an unsupervised learning technique developed by researchers at Stanford University.</li> 
   <li class="readable-text" id="p126"><em>FastTex</em><em>t</em>—FastText is an extension of Word2Vec developed by Facebook AI Research. It is particularly useful for handling misspellings and rare words.</li> 
   <li class="readable-text" id="p127"><em>ELM</em><em>o</em>—Embeddings from Language Models was developed by researchers at Allen Institute for AI. ELMo embeddings have been shown to improve performance on question answering and sentiment analysis tasks.</li> 
   <li class="readable-text" id="p128"><em>BERT</em>—Bidirectional Encoder Representations from Transformers, developed by researchers at Google, is a transformers-architecture-based model. It provides contextualized word embeddings by considering bidirectional context, achieving state-of-the-art performance on various NLP tasks.</li> 
  </ul> 
  <div class="readable-text" id="p129"> 
   <p>Training a custom embeddings model can prove to be beneficial in some use cases where the scope is limited. Training an embeddings model that generalizes well can be a laborious exercise. Collection and preprocessing text data can be cumbersome. The training process can turn out to be computationally expensive too. </p> 
  </div> 
  <div class="readable-text" id="p130"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.3.2</span> Common pre-trained embeddings models</h3> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>The good news for anyone building RAG systems is that embeddings once created can also generalize across tasks and domains. There are a variety of proprietary and open source pre-trained embeddings models available to use. This is also one of the reasons why the usage of embeddings has exploded in popularity across machine learning applications.</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p132"><em>Embeddings models by OpenAI</em>—OpenAI, the company behind ChatGPT and the GPT series of LLMs, also provides three embeddings models:  
    <ul> 
     <li><em>text-embedding-ada-002</em> was released in December 2022. It has a dimension of 1536, meaning that it converts text into a vector of 1536 dimensions.</li> 
     <li><em>text-embedding-3-small</em><strong> </strong>is the latest small embedding model of 1536 dimensions released in January 2024. The flexibility it provides over the ada-002 model is that users can adjust the size of the dimensions according to their needs.</li> 
     <li><em>text-embedding-3-large</em> is a large embedding model of 3072 dimensions, released together with the text-embedding-3-small model. It is the best performing model released by OpenAI yet.<br class="list-body-separator" /> OpenAI models are closed source and can be accessed using the OpenAI API. They are priced based on the number of input tokens for which embeddings are desired. </li> 
    </ul> </li> 
   <li class="readable-text" id="p133"><em>Gemini Embeddings Model by Googl</em><em>e</em>—<em>text-embedding-004</em> (last updated in April 2024)<strong> </strong>is the model offered by Google Gemini. It offers elastic embeddings size up to 768 dimensions and can be accessed via the Gemini API.</li> 
   <li class="readable-text" id="p134"><em>Voyage A</em><em>I</em>—These embeddings models are recommended by Anthropic, the provider of the Claude series of LLMs. Voyage offers several embedding models such as 
    <ul> 
     <li><em>voyage-large-2-instruct</em><strong> </strong>is a 1024-dimensional embeddings model that has become a leader in embeddings models.</li> 
     <li><em>voyage-law-2</em><strong> </strong>is a 1024-dimension model optimized for legal documents.</li> 
     <li><em>voyage-code-2</em> is a 1536-dimension model optimized for code retrieval.</li> 
     <li><em>voyage-large-2</em> is a 1536-dimension general-purpose model optimized for retrieval.<br class="list-body-separator" /> Voyage AI offers several free tokens before charging for using the embeddings models.</li> 
    </ul> </li> 
   <li class="readable-text" id="p135"><em>Mistral AI embedding</em><em>s</em>—Mistral is the company behind LLMs such as Mistral and Mixtral. They offer a 1024-dimensional embeddings model known as <em>mistral-embed</em>. This is an open source embeddings model. </li> 
   <li class="readable-text" id="p136"><em>Cohere embedding</em><em>s</em>—Cohere, the developers of Command, Command R, and Command R + LLMs also offer a variety of embeddings models, which can be accessed via the Cohere API. Some of these are 
    <ul> 
     <li><em>embed-english-v3.0</em> is a 1024-dimension model that works on embeddings for English only.</li> 
     <li><em>embed-english-light-v3.0</em> is a lighter version of the embed-english model, which has 384 dimensions.</li> 
     <li><em>embed-multilingual-v3.0</em> offers multilingual support for over 100 languages.</li> 
    </ul> </li> 
  </ul> 
  <div class="readable-text" id="p137"> 
   <p>These five models are in no way recommendations but just a list of the popular embeddings models. Apart from these providers, almost all LLM developers such Meta, TII, and LMSYS also offer pre-trained embeddings models. One place to check out all the popular embeddings models is the MTEB (Massive Text Embedding Benchmark) Leaderboard on Hugging Face (<a href="https://huggingface.co/spaces/mteb/leaderboard"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/huggingface.co/spaces/mteb/leaderboard</span></a>). The MTEB benchmark compares the embeddings models on tasks such as classification, retrieval, clustering, and more. You now know what embeddings are, but why are they useful? Let’s discuss that next with some examples of use cases.</p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.3.3</span> Embeddings use cases</h3> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>The reason why embeddings are so popular is because they help in establishing semantic relationships between words, phrases, and documents. In the simplest methods of searching or text matching, we use keywords, and if the keywords match, we can show the matching documents as results of the search. However, this approach fails to consider the semantic relationships or the meanings of the words while searching. This challenge is overcome by using embeddings.  </p> 
  </div> 
  <div class="readable-text" id="p140"> 
   <h4 class=" readable-text-h4">How is similarity calculated</h4> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>We discussed that embeddings are vector representations of words or sentences. Similar pieces of text lie close to each other. Closeness to each other is calculated by the distance between the points in the vector space. One of the most common measures of similarity is <em>cosine similarity</em>. Cosine similarity is calculated as the cosine value of the angle between the two vectors. Recall from trigonometry that the cosine of parallel lines (i.e., angle = 0<span class="_Superscript _idGenCharOverride-1">o</span><span class="_Body-Char">)</span> is 1, and the cosine of a right angle (i.e., 90<span class="CharOverride-4">o</span>) is 0. The cosine of the opposite lines (i.e., angle = 180<span class="CharOverride-4">o</span>) is −1. Therefore, the cosine similarity lies between −1 and 1, where unrelated terms have a value close to 0, and related terms have a value close to 1. Terms that are opposite in meaning have a value of −1. See figure 3.7.<span class="_Body-Char"> </span><span class="_Body-Char"></span></p> 
  </div> 
  <div class="browsable-container figure-container " id="p142">  
   <img src="../Images/CH03_F07_Kimothi.png" alt="A graph of a trigonometry

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.7</span><span class=""> </span><span class="">Cosine similarity of vectors in 2D vector space</span></h5>
  </div> 
  <div class="readable-text" id="p143"> 
   <p>Yet another measure of similarity is the <em>Euclidean distance</em> between two vectors. Close vectors have a small Euclidean distance. It can be calculated using the following formula:</p> 
  </div> 
  <div class="browsable-container equation-container" id="p144"> 
   <p>Distance (A, B) = sqrt((A<sub>i</sub>-B<sub>i</sub>)<sup>2</sup>),</p> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>where i is the i-th dimension of the <em>n</em>-dimensional vectors</p> 
  </div> 
  <div class="readable-text" id="p146"> 
   <h4 class=" readable-text-h4">Different use cases of embeddings</h4> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>Here are some different use cases of embeddings:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p148"><em>Text search</em>—Searching through the knowledge base for the right document chunk is a key component of RAG systems. Embeddings are used to calculate similarity between the user query and the stored documents. </li> 
   <li class="readable-text" id="p149"><em>Clustering</em>—Categorizing similar data together to find themes and groups in the data can result in valuable insights. Embeddings are used to group similar pieces of text together to find out, for example, the common themes in customer reviews.</li> 
   <li class="readable-text" id="p150"><em>Machine learning</em>—Advanced machine learning techniques can be used for different problems such as classification and regression. To convert text data into numerical features, embeddings prove to be a valuable technique.</li> 
   <li class="readable-text" id="p151"><em>Recommendation engines</em>—Shorter distances between product features mean greater similarity. Using embeddings for product and user features can be used to recommend similar products. </li> 
  </ul> 
  <div class="readable-text" id="p152"> 
   <p>Since we are focusing on RAG systems, here we examine using embeddings for text search— to find the document chunks that are closest to the user’s query. Let’s continue with our example of the Wikipedia page on the 2023 Cricket World Cup. In the last section, we created 67 chunks using a combination of specialized and fixed-width chunking. Now we will see how to create embeddings for each chunk. We will see how to use an open source as well as a proprietary embeddings model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Here is the code example for creating embeddings using an open source embeddings model all-MPnet-base-v2 via Hugging Face: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p154"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Import HuggingFaceEmbeddings from embeddings library</strong>
from langchain_huggingface import HuggingFaceEmbeddings

<strong># Instantiate the embeddings model. The embeddings model_name can be changed as desired</strong>
embeddings = 
HuggingFaceEmbeddings(
model_name=&quot;sentence-transformers/all-mpnet-base-v2&quot;
)

<strong># Create embeddings for all chunks</strong>
hf_embeddings = 
embeddings.embed_documents(
[chunk.page_content for chunk in final_chunks]
)

<strong>#Check the length(dimension) of the embedding</strong>
len(hf_embeddings [0])
&gt;&gt; 768</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>This model creates embeddings of dimension 768. The list <code>hf_embeddings </code>is made up of 285 lists, each containing 768 numbers for each chunk. Figure 3.8 shows the embeddings space of all the chunks.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p156">  
   <img src="../Images/CH03_F08_Kimothi.png" alt="A blank lined paper with numbers

AI-generated content may be incorrect." style="width: 100%; max-width: max-content;" /> 
   <h5 class=" figure-container-h5"><span class="">Figure 3.8</span><span class=""> </span><span class="">Embeddings created for chunks of Wikipedia page using the all-MiniLM-l6-v2 model.</span></h5>
  </div> 
  <div class="readable-text" id="p157"> 
   <p>Similarly, we can use a proprietary model such as the text-embedding-3-small model, hosted by OpenAI. The only prerequisite is obtaining an API key and setting up a billing account with OpenAI. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p158"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Install the langchain openai library</strong>
%pip install langchain-openai==0.3.7 --quiet

<strong># Import OpenAIEmbeddings from the library</strong>
from langchain_openai import OpenAIEmbeddings

<strong># Set the OPENAI_API_KEY as the environment variable</strong>
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &lt;YOUR_API_KEY&gt;

<strong># Instantiate the embeddings object</strong>
embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)

<strong># Create embeddings for all chunks</strong>
openai_embeddings = 
embeddings.embed_documents(
[chunk.page_content for chunk in chunks]
)

<strong>#Check the length(dimension) of the embedding</strong>
len(openai_embedding[0])

&gt;&gt; 1536</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>This text-embedding-3-small model creates embeddings for the same chunks of dimension 1536.</p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>There are several embeddings models available, and new ones are being added every day. The choice of embeddings can be dictated by certain factors. Let’s look at a few factors. </p> 
  </div> 
  <div class="readable-text" id="p161"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.3.4</span> How to choose embeddings?</h3> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>There are a few major factors that will affect your choice of embeddings. </p> 
  </div> 
  <div class="readable-text" id="p163"> 
   <h4 class=" readable-text-h4">Use case</h4> 
  </div> 
  <div class="readable-text" id="p164"> 
   <p>Your application use case may determine your choice of embeddings. The MTEB leaderboard scores each of the embeddings models across seven use cases: classification, clustering, pair classification, reranking, retrieval, semantic text similarity, and summarization. At the time of writing this book, the <code>SFR-Embedding-Mistral</code> model developed by Salesforce performs the best for retrieval tasks.</p> 
  </div> 
  <div class="readable-text" id="p165"> 
   <h4 class=" readable-text-h4">Cost</h4> 
  </div> 
  <div class="readable-text" id="p166"> 
   <p>Cost is another important factor to consider. To create the knowledge base, you may have to create embeddings for thousands of documents, thus running into millions of tokens.</p> 
  </div> 
  <div class="readable-text intended-text" id="p167"> 
   <p>Embeddings are powerful data patterns that are most effective in finding similarities between texts. In RAG systems, embeddings play a critical role in search and retrieval of data relevant to the user query. Once the embeddings have been created, they need to be stored in persistent memory for real-time access. To store embeddings, a new kind of database called a <em>vector database</em> have become increasingly popular.</p> 
  </div> 
  <div class="readable-text" id="p168"> 
   <h2 class=" readable-text-h2"><span class="num-string">3.4</span> Storage (vector databases)</h2> 
  </div> 
  <div class="readable-text" id="p169"> 
   <p>Now we are at the last step of the indexing pipeline. The data has been loaded, split, and converted to embeddings. To use this information repeatedly, we need to store it in memory so that it can be accessed on demand.</p> 
  </div> 
  <div class="readable-text" id="p170"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.4.1</span> What are vector databases? </h3> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>The evolution of databases can be traced back to the early days of computing. Databases are organized collections of data, designed to be easily accessed, managed, and updated. Relational databases such as MySQL organize structured data into rows and columns. NoSQL databases such as MongoDB specialize in handling unstructured and semi-structured data. Graph databases such as Neo4j are optimized for querying graph data. In the same manner, vector databases are built to handle high-dimensional vectors. These databases specialize in indexing and storing vector embeddings for fast semantic search and retrieval. </p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>Apart from efficiently storing high-dimensional vector data, modern vector databases offer traditional features such as scalability, security, multi-tenancy, versioning and management, and similar. However, vector databases are unique in offering similarity searches based on Euclidean distance or cosine similarity. They also employ specialized indexing techniques.</p> 
  </div> 
  <div class="readable-text" id="p173"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.4.2</span> Types of vector databases</h3> 
  </div> 
  <div class="readable-text" id="p174"> 
   <p>Vector databases started as a specialized database offering, but propelled by the growth in demand for storing vector data, all major database providers have added the vector indexing capability. We can categorize the popular vector databases available today into six broad categories.</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p175"><em>Vector indexe</em><em>s</em>—These are libraries that focus on the core features of indexing and search. They do not support data management, query processing, or interfaces. They can be considered a bare-bones vector database. Examples of vector indexes are Facebook AI Similarity Search (FAISS), Non-Metric Space Library (NMSLIB), Approximate Nearest Neighbors Oh Yeah (ANNOY), Scalable Nearest Neighbors (ScaNN), and similar.</li> 
   <li class="readable-text" id="p176"><em>Specialized vector DB</em><em>s</em>—These databases focus on the core feature of high-dimensional vector support, indexing, search, and retrieval such as vector indexes, but also offer database features such as data management, extensibility, security, scalability, non-vector data support, and similar. Examples of specialized vector DBs are Pinecone, ChromaDB, Milvus, Qdrant, Weaviate, Vald, LanceDB, Vespa, and Marqo.</li> 
   <li class="readable-text" id="p177"><em>Search platform</em><em>s</em>—Solr, Elastic Search, Open Search, and Apache Lucene are traditional text search platforms and engines built for full text search. They have now added vector similarity search capabilities to their existing search capabilities.</li> 
   <li class="readable-text" id="p178"><em>Vector capabilities for SQL database</em><em>s</em>—Azure SQL, Postgres SQL(pgvector), SingleStore, and CloudSQL are traditional SQL databases that have now added vector data-handling capabilities.</li> 
   <li class="readable-text" id="p179"><em>Vector capabilities for NoSQL database</em><em>s</em>—Like SQL DBs, NoSQL DBs such as MongoDB have also added vector search capabilities.</li> 
   <li class="readable-text" id="p180"><em>Graph databases with vector capabilitie</em><em>s</em>—Graph DBs such as Neo4j, have also opened new possibilities by adding vector capabilities, . </li> 
  </ul> 
  <div class="readable-text" id="p181"> 
   <p>Using a vector index such as FAISS is supported by LangChain. To use FAISS, we first must install the <code>faiss-cpu</code> library. We will use the chunks already created in section 3.2 and the OpenAI embeddings that we used in section 3.3:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p182"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Install FAISS-CPU</strong>
%pip install faiss-cpu==1.10.0 --quiet

<strong># Import FAISS class from vectorstore library</strong>
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

<strong># Import OpenAIEmbeddings from the library</strong>
from langchain_openai import OpenAIEmbeddings

<strong># Set the OPENAI_API_KEY as the environment variable</strong>
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &lt;YOUR_API_KEY&gt;

<strong># Chunks from Section 3.3</strong>
Final_chunks=final_chunks

<strong># Instantiate the embeddings object</strong>
embeddings=OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)

<strong># Instantiate the FAISS object</strong>
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
)

<strong># Add the chunks</strong>
vector_store.add_documents(documents=final_chunks)

<strong># Check the number of chunks that have been indexed</strong>
vector_store.index.ntotal

&gt;&gt; 285</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p183"> 
   <p>With this code, the 285 chunks of data have been converted to vector embeddings, and these embeddings are stored in a FAISS vector index. The FAISS vector index can also be saved to memory using the <code>vector_store.save_local(folder_path,index_name)</code> and <code>FAISS.load_local(folder_path,index_name)</code> functions. Let’s now take a cursory look at how a vector store can be used. We will take the original question that we have been asking from the beginning of this book: “Who won the 2023 Cricket World Cup?”</p> 
  </div> 
  <div class="browsable-container listing-container" id="p184"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong># Original Question</strong>
query = &quot;Who won the 2023 Cricket World Cup?&quot;

<strong># Ranking the chunks in descending order of similarity</strong>
docs = <code>vector_store</code>.similarity_search(query)

<strong># Printing one of the top-ranked chunk</strong>
print(docs[0].page_content)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p185"> 
   <p>Similarity search orders the chunks in descending order of similarity, meaning that the most similar chunks to the query are ranked on top. In the previous example, we can observe that the chunk that speaks about the world cup final has been ranked <br />on top.</p> 
  </div> 
  <div class="readable-text intended-text" id="p186"> 
   <p>FAISS is a stripped-down high-performance vector index that works for many applications. ChromaDB is another user-friendly vector DB that has gained popularity. Pinecone offers managed services and customization. Milvus claims higher performance on similarity search, while Qdrant provides an advanced filtering system. We will now discuss some points on how to choose a vector database that works best for your requirements.</p> 
  </div> 
  <div class="readable-text" id="p187"> 
   <h3 class=" readable-text-h3"><span class="num-string">3.4.3</span> Choosing a vector database</h3> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>All vector databases offer the same basic capabilities, but each one of them also claims a differentiated value. Your choice should be influenced by the nuance of your use case matching with the value proposition of the database. Here are a few things to consider while evaluating and implementing a vector database: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p189"><em>Accuracy vs. spee</em><em>d</em>—Certain algorithms are more accurate but slower. A balance between search accuracy and query speed must be achieved based on application needs. It will become important to evaluate vector DBs on these parameters.</li> 
   <li class="readable-text" id="p190"><em>Flexibility vs. performanc</em><em>e</em>—Vector DBs provide customizations to the user. While it may help you in tailoring the DB to your specific requirements, more customizations can add overhead and slow systems down. </li> 
   <li class="readable-text" id="p191"><em>Local vs. cloud storag</em><em>e</em>—Assess tradeoffs between local storage speed and access versus cloud storage benefits like security, redundancy, and scalability.</li> 
   <li class="readable-text" id="p192"><em>Direct access vs. AP</em><em>I</em>—Determine if tight integration control via direct libraries is required or if ease-of-use abstractions like APIs better suit your use case. </li> 
   <li class="readable-text" id="p193"><em>Simplicity vs. advanced feature</em><em>s</em>—Compare advanced algorithm optimizations, query features, and indexing versus how much complexity your use case necessitates versus needs for simplicity.</li> 
   <li class="readable-text" id="p194"><em>Cos</em><em>t</em>—While you may incur regular costs in a fully managed solution, a self-hosted one might prove costlier if not managed well.</li> 
  </ul> 
  <div class="readable-text" id="p195"> 
   <p>We have now completed an end-to-end indexing of a document. We continued with the same question (“Who won the 2023 Cricket World Cup?”) and the same external source—the Wikipedia page of the 2023 Cricket World Cup (<a href="https://mng.bz/yN4J"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/yN4J</span></a>). In this chapter, we started with the programmatic loading of this Wikipedia page extracting the HTML document and then parsing the HTML document to extract. Thereafter, we divided the text into small-sized chunks using a specialized and fixed-width chunking method. We converted these chunks into embeddings using OpenAI’s text-embedding-003-large model. Finally, we stored the embeddings into a FAISS vector index. We also saw how using similarity search on this vector index helped us retrieve relevant chunks.</p> 
  </div> 
  <div class="readable-text intended-text" id="p196"> 
   <p>When several such documents in different formats from different sources are indexed using a combination of methods and strategies, we can store all the information in the form of vector embeddings creating a non-parametric knowledge base for our RAG system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p197"> 
   <p>This concludes our discussion on the indexing pipeline. By now, you must have built a solid understanding of the four components of the indexing pipeline and should be ready to build a knowledge base for a RAG system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p198"> 
   <p>In the next chapter, we will use this knowledge base to generate real-time responses to user queries through the generation pipeline.</p> 
  </div> 
  <div class="readable-text" id="p199"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <div class="readable-text" id="p200"> 
   <h3 class=" readable-text-h3">Data loading</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p201">The process of sourcing data from its original location is called <em>data loading</em>, and it includes the following four steps: connecting to the source, extracting and parsing text, reviewing and updating metadata, and cleaning and transforming data.</li> 
   <li class="readable-text" id="p202">Loading documents from a list of sources may turn out to be a complicated process. Make sure to plan for all the sources and loaders in advance.</li> 
   <li class="readable-text" id="p203">A variety of data loaders from LangChain can be used. </li> 
   <li class="readable-text" id="p204">Breaking down long pieces of text into manageable sizes is called <em>data splitting</em> or <em>chunking</em>.</li> 
   <li class="readable-text" id="p205">Chunking addresses context window limits of LLMs, mitigates the lost-in-the-middle problem for long prompts, and enables easier search and retrieval.</li> 
   <li class="readable-text" id="p206">The chunking process involves dividing longer texts into small units, merging small units into chunks, and including an overlap between chunks to preserve contextual continuity.</li> 
   <li class="readable-text" id="p207">Chunking can be fixed size, specialized (or adaptive), or semantic. Newer chunking methods are constantly being introduced.</li> 
   <li class="readable-text" id="p208">Your choice of the chunking strategy should be based on the nature of the content, expected length and complexity of user query, application use case, and the embeddings model being used. </li> 
   <li class="readable-text" id="p209">A chunking strategy can include multiple methods.</li> 
  </ul> 
  <div class="readable-text" id="p210"> 
   <h3 class=" readable-text-h3">Data conversion</h3> 
  </div> 
  <ul> 
   <li class="readable-text" id="p211">For processing, text needs to be converted into a numerical format.</li> 
   <li class="readable-text" id="p212">Embeddings are vector representations of data (words, sentences, documents, etc.).</li> 
   <li class="readable-text" id="p213">The goal of an embedding algorithm is to position similar data points close to each other in a vector space.</li> 
   <li class="readable-text" id="p214">Several pre-trained, open source and proprietary, embedding models are available for use.</li> 
   <li class="readable-text" id="p215">Embeddings models enable similarity search. Embeddings can be used for text search, clustering, ML models, and recommendation engines.</li> 
   <li class="readable-text" id="p216">The choice of embeddings is largely based on the use case and the cost implications.</li> 
   <li class="readable-text" id="p217">Vector databases are designed to efficiently store and retrieve high-dimensional vector data such as embeddings.</li> 
   <li class="readable-text" id="p218">Vector databases provide similarity searches based on distance metrics such as cosine similarity.</li> 
   <li class="readable-text" id="p219">Apart from the similarity search, vector databases offer traditional services such as scalability, security, versioning, and the like.</li> 
   <li class="readable-text" id="p220">Vector capabilities can be offered by standalone vector indexes, specialized vector databases, or legacy offerings such as search platforms, SQL, and NoSQL databases with added vector capabilities.</li> 
   <li class="readable-text" id="p221">Accuracy, speed, flexibility, storage, performance, simplicity, access, and cost are some of the factors that can influence the choice of a vector database.</li> 
  </ul>
 </body>
</html>