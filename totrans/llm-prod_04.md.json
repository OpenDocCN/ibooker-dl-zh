["```py\n$ gcloud init\n```", "```py\n$ gcloud compute images list --project deeplearning-platform-release \n    --format=”value(NAME)” --no-standard-images\n```", "```py\n$ INSTANCE_NAME=\"g2-llminprod-example\"\n    $ gcloud compute instances create ${INSTANCE_NAME} --zone=us-west1-a \n    --machine-type=g2-standard-24 --image-project=deeplearning-platform-release\n    --image=common-gpu-v20230925-debian-11-py310 --boot-disk-size=200GB --scopes\n    cloud-platform --metadata=install-unattended-upgrades=False,install-nvidia-\n    driver=True --maintenance-policy TERMINATE --restart-on-failure\n```", "```py\n$ gcloud compute instances describe ${INSTANCE_NAME}\n```", "```py\n$ gcloud compute ssh ${INSTANCE_NAME}\n```", "```py\n$ gcloud compute scp requirements.txt ${INSTANCE_NAME}:~/requirements.txt\n$ gcloud compute scp --recurse ~/local-app-folder/ \n${INSTANCE_NAME}:~/vm-app-folder\n```", "```py\n$ gcloud compute instances delete ${INSTANCE_NAME} --quiet\n```", "```py\n$ gcloud compute config-ssh\n```", "```py\nimport os\nimport torch\nfrom accelerate import Accelerator\n\nimport bitsandbytes as bnb  \n\nclass GPT(torch.nn.Module):      #1\n    def __init__(self):\n        super().__init__()\n        self.token_embedding = torch.nn.Embedding(vocab_size, n_embed)\n        self.positional_embedding = torch.nn.Embedding(block_size, n_embed)\n        self.blocks = torch.nn.Sequential(\n            *[Block(n_embed, n_head=n_head) for _ in range(n_layer)]\n        )\n        self.ln_f = torch.nn.LayerNorm(n_embed)\n        self.lm_head = torch.nn.Linear(n_embed, vocab_size)\n\n        self.apply(self._init_weights)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        tok_emb = self.token_embedding(idx)\n        pos_emb = self.positional_embedding(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = torch.nn.functional.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def _init_weights(self, module):\n        if isinstance(module, torch.nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, torch.nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = torch.nn.functional.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\nclass Block(torch.nn.Module):          #2\n    def __init__(self, n_embed, n_head):\n        super().__init__()\n        head_size = n_embed // n_head\n        self.self_attention = MultiHeadAttention(n_head, head_size)\n        self.feed_forward = FeedFoward(n_embed)\n        self.ln1 = torch.nn.LayerNorm(n_embed)\n        self.ln2 = torch.nn.LayerNorm(n_embed)\n\n    def forward(self, x):\n        x = x + self.self_attention(self.ln1(x))\n        x = x + self.feed_forward(self.ln2(x))\n        return x\n\nclass MultiHeadAttention(torch.nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = torch.nn.ModuleList(\n            [Head(head_size) for _ in range(num_heads)]\n        )\n        self.projection = torch.nn.Linear(head_size * num_heads, n_embed)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.projection(out))\n        return out\n\nclass Head(torch.nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = torch.nn.Linear(n_embed, head_size, bias=False)\n        self.query = torch.nn.Linear(n_embed, head_size, bias=False)\n        self.value = torch.nn.Linear(n_embed, head_size, bias=False)\n        self.register_buffer(\n            \"tril\", torch.tril(torch.ones(block_size, block_size))\n        )\n\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x):\n        _, T, _ = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        attention = q @ k.transpose(-2, -1) * k.shape[-1] ** 0.5\n        attention = attention.masked_fill(\n            self.tril[:T, :T] == 0, float(\"-inf\")\n        )\n        attention = torch.nn.functional.softmax(attention, dim=-1)\n        attention = self.dropout(attention)\n\n        v = self.value(x)\n        out = attention @ v\n        return out\n\nclass FeedFoward(torch.nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n        self.net = torch.nn.Sequential(\n            torch.nn.Linear(n_embed, 4 * n_embed),\n            torch.nn.ReLU(),\n            torch.nn.Linear(4 * n_embed, n_embed),\n            torch.nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\ndef encode(string):      #3\n    return [utt2int[c] for c in string]\n\ndef decode(line):\n    return \"\".join([int2utt[i] for i in line])\n\ndef get_batch(split):\n    data = train_data if split == \"train\" else val_data\n    idx = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i : i + block_size] for i in idx])\n    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in [\"train\", \"val\"]:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nif __name__ == \"__main__\":       #4\n    batch_size = 64  # Number of utterances at once      #5\n    block_size = 256  # Maximum context window size\n    max_iters = 5000\n    eval_interval = 500\n    learning_rate = 3e-4\n    eval_iters = 200\n    n_embed = 384\n    n_head = 6\n    n_layer = 6\n    dropout = 0.2\n    accelerator = Accelerator()\n    device = accelerator.device\n    doing_quantization = False  # Change to True if imported bitsandbytes\n\n    with open(\"./data/crimeandpunishment.txt\", \"r\", encoding=\"utf-8\") as f:  #6\n        text = f.read()\n\n    chars = sorted(list(set(text)))     #7\n    vocab_size = len(chars)\n    utt2int = {ch: i for i, ch in enumerate(chars)}\n    int2utt = {i: ch for i, ch in enumerate(chars)}\n\n    data = torch.tensor(encode(text), dtype=torch.long)\n    n = int(0.9 * len(data))\n    train_data = data[:n]\n    val_data = data[n:]\n\n    model = GPT().to(device)          #8\n    print(\"Instantiated Model\")\n    print(\n        sum(param.numel() for param in model.parameters()) / 1e6,\n        \"Model parameters\",\n    )\n\n    optimizer = (\n        torch.optim.AdamW(model.parameters(), lr=learning_rate)\n        if not doing_quantization\n        else bnb.optim.Adam(model.parameters(), lr=learning_rate)\n    )\n    print(\"Instantiated Optimizer\")\n\n    model, optimizer, train_data = accelerator.prepare(\n        model, optimizer, train_data\n    )\n    print(\"Prepared model, optimizer, and data\")\n\n    # \n    for iter in range(max_iters):     #9\n        print(f\"Running Epoch {iter}\")\n        if iter % eval_interval == 0 or iter == max_iters - 1:\n            losses = estimate_loss()\n            print(\n                f\"| step {iter}: train loss {losses['train']:.4f} \"\n                \"| validation loss {losses['val']:.4f} |\"\n            )\n\n        xb, yb = get_batch(\"train\")\n        logits, loss = model(xb, yb)\n        optimizer.zero_grad(set_to_none=True)\n        accelerator.backward(loss)\n        optimizer.step()\n\n    model_dir = \"./models/scratchGPT/\"      #10\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_path = model_dir + \"model.pt\"    #11\n    torch.save(\n        model.state_dict(),\n        model_path,\n    )\n\n    loaded = GPT().load_state_dict(model_path)      #12\n\n    context = torch.zeros((1, 1), dtype=torch.long, device=device)    #13\n    print(decode(loaded.generate(context, max_new_tokens=500)[0].tolist()))\n```", "```py\nimport os\nfrom transformers import (\n    GPT2Tokenizer,\n    GPT2LMHeadModel,\n    GPT2Config,\n    DataCollatorForLanguageModeling,\n    TrainingArguments,\n    Trainer,\n)\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"text\", data_files=\"./data/crimeandpunishment.txt\")  #1\ndataset = dataset.filter(lambda sentence: len(sentence[\"text\"]) > 1)\nprint(dataset[\"train\"][0])\n\nmodel_dir = \"./models/betterGPT/\"    #2\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nconfig = GPT2Config(       #3\n    vocab_size=50261,\n    n_positions=256,\n    n_embd=768,\n    activation_function=\"gelu\",\n)\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")    #4\nspecial_tokens_dict = {\n    \"bos_token\": \"<BOS>\",\n    \"eos_token\": \"<EOS>\",\n    \"pad_token\": \"<PAD>\",\n    \"mask_token\": \"<MASK>\",\n}\ntokenizer.add_special_tokens(special_tokens_dict)\n\nmodel = GPT2LMHeadModel.from_pretrained(                 #5\n    \"gpt2\", config=config, ignore_mismatched_sizes=True\n)\n\ndef tokenize(batch):     #6\n    return tokenizer(\n        str(batch), padding=\"max_length\", truncation=True, max_length=256\n    )\n\ntokenized_dataset = dataset.map(tokenize, batched=False)    #7\nprint(f\"Tokenized: {tokenized_dataset['train'][0]}\")\n\ndata_collator = DataCollatorForLanguageModeling(       #8\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)  # Masked Language Modeling - adds <MASK> tokens to guess the words\n\ntrain_args = TrainingArguments(      #9\n    output_dir=model_dir,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    save_steps=5000,\n    save_total_limit=2,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(      #10\n    model=model,\n    args=train_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n)\n\ntrainer.train()                #11\ntrainer.save_model(model_dir)\ntokenizer.save_pretrained()\n\nmodel = GPT2LMHeadModel.from_pretrained(model_dir)   #12\ninput = \"To be or not\"                                    #13\ntokenized_inputs = tokenizer(input, return_tensors=\"pt\")\nout = model.generate(\n    input_ids=tokenized_inputs[\"input_ids\"],\n    attention_mask=tokenized_inputs[\"attention_mask\"],\n    max_length=256,\n    num_beams=5,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.90,\n    no_repeat_ngram_size=2,\n)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))\n```", "```py\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI()\nclient.api_key = os.getenv(\"OPENAI_API_KEY\")\nclient.files.create(\n  file=open(\"mydata.jsonl\", \"rb\"),\n  purpose='fine-tune'\n)\n```", "```py\nclient.fine_tuning.jobs.create(training_file=\"file-abc123\", model=\"gpt-3.5-turbo\")\n```", "```py\ncompletion = client.chat.completion.create(\n  model=\"ft:gpt-3.5-turbo:my-org:custom_suffix:id\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\nprint(completion.choices[0].message)\n```", "```py\nimport os\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    default_data_collator,\n    get_linear_schedule_with_warmup,\n)\nfrom peft import (\n    get_peft_model,\n    PromptTuningInit,\n    PromptTuningConfig,\n    TaskType,\n)\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef preprocess_function(examples):          #1\n    batch_size = len(examples[text_column])\n    inputs = [\n        f\"{text_column} : {x} Label : \" for x in examples[text_column]\n    ]\n    targets = [str(x) for x in examples[label_column]]\n    model_inputs = tokenizer(inputs)\n    labels = tokenizer(targets)\n\n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n        labels[\"input_ids\"][i] = [-100] * len(\n            sample_input_ids\n        ) + label_input_ids\n        model_inputs[\"attention_mask\"][i] = [1] * len(\n            model_inputs[\"input_ids\"][i]\n        )\n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i]\n        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n            max_length - len(sample_input_ids)\n        ) + sample_input_ids\n        model_inputs[\"attention_mask\"][i] = [0] * (\n            max_length - len(sample_input_ids)\n        ) + model_inputs[\"attention_mask\"][i]\n        labels[\"input_ids\"][i] = [-100] * (\n            max_length - len(sample_input_ids)\n        ) + label_input_ids\n        model_inputs[\"input_ids\"][i] = torch.tensor(\n            model_inputs[\"input_ids\"][i][:max_length]\n        )\n        model_inputs[\"attention_mask\"][i] = torch.tensor(\n            model_inputs[\"attention_mask\"][i][:max_length]\n        )\n        labels[\"input_ids\"][i] = torch.tensor(\n            labels[\"input_ids\"][i][:max_length]\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nif __name__ == \"__main__\":         #2\n    # Define training parameters\n    device = \"cuda\"\n    model_name_or_path = \"bigscience/bloomz-560m\"\n    tokenizer_name_or_path = \"bigscience/bloomz-560m\"\n    dataset_name = \"twitter_complaints\"\n    text_column = \"Tweet text\"\n    label_column = \"text_label\"\n    max_length = 64\n    lr = 3e-2\n    num_epochs = 1\n    batch_size = 8\n\n    peft_config = PromptTuningConfig(      #3\n        task_type=TaskType.CAUSAL_LM,\n        prompt_tuning_init=PromptTuningInit.TEXT,\n        num_virtual_tokens=8,\n        prompt_tuning_init_text=\"Classify if the tweet \"\n        \"is a complaint or not:\",\n        tokenizer_name_or_path=model_name_or_path,\n    )\n    checkpoint_name = (\n        f\"{dataset_name}_{model_name_or_path}\"\n        f\"_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n            \"/\", \"_\"\n        )\n    )\n    dataset = load_dataset(\"ought/raft\", dataset_name)    #4\n    print(f\"Dataset 1: {dataset['train'][0]}\")\n\n    classes = [          #5\n        label.replace(\"_\", \" \")\n        for label in dataset[\"train\"].features[\"Label\"].names\n    ]\n    dataset = dataset.map(\n        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n        batched=True,\n        num_proc=1,\n    )\n    print(f\"Dataset 2: {dataset['train'][0]}\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)    #6\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    target_max_length = max(\n        [\n            len(tokenizer(class_label)[\"input_ids\"])\n            for class_label in classes\n        ]\n    )\n    print(f\"Target Max Length: {target_max_length}\")\n\n    processed_datasets = dataset.map(           #7\n        preprocess_function,\n        batched=True,\n        num_proc=1,\n        remove_columns=dataset[\"train\"].column_names,\n        load_from_cache_file=False,\n        desc=\"Running tokenizer on dataset\",\n    )\n\n    train_dataset = processed_datasets[\"train\"]     #8\n    eval_dataset = processed_datasets[\"test\"]\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        shuffle=True,\n        collate_fn=default_data_collator,\n        batch_size=batch_size,\n        pin_memory=True,\n    )\n    eval_dataloader = DataLoader(\n        eval_dataset,\n        collate_fn=default_data_collator,\n        batch_size=batch_size,\n        pin_memory=True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)   #9\n    model = get_peft_model(model, peft_config)\n    print(model.print_trainable_parameters())\n    model = model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)    #10\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=(len(train_dataloader) * num_epochs),\n    )\n\n    for epoch in range(num_epochs):    #11\n        model.train()\n        total_loss = 0\n        for step, batch in enumerate(tqdm(train_dataloader)):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.detach().float()\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        model.eval()\n        eval_loss = 0\n        eval_preds = []\n        for step, batch in enumerate(tqdm(eval_dataloader)):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.no_grad():\n                outputs = model(**batch)\n            loss = outputs.loss\n            eval_loss += loss.detach().float()\n            eval_preds.extend(\n                tokenizer.batch_decode(\n                    torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n                    skip_special_tokens=True,\n                )\n            )\n\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n        eval_ppl = torch.exp(eval_epoch_loss)\n        train_epoch_loss = total_loss / len(train_dataloader)\n        train_ppl = torch.exp(train_epoch_loss)\n        print(\n            f\"{epoch=}: {train_ppl=} {train_epoch_loss=} \"\n            f\"{eval_ppl=} {eval_epoch_loss=}\"\n        )\n\n    model_dir = \"./models/PromptTunedPEFT\"    #12\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    tokenizer.save_pretrained(model_dir)     #13\n    model.save_pretrained(model_dir)\n\n    with torch.no_grad():      #14\n        inputs = tokenizer(\n            f'{text_column} : {{\"@nationalgridus I have no water and '\n            \"the bill is current and paid. Can you do something about \"\n            'this?\"}} Label : ',\n            return_tensors=\"pt\",\n        )\n\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_new_tokens=10,\n            eos_token_id=3,\n        )\n        print(\n            tokenizer.batch_decode(\n                outputs.detach().cpu().numpy(), skip_special_tokens=True\n            )\n        )\n```", "```py\nimport os\nfrom transformers import (\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n)\nfrom datasets import load_dataset, load_metric\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\ndef process(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"sentence\"], truncation=True, max_length=256\n    )\n    return tokenized_inputs\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    acc = accuracy_metric.compute(\n        predictions=predictions, references=labels\n    )\n    return {\n        \"accuracy\": acc[\"accuracy\"],\n    }\n\nclass DistillationTrainingArguments(TrainingArguments):\n    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alpha = alpha\n        self.temperature = temperature\n\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher = teacher_model\n        self._move_model_to_device(self.teacher, self.model.device)   #1\n        self.teacher.eval()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs_student = model(**inputs)      #2\n        student_loss = outputs_student.loss\n        with torch.no_grad():                         #3\n            outputs_teacher = self.teacher(**inputs)\n\n        assert (                  #4\n            outputs_student.logits.size() == outputs_teacher.logits.size()\n        )\n\n        # Soften probabilities and compute distillation loss\n        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n        loss_logits = loss_function(\n            F.log_softmax(\n                outputs_student.logits / self.args.temperature, dim=-1\n            ),\n            F.softmax(\n                outputs_teacher.logits / self.args.temperature, dim=-1\n            ),\n        ) * (self.args.temperature**2)\n        loss = (                               #5\n            self.args.alpha * student_loss\n            + (1.0 - self.args.alpha) * loss_logits\n        )\n        return (loss, outputs_student) if return_outputs else loss\n\nif __name__ == \"__main__\":\n    model_dir = \"./models/KDGPT/\"          #6\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    student_id = \"gpt2\"            #7\n    teacher_id = \"gpt2-medium\"\n\n    teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_id)\n    student_tokenizer = AutoTokenizer.from_pretrained(student_id)\n\n    sample = \"Here's our sanity check.\"\n\n    assert teacher_tokenizer(sample) == student_tokenizer(sample), (\n        \"Tokenizers need to have the same output! \"\n        f\"{teacher_tokenizer(sample)} != {student_tokenizer(sample)}\"\n    )\n    del teacher_tokenizer\n    del student_tokenizer\n\n    tokenizer = AutoTokenizer.from_pretrained(teacher_id)\n    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n\n    dataset_id = \"glue\"\n    dataset_config = \"sst2\"\n    dataset = load_dataset(dataset_id, dataset_config)\n\n    tokenized_dataset = dataset.map(process, batched=True)\n    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n\n    print(tokenized_dataset[\"test\"].features)\n\n    labels = tokenized_dataset[\"train\"].features[\"labels\"].names    #8\n    num_labels = len(labels)\n    label2id, id2label = dict(), dict()\n    for i, label in enumerate(labels):\n        label2id[label] = str(i)\n        id2label[str(i)] = label\n\n    training_args = DistillationTrainingArguments(    #9\n        output_dir=model_dir,\n        num_train_epochs=1,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        fp16=True,\n        learning_rate=6e-5,\n        seed=8855,\n        Evaluation strategies\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        report_to=\"none\",\n        push_to_hub=False,       #10\n        alpha=0.5,            #11\n        temperature=4.0,\n    )\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)    #12\n\n    teacher_model = AutoModelForSequenceClassification.from_pretrained(   #13\n        teacher_id,\n        num_labels=num_labels,\n        id2label=id2label,\n        label2id=label2id,\n    )\n\n    student_model = AutoModelForSequenceClassification.from_pretrained(   #14\n        student_id,\n        num_labels=num_labels,\n        id2label=id2label,\n        label2id=label2id,\n    )\n    accuracy_metric = load_metric(\"accuracy\")    #15\n\n    trainer = DistillationTrainer(\n        student_model,\n        training_args,\n        teacher_model=teacher_model,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"validation\"],\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    trainer.train()\n\n    trainer.save_model(model_dir)\n```", "```py\nimport os\nfrom transformers import (\n    AutoTokenizer,\n    SwitchTransformersForConditionalGeneration,\n    SwitchTransformersConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n)\nfrom datasets import load_dataset\nimport torch\n\ndataset = load_dataset(\"text\", data_files=\"./data/crimeandpunishment.txt\")  #1\ndataset = dataset.filter(lambda sentence: len(sentence[\"text\"]) > 1)\nprint(f\"Dataset 1: {dataset['train'][0]}\")\n\nmodel_dir = \"./models/MoE/\"          #2\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")    #3\n\nconfig = SwitchTransformersConfig(                   #4\n    decoder_start_token_id=tokenizer.pad_token_id\n)\n\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(   #5\n    \"google/switch-base-8\",\n    config=config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n)\n\ndef tokenize(batch):     #6\n    return tokenizer(\n        str(batch), padding=\"max_length\", truncation=True, max_length=256\n    )\n\ntokenized_dataset = dataset.map(tokenize, batched=False)        #7\nprint(f\"Tokenized: {tokenized_dataset['train'][0]}\")\n\ndata_collator = DataCollatorForLanguageModeling(            #8\n    tokenizer=tokenizer, mlm=False, mlm_probability=0.0\n)  # Causal Language Modeling - Does not use mask\n\ntrain_args = TrainingArguments(    #9\n    output_dir=model_dir,\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    save_steps=5000,\n    save_total_limit=2,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(          #10\n    model=model,\n    args=train_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n)\n\ntrainer.train()                  #11\ntrainer.save_model(model_dir)\ntokenizer.save_pretrained(model_dir)\n\nmodel = SwitchTransformersForConditionalGeneration.from_pretrained(   #12\n    model_dir,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n)\n\ninput = \"To be or not <extra_id_0> <extra_id_0>\"       #13\ntokenized_inputs = tokenizer(input, return_tensors=\"pt\")\nout = model.generate(\n    input_ids=tokenized_inputs[\"input_ids\"].to(\"cuda\"),\n    attention_mask=tokenized_inputs[\"attention_mask\"],\n    max_length=256,\n    num_beams=5,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.90,\n    no_repeat_ngram_size=2,\n)\nprint(f\"To be or not {tokenizer.decode(out[0], skip_special_tokens=True)}\")\n```", "```py\nimport os\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    TrainingArguments,\n    Trainer,\n)\nfrom peft import (\n    PeftModel,\n    PeftConfig,\n    get_peft_model,\n    LoraConfig,\n    TaskType,\n)\nimport evaluate\nimport torch\nimport numpy as np\n\nmodel_checkpoint = \"meta-llama/Llama-2-7b-hf\"\nlr = 1e-3\nbatch_size = 16\nnum_epochs = 10\n\nmodel_dir = \"./models/LoRAPEFT\"     #1\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\nbionlp = load_dataset(\"tner/bionlp2004\")\n\nseqeval = evaluate.load(\"seqeval\")\n\nlabel_list = [\n    \"O\",\n    \"B-DNA\",\n    \"I-DNA\",\n    \"B-protein\",\n    \"I-protein\",\n    \"B-cell_type\",\n    \"I-cell_type\",\n    \"B-cell_line\",\n    \"I-cell_line\",\n    \"B-RNA\",\n    \"I-RNA\",\n]\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = seqeval.compute(\n        predictions=true_predictions, references=true_labels\n    )\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_checkpoint, add_prefix_space=True\n)\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    labels = []\n    for i, label in enumerate(examples[\"tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_bionlp = bionlp.map(tokenize_and_align_labels, batched=True)\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\nid2label = {\n    0: \"O\",\n    1: \"B-DNA\",\n    2: \"I-DNA\",\n    3: \"B-protein\",\n    4: \"I-protein\",\n    5: \"B-cell_type\",\n    6: \"I-cell_type\",\n    7: \"B-cell_line\",\n    8: \"I-cell_line\",\n    9: \"B-RNA\",\n    10: \"I-RNA\",\n}\nlabel2id = {\n    \"O\": 0,\n    \"B-DNA\": 1,\n    \"I-DNA\": 2,\n    \"B-protein\": 3,\n    \"I-protein\": 4,\n    \"B-cell_type\": 5,\n    \"I-cell_type\": 6,\n    \"B-cell_line\": 7,\n    \"I-cell_line\": 8,\n    \"B-RNA\": 9,\n    \"I-RNA\": 10,\n}\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_checkpoint, num_labels=11, id2label=id2label, label2id=label2id\n)\n\npeft_config = LoraConfig(\n    task_type=TaskType.TOKEN_CLS,\n    inference_mode=False,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias=\"all\",\n)\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\ntraining_args = TrainingArguments(\n    output_dir=model_dir,\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_bionlp[\"train\"],\n    eval_dataset=tokenized_bionlp[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\npeft_model_id = \"stevhliu/roberta-large-lora-token-classification\"\nconfig = PeftConfig.from_pretrained(model_dir)\ninference_model = AutoModelForTokenClassification.from_pretrained(\n    config.base_model_name_or_path,\n    num_labels=11,\n    id2label=id2label,\n    label2id=label2id,\n)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\nmodel = PeftModel.from_pretrained(inference_model, peft_model_id)\n\ntext = (\n    \"The activation of IL-2 gene expression and NF-kappa B through CD28 \"\n    \"requires reactive oxygen production by 5-lipoxygenase.\"\n)\ninputs = tokenizer(text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\ntokens = inputs.tokens()\npredictions = torch.argmax(logits, dim=2)\n\nfor token, prediction in zip(tokens, predictions[0].numpy()):\n    print((token, model.config.id2label[prediction]))\n```"]