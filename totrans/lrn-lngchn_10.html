<html><head></head><body><section data-pdf-bookmark="Chapter 10. Testing: Evaluation, Monitoring, and Continuous Improvement" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch10_testing_evaluation_monitoring_and_continuous_im_1736545678108525">
<h1><span class="label">Chapter 10. </span>Testing: Evaluation, Monitoring, and Continuous Improvement</h1>

<p>In <a data-type="xref" href="ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604">Chapter 9</a>, you learned how to deploy your AI application into production and utilize LangGraph Platform to host and debug your app.</p>

<p>Although your app can respond to user inputs and execute complex tasks, its underlying LLM is nondeterministic and prone to<a contenteditable="false" data-primary="hallucinations" data-type="indexterm" id="id882"/> hallucination. As discussed in previous chapters, LLMs can generate inaccurate and outdated outputs due to a variety of reasons including the prompt, format of user’s input, and retrieved context. In addition, harmful or misleading LLM outputs can significantly damage a company’s brand and customer loyalty.</p>

<p>To combat this tendency toward hallucination, you need to build an efficient system to test, evaluate, monitor, and continuously improve your LLM applications’ performance. This robust testing process will enable you to quickly debug and fix AI-related issues before and after your app is in production.</p>

<p>In this chapter, you’ll learn how to build an iterative testing system across the key stages of the LLM app development life-cycle and maintain high performance of your application.</p>

<section class="pagebreak-before" data-pdf-bookmark="Testing Techniques Across the LLM App Development Cycle" data-type="sect1"><div class="sect1" id="ch10_testing_techniques_across_the_llm_app_development_1736545678108791">
<h1 class="less_space">Testing Techniques Across the LLM App <span class="keep-together">Development Cycle</span></h1>

<p>Before<a contenteditable="false" data-primary="testing" data-secondary="key stages of app development" data-type="indexterm" id="id883"/><a contenteditable="false" data-primary="LLM applications" data-secondary="key stages of app development" data-type="indexterm" id="id884"/> we construct the testing system, let’s briefly review how testing can be applied across the three key stages of LLM app development:</p>

<dl>
	<dt>Design</dt>
	<dd>
	<p>In this stage, LLM tests are applied directly to your application. These tests can be assertions executed at runtime that feed failures back to the LLM for self-correction. The purpose of testing at this stage is error handling within your app before it affects users.</p>
	</dd>
	<dt>Preproduction</dt>
	<dd>
	<p>In this stage, tests are run right before deployment into production. The purpose of testing at this stage is to catch and fix any regressions before the app is released to real users.</p>
	</dd>
	<dt>Production</dt>
	<dd>
	<p>In this stage, tests are run while your application is in production to help monitor and catch errors affecting real users. The purpose is to identify issues and feed them back into the design or preproduction phases.</p>
	</dd>
</dl>

<p>The combination of testing across these stages creates a<a contenteditable="false" data-primary="continuous improvement cycle" data-type="indexterm" id="id885"/> continuous improvement cycle where these steps are repeated: design, test, deploy, monitor, fix, and redesign. See <a data-type="xref" href="#ch10_figure_1_1736545678095728">Figure 10-1</a>.</p>

<figure><div class="figure" id="ch10_figure_1_1736545678095728"><img alt="A diagram of a process  Description automatically generated" src="assets/lelc_1001.png"/>
<h6><span class="label">Figure 10-1. </span>The three key stages of the LLM app development cycle</h6>
</div></figure>

<p>In essence, this cycle helps you to identify and fix production issues in an efficient and quick manner.</p>

<p>Let’s dive deeper into testing techniques across each of these stages.</p>
</div></section>

<section data-pdf-bookmark="The Design Stage: Self-Corrective RAG" data-type="sect1"><div class="sect1" id="ch10_the_design_stage_self_corrective_rag_1736545678108876">
<h1>The Design Stage: Self-Corrective RAG</h1>

<p>As<a contenteditable="false" data-primary="testing" data-secondary="design stage" data-type="indexterm" id="Tdesign10"/><a contenteditable="false" data-primary="design stage, testing during" data-type="indexterm" id="dstest10"/><a contenteditable="false" data-primary="retrieval-augmented generation (RAG)" data-secondary="self-corrective RAG" data-type="indexterm" id="RAGself10"/> discussed previously, your application can incorporate error handling at runtime that feeds errors to the LLM for self-correction. Let’s explore a RAG use case using LangGraph as the framework to orchestrate<a contenteditable="false" data-primary="error handling" data-type="indexterm" id="id886"/> error handling.</p>

<p>Basic RAG-driven AI applications are prone to hallucination due to inaccurate or incomplete retrieval of relevant context to generate outputs. But you can utilize an LLM to grade retrieval relevance and fix hallucination issues.</p>

<p>LangGraph enables you to effectively implement the<a contenteditable="false" data-primary="LangGraph" data-secondary="self-corrective RAG" data-type="indexterm" id="LGself10"/> control flow of this process, as shown in <a data-type="xref" href="#ch10_figure_2_1736545678095764">Figure 10-2</a>.</p>

<figure><div class="figure" id="ch10_figure_2_1736545678095764"><img alt="A diagram of a diagram  Description automatically generated" src="assets/lelc_1002.png"/>
<h6><span class="label">Figure 10-2. </span>Self-corrective RAG control flow</h6>
</div></figure>

<p>The control flow steps are as follows:</p>

<ol>
	<li>
	<p>In the routing step, each question is routed to the relevant retrieval method, that is, vector store and web search.</p>
	</li>
	<li>
	<p>If, for example, the question is routed to a vector store for retrieval, the LLM in the control flow will retrieve and grade the documents for relevancy.</p>
	</li>
	<li>
	<p>If the document is relevant, the LLM proceeds to generate an answer.</p>
	</li>
	<li>
	<p>The LLM will check the answer for hallucinations and only proceed to display the answer to the user if the output is accurate and relevant.</p>
	</li>
	<li>
	<p>As a fallback, if the retrieved document is irrelevant or the generated answer doesn’t answer the user’s question, the flow utilizes web search to retrieve relevant information as context.</p>
	</li>
</ol>

<p>This process enables your app to iteratively generate answers, self-correct errors<a contenteditable="false" data-primary="self-correction" data-type="indexterm" id="id887"/> and hallucinations, and improve the quality of outputs.</p>

<p>Let’s run through an example code implementation of this control flow. First, download the required packages and initialize relevant API keys. For these examples, you’ll need to set your OpenAI and LangSmith API keys as environment variables.</p>


<p>First, we’ll create an index of three blog posts:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">langchain.text_splitter</code> <code class="kn">import</code> <code class="n">RecursiveCharacterTextSplitter</code>
<code class="kn">from</code> <code class="nn">langchain_community.document_loaders</code> <code class="kn">import</code> <code class="n">WebBaseLoader</code>
<code class="kn">from</code> <code class="nn">langchain_community.vectorstores</code> <code class="kn">import</code> <code class="n">InMemoryVectorStore</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">OpenAIEmbeddings</code>
<code class="kn">from</code> <code class="nn">langchain_core.prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>
<code class="kn">from</code> <code class="nn">pydantic</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
    
    
<code class="c1"># --- Create an index of documents ---</code>
    
<code class="n">urls</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s2">"https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/"</code><code class="p">,</code>
    <code class="s2">"https://blog.langchain.dev/langchain-state-of-ai-2024/"</code><code class="p">,</code>
    <code class="s2">"https://blog.langchain.dev/introducing-ambient-agents/"</code><code class="p">,</code>
<code class="p">]</code>
    
<code class="n">docs</code> <code class="o">=</code> <code class="p">[</code><code class="n">WebBaseLoader</code><code class="p">(</code><code class="n">url</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">()</code> <code class="k">for</code> <code class="n">url</code> <code class="ow">in</code> <code class="n">urls</code><code class="p">]</code>
<code class="n">docs_list</code> <code class="o">=</code> <code class="p">[</code><code class="n">item</code> <code class="k">for</code> <code class="n">sublist</code> <code class="ow">in</code> <code class="n">docs</code> <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">sublist</code><code class="p">]</code>
    
<code class="n">text_splitter</code> <code class="o">=</code> <code class="n">RecursiveCharacterTextSplitter</code><code class="o">.</code><code class="n">from_tiktoken_encoder</code><code class="p">(</code>
    <code class="n">chunk_size</code><code class="o">=</code><code class="mi">250</code><code class="p">,</code> <code class="n">chunk_overlap</code><code class="o">=</code><code class="mi">0</code>
<code class="p">)</code>
<code class="n">doc_splits</code> <code class="o">=</code> <code class="n">text_splitter</code><code class="o">.</code><code class="n">split_documents</code><code class="p">(</code><code class="n">docs_list</code><code class="p">)</code>
    
<code class="c1"># Add to vectorDB</code>
<code class="n">vectorstore</code> <code class="o">=</code> <code class="n">InMemoryVectorStore</code><code class="o">.</code><code class="n">from_documents</code><code class="p">(</code>
    <code class="n">documents</code><code class="o">=</code><code class="n">doc_splits</code><code class="p">,</code>
    <code class="n">embedding</code><code class="o">=</code><code class="n">OpenAIEmbeddings</code><code class="p">(),</code>
<code class="p">)</code>
<code class="n">retriever</code> <code class="o">=</code> <code class="n">vectorstore</code><code class="o">.</code><code class="n">as_retriever</code><code class="p">()</code>
    
<code class="c1"># Retrieve the relevant documents</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code>
    <code class="s2">"What are 2 LangGraph agents used in production in 2024?"</code><code class="p">)</code>
    
<code class="nb">print</code><code class="p">(</code><code class="s2">"Results: </code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">results</code><code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code> <code class="nx">RecursiveCharacterTextSplitter</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/textsplitters'</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code>
  <code class="nx">CheerioWebBaseLoader</code>
<code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/community/document_loaders/web/cheerio"</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> 
  <code class="nx">InMemoryVectorStore</code> 
<code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/community/vectorstores/in_memory'</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">OpenAIEmbeddings</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatPromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/core/prompts'</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">z</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'zod'</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'@langchain/openai'</code><code class="p">;</code>
    
<code class="kr">const</code> <code class="nx">urls</code> <code class="o">=</code> <code class="p">[</code>
  <code class="s1">'https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/'</code><code class="p">,</code>
  <code class="s1">'https://blog.langchain.dev/langchain-state-of-ai-2024/'</code><code class="p">,</code>
  <code class="s1">'https://blog.langchain.dev/introducing-ambient-agents/'</code><code class="p">,</code>
<code class="p">];</code>
    
<code class="c1">// Load documents from URLs</code>
<code class="kr">const</code> <code class="nx">loadDocs</code> <code class="o">=</code> <code class="nx">async</code> <code class="p">(</code><code class="nx">urls</code><code class="p">)</code> <code class="o">=&gt;</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="p">[];</code>
  <code class="k">for</code> <code class="p">(</code><code class="kr">const</code> <code class="nx">url</code> <code class="k">of</code> <code class="nx">urls</code><code class="p">)</code> <code class="p">{</code>
    <code class="kr">const</code> <code class="nx">loader</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">CheerioWebBaseLoader</code><code class="p">(</code><code class="nx">url</code><code class="p">);</code>
    <code class="kr">const</code> <code class="nx">loadedDocs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loader</code><code class="p">.</code><code class="nx">load</code><code class="p">();</code>
    <code class="nx">docs</code><code class="p">.</code><code class="nx">push</code><code class="p">(...</code><code class="nx">loadedDocs</code><code class="p">);</code>
  <code class="p">}</code>
  <code class="k">return</code> <code class="nx">docs</code><code class="p">;</code>
<code class="p">};</code>
    
<code class="kr">const</code> <code class="nx">docsList</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">loadDocs</code><code class="p">(</code><code class="nx">urls</code><code class="p">);</code>
    
<code class="c1">// Initialize the text splitter</code>
<code class="kr">const</code> <code class="nx">textSplitter</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">RecursiveCharacterTextSplitter</code><code class="p">({</code>
  <code class="nx">chunkSize</code><code class="o">:</code> <code class="mi">250</code><code class="p">,</code>
  <code class="nx">chunkOverlap</code><code class="o">:</code> <code class="mi">0</code><code class="p">,</code>
<code class="p">});</code>
    
<code class="c1">// Split the documents into smaller chunks</code>
<code class="kr">const</code> <code class="nx">docSplits</code> <code class="o">=</code> <code class="nx">textSplitter</code><code class="p">.</code><code class="nx">splitDocuments</code><code class="p">(</code><code class="nx">docsList</code><code class="p">);</code>
    
<code class="c1">// Add to vector database</code>
<code class="kr">const</code> <code class="nx">vectorstore</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">InMemoryVectorStore</code><code class="p">.</code><code class="nx">fromDocuments</code><code class="p">(</code>
  <code class="nx">docSplits</code><code class="p">,</code>
  <code class="k">new</code> <code class="nx">OpenAIEmbeddings</code><code class="p">()</code>
<code class="p">);</code>
    
<code class="c1">// The `retriever` object can now be used for querying</code>
<code class="kr">const</code> <code class="nx">retriever</code> <code class="o">=</code> <code class="nx">vectorstore</code><code class="p">.</code><code class="nx">asRetriever</code><code class="p">();</code> 
    
<code class="kr">const</code> <code class="nx">question</code> <code class="o">=</code> <code class="s1">'What are 2 LangGraph agents used in production in 2024?'</code><code class="p">;</code>
    
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">question</code><code class="p">);</code>
    
<code class="nx">console</code><code class="p">.</code><code class="nx">log</code><code class="p">(</code><code class="s1">'Retrieved documents: \n'</code><code class="p">,</code> <code class="nx">docs</code><code class="p">[</code><code class="mi">0</code><code class="p">].</code><code class="nx">page_content</code><code class="p">);</code></pre>

<p>As discussed previously, the LLM will grade the relevancy of the retrieved documents from the index. We can construct this instruction in a system prompt:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">### Retrieval Grader</code>

<code class="kn">from</code> <code class="nn">langchain_core</code><code class="nn">.</code><code class="nn">prompts</code> <code class="kn">import</code> <code class="n">ChatPromptTemplate</code>
<code class="kn">from</code> <code class="nn">langchain_core</code><code class="nn">.</code><code class="nn">pydantic_v1</code> <code class="kn">import</code> <code class="n">BaseModel</code><code class="p">,</code> <code class="n">Field</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>


<code class="c1"># Data model</code>
<code class="k">class</code> <code class="nc">GradeDocuments</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">)</code><code class="p">:</code>
    <code class="sd">"""Binary score for relevance check on retrieved documents."""</code>

    <code class="n">binary_score</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">Field</code><code class="p">(</code>
        <code class="n">description</code><code class="o">=</code><code class="s2">"</code><code class="s2">Documents are relevant to the question, </code><code class="s2">'</code><code class="s2">yes</code><code class="s2">'</code><code class="s2"> or </code><code class="s2">'</code><code class="s2">no</code><code class="s2">'</code><code class="s2">"</code>
    <code class="p">)</code>


<code class="c1"># LLM with function call</code>
<code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"</code><code class="s2">gpt-3.5-turbo</code><code class="s2">"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">structured_llm_grader</code> <code class="o">=</code> <code class="n">llm</code><code class="o">.</code><code class="n">with_structured_output</code><code class="p">(</code><code class="n">GradeDocuments</code><code class="p">)</code>

<code class="c1"># Prompt</code>
<code class="n">system</code> <code class="o">=</code> <code class="s2">"""</code><code class="s2">You are a grader assessing relevance of a retrieved document to a</code>
<code class="s2">    user question.  </code>
<code class="s2">    If the document contains keyword(s) or semantic meaning related to the </code>
<code class="s2">    question, grade it as relevant. </code>
<code class="s2">    Give a binary score </code><code class="s2">'</code><code class="s2">yes</code><code class="s2">'</code><code class="s2"> or </code><code class="s2">'</code><code class="s2">no</code><code class="s2">'</code><code class="s2"> to indicate whether the document is </code>
<code class="s2">    relevant to the question.</code><code class="s2">"""</code>
<code class="n">grade_prompt</code> <code class="o">=</code> <code class="n">ChatPromptTemplate</code><code class="o">.</code><code class="n">from_messages</code><code class="p">(</code>
    <code class="p">[</code>
        <code class="p">(</code><code class="s2">"</code><code class="s2">system</code><code class="s2">"</code><code class="p">,</code> <code class="n">system</code><code class="p">)</code><code class="p">,</code>
        <code class="p">(</code><code class="s2">"</code><code class="s2">human</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"""</code><code class="s2">Retrieved document: </code><code class="se">\n</code><code class="se">\n</code> <code class="si">{document}</code> <code class="se">\n</code><code class="se">\n</code><code class="s2"> User question: </code>
            <code class="si">{question}</code><code class="s2">"""</code><code class="p">)</code><code class="p">,</code>
    <code class="p">]</code>
<code class="p">)</code>

<code class="n">retrieval_grader</code> <code class="o">=</code> <code class="n">grade_prompt</code> <code class="o">|</code> <code class="n">structured_llm_grader</code>
<code class="n">question</code> <code class="o">=</code> <code class="s2">"</code><code class="s2">agent memory</code><code class="s2">"</code>
<code class="n">docs</code> <code class="o">=</code> <code class="n">retriever</code><code class="o">.</code><code class="n">get_relevant_documents</code><code class="p">(</code><code class="n">question</code><code class="p">)</code>
<code class="n">doc_txt</code> <code class="o">=</code> <code class="n">docs</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">page_content</code> <code class="c1"># as an example</code>
<code class="n">retrieval_grader</code><strong><code class="o">.</code></strong><code class="n">invoke</code><code class="p">(</code><code class="p">{</code><code class="s2">"</code><code class="s2">question</code><code class="s2">"</code><code class="p">:</code> <code class="n">question</code><code class="p">,</code> <code class="s2">"</code><code class="s2">document</code><code class="s2">"</code><code class="p">:</code> <code class="n">doc_txt</code><code class="p">}</code><code class="p">)</code></pre>

<p class="pagebreak-before less_space"><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatPromptTemplate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/core/prompts"</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">z</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"zod"</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"@langchain/openai"</code><code class="p">;</code>

<code class="c1">// Define the schema using Zod</code>
<code class="kr">const</code> <code class="nx">GradeDocumentsSchema</code> <code class="o">=</code> <code class="nx">z</code><code class="p">.</code><code class="nx">object</code><code class="p">({</code>
  <code class="nx">binary_score</code><code class="o">:</code> <code class="nx">z</code><code class="p">.</code><code class="nx">string</code><code class="p">().</code><code class="nx">describe</code><code class="p">(</code><code class="sb">`Documents are relevant to the question, </code>
<code class="sb">      'yes' or 'no'`</code><code class="p">),</code>
<code class="p">});</code>

<code class="c1">// Initialize LLM with structured output using Zod schema</code>
<code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code> <code class="nx">model</code><code class="o">:</code> <code class="s2">"gpt-3.5-turbo"</code><code class="p">,</code> <code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code> <code class="p">});</code>
<code class="kr">const</code> <code class="nx">structuredLLMGrader</code> <code class="o">=</code> <code class="nx">llm</code><code class="p">.</code><code class="nx">withStructuredOutput</code><code class="p">(</code><code class="nx">GradeDocumentsSchema</code><code class="p">);</code>

<code class="c1">// System and prompt template</code>
<code class="kr">const</code> <code class="nx">systemMessage</code> <code class="o">=</code> <code class="sb">`You are a grader assessing relevance of a retrieved </code>
<code class="sb">  document to a user question. </code>
<code class="sb">If the document contains keyword(s) or semantic meaning related to the </code>
<code class="sb">  question, grade it as relevant.</code>
<code class="sb">Give a binary score 'yes' or 'no' to indicate whether the document is relevant </code>
<code class="sb">  to the question.`</code><code class="p">;</code>

<code class="kr">const</code> <code class="nx">gradePrompt</code> <code class="o">=</code> <code class="nx">ChatPromptTemplate</code><code class="p">.</code><code class="nx">fromMessages</code><code class="p">([</code>
  <code class="p">{</code> <code class="nx">role</code><code class="o">:</code> <code class="s2">"system"</code><code class="p">,</code> <code class="nx">content</code><code class="o">:</code> <code class="nx">systemMessage</code> <code class="p">},</code>
  <code class="p">{</code>
    <code class="nx">role</code><code class="o">:</code> <code class="s2">"human"</code><code class="p">,</code>
    <code class="nx">content</code><code class="o">:</code> <code class="s2">"Retrieved document: \n\n {document} \n\n </code>
<code class="s2">      User question: {question}"</code><code class="p">,</code>
  <code class="p">},</code>
<code class="p">]);</code>

<code class="c1">// Combine prompt with the structured output</code>
<code class="kr">const</code> <code class="nx">retrievalGrader</code> <code class="o">=</code> <code class="nx">gradePrompt</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">structuredLLMGrader</code><code class="p">);</code>

<code class="kr">const</code> <code class="nx">question</code> <code class="o">=</code> <code class="s2">"agent memory"</code><code class="p">;</code>
<code class="kr">const</code> <code class="nx">docs</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">retriever</code><code class="p">.</code><code class="nx">getRelevantDocuments</code><code class="p">(</code><code class="nx">question</code><code class="p">);</code>

<code class="nx">await</code> <code class="nx">retrievalGrader</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>
  <code class="nx">question</code><code class="p">,</code>
  <code class="nb">document</code><code class="o">:</code> <code class="nx">docs</code><code class="p">[</code><code class="mi">1</code><code class="p">].</code><code class="nx">pageContent</code><code class="p">,</code>
<code class="p">});</code></pre>

<p><em>The output:</em></p>

<pre data-type="programlisting">
binary_score='yes'</pre>

<p>Notice the use of Pydantic/Zod to help model the binary decision output in a format that can be used to programmatically decide which node in the control flow to move toward.</p>

<p>In<a contenteditable="false" data-primary="LangSmith" data-secondary="trace results in" data-type="indexterm" id="id888"/> LangSmith, you can see a trace of the logic flow across the nodes discussed previously (see <a data-type="xref" href="#ch10_figure_3_1736545678095796">Figure 10-3</a>).</p>

<figure><div class="figure" id="ch10_figure_3_1736545678095796"><img alt="A screenshot of a chat  Description automatically generated" src="assets/lelc_1003.png"/>
<h6><span class="label">Figure 10-3. </span>LangSmith trace results</h6>
</div></figure>

<p>Let’s test to see what happens when the input question cannot be answered by the retrieved documents in the index.</p>

<p>First, utilize LangGraph to make it easier to construct, execute, and debug the full control flow. See the full graph definition in the book’s <a href="https://oreil.ly/v63Vr">GitHub repository</a>. Notice that we’ve added a <code>transform_query</code> node to help rewrite the input query in a format that web search can use to retrieve higher-quality results.</p>

<p>As a final step, we set up our web search tool and execute the graph using the out-of-context question. The LangSmith trace shows that the web search tool was used as a fallback to retrieve relevant information prior to the final LLM generated answer (see <a data-type="xref" href="#ch10_figure_4_1736545678095829">Figure 10-4</a>).</p>

<figure><div class="figure" id="ch10_figure_4_1736545678095829"><img alt="A screenshot of a chat  Description automatically generated" src="assets/lelc_1004.png"/>
<h6><span class="label">Figure 10-4. </span>LangSmith trace of self-corrective RAG utilizing web search as a fallback</h6>
</div></figure>

<p>Let’s<a contenteditable="false" data-primary="" data-startref="Tdesign10" data-type="indexterm" id="id889"/><a contenteditable="false" data-primary="" data-startref="dstest10" data-type="indexterm" id="id890"/><a contenteditable="false" data-primary="" data-startref="RAGself10" data-type="indexterm" id="id891"/><a contenteditable="false" data-primary="" data-startref="LGself10" data-type="indexterm" id="id892"/> move on to the next stage in LLM app testing: preproduction.</p>
</div></section>

<section data-pdf-bookmark="The Preproduction Stage" data-type="sect1"><div class="sect1" id="ch10_the_preproduction_stage_1736545678108980">
<h1>The Preproduction Stage</h1>

<p>The<a contenteditable="false" data-primary="testing" data-secondary="preproduction stage" data-type="indexterm" id="Tpre10"/><a contenteditable="false" data-primary="preproduction stage testing" data-secondary="purpose of" data-type="indexterm" id="id893"/> purpose of the preproduction stage of testing is to measure and evaluate the performance of your application prior to production. This will enable you to efficiently assess the accuracy, latency, and cost of utilizing the LLM.</p>

<section data-pdf-bookmark="Creating Datasets" data-type="sect2"><div class="sect2" id="ch10_creating_datasets_1736545678109076">
<h2>Creating Datasets</h2>

<p>Prior<a contenteditable="false" data-primary="preproduction stage testing" data-secondary="dataset creation" data-type="indexterm" id="PSTdataset10"/><a contenteditable="false" data-primary="datasets" data-secondary="creating" data-type="indexterm" id="Dcreat10"/> to testing, you need to define a set of scenarios you’d like to test and evaluate. A <em>dataset</em> is a collection of examples that provide inputs and expected outputs used to evaluate your LLM app.</p>

<p>These are three common methods to build datasets for valuation:</p>

<dl>
	<dt>Manually curated examples</dt>
	<dd>
	<p>These are handwritten examples based on expected user inputs and ideal generated outputs. A small dataset consists of between 10 and 50 quality examples. Over time, more examples can be added to the dataset based on edge cases that emerge in production.</p>
	</dd>
	<dt>Application logs</dt>
	<dd>
	<p>Once<a contenteditable="false" data-primary="application logs" data-type="indexterm" id="id894"/> the application is in production, you can store real-time user inputs and later add them to the dataset. This will help ensure the dataset is realistic and covers the most common user questions.</p>
	</dd>
	<dt>Synthetic data</dt>
	<dd>
	<p>These<a contenteditable="false" data-primary="synthetic data" data-type="indexterm" id="id895"/> are artificially generated examples that simulate various scenarios and edge cases. This enables you to generate new inputs by sampling existing inputs, which is useful when you don’t have enough real data to test on.</p>
	</dd>
</dl>

<p>In<a contenteditable="false" data-primary="LangSmith" data-secondary="dataset creation" data-type="indexterm" id="id896"/> LangSmith, you can create a new dataset by selecting Datasets and Testing in the sidebar and clicking the “+ New Dataset” button on the top right of the app, as shown in <a data-type="xref" href="#ch10_figure_5_1736545678095862">Figure 10-5</a>.</p>

<p>In the opened window, enter the relevant dataset details, including a name, description, and dataset type. If you’d like to use your own dataset, click the “Upload a CSV dataset” button.</p>

<figure><div class="figure" id="ch10_figure_5_1736545678095862"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_1005.png"/>
<h6><span class="label">Figure 10-5. </span>Creating a new dataset in the LangSmith UI</h6>
</div></figure>



<p>LangSmith offers three different dataset types:</p>

<dl>
	<dt><code>kv</code> (key-value) dataset</dt>
	<dd>
	<ul>
		<li>
		<p><em>Inputs</em> and <em>outputs</em> are<a contenteditable="false" data-primary="kv (key-value) dataset" data-type="indexterm" id="id897"/> represented as arbitrary key-value pairs.</p>
		</li>
		<li>
		<p>The <code>kv</code> dataset is the most versatile, and it is the default type. The <code>kv</code> dataset is suitable for a wide range of evaluation scenarios.</p>
		</li>
		<li>
		<p>This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.</p>
		</li>
	</ul>
	</dd>
	<dt>
	<p><code>llm</code> (large language model) dataset</p>
	</dt>
	<dd>
	<ul>
		<li>
		<p>The <code>llm</code> dataset<a contenteditable="false" data-primary="llm (large language model) dataset" data-type="indexterm" id="id898"/><a contenteditable="false" data-primary="large language model (llm) dataset" data-type="indexterm" id="id899"/> is designed for evaluating completion style language models.</p>
		</li>
		<li>
		<p>The inputs dictionary contains a single input key mapped to the prompt string.</p>
		</li>
		<li>
		<p>The outputs dictionary contains a single output key mapped to the corresponding response string.</p>
		</li>
		<li>
		<p>This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.</p>
		</li>
	</ul>
	</dd>
	<dt class="pagebreak-before less_space">
	<p><code>chat</code> dataset</p>
	</dt>
	<dd>
	<ul>
		<li>
		<p>The<a contenteditable="false" data-primary="chat dataset" data-type="indexterm" id="id900"/> <code>chat</code> dataset is designed for evaluating LLM structured chat messages as inputs and outputs.</p>
		</li>
		<li>
		<p>The <em>inputs</em> dictionary contains a single <em>input</em> key mapped to a list of serialized chat messages.</p>
		</li>
		<li>
		<p>The <em>outputs</em> dictionary contains a single <em>output</em> key mapped to a list of serialized chat messages.</p>
		</li>
		<li>
		<p>This dataset type is useful for evaluating conversational AI systems or chatbots.</p>
		</li>
	</ul>
	</dd>
</dl>

<p>The most flexible option is the key-value data type (see <a data-type="xref" href="#ch10_figure_6_1736545678095893">Figure 10-6</a>).</p>

<figure><div class="figure" id="ch10_figure_6_1736545678095893"><img alt="A screenshot of a chat  Description automatically generated" src="assets/lelc_1006.png"/>
<h6><span class="label">Figure 10-6. </span>Selecting a dataset type in the LangSmith UI</h6>
</div></figure>

<p>Next, add examples to the dataset by clicking Add Example. Provide the input and output examples as JSON objects, as shown in <a data-type="xref" href="#ch10_figure_7_1736545678095923">Figure 10-7</a>.</p>

<figure class="width-80"><div class="figure" id="ch10_figure_7_1736545678095923"><img alt="A white background with black lines  Description automatically generated" src="assets/lelc_1007.png"/>
<h6><span class="label">Figure 10-7. </span>Add key-value dataset examples in the LangSmith UI</h6>
</div></figure>

<p>You can also define a schema for your dataset in the “Dataset schema” section, as shown in <a data-type="xref" href="#ch10_figure_8_1736545678095955">Figure 10-8</a>.</p>

<figure class="width-80"><div class="figure" id="ch10_figure_8_1736545678095955"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_1008.png"/>
<h6><span class="label">Figure 10-8. </span>Adding a dataset schema in the LangSmith UI</h6>
</div></figure>
</div></section>

<section data-pdf-bookmark="Defining Your Evaluation Criteria" data-type="sect2"><div class="sect2" id="ch10_defining_your_evaluation_criteria_1736545678109149">
<h2>Defining Your Evaluation Criteria</h2>

<p>After<a contenteditable="false" data-primary="" data-startref="PSTdataset10" data-type="indexterm" id="id901"/><a contenteditable="false" data-primary="" data-startref="Dcreat10" data-type="indexterm" id="id902"/><a contenteditable="false" data-primary="preproduction stage testing" data-secondary="defining evaluation criteria" data-type="indexterm" id="PPSTcriteria10"/> creating your dataset, you need to define<a contenteditable="false" data-primary="evaluation metrics" data-secondary="defining" data-type="indexterm" id="id903"/> evaluation metrics to assess your application’s outputs before deploying into production. This batch evaluation on a predetermined test suite is often referred to as<a contenteditable="false" data-primary="offline evaluation" data-type="indexterm" id="id904"/><strong> </strong><em>offline evaluation</em><strong>. </strong></p>

<p>For offline evaluation, you can optionally label expected outputs (that is, ground truth references) for the data points you are testing on. This enables you to compare your application’s response with the<a contenteditable="false" data-primary="ground truth references" data-type="indexterm" id="id905"/> ground truth references, as shown in <a data-type="xref" href="#ch10_figure_9_1736545678095982">Figure 10-9</a>.</p>

<figure><div class="figure" id="ch10_figure_9_1736545678095982"><img alt="A diagram of an application process  Description automatically generated" src="assets/lelc_1009.png"/>
<h6><span class="label">Figure 10-9. </span>AI evaluation diagram</h6>
</div></figure>

<p>There<a contenteditable="false" data-primary="evaluation metrics" data-secondary="main evaluators" data-type="indexterm" id="id906"/> are three main evaluators to score your LLM app performance:</p>

<dl>
	<dt>Human evaluators</dt>
	<dd>
	<p>If<a contenteditable="false" data-primary="human evaluators" data-type="indexterm" id="id907"/> you can’t express your testing requirements as code, you can use human feedback to express qualitative characteristics and label app responses with scores. LangSmith speeds up the process of collecting and incorporating human feedback with annotation queues.</p>
	</dd>
	<dt>Heuristic evaluators</dt>
	<dd>
	<p>These<a contenteditable="false" data-primary="heuristic evaluators" data-type="indexterm" id="id908"/> are hardcoded functions and assertions that perform computations to determine a score. You can use reference-free heuristics (for example, checking whether output is valid JSON) or reference-based heuristics such as accuracy. Reference-based evaluation compares an output to a predefined ground truth, whereas reference-free evaluation assesses qualitative characteristics without a ground truth. Custom heuristic evaluators are useful for code-generation tasks such as schema checking and unit testing with hardcoded evaluation logic.</p>
	</dd>
	<dt>LLM-as-a-judge evaluators</dt>
	<dd>
	<p>This<a contenteditable="false" data-primary="LLM-as-a-judge evaluators" data-type="indexterm" id="id909"/> evaluator integrates human grading rules into an LLM prompt to evaluate whether the output is correct relative to the reference answer supplied from the dataset output. As you iterate in preproduction, you’ll need to audit the scores and tune the LLM-as-a-judge to produce reliable scores.</p>
	</dd>
</dl>

<p>To<a contenteditable="false" data-primary="evaluation metrics" data-secondary="getting started" data-type="indexterm" id="id910"/> get started with evaluation, start simple with heuristic evaluators. Then implement human evaluators before moving on to LLM-as-a-judge to automate your human review. This enables you to add depth and scale once your criteria are well-defined.</p>

<div data-type="tip"><h6>Tip</h6>
<p>When using LLM-as-a-judge evaluators, use straightforward prompts that can easily be replicated and understood by a human. For example, avoid asking an LLM to produce scores on a range of 0 to 10 with vague distinctions between scores.</p>
</div>

<p><a data-type="xref" href="#ch10_figure_10_1736545678096002">Figure 10-10</a> illustrates LLM-as-a-judge evaluator in the context of a RAG use case. Note that the reference answer is the ground truth.</p>

<figure><div class="figure" id="ch10_figure_10_1736545678096002"><img alt="A diagram of a brain  Description automatically generated" src="assets/lelc_1010.png"/>
<h6><span class="label">Figure 10-10. </span>LLM-as-a-judge evaluator used in a RAG use case</h6>
</div></figure>

<section data-pdf-bookmark="Improving LLM-as-a-judge evaluators performance" data-type="sect3"><div class="sect3" id="ch10_improving_llm_as_a_judge_evaluators_performance_1736545678109220">
<h3>Improving LLM-as-a-judge evaluators performance</h3>

<p>Using<a contenteditable="false" data-primary="evaluation metrics" data-secondary="improving LLM-as-a-judge performance" data-type="indexterm" id="EMllm10"/> an LLM-as-a-judge is an effective method to grade natural language outputs from LLM applications. This involves passing the generated output to a separate LLM for judgment and evaluation. But how can you trust the results of LLM-as-a-judge evaluation?</p>

<p>Often, rounds of prompt engineering are required to improve accuracy, which is cumbersome and time-consuming. Fortunately, LangSmith<a contenteditable="false" data-primary="LangSmith" data-secondary="few-shot prompting" data-type="indexterm" id="LSfew10"/> provides<a contenteditable="false" data-primary="few-shot prompting" data-type="indexterm" id="id911"/> a <em>few-shot</em> prompt solution whereby human corrections to LLM-as-a-judge outputs are stored as few-shot examples, which are then fed back into the prompt in future iterations.</p>

<p class="pagebreak-before less_space">By utilizing few-shot learning, the LLM can improve accuracy and align outputs with human preferences by providing examples of correct behavior. This is especially useful when it’s difficult to construct instructions on how the LLM should behave or be formatted.</p>

<p>The few-shot evaluator follows these steps:</p>

<ol>
	<li>
	<p>The LLM evaluator provides feedback on generated outputs, assessing factors such as correctness, relevance, or other criteria.</p>
	</li>
	<li>
	<p>It adds human corrections to modify or correct the LLM evaluator’s feedback in LangSmith. This is where human preferences and judgment are captured.</p>
	</li>
	<li>
	<p>These corrections are stored as few-shot examples in LangSmith, with an option to leave explanations for corrections.</p>
	</li>
	<li>
	<p>The few-shot examples are incorporated into future prompts as subsequent evaluation runs.</p>
	</li>
</ol>

<p>Over time, the few-shot evaluator will become increasingly aligned with human preferences. This self-improving mechanism reduces the need for time-consuming prompt engineering, while improving the accuracy and relevance of LLM-as-a-judge evaluations.</p>

<p>Here’s how to easily set up the LLM-as-a-judge evaluator in LangSmith for offline evaluation. First, navigate to the “Datasets and Testing” section in the sidebar and select the dataset you want to configure the evaluator for. Click the Add Auto-Evaluator button at the top right of the dashboard to add an evaluator to the dataset. This will open a modal you can use to configure the evaluator.</p>

<p>Select the LLM-as-a-judge option and give your evaluator a name. You will now have the option to set an inline prompt or load a prompt from the prompt hub that will be used to evaluate the results of the runs in the experiment. For<a contenteditable="false" data-primary="auto-evaluators, adding" data-type="indexterm" id="id912"/> the sake of this example, choose the Create Few-Shot Evaluator option, as shown in <a data-type="xref" href="#ch10_figure_11_1736545678096023">Figure 10-11</a>.</p>

<figure><div class="figure" id="ch10_figure_11_1736545678096023"><img alt="A screenshot of a survey  Description automatically generated" src="assets/lelc_1011.png"/>
<h6><span class="label">Figure 10-11. </span>LangSmith UI options for the LLM-as-a-judge evaluator</h6>
</div></figure>

<p>This option will create a dataset that holds few-shot examples that will autopopulate when you make corrections on the evaluator feedback. The examples in this dataset will be inserted in the system prompt message.</p>

<p>You can also specify the scoring criteria in the Schema field and toggle between primitive types—for example, integer and Boolean (see <a data-type="xref" href="#ch10_figure_12_1736545678096045">Figure 10-12</a>).</p>

<figure><div class="figure" id="ch10_figure_12_1736545678096045"><img alt="A screenshot of a quiz  Description automatically generated" src="assets/lelc_1012.png"/>
<h6><span class="label">Figure 10-12. </span>LLM-as-a-judge evaluator scoring criteria</h6>
</div></figure>

<p>Save the evaluator and navigate back to the dataset details page. Moving forward, each subsequent experiment run from the dataset will be evaluated by the evaluator you configured.<a contenteditable="false" data-primary="" data-startref="EMllm10" data-type="indexterm" id="id913"/><a contenteditable="false" data-primary="" data-startref="LSfew10" data-type="indexterm" id="id914"/></p>
</div></section>

<section data-pdf-bookmark="Pairwise evaluation" data-type="sect3"><div class="sect3" id="ch10_pairwise_evaluation_1736545678109281">
<h3>Pairwise evaluation</h3>

<p>Ranking<a contenteditable="false" data-primary="evaluation metrics" data-secondary="pairwise evaluation" data-type="indexterm" id="id915"/><a contenteditable="false" data-primary="pairwise evaluation" data-type="indexterm" id="id916"/> LLM outputs by preference can be less cognitively demanding for human or LLM-as-a-judge evaluators. For example, assessing which output is more informative, specific, or safe. Pairwise evaluation compares two outputs simultaneously from different versions of an application to determine which version better meets evaluation criteria.</p>

<p>LangSmith<a contenteditable="false" data-primary="LangSmith" data-secondary="pairwise evaluation" data-type="indexterm" id="id917"/> natively supports running and visualizing pairwise LLM app generations, highlighting preference for one generation over another based on guidelines set by the pairwise evaluator. LangSmith’s pairwise evaluation enables you to do the following:</p>

<ul>
	<li>
	<p>Define a custom pairwise LLM-as-a-judge evaluator using any desired criteria</p>
	</li>
	<li>
	<p>Compare two LLM generations using this evaluator</p>
	</li>
</ul>

<p>As per the LangSmith <a href="https://oreil.ly/ruFvy">docs</a>, you can use custom pairwise evaluators in the LangSmith SDK and visualize the results of pairwise evaluations in the LangSmith UI.</p>

<p>After creating an evaluation experiment, you can navigate to the Pairwise Experiments tab in the Datasets &amp; Experiments section. The UI enables you to dive into each pairwise experiment, showing which LLM generation is preferred based upon our criteria. If you click the RANKED_PREFERENCE score under each answer, you can dive deeper into each evaluation trace<a contenteditable="false" data-primary="" data-startref="PPSTcriteria10" data-type="indexterm" id="id918"/> (see <a data-type="xref" href="#ch10_figure_13_1736545678096064">Figure 10-13</a>).</p>

<figure><div class="figure" id="ch10_figure_13_1736545678096064"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_1013.png"/>
<h6><span class="label">Figure 10-13. </span>Pairwise experiment UI evaluation trace</h6>
</div></figure>
</div></section>
</div></section>

<section data-pdf-bookmark="Regression Testing" data-type="sect2"><div class="sect2" id="ch10_regression_testing_1736545678109342">
<h2>Regression Testing</h2>

<p>In<a contenteditable="false" data-primary="preproduction stage testing" data-secondary="regression testing" data-type="indexterm" id="id919"/><a contenteditable="false" data-primary="regression testing" data-type="indexterm" id="id920"/> traditional software development, tests are expected to pass 100% based on functional requirements. This ensures stable behavior once the test is validated. In contrast, however, AI models’ output performances can vary significantly due<a contenteditable="false" data-primary="model drift" data-type="indexterm" id="id921"/><a contenteditable="false" data-primary="drift" data-type="indexterm" id="id922"/> to model <em>drift</em> (degradation due to changes in data distribution or updates to the model). As a result, testing AI applications may not always lead to a perfect score on the evaluation dataset.</p>

<p>This has several implications. First, it’s important to track results and performance of your tests over time to prevent regression of your app’s performance. <em>Regression</em> testing ensures that the latest updates or changes of the LLM model of your app do not <em>regress</em> (perform worse) relative to the baseline.</p>

<p>Second, it’s crucial to compare the individual data points between two or more experimental runs to see where the model got it right or wrong.</p>

<p>LangSmith’s comparison view<a contenteditable="false" data-primary="LangSmith" data-secondary="comparison view/regression testing" data-type="indexterm" id="id923"/> has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Runs that regressed or improved are highlighted differently in the LangSmith dashboard (see <a data-type="xref" href="#ch10_figure_14_1736545678096104">Figure 10-14</a>).</p>

<figure><div class="figure" id="ch10_figure_14_1736545678096104"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_1014.png"/>
<h6><span class="label">Figure 10-14. </span>LangSmith’s experiments comparison view</h6>
</div></figure>

<p class="pagebreak-before less_space">In LangSmith’s Comparing Experiments dashboard, you can do the following:</p>

<ul>
	<li>
	<p>Compare multiple experiments and runs associated with a dataset. Aggregate stats of runs is useful for migrating models or prompts, which may result in performance improvements or regression on specific examples.</p>
	</li>
	<li>
	<p>Set a baseline run and compare it against prior app versions to detect unexpected regressions. If a regression occurs, you can isolate both the app version and the specific examples that contain performance changes.</p>
	</li>
	<li>
	<p>Drill into data points that behaved differently between compared experiments and runs.</p>
	</li>
</ul>

<p>This regression testing is crucial to ensure that your application maintains high performance over time regardless of updates and LLM changes.</p>

<p>Now that we’ve covered various preproduction testing strategies, let’s explore a specific use case.</p>
</div></section>

<section data-pdf-bookmark="Evaluating an Agent’s End-to-End Performance" data-type="sect2"><div class="sect2" id="ch10_evaluating_an_agent_s_end_to_end_performance_1736545678109419">
<h2>Evaluating an Agent’s End-to-End Performance</h2>

<p>Although<a contenteditable="false" data-primary="preproduction stage testing" data-secondary="end-to-end performance" data-type="indexterm" id="PPSTendtoend10"/><a contenteditable="false" data-primary="agents, evaluating performance of" data-secondary="levels of testing" data-type="indexterm" id="id924"/> agents show a lot of promise in executing autonomous tasks and workflows, testing an agent’s performance can be challenging. In previous chapters, you learned how agents use tool calling with planning and memory to generate responses. In particular, tool calling enables the model to respond to a given prompt by generating a tool to invoke and the input arguments required to execute the tool.</p>

<p>Since agents use an LLM to decide the control flow of the application, each agent run can have significantly different outcomes. For example, different tools might be called, agents might get stuck in a loop, or the number of steps from start to finish can vary significantly.</p>

<p>Ideally, agents should be tested at three different levels of granularity:</p>

<dl>
	<dt>Response</dt>
	<dd>
	<p>The<a contenteditable="false" data-primary="response, evaluating" data-type="indexterm" id="id925"/> agent’s final response to focus on the end-to-end performance. The inputs are a prompt and an optional list of tools, whereas the output is the final agent response.</p>
	</dd>
	<dt>Single step</dt>
	<dd>
	<p>Any<a contenteditable="false" data-primary="single steps, evaluating" data-type="indexterm" id="id926"/> single, important step of the agent to drill into specific tool calls or decisions. In this case, the output is a tool call.</p>
	</dd>
	<dt>Trajectory</dt>
	<dd>
	<p>The<a contenteditable="false" data-primary="trajectory, evaluating" data-type="indexterm" id="id927"/> full trajectory of the agent. In this case, the output is the list of tool calls.</p>
	</dd>
</dl>

<p class="pagebreak-before less_space"><a data-type="xref" href="#ch10_figure_15_1736545678096126">Figure 10-15</a> illustrates these levels:</p>

<figure><div class="figure" id="ch10_figure_15_1736545678096126"><img alt="A diagram of a tool call  Description automatically generated" src="assets/lelc_1015.png"/>
<h6><span class="label">Figure 10-15. </span>An example of an agentic app’s flow</h6>
</div></figure>

<p>Let’s dive deeper into each of these three agent-testing granularities.</p>

<section data-pdf-bookmark="Testing an agent’s final response" data-type="sect3"><div class="sect3" id="ch10_testing_an_agent_s_final_response_1736545678109493">
<h3>Testing an agent’s final response</h3>

<p>In<a contenteditable="false" data-primary="final response, evaluating" data-type="indexterm" id="FReval10"/><a contenteditable="false" data-primary="agents, evaluating performance of" data-secondary="final response" data-type="indexterm" id="AEPfinal10"/> order to assess the overall performance of an agent on a task, you can treat the agent as a black box and define success based on whether or not it completes the task.</p>

<p>Testing for the agent’s final response typically involves the following:</p>

<dl>
	<dt>Inputs</dt>
	<dd>
	<p>User input and (optionally) predefined tools</p>
	</dd>
	<dt>Output</dt>
	<dd>
	<p>Agent’s final response</p>
	</dd>
	<dt>Evaluator</dt>
	<dd>
	<p>LLM-as-a-judge</p>
	</dd>
</dl>

<p>To implement this in a programmatic manner, first create a dataset that includes questions and expected answers from the agent:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">langsmith</code> <code class="kn">import</code> <code class="n">Client</code>

<code class="n">client</code> <code class="o">=</code> <code class="n">Client</code><code class="p">()</code>

<code class="c1"># Create a dataset</code>
<code class="n">examples</code> <code class="o">=</code> <code class="p">[</code>
    <code class="p">(</code><code class="s2">"Which country's customers spent the most? And how much did they spend?"</code><code class="p">,</code>
        <code class="sd">"""The country whose customers spent the most is the USA, with a total </code>
<code class="sd">        expenditure of $523.06"""</code><code class="p">),</code>
    <code class="p">(</code><code class="s2">"What was the most purchased track of 2013?"</code><code class="p">,</code> 
        <code class="s2">"The most purchased track of 2013 was Hot Girl."</code><code class="p">),</code>
    <code class="p">(</code><code class="s2">"How many albums does the artist Led Zeppelin have?"</code><code class="p">,</code>
        <code class="s2">"Led Zeppelin has 14 albums"</code><code class="p">),</code>
    <code class="p">(</code><code class="s2">"What is the total price for the album “Big Ones”?"</code><code class="p">,</code>
        <code class="s2">"The total price for the album 'Big Ones' is 14.85"</code><code class="p">),</code>
    <code class="p">(</code><code class="s2">"Which sales agent made the most in sales in 2009?"</code><code class="p">,</code> 
        <code class="s2">"Steve Johnson made the most sales in 2009"</code><code class="p">),</code>
<code class="p">]</code>

<code class="n">dataset_name</code> <code class="o">=</code> <code class="s2">"SQL Agent Response"</code>
<code class="k">if</code> <code class="ow">not</code> <code class="n">client</code><code class="o">.</code><code class="n">has_dataset</code><code class="p">(</code><code class="n">dataset_name</code><code class="o">=</code><code class="n">dataset_name</code><code class="p">):</code>
    <code class="n">dataset</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">create_dataset</code><code class="p">(</code><code class="n">dataset_name</code><code class="o">=</code><code class="n">dataset_name</code><code class="p">)</code>
    <code class="n">inputs</code><code class="p">,</code> <code class="n">outputs</code> <code class="o">=</code> <code class="nb">zip</code><code class="p">(</code>
        <code class="o">*</code><code class="p">[({</code><code class="s2">"input"</code><code class="p">:</code> <code class="n">text</code><code class="p">},</code> <code class="p">{</code><code class="s2">"output"</code><code class="p">:</code> <code class="n">label</code><code class="p">})</code> <code class="k">for</code> <code class="n">text</code><code class="p">,</code> <code class="n">label</code> <code class="ow">in</code> <code class="n">examples</code><code class="p">]</code>
    <code class="p">)</code>
    <code class="n">client</code><code class="o">.</code><code class="n">create_examples</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="n">inputs</code><code class="p">,</code> <code class="n">outputs</code><code class="o">=</code><code class="n">outputs</code><code class="p">,</code> <code class="n">dataset_id</code><code class="o">=</code><code class="n">dataset</code><code class="o">.</code><code class="n">id</code><code class="p">)</code>

<code class="c1">## chain</code>
<code class="k">def</code> <code class="nf">predict_sql_agent_answer</code><code class="p">(</code><code class="n">example</code><code class="p">:</code> <code class="nb">dict</code><code class="p">):</code>
    <code class="sd">"""Use this for answer evaluation"""</code>
    <code class="n">msg</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"messages"</code><code class="p">:</code> <code class="p">(</code><code class="s2">"user"</code><code class="p">,</code> <code class="n">example</code><code class="p">[</code><code class="s2">"input"</code><code class="p">])}</code>
    <code class="n">messages</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">msg</code><code class="p">,</code> <code class="n">config</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"response"</code><code class="p">:</code> <code class="n">messages</code><code class="p">[</code><code class="s1">'messages'</code><code class="p">][</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">content</code><code class="p">}</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code> <code class="nx">Client</code> <code class="p">}</code> <code class="nx">from</code> <code class="s1">'langsmith'</code><code class="p">;</code>

<code class="kr">const</code> <code class="nx">client</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">Client</code><code class="p">();</code>

<code class="c1">// Create a dataset</code>
<code class="kr">const</code> <code class="nx">examples</code> <code class="o">=</code> <code class="p">[</code>
  <code class="p">[</code><code class="s2">"Which country's customers spent the most? And how much did they spend?"</code><code class="p">,</code> 
    <code class="sb">`The country whose customers spent the most is the USA, with a total </code>
<code class="sb">    expenditure of $523.06`</code><code class="p">],</code>
  <code class="p">[</code><code class="s2">"What was the most purchased track of 2013?"</code><code class="p">,</code> 
    <code class="s2">"The most purchased track of 2013 was Hot Girl."</code><code class="p">],</code>
  <code class="p">[</code><code class="s2">"How many albums does the artist Led Zeppelin have?"</code><code class="p">,</code> 
    <code class="s2">"Led Zeppelin has 14 albums"</code><code class="p">],</code>
  <code class="p">[</code><code class="s2">"What is the total price for the album 'Big Ones'?"</code><code class="p">,</code> 
    <code class="s2">"The total price for the album 'Big Ones' is 14.85"</code><code class="p">],</code>
  <code class="p">[</code><code class="s2">"Which sales agent made the most in sales in 2009?"</code><code class="p">,</code> 
    <code class="s2">"Steve Johnson made the most sales in 2009"</code><code class="p">],</code>
<code class="p">];</code>

<code class="kr">const</code> <code class="nx">datasetName</code> <code class="o">=</code> <code class="s2">"SQL Agent Response"</code><code class="p">;</code>

<code class="nx">async</code> <code class="kd">function</code> <code class="nx">createDataset</code><code class="p">()</code> <code class="p">{</code>
  <code class="kr">const</code> <code class="nx">hasDataset</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">client</code><code class="p">.</code><code class="nx">hasDataset</code><code class="p">({</code> <code class="nx">datasetName</code> <code class="p">});</code>

  <code class="k">if</code> <code class="p">(</code><code class="o">!</code><code class="nx">hasDataset</code><code class="p">)</code> <code class="p">{</code>
    <code class="kr">const</code> <code class="nx">dataset</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">client</code><code class="p">.</code><code class="nx">createDataset</code><code class="p">(</code><code class="nx">datasetName</code><code class="p">);</code>
    <code class="kr">const</code> <code class="nx">inputs</code> <code class="o">=</code> <code class="nx">examples</code><code class="p">.</code><code class="nx">map</code><code class="p">(([</code><code class="nx">text</code><code class="p">])</code> <code class="o">=&gt;</code> <code class="p">({</code> <code class="nx">input</code><code class="o">:</code> <code class="nx">text</code> <code class="p">}));</code>
    <code class="kr">const</code> <code class="nx">outputs</code> <code class="o">=</code> <code class="nx">examples</code><code class="p">.</code><code class="nx">map</code><code class="p">(([,</code> <code class="nx">label</code><code class="p">])</code> <code class="o">=&gt;</code> <code class="p">({</code> <code class="nx">output</code><code class="o">:</code> <code class="nx">label</code> <code class="p">}));</code>

    <code class="nx">await</code> <code class="nx">client</code><code class="p">.</code><code class="nx">createExamples</code><code class="p">({</code> <code class="nx">inputs</code><code class="p">,</code> <code class="nx">outputs</code><code class="p">,</code> <code class="nx">datasetId</code><code class="o">:</code> <code class="nx">dataset</code><code class="p">.</code><code class="nx">id</code> <code class="p">});</code>
  <code class="p">}</code>
<code class="p">}</code>

<code class="nx">createDataset</code><code class="p">();</code>

<code class="c1">// Chain function</code>
<code class="nx">async</code> <code class="kd">function</code> <code class="nx">predictSqlAgentAnswer</code><code class="p">(</code><code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
  <code class="c1">// Use this for answer evaluation</code>
  <code class="kr">const</code> <code class="nx">msg</code> <code class="o">=</code> <code class="p">{</code> <code class="nx">messages</code><code class="o">:</code> <code class="p">[{</code> <code class="nx">role</code><code class="o">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="nx">content</code><code class="o">:</code> <code class="nx">example</code><code class="p">.</code><code class="nx">input</code> <code class="p">}]</code> <code class="p">};</code>
  <code class="kr">const</code> <code class="nx">output</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">msg</code><code class="p">,</code> <code class="nx">config</code><code class="p">);</code>
  <code class="k">return</code> <code class="p">{</code> <code class="nx">response</code><code class="o">:</code> <code class="nx">output</code><code class="p">.</code><code class="nx">messages</code><code class="p">[</code><code class="nx">output</code><code class="p">.</code><code class="nx">messages</code><code class="p">.</code><code class="nx">length</code> <code class="o">-</code> <code class="mi">1</code><code class="p">].</code><code class="nx">content</code> <code class="p">};</code>
<code class="p">}</code></pre>

<p>Next, as discussed earlier, we can utilize the LLM to compare the generated answer with the reference answer:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">langchain</code> <code class="kn">import</code> <code class="n">hub</code>
<code class="kn">from</code> <code class="nn">langchain_openai</code> <code class="kn">import</code> <code class="n">ChatOpenAI</code>
<code class="kn">from</code> <code class="nn">langsmith.evaluation</code> <code class="kn">import</code> <code class="n">evaluate</code>

<code class="c1"># Grade prompt</code>
<code class="n">grade_prompt_answer_accuracy</code> <code class="o">=</code> <code class="n">hub</code><code class="o">.</code><code class="n">pull</code><code class="p">(</code><code class="s2">"langchain-ai/rag-answer-vs-reference"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">answer_evaluator</code><code class="p">(</code><code class="n">run</code><code class="p">,</code> <code class="n">example</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">:</code>
    <code class="sd">"""</code>
<code class="sd">    A simple evaluator for RAG answer accuracy</code>
<code class="sd">    """</code>

    <code class="c1"># Get question, ground truth answer, RAG chain answer</code>
    <code class="n">input_question</code> <code class="o">=</code> <code class="n">example</code><code class="o">.</code><code class="n">inputs</code><code class="p">[</code><code class="s2">"input"</code><code class="p">]</code>
    <code class="n">reference</code> <code class="o">=</code> <code class="n">example</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s2">"output"</code><code class="p">]</code>
    <code class="n">prediction</code> <code class="o">=</code> <code class="n">run</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s2">"response"</code><code class="p">]</code>

    <code class="c1"># LLM grader</code>
    <code class="n">llm</code> <code class="o">=</code> <code class="n">ChatOpenAI</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="s2">"gpt-4o"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

    <code class="c1"># Structured prompt</code>
    <code class="n">answer_grader</code> <code class="o">=</code> <code class="n">grade_prompt_answer_accuracy</code> <code class="o">|</code> <code class="n">llm</code>

    <code class="c1"># Run evaluator</code>
    <code class="n">score</code> <code class="o">=</code> <code class="n">answer_grader</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"question"</code><code class="p">:</code> <code class="n">input_question</code><code class="p">,</code>
                                  <code class="s2">"correct_answer"</code><code class="p">:</code> <code class="n">reference</code><code class="p">,</code>
                                  <code class="s2">"student_answer"</code><code class="p">:</code> <code class="n">prediction</code><code class="p">})</code>
    <code class="n">score</code> <code class="o">=</code> <code class="n">score</code><code class="p">[</code><code class="s2">"Score"</code><code class="p">]</code>

    <code class="k">return</code> <code class="p">{</code><code class="s2">"key"</code><code class="p">:</code> <code class="s2">"answer_v_reference_score"</code><code class="p">,</code> <code class="s2">"score"</code><code class="p">:</code> <code class="n">score</code><code class="p">}</code>


<code class="c1">## Run evaluation</code>
<code class="n">experiment_results</code> <code class="o">=</code> <code class="n">evaluate</code><code class="p">(</code>
    <code class="n">predict_sql_agent_answer</code><code class="p">,</code>
    <code class="n">data</code><code class="o">=</code><code class="n">dataset_name</code><code class="p">,</code>
    <code class="n">evaluators</code><code class="o">=</code><code class="p">[</code><code class="n">answer_evaluator</code><code class="p">],</code>
    <code class="n">num_repetitions</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
<code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code> <code class="nx">pull</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain/hub"</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">ChatOpenAI</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langchain_openai"</code><code class="p">;</code>
<code class="kr">import</code> <code class="p">{</code> <code class="nx">evaluate</code> <code class="p">}</code> <code class="nx">from</code> <code class="s2">"langsmith/evaluation"</code><code class="p">;</code>

<code class="nx">async</code> <code class="kd">function</code> <code class="nx">answerEvaluator</code><code class="p">(</code><code class="nx">run</code><code class="p">,</code> <code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
  <code class="cm">/**</code>
<code class="cm">   * A simple evaluator for RAG answer accuracy</code>
<code class="cm">   */</code>

  <code class="c1">// Get question, ground truth answer, RAG chain answer</code>
  <code class="kr">const</code> <code class="nx">inputQuestion</code> <code class="o">=</code> <code class="nx">example</code><code class="p">.</code><code class="nx">inputs</code><code class="p">[</code><code class="s2">"input"</code><code class="p">];</code>
  <code class="kr">const</code> <code class="nx">reference</code> <code class="o">=</code> <code class="nx">example</code><code class="p">.</code><code class="nx">outputs</code><code class="p">[</code><code class="s2">"output"</code><code class="p">];</code>
  <code class="kr">const</code> <code class="nx">prediction</code> <code class="o">=</code> <code class="nx">run</code><code class="p">.</code><code class="nx">outputs</code><code class="p">[</code><code class="s2">"response"</code><code class="p">];</code>

  <code class="c1">// LLM grader</code>
  <code class="kr">const</code> <code class="nx">llm</code> <code class="o">=</code> <code class="k">new</code> <code class="nx">ChatOpenAI</code><code class="p">({</code> <code class="nx">model</code><code class="o">:</code> <code class="s2">"gpt-4o"</code><code class="p">,</code> <code class="nx">temperature</code><code class="o">:</code> <code class="mi">0</code> <code class="p">});</code>

  <code class="c1">// Grade prompt </code>
  <code class="kr">const</code> <code class="nx">gradePromptAnswerAccuracy</code> <code class="o">=</code> <code class="nx">pull</code><code class="p">(</code>
    <code class="s2">"langchain-ai/rag-answer-vs-reference"</code>
  <code class="p">);</code>

  <code class="c1">// Structured prompt</code>
  <code class="kr">const</code> <code class="nx">answerGrader</code> <code class="o">=</code> <code class="nx">gradePromptAnswerAccuracy</code><code class="p">.</code><code class="nx">pipe</code><code class="p">(</code><code class="nx">llm</code><code class="p">);</code>

  <code class="c1">// Run evaluator</code>
  <code class="kr">const</code> <code class="nx">scoreResult</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">answerGrader</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code>
    <code class="nx">question</code><code class="o">:</code> <code class="nx">inputQuestion</code><code class="p">,</code>
    <code class="nx">correct_answer</code><code class="o">:</code> <code class="nx">reference</code><code class="p">,</code>
    <code class="nx">student_answer</code><code class="o">:</code> <code class="nx">prediction</code>
  <code class="p">});</code>

  <code class="kr">const</code> <code class="nx">score</code> <code class="o">=</code> <code class="nx">scoreResult</code><code class="p">[</code><code class="s2">"Score"</code><code class="p">];</code>

  <code class="k">return</code> <code class="p">{</code> <code class="nx">key</code><code class="o">:</code> <code class="s2">"answer_v_reference_score"</code><code class="p">,</code> <code class="nx">score</code><code class="o">:</code> <code class="nx">score</code> <code class="p">};</code>
<code class="p">}</code>

<code class="c1">// Run evaluation</code>
<code class="kr">const</code> <code class="nx">experimentResults</code> <code class="o">=</code> <code class="nx">evaluate</code><code class="p">(</code><code class="nx">predictSqlAgentAnswer</code><code class="p">,</code> <code class="p">{</code>
  <code class="nx">data</code><code class="o">:</code> <code class="nx">datasetName</code><code class="p">,</code>
  <code class="nx">evaluators</code><code class="o">:</code> <code class="p">[</code><code class="nx">answerEvaluator</code><code class="p">],</code>
  <code class="nx">numRepetitions</code><code class="o">:</code> <code class="mi">3</code><code class="p">,</code>
<code class="p">});</code></pre>
</div></section>

<section data-pdf-bookmark="Testing a single step of an agent" data-type="sect3"><div class="sect3" id="ch10_testing_a_single_step_of_an_agent_1736545678109574">
<h3>Testing a single step of an agent</h3>

<p>Testing<a contenteditable="false" data-primary="" data-startref="AEPfinal10" data-type="indexterm" id="id928"/><a contenteditable="false" data-primary="" data-startref="FReval10" data-type="indexterm" id="id929"/> an agent’s individual action<a contenteditable="false" data-primary="agents, evaluating performance of" data-secondary="single steps" data-type="indexterm" id="AEPsingle10"/> or decision enables you to identify and analyze specifically where your application is underperforming. Testing for a single step of an agent involves the following:</p>

<dl>
	<dt>Inputs</dt>
	<dd>
	<p>User input to a single step (for example, user prompt, set of tools). This can also include previously completed steps.</p>
	</dd>
	<dt>Output</dt>
	<dd>
	<p>LLM response from the inputs step, which often contains tool calls indicating what action the agent should take next.</p>
	</dd>
	<dt>Evaluator</dt>
	<dd>
	<p>Binary score for correct tool selection and heuristic assessment of the tool input’s accuracy.</p>
	</dd>
</dl>

<p>The following example checks a specific tool call using a custom evaluator:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">langsmith.schemas</code> <code class="kn">import</code> <code class="n">Example</code><code class="p">,</code> <code class="n">Run</code>

<code class="k">def</code> <code class="nf">predict_assistant</code><code class="p">(</code><code class="n">example</code><code class="p">:</code> <code class="nb">dict</code><code class="p">):</code>
    <code class="sd">"""Invoke assistant for single tool call evaluation"""</code>
    <code class="n">msg</code> <code class="o">=</code> <code class="p">[</code> <code class="p">(</code><code class="s2">"user"</code><code class="p">,</code> <code class="n">example</code><code class="p">[</code><code class="s2">"input"</code><code class="p">])</code> <code class="p">]</code>
    <code class="n">result</code> <code class="o">=</code> <code class="n">assistant_runnable</code><code class="o">.</code><code class="n">invoke</code><code class="p">({</code><code class="s2">"messages"</code><code class="p">:</code><code class="n">msg</code><code class="p">})</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"response"</code><code class="p">:</code> <code class="n">result</code><code class="p">}</code>


<code class="k">def</code> <code class="nf">check_specific_tool_call</code><code class="p">(</code><code class="n">root_run</code><code class="p">:</code> <code class="n">Run</code><code class="p">,</code> <code class="n">example</code><code class="p">:</code> <code class="n">Example</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">:</code>
    <code class="sd">"""</code>
<code class="sd">    Check if the first tool call in the response matches the expected tool call.</code>
<code class="sd">    """</code>
    <code class="c1"># Expected tool call</code>
    <code class="n">expected_tool_call</code> <code class="o">=</code> <code class="s1">'sql_db_list_tables'</code>

    <code class="c1"># Run</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">root_run</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s2">"response"</code><code class="p">]</code>

    <code class="c1"># Get tool call</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="n">tool_call</code> <code class="o">=</code> <code class="nb">getattr</code><code class="p">(</code><code class="n">response</code><code class="p">,</code> <code class="s1">'tool_calls'</code><code class="p">,</code> <code class="p">[])[</code><code class="mi">0</code><code class="p">][</code><code class="s1">'name'</code><code class="p">]</code>
    <code class="k">except</code> <code class="p">(</code><code class="ne">IndexError</code><code class="p">,</code> <code class="ne">KeyError</code><code class="p">):</code>
        <code class="n">tool_call</code> <code class="o">=</code> <code class="kc">None</code>

    <code class="n">score</code> <code class="o">=</code> <code class="mi">1</code> <code class="k">if</code> <code class="n">tool_call</code> <code class="o">==</code> <code class="n">expected_tool_call</code> <code class="k">else</code> <code class="mi">0</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"score"</code><code class="p">:</code> <code class="n">score</code><code class="p">,</code> <code class="s2">"key"</code><code class="p">:</code> <code class="s2">"single_tool_call"</code><code class="p">}</code>

<code class="n">experiment_results</code> <code class="o">=</code> <code class="n">evaluate</code><code class="p">(</code>
    <code class="n">predict_assistant</code><code class="p">,</code>
    <code class="n">data</code><code class="o">=</code><code class="n">dataset_name</code><code class="p">,</code>
    <code class="n">evaluators</code><code class="o">=</code><code class="p">[</code><code class="n">check_specific_tool_call</code><code class="p">],</code>
    <code class="n">num_repetitions</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
    <code class="n">metadata</code><code class="o">=</code><code class="p">{</code><code class="s2">"version"</code><code class="p">:</code> <code class="n">metadata</code><code class="p">},</code>
<code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code><code class="nx">evaluate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'langsmith/evaluation'</code><code class="p">;</code>

<code class="c1">// Predict Assistant</code>
<code class="kd">function</code> <code class="nx">predictAssistant</code><code class="p">(</code><code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
    <code class="cm">/**</code>
<code class="cm">     * Invoke assistant for single tool call evaluation</code>
<code class="cm">     */</code>
    <code class="kr">const</code> <code class="nx">msg</code> <code class="o">=</code> <code class="p">[{</code> <code class="nx">role</code><code class="o">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="nx">content</code><code class="o">:</code> <code class="nx">example</code><code class="p">.</code><code class="nx">input</code> <code class="p">}];</code>
    <code class="kr">const</code> <code class="nx">result</code> <code class="o">=</code> <code class="nx">assistantRunnable</code><code class="p">.</code><code class="nx">invoke</code><code class="p">({</code> <code class="nx">messages</code><code class="o">:</code> <code class="nx">msg</code> <code class="p">});</code>
    <code class="k">return</code> <code class="p">{</code> <code class="nx">response</code><code class="o">:</code> <code class="nx">result</code> <code class="p">};</code>
<code class="p">}</code>

<code class="c1">// Check Specific Tool Call</code>
<code class="kd">function</code> <code class="nx">checkSpecificToolCall</code><code class="p">(</code><code class="nx">rootRun</code><code class="p">,</code> <code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
    <code class="cm">/**</code>
<code class="cm">     * Check if the first tool call in the response matches the expected </code>
<code class="cm">     * tool call.</code>
<code class="cm">     */</code>

    <code class="c1">// Expected tool call</code>
    <code class="kr">const</code> <code class="nx">expectedToolCall</code> <code class="o">=</code> <code class="s2">"sql_db_list_tables"</code><code class="p">;</code>

    <code class="c1">// Run</code>
    <code class="kr">const</code> <code class="nx">response</code> <code class="o">=</code> <code class="nx">rootRun</code><code class="p">.</code><code class="nx">outputs</code><code class="p">.</code><code class="nx">response</code><code class="p">;</code>

    <code class="c1">// Get tool call</code>
    <code class="kd">let</code> <code class="nx">toolCall</code><code class="p">;</code>
    <code class="k">try</code> <code class="p">{</code>
        <code class="nx">toolCall</code> <code class="o">=</code> <code class="nx">response</code><code class="p">.</code><code class="nx">tool_calls</code><code class="o">?</code><code class="p">.[</code><code class="mi">0</code><code class="p">]</code><code class="o">?</code><code class="p">.</code><code class="nx">name</code><code class="p">;</code>
    <code class="p">}</code> <code class="k">catch</code> <code class="p">(</code><code class="nx">error</code><code class="p">)</code> <code class="p">{</code>
        <code class="nx">toolCall</code> <code class="o">=</code> <code class="kc">null</code><code class="p">;</code>
    <code class="p">}</code>

    <code class="kr">const</code> <code class="nx">score</code> <code class="o">=</code> <code class="nx">toolCall</code> <code class="o">===</code> <code class="nx">expectedToolCall</code> <code class="o">?</code> <code class="mi">1</code> <code class="o">:</code> <code class="mi">0</code><code class="p">;</code>
    <code class="k">return</code> <code class="p">{</code> <code class="nx">score</code><code class="p">,</code> <code class="nx">key</code><code class="o">:</code> <code class="s2">"single_tool_call"</code> <code class="p">};</code>
<code class="p">}</code>

<code class="c1">// Experiment Results</code>
<code class="kr">const</code> <code class="nx">experimentResults</code> <code class="o">=</code> <code class="nx">evaluate</code><code class="p">(</code><code class="nx">predictAssistant</code><code class="p">,</code> <code class="p">{</code>
    <code class="nx">data</code><code class="o">:</code> <code class="nx">datasetName</code><code class="p">,</code>
    <code class="nx">evaluators</code><code class="o">:</code> <code class="p">[</code><code class="nx">checkSpecificToolCall</code><code class="p">],</code>
    <code class="nx">numRepetitions</code><code class="o">:</code> <code class="mi">3</code><code class="p">,</code>
<code class="p">});</code></pre>

<p>The preceding code block implements these distinct components:</p>

<ul>
	<li>
	<p>Invoke the assistant, <code>assistant_runnable</code>, with a prompt and check if the resulting tool call is as expected.</p>
	</li>
	<li>
	<p>Utilize a specialized agent where the tools are hardcoded rather than passed with the dataset input.</p>
	</li>
	<li>
	<p>Specify the reference tool call for the step that we are evaluating for <code>expected_tool_call</code>.<a contenteditable="false" data-primary="" data-startref="AEPsingle10" data-type="indexterm" id="id930"/></p>
	</li>
</ul>
</div></section>

<section data-pdf-bookmark="Testing an agent’s trajectory" data-type="sect3"><div class="sect3" id="ch10_testing_an_agent_s_trajectory_1736545678109667">
<h3>Testing an agent’s trajectory</h3>

<p>It’s<a contenteditable="false" data-primary="agents, evaluating performance of" data-secondary="trajectory" data-type="indexterm" id="AEPtraject10"/><a contenteditable="false" data-primary="trajectory, evaluating" data-type="indexterm" id="trajecteval10"/> important to look back on the steps an agent took in order to assess whether or not the trajectory lined up with expectations of the agent—that is, the number of steps or sequence of steps taken.</p>

<p>Testing an agent’s trajectory involves the following:</p>

<dl>
	<dt>Inputs</dt>
	<dd>
	<p>User input and (optionally) predefined tools.</p>
	</dd>
	<dt>Output</dt>
	<dd>
	<p>Expected sequence of tool calls or a list of tool calls in any order.</p>
	</dd>
	<dt>Evaluator</dt>
	<dd>
	<p>Function over the steps taken. To test the outputs, you can look at an exact match binary score or metrics that focus on the number of incorrect steps. You’d need to evaluate the full agent’s trajectory against a reference trajectory and then compile as a set of messages to pass into the LLM-as-a-judge.</p>
	</dd>
</dl>

<p>The following example assesses the trajectory of tool calls using custom evaluators:</p>

<p><em>Python</em></p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">predict_sql_agent_messages</code><code class="p">(</code><code class="n">example</code><code class="p">:</code> <code class="nb">dict</code><code class="p">):</code>
    <code class="sd">"""Use this for answer evaluation"""</code>
    <code class="n">msg</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"messages"</code><code class="p">:</code> <code class="p">(</code><code class="s2">"user"</code><code class="p">,</code> <code class="n">example</code><code class="p">[</code><code class="s2">"input"</code><code class="p">])}</code>
    <code class="n">messages</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">invoke</code><code class="p">(</code><code class="n">msg</code><code class="p">,</code> <code class="n">config</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"response"</code><code class="p">:</code> <code class="n">messages</code><code class="p">}</code>

<code class="k">def</code> <code class="nf">find_tool_calls</code><code class="p">(</code><code class="n">messages</code><code class="p">):</code>
    <code class="sd">"""</code>
<code class="sd">    Find all tool calls in the messages returned</code>
<code class="sd">    """</code>
    <code class="n">tool_calls</code> <code class="o">=</code> <code class="p">[</code>
        <code class="n">tc</code><code class="p">[</code><code class="s1">'name'</code><code class="p">]</code>
        <code class="k">for</code> <code class="n">m</code> <code class="ow">in</code> <code class="n">messages</code><code class="p">[</code><code class="s1">'messages'</code><code class="p">]</code> <code class="k">for</code> <code class="n">tc</code> <code class="ow">in</code> <code class="nb">getattr</code><code class="p">(</code><code class="n">m</code><code class="p">,</code> <code class="s1">'tool_calls'</code><code class="p">,</code> <code class="p">[])</code>
    <code class="p">]</code>
    <code class="k">return</code> <code class="n">tool_calls</code>

<code class="k">def</code> <code class="nf">contains_all_tool_calls_any_order</code><code class="p">(</code>
    <code class="n">root_run</code><code class="p">:</code> <code class="n">Run</code><code class="p">,</code> <code class="n">example</code><code class="p">:</code> <code class="n">Example</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">:</code>
    <code class="sd">"""</code>
<code class="sd">    Check if all expected tools are called in any order.</code>
<code class="sd">    """</code>
    <code class="n">expected</code> <code class="o">=</code> <code class="p">[</code>
        <code class="s1">'sql_db_list_tables'</code><code class="p">,</code>
        <code class="s1">'sql_db_schema'</code><code class="p">,</code>
        <code class="s1">'sql_db_query_checker'</code><code class="p">,</code>
        <code class="s1">'sql_db_query'</code><code class="p">,</code>
        <code class="s1">'check_result'</code>
    <code class="p">]</code>
    <code class="n">messages</code> <code class="o">=</code> <code class="n">root_run</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s2">"response"</code><code class="p">]</code>
    <code class="n">tool_calls</code> <code class="o">=</code> <code class="n">find_tool_calls</code><code class="p">(</code><code class="n">messages</code><code class="p">)</code>
    <code class="c1"># Optionally, log the tool calls -</code>
    <code class="c1">#print("Here are my tool calls:")</code>
    <code class="c1">#print(tool_calls)</code>
    <code class="k">if</code> <code class="nb">set</code><code class="p">(</code><code class="n">expected</code><code class="p">)</code> <code class="o">&lt;=</code> <code class="nb">set</code><code class="p">(</code><code class="n">tool_calls</code><code class="p">):</code>
        <code class="n">score</code> <code class="o">=</code> <code class="mi">1</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">score</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"score"</code><code class="p">:</code> <code class="nb">int</code><code class="p">(</code><code class="n">score</code><code class="p">),</code> <code class="s2">"key"</code><code class="p">:</code> <code class="s2">"multi_tool_call_any_order"</code><code class="p">}</code>

<code class="k">def</code> <code class="nf">contains_all_tool_calls_in_order</code><code class="p">(</code><code class="n">root_run</code><code class="p">:</code> <code class="n">Run</code><code class="p">,</code> <code class="n">example</code><code class="p">:</code> <code class="n">Example</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">:</code>
    <code class="sd">"""</code>
<code class="sd">    Check if all expected tools are called in exact order.</code>
<code class="sd">    """</code>
    <code class="n">messages</code> <code class="o">=</code> <code class="n">root_run</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s2">"response"</code><code class="p">]</code>
    <code class="n">tool_calls</code> <code class="o">=</code> <code class="n">find_tool_calls</code><code class="p">(</code><code class="n">messages</code><code class="p">)</code>
    <code class="c1"># Optionally, log the tool calls -</code>
    <code class="c1">#print("Here are my tool calls:")</code>
    <code class="c1">#print(tool_calls)</code>
    <code class="n">it</code> <code class="o">=</code> <code class="nb">iter</code><code class="p">(</code><code class="n">tool_calls</code><code class="p">)</code>
    <code class="n">expected</code> <code class="o">=</code> <code class="p">[</code>
        <code class="s1">'sql_db_list_tables'</code><code class="p">,</code> 
        <code class="s1">'sql_db_schema'</code><code class="p">,</code> 
        <code class="s1">'sql_db_query_checker'</code><code class="p">,</code>
        <code class="s1">'sql_db_query'</code><code class="p">,</code> 
        <code class="s1">'check_result'</code>
    <code class="p">]</code>
    <code class="k">if</code> <code class="nb">all</code><code class="p">(</code><code class="n">elem</code> <code class="ow">in</code> <code class="n">it</code> <code class="k">for</code> <code class="n">elem</code> <code class="ow">in</code> <code class="n">expected</code><code class="p">):</code>
        <code class="n">score</code> <code class="o">=</code> <code class="mi">1</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">score</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">return</code> <code class="p">{</code><code class="s2">"score"</code><code class="p">:</code> <code class="nb">int</code><code class="p">(</code><code class="n">score</code><code class="p">),</code> <code class="s2">"key"</code><code class="p">:</code> <code class="s2">"multi_tool_call_in_order"</code><code class="p">}</code>

<code class="k">def</code> <code class="nf">contains_all_tool_calls_in_order_exact_match</code><code class="p">(</code>
    <code class="n">root_run</code><code class="p">:</code> <code class="n">Run</code><code class="p">,</code> <code class="n">example</code><code class="p">:</code> <code class="n">Example</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">:</code>
    <code class="sd">"""</code>
<code class="sd">    Check if all expected tools are called in exact order and without any </code>
<code class="sd">        additional tool calls.</code>
<code class="sd">    """</code>
    <code class="n">expected</code> <code class="o">=</code> <code class="p">[</code>
        <code class="s1">'sql_db_list_tables'</code><code class="p">,</code>
        <code class="s1">'sql_db_schema'</code><code class="p">,</code>
        <code class="s1">'sql_db_query_checker'</code><code class="p">,</code>
        <code class="s1">'sql_db_query'</code><code class="p">,</code>
        <code class="s1">'check_result'</code>
    <code class="p">]</code>
    <code class="n">messages</code> <code class="o">=</code> <code class="n">root_run</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s2">"response"</code><code class="p">]</code>
    <code class="n">tool_calls</code> <code class="o">=</code> <code class="n">find_tool_calls</code><code class="p">(</code><code class="n">messages</code><code class="p">)</code>
    <code class="c1"># Optionally, log the tool calls -</code>
    <code class="c1">#print("Here are my tool calls:")</code>
    <code class="c1">#print(tool_calls)</code>
    <code class="k">if</code> <code class="n">tool_calls</code> <code class="o">==</code> <code class="n">expected</code><code class="p">:</code>
        <code class="n">score</code> <code class="o">=</code> <code class="mi">1</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">score</code> <code class="o">=</code> <code class="mi">0</code>

    <code class="k">return</code> <code class="p">{</code><code class="s2">"score"</code><code class="p">:</code> <code class="nb">int</code><code class="p">(</code><code class="n">score</code><code class="p">),</code> <code class="s2">"key"</code><code class="p">:</code> <code class="s2">"multi_tool_call_in_exact_order"</code><code class="p">}</code>

<code class="n">experiment_results</code> <code class="o">=</code> <code class="n">evaluate</code><code class="p">(</code>
    <code class="n">predict_sql_agent_messages</code><code class="p">,</code>
    <code class="n">data</code><code class="o">=</code><code class="n">dataset_name</code><code class="p">,</code>
    <code class="n">evaluators</code><code class="o">=</code><code class="p">[</code>
        <code class="n">contains_all_tool_calls_any_order</code><code class="p">,</code>
        <code class="n">contains_all_tool_calls_in_order</code><code class="p">,</code>
        <code class="n">contains_all_tool_calls_in_order_exact_match</code>
    <code class="p">],</code>
    <code class="n">num_repetitions</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
<code class="p">)</code></pre>

<p><em>JavaScript</em></p>

<pre data-code-language="javascript" data-type="programlisting">
<code class="kr">import</code> <code class="p">{</code><code class="nx">evaluate</code><code class="p">}</code> <code class="nx">from</code> <code class="s1">'langsmith/evaluation'</code><code class="p">;</code>

<code class="c1">// Predict SQL Agent Messages</code>
<code class="kd">function</code> <code class="nx">predictSqlAgentMessages</code><code class="p">(</code><code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
  <code class="cm">/**</code>
<code class="cm">   * Use this for answer evaluation</code>
<code class="cm">   */</code>
  <code class="kr">const</code> <code class="nx">msg</code> <code class="o">=</code> <code class="p">{</code> <code class="nx">messages</code><code class="o">:</code> <code class="p">[{</code> <code class="nx">role</code><code class="o">:</code> <code class="s2">"user"</code><code class="p">,</code> <code class="nx">content</code><code class="o">:</code> <code class="nx">example</code><code class="p">.</code><code class="nx">input</code> <code class="p">}]</code> <code class="p">};</code>
  <code class="c1">// Replace with your graph and config</code>
  <code class="kr">const</code> <code class="nx">messages</code> <code class="o">=</code> <code class="nx">graph</code><code class="p">.</code><code class="nx">invoke</code><code class="p">(</code><code class="nx">msg</code><code class="p">,</code> <code class="nx">config</code><code class="p">);</code> 
  <code class="k">return</code> <code class="p">{</code> <code class="nx">response</code><code class="o">:</code> <code class="nx">messages</code> <code class="p">};</code>
<code class="p">}</code>

<code class="c1">// Find Tool Calls</code>
<code class="kd">function</code> <code class="nx">findToolCalls</code><code class="p">({</code><code class="nx">messages</code><code class="p">})</code> <code class="p">{</code>
  <code class="cm">/**</code>
<code class="cm">   * Find all tool calls in the messages returned</code>
<code class="cm">   */</code>
  <code class="k">return</code> <code class="nx">messages</code><code class="p">.</code><code class="nx">flatMap</code><code class="p">(</code><code class="nx">m</code> <code class="o">=&gt;</code> <code class="nx">m</code><code class="p">.</code><code class="nx">tool_calls</code><code class="o">?</code><code class="p">.</code><code class="nx">map</code><code class="p">(</code><code class="nx">tc</code> <code class="o">=&gt;</code> <code class="nx">tc</code><code class="p">.</code><code class="nx">name</code><code class="p">)</code> <code class="o">||</code> <code class="p">[]);</code>
<code class="p">}</code>

<code class="c1">// Contains All Tool Calls (Any Order)</code>
<code class="kd">function</code> <code class="nx">containsAllToolCallsAnyOrder</code><code class="p">(</code><code class="nx">rootRun</code><code class="p">,</code> <code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
  <code class="cm">/**</code>
<code class="cm">   * Check if all expected tools are called in any order.</code>
<code class="cm">   */</code>
  <code class="kr">const</code> <code class="nx">expected</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s2">"sql_db_list_tables"</code><code class="p">,</code>
    <code class="s2">"sql_db_schema"</code><code class="p">,</code>
    <code class="s2">"sql_db_query_checker"</code><code class="p">,</code>
    <code class="s2">"sql_db_query"</code><code class="p">,</code>
    <code class="s2">"check_result"</code>
  <code class="p">];</code>
  <code class="kr">const</code> <code class="nx">messages</code> <code class="o">=</code> <code class="nx">rootRun</code><code class="p">.</code><code class="nx">outputs</code><code class="p">.</code><code class="nx">response</code><code class="p">;</code>
  <code class="kr">const</code> <code class="nx">toolCalls</code> <code class="o">=</code> <code class="nx">findToolCalls</code><code class="p">(</code><code class="nx">messages</code><code class="p">);</code>

  <code class="kr">const</code> <code class="nx">score</code> <code class="o">=</code> <code class="nx">expected</code><code class="p">.</code><code class="nx">every</code><code class="p">(</code><code class="nx">tool</code> <code class="o">=&gt;</code> <code class="nx">toolCalls</code><code class="p">.</code><code class="nx">includes</code><code class="p">(</code><code class="nx">tool</code><code class="p">))</code> <code class="o">?</code> <code class="mi">1</code> <code class="o">:</code> <code class="mi">0</code><code class="p">;</code>
  <code class="k">return</code> <code class="p">{</code> <code class="nx">score</code><code class="p">,</code> <code class="nx">key</code><code class="o">:</code> <code class="s2">"multi_tool_call_any_order"</code> <code class="p">};</code>
<code class="p">}</code>

<code class="c1">// Contains All Tool Calls (In Order)</code>
<code class="kd">function</code> <code class="nx">containsAllToolCallsInOrder</code><code class="p">(</code><code class="nx">rootRun</code><code class="p">,</code> <code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
  <code class="cm">/**</code>
<code class="cm">   * Check if all expected tools are called in exact order.</code>
<code class="cm">   */</code>
  <code class="kr">const</code> <code class="nx">messages</code> <code class="o">=</code> <code class="nx">rootRun</code><code class="p">.</code><code class="nx">outputs</code><code class="p">.</code><code class="nx">response</code><code class="p">;</code>
  <code class="kr">const</code> <code class="nx">toolCalls</code> <code class="o">=</code> <code class="nx">findToolCalls</code><code class="p">(</code><code class="nx">messages</code><code class="p">);</code>

  <code class="kr">const</code> <code class="nx">expected</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s2">"sql_db_list_tables"</code><code class="p">,</code>
    <code class="s2">"sql_db_schema"</code><code class="p">,</code>
    <code class="s2">"sql_db_query_checker"</code><code class="p">,</code>
    <code class="s2">"sql_db_query"</code><code class="p">,</code>
    <code class="s2">"check_result"</code>
  <code class="p">];</code>

  <code class="kr">const</code> <code class="nx">score</code> <code class="o">=</code> <code class="nx">expected</code><code class="p">.</code><code class="nx">every</code><code class="p">(</code><code class="nx">tool</code> <code class="o">=&gt;</code> <code class="p">{</code>
    <code class="kd">let</code> <code class="nx">found</code> <code class="o">=</code> <code class="kc">false</code><code class="p">;</code>
    <code class="k">for</code> <code class="p">(</code><code class="kd">let</code> <code class="nx">call</code> <code class="k">of</code> <code class="nx">toolCalls</code><code class="p">)</code> <code class="p">{</code>
      <code class="k">if</code> <code class="p">(</code><code class="nx">call</code> <code class="o">===</code> <code class="nx">tool</code><code class="p">)</code> <code class="p">{</code>
          <code class="nx">found</code> <code class="o">=</code> <code class="kc">true</code><code class="p">;</code>
          <code class="k">break</code><code class="p">;</code>
      <code class="p">}</code>
    <code class="p">}</code>
    <code class="k">return</code> <code class="nx">found</code><code class="p">;</code>
  <code class="p">})</code> <code class="o">?</code> <code class="mi">1</code> <code class="o">:</code> <code class="mi">0</code><code class="p">;</code>

  <code class="k">return</code> <code class="p">{</code> <code class="nx">score</code><code class="p">,</code> <code class="nx">key</code><code class="o">:</code> <code class="s2">"multi_tool_call_in_order"</code> <code class="p">};</code>
<code class="p">}</code>

<code class="c1">// Contains All Tool Calls (Exact Order, Exact Match)</code>
<code class="kd">function</code> <code class="nx">containsAllToolCallsInOrderExactMatch</code><code class="p">(</code><code class="nx">rootRun</code><code class="p">,</code> <code class="nx">example</code><code class="p">)</code> <code class="p">{</code>
  <code class="cm">/**</code>
<code class="cm">   * Check if all expected tools are called in exact order and without any </code>
<code class="cm">   * additional tool calls.</code>
<code class="cm">   */</code>
  <code class="kr">const</code> <code class="nx">expected</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s2">"sql_db_list_tables"</code><code class="p">,</code>
    <code class="s2">"sql_db_schema"</code><code class="p">,</code>
    <code class="s2">"sql_db_query_checker"</code><code class="p">,</code>
    <code class="s2">"sql_db_query"</code><code class="p">,</code>
    <code class="s2">"check_result"</code>
  <code class="p">];</code>
  <code class="kr">const</code> <code class="nx">messages</code> <code class="o">=</code> <code class="nx">rootRun</code><code class="p">.</code><code class="nx">outputs</code><code class="p">.</code><code class="nx">response</code><code class="p">;</code>
  <code class="kr">const</code> <code class="nx">toolCalls</code> <code class="o">=</code> <code class="nx">findToolCalls</code><code class="p">(</code><code class="nx">messages</code><code class="p">);</code>

  <code class="kr">const</code> <code class="nx">score</code> <code class="o">=</code> <code class="nx">JSON</code><code class="p">.</code><code class="nx">stringify</code><code class="p">(</code><code class="nx">toolCalls</code><code class="p">)</code> <code class="o">===</code> <code class="nx">JSON</code><code class="p">.</code><code class="nx">stringify</code><code class="p">(</code><code class="nx">expected</code><code class="p">)</code> 
    <code class="o">?</code> <code class="mi">1</code> 
    <code class="o">:</code> <code class="mi">0</code><code class="p">;</code>
  <code class="k">return</code> <code class="p">{</code> <code class="nx">score</code><code class="p">,</code> <code class="nx">key</code><code class="o">:</code> <code class="s2">"multi_tool_call_in_exact_order"</code> <code class="p">};</code>
<code class="p">}</code>

<code class="c1">// Experiment Results</code>
<code class="kr">const</code> <code class="nx">experimentResults</code> <code class="o">=</code> <code class="nx">evaluate</code><code class="p">(</code><code class="nx">predictSqlAgentMessages</code><code class="p">,</code> <code class="p">{</code>
  <code class="nx">data</code><code class="o">:</code> <code class="nx">datasetName</code><code class="p">,</code>
  <code class="nx">evaluators</code><code class="o">:</code> <code class="p">[</code>
    <code class="nx">containsAllToolCallsAnyOrder</code><code class="p">,</code>
    <code class="nx">containsAllToolCallsInOrder</code><code class="p">,</code>
    <code class="nx">containsAllToolCallsInOrderExactMatch</code>
  <code class="p">],</code>
  <code class="nx">numRepetitions</code><code class="o">:</code> <code class="mi">3</code><code class="p">,</code>
<code class="p">});</code></pre>

<p>This implementation example includes the following:</p>

<ul>
	<li>
	<p>Invoking a precompiled LangGraph agent <code>graph.invoke</code> with a prompt</p>
	</li>
	<li>
	<p>Utilizing a specialized agent where the tools are hardcoded rather than passed with the dataset input</p>
	</li>
	<li>
	<p>Extracting of the list of tools called using the function <code>find_tool_calls</code></p>
	</li>
	<li>
	<p class="fix_tracking">Checking if all expected tools are called in any order using the function <code>contains_all_tool_calls_any_order</code> or called in order using <span class="keep-together"><code>contains_all_tool_calls_in_order</code></span></p>
	</li>
	<li>
	<p>Checking whether all expected tools are called in the exact order using <span class="keep-together"><code>contains_all_tool_calls_in_order_exact_match</code></span></p>
	</li>
</ul>

<p>All<a contenteditable="false" data-primary="LangSmith" data-secondary="agent evaluation example" data-type="indexterm" id="id931"/> three of these agent evaluation methods can be observed and debugged in <span class="keep-together">LangSmith’s</span> experimentation UI (see <a data-type="xref" href="#ch10_figure_16_1736545678096148">Figure 10-16</a>).</p>

<figure><div class="figure" id="ch10_figure_16_1736545678096148"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_1016.png"/>
<h6><span class="label">Figure 10-16. </span>Example of an agent evaluation test in the LangSmith UI</h6>
</div></figure>

<p>In general, these tests are a solid starting point to help mitigate an agent’s cost and unreliability due to LLM invocations and variability in tool calling.<a contenteditable="false" data-primary="" data-startref="Tpre10" data-type="indexterm" id="id932"/><a contenteditable="false" data-primary="" data-startref="trajecteval10" data-type="indexterm" id="id933"/><a contenteditable="false" data-primary="" data-startref="AEPtraject10" data-type="indexterm" id="id934"/><a contenteditable="false" data-primary="" data-startref="PPSTendtoend10" data-type="indexterm" id="id935"/></p>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Production" data-type="sect1"><div class="sect1" id="ch10_production_1736545678109752">
<h1>Production</h1>

<p>Although<a contenteditable="false" data-primary="testing" data-secondary="production stage" data-type="indexterm" id="Tprod10"/> testing in the preproduction phase is useful, certain bugs and edge cases may not emerge until your LLM application interacts with live users. These issues can affect latency, as well as the relevancy and accuracy of outputs. In addition, observability<a contenteditable="false" data-primary="observability" data-type="indexterm" id="id936"/> and the process of<a contenteditable="false" data-primary="online evaluation" data-type="indexterm" id="id937"/> <em>online evaluation</em> can help ensure that there are guardrails for LLM inputs or outputs. These guardrails can provide much-needed protection from prompt injection and toxicity.</p>

<p>The first step in this process is to set up LangSmith’s tracing feature.</p>

<section data-pdf-bookmark="Tracing" data-type="sect2"><div class="sect2" id="ch10_tracing_1736545678109812">
<h2>Tracing</h2>

<p>A <em>trace</em> is<a contenteditable="false" data-primary="production stage testing" data-secondary="tracing" data-type="indexterm" id="id938"/><a contenteditable="false" data-primary="tracing" data-type="indexterm" id="id939"/> a series of steps that your application takes to go from input to output. LangSmith makes it easy to visualize, debug, and test each trace generated from your app.</p>

<p>Once you’ve installed the relevant LangChain and LLM dependencies, all you need to do is configure the tracing environment variables based on your LangSmith account credentials:</p>

<pre data-type="programlisting">
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=&lt;your-api-key&gt;

# The below examples use the OpenAI API, though you can use other LLM providers

export OPENAI_API_KEY=&lt;your-openai-api-key&gt;</pre>

<p>After the environment variables are set, no other code is required to enable tracing. Traces will be automatically logged to their specific project in the “Tracing projects” section of the LangSmith<a contenteditable="false" data-primary="LangSmith" data-secondary="tracing" data-type="indexterm" id="id940"/> dashboard. The metrics provided include trace volume, success and failure rates, latency, token count and cost, and more—as shown in <a data-type="xref" href="#ch10_figure_17_1736545678096172">Figure 10-17</a>.</p>

<figure><div class="figure" id="ch10_figure_17_1736545678096172"><img alt="A screenshot of a computer  Description automatically generated" src="assets/lelc_1017.png"/>
<h6><span class="label">Figure 10-17. </span>An example of LangSmith’s trace performance metrics</h6>
</div></figure>

<p>You can review a variety of strategies to implement tracing based on your needs.</p>
</div></section>

<section data-pdf-bookmark="Collect Feedback in Production" data-type="sect2"><div class="sect2" id="ch10_collect_feedback_in_production_1736545678109874">
<h2>Collect Feedback in Production</h2>

<p>Unlike<a contenteditable="false" data-primary="production stage testing" data-secondary="collecting feedback" data-type="indexterm" id="id941"/><a contenteditable="false" data-primary="feedback, collecting in production" data-type="indexterm" id="id942"/> the preproduction phase, evaluators for production testing don’t have grounded reference responses for the LLM to compare against. Instead, evaluators need to score performance in real time as your application processes user inputs. This reference-free, real-time evaluation is often referred to as <em>online evaluation</em>.</p>

<p>There are at least two types of feedback you can collect in production to improve app performance:</p>

<dl>
	<dt>Feedback from users</dt>
	<dd>
	<p>You can directly collect user feedback explicitly or implicitly. For example, giving users the ability to click a like and dislike button or provide detailed feedback based on the application’s output is an effective way to track user satisfaction. In LangSmith, you can attach user feedback to any trace or intermediate run (that is, span) of a trace, including annotating traces inline or reviewing runs together in an annotation queue.</p>
	</dd>
	<dt>Feedback from LLM-as-a judge evaluators</dt>
	<dd>
	<p>As discussed previously, these evaluators can be implemented directly on traces to identify hallucination and toxic responses.</p>
	</dd>
</dl>

<p>The earlier preproduction section already discussed how to set up LangSmith’s auto evaluation in the Datasets &amp; Experiments section of the dashboard.</p>
</div></section>

<section data-pdf-bookmark="Classification and Tagging" data-type="sect2"><div class="sect2" id="ch10_classification_and_tagging_1736545678109933">
<h2>Classification and Tagging</h2>

<p>In<a contenteditable="false" data-primary="production stage testing" data-secondary="classification and tagging" data-type="indexterm" id="id943"/> order to implement effective guardrails against toxicity or gather insights on user sentiment analysis, we need to build an effective system for labeling user inputs and generated outputs.</p>

<p>This system is largely dependent on whether or not you have a dataset that contains reference labels. If you don’t have preset labels, you can use the LLM-as-a-judge evaluator to assist in performing classification and tagging based upon specified criteria.</p>

<p>If, however, ground truth classification labels are provided, then a custom heuristic evaluator can be used to score the chain’s output relative to the ground truth class labels.</p>
</div></section>

<section data-pdf-bookmark="Monitoring and Fixing Errors" data-type="sect2"><div class="sect2" id="ch10_monitoring_and_fixing_errors_1736545678109990">
<h2>Monitoring and Fixing Errors</h2>

<p>Once<a contenteditable="false" data-primary="production stage testing" data-secondary="monitoring and fixing errors" data-type="indexterm" id="id944"/><a contenteditable="false" data-primary="errors, monitoring" data-type="indexterm" id="id945"/><a contenteditable="false" data-primary="monitoring errors" data-type="indexterm" id="id946"/> your application is in production, LangSmith’s tracing will catch errors and edge cases. You can add these errors into your test dataset for offline evaluation in order to prevent recurrences of the same issues.</p>

<p>Another useful strategy is to release your app in phases to a small group of beta users before a larger audience can access its features. This will enable you to uncover crucial bugs, develop a solid evaluation dataset with ground truth references, and assess the general performance of the app including cost, latency, and quality of outputs.<a contenteditable="false" data-primary="" data-startref="Tprod10" data-type="indexterm" id="id947"/></p>
</div></section>
</div></section>

<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch10_summary_1736545678110043">
<h1>Summary</h1>

<p>As discussed in this chapter, robust testing is crucial to ensure that your LLM application is accurate, reliable, fast, toxic-free, and cost-efficient. The three key stages of LLM app development create a data cycle that helps to ensure high performance throughout the lifetime of the application.</p>

<p>During the design phase, in-app error handling enables self-correction before the error reaches the user. Preproduction testing ensures each of your app’s updates avoids regression in performance metrics. Finally, production monitoring gathers real-time insights and application errors that inform the subsequent design process and the cycle repeats.</p>

<p>Ultimately, this process of testing, evaluation, monitoring, and continuous improvement, will help you fix issues and iterate faster, and most importantly, deliver a product that users can trust to consistently deliver their desired results.</p>
</div></section>
</div></section></body></html>