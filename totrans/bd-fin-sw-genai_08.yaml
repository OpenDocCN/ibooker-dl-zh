- en: 7 Our minimum viable product
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 我们的最小可行产品
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The purpose of minimum viable product (MVP)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小可行产品（MVP）的目的
- en: Putting everything together into an MVP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一切整合到MVP中
- en: Security concerns
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全问题
- en: Testing with Playwright
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Playwright进行测试
- en: Over the previous sprints, we worked on spike stories examining our project’s
    various components. You were introduced to generative AI and different tools that
    can expedite software development. We also reviewed specific technologies, such
    as Python/FastAPI, Postgres, Docker, and Next.js. Now, the line of business would
    like to see the fruit of our labor. This chapter focuses on putting everything
    together into a minimum viable product (MVP), which allows a user to
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的迭代中，我们研究了检查项目各个组件的spike故事。你被介绍到生成式AI和可以加速软件开发的不同工具。我们还回顾了特定的技术，如Python/FastAPI、Postgres、Docker和Next.js。现在，业务线希望看到我们劳动的成果。本章重点是将一切整合成一个最小可行产品（MVP），使用户能够
- en: Upload an ACH file
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上传ACH文件
- en: Parse an ACH file and store it in the database
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析ACH文件并将其存储在数据库中
- en: Visualize the results in our dashboard
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的仪表板上可视化结果
- en: The functionality will not expand significantly from what was developed in the
    previous sprints. Instead, we want to ensure those individual components are now
    integrated seamlessly. Along the way, we will explore what happens in the system
    demo and validate our MVP using Playwright. Having the MVP will allow us to release
    something to our customers and gather feedback, which we’ll use to enhance our
    project in the coming chapters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 功能性不会从之前迭代中开发的内容显著扩展。相反，我们想要确保这些个别组件现在可以无缝集成。在这个过程中，我们将探索系统演示中会发生什么，并使用Playwright验证我们的MVP。拥有MVP将使我们能够向客户发布产品并收集反馈，我们将利用这些反馈在接下来的章节中改进我们的项目。
- en: 7.1 Which minimum are we talking about?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 我们在谈论哪个最小值？
- en: During your career, you have probably heard about at least a few different minimums,
    such as
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的职业生涯中，你可能已经听说过至少几个不同的最小值，例如
- en: '*Minimum lovable product (MLP**)*—Similar to an MVP but focuses on delivering
    something that customers love from the start'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小可爱产品（MLP**）*—类似于MVP，但专注于从一开始就提供客户喜爱的东西'
- en: '*Minimum marketable product (MMP**)*—Focuses on delivering the smallest amount
    of functionality that can be sold to our customers'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小市场产品（MMP**）*—专注于提供可以销售给客户的最小功能集'
- en: '*Minimum marketable feature (MMF**)*—Focuses on delivering software features
    that are of value to our customers'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最小市场特性（MMF**）*—专注于提供对我们客户有价值的软件特性'
- en: The MVP concept was first introduced by Frank Robinson and popularized by Eric
    Ries in his book *The Lean Startup*. Ries defines MVP as “that version of a new
    product which allows a team to collect the maximum amount of validated learning
    about customers with the least effort.”
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最小可行产品（MVP）的概念最初由Frank Robinson提出，并由Eric Ries在其著作《精益创业》中推广。Ries将MVP定义为“那种新产品版本，它允许团队以最少的努力收集关于客户的最大量的验证学习。”
- en: Let’s take a moment to break this statement down. Obviously, our new product
    represents our modernization of the ACH dashboard, but what about “validated learning”?
    With validated learning, we are gaining knowledge about just how close we are
    to progressing toward a product that satisfies our customers. Validated learning
    should be evidence based and actionable, giving us something that we can use to
    improve our product in the next iteration. The “least effort” part does not mean
    producing something subpar but rather something achievable in the short term and
    with minimal investment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间来分析这个声明。显然，我们的新产品代表了我们对ACH仪表板的现代化，但“验证学习”又是什么呢？通过验证学习，我们正在获取关于我们如何接近满足客户需求的产品进展的知识。验证学习应该是基于证据和可操作的，给我们一些东西，我们可以用它来改进下一迭代的产品。这里的“最少努力”并不意味着生产出次品，而是指在短期内以最小的投资实现的目标。
- en: Our goal with the MVP of the ACH dashboard is to provide our customers with
    a product that begins to address their concerns about the existing dashboard but
    ultimately helps us learn about the final product. This approach lets us transform
    the product from something that may have been developed in relative isolation
    to something usable by the customer (in larger companies, customer requests may
    have been filtered down to us through multiple layers, with each request putting
    the company’s preferences on the product specs). Once our customers have the product
    in hand, we can use their feedback to help drive the direction of the product
    and provide the required features quickly. Compare this to other approaches where
    the entire product may be developed and delivered without meaningful feedback
    from the client base. We may end up with a product that is disliked because it
    is missing a feature customers may deem important, or it may not even be usable
    in a customer’s environment due to their unique needs. Indeed, we have seen fully
    developed products that had to be shelved because basic concerns about the problem
    at hand were not addressed or the environment in which they were needed to run
    was not considered. Table 7.1 provides a quick breakdown of key concepts regarding
    MVP, MLP, MMP, and MMF.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对ACH仪表板MVP的目标是向客户提供一款产品，它开始解决他们对现有仪表板的担忧，但最终帮助我们了解最终产品。这种方法使我们能够将产品从可能相对孤立开发的东西转变为客户可用的东西（在大型公司中，客户请求可能通过多个层级过滤给我们，每个请求都将公司的偏好体现在产品规格中）。一旦我们的客户拥有产品，我们可以利用他们的反馈来帮助推动产品的方向，并快速提供所需的功能。与此相比，其他方法可能在整个产品开发并交付过程中没有从客户群体中获得有意义的反馈。我们可能会得到一个不受欢迎的产品，因为它缺少客户可能认为重要的功能，或者它可能因为客户独特的需求而在客户环境中无法使用。确实，我们看到了完全开发的产品，因为未解决当前问题的基本担忧或未考虑它们需要运行的特定环境而被搁置。表7.1提供了关于MVP、MLP、MMP和MMF的关键概念的快速概述。
- en: Table 7.1 MVP, MLP, MMP, and MMF key concepts
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1 MVP、MLP、MMP和MMF的关键概念
- en: '|  | MVP | MMP | MLP | MMF |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | MVP | MMP | MLP | MMF |'
- en: '| Purpose | Tests and validates core assumptions | Creates a sellable version
    of the product | Delivers a product customers can connect with and love | Provides
    features that add value and can be marketed separately |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 测试和验证核心假设 | 创建可销售的产品版本 | 提供客户可以连接并喜爱的产品 | 提供增值功能并可单独进行市场推广 |'
- en: '| Developmentapproach | Prioritizes testing and learning | Balances functionality
    with market needs | Focuses on design and emotional engagement | Focuses on a
    single effective feature |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 开发方法 | 优先考虑测试和学习 | 平衡功能与市场需求 | 专注于设计和情感参与 | 专注于单一有效功能 |'
- en: '| Customervalue | Provides core value to early adopters | Provides a complete
    solution to target market needs | Creates an emotional connection and stands out
    in the market | Provides incremental improvement to the existing product |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 客户价值 | 为早期采用者提供核心价值 | 为目标市场需求提供完整解决方案 | 建立情感联系并在市场中脱颖而出 | 为现有产品提供渐进式改进 |'
- en: '| Successful outcome | Validation of core assumptions | Sales and market acceptance
    | Customer love and brand loyalty | Adoption rate and user engagement |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 成功结果 | 核心假设的验证 | 销售和市场接受度 | 客户喜爱和品牌忠诚度 | 采用率和用户参与度 |'
- en: 7.2 Preparing for the MVP
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 准备MVP
- en: In the following sections, we ensure that all our previous work is integrated
    into the MVP. But before embarking on that journey, we review the individual pieces
    and parts that will make up our MVP—ACH parsing, database design, and UI—which
    were discussed in previous chapters. Figure 7.1 shows the components necessary
    for this part of the project.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们确保所有之前的工作都整合到最小可行产品（MVP）中。但在开始这段旅程之前，我们回顾了将构成我们的MVP的各个部分和组件——ACH解析、数据库设计和用户界面，这些内容在之前的章节中已有讨论。图7.1展示了这个项目部分所需的组件。
- en: '![A diagram of a product  Description automatically generated](../Images/CH07_F01_Kardell.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![产品图  自动生成的描述](../Images/CH07_F01_Kardell.png)'
- en: Figure 7.1  The pieces and parts fed into the MVP
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1  输入到MVP中的各个部分和组件
- en: 'If our initial work is done properly, the building of the MVP should be relatively
    straightforward. After all, we’ve done a fair amount of upfront work to get various
    components to this point. Remember, our previous research spikes built out the
    following components:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的初步工作做得恰当，构建MVP的过程应该相对直接。毕竟，我们已经做了大量的前期工作，使各种组件达到这个阶段。记住，我们之前的研究峰值为以下组件的建设：
- en: An ACH parser in Python
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中的ACH解析器
- en: APIs written in Python using FastAPI that generated OpenAPI documentation
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用FastAPI编写的Python API生成的OpenAPI文档
- en: Design of a Postgres database to store the ACH data
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储ACH数据的Postgres数据库设计
- en: UI written in Next.js
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Next.js编写的UI
- en: In the worst-case scenario, we may come across the proverbial “square peg into
    a round hole” type of situation where we did not quite document (or adhere to
    documented functionality), which may require some rework to get those pieces to
    fit correctly. We also have the opportunity to include some final touches such
    as using Playwright for integration/system testing, which will allow us to ensure
    the product functions from an end-user’s perspective. Finally, we explore some
    additional security we may want to include in the project.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在最坏的情况下，我们可能会遇到“方枘圆凿”的典型情况，即我们没有很好地记录（或遵守记录的功能），这可能需要一些重工作以使这些部分正确对齐。我们还有机会添加一些最后的修饰，例如使用Playwright进行集成/系统测试，这将使我们能够从最终用户的角度确保产品功能。最后，我们探讨了一些可能希望在项目中包含的额外安全性。
- en: 7.3 Building out the /files APIs
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 构建出/files API
- en: In chapter 4, you learned how to build APIs in Python using FastAPI. Then, we
    used Next.js and WireMock to create mock API responses that could be returned
    to the UI with the needed data. At this point, we want to verify that the backend
    and the APIs align properly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，你学习了如何使用FastAPI在Python中构建API。然后，我们使用了Next.js和WireMock来创建可以返回给UI的模拟API响应，这些响应包含所需的数据。在这个阶段，我们想要验证后端和API是否正确对齐。
- en: First, we bring up the different components through our JetBrains IDE (PyCharm),
    which allows us to start our current database in Docker, API, and the Dashboard
    UI. By navigating to http://localhost:8000/docs (OpenAPI documentation), we can
    take advantage of the `POST` request we had built during our research (it may
    not be perfect but should load the file). Figure 7.2 shows the sample `POST` request
    that will also let us choose our sample.ach file.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过JetBrains IDE（PyCharm）调用了不同的组件，这使得我们可以启动当前数据库的Docker、API和仪表板UI。通过导航到http://localhost:8000/docs（OpenAPI文档），我们可以利用我们在研究期间构建的`POST`请求（可能不是完美的，但应该可以加载文件）。图7.2显示了将允许我们选择样本的示例`POST`请求。
- en: '![A white and green striped background  Description automatically generated
    with medium confidence](../Images/CH07_F02_Kardell.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![白色和绿色的条纹背景，描述自动生成，中等置信度](../Images/CH07_F02_Kardell.png)'
- en: Figure 7.2  The sample `POST` request
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2  示例`POST`请求
- en: 'If we refresh our page, the Recent ACH Uploads shows NaN (not a number) for
    many fields (see figure 7.3). There could be several reasons for getting NaN:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们刷新页面，最近上传的ACH文件在许多字段中显示NaN（不是一个数字）（见图7.3）。出现NaN可能有几个原因：
- en: '*Parsing error**s*—Parsing non-numeric data such as `parseInt("abcd")`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析错误*——解析非数字数据，如`parseInt("abcd")`'
- en: '`*   *Undefined or null value**s*—Using a field from the API that does not
    exist*   *Arithmetic error**s*—Dividing by zero`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`*   *未定义或null值*——使用API中不存在的字段*   *算术错误*——除以零`'
- en: '[PRE0] [  {  "ach_files_id": "dcb8e6a4-79e9-4cf7-a138-491e58c1ebb8",   "file_name":
    "sample.ach",     "file_hash": "application/octet-stream",     "created_at": "2024-02-18T01:05:37.513025"   }
    ] [PRE1] interface AchUpload {     id: number;     date: string;     filename:
    string;     creditTotal: number;     debitTotal: number; } [PRE2] total_debit_entry_dollar_amount
    NUMERIC(12, 2) NOT NULL, total_credit_entry_dollar_amount NUMERIC(12, 2) NOT NULL,
    [PRE3] total_debit_entry_dollar_amount= #1    Decimal(f"{record[31:41]}.{record[41:43]}"),  #1
    total_credit_entry_dollar_amount=  #1    Decimal(f"{record[43:53]}.{record[53:55]}"),  #1
    [PRE4] schemas/ ├─ api/ ├─ database/ │  ├─ ach_record/ │  │  ├─ ach_record_base_schema.py
    │  │  ├─ ... │  ├─ ach_file_schema.py │  ├─ ... [PRE5] from decimal import Decimal
    from datetime import datetime from pydantic import BaseModel, UUID4, Field   class
    AchFilesResponse(BaseModel):     id: UUID4 = Field(...,  #1         description="Unique
    identifier for the ACH file", #2         title="ID")  #3     date: datetime =
    Field(...,         description="The date and time the ACH file was uploaded.",          title="Date")     filename:
    str = Field(...,         title="Filename",         description="The name of the
    file the ACH file was loaded from.",         max_length=255, #4     )     credit_total:
    Decimal = Field(         ...,         description="The total amount of credit
    ➥ transactions in the ACH file.",         ge=0, #5         title="Credit Total",     )     debit_total:
    Decimal = Field(         ...,         description="The total amount of debit  ➥transactions
    in the ACH file.",         ge=0,         title="Debit Total",     )      class
    Config:                                       json_schema_extra = {                               "example":
    {                                        "id": "123e4567-e89b-12d3-a456-426614174000",                  "date":
    "2024-01-01T12:00:00",                   "filename": "ACH_20240101_123.ach",                  "creditTotal":
    "1000.00",                 "debitTotal": "5000.23",             }         } [PRE6]
    def get_files_response(     self, limit: Optional[int] = None, offset: Optional[int]
    = None ) -> list[AchFilesResponse]:     with get_db_connection(row_factory=dict_row)
    as conn:         result = conn.execute(             """             SELECT af.ach_files_id
    AS id, #1                    af.file_name AS filename,  #1                    af.created_at
    AS date,  #1                    afcr.total_debit_entry_dollar_amount AS debit_total,  #1                    afcr.total_credit_entry_dollar_amount
    AS credit_total #1             FROM ach_files AS af             INNER JOIN ach_records_type_1
    AS art1 USING (ach_files_id) #2             INNER JOIN ach_records_type_9 AS art9                 USING
    (ach_records_type_1_id)             INNER JOIN ach_file_control_records AS afcr                 USING
    (ach_records_type_9_id)             ORDER BY af.created_at DESC  #3             LIMIT
    %s  #4             OFFSET %s                """,             [limit, offset],         )         return
    result.fetchall() [PRE7] @router.get( #1     "",  #1     response_model=list[AchFilesResponse],  #1     summary="Retrieve
    Uploaded ACH Files",  #1     description="Retrieve the details of an ACH #1 ➥
    file including credit/debit totals.",  #1     response_description="The details
    of the #1 ➥ requested ACH file.",  #1 ) async def read_files() -> list[AchFilesResponse]:     return
    AchFileSql().get_files_response() [PRE8] interface AchFilesResponse {     id:
    number;     date: string;     filename: string;     credit_total: number;     debit_total:
    number; } [PRE9] axios.get<AchFilesResponse[]>(`${apiUrl}/files`)    .then(response
    => {       console.log(`Response data ➥ ${JSON.stringify(response.data)}`); #1       const
    transformedData: AchFiles[] = ➥ response.data.map((row: AchFilesResponse) => ({
    #2  id: row.id, #2  date: row.date, #2  filename: row.filename, #2  creditTotal:
    row.credit_total, #2  debitTotal: row.debit_total #2  }));       setRows(transformedData);  #3    })
    [PRE10] Feature: The /files endpoint   Test the functionality of the /files endpoint    Scenario:
    I want to get a list that contains a single file     Given that I have a clean
    database     And that I have posted the file "sample.ach"     When I request a
    list of files     Then I should have a single file named "sample.ach"     And
    it should have a total credit amount of "1146114.80"     And it should have a
    total debit amount of "78.25"     And there should be no exceptions    Scenario:
    I want to get a list of files     Given that I have a clean database     And that
    I have posted the file "sample.ach"     And that I have posted the file "sample1.ach"     When
    I request a list of files     Then I should have a file that includes the file
    "sample.ach"     And I should have a file that includes the file "sample1.ach"     And
    there should be no exceptions [PRE11] … client = TestClient(app) #1  scenarios("../features/ach_files_endpoint.feature")
    #2  @pytest.fixture   #3 def ach_file_processor() -> AchFileProcessor:  #3     return
    AchFileProcessor()   @pytest.fixture    #4 def api_response():  #4     return
    {}   @given("that I have a clean database") #5 def truncate_database():    #5   SqlUtils.truncate_all()   @given(parsers.re(r''that
    I have posted the file#6 ➥ "(?P<ach_file>.*)"''))  #6 def parse_the_given_file(ach_file):  #6     dir_path
    = os.path.dirname(os.path.realpath #6 ➥(__file__))  #6     file_path = os.path.join(dir_path,
    "../data", #6 ➥ ach_file)  #6     parser = AchFileProcessor()  #6     ach_files_id
    = SqlUtils.create_ach_file_record(ach_file, ➥ str(randint(1, 99999999))) #7     parser.parse(ach_files_id,
    file_path)  … [PRE12] … @when("I request a list of files") #1 def request_files(api_response):
    #2  response = client.get("/api/v1/files") #3  assert response.status_code ==
    200, response.text #4   print(response.json()) #5     api_response["response"]
    = response.json() #6 … [PRE13] @then(parsers.re(''I should have a file that ➥
    includes the file "(?P<filename>.*)"'')) #1 def then_file_check(filename, api_response):
    #2     assert any(          response["filename"] == filename for response  #3             in
    api_response["response"]  #3     ), (f"Expected {filename} in"  #3 ➥f" {api_response[''response'']}")  #3  #3
    @then(parsers.parse(''it should have a total credit ➥ amount of "{credit_amount}"''))
    #4 def then_total_credit(credit_amount, api_response): #5     response = api_response["response"][0]
    ➥["credit_total"] #6     assert credit_amount == response, f"Expected ➥ {credit_amount}
    in {response}" #7 [PRE14] class TestAchBatchesApi:     client: TestClient = TestClient(app)
    #1     ach_files_id: Optional[str] = None      def setup_method(self, method:
    Callable) -> None:         ach_file = "../data/sample.ach" #2         SqlUtils.truncate_all()  #2         self.ach_files_id
    = SqlUtils.create_ach_file_record(  #2             ach_file, str(randint(1, 99999999))  #2         )  #2         AchFileProcessor().parse(self.ach_files_id,
    #2 ➥ "data/sample.ach")  #2      def test_get_batches_api(self):  #3         response
    =   self.client.get("/api/v1/files/{self. #3 ➥ach_files_id}/batches")  #3         assert
    response.status_code == 200, #3 ➥ response.text #3    def teardown_method(self,
    method: Callable)#4 ➥ -> None:  #4         print(f"\nTeardown for {method.__name__}
    #4 ➥ test method execution")  [PRE15] @router.get("/{file_id}/batches") async
    def read_batches_from_file(file_id: UUID) -> str:  #1     return f"File {file_id}
    batches"   [PRE16] @router.get("/{file_id}/batches") async def read_batches_from_file(file_id:
    UUID) -> list[AchBatchesResponse]:  #1     return AchFileSql().get_batches(file_id)
    #2 [PRE17] from decimal import Decimal from pydantic import BaseModel, UUID4,
    Field   class AchBatchesResponse(BaseModel): #1     id: UUID4 = Field(         ...,          description="Unique
    identifier for the ACH batch",         title="ID"     )     company_name: str
    = Field(         None,          description="The name of the company.",         title="Company
    Name"     )     batch_number: int = Field( #2         ...,  #2         title="Batch
    Number",  #2         description="The number associated with the batch.",  #2         ge=0,  #2     )  #2     credit_total:
    Decimal = Field( #3         ...,  #3         description="The total amount of
    credit #3 ➥ transactions in the ACH batch.",  #3         title="Credit Total",  #3     )  #3     debit_total:
    Decimal = Field(         ...,         description="The total amount of debit ➥
    transactions in the ACH batch.",         title="Debit Total",     )     entry_addenda_count:
    int = Field(         ...,         description="The number of addenda records in
    the ACH batch.",         title="Entry Addenda Count",         ge=0,     )      class
    Config:         json_schema_extra = {             "example": {                 "id":
    "123e4567-e89b-12d3-a456-426614174000",                 "company_name": "Company
    Name",                 "batch_number": 1, #4                 "credit_total": "100.00",  #4                 "debit_total":
    "100.00",  #4                 "entry_addenda_count": 2,  #4             }         }
    [PRE18] CREATE TABLE ach_batch_control_records (     ach_records_type_8_id UUID
    UNIQUE NOT ➥ NULL REFERENCES ach_records_type_8(ach_records_type_8_id) ➥ ON DELETE
    CASCADE ON UPDATE CASCADE,     record_type_code VARCHAR(1) NOT NULL,     service_class_code
    NUMERIC(3) NOT NULL,  entry_addenda_count NUMERIC(6) NOT NULL, #1  entry_hash
    NUMERIC(10) NOT NULL, #1  total_debit_entry_dollar_amount NUMERIC(12,2) NOT NULL,
    #1  total_credit_entry_dollar_amount NUMERIC(12,2) NOT NULL, #1     company_identification
    VARCHAR(10) NOT NULL,     message_authentication_code VARCHAR(19) NOT NULL,     reserved
    VARCHAR(6) NOT NULL,     originating_dfi_identification VARCHAR(8) NOT NULL,  batch_number
    NUMERIC(7) NOT NULL #A ); [PRE19] [   {     "id": "e89511cc-9791-4318-b3b7-76406c191442",     "company_name":
    "",     "batch_number": 4,     "credit_total": "114609873.00",     "debit_total":
    "0.00",     "entry_addenda_count": 32   },   {     "id": "92ece8a1-e9b8-4bf6-b3df-416d7efa2909",     "company_name":
    "CCD coname",     "batch_number": 2,     "credit_total": "201.00",     "debit_total":
    "404.00",     "entry_addenda_count": 6   }, … [PRE20] import os #1 from typing
    import TYPE_CHECKING #1 import pdb #1 from pylint.checkers import BaseChecker
    #1  ALLOWED_MODULES = ["ach_file_sql"] #2 ALLOWED_DIRECTORIES = ["ach_processor/database"]   if
    TYPE_CHECKING:     from pylint.lint import PyLinter  class PsycopgImportChecker(BaseChecker):      name
    = "psycopg-import-checker" #3     msgs = {  #3         "E9999": ( #3             "psycopg
    usage only allowed in specific modules",  #3             "psycopg-disallowed-import",  #3             "Used
    when psycopg is imported outside #3 ➥ designated modules.",  #3         ),  #3     }  #3     options
    = () #3      def visit_import(self, node): #4         if any(modname == "psycopg"
    for #4 ➥ modname, _ in node.names):  #4             self._check_psycopg_usage(node)  #4         elif
    any(modname.startswith("psycopg") #4 ➥ for modname, _ in node.names):  #4             self._check_psycopg_usage(node)  #4  #4     def
    visit_importfrom(self, node):  #4         if node.modname.startswith("psycopg"):  #4             self._check_psycopg_usage(node)  #4  #4     def
    _check_psycopg_usage(self, node): #5         module_name = node.root().name #5         module_file
    = node.root().file #5         if module_name in ALLOWED_MODULES:  #5             return  #
    Allow usage in specific modules #5  #5         for allowed_dir in ALLOWED_DIRECTORIES:  #5             #
    Ensure paths are absolute or relative to #5 ➥ the project root #5             allowed_path
    = os.path.normpath(allowed_dir)  #5             module_path = os.path.normpath(module_file)  #5             if
    allowed_path in module_path:  #5                 return #5  #5         self.add_message("psycopg-disallowed-import",
    ➥ node=node) #6  def register(linter): #7  linter.register_checker #7 ➥(PsycopgImportChecker(linter))   #7
    [PRE21] {    "batches" : [        {           "companyName": "example name",           "companyBatches":
    [              {                 "batchId": "value"                 "batchNumber":
    "value"                 "recordCount": "value"                 "totalDebitAmount":
    "value"                 "totalCreditAmount": "value"              }           ]          }
    } [PRE22] export interface AchCompanyBatchInfo { #1     companyName: string;      companyBatches:
    AchBatchInfo[];  }   export interface AchBatchInfo {      id: string;      batchNumber:
    number;      debitTotal: Decimal; #2     creditTotal: Decimal;      recordCount:
    number;  }   export interface AchBatchInfoResponse { #3     id: string;  #3     company_name:
    string;  #3     batch_number: number;  #3     debit_total: string;  #3     credit_total:
    string;  #3     entry_addenda_count: number;  #3 }  #3 [PRE23] function createAchCompanyBatchInfoRecords
    ➥(response: AchBatchInfoResponse[]): AchCompanyBatchInfo[] {     const condensedRecords:
    Record<string, ➥ AchCompanyBatchInfo> = {}; #1      response.forEach((record:
    AchBatchInfoResponse) => {         if (!condensedRecords[record.company_id]) {
    #2             condensedRecords[record.company_id] = {  #2                 companyId:
    record.company_id,  #2                 companyBatches: []  #2             };  #2         }  #2          condensedRecords[record.company_name]
    ➥.companyBatches.push({ #3             id: record.id,  #3         companyId: record.company_id,  #3             companyName:
    record.company_name,  #3             batchNumber: record.batch_number,  #3             debitTotal:
    new Decimal(record.debit_total),  #3             creditTotal: new Decimal(record.credit_total),  #3             recordCount:
    record.entry_addenda_count,  #3         });  #3     });      return Object.values(condensedRecords);
    [PRE24] totalCreditAndDebits: companyBatch.creditTotal.add(companyBatch.debitTotal)
    [PRE25] …     const [isLoading, setIsLoading] =  ➥useState<boolean>(false); #1
    …     const uploadFile = (file: File) => {         const formData = new FormData();
    #2         formData.append(''file'', file);           const apiUrl = process.env.NEXT_PUBLIC_API_URL
    ?? ''''; #3         setIsLoading(true); #4         axios.post(`${apiUrl}/files`,
    formData, { #5             headers: {  #6                 ''Content-Type'': ''multipart/form-data'',  #6             },  #6         })  #6             .then((response)
    => {                  console.log(''File uploaded successfully'', response);              })              .catch((error)
    => {                  console.error(''Error uploading file'', error);              }).finally(()
    => { #8             setIsLoading(false);  #9             route.replace(''/'');  #9         });     }
    [PRE26]     const handleDrop = useCallback((event: ➥ DragEvent<HTMLDivElement>)
    => { …         if (files.length > 0 && files[0].name !== '''') {  console.log(files);  setFileInfo(files[0]);
    uploadFile(files[0]); #1         }      }, []); [PRE27] …     const fileInputRef
    = useRef<HTMLInputElement> ➥(null); #1 … const handleFileChange = (event:  ➥ChangeEvent<HTMLInputElement>)
    => {         if (event.target.files &&  ➥event.target.files.length > 0) { #2             const
    file = event.target.files[0];  #2             console.log(file);  #2             uploadFile(file);  #2         }     };
    …     const handleButtonClick = () => {         fileInputRef.current?.click();
    #3     }; …          {isLoading ? <CircularProgress/> : <Typography              variant="h6">                 {fileInfo?.name
    ?  ➥fileInfo.name : ''Drag and drop files here''}               </Typography>}
    …     <Button onClick={handleButtonClick}             startIcon={<CloudUpload/>}>        Upload     </Button>     <input        type="file"        ref={fileInputRef}        onChange={handleFileChange}
    #4        style={{display: ''none''}}     /> [PRE28] origins = [     "http://localhost:4000",  #
    For local development     # "*" # Allow all, but not recommended for production
    ] … app.add_middleware(     CORSMiddleware,     allow_origins=origins,     allow_credentials=True,     allow_methods=["GET",
    "POST"],     allow_headers=["*"], ) … [PRE29] -r requirements.txt pytest==8.0.1
    pytest-bdd==7.0.1 iniconfig==2.0.0 pluggy==1.4.0 [PRE30] {   "compilerOptions":
    { …     "sourceMap": false,     "types": [], …   "exclude": ["**/*.test.ts", "**/*.spec.ts",
    "node_modules"] } [PRE31]   dashboard:     build:        context: ./dashboard       dockerfile:
    Dockerfile       args:  - NEXT_PUBLIC_API_URL=http://localhost:8000/api/v1     ports:       -
    4000:3000 [PRE32] NAMES                     STATUS dockermvp-dashboard-1     Up
    58 minutes#1 dockermvp-api-1           Up 58 minutes #1 dockermvp-cloudbeaver-1   Up
    58 minutes #1 dockermvp-postgres-1      Up 58 minutes (healthy)  #1 [PRE33]   Scenario:
    Create an ACH file with a single batch and a single debit     Given I want to
    create an ACH file named "ppd-single-debit.ach"     And I want to have an immediate
    destination of "123456789"     And I want to have an immediate origin of "987654321"     And
    I want to have 1 batch with ACH debits only ➥ and a standard entry class code
    of "PPD"     And I want 1 entries per batch with random amounts between 100 and
    100     And I want to use individual names of "John Doe"     And I want to have
    company name "My Company" and ➥ company id "1234567890"     When my ACH is created     Then
    I should have a file named "ppd-single-debit.ach"     And there should be 1 batch
    in the file     And there should be 1 entries in the file [PRE34] @given(     parsers.re(         r''I
    want to have (?P<batch_count>\d+) ➥ batch with ACH debits only and a standard
    entry  class code of "(?P<standard_entry_class>.*)"''     ) ) def set_number_of_debit_batches_to_create(     setup_info,
    batch_count, standard_entry_class ):     setup_info["batch_count"] = int(batch_count)
    #1     setup_info["service_class_code"] = "225" #2     setup_info["transaction_code"]
    = ["27"]      setup_info["standard_entry_class"] = standard_entry_class #3 [PRE35]
    @then(parsers.parse("there should be ➥ {expected_entry_count:d} entries in the
    file")) def validate_entry_count(setup_info, expected_entry_count):     count
    = 0     with open(f"../output/{setup_info[''filename'']}", ➥ "r", encoding="utf8")
    as f: #1         for line in f: #2             if line.startswith("6"):  #2                 count
    += 1     assert (         count == expected_entry_count     ), f"Expected {expected_entry_count},
    but got {count}" #3 [PRE36] def create_file_control(setup_info, total_debits_in_file,
    ➥ total_credits_in_file):     total_credits_in_file = ➥ str(total_credits_in_file).rjust(12,
    "0") #1     total_debits_in_file = #1 ➥ str(total_debits_in_file).rjust(12, "0")  #1     batch_count
    = #1 ➥ str(setup_info["batch_count"]).rjust(6, "0")  #1     entry_count = ➥ str(setup_info["entry_count"]
    *  ➥setup_info["batch_count"]).rjust(         8, "0"     )     return return f"9{batch_count}000010{entry_count}0198019800
    ➥{total_debits_in_file}{total_credits_in_file}".ljust(94," ") [PRE37] export default
    NextAuth(authConfig).auth;  export const config = {     matcher: [''/uploads''],
    }; [PRE38] import type { NextAuthConfig } from ''next-auth'';  export const authConfig
    = {     debug: true,                         #1     pages: { #2  signIn: ''/login'',
    #2   },                                        callbacks: {         authorized({
    auth, request: { nextUrl } }) {             const isLoggedIn = !!auth?.user;             const
    isOnDashboard =                 nextUrl.pathname.startsWith(''/uploads''); #3             if
    (isOnDashboard) {                 return isLoggedIn;             } else if (isLoggedIn)
    {                 return Response.redirect(new URL(''/login'', nextUrl));             }             return
    true;         },         async redirect({ url, baseUrl }) {             const
    queryParams = new URL(url).searchParams;  #4             const callbackUrl = queryParams.get(''callbackUrl'');  #4             if
    (callbackUrl) {  #4                 return callbackUrl;  #4             } else
    {  #4                 return url.startsWith(baseUrl) ? url : baseUrl;  #4             }  #4         }     },     providers:
    [], #5 } satisfies NextAuthConfig; [PRE39] import NextAuth from ''next-auth'';
    import Credentials from ''next-auth/providers/credentials''; import { z } from
    ''zod''; import { authConfig } from ''./auth.config'';  const validUsers = [ #1     {
    email: ''admin@futuristicfintech.com'', password: ''password''},  #2     { email:
    ''user@futuristicfintech.com'', password: ''password''}  #2     ]  #2  export
    const { auth, signIn, signOut } = NextAuth({     ...authConfig,  #2     providers:
    [         Credentials({             async authorize(credentials) {                 const
    parsedCredentials = z.object({  #3                     email: z.string(),   #3                     password:
    z.string(),  #3                 }).safeParse(credentials);  #3                  if
    (!parsedCredentials.success) {  #4                     return null;    #4                 }    const
    { email, password } = parsedCredentials.data; #5    const user = validUsers.find(user
    => user.email ➥ === email && user.password === password); #6                  if
    (!user) { #7                     return null;  #7                 }   #7                  return
    { email: user.email }; #8             },         }),     ], }); [PRE40] …     const
    [errorMessage, dispatch] = useFormState(authenticate, undefined);     const [showErrorDialog,
    setShowErrorDialog] = useState<boolean>(false);     const route = useRouter();
    …     const handleSubmit = async (event: ➥ React.FormEvent<HTMLFormElement>) =>
    {          event.preventDefault();         const formData = new FormData(event.currentTarget);          dispatch(formData);     };      useLayoutEffect(()
    => {         if (errorMessage && errorMessage !== ''success'') {             setShowErrorDialog(true);         }
    else if (errorMessage === ''success'') {             route.push(''/uploads'');             setShowErrorDialog(false);         }     },
    [errorMessage, route]);      return ( …                     {showErrorDialog &&
    <AlertMessage                         open={showErrorDialog}                         setOpen={()
    => setShowErrorDialog(false)}                         title={"Error Signing In"}                         message={"Either
    your email or ➥ password is incorrect. Please try again."}                     />}
    …     <Box                     component={"img"} …                     alt="Company
    Logo"                     src="/images/logo-light.png"   …                     <Box
    component="form" onSubmit={handleSubmit} ➥ noValidate sx={{ mt: 1 }}> [PRE41]         axios.post(`${apiUrl}/files`,
    formData, {             headers: {                 ''Content-Type'': ''multipart/form-data'',                 ''Authorization'':
    `Bearer  token_string_that_is_assigned_after_login`,             },         })
    [PRE42] async def verify_token(Authorization: str = Header(...)):     if Authorization
    != "Bearer secret_token":         raise HTTPException(status_code=400, detail="Invalid
    token")     return Authorization [PRE43] @router.post("", status_code=status.HTTP_201_CREATED,
    ➥ tags=["ACH Files"], dependencies=[Depends(verify_token)]) [PRE44]   nginx:     build:       context:
    ./nginx       dockerfile: Dockerfile #1     ports:       - 443:443 #2       -
    80:80    [PRE45] FROM nginx:latest  #1  WORKDIR /etc/nginx #2  RUN apt-get update
    && \     apt-get install -y openssl #3  RUN mkdir -p /etc/ssl/certs  #4 RUN mkdir
    -p /etc/ssl/private  #4 RUN chmod 700 /etc/ssl/private  #4  RUN openssl req -x509
    -nodes -days 365 -newkey rsa:2048 \#5     -keyout /etc/ssl/private/localhost.key
    -out #5 ➥ /etc/ssl/certs/localhost.crt \ #5   -subj ➥ #5 "/C=US/ST=YourState/L=YourCity/
    #5 ➥O=YourOrganization/OU=YourUnit/CN=localhost"  #5  COPY ./dashboard.conf /etc/nginx/conf.d/default.conf
    #6  CMD ["nginx", "-g", "daemon off;"] [PRE46] server {     listen 443 ssl; #1     server_name
    localhost 127.0.0.1;  #2      ssl_certificate /etc/ssl/certs/localhost.crt; #3     ssl_certificate_key
    /etc/ssl/private/localhost.key;       location / {   #4         proxy_pass http://dashboard:3000;  #4         proxy_set_header
    Host $host;  #4         proxy_set_header X-Real-IP $remote_addr;  #4         proxy_set_header
    X-Forwarded-For #4 ➥ $proxy_add_x_forwarded_for;  #4         proxy_set_header
    X-Forwarded-Proto $scheme;  #4     }  #4 }  server {     listen 80; #5     server_name
    localhost 127.0.0.1; #6      location / { #7         return 301 https://localhost:443$request_uri;  #7     }  #7
    } [PRE47] from playwright.sync_api import sync_playwright playwright = sync_playwright().start()
    browser = playwright.chromium.launch(headless=False) #1 page = browser.new_page()
    #2 [PRE48] def test_login_screen(page: Page):     page.goto("http://localhost:3000/uploads")     email
    = page.get_by_role("textbox", name="Email") #1     expect(email).to_have_id("email")     email.fill("admin@futuristicfintech.com")
    #2     password = page.get_by_role("textbox", name="Password")     expect(password).to_have_id("password")     password.fill("password")     page.screenshot(path="screenshots/login_screen.png")
    #3 [PRE49] def test_login_exception_message(page: Page):     page.goto("http://localhost:3000/uploads")
    #1     page.expect_navigation(wait_until="load")  #1     expect(page).to_have_url(re.compile
    #1 ➥("http://localhost:3000/login.*"))  #1     signin_button = page.get_by_role("button",
    name="Sign In")     expect(signin_button).to_have_id("signin")     signin_button.click()
    #2     alert = page.get_by_role("heading") #3     expect(alert).to_have_id("alert-message")  #3     expect(alert).to_have_text("Error
    Signing In")      page.wait_for_selector("#alert-message", ➥ state="visible")
    #4     page.screenshot(path= #4 ➥"screenshots/login_exception.png")  #4 [PRE50]      The
    user requested starlette==0.37.1      fastapi 0.110.0 depends on starlette<0.37.0
    and >=0.36.3 [PRE51]`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0] [  {  "ach_files_id": "dcb8e6a4-79e9-4cf7-a138-491e58c1ebb8",   "file_name":
    "sample.ach",     "file_hash": "application/octet-stream",     "created_at": "2024-02-18T01:05:37.513025"   }
    ] [PRE1] 接口 AchUpload {     id: number;     date: string;     filename: string;     creditTotal:
    number;     debitTotal: number; } [PRE2] total_debit_entry_dollar_amount NUMERIC(12,
    2) NOT NULL, total_credit_entry_dollar_amount NUMERIC(12, 2) NOT NULL, [PRE3]
    total_debit_entry_dollar_amount= #1    Decimal(f"{record[31:41]}.{record[41:43]}"),  #1
    total_credit_entry_dollar_amount=  #1    Decimal(f"{record[43:53]}.{record[53:55]}"),  #1
    [PRE4] schemas/ ├─ api/ ├─ database/ │  ├─ ach_record/ │  │  ├─ ach_record_base_schema.py
    │  │  ├─ ... │  ├─ ach_file_schema.py │  ├─ ... [PRE5] from decimal import Decimal
    from datetime import datetime from pydantic import BaseModel, UUID4, Field   class
    AchFilesResponse(BaseModel):     id: UUID4 = Field(...,  #1         description="ACH文件的唯一标识符",
    #2         title="ID")  #3     date: datetime = Field(...,         description="上传ACH文件的日期和时间",          title="日期")     filename:
    str = Field(...,         title="文件名",         description="加载ACH文件的文件名",         max_length=255,
    #4     )     credit_total: Decimal = Field(         ...,         description="ACH文件中信用交易的总金额",         ge=0,
    #5         title="信用总额",     )     debit_total: Decimal = Field(         ...,         description="ACH文件中借记交易的总金额",         ge=0,         title="借记总额",     )      class
    Config:                                       json_schema_extra = {                               "example":
    {                                        "id": "123e4567-e89b-12d3-a456-426614174000",                  "date":
    "2024-01-01T12:00:00",                   "filename": "ACH_20240101_123.ach",                  "creditTotal":
    "1000.00",                 "debitTotal": "5000.23",             }         } [PRE6]
    def get_files_response(     self, limit: Optional[int] = None, offset: Optional[int]
    = None ) -> list[AchFilesResponse]:     with get_db_connection(row_factory=dict_row)
    as conn:         result = conn.execute(             """             SELECT af.ach_files_id
    AS id, #1                    af.file_name AS filename,  #1                    af.created_at
    AS date,  #1                    afcr.total_debit_entry_dollar_amount AS debit_total,  #1                    afcr.total_credit_entry_dollar_amount
    AS credit_total #1             FROM ach_files AS af             INNER JOIN ach_records_type_1
    AS art1 USING (ach_files_id) #2             INNER JOIN ach_records_type_9 AS art9                 USING
    (ach_records_type_1_id)             INNER JOIN ach_file_control_records AS afcr                 USING
    (ach_records_type_9_id)             ORDER BY af.created_at DESC  #3             LIMIT
    %s  #4             OFFSET %s                """,             [limit, offset],         )         return
    result.fetchall() [PRE7] @router.get( #1     "",  #1     response_model=list[AchFilesResponse],  #1     summary="检索上传的ACH文件",  #1     description="检索ACH文件的详细信息，包括信用/借记总额",  #1     response_description="请求的ACH文件的详细信息"  #1
    ) async def read_files() -> list[AchFilesResponse]:     return AchFileSql().get_files_response()
    [PRE8] 接口 AchFilesResponse {     id: number;     date: string;     filename: string;     credit_total:
    number;     debit_total: number; } [PRE9] axios.get<AchFilesResponse[]>(`${apiUrl}/files`)    .then(response
    => {       console.log(`Response data ➥ ${JSON.stringify(response.data)}`); #1       const
    transformedData: AchFiles[] = ➥ response.data.map((row: AchFilesResponse) => ({
    #2  id: row.id, #2  date: row.date, #2  filename: row.filename, #2  credit'
