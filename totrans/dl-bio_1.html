<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" class="pagenumrestart" data-pdf-bookmark="Chapter 1. Introduction"><div class="chapter" id="introduction">
 <h1><span class="label">Chapter 1. </span>Introduction</h1>
 <p>Biology is increasingly becoming a data-driven science, and deep learning—a powerful subfield of machine learning—is opening new ways to uncover patterns in complex, high-dimensional datasets. As these two fields converge, new opportunities are emerging to extract meaningful insights using modern computational tools. This book is a practical introduction to working at that intersection, focused on developing the skills and mindset needed to apply deep learning effectively in biological contexts.
 </p>
 
 <section data-type="sect1" data-pdf-bookmark="Getting Started"><div class="sect1" id="getting-started">
 <h1>Getting Started</h1>
 <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-type="indexterm" id="ch01_intro.html0"/>This opening chapter helps you get oriented. Before jumping into code, we walk through how to frame a project, evaluate your data, and avoid common pitfalls. A bit of structure and planning up front will make your work more reproducible, more flexible, and ultimately more useful and impactful.
 </p>
 <section data-type="sect2" data-pdf-bookmark="Deciding What Your Model Will Replace"><div class="sect2" id="deciding-what-your-model-will-replace">
  <h2>Deciding What Your Model Will Replace</h2>
  <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="deciding what existing process your model will replace" data-type="indexterm" id="ch01_intro.html1"/>The success of a deep learning project in biology often hinges on what happens before you write a single line of code. It’s easy to get lost in technical details or spend weeks exploring data and architecture variants that don’t lead to meaningful outcomes. Especially in a field as interesting as this one, the temptation to tinker is strong. To stay focused, it helps to ask a few grounding questions up front.
  </p>
  <p>One of the most useful is: <em>What existing process will my model replace or improve?</em> The most impactful projects in this field often (though not always) have a clear answer. Here are some examples across different domains:
  </p>
  <p>In healthcare and drug discovery:
  </p>
  <dl>
  <dt>Skin cancer classification models</dt>
   <dd>
    <p>Aim to replicate a dermatologist’s ability to visually diagnose melanoma or other lesions from clinical images, offering faster, more scalable screening for at-risk populations</p></dd>
    <dt>Pathogen detection systems</dt>
   <dd><p>Trained on sequencing data or imaging to detect bacterial or viral infections directly from raw clinical samples (e.g., blood, saliva, or tissue), potentially replacing slower, culture-based diagnostics</p></dd>
   <dt>Brain tumor segmentation models</dt>
   <dd><p>Automate or accelerate the process of outlining tumors on MRI scans, a task that radiologists typically perform manually and with great time investment</p>
   </dd>
   <dt>Drug-target interaction prediction tools</dt>
   <dd><p>Aim to prioritize the most promising compound-target pairs, reducing the need for costly wet-lab screening of massive chemical libraries</p></dd>
   <dt>Antibiotic resistance prediction models</dt>
   <dd><p>Forecast whether a given bacterial strain will resist certain treatments, helping clinicians select effective antibiotics more quickly</p></dd>
  </dl>
  <p>In molecular biology:
  </p>
  <dl>
  <dt>AlphaFold</dt>
   <dd><p>This protein structure prediction model, in many cases, replaces the need for experimentally determining 3D protein shapes via expensive lab-based techniques like X-ray crystallography, cryo-EM, or NMR</p></dd>
   <dt>Gene expression prediction models</dt>
   <dd><p>Forecast gene activity from raw genomic sequences, offering a computational alternative to RNA sequencing (RNA-seq) experiments</p></dd>
   <dt>Variant effect prediction models</dt>
   <dd><p>Help automate the interpretation of genetic mutations, supporting clinical decision making by prioritizing likely pathogenic variants for follow-up analysis or experimental validation</p></dd>
  </dl>
  <p>And in ecology and environmental science:
  </p>
  <dl>
  <dt>Acoustic species classification systems</dt>
   <dd><p>Use forest sound recordings to identify animal species present, offering a scalable and less labor-intensive alternative to in-person biodiversity surveys</p></dd>
   <dt>Crop disease detection</dt>
   <dd><p>Via drone or satellite imagery, enables early identification of plant stress, reducing the need for manual scouting across large fields</p></dd>
   <dt>Animal facial recognition tools</dt>
   <dd><p>Track individual animals, reptiles, birds, and mammals over time without the need for tagging, collars, or other invasive methods</p></dd>
   <dt>Poaching detection systems</dt>
   <dd><p>Trained on infrared or motion sensor data, can automatically flag human activity in protected wildlife zones, assisting conservation efforts</p></dd>
  </dl>
  <p>Ideally this gives you a flavor of the kinds of workflows deep learning can improve or even replace. Where possible, try to estimate the potential impact of your model—how much time, cost, or manual labor it could save, or what new insights it might enable. This will help you stay grounded in the real-world utility of your work and communicate its value to collaborators, stakeholders, or the public.</p>
  <div data-type="tip"><h6>Tip</h6>
   <p>That said, not every valuable model needs to replace an existing process. Some open up entirely new capabilities—like generating novel biological sequences, uncovering hidden patterns in large datasets, or linking data types that were never connected before. These models might not streamline a lab task, but they can enable new kinds of discovery, expand what questions we can ask, or offer fresh ways to interpret complex systems. If your model creates something new, just be clear about what that is and why it matters—and be thoughtful about how you evaluate success when no established benchmark exists.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html1" data-type="indexterm" id="id408"/>
   </p>
  </div>
 </div></section>
 <section data-type="sect2" data-pdf-bookmark="Determining Your Criteria for Success"><div class="sect2" id="determining-your-criteria-for-success">
  <h2>Determining Your Criteria for Success</h2>
  <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="determining your criteria for success" data-type="indexterm" id="id409"/><a contenteditable="false" data-primary="success criteria, determining" data-type="indexterm" id="id410"/>It’s important to define, as early and explicitly as possible, what success looks like for your project. Research can be time-consuming and open-ended, so clear goals help you stay focused and avoid endless tweaking—repeatedly changing models, architectures, or training settings without a clear hypothesis or evaluation plan. This kind of trial-and-error loop is common in deep learning due to the large number of design choices and hyperparameters. Without structure, it can waste time and produce results that are hard to interpret or reproduce.
  </p>
  <p>Examples of success criteria include:
  </p>
  <dl>
  <dt>Performance metric (e.g., accuracy, AUC, F1)</dt>
   <dd><p>You might aim to match the performance of a human expert, achieve a correlation with experimental results comparable to a technical replicate, or keep the false-positive rate below a certain number.</p></dd>
   <dt>Level of interpretability</dt>
   <dd><p>In many applications, it’s important not only that a model performs well, but also that its decisions can be understood by domain experts. For instance, you may prioritize well-calibrated uncertainty estimates or interpretable feature attributions, especially when trust and explainability are critical.</p></dd>
   <dt>Model size or inference latency</dt>
   <dd><p>If your model needs to operate in a resource-constrained environment (e.g., smartphones or embedded devices) or meet real-time throughput targets (e.g., process 20 frames per second), your success criterion might focus on efficiency—such as achieving high performance per floating point operation (FLOP), which measures how effectively the model uses computational resources. In such cases, metrics like inference time, memory usage, or energy consumption may matter more than raw accuracy.</p></dd>
   <dt>Training time and efficiency</dt>
   <dd><p>When compute is limited—or for educational contexts—you may prioritize fast training or minimal hardware requirements. Since training deep learning models typically involves large matrix operations, they are often accelerated using graphics processing units (GPUs). In low-resource settings, developing a simpler model that trains quickly on a CPU may be a more practical goal than maximizing performance.</p></dd>
   <dt>Generalizability</dt>
   <dd><p>In some cases, the goal is to build a model that works well across many datasets or tasks, rather than one that is finely tuned to a single benchmark. For example, <em>foundational models</em>—large models trained on broad datasets that can be adapted to many downstream applications—prioritize flexibility and reuse. In such settings, broad applicability may be more valuable than squeezing out the best possible performance on a specific task.</p></dd>
  </dl>
  <p>Defining these goals up front helps you answer the key question: <em>When is the project done?</em> You’ll likely need to balance multiple criteria—but having them laid out early will keep your efforts aligned and your scope realistic.</p>
 </div></section>
 
 <section data-type="sect2" data-pdf-bookmark="Invest Heavily in Evaluations"><div class="sect2" id="invest-heavily-in-evaluations">
  <h2>Invest Heavily in Evaluations</h2>
  <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="prioritizing evaluation" data-type="indexterm" id="id411"/><a contenteditable="false" data-primary="evaluation" data-secondary="importance of" data-type="indexterm" id="id412"/>Once you’ve defined your criteria for success, it’s time to prioritize <em>evaluation</em>. This means thinking carefully about precisely how you’ll measure progress—including what metrics you’ll use, how you’ll validate results, and which baselines you’ll compare against. Without a clear, well-designed evaluation strategy, even a technically impressive model can fail to produce meaningful conclusions.</p>
  <p>Strong evaluations don’t just help you measure progress. They also help you detect bugs, estimate task difficulty, and build intuition. The key idea is simple: you need a known point of comparison to understand if your model is doing anything <span class="keep-together">meaningful</span>.</p>
  <div data-type="tip"><h6>Tip</h6>
   <p>While no general rule exists, it wouldn’t be surprising if successful machine learning projects spent 50% of their time designing evaluation strategies and running baselines, 25% curating or processing data, and only 25% on model architecture. Without good evaluations, you’re flying blind: you won’t know whether your model is actually improving, what trade-offs you’re making, or even whether it’s learning anything meaningful at all.</p>
  </div>
  <p>Spend time here. Evaluation isn’t just something you do at the end. It’s something you design at the beginning, and it guides the entire project.</p>
 </div></section>
 
 <section data-type="sect2" data-pdf-bookmark="Designing Baselines"><div class="sect2" id="designing-baselines">
  <h2>Designing Baselines</h2>
  <p><a contenteditable="false" data-primary="baselines, designing" data-type="indexterm" id="ch01_intro.html2"/><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="designing baselines" data-type="indexterm" id="ch01_intro.html3"/>One of the most practical evaluation tools is a strong <em>baseline</em>—a simple method that gives you something to beat. Good baselines help you measure progress, catch bugs early, and understand the difficulty of your task. Sometimes they can even be surprisingly hard to beat.
  </p>
  <p><a contenteditable="false" data-primary="classification" data-secondary="common strategies for" data-type="indexterm" id="id413"/>Designing good baselines requires thinking carefully about the task. Here are a few common baseline strategies for classification tasks:
  </p>
  <dl>
  <dt>Random prediction</dt>
   <dd><p>Assign labels completely at random, with equal probability for each class. This tells you what performance looks like with no information at all.</p></dd>
   <dt>Random prediction weighted by class frequencies</dt>
   <dd><p>Sample labels randomly, but in proportion to how often they occur in the training data. This is useful for imbalanced datasets.</p></dd>
   <dt>Majority class</dt>
   <dd><p>Always predict the most common class. This can be a surprisingly hard baseline to beat in highly class imbalanced settings.</p></dd>
   <dt>Nearest neighbor</dt>
   <dd><p>Predict the label of the most similar example in the training data (e.g., 1-nearest neighbor using Euclidean distance). This is often effective when inputs are low dimensional or well structured.</p></dd>
  </dl>
  <p>
   <a contenteditable="false" data-primary="regression tasks, common strategies for" data-type="indexterm" id="id414"/>And for regression tasks:
  </p>
  <dl>
  <dt>Mean or median of the target</dt>
   <dd><p>Always predict the average or median target value from the training set. This often matches what a model would do if it’s not learning anything meaningful.</p></dd>
   <dt>Linear regression with a single feature</dt>
   <dd><p>Fit a line using just the strongest individual predictor (e.g., one biomarker). This helps gauge how much a more complex model improves over a simple signal.</p></dd>
   <dt>K-nearest neighbor regression</dt>
   <dd><p>Predict the target as the average (or weighted average) of the <em>k</em> most similar data points. This is simple to implement and often surprisingly competitive on structured datasets.</p></dd>
  </dl>
  <p>
   And for both:
  </p>
  <dl>
  <dt>Simple heuristics</dt>
   <dd><p>Use straightforward rules based on domain knowledge. For example, in diagnostics, classify a patient as positive if a single biomarker or measurement exceeds a threshold. For skin cancer images, rank lesions by average pixel intensity. In genomics, if the task is to predict which gene a mutation affects, a simple baseline is to assume it affects the nearest gene in the genome.</p></dd>
  </dl>
  <div data-type="warning" epub:type="warning"><h6>Warning</h6>
   <p>If your model can’t beat a basic baseline, something is likely off—and that’s useful to know. It’s a key signal to revisit your data, features, or modeling approach.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html3" data-type="indexterm" id="id415"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html2" data-type="indexterm" id="id416"/>
   </p>
  </div>
 </div></section>
 
 <section data-type="sect2" data-pdf-bookmark="Time-Boxing Your Project"><div class="sect2" id="time-boxing-your-project">
  <h2>Time-Boxing Your Project</h2>
  <p>
   <a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="time-boxing your project" data-type="indexterm" id="id417"/><a contenteditable="false" data-primary="time-boxing" data-type="indexterm" id="id418"/>It’s important to time-box your project—that is, set a fixed amount of time to work on it, after which you pause or stop regardless of the outcome. Many research ideas “fail” in the sense that they don’t achieve the desired metrics. That’s normal. All projects, even the unsuccessful ones, generate insights that inform future work. Time-boxing helps ensure that failed experiments still move you forward—without consuming unlimited time and energy.
  </p>
  <div data-type="note" epub:type="note"><h6>Note</h6>
   <p>
    Time-boxing doesn’t mean giving up easily—it means setting boundaries to maintain focus, avoid burnout, and keep making progress.
   </p>
  </div>
  <p>
   Here are some tips for time-boxing effectively:
  </p>
  <dl>
    <dt>1. Set a clear deadline</dt>
   <dd>
    <p>Choose a realistic time frame (e.g., two weeks, three months) and stick to it.
    </p>
  </dd>
  <dt>2. Define checkpoints</dt>
   <dd>
    <p>Identify intermediate milestones—like completing dataset preprocessing, training a baseline model, or hitting a certain accuracy—to track progress.
    </p>
  </dd>
  <dt>3. Reflect at the end</dt>
   <dd>
    <p>Take time to evaluate what worked, what didn’t, and what you learned.
    </p>
  </dd>
  </dl>
  <div data-type="tip"><h6>Tip</h6>
   <p>
    Time-boxing is also useful
    <em>within</em>
    a project. For example: “I’ll experiment with this new processing or modeling idea for one week, and if it doesn’t help, I’ll move on.”
   </p>
  </div>
  <p>The biggest risk with time-boxing is yourself. It’s easy to justify extensions, add new ideas, or convince yourself you’ll strike gold if you just try 10 more things. Scope creep and perfectionism are common traps. In these cases, it can help to talk to someone else—a collaborator, mentor, or friend—to get perspective and avoid spinning your wheels. A quick conversation can often cut through indecision or obsessiveness and help you refocus on the broader context.
  </p>
 </div></section>
 <section data-type="sect2" data-pdf-bookmark="Deciding Whether You Really Need Deep Learning"><div class="sect2" id="deciding-whether-you-really-need-deep-learning">
  <h2>Deciding Whether You Really Need Deep Learning</h2>
  <p>
   <a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="deciding whether or not to use deep learning" data-type="indexterm" id="id419"/>This might seem like strange advice in a deep learning book, but before diving in, take a moment to ask yourself: <em>Do I actually need deep learning for this problem?</em> We’ll say it again—seriously consider simpler approaches.
  </p>
  <p>
   Deep learning models are powerful (and undeniably interesting), but they’re also resource intensive, complex to train, and difficult to debug. In many cases, traditional methods—like linear regression, decision trees, or basic statistical techniques—can achieve your goals with far less effort. These approaches are often:
  </p>
  <dl>
  <dt>Easier to implement</dt>
   <dd><p>Quicker to set up and require less expertise</p></dd>
   <dt>Less computationally demanding</dt>
   <dd><p>Can run on standard hardware with minimal training time</p></dd>
   <dt>More interpretable</dt>
   <dd><p>Easier to explain, troubleshoot, and validate</p></dd>
  </dl>
  <p>Carefully weigh the trade-offs. If a simpler method delivers the insights or performance you need, it’s often the smarter and more efficient path.
  </p>
 </div></section>
 <section data-type="sect2" data-pdf-bookmark="Ensuring That You Have Enough Good Data"><div class="sect2" id="ensuring-you-have-enough-good-data">
  <h2>Ensuring That You Have Enough Good Data</h2>
  <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="ensuring high-quality data" data-type="indexterm" id="id420"/>Deep learning models don’t just need a lot of data—they also generally need high-quality data. Models trained on poor data can often fail catastrophically, regardless of their sophistication.
  </p>
  <p>
   Make sure you have:
  </p>
  <dl>
  <dt>Sufficient quantity</dt>
   <dd><p>Deep models typically need thousands of examples or more. What counts as “enough” data depends on your problem and architecture. Check relevant <span class="keep-together">literature</span> for benchmarks. If you’re working with a small dataset, consider <em>transfer learning</em>, where you start from a model trained on a related task and fine-tune it on your own data. This approach can dramatically reduce the amount of data needed to achieve good performance.</p></dd>
   <dt>Sufficient quality</dt>
   <dd>
    <p>Clean, consistent data is critical. Label errors, noise, or inconsistencies can seriously degrade performance. Even large language models—like those powering modern chat-based assistant systems—can benefit substantially from training on carefully curated, high-quality data. Prioritize quality checks and thoughtful curation.
    </p>
   </dd>
  </dl>
 </div></section>
 
 <section data-type="sect2" data-pdf-bookmark="Assembling a Team"><div class="sect2" id="assembling-a-team">
  <h2>Assembling a Team</h2>
  <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="assembling a team" data-type="indexterm" id="ch01_intro.html4"/><a contenteditable="false" data-primary="teams, assembling" data-type="indexterm" id="ch01_intro.html5"/>Working alone is totally fine—but teaming up can accelerate progress, improve your ideas, and make the process more enjoyable. Here are some tips for finding great collaborators:
  </p>
  <dl>
  <dt>Engage with the community</dt>
   <dd><p>Join relevant forums, online groups, or webinars to connect with others, share ideas, and discover potential collaborators. Communities like Reddit, Discord servers, X, and specialized Slack groups can be great starting points.</p></dd>
   <dt>Participate in hackathons and competitions</dt>
   <dd><p>Platforms like Kaggle, Zindi, or local university events offer structured challenges, feedback, and opportunities to meet people with similar interests.</p></dd>
   <dt>Form an interdisciplinary team</dt>
   <dd>
    <p>Combining expertise from different areas often leads to stronger projects. If you’re a biologist, team up with someone in machine learning, and vice versa.
    </p>
   </dd>
   <dt>Collaborate with experts</dt>
   <dd>
    <p>Domain experts can help shape your approach and identify blind spots early. Look for collaborators at conferences or workshops, or reach out to authors of relevant papers. Strangers are surprisingly responsive to genuine cold requests from interested people.
    </p>
   </dd>
  </dl>
  <p>And here are some tips for once you find people to work with:
  </p>
  <dl>
  <dt>Define clear goals and roles</dt>
   <dd>
    <p>When working with others, it helps to clarify responsibilities early—who’s doing what, what success looks like, and how decisions will be made. This avoids misunderstandings and keeps the project moving.
    </p>
   </dd>
   <dt>Use shared tools for collaboration</dt>
   <dd>
    <p>Version control (like Git), shared notebooks (e.g., Google Colab), and simple task trackers (like Notion or Trello, or just a shared Google Doc with some lists) can make it much easier to coordinate and stay organized.
    </p>
   </dd>
   <dt>Support specialization</dt>
   <dd>
    <p>Let people lean into the parts of the project they enjoy most—some may focus on infrastructure and software engineering, others on data curation, modeling, or biological interpretation.
    </p>
   </dd>
   <dt>Start small</dt>
   <dd><p>If you’re unsure about long-term compatibility, try a short project or exploration together first. A small, low-pressure collaboration is a great way to test the waters.
    </p>
   </dd>
  </dl>
  <p>Whether you’re working solo or as part of a team, the most important thing is to stay curious, keep learning, and take that first step.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html5" data-type="indexterm" id="id421"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html4" data-type="indexterm" id="id422"/>
  </p>
 </div></section>
 <section data-type="sect2" data-pdf-bookmark="You Don’t Need a Supercomputer or a PhD"><div class="sect2" id="you-don-t-need-a-supercomputer-or-a-phd">
  <h2>You Don’t Need a Supercomputer or a PhD</h2>
  <p>
   <a contenteditable="false" data-primary="deep learning (basics)" data-secondary="non-technical issues" data-tertiary="misconceptions about expertise and computing power" data-type="indexterm" id="ch01_intro.html6"/>There are a few common misconceptions about working in deep learning for biology that are worth challenging:
  </p>
  <dl>
    <dt>You need huge budgets and compute power.</dt>
    <dd>
      <p>In an age of multimillion-dollar training runs for massive language models, it’s easy to assume you need vast resources. But that’s not always the case:</p>
      <dl>
        <dt>Prototype with small models</dt>
        <dd>
          <p>Start small to iterate quickly. You might find that lightweight models are more than enough for your goals.</p>
        </dd>
        <dt>Use free or affordable compute</dt>
        <dd>
          <p>Platforms like Google Colab and Kaggle offer free GPU access for smaller projects. For more demanding workloads, cloud providers such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) offer scalable paid instances.</p>
        </dd>
        <dt>Not everything is about scale</dt>
        <dd>
          <p>Many valuable projects focus on analyzing existing models rather than training new ones. These often require modest compute but can yield deep insight. There’s still a lot we don’t understand about how deep models behave.</p>
        </dd>
      </dl>
    </dd> <!-- closed first top-level dd -->
  
    <dt class="less_space pagebreak-before">You need deep expertise in machine learning or biology (or both).</dt>
    <dd>
      <p>Another myth is that only highly trained experts can contribute meaningfully. In reality:</p>
      <dl>
        <dt>Better tools</dt>
        <dd>
          <p>Modern frameworks make it easier than ever to build and experiment with powerful models.</p>
        </dd>
        <dt>Open source culture</dt>
        <dd>
          <p>Freely available code and pretrained models let you learn from and build on existing work.</p>
        </dd>
        <dt>Educational resources</dt>
        <dd>
          <p>There’s now no shortage of tutorials, videos, and walkthroughs online to help you get started.</p>
        </dd>
        <dt>Plenty of untapped problems</dt>
        <dd>
          <p>Many important biological questions remain unexplored by machine learning. You don’t need a PhD or a Kaggle medal to work on them.</p>
        </dd>
      </dl>
    </dd>
  </dl>
  <p>While the bleeding edge of research may require specialized knowledge and high-end infrastructure, there’s plenty of room in this field for curiosity, creativity, and new perspectives—no supercomputer required.
  </p>
  <div data-type="tip"><h6>Tip</h6>
   <p>As you explore this field, you’ll almost certainly come across academic papers—whether you’re digging into a specific method, reading related work, or looking for project ideas. Both biology and machine learning papers can feel impenetrable at first: the language is dense, the ideas are highly condensed, and there’s often a lot of jargon. But remember:
   </p>
   <ul>
    <li>
     <p>You’re seeing the result of months or years of work by a team of researchers—and encountering it for the first time.</p>
    </li>
    <li>
     <p>Reading papers is a skill, and like any skill, it improves with practice.</p>
    </li>
    <li>
     <p>Blog posts, YouTube videos, and open source projects can also be great, more accessible ways to learn the same concepts.</p>
    </li>
   </ul>
  </div>
  <p>With that background in place, let’s dive into the technical foundations of this book<a contenteditable="false" data-primary="" data-startref="ch01_intro.html6" data-type="indexterm" id="id423"/>.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html0" data-type="indexterm" id="id424"/></p>
 </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Technical Introduction"><div class="sect1" id="technical-introduction">
  <h1>Technical Introduction</h1>
  <p>
   <a contenteditable="false" data-primary="deep learning (basics)" data-secondary="technical introduction" data-type="indexterm" id="ch01_intro.html7"/>We’ll be using Python-based deep learning frameworks, in particular, JAX and Flax. JAX is a system for high-performance numerical computing and machine learning, and Flax is a flexible neural network library built on top of JAX. We’ll motivate by explaining why we’re using JAX and reviewing a few Python features that tend to come up in a lot of machine learning (ML) code. Then, we’ll introduce some foundational machine learning concepts recurring throughout, with the main one being how a training loop is structured.
  </p>
  <div data-type="tip"><h6>Tip</h6>
<p>As mentioned in <a data-type="xref" href="preface01.html#id109">“Prerequisites”</a>, this book assumes basic knowledge of Python. If you’re new to Python, check out the recommended resources listed there first.</p>
</div>
  <p>Finally, to avoid repetition across chapters, we’ve created a small companion library called <code>dlfb</code> (Deep Learning for Biology), which wraps up common utilities and components. We’ll reference it throughout the book.
  </p>

    <div data-type="tip"><h6>Tip</h6>
<p>Don’t worry if some parts of this technical introduction feel unfamiliar or challenging at first. You’re welcome to skim or skip ahead. Many of the concepts will become clearer when you see them in action later in the book.</p>
</div>
  <section data-type="sect2" data-pdf-bookmark="Why JAX and Flax?"><div class="sect2" id="why-jax-and-flax">
   <h2>Why JAX and Flax?</h2>
   <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="technical introduction" data-tertiary="JAX/Flax ecosystem" data-type="indexterm" id="ch01_intro.html8"/><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="reasons to use" data-type="indexterm" id="ch01_intro.html9"/>This book uses the JAX and Flax ecosystem. But why did we make this choice, when other options like PyTorch or Keras are more common?
   </p>
   <p>First, some honesty: there is no one “best” framework. All of them can be used to build effective biological models, and many of the concepts in this book will carry over easily if you’re using PyTorch or Keras.
   </p>
   <p>
    We’ve chosen JAX/Flax primarily because:
   </p>
   <dl>
   <dt>Familiar NumPy API</dt>
   <dd><p>JAX’s <code>jax.numpy</code> module (commonly imported as <code>jnp</code>) offers an API that so closely mirrors standard NumPy for array manipulation and mathematical operations that NumPy <code>np</code> calls can often be directly substituted with <code>jnp</code>. This means users already proficient with NumPy can transition to JAX with a significantly reduced learning curve, leveraging their existing knowledge to quickly build and adapt code while gaining JAX’s powerful transformations and accelerator support.</p></dd>
   <dt>Functional programming encourages clarity</dt>
    <dd>
     <p>JAX’s pure function style can reduce hidden state and make training logic more transparent. This fits well with the educational goals of the book—explicit is better than implicit.
     </p>
    </dd>
    <dt>Transformations are first-class</dt>
    <dd>
     <p>JAX provides powerful, composable transformations like <code>jit</code> (just-in-time compilation), <code>grad</code> (automatic differentiation), and <code>vmap</code> (vectorization) that work cleanly on Python functions. These tools simplify and unify many aspects of model training and evaluation.</p>
    </dd>
    <dt>JAX aligns with cutting-edge research</dt>
    <dd>
     <p>JAX has gained traction in recent machine learning research, particularly for biology, physics, and large-scale models. Using it here helps you align with newer toolchains and experiment with modern practices.
     </p>
    </dd>
    <dt>Speed</dt>
    <dd><p>JAX uses a compiler that can yield significant performance gains on specialized hardware such as GPUs (e.g., from NVIDIA or AMD) and TPUs (made by Google), making it well-suited for large-scale deep learning workloads. This compiler is based on XLA, a low-level system for optimizing numerical computations on accelerators.</p></dd>
   </dl>
   <p><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="trade-offs compared to other systems" data-type="indexterm" id="id425"/>That said, JAX and Flax come with trade-offs: a smaller ecosystem and APIs that evolve quickly (sometimes breaking things along the way). And while JAX can offer impressive speedups, that speed isn’t exclusive to JAX/Flax. For example, <a href="https://keras.io">Keras</a> now supports a JAX backend, offering another option for users who prefer a higher-level API. If you’re already comfortable with PyTorch, Keras, or TensorFlow, you’re welcome to implement the ideas in this book using those tools—and even contribute your own version to this book’s repository.</p>
   <p>It’s not necessary when you’re just starting out, but over time it can be helpful to become familiar with more than one deep learning framework. Each has strengths in different ecosystems—for example, we use PyTorch in Chapter 2 to extract pretrained embeddings from a Hugging Face model, since many models on Hugging Face are primarily released and maintained in PyTorch.</p>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>The deep learning field moves fast. While we use the <code>linen</code> API in Flax throughout this book, a newer API called <code>nnx</code> has recently emerged as the recommended way to build models. <code>linen</code> remains fully supported, but be aware that you may come across other tutorials or examples that use <code>nnx</code>, which has a slightly different syntax.
    </p>
   </div>
   <p>We’ll introduce key JAX concepts as needed throughout the book, but we won’t cover the entire library in detail. For more detailed hands-on learning, check out the official <a href="https://oreil.ly/Jcqtu">JAX tutorials</a>. And if you run into unexpected behavior, the JAX <a href="https://oreil.ly/TcOzQ">“sharp bits” notebook</a> is an excellent reference for common gotchas and how to avoid them.</p>
  
  <section data-type="sect3" data-pdf-bookmark="A note on performance"><div class="sect3" id="a-note-on-performance">
   <h3>A note on performance</h3>
   <p>As this is an educational book, our focus is on clarity rather than peak performance. That means we won’t cover things like precision tuning, advanced hardware strategies, or distributed training. But these things can matter a lot in real-world setups.</p>
   <p>If you’re comfortable with the basics and want to go deeper, here are a few areas worth exploring:</p>
   <dl>
    <dt>Numerical precision and tuning</dt>
     <dd><p><a contenteditable="false" data-primary="matrix multiplications (matmuls)" data-type="indexterm" id="id426"/>Many machine learning operations, especially matrix multiplications (matmuls), benefit from reduced-precision formats like <code>bfloat16</code>, which can significantly improve speed and memory usage with minimal impact on model accuracy. JAX lets you control the precision used for matmuls via <code>jax.default_matmul_precision</code>, helping you take advantage of specialized hardware like Tensor Cores (on NVIDIA GPUs) or matrix units (on TPUs). Lower-precision training is widely used in large-scale setups because it enables training larger models more efficiently and cost-effectively.</p></dd>
    <dt>Profiling tools like <code>jax.profiler</code> or TensorBoard</dt>
     <dd><p>Profiling helps you identify where your code is spending time and memory so you can spot bottlenecks in training and optimize the most expensive operations.</p></dd>
    <dt>Memory-efficient training techniques</dt>
     <dd><p>Methods like gradient checkpointing (remat in JAX) let you trade off computation for memory, allowing you to train deeper models without running out of RAM.</p></dd>
    <dt>Multihost/multidevice training</dt>
    <dd><p>Training across multiple GPUs, TPUs, or even machines allows you to scale up models and datasets that wouldn’t fit on a single device.</p></dd>
   </dl>
   <p>You won’t need any of these to follow this book, but they are good to be aware of and are worth exploring as you grow more comfortable with the JAX ecosystem.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html9" data-type="indexterm" id="id427"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html8" data-type="indexterm" id="id428"/></p>
  </div></section>
  </div></section>
  
  <section data-type="sect2" data-pdf-bookmark="Python Tips"><div class="sect2" id="python-tips">
   <h2>Python Tips</h2>
   <p><a contenteditable="false" data-primary="deep learning (basics)" data-secondary="technical introduction" data-tertiary="Python tips" data-type="indexterm" id="ch01_intro.html10"/><a contenteditable="false" data-primary="Python" data-secondary="tips for using" data-type="indexterm" id="ch01_intro.html11"/>While this book doesn’t cover Python background knowledge in depth, this section highlights a few helpful Python concepts you’re likely to encounter when working with machine learning code in general—and with JAX and Flax in particular.
  </p>
   <section data-type="sect3" data-pdf-bookmark="Type annotations and docstrings"><div class="sect3" id="type-annotations-and-docstrings">
    <h3>Type annotations and docstrings</h3>
    <p>
    <a contenteditable="false" data-primary="docstrings" data-type="indexterm" id="ch01_intro.html12"/><a contenteditable="false" data-primary="Python" data-secondary="type annotations and docstrings" data-type="indexterm" id="ch01_intro.html13"/><a contenteditable="false" data-primary="type annotations" data-type="indexterm" id="ch01_intro.html14"/> Python is a dynamically typed language, meaning you don’t need to declare variable types (such as strings or integers) explicitly. Instead, types are determined at runtime, which makes the language flexible—but this flexibility can also make bugs harder to catch, especially in larger codebases. Adding <em>type annotations</em>
     helps mitigate this by improving readability, enabling static type checks with tools like <code>mypy</code>, and making debugging easier.
    </p>
    <p>Here’s a simple function that computes the mean squared error (MSE) between two NumPy arrays:
    </p>
    
     
<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>


<code class="k">def</code> <code class="nf">mean_squared_error</code><code class="p">(</code><code class="n">y_true</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">):</code>
  <code class="n">squared_errors</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_true</code> <code class="o">-</code> <code class="n">y_pred</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>
  <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">squared_errors</code><code class="p">)</code>
</pre>
     
    <p>Here is an example of its use:</p>

      <pre data-type="programlisting" data-code-language="python"><code class="n">y_true</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mf">1.1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">])</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="mf">1.2</code><code class="p">])</code>
<code class="n">mean_squared_error</code><code class="p">(</code><code class="n">y_true</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code>
</pre>

<p>Output:</p>
      <pre data-type="programlisting">np.float64(0.030000000000000002)
</pre>
     
    
    <p>We can improve this function by adding type hints to specify that the inputs are <code>np.ndarray</code> objects and the return type is a <code>float</code>, along with a docstring to explain what the function does:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">mean_squared_error</code><code class="p">(</code><code class="n">y_true</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">ndarray</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>
  <code class="sd">"""</code>
<code class="sd">  Calculate the Mean Squared Error (MSE) between two NumPy arrays.</code>

<code class="sd">  Args:</code>
<code class="sd">      y_true (np.ndarray): Ground-truth values.</code>
<code class="sd">      y_pred (np.ndarray): Predicted values.</code>
<code class="sd">  """</code>
  <code class="n">squared_errors</code> <code class="o">=</code> <code class="p">(</code><code class="n">y_true</code> <code class="o">-</code> <code class="n">y_pred</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>
  <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">squared_errors</code><code class="p">)</code>
</pre>
     
    
    <p>
     These changes don’t affect the function’s behavior, but they offer several benefits:
    </p>
    <dl>
    <dt>Clarify input and output types</dt>
     <dd>
      <p>It’s immediately clear that <code>y_true</code> and <code>y_pred</code> should be NumPy arrays and the return value is a <code>float</code>. Note that some machine learning code goes further by specifying the data type within the array (e.g., <code>arr: NDArray[np.float64]</code>), but we will not do this in this book.
      </p></dd>
      <dt>Enhance documentation</dt>
     <dd>
      <p>IDEs and documentation tools can provide better inline help and autocompletion. This can really improve productivity.
      </p>
     </dd>
     <dt>Improve readability</dt>
     <dd><p>The function is easier to understand for others (or your future self).</p>
     </dd>
     <dt>Enable static checking</dt>
     <dd>
      <p>Tools like <code>mypy</code> can catch type-related errors.
      </p>
     </dd>
    </dl>
    <p>The MSE example is very simple, so adding type hints and a full docstring is arguably overkill—but the principle is important.
    </p>
    <p>We won’t necessarily always use typing and docstrings throughout this book due to space constraints, but they’re good habits to adopt in your own projects. When we do include docstrings in the main book text, we’ll usually keep them to a single line, which will save a few trees in printing.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html14" data-type="indexterm" id="id429"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html13" data-type="indexterm" id="id430"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html12" data-type="indexterm" id="id431"/></p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Decorators"><div class="sect3" id="decorators">
    <h3>Decorators</h3>
    <p><a contenteditable="false" data-primary="decorators" data-type="indexterm" id="ch01_intro.html15"/><a contenteditable="false" data-primary="Python" data-secondary="decorators" data-type="indexterm" id="ch01_intro.html16"/><em>Decorators</em> are functions that modify the behavior of other functions or methods. In machine learning and data science, decorators are often used to enhance performance, cache results, or log function behavior.</p>
    <section data-type="sect4" data-pdf-bookmark="Just-in-time (JIT) compilation with JAX"><div class="sect4" id="just-in-time-jit-compilation-with-jax">
     <h4>Just-in-time (JIT) compilation with JAX</h4>
     <p><a contenteditable="false" data-primary="decorators" data-secondary="just-in-time compilation with JAX" data-type="indexterm" id="ch01_intro.html17"/><a contenteditable="false" data-primary="JAX" data-secondary="just-in-time compilation with JAX" data-type="indexterm" id="ch01_intro.html19"/><a contenteditable="false" data-primary="jax.jit (decorator)" data-type="indexterm" id="ch01_intro.html21"/><a contenteditable="false" data-primary="just-in-time (JIT) compilation" data-secondary="with JAX" data-type="indexterm" id="ch01_intro.html22"/>One of the most common decorators when working in JAX
      is <code>jax.jit</code>, which performs JIT compilation to accelerate code execution. The first run of a JIT-compiled function is slower because it is compiled to XLA (Accelerated Linear Algebra) machine code. However, subsequent calls run significantly faster.
     </p>
     <p>Suppose we have a function that takes a JAX array, raises all values to the 10th power, and sums them:</p>

       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">jax</code>
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>


<code class="k">def</code> <code class="nf">compute_ten_power_sum</code><code class="p">(</code><code class="n">arr</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>
  <code class="sd">"""Raise values to the power of 10 and then sum."""</code>
  <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">arr</code><code class="o">**</code><code class="mi">10</code><code class="p">)</code>


<code class="n">arr</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">])</code>
<code class="n">compute_ten_power_sum</code><code class="p">(</code><code class="n">arr</code><code class="p">)</code>
</pre>
<p>Output:</p>
       <pre data-type="programlisting">Array(10874275, dtype=int32)
</pre>
      <p>We can speed up this function in one of two ways. Either directly:</p>

             <pre data-type="programlisting" data-code-language="python"><code class="n">jitted_compute_ten_power_sum</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">jit</code><code class="p">(</code><code class="n">compute_ten_power_sum</code><code class="p">)</code>
<code class="n">jitted_compute_ten_power_sum</code><code class="p">(</code><code class="n">arr</code><code class="p">)</code>
</pre>
<p>Output:</p>
             <pre data-type="programlisting">Array(10874275, dtype=int32)
</pre>

     
     <p>Or, via the Python syntactic sugar, we can apply it when defining the function with the <code>@jax.jit</code> decorator:</p>
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">compute_ten_power_sum</code><code class="p">(</code><code class="n">arr</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>
  <code class="sd">"""Raise values to the power of 10 and then sum."""</code>
  <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">arr</code><code class="o">**</code><code class="mi">10</code><code class="p">)</code>


<code class="n">compute_ten_power_sum</code><code class="p">(</code><code class="n">arr</code><code class="p">)</code>
</pre>
<p>Output:</p>
<pre data-type="programlisting">Array(10874275, dtype=int32)
</pre>
      
     
     <p>The output remains the same for all, but the jitted functions run much faster. If you are working in a Jupyter notebook, you can use <code>%timeit</code>
      to measure the execution time for a line of code (or <code>%%timeit</code> for entire cells). Try timing the function with and without <code>@jax.jit</code>. On a GPU, you may see an ~20× speedup.</p>
     
     <div data-type="note" epub:type="note"><h6>Note</h6>
     <p>How does <code>@jax.jit</code> work? Briefly, when you apply <code>@jax.jit</code>, JAX doesn’t run your function like regular Python. Instead, it first traces the function—it runs through it once using special tracer objects (not real data) to build a computation graph. This graph is a static representation of all the numerical operations performed, with control flow unrolled and variable shapes and types fixed.</p>
     <p>Once the graph is built, JAX compiles it using XLA (Accelerated Linear Algebra), a backend that generates highly optimized machine code. This compiled version is cached and reused whenever the function is called again with the same input shapes and types—leading to significant speedups.</p>
     </div>
    
    <p><a contenteditable="false" data-primary="just-in-time (JIT) compilation" data-secondary="trade-offs when debugging" data-type="indexterm" id="id432"/>JIT compilation is powerful, but it comes with trade-offs, especially for debugging. This is because:</p>
    <ul>
     <li>Python debugging tools like <code>print()</code> statements or <code>pdb</code> don’t behave as expected inside jitted functions.</li>
     <li>Side effects (e.g., <code>print()</code>, logging, or modifying a list) are not actually executed during tracing, since JAX skips anything that doesn’t affect the computation graph.</li>
     <li>Error messages can refer to internal JAX or XLA code instead of your original function and can be quite cryptic.</li>
    </ul>
    <p>While you can temporarily disable jitting by commenting out <code>@jax.jit</code>, this becomes impractical if many functions rely on JIT. Fortunately, you can globally disable JIT by setting the environment variable <code>JAX_DISABLE_JIT=True</code>, which forces all jitted functions to run normally. This is a convenient way to debug without rewriting your code. See the <a href="https://oreil.ly/oXI97">JAX debugging documentation</a> for more details.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html22" data-type="indexterm" id="id433"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html21" data-type="indexterm" id="id434"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html19" data-type="indexterm" id="id435"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html17" data-type="indexterm" id="id436"/></p>
</div></section>
    <section data-type="sect4" data-pdf-bookmark="Preconfiguring JAX jit with partial"><div class="sect4" id="preconfiguring-jax-jit-with-partial">
      <h4>Preconfiguring JAX jit with partial</h4>
      <p><a contenteditable="false" data-primary="partial, preconfiguring jax.jit with" data-type="indexterm" id="ch01_intro.html23"/><a contenteditable="false" data-primary="JAX" data-secondary="preconfiguring jax.jit with partial" data-type="indexterm" id="ch01_intro.html20"/><a contenteditable="false" data-primary="decorators" data-secondary="preconfiguring JAX jit with partial" data-type="indexterm" id="ch01_intro.html18"/>There is a common source of confusion in machine learning code regarding usage of <code>partial</code>, especially with JAX code. The use of <code>functools.partial</code> is to prefill (or “bind”) some arguments of a function, returning a new function with those values fixed. This is a general Python utility and is not specific to JAX or ML. </p>
      
      <p>Here, we adapt the <code>scale</code> function and create a new function, <code>scale_by_10</code>:</p>
<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">functools</code> <code class="kn">import</code> <code class="n">partial</code>


<code class="k">def</code> <code class="nf">scale</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">scaling_factor</code><code class="p">):</code>
  <code class="k">return</code> <code class="n">x</code> <code class="o">*</code> <code class="n">scaling_factor</code>


<code class="c1"># Create a new function 'scale_by_10' where 'scaling_factor' is fixed to 10.</code>
<code class="n">scale_by_10</code> <code class="o">=</code> <code class="n">partial</code><code class="p">(</code><code class="n">scale</code><code class="p">,</code> <code class="n">scaling_factor</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="n">scale_by_10</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">30</pre>

<p>Here, <code>`scale_by_10`</code> is a new function that behaves like <code>scale(x, 10)</code>.</p>

<p>In the context of <code>JAX</code>, <code>partial</code> is often used to customize a decorator before applying it, like this: <code>@partial(jax.jit, static_argnums=...)</code>. This is a way to configure the <code>jax.jit</code> decorator itself. As mentioned previously, <code>jax.jit</code> compiles your Python function for speed. However, JAX needs to know if certain arguments are static. Static arguments are typically non-JAX array types (like integers, strings, or booleans) that control the structure of the computation (e.g., in if/else statements). If a static argument changes, JAX may need to recompile the function.</p>

<p>Let’s say we want to compute a summary statistic over an array, choosing either the mean or the median based on a string argument <code>average_method</code>. Since this choice affects the control flow, JAX needs to know the value of <code>average_method</code> at compile time:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">functools</code> <code class="kn">import</code> <code class="n">partial</code>

<code class="kn">import</code> <code class="nn">jax</code>
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>


<code class="nd">@partial</code><code class="p">(</code><code class="n">jax</code><code class="o">.</code><code class="n">jit</code><code class="p">,</code> <code class="n">static_argnums</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,))</code>
<code class="k">def</code> <code class="nf">summarize</code><code class="p">(</code><code class="n">average_method</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>
  <code class="k">if</code> <code class="n">average_method</code> <code class="o">==</code> <code class="s2">"mean"</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
  <code class="k">elif</code> <code class="n">average_method</code> <code class="o">==</code> <code class="s2">"median"</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
  <code class="k">else</code><code class="p">:</code>
    <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Unsupported average type: </code><code class="si">{</code><code class="n">average_method</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>


<code class="n">data_array</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">2.0</code><code class="p">,</code> <code class="mf">100.0</code><code class="p">])</code>

<code class="c1"># JAX compiles one version of 'summarize' for average_method="mean".</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Mean: </code><code class="si">{</code><code class="n">summarize</code><code class="p">(</code><code class="s1">'mean'</code><code class="p">,</code> <code class="n">data_array</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

<code class="c1"># JAX compiles another version for average_method="median".</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Median: </code><code class="si">{</code><code class="n">summarize</code><code class="p">(</code><code class="s1">'median'</code><code class="p">,</code> <code class="n">data_array</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

<code class="c1"># Calling with "mean" again uses the cached compiled version.</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Mean again: </code><code class="si">{</code><code class="n">summarize</code><code class="p">(</code><code class="s1">'mean'</code><code class="p">,</code> <code class="n">data_array</code><code class="p">)</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">
Mean: 34.333335876464844
Median: 2.0
Mean again: 34.333335876464844
</pre>

<p>If we didn’t mark <code>average</code> as static with <code>static_argnums=(0,)</code>, JAX would throw an error, because it can’t trace control flow that depends on strings unless it knows their value ahead of time. Marking arguments as static tells JAX to compile a separate, specialized version of the function for each unique value of that static argument it encounters.
</p>

<div data-type="tip"><h6>Tip</h6>
<p><a contenteditable="false" data-primary="dynamic inputs" data-type="indexterm" id="id437"/><a contenteditable="false" data-primary="inputs, static versus dynamic" data-type="indexterm" id="id438"/><a contenteditable="false" data-primary="static inputs" data-type="indexterm" id="id439"/>A bit of a clarification on the meaning of “static” versus “dynamic”: JAX treats most numerical inputs (like <code>jax.Array</code>, <code>float</code>, or <code>int</code>) as dynamic, meaning they can vary between calls without requiring recompilation, as long as their shapes and types stay the same.</p>

<p>Other inputs, like strings, Python objects, or functions, are static: they affect control flow or can’t be traced as part of the computation graph. If you pass them into a jitted function, you must mark them as static using <code>static_argnums</code> or close over them using a closure instead (see the next section).<a contenteditable="false" data-primary="" data-startref="ch01_intro.html23" data-type="indexterm" id="id440"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html20" data-type="indexterm" id="id441"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html18" data-type="indexterm" id="id442"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html16" data-type="indexterm" id="id443"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html15" data-type="indexterm" id="id444"/></p>
</div>
</div></section>
    </div></section>
    <section data-type="sect3" data-pdf-bookmark="Closures"><div class="sect3" id="closures">
    <h3>Closures</h3>
    <p><a contenteditable="false" data-primary="closures" data-type="indexterm" id="id445"/><a contenteditable="false" data-primary="decorators" data-secondary="closures" data-type="indexterm" id="id446"/>A closure is a function that “remembers” the environment in which it was created. This means it can access variables from its enclosing (outer) function’s scope, even after that scope has finished executing:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">outer_function</code><code class="p">(</code><code class="n">x</code><code class="p">):</code>
  <code class="k">def</code> <code class="nf">inner_function</code><code class="p">(</code><code class="n">y</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">x</code> <code class="o">+</code> <code class="n">y</code>  <code class="c1"># inner_function "closes over" x.</code>

  <code class="k">return</code> <code class="n">inner_function</code>


<code class="n">add_five</code> <code class="o">=</code> <code class="n">outer_function</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>  <code class="c1"># x is 5.</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">add_five</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>  <code class="c1"># y is 10.</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Closure result: </code><code class="si">{</code><code class="n">result</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">Closure result: 15</pre>

<p>In this example, <code>add_five</code> is a closure. It “remembers” that <code>x</code> was 5 when <code>outer_function</code> was called.</p>

<p>Closures are used extensively in JAX-based machine learning code. Many components—like loss functions, regularizers, and augmentation pipelines—are parameterized by configuration values. Instead of passing these values as arguments (which might require <code>static_argnums</code> if used inside control flow), they’re often closed over. A little later we will see this in action when defining the JAX training loop.</p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Generators"><div class="sect3" id="generators">
    <h3>Generators</h3>
    <p><a contenteditable="false" data-primary="decorators" data-secondary="generators" data-type="indexterm" id="id447"/><a contenteditable="false" data-primary="generators" data-type="indexterm" id="id448"/><em>Generators</em>  are functions that allow you to iterate over data lazily, yielding one item at a time.  They are especially useful for working with large datasets where loading everything into memory (RAM) at once is impractical or impossible.
    </p>
    <p>Here’s a simple generator function that simulates streaming batches of data:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Iterator</code>

<code class="k">def</code> <code class="nf">data_generator</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="n">Iterator</code><code class="p">[</code><code class="nb">dict</code><code class="p">]:</code>
  <code class="sd">"""Yield data samples with features and labels."""</code>
  <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">):</code>
    <code class="k">yield</code> <code class="p">{</code><code class="s2">"feature"</code><code class="p">:</code> <code class="n">i</code><code class="p">,</code> <code class="s2">"label"</code><code class="p">:</code> <code class="n">i</code> <code class="o">%</code> <code class="mi">2</code><code class="p">}</code>

<code class="c1"># Example usage.</code>
<code class="n">generator</code> <code class="o">=</code> <code class="n">data_generator</code><code class="p">()</code>
<code class="nb">next</code><code class="p">(</code><code class="n">generator</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">{'feature': 0, 'label': 0}
</pre>
     
    
    <p><a contenteditable="false" data-primary="TensorFlow Datasets (TFDS)" data-type="indexterm" id="id449"/>We will be working with TensorFlow datasets (TFDSs) in some chapters. Because JAX doesn’t include a native data-loading library, it’s common to see hybrid setups using TFDS. If you have your data in NumPy arrays, you can easily create a TensorFlow dataset using <code>tf.data.Dataset.from_tensor_slices</code>. This allows you to integrate NumPy data into TensorFlow pipelines for efficient training and preprocessing. It provides a clean API for batching, shuffling, and prefetching (preloading data before it’s needed to increase training speed), which is helpful when getting started:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="k">as</code> <code class="nn">tf</code>

<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">])</code>
<code class="n">labels</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code>

<code class="c1"># Create a TensorFlow dataset from the NumPy arrays.</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">((</code><code class="n">features</code><code class="p">,</code> <code class="n">labels</code><code class="p">))</code>

<code class="c1"># Batch dataset with batch size of 2 and drop the final batch if incomplete.</code>
<code class="n">batched_dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">drop_remainder</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Create a dataset (ds) iterator and retrieve the first batch using next().</code>
<code class="n">ds</code> <code class="o">=</code> <code class="nb">iter</code><code class="p">(</code><code class="n">batched_dataset</code><code class="p">)</code>
<code class="nb">next</code><code class="p">(</code><code class="n">ds</code><code class="p">)</code>
</pre>

<p>Output:</p>
<pre data-type="programlisting">(&lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])&gt;,
 &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])&gt;)
</pre>
     
    
    <p>In later chapters, we will also be writing custom data pipelines that offer more control.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html11" data-type="indexterm" id="id450"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html10" data-type="indexterm" id="id451"/></p>
   </div></section>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Anatomy of a Training Loop with JAX/Flax"><div class="sect2" id="anatomy-of-a-training-loop-with-jax-flax">
   <h2>Anatomy of a Training Loop with JAX/Flax</h2>
   <p> <a contenteditable="false" data-primary="deep learning (basics)" data-secondary="technical introduction" data-tertiary="anatomy of training loop with JAX/Flax" data-type="indexterm" id="ch01_intro.html24"/><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-type="indexterm" id="ch01_intro.html25"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-type="indexterm" id="ch01_intro.html26"/>While the details of machine learning projects may vary, the core structure of training a model remains fairly consistent. This core structure will serve as a foundation throughout the chapters of this book. Let’s walk through the basic layout of training a model using JAX and Flax—a pattern you can build upon as you explore more complex examples.</p>
   
   <section data-type="sect3" data-pdf-bookmark="Defining a dataset"><div class="sect3" id="defining-a-dataset">
    <h3>Defining a dataset</h3>
    <p><a contenteditable="false" data-primary="dataset" data-secondary="defining for training loop" data-type="indexterm" id="id452"/><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-tertiary="defining a dataset" data-type="indexterm" id="id453"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-tertiary="defining a dataset" data-type="indexterm" id="id454"/>Let’s create some toy data where the target <code>y</code> values are a linear transformation of the <code>x</code> values with a bit of random noise added. We’ll use the relationship <code>y = 2x + 1 </code> with Gaussian noise, as you can see in <a data-type="xref" href="#fig1-1">Figure 1-1</a>.</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">jax</code>
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">flax</code> <code class="kn">import</code> <code class="n">linen</code> <code class="k">as</code> <code class="n">nn</code>

<code class="c1"># In JAX, randomness is handled explicitly by passing a random key. </code>
<code class="c1"># We create a key here to seed the random number generator.</code>
<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>

<code class="c1"># Generate toy data: x values uniformly sampled between 0 and 1.</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_data</code><code class="p">,</code> <code class="n">rng_noise</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">x_data</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">rng_data</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>

<code class="c1"># Add Gaussian noise.</code>
<code class="n">noise</code> <code class="o">=</code> <code class="mf">0.1</code> <code class="o">*</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">rng_noise</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>

<code class="c1"># Define target: y = 2x + 1 + noise.</code>
<code class="n">y_data</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x_data</code> <code class="o">+</code> <code class="mi">1</code> <code class="o">+</code> <code class="n">noise</code>

<code class="c1"># Visualize the noisy linear relationship.</code>
<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x_data</code><code class="p">,</code> <code class="n">y_data</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"x"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"y"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Toy Dataset: y = 2x + 1 + noise"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
</pre>
     
   <figure><div id="fig1-1" class="figure">
    <img alt="" src="assets/dlfb_0101.png" width="600" height="464"/>
    <h6><span class="label">Figure 1-1. </span>This scatterplot visualizes the underlying relationship that we want our model to learn.
    </h6>
   </div></figure>
    
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Defining a model"><div class="sect3" id="defining-a-model">
    <h3>Defining a model</h3>
    <p><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-tertiary="defining a model" data-type="indexterm" id="id455"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-tertiary="defining a model" data-type="indexterm" id="id456"/>In Flax, we define a model by inheriting from <code>nn.Module</code>. The <code>@nn.compact</code> decorator allows us to define layers directly inside the <code>__call__</code> method, rather than in the class’s <code>setup()</code> method. This is especially useful for simple, sequential models.</p>

    <p>Here’s a minimal example, a single linear (dense) layer with one output unit and no activation function:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">LinearModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="c1"># Applies a single dense (fully connected) layer with 1 output neuron.</code>
    <code class="c1"># That is, it computes y = xW + b, where the output has dimension 1.</code>
    <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">features</code><code class="o">=</code><code class="mi">1</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
</pre>
     
    <p>We can instantiate the model like this:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">LinearModel</code><code class="p">()</code>
</pre>
     
    <p>To initialize the model’s parameters, use the <code>.init</code> method with a random key and a sample input. This allows Flax to infer the input and output shapes. Here, we pass in a dummy input with shape <code>[1, 1]</code>—one example (batch size of 1) with one input feature, matching the shape of our toy data:</p>
    
     
<pre data-type="programlisting" data-code-language="python"><code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="n">variables</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ones</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">]))</code>
</pre>
     
    <p>This initializes the model’s parameters. The result is a dictionary containing the weights and bias for the dense layer:</p>

    <pre data-type="programlisting" data-code-language="python"><code class="n">print_short_dict</code><code class="p">(</code><code class="n">variables</code><code class="p">)</code>
</pre>

<p>Output:</p>
    <pre data-type="programlisting">{'params': {'Dense_0': {'kernel': Array([[-0.5220277]], dtype=float32), 'bias':
Array([0.], dtype=float32)}}}
</pre>

<p>Here:</p>

<ul>
<li><code>kernel</code> is the learned weight matrix (shape <code>[1, 1]</code>, since our input and output dimensions are both 1).</li>
<li><code>bias</code> is the learned bias term added after the matrix multiplication.</li>
</ul>

<p>Note that while the Flax API has evolved over time, the core ideas remain stable: defining model layers, initializing parameters, and inferring shapes from inputs. Moreover, even if the syntax changes, these fundamentals will carry over to nearly any deep learning framework.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why infer shapes from input? In Flax, the shapes of the model’s weight and bias parameters are actually unknown until you run data through the model. That’s because Flax follows a <em>functional</em> style: layers don’t store input shape information when you define them. Instead, you provide a sample input during initialization, and Flax infers the necessary parameter shapes on the fly.</p>
<p>Other libraries, like PyTorch or Keras, use an object-oriented style, where layers often remember input shapes internally. This can make model construction feel more automatic, but Flax’s approach gives you more control and makes model behavior easier to inspect and debug, especially when working with JAX’s just-in-time (JIT) compilation.</p>
</div>

   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Creating a training state"><div class="sect3" id="creating-a-training-state">
    <h3>Creating a training state</h3>
    <p><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-tertiary="creating a training state" data-type="indexterm" id="id457"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-tertiary="creating a training state" data-type="indexterm" id="id458"/><a contenteditable="false" data-primary="training state" data-type="indexterm" id="id459"/>A <em>training state</em> in Flax is a container that packages together everything you need for training: the model’s parameters, the optimizer, and the function used to apply the model. Let’s build one:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">optax</code>
<code class="kn">from</code> <code class="nn">flax.training</code> <code class="kn">import</code> <code class="n">train_state</code>

<code class="c1"># Define an optimizer — here we use Adam with a learning rate of 1.0.</code>
<code class="c1"># (Note: in most real settings you'd use a smaller learning rate like 1e-3).</code>
<code class="n">tx</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="mf">1.0</code><code class="p">)</code>

<code class="c1"># Create the training state.</code>
<code class="n">state</code> <code class="o">=</code> <code class="n">train_state</code><code class="o">.</code><code class="n">TrainState</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
  <code class="n">apply_fn</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code>  <code class="c1"># The model's forward pass function.</code>
  <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code>  <code class="c1"># The initialized model parameters.</code>
  <code class="n">tx</code><code class="o">=</code><code class="n">tx</code><code class="p">,</code>  <code class="c1"># The optimizer.</code>
<code class="p">)</code>
</pre>
     
    <p>The <code>TrainState</code> object is designed to make training in Flax cleaner and more manageable. It holds everything needed for model updates:</p>
    <dl>
     <dt><code>params</code></dt>
      <dd>The current model parameters.</dd>
     <dt><code>tx</code></dt>
      <dd>The optimizer (in this case, Adam). “Tx” is short for transformation. In Optax (JAX’s optimization library), optimizers are defined as transformations of gradients. For example, Adam transforms raw gradients using momentum and adaptive scaling.</dd>
     <dt><code>apply_fn</code></dt>
      <dd>The function that runs the model’s forward pass.</dd>
    </dl>
    <p>Importantly, the training state is immutable—rather than modifying it in place, each update returns a new <code>TrainState</code> with the updated parameters. This functional style is consistent with JAX’s overall design and helps keep computations pure and traceable.</p>
    <div data-type="tip"><h6>Tip</h6>
    <p>Although <code>TrainState</code> is immutable and each update returns a new object, this doesn’t lead to memory issues. JAX reuses memory efficiently, especially inside <code>@jit</code>-compiled functions.</p>
    </div>
   </div></section>
   
   <section data-type="sect3" data-pdf-bookmark="Defining a loss function"><div class="sect3" id="define-a-loss-function">
    <h3>Defining a loss function</h3>
    <p><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-tertiary="defining a loss function" data-type="indexterm" id="id460"/><a contenteditable="false" data-primary="loss function" data-secondary="defining for training loop" data-type="indexterm" id="id461"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-tertiary="defining a loss function" data-type="indexterm" id="id462"/>The loss function measures how close the model’s predictions are to the true targets. Here, we use MSE, which is common for regression tasks:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
  <code class="c1"># Run a forward pass of the model to get predictions.</code>
  <code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">apply</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="n">x</code><code class="p">)</code>

  <code class="c1"># Compute MSE loss.</code>
  <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">((</code><code class="n">predictions</code> <code class="o">-</code> <code class="n">y</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code>
</pre>

<p>Note that <code>model.apply</code> works here because <code>model</code> was defined earlier and is available in the current scope (e.g., the same notebook or script). We don’t need to pass it as an argument, because the function is still pure—all variable model state comes from the <code>params</code> we pass in.</p>

<p>Here’s how you would call this function with your current model and data:</p>

      <pre data-type="programlisting" data-code-language="python"><code class="n">loss</code> <code class="o">=</code> <code class="n">calculate_loss</code><code class="p">(</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code> <code class="n">x_data</code><code class="p">,</code> <code class="n">y_data</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Loss: </code><code class="si">{</code><code class="n">loss</code><code class="si">:</code><code class="s2">.4f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>

<p>Output:</p>
      <pre data-type="programlisting">Loss: 5.2768
</pre>

<p>This computes the current loss by applying the model to the data using the (random) parameters stored in <code>state</code>. Since the model hasn’t learned from the data yet, the loss is expected to be relatively high. As training progresses, this number should steadily decrease—which would tell us that the model is improving at its task of predicting the targets from the inputs.</p>
     
    
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Defining the training step"><div class="sect3" id="define-the-training-step">
    <h3>Defining the training step</h3>
    <p><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-tertiary="defining the training step" data-type="indexterm" id="id463"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-tertiary="defining the training step" data-type="indexterm" id="id464"/><a contenteditable="false" data-primary="training step, defining" data-type="indexterm" id="id465"/>The training step performs a forward pass, computes the loss and gradients, and updates the model parameters. <a contenteditable="false" data-primary="jax.jit (decorator)" data-type="indexterm" id="id466"/>We use <code>jax.jit</code> to compile the entire step for efficiency. While JAX can run on GPUs and TPUs without it, using <code>jit</code> ensures that the code is compiled into a single optimized graph—which runs significantly faster and takes full advantage of accelerator hardware.
    </p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
  <code class="c1"># Compute the loss and its gradients with respect to the parameters.</code>
  <code class="n">loss</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">compute_loss</code><code class="p">)(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

  <code class="c1"># Apply gradient updates.</code>
  <code class="n">new_state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>

  <code class="k">return</code> <code class="n">new_state</code><code class="p">,</code> <code class="n">loss</code>
</pre>
     
<p>Often, you don’t just want the gradients (to update the parameters). You also want to log the loss explicitly (e.g., to plot it over time). But computing gradients requires computing the loss anyway. Instead of doing this twice, <code>jax.value_and_grad</code> is a convenient utility that does both at once:</p>
<ul>
  <li><p>It evaluates the function you give it (in this case, <code>calculate_loss</code>) to get the loss value.</p></li>
  <li><p>It computes the gradients of that loss with respect to the parameters.</p></li></ul>

<p>This avoids redundant computation.</p>
<p>The result of our <code>train_step</code> function is a new <code>TrainState</code> with updated parameters, along with the current loss, which we can use to monitor training progress.</p>

<p>Note that often you will encounter the training step defined with a closure like so:</p>

      <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
  <code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">):</code>
    <code class="c1"># state, x and y are not part of the function signature but are accessed.</code>
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">((</code><code class="n">predictions</code> <code class="o">-</code> <code class="n">y</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code>

  <code class="n">loss</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">)(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">loss</code>
</pre>

<p>Here, <code>state</code>, <code>x</code>, and <code>y</code> are closed over—they’re not part of the <code>calculate_loss</code> function’s input signature, making the code more compact and easier to read and reason about.</p>
</div></section>
   <section data-type="sect3" data-pdf-bookmark="Handling auxiliary outputs in the loss function"><div class="sect3" id="handling-auxiliary-outputs">
    <h3>Handling auxiliary outputs in the loss function</h3>

    <p><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-tertiary="handling auxiliary outputs in loss function" data-type="indexterm" id="id467"/><a contenteditable="false" data-primary="loss function" data-secondary="handling auxiliary outputs in" data-type="indexterm" id="id468"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-tertiary="handling auxiliary outputs in loss function" data-type="indexterm" id="id469"/>As a slight aside (but one that comes up often), sometimes we want to return extra information from the loss function—for example, predictions or metrics for logging—without affecting the gradient computation. JAX makes this easy with the <code>has_aux=True</code> flag. It tells <code>value_and_grad</code> to treat everything after the loss as “auxiliary” and to exclude it from gradient computation.</p>

<p>For example, let’s modify our loss function to also return predictions and accommodate this by using <code>has_aux=True</code> in <code>jax.value_and_grad</code> in our <code>train_step</code>:
</p>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
  <code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">):</code>
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="n">x</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">((</code><code class="n">predictions</code> <code class="o">-</code> <code class="n">y</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">loss</code><code class="p">,</code> <code class="n">predictions</code>  <code class="c1"># Return both loss and preds (aux info).</code>

  <code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">predictions</code><code class="p">),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">True</code><code class="p">)(</code>
    <code class="n">state</code><code class="o">.</code><code class="n">params</code>
  <code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">predictions</code><code class="p">)</code>
</pre>

<p>Without <code>has_aux=True</code>, JAX expects the loss function to return a single scalar. Returning anything else (like predictions) will raise an error. By setting <code>has_aux=True</code>, you’re telling JAX: “only differentiate with respect to the loss; ignore any extra outputs, like predictions.”</p>
    
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Defining the training loop"><div class="sect3" id="define-the-training-loop">
    <h3>Defining the training loop</h3>
    <p><a contenteditable="false" data-primary="JAX/Flax ecosystem" data-secondary="anatomy of training loop with" data-tertiary="defining the training loop" data-type="indexterm" id="ch01_intro.html27"/><a contenteditable="false" data-primary="training loop" data-secondary="with JAX/Flax ecosystem" data-tertiary="defining the training loop" data-type="indexterm" id="ch01_intro.html28"/>Now that all the components are in place, we can define the training loop that actually updates the model parameters based on the data.
    </p>
    <p>In most machine learning workflows, training happens over either <em>steps</em> or <em>epochs</em>:</p>
    <ul>
     <li><a contenteditable="false" data-primary="steps (defined in training context)" data-type="indexterm" id="id470"/>A <em>step</em> refers to one update of the model using a single batch of data.</li>
     <li><a contenteditable="false" data-primary="epochs (defined in training context)" data-type="indexterm" id="id471"/>An <em>epoch</em> is a full pass through the entire training dataset—typically consisting of many steps.
In our toy example, we feed the entire dataset (100 input–output pairs) into the model at once, without batching. This means each step is equivalent to a full epoch. In more realistic settings, you’d typically break the data into batches, resulting in many steps per epoch.</li>
    </ul>
    <p>Let’s now train the model:</p>
     
      <pre data-type="programlisting" data-code-language="python"><code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">150</code>  <code class="c1"># Number of full passes through the training data.</code>

<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">):</code>
  <code class="n">state</code><code class="p">,</code> <code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">_</code><code class="p">)</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">x_data</code><code class="p">,</code> <code class="n">y_data</code><code class="p">)</code>
  <code class="k">if</code> <code class="n">epoch</code> <code class="o">%</code> <code class="mi">10</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Epoch </code><code class="si">{</code><code class="n">epoch</code><code class="si">}</code><code class="s2">, Loss: </code><code class="si">{</code><code class="n">loss</code><code class="si">:</code><code class="s2">.4f</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>

<p>Output:</p>
      <pre data-type="programlisting">Epoch 0, Loss: 5.2768
Epoch 10, Loss: 0.9498
Epoch 20, Loss: 0.1091
Epoch 30, Loss: 0.0845
Epoch 40, Loss: 0.0283
Epoch 50, Loss: 0.0258
Epoch 60, Loss: 0.0106
Epoch 70, Loss: 0.0105
Epoch 80, Loss: 0.0106
Epoch 90, Loss: 0.0102
Epoch 100, Loss: 0.0101
Epoch 110, Loss: 0.0100
Epoch 120, Loss: 0.0100
Epoch 130, Loss: 0.0100
Epoch 140, Loss: 0.0100
</pre>
     
    
    <p>The model converges quickly—its loss decreases rapidly to a stable, low value. After training, we can test how well the model has learned the underlying pattern by comparing its predictions to the true target values, as shown in <a data-type="xref" href="#fig1-2">Figure 1-2</a>:</p>
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="c1"># Generate test data (x values between 0 and 1).</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x_test</code> <code class="o">+</code> <code class="mi">1</code>  <code class="c1"># Ground truth: linear function without noise.</code>

<code class="c1"># Get model predictions.</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="n">x_test</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"True values"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"red"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Model predictions"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"x"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"y"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Linear Model Predictions vs. True Relationship"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
</pre>
     
<figure><div id="fig1-2" class="figure">
    <img alt="" src="assets/dlfb_0102.png" width="600" height="466"/>
    <h6><span class="label">Figure 1-2. </span>Scatterplot comparing the linear model predictions with the true values.</h6>
   </div></figure>

   <p>We can see that after training, the model has learned to approximate the true function y=2x+1 very closely. The predicted outputs are nearly identical to the expected values, which is exactly what we want in this toy regression task.</p>

<p>This may be a simple example, but it captures the core structure of almost every deep learning workflow: define a model, compute a loss, update parameters, and repeat. While real projects involve more complexity—batching, data pipelines, <span class="keep-together">regularization</span>, metrics, logging, and so on—the basic loop you’ve built here is the foundation of it all. You now have a solid mental model for how training works in JAX and Flax—and the foundation to build much more powerful systems.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html28" data-type="indexterm" id="id472"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html27" data-type="indexterm" id="id473"/></p>

<div data-type="tip"><h6>Tip</h6>
<p>Where to go from here? Once you’ve built a working training loop (an important milestone), it’s common to expand in several directions: add metrics that capture key aspects of model performance, split off a validation set to check generalization, and track progress over training time. These are all building blocks of real-world deep learning workflows, and we’ll walk through them in upcoming chapters<a contenteditable="false" data-primary="" data-startref="ch01_intro.html26" data-type="indexterm" id="id474"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html25" data-type="indexterm" id="id475"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html24" data-type="indexterm" id="id476"/>.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html7" data-type="indexterm" id="id477"/></p>
</div>
    
   </div></section>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Machine Learning Tips"><div class="sect1" id="machine-learning-tips">
  <h1>Machine Learning Tips</h1>
  <p><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-type="indexterm" id="ch01_intro.html29"/>Here’s a brief reminder of some machine learning concepts, focusing on what we use throughout the book. We explain these ideas in more detail as necessary when they come up.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Types of Tasks"><div class="sect2" id="types-of-tasks">
   <h2>Types of Tasks</h2>
   <p>
    <a contenteditable="false" data-primary="classification" data-secondary="types of" data-type="indexterm" id="id478"/><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="types of tasks" data-type="indexterm" id="id479"/>In <em>classification</em>, the model predicts a label or a probability distribution over labels. There are three types of classification:
   </p>
   <dl>
   <dt>Binary classification</dt>
    <dd>
     <p><a contenteditable="false" data-primary="binary classification" data-secondary="defined" data-type="indexterm" id="id480"/>Choose between two options; for example, whether a cell is healthy or not.
     </p>
    </dd>
    <dt>Multiclass classification</dt>
    <dd>
     <p><a contenteditable="false" data-primary="multiclass classification" data-type="indexterm" id="id481"/>Choose one label out of several; for example, based on patterns in a biological sample, a model might predict which part of the body it came from—like the brain, liver, or skin. Each sample belongs to exactly one class.
     </p>
    </dd>
    <dt>Multilabel classification</dt>
    <dd>
     <p><a contenteditable="false" data-primary="multilabel classification" data-type="indexterm" id="id482"/>Choose all labels that apply, not just one. For example, when analyzing a cell image, the model might predict which structures are visible—such as the nucleus, membrane, and mitochondria. In this case, multiple labels can be true for the same image.
     </p>
    </dd>
   </dl>
   <p><em><a contenteditable="false" data-primary="regression, defined" data-type="indexterm" id="id483"/>Regression</em> involves predicting a continuous value, such as the binding strength between two molecules.
   </p>
   <p>We also use <em>representation learning</em>—learning useful embeddings or feature representations without direct supervision. These embeddings capture patterns or structure in the data, which can then be used for tasks like clustering, visualization, or as inputs to downstream models.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Types of Architectures"><div class="sect2" id="types-of-architectures">
   <h2>Types of Architectures</h2>
   <p>
    <a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="types of architectures" data-type="indexterm" id="id484"/><a contenteditable="false" data-primary="model architectures, types of" data-type="indexterm" id="id485"/>In this book, we primarily use the following model architectures:
   </p>
   <dl>
   <dt>Linear models and multilayer perceptrons (MLPs)</dt>
    <dd><p><a contenteditable="false" data-primary="linear models, defined" data-type="indexterm" id="id486"/><a contenteditable="false" data-primary="multilayer perceptrons (MLPs)" data-secondary="defined" data-type="indexterm" id="id487"/>These are fully connected networks that transform input vectors into output vectors through stacked dense layers. They are simple and widely used.
     </p>
    </dd>
    <dt>Convolutional neural networks (CNNs)</dt>
    <dd>
     <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="defined" data-type="indexterm" id="id488"/>CNNs apply spatial filters to model local structure, typically mapping images to images or feature maps. They are especially effective for image and sequence data.
     </p>
    </dd>
    <dt>Transformers</dt>
    <dd>
     <p><a contenteditable="false" data-primary="transformers" data-type="indexterm" id="id489"/>Model long-range dependencies in sequences using attention. They operate on sets or sequences and are now state of the art in many areas of biology and language modeling.
     </p>
    </dd>
    <dt>Graph neural networks (GNNs)</dt>
    <dd><p><a contenteditable="false" data-primary="graph neural networks (GNNs)" data-secondary="defined" data-type="indexterm" id="id490"/>Operate on graph-structured data, passing messages between nodes and their neighbors. They are useful when data has relational or interaction-based <span class="keep-together">structure</span>.
     </p>
    </dd>
    <dt>Autoencoders</dt>
    <dd>
     <p><a contenteditable="false" data-primary="autoencoders (AEs)" data-secondary="defined" data-type="indexterm" id="id491"/>Learn compact representations by encoding inputs into a latent space and reconstructing them. They are common in unsupervised learning and denoising tasks..
     </p>
    </dd>
   </dl>
   <p>
    We’ll explain each of these as they appear throughout the book.
   </p>

   <aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id492">
  <h5>Inductive Biases</h5>
  <p><a contenteditable="false" data-primary="inductive biases" data-type="indexterm" id="id493"/>Modern deep learning architectures are increasingly mature. Rather than trying to invent the next transformer, you’ll often get further by composing existing building blocks in thoughtful ways, guided by domain knowledge.</p>

<p>Think about what <em>inductive biases</em> (assumptions about the structure of the data) make sense for your problem: Are nearby data points related (like the pixels in images)? Is there a sequential structure (like in DNA or text)? Do entities interact in a graph-like way? These structural assumptions help your model learn more efficiently by narrowing the search space of possible functions.</p>

<p>A common type of inductive bias is an <em>invariance</em>—where the model assumes that certain transformations shouldn’t affect the output. For example:</p>
<ul>
  <li><p>CNNs assume <em>translation invariance</em>: shifting an image slightly shouldn’t change the prediction.</p></li>
  <li><p>GNNs assume <em>permutation invariance</em>: the order of nodes in a graph shouldn’t affect the output.</p></li>
  <li><p>Transformers are <em>permutation invariant</em> by default: they ignore input order unless positional encodings are added. This gives them flexibility, but it also means they rely on you to encode any sequential structure.</p></li>    
</ul>
</div></aside>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Dataset Splits"><div class="sect2" id="dataset-splits">
   <h2>Dataset Splits</h2>
   <p><a contenteditable="false" data-primary="dataset splits" data-type="indexterm" id="id494"/><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="dataset splits" data-type="indexterm" id="id495"/>Machine learning models are typically trained using three dataset partitions:
   </p>
   <dl>
   <dt>Training set</dt>
    <dd>
     <p>Used to fit the model’s parameters (its weights) by minimizing a loss function
     </p>
    </dd>
    <dt>Validation set</dt>
    <dd>
     <p>Used to tune <em>hyperparameters</em> (discussed next) and evaluate performance during development
     </p>
    </dd>
    <dt>Test set</dt>
    <dd>
     <p>Held out until the end to assess the model’s final performance on truly unseen data, providing an estimate of how well it generalizes
     </p>
         </dd>
   </dl>
     <p>This split structure is good practice because it helps ensure your model generalizes well to new data and gives you a reliable estimate of its real-world <span class="keep-together">performance</span>.</p>

  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Hyperparameters"><div class="sect2" id="hyperparameters">
   <h2>Hyperparameters</h2>
   <p>
    <a contenteditable="false" data-primary="hyperparameters" data-type="indexterm" id="id496"/><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="hyperparameters" data-type="indexterm" id="id497"/>Hyperparameters control how a model is trained. They’re set before training begins and are not updated by the optimizer. Common examples include:
   </p>
   <dl>
   <dt>Learning rate</dt>
    <dd>
     <p>How quickly the model updates its weights
     </p>
    </dd>
    <dt>Batch size</dt>
    <dd>
     <p>How many examples are processed together during one update
     </p>
    </dd>
    <dt>Model size</dt>
    <dd>
     <p>The number of layers, hidden units, or attention heads
     </p>
    </dd>
    <dt>Regularization</dt>
    <dd>
     <p>The dropout rate or weight decay, to prevent overfitting
     </p>
    </dd>
   </dl>
   <p>We evaluate different hyperparameter settings using performance on the validation set. <a contenteditable="false" data-primary="overfitting" data-secondary="defined" data-type="indexterm" id="id498"/>If we tune hyperparameters based on training performance alone, we risk <em>overfitting</em>—the model may simply memorize the training data, including noise or outliers.
   </p>
   <p class="pagebreak-before">Overfitting leads to poor performance on new data. <a contenteditable="false" data-primary="generalization" data-secondary="overfitting versus" data-type="indexterm" id="id499"/>In contrast, <em>generalization</em> means the model has learned patterns that apply beyond the training examples. This is a key requirement in biological applications, where test data can often come from different experiments or conditions.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Activation Functions"><div class="sect2" id="activation-functions">
   <h2>Activation Functions</h2>
   <p><a contenteditable="false" data-primary="activation functions" data-type="indexterm" id="ch01_intro.html30"/><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="activation functions" data-type="indexterm" id="ch01_intro.html31"/>Activation functions introduce nonlinearity into neural networks, enabling them to model complex relationships between inputs and outputs. </p>
    <p>As data flows through a model, it’s transformed into intermediate tensors known as <em>activations</em>. These activations pass through a series of linear projections and nonlinear activation functions at each layer. The output of the final hidden layer is then often passed through a final activation function, which produces the model’s ultimate prediction.</p>
<p>Choosing the right final activation is important—it should reflect the kind of data you’re trying to predict. For example:</p>
<ul>
  <li><p>Use <code>sigmoid</code> for binary classification, where outputs should lie between 0 and 1.</p></li>
  <li><p>Use <code>softmax</code> for multiclass classification, where outputs represent probabilities across categories.</p></li>
  <li><p>Use <em>no</em> final activation if your loss function expects raw logits (e.g., <code>sigmoid_cross_entropy</code> or <code>softmax_cross_entropy</code>). These loss functions apply the activation internally, so applying it yourself would be redundant or even harmful.</p></li>
  <li><p>Avoid ReLU or GELU as final activations when predicting real-valued outputs that can be negative—they will clip or distort negative values.</p></li>
  <li><p>For regression tasks, no activation is often best—or use tanh or sigmoid only if your target values are known to be bounded (e.g., between -1 and 1 or 0 and 1).</p></li>
</ul>
<p>When in doubt, check the range of your target values, and choose an activation (or none) that allows the model to produce outputs in that range. The term <em>logits</em> is often used to describe the raw, unnormalized outputs of the final layer, especially before applying softmax or sigmoid. But it’s a somewhat overloaded term that is sometimes used more loosely—for example, to describe intermediate values passed into a softmax, such as attention scores in a transformer.</p>
   <p>The following are the most commonly used activation functions, with their shapes shown in <a data-type="xref" href="#fig1-3">Figure 1-3</a>:
   </p>

   <figure><div id="fig1-3" class="figure"><img src="assets/dlfb_0103.png" width="600" height="451"/>
    <h6><span class="label">Figure 1-3. </span>Common activation functions used in deep learning.</h6>
   </div></figure>
   <p>A bit more detail on these common activation functions:</p>
   <dl>
   <dt>ReLU (rectified linear unit)</dt>
    <dd>
     <p>Outputs zero for negative inputs and the input itself for positive inputs. It is simple and effective, especially in deep networks.
     </p>
    </dd>
    <dt>GELU (Gaussian error linear unit)</dt>
    <dd>
     <p>A smoother alternative to ReLU, often used in transformer models. It can yield slightly better results.
     </p>
    </dd>
    <dt>Sigmoid and tanh</dt>
    <dd>
     <p>Older activation functions that squash values into fixed ranges. Sigmoid maps inputs to a range between 0 and 1, while tanh maps to a range between –1 and 1. They’re useful in certain settings, such as output layers for binary classification, but can suffer from gradient issues in deeper models.
     </p>
    </dd>
   <dt>Softmax</dt>
    <dd>
     <p>Converts a vector of values into a probability distribution that sums to 1. Used in the final layer of multiclass classification models. Not applied element-wise; it operates across the whole vector.
     </p>
    </dd>
   </dl>
   <p>In this book, we’ll usually stick to ReLU or GELU for hidden layers, and choose the final activation based on the prediction task.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html31" data-type="indexterm" id="id500"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html30" data-type="indexterm" id="id501"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Optimizers"><div class="sect2" id="optimizers">
   <h2>Optimizers</h2>
   <p>
    <a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="optimizers" data-type="indexterm" id="id502"/><a contenteditable="false" data-primary="optimizers" data-type="indexterm" id="id503"/>Optimizers are algorithms that adjust a model’s parameters (its weights and biases) to reduce the error (loss) during training. They do this using <em>gradient descent</em>, which computes how each parameter affects the loss and updates it to reduce the error.
   </p>
   <p>We mostly use <em>Adam</em>, a widely used optimizer that adapts the learning rate for each parameter and combines ideas from momentum and RMSProp. It usually trains faster and more reliably than plain gradient descent, especially in noisy or sparse settings.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Initialization Strategy"><div class="sect2" id="initialization-strategy">
   <h2>Initialization Strategy</h2>
   <p>
    <a contenteditable="false" data-primary="initialization, strategy for" data-type="indexterm" id="id504"/><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="initialization strategy" data-type="indexterm" id="id505"/>Before training starts, we need to set initial values for model parameters. This step is more important than it sounds. Poor initialization can cause gradients to vanish or explode, making training unstable.
   </p>
   <p>We typically use Xavier (Glorot) initialization, which is designed to keep the scale of activations and gradients roughly stable across layers. This helps training proceed smoothly. By default, Flax uses Xavier initialization for most layers like <code>Dense</code> and <code>Conv</code>, so you don’t usually need to specify it manually.
   </p>
   
   </div></section>
   <section data-type="sect2" data-pdf-bookmark="Model Checkpointing"><div class="sect2" id="model-checkpointing">
    <h2>Model Checkpointing</h2>
  <p><a contenteditable="false" data-primary="checkpointing" data-type="indexterm" id="id506"/><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="model checkpointing" data-type="indexterm" id="id507"/><a contenteditable="false" data-primary="model checkpointing" data-type="indexterm" id="id508"/>During training, it’s often useful to periodically save your model parameters so that you can resume later or avoid losing progress. While production training pipelines often use robust checkpointing strategies (e.g., saving best-performing checkpoints, versioned histories, etc.), that level of complexity isn’t necessary for the teaching examples in this book. Instead, we provide a lightweight utility that saves and restores just the most recent checkpoint. This is enough to pause and resume training or to store the final model output for later use. You’ll see it used in several chapters to simplify experimentation and reduce boilerplate.</p>
  <p>If you’re building your own training loop for production or research, consider upgrading to a more complete checkpointing solution—for example, using <a href="https://oreil.ly/aazC8">Orbax</a>, Flax’s newer checkpointing system.</p>
 </div></section>

 <section data-type="sect2" data-pdf-bookmark="Early Stopping"><div class="sect2" id="early-stopping">
  <h2>Early Stopping</h2>
  <p><a contenteditable="false" data-primary="early stopping" data-type="indexterm" id="id509"/><a contenteditable="false" data-primary="Flax" data-seealso="JAX/Flax ecosystem" data-type="indexterm" id="id510"/><a contenteditable="false" data-primary="machine learning" data-secondary="tips" data-tertiary="early stopping" data-type="indexterm" id="id511"/><a contenteditable="false" data-primary="training" data-secondary="early stopping" data-type="indexterm" id="id512"/>In real-world training, it’s often a good idea to stop training when your validation performance stops improving. This is known as <em>early stopping</em>, which helps prevent overfitting and saves compute.</p>
  <p>In this book, we often show longer training runs to visualize how the loss evolves over time. But in practice, you’ll likely want to use early stopping when training your own models.</p>
  <p>Flax includes a simple utility for this:</p>

  <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">flax.training.early_stopping</code> <code class="kn">import</code> <code class="n">EarlyStopping</code>
</pre>

<p>You can track validation metrics and stop training when they stop improving for a certain number of steps (the <code>patience</code> parameter).</p>

  </div></section>
 </div></section>
 
 <section data-type="sect1" data-pdf-bookmark="Selecting a Working Environment"><div class="sect1" id="selecting-a-working-environment">
  <h1>Selecting a Working Environment</h1>
  <p>
   <a contenteditable="false" data-primary="deep learning (basics)" data-secondary="selecting a working environment" data-type="indexterm" id="ch01_intro.html32"/><a contenteditable="false" data-primary="working environment" data-type="indexterm" id="ch01_intro.html33"/>Training <a contenteditable="false" data-primary="GPU acceleration" data-type="indexterm" id="id513"/>large neural networks often benefits from
   <em>GPU acceleration</em>. There are a few ways to access a GPU, depending on your budget, goals, and technical comfort level.
  </p>
  <p>One option is to use a local machine with a GPU, such as a gaming desktop or a workstation you’ve set up yourself. This gives you full control and avoids ongoing cloud costs, but it requires up-front hardware investment and setup. Another option is to rent GPU access from cloud providers like AWS, GCP, or Azure. These offer flexibility and scalability but can get expensive over time, especially if you’re training large models.
  </p>
  <p>For many beginners and small projects, Google Colab is a great place to start. It provides free, cloud-based Jupyter notebooks with GPU or TPU support and minimal setup.
  </p>
  <p>In the following sections, we’ll briefly walk through your options, from interactive notebooks to fully customized GPU development environments.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Selecting an Interactive Notebook"><div class="sect2" id="interactive-notebooks">
   <h2>Selecting an Interactive Notebook</h2>
   <p>
    <a contenteditable="false" data-primary="Jupyter notebooks" data-type="indexterm" id="id514"/><a contenteditable="false" data-primary="notebooks, selecting" data-type="indexterm" id="id515"/><a contenteditable="false" data-primary="working environment" data-secondary="selecting an interactive notebook" data-type="indexterm" id="id516"/>Jupyter notebooks are a popular and powerful tool for interactive coding, making them ideal for running the code examples in this book. They let you write code, execute it in cells, visualize results, and document your work, all in one place. This interactivity makes it easy to experiment, debug, and iterate quickly. Common notebook environments include JupyterLab, VSCode with Jupyter extensions, and Google Colab.
   </p>
   <p>
    <a contenteditable="false" data-primary="Google Colab" data-type="indexterm" id="id517"/>Google Colab in particular provides cloud-based notebooks with free access to GPUs and TPUs, making it a great option if you don’t have a powerful local machine. It runs entirely in your browser and requires no setup beyond a Google account. You can install libraries with commands like
    <code>!pip install jax flax optax</code>
    and save notebooks directly to Google Drive. To enable hardware acceleration, navigate to the Runtime menu. From the drop-down menu, select “Change runtime type” and then select a GPU or TPU.
   </p>
   <p>We provide all code in this book as Google Colab notebooks that you can open, run, and modify interactively.
   </p>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>
     Colab sessions can time out after periods of inactivity, so remember to save your work frequently.
    </p>
   </div>
   <p>
    While notebooks are excellent for exploration and prototyping, they can make version control, debugging, and maintaining complex projects more difficult. For longer-term work, setting up a dedicated development environment with GPU support offers more control, scalability, and reproducibility.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Structuring Your Code for Reuse and Debugging"><div class="sect2" id="structuring-your-code-for-reuse-and-debugging">
    <h2>Structuring Your Code for Reuse and Debugging</h2>
    <p>Notebooks<a contenteditable="false" data-primary="working environment" data-secondary="structuring code for reuse and debugging" data-type="indexterm" id="id518"/> are great for interactive use, but as you begin building your own projects, it’s worth thinking about how you will structure it early. Organize your code into clear, reusable Python modules: one for datasets, one for models, one for metrics and evaluation. Keeping these pieces separate makes your code easier to debug, scale, and reuse later.</p>
<p>This modular approach mirrors how this book is structured. Each subsection of each chapter generally focuses on a specific building block: dataset, model, training loop, and so on. As you follow along, you might try using this structure in your own code. Build clean components you can plug into future projects, and you’ll move faster each time.</p>
<p>A good dataset class in particular will help you catch issues early. There are plenty of reasons you might get confused during a project. If your dataset and metrics are cleanly separated and easy to inspect, you can at least rule them out as the source of the problem.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Add frequent sanity checks. Can your model overfit a tiny dataset (like 10 examples)? Do loss and accuracy behave as expected when you shuffle labels?</p>
<p>Plot everything. Visualizing predictions, losses, or inputs will often reveal issues you wouldn’t notice in the final numbers alone—like shifted targets, mislabeled data, empty tensors, or models that haven’t learned anything at all.</p>
</div>
  </div></section>
  
  <section data-type="sect2" data-pdf-bookmark="Setting Up a GPU Development Environment"><div class="sect2" id="setting-up-a-gpu-development-environment">
   <h2>Setting Up a GPU Development Environment</h2>
   <p>
    <a contenteditable="false" data-primary="GPU development environment" data-type="indexterm" id="id519"/><a contenteditable="false" data-primary="working environment" data-secondary="setting up a GPU development environment" data-type="indexterm" id="id520"/>As your models grow more complex, you may want to move beyond notebooks into a more powerful GPU-enabled setup, either locally or in the cloud. A robust development environment can speed up experimentation, simplify debugging, and make your workflow more reproducible.
   </p>
   <p class="pagebreak-before">For local development, we recommend the following:
   </p>
   <ul class="simple">
    <li>
     <p>
      Use Docker with NVIDIA Docker to create containerized environments that seamlessly access your GPU.
     </p>
    </li>
    <li>
     <p>
      Pair it with an editor like VSCode, which supports Docker and remote development for a smooth workflow.
     </p>
    </li>
    <li>
     <p>
      Use Git for version control to track changes, collaborate, and back up your work.
     </p>
    </li>
   </ul>
   <p>Mac users with Apple Silicon can try <a href="https://oreil.ly/_FYSk"><code>jax-metal</code></a> to enable GPU acceleration via Apple’s Metal backend. While it’s improving rapidly, not all features are fully supported yet, so expect occasional compatibility issues.</p>
   <div data-type="tip"><h6>Tip</h6>
    <p>Paid cloud services like AWS, GCP, or Azure offer on-demand GPU instances if you don’t have a local GPU. Alternatives like Paperspace, Lambda Labs, or RunPod can offer simpler setup and better value for smaller-scale projects.
    </p>
   </div>
  </div></section>
 <section data-type="sect2" data-pdf-bookmark="Version Conflicts"><div class="sect2" id="version-conflicts">
  <h2>Version Conflicts</h2>
  <p><a contenteditable="false" data-primary="version conflicts" data-type="indexterm" id="id521"/><a contenteditable="false" data-primary="working environment" data-secondary="version conflicts" data-type="indexterm" id="id522"/>Machine learning and scientific computing libraries evolve quickly—and not always in sync. You’ll inevitably hit version conflicts between tools like NumPy, JAX, Flax, PyTorch, or Hugging Face Transformers. Strive for a balance: use versions recent enough to benefit from improvements, but not so new that they break compatibility with everything else.</p>
<p>Tools like <a href="https://oreil.ly/_aezy"><code>uv</code></a>—a faster, more flexible alternative to <code>pip</code>—can help override incompatibilities and install packages even when their metadata says they’re not compatible. It’s not a permanent solution, but it can get you unstuck.</p>

<p>If you’re working outside a notebook, always use a virtual environment (<code>venv</code>, <code>conda</code>, or <code>uv venv</code>) to keep dependencies isolated. Another option is to define your environment in a Docker container, which guarantees full reproducibility across machines. This setup is especially useful when working on remote GPU instances or in the cloud—something we’ll explain later in the book.</p>
<p>When in doubt, check GitHub issues or forums—version mismatches are a common pain point for everyone, and at the very least, you can commiserate with others.</p>
<p>If you’re working in a notebook, version issues can be especially tricky. Google Colab, for example, comes with many preinstalled packages—but these can be outdated or incompatible with the latest JAX/Flax stack. You can install or override versions directly in a cell using <code>!pip install</code>, but you may need to restart the runtime afterward for changes to take effect (Runtime &gt; Restart runtime)<a contenteditable="false" data-primary="" data-startref="ch01_intro.html33" data-type="indexterm" id="id523"/><a contenteditable="false" data-primary="" data-startref="ch01_intro.html32" data-type="indexterm" id="id524"/>.<a contenteditable="false" data-primary="" data-startref="ch01_intro.html29" data-type="indexterm" id="id525"/><a contenteditable="false" data-primary="AEs" data-see="autoencoders" data-type="indexterm" id="id526"/><a contenteditable="false" data-primary="cancer" data-see="skin cancer, detecting in medical images" data-type="indexterm" id="id527"/><a contenteditable="false" data-primary="CNNs" data-see="convolutional neural networks" data-type="indexterm" id="id528"/><a contenteditable="false" data-primary="GNNs" data-see="graph neural networks" data-type="indexterm" id="id529"/><a contenteditable="false" data-primary="graphs" data-secondary="understanding drug–drug interactions through" data-see="drug–drug interactions, understanding using graphs" data-type="indexterm" id="id530"/><a contenteditable="false" data-primary="ISM" data-see="in silico saturation mutagenesis" data-type="indexterm" id="id531"/><a contenteditable="false" data-primary="medical images, detecting skin cancer in" data-see="skin cancer, detecting in medical images" data-type="indexterm" id="id532"/><a contenteditable="false" data-primary="MLPs" data-see="multilayer perceptrons" data-type="indexterm" id="id533"/><a contenteditable="false" data-primary="spatial organization patterns, within cells" data-see="cells, spatial organization patterns within" data-type="indexterm" id="id534"/><a contenteditable="false" data-primary="TFs" data-see="transcription factors" data-type="indexterm" id="id535"/><a contenteditable="false" data-primary="tips and tricks" data-see="debugging" data-type="indexterm" id="id536"/><a contenteditable="false" data-primary="VQ-VAEs" data-see="vector-quantized variational autoencoders" data-type="indexterm" id="id537"/></p>
 </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="A Living Document"><div class="sect1" id="a-living-document">
  <h1>A Living Document</h1>
  <p>While this book endeavors to capture the state of the field at the time of writing, deep learning and biology remain fast-moving frontiers. A printed book is a static artifact; in contrast, frameworks evolve, APIs break, and new ideas emerge constantly.
  </p>
  <div data-type="note" epub:type="note"><h6>Note</h6>
   <p>We’ve done our best to future-proof examples and flag potential breaking changes (like Flax’s transition from
    <code>linen</code> to <code>nnx</code>). Still, some discrepancies may arise over time. If you spot anything out of date—or have corrections, improvements, suggestions, or extensions—please let us know. The online repository can evolve even if the book is in a fixed state.
   </p>
  </div>
  <p>We also encourage you to look beyond these pages. Resources like <a href="http://D2L.ai">D2L.ai</a>, fast.ai, the JAX ecosystem, and preprints on bioRxiv and arXiv are excellent places to go deeper.
  </p>
  <p>
   Most of all, experiment, build, break things, train bad models, and learn. The best way to grow in deep learning for biology is to get your hands dirty.
  </p>
 </div></section>
</div></section></div></div></body></html>