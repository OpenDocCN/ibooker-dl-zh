["```py\n!pip install datasets einops diffusers openai\n```", "```py\nfrom datasets import load_dataset\nfrom utils.ch15util import transforms\n\ndataset = load_dataset(\"huggan/flowers-102-categories\",\n    split=\"train\",)                                      ①\ndataset.set_transform(transforms)\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\n# Plot all the images of the 1st batch in grid\ngrid = make_grid(dataset[:16][\"input\"], 8, 2)            ②\nplt.figure(figsize=(8,2),dpi=300)\nplt.imshow(grid.numpy().transpose((1,2,0)))\nplt.axis(\"off\")\nplt.show()\n```", "```py\nimport torch\nresolution=64\nbatch_size=4\ntrain_dataloader=torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size, shuffle=True)\n```", "```py\nclean_images=next(iter(train_dataloader))[\"input\"]*2-1    ①\nprint(clean_images.shape)\nnums=clean_images.shape[0]\nnoise=torch.randn(clean_images.shape)                     ②\nprint(noise.shape)\n```", "```py\ntorch.Size([4, 3, 64, 64])\ntorch.Size([4, 3, 64, 64])\n```", "```py\nfrom utils.ch15util import DDIMScheduler\n\nnoise_scheduler=DDIMScheduler(num_train_timesteps=1000)    ①\nallimgs=clean_images\nfor step in range(200,1001,200):                           ②\n    timesteps=torch.tensor([step-1]*4).long()\n    noisy_images=noise_scheduler.add_noise(clean_images,\n                 noise, timesteps)                         ③\n    allimgs=torch.cat((allimgs,noisy_images))              ④\n\nimport torchvision\nimgs=torchvision.utils.make_grid(allimgs,4,6)\nfig = plt.figure(dpi=300)\nplt.imshow((imgs.permute(2,1,0)+1)/2)                      ⑤\nplt.axis(\"off\")\nplt.show()\n```", "```py\nimport torch\nfrom torch import nn, einsum\nfrom einops import rearrange\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads=4, dim_head=32):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim=1)                      ①\n        q, k, v = map(\n        lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads),\n        qkv)                                                      ②\n        q = q * self.scale    \n        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n        attn = sim.softmax(dim=-1)                                ③\n        out = einsum('b h i j, b h d j -> b h i d', attn, v)      ④\n        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=h, y=w)\n        return self.to_out(out)                                   ⑤\nattn=Attention(128)\nx=torch.rand(1,128,64,64)\nout=attn(x)\nprint(out.shape)\n```", "```py\ntorch.Size([1, 128, 64, 64])\n```", "```py\nclass UNet(nn.Module):\n… \n    def forward(self, sample, timesteps):                         ①\n        if not torch.is_tensor(timesteps):\n            timesteps = torch.tensor([timesteps],\n                                     dtype=torch.long,\n                                     device=sample.device)\n        timesteps = torch.flatten(timesteps)\n        timesteps = timesteps.broadcast_to(sample.shape[0])\n        t_emb = sinusoidal_embedding(timesteps, self.hidden_dims[0])\n        t_emb = self.time_embedding(t_emb)                        ②\n        x = self.init_conv(sample)\n        r = x.clone()\n        skips = []\n        for block1, block2, attn, downsample in self.down_blocks: ③\n            x = block1(x, t_emb)\n            skips.append(x)\n            x = block2(x, t_emb)\n            x = attn(x)\n            skips.append(x)\n            x = downsample(x)\n        x = self.mid_block1(x, t_emb)\n        x = self.mid_attn(x)\n        x = self.mid_block2(x, t_emb)                             ④\n        for block1, block2, attn, upsample in self.up_blocks:    \n            x = torch.cat((x, skips.pop()), dim=1)                ⑤\n            x = block1(x, t_emb)\n            x = torch.cat((x, skips.pop()), dim=1)\n            x = block2(x, t_emb)\n            x = attn(x)\n            x = upsample(x)\n        x = self.out_block(torch.cat((x, r), dim=1), t_emb)\n        out = self.conv_out(x)\n        return {\"sample\": out}                                    ⑥\n```", "```py\nfrom utils.unet_util import UNet\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nresolution=64\nmodel=UNet(3,hidden_dims=[128,256,512,1024],\n           image_size=resolution).to(device)\nnum=sum(p.numel() for p in model.parameters())\nprint(\"number of parameters: %.2fM\" % (num/1e6,))\nprint(model) \n```", "```py\nnumber of parameters: 133.42M\n```", "```py\nfrom diffusers.optimization import get_scheduler\n\nnum_epochs=100                                             ①\noptimizer=torch.optim.AdamW(model.parameters(),lr=0.0001,\n    betas=(0.95,0.999),weight_decay=0.00001,eps=1e-8)      ②\nlr_scheduler=get_scheduler(                                ③\n    \"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=300,\n    num_training_steps=(len(train_dataloader) * num_epochs))\n```", "```py\nfor epoch in range(num_epochs):\n    model.train()\n    tloss = 0\n    print(f\"start epoch {epoch}\")\n    for step, batch in enumerate(train_dataloader):\n        clean_images = batch[\"input\"].to(device)*2-1\n        nums = clean_images.shape[0]\n        noise = torch.randn(clean_images.shape).to(device)\n        timesteps = torch.randint(0,\n                noise_scheduler.num_train_timesteps,\n                (nums, ),\n                device=device).long()\n        noisy_images = noise_scheduler.add_noise(clean_images,\n                     noise, timesteps)                         ①\n\nnoise_pred = model(noisy_images, \n                       timesteps)[\"sample\"]                    ②\n        loss=torch.nn.functional.l1_loss(noise_pred, noise)    ③\n        loss.backward()\n        optimizer.step()                                       ④\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        tloss += loss.detach().item()\n        if step%100==0:\n            print(f\"step {step}, average loss {tloss/(step+1)}\")\ntorch.save(model.state_dict(),'files/diffusion.pth')\n```", "```py\n    @torch.no_grad()\n    def generate(self,model,device,batch_size=1,generator=None,\n         eta=1.0,use_clipped_model_output=True,num_inference_steps=50):\n        imgs=[]\n        image=torch.randn((batch_size,model.in_channels,model.sample_size,\n\nmodel.sample_size),\n            generator=generator).to(device)              ①\n\n        self.set_timesteps(num_inference_steps)\n        for t in tqdm(self.timesteps):                   ②\n            model_output = model(image, t)[\"sample\"]     ③\n            image = self.step(model_output,t,image,eta,\n                  use_clipped_model_output=\\\n                  use_clipped_model_output)              ④\n            img = unnormalize_to_zero_to_one(image)\n            img = img.cpu().permute(0, 2, 3, 1).numpy()\n            imgs.append(img)                             ⑤\n        image = unnormalize_to_zero_to_one(image)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n        return {\"sample\": image}, imgs \n```", "```py\nsd=torch.load('files/diffusion.pth',map_location=device)\nmodel.load_state_dict(sd)\nwith torch.no_grad():\n    generator = torch.manual_seed(1)                     ①\n    generated_images,imgs = noise_scheduler.generate(\n        model,device,\n        num_inference_steps=50,\n        generator=generator,\n        eta=1.0,\n        use_clipped_model_output=True,\n        batch_size=10)                                   ②\nimgnp=generated_images[\"sample\"]    \nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,4),dpi=300)\nfor i in range(10):                                      ③\n    ax = plt.subplot(2,5, i + 1)\n    plt.imshow(imgnp[i])\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\nplt.show()  \n```", "```py\nsteps=imgs[9::10]                                ①\nimgs20=[]\nfor j in [1,3,6,9]:\n    for i in range(5):\n        imgs20.append(steps[i][j])               ②\nplt.figure(figsize=(10,8),dpi=300)\nfor i in range(20):                              ③\n    k=i%5\n    ax = plt.subplot(4,5, i + 1)\n    plt.imshow(imgs20[i])\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n    plt.title(f't={800-200*k}',fontsize=15,c=\"r\")\nplt.show()\n```", "```py\nfrom openai import OpenAI\n\nopenai_api_key=your actual OpenAI API key here, in quotes   ①\nclient=OpenAI(api_key=openai_api_key)                       ②\n\nresponse = client.images.generate(\n  model=\"dall-e-2\",\n  prompt=\"an astronaut in a space suit riding a unicorn\",\n  size=\"512x512\",\n  quality=\"standard\",\n  n=1,\n)                                                           ③\nimage_url = response.data[0].url\nprint(image_url)                                            ④\n```"]