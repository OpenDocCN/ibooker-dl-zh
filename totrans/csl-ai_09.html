<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span> </span> <span class="chapter-title-text">Interventions and causal effects</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Case studies of interventions in machine learning engineering contexts</li>
<li class="readable-text" id="p3">How interventions relate to A/B tests and randomized experiments</li>
<li class="readable-text" id="p4">Implementing interventions on causal models with intervention operators</li>
<li class="readable-text" id="p5">Using a causal model to represent many interventional distributions</li>
<li class="readable-text" id="p6">Causal effects as natural extensions of an intervention distribution</li>
</ul>
</div>
<div class="readable-text" id="p7">
<p>An intervention is something an agent <em>does</em> to cause other things to happen. Interventions <em>change</em> the data generating process (DGP).</p>
</div>
<div class="readable-text intended-text" id="p8">
<p>Interventions are the most fundamental concept in how we define causality. For example, the concept of intervention, written in terms of “manipulation” and “varying” a factor, is central to this definition from an influential 1979 textbook on experimental design:</p>
</div>
<div class="readable-text" id="p9">
<blockquote>
<div>
     The paradigmatic assertion in causal relationships is that manipulation of a cause will result in the manipulation of an effect . . . . Causation implies that by varying one factor I can make another vary. 
     <a href="#footnote-211"><sup class="footnote-reference" id="footnote-source-1">1</sup></a>
</div>
</blockquote>
</div>
<div class="readable-text" id="p10">
<p>Interventions are how we go from correlation to causality. Correlation is symmetric; the statements “Amazon’s laptop sales correlate with Amazon’s laptop bag sales” and “Amazon’s laptop bag sales correlate with Amazon’s laptop sales” are equivalent. But interventions make causality a one-way street: if Amazon recommends the sale of laptops, laptop bag sales will increase, but if Amazon promotes the sale of laptop bags, we wouldn’t expect people to respond by buying new laptops to fill them.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>A model must have a way of reasoning about intervention to be admitted to the club of causal models. Any model that lets you reason about how interventions change the DGP is, by definition, a causal model.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>You are probably already familiar with interventions in the form of experiments, such as A/B tests or randomized clinical trials. Such experiments focus on inferring causal effects. Put simply, a causal effect is just a comparison of the expected results of different interventions (e.g., a treatment and a control, or “A” and “B” in an A/B test). </p>
</div>
<div class="readable-text intended-text" id="p13">
<p>In this chapter, you’ll learn how to model an intervention and causal effects even if, indeed <em>especially</em> if, we do not or cannot do the intervention in real life. We’ll start this chapter with case studies that motivate modeling interventions. All the datasets and notebooks for executing them are available at <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a>.</p>
</div>
<div class="readable-text" id="p14">
<h2 class="readable-text-h2" id="sigil_toc_id_149"><span class="num-string">7.1</span> Case studies of interventions</h2>
</div>
<div class="readable-text" id="p15">
<p>A machine learning model can drive decisions to make “interventions.” Those interventions can, in turn, create conditions different from those that occurred during model training. This mismatch in training conditions and deployment conditions can lead to problems.</p>
</div>
<div class="readable-text" id="p16">
<h3 class="readable-text-h3" id="sigil_toc_id_150"><span class="num-string">7.1.1</span> Case study: Predicting the weather vs. business performance</h3>
</div>
<div class="readable-text" id="p17">
<p>Every day you wake up, look out the window, and guess whether or not it will rain. Based on that guess, you decide whether to take an umbrella on your morning walk to work. Several times you guess and choose incorrectly; you either take an umbrella and it doesn’t rain, making you look like a fop, or you don’t take the umbrella, and it rains, making you look wet. You decide to train a machine learning model that will take detailed atmospheric readings in the morning and produce a prediction of whether or not it will rain. By leveraging machine learning to get more accurate predictions, you expect fewer mistakes in deciding whether to bring the umbrella.</p>
</div>
<div class="readable-text intended-text" id="p18">
<p>You start by collecting daily atmospheric readings as features, and record whether it rained as labels. After enough days, you have your first block of training data. Next, you train the model on that training data and validate its accuracy on hold-out data. Finally, you deploy the trained model, meaning that you use it daily to decide whether to take or leave your umbrella. As you use the deployed model, you continue to log features and labels daily. Eventually, you have enough additional data for a second training block, and you retrain your model to benefit from both blocks of data, leading to higher accuracy than you had after training on just the first block. You continue to iteratively train the model as you collect more blocks of data. Figure 7.1 illustrates the workflow.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p19">
<img alt="figure" height="217" src="../Images/CH07_F01_Ness.png" width="1009"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.1</span> Example of a machine learning training workflow where the sensor data is the features, weather is the label, and bringing an umbrella is the decision. After each training block, the new data is used to update the old model, and a new model is deployed. In this case, the decision does not affect future data.</h5>
</div>
<div class="readable-text" id="p20">
<p>Now let’s consider a parallel example in business. You are a data scientist at a company. Instead of atmospheric readings, you have economic and industry data. Instead of predicting whether the day will be rainy, you are predicting whether the quarter will end with low revenues. Instead of deciding whether to bring an umbrella, you are deciding whether to advertise. Figure 7.2 illustrates the workflow, which mirrors the weather example in figure 7.1 exactly; sunny and rainy days in figure 7.1 map to good and bad quarters in figure 7.2, and the decision to bring or leave the umbrella maps to the decision to advertise or not.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p21">
<img alt="figure" height="224" src="../Images/CH07_F02_Ness.png" width="1009"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.2</span> This is a mirror example of the workflow in figure 7.1. Business indicators are the features, quarterly performance is the label, and advertising is the decision. In this case, the decision affects future data.</h5>
</div>
<div class="readable-text" id="p22">
<p>Even though the labels and decisions in the two examples mirror one another, the causal structure of the business example is fundamentally different; the act of bringing an umbrella will not affect the weather in future days, but the act of advertising will affect business in future quarters. As a result, training block 2 represents a different DGP than training block 1 because revenue in training block 2 was affected by advertising. During training, a naive predictive model might go so far as to associate signs of a lousy quarter with <em>high</em> revenue, since, in the past, signs of bad quarters led your company to advertise, which consequently boosted revenue.</p>
</div>
<div class="readable-text intended-text" id="p23">
<p>We deploy machine learning models to drive or automate decisions. Those decisions do not impact the data in domains like meteorology, geology, and astronomy. But in many, if not most, domains where we want to use machine learning, those model-driven decisions are interventions—actions that change the DGP. That can lead to a mismatch between the model’s training and deployment conditions, leading to problems in the model’s reliability.</p>
</div>
<div class="readable-text intended-text" id="p24">
<p>Another real-world example of this problem occurs in anomaly detection.</p>
</div>
<div class="readable-text" id="p25">
<h3 class="readable-text-h3" id="sigil_toc_id_151"><span class="num-string">7.1.2</span> Case study: Credit fraud detection</h3>
</div>
<div class="readable-text" id="p26">
<p>Anomaly detection seeks to predict when an abnormal event is occurring. One example is detecting a fraudulent transaction on a credit card. Credit card companies do supervised training of predictive models of fraud using transaction data, where attributes of credit card transactions (buying patterns, location, cost of the item, etc.) are the features, and whether the customer later reports the transaction as fraudulent is the label.</p>
</div>
<div class="readable-text intended-text" id="p27">
<p>As in the weather and business examples, you train a model on an initial training block. After training, you can deploy the algorithm to predict fraud in real time. When a transaction is initiated, the algorithm is run, and a prediction is generated. If the algorithm predicts fraud, the transaction is rejected.</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>While this system is in deployment, a second training set is being compiled. Some fraud still gets through and is later reported as fraudulent by the customers. Those transactions are labeled fraudulent in this new block of data, but the DGP has changed from the initial training set. The deployed version 1.0 predictive model is rejecting transactions that it predicted were fraudulent, but because they were rejected, you don’t know if they were actual cases of fraud. These rejected transactions are excluded from the next training set because they lack labels. </p>
</div>
<div class="readable-text intended-text" id="p29">
<p>If the model is retrained on the second block, it may develop a bias toward fraud that slipped past the fraud rejection system and against the cases of fraud that were rejected. This bias can become more severe over several iterations. This process is analogous to a homicide detective who, over time, does well in solving cases involving uncommon weapons but poorly in cases involving guns.</p>
</div>
<div class="readable-text intended-text" id="p30">
<p>The filtering of fraudulent transactions in deployment is an intervention. In practice, anomaly detection algorithms address this problem by accounting for interventions in some way.</p>
</div>
<div class="readable-text" id="p31">
<h3 class="readable-text-h3" id="sigil_toc_id_152"><span class="num-string">7.1.3</span> Case study: Statistical analysis for an online role-playing game</h3>
</div>
<div class="readable-text" id="p32">
<p>Suppose you are a data scientist at an online role-playing game company. Your leadership wants to know if side-quest engagement (mini-objectives that are tangential to the game’s primary objectives) is a driver of in-game purchases of virtual artifacts. If the answer is yes, the company will intervene in the game dynamics such that players engage in more side-quests.</p>
</div>
<div class="readable-text intended-text" id="p33">
<p>You do an analysis. You query the database and pull records for a thousand players, the first five of which are shown in table 7.1. This is observational data (in contrast to experimental data) because the data is logged observations of the natural behavior of players as they log in and play. (The full dataset is available in the notebooks for the chapter: <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a>.)</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p34">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.1</span> Example rows from observational data on <em>Side-Quest Engagement</em> and <em>In-Game Purchases</em></h5>
<table>
<thead>
<tr>
<th>
<div>
        User ID
       </div></th>
<th>
<div>
        Side-Quest Engagement
       </div></th>
<th>
<div>
        In-Game Purchases
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> 71d44ad5<br/></td>
<td> high<br/></td>
<td> 156.77<br/></td>
</tr>
<tr>
<td> e6397485<br/></td>
<td> low<br/></td>
<td> 34.89<br/></td>
</tr>
<tr>
<td> 87a5eaf7<br/></td>
<td> high<br/></td>
<td> 172.86<br/></td>
</tr>
<tr>
<td> c5d78ca4<br/></td>
<td> low<br/></td>
<td> 215.74<br/></td>
</tr>
<tr>
<td> d3b2a8ed<br/></td>
<td> high<br/></td>
<td> 201.07<br/></td>
</tr>
<tr>
<td> dc85d847<br/></td>
<td> low<br/></td>
<td> 12.93<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p35">
<p>The standard data science analysis would involve running a statistical test of the hypothesis that there is a difference between the <em>In-Game Purchases</em> of players highly engaged in side-quests and those with low <em>Side-Quest Engagement</em>. The test calculates the mathematical difference between the sample means of <em>In-Game Purchases</em> for both groups. In statistical terms, this difference estimates an <em>effect size</em>. The test will examine whether this estimated effect size is significantly different from zero.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p36">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Setting up your environment</h5>
</div>
<div class="readable-text" id="p37">
<p>The code for this chapter was written with Pyro version 1.9.0, pandas version 2.2.1, and pgmpy version 0.1.25. Using Pyro’s <code>render</code> function to visualize a Pyro model as a DAG will require Graphviz. Visit <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a> for a link to a notebook that contains the code.</p>
</div>
</div>
<div class="readable-text" id="p38">
<p>We’ll perform this hypothesis test with the pandas library. First, we’ll pull the data and get the sample means and standard deviations within each group.</p>
</div>
<div class="browsable-container listing-container" id="p39">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.1</span> Load <em>Side-Quest Engagement</em> vs.<em> </em><em>In-Game Purchases</em> data and summarize</h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
data_url = (   <span class="aframe-location"/> #1
    "https://raw.githubusercontent.com/altdeep/causalML/master/"     #1
    "datasets/sidequests_and_purchases_obs.csv"    #1
)     #1
df = pd.read_csv(data_url)    #1
summary = df.drop('User ID', axis=1).groupby(    <span class="aframe-location"/> #2
    ["Side-quest Engagement"]     #2
).agg(     #2
    ['count', 'mean', 'std']     #2
)     #2
summary</pre>
<div class="code-annotations-overlay-container">
     #1 Load the data from the database query into a pandas DataFrame.
     <br/>#2 For each level of Side-Quest Engagement (“low”, “high”), calculate the sample count (number of players), the sample mean In-Game Purchases amount, and the standard deviation.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p40">
<p>This produces the summary in table 7.2.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p41">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.2</span> Summary statistics from the online game data</h5>
<table>
<thead>
<tr>
<th>
<div>
        Side-Quest Engagement
       </div></th>
<th>
<div>
        mean purchases
       </div></th>
<th>
<div>
        std
       </div></th>
<th>
<div>
        n
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> low<br/></td>
<td> 73.10<br/></td>
<td> 75.95<br/></td>
<td> 518<br/></td>
</tr>
<tr>
<td> high<br/></td>
<td> 111.61<br/></td>
<td> 55.56<br/></td>
<td> 482<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p42">
<p>This database query pulled 1,000 players, where 482 of them were highly engaged in side-quests and 518 were not. The mean <em>In-Game Purchases</em> amount for highly engaged players is around $112 for high <em>Side-Quest Engagement</em> and $73 for low <em>Side-Quest Engagement</em>. Generalizing beyond this data, we conclude that players who are highly engaged in side-quests spend, on average 112 – 73 = $39 dollars more than those who aren’t. We can run a two-sample <em>Z</em>-test to make sure this difference is significant.</p>
</div>
<div class="browsable-container listing-container" id="p43">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><strong><span class="num-string">Listing 7.2</span> Test if effect of engagement on <em>In-Game Purchases</em> is statistically significant</strong></h5>
<div class="code-area-container">
<pre class="code-area">n1, n2 = summary['In-game Purchases']['count']   <span class="aframe-location"/> #1
m1, m2 = summary['In-game Purchases']['mean']  <span class="aframe-location"/> #2
s1, s2 =  summary['In-game Purchases']['std']    <span class="aframe-location"/> #3
pooled_std = (s1**2 / n1 + s2**2 / n2) **.5  <span class="aframe-location"/> #4
z_score = (m1 - m2) / pooled_std   <span class="aframe-location"/> #5
abs(z_score) &gt; 2.  <span class="aframe-location"/> #6</pre>
<div class="code-annotations-overlay-container">
     #1 n1 and n2 are the numbers of players in each group (high vs. low engagement).
     <br/>#2 m1 and m2 are the group sample means.
     <br/>#3 s1 and s2 are the group standard deviations.
     <br/>#4 Estimate the standard error of the difference in mean spending by pooling (combining) the group standard deviations.
     <br/>#5 Convert to a z-score, which has a standard norm under the (null) hypothesis of no difference in spending across engagement levels.
     <br/>#6 Test if the z-score is more than 2 standard deviations from 0, which beats a 5% significance threshold.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p44">
<p>Running this code shows that the difference in means is significant. Great, you did some data science that showed you have a statistically significant effect size: <em>In-Game Purchases</em> are significantly higher for players who are highly engaged in side-quests relative to those who are not. Based on your findings, leadership decides to modify the game dynamics to draw players into more side-quests. As a result, <em>In-Game Purchases</em> <em>decline</em>. How could this happen? </p>
</div>
<div class="readable-text" id="p45">
<h3 class="readable-text-h3" id="sigil_toc_id_153"><span class="num-string">7.1.4</span> From randomized experiments to interventions</h3>
</div>
<div class="readable-text" id="p46">
<p>By now you’ve probably recognized that the result from listing 7.2 is a textbook example of how correlation doesn’t imply causation. If management wanted to know if intervening on game dynamics would lead to an increase in <em>In-Game Purchases</em>, they should have relied on analysis from a randomized experiment, not simple observational data. We’ll use the randomized experiment to build more intuition for a formal model of intervention and see how that intervention model could simulate a randomized experiment.</p>
</div>
<div class="readable-text" id="p47">
<h3 class="readable-text-h3" id="sigil_toc_id_154"><span class="num-string">7.1.5</span> From observations to experiments</h3>
</div>
<div class="readable-text" id="p48">
<p>Suppose that instead of running an observational study, you run an experiment. Rather than pull data from a SQL query, you randomly select a set of 1,000 players and randomly assign them to one of two groups of 500. In one group, the game dynamics are modified such that <em>Side-Quest Engagement</em> is artificially fixed at “low,” and in the other group it is fixed to “high.” We’ll then observe their level of <em>In-Game Purchases</em>.</p>
</div>
<div class="readable-text intended-text" id="p49">
<p>This will create experimental data that is the same size and has roughly the same split between engaged and unengaged players as the observational data in section 7.1.3. Similarly, we’ll run the same downstream analysis. This will let us make an apples-to-apples comparison of using observational versus experimental data.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>Table 7.3 shows examples from the experimental data. You can find links to the data in <a href="https://www.altdeep.ai/p/causalaibook">https://www.altdeep.ai/p/causalaibook</a>.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p51">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.3</span> Example rows from the experimental data evaluating the effect of <em>Side-Quest Engagement</em> on <em>In-Game Purchases</em> </h5>
<table>
<thead>
<tr>
<th>
<div>
        User ID
       </div></th>
<th>
<div>
        Side-Quest Engagement
       </div></th>
<th>
<div>
        In-Game Purchases
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> 2828924d<br/></td>
<td> low<br/></td>
<td> 224.39<br/></td>
</tr>
<tr>
<td> 7e7c2452<br/></td>
<td> low<br/></td>
<td> 19.89<br/></td>
</tr>
<tr>
<td> 3ddf2915<br/></td>
<td> low<br/></td>
<td> 221.26<br/></td>
</tr>
<tr>
<td> 10c3d883<br/></td>
<td> high<br/></td>
<td> 93.21<br/></td>
</tr>
<tr>
<td> c5080957<br/></td>
<td> high<br/></td>
<td> 61.82<br/></td>
</tr>
<tr>
<td> 241c8fcf<br/></td>
<td> high<br/></td>
<td> 188.76<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p52">
<p>Again, we summarize the data with the following code.</p>
</div>
<div class="browsable-container listing-container" id="p53">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.3</span> Load experimental data and summarize</h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
exp_data_url = (    <span class="aframe-location"/> #1
    "https://raw.githubusercontent.com/altdeep/causalML/master/"   #1
    "datasets/sidequests_and_purchases_exp.csv"     #1
)    #1
df = pd.read_csv(exp_data_url)     #1
summary = df.drop('User ID', axis=1).groupby(   <span class="aframe-location"/> #2
    ["Side-quest Engagement"]     #2
).agg(     #2
    ['count', 'mean', 'std']    #2
)
print(summary)</pre>
<div class="code-annotations-overlay-container">
     #1 Load the experimental data from the database query into a pandas DataFrame.
     <br/>#2 For each level of Side-Quest Engagement (“low”, “high”), calculate the sample count (number of players), the sample mean in-game purchase amount, and the standard deviation.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p54">
<p>Table 7.4 shows the same summary statistics for the experimental data as table 7.2 does for the observational data.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p55">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.4</span> Summary statistics from the online game experimental data</h5>
<table>
<thead>
<tr>
<th>
<div>
        Side-Quest Engagement
       </div></th>
<th>
<div>
        mean purchases
       </div></th>
<th>
<div>
        std
       </div></th>
<th>
<div>
        n
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> low<br/></td>
<td> 92.99<br/></td>
<td> 51.67<br/></td>
<td> 500<br/></td>
</tr>
<tr>
<td> high<br/></td>
<td> 131.38<br/></td>
<td> 94.84<br/></td>
<td> 500<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p56">
<p>The experiment reflects what happened when the company intervened to increase <em>Side-Quest Engagement</em>. The sign of the effect size is negative relative to our first analysis; we got –38.39, meaning the mean purchases went down $38.39. When we rerun the test of significance in listing 7.4, we see the difference is significant for the experimental data, just as it was for the observational data.</p>
</div>
<div class="browsable-container listing-container" id="p57">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.4</span> Conduct significance test on (experimental) difference in mean purchases</h5>
<div class="code-area-container">
<pre class="code-area">n1, n2 = summary['In-game Purchases']['count']    <span class="aframe-location"/> #1
m1, m2 = summary['In-game Purchases']['mean']    <span class="aframe-location"/> #2
s1, s2 =  summary['In-game Purchases']['std']   <span class="aframe-location"/> #3
pooled_std = (s1**2 / n1 + s2**2 / n2) **.5   <span class="aframe-location"/> #4
z_score = (m1 - m2) / pooled_std    <span class="aframe-location"/> #5
abs(z_score) &gt; 2.   <span class="aframe-location"/> #6</pre>
<div class="code-annotations-overlay-container">
     #1 n1 and n2 are the number of players in each group (high vs low engagement).
     <br/>#2 m1 and m2 are the group sample means.
     <br/>#3 s1 and s2 are the group standard deviations.
     <br/>#4 Estimate the standard error of the difference in mean spend by pooling (combining) the group standard deviations.
     <br/>#5 Convert to a z-score, which has a standard norm under the (null) hypothesis of no difference in spend across engagement levels.
     <br/>#6 Tests if the z-score is more than 2 standard deviations from 0, which beats a 5% significance threshold.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p58">
<p>The result shows the difference in group means is again significant. If you had reported the results of this experiment instead of the results of the observational study, you would have correctly concluded that a policy of encouraging higher <em>Side-Quest Engagement</em> would lead to a drop in average <em>In-Game Purchases</em> (and you wouldn’t have recommended doing so). </p>
</div>
<div class="readable-text intended-text" id="p59">
<p>This experiment had a cost. Many of those 1,000 players who were included in the experiment would have spent more on <em>In-Game Purchases</em> had they not been included in the experiment, and this is especially true for the 500 players assigned to the high side-quests group. That amounts to lost revenue that would have been realized had you not run the experiment. Moreover, the experiment created a suboptimal gaming experience for players who were assigned a level of <em>Side-Quest Engagement</em> that was different from their preferred level. These players are paying the company for a certain experience, and the experiment degraded that experience.</p>
</div>
<div class="readable-text intended-text" id="p60">
<p>The least ideal outcome is reporting based on our simple two-sample analysis of the observational data; this had no cost, but it gave the wrong answer. A better outcome is running the experiment and getting the correct answer, though this comes at a cost. The ideal outcome is getting the right answer on the observational data for free. To do that, we need a causal model.</p>
</div>
<div class="readable-text" id="p61">
<h3 class="readable-text-h3" id="sigil_toc_id_155"><span class="num-string">7.1.6</span> From experiments to interventions</h3>
</div>
<div class="readable-text" id="p62">
<p>Let’s see how we can use a causal model to simulate the results of the experiment from the observational data. First, let’s assume the causal DAG in figure 7.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p63">
<img alt="figure" height="205" src="../Images/CH07_F03_Ness.png" width="449"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.3</span> A simple DAG showing the causal relationship between <em>Side-Quest Engagement</em> and <em>In-Game Purchases. Guild Membership</em><em> </em>is a common cause of both.</h5>
</div>
<div class="readable-text intended-text" id="p64">
<p>In our online game, many players are members of guilds. Guilds are groups of players who pool resources and coordinate their gameplay, such as working together on side-quests. Our model assumes that the amount of <em>In-Game Purchases</em> a player makes also depends on whether they are in a guild; members of the same guild pool resources, and many resources are virtual items they must purchase.</p>
</div>
<div class="readable-text intended-text" id="p65">
<p>Suppose you run a modified version of that initial database query. The query produces the same exact observational data seen in table 7.1, except this time it includes an additional column indicating <em>Guild Membership</em>. Again, we see six players in table 7.5 (the same six as players shown in table 7.1).</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p66">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.5</span> The same observational data as in table 7.1, but with a <em>Guild Membersh</em><em>ip</em><em> </em>column</h5>
<table>
<thead>
<tr>
<th>
<div>
        User ID
       </div></th>
<th>
<div>
        Side-Quest Engagement
       </div></th>
<th>
<div>
        Guild Membership
       </div></th>
<th>
<div>
        In-Game Purchases
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> 71d44ad5<br/></td>
<td> high<br/></td>
<td> member<br/></td>
<td> 156.77<br/></td>
</tr>
<tr>
<td> e6397485<br/></td>
<td> low<br/></td>
<td> nonmember<br/></td>
<td> 34.89<br/></td>
</tr>
<tr>
<td> 87a5eaf7<br/></td>
<td> high<br/></td>
<td> member<br/></td>
<td> 172.86<br/></td>
</tr>
<tr>
<td> c5d78ca4<br/></td>
<td> low<br/></td>
<td> member<br/></td>
<td> 215.74<br/></td>
</tr>
<tr>
<td> d3b2a8ed<br/></td>
<td> high<br/></td>
<td> member<br/></td>
<td> 201.07<br/></td>
</tr>
<tr>
<td> dc85d847<br/></td>
<td> low<br/></td>
<td> nonmember<br/></td>
<td> 12.93<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p67">
<p>We are going to build a causal graphical model on this observational data using Pyro. To do this, we’ll need to model the causal Markov kernels: the probability distributions of <em>Guild Membership</em>, <em>Side-Quest Engagement</em> given <em>Guild Membership</em>, and <em>In-Game Purchases</em> given <em>Guild Membership </em>and <em>Side-Quest Engagement</em>. In our Pyro model, we’ll need to specify some canonical distributions for these variables and estimate their parameters.</p>
</div>
<div class="readable-text" id="p68">
<h4 class="readable-text-h4 sigil_not_in_toc">Estimating parameters and building the model</h4>
</div>
<div class="readable-text" id="p69">
<p>Pyro can jointly estimate the parameters of each of our causal Markov kernels just as it could the parameters across a complex neural network architecture. But it will make our lives easier to estimate the parameters of each kernel one at a time using everyday data science analysis, leveraging the concept of <em>parameter modularity</em> discussed in chapter 2. Let’s start with <em>Guild Membership</em>.</p>
</div>
<div class="browsable-container listing-container" id="p70">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.5</span> Estimate the probability distribution of <em>Guild Membership</em></h5>
<div class="code-area-container">
<pre class="code-area">import pandas as pd
full_obs_url = (    <span class="aframe-location"/> #1
    "https://raw.githubusercontent.com/altdeep/causalML/master/"     #1
    "datasets/sidequests_and_purchases_full_obs.csv"    #1
)    #1
df = pd.read_csv(full_obs_url)      #1
membership_counts = df['Guild Membership'].value_counts()   <span class="aframe-location"/> #2
dist_guild_membership = membership_counts / sum(membership_counts)     #2
print(dist_guild_membership)    #2</pre>
<div class="code-annotations-overlay-container">
     #1 Load the data from the database query into a pandas DataFrame.
     <br/>#2 Calculate the proportions of members vs. nonmembers.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p71">
<p>This prints out the following result:</p>
</div>
<div class="browsable-container listing-container" id="p72">
<div class="code-area-container">
<pre class="code-area">nonmember    0.515
member       0.485
Name: Guild Membership, dtype: float64</pre>
</div>
</div>
<div class="readable-text" id="p73">
<p>These are the proportions of guild members vs. nonmembers in the data. We can use these as estimates of the probability that a player is a member or a nonmember. If we took these proportions as is, they would be maximum likelihood estimates of the probabilities, but for simplicity, we’ll just put it at 50/50 (the probability of being a member is .5).</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>Next, we’ll do the same for the conditional probability distribution (CPD) of <em>Side-Quest Engagement</em> level given <em>Guild Membership</em>.</p>
</div>
<div class="browsable-container listing-container" id="p75">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.6</span> Estimate the CPD of <em>Side-Quest Engagement</em> given <em>Guild Membership</em></h5>
<div class="code-area-container">
<pre class="code-area">member_subset = df[(df['Guild Membership'] == 'member')]    <span class="aframe-location"/> #1
member_engagement_counts = (   #1
    member_subset['Side-quest Engagement'].value_counts()   #1
)    #1
dist_engagement_member = (    #1
    member_engagement_counts / sum(member_engagement_counts)   #1
)    #1
print(dist_engagement_member)    #1

nonmember_subset = df[(df['Guild Membership'] == 'nonmember')]   <span class="aframe-location"/> #2
nonmember_engagement_counts = (   #2
    nonmember_subset['Side-quest Engagement'].value_counts()    #2
)     #2
dist_engagement_nonmember = (    #2
    nonmember_engagement_counts /   #2
sum(nonmember_engagement_counts)    #2
)     #2
print(dist_engagement_nonmember)     #2</pre>
<div class="code-annotations-overlay-container">
     #1 Calculate the probability distribution of Side-Quest Engagement level (“high” vs. “low”) given that a player is a member of a guild.
     <br/>#2 Calculate the probability distribution of Side-Quest Engagement level (“high” vs. “low”) given that a player is not a member of a guild.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p76">
<p>Listing 7.6 prints the following output proportions of <em>Side-Quest Engagement</em> levels for guild members:</p>
</div>
<div class="browsable-container listing-container" id="p77">
<div class="code-area-container">
<pre class="code-area">high    0.797938
low     0.202062</pre>
</div>
</div>
<div class="readable-text" id="p78">
<p>The following proportions are for non-guild-members:</p>
</div>
<div class="browsable-container listing-container" id="p79">
<div class="code-area-container">
<pre class="code-area">high    0.184466
low     0.815534</pre>
</div>
</div>
<div class="readable-text" id="p80">
<p>Again, we’ll round these results. Guild members have an 80% chance of being highly engaged in side-quests, while nonmembers have only a 20% chance of being highly engaged.</p>
</div>
<div class="readable-text intended-text" id="p81">
<p>Finally, for each combination of <em>Guild Membership </em>and <em>Side-Quest Engagement</em>, we’ll calculate the sample mean and standard deviation of <em>In-Game Purchases</em>. We’ll use these sample statistics as estimates for mean and location parameters in a canonical distribution when we code the causal Markov kernel for <em>In-Game Purchases</em> in the causal model. </p>
</div>
<div class="browsable-container listing-container" id="p82">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><strong><span class="num-string">Listing 7.7</span> Calculate purchase stats across levels of engagement and <em>Guild Membership</em></strong></h5>
<div class="code-area-container">
<pre class="code-area">purchase_dist_nonmember_low_engagement = df[    <span class="aframe-location"/> #1
    (df['Guild Membership'] == 'nonmember') &amp;    #1
    (df['Side-quest Engagement'] == 'low')    #1
].drop(     #1
  ['User ID', 'Side-quest Engagement', 'Guild Membership'], axis=1   #1  
).agg(['mean', 'std'])    #1
print(round(purchase_dist_nonmember_low_engagement, 2))    #1

purchase_dist_nonmember_high_engagement = df[    <span class="aframe-location"/> #2
    (df['Guild Membership'] == 'nonmember') &amp;    #2
    (df['Side-quest Engagement'] == 'high')    #2
].drop(     #2
  ['User ID', 'Side-quest Engagement', 'Guild Membership'], axis=1     #2
).agg(['mean', 'std'])    #2
print(round(purchase_dist_nonmember_high_engagement, 2))    #2

purchase_dist_member_low_engagement = df[    <span class="aframe-location"/> #3
    (df['Guild Membership'] == 'member') &amp;    #3
    (df['Side-quest Engagement'] == 'low')    #3
].drop(     #3
  ['User ID', 'Side-quest Engagement', 'Guild Membership'], axis=1     #3
).agg(['mean', 'std'])     #3
print(round(purchase_dist_member_low_engagement, 2))     #3

purchase_dist_member_high_engagement = df[ <span class="aframe-location"/>    #4
    (df['Guild Membership'] == 'member') &amp;    #4
    (df['Side-quest Engagement'] == 'high')    #4
].drop(    #4
  ['User ID', 'Side-quest Engagement', 'Guild Membership'], axis=1    #4
).agg(['mean', 'std'])     #4
print(round(purchase_dist_member_high_engagement, 2))   #4</pre>
<div class="code-annotations-overlay-container">
     #1 Estimate the sample mean and standard deviation of In-Game Purchases for non-guild-members with low Side-Quest Engagement.
     <br/>#2 Estimate the sample mean and standard deviation of In-Game Purchases for non-guild-members with high Side-Quest Engagement.
     <br/>#3 Estimate the sample mean and standard deviation of In-Game Purchases for guild members with low Side-Quest Engagement.
     <br/>#4 Estimate the sample mean and standard deviation of In-Game Purchases for guild members with high Side-Quest Engagement.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p83">
<p>For non-guild-members with low <em>Side-Quest Engagement</em>, we have these results:</p>
</div>
<div class="browsable-container listing-container" id="p84">
<div class="code-area-container">
<pre class="code-area">      In-game Purchases
mean              37.95
std               23.80</pre>
</div>
</div>
<div class="readable-text" id="p85">
<p>For non-guild-members with high <em>Side-Quest Engagement</em>, we have</p>
</div>
<div class="browsable-container listing-container" id="p86">
<div class="code-area-container">
<pre class="code-area">      In-game Purchases
mean              54.92
std                4.92</pre>
</div>
</div>
<div class="readable-text" id="p87">
<p>For guild members with low <em>Side-Quest Engagement</em>, we have</p>
</div>
<div class="browsable-container listing-container" id="p88">
<div class="code-area-container">
<pre class="code-area">      In-game Purchases
mean             223.71
std                5.30</pre>
</div>
</div>
<div class="readable-text" id="p89">
<p>For guild members with high <em>Side-Quest Engagement</em>, we have</p>
</div>
<div class="browsable-container listing-container" id="p90">
<div class="code-area-container">
<pre class="code-area">      In-game Purchases
mean             125.53
std               53.44</pre>
</div>
</div>
<div class="readable-text" id="p91">
<p>Finally, in listing 7.8, we use these various statistics as parameter estimates in a causal graphical model built in Pyro. </p>
</div>
<div class="browsable-container listing-container" id="p92">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.8</span> Building a causal model of <em>In-Game Purchases</em> in Pyro</h5>
<div class="code-area-container">
<pre class="code-area">import pyro
from torch import tensor
from pyro.distributions import Bernoulli, Normal

def model():
    p_member = tensor(0.5)   <span class="aframe-location"/> #1
    is_guild_member = pyro.sample(    #1
        "Guild Membership",     #1
        Bernoulli(p_member)     #1
    )     #1
    p_engaged = (tensor(0.8)*is_guild_member +   <span class="aframe-location"/> #2
                 tensor(.2)*(1-is_guild_member))    #2
    is_highly_engaged = pyro.sample(    #2
        "Side-quest Engagement",     #2
        Bernoulli(p_engaged)     #2
    )     #2
    get_purchase_param = lambda param1, param2, param3, param4: (    <span class="aframe-location"/> #3
        param1 * (1-is_guild_member) * (1-is_highly_engaged) +     #3
        param2 * (1-is_guild_member) * (is_highly_engaged) +   #3
        param3 * (is_guild_member)   * (1-is_highly_engaged) +    #3
        param4 * (is_guild_member)   * (is_highly_engaged)     #3
    )     #3
    μ = get_purchase_param(37.95, 54.92, 223.71, 125.50)    <span class="aframe-location"/> #4
    σ = get_purchase_param(23.80, 4.92, 5.30, 53.49)    #4
    in_game_purchases = pyro.sample(    #4
        "In-game Purchases",   #4
        Normal(μ, σ)     #4
    )   #4
    guild_membership = "member" if is_guild_member else "nonmember"    <span class="aframe-location"/> #5
    engagement = "high" if is_highly_engaged else "low"   #5
    in_game_purchases = float(in_game_purchases)     #5
    return guild_membership, engagement, in_game_purchases     #5</pre>
<div class="code-annotations-overlay-container">
     #1 Probability of being a guild member vs. a nonmember is .5. Using this probability, we generate a Guild Membership value (1 for member, 0 for nonmember) from a Bernoulli distribution.
     <br/>#2 We generate a value for Side-Quest Engagement from a Bernoulli distribution (1 for high, 0 for low) with a parameter that depends on Guild Membership.
     <br/>#3 Helper function for calculating parameters for In-Game Purchases
     <br/>#4 We specify the location parameter of a normal distribution on In-Game Purchases using the sample means we found in the observational data.
     <br/>#5 As with the mean parameters, we specify the scale parameters for a canonical distribution on In-Game Purchases using the standard deviations we found in the data.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p93">
<p>To confirm that the Pyro model encodes a causal DAG, we can run <code>pyro.render_ model(model)</code>, which produces figure 7.4.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p94">
<img alt="figure" height="257" src="../Images/CH07_F04_Ness.png" width="314"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.4</span> Result of calling <code>pyro.render_model</code> with the causal model</h5>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p95">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Leveraging the parametric flexibility of probabilistic programming</h5>
</div>
<div class="readable-text" id="p96">
<p>Note the flexibility of our choices for modeling the variables in the Pyro model. For example, in modeling the distribution of <em>In-Game Purchases</em>, we used the normal distribution, but we could have used other distributions. For example, <em>In-Game Purchases</em> cannot be a negative number, so we could have selected a canonical distribution that is only defined for positive numbers, rather than a normal distribution, which is defined for negative and positive numbers. This would be especially useful for non-guild-members with low <em>Side-Quest Engagement</em>, because generation from a normal distribution with a mean of 37.95 and a scale parameter of 23.80 will have about a 5.5% chance of generating a negative value. However, we’re choosing to be a bit lazy and use the normal distribution in this case, since a few negative numbers for <em>In-Game Purchases</em> won’t have much impact on the results of our analysis.</p>
</div>
<div class="readable-text" id="p97">
<p>The point is that probabilistic programming tools like Pyro provide us with parametric flexibility, unlike tools like pgmpy. It is good practice to leverage that flexibility to reflect your assumptions about the DGP.</p>
</div>
</div>
<div class="readable-text" id="p98">
<h4 class="readable-text-h4 sigil_not_in_toc">Pyro’s intervention abstraction</h4>
</div>
<div class="readable-text" id="p99">
<p>Pyro has an abstraction for representing an intervention in <code>pyro.do</code>. It takes a model and returns a new model that reflects the intervention. Listing 7.9 shows how we can use <code>pyro.do</code> to change the previous model into one that reflects an intervention that sets <em>Side-Quest Engagement</em> to “high” and to “low.”</p>
</div>
<div class="browsable-container listing-container" id="p100">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.9</span> Representing interventions with <code>pyro.do</code> </h5>
<div class="code-area-container">
<pre class="code-area">int_engaged_model = pyro.do(   <span class="aframe-location"/> #1
    model,    #1
    {"Side-quest Engagement": tensor(1.)}   #1
)    #1
int_unengaged_model = pyro.do(    <span class="aframe-location"/> #2
    model,    #2
    {"Side-quest Engagement": tensor(0.)}     #2
)    #2</pre>
<div class="code-annotations-overlay-container">
     #1 An intervention that sets Side-Quest Engagement to 1.0 (i.e., “high”). This returns a new model.
     <br/>#2 An intervention that sets Side-Quest Engagement to 0.0 (i.e., “low”). This returns a new model.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p101">
<p>Now we have two new models: one with an intervention that sets <em>Side-Quest Engagement</em> to “high” and one that sets it to “low.” If our original model is correct, generating 500 examples from each of these new intervened-upon models, and combining them into 1000 examples, effectively <em>simulates</em> the experiment. Remember, we estimated the parameters of this causal model using only the observational data illustrated in table 7.4. If we can train a model on observational data and use it to accurately simulate the results of an experiment, that saves us from actually having to run the experiment.</p>
</div>
<div class="readable-text intended-text" id="p102">
<p>Listing 7.10 uses <code>int_engaged_model</code> and <code>int_unengaged_model</code> to simulate experimental data. We can confirm that the simulation was effective by comparing the summary statistics of this simulated data to the summary statistics of the actual experimental data.</p>
</div>
<div class="browsable-container listing-container" id="p103">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.10</span> Simulating experimental data with <code>pyro.do</code> interventions</h5>
<div class="code-area-container">
<pre class="code-area">pyro.util.set_rng_seed(123)   <span class="aframe-location"/> #1
simulated_experimental_data = [   <span class="aframe-location"/> #2
    int_engaged_model() for _ in range(500)    #2
] + [     #2
    int_unengaged_model() for _ in range(500)     #2
]    #2
simulated_experimental_data = pd.DataFrame(    #2
    simulated_experimental_data,    #2
    columns=[    #2
        "Guild Membership",    #2
        "Side-quest Engagement",    #2
        "In-Game Purchases"    #2
    ]    #2
)     #2
sim_exp_df = simulated_experimental_data.drop(    <span class="aframe-location"/> #3
    "Guild Membership", axis=1)    #3
summary = sim_exp_df.groupby(    <span class="aframe-location"/> #4
        ["Side-quest Engagement"]    #4
    ).agg(    #4
        ['count', 'mean', 'std']     #4
    )     #4
print(summary)    #4</pre>
<div class="code-annotations-overlay-container">
     #1 Set a random seed for reproducibility.
     <br/>#2 Simulate 500 rows from each intervention model, and combine them to create simulated experimental data.
     <br/>#3 The simulated data will include a Guild Membership column. We can drop it to get simulated data that looks like the original experiment.
     <br/>#4 Recreate the statistical summaries of In-Game Purchases for each level of engagement.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p104">
<p>This code simulates the experiment, providing the summaries in table 7.6. Again, these are sample statistics from a simulated experiment we created by first estimating some parameters on observational data, second, building a causal generative model with those parameters, and third, using <code>pyro.do</code> to simulate the results of an intervention. Contrast these with the statistics in table 7.7 that we obtained from the <em>actual</em> experimental data.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p105">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.6</span> Summary statistics from the simulated experiment</h5>
<table>
<thead>
<tr>
<th>
<div>
        Side-Quest Engagement
       </div></th>
<th>
<div>
        count
       </div></th>
<th>
<div>
        mean
       </div></th>
<th>
<div>
        std
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> high<br/></td>
<td> 500<br/></td>
<td> 89.897309<br/></td>
<td> 52.696709<br/></td>
</tr>
<tr>
<td> low<br/></td>
<td> 500<br/></td>
<td> 130.674021<br/></td>
<td> 93.921543<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p106">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.7</span> Summary statistics from the actual experiment</h5>
<table>
<thead>
<tr>
<th>
<div>
        Side-Quest Engagement
       </div></th>
<th>
<div>
        count
       </div></th>
<th>
<div>
        mean
       </div></th>
<th>
<div>
        std
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> high<br/></td>
<td> 500<br/></td>
<td> 92.99054<br/></td>
<td> 51.673631<br/></td>
</tr>
<tr>
<td> low<br/></td>
<td> 500<br/></td>
<td> 131.38228<br/></td>
<td> 94.840705<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p107">
<p>The two sets of summaries are similar enough that we can say that we’ve successfully replicated the experimental results from the observational data. </p>
</div>
<div class="readable-text" id="p108">
<h3 class="readable-text-h3" id="sigil_toc_id_156"><span class="num-string">7.1.7</span> Recap</h3>
</div>
<div class="readable-text" id="p109">
<p>Let’s recap. A causal DAG combined with a Pyro abstraction for an intervention allowed us to do an analysis on an observational dataset that produced the same results as an analysis on an experimental dataset. Had you run this analysis on the initial observational data instead of the simple two-sample statistical test, you would have provided the correct answer to leadership, and they would not have changed the dynamics to increase <em>Side-Quest Engagement</em>.</p>
</div>
<div class="readable-text intended-text" id="p110">
<p>Note that this wasn’t a free lunch. This analysis required causal assumptions in the form of a causal DAG. Errors in specifying the causal DAG can lead to errors in the output of the analysis. But assuming your causal DAG was correct (or close enough), it would have saved you the actual costs and opportunity costs of running that experiment.</p>
</div>
<div class="readable-text intended-text" id="p111">
<p>So how exactly does <code>pyro.do</code> work? How does it modify the model to represent an intervention? We’ll answer these questions with the ideas of <em>ideal interventions</em> and <em>intervention operators</em>. </p>
</div>
<div class="readable-text" id="p112">
<h2 class="readable-text-h2" id="sigil_toc_id_157"><span class="num-string">7.2</span> The ideal intervention and intervention operator</h2>
</div>
<div class="readable-text" id="p113">
<p>To understand how our simulated experiment worked, we need a concrete definition of intervention. We’ll use a specific definition, called the <em>ideal intervention</em>, and also known as the <em>atomic intervention</em>, <em>structural intervention</em>, <em>surgical intervention</em>, and <em>independent intervention</em>.</p>
</div>
<div class="readable-text intended-text" id="p114">
<p>The definition of an ideal intervention breaks down into three parts:</p>
</div>
<ul>
<li class="readable-text" id="p115"> The ideal intervention targets a specific variable or set of variables in the DGP. </li>
<li class="readable-text" id="p116"> The operation sets those variables to a fixed value. </li>
<li class="readable-text" id="p117"> By setting the variable to a fixed value, the intervention blocks the influence from the target’s causes, such that the target is now statistically independent of its causes. </li>
</ul>
<div class="readable-text" id="p118">
<p>We’ll use the notation do(<em>X</em><em> </em>=<em> </em><em>x</em>) to represent an ideal intervention that sets <em>X</em> to <em>x</em>. Note that we can have interventions on sets of variables, as in do(<em>X</em><em> </em>=<em> </em><em>x</em>, <em>Y</em><em> </em>=<em> </em><em>y</em>, <em>Z</em><em> </em>=<em> </em><em>z</em>).</p>
</div>
<div class="readable-text" id="p119">
<h3 class="readable-text-h3" id="sigil_toc_id_158"><span class="num-string">7.2.1</span> Intervention operators</h3>
</div>
<div class="readable-text" id="p120">
<p>A causal model represents relationships in the DGP. The preceding definition of the ideal intervention describes how it <em>changes</em> the DGP. Now it remains to us to define how our causal models will reflect that change.</p>
</div>
<div class="readable-text intended-text" id="p121">
<p>An <em>intervention operator</em> is some way of <em>changing</em> our causal model to reflect an intervention. One of the first tasks of creating <em>any </em>novel<em> </em>computational representation of causality is to define an intervention operator for the ideal intervention.</p>
</div>
<div class="readable-text intended-text" id="p122">
<p>Intervention operators can implement ideal interventions, stochastic interventions (discussed in section 7.5), and other types of interventions. Unless I indicate otherwise, you can assume that “intervention operator” means “intervention operator for ideal interventions.”</p>
</div>
<div class="readable-text intended-text" id="p123">
<p>Fortunately, structural causal models and general causal graphical models have well-defined intervention operators. We’ll explore those, as well as look at intervention operators designed for causal programs like <code>pyro.do</code>.</p>
</div>
<div class="readable-text" id="p124">
<h3 class="readable-text-h3" id="sigil_toc_id_159"><span class="num-string">7.2.2</span> Ideal interventions in structural causal models</h3>
</div>
<div class="readable-text" id="p125">
<p>We’ll start with the structural causal model. Let <em>M</em> represent a structural causal model of the online game. We’d write <em>M</em> as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p126">
<img alt="figure" height="61" src="../Images/ness-ch7-eqs-0x.png" width="497"/>
</div>
<div class="readable-text" id="p127">
<p><em>f</em><sub><em>G</em></sub>, <em>f</em><sub><em>E</em></sub>, and <em>f</em><sub><em>I</em></sub> are the assignment functions for <em>G</em> (<em>Guild Membership</em>), <em>E</em> (<em>Side-Quest Engagement</em><em> </em>), and <em>I</em> (<em>In-Game Purchases</em><em> </em>), respectively.</p>
</div>
<div class="readable-text intended-text" id="p128">
<p>An ideal intervention do(<em>E</em>=“high”) transforms the model as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p129">
<img alt="figure" height="69" src="../Images/ness-ch7-eqs-1x.png" width="578"/>
</div>
<div class="readable-text" id="p130">
<p>The intervention operator for the SCM replaces the intervention target <em>E</em>’s assignment function with the intervention value “high.”</p>
</div>
<div class="readable-text intended-text" id="p131">
<p>Suppose you have an SCM with a variable (or set of variables) <em>X</em>. You want to apply an intervention do(<em>X</em><em> </em>=<em> </em><em>x</em>). The intervention operator replaces the intervention target’s assignment function with the intervention value.</p>
</div>
<div class="readable-text intended-text" id="p132">
<p>Consider how this meets the three elements of our definition of an ideal intervention:</p>
</div>
<ul>
<li class="readable-text" id="p133"> The intervention do(<em>X</em><em> </em>=<em> </em><em>x</em>) only directly affects the assignment function for <em>X</em>. No other assignment function is affected. </li>
<li class="readable-text" id="p134"> The intervention explicitly sets <em>X</em> to a specific value. </li>
<li class="readable-text" id="p135"> Since the value of <em>X</em> is set to a constant, it no longer depends on its direct causal parents. </li>
</ul>
<div class="readable-text" id="p136">
<h3 class="readable-text-h3" id="sigil_toc_id_160"><span class="num-string">7.2.3</span> Graph surgery: The ideal intervention in causal DAGs and causal graphical models</h3>
</div>
<div class="readable-text" id="p137">
<p>Now we’ll consider how to think graphically about the ideal intervention. First, let’s reexamine the online game’s causal DAG in figure 7.5.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p138">
<img alt="figure" height="205" src="../Images/CH07_F05_Ness.png" width="449"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.5</span> The causal DAG for the online game</h5>
</div>
<div class="readable-text intended-text" id="p139">
<p>According to our graph, <em>Guild Membership </em>is the causal parent of <em>Side-Quest Engagement</em>. That parent-child relationship determines a causal Markov kernel—the conditional probability distribution of <em>Side-Quest Engagement</em>, given <em>Guild Membership</em>. Recall our model of this causal Markov kernel, shown in table 7.8. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p140">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.8</span> Conditional probability table for causal Markov kernel of <em>Side-Quest Engagement</em><em/></h5>
<table>
<thead>
<tr>
<th colspan="2" rowspan="3"/>
<th colspan="2">
<div>
        Guild Membership
       </div></th>
</tr>
<tr>
<th>
<div>
        nonmember
       </div></th>
<th>
<div>
        member
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="2"> Side-Quest Engagement<br/></td>
<td> low<br/></td>
<td> .8<br/></td>
<td> .2<br/></td>
</tr>
<tr>
<td> high<br/></td>
<td> .2<br/></td>
<td> .8<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p141">
<p>Imagine the mechanics of our experiment. Players log on, and the digital experimentation platform selects some players for participation in the experiment. Some of those players are guild members, and some are not.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>Consider a player named Jojo, who is not a guild member, who is logging on. Given this information only, he will have a 20% chance of engaging highly in side-quests during this session of gameplay, according to our model.</p>
</div>
<div class="readable-text intended-text" id="p143">
<p>But the experimentation platform selects him for the experiment. It randomly assigns him to the high <em>Side-Quest Engagement</em> group. Once he is in that group, what is the probability that Jojo will engage highly in side-quests? The answer is 100%. In experimental terms, what is the probability that someone assigned to the treatment group will be exposed to the treatment? 100%. For data scientists familiar with the jargon of A/B testing, what is the probability that someone assigned to group A will be exposed to variant A? 100%.</p>
</div>
<div class="readable-text intended-text" id="p144">
<p>Indeed, supposing instead of Jojo, the subject was Ngozi, who is a guild member. While originally Ngozi had an 80% chance of being highly engaged in side-quests, upon being assigned to the high <em>Side-Quest Engagement</em> group in the experiment, she changes to having a 100% chance of being highly engaged.</p>
</div>
<div class="readable-text intended-text" id="p145">
<p>We need to rewrite our conditional probability distribution of <em>Side-Quest Engagement</em> to reflect these new probabilities, as in table 7.9.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p146">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.9</span> Rewriting the conditional probability table of <em>Side-Quest Engagement</em> to reflect the certainty of engagement level upon being assigned to the high-engagement group in the experiment</h5>
<table>
<thead>
<tr>
<th colspan="2" rowspan="3"/>
<th colspan="2">
<div>
        Guild Membership
       </div></th>
</tr>
<tr>
<th>
<div>
        nonmember
       </div></th>
<th>
<div>
        member
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="2"> Side-Quest Engagement<br/></td>
<td> low<br/></td>
<td> 0.0<br/></td>
<td> 0.0<br/></td>
</tr>
<tr>
<td> high<br/></td>
<td> 1.0<br/></td>
<td> 1.0<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p147">
<p>Now we see that this modified distribution of <em>Side-Quest Engagement</em> is the same regardless of <em>Guild Membership</em>. That is the definition of probabilistic independence, so we should simplify this conditional probability table to reflect that; we can reduce table 7.9 to table 7.10.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p148">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.10</span> Rewriting the conditional probability table of <em>Side-Quest Engagement</em> to reflect the fact that engagement level no longer depends on <em>Guild Membership</em></h5>
<table>
<tbody>
<tr>
<td rowspan="2"> Side-Quest Engagement<br/></td>
<td> low<br/></td>
<td> 0.0<br/></td>
</tr>
<tr>
<td> high<br/></td>
<td> 1.0<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p149">
<p>When we simplify the distribution in this way, we have to recall that this is a model of a causal Markov kernel, which is defined by the graph. Our initial graph says <em>Side-Quest Engagement</em> is caused by <em>Guild Membership</em>. But it seems that after the experiment randomly allocates players either to the high engagement or low engagement group, that causal dependency is broken; a player’s engagement level is solely determined by the group they are assigned to. </p>
</div>
<div class="readable-text intended-text" id="p150">
<p>We need an intervention operator that changes our causal graph to reflect this broken causal dependency. This intervention operator is called <em>graph surgery</em> (also known as <em>graph mutlitation</em>), and it’s illustrated in figure 7.6. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p151">
<img alt="figure" height="242" src="../Images/CH07_F06_Ness.png" width="927"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.6</span> Graph surgery removes an incoming edge to the intervention target <em>Side-Quest Engagement</em>.</h5>
</div>
<div class="readable-text" id="p152">
<p>While <em>Guild Membership </em>is a cause of <em>Side-Quest Engagement</em> in normal settings, the experiment’s intervention on <em>Side-Quest Engagement</em> broke that variable’s causal dependence on <em>Guild Membership</em>. Since that causal dependence is gone, graph surgery changes the graph to one where the edge from <em>Guild Membership </em>to <em>Side-Quest Engagement</em> is snipped.</p>
</div>
<div class="readable-text intended-text" id="p153">
<p>In general, suppose you have a causal graph with node <em>X</em>. You want to apply an intervention do(<em>X</em><em> </em>=<em> </em><em>x</em>). Then you represent that intervention on the causal DAG by “surgery” removing all incoming edges to <em>X</em>. Graph surgery is available in libraries such as<em> </em>pgmpy. For example, here is how we would use pgmpy to apply graph surgery to the online gaming DAG.</p>
</div>
<div class="browsable-container listing-container" id="p154">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.11</span> Graph surgery on a DAG in pgmpy</h5>
<div class="code-area-container">
<pre class="code-area">from pgmpy.base import DAG

G = DAG([    <span class="aframe-location"/> #1
    ('Guild Membership', 'Side-quest Engagement'),     #1
    ('Side-quest Engagement', 'In-game Purchases'),     #1
    ('Guild Membership', 'In-game Purchases')    #1
])     #1
G_int = G.do('Side-quest Engagement')   <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Build the causal DAG.
     <br/>#2 The do method in the DAG class applies graph surgery.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p155">
<p>We can now plot both the original DAG and the transformed DAG and compare them.</p>
</div>
<div class="browsable-container listing-container" id="p156">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.12</span> Plot the transformed DAG</h5>
<div class="code-area-container">
<pre class="code-area">import pylab as plt
import networkx as nx

pos = {    <span class="aframe-location"/> #1
    'Guild Membership': [0.0, 1.0],     #1
    'Side-quest Engagement': [-1.0, 0.0],     #1
    'In-game Purchases': [1.0, -0.5]     #1
}     #1

ax = plt.subplot()    <span class="aframe-location"/> #2
ax.margins(0.3)    #2
nx.draw(G, ax=ax, pos=pos, node_size=3000,    #2
        node_color='w', with_labels=True)   #2
plt.show()    #2

ax = plt.subplot()    <span class="aframe-location"/> #3
ax.margins(0.3)     #3
nx.draw(G_int, ax=ax, pos=pos,    #3
        node_size=3000, node_color='w', with_labels=True)     #3
plt.show()     #3</pre>
<div class="code-annotations-overlay-container">
     #1 Create a dictionary of node positions that we can use to visualize both graphs.
     <br/>#2 Visualize the original graph.
     <br/>#3 Visualize the transformed graph.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p157">
<p>These visualizations produce the same DAG as in figure 7.6. </p>
</div>
<div class="readable-text intended-text" id="p158">
<p>Next, we’ll look at the effect of graph surgery on d-separation and its implications for conditional independence.</p>
</div>
<div class="readable-text" id="p159">
<h3 class="readable-text-h3" id="sigil_toc_id_161"><span class="num-string">7.2.4</span> Graph surgery and d-separation</h3>
</div>
<div class="readable-text" id="p160">
<p>Consider how graph surgery affects reasoning with d-separation, as in figure 7.7. Initially, we have two d-connecting paths between <em>Side-Quest Engagement</em> and <em>In-Game Purchases</em>: one path was the direct cause path, and the other was through the common cause of <em>Guild Membership</em>. After graph surgery, only the direct causal path remains.</p>
</div>
<div class="browsable-container figure-container" id="p161">
<img alt="figure" height="273" src="../Images/CH07_F07_Ness.png" width="909"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.7</span> In the original DAG on the left, there are two d-connected paths between <em>Side-Quest Engagement</em> and <em>In-Game Purchases</em>. These paths equate to two sources of statistical dependence between the two variables. After graph surgery, only the causal path remains, reflecting causal dependence.</h5>
</div>
<div class="readable-text intended-text" id="p162">
<p>Recall that each d-connected path between two variables is a source of statistical dependence between those variables. When we represent an intervention with graph surgery that removes incoming edges to the intervention target(s), we remove any paths to other nodes that go through that variable’s causes. Only outgoing paths to other nodes remain. As a result, the remaining paths from that variable reflect dependence due to that variable’s causal influence on other variables. The ideal intervention removes the causal influence the target variable receives from its direct parents. Thus, it removes any dependence on other variables that flows through those parents.<span class="aframe-location"/></p>
</div>
<div class="readable-text" id="p163">
<h3 class="readable-text-h3" id="sigil_toc_id_162"><span class="num-string">7.2.5</span> Ideal interventions and causal Markov kernels</h3>
</div>
<div class="readable-text" id="p164">
<p>Graph surgery on the causal DAG removes the incoming edges to the target node(s). However, for a causal graphical model, we need an intervention operator that changes the graph <em>and</em> goes one step farther to rewrite the causal Markov kernel of the intervention target, as we did when we collapsed table 7.9 into table 7.10.</p>
</div>
<div class="readable-text intended-text" id="p165">
<p>Initially, our online gaming model has causal Markov kernels {<em>P</em><em> </em>(<em>G</em><em> </em>), <em>P</em><em> </em>(<em>E</em><em> </em>|<em>G</em><em> </em>), and <em>P</em><em> </em>(<em>I</em><em> </em>|<em>E</em>, <em>G</em><em> </em>)}. In table 7.9 we saw the conditional probability table representation of <em>P</em><em> </em>(<em>E</em><em> </em>|<em>G</em><em> </em>) and how an intervention reduced it to table 7.10, where 100% of the probability is placed on the outcome “high.”</p>
</div>
<div class="readable-text intended-text" id="p166">
<p>Generally, the intervention operator for causal graphical models replaces the causal Markov kernel(s) of the intervention target(s) with a degenerate distribution, meaning a distribution that puts 100% of the probability on one value, namely the intervention value.</p>
</div>
<div class="readable-text intended-text" id="p167">
<p>When we combine graph surgery with this replacement of the target node’s causal Markov kernel with a degenerate distribution, we have an intervention operator on a causal graphical model that meets the three elements of the definition of ideal intervention:</p>
</div>
<ul>
<li class="readable-text" id="p168"> You only remove incoming edges for the nodes targeted by the intervention. </li>
<li class="readable-text" id="p169"> 100% of the probability is assigned to a fixed value. </li>
<li class="readable-text" id="p170"> Removing the incoming edges to the intervention target means that the variable is no longer causally dependent on its parents. </li>
</ul>
<div class="readable-text" id="p171">
<p>In listing 7.11, graph surgery is implemented in the <code>do</code> method in the <code>DAG</code> class. The <code>BayesianNetwork</code> class, our default for building causal graphical models, also has a <code>do</code> method. Like the <code>DAG</code> method, it takes an intervention target. At the time of writing, the method does not take an intervention value and thus does not satisfy the second element of the definition of ideal intervention.</p>
</div>
<div class="readable-text intended-text" id="p172">
<p>pgmpy uses objects from subclasses of the <code>BaseFactor</code> class (e.g., the <code>TabularCPD</code> class) to represent causal Markov kernels. The <code>do</code> method in the <code>BayesianNetwork</code> class first does graph surgery and then replaces the factor object representing the intervention target’s causal Markov kernel. However, that replacement factor object is not degenerate; it does not assign all the probability value to one outcome. Rather, it returns an object representing the probability distribution of the target variable after marginalizing over its parents in the original unmodified graph. Technically, this is an intervention operator for a stochastic intervention, which I’ll discuss in section 7.5. To build an intervention operator for the ideal intervention, you need to write additional code to modify the factor to assign all probability to the intervention value.</p>
</div>
<div class="readable-text" id="p173">
<h3 class="readable-text-h3" id="sigil_toc_id_163"><span class="num-string">7.2.6</span> Ideal interventions in a causal program</h3>
</div>
<div class="readable-text" id="p174">
<p>Recall that in listing 7.10 we simulated an experiment where players were assigned to high-engagement and low-engagement groups using the <code>pyro.do</code> operator. Specifically, we called <code>pyro.do</code> as in following listing.</p>
</div>
<div class="browsable-container listing-container" id="p175">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.13</span> Revisiting <code>pyro.do</code></h5>
<div class="code-area-container">
<pre class="code-area">int_engaged_model = pyro.do(    <span class="aframe-location"/> #1
    model,    #1
    {"Side-quest Engagement": tensor(1.)}    #1
)    #1
int_unengaged_model = pyro.do(    <span class="aframe-location"/> #2
    model,     #2
    {"Side-quest Engagement": tensor(0.)}     #2
)     #2</pre>
<div class="code-annotations-overlay-container">
     #1 An intervention that sets Side-Quest Engagement to 1.0 (i.e., “high”). This returns a new model.
     <br/>#2 An intervention that sets Side-Quest Engagement to 0.0 (i.e., “low”). This returns a new model.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p176">
<p>What exactly does <code>pyro.do</code> <em>do</em>? <code>pyro.do</code> is Pyro’s intervention operator. We saw, by using <code>pyro.render_model</code> to generate figure 7.4, that our online gaming model in Pyro has an underlying causal DAG, and therefore is a causal graphical model.</p>
</div>
<div class="readable-text intended-text" id="p177">
<p>But a deep probabilistic machine learning framework like Pyro allows you to do things that we can’t easily represent with a causal DAG, such as recursion, conditional control flow, or having a random number of variables not realized until runtime. As an intervention operator, <code>pyro.do</code> must work in these cases as well.</p>
</div>
<div class="readable-text intended-text" id="p178">
<p>The intervention operator in Pyro works by finding calls to <code>pyro.sample</code>, and replacing those calls with an assignment to the intervention value. For example, the online game model had the following line:</p>
</div>
<div class="browsable-container listing-container" id="p179">
<div class="code-area-container">
<pre class="code-area">is_highly_engaged = pyro.sample("Side-quest Engagement", Bernoulli(p_engaged))</pre>
</div>
</div>
<div class="readable-text" id="p180">
<p>This <code>pyro.sample</code> call generates a value for <em>Side-Quest Engagement</em>. <code>pyro.do(model, {"Side-quest Engagement": tensor(1.)})</code> <em>essentially</em> replaces that line with this:</p>
</div>
<div class="browsable-container listing-container" id="p181">
<div class="code-area-container">
<pre class="code-area">is_highly_engaged = tensor(1.)</pre>
</div>
</div>
<div class="readable-text" id="p182">
<p>(I say “essentially” because <code>pyro.do</code> does a few other things too, which I’ll discuss in chapter 10). </p>
</div>
<div class="readable-text intended-text" id="p183">
<p>This replacement is much like the replacement of the assignment function in the SCM, or a causal Markov kernel with a degenerate kernel in a causal graphical model. As an intervention operator, it meets the criteria for an ideal intervention. It targets a specific variable, and it assigns it a specific value. It eliminates its dependence on its causes by removing <em>flow dependence</em> (dependence on results of executing preceding statements in the program).</p>
</div>
<div class="readable-text intended-text" id="p184">
<p>Using a flexible deep probabilistic machine learning tool like Pyro to build a causal model allows you to construct causal representations beyond DAGs and simple ideal interventions. Doing so puts you in underdeveloped territory in terms of theoretical grounding, but it could lead to interesting new applications. </p>
</div>
<div class="readable-text intended-text" id="p185">
<p>In the next section, we’ll consider how interventions affect probability distributions.</p>
</div>
<div class="readable-text" id="p186">
<h2 class="readable-text-h2" id="sigil_toc_id_164"><span class="num-string">7.3</span> Intervention variables and distributions</h2>
</div>
<div class="readable-text" id="p187">
<p>An ideal intervention fixes the random variable it targets, essentially turning it into a constant. But the intervention indirectly affects all the random variables causally downstream of the target variable. As a result, their probability distributions (joint, conditional, or marginal) change from what they were. </p>
</div>
<div class="readable-text" id="p188">
<h3 class="readable-text-h3" id="sigil_toc_id_165"><span class="num-string">7.3.1</span> “Do” and counterfactual notation</h3>
</div>
<div class="readable-text" id="p189">
<p>Causal modeling uses special notation to help reason about how interventions affect random variables and their distributions. One common approach is to use the <em>“do”-notation</em>. Using our online game as an example, <em>P</em>(<em>I</em>) is the probability distribution of <em>In-Game Purchases</em> across all players, <em>P</em>(<em>I</em>|<em>E</em>=“high”) is the probability distribution of <em>In-Game Purchases</em> given players with “high” engagement, and <em>P</em>(<em>I</em>|do(<em>E</em>= “high”)) is the probability distribution of <em>In-Game Purchases</em> given an intervention that sets a player’s engagement level to “high.” The second column of table 7.11 illustrates extensions of this notation to joint distributions, multiple interventions, and mixing interventions with observations.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p190">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 7.11</span> Examples of do-notation and counterfactual notation</h5>
<table>
<thead>
<tr>
<th>
<div>
        Literal
       </div></th>
<th>
<div>
        Do-notation
       </div></th>
<th>
<div>
        Counterfactual notation
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> The probability distribution of <em>In-Game Purchases</em> across all players<br/></td>
<td> <em>P</em>(<em>I</em><em> </em>)<br/></td>
<td> <em>P</em>(<em>I</em><em> </em>)<br/></td>
</tr>
<tr>
<td> The probability distribution of <em>In-Game Purchases</em> for players with “high” engagement<br/></td>
<td> <em>P</em>(<em>I</em><em> </em>|<em>E</em>=“high”)<br/></td>
<td> <em>P</em>(<em>I</em><em> </em>|<em>E</em>=“high”)<br/></td>
</tr>
<tr>
<td> The probability distribution of <em>In-Game Purchases</em> when a player’s engagement level is set (by intervention) to “high”<br/></td>
<td> <em>P</em>(<em>I</em><em> </em>|do(<em>E</em>= “high”))<br/></td>
<td> <em>P</em>(<em>I</em><em> </em><em><sub>E</sub></em><sub>= “high”</sub>)<br/></td>
</tr>
<tr>
<td> The joint probability distribution of <em>In-Game Purchases</em> and <em>Guild Membership</em> when engagement is set to “high”<br/></td>
<td> <em>P</em>(<em>I</em>, <em>G</em>|do(<em>E</em>=“high”))<br/></td>
<td> <em>P</em>(<em>I</em><em> </em><sub>E=“high”</sub>, <em>G</em><sub>E=“high”</sub>)<br/></td>
</tr>
<tr>
<td> The probability distribution of In-Game Purchases when engagement is set to “high” and membership is set to “nonmember”<br/></td>
<td> <em>P</em>(<em>I</em><em> </em>|do(<em>E</em><em> </em>=“high”, <em>G</em>=“nonmember”))<br/></td>
<td> <em>P</em>(<em>I</em><em> </em><em><sub>E</sub></em><sub>= “high”,</sub> <em><sub>G</sub></em><sub>=“nonmember”</sub>)<br/></td>
</tr>
<tr>
<td> The probability distribution of <em>In-Game Purchases</em> for guild members when engagement is set to “high”<br/></td>
<td> <em>P</em>(<em>I</em><em> </em>|do(<em>E</em><em> </em>=“high”), <em>G</em><em> </em>=<em> </em>“member”)<br/></td>
<td> <em>P</em>(<em>I</em><em><sub>E</sub></em><em> </em><sub>=</sub><sub>“high”</sub>|<em>G</em><em> </em>=<em> </em>“member”)<br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p191">
<p>An alternative is to use counterfactual notation, which uses subscripts to represent a new version of a variable after the system has been exposed to intervention. For example, if <em>I</em> is a variable that represents <em>In-Game Purchases</em>, <em>I</em><sub><em>E</em></sub><sub>=“high”</sub> represents <em>In-Game Purchases</em> under an intervention that sets <em>Side-Quest Engagement</em> to “high.” If <em>P</em>(<em>I</em><em>  </em>) is the probability distribution of <em>In-Game Purchases</em>, then <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>) is the probability distribution. Again, table 7.11 contrasts do-notation with counterfactual notation in the third column. Going forward, I’ll mostly use counterfactual notation.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p192">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">From causal language to symbols</h5>
</div>
<div class="readable-text" id="p193">
<p>In many cases in statistics and machine learning, notation only serves to add formalism and rigor to something just as easily explained in plain language. However, notation is important in causality, because it makes a clear distinction between when we are talking about something causal and when we are not. It is important because making the distinction is harder in plain English. For example, consider the following two questions:</p>
</div>
<div class="readable-text" id="p194">
<p>“What would <em>In-Game Purchases</em> be for a player who was highly engaged in side-quests?”</p>
</div>
<div class="readable-text" id="p195">
<p>“What would <em>In-Game Purchases</em> be if a player was highly engaged in side-quests?”</p>
</div>
<div class="readable-text" id="p196">
<p>Is it obvious to you that the first question corresponds to <em>P</em>(<em>I</em>|<em>E</em>=“high”) and the second to <em>P</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub>)? The first question corresponds to a subset of players who are highly engaged. The traditional conditional probability notation is fine when we want to zoom in on a subset of a distribution or population. The second question asks <em>what if</em> someone were highly engaged. In the next chapter, we’ll see that “what if” hypothetical questions imply an intervention. But because of the ambiguity of language, someone could ask one question while really meaning the other. The notation gives us an unambiguous way of constructing our causal queries. </p>
</div>
<div class="readable-text" id="p197">
<p>Again, in chapter 8, we’ll investigate more examples of mapping language to counterfactual notation.</p>
</div>
</div>
<div class="readable-text" id="p198">
<h3 class="readable-text-h3" id="sigil_toc_id_166"><span class="num-string">7.3.2</span> When causal notation reduces to traditional notation</h3>
</div>
<div class="readable-text" id="p199">
<p>It is crucial to recognize when a variable and that same variable under intervention are the same. Consider the intervention on engagement, as in figure 7.8.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p200">
<img alt="figure" height="273" src="../Images/CH07_F08_Ness.png" width="909"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.8</span> In the original DAG (left), there are two d-connected paths between <em>Side-Quest Engagement</em> and <em>In-Game Purchases</em>. These paths equate to two sources of statistical dependence between the two variables. After graph surgery, only the causal path remains, reflecting causal dependence.</h5>
</div>
<div class="readable-text" id="p201">
<p>Is <em>P</em><em> </em>(<em>G</em><em> </em>|<em>E</em>=“high”) (the probability distribution of <em>Guild Membership</em> given high <em>Side-Quest Engagement</em>) the same as <em>P</em>(<em>G</em>)? No. In graphical terms, <em>G</em> and <em>E</em> are d-connected. In probabilistic terms, we can reason that knowing a player’s level of <em>Side-Quest Engagement</em> is predictive of whether they are in a guild.</p>
</div>
<div class="readable-text intended-text" id="p202">
<p>But is <em>P</em><em> </em>(<em>G</em><sub><em>E</em></sub><sub>=“high”</sub>) the same as <em>P</em>(<em>G</em><em> </em>)? Yes. <em>Guild Membership </em>is not affected by the intervention on <em>Side-Quest Engagement</em> because it can only affect variables causally downstream of <em>Side-Quest Engagement</em>. Thus <em>P</em><em> </em>(<em>G</em><sub><em>E</em></sub><sub>=“high”</sub>) is equivalent to <em>P</em>(<em>G</em>).</p>
</div>
<div class="readable-text intended-text" id="p203">
<p>In general terms, empirically learning a distribution for a variable <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>x</em></sub> requires doing the intervention do(<em>X</em><em> </em>=<em> </em><em>x</em>) in real life. However, that real-life intervention, at best, has a cost and, at worst, is infeasible or impossible. So if we can equate <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>x</em></sub> to some distribution involving <em>Y</em> that we can learn from observational data, that’s a win. That’s going from correlation to causation. In trivial cases, we can do this by looking at the graph, as we did with <em>G</em> and <em>G</em><sub><em>E</em></sub><sub>=“high”</sub>. But usually, we’ll need to do some mathematical derivation, either by hand or using algorithms. </p>
</div>
<div class="readable-text intended-text" id="p204">
<p>This task of deriving equality between variables that are and aren’t subject to intervention is called <em>identification</em>, and it is the heart of causal inference theory. We’ll examine identification at length in chapter 10.</p>
</div>
<div class="readable-text" id="p205">
<h3 class="readable-text-h3" id="sigil_toc_id_167"><span class="num-string">7.3.3</span> Causal models represent all intervention distributions</h3>
</div>
<div class="readable-text" id="p206">
<p>As generative models, the causal models we’ve worked with encode a joint probability distribution of components of the DGP. Inference algorithms enable those models to represent (e.g., through Monte Carlo sampling) the conditional distribution of some subset of those components, given the state of the other components.</p>
</div>
<div class="readable-text intended-text" id="p207">
<p>We’ve now introduced the ideal intervention and how it changes the DGP and, consequently, the joint probability distribution of the variables. Figure 7.9 illustrates how the generative causal model captures the original DGP (and corresponding probability distributions) and any new DGP (and corresponding probability distributions) created by intervening in the original process.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p208">
<img alt="figure" height="473" src="../Images/CH07_F09_Ness.png" width="719"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.9</span> Suppose our DGP has variables <em>X</em>, <em>Y</em>, and <em>Z</em>. A traditional generative model (left) uses observations of <em>X</em>, <em>Y</em>, and <em>Z</em> to statistically learn a representation of <em>P</em>(<em>X</em>, <em>Y</em>, <em>Z</em>). A generative causal model (right) encodes a representation <em>P</em>(<em>X</em>, <em>Y</em>, <em>Z</em>) and distributions derived by interventions on <em>X</em>, <em>Y</em>, and <em>Z</em>. In that way, the generative causal model represents a broad family of distributions.</h5>
</div>
<div class="readable-text" id="p209">
<p>Consider the statistical implications of this idea. Given data, an ordinary generative model learns a representation of the joint probability distribution. But a generative causal model learns not only that distribution but any new distribution that would be derived by applying some set of ideal interventions. That’s how our causal model of the online game was able to reproduce the outcome of an experiment from observational data alone.</p>
</div>
<div class="readable-text" id="p210">
<h2 class="readable-text-h2" id="sigil_toc_id_168"><span class="num-string">7.4</span> Interventions and causal effects</h2>
</div>
<div class="readable-text" id="p211">
<p>The most common use case for our formal model of an intervention will be to model <em>causal effects</em>. Now that we’ve defined and formalized interventions, causal effects are easy to think about; they are simply comparisons between the outcomes of interventions. </p>
</div>
<div class="readable-text" id="p212">
<h3 class="readable-text-h3" id="sigil_toc_id_169"><span class="num-string">7.4.1</span> Average treatment effects with binary causes</h3>
</div>
<div class="readable-text" id="p213">
<p>The most common causal effect query is the average treatment effect (ATE). Here, we’ll focus on the case where we are interested in the causal effect of <em>X</em> on <em>Y</em>, and <em>X</em> is binary, meaning it has two outcomes: 1 and 0. Binary causes entail experiments where the cause has a “treatment” value and a “control” value, such as “A/B tests.” Using do-notation, the ATE is <em>E</em><em> </em>(<em>Y</em><em>  </em>|do(<em>X</em><em> </em>=<em> </em>1)) – <em>E</em><em> </em>(<em>Y</em><em>  </em>|do(<em>X</em><em> </em>=<em> </em>0)) (recall <em>E</em><em> </em>(…) means “the expectation of …”). Using counterfactual notation, the ATE is <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>1</sub>) – <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>0</sub>). The advantage of the counterfactual notation is that we can collapse this into one expectation term, <em>E</em>(<em>Y</em><sub><em>X</em></sub><sub>=1 </sub>– <em>Y</em><sub><em>X</em></sub><sub>=0</sub>).</p>
</div>
<div class="readable-text" id="p214">
<h3 class="readable-text-h3" id="sigil_toc_id_170"><span class="num-string">7.4.2</span> Average treatment effect with categorical causes</h3>
</div>
<div class="readable-text" id="p215">
<p>When the cause is categorical, the ATE requires choosing which levels of the cause you want to compare. For example, if <em>X</em> has possible outcomes {a, b, c}, you might select “a” to be a baseline, and work with two ATEs, <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>b</em></sub> – <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>a</em></sub>) and <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>c</em></sub> – <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>a</em></sub>). Alternatively, you may choose to work with all pairwise comparisons of levels of <em>X</em>, or just convert <em>X</em> to a binary variable with outcomes “a” and “not a” The choice depends on which ATE is most meaningful to you.</p>
</div>
<div class="readable-text" id="p216">
<h3 class="readable-text-h3" id="sigil_toc_id_171"><span class="num-string">7.4.3</span> Average treatment effect for continuous causes</h3>
</div>
<div class="readable-text" id="p217">
<p>If we want to generalize <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>1</sub> – <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>0</sub>) to the case where <em>X</em> is continuous, we arrive at derivative calculus. For some baseline do(<em>X</em><em> </em>=<em> </em><em>x</em>), imagine changing the intervention value <em>x</em> by some small amount <span class="regular-symbol">Δ</span>, i.e., do(<em>X</em><em> </em>=<em> </em><em>x</em><em> </em>+<em> </em><span class="regular-symbol">Δ</span>). Taking the difference between the two outcomes, we get <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>x</em></sub><em> </em><sub>+</sub><em> </em><sub><span class="regular-symbol">Δ</span></sub> – <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>x</em></sub>). Then we can ask, what is the rate of change of <em>E</em>(<em>Y</em><sub><em>X</em></sub>) as we make <span class="regular-symbol">Δ</span> infinitesimally smaller. This brings us to the definition of the derivative:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p218">
<img alt="figure" height="51" src="../Images/ness-ch7-eqs-2x.png" width="399"/>
</div>
<div class="readable-text" id="p219">
<p>Note that this is a function, rather than a point value; when you plug in a value of <em>x</em>, you get the rate of change of <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>x</em></sub> of the <em>X</em> versus <em>Y</em><sub><em>x</em></sub> curve. </p>
</div>
<div class="readable-text intended-text" id="p220">
<p>As a practical example, consider the case of pharmacology, where we want to establish the ATE of a drug dose on a health outcome. The drug dose is continuous, and it usually follows a nonlinear S-curve-like shape; we get more effect as we increase the dose, but eventually the effect gets diminishing returns at higher doses. The derivative gives us the rate of change of the average response for a given dose on the dose-response curve.</p>
</div>
<div class="readable-text" id="p221">
<h3 class="readable-text-h3" id="sigil_toc_id_172"><span class="num-string">7.4.4</span> Conditional average treatment effect</h3>
</div>
<div class="readable-text" id="p222">
<p>The conditional average treatment effect (CATE) is an ATE conditioned on other covariates. For example, in our online game example, <em>E</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub> – <em>I</em><sub><em>E</em></sub><sub>=“low”</sub>) is the ATE on <em>In-Game Purchases</em> for <em>Side-Quest Engagement</em>. If we wished to understand the ATE for guild members, we’d want <em>E</em>(<em>I</em><sub><em>E</em></sub><sub>=“high”</sub> – <em>I</em><sub><em>E</em></sub><sub>=“low”</sub>| <em>G</em>=“member”).</p>
</div>
<div class="readable-text intended-text" id="p223">
<p>In practical settings, it is often important to work with CATEs instead of ATEs, because CATEs can have big differences with ATEs and other CATEs with different conditions. In other words, CATEs better reflect the <em>heterogeneity</em> of treatment effects across a population. For example, it is possible that the ATE of a drug on a health outcome is positive across the overall population, but the CATE conditioned on a specific subpopulation (e.g., people with a certain allergy) could be negative. Similarly, in advertising, certain ad copy might drive your customers to purchase more on average, but cause some segment of your customers to purchase less. You can optimize the return on investment for your ad campaign by understanding the CATEs for each segment, or to use CATE-based reasoning to do customer segmentation.</p>
</div>
<div class="readable-text intended-text" id="p224">
<p>Experts often emphasize the importance of measuring <em>heterogenous treatment effects</em> with CATEs, lest one think a point value estimate of an ATE tells the full picture. But in our probabilistic modeling approach, heterogeneity is front and center. If we have a causal graphical model and a model of ideal intervention, then we can model <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>x</em></sub>). If we can model <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub><em>x</em></sub>), then we can model <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>1</sub> – <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>0</sub>). We can then use that model to inspect all the variation within <em>P</em><em> </em>(<em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>1</sub> – <em>Y</em><sub><em>X</em></sub><em> </em><sub>=</sub><em> </em><sub>0</sub>), including who in the target population falls above or below 0 or some other threshold. </p>
</div>
<div class="readable-text" id="p225">
<h3 class="readable-text-h3" id="sigil_toc_id_173"><span class="num-string">7.4.5</span> Statistical measures of association and causality</h3>
</div>
<div class="readable-text" id="p226">
<p>In statistics, an effect size is a value that measures the strength or intensity of the relationship between two variables or groups. For example, in our observational analysis of the online gaming data, we quantified the relationship between <em>Side-Quest Engagement</em> <em>E</em> and <em>In-Game Purchases</em> <em>I</em> as <em>E</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em> </em>=“high”) – <em>E</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em> </em>=“low”). Our statistical procedure estimated this <em>true</em> effect size with a difference in sample averages between both groups. We then conducted a hypothesis test. We specified a null hypothesis <em>E</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em> </em>=“high”) – <em>E</em><em> </em>(<em>I</em><em>  </em>|<em>E</em><em> </em>=“low”) = 0, and then tested if this effect size estimate was statistically different from 0 using a <em>p</em>-value calculated under some null hypothesis distribution (usually a normal or t-distribution).</p>
</div>
<div class="readable-text intended-text" id="p227">
<p>A causal effect is just an <em>interventional</em> effect size; in our example, it was <em>E</em><em> </em>(<em>I</em><em>  </em>|do(<em>E</em><em> </em>=“high”)) – <em>E</em>(<em>I</em><em>  </em>|do(<em>E</em><em> </em>=“low”)) = <em>E</em><em> </em>(<em>I</em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub>“high”</sub> – <em>I</em><sub><em>E</em></sub><em> </em><sub>=</sub><em> </em><sub>“low”</sub>), which is the ATE. The statistical hypothesis testing procedure is the same as before. Indeed, we still need to test if sample-based estimates of ATEs and CATEs are statistically significant. When you conduct a statistical significance test with data from an experiment with a treatment and control, you are testing an estimate of the ATE by definition.</p>
</div>
<div class="readable-text" id="p228">
<h3 class="readable-text-h3" id="sigil_toc_id_174"><span class="num-string">7.4.6</span> Causality and regression models</h3>
</div>
<div class="readable-text" id="p229">
<p>Suppose <em>X</em> is continuous, but its relationship with <em>Y</em><sub><em>X</em></sub> is linear. Then the ATE <em>d</em> <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub><sub>=</sub><sub><em>x</em></sub>)/<em>dx</em> is a point value because the derivative of a linear function is a constant. Therefore, if you use a linear model of <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub>), then the coefficient for <em>X</em> in that model corresponds to the ATE for <em>X</em>.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p230">
<img alt="figure" height="23" src="../Images/ness-ch7-eqs-3x.png" width="406"/>
</div>
<div class="readable-text" id="p231">
<p>For this reason, linear regression modeling is a popular approach to modeling causal effects (even when people don’t really believe the causal relationship is linear).</p>
</div>
<div class="readable-text intended-text" id="p232">
<p>This convenience extends to other generalized linear models. Suppose Poisson regression or logistic regression are better models of <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub>) than linear regression. These models capture measures of association between two variables not as a difference in means, but as ratios. For example, we can read relative risk (RR) directly from a Poisson regression model and odds ratios (OR) directly from a logistic regression model. In general, these measures of association have no causal interpretation, but we give them a causal interpretation once we use them with interventional variables. For example, if we are modeling <em>E</em><em> </em>(<em>Y</em><sub><em>X</em></sub>), and <em>Y</em><sub><em>X</em></sub> is binary, the relative risk and odds ratios are as follows:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p233">
<img alt="figure" height="148" src="../Images/ness-ch7-eqs-4x.png" width="315"/>
</div>
<div class="readable-text" id="p234">
<p>Thus, traditional non-causal ways of quantifying statistical association become measures of <em>causal</em> association once we use them in an interventional context. And when we fit these regression models to data, we can still use all the traditional regression methods for significance testing (Wald tests, F-tests, likelihood ratio tests, etc.). </p>
</div>
<div class="readable-text" id="p235">
<h2 class="readable-text-h2" id="sigil_toc_id_175"><span class="num-string">7.5</span> Stochastic interventions</h2>
</div>
<div class="readable-text" id="p236">
<p>Stochastic interventions are an important generalization of ideal interventions. The second rule of the ideal intervention is that the intervention is set to a fixed value. In the stochastic intervention, that value is the outcome of a random process; i.e., it is itself a random variable. Most texts treat stochastic interventions as an advanced topic beyond the scope of an introduction to causal modeling, but I make special mention of them as they are important in machine learning, where we often seek data-driven automation. Stochastic interventions are important for automatic selection of interventions.</p>
</div>
<div class="readable-text" id="p237">
<h3 class="readable-text-h3" id="sigil_toc_id_176"><span class="num-string">7.5.1</span> Random assignment in an experiment is a stochastic intervention</h3>
</div>
<div class="readable-text" id="p238">
<p>For example, the digital experimentation platform in our online gaming experiment automatically assigned players to high- and low-engagement groups. It did so <em>randomly</em>. Random assignment is a stochastic intervention; it targets the <em>Side-Quest Engagement</em> variable and sets its value by digitally flipping a coin.</p>
</div>
<div class="readable-text intended-text" id="p239">
<p>Note that randomization is more than what we need to arrive at the right answer. Indeed, in our simulation of the experiment, there was no randomization, only ideal interventions. Those ideal interventions were sufficient to d-separate the path<strong> </strong><em>Side-Quest Engagement</em> ← <em>Guild Membership </em>→<em> In-Game Purchases</em>, removing the statistical dependence that comes from that path. If randomization is not necessary to quantify the causal relationship, why is it called “the gold standard of causal inference?” The answer is that randomization works when your causal DAG is <em>wrong</em>.</p>
</div>
<div class="readable-text intended-text" id="p240">
<p>For example, suppose that when we did the experiment, rather than randomizing players into the high versus low <em>Side-Quest Engagement</em> group, the digital experimentation platform automatically assigned the first 500 players who logged on to the group with high <em>Side-Quest Engagement</em> and the next 500 players to the group with low <em>Side-Quest Engagement</em>. This intervention would be sufficient to d-separate the path<strong> </strong><em>Side-Quest Engagement</em> ← <em>Guild Membership </em>→ <em>In-Game Purchases</em>. But what if our DAG was wrong, and there are other paths between <em>Side-Quest Engagement</em> and <em>In-Game Purchases</em> through unknown common causes?</p>
</div>
<div class="readable-text intended-text" id="p241">
<p>Figure 7.10 considers what happens when our DAG is wrong—our model is the DAG on the right. Consider what would happen if, instead, the true DAG were the DAG on the left. For the DAG on the left, the time of day when the player logs on drives both the <em>Side-Quest Engagement</em> and <em>In-Game Purchases</em>.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p242">
<img alt="figure" height="284" src="../Images/CH07_F10_Ness.png" width="528"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.10</span> Left: the true causal relationships. Right: your (incorrect) causal DAG.</h5>
</div>
<div class="readable-text" id="p243">
<p>Suppose, for example, people who log on earlier tend not to be logging on with friends. They tend to engage more in side-quests because side-quests are amenable to solo gameplay. People who plan missions with friends tend to log on later, since some friends have real-world appointments during the day. Friends playing together focus more on the game’s primary narrative and avoid side-quests. Also, players tend to spend more money on <em>In-Game Purchases</em> later in the day, corresponding to the broader trend of late-day spending in e-commerce.</p>
</div>
<div class="readable-text intended-text" id="p244">
<p>When we intervene on a player to assign them to one group or another based on their login, that intervention value now depends on the time of day, as shown in figure 7.11.</p>
</div>
<div class="readable-text intended-text" id="p245">
<p>The left side of figure 7.11 illustrates the result of an intervention on <em>Side-Quest Engagement</em> that depends on the time of day. As we expected, the intervention performs graph surgery, removing the incoming edges to <em>Side-Quest Engagement</em> <em>E</em>: <em>T</em>→<em>E</em> and <em>G</em><em> </em>→<em>E</em>. However, the value set by the intervention is now determined by time of day <em>T</em>, via a <code>time_select</code> function. The <code>time_select</code> function assigns “high” engagement to every player whose login time is before that of the 501<sup>st</sup> player to log on and “low” for those who logged in after. After graph surgery, we add back a new causal edge <em>T</em>→<em>E</em> whose mechanism is <code>time_select</code>. Thus, there is still a noncausal statistical association that biases the experiment via the d-connected path <em>I</em><em> </em>←<em>T</em>→<em>E</em>. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p246">
<img alt="figure" height="416" src="../Images/CH07_F11_Ness.png" width="1012"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.11</span> An intervention that sets the level of <em>Side-Quest Engagement</em> based on login time changes, but doesn’t eliminate, the causal relationship T→<em>E</em>. In contrast, randomization eliminates incoming edges to <em>E</em>.</h5>
</div>
<div class="readable-text intended-text" id="p247">
<p>In contrast, randomization on the right side of figure 7.11 did what we hoped, removing all the incoming edges to <em>E</em>. It removed the edge from <em>T</em>→<em>E</em> even though our assumed DAG did not know that <em>T</em>→<em>E</em> existed. Indeed, if there are other unknown common causes between <em>E</em> and <em>I</em>, randomization will remove those incoming edges to <em>E</em>, as in figure 7.12.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p248">
<img alt="figure" height="328" src="../Images/CH07_F12_Ness.png" width="617"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.12</span> Randomization eliminates incoming edges from unknown common causes.</h5>
</div>
<div class="readable-text intended-text" id="p249">
<p>The ability of randomization to eliminate statistical bias from common causes we failed to account for in our assumptions is why it is considered “the gold standard of causal inference.” But to understand stochastic interventions, note that both assignment mechanisms: one based on login time and the other using randomization, are stochastic interventions. Both set the <em>Side-Quest Engagement</em> level of a player using a random process; one depends on when someone logs in, and the other depends on a coin flip.</p>
</div>
<div class="readable-text" id="p250">
<h3 class="readable-text-h3" id="sigil_toc_id_177"><span class="num-string">7.5.2</span> Intervention policies</h3>
</div>
<div class="readable-text" id="p251">
<p>Stochastic interventions are closely related to policies in automated decision-making domains, such as bandit algorithms and reinforcement learning. In these domains, an agent (e.g., a robot, a recommender algorithm) operates in some environment. A <em>policy</em> is an algorithm that takes as input the state of some variables in the environment and returns an action for the agent to execute. If there are elements of randomness in the selection of that action, it is a stochastic intervention.</p>
</div>
<div class="readable-text intended-text" id="p252">
<p>In our previous example, randomization is a <em>policy</em> that selects interventions at random. But in automated decision-making, most policies choose interventions based on the state of other variables in the system, much like the biased experiment that intervenes based on the <em>time of day</em> variable. Of course, policies in automated decision-making are typically trying to optimize some utility function rather than bias an experiment. We’ll focus on causality in automated decision-making in chapter 12.</p>
</div>
<div class="readable-text" id="p253">
<h2 class="readable-text-h2" id="sigil_toc_id_178"><span class="num-string">7.6</span> Practical considerations in modeling interventions</h2>
</div>
<div class="readable-text" id="p254">
<p>I’ll close this chapter with some practical considerations for modeling interventions. We’ll consider how ideal (and stochastic) interventions allow us to model the impossible. Then we’ll make sure we ground that modeling in pragmatism. </p>
</div>
<div class="readable-text" id="p255">
<h3 class="readable-text-h3" id="sigil_toc_id_179"><span class="num-string">7.6.1</span> Reasoning about interventions that we can’t do in reality</h3>
</div>
<div class="readable-text" id="p256">
<p>In our online gaming example, we used an intervention operator on a causal model to replicate the results of an experiment. I presented a choice between actually running an experiment and simulating the experiment. Simulation avoids the costs of running the experiment, but running the experiment is more robust to errors in causal assumptions, especially with tools like randomization.</p>
</div>
<div class="readable-text intended-text" id="p257">
<p>However, there are many times when we can’t run an experiment, because doing so is either infeasible, unethical, or impossible.</p>
</div>
<ul>
<li class="readable-text" id="p258"> <em>Example of an infeasible experiment</em><em> </em>—A randomized experiment that tests the effect of interest rates on intergenerational wealth. </li>
<li class="readable-text" id="p259"> <em>Example of an unethical experiment</em><em> </em>—A randomized experiment that tests the effect of caffeine on miscarriages. </li>
<li class="readable-text" id="p260"> <em>Example of an impossible experiment</em><em> </em>—A randomized experiment that tests the effects of black hole size on spectroscopic redshift. </li>
</ul>
<div class="readable-text" id="p261">
<p>In these scenarios, simulation with a causal model is our only choice.</p>
</div>
<div class="readable-text" id="p262">
<h3 class="readable-text-h3" id="sigil_toc_id_180"><span class="num-string">7.6.2</span> Refutation and real-world interventions</h3>
</div>
<div class="readable-text" id="p263">
<p>Suppose your causal model predicts the outcome of an intervention. You then do that intervention in the real world, such as with a controlled experiment. If your predicted intervention outcome conflicts with your actual intervention outcome, your causal model is wrong.</p>
</div>
<div class="readable-text intended-text" id="p264">
<p>In chapter 4, we discussed the concept of validating, or rather, <em>refuting</em>, a causal model by checking data for evidence of dependence that violates the conditional independence implications of the model’s DAG. In chapter 11, we’ll extend refutation from the causal DAG all the way to a causal inference of interest (e.g., estimating a causal effect). However, comparing predicted and actual intervention outcomes gives us a stronger refutation standard than the methods in chapters 4 and 11. The catch, of course, is that doing these real-world interventions must be feasible.</p>
</div>
<div class="readable-text intended-text" id="p265">
<p>Assuming they are, comparing predicted and real-world intervention outcomes provides a nice iterative framework for building a causal model. First, enumerate a set of interventions you can apply in the real world. Select one of those interventions, use your model to predict its outcome, and then do the intervention in the real world. If the outcomes don’t match, update your model so that it does. Repeat until you have exhausted your ability to run real-world interventions.</p>
</div>
<div class="readable-text intended-text" id="p266">
<p>Doing a real-world intervention usually costs resources and time. To save on costs, use your causal model to predict all the interventions you can run, rank the predicted outcomes according to which are more interesting or surprising, and then prioritize running real-world interventions according to this ranking. Interesting or surprising intervention predictions are likely a sign your model is wrong, so prioritizing them means you’ll make big updates to your model sooner and at less cost. And if your model turns out to be right, you will have spent less to arrive at some important insights into your DGP.</p>
</div>
<div class="readable-text" id="p267">
<h3 class="readable-text-h3" id="sigil_toc_id_181"><span class="num-string">7.6.3</span> “No causation without manipulation”</h3>
</div>
<div class="readable-text" id="p268">
<p>The idea behind “no causation without manipulation” is that one should define the variables in the causal model such that the mechanics of <em>how</em> one might intervene in it is clear. Clarity here means you could run a real-world experiment that implemented the intervention, or, if the experiment were infeasible, unethical, or impossible, you could at least clearly articulate how the hypothetical experiment would work. “No causation without manipulation” is essentially trying to tether a causal model’s abstractions to experimental semantics. </p>
</div>
<div class="readable-text intended-text" id="p269">
<p>For example, proponents of this idea might object to having “race” as a cause in a causal model, because the concept of race is nebulous from the standpoint of an intervention applied in an experiment—how would you change somebody’s race while holding constant everything about that person not caused by their race? They would prefer defining the variable in terms precise enough to be theoretically intervenable, such as “racial bias of loan officer” or “racial indicators on application form.” Of course, we have important questions to ask about fuzzy abstractions like “race,” so we don’t want to add so much precision that we can’t generalize the results of our analyses in ways that help answer those questions.</p>
</div>
<div class="readable-text intended-text" id="p270">
<p>One strategy for establishing this tether to experimentation is to include variables in our model that we can manipulate in a hypothetical experiment. For example, if we are interested in the causal relationship wealth → anxiety, we could add a “cash subsidy” variable and cash subsidy → wealth edge. Cash subsidy represents direct payments to an individual, which is easier to do in an experiment than directly manipulating an individual’s wealth. </p>
</div>
<div class="readable-text" id="p271">
<h3 class="readable-text-h3" id="sigil_toc_id_182"><span class="num-string">7.6.4</span> Modeling “non-ideal” interventions</h3>
</div>
<div class="readable-text" id="p272">
<p>Often the types of interventions we use in practical settings can be challenging to map to ideal interventions. For example, a biologist might be studying the causal relationships between the expression of different genes in a cell, with causal relationships like gene A → gene B → gene C. The biologist might want to know how a stressor in the cellular environment (e.g., a toxin or hypoxia) affects gene expression. The stressor is an intervention; it changes the DGP. However, modeling it as an ideal intervention is challenging because it will likely be unclear which genes those stressors affect directly or what specific amount of gene expression is set by the stressor. A practical solution for these interventions is to model them explicitly as root nodes in the causal DAG, such as the hypoxia node in figure 7.13.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p273">
<img alt="figure" height="175" src="../Images/CH07_F13_Ness.png" width="433"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.13</span> “Hypoxia” is an intervention that has no specific target. Include it as a root node with edges to all variables that are possibly affected.</h5>
</div>
<div class="readable-text" id="p274">
<p>Explicit representation of interventions as part of the DGP is less expressive than the ideal (or stochastic) intervention, which captures how an arbitrary intervention can <em>change</em> the DGP. </p>
</div>
<div class="readable-text" id="p275">
<h2 class="readable-text-h2" id="sigil_toc_id_183">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p276"> An intervention is an action that changes the data generating process (DGP). Interventions are fundamental to defining causality and causal models. </li>
<li class="readable-text" id="p277"> Many, if not most, machine learning–driven decisions are interventions that can render the model’s deployment environment different from its training environment. </li>
<li class="readable-text" id="p278"> The ability to model an intervention allows one to simulate the outcome of experiments. </li>
<li class="readable-text" id="p279"> Simulating experiments with an intervention model can save costs or enable simulated experiments when running an actual experiment is infeasible, unethical, or impossible. </li>
<li class="readable-text" id="p280"> An ideal intervention targets specific variables, fixes them to a specific value, and renders the target independent of its causal parents. </li>
<li class="readable-text" id="p281"> Causal effects are simple extensions of intervention distributions. For example, the average treatment effect (ATE) of <em>X</em> on <em>Y</em> is <em>E</em>(<em>Y</em><sub><em>X</em></sub><sub>=1</sub>) – <em>E</em>(<em>Y</em><sub><em>X</em></sub><sub>=0</sub>), the difference in means between two intervention distributions for <em>Y</em>. Conditional average treatment effects (CATEs) are simply differences in conditional expectations for intervention distributions on <em>Y</em>. </li>
<li class="readable-text" id="p282"> Stochastic interventions are like ideal interventions, but they fix the intervention targets at a value determined by some random process. That value could depend on the states of other variables in the system. In this way, they are related to policies in automated decision-making domains such as bandit algorithms and reinforcement learning. </li>
<li class="readable-text" id="p283"> An intervention operator describes how a causal model is altered to reflect an ideal (or stochastic) intervention. </li>
<li class="readable-text" id="p284"> The intervention operator for a structural causal model replaces the target variables assignment function with the intervention value. </li>
<li class="readable-text" id="p285"> Graph surgery is the intervention operator for causal DAGs. </li>
<li class="readable-text" id="p286"> The intervention operator for causal graphical models applies graph surgery and replaces the causal Markov kernel for the target with a degenerate distribution that places all probability on the intervention value. </li>
<li class="readable-text" id="p287"> Causal models can use observational data to statistically learn the observational distribution and any interventional distribution that can be derived through the intervention operator. </li>
<li class="readable-text" id="p288"> Randomization is a stochastic intervention that eliminates causal influence on the intervention target from unknown causes. </li>
<li class="readable-text" id="p289"> “No causation without manipulation” suggests defining your causal model so that interventions are tethered to hypothetical experiments. </li>
<li class="readable-text" id="p290"> You can model interventions that don’t meet the ideal intervention standard as root nodes with outgoing edges to variables they may affect. </li>
</ul>
<div class="readable-text footnote-readable-text" id="p291">
<p><a href="#footnote-source-1"><span class="footnote-definition" id="footnote-211">[1]</span></a> D.T. Campbell and T.D. Cook, <em>Quasi-experimentation: Design &amp; Analysis Issues for Field Settings</em> (Rand McNally, 1979), p36.</p>
</div>
</div></body></html>