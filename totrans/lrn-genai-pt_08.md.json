["```py\nimport torchvision\nimport torchvision.transforms as T\n\ntransform=T.Compose([\n    T.ToTensor()])\ntrain_set=torchvision.datasets.MNIST(root=\".\",        ①\n    train=True,download=True,transform=transform)     ②\ntest_set=torchvision.datasets.MNIST(root=\".\",\n    train=False,download=True,transform=transform)    ③\n```", "```py\nimport torch\n\nbatch_size=32\ntrain_loader=torch.utils.data.DataLoader(\n    train_set,batch_size=batch_size,shuffle=True)\ntest_loader=torch.utils.data.DataLoader(\n    test_set,batch_size=batch_size,shuffle=True)\n```", "```py\nimport torch.nn.functional as F\nfrom torch import nn\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\ninput_dim = 784                                       ①\nz_dim = 20                                            ②\nh_dim = 200\nclass AE(nn.Module):\n    def __init__(self,input_dim,z_dim,h_dim):\n        super().__init__()\n        self.common = nn.Linear(input_dim, h_dim)\n        self.encoded = nn.Linear(h_dim, z_dim)\n        self.l1 = nn.Linear(z_dim, h_dim)\n        self.decode = nn.Linear(h_dim, input_dim)\n    def encoder(self, x):                             ③\n        common = F.relu(self.common(x))\n        mu = self.encoded(common)\n        return mu\n    def decoder(self, z):                             ④\n        out=F.relu(self.l1(z))\n        out=torch.sigmoid(self.decode(out))\n        return out\n    def forward(self, x):                             ⑤\n        mu=self.encoder(x)\n        out=self.decoder(mu)\n        return out, mu\n```", "```py\nmodel = AE(input_dim,z_dim,h_dim).to(device)\nlr=0.00025\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n```", "```py\nimport matplotlib.pyplot as plt\n\noriginals = []                                           ①\nidx = 0\nfor img,label in test_set:\n    if label == idx:\n        originals.append(img)\n        idx += 1\n    if idx == 10:\n        break\ndef plot_digits():\n    reconstructed=[]\n    for idx in range(10):\n        with torch.no_grad():\n            img = originals[idx].reshape((1,input_dim))\n            out,mu = model(img.to(device))               ②\n        reconstructed.append(out)                        ③\n    imgs=originals+reconstructed\n    plt.figure(figsize=(10,2),dpi=50)\n    for i in range(20):\n        ax = plt.subplot(2,10, i + 1)\n        img=(imgs[i]).detach().cpu().numpy()\n        plt.imshow(img.reshape(28,28),                   ④\n                   cmap=\"binary\")\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()  \n```", "```py\nplot_digits()\n```", "```py\nfor epoch in range(10):\n    tloss=0\n    for imgs, labels in train_loader:                   ①\n        imgs=imgs.to(device).view(-1, input_dim)\n        out, mu=model(imgs)                             ②\n        loss=((out-imgs)**2).sum()                      ③\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tloss+=loss.item()\n    print(f\"at epoch {epoch} toal loss = {tloss/len(train_loader)}\")\n    plot_digits()                                       ④\n```", "```py\nscripted = torch.jit.script(model) \nscripted.save('files/AEdigits.pt') \n```", "```py\nmodel=torch.jit.load('files/AEdigits.pt',map_location=device)\nmodel.eval()\n```", "```py\nplot_digits()\n```", "```py\ntransform = T.Compose([\n            T.Resize(256),                                 ①\n            T.ToTensor(),                                  ②\n            ])\ndata = torchvision.datasets.ImageFolder(\n    root=\"files/glasses\",    \n    transform=transform)                                   ③\nbatch_size=16\nloader = torch.utils.data.DataLoader(data,                 ④\n     batch_size=batch_size,shuffle=True)\n```", "```py\nlatent_dims=100                                             ①\nclass Encoder(nn.Module):\n    def __init__(self, latent_dims=100):  \n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\n        self.batch2 = nn.BatchNorm2d(16)\n        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)\n        self.linear1 = nn.Linear(31*31*32, 1024)\n        self.linear2 = nn.Linear(1024, latent_dims)\n        self.linear3 = nn.Linear(1024, latent_dims)\n        self.N = torch.distributions.Normal(0, 1)\n        self.N.loc = self.N.loc.cuda() \n        self.N.scale = self.N.scale.cuda()\n    def forward(self, x):\n        x = x.to(device)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.batch2(self.conv2(x)))\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.linear1(x))\n        mu =  self.linear2(x)                               ②\n        std = torch.exp(self.linear3(x))                    ③\n        z = mu + std*self.N.sample(mu.shape)                ④\n        return mu, std, z\n```", "```py\nclass Decoder(nn.Module):   \n    def __init__(self, latent_dims=100):\n        super().__init__()\n        self.decoder_lin = nn.Sequential(                   ①\n            nn.Linear(latent_dims, 1024),\n            nn.ReLU(True),\n            nn.Linear(1024, 31*31*32),                      ②\n            nn.ReLU(True))\n        self.unflatten = nn.Unflatten(dim=1, \n                  unflattened_size=(32,31,31))\n        self.decoder_conv = nn.Sequential(                  ③\n            nn.ConvTranspose2d(32,16,3,stride=2,\n                               output_padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, 8, 3, stride=2, \n                               padding=1, output_padding=1),\n            nn.BatchNorm2d(8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(8, 3, 3, stride=2,\n                               padding=1, output_padding=1))\n\n    def forward(self, x):\n        x = self.decoder_lin(x)\n        x = self.unflatten(x)\n        x = self.decoder_conv(x)\n        x = torch.sigmoid(x)                                ④\n        return x  \n```", "```py\nclass VAE(nn.Module):\n    def __init__(self, latent_dims=100):\n        super().__init__()\n        self.encoder = Encoder(latent_dims)                ①\n        self.decoder = Decoder(latent_dims)                ②\n    def forward(self, x):\n        x = x.to(device)\n        mu, std, z = self.encoder(x)                       ③\n        return mu, std, self.decoder(z)                    ④\n```", "```py\nvae=VAE().to(device)\nlr=1e-4 \noptimizer=torch.optim.Adam(vae.parameters(),\n                           lr=lr,weight_decay=1e-5)\n```", "```py\ndef train_epoch(epoch):\n    vae.train()\n    epoch_loss = 0.0\n    for imgs, _ in loader: \n        imgs = imgs.to(device)\n        mu, std, out = vae(imgs)                                   ①\n        reconstruction_loss = ((imgs-out)**2).sum()                ②\n        kl = ((std**2)/2 + (mu**2)/2 - torch.log(std) - 0.5).sum() ③\n        loss = reconstruction_loss + kl                            ④\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss+=loss.item()\n    print(f'at epoch {epoch}, loss is {epoch_loss}')  \n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_epoch():\n    with torch.no_grad():\n        noise = torch.randn(18,latent_dims).to(device)\n        imgs = vae.decoder(noise).cpu()\n        imgs = torchvision.utils.make_grid(imgs,6,3).numpy()\n        fig, ax = plt.subplots(figsize=(6,3),dpi=100)\n        plt.imshow(np.transpose(imgs, (1, 2, 0)))\n        plt.axis(\"off\")\n        plt.show()\n```", "```py\nfor epoch in range(1,11):\n    train_epoch(epoch)\n    plot_epoch()\ntorch.save(vae.state_dict(),\"files/VAEglasses.pth\")\n```", "```py\nvae.eval()\nvae.load_state_dict(torch.load('files/VAEglasses.pth',\n    map_location=device))\n```", "```py\nimgs,_=next(iter(loader))\nimgs = imgs.to(device)\nmu, std, out = vae(imgs)\nimages=torch.cat([imgs[:8],out[:8],imgs[8:16],out[8:16]],\n                 dim=0).detach().cpu()\nimages = torchvision.utils.make_grid(images,8,4)\nfig, ax = plt.subplots(figsize=(8,4),dpi=100)\nplt.imshow(np.transpose(images, (1, 2, 0)))\nplt.axis(\"off\")\nplt.show()\n```", "```py\nplot_epoch()  \n```", "```py\ntorch.manual_seed(0)  \nglasses=[]\nfor i in range(25):                                        ①\n    img,label=data[i]\n    glasses.append(img)\n    plt.subplot(5,5,i+1)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.axis(\"off\")\nplt.show()\nmen_g=[glasses[0],glasses[3],glasses[14]]                  ②\nwomen_g=[glasses[9],glasses[15],glasses[21]]               ③\n\nnoglasses=[]\nfor i in range(25):                                        ④\n    img,label=data[-i-1]\n    noglasses.append(img)\n    plt.subplot(5,5,i+1)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.axis(\"off\")\nplt.show()\nmen_ng=[noglasses[1],noglasses[7],noglasses[22]]           ⑤\nwomen_ng=[noglasses[4],noglasses[9],noglasses[19]])        ⑥\n```", "```py\n# create a batch of images of men with glasses\nmen_g_batch = torch.cat((men_g[0].unsqueeze(0),              ①\n             men_g[1].unsqueeze(0),\n             men_g[2].unsqueeze(0)), dim=0).to(device)\n# Obtain the three encodings\n_,_,men_g_encodings=vae.encoder(men_g_batch)\n# Average over the three images to obtain the encoding for the group\nmen_g_encoding=men_g_encodings.mean(dim=0)                   ②\n# Decode the average encoding to create an image of a man with glasses \nmen_g_recon=vae.decoder(men_g_encoding.unsqueeze(0))         ③\n\n# Do the same for the other three groups\n# group 2, women with glasses\nwomen_g_batch = torch.cat((women_g[0].unsqueeze(0),\n             women_g[1].unsqueeze(0),\n             women_g[2].unsqueeze(0)), dim=0).to(device)\n# group 3, men without glasses\nmen_ng_batch = torch.cat((men_ng[0].unsqueeze(0),\n             men_ng[1].unsqueeze(0),\n             men_ng[2].unsqueeze(0)), dim=0).to(device)\n# group 4, women without glasses\nwomen_ng_batch = torch.cat((women_ng[0].unsqueeze(0),\n             women_ng[1].unsqueeze(0),\n             women_ng[2].unsqueeze(0)), dim=0).to(device)\n# obtain average encoding for each group\n_,_,women_g_encodings=vae.encoder(women_g_batch)\nwomen_g_encoding=women_g_encodings.mean(dim=0)\n_,_,men_ng_encodings=vae.encoder(men_ng_batch)\nmen_ng_encoding=men_ng_encodings.mean(dim=0)\n_,_,women_ng_encodings=vae.encoder(women_ng_batch)\nwomen_ng_encoding=women_ng_encodings.mean(dim=0)              ④\n# decode for each group\nwomen_g_recon=vae.decoder(women_g_encoding.unsqueeze(0))\nmen_ng_recon=vae.decoder(men_ng_encoding.unsqueeze(0))\nwomen_ng_recon=vae.decoder(women_ng_encoding.unsqueeze(0))    ⑤\n```", "```py\nimgs=torch.cat((men_g_recon,\n                women_g_recon,\n                men_ng_recon,\n                women_ng_recon),dim=0)\nimgs=torchvision.utils.make_grid(imgs,4,1).cpu().numpy()\nimgs=np.transpose(imgs,(1,2,0))\nfig, ax = plt.subplots(figsize=(8,2),dpi=100)\nplt.imshow(imgs)\nplt.axis(\"off\")\nplt.show()\n```", "```py\nz=men_g_encoding-women_g_encoding+women_ng_encoding         ①\nout=vae.decoder(z.unsqueeze(0))                             ②\nimgs=torch.cat((men_g_recon,\n                women_g_recon,\n                women_ng_recon,out),dim=0)\nimgs=torchvision.utils.make_grid(imgs,4,1).cpu().numpy()\nimgs=np.transpose(imgs,(1,2,0))\nfig, ax = plt.subplots(figsize=(8,2),dpi=100)\nplt.imshow(imgs)                                            ③\nplt.title(\"man with glasses - woman \\\nwith glasses + woman without \\\nglasses = man without glasses \",fontsize=10,c=\"r\")          ④\nplt.axis(\"off\")\nplt.show()\n```", "```py\nresults=[]\nfor w in [0, 0.2, 0.4, 0.6, 0.8, 1.0]:           ①\n    z=w*women_ng_encoding+(1-w)*women_g_encoding ②\n    out=vae.decoder(z.unsqueeze(0))              ③\n    results.append(out)\nimgs=torch.cat((results[0],results[1],results[2],\n                results[3],results[4],results[5]),dim=0)\nimgs=torchvision.utils.make_grid(imgs,6,1).cpu().numpy()\nimgs=np.transpose(imgs,(1,2,0))\nfig, ax = plt.subplots(dpi=100)\nplt.imshow(imgs)                                 ④\nplt.axis(\"off\")\nplt.show()\n```"]