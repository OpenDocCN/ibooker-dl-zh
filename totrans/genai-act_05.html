<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span> </span> <span class="chapter-title-text">What else can AI generate?</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Using generative AI for code creation and code-related tasks</li> 
    <li class="readable-text" id="p3">Tools that allow code generation and how to use them</li> 
    <li class="readable-text" id="p4">Best code generation practices</li> 
    <li class="readable-text" id="p5">Generating video and related tools</li> 
    <li class="readable-text" id="p6">Generating audio, music, and related tools</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Code that writes itself with little prompting and without much input seems magical, resembling a holy grail, at least to those working in computing. Given the advancements in artificial intelligence (AI) with generative AI, this endeavor seems possible today. We have seen some amazing and interesting things AI can generate—from language to images to holding an ongoing back-and-forth multiturn conversation—and many of them have strong use cases in enterprises. This chapter outlines the remaining things we can generate using AI.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>We will first talk about code generation, what it means, how one should go about it, and the tools enterprises use. For example, Andrej Karpathy, one of the OpenAI cofounders, who used to lead Tesla’s AI and Vision team, recently said that GitHub Copilot helps him write approximately 80% of his code, which is a huge boost in productivity. Then, we will cover a few very early generations and explore application in videos and music. Let’s see how code generation works.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_83"><span class="num-string">5.1</span> Code generation</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Generative AI is not just about completions, chats, or generating images. It’s a technology that can significantly enhance developers’ productivity and improve software development processes in enterprises. One of its most intriguing aspects is the ability to generate code and aid in code understanding and documentation. From a development lifecycle perspective, the term “code generation” can be misleading, as it encompasses much more than code generation itself. It spans various aspects of software development. Here are a few examples of how enterprises employ code generation:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p11"> <em>Code generation</em><em> </em>—Augments development by generating code for a given prompt. This isn’t complete code for whatever is being built but code at the function level. </li> 
   <li class="readable-text" id="p12"> <em>Productivity improvements</em><em> </em>—Tools based on generative AI can help improve developers’ productivity, especially when using new libraries and software develop-ment kits (SDKs) or programming languages that might be new for a developer. We can also improve the speed of implementation of much of the scaffolding (such as AI wrappers, database queries, etc.) that many enterprise applications need to implement, such as access control, encryption, and security, to name a few. </li> 
   <li class="readable-text" id="p13"> <em>Onboarding new employees</em><em> </em>—For enterprises, it is quite common to have internal proprietary development standards, internal libraries, and SDKs that encapsulate a lot of domain and institutional knowledge and IP. Generative AI tools can help new full-time employees (FTEs) get ramped up and trained quickly using these SDKs and libraries. New FTEs can also serve as a model to explain snippets, helping developers learn quickly. </li> 
   <li class="readable-text" id="p14"> <em>Automation</em><em> </em>—Many development tasks are repetitive, and it is common for many developers to skip them or take shortcuts, which can cause problems down the road. Generative AI can help automate repetitive tasks such as code reviews, testing, documentation, design iterations, UI mockups, and so forth. </li> 
   <li class="readable-text" id="p15"> <em>Fostering creativity</em><em> </em>—Generative AI tools can help developers see different approaches and ideas when coding or rapidly prototyping, encouraging them to explore newer techniques that might be better and help teach. </li> 
  </ul> 
  <div class="readable-text" id="p16"> 
   <p>Before we get into the details, we will start with code generation examples. Say we want to write a function to calculate its time complexity. Time complexity measures the length (i.e., the time) a function will take to execute. It is often expressed using Big O notations—constant, linear, quadratic, and exponential time.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p17"> 
   <p><span class="print-book-callout-head">Note</span>  For brevity, we won’t show the full test code generation here; this can be found in the books accompanying the GitHub repository at <a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>.</p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>Let’s start with a simple toy example using GitHub Copilot in our IDE. A comment is the prompt, and the model completes the code generation, as shown in figure 5.1. Regarding the developers’ experience, this might seem like a fancier version of autocomplete, but it is much more than that. We can think of the code generation as the completion API we saw earlier, with the difference being that what will be generated will be code. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p19">  
   <img alt="figure" src="../Images/CH05_F01_Bahree.png" width="1100" height="434"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.1</span> Code generation to calculate time complexity</h5>
  </div> 
  <div class="readable-text" id="p20"> 
   <p>The first suggestion, in the grey text (also called ghost text), seems good; if we want, we can get up to 10 suggestions and find a better one. Figure 5.2 shows a snippet of these alternate generations.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p21">  
   <img alt="figure" src="../Images/CH05_F02_Bahree.png" width="1100" height="1201"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.2</span> GitHub Copilot code completion suggestions</h5>
  </div> 
  <div class="readable-text" id="p22"> 
   <p>In this instance, the last suggestion (number 10) seems better and is what we will use, as shown in figure 5.3.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p23">  
   <img alt="figure" src="../Images/CH05_F03_Bahree.png" width="1100" height="299"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.3</span> AI-generated code to calculate the time complexity of a function</h5>
  </div> 
  <div class="readable-text" id="p24"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_84"><span class="num-string">5.1.1</span> Can I trust the code?</h3> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>In the context of code generation, one of the areas that many enterprises are considering is how to trust the generated code. Let’s take the example of generating complex code, such as implementing OAuth2 for a web application, as shown in figure 5.4. In<span class="aframe-location"/> general, code-generation tools are becoming increasingly reliable and accurate. However, it is still important to be aware of the code limitations; whether one can trust generated code depends on several factors, including</p> 
  </div> 
  <div class="browsable-container figure-container" id="p26">  
   <img alt="figure" src="../Images/CH05_F04_Bahree.png" width="1100" height="720"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.4</span> Code generation showing OAuth2 implementation</h5>
  </div> 
  <ul> 
   <li class="readable-text" id="p27"> The quality of the tool and underlying model pinning that code generation tool. </li> 
   <li class="readable-text" id="p28"> The complexity of the task the code is being generated for; some tools are better suited for well-defined tasks than complex logic and reasoning tasks that can result in error. </li> 
   <li class="readable-text" id="p29"> When using AI-generated code, trust and review are paramount. The code and associated tools should always be used with other development tools and processes, such as code reviews and unit tests, which ensure that the generated code meets the required standards and is free from errors or vulnerabilities. </li> 
  </ul> 
  <div class="readable-text" id="p30"> 
   <p>It is important to note that GitHub Copilot does not guarantee that the code it generates is correct, bug-free, or secure. The developer is still responsible for reviewing, testing, and verifying the code before using it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>GitHub Copilot does provide some features to help developers ensure the quality of the code, such as code review, testing, and feedback. In addition, it has several guardrails in place to help prevent it from generating incorrect or harmful code. For example, GitHub Copilot has filters that block offensive words and code that is likely biased or discriminatory.</p> 
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>In addition, GitHub Copilot also performs several safety checks before generating code, such as for potential syntax errors and security vulnerabilities. GitHub Copilot’s AI-based vulnerability prevention system is a feature that aims to make the code suggestions more secure and help developers avoid common security flaws in their code. It works using a machine learning model that can detect insecure coding patterns in real time and block them from being suggested. It also generates a new suggestion that does not contain the vulnerability. Some of the vulnerabilities that the system can protect against are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p33"> <em>Hardcoded credentials</em><em> </em>—This is when sensitive information such as passwords, API keys, or tokens is embedded in the source code, meaning attackers can access it easily. The system can identify hardcoded credentials and replace them with placeholders or environment variables. </li> 
   <li class="readable-text" id="p34"> <em>SQL injection</em><em> </em>—This is when user input is directly inserted into a SQL query, allowing attackers to execute malicious commands on the database. The system can identify SQL injection vulnerabilities and suggest using parameterized queries or prepared statements instead. </li> 
   <li class="readable-text" id="p35"> <em>Path injection</em><em> </em>—This occurs when user input is used to construct a file path, allowing attackers to access or modify files outside the intended scope. The system can identify path injection vulnerabilities and suggest using sanitization functions or validation checks before using the input. </li> 
  </ul> 
  <div class="readable-text" id="p36"> 
   <p>Code generation tools can be powerful allies for enterprise developers but require careful and responsible use. As outlined by the National Institute of Standards and Technology, one of the best ways to secure code is to use a secure software development lifecycle.</p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Now that we have seen a simple example of what is possible, let’s see how we can do this. The next section will explore common tools such as Tabnine, Code Llama, and Amazon’s CodeWhisperer. However, in this section, we will talk about GitHub Copilot.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_85"><span class="num-string">5.1.2</span> GitHub Copilot</h3> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>A few tools are now available for code generation. Most enterprises use GitHub Copilot, one of the first code-generation tools on the market. GitHub Copilot is a cloud-based generative AI tool that helps developers by generating code based on natural language prompts. It uses models from OpenAI, has been trained on billions of lines of code, and is positioned as our new AI pair programmer—one that helps us write code better, solve problems, understand new APIs, and write tests without trawling through a ton of information and sites searching for answers. The high-level flow is shown in figure 5.5.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p40">  
   <img alt="figure" src="../Images/CH05_F05_Bahree.png" width="1015" height="279"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.5</span> GitHub Copilot high-level flow using Visual Studio Code</h5>
  </div> 
  <div class="readable-text" id="p41"> 
   <p>GitHub Copilot runs as an add-in and supports many of the leading programming languages available for some of the leading IDEs (e.g., Visual Studio, Visual Studio Code, Neovim, and JetBrains). It supports about a dozen primary programming languages, such as C, C++, Java, C#, Python, Go, Ruby, and many more, as well as secondary and relatively less-supported languages (such as COBOL). All the languages that GitHub Copilot supports are listed at <a href="https://docs.github.com/">https://docs.github.com/</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>As we have seen, the current version of GitHub Copilot takes a prompt via a comment, considers the context of the file a developer works on in the IDE, and then helps make the suggestions in the code. The results for developers across the board have been amazing. According to research published by GitHub, 96% of developers are faster on repetitive tasks, 88% feel more productive, and nearly 75% focus on more satisfying things. Code generation is not about creating complete solutions or end-to-end code but rather about creating parts of code that can help with a specific function or some core logic within a function. </p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>Copilot requires a subscription and comes in two versions, one targeting individuals and the other targeting enterprises. The underlying model powering both is the same, the main differences being that the enterprise version has additional controls for managing telemetry and enterprises can enforce organization-wide policies.</p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>When considering privacy and data protection, GitHub Copilot (the business edition) collects information in three areas, as outlined in the following list. These help with the overall service health, experience-latency, and feature engagement and also help fine-tune and improve the algorithms for ranking and sorting completions. In addition, they can aid in detecting abuse of the service and policy violations:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p45"> <em>End-user engagement data</em><em> </em>—GitHub Copilot collects the end-user’s interaction with the IDE when using Copilot. This includes usage and error details, as well as data on actions taken by the user, such as which of the generated completions was accepted. Some personal data might be included but is not tied directly to the user.  </li> 
   <li class="readable-text" id="p46"> <em>Prompts</em><em> </em>—For enterprise users, the prompts are ephemeral, employed only when using the service, and not retained. For individual users, the prompts persist, but the user has the option of deactivating them. </li> 
   <li class="readable-text" id="p47"> <em>Completions (i.e., suggestions)</em><em> </em>—The completions, similar to the prompts, are ephemeral, transmitted back to the Copilot extension running in the IDE, and are not persisted. </li> 
  </ul> 
  <div class="readable-text" id="p48"> 
   <p>Copilot uses more than just the prompt when trying to create suggestions. In addition to the prompt, it also factors in the edited file and the other tabs and files in the solutions open for context. Furthermore, it combines all of that as grounding and context information to allow for more meaningful and better generations. And this generation goes beyond the code, stylistic patterns, and syntactic sugar.</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>Let us use a simple example. Say we want to generate a function that we will employ to generate an image using Stability AI, which we did in the previous chapter. We use the following prompt.</p> 
  </div> 
  <div class="readable-text prompt" id="p50"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Prompt.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Write a Python function that takes a prompt and uses stability AI to generate an image and save it to a file. </p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>When we have an empty solution with just a few lines of code to get this started, we get the code shown in figure 5.6 with the <code>generate()</code> function generated by GitHub Copilot. As we can see, this is rather simple and goes through the mechanics of first encoding the prompt to a <code>base64</code> format. It calls the completion API, extracts the image from the API response, decodes it from <code>base64</code>, and then finally saves it to a file using a date–time stamp as the file name. This was discussed in detail in the previous chapter, and there is nothing wrong with the code. It is a pretty vanilla implementation. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p52">  
   <img alt="figure" src="../Images/CH05_F06_Bahree.png" width="1100" height="624"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.6</span> GitHub Copilot code generation<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p53"> 
   <p>However, we must follow programming standards, architecture patterns, and methodologies. Otherwise, the code shown in the previous example would not work and would require more manual effort. So how can we address this?</p> 
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>Let’s generate another function using the same prompt. This time, we open a file in our existing solution for image generation that we used in the previous chapter. Listing 5.1 shows the generated code. This code seems quite familiar, as it closely follows our syntax and patterns for generating images from the previous chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>The interesting thing in this example is how GitHub Copilot generated the helper functions to check for paths, clean up filenames, and so forth, even when we did not explicitly ask for it. This pattern was common across a few files in the image generation solution (from the last chapter), which was picked up as context. The updated code saves the prompt as part of the filename, not just a date–time stamp. Again, this was not explicitly asked, and while it might seem like syntactic sugar, patterns and architecture requirements such as these make the codebase maintainable, robust, and familiar in an enterprise setting.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p56"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 5.1</span> GitHub Copilot generation in an existing solution </h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"># write a python function that takes a prompt and uses stability AI 
# to generate a image and save it to a file
def generate_image(prompt):
    if api_key is None:
        raise Exception("Missing Stability API key.")

    # Set the directory where we'll store the image
    image_dir = os.path.join(os.curdir, 'images')

    # Make sure the directory exists
    if not os.path.isdir(image_dir):
        os.mkdir(image_dir)

    # Function to clean up filenames
    def valid_filename(s):
        s = re.sub(r'[^\w_.)( -]', '', s).strip()
        return re.sub(r'[\s]+', '_', s)

    response = requests.post(
        f"{api_host}/v1/generation/{engine_id}/text-to-image",
        headers={
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Authorization": f"Bearer {api_key}"
        },
        json={
            "text_prompts": [{ "text": f"{prompt}", "weight": 1.0}],
            "cfg_scale": 7, "height": 1024, "width": 1024,
            "samples": 1, "steps": 50,
        },
    )

    if response.status_code != 200:
        raise Exception("Non-200 response: " + str(response.text))

    data = response.json()

    for i, image in enumerate(data["artifacts"]):

filename = f"sd_{valid_filename(prompt)}_{i}_{
                      <span class="">↪</span>datetime.datetime.now().strftime(
                      <span class="">↪</span>'%Y%m%d_%H%M%S')}.png"
        image_path = os.path.join(image_dir, filename)"
        image_path = os.path.join(image_dir, filename)
        with open(image_path, "wb") as f:
            f.write(base64.b64decode(image["base64"]))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_86"><span class="num-string">5.1.3</span> How Copilot works</h3> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>When GitHub Copilot was first released, GitHub worked closely with OpenAI to create a special version of GPT3 called Codex. This version was trained on both natural language and billions of lines of code. Codex supports multiple programming languages and can be used for multiple code-related tasks. Today, Codex is deprecated, as the same learnings have been incorporated into the mainline GPT models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>Copilot is building a separate prompt all the time in the background, which is one reason we see completions not only when prompted but throughout when writing code—starting or in the middle of something else. Starting with a prompt line and the corresponding code file using Codex was just the beginning. Copilot now looks at several things when suggesting generations. The prompt library is where algorithms take into account the broader context of what a developer is doing and create the prompt used by the model. In addition to the code file and the prompts we enter, this also considers the other open tabs and the broader solution, as shown in our earlier demo. Figure 5.7 illustrates this high-level flow and the life cycle.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>One behavior of particular interest is a feature called fill-in-the-middle (or FIM). As the name suggests, the code is not generated at the end of a file, but in the middle. Before FIM was implemented, the code after the cursor’s current position was ignored; now it helps fill in the missing code, considering the code before and after the insertion point, taken in the full context.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/CH05_F07_Bahree.png" width="927" height="287"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.7</span> Copilot completion lifecycle</h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <p>The newer version, Copilot Chat, uses a chat-like interface similar to ChatGPT. This chat-like feature offers a much richer experience and modality from a developer’s perspective and allows us to take in more than just the prompt or the code. It helps us with much richer context (of the code and errors) and lets us spot any possible problems. This is also extensible to other aspects that developers use daily—from helping understand legacy code to unit test generation. The original version of Copilot used Codex, a fine-tuned version of GPT-3. Codex is now retired, and the newer versions of Copilot Chat use newer models. Let’s examine some of these areas in more depth. </p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_87"><span class="num-string">5.2</span> Additional code-related tasks</h2> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>In addition to code generation, there are other use cases that can be utilized in the context of code and improving developer productivity. Some of these are the generation of other aspects, such as unit tests or documentation. Let’s start with one of the features called code explanation.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_88"><span class="num-string">5.2.1</span> Code explanation</h3> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>One of the powerful features of GitHub Copilot Chat is that it offers a more expressive medium to interact with the code. One example is being able to chat and ask for an explanation of the selected code in the IDE.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>Figure 5.8 illustrates an example of code explanation where we use one of our earlier completions and naturally interact with and use the AI to help us generate an explanation. The screenshot doesn’t show it, but GitHub Copilot Chat explains different parameters and their meaning.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p68">  
   <img alt="figure" src="../Images/CH05_F08_Bahree.png" width="1100" height="605"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.8</span> GitHub Copilot explanation example</h5>
  </div> 
  <div class="readable-text" id="p69"> 
   <p>As outlined earlier, Copilot can also help explain legacy code, which might be in legacy languages such as COBOL, as shown in figure 5.9.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p70">  
   <img alt="figure" src="../Images/CH05_F09_Bahree.png" width="1100" height="608"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.9</span> Copilot Chat explaining the COBOL code</h5>
  </div> 
  <div class="readable-text" id="p71"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_89"><span class="num-string">5.2.2</span> Generate tests</h3> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>We can build on the previous example to demonstrate how to help generate tests for a given code set, as shown in figure 5.10. This feature helps developers save precious time and effort in writing unit tests, making them more productive. It can also help produce novel and diverse test cases that cover different scenarios and edge cases compared to what most developers could create themselves.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p73">  
   <img alt="figure" src="../Images/CH05_F10_Bahree.png" width="1100" height="566"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.10</span> Generating unit tests</h5>
  </div> 
  <div class="readable-text" id="p74"> 
   <p>GitHub Copilot Chat helps generate unit tests and will test whether the <code>openai .completion.create()</code> method works as expected if the print statements output the correct strings. The unit tests can handle the nondeterministic behavior of AI by using mocking, following the steps as listed:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p75"> Import the necessary modules for testing, such as <code>unittest</code> and <code>mock</code>. </li> 
   <li class="readable-text" id="p76"> Create a new class for the test case, inheriting from <code>unittest.TestCase</code>. </li> 
   <li class="readable-text" id="p77"> Within this class, create a setup method to initialize the environment for the tests. </li> 
   <li class="readable-text" id="p78"> Create a test method to test the <code>openai.completion.create()</code> method. Use <code>'mock'</code> to simulate the response from the OpenAI API. </li> 
   <li class="readable-text" id="p79"> Create a test method to test the output of the <code>print</code> statements. </li> 
   <li class="readable-text" id="p80"> At the end of the script, add a line to run all the tests when the script is executed. </li> 
  </ol> 
  <div class="readable-text" id="p81"> 
   <p>Of course, a developer still needs to check the tests and ensure they fit the purpose. Generated tests can have many limitations, from covering only some possible scenarios (e.g., complex data behavior or accounting for user interactions) at one end to code maintainability.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_90"><span class="num-string">5.2.3</span> Code referencing</h3> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Code referencing is a feature that helps developers detect the code generated by Copilot against public repositories on GitHub for any matches. This action is not default and is a setting that needs to be enabled in the Copilot configuration. The advantage of code referencing is that it helps developers make more informed decisions about their code. Code referencing shows when a code suggestion matches public code on GitHub and provides information about the repositories where that code appears and their licenses.</p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>This way, developers can learn from others’ work, discover documentation, avoid potential legal problems, and give or receive credit for similar work. Furthermore, code referencing allows developers to ask GitHub Copilot to rewrite the code if they want a different implementation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>GitHub Copilot automatically matches the (approximately 150 characters) code it generates against repositories. It finds similar code and outlines its associated licensing terms, if any. This allows us to accept or reject the code suggestion. We can also ask Copilot to rewrite and create a new generation that differs from the matching one.</p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>According to research released by GitHub [1], less than 1% of code generation ends up matching, and while that is a small percentage, it is not evenly distributed across the spectrum. Most of it occurs when the code file is new and empty, as there is little additional context for the solution. This is rare in cases when there are multiple files and existing solutions, as the code generation is much more specific to the situation and prompt.</p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>In addition, many of these matches are patterns of libraries that are code fragments posted to popular sites such as Stack Overflow, often without attribution. Frequently, many are also core APIs of common libraries used across many projects that are taking a dependency on those specific libraries. From an enterprise and developers’ perspective, there are several benefits to using code referencing:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p88"> It helps enterprises make a build-versus-buy decision by understanding whether they can depend on an existing open source library to reduce the need for new business logic and cost. </li> 
   <li class="readable-text" id="p89"> It helps developers improve their coding skills, especially by examining how others have solved similar problems. </li> 
   <li class="readable-text" id="p90"> For many enterprises, the default position is often to avoid code matching public repositories; thus, code referencing allows them to choose the source appropriately and give credit to the author. </li> 
   <li class="readable-text" id="p91"> It helps developers understand the relevance and quality of the code before taking a dependency and accepting a suggestion that matches the public code. </li> 
   <li class="readable-text" id="p92"> When the topic or library is new, it helps developers explore new projects and collaborate with other developers. </li> 
  </ul> 
  <div class="readable-text" id="p93"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_91"><span class="num-string">5.2.4</span> Code refactoring</h3> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>GitHub Copilot Chat helps with code refactoring by providing intelligent suggestions across the solution, thus improving the code’s structure, readability, and maintainability. Some ways it can assist with code refactoring are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p95"> Simplifying complex expressions or statements </li> 
   <li class="readable-text" id="p96"> Extracting repeated code into functions or methods </li> 
   <li class="readable-text" id="p97"> Adding comments or documentation to explain the code logic </li> 
   <li class="readable-text" id="p98"> Renaming variables or functions to follow naming conventions </li> 
  </ul> 
  <div class="readable-text" id="p99"> 
   <p>Another set of experimental features of Copilot is called Labs, where we can use different aspects to understand the code and help refactor it—whether by making it more readable, more robust, or more error-proof or even by helping us isolate and understand a bug in the existing code (figure 5.11).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p100">  
   <img alt="figure" src="../Images/CH05_F11_Bahree.png" width="1100" height="926"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.11</span> Copilot tools for refactoring</h5>
  </div> 
  <div class="readable-text" id="p101"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_92"><span class="num-string">5.3</span> Other code generation tools</h2> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>GitHub Copilot is one of the first and, as of now, most commonly used code generation tools, especially in enterprises. However, other code generation tools are learning from Copilot and are starting to appear. While the details of how each works differ slightly, using different language learning models (LLMs) at a high level, they all operate very similarly to what we outlined earlier in the chapter. This section provides a quick overview of some of the other code generation tools available on the market. The intent is not to go deeply into them, as many are clones and offer the same functionality. It is to show how enterprises can evaluate and choose the ones that work best in their context and work more easily with their organizational development culture.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_93"><span class="num-string">5.3.1</span> Amazon CodeWhisperer</h3> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Amazon has CodeWhisperer, AWS’s answer to GitHub Copilot. It can generate code based on prompts and help write functions. It supports a narrower set of programming languages than Copilot and similar IDEs. CodeWhisperer is available via the AWS toolkit extensions, as shown in figure 5.12. </p> 
  </div> 
  <div class="readable-text intended-text" id="p105"> 
   <p>We don’t know the technical details of how CodeWhisperer works, so we can’t compare it directly with GitHub Copilot. However, we can say that CodeWhisperer and GitHub Copilot focus on different things. CodeWhisperer is more specialized for AWS services (such as EC2, S3, Lambda, etc.), while GitHub Copilot is more general purpose.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p106">  
   <img alt="figure" src="../Images/CH05_F12_Bahree.png" width="878" height="744"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.12</span> Amazon CodeWhisperer</h5>
  </div> 
  <div class="readable-text" id="p107"> 
   <p>Additional details on Amazon CodeWhisperer can be found at <a href="https://aws.amazon.com/codewhisperer/">https://aws.amazon.com/codewhisperer/</a>.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Amazon Q AI assistant</h4> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>Amazon recently announced Amazon Q as a new AI assistant for AWS that targets enterprise customers. It can do more than help with coding. It can talk, offer advice, create content, and access different data sources and systems. Developers can use it to fix, improve, and understand code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>Amazon Q is an AI assistant that helps with coding and AWS tasks. It depends on CodeWhisperer. To use Amazon Q, you must pay for the Amazon CodeWhisperer Professional tier and install the latest AWS Toolkit. Amazon Q understands AWS better than CodeWhisperer, which mainly helps with coding. More details on Amazon Q can be found at <a href="https://aws.amazon.com/q">https://aws.amazon.com/q</a>.</p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_94"><span class="num-string">5.3.2</span> Code Llama</h3> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>Meta recently released Code Llama, an LLM model targeting coding similar to Codex. Code Llama builds on Llama 2 by training it on more code-specific datasets. It can generate code and understand natural language about code. Like Codex and GPT4, it supports some of the more popular programming languages—Python, C++, Java, C#, and so forth.</p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>Code Llama is released as an OSS model, including the weights, and is free for commercial and research purposes, although it has a special license. It is available in three sizes: 7B-, 13B-, and 30B-parameter base models. Each base model is further fine-tuned and available in two variants—one specifically for Python and another for Instruct. Code Llama also supports input sequences of 100K tokens, allowing sending a longer application code base as context.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p114"> 
   <p><span class="print-book-callout-head">NOTE</span>  Meta has chosen to release Code Llama under the same license as Llama 2, which is permissive. This also ensures that enthusiasts, researchers, and businesses can use these models in academic research and commercial applications without restrictions. However, the license forbids using Llama 2 to train other LLMs, requiring a special license from Meta if the model is used in an app or service with over 700 million monthly users.</p> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>Being smaller in a production deployment, the 7B and 13B base models require fewer resources in the sense of computing power (GPU), memory, and power; therefore, these models can be faster for inference and are better suited for low-latency scenarios where faster responses are required. Note that the exact definition of low-latency, of course, would be dependent on the use case and scenarios at hand. These two base models and their fine-tuned versions also support FIM capabilities, which Meta calls infilling.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p116"> 
   <p><span class="print-book-callout-head">Note</span>  Consumer-class GPUs are for general consumers who want to play games or edit videos. They are cheaper, use less power, and have less memory than data-center-class GPUs. Data-center-class GPUs are for professionals who need high performance and reliability. They are more expensive, powerful, and have more memory and special features than consumer-class GPUs.</p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <p>This is the model itself, and as of publication, there isn’t a toolset around it like GitHub Copilot. Enterprises and other companies would need to take the model, host it themselves, and require GPUs for inference and managing lifecycles. The small models can be run on a consumer-class GPU when quantized. Quantization is a technique that reduces the number of bits used to represent the model’s parameters, which can save memory, speed up inference, and improve energy efficiency. However, quantization can also introduce accuracy loss or hardware inefficiency if not done properly.</p> 
  </div> 
  <div class="readable-text intended-text" id="p118"> 
   <p>Figure 5.13 shows the generation using the chat completion of Code Llama. While it is a little different, it is still similar to what we have seen thus far. The full generated code can be found in books accompanying the GitHub repository at <a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p119">  
   <img alt="figure" src="../Images/CH05_F13_Bahree.png" width="990" height="856"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.13</span> Code Llama generating function </h5>
  </div> 
  <div class="readable-text" id="p120"> 
   <p>You can find more details on Code LLama at Meta’s site (<a href="https://llama.meta.com/code-llama">https://llama.meta.com/code-llama</a>).</p> 
  </div> 
  <div class="readable-text" id="p121"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_95"><span class="num-string">5.3.3</span> Tabnine</h3> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>Tabnine is another AI-powered assistant that helps a developer, similar to GitHub Copilot. It provides real-time completions, and it has recently announced a chat-like feature. Tabnine can help complete code blocks and functions (see figure 5.14). As an advantage, Tabnine offers an option to be run locally or in the cloud, although its default mode is hybrid (i.e., using both). Tabnine supports more IDEs and the same programming languages, including C, C++, C#, Java, Python, React, NodeJS, and so forth. Tabnine uses a proprietary LLM trained on OSS libraries, and enterprises can run in a Kubernetes cluster on-premises. More details on Tabnine can be found at <a href="https://www.tabnine.com/install">https://www.tabnine.com/install</a>.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p123">  
   <img alt="figure" src="../Images/CH05_F14_Bahree.png" width="1100" height="252"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.14</span> Tabnine code generation in Visual Studio Code</h5>
  </div> 
  <div class="readable-text" id="p124"> 
   <p>Note that this is not an exhaustive list of tools that enterprises and developers can use as AI-based tools for code generation and other code-related tasks. It does show the more commonly used ones in the context of enterprises. A few additional notable ones are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p125"> <em>Codey</em><em/>—Google’s foundation code generation model supports over 20 languages. </li> 
   <li class="readable-text" id="p126"> <em>Gemini</em><em/>—Google’s answer to ChatGPT now supports code generation. At the time of publication, it still did not offer integration into an IDE. It was a standalone in the chat paradigm that allowed the copy and exporting of the code into Google Colab notebooks. Google launched this feature as Bard, which was rebranded and powered by a new multimodality model called Gemini. </li> 
   <li class="readable-text" id="p127"> <em>CodeT5+</em><em/>—Salesforce has a new family of code LLMs that are OSS and can support both generation and understanding; these can be adapted to downstream tasks. </li> 
   <li class="readable-text" id="p128"> <em>StableCode</em><em/>—Stability AI, the company behind the Vision models we saw earlier, recently announced a code-based base LLM. This is an OSS model that also supports multiple programming languages. In addition to the base model, there is an instruct model that would be more useful for most developers. Out of the box, it has no IDE integration. </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p129"> 
   <p><span class="print-book-callout-head">Note</span> Many of the OSS models that do not have an IDE integration can be hosted on Hugging Face and called by another Visual Studio Code extension—<code>huggingface-vscode</code>. This code completion extension allows us to use most OSS models. More details on the extension can be found at the GitHub repository (<a href="https://github.com/huggingface/huggingface-vscode">https://github.com/huggingface/huggingface-vscode</a>). This extension can also be configured to call a custom endpoint that is not a Hugging Face interference API.</p> 
  </div> 
  <div class="readable-text" id="p130"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_96"><span class="num-string">5.3.4</span> Check yourself</h3> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>Code generation tools can be very helpful for enterprise developers, as they can save time, reduce errors, and improve productivity. However, code generation tools are imperfect and require human oversight, and validation. Here are some tips on how to trust and use these code generation tools effectively:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p132"> <em>Choose the right tool for the right task</em>. Code generation tools vary in their capabilities, quality, and suitability for domains and languages. Developers should evaluate the available tools and select the ones that best suit their needs and preferences. For example, some tools may be better for generating UI components, while others may be better for generating business logic or data access layers. </li> 
   <li class="readable-text" id="p133"> <em>Follow the best practices and guidelines for code generation</em>. Code generation tools often provide documentation and examples of using them properly and efficiently. Developers should follow these best practices and guidelines to ensure the quality and consistency of the generated code. For instance, some tools may require certain naming conventions, annotations, or templates to work correctly. </li> 
   <li class="readable-text" id="p134"> <em>Review, test, and verify the generated code</em>. Code generation tools are not a substitute for human expertise and judgment. Developers should always review, test, and verify the generated code before production. They should check for errors, bugs, security vulnerabilities, performance problems, readability, maintainability, and compliance with standards and regulations. They should also compare the generated code with similar snippets and suggest improvements if needed. </li> 
   <li class="readable-text" id="p135"> <em>Provide feedback and report problems to the tool providers</em>. Code generation tools are constantly learning from new code and feedback from developers. Developers should provide feedback and report problems to the tool providers to help them improve their products and services. They should also keep track of the updates and enhancements of the tools and learn how to use them effectively. </li> 
  </ul> 
  <div class="readable-text" id="p136"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_97"><span class="num-string">5.3.5</span> Best practices for code generation</h3> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>Irrespective of the tool we use, the concept of using LLMs for code generation and other code-related tasks is still very novel. Some best practices that should be considered in an enterprise when thinking about using generative AI and LLMs are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p138"> <em>Design for imperfections</em><em> </em>—The LLMs will be wrong and will hallucinate. The generated code could outline APIs that look good at the surface but might not be real. They also can be wrong and produce code that doesn’t compile and execute. In addition to being incorrect, sometimes the generated code can be inefficient. It is important to be aware of these limitations and take steps to mitigate them, including checking yourself as outlined earlier and using a technique called prompt engineering, which we will cover later in chapter 6. </li> 
   <li class="readable-text" id="p139"> <em>Clear and specific goals</em><em> </em>—For the code generation task, ensure the goal is clear and specific. Consider the code needed, the inputs and outputs, and specific quality criteria. A clear vision of the desired outcome can help our code generation more effectively. This includes adding details on specific libraries and packages the code should use when not obvious, as it cannot guess our intent. </li> 
   <li class="readable-text" id="p140"> <em>Iterative prompts</em><em> </em>—Small changes in the prompt can significantly change the generation. Consequently, iterating through prompts in small steps and their generated results would be important to managing this. The vaguer the prompt, the poorer the resulting generated code. Understanding the prompts is a combination of both art and science. We will cover details of prompt engineering later in the book. </li> 
   <li class="readable-text" id="p141"> <em>Evaluatio</em><em>n</em><em> </em>—Use multiple metrics and methods to evaluate the quality of the generated code. This has many attributes, for example, syntax, semantics, functionality, readability, and maintainability. Where possible, we should use different dimensions of automated metrics (e.g., BLEU, ROUGE), human evaluation (e.g., surveys, interviews), testing (e.g., unit tests, integration tests), debugging (e.g., static analysis, dynamic analysis), and so forth. </li> 
   <li class="readable-text" id="p142"> <em>Development standards</em><em> </em>—Follow coding standards and best practices for the target programming language or framework you want to generate code for; if there are enterprise or industry standards, including them in existing code solutions will provide the context and hints for the generated code. </li> 
  </ul> 
  <div class="readable-text" id="p143"> 
   <p>Let us switch modalities and outline a few areas of video and music generation that are still quite new and cover science and research. Given the speed of innovation, it won’t be long before these are more commonly available. Both generative AI music and video generation have the potential to revolutionize the way enterprises create and distribute content. As technology continues to develop and become more accessible, we can expect to see more and more enterprises using it to create innovative and engaging experiences for their customers and employees.</p> 
  </div> 
  <div class="readable-text" id="p144"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_98"><span class="num-string">5.4</span> Video generation</h2> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>Video generation using generative AI is a young but rapidly developing field, with many potential applications. Some organizations use video generation to enhance creativity and innovation by generating novel and original content that can attract and engage customers. Others use it to personalize customer experience by creating video content according to the preferences and needs of individual customers, such as their mood, taste, location, or behavior.</p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>Some companies are already using this in production. YouTube is using generative AI to create personalized video thumbnails for its creators. Walmart uses generative AI to create personalized video ads for its customers. Some use cases are even more compelling. For example, ALICE Receptionist is a company that provides a virtual receptionist service for businesses. They use generative AI to create videos of multilingual customer support agents that can greet and assist visitors in different languages. Ran is a sports broadcasting company that covers various sports events and leagues. They use generative AI to create sports coverage with virtual anchors that can commentate and analyze the games in real time. Some of the key use cases for video generation are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p147"> <em>Marketing content</em><em> </em>—Generative AI can be used to create marketing videos that are more personalized and targeted, such as videos that promote a product to a specific audience based on their interests. </li> 
   <li class="readable-text" id="p148"> <em>Entertainment content</em><em> </em>—Generative AI can be used to create entertainment videos that are more creative and innovative. For example, it is possible to create videos that help enhance a movie or TV program, tell a story, or play a game. </li> 
   <li class="readable-text" id="p149"> <em>Educational content</em><em> </em>—Generative AI can be used to create educational videos that are more engaging and interactive than traditional ones. For example, a generative AI model could be used to create a video that explains a complex concept by using animation and narration and can be used in the context of the difficulty level of the student. </li> 
   <li class="readable-text" id="p150"> <em>Synthetic data</em><em> </em>—Generative AI is capable of generating data that is not real (i.e., synthetic data) and that can be used as the input training data for other ML model creation. This is helpful in scenarios where the real data is impossible or impractical. For example, NVIDIA uses generative AI to create synthetic training data for its self-driving cars, allowing them to obtain data on various edge cases. Disney is using synthetic data to develop new ride and attraction concepts and optimize the layout of its theme parks, which allows it to use synthetic data to test and refine new products and services before releasing them to the public. </li> 
  </ul> 
  <div class="readable-text" id="p151"> 
   <p>Some of the most common methods that allow this video generation are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p152"> <em>Text-to-video synthesis</em><em> </em>—This method follows the paradigm we have seen so far: generating a video using a prompt. Like image generation, the model learns to associate words and phrases with visual concepts and then uses this knowledge to create a video that matches the text description. </li> 
   <li class="readable-text" id="p153"> <em>Image-to-video synthesis</em><em> </em>—This method generates a video from a source image instead of a prompt. The model learns to associate image features with visual concepts and then uses this knowledge to create a video that matches the image. </li> 
   <li class="readable-text" id="p154"> <em>Video-to-video synthesis</em><em> </em>—Similar to the earlier method, this method uses a source video to create a new video. The model learns to identify the underlying structure of the original video and then uses this knowledge to create a new video with the same structure but different content. </li> 
   <li class="readable-text" id="p155"> <em>GAN-based video generation</em><em> </em>—This method uses a generative adversarial network (GAN) to create a video. </li> 
  </ul> 
  <div class="readable-text" id="p156"> 
   <p>Several AI video generators are available that can help you easily create videos. Here are some examples of AI video generators that use generative AI:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p157"> <em>Sora</em><em> </em>—A diffusion model that differs from usual video generation methods that directly predict each frame. OpenAI announced this new AI model to make realistic and creative video scenes from text instructions. Sora begins with a basic static noise pattern and slowly changes it into a detailed video, frame by frame. It starts with noisy video frames. Each step removes noise to produce fine details. This process ensures the videos are visually pleasing and contextually correct based on the input text. When Sora was published, it was not given access by Open AI. </li> 
   <li class="readable-text" id="p158"> <em>Pictory</em><em> </em>—An AI-powered video creation tool that allows users to create videos from text, images, and videos. It offers various features for editing and customizing videos, such as adding captions, transitions, and music. Pictory can also help summarize long videos into shorter ones. </li> 
   <li class="readable-text" id="p159"> <em>Synthesia</em><em> </em>—A cloud-based platform that allows users to create videos with AI-generated presenters. Users can choose from various avatars and voices and add text, images, and gestures to their videos. </li> 
   <li class="readable-text" id="p160"> <em>NVIDIA Canvas</em><em> </em>—A cloud-based AI tool that allows users to create realistic paintings from text descriptions. It uses a GAN-based approach to generate paintings and can be used to create paintings of various subjects. </li> 
   <li class="readable-text" id="p161"> <em>Meta Make-a-Video</em><em> </em>—A generative AI system that can create videos from text or image inputs. It uses many text-image pairs and unlabeled videos to learn how to generate realistic and diverse videos that match the given prompts. It can also create variations of existing videos or add motion to static images. </li> 
   <li class="readable-text" id="p162"> <em>Viddyoze</em><em> </em>—A desktop application that allows users to create videos from text, images, and audio. Viddyoze uses various AI techniques to generate realistic videos, giving users more control over the creative process, including features such as transitions, effects, graphics, and so forth. </li> 
   <li class="readable-text" id="p163"> <em>Powtoon</em><em> </em>—A cloud-based platform that allows users to create videos from text, images, and audio. It uses various AI techniques to generate realistic videos using a variety of templates and features that can be used to create videos for different purposes. </li> 
   <li class="readable-text" id="p164"> <em>Dream</em><em> </em>—An app by WOMBO that uses AI to generate images and videos based on a user’s input of a keyword or phrase. Wombo Dream will generate a creative and visually appealing image or video. </li> 
   <li class="readable-text" id="p165"> <em>Wochit</em><em> </em>—A cloud-based platform that allows users to create videos from text, images, and videos. It focuses on making the process as collaborative as possible. Wochit allows users to work together to create videos and offers various features for sharing and distributing videos. </li> 
  </ul> 
  <div class="readable-text" id="p166"> 
   <p>Some of these tools make it very simple to interact with and edit via a GUI before generating a video. Figure 5.15 shows that by using Wochit, we can edit scenes, including music being used, the look and feel of text, and any other elements in a generated video. In our example, we use the following prompt: </p> 
  </div> 
  <div class="readable-text prompt" id="p167"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Prompt.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Top 5 places I should visit when on a trip to Seattle. </p> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>The video generated can be found in the books accompanying the GitHub repository at <a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p169">  
   <img alt="figure" src="../Images/CH05_F15_Bahree.png" width="1100" height="447"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.15</span> Wochit AI video generation</h5>
  </div> 
  <div class="readable-text" id="p170"> 
   <p>These are just a few examples of how generative AI is used to create videos. Now let’s explore music generation.</p> 
  </div> 
  <div class="readable-text" id="p171"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_99"><span class="num-string">5.5</span> Audio and music generation</h2> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>If we thought video generation was in its infancy, in the context of enterprises, audio and music generation is much earlier in its lifecycle. Generative AI can generate audio, speech, music, or sound effects. Audio and music generation share many of the same AI methods, such as autoregressive models, GANs, and transformer models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>Although audio and music generation is a very new area, some of the potential applications of generative AI audio generation are quite interesting for enterprises to explore:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p174"> Generating realistic sound effects for entertainment, such as movies and video games </li> 
   <li class="readable-text" id="p175"> Creating personalized audio experiences for users </li> 
   <li class="readable-text" id="p176"> Generating music for movies, video games, and other media </li> 
   <li class="readable-text" id="p177"> Improving the quality of speech recognition and translation systems </li> 
   <li class="readable-text" id="p178"> Developing new ways to communicate with computers, either by using new modalities or helping differently abled people </li> 
  </ul> 
  <div class="readable-text" id="p179"> 
   <p>Some of the examples of generative AI tools for music and audio are</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p180"> <em>OpenAI's Jukebox</em><em> </em>—Jukebox is a generative AI model that can create music in various classical, jazz, and pop styles. It has been trained on a massive music dataset, and it can generate new music indistinguishable from human-created music. This builds on OpenAI’s work for MuseNet; for more details on Jukebox, visit <a href="https://openai.com/research/jukebox">https://openai.com/research/jukebox</a>. </li> 
   <li class="readable-text" id="p181"> <em>OpenAI's MuseNet</em><em> </em>—MuseNet is another generative AI model that can create music in various styles. It has been trained on a dataset of over 1.5 million songs, and it can generate new music that is both creative and original. </li> 
   <li class="readable-text" id="p182"> <em>Meta's AudioCraft</em><em> </em>—AudioCraft is a generative AI tool that can create music from text prompts. It has been trained on a dataset of over 20,000 hours of music and can generate music tailored to the specific text prompt. </li> 
   <li class="readable-text" id="p183"> <em>NVIDIA’s Vocoder</em><em> </em>—Vocoder is a generative AI tool that can generate realistic speech from text prompts. It has been trained on a dataset of human speech, and it can generate natural and intelligible speech. </li> 
   <li class="readable-text" id="p184"> <em>Google MusicLM</em><em> </em>—This language model was created by Google to generate music compositions based on text prompts. This is an experimental tool that, at the time of publication, was only available as part of Google’s AI Test Kitchen program, which essentially is a playground for Google and its customers to try things out (<a href="https://mng.bz/0MmJ">https://mng.bz/0MmJ</a>). </li> 
   <li class="readable-text" id="p185"> <em>MusicGen</em><em> </em>—This language model uses prompts to create and generate music based on the provided prompt. Meta developed it as part of their AudioCraft research project, and it is an open source tool that anyone can use to create their music using Hugging Face Spaces. You can hear demos and read more details at <a href="https://ai.honu.io/papers/musicgen/">https://ai.honu.io/papers/musicgen/</a>. </li> 
   <li class="readable-text" id="p186"> <em>Riffusion</em><em> </em>—This audio and music generation library works with stable diffusion. It essentially is a fine-tuned version of stable diffusion, where instead of images, the library creates images of spectrograms; these spectrograms can then be converted into audio clips. Riffusion supports different styles of music generation, such as funk, jazz, and so forth. More details can be found at <a href="https://about.riffusion.com/">https://about.riffusion.com/</a>. </li> 
   <li class="readable-text" id="p187"> <em>Mo</em><em>û</em><em>sai</em><em> </em>—This text-to-music generation system uses diffusion models to create high-quality music using prompts. It has two sets of diffusion models—one for generating melody and harmony and the second for generating the timbre and dynamics. Combining them allows us to handle complex musical notes and helps generate music in various genres and styles. More info is available at <a href="https://mng.bz/j04a">https://mng.bz/j04a</a>. </li> 
  </ul> 
  <div class="readable-text" id="p188"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_100">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p189"> Generative AI allows us to generate code snippets and functions using a prompt. </li> 
   <li class="readable-text" id="p190"> Code generation is influenced by the context of the software solution, including the libraries being used, programming languages, code, and design patterns implemented. </li> 
   <li class="readable-text" id="p191"> Generative AI can also generate other software development lifecycle artifacts such as code understanding and documentation, testing code, and code refactoring. </li> 
   <li class="readable-text" id="p192"> Code generation can help enterprises by augmenting developers, improving productivity, onboarding new employees, automating repetitive tasks, and fostering creativity. </li> 
   <li class="readable-text" id="p193"> GitHub Copilot and Copilot Chat are the leading tools enterprises use and give a big productivity boost. </li> 
   <li class="readable-text" id="p194"> There are additional code generation tools and open source models, such as AWS’s CodeWhisperer, Tabine, and Code Lama, as examples that are also available to enterprises. </li> 
   <li class="readable-text" id="p195"> Video generation is in its infancy, but several AI video generation tools, such as Pictory and Synethica, let enterprises use them. </li> 
   <li class="readable-text" id="p196"> Similarly, audio and sound generation are still early in their development, but many tools and associated models, such as Jukebox, MuseNet, and AudioCraft, are available to enterprises. </li> 
  </ul>
 </div></div></body></html>