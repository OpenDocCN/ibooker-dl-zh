- en: 12 Blending gradient boosting and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: A review of the end-to-end gradient boosting example from chapter 7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparison of the results of the gradient boosting example from chapter 7
    with a deep learning solution for the same problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of ensembling a gradient boosted model with a deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 7, we did an in-depth exploration of an end-to-end example of using
    gradient boosting. We explored a dataset of Airbnb listings for Tokyo, we engineered
    features suitable for a pricing regression task, and then we created a baseline
    model trained on this dataset to predict prices. Finally, applying the techniques
    we had learned in the book up to that point, we optimized an XGBoost model trained
    on this dataset and examined some approaches to explain the behavior of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we evaluate if using deep learning would have led to different
    results and performance on the same problem, determine what approach works best,
    and discover how to use and integrate the strengths and weaknesses of each method.
    To do this, we begin this chapter by revisiting the gradient boosting approach
    to the Airbnb Tokyo problem from chapter 7\. Next, we review some of the approaches
    we could take to apply deep learning to the same problem and share our chosen
    deep learning approach to this problem. Finally, we compare the two solutions
    and determine what we can learn from such a comparison when it comes to deciding
    whether to use gradient boosting or deep learning to tackle a tabular data problem,
    whether it’s a regression or classification problem. In addition to comparing
    the solutions according to core performance (such as how well each solution makes
    predictions and their inference time), we explore how the two solutions compare
    according to more business-oriented metrics, such as the cost of maintenance,
    clarity to business stakeholders, and stability postdeployment.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter pulls together themes that you have seen across this book, incorporating
    what we learned in chapter 7 about XGBoost and what we learned about how to approach
    tabular problems with deep learning in chapters 1 and 8.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 1, we reviewed some of the papers that compare tabular data applications
    of classical machine learning with applications of deep learning. In this chapter,
    we will see that the results of our comparison of XGBoost with deep learning align
    with the observations of one of the papers we cited in chapter 1\. In chapter
    8, we assessed a variety of deep learning approaches to working with tabular data.
    Now we will use what we learned there to help us select a deep learning approach
    that has the best chance of being competitive with the performance of XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: By combining the two main threads that we have explored in this book (classical
    machine learning approaches and deep learning approaches to solving tabular data
    problems), this chapter provides a summary of what we have covered so far in this
    book and guidance to set you up for success in your application of machine learning
    to tabular data. The code shown in the chapter is available at [https://mng.bz/vKPp](https://mng.bz/vKPp).
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Review of the gradient boosting solution from chapter 7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In chapter 7, we assembled a dataset for Airbnb listings in Tokyo, analyzed
    the key characteristics of this dataset, and created an XGBoost model to predict
    the price of a listing. Starting at the Inside Airbnb Network website ([http://insideairbnb.com/](http://insideairbnb.com/)),
    we downloaded the following files related to the city of Tokyo:'
  prefs: []
  type: TYPE_NORMAL
- en: '`listings.csv`, which contains the summary listings and other information about
    the Airbnb accommodations in Tokyo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`calendar.csv.gz`, a zipped file containing `calendar.csv`, a dataset containing
    occupancy and price information for a given year for each listing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recall that the dataset in `listings.csv` has the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`—This is a unique identifier for each listing on Airbnb. It is an `int64`
    data type, meaning it is a numerical ID representation. In other tables, it can
    be referred to as `listing_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`—The description of the Airbnb listing. It is of the `object` data type,
    which typically represents a string or text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host_id`—This is a unique identifier for each host on Airbnb. It is an `int64`
    data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host_name`—The name of the host who owns the listing. It is of the `object`
    data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neighbourhood_group`—This field represents the broader area or region the
    neighborhood belongs to. It is stored as a `float64` data type, but it is important
    to note that using a float data type to represent groups or categories is uncommon.
    In this case, the presence of float values indicates that the data for this field
    is entirely made up of missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neighbourhood`—The specific neighborhood where the listing is located. It
    is of the `object` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latitude`—The latitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`longitude`—The longitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`room_type`—The type of room or accommodation offered in the listing (e.g.,
    entire home/apartment, private room, shared room). It is of the `object` data
    type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`price`—The price per night to rent the listing. It is of the `int64` data
    type, representing an integer price value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minimum_nights`—The minimum number of nights that is required for booking
    the listing. It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_reviews`—The total number of reviews received by the listing. It
    is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_review`—The date of the last review received by the listing. It is of
    the `object` data type, which could represent date and time information, but it
    might require further parsing to be used effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reviews_per_month`—The average number of reviews per month for the listing.
    It is of the `float64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`calculated_host_listings_count`—The total number of listings the host has
    on Airbnb. It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`availability_365`—The number of days the listing is available for booking
    in a year (out of 365 days). It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_reviews_ltm`—The number of reviews received in the last 12 months.
    It is of the `int64` data type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`license`—The license number or information related to the listing. It is of
    the `object` data type, which typically represents a string or text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of the model we created in chapter 7 was to predict the price of a
    new listing. This is actually a more challenging problem for a deep learning model
    than the Airbnb NYC problem that we tackled in chapter 8, as we can see from the
    comparison in table 12.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.1 Airbnb NYC problem vs Airbnb Tokyo problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Airbnb NYC | Airbnb Tokyo |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rows in dataset | 48,000 | 10,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Columns in dataset | 18 | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| Target | Classification: predict whether the price is over or under the median
    price | Regression: predict price |'
  prefs: []
  type: TYPE_TB
- en: In fact, the Tokyo Airbnb dataset has fewer than 25% of the records of the NYC
    Airbnb dataset and over double the number of columns as the NYC Airbnb dataset.
    With fewer data points, you may need to rely more on domain expertise (hence the
    role of feature engineering). Having more columns implies there’s more risk of
    overfitting during training, and in any case, you have to deal with more complex
    relationships between features and the target variable itself.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, classical machine learning approaches can get better results
    on small datasets than deep learning. Techniques such as data augmentation can
    mitigate this downside of deep learning, but deep learning approaches will struggle
    with a dataset with fewer than tens of thousands of rows. Research on why deep
    learning requires more data is not complete, but the large number of parameters
    in deep learning architectures and the need for at least a certain amount of data
    for the model to generalize is identified as one of the reasons why deep learning
    struggles with problems having smaller datasets [see, for instance, “The Computational
    Limits of Deep Learning” by Thompson et al. ([https://arxiv.org/pdf/2007.05558.pdf](https://arxiv.org/pdf/2007.05558.pdf)),
    which also argues how computational efficiency is necessary for the approach to
    progress].
  prefs: []
  type: TYPE_NORMAL
- en: The smaller number of rows in the Tokyo Airbnb dataset hence certainly presents
    a challenge for the successful implementation of a deep learning solution. In
    addition, the larger number of columns in the dataset necessitates more extensive
    preprocessing and feature engineering to ensure that the resulting features are
    relevant and usable for modeling purposes. Since we have followed the same data
    preparation steps as in chapter 7, we should not have to worry about having to
    deal with more columns for this particular application, but it is worth keeping
    in mind for other datasets that more columns equate to coming up with effective
    strategies for handling missing values, dealing with multicollinearity (as we
    discussed in chapter 2), and selecting the most informative features. In addition
    to requiring more data preparation, having more columns opens the possibility
    of having redundant or noisy features that could reduce the effectiveness of a
    deep learning solution on this dataset, although deep learning models, given their
    capability to capture complex relationships between features and the target variable,
    tend to be more robust to noise in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Further, the problem we are tackling with the Tokyo Airbnb dataset is a regression
    (predicting the price of a listing) as opposed to a binary classification (predicting
    whether a given listing has a price above or below the median price) as we attempted
    to solve with the NYC Airbnb dataset, which has different business implications.
    If the goal is to provide a solution that returns business benefits, our solution
    should predict correctly as often as possible. With a binary classification problem,
    the solution is discrete (one class or the other), and the likelihood of the solution
    making a prediction that is correct from a business perspective is higher than
    it appears for a regression problem, where the business expectation is for the
    model to predict a price that closely matches the actual price but the outputs
    are continuous values that can be significantly different from the expected values.
    In short, a binary classification problem looks easier (from the point of view
    of satisfying the business need)—since it has a clear threshold for correctness—than
    a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 7, after completing a set of transformations on the Airbnb Tokyo
    dataset, we end up with the following set of features for the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: All these features are numeric, and we also properly dealt with any missing
    values to be able to work with a linear model baseline. In fact, we started by
    creating a linear regression model to act as a baseline and give us a measuring
    stick against which to compare further improvements to the XGBoost model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing your data for linear regression or logistic regression (depending
    on whether it’s a regression or classification problem) automatically prepares
    your model for being processed by a neural network as well. However, while this
    is convenient, it might miss out on some specific preparations suitable only for
    neural networks. For instance, categorical features are commonly dealt with using
    one-hot-encoding in a linear model, whereas with a neural network, you can employ
    an encoding layer to convert categorical values into numeric ones directly during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: After getting the baseline results for linear regression, we conducted a set
    of optimizations on the XGBoost model. We use that optimized XGBoost code as the
    basis for the gradient boosted solution to the Tokyo Airbnb problem, which we
    will use for the comparison with deep learning in this chapter. Listing 12.1 shows
    the XGBoost code that we will use to compare with a deep learning solution. This
    version of the XGBoost code is very close to the final XGBoost code used in chapter
    7\. The hyperparameters match the best hyperparameters from the notebook used
    in chapter 7; [https://mng.bz/4a6R](https://mng.bz/4a6R)). In the version of the
    code used in this chapter, the predictions are saved in the `xgb_oof_preds` array
    so that they can be further processed or used in conjunction with the predictions
    we are going to obtain from a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.1 Code for training the final XGBoost model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports required libraries
  prefs: []
  type: TYPE_NORMAL
- en: ③ Sets up an XGBoost regressor with the specified hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines cross-validation splits based on the neighbourhood_more_than_30 feature
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Generates cross-validation splits based on the neighbourhood_more_than_30
    feature
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Performs cross-validated predictions
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Calculates R-squared, root mean squared error, and mean absolute error evaluation
    metrics to assess the model’s performance
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Prints mean values for R-squared, root mean squared error, and mean absolute
    error
  prefs: []
  type: TYPE_NORMAL
- en: The optimizations performed on the XGBoost solution produce results that are
    significantly better than the linear regression baseline, as summarized in table
    12.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.2 Summary of the results from chapter 7
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric | Linear regression baseline | Optimized XGBoost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| R-squared | 0.320 | 0.729 |'
  prefs: []
  type: TYPE_TB
- en: '| Root mean squared error | 17197.323 | 10853.661 |'
  prefs: []
  type: TYPE_TB
- en: '| Mean absolute error | 12568.371 | 6611.609 |'
  prefs: []
  type: TYPE_TB
- en: As a further check, you can verify that for all three of the metrics we tracked
    in chapter 7—that is, R-squared, root mean squared error (RMSE), and mean absolute
    error (MAE)—the optimized XGBoost model is definitely always an improvement on
    the linear regression baseline. Now, using the XGBoost model as our reference,
    we will explore what kind of results we can get with the same dataset using a
    deep learning model in the remainder of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Selecting a deep learning solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 8, we reviewed a set of different deep learning stacks for working
    with tabular data, including Keras, fastai, and other libraries specifically designed
    for tabular data, like TabNet. If we now want to compare the XGBoost solution
    to the Tokyo Airbnb problem from chapter 7, which deep learning approach should
    we use?
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, we shared a comparison of the different deep learning approaches
    using the NYC Airbnb dataset, which is shown again in table 12.3.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.3 Comparison of deep learning options
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Keras | fastai | Tabular data library (e.g., TabNet) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pro | Model details are transparent.A large community using the framework
    means that it’s easy to find solutions to common problems. | The framework includes
    explicit support for tabular data models, which means the code will be more compact.
    It also sets intelligent defaults so we can quickly reach reasonable results.
    | A bespoke library explicitly created to handle tabular datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Con | No built-in support for tabular data | If we run into a problem, we
    could be on our own getting a resolution because the community is smaller than
    the Keras one. | Up to now, no library has emerged as an obvious choice; the fragmented
    community and inconsistent maintenance of some libraries make it a challenge to
    get the basic code to run reliably. |'
  prefs: []
  type: TYPE_TB
- en: The differences between the dataset we used in chapter 8 and the dataset we
    will use now for the comparison between XGBoost and deep learning—that is, a dataset
    that was previously both much larger (with four times as many rows as the Tokyo
    Airbnb dataset) and much simpler (with less than half as many columns as the Tokyo
    Airbnb dataset)—for our comparison with XGBoost do not significantly weight in
    favor of any of the proposed solutions. In fact, Keras and fastai are general-purpose
    deep learning frameworks and are not specifically designed for small or complex
    datasets. TabNet’s design gives it an edge with high-dimensional data, but when
    applied to smaller datasets, the advantage over Keras or fastai turns out to be
    less significant.
  prefs: []
  type: TYPE_NORMAL
- en: What actually weighs more in our choice is the fact that we want a fair comparison
    between XGBoost and a deep learning approach. As you saw in chapter 7, XGBoost
    shines with its ease of use, and it gets good results without a lot of tweaking.
    If we decide on a complex deep learning model that takes a long time to set up,
    tweak, and optimize, it wouldn’t turn into a fair comparison for the deep learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Given this, which deep learning framework should we choose? We will pass over
    TabNet because of the complexity of getting it to run reliably. That leaves us
    with a choice between Keras and fastai. As mentioned in table 12.3, Keras is indeed
    popular in production and boasts a larger community. However, fastai aligns better
    with our goals. Recall that in chapter 8 we noted that fastai is built for tabular
    data like ours, and it comes with smart defaults. This means you can get decent
    results quickly, without spending ages on optimizations. fastai handles a lot
    of the nitty-gritty stuff and details for you behind the scenes. As you’ll see
    later, choosing fastai for this problem paid off. For the moment, we believe it
    provides a strong deep learning solution for the Tokyo Airbnb problem without
    much hassle, ready in a few steps to take on the XGBoost model from chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Selected deep learning solution to the Tokyo Airbnb problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far in this chapter we have reviewed the XGBoost solution to the Tokyo Airbnb
    problem, reviewed options for a deep learning solution to compare it with, and
    selected fastai as the deep learning framework to use for the comparison with
    XGBoost. In this section, we’ll go through the details of the fastai solution
    for the Tokyo Airbnb problem.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 shows the core of the fastai model that we used to compare with
    the XGBoost solution. This code trains a fastai regression model on the Tokyo
    Airbnb dataset using the `TabularPandas` function ([https://mng.bz/QDR6](https://mng.bz/QDR6)),
    which is a wrapper providing all the necessary transformations under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.2 fastai model for the Tokyo Airbnb problem
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a fastai TabularPandas object
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines a dataloaders object based on the TabularPandas object
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a tabular_learner object based on the dataloaders object
  prefs: []
  type: TYPE_NORMAL
- en: ④ Trains the model
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Gets predictions from the model on the test set
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Saves the metrics
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the code in listing 12.2, the solution for fastai is straightforward.
    First, we define the preprocessing steps (`procs`), such as filling missing values,
    normalization, and categorization. Then we separate categorical and continuous
    variables from the dataset and choose the dependent variable (`dep_var`). After
    that, we iterate through a stratified k-fold cross-validation in the same manner
    as we did for the XGBoost solution.
  prefs: []
  type: TYPE_NORMAL
- en: During the iterations, the training data is preprocessed using `TabularPandas`,
    specifying categorical and continuous variables, the target variable, and data
    splits. DataLoader objects (`dls`) are created for training and validation batches.
    After defining a neural network model (`tabular_learner`) consisting of two layers,
    the first with 1,000 neurons and the following with 500 nodes, and their dropout
    rates (using `tabular_config` and setting a higher dropout for the last layer),
    we train the model using the learning rate found by `lr_find` and using the one
    cycle (`fit_one_cycle`) procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining the `lr_find` and `fit_one_cycle` procedures, we automatically
    tune the learning rate parameter for the best results on the type of data we are
    processing, thus achieving a straightforward solution without much tweaking and
    experimentation. The procedure `lr_find` ([https://mng.bz/Xxq9](https://mng.bz/Xxq9))
    explores a range of learning rates on a sample of the data, stopping when the
    learning rate is so high that the learning diverges. While the procedure takes
    some time, it is relatively fast and returns the value of the learning parameter
    that is two-thirds of the way along the section of the loss curve where the loss
    is decreasing. We use that value as the upper boundary of another procedure, the
    `fit_one_cycle` ([https://mng.bz/yWZp](https://mng.bz/yWZp)), which is a training
    method where the learning rate is not fixed or constantly decreasing but rather
    oscillates between a minimum and a maximum value. The oscillations allow for the
    network to not get stuck in a local minima, and overall the resulting network
    performs better than using other approaches, especially when dealing with tabular
    data. Both methods have been developed by Leslie Smith in a series of papers:'
  prefs: []
  type: TYPE_NORMAL
- en: “Cyclical Learning Rates for Training Neural Networks” ([https://arxiv.org/abs/1506.01186](https://arxiv.org/abs/1506.01186))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Super-Convergence: Very Fast Training of Neural Networks Using Large Learning
    Rates” ([https://arxiv.org/abs/1708.07120](https://arxiv.org/abs/1708.07120))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning
    Rate, Batch Size, Momentum, and Weight Decay” ([https://arxiv.org/abs/1803.09820](https://arxiv.org/abs/1803.09820))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To our knowledge, the implementation by fastai of these methods is the most
    efficient and high-performing available in the open-source community.
  prefs: []
  type: TYPE_NORMAL
- en: Proceeding with the code, for each pass through the `for` loop, the predictions
    for the current fold and the R-squared, RMSE, and MAE evaluations are saved. The
    mean values for all the metrics are printed after the loop so we can get an idea
    of the overall values for the fastai solution. Note that we will recalculate these
    values as we go through the ensembling process when we compare the predictions
    and actual y values for an ensemble that is 100% fastai results.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Comparing the XGBoost and fastai solutions to the Tokyo Airbnb problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a deep learning solution for the Tokyo Airbnb problem, we can
    compare its results against the XGBoost solution. By comparing the metrics that
    we collected for both solutions (R-squared, RMSE, and MAE), we can get an idea
    of how effective each solution is at solving the Tokyo Airbnb problem. Table 12.4
    contains a summary of the results of both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12.4 Comparison of results from XGBoost and fastai models
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric | XGBoost | Fastai |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| R-squared | 0.599 | 0.572 |'
  prefs: []
  type: TYPE_TB
- en: '| RMSE | 10783.027 | 11719.387 |'
  prefs: []
  type: TYPE_TB
- en: '| MAE | 6531.102 | 7152.143 |'
  prefs: []
  type: TYPE_TB
- en: Table 12.4 demonstrates that XGBoost provides better results than fastai for
    all three error metrics—in fact, the R-squared value for XGBoost is significantly
    higher, and its RMSE and MAE values are about 8% to 9% lower.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the basic comparison of error metrics in table 12.4, we can visualize
    some differences relative to how the two approaches deal with the problem. For
    example, we can directly compare XGBoost and fastai predictions by examining each
    predicted data point in the Tokyo Airbnb test set. In figure 12.1, the x-axis
    represents the prediction from XGBoost, while the y-axis represents the prediction
    from fastai for each respective data point. The figure shows the relationship
    between XGBoost and fastai predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 Scatterplot of predictions for XGBoost and fastai
  prefs: []
  type: TYPE_NORMAL
- en: On the diagonal of the chart, the solid trend line coincides fairly well with
    the dashed diagonal line, showing that, overall, there is not a huge variance
    in the predictions between XGBoost and fastai. The solid trend line, showing the
    smoothed regression line (a technique called LOWESS [LOcally WEighted Scatterplot
    Smoothing]) between the predictions from XGBoost and fastai, doesn’t deviate significantly
    in comparison from the dashed line, confirming that, even if the algorithms tend
    to disagree in their predictions, there is no systematic over- or underestimation
    from one of the two in respect of the other.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we can also try to explore how their predictions relate, using
    the value of 70,000 on the x-axis as a pivot, since we can spot two distinct clusters
    of predictions. We observe that the average fastai prediction is around 81,300
    against a XGBoost average at 81,550 for XGBoost predictions over 70,000 and around
    23,050 against 22,500 for XGBoost predictions below 70,000.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.3 Average fastai predictions for XGBoost predictions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Gets a pandas DataFrame with a column each for XGBoost and fastai predictions
  prefs: []
  type: TYPE_NORMAL
- en: ② Computes mean statistics for XGBoost and fastai predictions depending on XGBoost
    predicted value
  prefs: []
  type: TYPE_NORMAL
- en: The differences between the two models are minimal; on average, fastai and XGBoost
    predictions tend to be aligned. Fastai tends to overestimate for lower predicted
    pricing levels and slightly underestimate for higher predicted pricing levels.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to figure 12.1, now that we have examined the chart comparing the
    predictions, let’s look at a chart comparing the error for XGBoost with the error
    for fastai. For each data point in the Tokyo Airbnb test set, the plot in figure
    12.2 shows the error for that data point (the absolute value of the difference
    between the prediction and the actual value), with the x value being the XGBoost
    error and the y value being the fastai error.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 shows that there is a large cluster of data points where the error
    for both XGBoost and fastai is lower than 20,000\. The overall LOWESS line, in
    red, shows that the error for XGBoost is overall lower than for fastai most of
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: As shown by the aggregate error metrics and the plots for predictions and errors,
    the XGBoost model displays better performance than the fastai model. However,
    given the spread of the predictions between the two models, we believe that there
    is an opportunity to do even better than XGBoost alone by ensembling the two models
    because there is a strong hint that, apart from the differences in performance,
    the two models operate differently in their predictions by capturing different
    data patterns and characteristics. In the next section, we will find out whether
    ensembling improves the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 Scatterplot of errors for XGBoost and fastai
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Ensembling the two solutions to the Tokyo Airbnb problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have established the performance of the XGBoost and fastai solutions
    to the Tokyo Airbnb problem in isolation, we will look at ensembling the two solutions
    to see whether a combination of the two approaches provides any improvements.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the loop where we ensemble the results of the two
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 12.4 Code to ensemble the two models
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Ensembles ratios to iterate through
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates predictions that are blended according to the ensembling ratios
  prefs: []
  type: TYPE_NORMAL
- en: ③ Gets R-squared, RMSE, and MAE for the blended predictions
  prefs: []
  type: TYPE_NORMAL
- en: The code in listing 12.4 combines results from XGBoost and fastai according
    to the blending values in `blend_list`. Note that these blending values are not
    optimized to find the absolute optimum—we are simply using a set of fixed blending
    values to get a general sense of the effect of blending results. Also, note that
    we are evaluating the results using out-of-fold predictions. Nevertheless, by
    combining the predictions from XGBoost and fastai according to the proportions
    specified by `blend_list`, we can see the effect of ensembling the two approaches
    across a range of values.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the blending code for a typical run is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: These figures may be a bit difficult to interpret, so let’s see what they look
    like in the form of a chart. Figure 12.3 shows the results for R-squared, RMSE,
    and MAE for a range of blending between the XGBoost and fastai models.
  prefs: []
  type: TYPE_NORMAL
- en: As figure 12.3 shows, we get the optimal results for R-squared evaluation when
    we use a 50/50 blend of the predictions from the XGBoost and fastai models. For
    error-based measures, RMSE and MAE, we obtain a better result weighting more XGBoost
    than fastai; however, if we were to use a 50/50 blend, we would obtain only slightly
    worse scores. The worst results are when we use 100% of prediction from fastai,
    as we would expect from the results we got from looking at each model in isolation,
    but it is interesting to notice that using only XGBoost is always worse than blending
    it with the other solution using a 75/25 or 50/50 share.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH12_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 Results of blending XGBoost and fastai models
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling the XGBoost and fastai models gets better results than using either
    model in isolation. As we will see in the next section, our observations from
    ensembling are consistent with the results shared in an important research paper
    that compared classical machine learning approaches and deep learning on tabular
    data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6 Overall comparison of gradient boosting and deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In chapter 1, we introduced the controversy over whether deep learning is needed
    to solve problems involving tabular data. We cited academic papers that support
    both sides of the argument—those that advocate for deep learning approaches and
    those that maintain that classical machine learning approaches, in particular
    gradient boosting, consistently outperform deep learning. One of the papers that
    we mentioned in chapter 1 is worth revisiting here: “Tabular Data: Deep Learning
    Is Not All You Need,” by Ravid Shwartz-Ziv and Amitai Armon ([https://arxiv.org/abs/2106.03253](https://arxiv.org/abs/2106.03253)).
    In the Discussion and Conclusions section of this paper, the authors make the
    following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: In our analysis, the deep models were weaker on datasets that did not appear
    in their original papers, and they were weaker than XGBoost, the baseline model.
    Therefore, we proposed using an ensemble of these deep models with XGBoost. This
    ensemble performed better on these datasets than any individual model and the
    'non-deep’ classical ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Their observation is not a hard and fast rule, because, in our experience, we
    encountered situations where using a gradient boosting solution or a deep learning
    one alone resulted in the best performances. However, in many of the situations
    we faced, we can confirm that simply averaging solutions resulted in better predictions.
    We are strongly convinced that this happened because the two algorithms have two
    different ways of optimizing predictions. Gradient boosting is based on decision
    trees, which are a form of analogy search because, as an algorithm, trees split
    your dataset into parts where the features tend to be similar in values between
    themselves and map to similar target outputs. The gradient part of the algorithm
    intelligently ensembles multiple trees together for a better prediction, although
    it doesn’t change the basic way in which decision trees behave. Deep learning,
    on the other hand, is purely based on the principle of differentiation and nonlinear
    transformations, where the algorithm looks for the best weights to combine the
    transformed inputs.
  prefs: []
  type: TYPE_NORMAL
- en: These two different approaches result in quite different estimations whose errors
    tend to partially cancel each other out because they are strongly uncorrelated,
    in a similar fashion to what actually happens in a random forest algorithm when
    you average the results of uncorrelated decision trees. This conclusion matches
    our experience comparing the results of gradient boosting and deep learning on
    the Tokyo Airbnb problem. The ensemble of the XGBoost model with the fastai model
    produced the best results, as shown in figure 12.3.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The XGBoost solution to the Tokyo Airbnb problem shown in chapter 7 provides
    a baseline that we can use to assess the efficacy of deep learning to solve the
    same problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the XGBoost solution from chapter 7 as a starting point, we can create
    a deep learning solution for the Tokyo Airbnb problem. The fastai library provides
    a compact and relatively well-performing deep learning solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By blending the predictions of the XGBoost and fastai models across a range
    of ratios, from 100% XGBoost to 100% fastai, we can see the effect of ensembling
    the models. We get the optimal results with a 50/50 ensemble of the two models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This result matches the recommendation from research focused on scrutinizing
    the claims of the efficacy of deep learning on tabular data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
