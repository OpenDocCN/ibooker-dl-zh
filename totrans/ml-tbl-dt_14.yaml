- en: 12 Blending gradient boosting and deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 混合梯度提升和深度学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A review of the end-to-end gradient boosting example from chapter 7
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对第7章中端到端梯度提升示例的回顾
- en: A comparison of the results of the gradient boosting example from chapter 7
    with a deep learning solution for the same problem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将第7章中梯度提升示例的结果与同一问题的深度学习解决方案进行比较
- en: The result of ensembling a gradient boosted model with a deep learning model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将梯度提升模型与深度学习模型集成后的结果
- en: In chapter 7, we did an in-depth exploration of an end-to-end example of using
    gradient boosting. We explored a dataset of Airbnb listings for Tokyo, we engineered
    features suitable for a pricing regression task, and then we created a baseline
    model trained on this dataset to predict prices. Finally, applying the techniques
    we had learned in the book up to that point, we optimized an XGBoost model trained
    on this dataset and examined some approaches to explain the behavior of the model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们对使用梯度提升的端到端示例进行了深入探索。我们探索了东京Airbnb列表的数据集，我们为定价回归任务创建了合适的特征，然后我们创建了一个基于此数据集的基线模型来预测价格。最后，应用我们在书中学到的方法，我们优化了基于此数据集训练的XGBoost模型，并检查了一些解释模型行为的方法。
- en: In this chapter, we evaluate if using deep learning would have led to different
    results and performance on the same problem, determine what approach works best,
    and discover how to use and integrate the strengths and weaknesses of each method.
    To do this, we begin this chapter by revisiting the gradient boosting approach
    to the Airbnb Tokyo problem from chapter 7\. Next, we review some of the approaches
    we could take to apply deep learning to the same problem and share our chosen
    deep learning approach to this problem. Finally, we compare the two solutions
    and determine what we can learn from such a comparison when it comes to deciding
    whether to use gradient boosting or deep learning to tackle a tabular data problem,
    whether it’s a regression or classification problem. In addition to comparing
    the solutions according to core performance (such as how well each solution makes
    predictions and their inference time), we explore how the two solutions compare
    according to more business-oriented metrics, such as the cost of maintenance,
    clarity to business stakeholders, and stability postdeployment.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们评估使用深度学习是否会导致同一问题上的不同结果和性能，确定哪种方法最有效，并发现如何使用和整合每种方法的优点和缺点。为此，我们首先回顾第7章中关于Airbnb东京问题的梯度提升方法。接下来，我们回顾一些我们可以采取的方法来将深度学习应用于同一问题，并分享我们选择的针对该问题的深度学习方法。最后，我们比较这两种解决方案，并确定在决定是否使用梯度提升或深度学习来解决表格数据问题时，我们可以从这种比较中学到什么，无论是回归问题还是分类问题。除了根据核心性能（例如每个解决方案的预测能力和推理时间）比较解决方案外，我们还探讨了两种解决方案在更多面向业务的指标（如维护成本、业务利益相关者的清晰度以及部署后的稳定性）方面的比较。
- en: This chapter pulls together themes that you have seen across this book, incorporating
    what we learned in chapter 7 about XGBoost and what we learned about how to approach
    tabular problems with deep learning in chapters 1 and 8.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将本书中你看到的一些主题汇总在一起，结合我们在第7章中学到的XGBoost知识，以及我们在第1章和第8章中学到的如何使用深度学习处理表格数据的方法。
- en: In chapter 1, we reviewed some of the papers that compare tabular data applications
    of classical machine learning with applications of deep learning. In this chapter,
    we will see that the results of our comparison of XGBoost with deep learning align
    with the observations of one of the papers we cited in chapter 1\. In chapter
    8, we assessed a variety of deep learning approaches to working with tabular data.
    Now we will use what we learned there to help us select a deep learning approach
    that has the best chance of being competitive with the performance of XGBoost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们回顾了一些比较经典机器学习在表格数据应用与深度学习应用的论文。在本章中，我们将看到我们比较XGBoost与深度学习的结果与我们在第1章中引用的一篇论文的观察结果一致。在第8章中，我们评估了多种处理表格数据的深度学习方法。现在，我们将使用在那里学到的知识来帮助我们选择一个与XGBoost性能具有最佳竞争力的深度学习方法。
- en: By combining the two main threads that we have explored in this book (classical
    machine learning approaches and deep learning approaches to solving tabular data
    problems), this chapter provides a summary of what we have covered so far in this
    book and guidance to set you up for success in your application of machine learning
    to tabular data. The code shown in the chapter is available at [https://mng.bz/vKPp](https://mng.bz/vKPp).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合本书中探讨的两个主要主题（经典机器学习方法和对解决表格数据问题的深度学习方法），本章总结了本书到目前为止所涵盖的内容，并为将机器学习应用于表格数据的应用提供了指导。本章中展示的代码可在[https://mng.bz/vKPp](https://mng.bz/vKPp)找到。
- en: 12.1 Review of the gradient boosting solution from chapter 7
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 第7章中梯度提升解决方案的回顾
- en: 'In chapter 7, we assembled a dataset for Airbnb listings in Tokyo, analyzed
    the key characteristics of this dataset, and created an XGBoost model to predict
    the price of a listing. Starting at the Inside Airbnb Network website ([http://insideairbnb.com/](http://insideairbnb.com/)),
    we downloaded the following files related to the city of Tokyo:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们为东京的Airbnb列表创建了一个数据集，分析了该数据集的关键特征，并创建了一个XGBoost模型来预测列表的价格。从Inside Airbnb
    Network网站([http://insideairbnb.com/](http://insideairbnb.com/))开始，我们下载了以下与东京市相关的文件：
- en: '`listings.csv`, which contains the summary listings and other information about
    the Airbnb accommodations in Tokyo'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`listings.csv`，其中包含东京Airbnb住宿的摘要列表和其他信息'
- en: '`calendar.csv.gz`, a zipped file containing `calendar.csv`, a dataset containing
    occupancy and price information for a given year for each listing'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calendar.csv.gz`，一个包含`calendar.csv`的压缩文件，该数据集包含每个列表在给定年份的占用和价格信息'
- en: 'Recall that the dataset in `listings.csv` has the following columns:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`listings.csv`数据集包含以下列：
- en: '`id`—This is a unique identifier for each listing on Airbnb. It is an `int64`
    data type, meaning it is a numerical ID representation. In other tables, it can
    be referred to as `listing_id`.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id`—这是Airbnb上每个列表的唯一标识符。它是一个`int64`数据类型，意味着它是一个数值ID表示。在其他表中，它可以被称为`listing_id`。'
- en: '`name`—The description of the Airbnb listing. It is of the `object` data type,
    which typically represents a string or text.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`—Airbnb列表的描述。它属于`object`数据类型，通常表示字符串或文本。'
- en: '`host_id`—This is a unique identifier for each host on Airbnb. It is an `int64`
    data type.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host_id`—这是Airbnb上每个房东的唯一标识符。它是一个`int64`数据类型。'
- en: '`host_name`—The name of the host who owns the listing. It is of the `object`
    data type.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host_name`—拥有列表的房东的姓名。它属于`object`数据类型。'
- en: '`neighbourhood_group`—This field represents the broader area or region the
    neighborhood belongs to. It is stored as a `float64` data type, but it is important
    to note that using a float data type to represent groups or categories is uncommon.
    In this case, the presence of float values indicates that the data for this field
    is entirely made up of missing values.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbourhood_group`—此字段表示该社区所属的更广泛的区域或地区。它存储为`float64`数据类型，但需要注意的是，使用浮点数据类型来表示组或类别是不常见的。在这种情况下，浮点值的存在表明该字段的全部数据都是缺失值。'
- en: '`neighbourhood`—The specific neighborhood where the listing is located. It
    is of the `object` data type.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`neighbourhood`—列表所在的具体社区。它属于`object`数据类型。'
- en: '`latitude`—The latitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latitude`—列表位置的纬度坐标。它属于`float64`数据类型。'
- en: '`longitude`—The longitude coordinates of the listing’s location. It is of the
    `float64` data type.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`longitude`—列表位置的经度坐标。它属于`float64`数据类型。'
- en: '`room_type`—The type of room or accommodation offered in the listing (e.g.,
    entire home/apartment, private room, shared room). It is of the `object` data
    type.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`room_type`—列表中提供的房间或住宿类型（例如，整个房屋/公寓、私人房间、共享房间）。它属于`object`数据类型。'
- en: '`price`—The price per night to rent the listing. It is of the `int64` data
    type, representing an integer price value.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`price`—租赁列表每晚的价格。它属于`int64`数据类型，表示一个整数值。'
- en: '`minimum_nights`—The minimum number of nights that is required for booking
    the listing. It is of the `int64` data type.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minimum_nights`—预订列表所需的最少夜晚数。它属于`int64`数据类型。'
- en: '`number_of_reviews`—The total number of reviews received by the listing. It
    is of the `int64` data type.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_of_reviews`—列表收到的总评论数。它属于`int64`数据类型。'
- en: '`last_review`—The date of the last review received by the listing. It is of
    the `object` data type, which could represent date and time information, but it
    might require further parsing to be used effectively.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_review`—列表收到的最后一条评论的日期。数据类型为`object`，可能代表日期和时间信息，但可能需要进一步解析才能有效使用。'
- en: '`reviews_per_month`—The average number of reviews per month for the listing.
    It is of the `float64` data type.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reviews_per_month`—列表每月的平均评论数量。数据类型为`float64`。'
- en: '`calculated_host_listings_count`—The total number of listings the host has
    on Airbnb. It is of the `int64` data type.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculated_host_listings_count`—房东在Airbnb上的列表总数。数据类型为`int64`。'
- en: '`availability_365`—The number of days the listing is available for booking
    in a year (out of 365 days). It is of the `int64` data type.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`availability_365`—列表一年中可供预订的天数（365天中的天数）。数据类型为`int64`。'
- en: '`number_of_reviews_ltm`—The number of reviews received in the last 12 months.
    It is of the `int64` data type.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_of_reviews_ltm`—过去12个月内收到的评论数量。数据类型为`int64`。'
- en: '`license`—The license number or information related to the listing. It is of
    the `object` data type, which typically represents a string or text.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`license`—列表的许可证号码或相关信息。数据类型为`object`，通常表示字符串或文本。'
- en: The goal of the model we created in chapter 7 was to predict the price of a
    new listing. This is actually a more challenging problem for a deep learning model
    than the Airbnb NYC problem that we tackled in chapter 8, as we can see from the
    comparison in table 12.1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章中我们创建的模型的目标是预测新列表的价格。这实际上比我们在第8章中解决的Airbnb纽约问题对深度学习模型更具挑战性，正如表12.1中的比较所示。
- en: Table 12.1 Airbnb NYC problem vs Airbnb Tokyo problem
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1 Airbnb纽约问题与Airbnb东京问题比较
- en: '|  | Airbnb NYC | Airbnb Tokyo |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | Airbnb纽约 | Airbnb东京 |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Rows in dataset | 48,000 | 10,000 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 数据集行数 | 48,000 | 10,000 |'
- en: '| Columns in dataset | 18 | 31 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 数据集列数 | 18 | 31 |'
- en: '| Target | Classification: predict whether the price is over or under the median
    price | Regression: predict price |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 分类：预测价格是否超过或低于中位数价格 | 回归：预测价格 |'
- en: In fact, the Tokyo Airbnb dataset has fewer than 25% of the records of the NYC
    Airbnb dataset and over double the number of columns as the NYC Airbnb dataset.
    With fewer data points, you may need to rely more on domain expertise (hence the
    role of feature engineering). Having more columns implies there’s more risk of
    overfitting during training, and in any case, you have to deal with more complex
    relationships between features and the target variable itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，东京Airbnb数据集的记录数少于纽约Airbnb数据集的25%，并且列数是纽约Airbnb数据集的两倍以上。数据点较少时，你可能需要更多地依赖领域专业知识（因此特征工程的作用）。列数较多意味着在训练过程中过拟合的风险更高，并且无论如何，你必须处理特征与目标变量本身之间更复杂的关系。
- en: Generally speaking, classical machine learning approaches can get better results
    on small datasets than deep learning. Techniques such as data augmentation can
    mitigate this downside of deep learning, but deep learning approaches will struggle
    with a dataset with fewer than tens of thousands of rows. Research on why deep
    learning requires more data is not complete, but the large number of parameters
    in deep learning architectures and the need for at least a certain amount of data
    for the model to generalize is identified as one of the reasons why deep learning
    struggles with problems having smaller datasets [see, for instance, “The Computational
    Limits of Deep Learning” by Thompson et al. ([https://arxiv.org/pdf/2007.05558.pdf](https://arxiv.org/pdf/2007.05558.pdf)),
    which also argues how computational efficiency is necessary for the approach to
    progress].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，经典机器学习方法在小数据集上可以获得比深度学习更好的结果。数据增强等技术可以减轻深度学习的这一缺点，但深度学习方法在行数少于数万的数据集上会面临挑战。关于为什么深度学习需要更多数据的研究尚不完整，但深度学习架构中的大量参数以及模型泛化至少需要一定数量的数据被认为是深度学习在处理小数据集问题时遇到困难的原因之一[例如，参见Thompson等人撰写的“深度学习的计算限制”（[https://arxiv.org/pdf/2007.05558.pdf](https://arxiv.org/pdf/2007.05558.pdf)），该文还论证了计算效率对于方法进步的必要性]。
- en: The smaller number of rows in the Tokyo Airbnb dataset hence certainly presents
    a challenge for the successful implementation of a deep learning solution. In
    addition, the larger number of columns in the dataset necessitates more extensive
    preprocessing and feature engineering to ensure that the resulting features are
    relevant and usable for modeling purposes. Since we have followed the same data
    preparation steps as in chapter 7, we should not have to worry about having to
    deal with more columns for this particular application, but it is worth keeping
    in mind for other datasets that more columns equate to coming up with effective
    strategies for handling missing values, dealing with multicollinearity (as we
    discussed in chapter 2), and selecting the most informative features. In addition
    to requiring more data preparation, having more columns opens the possibility
    of having redundant or noisy features that could reduce the effectiveness of a
    deep learning solution on this dataset, although deep learning models, given their
    capability to capture complex relationships between features and the target variable,
    tend to be more robust to noise in the data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 东京Airbnb数据集中行数较少，这无疑给深度学习解决方案的成功实施带来了挑战。此外，数据集中列数较多，需要更广泛的数据预处理和特征工程，以确保生成的特征对建模目的相关且可用。由于我们遵循了与第7章相同的数据准备步骤，我们不应该担心需要处理更多列的特定应用，但值得记住，对于其他数据集来说，更多的列意味着需要制定有效的策略来处理缺失值、处理多重共线性（如我们在第2章中讨论的）以及选择最有信息量的特征。除了需要更多的数据准备外，更多的列还可能导致冗余或噪声特征，这可能会降低深度学习解决方案在该数据集上的有效性，尽管深度学习模型由于其能够捕捉特征与目标变量之间复杂关系的能力，通常对数据中的噪声更具有鲁棒性。
- en: Further, the problem we are tackling with the Tokyo Airbnb dataset is a regression
    (predicting the price of a listing) as opposed to a binary classification (predicting
    whether a given listing has a price above or below the median price) as we attempted
    to solve with the NYC Airbnb dataset, which has different business implications.
    If the goal is to provide a solution that returns business benefits, our solution
    should predict correctly as often as possible. With a binary classification problem,
    the solution is discrete (one class or the other), and the likelihood of the solution
    making a prediction that is correct from a business perspective is higher than
    it appears for a regression problem, where the business expectation is for the
    model to predict a price that closely matches the actual price but the outputs
    are continuous values that can be significantly different from the expected values.
    In short, a binary classification problem looks easier (from the point of view
    of satisfying the business need)—since it has a clear threshold for correctness—than
    a regression problem.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们用东京Airbnb数据集解决的问题是回归（预测列表价格），而不是二分类（预测给定列表的价格是否高于或低于中位数价格），这是我们尝试用纽约Airbnb数据集解决的问题，这具有不同的商业影响。如果目标是提供能够带来商业效益的解决方案，我们的解决方案应该尽可能正确地预测。在二分类问题中，解决方案是离散的（一个类别或另一个类别），从商业角度来看，解决方案做出正确预测的可能性比回归问题中看起来要高，在回归问题中，商业期望模型预测的价格与实际价格非常接近，但输出是连续值，可能显著不同于预期值。简而言之，从满足商业需求的角度来看，二分类问题看起来比回归问题更容易——因为它有一个明确的正确性阈值。
- en: 'In chapter 7, after completing a set of transformations on the Airbnb Tokyo
    dataset, we end up with the following set of features for the dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，在完成对Airbnb东京数据集的一系列转换后，我们得到了以下一组数据集特征：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All these features are numeric, and we also properly dealt with any missing
    values to be able to work with a linear model baseline. In fact, we started by
    creating a linear regression model to act as a baseline and give us a measuring
    stick against which to compare further improvements to the XGBoost model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些特征都是数值型的，我们也妥善处理了任何缺失值，以便能够使用线性模型基线。事实上，我们首先创建了一个线性回归模型作为基线，并为我们提供了一个衡量标准，可以用来比较XGBoost模型的进一步改进。
- en: Preparing your data for linear regression or logistic regression (depending
    on whether it’s a regression or classification problem) automatically prepares
    your model for being processed by a neural network as well. However, while this
    is convenient, it might miss out on some specific preparations suitable only for
    neural networks. For instance, categorical features are commonly dealt with using
    one-hot-encoding in a linear model, whereas with a neural network, you can employ
    an encoding layer to convert categorical values into numeric ones directly during
    training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 准备你的数据以进行线性回归或逻辑回归（取决于是否是回归或分类问题）会自动使你的模型准备好由神经网络处理。然而，虽然这很方便，但它可能会错过一些仅适用于神经网络的特定准备。例如，在线性模型中，分类特征通常使用one-hot-encoding处理，而在神经网络中，你可以在训练过程中使用编码层直接将分类值转换为数值。
- en: After getting the baseline results for linear regression, we conducted a set
    of optimizations on the XGBoost model. We use that optimized XGBoost code as the
    basis for the gradient boosted solution to the Tokyo Airbnb problem, which we
    will use for the comparison with deep learning in this chapter. Listing 12.1 shows
    the XGBoost code that we will use to compare with a deep learning solution. This
    version of the XGBoost code is very close to the final XGBoost code used in chapter
    7\. The hyperparameters match the best hyperparameters from the notebook used
    in chapter 7; [https://mng.bz/4a6R](https://mng.bz/4a6R)). In the version of the
    code used in this chapter, the predictions are saved in the `xgb_oof_preds` array
    so that they can be further processed or used in conjunction with the predictions
    we are going to obtain from a deep learning model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得线性回归的基线结果后，我们对XGBoost模型进行了一系列优化。我们使用这个优化的XGBoost代码作为解决东京Airbnb问题的梯度提升解决方案的基础，我们将在本章中将它用于与深度学习的比较。列表12.1显示了我们将用于与深度学习解决方案比较的XGBoost代码。这个版本的XGBoost代码与第7章中使用的最终XGBoost代码非常接近。超参数与第7章中使用的笔记本的最佳超参数相匹配；[https://mng.bz/4a6R](https://mng.bz/4a6R))。在本章使用的代码版本中，预测结果保存在`xgb_oof_preds`数组中，以便进一步处理或与我们将从深度学习模型获得的预测一起使用。
- en: Listing 12.1 Code for training the final XGBoost model
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.1 训练最终XGBoost模型的代码
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Sets hyperparameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置超参数
- en: ② Imports required libraries
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ② 导入所需的库
- en: ③ Sets up an XGBoost regressor with the specified hyperparameters
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用指定的超参数设置XGBoost回归器
- en: ④ Defines cross-validation splits based on the neighbourhood_more_than_30 feature
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 基于neighbourhood_more_than_30特征定义交叉验证分割
- en: ⑤ Generates cross-validation splits based on the neighbourhood_more_than_30
    feature
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 基于neighbourhood_more_than_30特征生成交叉验证分割
- en: ⑥ Performs cross-validated predictions
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 执行交叉验证预测
- en: ⑦ Calculates R-squared, root mean squared error, and mean absolute error evaluation
    metrics to assess the model’s performance
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 计算R-squared、均方根误差和平均绝对误差评估指标以评估模型性能
- en: ⑧ Prints mean values for R-squared, root mean squared error, and mean absolute
    error
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 打印R-squared、均方根误差和平均绝对误差的均值
- en: The optimizations performed on the XGBoost solution produce results that are
    significantly better than the linear regression baseline, as summarized in table
    12.2.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对XGBoost解决方案进行的优化产生了显著优于线性回归基线的结果，如表12.2所示。
- en: Table 12.2 Summary of the results from chapter 7
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.2 第7章结果总结
- en: '| Metric | Linear regression baseline | Optimized XGBoost |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 线性回归基线 | 优化XGBoost |'
- en: '| --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| R-squared | 0.320 | 0.729 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| R-squared | 0.320 | 0.729 |'
- en: '| Root mean squared error | 17197.323 | 10853.661 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 均方根误差 | 17197.323 | 10853.661 |'
- en: '| Mean absolute error | 12568.371 | 6611.609 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 平均绝对误差 | 12568.371 | 6611.609 |'
- en: As a further check, you can verify that for all three of the metrics we tracked
    in chapter 7—that is, R-squared, root mean squared error (RMSE), and mean absolute
    error (MAE)—the optimized XGBoost model is definitely always an improvement on
    the linear regression baseline. Now, using the XGBoost model as our reference,
    we will explore what kind of results we can get with the same dataset using a
    deep learning model in the remainder of this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为进一步的检查，你可以验证在第7章中跟踪的所有三个指标——即R-squared、均方根误差（RMSE）和平均绝对误差（MAE）——优化的XGBoost模型在线性回归基线上的确总是有所改进。现在，以XGBoost模型作为我们的参考，我们将在本章的剩余部分探索使用相同的深度学习模型可以取得什么样的结果。
- en: 12.2 Selecting a deep learning solution
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 选择深度学习解决方案
- en: In chapter 8, we reviewed a set of different deep learning stacks for working
    with tabular data, including Keras, fastai, and other libraries specifically designed
    for tabular data, like TabNet. If we now want to compare the XGBoost solution
    to the Tokyo Airbnb problem from chapter 7, which deep learning approach should
    we use?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在第八章中，我们回顾了一系列用于处理表格数据的不同深度学习框架，包括Keras、fastai以及其他专门为表格数据设计的库，如TabNet。如果我们现在想要比较第七章中东京Airbnb问题的XGBoost解决方案，我们应该使用哪种深度学习方法？
- en: As a reminder, we shared a comparison of the different deep learning approaches
    using the NYC Airbnb dataset, which is shown again in table 12.3.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们分享了对使用纽约Airbnb数据集的不同深度学习方法的比较，该比较再次在表12.3中展示。
- en: Table 12.3 Comparison of deep learning options
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.3 深度学习选项比较
- en: '|  | Keras | fastai | Tabular data library (e.g., TabNet) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | Keras | fastai | 表格数据库（例如，TabNet） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Pro | Model details are transparent.A large community using the framework
    means that it’s easy to find solutions to common problems. | The framework includes
    explicit support for tabular data models, which means the code will be more compact.
    It also sets intelligent defaults so we can quickly reach reasonable results.
    | A bespoke library explicitly created to handle tabular datasets |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Pro | 模型细节透明。使用该框架的大规模社区意味着可以轻松找到常见问题的解决方案。 | 框架包括对表格数据模型的显式支持，这意味着代码将更加紧凑。它还设置了智能默认值，因此我们可以快速达到合理的结果。
    | 专门创建以处理表格数据集的定制库 |'
- en: '| Con | No built-in support for tabular data | If we run into a problem, we
    could be on our own getting a resolution because the community is smaller than
    the Keras one. | Up to now, no library has emerged as an obvious choice; the fragmented
    community and inconsistent maintenance of some libraries make it a challenge to
    get the basic code to run reliably. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Con | 没有内置对表格数据的支持 | 如果我们遇到问题，我们可能需要自己解决问题，因为社区规模小于Keras。 | 到目前为止，还没有出现一个明显的选择；社区碎片化和一些库的不一致维护使得可靠地运行基本代码成为一项挑战。
    |'
- en: The differences between the dataset we used in chapter 8 and the dataset we
    will use now for the comparison between XGBoost and deep learning—that is, a dataset
    that was previously both much larger (with four times as many rows as the Tokyo
    Airbnb dataset) and much simpler (with less than half as many columns as the Tokyo
    Airbnb dataset)—for our comparison with XGBoost do not significantly weight in
    favor of any of the proposed solutions. In fact, Keras and fastai are general-purpose
    deep learning frameworks and are not specifically designed for small or complex
    datasets. TabNet’s design gives it an edge with high-dimensional data, but when
    applied to smaller datasets, the advantage over Keras or fastai turns out to be
    less significant.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第八章中使用的数据集与我们现在用于比较XGBoost和深度学习的数据集之间的差异——即一个之前既大得多（行数是东京Airbnb数据集的四倍）又简单得多的数据集（列数不到东京Airbnb数据集的一半）——对于我们的XGBoost比较来说，并没有显著地偏向任何一种提出的解决方案。事实上，Keras和fastai是通用深度学习框架，并不是专门为小型或复杂数据集设计的。TabNet的设计使其在处理高维数据时具有优势，但当应用于较小的数据集时，与Keras或fastai相比的优势就不那么显著了。
- en: What actually weighs more in our choice is the fact that we want a fair comparison
    between XGBoost and a deep learning approach. As you saw in chapter 7, XGBoost
    shines with its ease of use, and it gets good results without a lot of tweaking.
    If we decide on a complex deep learning model that takes a long time to set up,
    tweak, and optimize, it wouldn’t turn into a fair comparison for the deep learning
    model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的选择中，真正重要的是我们想要在XGBoost和深度学习方法之间进行公平的比较。正如你在第七章中看到的，XGBoost以其易用性而闪耀，并且在不进行大量调整的情况下就能得到良好的结果。如果我们决定使用一个复杂的深度学习模型，这个模型需要花费很长时间来设置、调整和优化，那么这就不会对深度学习模型进行公平的比较。
- en: Given this, which deep learning framework should we choose? We will pass over
    TabNet because of the complexity of getting it to run reliably. That leaves us
    with a choice between Keras and fastai. As mentioned in table 12.3, Keras is indeed
    popular in production and boasts a larger community. However, fastai aligns better
    with our goals. Recall that in chapter 8 we noted that fastai is built for tabular
    data like ours, and it comes with smart defaults. This means you can get decent
    results quickly, without spending ages on optimizations. fastai handles a lot
    of the nitty-gritty stuff and details for you behind the scenes. As you’ll see
    later, choosing fastai for this problem paid off. For the moment, we believe it
    provides a strong deep learning solution for the Tokyo Airbnb problem without
    much hassle, ready in a few steps to take on the XGBoost model from chapter 7.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们应该选择哪个深度学习框架呢？由于将其可靠运行起来的复杂性，我们将跳过TabNet。这让我们在Keras和fastai之间做出选择。如表12.3中提到的，Keras在生产中确实很受欢迎，拥有更大的社区。然而，fastai与我们的目标更为契合。回想一下，在第8章中我们提到，fastai是为像我们这样的表格数据构建的，并且它带有智能默认设置。这意味着你可以快速获得不错的结果，而无需花费大量时间进行优化。fastai在幕后为你处理了很多琐碎的事情和细节。正如你稍后将会看到的，选择fastai来解决这个问题是值得的。目前，我们相信它为东京Airbnb问题提供了一个强大的深度学习解决方案，无需太多麻烦，只需几步即可应对第7章中的XGBoost模型。
- en: 12.3 Selected deep learning solution to the Tokyo Airbnb problem
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 针对东京Airbnb问题的所选深度学习解决方案
- en: So far in this chapter we have reviewed the XGBoost solution to the Tokyo Airbnb
    problem, reviewed options for a deep learning solution to compare it with, and
    selected fastai as the deep learning framework to use for the comparison with
    XGBoost. In this section, we’ll go through the details of the fastai solution
    for the Tokyo Airbnb problem.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经回顾了东京Airbnb问题的XGBoost解决方案，并审查了用于比较的深度学习解决方案选项，并选择了fastai作为与XGBoost进行比较的深度学习框架。在本节中，我们将详细介绍fastai针对东京Airbnb问题的解决方案。
- en: Listing 12.2 shows the core of the fastai model that we used to compare with
    the XGBoost solution. This code trains a fastai regression model on the Tokyo
    Airbnb dataset using the `TabularPandas` function ([https://mng.bz/QDR6](https://mng.bz/QDR6)),
    which is a wrapper providing all the necessary transformations under the hood.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.2展示了我们用来与XGBoost解决方案进行比较的fastai模型的核心理念。此代码使用`TabularPandas`函数（[https://mng.bz/QDR6](https://mng.bz/QDR6)）在东京Airbnb数据集上训练了一个fastai回归模型，`TabularPandas`是一个封装器，在底层提供了所有必要的转换。
- en: Listing 12.2 fastai model for the Tokyo Airbnb problem
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.2东京Airbnb问题的fastai模型
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Defines a fastai TabularPandas object
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义一个fastai TabularPandas对象
- en: ② Defines a dataloaders object based on the TabularPandas object
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ② 基于TabularPandas对象定义一个数据加载器对象
- en: ③ Defines a tabular_learner object based on the dataloaders object
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 基于数据加载器对象定义一个tabular_learner对象
- en: ④ Trains the model
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 训练模型
- en: ⑤ Gets predictions from the model on the test set
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从测试集上获取模型的预测结果
- en: ⑥ Saves the metrics
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 保存指标
- en: As you can see from the code in listing 12.2, the solution for fastai is straightforward.
    First, we define the preprocessing steps (`procs`), such as filling missing values,
    normalization, and categorization. Then we separate categorical and continuous
    variables from the dataset and choose the dependent variable (`dep_var`). After
    that, we iterate through a stratified k-fold cross-validation in the same manner
    as we did for the XGBoost solution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表12.2中的代码所示，fastai的解决方案非常直接。首先，我们定义了预处理步骤（`procs`），例如填充缺失值、归一化和分类。然后，我们将分类变量和连续变量从数据集中分离出来，并选择因变量（`dep_var`）。之后，我们以与XGBoost解决方案相同的方式在分层k折交叉验证中迭代。
- en: During the iterations, the training data is preprocessed using `TabularPandas`,
    specifying categorical and continuous variables, the target variable, and data
    splits. DataLoader objects (`dls`) are created for training and validation batches.
    After defining a neural network model (`tabular_learner`) consisting of two layers,
    the first with 1,000 neurons and the following with 500 nodes, and their dropout
    rates (using `tabular_config` and setting a higher dropout for the last layer),
    we train the model using the learning rate found by `lr_find` and using the one
    cycle (`fit_one_cycle`) procedure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代过程中，使用`TabularPandas`对训练数据进行预处理，指定分类变量、连续变量、目标变量和数据拆分。为训练和验证批次创建了数据加载器对象（`dls`）。在定义了一个由两层组成的神经网络模型（`tabular_learner`），第一层有1,000个神经元，接下来是500个节点，并设置了它们的dropout率（使用`tabular_config`并设置最后一层更高的dropout率）之后，我们使用`lr_find`找到的学习率以及单周期（`fit_one_cycle`）过程来训练模型。
- en: 'By combining the `lr_find` and `fit_one_cycle` procedures, we automatically
    tune the learning rate parameter for the best results on the type of data we are
    processing, thus achieving a straightforward solution without much tweaking and
    experimentation. The procedure `lr_find` ([https://mng.bz/Xxq9](https://mng.bz/Xxq9))
    explores a range of learning rates on a sample of the data, stopping when the
    learning rate is so high that the learning diverges. While the procedure takes
    some time, it is relatively fast and returns the value of the learning parameter
    that is two-thirds of the way along the section of the loss curve where the loss
    is decreasing. We use that value as the upper boundary of another procedure, the
    `fit_one_cycle` ([https://mng.bz/yWZp](https://mng.bz/yWZp)), which is a training
    method where the learning rate is not fixed or constantly decreasing but rather
    oscillates between a minimum and a maximum value. The oscillations allow for the
    network to not get stuck in a local minima, and overall the resulting network
    performs better than using other approaches, especially when dealing with tabular
    data. Both methods have been developed by Leslie Smith in a series of papers:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合`lr_find`和`fit_one_cycle`过程，我们自动调整学习率参数，以在处理的数据类型上获得最佳结果，从而实现一个简单直接的解决方案，无需过多调整和实验。`lr_find`过程([https://mng.bz/Xxq9](https://mng.bz/Xxq9))在数据样本上探索一系列学习率，当学习率过高导致学习发散时停止。虽然这个过程需要一些时间，但它相对较快，并返回损失曲线下降部分的二分之一处的学习参数值。我们使用这个值作为另一个过程`fit_one_cycle`([https://mng.bz/yWZp](https://mng.bz/yWZp))的上限边界，这是一个学习率不是固定或不断减少，而是在最小值和最大值之间振荡的训练方法。振荡允许网络不会陷入局部最小值，总体而言，使用这种方法得到的网络性能优于其他方法，尤其是在处理表格数据时。这两种方法都是由Leslie
    Smith在一系列论文中开发的：
- en: “Cyclical Learning Rates for Training Neural Networks” ([https://arxiv.org/abs/1506.01186](https://arxiv.org/abs/1506.01186))
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “循环学习率用于训练神经网络” ([https://arxiv.org/abs/1506.01186](https://arxiv.org/abs/1506.01186))
- en: '“Super-Convergence: Very Fast Training of Neural Networks Using Large Learning
    Rates” ([https://arxiv.org/abs/1708.07120](https://arxiv.org/abs/1708.07120))'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “超级收敛：使用大学习率快速训练神经网络” ([https://arxiv.org/abs/1708.07120](https://arxiv.org/abs/1708.07120))
- en: '“A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning
    Rate, Batch Size, Momentum, and Weight Decay” ([https://arxiv.org/abs/1803.09820](https://arxiv.org/abs/1803.09820))'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “神经网络超参数的纪律化方法：第一部分——学习率、批量大小、动量和权重衰减” ([https://arxiv.org/abs/1803.09820](https://arxiv.org/abs/1803.09820))
- en: To our knowledge, the implementation by fastai of these methods is the most
    efficient and high-performing available in the open-source community.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，fastai对这些方法的实现是开源社区中最有效率和性能最高的。
- en: Proceeding with the code, for each pass through the `for` loop, the predictions
    for the current fold and the R-squared, RMSE, and MAE evaluations are saved. The
    mean values for all the metrics are printed after the loop so we can get an idea
    of the overall values for the fastai solution. Note that we will recalculate these
    values as we go through the ensembling process when we compare the predictions
    and actual y values for an ensemble that is 100% fastai results.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 继续编写代码，对于`for`循环的每次迭代，都会保存当前折的预测以及R-squared、RMSE和MAE评估。循环结束后，打印所有指标的均值，以便我们可以了解fastai解决方案的整体值。请注意，当我们通过集成过程比较预测值和实际y值时，我们将重新计算这些值，因为我们比较的是100%的fastai结果。
- en: 12.4 Comparing the XGBoost and fastai solutions to the Tokyo Airbnb problem
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 比较XGBoost和fastai解决方案解决东京Airbnb问题
- en: Now that we have a deep learning solution for the Tokyo Airbnb problem, we can
    compare its results against the XGBoost solution. By comparing the metrics that
    we collected for both solutions (R-squared, RMSE, and MAE), we can get an idea
    of how effective each solution is at solving the Tokyo Airbnb problem. Table 12.4
    contains a summary of the results of both approaches.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了解决东京Airbnb问题的深度学习解决方案，我们可以将其结果与XGBoost解决方案进行比较。通过比较我们收集的两种解决方案的指标（R-squared、RMSE和MAE），我们可以了解每种解决方案在解决东京Airbnb问题上的有效性。表12.4包含了两种方法结果的总结。
- en: Table 12.4 Comparison of results from XGBoost and fastai models
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.4 XGBoost和fastai模型结果比较
- en: '| Metric | XGBoost | Fastai |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Metric | XGBoost | Fastai |'
- en: '| --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| R-squared | 0.599 | 0.572 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| R-squared | 0.599 | 0.572 |'
- en: '| RMSE | 10783.027 | 11719.387 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | 10783.027 | 11719.387 |'
- en: '| MAE | 6531.102 | 7152.143 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 6531.102 | 7152.143 |'
- en: Table 12.4 demonstrates that XGBoost provides better results than fastai for
    all three error metrics—in fact, the R-squared value for XGBoost is significantly
    higher, and its RMSE and MAE values are about 8% to 9% lower.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.4表明，XGBoost在所有三个错误指标上均优于fastai——事实上，XGBoost的R-squared值显著更高，其RMSE和MAE值大约低8%到9%。
- en: In addition to the basic comparison of error metrics in table 12.4, we can visualize
    some differences relative to how the two approaches deal with the problem. For
    example, we can directly compare XGBoost and fastai predictions by examining each
    predicted data point in the Tokyo Airbnb test set. In figure 12.1, the x-axis
    represents the prediction from XGBoost, while the y-axis represents the prediction
    from fastai for each respective data point. The figure shows the relationship
    between XGBoost and fastai predictions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表12.4中基本比较错误指标外，我们还可以可视化两种方法处理问题的差异。例如，我们可以通过检查东京Airbnb测试集中的每个预测数据点来直接比较XGBoost和fastai的预测。在图12.1中，x轴代表XGBoost的预测，而y轴代表每个相应数据点的fastai预测。该图显示了XGBoost和fastai预测之间的关系。
- en: '![](../Images/CH12_F01_Ryan2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH12_F01_Ryan2.png)'
- en: Figure 12.1 Scatterplot of predictions for XGBoost and fastai
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 XGBoost和fastai预测的散点图
- en: On the diagonal of the chart, the solid trend line coincides fairly well with
    the dashed diagonal line, showing that, overall, there is not a huge variance
    in the predictions between XGBoost and fastai. The solid trend line, showing the
    smoothed regression line (a technique called LOWESS [LOcally WEighted Scatterplot
    Smoothing]) between the predictions from XGBoost and fastai, doesn’t deviate significantly
    in comparison from the dashed line, confirming that, even if the algorithms tend
    to disagree in their predictions, there is no systematic over- or underestimation
    from one of the two in respect of the other.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的对角线上，实线趋势线与虚线对角线相当吻合，表明总体上，XGBoost和fastai之间的预测差异并不大。显示XGBoost和fastai预测之间平滑回归线（一种称为LOWESS
    [LOcally WEighted Scatterplot Smoothing]的技术）的实线趋势线与虚线相比没有显著偏离，这证实了即使算法在预测上存在分歧，两个算法之间也没有系统性的高估或低估。
- en: In addition, we can also try to explore how their predictions relate, using
    the value of 70,000 on the x-axis as a pivot, since we can spot two distinct clusters
    of predictions. We observe that the average fastai prediction is around 81,300
    against a XGBoost average at 81,550 for XGBoost predictions over 70,000 and around
    23,050 against 22,500 for XGBoost predictions below 70,000.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以尝试探索它们的预测如何相关，以x轴上的70,000值为基准，因为我们能观察到两个不同的预测簇。我们观察到，fastai的平均预测值约为81,300，而XGBoost的平均预测值在70,000以上的为81,550，在70,000以下的为约23,050，而XGBoost的预测值分别为22,500。
- en: Listing 12.3 Average fastai predictions for XGBoost predictions
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.3 XGBoost预测的平均fastai预测
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Gets a pandas DataFrame with a column each for XGBoost and fastai predictions
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取一个包含XGBoost和fastai预测列的pandas DataFrame
- en: ② Computes mean statistics for XGBoost and fastai predictions depending on XGBoost
    predicted value
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ② 根据XGBoost预测值计算XGBoost和fastai预测的平均统计量
- en: The differences between the two models are minimal; on average, fastai and XGBoost
    predictions tend to be aligned. Fastai tends to overestimate for lower predicted
    pricing levels and slightly underestimate for higher predicted pricing levels.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型之间的差异很小；平均而言，fastai和XGBoost的预测往往是一致的。fastai倾向于对较低的预测定价水平高估，对较高的预测定价水平略低估。
- en: Returning to figure 12.1, now that we have examined the chart comparing the
    predictions, let’s look at a chart comparing the error for XGBoost with the error
    for fastai. For each data point in the Tokyo Airbnb test set, the plot in figure
    12.2 shows the error for that data point (the absolute value of the difference
    between the prediction and the actual value), with the x value being the XGBoost
    error and the y value being the fastai error.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到图12.1，现在我们已经检查了比较预测的图表，让我们看看比较XGBoost与fastai错误的图表。对于东京Airbnb测试集中的每个数据点，图12.2中的图表显示了该数据点的错误（预测值与实际值之间差异的绝对值），x值是XGBoost的错误，y值是fastai的错误。
- en: Figure 12.2 shows that there is a large cluster of data points where the error
    for both XGBoost and fastai is lower than 20,000\. The overall LOWESS line, in
    red, shows that the error for XGBoost is overall lower than for fastai most of
    the time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2显示，存在一个数据点的大簇，其中XGBoost和fastai的误差都低于20,000。整体LOWESS线（红色）显示，XGBoost的误差总体上低于fastai，大多数时候都是如此。
- en: As shown by the aggregate error metrics and the plots for predictions and errors,
    the XGBoost model displays better performance than the fastai model. However,
    given the spread of the predictions between the two models, we believe that there
    is an opportunity to do even better than XGBoost alone by ensembling the two models
    because there is a strong hint that, apart from the differences in performance,
    the two models operate differently in their predictions by capturing different
    data patterns and characteristics. In the next section, we will find out whether
    ensembling improves the results.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如聚合误差指标和预测及误差的图表所示，XGBoost模型的表现优于fastai模型。然而，考虑到两个模型预测之间的差异，我们认为通过集成这两个模型，我们可以做得比单独使用XGBoost更好，因为有一个强烈的暗示，除了性能差异之外，两个模型在预测中表现不同，因为它们捕捉到不同的数据模式和特征。在下一节中，我们将找出集成是否改善了结果。
- en: '![](../Images/CH12_F02_Ryan2.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH12_F02_Ryan2.png)'
- en: Figure 12.2 Scatterplot of errors for XGBoost and fastai
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 XGBoost和fastai误差的散点图
- en: 12.5 Ensembling the two solutions to the Tokyo Airbnb problem
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 集成东京Airbnb问题的两个解决方案
- en: Now that we have established the performance of the XGBoost and fastai solutions
    to the Tokyo Airbnb problem in isolation, we will look at ensembling the two solutions
    to see whether a combination of the two approaches provides any improvements.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定了XGBoost和fastai解决方案在东京Airbnb问题上的独立性能，我们将查看集成这两个解决方案，看看两种方法的组合是否提供了任何改进。
- en: The following listing shows the loop where we ensemble the results of the two
    models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了我们将两个模型的结果进行集成的循环。
- en: Listing 12.4 Code to ensemble the two models
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.4 集成两个模型的代码
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Ensembles ratios to iterate through
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ① 集成比例以迭代方式遍历
- en: ② Generates predictions that are blended according to the ensembling ratios
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ② 生成根据集成比例混合的预测
- en: ③ Gets R-squared, RMSE, and MAE for the blended predictions
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 获取混合预测的R-squared、RMSE和MAE
- en: The code in listing 12.4 combines results from XGBoost and fastai according
    to the blending values in `blend_list`. Note that these blending values are not
    optimized to find the absolute optimum—we are simply using a set of fixed blending
    values to get a general sense of the effect of blending results. Also, note that
    we are evaluating the results using out-of-fold predictions. Nevertheless, by
    combining the predictions from XGBoost and fastai according to the proportions
    specified by `blend_list`, we can see the effect of ensembling the two approaches
    across a range of values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.4中的代码根据`blend_list`中的混合值结合了XGBoost和fastai的结果。请注意，这些混合值并未优化以找到绝对最优——我们只是使用一组固定的混合值来获得混合结果的一般感觉。另外，请注意，我们正在使用出卷预测来评估结果。尽管如此，通过根据`blend_list`中指定的比例结合XGBoost和fastai的预测，我们可以看到在一系列值上集成两种方法的效果。
- en: The output of the blending code for a typical run is
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 混合代码的典型运行输出如下
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These figures may be a bit difficult to interpret, so let’s see what they look
    like in the form of a chart. Figure 12.3 shows the results for R-squared, RMSE,
    and MAE for a range of blending between the XGBoost and fastai models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字可能有点难以解读，所以让我们看看它们以图表形式的样子。图12.3显示了XGBoost和fastai模型之间一系列混合的结果，包括R-squared、RMSE和MAE。
- en: As figure 12.3 shows, we get the optimal results for R-squared evaluation when
    we use a 50/50 blend of the predictions from the XGBoost and fastai models. For
    error-based measures, RMSE and MAE, we obtain a better result weighting more XGBoost
    than fastai; however, if we were to use a 50/50 blend, we would obtain only slightly
    worse scores. The worst results are when we use 100% of prediction from fastai,
    as we would expect from the results we got from looking at each model in isolation,
    but it is interesting to notice that using only XGBoost is always worse than blending
    it with the other solution using a 75/25 or 50/50 share.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如图12.3所示，当我们使用XGBoost和fastai模型预测的50/50混合时，我们得到了R-squared评估的最佳结果。对于基于误差的度量，RMSE和MAE，我们通过更多地使用XGBoost而不是fastai获得了更好的结果；然而，如果我们使用50/50的混合，我们只会得到略微差一些的分数。最差的结果是当我们使用100%的fastai预测时，正如我们从单独查看每个模型得到的结果所预期的那样，但值得注意的是，仅使用XGBoost总是比将其与其他解决方案（使用75/25或50/50的份额）混合要差。
- en: '![](../Images/CH12_F03_Ryan2.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3](../Images/CH12_F03_Ryan2.png)'
- en: Figure 12.3 Results of blending XGBoost and fastai models
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 XGBoost和fastai模型融合的结果
- en: Ensembling the XGBoost and fastai models gets better results than using either
    model in isolation. As we will see in the next section, our observations from
    ensembling are consistent with the results shared in an important research paper
    that compared classical machine learning approaches and deep learning on tabular
    data problems.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将XGBoost和fastai模型进行集成比单独使用任一模型得到的结果更好。正如我们将在下一节中看到的，我们从集成中得到的观察结果与一篇重要研究论文中分享的结果一致，该论文比较了经典机器学习方法和深度学习在表格数据问题上的表现。
- en: 12.6 Overall comparison of gradient boosting and deep learning
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.6 梯度提升和深度学习的整体比较
- en: 'In chapter 1, we introduced the controversy over whether deep learning is needed
    to solve problems involving tabular data. We cited academic papers that support
    both sides of the argument—those that advocate for deep learning approaches and
    those that maintain that classical machine learning approaches, in particular
    gradient boosting, consistently outperform deep learning. One of the papers that
    we mentioned in chapter 1 is worth revisiting here: “Tabular Data: Deep Learning
    Is Not All You Need,” by Ravid Shwartz-Ziv and Amitai Armon ([https://arxiv.org/abs/2106.03253](https://arxiv.org/abs/2106.03253)).
    In the Discussion and Conclusions section of this paper, the authors make the
    following statement:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们介绍了关于是否需要深度学习来解决涉及表格数据的问题的争议。我们引用了支持双方论点的学术论文——那些倡导深度学习方法的人和那些坚持认为经典机器学习方法，特别是梯度提升，始终优于深度学习的人。我们在第一章中提到的一篇论文值得在此重新审视：“表格数据：深度学习并非一切所需”，由Ravid
    Shwartz-Ziv和Amitai Armon撰写([https://arxiv.org/abs/2106.03253](https://arxiv.org/abs/2106.03253))。在这篇论文的讨论和结论部分，作者做出了以下陈述：
- en: In our analysis, the deep models were weaker on datasets that did not appear
    in their original papers, and they were weaker than XGBoost, the baseline model.
    Therefore, we proposed using an ensemble of these deep models with XGBoost. This
    ensemble performed better on these datasets than any individual model and the
    'non-deep’ classical ensemble.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，对于那些没有出现在它们原始论文中的数据集，深度模型的表现较弱，并且它们的表现不如基线模型XGBoost。因此，我们提出了使用这些深度模型与XGBoost的集成。这个集成在这些数据集上的表现优于任何单个模型和“非深度”的经典集成。
- en: Their observation is not a hard and fast rule, because, in our experience, we
    encountered situations where using a gradient boosting solution or a deep learning
    one alone resulted in the best performances. However, in many of the situations
    we faced, we can confirm that simply averaging solutions resulted in better predictions.
    We are strongly convinced that this happened because the two algorithms have two
    different ways of optimizing predictions. Gradient boosting is based on decision
    trees, which are a form of analogy search because, as an algorithm, trees split
    your dataset into parts where the features tend to be similar in values between
    themselves and map to similar target outputs. The gradient part of the algorithm
    intelligently ensembles multiple trees together for a better prediction, although
    it doesn’t change the basic way in which decision trees behave. Deep learning,
    on the other hand, is purely based on the principle of differentiation and nonlinear
    transformations, where the algorithm looks for the best weights to combine the
    transformed inputs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的观察并不是一个铁的规则，因为，根据我们的经验，我们遇到了使用梯度提升解决方案或深度学习解决方案单独使用时表现最佳的情况。然而，在许多我们面临的情况中，我们可以确认简单地平均解决方案导致了更好的预测。我们坚信，这是因为两种算法有优化预测的不同方式。梯度提升基于决策树，这是一种类比搜索的形式，因为作为一个算法，树将你的数据集分割成特征值之间相似的部分，并将它们映射到相似的目标输出。算法的梯度部分智能地集成多个树以获得更好的预测，尽管它并没有改变决策树的基本行为方式。另一方面，深度学习纯粹基于微分和非线性变换的原则，算法寻找最佳的权重来组合变换后的输入。
- en: These two different approaches result in quite different estimations whose errors
    tend to partially cancel each other out because they are strongly uncorrelated,
    in a similar fashion to what actually happens in a random forest algorithm when
    you average the results of uncorrelated decision trees. This conclusion matches
    our experience comparing the results of gradient boosting and deep learning on
    the Tokyo Airbnb problem. The ensemble of the XGBoost model with the fastai model
    produced the best results, as shown in figure 12.3.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种不同的方法导致了相当不同的估计结果，它们的误差往往部分相互抵消，因为它们高度不相关，这与在随机森林算法中平均不相关决策树的结果相似。这个结论与我们在比较东京Airbnb问题上的梯度提升和深度学习结果的经验相符。XGBoost模型与fastai模型的集成产生了最佳结果，如图12.3所示。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The XGBoost solution to the Tokyo Airbnb problem shown in chapter 7 provides
    a baseline that we can use to assess the efficacy of deep learning to solve the
    same problem.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7章中展示的东京Airbnb问题的XGBoost解决方案提供了一个基线，我们可以用它来评估深度学习解决相同问题的有效性。
- en: Using the XGBoost solution from chapter 7 as a starting point, we can create
    a deep learning solution for the Tokyo Airbnb problem. The fastai library provides
    a compact and relatively well-performing deep learning solution.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以第7章中的XGBoost解决方案为起点，我们可以为东京Airbnb问题创建一个深度学习解决方案。fastai库提供了一个紧凑且相对性能良好的深度学习解决方案。
- en: By blending the predictions of the XGBoost and fastai models across a range
    of ratios, from 100% XGBoost to 100% fastai, we can see the effect of ensembling
    the models. We get the optimal results with a 50/50 ensemble of the two models.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在从100% XGBoost到100% fastai的一系列比例中混合XGBoost和fastai模型的预测，我们可以看到集成模型的效果。我们通过两个模型的50/50集成获得了最佳结果。
- en: This result matches the recommendation from research focused on scrutinizing
    the claims of the efficacy of deep learning on tabular data.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个结果与研究对深度学习在表格数据上的有效性的声明进行审查的建议相符。
