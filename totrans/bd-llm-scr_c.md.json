["```py\nprint(tokenizer.encode(\"Ak\"))\nprint(tokenizer.encode(\"w\"))\n# ...\n```", "```py\n[33901]\n[86]\n# ...\n```", "```py\nprint(tokenizer.decode([33901, 86, 343, 86, 220, 959]))\n```", "```py\n'Akwirw ier'\n```", "```py\ndataloader = create_dataloader(raw_text, batch_size=4, max_length=2, stride=2)\n```", "```py\ntensor([[  40,  367],\n        [2885, 1464],\n        [1807, 3619],\n        [ 402,  271]])\n```", "```py\ndataloader = create_dataloader(raw_text, batch_size=4, max_length=8, stride=2)\n```", "```py\ntensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])\n```", "```py\nsa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\nsa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\nsa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\n```", "```py\nd_out = 1\nmha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\n```", "```py\nblock_size = 1024\nd_in, d_out = 768, 768\nnum_heads = 12\nmha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)\n\n```", "```py\nblock = TransformerBlock(GPT_CONFIG_124M)\n\ntotal_params = sum(p.numel() for p in block.ff.parameters())\nprint(f\"Total number of parameters in feed forward module: {total_params:,}\")\n\ntotal_params = sum(p.numel() for p in block.att.parameters())\nprint(f\"Total number of parameters in attention module: {total_params:,}\")\n```", "```py\nTotal number of parameters in feed forward module: 4,722,432\nTotal number of parameters in attention module: 2,360,064\n```", "```py\nGPT_CONFIG = GPT_CONFIG_124M.copy()\nGPT_CONFIG[\"emb_dim\"] = 1600\nGPT_CONFIG[\"n_layers\"] = 48\nGPT_CONFIG[\"n_heads\"] = 25\nmodel = GPTModel(GPT_CONFIG)\n```", "```py\ngpt2-xl:\nTotal number of parameters: 1,637,792,000\nNumber of trainable parameters considering weight tying: 1,557,380,800\nTotal size of the model: 6247.68 MB\n```", "```py\ncheckpoint = torch.load(\"model_and_optimizer.pth\")\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n```", "```py\ntrain_loss = calc_loss_loader(train_loader, gpt, device)\nval_loss = calc_loss_loader(val_loader, gpt, device)\n```", "```py\nTraining loss: 3.754748503367106\nValidation loss: 3.559617757797241\n```", "```py\nhparams, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\nmodel_name = \"gpt2-small (124M)\"\n```", "```py\nhparams, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\nmodel_name = \"gpt2-xl (1558M)\"\n```"]