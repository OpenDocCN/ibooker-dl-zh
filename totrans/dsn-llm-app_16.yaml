- en: Chapter 13\. Design Patterns and System Architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章 设计模式和系统架构
- en: Throughout this book, we have explored a variety of techniques to adapt LLMs
    to solve our tasks, including in-context learning, fine-tuning, RAG, and tool
    use. While these techniques can potentially be successful in satisfying the performance
    requirements of your use case, deploying an LLM-based application in production
    requires adherence to a variety of other criteria like cost, latency, and reliability.
    To achieve these goals, an LLM application needs a lot of software scaffolding
    and specialized components.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们探索了各种技术来适应LLM以解决我们的任务，包括上下文学习、微调、RAG和工具使用。虽然这些技术可能有助于满足你用例的性能要求，但在生产中部署基于LLM的应用需要遵守各种其他标准，如成本、延迟和可靠性。为了实现这些目标，LLM应用需要大量的软件支撑和专用组件。
- en: To this end, in this chapter we will discuss various techniques to compose a
    production-level LLM system that can power useful applications. We will explore
    how to leverage multi-LLM architectures to balance cost and performance. Finally,
    we will look into software frameworks like DSPy that integrate LLM application
    development into the conventional software programming paradigm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，在本章中，我们将讨论各种技术，以构建一个生产级别的LLM系统，它可以支持有用的应用。我们将探讨如何利用多LLM架构来平衡成本和性能。最后，我们将探讨将LLM应用开发集成到传统软件开发范式中的软件框架，如DSPy。
- en: Treating an LLM-based application as just a standalone LLM component is inadequate
    if we intend to deploy it as a production-grade system. We need to treat it as
    a system, made up of several software and model components that support the LLM
    and make it reliable, fast, and cost-effective. The way these components are composed
    and connected is referred to as the *system architecture.*
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打算将其部署为生产级系统，仅仅将基于LLM的应用视为一个独立的LLM组件是不够的。我们需要将其视为一个系统，由多个软件和模型组件组成，这些组件支持LLM，使其可靠、快速且经济高效。这些组件的组成和连接方式被称为*系统架构*。
- en: 'Let’s begin by discussing a specific type: multi-LLM architectures that leverage
    multiple LLMs to solve your task.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先讨论一个特定类型：利用多个LLM来解决任务的多LLM架构。
- en: Multi-LLM Architectures
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多LLM架构
- en: 'Throughout this book, we have discussed the tradeoffs involved in choosing
    the right LLM for a task. Often, it can be beneficial to leverage multiple LLMs
    to achieve the desired outcome. Multi-LLM architectures can exist in the following
    two modes (or a combination):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们讨论了选择合适的LLM来解决任务的权衡。通常，利用多个LLM来实现预期结果是有益的。多LLM架构可以存在于以下两种模式（或组合）中：
- en: Each LLM is specialized for a different subtask
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 每个LLM针对不同的子任务进行专门化
- en: Different problem subtasks may require different levels of capabilities. To
    minimize cost and latency, for each task we would like to use the smallest possible
    LLM that can solve the subtask at the performance threshold we set.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的子任务可能需要不同级别的功能。为了最小化成本和延迟，对于每个任务，我们希望使用最小的LLM来解决子任务，达到我们设定的性能阈值。
- en: All LLMs solve the same task
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所有LLM解决相同任务
- en: In this case, all the LLMs are solving the same task, but for each input, only
    one or a subset of LLMs may be chosen to solve it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，所有LLM都在解决相同任务，但对于每个输入，可能只选择一个或一组LLM来解决它。
- en: Tip
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: A given task can be solved by an ensemble of LLMs, and the final outputs can
    be chosen based on some rules (majority voting, interpolation, etc.). Refer to
    [Jiang et al.’s ensembling framework](https://oreil.ly/FEikT) called LLM-Blender
    for an example of thoughtful ensembling.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个任务可以通过LLM的集合来解决，最终输出可以根据某些规则（多数投票、插值等）来选择。有关精心设计的集合示例，请参阅[Jiang等人](https://oreil.ly/FEikT)的LLM-Blender集合框架。
- en: Let’s walk through some commonly used multi-LLM architectures.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些常用的多LLM架构。
- en: LLM Cascades
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM级联
- en: While using the state-of-the-art LLM for processing all our inputs is an option,
    realistically this might be cost-prohibitive or latency sensitive. To optimize
    costs while keeping performance standards high, we could leverage multiple LLMs,
    organized in a cascade architecture.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用最先进的LLM来处理所有我们的输入是一个选择，但现实中这可能会成本高昂或延迟敏感。为了在保持高性能标准的同时优化成本，我们可以利用多个LLM，组织成级联架构。
- en: 'Let’s illustrate LLM cascades. Consider you have an application using three
    LLMs: one small, one medium, and one large, as illustrated in [Figure 13-1](#llm-cascades).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过LLM级联来举例说明。假设你有一个使用三个LLM的应用程序：一个小型、一个中型和一个大型，如图[图13-1](#llm-cascades)所示。
- en: '![llm-cascades](assets/dllm_1301.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![llm-cascades](assets/dllm_1301.png)'
- en: Figure 13-1\. LLM cascades
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-1\. LLM级联
- en: 'The following process is observed during inference:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中观察到以下过程：
- en: Each input is fed to the small LLM.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个输入都喂给小型LLM。
- en: If the small LLM makes an output prediction with a confidence level greater
    than a threshold, then we accept the output as the final output.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果小型LLM的输出预测的置信度高于阈值，那么我们接受输出作为最终输出。
- en: If the small LLM makes an output prediction with a confidence level that doesn’t
    surpass the threshold, then we pass the input to the medium model.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果小型LLM的输出预测的置信度没有超过阈值，那么我们将输入传递给中型模型。
- en: Similarly, if the medium LLM makes an output prediction with a confidence level
    greater than a threshold, then we stop and accept this output as the final output.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，如果中型LLM的输出预测的置信度高于阈值，那么我们停止并接受这个输出作为最终输出。
- en: However, if the medium LLM makes an output prediction with a confidence level
    that doesn’t surpass the threshold, then we pass the input to the large model.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，如果中型LLM的输出预测的置信度没有超过阈值，那么我们将输入传递给大型模型。
- en: The large model generates the final output.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大型模型生成最终输出。
- en: This architecture is most beneficial when most user inputs can be processed
    by the small model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当大多数用户输入可以由小型模型处理时，这种架构最有益。
- en: If you are using encoder-only models like BERT, the output probability scores
    can be used as the measure of confidence. Thus, a group of well-calibrated models
    will enable us to efficiently route the input to the most suitable model. (Recall
    our discussion on model calibration in [Chapter 5](ch05.html#chapter_utilizing_llms).)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用仅编码器模型，如BERT，输出概率分数可以用作置信度的衡量标准。因此，一组校准良好的模型将使我们能够有效地将输入路由到最合适的模型。（回忆我们在[第5章](ch05.html#chapter_utilizing_llms)中关于模型校准的讨论。）
- en: For decoder models, a popular method is to use self-consistency as a measure
    of confidence. (Recall our discussion on self-consistency in [Chapter 1](ch01.html#chapter_llm-introduction).)
    If we generate multiple times from the model and the outputs are mostly consistent
    with each other, then we can say that the model is being confident in its predictions.
    If they are not consistent, then we can move down the cascade and apply the inputs
    to the next LLM in the cascade.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码器模型，一种流行的方法是使用自洽性作为置信度的衡量标准。（回忆我们在[第1章](ch01.html#chapter_llm-introduction)中关于自洽性的讨论。）如果我们从模型中多次生成，并且输出之间大部分是一致的，那么我们可以说模型对其预测有信心。如果它们不一致，那么我们可以向下移动级联，并将输入应用到级联中的下一个LLM。
- en: Warning
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Some works propose asking the LLM to explicitly state the confidence level of
    its output. This has not been proven to be effective yet. Beware of asking the
    LLM to verify its own work in any form!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作建议让LLM明确声明其输出的置信度。这还没有被证明是有效的。小心不要以任何形式要求LLM验证其工作！
- en: Another method for assessing confidence is to use margin sampling, as proposed
    by [Ramirez et al.](https://oreil.ly/5s1rJ) In the margin sampling method, we
    generate the first token and use the difference in the probability of the most
    probable token and the second most probable token as the margin. The assumption
    is that the higher the margin, the more confident the model. If the margin is
    below a certain threshold, then the input is sent to the next model in the cascade.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 评估置信度的另一种方法是使用边缘采样，如Ramirez等人所提出的。在边缘采样方法中，我们生成第一个标记，并使用最可能标记的概率与第二可能标记的概率之间的差异作为边缘。假设边缘越高，模型越有信心。如果边缘低于某个阈值，则将输入发送到级联中的下一个模型。
- en: An alternative to using cascades is using a router scheme.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用级联的另一种选择是使用路由器方案。
- en: Routers
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路由器
- en: A router is a program or a model that processes input queries and dispatches
    them to the appropriate model. The advantage of using the router architecture
    is that, unlike cascades, the same input need not be run on potentially multiple
    models. However, the effectiveness of this strategy relies on the router effectively
    dispatching inputs to the optimal model, which may not always be fulfilled.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器是一个程序或模型，它处理输入查询并将它们调度到适当的模型。使用路由器架构的优势在于，与级联不同，相同的输入不需要在可能多个模型上运行。然而，这种策略的有效性依赖于路由器有效地将输入调度到最优模型，这并不总是能够实现。
- en: A router can perform intent classification, i.e., understand the intention of
    the user and dispatch the input to a suitable LLM that can solve the task being
    requested. If all the LLMs in the architecture are intended to solve the same
    task, then the router assesses the difficulty of the input query and dispatches
    the input to the smallest model that can adequately solve the task.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器可以执行意图分类，即理解用户的意图并将输入分配给可以解决请求任务的合适LLM。如果架构中的所有LLM都旨在解决同一任务，那么路由器评估输入查询的难度，并将输入分配给可以充分解决任务的模型中最小的模型。
- en: '[Figure 13-2](#routers) illustrates the role of the router in picking the right
    model to solve a task.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-2](#routers)展示了路由器在挑选合适的模型来解决任务中的作用。'
- en: '![router](assets/dllm_1302.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![路由器](assets/dllm_1302.png)'
- en: Figure 13-2\. Router
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2\. 路由器
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Routers can also be used in RAG pipelines. The router can assess the input and
    dispatch it to one of several different types of retrievers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 路由器也可以用于RAG管道。路由器可以评估输入并将其分配给几个不同类型的检索器之一。
- en: Assessing the complexity of an input query can be done using either heuristics
    or a fine-tuned model. Heuristics can be based on certain keywords that appear
    in the input (with RAG, *When* queries are more easily answered than *How* queries)
    or the identity of the tasks (for instance, sentiment analysis is an easier task
    that can be accomplished by a smaller model).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用启发式方法或微调模型来评估输入查询的复杂性。启发式方法可以基于输入中出现的某些关键词（例如，在RAG中，*When*查询比*How*查询更容易回答）或任务的标识（例如，情感分析是一个较容易的任务，可以通过较小的模型完成）。
- en: Next, let’s discuss task-specialized LLMs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论任务专用型LLM。
- en: Task-Specialized LLMs
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务专用型LLM
- en: Yet another way of organizing multi-LLM architectures is to deploy a variety
    of task-specific LLMs, each of them specialized in solving a particular type of
    task or subtask.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种组织多LLM架构的方法是部署各种任务专用型LLM，每个都专门解决特定类型或子任务。
- en: Given a complex user query, a relatively powerful LLM can be used to decompose
    the query into its constituent subtasks. A router can then assign each of these
    subtasks to the specialized model most equipped to handle at the subtask. (Recall
    our discussion on task decomposition in [Chapter 8](ch08.html#ch8).)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的用户查询，可以使用相对强大的LLM将其分解为其构成子任务。然后，路由器可以将这些子任务分配给最适合处理子任务的专用模型。（回想我们在[第8章](ch08.html#ch8)中关于任务分解的讨论。）
- en: Specialized LLMs can be constructed by fine-tuning them on task- and domain-specific
    datasets.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在任务和领域特定数据集上微调来构建专用LLM。
- en: '[Figure 13-3](#task-specific-llms) illustrates how a complex query can be divided
    into several subtasks, with each subtask being dispatched to the model most likely
    to solve it in a cost-optimal way.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-3](#task-specific-llms)展示了如何将复杂的查询分解为几个子任务，每个子任务被分配给最有可能以成本最优的方式解决它的模型。'
- en: '![task-specific-llms](assets/dllm_1303.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![任务专用型LLM](assets/dllm_1303.png)'
- en: Figure 13-3\. Task-specific LLMs
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3\. 任务专用型LLM
- en: Let’s now explore some programming paradigms that facilitate more effective
    LLM application development.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨一些促进更有效的LLM应用开发的编程范式。
- en: Programming Paradigms
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编程范式
- en: As we have seen in this chapter, production-grade LLM systems can be composed
    of a lot of software components that help make the system robust and reliable.
    Naturally, we would like to use software design patterns to help us build these
    systems to be productive and maintainable. The developer community is still maturing
    in this regard, and it will take more time for tried and tested design patterns
    to emerge.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中看到的，生产级LLM系统可以由许多软件组件组成，这些组件有助于使系统更加健壮和可靠。自然地，我们希望使用软件设计模式来帮助我们构建这些系统，使其具有生产力和可维护性。开发社区在这方面仍在成熟，经过验证的设计模式的出现需要更多时间。
- en: At this juncture, there are several proposals for LLM programming paradigms.
    While many are not yet well-tested, some of these paradigms are mature enough
    to support production-grade applications. Let’s explore a couple of major ones.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，有几种关于LLM编程范式的提案。虽然许多尚未经过充分测试，但其中一些范式已经足够成熟，可以支持生产级应用。让我们探索其中几个主要的。
- en: DSPy
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DSPy
- en: LLM application development is a highly iterative process. You might want to
    experiment with a few candidate LLMs before selecting the right one. You might
    start with zero-shot prompting, which involves a lot of iterative prompt manipulation,
    also called prompt engineering. If zero-shot isn’t sufficient, you might venture
    into few-shot prompting, which involves iterating over various candidate examples.
    If few-shot prompting isn’t sufficient, you might want to fine-tune the model,
    which involves iteratively preparing a dataset and trying various hyperparameters
    for the model. *D*eclarative *S*elf-improving Language *P*rograms, p*y*thonically
    (DSPy) is an open source programming framework that seeks to abstract a large
    part of the iterative process. Programming, not prompting, as their motto goes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LLM应用程序开发是一个高度迭代的过程。在选择正确的LLM之前，你可能想尝试几个候选LLM。你可能从零样本提示开始，这涉及到大量的迭代提示操作，也称为提示工程。如果零样本不足以满足需求，你可能尝试进入少样本提示，这涉及到遍历各种候选示例。如果少样本提示仍然不足，你可能想要微调模型，这涉及到迭代准备数据集并尝试模型的多种超参数。*D*eclarative
    *S*elf-improving Language *P*rograms，p*y*thonically（DSPy）是一个开源编程框架，旨在抽象化迭代过程的大部分。正如他们的座右铭所说：“编程，而非提示。”
- en: DSPy presents a framework where the application’s control flow is separated
    from variables that need to be iterated. The variables can be prompts, parameters
    of LLMs, etc. The programming blocks that manage the control flow of the application
    are called *modules*, and the blocks that perform the iterative updates of variables
    are called *optimizers*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: DSPy提供了一个框架，其中应用程序的控制流与需要迭代的变量分离。这些变量可以是提示、LLM的参数等。管理应用程序控制流的编程块称为*模块*，执行变量迭代更新的块称为*优化器*。
- en: Modules
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模块
- en: A module is a building block of an LLM application. Each module corresponds
    to an underlying prompt in the prompt chain. Each module type is an abstraction
    of a different prompting technique, like CoT. A module can be declared using a
    *signature* that declaratively provides the input-output specification.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模块是LLM应用程序的构建块。每个模块对应于提示链中的底层提示。每个模块类型是不同提示技术的抽象，如CoT。模块可以使用提供输入输出规范的*签名*进行声明。
- en: 'Declaring a CoT prompting module with a signature is as simple as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用签名声明一个CoT提示模块就像这样简单：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`ChainOfThought` is a module that provides an abstraction for the CoT prompting
    technique. The module is declared with a signature `document → summary` that specifies
    the input and output types in a declarative form. For instance, if you are building
    a question-answering application, then the signature could be `question → answer`.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChainOfThought`是一个提供CoT提示技术抽象的模块。该模块使用`document → summary`签名声明，以声明性形式指定输入和输出类型。例如，如果你正在构建一个问答应用程序，那么签名可以是`question
    → answer`。'
- en: 'For some applications, you would like to provide more details on the input-output
    mapping than just a short string. For those instances, signatures can be declared
    using Python classes. Here’s an example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些应用程序，你可能希望提供比简短字符串更多的输入输出映射的详细信息。在这种情况下，可以使用Python类声明签名。以下是一个示例：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this example, instructions can be provided in three places:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，可以在三个地方提供指令：
- en: The docstring, with a more detailed description of the task
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档字符串，对任务的更详细描述
- en: The input field, with details on any input constraints
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入字段，包含任何输入约束的详细信息
- en: The output field, with details on any output constraints
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出字段，包含任何输出约束的详细信息
- en: Refer to the [DSPy documentation](https://oreil.ly/4Vy5c) for a full list of
    available modules. We can use these modules as building blocks for constructing
    complex LLM applications. Next let’s look at optimizers that work under the hood
    to *compile* our modules into an executable program.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参考DSPy文档（[DSPy documentation](https://oreil.ly/4Vy5c)）以获取可用模块的完整列表。我们可以使用这些模块作为构建复杂LLM应用程序的构建块。接下来，让我们看看在底层工作的优化器，它们将我们的模块*编译*成可执行程序。
- en: Optimizers
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Optimizers are components that update prompts or model parameters. Several
    optimizers are natively supported by DSPy. An optimizer can be used to update
    one of the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是更新提示或模型参数的组件。DSPy原生支持几个优化器。优化器可以用来更新以下之一：
- en: The instruction prompt
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令提示
- en: Few-shot training examples
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 少样本训练示例
- en: Model parameters (fine-tuning)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数（微调）
- en: An optimizer takes as input the modules it needs to be applied to, the metric
    to evaluate the output of the modules, and fine-tuning or few-shot training data
    consisting of input-output pairs or just inputs. Optimizers use algorithms to
    update the prompts or parameters to optimize the desired metric. DSPy supports
    metrics like *accuracy* or *precision* or *exact match*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器接收需要应用到的模块、评估模块输出的指标，以及由输入-输出对或仅输入组成的微调或少量样本训练数据作为输入。优化器使用算法来更新提示或参数以优化所需的指标。DSPy支持如*准确率*、*精确度*或*精确匹配*等指标。
- en: You can implement your own modules and optimizers if the ones provided by default
    are inadequate to your needs. Thus, DSPy is a powerful framework that separates
    the control flow of the LLM application from iterative aspects like LLM prompting
    and fine-tuning, and potentially automates the latter. The downsides of DSPy are
    that the optimizers might not be effective enough to work in an automated fashion
    and might need manual intervention to tune them correctly. More often than not,
    you will find yourself writing your own optimizers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果默认提供的模块和优化器不能满足您的需求，您可以自己实现模块和优化器。因此，DSPy是一个强大的框架，它将LLM应用程序的控制流与迭代方面（如LLM提示和微调）分离，并可能自动化后者。DSPy的缺点是优化器可能不足以自动工作，可能需要手动干预来正确调整它们。通常情况下，您将发现自己需要编写自己的优化器。
- en: Let’s now explore another framework called Language Model Query Language (LMQL).
    We have already been introduced to this framework in [Chapter 5](ch05.html#chapter_utilizing_llms)
    in the context of structured generation, but here we will look at how the same
    framework can be used as a programming paradigm for developing LLM applications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探索另一个名为语言模型查询语言（LMQL）的框架。我们已经在[第5章](ch05.html#chapter_utilizing_llms)中介绍了这个框架，是在结构化生成的背景下，但在这里我们将探讨如何将相同的框架用作开发LLM应用程序的编程范式。
- en: LMQL
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LMQL
- en: 'LMQL is a superset of Python that enables specifying prompts, output constraints,
    and program control flow using declarative Python code. Here is an example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LMQL是Python的超集，它允许使用声明性Python代码来指定提示、输出约束和程序控制流。以下是一个示例：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example we are asking the model to generate a Jeopardy! question. Jeopardy!
    is a TV show that executes a modified version of a trivia quiz; the host supplies
    the answers and the contestants provide the question for the given answer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们要求模型生成一个Jeopardy!问题。Jeopardy!是一档电视节目，它执行了一种修改版的智力问答；主持人提供答案，参赛者提供对应问题的提问。
- en: In LMQL, we achieve this by defining a function called `jeopardy` and supplying
    the prompt instructions in the doc string. The doc string contains the instruction
    `Generate a Jeopardy! question and answer`. The `[ANSWER]` and `[QUESTION]` markers
    refer to templates that the LLM will fill in based on the constraints specified
    in the `WHERE` clause.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在LMQL中，我们通过定义一个名为`jeopardy`的函数，并在文档字符串中提供提示指令来实现这一点。文档字符串包含指令`生成一个Jeopardy!问题和答案`。`[ANSWER]`和`[QUESTION]`标记指的是LLM将根据`WHERE`子句中指定的约束条件填充的模板。
- en: For the answer (which in Jeopardy is the question), we stop generation after
    generating the `?` symbol. Similarly, for the question (which in Jeopardy is the
    answer), we stop generation after the newline symbol. The `WHERE` clause can be
    used to provide complex constraints for generation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于答案（在Jeopardy中是问题），我们在生成`?`符号后停止生成。同样，对于问题（在Jeopardy中是答案），我们在生成换行符后停止生成。可以使用`WHERE`子句提供生成复杂约束。
- en: LMQL syntax might take a while to get used to, but overall it provides a robust
    programmatic foundation for developing LLM programs. Both LMQL and DSPy have a
    learning curve, so I recommend being patient during your first few iterations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LMQL的语法可能需要一段时间才能习惯，但总体上，它为开发LLM程序提供了一个强大的程序性基础。LMQL和DSPy都有一定的学习曲线，因此我建议在最初的几次迭代中保持耐心。
- en: As LLMs and LLM-driven applications mature, I expect more programming paradigms
    to emerge and for existing paradigms to vastly evolve. Current paradigms might
    be too brittle in many cases, so be cautious and verify they are effective before
    you adopt them in production.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM和LLM驱动的应用的成熟，我预计将出现更多的编程范式，并且现有的范式将发生巨大的演变。在许多情况下，当前的范式可能过于脆弱，因此在生产中采用之前，请谨慎并验证它们的有效性。
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the construction of LLM systems and various system
    architectures. We showcased how we can leverage multi-LLM architectures to optimize
    for cost and latency. Finally, we introduced LLM programming frameworks for streamlining
    LLM application development.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了构建LLM系统的结构和各种系统架构。我们展示了如何利用多LLM架构来优化成本和延迟。最后，我们介绍了LLM编程框架，以简化LLM应用开发流程。
