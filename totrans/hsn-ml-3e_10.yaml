- en: Chapter 9\. Unsupervised Learning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 无监督学习技术
- en: 'Although most of the applications of machine learning today are based on supervised
    learning (and as a result, this is where most of the investments go to), the vast
    majority of the available data is unlabeled: we have the input features **X**,
    but we do not have the labels **y**. The computer scientist Yann LeCun famously
    said that “if intelligence was a cake, unsupervised learning would be the cake,
    supervised learning would be the icing on the cake, and reinforcement learning
    would be the cherry on the cake.” In other words, there is a huge potential in
    unsupervised learning that we have only barely started to sink our teeth into.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管今天大多数机器学习应用都是基于监督学习的（因此，这也是大部分投资的方向），但绝大多数可用数据是无标签的：我们有输入特征**X**，但没有标签**y**。计算机科学家Yann
    LeCun曾经说过，“如果智能是一个蛋糕，无监督学习就是蛋糕，监督学习就是蛋糕上的糖衣，而强化学习就是蛋糕上的樱桃。”换句话说，无监督学习中有巨大的潜力，我们只是刚刚开始探索。
- en: Say you want to create a system that will take a few pictures of each item on
    a manufacturing production line and detect which items are defective. You can
    fairly easily create a system that will take pictures automatically, and this
    might give you thousands of pictures every day. You can then build a reasonably
    large dataset in just a few weeks. But wait, there are no labels! If you want
    to train a regular binary classifier that will predict whether an item is defective
    or not, you will need to label every single picture as “defective” or “normal”.
    This will generally require human experts to sit down and manually go through
    all the pictures. This is a long, costly, and tedious task, so it will usually
    only be done on a small subset of the available pictures. As a result, the labeled
    dataset will be quite small, and the classifier’s performance will be disappointing.
    Moreover, every time the company makes any change to its products, the whole process
    will need to be started over from scratch. Wouldn’t it be great if the algorithm
    could just exploit the unlabeled data without needing humans to label every picture?
    Enter unsupervised learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想创建一个系统，该系统将拍摄制造生产线上每个物品的几张照片，并检测哪些物品有缺陷。你可以相当容易地创建一个系统，自动拍照，这可能每天给你数千张照片。然后你可以在短短几周内构建一个相当大的数据集。但是等等，没有标签！如果你想训练一个普通的二元分类器，预测物品是否有缺陷，你需要将每张图片标记为“有缺陷”或“正常”。这通常需要人类专家坐下来手动查看所有图片。这是一项漫长、昂贵和繁琐的任务，因此通常只会在可用图片的一小部分上进行。结果，标记的数据集将非常小，分类器的性能将令人失望。此外，每当公司对其产品进行任何更改时，整个过程都需要从头开始。如果算法只能利用无标签数据而无需人类标记每张图片，那不是很好吗？这就是无监督学习的作用。
- en: 'In [Chapter 8](ch08.html#dimensionality_chapter) we looked at the most common
    unsupervised learning task: dimensionality reduction. In this chapter we will
    look at a few more unsupervised tasks:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html#dimensionality_chapter)中，我们看了最常见的无监督学习任务：降维。在本章中，我们将看一些更多的无监督任务：
- en: Clustering
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类
- en: The goal is to group similar instances together into *clusters*. Clustering
    is a great tool for data analysis, customer segmentation, recommender systems,
    search engines, image segmentation, semi-supervised learning, dimensionality reduction,
    and more.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将相似的实例分组到*簇*中。聚类是数据分析、客户细分、推荐系统、搜索引擎、图像分割、半监督学习、降维等领域的重要工具。
- en: Anomaly detection (also called *outlier detection*)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测（也称为*离群检测*）
- en: The objective is to learn what “normal” data looks like, and then use that to
    detect abnormal instances. These instances are called *anomalies*, or *outliers*,
    while the normal instances are called *inliers*. Anomaly detection is useful in
    a wide variety of applications, such as fraud detection, detecting defective products
    in manufacturing, identifying new trends in time series, or removing outliers
    from a dataset before training another model, which can significantly improve
    the performance of the resulting model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是学习“正常”数据的外观，然后利用它来检测异常实例。这些实例被称为*异常值*或*离群值*，而正常实例被称为*内群值*。异常检测在各种应用中非常有用，如欺诈检测、制造中检测有缺陷的产品、识别时间序列中的新趋势，或在训练另一个模型之前从数据集中去除离群值，这可以显著提高结果模型的性能。
- en: Density estimation
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 密度估计
- en: 'This is the task of estimating the *probability density function* (PDF) of
    the random process that generated the dataset. Density estimation is commonly
    used for anomaly detection: instances located in very low-density regions are
    likely to be anomalies. It is also useful for data analysis and visualization.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是估计生成数据集的随机过程的*概率密度函数*（PDF）的任务。密度估计通常用于异常检测：位于非常低密度区域的实例可能是异常值。它还可用于数据分析和可视化。
- en: Ready for some cake? We will start with two clustering algorithms, *k*-means
    and DBSCAN, then we’ll discuss Gaussian mixture models and see how they can be
    used for density estimation, clustering, and anomaly detection.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好享用蛋糕了吗？我们将从两个聚类算法*k*-means和DBSCAN开始，然后讨论高斯混合模型，看看它们如何用于密度估计、聚类和异常检测。
- en: 'Clustering Algorithms: k-means and DBSCAN'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法：k均值和DBSCAN
- en: 'As you enjoy a hike in the mountains, you stumble upon a plant you have never
    seen before. You look around and you notice a few more. They are not identical,
    yet they are sufficiently similar for you to know that they most likely belong
    to the same species (or at least the same genus). You may need a botanist to tell
    you what species that is, but you certainly don’t need an expert to identify groups
    of similar-looking objects. This is called *clustering*: it is the task of identifying
    similar instances and assigning them to *clusters*, or groups of similar instances.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like in classification, each instance gets assigned to a group. However,
    unlike classification, clustering is an unsupervised task. Consider [Figure 9-1](#classification_vs_clustering_plot):
    on the left is the iris dataset (introduced in [Chapter 4](ch04.html#linear_models_chapter)),
    where each instance’s species (i.e., its class) is represented with a different
    marker. It is a labeled dataset, for which classification algorithms such as logistic
    regression, SVMs, or random forest classifiers are well suited. On the right is
    the same dataset, but without the labels, so you cannot use a classification algorithm
    anymore. This is where clustering algorithms step in: many of them can easily
    detect the lower-left cluster. It is also quite easy to see with our own eyes,
    but it is not so obvious that the upper-right cluster is composed of two distinct
    subclusters. That said, the dataset has two additional features (sepal length
    and width) that are not represented here, and clustering algorithms can make good
    use of all features, so in fact they identify the three clusters fairly well (e.g.,
    using a Gaussian mixture model, only 5 instances out of 150 are assigned to the
    wrong cluster).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0901](assets/mls3_0901.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Classification (left) versus clustering (right)
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Clustering is used in a wide variety of applications, including:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Customer segmentation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: You can cluster your customers based on their purchases and their activity on
    your website. This is useful to understand who your customers are and what they
    need, so you can adapt your products and marketing campaigns to each segment.
    For example, customer segmentation can be useful in *recommender systems* to suggest
    content that other users in the same cluster enjoyed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: When you analyze a new dataset, it can be helpful to run a clustering algorithm,
    and then analyze each cluster separately.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Once a dataset has been clustered, it is usually possible to measure each instance’s
    *affinity* with each cluster; affinity is any measure of how well an instance
    fits into a cluster. Each instance’s feature vector **x** can then be replaced
    with the vector of its cluster affinities. If there are *k* clusters, then this
    vector is *k*-dimensional. The new vector is typically much lower-dimensional
    than the original feature vector, but it can preserve enough information for further
    processing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: The cluster affinities can often be useful as extra features. For example, we
    used *k*-means in [Chapter 2](ch02.html#project_chapter) to add geographic cluster
    affinity features to the California housing dataset, and they helped us get better
    performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '*Anomaly detection* (also called *outlier detection*)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Any instance that has a low affinity to all the clusters is likely to be an
    anomaly. For example, if you have clustered the users of your website based on
    their behavior, you can detect users with unusual behavior, such as an unusual
    number of requests per second.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: If you only have a few labels, you could perform clustering and propagate the
    labels to all the instances in the same cluster. This technique can greatly increase
    the number of labels available for a subsequent supervised learning algorithm,
    and thus improve its performance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Search engines
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Some search engines let you search for images that are similar to a reference
    image. To build such a system, you would first apply a clustering algorithm to
    all the images in your database; similar images would end up in the same cluster.
    Then when a user provides a reference image, all you’d need to do is use the trained
    clustering model to find this image’s cluster, and you could then simply return
    all the images from this cluster.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一些搜索引擎允许您搜索与参考图像相似的图像。要构建这样一个系统，您首先需要将聚类算法应用于数据库中的所有图像；相似的图像将最终位于同一聚类中。然后，当用户提供一个参考图像时，您只需要使用训练过的聚类模型来找到这个图像的聚类，然后简单地返回该聚类中的所有图像。
- en: Image segmentation
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割
- en: By clustering pixels according to their color, then replacing each pixel’s color
    with the mean color of its cluster, it is possible to considerably reduce the
    number of different colors in an image. Image segmentation is used in many object
    detection and tracking systems, as it makes it easier to detect the contour of
    each object.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过根据它们的颜色对像素进行聚类，然后用其聚类的平均颜色替换每个像素的颜色，可以大大减少图像中不同颜色的数量。图像分割在许多对象检测和跟踪系统中使用，因为它使得更容易检测每个对象的轮廓。
- en: 'There is no universal definition of what a cluster is: it really depends on
    the context, and different algorithms will capture different kinds of clusters.
    Some algorithms look for instances centered around a particular point, called
    a *centroid*. Others look for continuous regions of densely packed instances:
    these clusters can take on any shape. Some algorithms are hierarchical, looking
    for clusters of clusters. And the list goes on.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聚类的定义并没有普遍的定义：它真的取决于上下文，不同的算法将捕捉不同类型的聚类。一些算法寻找围绕特定点集中的实例，称为*中心点*。其他算法寻找连续的密集实例区域：这些聚类可以采用任何形状。一些算法是分层的，寻找聚类的聚类。等等。
- en: In this section, we will look at two popular clustering algorithms, *k*-means
    and DBSCAN, and explore some of their applications, such as nonlinear dimensionality
    reduction, semi-supervised learning, and anomaly detection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种流行的聚类算法，*k*-均值和DBSCAN，并探讨它们的一些应用，如非线性降维、半监督学习和异常检测。
- en: k-means
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-均值
- en: 'Consider the unlabeled dataset represented in [Figure 9-2](#blobs_plot): you
    can clearly see five blobs of instances. The *k*-means algorithm is a simple algorithm
    capable of clustering this kind of dataset very quickly and efficiently, often
    in just a few iterations. It was proposed by Stuart Lloyd at Bell Labs in 1957
    as a technique for pulse-code modulation, but it was only [published](https://homl.info/36)
    outside of the company in 1982.⁠^([1](ch09.html#idm45720207909616)) In 1965, Edward
    W. Forgy had published virtually the same algorithm, so *k*-means is sometimes
    referred to as the Lloyd–Forgy algorithm.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在[图9-2](#blobs_plot)中表示的无标签数据集：您可以清楚地看到五个实例的聚类。*k*-均值算法是一种简单的算法，能够非常快速和高效地对这种数据集进行聚类，通常只需几次迭代。它是由贝尔实验室的斯图尔特·劳埃德（Stuart
    Lloyd）在1957年提出的，作为脉冲编码调制的技术，但直到1982年才在公司外部[发表](https://homl.info/36)。在1965年，爱德华·W·福吉（Edward
    W. Forgy）几乎发表了相同的算法，因此*k*-均值有时被称为劳埃德-福吉算法。
- en: '![mls3 0902](assets/mls3_0902.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0902](assets/mls3_0902.png)'
- en: Figure 9-2\. An unlabeled dataset composed of five blobs of instances
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。由五个实例聚类组成的无标签数据集
- en: 'Let’s train a *k*-means clusterer on this dataset. It will try to find each
    blob’s center and assign each instance to the closest blob:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这个数据集上训练一个*k*-均值聚类器。它将尝试找到每个聚类的中心，并将每个实例分配给最近的聚类：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that you have to specify the number of clusters *k* that the algorithm
    must find. In this example, it is pretty obvious from looking at the data that
    *k* should be set to 5, but in general it is not that easy. We will discuss this
    shortly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您必须指定算法必须找到的聚类数*k*。在这个例子中，从数据看，*k*应该设置为5是相当明显的，但一般来说并不那么容易。我们将很快讨论这个问题。
- en: 'Each instance will be assigned to one of the five clusters. In the context
    of clustering, an instance’s *label* is the index of the cluster to which the
    algorithm assigns this instance; this is not to be confused with the class labels
    in classification, which are used as targets (remember that clustering is an unsupervised
    learning task). The `KMeans` instance preserves the predicted labels of the instances
    it was trained on, available via the `labels_` instance variable:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个实例将被分配到五个聚类中的一个。在聚类的上下文中，实例的*标签*是算法分配给该实例的聚类的索引；这不应与分类中使用的类标签混淆（请记住，聚类是一种无监督学习任务）。`KMeans`实例保留了它训练过的实例的预测标签，可以通过`labels_`实例变量获得：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also take a look at the five centroids that the algorithm found:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看一下算法找到的五个中心点：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can easily assign new instances to the cluster whose centroid is closest:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松地将新实例分配给最接近的中心点的聚类：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you plot the cluster’s decision boundaries, you get a Voronoi tessellation:
    see [Figure 9-3](#voronoi_plot), where each centroid is represented with an X.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果绘制聚类的决策边界，您将得到一个Voronoi图：请参见[图9-3](#voronoi_plot)，其中每个中心点用X表示。
- en: '![mls3 0903](assets/mls3_0903.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0903](assets/mls3_0903.png)'
- en: Figure 9-3\. k-means decision boundaries (Voronoi tessellation)
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3。k-均值决策边界（Voronoi图）
- en: The vast majority of the instances were clearly assigned to the appropriate
    cluster, but a few instances were probably mislabeled, especially near the boundary
    between the top-left cluster and the central cluster. Indeed, the *k*-means algorithm
    does not behave very well when the blobs have very different diameters because
    all it cares about when assigning an instance to a cluster is the distance to
    the centroid.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数实例明显被分配到了适当的聚类中，但有一些实例可能被错误标记，特别是在左上角聚类和中心聚类之间的边界附近。事实上，当聚类的直径差异很大时，*k*-均值算法的表现并不好，因为在将实例分配给聚类时，它只关心到中心点的距离。
- en: 'Instead of assigning each instance to a single cluster, which is called *hard
    clustering*, it can be useful to give each instance a score per cluster, which
    is called *soft clustering*. The score can be the distance between the instance
    and the centroid or a similarity score (or affinity), such as the Gaussian radial
    basis function we used in [Chapter 2](ch02.html#project_chapter). In the `KMeans`
    class, the `transform()` method measures the distance from each instance to every
    centroid:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与将每个实例分配到单个簇不同，即所谓的*硬聚类*，给每个实例分配每个簇的分数可能很有用，即所谓的*软聚类*。分数可以是实例与质心之间的距离或相似度分数（或亲和度），例如我们在[第2章](ch02.html#project_chapter)中使用的高斯径向基函数。在`KMeans`类中，`transform()`方法测量每个实例到每个质心的距离：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this example, the first instance in `X_new` is located at a distance of
    about 2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the
    third centroid, 1.49 from the fourth centroid, and 2.89 from the fifth centroid.
    If you have a high-dimensional dataset and you transform it this way, you end
    up with a *k*-dimensional dataset: this transformation can be a very efficient
    nonlinear dimensionality reduction technique. Alternatively, you can use these
    distances as extra features to train another model, as in [Chapter 2](ch02.html#project_chapter).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`X_new`中的第一个实例距离第一个质心约为2.81，距离第二个质心约为0.33，距离第三个质心约为2.90，距离第四个质心约为1.49，距离第五个质心约为2.89。如果你有一个高维数据集，并以这种方式进行转换，你最终会得到一个*k*维的数据集：这种转换可以是一种非常有效的非线性降维技术。或者，你可以使用这些距离作为额外特征来训练另一个模型，就像[第2章](ch02.html#project_chapter)中那样。
- en: The k-means algorithm
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k均值算法
- en: So, how does the algorithm work? Well, suppose you were given the centroids.
    You could easily label all the instances in the dataset by assigning each of them
    to the cluster whose centroid is closest. Conversely, if you were given all the
    instance labels, you could easily locate each cluster’s centroid by computing
    the mean of the instances in that cluster. But you are given neither the labels
    nor the centroids, so how can you proceed? Start by placing the centroids randomly
    (e.g., by picking *k* instances at random from the dataset and using their locations
    as centroids). Then label the instances, update the centroids, label the instances,
    update the centroids, and so on until the centroids stop moving. The algorithm
    is guaranteed to converge in a finite number of steps (usually quite small). That’s
    because the mean squared distance between the instances and their closest centroids
    can only go down at each step, and since it cannot be negative, it’s guaranteed
    to converge.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，算法是如何工作的呢？假设你已经得到了质心。你可以通过将每个实例分配到最接近的质心的簇中来轻松地为数据集中的所有实例打上标签。相反，如果你已经得到了所有实例的标签，你可以通过计算该簇中实例的平均值来轻松地找到每个簇的质心。但是你既没有标签也没有质心，那么你该如何继续呢？首先随机放置质心（例如，通过从数据集中随机选择*k*个实例并使用它们的位置作为质心）。然后给实例打标签，更新质心，给实例打标签，更新质心，依此类推，直到质心不再移动。该算法保证会在有限步数内收敛（通常非常小）。这是因为实例与最接近的质心之间的均方距离在每一步都只能减小，而且由于它不能为负，因此保证会收敛。
- en: 'You can see the algorithm in action in [Figure 9-4](#kmeans_algorithm_plot):
    the centroids are initialized randomly (top left), then the instances are labeled
    (top right), then the centroids are updated (center left), the instances are relabeled
    (center right), and so on. As you can see, in just three iterations the algorithm
    has reached a clustering that seems close to optimal.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图9-4](#kmeans_algorithm_plot)中看到算法的运行过程：质心被随机初始化（左上角），然后实例被标记（右上角），然后质心被更新（左中），实例被重新标记（右中），依此类推。正如你所看到的，在仅三次迭代中，算法已经达到了一个看似接近最优的聚类。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The computational complexity of the algorithm is generally linear with regard
    to the number of instances *m*, the number of clusters *k*, and the number of
    dimensions *n*. However, this is only true when the data has a clustering structure.
    If it does not, then in the worst-case scenario the complexity can increase exponentially
    with the number of instances. In practice, this rarely happens, and *k*-means
    is generally one of the fastest clustering algorithms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的计算复杂度通常与实例数*m*、簇数*k*和维度数*n*成线性关系。然而，这仅在数据具有聚类结构时才成立。如果没有，那么在最坏情况下，复杂度可能会随实例数呈指数增长。在实践中，这种情况很少发生，*k*-均值通常是最快的聚类算法之一。
- en: '![mls3 0904](assets/mls3_0904.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0904](assets/mls3_0904.png)'
- en: Figure 9-4\. The *k*-means algorithm
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。*k*-均值算法
- en: 'Although the algorithm is guaranteed to converge, it may not converge to the
    right solution (i.e., it may converge to a local optimum): whether it does or
    not depends on the centroid initialization. [Figure 9-5](#kmeans_variability_plot)
    shows two suboptimal solutions that the algorithm can converge to if you are not
    lucky with the random initialization step.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管算法保证会收敛，但它可能不会收敛到正确的解决方案（即可能会收敛到局部最优解）：它是否这样做取决于质心初始化。如果在随机初始化步骤中不幸的话，算法可能会收敛到两种次优解决方案，如[图9-5](#kmeans_variability_plot)所示。
- en: '![mls3 0905](assets/mls3_0905.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0905](assets/mls3_0905.png)'
- en: Figure 9-5\. Suboptimal solutions due to unlucky centroid initializations
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5。由于不幸的质心初始化而导致次优解决方案
- en: Let’s take a look at a few ways you can mitigate this risk by improving the
    centroid initialization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看通过改进质心初始化方式来减轻这种风险的几种方法。
- en: Centroid initialization methods
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 质心初始化方法
- en: 'If you happen to know approximately where the centroids should be (e.g., if
    you ran another clustering algorithm earlier), then you can set the `init` hyperparameter
    to a NumPy array containing the list of centroids, and set `n_init` to `1`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你大致知道质心应该在哪里（例如，如果你之前运行过另一个聚类算法），那么你可以将`init`超参数设置为包含质心列表的NumPy数组，并将`n_init`设置为`1`：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Another solution is to run the algorithm multiple times with different random
    initializations and keep the best solution. The number of random initializations
    is controlled by the `n_init` hyperparameter: by default it is equal to `10`,
    which means that the whole algorithm described earlier runs 10 times when you
    call `fit()`, and Scikit-Learn keeps the best solution. But how exactly does it
    know which solution is the best? It uses a performance metric! That metric is
    called the model’s *inertia*, which is the sum of the squared distances between
    the instances and their closest centroids. It is roughly equal to 219.4 for the
    model on the left in [Figure 9-5](#kmeans_variability_plot), 258.6 for the model
    on the right in [Figure 9-5](#kmeans_variability_plot), and only 211.6 for the
    model in [Figure 9-3](#voronoi_plot). The `KMeans` class runs the algorithm `n_init`
    times and keeps the model with the lowest inertia. In this example, the model
    in [Figure 9-3](#voronoi_plot) will be selected (unless we are very unlucky with
    `n_init` consecutive random initializations). If you are curious, a model’s inertia
    is accessible via the `inertia_` instance variable:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决方案是使用不同的随机初始化多次运行算法，并保留最佳解决方案。随机初始化的次数由`n_init`超参数控制：默认情况下等于`10`，这意味着当您调用`fit()`时，先前描述的整个算法会运行10次，Scikit-Learn会保留最佳解决方案。但它究竟如何知道哪个解决方案最佳呢？它使用性能指标！该指标称为模型的*惯性*，它是实例与其最近质心之间的平方距离之和。对于左侧的模型，它大约等于219.4，在[图9-5](#kmeans_variability_plot)中的右侧模型为258.6，在[图9-3](#voronoi_plot)中的模型仅为211.6。`KMeans`类运行算法`n_init`次，并保留具有最低惯性的模型。在这个例子中，将选择[图9-3](#voronoi_plot)中的模型（除非我们在`n_init`连续的随机初始化中非常不幸）。如果您感兴趣，可以通过`inertia_`实例变量访问模型的惯性：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `score()` method returns the negative inertia (it’s negative because a
    predictor’s `score()` method must always respect Scikit-Learn’s “greater is better”
    rule: if a predictor is better than another, its `score()` method should return
    a greater score):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`score()`方法返回负惯性（它是负数，因为预测器的`score()`方法必须始终遵守Scikit-Learn的“分数越高越好”的规则：如果一个预测器比另一个更好，则其`score()`方法应返回更高的分数）：'
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'An important improvement to the *k*-means algorithm, *k-means++*, was proposed
    in a [2006 paper](https://homl.info/37) by David Arthur and Sergei Vassilvitskii.⁠^([2](ch09.html#idm45720207492864))
    They introduced a smarter initialization step that tends to select centroids that
    are distant from one another, and this improvement makes the *k*-means algorithm
    much less likely to converge to a suboptimal solution. The paper showed that the
    additional computation required for the smarter initialization step is well worth
    it because it makes it possible to drastically reduce the number of times the
    algorithm needs to be run to find the optimal solution. The *k*-means++ initialization
    algorithm works like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means算法的一个重要改进*k-means++*是由David Arthur和Sergei Vassilvitskii在[2006年的一篇论文](https://homl.info/37)中提出的。他们引入了一个更智能的初始化步骤，倾向于选择彼此相距较远的质心，这一改进使*k*-means算法更不可能收敛到次优解。该论文表明，为更智能的初始化步骤所需的额外计算是非常值得的，因为这使得可以大大减少运行算法以找到最佳解决方案的次数。*k*-means++初始化算法的工作方式如下：'
- en: Take one centroid **c**^((1)), chosen uniformly at random from the dataset.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中均匀随机选择一个质心**c**^((1))。
- en: Take a new centroid **c**^((*i*)), choosing an instance **x**^((*i*)) with probability
    <math><mi>D</mi><msup><mfenced><msup><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced><mn>2</mn></msup></math>
    / <math><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>D</mi><msup><mfenced><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mfenced><mn>2</mn></msup></math>,
    where D(**x**^((*i*))) is the distance between the instance **x**^((*i*)) and
    the closest centroid that was already chosen. This probability distribution ensures
    that instances farther away from already chosen centroids are much more likely
    to be selected as centroids.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个新的质心**c**^((*i*))，以概率选择实例**x**^((*i*)) <math><mi>D</mi><msup><mfenced><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mfenced><mn>2</mn></msup></math>
    / <math><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>D</mi><msup><mfenced><msup><mi
    mathvariant="bold">x</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mfenced><mn>2</mn></msup></math>的概率，其中D(**x**^((*i*)))是实例**x**^((*i*))与已选择的最近质心之间的距离。这种概率分布确保距离已选择的质心更远的实例更有可能被选择为质心。
- en: Repeat the previous step until all *k* centroids have been chosen.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上一步，直到所有*k*个质心都被选择。
- en: The `KMeans` class uses this initialization method by default.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeans`类默认使用此初始化方法。'
- en: Accelerated k-means and mini-batch k-means
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加速的k均值和小批量k均值
- en: Another improvement to the *k*-means algorithm was proposed in a [2003 paper](https://homl.info/38)
    by Charles Elkan.⁠^([3](ch09.html#idm45720207428048)) On some large datasets with
    many clusters, the algorithm can be accelerated by avoiding many unnecessary distance
    calculations. Elkan achieved this by exploiting the triangle inequality (i.e.,
    that a straight line is always the shortest distance between two points⁠^([4](ch09.html#idm45720207425136)))
    and by keeping track of lower and upper bounds for distances between instances
    and centroids. However, Elkan’s algorithm does not always accelerate training,
    and sometimes it can even slow down training significantly; it depends on the
    dataset. Still, if you want to give it a try, set `algorithm="elkan"`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个对*k*-means算法的改进是由Charles Elkan在[2003年的一篇论文](https://homl.info/38)中提出的。在一些具有许多簇的大型数据集上，通过避免许多不必要的距离计算，可以加速算法。Elkan通过利用三角不等式（即，直线始终是两点之间的最短距离⁠^([4](ch09.html#idm45720207425136)）并跟踪实例和质心之间的距离的下限和上限来实现这一点。但是，Elkan的算法并不总是加速训练，有时甚至会显著减慢训练速度；这取决于数据集。但是，如果您想尝试一下，请设置`algorithm="elkan"`。
- en: 'Yet another important variant of the *k*-means algorithm was proposed in a
    [2010 paper](https://homl.info/39) by David Sculley.⁠^([5](ch09.html#idm45720207422480))
    Instead of using the full dataset at each iteration, the algorithm is capable
    of using mini-batches, moving the centroids just slightly at each iteration. This
    speeds up the algorithm (typically by a factor of three to four) and makes it
    possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements
    this algorithm in the `MiniBatchKMeans` class, which you can use just like the
    `KMeans` class:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means算法的另一个重要变体是由David Sculley在[2010年的一篇论文](https://homl.info/39)中提出的。⁠^([5](ch09.html#idm45720207422480))
    该算法不是在每次迭代中使用完整数据集，而是能够使用小批量数据，每次迭代时轻微移动质心。这加快了算法的速度（通常提高了三到四倍），并使其能够对不适合内存的大型数据集进行聚类。Scikit-Learn在`MiniBatchKMeans`类中实现了这个算法，您可以像使用`KMeans`类一样使用它：'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If the dataset does not fit in memory, the simplest option is to use the `memmap`
    class, as we did for incremental PCA in [Chapter 8](ch08.html#dimensionality_chapter).
    Alternatively, you can pass one mini-batch at a time to the `partial_fit()` method,
    but this will require much more work, since you will need to perform multiple
    initializations and select the best one yourself.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集不适合内存，最简单的选择是使用`memmap`类，就像我们在[第8章](ch08.html#dimensionality_chapter)中对增量PCA所做的那样。或者，您可以一次传递一个小批量到`partial_fit()`方法，但这将需要更多的工作，因为您需要执行多次初始化并自行选择最佳的初始化。
- en: 'Although the mini-batch *k*-means algorithm is much faster than the regular
    *k*-means algorithm, its inertia is generally slightly worse. You can see this
    in [Figure 9-6](#minibatch_kmeans_vs_kmeans_plot): the plot on the left compares
    the inertias of mini-batch *k*-means and regular *k*-means models trained on the
    previous five-blobs dataset using various numbers of clusters *k*. The difference
    between the two curves is small, but visible. In the plot on the right, you can
    see that mini-batch *k*-means is roughly 3.5 times faster than regular *k*-means
    on this dataset.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管小批量*k*-means算法比常规*k*-means算法快得多，但其惯性通常略差一些。您可以在[图9-6](#minibatch_kmeans_vs_kmeans_plot)中看到这一点：左侧的图比较了在先前的五个斑点数据集上使用不同数量的簇*k*训练的小批量*k*-means和常规*k*-means模型的惯性。两条曲线之间的差异很小，但是可见的。在右侧的图中，您可以看到小批量*k*-means在这个数据集上大约比常规*k*-means快3.5倍。
- en: '![mls3 0906](assets/mls3_0906.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0906](assets/mls3_0906.png)'
- en: Figure 9-6\. Mini-batch *k*-means has a higher inertia than *k*-means (left)
    but it is much faster (right), especially as *k* increases
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6。小批量*k*-means的惯性高于*k*-means（左侧），但速度更快（右侧），特别是当*k*增加时
- en: Finding the optimal number of clusters
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找最佳簇的数量
- en: So far, we’ve set the number of clusters *k* to 5 because it was obvious by
    looking at the data that this was the correct number of clusters. But in general,
    it won’t be so easy to know how to set *k*, and the result might be quite bad
    if you set it to the wrong value. As you can see in [Figure 9-7](#bad_n_clusters_plot),
    for this dataset setting *k* to 3 or 8 results in fairly bad models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将簇的数量*k*设置为5，因为通过观察数据，很明显这是正确的簇数量。但通常情况下，要知道如何设置*k*并不那么容易，如果将其设置为错误的值，结果可能会非常糟糕。如您在[图9-7](#bad_n_clusters_plot)中所见，对于这个数据集，将*k*设置为3或8会导致相当糟糕的模型。
- en: You might be thinking that you could just pick the model with the lowest inertia.
    Unfortunately, it is not that simple. The inertia for *k*=3 is about 653.2, which
    is much higher than for *k*=5 (211.6). But with *k*=8, the inertia is just 119.1\.
    The inertia is not a good performance metric when trying to choose *k* because
    it keeps getting lower as we increase *k*. Indeed, the more clusters there are,
    the closer each instance will be to its closest centroid, and therefore the lower
    the inertia will be. Let’s plot the inertia as a function of *k*. When we do this,
    the curve often contains an inflexion point called the *elbow* (see [Figure 9-8](#inertia_vs_k_plot)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会认为您可以选择惯性最低的模型。不幸的是，事情并不那么简单。*k*=3的惯性约为653.2，远高于*k*=5（211.6）。但是对于*k*=8，惯性只有119.1。当我们增加*k*时，惯性并不是一个好的性能指标，因为它会不断降低。事实上，簇越多，每个实例距离其最近的质心就越近，因此惯性就会越低。让我们将惯性作为*k*的函数绘制出来。当我们这样做时，曲线通常包含一个称为“拐点”的拐点（见[图9-8](#inertia_vs_k_plot)）。
- en: '![mls3 0907](assets/mls3_0907.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0907](assets/mls3_0907.png)'
- en: 'Figure 9-7\. Bad choices for the number of clusters: when k is too small, separate
    clusters get merged (left), and when k is too large, some clusters get chopped
    into multiple pieces (right)'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7。簇数量的错误选择：当*k*太小时，独立的簇会合并在一起（左侧），当*k*太大时，一些簇会被切割成多个部分（右侧）
- en: '![mls3 0908](assets/mls3_0908.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0908](assets/mls3_0908.png)'
- en: Figure 9-8\. Plotting the inertia as a function of the number of clusters *k*
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8。将惯性作为簇数量*k*的函数绘制出来
- en: 'As you can see, the inertia drops very quickly as we increase *k* up to 4,
    but then it decreases much more slowly as we keep increasing *k*. This curve has
    roughly the shape of an arm, and there is an elbow at *k* = 4\. So, if we did
    not know better, we might think 4 was a good choice: any lower value would be
    dramatic, while any higher value would not help much, and we might just be splitting
    perfectly good clusters in half for no good reason.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，随着*k*增加到4，惯性迅速下降，但随着继续增加*k*，下降速度变得更慢。这条曲线大致呈手臂形状，并且在*k*=4处有一个拐点。因此，如果我们不知道更好的选择，我们可能会认为4是一个不错的选择：任何更低的值都会有戏剧性的效果，而任何更高的值都不会有太大帮助，我们可能只是无缘无故地将完全良好的簇一分为二。
- en: This technique for choosing the best value for the number of clusters is rather
    coarse. A more precise (but also more computationally expensive) approach is to
    use the *silhouette score*, which is the mean *silhouette coefficient* over all
    the instances. An instance’s silhouette coefficient is equal to (*b* – *a*) /
    max(*a*, *b*), where *a* is the mean distance to the other instances in the same
    cluster (i.e., the mean intra-cluster distance) and *b* is the mean nearest-cluster
    distance (i.e., the mean distance to the instances of the next closest cluster,
    defined as the one that minimizes *b*, excluding the instance’s own cluster).
    The silhouette coefficient can vary between –1 and +1\. A coefficient close to
    +1 means that the instance is well inside its own cluster and far from other clusters,
    while a coefficient close to 0 means that it is close to a cluster boundary; finally,
    a coefficient close to –1 means that the instance may have been assigned to the
    wrong cluster.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the silhouette score, you can use Scikit-Learn’s `silhouette_score()`
    function, giving it all the instances in the dataset and the labels they were
    assigned:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s compare the silhouette scores for different numbers of clusters (see [Figure 9-9](#silhouette_score_vs_k_plot)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0909](assets/mls3_0909.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Figure 9-9\. Selecting the number of clusters *k* using the silhouette score
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, this visualization is much richer than the previous one: although
    it confirms that *k* = 4 is a very good choice, it also highlights the fact that
    *k* = 5 is quite good as well, and much better than *k* = 6 or 7\. This was not
    visible when comparing inertias.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: An even more informative visualization is obtained when we plot every instance’s
    silhouette coefficient, sorted by the clusters they are assigned to and by the
    value of the coefficient. This is called a *silhouette diagram* (see [Figure 9-10](#silhouette_analysis_plot)).
    Each diagram contains one knife shape per cluster. The shape’s height indicates
    the number of instances in the cluster, and its width represents the sorted silhouette
    coefficients of the instances in the cluster (wider is better).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The vertical dashed lines represent the mean silhouette score for each number
    of clusters. When most of the instances in a cluster have a lower coefficient
    than this score (i.e., if many of the instances stop short of the dashed line,
    ending to the left of it), then the cluster is rather bad since this means its
    instances are much too close to other clusters. Here we can see that when *k*
    = 3 or 6, we get bad clusters. But when *k* = 4 or 5, the clusters look pretty
    good: most instances extend beyond the dashed line, to the right and closer to
    1.0\. When *k* = 4, the cluster at index 1 (the second from the bottom) is rather
    big. When *k* = 5, all clusters have similar sizes. So, even though the overall
    silhouette score from *k* = 4 is slightly greater than for *k* = 5, it seems like
    a good idea to use *k* = 5 to get clusters of similar sizes.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0910](assets/mls3_0910.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Figure 9-10\. Analyzing the silhouette diagrams for various values of *k*
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Limits of k-means
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite its many merits, most notably being fast and scalable, *k*-means is
    not perfect. As we saw, it is necessary to run the algorithm several times to
    avoid suboptimal solutions, plus you need to specify the number of clusters, which
    can be quite a hassle. Moreover, *k*-means does not behave very well when the
    clusters have varying sizes, different densities, or nonspherical shapes. For
    example, [Figure 9-11](#bad_kmeans_plot) shows how *k*-means clusters a dataset
    containing three ellipsoidal clusters of different dimensions, densities, and
    orientations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, neither of these solutions is any good. The solution on the
    left is better, but it still chops off 25% of the middle cluster and assigns it
    to the cluster on the right. The solution on the right is just terrible, even
    though its inertia is lower. So, depending on the data, different clustering algorithms
    may perform better. On these types of elliptical clusters, Gaussian mixture models
    work great.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0911](assets/mls3_0911.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. k-means fails to cluster these ellipsoidal blobs properly
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to scale the input features (see [Chapter 2](ch02.html#project_chapter))
    before you run *k*-means, or the clusters may be very stretched and *k*-means
    will perform poorly. Scaling the features does not guarantee that all the clusters
    will be nice and spherical, but it generally helps *k*-means.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at a few ways we can benefit from clustering. We will use *k*-means,
    but feel free to experiment with other clustering algorithms.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Using Clustering for Image Segmentation
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Image segmentation* is the task of partitioning an image into multiple segments.
    There are several variants:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In *color segmentation*, pixels with a similar color get assigned to the same
    segment. This is sufficient in many applications. For example, if you want to
    analyze satellite images to measure how much total forest area there is in a region,
    color segmentation may be just fine.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *semantic segmentation*, all pixels that are part of the same object type
    get assigned to the same segment. For example, in a self-driving car’s vision
    system, all pixels that are part of a pedestrian’s image might be assigned to
    the “pedestrian” segment (there would be one segment containing all the pedestrians).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *instance segmentation*, all pixels that are part of the same individual
    object are assigned to the same segment. In this case there would be a different
    segment for each pedestrian.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state of the art in semantic or instance segmentation today is achieved
    using complex architectures based on convolutional neural networks (see [Chapter 14](ch14.html#cnn_chapter)).
    In this chapter we are going to focus on the (much simpler) color segmentation
    task, using *k*-means.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing the Pillow package (successor to the Python Imaging
    Library, PIL), which we’ll then use to load the *ladybug.png* image (see the upper-left
    image in [Figure 9-12](#image_segmentation_plot)), assuming it’s located at `filepath`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The image is represented as a 3D array. The first dimension’s size is the height;
    the second is the width; and the third is the number of color channels, in this
    case red, green, and blue (RGB). In other words, for each pixel there is a 3D
    vector containing the intensities of red, green, and blue as unsigned 8-bit integers
    between 0 and 255\. Some images may have fewer channels (such as grayscale images,
    which only have one), and some images may have more channels (such as images with
    an additional *alpha channel* for transparency, or satellite images, which often
    contain channels for additional light frequencies (like infrared).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code reshapes the array to get a long list of RGB colors, then
    it clusters these colors using *k*-means with eight clusters. It creates a `segmented_img`
    array containing the nearest cluster center for each pixel (i.e., the mean color
    of each pixel’s cluster), and lastly it reshapes this array to the original image
    shape. The third line uses advanced NumPy indexing; for example, if the first
    10 labels in `kmeans_.labels_` are equal to 1, then the first 10 colors in `segmented_img`
    are equal to `kmeans.cluster_centers_[1]`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This outputs the image shown in the upper right of [Figure 9-12](#image_segmentation_plot).
    You can experiment with various numbers of clusters, as shown in the figure. When
    you use fewer than eight clusters, notice that the ladybug’s flashy red color
    fails to get a cluster of its own: it gets merged with colors from the environment.
    This is because *k*-means prefers clusters of similar sizes. The ladybug is small—much
    smaller than the rest of the image—so even though its color is flashy, *k*-means
    fails to dedicate a cluster to it.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出了[图9-12](#image_segmentation_plot)右上方显示的图像。您可以尝试不同数量的簇，如图所示。当您使用少于八个簇时，请注意瓢虫鲜艳的红色未能获得自己的簇：它与环境中的颜色合并在一起。这是因为*k*-means更喜欢相似大小的簇。瓢虫很小——比图像的其他部分小得多——所以即使它的颜色很鲜艳，*k*-means也无法为其分配一个簇。
- en: '![mls3 0912](assets/mls3_0912.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0912](assets/mls3_0912.png)'
- en: Figure 9-12\. Image segmentation using *k*-means with various numbers of color
    clusters
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12\. 使用*k*-means进行图像分割，使用不同数量的颜色簇
- en: That wasn’t too hard, was it? Now let’s look at another application of clustering.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太难，对吧？现在让我们看看聚类的另一个应用。
- en: Using Clustering for Semi-Supervised Learning
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用聚类进行半监督学习
- en: 'Another use case for clustering is in semi-supervised learning, when we have
    plenty of unlabeled instances and very few labeled instances. In this section,
    we’ll use the digits dataset, which is a simple MNIST-like dataset containing
    1,797 grayscale 8 × 8 images representing the digits 0 to 9\. First, let’s load
    and split the dataset (it’s already shuffled):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的另一个用例是在半监督学习中，当我们有大量未标记的实例和很少带标签的实例时。在本节中，我们将使用数字数据集，这是一个简单的类似MNIST的数据集，包含1,797个灰度8×8图像，代表数字0到9。首先，让我们加载并拆分数据集（已经洗牌）：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will pretend we only have labels for 50 instances. To get a baseline performance,
    let’s train a logistic regression model on these 50 labeled instances:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假装只有50个实例的标签。为了获得基准性能，让我们在这50个带标签的实例上训练一个逻辑回归模型：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can then measure the accuracy of this model on the test set (note that the
    test set must be labeled):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以在测试集上测量这个模型的准确率（请注意测试集必须带标签）：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model’s accuracy is just 74.8%. That’s not great: indeed, if you try training
    the model on the full training set, you will find that it will reach about 90.7%
    accuracy. Let’s see how we can do better. First, let’s cluster the training set
    into 50 clusters. Then, for each cluster, we’ll find the image closest to the
    centroid. We’ll call these images the *representative images*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确率只有74.8%。这并不好：实际上，如果您尝试在完整的训练集上训练模型，您会发现它将达到约90.7%的准确率。让我们看看如何做得更好。首先，让我们将训练集聚类成50个簇。然后，对于每个簇，我们将找到距离质心最近的图像。我们将称这些图像为*代表性图像*：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Figure 9-13](#representative_images_plot) shows the 50 representative images.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-13](#representative_images_plot)显示了50个代表性图像。'
- en: '![mls3 0913](assets/mls3_0913.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0913](assets/mls3_0913.png)'
- en: Figure 9-13\. Fifty representative digit images (one per cluster)
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13\. 五十个代表性数字图像（每个簇一个）
- en: 'Let’s look at each image and manually label them:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个图像并手动标记它们：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we have a dataset with just 50 labeled instances, but instead of being
    random instances, each of them is a representative image of its cluster. Let’s
    see if the performance is any better:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个只有50个带标签实例的数据集，但它们不是随机实例，而是每个实例都是其簇的代表性图像。让我们看看性能是否有所提高：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Wow! We jumped from 74.8% accuracy to 84.9%, although we are still only training
    the model on 50 instances. Since it is often costly and painful to label instances,
    especially when it has to be done manually by experts, it is a good idea to label
    representative instances rather than just random instances.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们的准确率从74.8%跳升到84.9%，尽管我们仍然只在50个实例上训练模型。由于标记实例通常成本高昂且繁琐，特别是当必须由专家手动完成时，将代表性实例标记而不仅仅是随机实例是一个好主意。
- en: 'But perhaps we can go one step further: what if we propagated the labels to
    all the other instances in the same cluster? This is called *label propagation*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但也许我们可以再进一步：如果我们将标签传播到同一簇中的所有其他实例怎么办？这被称为*标签传播*：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let’s train the model again and look at its performance:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次训练模型并查看其性能：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We got another significant accuracy boost! Let’s see if we can do even better
    by ignoring the 1% of instances that are farthest from their cluster center: this
    should eliminate some outliers. The following code first computes the distance
    from each instance to its closest cluster center, then for each cluster it sets
    the 1% largest distances to –1\. Lastly, it creates a set without these instances
    marked with a –1 distance:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们又获得了显著的准确率提升！让我们看看如果忽略距离其簇中心最远的1%实例是否能做得更好：这应该消除一些异常值。以下代码首先计算每个实例到其最近簇中心的距离，然后对于每个簇，将1%最大的距离设置为-1。最后，它创建一个不包含这些标记为-1距离的实例的集合：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let’s train the model again on this partially propagated dataset and see
    what accuracy we get:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次在这部分传播的数据集上训练模型，看看我们能获得什么准确率：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Nice! With just 50 labeled instances (only 5 examples per class on average!)
    we got 90.9% accuracy, which is actually slightly higher than the performance
    we got on the fully labeled digits dataset (90.7%). This is partly thanks to the
    fact that we dropped some outliers, and partly because the propagated labels are
    actually pretty good—their accuracy is about 97.5%, as the following code shows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！只有50个带标签的实例（平均每类只有5个示例！）我们获得了90.9%的准确率，实际上略高于我们在完全带标签的数字数据集上获得的性能（90.7%）。这在一定程度上要归功于我们删除了一些异常值，另一部分原因是传播的标签实际上相当不错——它们的准确率约为97.5%，如下面的代码所示：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Tip
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Scikit-Learn also offers two classes that can propagate labels automatically:
    `LabelSpreading` and `LabelPropagation` in the `sklearn.semi_supervised` package.
    Both classes construct a similarity matrix between all the instances, and iteratively
    propagate labels from labeled instances to similar unlabeled instances. There’s
    also a very different class called `SelfTrainingClassifier` in the same package:
    you give it a base classifier (such as a `RandomForestClassifier`) and it trains
    it on the labeled instances, then uses it to predict labels for the unlabeled
    samples. It then updates the training set with the labels it is most confident
    about, and repeats this process of training and labeling until it cannot add labels
    anymore. These techniques are not magic bullets, but they can occasionally give
    your model a little boost.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn还提供了两个可以自动传播标签的类：`sklearn.semi_supervised`包中的`LabelSpreading`和`LabelPropagation`。这两个类构建了所有实例之间的相似性矩阵，并迭代地将标签从有标签的实例传播到相似的无标签实例。还有一个非常不同的类叫做`SelfTrainingClassifier`，在相同的包中：你给它一个基础分类器（比如`RandomForestClassifier`），它会在有标签的实例上训练它，然后用它来预测无标签样本的标签。然后，它会使用它最有信心的标签更新训练集，并重复这个训练和标记的过程，直到不能再添加标签为止。这些技术并非万能之策，但它们偶尔可以为你的模型提供一点提升。
- en: Before we move on to Gaussian mixture models, let’s take a look at DBSCAN, another
    popular clustering algorithm that illustrates a very different approach based
    on local density estimation. This approach allows the algorithm to identify clusters
    of arbitrary shapes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论高斯混合模型之前，让我们看一下DBSCAN，这是另一种流行的聚类算法，它基于局部密度估计展示了一种非常不同的方法。这种方法允许算法识别任意形状的簇。
- en: DBSCAN
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN
- en: 'The *density-based spatial clustering of applications with noise* (DBSCAN)
    algorithm defines clusters as continuous regions of high density. Here is how
    it works:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*带有噪声的基于密度的空间聚类应用*（DBSCAN）算法将连续的高密度区域定义为簇。它的工作原理如下：'
- en: For each instance, the algorithm counts how many instances are located within
    a small distance ε (epsilon) from it. This region is called the instance’s *ε-neighborhood*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个实例，算法计算距离它小于一个小距离ε（epsilon）的实例有多少个。这个区域被称为实例的*ε-邻域*。
- en: If an instance has at least `min_samples` instances in its ε-neighborhood (including
    itself), then it is considered a *core instance*. In other words, core instances
    are those that are located in dense regions.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个实例在其ε-邻域中至少有`min_samples`个实例（包括自身），那么它被视为*核心实例*。换句话说，核心实例是位于密集区域的实例。
- en: All instances in the neighborhood of a core instance belong to the same cluster.
    This neighborhood may include other core instances; therefore, a long sequence
    of neighboring core instances forms a single cluster.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有在核心实例邻域中的实例都属于同一个簇。这个邻域可能包括其他核心实例；因此，一长串相邻的核心实例形成一个单一的簇。
- en: Any instance that is not a core instance and does not have one in its neighborhood
    is considered an anomaly.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何不是核心实例且其邻域中没有核心实例的实例都被视为异常。
- en: 'This algorithm works well if all the clusters are well separated by low-density
    regions. The `DBSCAN` class in Scikit-Learn is as simple to use as you might expect.
    Let’s test it on the moons dataset, introduced in [Chapter 5](ch05.html#svm_chapter):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有簇都被低密度区域很好地分隔开，这个算法就会很有效。Scikit-Learn中的`DBSCAN`类使用起来就像你期望的那样简单。让我们在[第5章](ch05.html#svm_chapter)中介绍的moons数据集上测试一下：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The labels of all the instances are now available in the `labels_` instance
    variable:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有实例的标签都可以在`labels_`实例变量中找到：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Notice that some instances have a cluster index equal to –1, which means that
    they are considered as anomalies by the algorithm. The indices of the core instances
    are available in the `core_sample_indices_` instance variable, and the core instances
    themselves are available in the `components_` instance variable:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一些实例的簇索引等于-1，这意味着它们被算法视为异常值。核心实例的索引可以在`core_sample_indices_`实例变量中找到，核心实例本身可以在`components_`实例变量中找到：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This clustering is represented in the lefthand plot of [Figure 9-14](#dbscan_plot).
    As you can see, it identified quite a lot of anomalies, plus seven different clusters.
    How disappointing! Fortunately, if we widen each instance’s neighborhood by increasing
    `eps` to 0.2, we get the clustering on the right, which looks perfect. Let’s continue
    with this model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚类在[图9-14](#dbscan_plot)的左图中表示。正如你所看到的，它识别出了相当多的异常值，以及七个不同的簇。多么令人失望！幸运的是，如果我们通过增加`eps`到0.2来扩大每个实例的邻域，我们得到了右侧的聚类，看起来完美。让我们继续使用这个模型。
- en: '![mls3 0914](assets/mls3_0914.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0914](assets/mls3_0914.png)'
- en: Figure 9-14\. DBSCAN clustering using two different neighborhood radiuses
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-14。使用两个不同邻域半径的DBSCAN聚类
- en: 'Surprisingly, the `DBSCAN` class does not have a `predict()` method, although
    it has a `fit_predict()` method. In other words, it cannot predict which cluster
    a new instance belongs to. This decision was made because different classification
    algorithms can be better for different tasks, so the authors decided to let the
    user choose which one to use. Moreover, it’s not hard to implement. For example,
    let’s train a `KNeighborsClassifier`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，`DBSCAN`类没有`predict()`方法，尽管它有一个`fit_predict()`方法。换句话说，它不能预测一个新实例属于哪个簇。这个决定是因为不同的分类算法对不同的任务可能更好，所以作者决定让用户选择使用哪个。此外，这并不难实现。例如，让我们训练一个`KNeighborsClassifier`：
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, given a few new instances, we can predict which clusters they most likely
    belong to and even estimate a probability for each cluster:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，给定一些新实例，我们可以预测它们最有可能属于哪些簇，甚至为每个簇估计一个概率：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that we only trained the classifier on the core instances, but we could
    also have chosen to train it on all the instances, or all but the anomalies: this
    choice depends on the final task.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们只在核心实例上训练了分类器，但我们也可以选择在所有实例上或除异常值外的所有实例上进行训练：这个选择取决于最终的任务。
- en: 'The decision boundary is represented in [Figure 9-15](#cluster_classification_plot)
    (the crosses represent the four instances in `X_new`). Notice that since there
    is no anomaly in the training set, the classifier always chooses a cluster, even
    when that cluster is far away. It is fairly straightforward to introduce a maximum
    distance, in which case the two instances that are far away from both clusters
    are classified as anomalies. To do this, use the `kneighbors()` method of the
    `KNeighborsClassifier`. Given a set of instances, it returns the distances and
    the indices of the *k*-nearest neighbors in the training set (two matrices, each
    with *k* columns):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![mls3 0915](assets/mls3_0915.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 9-15\. Decision boundary between two clusters
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In short, DBSCAN is a very simple yet powerful algorithm capable of identifying
    any number of clusters of any shape. It is robust to outliers, and it has just
    two hyperparameters (`eps` and `min_samples`). If the density varies significantly
    across the clusters, however, or if there’s no sufficiently low-density region
    around some clusters, DBSCAN can struggle to capture all the clusters properly.
    Moreover, its computational complexity is roughly *O*(*m*²*n*), so it does not
    scale well to large datasets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You may also want to try *hierarchical DBSCAN* (HDBSCAN), which is implemented
    in the [`scikit-learn-contrib` project](https://github.com/scikit-learn-contrib/hdbscan),
    as it is usually better than DBSCAN at finding clusters of varying densities.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Other Clustering Algorithms
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn implements several more clustering algorithms that you should
    take a look at. I cannot cover them all in detail here, but here is a brief overview:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative clustering
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles
    floating on water and gradually attaching to each other until there’s one big
    group of bubbles. Similarly, at each iteration, agglomerative clustering connects
    the nearest pair of clusters (starting with individual instances). If you drew
    a tree with a branch for every pair of clusters that merged, you would get a binary
    tree of clusters, where the leaves are the individual instances. This approach
    can capture clusters of various shapes; it also produces a flexible and informative
    cluster tree instead of forcing you to choose a particular cluster scale, and
    it can be used with any pairwise distance. It can scale nicely to large numbers
    of instances if you provide a connectivity matrix, which is a sparse *m* × *m*
    matrix that indicates which pairs of instances are neighbors (e.g., returned by
    `sklearn.neighbors.kneighbors_graph()`). Without a connectivity matrix, the algorithm
    does not scale well to large datasets.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: BIRCH
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'The balanced iterative reducing and clustering using hierarchies (BIRCH) algorithm
    was designed specifically for very large datasets, and it can be faster than batch
    *k*-means, with similar results, as long as the number of features is not too
    large (<20). During training, it builds a tree structure containing just enough
    information to quickly assign each new instance to a cluster, without having to
    store all the instances in the tree: this approach allows it to use limited memory
    while handling huge datasets.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Mean-shift
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm starts by placing a circle centered on each instance; then for
    each circle it computes the mean of all the instances located within it, and it
    shifts the circle so that it is centered on the mean. Next, it iterates this mean-shifting
    step until all the circles stop moving (i.e., until each of them is centered on
    the mean of the instances it contains). Mean-shift shifts the circles in the direction
    of higher density, until each of them has found a local density maximum. Finally,
    all the instances whose circles have settled in the same place (or close enough)
    are assigned to the same cluster. Mean-shift has some of the same features as
    DBSCAN, like how it can find any number of clusters of any shape, it has very
    few hyperparameters (just one—the radius of the circles, called the *bandwidth*),
    and it relies on local density estimation. But unlike DBSCAN, mean-shift tends
    to chop clusters into pieces when they have internal density variations. Unfortunately,
    its computational complexity is *O*(*m*²*n*), so it is not suited for large datasets.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法首先在每个实例上放置一个圆圈；然后对于每个圆圈，计算其中所有实例的平均值，并将圆圈移动，使其位于平均值中心。接下来，迭代这个平移步骤，直到所有圆圈停止移动（即，直到它们每个都位于包含的实例的平均值上）。均值漂移将圆圈移动到更高密度的方向，直到每个圆圈找到局部密度最大值。最后，所有圆圈已经定居在同一位置（或足够接近）的实例被分配到同一个簇。均值漂移具有与DBSCAN相同的一些特性，例如它可以找到任意形状的任意数量的簇，它只有很少的超参数（只有一个——圆圈的半径，称为*带宽*），并且依赖于局部密度估计。但与DBSCAN不同的是，均值漂移在簇内密度变化时倾向于将簇切割成片。不幸的是，它的计算复杂度为*O*(*m*²*n*)，因此不适用于大型数据集。
- en: Affinity propagation
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和传播
- en: 'In this algorithm, instances repeatedly exchange messages between one another
    until every instance has elected another instance (or itself) to represent it.
    These elected instances are called *exemplars*. Each exemplar and all the instances
    that elected it form one cluster. In real-life politics, you typically want to
    vote for a candidate whose opinions are similar to yours, but you also want them
    to win the election, so you might choose a candidate you don’t fully agree with,
    but who is more popular. You typically evaluate popularity through polls. Affinity
    propagation works in a similar way, and it tends to choose exemplars located near
    the center of clusters, similar to *k*-means. But unlike with *k*-means, you don’t
    have to pick a number of clusters ahead of time: it is determined during training.
    Moreover, affinity propagation can deal nicely with clusters of different sizes.
    Sadly, this algorithm has a computational complexity of *O*(*m*²), so it is not
    suited for large datasets.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，实例之间反复交换消息，直到每个实例都选出另一个实例（或自己）来代表它。这些被选出的实例被称为*典范*。每个典范和所有选举它的实例形成一个簇。在现实生活中的政治中，你通常希望投票给一个观点与你相似的候选人，但你也希望他们赢得选举，所以你可能会选择一个你不完全同意的候选人，但他更受欢迎。你通常通过民意调查来评估受欢迎程度。亲和传播以类似的方式工作，它倾向于选择位于簇中心附近的典范，类似于*k*-means。但与*k*-means不同的是，你不必提前选择簇的数量：它是在训练过程中确定的。此外，亲和传播可以很好地处理不同大小的簇。不幸的是，这个算法的计算复杂度为*O*(*m*²)，因此不适用于大型数据集。
- en: Spectral clustering
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类
- en: This algorithm takes a similarity matrix between the instances and creates a
    low-dimensional embedding from it (i.e., it reduces the matrix’s dimensionality),
    then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s
    implementation uses *k*-means). Spectral clustering can capture complex cluster
    structures, and it can also be used to cut graphs (e.g., to identify clusters
    of friends on a social network). It does not scale well to large numbers of instances,
    and it does not behave well when the clusters have very different sizes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用实例之间的相似性矩阵，并从中创建一个低维嵌入（即，降低矩阵的维度），然后在这个低维空间中使用另一个聚类算法（Scikit-Learn的实现使用*k*-means）。谱聚类可以捕捉复杂的簇结构，也可以用于切割图形（例如，识别社交网络上的朋友簇）。当实例数量较大时，它不会很好地扩展，并且当簇的大小差异很大时，它的表现也不好。
- en: Now let’s dive into Gaussian mixture models, which can be used for density estimation,
    clustering, and anomaly detection.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入研究高斯混合模型，它可以用于密度估计、聚类和异常检测。
- en: Gaussian Mixtures
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: A *Gaussian mixture model* (GMM) is a probabilistic model that assumes that
    the instances were generated from a mixture of several Gaussian distributions
    whose parameters are unknown. All the instances generated from a single Gaussian
    distribution form a cluster that typically looks like an ellipsoid. Each cluster
    can have a different ellipsoidal shape, size, density, and orientation, just like
    in [Figure 9-11](#bad_kmeans_plot). When you observe an instance, you know it
    was generated from one of the Gaussian distributions, but you are not told which
    one, and you do not know what the parameters of these distributions are.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*高斯混合模型*（GMM）是一个概率模型，假设实例是从几个高斯分布的混合中生成的，这些分布的参数是未知的。从单个高斯分布生成的所有实例形成一个簇，通常看起来像一个椭圆体。每个簇可以具有不同的椭圆形状、大小、密度和方向，就像[图9-11](#bad_kmeans_plot)中所示。当你观察一个实例时，你知道它是从高斯分布中生成的一个，但你不知道是哪一个，也不知道这些分布的参数是什么。'
- en: 'There are several GMM variants. In the simplest variant, implemented in the
    `GaussianMixture` class, you must know in advance the number *k* of Gaussian distributions.
    The dataset **X** is assumed to have been generated through the following probabilistic
    process:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种GMM变体。在`GaussianMixture`类中实现的最简单的变体中，你必须事先知道高斯分布的数量*k*。假定数据集**X**是通过以下概率过程生成的：
- en: For each instance, a cluster is picked randomly from among *k* clusters. The
    probability of choosing the *j*^(th) cluster is the cluster’s weight *ϕ*^((*j*)).⁠^([6](ch09.html#idm45720205974560))
    The index of the cluster chosen for the *i*^(th) instance is noted *z*^((*i*)).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个实例，从*k*个簇中随机选择一个。选择第*j*个簇的概率是簇的权重*ϕ*^((*j*)).⁠^([6](ch09.html#idm45720205974560))
    选择第*i*个实例的簇的索引被记为*z*^((*i*)).
- en: If the *i*^(th) instance was assigned to the *j*^(th) cluster (i.e., *z*^((*i*))
    = *j*), then the location **x**^((*i*)) of this instance is sampled randomly from
    the Gaussian distribution with mean **μ**^((*j*)) and covariance matrix **Σ**^((*j*)).
    This is noted **x**^((*i*)) ~ 𝒩(**μ**^((*j*)), **Σ**^((*j*))).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果第*i*个实例被分配到第*j*个聚类（即*z*^((*i*)) = *j*），则该实例的位置**x**^((*i*))是从具有均值**μ**^((*j*))和协方差矩阵**Σ**^((*j*))的高斯分布中随机抽样的。这表示**x**^((*i*))
    ~ 𝒩(**μ**^((*j*)), **Σ**^((*j*))).
- en: 'So what can you do with such a model? Well, given the dataset **X**, you typically
    want to start by estimating the weights **ϕ** and all the distribution parameters
    **μ**^((1)) to **μ**^((*k*)) and **Σ**^((1)) to **Σ**^((*k*)). Scikit-Learn’s
    `GaussianMixture` class makes this super easy:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您可以用这样的模型做什么呢？嗯，给定数据集**X**，您通常会从估计权重**ϕ**和所有分布参数**μ**^((1))到**μ**^((*k*))和**Σ**^((1))到**Σ**^((*k*))开始。Scikit-Learn的`GaussianMixture`类使这变得非常容易：
- en: '[PRE29]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s look at the parameters that the algorithm estimated:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看算法估计的参数：
- en: '[PRE30]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Great, it worked fine! Indeed, two of the three clusters were generated with
    500 instances each, while the third cluster only contains 250 instances. So the
    true cluster weights are 0.4, 0.4, and 0.2, respectively, and that’s roughly what
    the algorithm found. Similarly, the true means and covariance matrices are quite
    close to those found by the algorithm. But how? This class relies on the *expectation-maximization*
    (EM) algorithm, which has many similarities with the *k*-means algorithm: it also
    initializes the cluster parameters randomly, then it repeats two steps until convergence,
    first assigning instances to clusters (this is called the *expectation step*)
    and then updating the clusters (this is called the *maximization step*). Sounds
    familiar, right? In the context of clustering, you can think of EM as a generalization
    of *k*-means that not only finds the cluster centers (**μ**^((1)) to **μ**^((*k*))),
    but also their size, shape, and orientation (**Σ**^((1)) to **Σ**^((*k*))), as
    well as their relative weights (*ϕ*^((1)) to *ϕ*^((*k*))). Unlike *k*-means, though,
    EM uses soft cluster assignments, not hard assignments. For each instance, during
    the expectation step, the algorithm estimates the probability that it belongs
    to each cluster (based on the current cluster parameters). Then, during the maximization
    step, each cluster is updated using *all* the instances in the dataset, with each
    instance weighted by the estimated probability that it belongs to that cluster.
    These probabilities are called the *responsibilities* of the clusters for the
    instances. During the maximization step, each cluster’s update will mostly be
    impacted by the instances it is most responsible for.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，运行得很顺利！事实上，三个聚类中有两个分别生成了500个实例，而第三个聚类只包含250个实例。因此，真实的聚类权重分别为0.4、0.4和0.2，这大致是算法找到的。同样，真实的均值和协方差矩阵与算法找到的相当接近。但是如何实现的呢？这个类依赖于*期望最大化*（EM）算法，它与*k*-means算法有许多相似之处：它也随机初始化聚类参数，然后重复两个步骤直到收敛，首先将实例分配给聚类（这称为*期望步骤*），然后更新聚类（这称为*最大化步骤*）。听起来很熟悉，对吧？在聚类的背景下，您可以将EM视为*k*-means的一种泛化，它不仅找到聚类中心（**μ**^((1))到**μ**^((*k*))），还找到它们的大小、形状和方向（**Σ**^((1))到**Σ**^((*k*))），以及它们的相对权重（*ϕ*^((1))到*ϕ*^((*k*))）。不过，与*k*-means不同，EM使用软聚类分配，而不是硬分配。对于每个实例，在期望步骤中，算法根据当前的聚类参数估计它属于每个聚类的概率。然后，在最大化步骤中，每个聚类使用数据集中的*所有*实例进行更新，每个实例的权重由估计的属于该聚类的概率加权。这些概率称为聚类对实例的*责任*。在最大化步骤中，每个聚类的更新将主要受到它最负责的实例的影响。
- en: Warning
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Unfortunately, just like *k*-means, EM can end up converging to poor solutions,
    so it needs to be run several times, keeping only the best solution. This is why
    we set `n_init` to 10\. Be careful: by default `n_init` is set to 1.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，就像*k*-means一样，EM可能会收敛到较差的解决方案，因此需要多次运行，仅保留最佳解决方案。这就是为什么我们将`n_init`设置为10。请注意：默认情况下，`n_init`设置为1。
- en: 'You can check whether or not the algorithm converged and how many iterations
    it took:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查算法是否收敛以及需要多少次迭代：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that you have an estimate of the location, size, shape, orientation, and
    relative weight of each cluster, the model can easily assign each instance to
    the most likely cluster (hard clustering) or estimate the probability that it
    belongs to a particular cluster (soft clustering). Just use the `predict()` method
    for hard clustering, or the `predict_proba()` method for soft clustering:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经估计了每个聚类的位置、大小、形状、方向和相对权重，模型可以轻松地将每个实例分配到最可能的聚类（硬聚类），或者估计它属于特定聚类的概率（软聚类）。只需使用`predict()`方法进行硬聚类，或者使用`predict_proba()`方法进行软聚类：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'A Gaussian mixture model is a *generative model*, meaning you can sample new
    instances from it (note that they are ordered by cluster index):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型是一种*生成模型*，这意味着您可以从中抽样新实例（请注意，它们按簇索引排序）：
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is also possible to estimate the density of the model at any given location.
    This is achieved using the `score_samples()` method: for each instance it is given,
    this method estimates the log of the *probability density function* (PDF) at that
    location. The greater the score, the higher the density:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以估计模型在任何给定位置的密度。这是通过使用`score_samples()`方法实现的：对于给定的每个实例，该方法估计该位置处的*概率密度函数*（PDF）的对数。得分越高，密度越大：
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you compute the exponential of these scores, you get the value of the PDF
    at the location of the given instances. These are not probabilities, but probability
    *densities*: they can take on any positive value, not just a value between 0 and
    1\. To estimate the probability that an instance will fall within a particular
    region, you would have to integrate the PDF over that region (if you do so over
    the entire space of possible instance locations, the result will be 1).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算这些分数的指数，您将得到给定实例位置处PDF的值。这些不是概率，而是概率*密度*：它们可以取任何正值，而不仅仅是0到1之间的值。要估计实例将落入特定区域的概率，您需要在该区域上积分（如果在可能实例位置的整个空间上这样做，结果将为1）。
- en: '[Figure 9-16](#gaussian_mixtures_plot) shows the cluster means, the decision
    boundaries (dashed lines), and the density contours of this model.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-16](#gaussian_mixtures_plot)显示了聚类均值、决策边界（虚线）和该模型的密度轮廓。'
- en: '![mls3 0916](assets/mls3_0916.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0916](assets/mls3_0916.png)'
- en: Figure 9-16\. Cluster means, decision boundaries, and density contours of a
    trained Gaussian mixture model
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-16\. 训练的高斯混合模型的聚类均值、决策边界和密度轮廓
- en: 'Nice! The algorithm clearly found an excellent solution. Of course, we made
    its task easy by generating the data using a set of 2D Gaussian distributions
    (unfortunately, real-life data is not always so Gaussian and low-dimensional).
    We also gave the algorithm the correct number of clusters. When there are many
    dimensions, or many clusters, or few instances, EM can struggle to converge to
    the optimal solution. You might need to reduce the difficulty of the task by limiting
    the number of parameters that the algorithm has to learn. One way to do this is
    to limit the range of shapes and orientations that the clusters can have. This
    can be achieved by imposing constraints on the covariance matrices. To do this,
    set the `covariance_type` hyperparameter to one of the following values:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！算法显然找到了一个很好的解决方案。当然，我们通过使用一组二维高斯分布生成数据使其任务变得容易（不幸的是，现实生活中的数据并不总是如此高斯和低维）。我们还给出了正确的聚类数。当维度很多、聚类很多或实例很少时，EM可能会难以收敛到最佳解决方案。您可能需要通过限制算法需要学习的参数数量来降低任务的难度。一种方法是限制聚类可以具有的形状和方向的范围。这可以通过对协方差矩阵施加约束来实现。为此，请将`covariance_type`超参数设置为以下值之一：
- en: '`"spherical"`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`"spherical"`'
- en: All clusters must be spherical, but they can have different diameters (i.e.,
    different variances).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 所有聚类必须是球形的，但它们可以具有不同的直径（即，不同的方差）。
- en: '`"diag"`'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`"diag"`'
- en: Clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s
    axes must be parallel to the coordinate axes (i.e., the covariance matrices must
    be diagonal).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以采用任何椭球形状的任何大小，但椭球体的轴必须与坐标轴平行（即，协方差矩阵必须是对角线的）。
- en: '`"tied"`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`"tied"`'
- en: All clusters must have the same ellipsoidal shape, size, and orientation (i.e.,
    all clusters share the same covariance matrix).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所有聚类必须具有相同的椭球形状、大小和方向（即，所有聚类共享相同的协方差矩阵）。
- en: By default, `covariance_type` is equal to `"full"`, which means that each cluster
    can take on any shape, size, and orientation (it has its own unconstrained covariance
    matrix). [Figure 9-17](#covariance_type_plot) plots the solutions found by the
    EM algorithm when `covariance_type` is set to `"tied"` or `"spherical"`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`covariance_type`等于`"full"`，这意味着每个聚类可以采用任何形状、大小和方向（它有自己的无约束协方差矩阵）。[图9-17](#covariance_type_plot)显示了当`covariance_type`设置为`"tied"`或`"spherical"`时EM算法找到的解决方案。
- en: '![mls3 0917](assets/mls3_0917.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0917](assets/mls3_0917.png)'
- en: Figure 9-17\. Gaussian mixtures for tied clusters (left) and spherical clusters
    (right)
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-17\. 绑定聚类（左）和球形聚类（右）的高斯混合模型
- en: Note
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The computational complexity of training a `GaussianMixture` model depends on
    the number of instances *m*, the number of dimensions *n*, the number of clusters
    *k*, and the constraints on the covariance matrices. If `covariance_type` is `"spherical"`
    or `"diag"`, it is *O*(*kmn*), assuming the data has a clustering structure. If
    `covariance_type` is `"tied"` or `"full"`, it is *O*(*kmn*² + *kn*³), so it will
    not scale to large numbers of features.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 训练`GaussianMixture`模型的计算复杂度取决于实例数*m*、维度数*n*、聚类数*k*以及协方差矩阵的约束。如果`covariance_type`为`"spherical"`或`"diag"`，则为*O*(*kmn*)，假设数据具有聚类结构。如果`covariance_type`为`"tied"`或`"full"`，则为*O*(*kmn*²
    + *kn*³)，因此不适用于大量特征。
- en: Gaussian mixture models can also be used for anomaly detection. We’ll see how
    in the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型也可以用于异常检测。我们将在下一节中看到如何使用。
- en: Using Gaussian Mixtures for Anomaly Detection
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用高斯混合模型进行异常检测
- en: 'Using a Gaussian mixture model for anomaly detection is quite simple: any instance
    located in a low-density region can be considered an anomaly. You must define
    what density threshold you want to use. For example, in a manufacturing company
    that tries to detect defective products, the ratio of defective products is usually
    well known. Say it is equal to 2%. You then set the density threshold to be the
    value that results in having 2% of the instances located in areas below that threshold
    density. If you notice that you get too many false positives (i.e., perfectly
    good products that are flagged as defective), you can lower the threshold. Conversely,
    if you have too many false negatives (i.e., defective products that the system
    does not flag as defective), you can increase the threshold. This is the usual
    precision/recall trade-off (see [Chapter 3](ch03.html#classification_chapter)).
    Here is how you would identify the outliers using the fourth percentile lowest
    density as the threshold (i.e., approximately 4% of the instances will be flagged
    as anomalies):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高斯混合模型进行异常检测非常简单：位于低密度区域的任何实例都可以被视为异常。您必须定义要使用的密度阈值。例如，在一个试图检测有缺陷产品的制造公司中，有缺陷产品的比例通常是众所周知的。假设等于2%。然后，您将密度阈值设置为导致有2%的实例位于低于该阈值密度区域的值。如果您注意到您得到了太多的假阳性（即被标记为有缺陷的完全良好产品），您可以降低阈值。相反，如果您有太多的假阴性（即系统未标记为有缺陷的有缺陷产品），您可以增加阈值。这是通常的精确度/召回率权衡（参见[第3章](ch03.html#classification_chapter)）。以下是使用第四百分位数最低密度作为阈值（即，大约4%的实例将被标记为异常）来识别异常值的方法：
- en: '[PRE35]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[Figure 9-18](#mixture_anomaly_detection_plot) represents these anomalies as
    stars.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-18](#mixture_anomaly_detection_plot)将这些异常值表示为星号。'
- en: 'A closely related task is *novelty detection*: it differs from anomaly detection
    in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated
    by outliers, whereas anomaly detection does not make this assumption. Indeed,
    outlier detection is often used to clean up a dataset.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gaussian mixture models try to fit all the data, including the outliers; if
    you have too many of them this will bias the model’s view of “normality”, and
    some outliers may wrongly be considered as normal. If this happens, you can try
    to fit the model once, use it to detect and remove the most extreme outliers,
    then fit the model again on the cleaned-up dataset. Another approach is to use
    robust covariance estimation methods (see the `EllipticEnvelope` class).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0918](assets/mls3_0918.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 9-18\. Anomaly detection using a Gaussian mixture model
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just like *k*-means, the `GaussianMixture` algorithm requires you to specify
    the number of clusters. So how can you find that number?
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Number of Clusters
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With *k*-means, you can use the inertia or the silhouette score to select the
    appropriate number of clusters. But with Gaussian mixtures, it is not possible
    to use these metrics because they are not reliable when the clusters are not spherical
    or have different sizes. Instead, you can try to find the model that minimizes
    a *theoretical information criterion*, such as the *Bayesian information criterion*
    (BIC) or the *Akaike information criterion* (AIC), defined in [Equation 9-1](#information_criteria_equation).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Equation 9-1\. Bayesian information criterion (BIC) and Akaike information criterion
    (AIC)
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>B</mi>
    <mi>I</mi> <mi>C</mi> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo
    form="prefix">log</mo> <mrow><mo>(</mo> <mi>m</mi> <mo>)</mo></mrow> <mi>p</mi>
    <mo>-</mo> <mn>2</mn> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mover accent="true"><mi
    mathvariant="script">L</mi> <mo>^</mo></mover> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>A</mi> <mi>I</mi> <mi>C</mi> <mo>=</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mn>2</mn> <mi>p</mi> <mo>-</mo> <mn>2</mn> <mo
    form="prefix">log</mo> <mo>(</mo> <mover accent="true"><mi mathvariant="script">L</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'In these equations:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '*m* is the number of instances, as always.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* is the number of parameters learned by the model.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math><mover><mi mathvariant="script">L</mi><mo>^</mo></mover></math> is the
    maximized value of the *likelihood function* of the model.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the BIC and the AIC penalize models that have more parameters to learn
    (e.g., more clusters) and reward models that fit the data well. They often end
    up selecting the same model. When they differ, the model selected by the BIC tends
    to be simpler (fewer parameters) than the one selected by the AIC, but tends to
    not fit the data quite as well (this is especially true for larger datasets).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the BIC and AIC, call the `bic()` and `aic()` methods:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[Figure 9-20](#aic_bic_vs_k_plot) shows the BIC for different numbers of clusters
    *k*. As you can see, both the BIC and the AIC are lowest when *k*=3, so it is
    most likely the best choice.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0920](assets/mls3_0920.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: Figure 9-20\. AIC and BIC for different numbers of clusters *k*
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bayesian Gaussian Mixture Models
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rather than manually searching for the optimal number of clusters, you can
    use the `BayesianGaussianMixture` class, which is capable of giving weights equal
    (or close) to zero to unnecessary clusters. Set the number of clusters `n_components`
    to a value that you have good reason to believe is greater than the optimal number
    of clusters (this assumes some minimal knowledge about the problem at hand), and
    the algorithm will eliminate the unnecessary clusters automatically. For example,
    let’s set the number of clusters to 10 and see what happens:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Perfect: the algorithm automatically detected that only three clusters are
    needed, and the resulting clusters are almost identical to the ones in [Figure 9-16](#gaussian_mixtures_plot).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'A final note about Gaussian mixture models: although they work great on clusters
    with ellipsoidal shapes, they don’t do so well with clusters of very different
    shapes. For example, let’s see what happens if we use a Bayesian Gaussian mixture
    model to cluster the moons dataset (see [Figure 9-21](#moons_vs_bgm_plot)).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Oops! The algorithm desperately searched for ellipsoids, so it found eight different
    clusters instead of two. The density estimation is not too bad, so this model
    could perhaps be used for anomaly detection, but it failed to identify the two
    moons. To conclude this chapter, let’s take a quick look at a few algorithms capable
    of dealing with arbitrarily shaped clusters.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0921](assets/mls3_0921.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Figure 9-21\. Fitting a Gaussian mixture to nonellipsoidal clusters
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other Algorithms for Anomaly and Novelty Detection
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-Learn implements other algorithms dedicated to anomaly detection or
    novelty detection:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Fast-MCD (minimum covariance determinant)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Implemented by the `EllipticEnvelope` class, this algorithm is useful for outlier
    detection, in particular to clean up a dataset. It assumes that the normal instances
    (inliers) are generated from a single Gaussian distribution (not a mixture). It
    also assumes that the dataset is contaminated with outliers that were not generated
    from this Gaussian distribution. When the algorithm estimates the parameters of
    the Gaussian distribution (i.e., the shape of the elliptic envelope around the
    inliers), it is careful to ignore the instances that are most likely outliers.
    This technique gives a better estimation of the elliptic envelope and thus makes
    the algorithm better at identifying the outliers.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Isolation forest
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an efficient algorithm for outlier detection, especially in high-dimensional
    datasets. The algorithm builds a random forest in which each decision tree is
    grown randomly: at each node, it picks a feature randomly, then it picks a random
    threshold value (between the min and max values) to split the dataset in two.
    The dataset gradually gets chopped into pieces this way, until all instances end
    up isolated from the other instances. Anomalies are usually far from other instances,
    so on average (across all the decision trees) they tend to get isolated in fewer
    steps than normal instances.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Local outlier factor (LOF)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is also good for outlier detection. It compares the density of
    instances around a given instance to the density around its neighbors. An anomaly
    is often more isolated than its *k*-nearest neighbors.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: One-class SVM
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm is better suited for novelty detection. Recall that a kernelized
    SVM classifier separates two classes by first (implicitly) mapping all the instances
    to a high-dimensional space, then separating the two classes using a linear SVM
    classifier within this high-dimensional space (see [Chapter 5](ch05.html#svm_chapter)).
    Since we just have one class of instances, the one-class SVM algorithm instead
    tries to separate the instances in high-dimensional space from the origin. In
    the original space, this will correspond to finding a small region that encompasses
    all the instances. If a new instance does not fall within this region, it is an
    anomaly. There are a few hyperparameters to tweak: the usual ones for a kernelized
    SVM, plus a margin hyperparameter that corresponds to the probability of a new
    instance being mistakenly considered as novel when it is in fact normal. It works
    great, especially with high-dimensional datasets, but like all SVMs it does not
    scale to large datasets.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: PCA and other dimensionality reduction techniques with an `inverse_transform()`
    method
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: If you compare the reconstruction error of a normal instance with the reconstruction
    error of an anomaly, the latter will usually be much larger. This is a simple
    and often quite efficient anomaly detection approach (see this chapter’s exercises
    for an example).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you define clustering? Can you name a few clustering algorithms?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the main applications of clustering algorithms?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe two techniques to select the right number of clusters when using *k*-means.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is label propagation? Why would you implement it, and how?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name two clustering algorithms that can scale to large datasets? And
    two that look for regions of high density?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you think of a use case where active learning would be useful? How would
    you implement it?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between anomaly detection and novelty detection?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Gaussian mixture? What tasks can you use it for?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name two techniques to find the right number of clusters when using
    a Gaussian mixture model?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images
    of faces. Each image is flattened to a 1D vector of size 4,096\. Forty different
    people were photographed (10 times each), and the usual task is to train a model
    that can predict which person is represented in each picture. Load the dataset
    using the `sklearn.datasets.fetch_olivetti_faces()` function, then split it into
    a training set, a validation set, and a test set (note that the dataset is already
    scaled between 0 and 1). Since the dataset is quite small, you will probably want
    to use stratified sampling to ensure that there are the same number of images
    per person in each set. Next, cluster the images using *k*-means, and ensure that
    you have a good number of clusters (using one of the techniques discussed in this
    chapter). Visualize the clusters: do you see similar faces in each cluster?'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Continuing with the Olivetti faces dataset, train a classifier to predict which
    person is represented in each picture, and evaluate it on the validation set.
    Next, use *k*-means as a dimensionality reduction tool, and train a classifier
    on the reduced set. Search for the number of clusters that allows the classifier
    to get the best performance: what performance can you reach? What if you append
    the features from the reduced set to the original features (again, searching for
    the best number of clusters)?'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the
    algorithm, you should probably reduce the dataset’s dimensionality (e.g., use
    PCA, preserving 99% of the variance). Use the model to generate some new faces
    (using the `sample()` method), and visualize them (if you used PCA, you will need
    to use its `inverse_transform()` method). Try to modify some images (e.g., rotate,
    flip, darken) and see if the model can detect the anomalies (i.e., compare the
    output of the `score_samples()` method for normal images and for anomalies).
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some dimensionality reduction techniques can also be used for anomaly detection.
    For example, take the Olivetti faces dataset and reduce it with PCA, preserving
    99% of the variance. Then compute the reconstruction error for each image. Next,
    take some of the modified images you built in the previous exercise and look at
    their reconstruction error: notice how much larger it is. If you plot a reconstructed
    image, you will see why: it tries to reconstruct a normal face.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch09.html#idm45720207909616-marker)) Stuart P. Lloyd, “Least Squares
    Quantization in PCM”, *IEEE Transactions on Information Theory* 28, no. 2 (1982):
    129–137.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch09.html#idm45720207492864-marker)) David Arthur and Sergei Vassilvitskii,
    “k-Means++: The Advantages of Careful Seeding”, *Proceedings of the 18th Annual
    ACM-SIAM Symposium on Discrete Algorithms* (2007): 1027–1035.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch09.html#idm45720207428048-marker)) Charles Elkan, “Using the Triangle
    Inequality to Accelerate k-Means”, *Proceedings of the 20th International Conference
    on Machine Learning* (2003): 147–153.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch09.html#idm45720207425136-marker)) The triangle inequality is AC ≤ AB
    + BC, where A, B and C are three points and AB, AC, and BC are the distances between
    these points.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch09.html#idm45720207422480-marker)) David Sculley, “Web-Scale K-Means
    Clustering”, *Proceedings of the 19th International Conference on World Wide Web*
    (2010): 1177–1178.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch09.html#idm45720205974560-marker)) Phi (*ϕ* or *φ*) is the 21st letter
    of the Greek alphabet.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
