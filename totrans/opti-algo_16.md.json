["```py\nimport numpy as np\n\nK = 3                                                             ①\npayoff_params = [\n    {\"mean\": 8, \"std_dev\": 0.5},\n    {\"mean\": 10, \"std_dev\": 0.7},\n    {\"mean\": 5, \"std_dev\": 0.45}\n]                                                                 ②\nnum_trials = 90                                                   ③\n\ndef sample_payoff(slot_machine):\n    return np.random.normal(payoff_params[slot_machine][\"mean\"], \n➥        payoff_params[slot_machine][\"std_dev\"])                 ④\n\nmax_reward = max([payoff_params[i][\"mean\"] for i in range(K)])    ⑤\n```", "```py\ndef explore_only():\n    total_regret = 0\n    for _ in range(num_trials):\n        selected_machine = np.random.randint(K)   ①\n        reward = sample_payoff(selected_machine)  ②\n        regret = max_reward - reward              ③\n        total_regret += regret                    ④\n    average_regret = total_regret / num_trials    ⑤\n    return average_regret\n```", "```py\ndef exploit_only_greedy():\n    total_regret = 0\n    for _ in range(num_trials):\n        selected_machine = np.argmax([payoff_params[i][\"mean\"] for i in  \n        range(K)])                                                    ①\n        reward = sample_payoff(selected_machine)                      ②\n        regret = max_reward – reward                                  ③\n        total_regret += regret                                        ④\n    average_regret = total_regret / num_trials                        ⑤\n    return average_regret\n```", "```py\ndef epsilon_greedy(epsilon):\n    total_regret = 0\n    for _ in range(num_trials):\n        if np.random.random() < epsilon:\n            selected_machine = np.random.randint(K)                ①\n        else:\n            selected_machine = np.argmax([payoff_params[i][\"mean\"] \n            ➥ for i in range(K)])                                 ②\n        reward = sample_payoff(selected_machine)                   ③\n        regret = max_reward - reward\n        total_regret += regret\n    average_regret = total_regret / num_trials\n    return average_regret\n```", "```py\ndef ucb(c):\n    num_plays = np.zeros(K)                                                       ①\n    sum_rewards = np.zeros(K)                                                     ②\n    total_regret = 0                                                              ③\n\n    for i in range(K):                                                            ④\n        reward = sample_payoff(i)\n        num_plays[i] += 1\n        sum_rewards[i] += reward\n\n    for t in range(K, num_trials):                                                ⑤\n        ucb_values = sum_rewards / num_plays + c * np.sqrt(np.log(t) / num_plays) ⑥\n        selected_machine = np.argmax(ucb_values)\n        reward = sample_payoff(selected_machine)\n        num_plays[selected_machine] += 1\n        sum_rewards[selected_machine] += reward\n        optimal_reward = max_reward\n        regret = optimal_reward - reward\n        total_regret += regret\n\n    average_regret = total_regret / num_trials\n    return average_regret\n```", "```py\navg_regret_explore = explore_only()\navg_regret_exploit = exploit_only_greedy()\navg_regret_epsilon_greedy = epsilon_greedy(0.1)                    ①\navg_regret_ucb = ucb(2)                                            ②\n\nprint(f\"Average Regret - Explore only Strategy: {round(avg_regret_ ③\nexplore,4)}\")                                                      ③\nprint(f\"Average Regret - Exploit only Greedy Strategy:\n➥    {round(avg_regret_exploit,4)}\")                              ③\nprint(f\"Average Regret - Epsilon-greedy Strategy:\n➥    {round(avg_regret_epsilon_greedy,4)}\")                       ③\nprint(f\"Average Regret - UCB Strategy: {round(avg_regret_ucb,4)}\") ③\n```", "```py\nAverage Regret - Explore only Strategy: 2.246\nAverage Regret - Exploit only Greedy Strategy: 0.048\nAverage Regret - Epsilon-greedy Strategy: 0.3466\nAverage Regret - UCB Strategy: 0.0378\n```", "```py\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pandas as pd\nimport seaborn as sns\n```", "```py\nclass Actor(nn.Module):\n\n    def __init__(self, state_dim,  action_dim):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, action_dim)\n\n    def forward(self, state):\n        x1 = F.relu(self.fc1(state))\n        x2 = F.relu(self.fc2(x1))\n        action_probs = F.softmax(self.fc3(x2), dim=-1)\n        return action_probs\n```", "```py\nclass Critic(nn.Module):\n\n    def __init__(self, state_dim):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 1)\n\n    def forward(self, state):\n        x1 = F.relu(self.fc1(state))\n        x2 = F.relu(self.fc2(x1))\n        value = self.fc3(x2)\n        return value\n```", "```py\nenv = gym.make(\"CartPole-v1\")                               ①\nenv.seed(0)                                                 ②\n\nstate_dim = env.observation_space.shape[0]                  ③\nn_actions = env.action_space.n                              ④\n```", "```py\nactor = Actor(state_dim, n_actions)                          ①\ncritic = Critic(state_dim)                                   ②\nadam_actor = torch.optim.Adam(actor.parameters(), lr=1e-3)   ③\nadam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3) ④\ngamma = 0.99                                                 ⑤\n```", "```py\nnum_episodes=500                                                    ①\nepisode_rewards = []                                                ②\nstats={'actor loss':[], 'critic loss':[], 'return':[]}              ③\npbar = tqdm(total=num_episodes, ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/ \n➥ {total_fmt}')                                                    ④\n```", "```py\nfor episode in range(num_episodes):\n    done = False\n    total_reward = 0\n    state = env.reset()\n    env.seed(0)\n```", "```py\nwhile not done:        \n    probs = actor(torch.from_numpy(state).float())                          ①\n    dist = torch.distributions.Categorical(probs=probs)                     ②\n    action = dist.sample()                                                  ③\n\n    next_state, reward, done, info = env.step(action.detach().data.numpy()) ④\n```", "```py\nadvantage = reward + (1-\n➥ done)*gamma*critic(torch.from_numpy(next_state).float()) - \n➥    critic(torch.from_numpy(state).float())       ①\n\ntotal_reward += reward                              ②\nstate = next_state                                  ③\n```", "```py\ncritic_loss = advantage.pow(2).mean()\nadam_critic.zero_grad()\ncritic_loss.backward()\nadam_critic.step()\n```", "```py\nactor_loss = -dist.log_prob(action)*advantage.detach()\nadam_actor.zero_grad()\nactor_loss.backward()\nadam_actor.step()\n```", "```py\nepisode_rewards.append(total_reward)\n```", "```py\n    stats['actor loss'].append(actor_loss)\n    stats['critic loss'].append(critic_loss)\n    stats['return'].append(total_reward)\n    print('Actor loss= ', round(stats['actor loss'][episode].item(), 4), 'Critic \n       ➥ loss= ', round(stats['critic loss'][episode].item(), 4), 'Return= ', \n       ➥ stats['return'][episode])                           ①\n    pbar.set_description(f\"Episode {episode + 1}\")            ②\n    pbar.set_postfix({\"Reward\": episode_rewards})             ②\n    pbar.update(1)                                            ②\npbar.close()                                                  ③\n```", "```py\ndata = pd.DataFrame({\"Episode\": range(1, num_episodes + 1), \"Reward\": \n➥    episode_rewards})                                                     ①\n\nplt.figure(figsize=(12,6))                                                  ②\nsns.set(style=\"whitegrid\")                                                  ③\nsns.regplot(data=data, x=\"Episode\", y=\"Reward\", scatter_kws={\"alpha\": 0.5}) ④\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Reward\")\nplt.title(\"Episode Rewards with Trend Line\")\nplt.show()\n```", "```py\nimport gymnasium as gym                                              ①\nfrom stable_baselines3 import A2C                                    ②\nfrom stable_baselines3.common.evaluation import evaluate_policy      ③\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")               ④\n\nmodel = A2C(\"MlpPolicy\", env, verbose=1)                             ⑤\n\nmodel.learn(total_timesteps=10000, progress_bar=True)                ⑥\n\nmean_reward, std_reward = evaluate_policy(model, model.get_env(),\n➥    n_eval_episodes=10)                                            ⑦\nvec_env = model.get_env()                                            ⑦\nobs = vec_env.reset()                                                ⑦\nfor i in range(1000):                                                ⑦\n    action, _states = model.predict(obs, deterministic=True)         ⑦\n    obs, rewards, dones, info = vec_env.step(action)                 ⑦\n    vec_env.render(\"human\")                                          ⑧\n```", "```py\nimport gymnasium as gym\nfrom stable_baselines3 import PPO                                  ①\nfrom stable_baselines3.common.evaluation import evaluate_policy\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")             ②\n\nmodel = PPO(\"MlpPolicy\", env, verbose=1)                           ③\n\nmodel.learn(total_timesteps=10000, progress_bar=True)              ④\n```", "```py\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 61.8         |\n|    ep_rew_mean          | 61.8         |\n| time/                   |              |\n|    fps                  | 362          |\n|    iterations           | 5            |\n|    time_elapsed         | 28           |\n|    total_timesteps      | 10240        |\n| train/                  |              |\n|    approx_kl            | 0.0064375857 |\n|    clip_fraction        | 0.051        |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.61        |\n|    explained_variance   | 0.245        |\n|    learning_rate        | 0.0003       |\n|    loss                 | 26.1         |\n|    n_updates            | 40           |\n|    policy_gradient_loss | -0.0141      |\n|    value_loss           | 65.2         |\n------------------------------------------\n```", "```py\nmean_reward, std_reward = evaluate_policy(model, model.get_env(), \n➥    n_eval_episodes=10)                                         ①\nvec_env = model.get_env()                                         ②\nobs = vec_env.reset()                                             ③\nfor i in range(1000):                                             ④\n    action, _states = model.predict(obs, deterministic=True) \n    obs, rewards, dones, info = vec_env.step(action) \n    vec_env.render(\"human\")\n```", "```py\nimport gymnasium\nimport matplotlib.pyplot as plt\nimport mobile_env\nfrom IPython import display\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.ppo import MlpPolicy\n\nenv = gymnasium.make(\"mobile-small-central-v0\", render_mode=\"rgb_array\")     ①\nprint(f\"\\nSmall environment with {env.NUM_USERS} users and {env.NUM_STATIONS}\n➥    cells.\")                                                               ②\n```", "```py\npip install tensorboard\ntensorboard --logdir .\n```", "```py\nmodel = PPO(MlpPolicy, env, tensorboard_log='results_sb', verbose=1)\nmodel.learn(total_timesteps=30000, progress_bar=True)\n```", "```py\nobs, info = env.reset()                                         ①\ndone = False                                                    ②\n\nwhile not done:\n    action, _ = model.predict(obs)                              ③\n    obs, reward, terminated, truncated, info = env.step(action) ④\n    done = terminated or truncated                              ⑤\n\n    plt.imshow(env.render())                                    ⑥\n    display.display(plt.gcf())                                  ⑥\n    display.clear_output(wait=True)                             ⑥\n```", "```py\nimport ray\nfrom ray.tune.registry import register_env\nimport ray.air\nfrom ray.rllib.algorithms.ppo import PPOConfig\nfrom ray.rllib.policy.policy import PolicySpec\nfrom ray.tune.stopper import MaximumIterationStopper\nfrom ray.rllib.algorithms.algorithm import Algorithm\nfrom mobile_env.wrappers.multi_agent import RLlibMAWrapper\n```", "```py\ndef register(config):  \n    env = gymnasium.make(\"mobile-small-ma-v0\")\n    return RLlibMAWrapper(env)\n```", "```py\nray.init(\n  num_cpus=2,                ①\n  include_dashboard=False,   ②\n  ignore_reinit_error=True,  ③\n  log_to_driver=False,       ④\n)\n```", "```py\nconfig = (\n    PPOConfig()\n    .environment(env=\"mobile-small-ma-v0\")                  ①\n    .multi_agent(\n        policies={\"shared_policy\": PolicySpec()},\n        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \n       ➥ \"shared_policy\",\n    )                                                       ②\n\n    .resources(num_cpus_per_worker=1)                       ③\n    .rollouts(num_rollout_workers=1)                        ④\n)\n```", "```py\ntuner = ray.tune.Tuner(\n    \"PPO\",                                                                  ①\n    run_config=ray.air.RunConfig(\n        storage_path=\"./results_rllib\",                                     ②\n        stop=MaximumIterationStopper(max_iter=10),                          ③\n        checkpoint_config=ray.air.CheckpointConfig(checkpoint_at_end=True), ④\n    ),\n    param_space=config,                                                     ⑤\n)\n\nresult_grid = tuner.fit()                                                   ⑥\n```", "```py\nbest_result = result_grid.get_best_result(metric=\"episode_reward_mean\", \n➥ mode=\"max\")                                           ①\nppo = Algorithm.from_checkpoint(best_result.checkpoint)  ②\n```", "```py\nenv = gymnasium.make(\"mobile-small-ma-v0\", render_mode=\"rgb_array\")  ①\nobs, info = env.reset()                                              ②\ndone = False\n\nwhile not done:                                                      ③\n    action = {}                                                      ④\n    for agent_id, agent_obs in obs.items():                          ⑤\n        action[agent_id] = ppo.compute_single_action(agent_obs,\n            ➥ policy_id=\"shared_policy\")                            ⑤\n\n    obs, reward, terminated, truncated, info = env.step(action)      ⑥\n    done = terminated or truncated                                   ⑦\n\n    plt.imshow(env.render())                                         ⑧\n    display.display(plt.gcf())                                       ⑧\n    display.clear_output(wait=True)                                  ⑧\n```", "```py\nimport vowpalwabbit\nimport torch\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport numpy as np\nfrom tqdm import tqdm\n\nshared_contexts = ['city', 'interstate']                          ①\n\nsize_types = ['small', 'medium', 'large']                         ②\nengine_types = ['petrol', 'diesel', 'electric']                   ②\ntire_types = ['all_season', 'snow', 'performance', 'all_terrain'] ②\n```", "```py\ndef reward_function(shared_context, size_index, engine_index, tire_index):\n    size_value = [0.8, 1.0, 0.9]                                   ①\n    engine_value = [0.7, 0.9, 1.0]                                 ②\n    tire_value = [0.9, 0.8, 1.0, 0.95]                             ③\n\n    reward = (\n        size_value[size_index]\n        * engine_value[engine_index]\n        * tire_value[tire_index]\n    )                                                              ④\n\n    noise_scale = 0.05                                             ⑤\n    noise_value = np.random.normal(loc=0, scale=noise_scale)       ⑤\n    reward += noise_value                                          ⑤\n\n    return reward `                                                ⑥\n```", "```py\ndef generate_combinations(shared_context, size_types, engine_types, tire_types):\n    examples = [f\"shared |User {shared_context}\"]\n    descriptions = []\n    for i, size in enumerate(size_types):\n        for j, engine in enumerate(engine_types):\n            for k, tire in enumerate(tire_types):\n                examples.append(f\"|Action truck_size={size} engine={engine} \n                    ➥ tire={tire}\")\n                descriptions.append((i, j, k))\n    return examples, descriptions\n```", "```py\ndef sample_truck_pmf(pmf):\n    pmf_tensor = torch.tensor(pmf)                   ①\n    index = torch.multinomial(pmf_tensor, 1).item()  ②\n    chosen_prob = pmf[index]                         ③\n\n    return index, chosen_prob                        ④\n```", "```py\ncb_vw = vowpalwabbit.Workspace(\n    \"--cb_explore_adf --epsilon 0.2 --interactions AA AU AAU -l 0.05 --power_t 0\",\n    quiet=True,\n)\n```", "```py\nnum_iterations = 2500\ncb_rewards = []\nwith tqdm(total=num_iterations, desc=\"Training\") as pbar:\n    for _ in range(num_iterations):\n        shared_context = random.choice(shared_contexts)                    ①\n        examples, indices = generate_combinations(\n            shared_context, size_types, engine_types, tire_types\n        )                                                                  ②\n        cb_prediction = cb_vw.predict(examples)                            ③\n        chosen_index, prob = sample_truck_pmf(cb_prediction)               ④\n        size_index, engine_index, tire_index = indices[chosen_index]       ⑤\n        reward = reward_function(shared_context, size_index, engine_index, ⑥\n           ➥ tire_index)                                                  ⑥\n        cb_rewards.append(reward)                                          ⑥\n        examples[chosen_index + 1] = f\"0:{-1*reward}:{prob} {examples[chosen_\n           ➥ index + 1]}\"                                                 ⑦\n        cb_vw.learn(examples)                                              ⑧\n        pbar.set_postfix({'Reward': reward})\n        pbar.update(1)\ncb_vw.finish()                                                             ⑨\n```", "```py\ndef test_model(shared_context, size_types, engine_types, tire_types):\n    examples, indices = generate_combinations(shared_context, size_types, \n       ➥ engine_types, tire_types)\n    cb_prediction = cb_vw.predict(examples)\n    chosen_index, prob = sample_truck_pmf(cb_prediction)\n    chosen_action = examples[chosen_index]\n    size_index, engine_index, tire_index = indices[chosen_index]\n    expected_reward = reward_function(shared_context, size_index,\n                ➥ engine_index, tire_index)\n    print(\"Chosen Action:\", chosen_action)\n    print(\"Expected Reward:\", expected_reward)\n\ntest_shared_context = 'city'\ntest_model(test_shared_context, size_types, engine_types, tire_types)\n```", "```py\nChosen Action: Action truck_size=medium engine=electric tire=snow\nExpected Reward: 1.012\n```"]