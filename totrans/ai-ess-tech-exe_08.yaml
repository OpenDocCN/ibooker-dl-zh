- en: 'Chapter 8\. Looking at Data: Our Your Secret Weapon'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve touched on the importance of having metrics that are specific to your
    business and a systematic process for measuring and reviewing them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I want to share a recent experience that highlights a crucial
    lesson for AI projects:'
  prefs: []
  type: TYPE_NORMAL
- en: There’s no substitute for examining your data firsthand.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I was recently working with a company that automates HR functions like recruiting
    and onboarding. Their engineering team had developed an evaluation suite with
    various metrics to measure the AI’s performance. One metric in particular caught
    my attention: the *edit distance* between the AI-generated email and the recruiter’s
    final version. In case you haven’t heard of it, edit distance is a measurement
    of how similar two texts are. This metric seemed like it would be a good one—it’s
    business-specific, and it can be systematically measured.'
  prefs: []
  type: TYPE_NORMAL
- en: The team had found that the average edit distance between the AI-generated emails
    and the recruiter’s final version was 12%. This seemed like a good result, but
    the team was struggling with user adoption. It turned out that the metric was
    hiding a critical flaw.
  prefs: []
  type: TYPE_NORMAL
- en: Look at the Raw Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my experience, the best way to understand a metric is to look at the raw
    data. It might sound simple, but it’s a secret weapon that almost always uncovers
    something unexpected.
  prefs: []
  type: TYPE_NORMAL
- en: 'I asked to review some of these emails myself, and what I found was shocking:
    for the most part, the AI-generated emails were perfectly reasonable. Instead,
    it was actually the human edits that were causing problems! The “improvements”
    that humans made often introduced grammatical errors, wordiness, and unclear messaging.'
  prefs: []
  type: TYPE_NORMAL
- en: The assumption that human edits would always improve the emails was a fundamental
    flaw in the edit-distance metric. This discovery took me just a few hours. The
    team was stunned by how quickly I identified this issue that had eluded them for
    so long.
  prefs: []
  type: TYPE_NORMAL
- en: You (Yes, You) Need to Review Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my consulting engagements, I *always* train executives on reviewing data.
    This approach is the main reason our clients see [tangible results](https://oreil.ly/ECo1I).
  prefs: []
  type: TYPE_NORMAL
- en: '*Understanding the nuances of your AI systems isn’t just the domain of technicians
    and engineers—it’s a critical responsibility for executives.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an executive, you’re in a unique position to bridge the gap between technical
    capabilities and business objectives. AI systems often interact with users in
    natural language, making the data more accessible than you might think. By personally
    reviewing AI interactions, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure alignment with business values
  prefs: []
  type: TYPE_NORMAL
- en: Verify that the AI represents your brand and communicates in a manner consistent
    with your company’s ethos.
  prefs: []
  type: TYPE_NORMAL
- en: Uncover hidden issues
  prefs: []
  type: TYPE_NORMAL
- en: Spot user experience problems, interface issues, or workflow bottlenecks that
    aggregated reports might miss.
  prefs: []
  type: TYPE_NORMAL
- en: Provide valuable feedback
  prefs: []
  type: TYPE_NORMAL
- en: Your critiques can directly improve the AI’s performance, much like coaching
    a new employee.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we provide you with a toolkit that provides a systematic approach to
    reviewing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Set Up a User-Centric Data Viewer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively review AI interactions, you need to see them as your users do.
    Technical logs and observability platforms are filled with jargon and irrelevant
    details that can obscure the real user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collaborate with your team
  prefs: []
  type: TYPE_NORMAL
- en: Discuss the need for a data viewer that mirrors the user interface your customers
    interact with.
  prefs: []
  type: TYPE_NORMAL
- en: Eliminate technical distractions
  prefs: []
  type: TYPE_NORMAL
- en: Ensure the viewer focuses solely on the AI-user interactions without backend
    codes, logs, or intermediate AI processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Simplify access
  prefs: []
  type: TYPE_NORMAL
- en: The data viewer should be easily accessible—consider bookmarking it or setting
    it as your homepage for quick access.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start with the simplest thing that could possibly work. Here are some requirements
    that you should keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a spreadsheet
  prefs: []
  type: TYPE_NORMAL
- en: For simpler applications, a well-organized spreadsheet can be a quick and effective
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize visual clarity
  prefs: []
  type: TYPE_NORMAL
- en: The data viewer should present information clearly, using visuals where appropriate
    to enhance understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Include key context
  prefs: []
  type: TYPE_NORMAL
- en: Make sure each interaction includes relevant details like timestamps, user segments,
    or any categorization that helps in understanding the context.
  prefs: []
  type: TYPE_NORMAL
- en: Example of a Data Viewer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 8-1](#fig0801) shows a real example of a data viewer used by the CTO
    of Rechat, that allows him to quickly look at pertinent data. This kind of thing
    can be built in just a few hours.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aete_08in01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. A user-centric LLM annotation tool^([1](ch08.html#id132))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You don’t need to build anything as fancy as this, but it shows that it can
    be done. As an in-between step, I often use [Airtable](https://airtable.com) for
    this task. Remember, do the simplest thing that could possibly work, including
    a simple spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Establish a Daily Data Review Routine'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consistent, hands-on review keeps you connected to your AI’s performance and
    user experience. By dedicating a small portion of your day, you can catch issues
    early and guide your team more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Schedule time on your calendar
  prefs: []
  type: TYPE_NORMAL
- en: Treat data review as a critical meeting with yourself—block out 15–20 minutes
    daily.
  prefs: []
  type: TYPE_NORMAL
- en: Focus on failure modes
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize reviewing interactions where the AI may have underperformed or failed
    to meet user needs.
  prefs: []
  type: TYPE_NORMAL
- en: Review a representative sample
  prefs: []
  type: TYPE_NORMAL
- en: Don’t just look at problems; include successful interactions to understand what
    works well.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for Setting Up a Daily Data Review Routine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most successful leaders I’ve worked with incorporate the following into
    their routine:'
  prefs: []
  type: TYPE_NORMAL
- en: Set themes for each day
  prefs: []
  type: TYPE_NORMAL
- en: Focus on different features or user segments each day to cover more ground over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Use random sampling
  prefs: []
  type: TYPE_NORMAL
- en: To avoid bias, randomly select interactions to review alongside targeted ones.
  prefs: []
  type: TYPE_NORMAL
- en: Keep notes handy
  prefs: []
  type: TYPE_NORMAL
- en: Maintain a journal or digital note where you can jot down immediate thoughts
    or patterns you notice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Categorize Data for Efficient Review'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Organizing data into categories helps you spot patterns, understand context,
    and make your review process more efficient. It allows you to focus on specific
    areas that are critical to your business objectives. *Your team needs to categorize
    this data prior to you reviewing it.*
  prefs: []
  type: TYPE_NORMAL
- en: Categorize Before You Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the data is not categorized, it will be hard to navigate and analyze. This
    categorization can be automated with code that inserts tags into the data, with
    LLMs, or with a combination of both.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Categorize by features, tools, and skills
  prefs: []
  type: TYPE_NORMAL
- en: Identify the different functionalities your AI offers, such as email drafting,
    scheduling, or contact searching.
  prefs: []
  type: TYPE_NORMAL
- en: Define scenarios
  prefs: []
  type: TYPE_NORMAL
- en: List specific conditions the AI must handle, like “Contact Not Found,” “User
    Provided Invalid Date,” or “Multiple Contacts Found.”
  prefs: []
  type: TYPE_NORMAL
- en: Create a matrix or table
  prefs: []
  type: TYPE_NORMAL
- en: Use a visual aid to map out categories and scenarios for quick reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-1](#table0801) shows an example of a matrix of features and scenarios
    that you might use to categorize interactions with an AI.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Example features and scenarios matrix
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Scenario | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Contact Search** | Contact Found | Successfully retrieved the correct contact
    information. |'
  prefs: []
  type: TYPE_TB
- en: '| **Contact Search** | Contact Not Found | No matching contact; AI should suggest
    creating a new contact or check for typos. |'
  prefs: []
  type: TYPE_TB
- en: '| **Contact Search** | Multiple Contacts Found | Multiple contacts match the
    query; AI should present options for the user to select. |'
  prefs: []
  type: TYPE_TB
- en: '| **Email Writing** | Successful Send | Email drafted and sent without issues.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Email Writing** | Formatting Errors | Issues with email layout, style,
    or inserted errors by nonnative speakers worsening content. |'
  prefs: []
  type: TYPE_TB
- en: '| **Scheduling** | Successful Booking | Meeting scheduled correctly with all
    details accurate. |'
  prefs: []
  type: TYPE_TB
- en: '| **Scheduling** | Invalid Date Input | User provides an invalid date; AI should
    recognize and prompt for correction. |'
  prefs: []
  type: TYPE_TB
- en: '| **Scheduling** | Conflicting Appointments | Scheduling conflicts detected;
    AI should notify user and suggest alternatives. |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Entry** | Error Handling | AI encounters errors during data input;
    should provide clear guidance to the user. |'
  prefs: []
  type: TYPE_TB
- en: '| **General Query** | Unhandled Questions | AI doesn’t understand the query;
    should ask for clarification or redirect appropriately. |'
  prefs: []
  type: TYPE_TB
- en: '[Table 8-2](#table0802) shows an example of applying the previous matrix to
    actual data.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-2\. Example data with features and scenario grouping applied
  prefs: []
  type: TYPE_NORMAL
- en: '| Interaction ID | Feature | Scenario | User query | AI response |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 001 | Contact Search | Contact Not Found | “Find John’s contact info.” |
    “No contact found for ‘John’. Would you like to create a new contact?” |'
  prefs: []
  type: TYPE_TB
- en: '| 002 | Scheduling | User Provided Invalid Date | “Schedule a meeting on April
    31st.” | “April has only 30 days. Please provide a valid date.” |'
  prefs: []
  type: TYPE_TB
- en: '| 003 | Scheduling | User Provided Time in the Past | “Set up a call yesterday
    at 3 PM.” | “Cannot schedule meetings in the past. Please choose a future time.”
    |'
  prefs: []
  type: TYPE_TB
- en: '| 004 | Contact Search | Multiple Contacts Found | “Get Alex’s email address.”
    | “Multiple contacts named ‘Alex’ found: Alex Smith, Alex Johnson. Please specify.”
    |'
  prefs: []
  type: TYPE_TB
- en: '| 005 | Scheduling | Conflict with Existing Event | “Book a meeting with Mark
    at 2 PM tomorrow.” | “You already have a meeting scheduled at 2 PM tomorrow. Would
    you like to reschedule?” |'
  prefs: []
  type: TYPE_TB
- en: '| 006 | Email Writing | Attachment Missing | “Send the report to the team.”
    | “Please attach the report file before sending the email.” |'
  prefs: []
  type: TYPE_TB
- en: '| 007 | Data Entry | Invalid Input Format | “Update sales figures to ‘ten thousand’.”
    | “Please provide the sales figures in numeric format.” |'
  prefs: []
  type: TYPE_TB
- en: '| 008 | Contact Search | User Provided Incomplete Info | “Find contact with
    phone number 555-1234.” | “Multiple contacts have that number. Can you provide
    a name?” |'
  prefs: []
  type: TYPE_TB
- en: '| 009 | General Query | Unhandled Question | “What’s the weather today?” |
    “I’m sorry, but I cannot provide weather information.” |'
  prefs: []
  type: TYPE_TB
- en: 'Here is a more detailed explanation of this table’s updated scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contact Search**'
  prefs: []
  type: TYPE_NORMAL
- en: Contact Not Found
  prefs: []
  type: TYPE_NORMAL
- en: The user searches for a contact that doesn’t exist in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Contacts Found
  prefs: []
  type: TYPE_NORMAL
- en: The search query matches more than one contact, requiring user clarification.
  prefs: []
  type: TYPE_NORMAL
- en: User Provided Incomplete Info
  prefs: []
  type: TYPE_NORMAL
- en: Insufficient details are provided to uniquely identify a contact.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scheduling**'
  prefs: []
  type: TYPE_NORMAL
- en: User Provided Invalid Date
  prefs: []
  type: TYPE_NORMAL
- en: The user specifies a date that doesn’t exist, such as “April 31st”.
  prefs: []
  type: TYPE_NORMAL
- en: User Provided Time in the Past
  prefs: []
  type: TYPE_NORMAL
- en: The user attempts to schedule an event in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Conflict with Existing Event
  prefs: []
  type: TYPE_NORMAL
- en: The requested time slot conflicts with another event on the user’s calendar.
  prefs: []
  type: TYPE_NORMAL
- en: '**Email Writing**'
  prefs: []
  type: TYPE_NORMAL
- en: Attachment Missing
  prefs: []
  type: TYPE_NORMAL
- en: The user mentions an attachment but doesn’t include it; the AI should prompt
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Entry**'
  prefs: []
  type: TYPE_NORMAL
- en: Invalid Input Format
  prefs: []
  type: TYPE_NORMAL
- en: The user provides data in an incorrect format; the AI should request proper
    formatting.
  prefs: []
  type: TYPE_NORMAL
- en: '**General Query**'
  prefs: []
  type: TYPE_NORMAL
- en: Unhandled Question
  prefs: []
  type: TYPE_NORMAL
- en: The AI receives a query outside its capabilities and should gracefully inform
    the user.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The examples above illustrate the AI doing the right thing. The point of this
    example is to show example categories and scenarios, rather than show you examples
    of AI failures.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for Effective Categorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are ways to make sure your categories are effective:'
  prefs: []
  type: TYPE_NORMAL
- en: Use specific scenarios
  prefs: []
  type: TYPE_NORMAL
- en: Clearly define conditions that the AI must handle, which helps in assessing
    its performance in various situations. By fleshing out these scenarios, you can
    pinpoint areas for improvement and ensure that the AI responds appropriately to
    user needs.
  prefs: []
  type: TYPE_NORMAL
- en: Reflect real user behavior
  prefs: []
  type: TYPE_NORMAL
- en: Include scenarios that represent common user mistakes or challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Use color coding
  prefs: []
  type: TYPE_NORMAL
- en: Assign colors to different scenarios for quicker visual scanning.
  prefs: []
  type: TYPE_NORMAL
- en: Leverage filters
  prefs: []
  type: TYPE_NORMAL
- en: If you’re ‘using a spreadsheet or data viewer, use filter functions to focus
    on specific scenarios or features.
  prefs: []
  type: TYPE_NORMAL
- en: Update regularly
  prefs: []
  type: TYPE_NORMAL
- en: As new features are added or scenarios emerge, update your categories to keep
    them relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Conduct Binary Evaluations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simplifying your evaluation to a Yes or No question—“Did the AI solve the customer’s
    problem?”—helps you make quick, decisive assessments without getting bogged down
    in complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ask the key question
  prefs: []
  type: TYPE_NORMAL
- en: For each interaction, determine whether the AI met the user’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid complex scoring systems
  prefs: []
  type: TYPE_NORMAL
- en: Resist the urge to rate on a scale; keep it simple to maintain consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Document your decision
  prefs: []
  type: TYPE_NORMAL
- en: Clearly mark each interaction as a success or failure.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for Effective Binary Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Simplifying your evaluation to a Yes or No question can be difficult at first.
    Here are some tips to help you get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Consistency is key
  prefs: []
  type: TYPE_NORMAL
- en: Use the same criteria for each evaluation to ensure fairness and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Trust your instincts
  prefs: []
  type: TYPE_NORMAL
- en: Your business acumen is valuable—don’t second-guess your initial judgment.
  prefs: []
  type: TYPE_NORMAL
- en: Note ambiguities
  prefs: []
  type: TYPE_NORMAL
- en: If an interaction isn’t a clear Yes or No, make a note and consider discussing
    it with your team for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Write Constructive Critiques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detailed, actionable feedback is essential for improving your AI system. Think
    of it as coaching an employee—the more specific you are, the better the AI can
    become.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frame feedback as instructions
  prefs: []
  type: TYPE_NORMAL
- en: Write critiques as if you’re guiding a new team member on how to improve.
  prefs: []
  type: TYPE_NORMAL
- en: Be specific
  prefs: []
  type: TYPE_NORMAL
- en: Point out exactly what went wrong, and suggest how it could be corrected.
  prefs: []
  type: TYPE_NORMAL
- en: Highlight positive outcomes
  prefs: []
  type: TYPE_NORMAL
- en: Occasionally note why certain interactions were successful to reinforce good
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Example Critiques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Table 8-3](#table0803) contains an example of a data review log with detailed
    critiques that you might write.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-3\. Example critiques
  prefs: []
  type: TYPE_NORMAL
- en: '| Date | ID | Feature | Scenario | Problem solved? (Y/N) | Critique | Metrics
    aligned? (Y/N) | Notes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-10-11 | 005 | Contact Search | Multiple Contacts Found | N | **Critique**:
    “When the user searched for ‘Alex’, the AI found multiple contacts, named ‘Alex
    Johnson’ and ‘Alex Smith’, but did not prompt the user to select the correct one.
    Next time, please display the list of matching contacts and ask the user to choose
    one.” | N | Discuss with team |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-10-11 | 006 | Scheduling | Invalid Date Input | N | **Critique**: “The
    user tried to schedule a meeting on ‘February 30th’, which is an invalid date.
    The AI should recognize this and inform the user that the date doesn’t exist,
    then prompt them to provide a valid date for scheduling.” | Y | Improvement needed
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-10-11 | 007 | Email Writing | Edits Worsening Content | N | **Critique**:
    “After drafting an email, the AI accepted user edits that introduced grammatical
    errors and unclear phrasing. The AI should assist users by suggesting corrections
    to maintain professionalism and clarity in communications, especially when edits
    reduce quality.” | N | Needs attention |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-10-11 | 008 | Data Entry | Error Handling | N | **Critique**: “The AI
    encountered an error when updating sales figures but only displayed a generic
    error message. It should provide specific details about the error and guide the
    user on how to correct the input or whom to contact for support.” | Y | Error
    specifics |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-10-11 | 009 | General Query | Unhandled Questions | N | **Critique**:
    “The user asked for ‘company’s quarterly revenue growth’, but the AI responded
    with ‘I can’t assist with that request.’ Instead, the AI should recognize this
    as a request for financial data and either provide the information or guide the
    user to the appropriate resource.” | N | Expand knowledge |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-10-11 | 010 | Scheduling | Conflicting Appointments | N | **Critique**:
    “When scheduling a meeting at 3 PM on Thursday, the AI didn’t alert the user of
    an existing appointment at that time. The AI should check the user’s calendar
    for conflicts and suggest alternative times if necessary.” | N | Calendar sync
    needed |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-10-11 | 011 | Contact Search | Contact Not Found | Y | **Praise**: “Good
    job informing the user that ‘Emma Thompson’ was not found and offering to create
    a new contact. This helps keep contact lists up-to-date and assists the user efficiently.”
    | Y | Positive example |'
  prefs: []
  type: TYPE_TB
- en: 'Here is further explanation of this table’s critiques:'
  prefs: []
  type: TYPE_NORMAL
- en: Interaction ID 005
  prefs: []
  type: TYPE_NORMAL
- en: The AI failed to handle a situation with multiple contacts. The critique guides
    the AI to prompt the user for selection next time.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction ID 006
  prefs: []
  type: TYPE_NORMAL
- en: The AI didn’t recognize an invalid date. The critique instructs the AI to validate
    dates and provide corrective prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction ID 007
  prefs: []
  type: TYPE_NORMAL
- en: The AI allowed edits that worsened the content. The critique emphasizes maintaining
    communication quality and suggests proactive assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction ID 008
  prefs: []
  type: TYPE_NORMAL
- en: The AI’s error handling was insufficiently informative. The critique advises
    providing specific error details and user guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction ID 009
  prefs: []
  type: TYPE_NORMAL
- en: The AI didn’t handle a general query appropriately. The critique encourages
    expanding the AI’s knowledge base or redirecting the user effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction ID 010
  prefs: []
  type: TYPE_NORMAL
- en: The AI missed a scheduling conflict. The critique suggests implementing calendar
    checks and conflict notifications.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction ID 011
  prefs: []
  type: TYPE_NORMAL
- en: Highlighting successful interactions reinforces good practices and provides
    models for desired AI behavior. However, you should focus your critiques more
    on failures.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 8-3](#table0803) contains a column named “Metrics aligned? (Y/N)”. We
    will discuss metrics in the next step.'
  prefs: []
  type: TYPE_NORMAL
- en: Tips for Writing Constructive Critiques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some ways to maximize the impact of your critiques:'
  prefs: []
  type: TYPE_NORMAL
- en: Use clear language
  prefs: []
  type: TYPE_NORMAL
- en: Avoid technical jargon; focus on the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Be objective
  prefs: []
  type: TYPE_NORMAL
- en: Focus on the interaction, not the technology behind it.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize impact
  prefs: []
  type: TYPE_NORMAL
- en: Spend more time on critiques that could significantly improve user satisfaction
    or business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Cross-Reference Metrics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics are only valuable if they align with real-world outcomes. By comparing
    your evaluations with the associated metrics, you ensure that your team measures
    what truly matters.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compare evaluations with metrics
  prefs: []
  type: TYPE_NORMAL
- en: For each interaction, look at the metrics your team has recorded.
  prefs: []
  type: TYPE_NORMAL
- en: Sanity-check the metrics
  prefs: []
  type: TYPE_NORMAL
- en: Ask yourself whether the metrics accurately reflect the success or failure of
    the interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Provide feedback on metrics
  prefs: []
  type: TYPE_NORMAL
- en: If you notice discrepancies, discuss them with your team to refine the measurement
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Interaction ID*: 003'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Your Evaluation*: Problem Not Solved'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metric Reported*: Success (Score of 90%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Discrepancy*: “The AI didn’t recognize an invalid date, but the metric indicates
    a high success rate. This suggests the metric isn’t capturing date validation
    errors.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tips on Cross-Referencing Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some tactics to make sure the metrics are aligned with the data you
    are reviewing:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand key metrics
  prefs: []
  type: TYPE_NORMAL
- en: Familiarize yourself with the metrics your team uses to evaluate AI.
  prefs: []
  type: TYPE_NORMAL
- en: Look for patterns
  prefs: []
  type: TYPE_NORMAL
- en: If certain metrics misalign with your evaluations, it indicates a need for metric
    refinement.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborate on solutions
  prefs: []
  type: TYPE_NORMAL
- en: Work with your team to adjust metrics to better align with user satisfaction
    and business goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Share Insights and Lead by Example'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your active involvement signals to your team the importance of data review and
    continuous improvement. It fosters a culture where everyone is engaged in improving
    AI performance.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Share your findings
  prefs: []
  type: TYPE_NORMAL
- en: Present interesting insights or issues during team meetings or via email updates.
  prefs: []
  type: TYPE_NORMAL
- en: Encourage open dialogue
  prefs: []
  type: TYPE_NORMAL
- en: Invite team members to discuss their observations and suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: Celebrate successes and learn from failures
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledge both the AI’s strengths and areas for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Your Team Onboard With Data Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are simple steps you can take to make sure your team incorporates data
    review into their work:'
  prefs: []
  type: TYPE_NORMAL
- en: Lead by example
  prefs: []
  type: TYPE_NORMAL
- en: Your commitment will inspire others to take data review seriously.
  prefs: []
  type: TYPE_NORMAL
- en: Create a feedback loop
  prefs: []
  type: TYPE_NORMAL
- en: Establish regular meetings or channels where data review insights can be shared
    and acted upon.
  prefs: []
  type: TYPE_NORMAL
- en: Recognize contributions
  prefs: []
  type: TYPE_NORMAL
- en: Highlight team members who make significant improvements based on data review.
  prefs: []
  type: TYPE_NORMAL
- en: Recap of the Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Figure 8-2](#fig0802) is a flowchart that summarizes the data review process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aete_08in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. The data review process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is further explanation of the flowchart:'
  prefs: []
  type: TYPE_NORMAL
- en: Start daily data review
  prefs: []
  type: TYPE_NORMAL
- en: Begin your dedicated time for reviewing AI interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Access user-centric data viewer
  prefs: []
  type: TYPE_NORMAL
- en: Use the tool that mirrors the user experience to view AI interactions without
    technical distractions.
  prefs: []
  type: TYPE_NORMAL
- en: Select interactions to review
  prefs: []
  type: TYPE_NORMAL
- en: Choose a mix of interactions, focusing on failure modes and challenging scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: View interactions in user interface
  prefs: []
  type: TYPE_NORMAL
- en: Examine the AI’s behavior as the user sees it, within the actual user interface.
  prefs: []
  type: TYPE_NORMAL
- en: Conduct binary evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Determine if the AI solved the user’s problem (Yes/No).
  prefs: []
  type: TYPE_NORMAL
- en: Problem solved?
  prefs: []
  type: TYPE_NORMAL
- en: '*Yes*: Note the success and consider what worked well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*No*: Write a detailed critique as if coaching a new employee.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-reference with metrics
  prefs: []
  type: TYPE_NORMAL
- en: Compare your evaluation with the team’s metrics for that interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics align?
  prefs: []
  type: TYPE_NORMAL
- en: '*Yes*: Proceed to the next interaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*No*: Provide feedback to the team about discrepancies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify patterns and insights
  prefs: []
  type: TYPE_NORMAL
- en: Look for recurring issues or successes to understand broader trends.
  prefs: []
  type: TYPE_NORMAL
- en: Share findings with team
  prefs: []
  type: TYPE_NORMAL
- en: Communicate your insights, fostering collaboration and continuous improvement.
  prefs: []
  type: TYPE_NORMAL
- en: End daily review
  prefs: []
  type: TYPE_NORMAL
- en: Conclude your session, confident that you’ve contributed valuable feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Common Pitfalls to Avoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting this process right takes time and practice. Here are some common pitfalls
    to avoid:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pitfall | Issue | Solution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Relying solely on technical tools | Observability platforms and technical
    logs are designed for engineers and can be overwhelming. | Use a user-centric
    data viewer that presents interactions as customers experience them. |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Avoiding data review due to friction | Complexity and technical barriers
    can discourage regular data review. | Simplify the process with easy-to-access
    tools and a streamlined workflow. |'
  prefs: []
  type: TYPE_TB
- en: '| 3\. Trying to outsource data review | Delegating this task entirely to others
    or relying on AI to review AI can miss critical insights. | Personally engage
    in data review to leverage your unique perspective. |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Overcomplicating evaluations | Using complex scoring systems can create
    inconsistency and confusion. | Stick to a simple Yes or No evaluation to maintain
    clarity. |'
  prefs: []
  type: TYPE_TB
- en: '| 5\. Ignoring failure modes | Focusing only on successes doesn’t address areas
    needing improvement. | Prioritize reviewing and critiquing interactions where
    the AI failed to meet expectations. |'
  prefs: []
  type: TYPE_TB
- en: '| 6\. Not providing specific feedback | Vague critiques are less actionable
    and don’t guide improvement. | Offer detailed, specific feedback as you would
    to a human employee. |'
  prefs: []
  type: TYPE_TB
- en: Best Practices for Effective Data Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to common pitfalls, here are some best practices to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Stay user-focused
  prefs: []
  type: TYPE_NORMAL
- en: Always consider interactions from the user’s perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Be consistent
  prefs: []
  type: TYPE_NORMAL
- en: Regular reviews yield better insights over time.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborate with your team
  prefs: []
  type: TYPE_NORMAL
- en: Use your findings to guide team efforts and improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Embracing a hands-on approach to AI data review empowers you to:'
  prefs: []
  type: TYPE_NORMAL
- en: Align AI performance with business goals
  prefs: []
  type: TYPE_NORMAL
- en: Ensure your AI systems are meeting the needs of your customers and representing
    your brand effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Enhance AI capabilities
  prefs: []
  type: TYPE_NORMAL
- en: Your feedback directly contributes to improving the AI, much like mentoring
    a high-potential employee.
  prefs: []
  type: TYPE_NORMAL
- en: Lead by example
  prefs: []
  type: TYPE_NORMAL
- en: Foster a culture of continuous improvement within your team.
  prefs: []
  type: TYPE_NORMAL
- en: Inform strategy
  prefs: []
  type: TYPE_NORMAL
- en: Use data review insights to shape AI strategy and priorities.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating this data review process into your routine, you’re not just overseeing
    an AI system—you’re actively shaping a powerful tool that can significantly enhance
    your organization’s performance. Your hands-on involvement ensures that the AI
    aligns with your vision and delivers genuine value to your customers.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, your AI is like a super-employee capable of exponential impact. Investing
    time in its development and alignment is not just beneficial—it’s essential.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.html#id132-marker)) From my blog post, [“Your AI Product Needs Evals”](https://hamel.dev/blog/posts/evals).
  prefs: []
  type: TYPE_NORMAL
