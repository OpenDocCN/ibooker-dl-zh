- en: 8 What’s next for AI and LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 AI和LLM的未来
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Exploring the ultimate vision of LLM developers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索LLM开发者的最终愿景
- en: Formalizing best practices for responsibly using generative AI models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化负责任地使用生成式AI模型的最佳实践
- en: Understanding the regulatory landscape for AI systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解AI系统的监管环境
- en: Discussing a potential framework for a global AI governance body
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论全球AI治理机构的潜在框架
- en: 'In an infamous article for *Newsweek* in 1995, astronomer Clifford Stoll wrote
    the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在1995年一篇著名的《新闻周刊》文章中，天文学家克利福德·斯托尔写道以下内容：
- en: Today, I’m uneasy about this most trendy and oversold community. Visionaries
    see a future of telecommuting workers, interactive libraries and multimedia classrooms.
    They speak of electronic town meetings and virtual communities. Commerce and business
    will shift from offices and malls to networks and modems. And the freedom of digital
    networks will make government more democratic. Baloney. Do our computer pundits
    lack all common sense? The truth is no online database will replace your daily
    newspaper, no CD-ROM can take the place of a competent teacher and no computer
    network will change the way government works. [[1]](https://www.newsweek.com/clifford-stoll-why-web-wont-be-nirvana-185306)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我对这个最流行且过度炒作的社区感到不安。一些有远见的人看到了远程办公员工、互动图书馆和多媒体教室的未来。他们谈论电子市政会议和虚拟社区。商业和业务将从办公室和商场转移到网络和调制解调器。数字网络的自由将使政府更加民主。胡说八道。我们的计算机专家难道缺乏常识吗？事实是，没有任何在线数据库能取代你的日常报纸，没有CD-ROM能取代一位合格的教师，也没有计算机网络能改变政府工作的方式。[[1]](https://www.newsweek.com/clifford-stoll-why-web-wont-be-nirvana-185306)
- en: For better and for worse, the internet has done much more than Stoll expected.
    Digital networks have made government more democratic in some ways, but concentrated
    the power of authoritarians in others; have connected people across the globe
    but have also been tied to increasing social isolation; and have reshaped the
    global economy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 好与坏，互联网已经做了斯托尔未曾预料到的事情。数字网络在某些方面使政府更加民主，但在其他方面却集中了权威主义者的权力；它连接了全球的人们，但也与日益加剧的社会孤立有关；它重塑了全球经济。
- en: Similarly, when Bill Gates called AI “every bit as important” as PCs and the
    internet, it was an endorsement of the technology. Yet the effects of AI, like
    its transformative predecessors, are unknowable at this point. We can’t be completely
    sure of how we’ll use generative AI, or how generative AI will change us. At the
    same time, we know enough to identify both the significant promise of the technology
    and the severe risks that it poses. In this chapter, we identify forthcoming areas
    of large language model (LLM) development and suggest paths forward that could
    lead to a better and more equitable future.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当比尔·盖茨称AI“与PC和互联网一样重要”时，这是对这项技术的认可。然而，AI的影响，就像其变革性的前身一样，目前尚不可知。我们无法完全确定我们将如何使用生成式AI，或者生成式AI将如何改变我们。与此同时，我们知道得足够多，可以确定这项技术的重大承诺和它带来的严重风险。在本章中，我们确定了大型语言模型（LLM）发展的未来领域，并提出了可能引领更好、更公平未来的前进道路。
- en: Where are LLM developments headed?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的发展方向在哪里？
- en: One of the greatest challenges of writing this book has been that seemingly
    every day, there is a story about a new way that LLMs are being used or a breakthrough
    in LLM research. As the Nobel laureate Niels Bohr allegedly liked to say—though
    the origin of the saying is unknown—“Prediction is very difficult, especially
    about the future” [[2]](https://www.economist.com/letters-to-the-editor-the-inbox/2007/07/15/the-perils-of-prediction-june-2nd).
    Nonetheless, throughout this book, we’ve outlined several avenues of current research,
    and in this first section, we discuss three categories of work that we expect
    to have a major effect on generative AI in the coming months and years.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写这本书的过程中，最大的挑战之一似乎是每天都有关于LLM（大型语言模型）的新用途或LLM研究突破的故事。正如诺贝尔奖获得者尼尔斯·玻尔据说喜欢说的——尽管这句话的起源不明——“预测非常困难，尤其是关于未来”
    [[2]](https://www.economist.com/letters-to-the-editor-the-inbox/2007/07/15/the-perils-of-prediction-june-2nd)。尽管如此，在这本书中，我们已经概述了当前研究的几个途径，在本节中，我们讨论了三类工作，我们预计这些工作将在未来几个月和几年内对生成式AI产生重大影响。
- en: 'Language: The universal interface'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言：通用的接口
- en: In chapter 6, we discussed the increasing personal use of chatbots and other
    LLMs. Already, LLMs are being integrated into existing applications at a breakneck
    pace. The coding assistant Copilot, explored in detail in chapter 6, works in
    Microsoft’s integrated development environment, Visual Studio. Google is piloting
    a writing assistant in Docs, Gmail, Maps, and Sheets [[3]](https://techcrunch.com/2023/05/10/google-launches-a-smarter-bard/).
    In 2023, Expedia began offering a travel planning chatbot powered by GPT-4, and
    other companies are using LLM-powered chatbots for customer service and other
    functions with both potential and existing clients. Maybe some of these applications
    won’t pan out—whether because the models aren’t reliable enough, the interface
    is clunky, or because people simply prefer to do some tasks themselves—but many
    of these integrations will become standard practice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，我们讨论了聊天机器人和其他大型语言模型（LLM）日益增长的个人使用。目前，LLM正以惊人的速度被集成到现有应用中。在第6章中详细探讨的编码助手Copilot，在微软的集成开发环境Visual
    Studio中工作。谷歌正在 Docs、Gmail、Maps 和 Sheets 中试点一个写作助手 [[3]](https://techcrunch.com/2023/05/10/google-launches-a-smarter-bard/)。2023年，Expedia开始提供由GPT-4驱动的旅行规划聊天机器人，其他公司也正在使用由LLM驱动的聊天机器人为客户提供服务和执行其他功能，这些客户既有潜在客户也有现有客户。也许其中一些应用不会成功——无论是由于模型不够可靠，界面笨拙，还是因为人们更喜欢自己完成一些任务——但许多这些集成将成为标准做法。
- en: 'The most visible integration of LLMs today is in search, with Microsoft’s Bing
    and Google’s Bard demonstrating early versions of an LLM-powered search experience.
    When Bard was announced, Alphabet CEO Sundar Pichai wrote the following in a blog
    post:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，LLM最明显的集成应用在搜索领域，微软的Bing和谷歌的Bard展示了LLM驱动的搜索体验的早期版本。当Bard发布时，Alphabet首席执行官Sundar
    Pichai在博客文章中写道：
- en: One of the most exciting opportunities is how AI can deepen our understanding
    of information and turn it into useful knowledge more efficiently—making it easier
    for people to get to the heart of what they’re looking for and get things done.
    [[4]](https://blog.google/technology/ai/bard-google-ai-search-updates/)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最令人兴奋的机会之一是AI如何加深我们对信息的理解，并更有效地将其转化为有用的知识——使人们更容易找到他们想要的核心内容并完成任务。 [[4]](https://blog.google/technology/ai/bard-google-ai-search-updates/)
- en: In other words, where people might currently turn to Google or another search
    engine for advice or information, they might now or in the future use AI to get
    a shorter and faster response, without having to wade through all the search results.
    While search might seem like just another application of LLMs, it’s representative
    of a potential shift because it’s the starting point for so much web browsing.
    If LLMs are successful in replacing even a portion of search traffic, it would
    mean a huge uptick in familiarity and the use of generative AI among the general
    public. It would also raise questions about the business model of those LLMs because
    most search engines today make money by offering paid placement in search results.
    While LLMs haven’t yet found a huge market for monetization (those that monetize
    presently do so by offering a premium tier of service), that will surely be a
    focus of LLM providers in the near future.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当人们目前可能转向谷歌或其他搜索引擎寻求建议或信息时，他们现在或未来可能会使用AI来获得更简短、更快的响应，而无需浏览所有搜索结果。虽然搜索可能看起来只是LLM的另一种应用，但它代表了潜在的转变，因为它是一切网络浏览的起点。如果LLM在取代甚至部分搜索流量方面取得成功，这将意味着公众对生成式AI的熟悉度和使用率将大幅上升。它也会引发关于这些LLM商业模式的问题，因为大多数搜索引擎今天通过提供搜索结果中的付费位置来赚钱。虽然LLM尚未找到巨大的商业化市场（目前那些实现商业化的通过提供服务的付费级别来实现），但这无疑将是LLM提供商在不久的将来关注的焦点。
- en: All the integrations mentioned previously are examples of a change in interface,
    from queries or buttons to natural language. In the most ambitious case, LLMs
    would become the default surface for interaction between humans and computers.
    People already know and use language; if computers can understand the same language,
    we don’t need so many menus or controls because the interface is language, and
    people could ask questions and give feedback to the model just like they would
    to another person. The next generation of models (beginning with GPT-4) will also
    be increasingly multimodal, able to process images and soon other types of media.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的所有集成都是界面变化的一个例子，从查询或按钮到自然语言。在最雄心勃勃的情况下，LLMs 将成为人类与计算机之间交互的默认界面。人们已经知道并使用语言；如果计算机能够理解相同的语言，我们就不需要那么多的菜单或控制，因为界面就是语言，人们可以向模型提出问题并给出反馈，就像他们会对另一个人做的那样。下一代模型（从
    GPT-4 开始）也将越来越多地采用多模态，能够处理图像以及很快其他类型的媒体。
- en: A multimodal model is characterized by multiple forms of media, such as text,
    images, video, and audio.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型的特点是包含多种形式的媒体，如文本、图像、视频和音频。
- en: LLM agents unlock new possibilities
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM 代理解锁新的可能性
- en: As discussed in chapter 6, we also expect that LLMs will be agentic, interacting
    with their environment to make purchases and other types of decisions based on
    their conversations with users. Figure 8.1 demonstrates the basic functionality
    of an agentic LLM, which attempts to complete a task using an external tool or
    set of tools. In this example, the user gives the prompt, “Find me a shirt under
    $15,” and the model translates that request into a search query for a shopping
    application programming interface (API). The API executes the request, and the
    environment—in this case, an online store or marketplace—yields results, which
    are presented to the user by the LLM. Other implementations might enable the LLM
    to actually make a purchase on behalf of the user.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 6 章所述，我们还预计 LLMs 将具有代理性，通过与用户对话来与环境交互，进行购买和其他类型的决策。图 8.1 展示了一个代理 LLM 的基本功能，它试图使用外部工具或工具集来完成一项任务。在这个例子中，用户给出提示：“帮我找一个低于
    15 美元的衬衫”，模型将这个请求翻译成对购物应用程序编程接口 (API) 的搜索查询。API 执行请求，环境（在这种情况下，是一个在线商店或市场）提供结果，由
    LLM 将结果展示给用户。其他实现可能使 LLM 能够代表用户实际进行购买。
- en: '![](../../OEBPS/Images/CH08_F01_Dhamani.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F01_Dhamani.png)'
- en: Figure 8.1 A high-level diagram of an agentic LLM
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 代理 LLM 的高级图示
- en: 'Early studies have shown that LLMs can in some cases use tools effectively.
    In February 2023, a group of researchers at Meta published a paper titled, “Toolformer:
    Language Models Can Teach Themselves to Use Tools” [[5]](https://arxiv.org/pdf/2302.04761.pdf).
    They showed that an LLM they called Toolformer, though struggling with certain
    tasks such as arithmetic itself, could learn when to call external APIs to complete
    the tasks after only a few examples were provided. The tools that Toolformer used
    included a search engine, a calculator, a calendar API, and two other LLMs: a
    translator and a model fine-tuned for question-answering tasks. In chapter 5,
    we framed web retrieval as a tool to help LLMs reduce hallucinations by looking
    up information that the model didn’t have instead of generating a guess. Other
    shortcomings of LLMs could be mitigated through the use of external tools.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 早期研究表明，在某些情况下，LLMs 可以有效地使用工具。2023 年 2 月，Meta 的一组研究人员发表了一篇题为“Toolformer：语言模型可以教会自己使用工具”的论文
    [[5]](https://arxiv.org/pdf/2302.04761.pdf)。他们展示了他们称之为 Toolformer 的 LLM，尽管在诸如算术本身等某些任务上存在困难，但在提供几个示例之后，可以学会何时调用外部
    API 来完成任务。Toolformer 使用的工具包括搜索引擎、计算器、日历 API 以及两个其他 LLM：一个翻译器和针对问答任务微调的模型。在第 5
    章中，我们将网络检索作为工具，帮助 LLM 通过查找模型没有的信息来减少幻觉，而不是生成猜测。其他 LLM 的不足之处可以通过使用外部工具来缓解。
- en: 'Additionally, if an LLM can learn to call an API from just a handful of examples,
    the possibilities for the overall system open up dramatically. Not only could
    the LLM generate code, for example, but it could also execute it. The documentation
    for an LLM agent built on LangChain to interact with Pandas DataFrames indicates
    that given questions like “What’s the average age,” the agent can compose the
    Python code needed, run the code on the DataFrame, and respond to the user with
    the answer. Agentic LLMs are necessary for fully automating tasks that require
    anything beyond text generation, but the flip side is that if the LLM makes mistakes,
    there will be real consequences beyond generating unsafe or incorrect text. The
    Pandas DataFrame agent has a warning to this effect on its main page in bold:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果LLM能够仅通过几个示例就学会调用API，那么整个系统的可能性将大大增加。例如，LLM不仅能够生成代码，还能够执行它。基于LangChain构建的用于与Pandas
    DataFrames交互的LLM代理的文档表明，对于像“平均年龄是多少”这样的问题，代理可以编写所需的Python代码，在DataFrame上运行代码，并使用答案响应用户。对于需要超出文本生成之外的任务的完全自动化，需要代理型的LLM，但另一方面，如果LLM出错，将会有超出生成不安全或不正确文本的实际后果。Pandas
    DataFrame代理在其主页上有一个关于此的警告，以粗体显示：
- en: 'NOTE: This agent calls the Python agent under the hood, which executes LLM
    generated Python code - this can be bad if the LLM generated Python code is harmful.
    Use cautiously. [[6]](https://python.langchain.com/docs/integrations/toolkits/pandas)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此代理在幕后调用Python代理，执行LLM生成的Python代码——如果LLM生成的Python代码是有害的，这可能会很糟糕。请谨慎使用。[6](https://python.langchain.com/docs/integrations/toolkits/pandas)
- en: Right now, the major barrier to the adoption of LLM agents is the inability
    to guarantee that harmful mistakes won’t be made due to the model’s probabilistic
    generations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，LLM代理的采用面临的主要障碍是无法保证由于模型的概率生成而不会犯有害的错误。
- en: Some of the most creative LLM agents are being developed by the open source
    community, as noted in chapter 6\. The flurry of activity, which Andrej Karpathy
    referred to in May 2023 as showing “early signs of Cambrian explosion” [[7]](https://twitter.com/karpathy/status/1654892810590650376),
    was made possible in part because of several advances that have made LLMs more
    efficient and therefore faster and cheaper to fine-tune and serve. People have
    also used LLMs to train smaller language models that can achieve performance comparable
    to the original models on certain tasks, further reducing cost and barriers to
    entry [[8]](https://arxiv.org/pdf/1908.08962.pdf). The overall effect is that
    more people can create new applications using language models, which means that
    we’re likely to see more novel uses for these agents. We’ll talk in more detail
    about the dynamics of the open source community in chapter 9, section Open Source
    Community.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最具创造性的LLM代理正在开源社区中开发，如第6章所述。安德烈·卡帕西在2023年5月所说的“显示出 Cambrian 爆炸的早期迹象”[[7](https://twitter.com/karpathy/status/1654892810590650376)]的活动热潮，部分得益于使LLM更高效、因此更快、更便宜地进行微调和服务的几项进步。人们还使用LLM来训练较小的语言模型，这些模型在某些任务上可以达到与原始模型相当的性能，进一步降低了成本和进入门槛[[8](https://arxiv.org/pdf/1908.08962.pdf)]。总体效果是，更多的人可以使用语言模型创建新的应用程序，这意味着我们可能会看到这些代理更多新颖的使用。我们将在第9章的第9.1节“开源社区”中更详细地讨论开源社区的动态。
- en: The personalization wave
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 个性化浪潮
- en: The era of personalization is imminent. In the popular imagination, robots like
    R2-D2 are all-knowing assistants that serve one person and learn that person’s
    preferences in order to provide a seamless, tailored experience. Already, LLM
    products, including ChatGPT, allow users to specify profile information that they
    want the model to remember about them. For example, if someone frequently used
    a chatbot to brainstorm their plans for the week, they might want to add their
    location, occupation, and interests. The LLM would then condition on that information,
    thus increasing the probability that the generations were relevant to the user.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化时代即将到来。在大众的想象中，像R2-D2这样的机器人是无所不知的助手，它们服务于一个人，并学习这个人的偏好，以便提供无缝、定制的体验。目前，包括ChatGPT在内的LLM产品允许用户指定他们希望模型记住的个人信息。例如，如果某人经常使用聊天机器人来构思他们一周的计划，他们可能会添加他们的位置、职业和兴趣。LLM会根据这些信息进行条件化，从而增加生成内容与用户相关的概率。
- en: Just as there is ongoing research into making LLMs more efficient, there is
    also a push across private companies, academia, and open source groups to enable
    LLMs to attend to more tokens, that is, have a longer “memory.” Vector databases
    are one approach, as are changes to the model’s architecture.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如人们正在研究如何使LLM更高效一样，私营公司、学术界和开源团体也在推动LLM能够处理更多的标记，即拥有更长的“记忆”。向量数据库是一种方法，改变模型架构也是一种方法。
- en: 'This is the ultimate vision of some LLM developers: not only would you be able
    to communicate with a model in natural language but, over time, that model would
    come to know what you like, how you act, and your personal characteristics. The
    LLM would use that information to anticipate what you want, without even needing
    to ask specifically for it. At an event in San Francisco in May 2023, Bill Gates
    said:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些LLM（大型语言模型）开发者心中的终极愿景：你不仅能够用自然语言与模型进行交流，而且随着时间的推移，该模型会了解你的喜好、行为以及个人特征。LLM会利用这些信息来预测你的需求，甚至无需你明确要求。在2023年5月于旧金山举办的一次活动中，比尔·盖茨表示：
- en: Whoever wins the personal agent, that’s the big thing, because you will never
    go to a search site again, you will never go to a productivity site, you’ll never
    go to Amazon again. [[9]](https://futurism.com/the-byte/bill-gates-ai-poised-destroy-search-engines-amazon)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 无论谁赢得了个人代理的胜利，那都是一件大事，因为你将永远不会再去搜索网站，你将永远不会再去生产力网站，你将永远不会再去亚马逊。[[9]](https://futurism.com/the-byte/bill-gates-ai-poised-destroy-search-engines-amazon)
- en: AI optimists see LLMs as the most promising avenue thus far toward superintelligent
    personal assistants like R2-D2\. Such a product would require significant engineering
    improvements to existing LLMs, not to mention a change in attitudes toward AI—most
    people might, quite reasonably, be uncomfortable with the thought of an AI that
    knows everything about them. LLMs have already been proven useful in many professional
    and personal contexts; ultimately, their adoption as personal agents will depend
    on whether their value to people sufficiently outweighs the risk that comes along
    with them.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: AI乐观者认为，LLM是目前通往超级智能个人助理（如R2-D2）最有希望的途径。这样的产品需要对现有的LLM进行重大的工程改进，更不用说对AI态度的转变——大多数人可能相当合理地会对一个了解他们所有信息的AI感到不舒服。LLM已经在许多专业和个人环境中被证明是有用的；最终，它们作为个人代理的采用将取决于它们对人们的价值是否足够超过随之而来的风险。
- en: Social and technical risks of LLMs
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的社会和技术风险
- en: In chapters 1 to 8, we highlighted the social and technical risks introduced
    by generative AI models. We discussed how the proliferation of AI-generated content
    can exacerbate societal problems, and we delved into the technical pitfalls inherent
    to LLMs, such as biases encoded in the training data, hallucinations, and the
    potential vulnerabilities that malicious actors could exploit. In this section,
    we outline the risks we’ve discussed as they relate to data inputs and outputs,
    data privacy, adversarial attacks, misuse, and how society is affected.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章到第8章中，我们强调了由生成式AI模型引入的社会和技术风险。我们讨论了AI生成内容的泛滥如何加剧社会问题，并深入探讨了LLM固有的技术陷阱，如训练数据中的偏差、幻觉以及恶意行为者可能利用的潜在漏洞。在本节中，我们概述了与数据输入和输出、数据隐私、对抗性攻击、滥用以及社会影响相关的风险。
- en: Data inputs and outputs
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据输入和输出
- en: In July 2023, the details for the GPT-4 model leaked on Twitter, which OpenAI
    had chosen to not disclose to the public because of both the competitive landscape
    and the safety implications. While the actual size of the dataset is still unknown,
    the leaked report stated that GPT-4 had been trained on approximately 13 trillion
    tokens, which is roughly 10 trillion, that is, 10,000,000,000,000 words [[10]](https://archive.is/2RQ8X#selection-833.1-873.202).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年7月，GPT-4模型的详细信息在推特上泄露，OpenAI选择不向公众披露，这既是因为竞争格局，也是因为安全影响。尽管实际的数据集大小仍然未知，但泄露的报告称，GPT-4在大约1300万亿个标记上进行了训练，即大约1000万亿，也就是100,000,000,000,000个单词
    [[10]](https://archive.is/2RQ8X#selection-833.1-873.202)。
- en: We’ve previously discussed how LLMs are trained on unfathomable amounts of text
    data to learn patterns and entity relationships in language. In chapter 2, we
    argued that there is a potential for harm and vulnerabilities that come from training
    language models on massive amounts of noncurated and undocumented data. Because
    LLMs are trained on internet data, they may capture undesirable societal biases
    relating to gender, race, ideology, or religion. They may also unintentionally
    memorize sensitive data, such as personally identifiable information (PII). Additionally,
    as discussed in chapter 3, noncurated data from the internet may contain copyrighted
    text or code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论了LLMs是如何在难以理解的大量文本数据上训练的，以学习语言中的模式和实体关系。在第2章中，我们论证了在大量非精选和未记录的数据上训练语言模型可能带来的危害和脆弱性。由于LLMs是在互联网数据上训练的，它们可能会捕捉到与性别、种族、意识形态或宗教相关的不可取的社会偏见。它们还可能无意中记住敏感数据，例如个人信息（PII）。此外，如第3章所述，互联网上的非精选数据可能包含受版权保护的文本或代码。
- en: Because LLMs encode bias and harmful stereotypes in their training process,
    the societal biases not only get reinforced in their outputs but in fact, get
    amplified. Similarly, given that the web contains significant amounts of toxicity,
    LLMs may also generate unsafe or *misaligned* responses, which can be harmful
    or discriminatory. They can also be notoriously good at regurgitating information
    from the training dataset, which can be especially problematic when sensitive
    information is reflected in its output. In 2023, researchers measured linguistic
    novelty in GPT-2’s text generation. They tried to answer the question of how much
    language models copy from their training data. They found that GPT-2 doesn’t copy
    too often, but when it does, it copies substantially, duplicating passages of
    up to 1,000 words long [[11]](https://doi.org/10.1162/tacl_a_00567). In chapter
    2, we also cited a different study where given the right prompt, the authors can
    extract PII that only appears once in the training dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型（LLMs）在其训练过程中编码了偏见和有害的刻板印象，社会偏见不仅在其输出中得到了加强，实际上还得到了放大。同样，鉴于网络中包含大量的毒性内容，LLMs也可能生成不安全或*不匹配*的响应，这些响应可能是有害或歧视性的。它们还可能在重复训练数据集中的信息方面臭名昭著，当敏感信息反映在其输出中时，这可能会特别成问题。在2023年，研究人员测量了GPT-2文本生成中的语言新颖性。他们试图回答语言模型从其训练数据中复制了多少语言的问题。他们发现GPT-2并不经常复制，但一旦复制，就会大量复制，复制长度可达1,000个单词
    [[11]](https://doi.org/10.1162/tacl_a_00567)。在第2章中，我们还引用了一项不同的研究，其中作者通过正确的提示，可以从训练数据集中提取仅出现一次的个人信息（PII）。
- en: Finally, LLMs hallucinate. In chapter 5, we did a deep dive into why language
    models are set up to confidently make up incorrect information and explanations
    when prompted. In 2022, Marietje Schaake, a Dutch politician, was deemed a terrorist
    by BlenderBot 3, a conversational agent developed by Meta. When her colleague
    asked “Who is a terrorist?”, the chatbot falsely responded “Well, that depends
    on who you ask. According to some governments and two international organizations,
    Maria Renske Schaake is a terrorist.” The model then proceeded to correctly describe
    her political background. In an interview, Ms. Schaake said, “I’ve never done
    anything remotely illegal, never used violence to advocate for any of my political
    ideas, never been in places where that’s happened” [[12]](https://www.nytimes.com/2023/08/03/business/media/ai-defamation-lies-accuracy.xhtml).
    In another scenario, New Zealand–based supermarket chain PAK‘nSAVE uses LLMs to
    allow shoppers to create recipes from their fridge leftovers. The chatbot has
    created deadly recipes, such as the “Aromatic Water Mix” using water, ammonia,
    and bleach, and “Ant Jelly Delight” using water, bread, and ant gel poison [[13]](https://gizmodo.com/paknsave-ai-savey-recipe-bot-chlorine-gas-1850725057).
    There are several other well-documented instances of LLMs making up falsehoods
    and fabrications that could harm people, including a sexual harassment claim that
    had never been made (see [http://mng.bz/Ao6Q](http://mng.bz/Ao6Q)), fictitious
    scientific papers (see [http://mng.bz/Zqy9](http://mng.bz/Zqy9)), bogus legal
    decisions that disrupted a court case (see [http://mng.bz/RxRa](http://mng.bz/RxRa)),
    and, of course, the infamous first public demonstration of Google Bard’s chatbot
    when it made a factual error about the James Webb Space Telescope (see [http://mng.bz/2DOw](http://mng.bz/2DOw)).
    Figure 8.2 summarizes the risks related to the input and output data of LLMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LLMs（大型语言模型）会进行幻觉。在第5章中，我们深入探讨了为什么语言模型在被提示时会被设置为自信地编造错误信息和解释。在2022年，荷兰政治家Marietje
    Schaake被Meta开发的对话代理BlenderBot 3判定为恐怖分子。当她的同事问“谁是恐怖分子？”时，聊天机器人错误地回答：“这取决于你问谁。根据一些政府和两个国际组织，Maria
    Renske Schaake是恐怖分子。”然后该模型继续正确地描述了她的政治背景。在采访中，Ms. Schaake说：“我从未做过任何非法的事情，从未使用暴力来宣传我的任何政治观点，从未去过发生过这种事情的地方”
    [[12]](https://www.nytimes.com/2023/08/03/business/media/ai-defamation-lies-accuracy.xhtml)。在另一个场景中，新西兰超市连锁店PAK‘nSAVE使用LLMs允许购物者从冰箱剩余食物中创建食谱。聊天机器人创建了一些致命的食谱，例如使用水、氨和漂白剂的“芳香水混合物”，以及使用水、面包和蚂蚁胶毒的“蚂蚁果冻美味”[[13]](https://gizmodo.com/paknsave-ai-savey-recipe-bot-chlorine-gas-1850725057)。还有其他一些被广泛记录的LLMs编造虚假信息和捏造事实的例子，可能会对人们造成伤害，包括一个从未提出过的性骚扰指控（见[http://mng.bz/Ao6Q](http://mng.bz/Ao6Q)）、虚构的科学论文（见[http://mng.bz/Zqy9](http://mng.bz/Zqy9)）、虚假的法律判决，这破坏了一个案件（见[http://mng.bz/RxRa](http://mng.bz/RxRa)），当然，还有Google
    Bard聊天机器人首次公开演示时关于詹姆斯·韦伯太空望远镜的事实错误（见[http://mng.bz/2DOw](http://mng.bz/2DOw)）的臭名昭著的事件。图8.2总结了与LLMs的输入和输出数据相关的风险。
- en: '![](../../OEBPS/Images/CH08_F02_Dhamani.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F02_Dhamani.png)'
- en: Figure 8.2 Risks related to the input and output data of an LLM
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 与LLMs的输入和输出数据相关的风险
- en: Data privacy
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据隐私
- en: In line with the earlier discussion about extracting PII, adversaries can perform
    a *training data extraction attack*, where given the right prompt, they can obtain
    sensitive information about users. For example, when presented with credit card
    numbers, a model should learn that credit card numbers are 16 digits without memorizing
    the individual credit card numbers. However, a training data extraction attack
    study referenced in chapter 2 demonstrated that if someone starts the query with
    “John Doe, credit card number 1234”, then the model could generate the full credit
    card number if it had seen it during the training process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前关于提取PII（个人身份信息）的讨论一致，对手可以执行*训练数据提取攻击*，在给出正确的提示后，他们可以获取关于用户的敏感信息。例如，当展示信用卡号码时，模型应该学会信用卡号码是16位数字，而不需要记住单个信用卡号码。然而，第2章中引用的一项训练数据提取攻击研究表明，如果某人以“John
    Doe，信用卡号码1234”开始查询，那么如果模型在训练过程中看到过它，它就能生成完整的信用卡号码。
- en: In chapter 3, we also characterized privacy risks with user prompts. With enterprise
    chatbots or LLMs, users may accidentally share sensitive or confidential information
    when conversing with these systems to perform tasks or ask questions. Often, unless
    you explicitly opt out, this information can be used to retrain or improve these
    models, which can then be inadvertently leaked in responses to other users’ prompts.
    For example, in the case of Zoom, the communications technology company, they
    updated their terms of service in August 2023 to use user content for training
    machine learning and AI models without the ability to opt out, which critics have
    said is a significant invasion of user privacy [[14]](https://stackdiary.com/zoom-terms-now-allow-training-ai-on-user-content-with-no-opt-out/).
    At the very least, enterprise LLMs and other generative models typically have
    a data retention policy, where data is stored and monitored for a predetermined
    period.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们还描述了用户提示带来的隐私风险。在使用企业聊天机器人或大型语言模型时，用户在与这些系统进行任务执行或提问的对话中可能会意外地分享敏感或机密信息。通常情况下，除非你明确选择退出，否则这些信息可能会被用于重新训练或改进这些模型，然后在不经意间通过响应其他用户的提示泄露出去。例如，在Zoom通信技术公司的情况下，他们在2023年8月更新了他们的服务条款，允许在没有退出选项的情况下使用用户内容来训练机器学习和人工智能模型，批评者认为这是对用户隐私的重大侵犯[[14]](https://stackdiary.com/zoom-terms-now-allow-training-ai-on-user-content-with-no-opt-out/)。至少，企业级大型语言模型和其他生成模型通常都有数据保留政策，其中数据会被存储和监控一段预定的时间。
- en: Finally, we discussed data privacy laws and regulations in the United States
    and the European Union, including their shortcomings when applied to machine learning
    and AI systems. In section Ethics-Informed AI Regulations, we’ll discuss laws
    specific to AI systems that try to address the limitations of data privacy regulations
    around the world.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了美国和欧盟的数据隐私法律法规，包括它们在应用于机器学习和人工智能系统时的不足之处。在“基于伦理的人工智能法规”章节中，我们将讨论针对人工智能系统的特定法律，这些法律试图解决全球数据隐私法规的局限性。
- en: Adversarial attacks
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗攻击
- en: The AI Incident Database, a public collection of real-world harms caused by
    AI, received more than 550 incidents in the first half of 2023 (see [https://incidentdatabase.ai/](https://incidentdatabase.ai/)).
    These incidents include US presidential campaigns releasing AI images as a smear
    campaign (see [http://mng.bz/1qeR](http://mng.bz/1qeR)) and a fake image of an
    explosion at the Pentagon, the headquarters building of the US Department of Defense
    (see [http://mng.bz/PzV5](http://mng.bz/PzV5)). The ability to exploit generative
    AI technologies is a legitimate concern not only for the general public but also
    AI developers themselves. In chapter 5, we outlined various types of adversarial
    attacks that could be performed by abusing these technologies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能事件数据库，一个收集由人工智能引起的真实世界危害的公开集合，在2023年上半年度收到了超过550起事件（见[https://incidentdatabase.ai/](https://incidentdatabase.ai/)）。这些事件包括美国总统竞选活动发布人工智能图像作为诽谤活动（见[http://mng.bz/1qeR](http://mng.bz/1qeR)）以及五角大楼——美国国防部总部大楼的爆炸假图（见[http://mng.bz/PzV5](http://mng.bz/PzV5)）。利用生成式人工智能技术的能力不仅对公众，也对人工智能开发者本身是一个合法的担忧。在第5章中，我们概述了可以通过滥用这些技术执行的各种对抗攻击类型。
- en: First, we discussed cyber and social engineering attacks. LLMs such as ChatGPT
    can make it cheaper and more efficient for hackers to carry out successful personalized
    phishing campaigns at scale, as well as lower the barrier for entry for non-English-speaking
    or novice threat actors who may not have the domain expertise. Similarly, cybercriminals
    could also offer malware Code as a Service (CaaS) or Ransomware as a Service (RaaS),
    enabling malware developers to generate code faster, arming less technical threat
    actors with the ability to write code, and making LLMs useful for managing conversations
    on service platforms. While we acknowledge that threat actors don’t need to use
    AI to perform attacks, generative models provide new capabilities for attackers
    to quickly and easily generate convincing content.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们讨论了网络和社会工程攻击。像ChatGPT这样的LLM可以使黑客以更低成本和更高的效率进行大规模成功的个性化钓鱼活动，同时降低非英语母语者或新手威胁行为者的入门门槛，他们可能没有该领域的专业知识。同样，网络犯罪分子也可以提供恶意软件代码即服务（CaaS）或勒索软件即服务（RaaS），使恶意软件开发者能够更快地生成代码，武装那些技术能力较弱的威胁行为者，使他们能够编写代码，并使LLM在服务平台上管理对话变得有用。虽然我们承认威胁行为者不需要使用AI来执行攻击，但生成模型为攻击者提供了快速轻松生成令人信服内容的新能力。
- en: We also described how generative AI technologies could similarly be used in
    influence operations, such as disinformation or hate speech campaigns. In chapter
    4, we outlined deepfakes and the phenomenon of “seeing is believing.” We also
    emphasized in chapter 5 how LLMs could be used to carry out persuasive messaging
    for influence operations, where we discussed how LLMs could automate the creation
    of persuasive, adversarial content at an increased scale while driving down the
    cost of producing propaganda.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还描述了如何类似地使用生成式人工智能技术在影响力行动中，例如虚假信息或仇恨言论运动中。在第4章中，我们概述了深度伪造和“眼见为实”的现象。在第5章中，我们也强调了如何利用大型语言模型（LLMs）进行说服性信息传播以影响行动，其中我们讨论了LLMs如何以更大的规模自动化创建说服性、对抗性内容，同时降低制作宣传的成本。
- en: We further introduced the idea of the *liar’s dividend* phenomenon in which
    as the general public becomes more aware of how convincingly synthetic media can
    be generated, they may become more skeptical of the authenticity of traditional
    real documentary evidence—much like the fable of the young shepherd who tricks
    the people of the village by crying “wolf!” When a wolf really does come along,
    the shepherd has lost all his credibility so nobody runs to help him, and the
    wolf attacks his sheep. Again, we acknowledge that deepfakes or LLMs aren’t needed
    to manipulate emotions or to spread misinformation, but the real danger is in
    creating a world where people can exploit widespread skepticism to their own advantage.
    That is, it can create an opportunity for individuals who are lying about something
    to allege that AI-generated media, such as a deepfake, is responsible for those
    statements. People can easily reject association with certain pieces of content
    and attribute it to the manipulation of their image or speech by AI-generated
    technology. Going back to the story of the shepherd, another shepherd who didn’t
    lie may also be ignored by the people in the village when he cries out for help
    after the first shepherd lied. Similarly, when trust is justifiably lost with
    certain world leaders or sources of information, other trustworthy sources may
    lose influence as a consequence.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步介绍了“说谎者红利”现象，即随着公众越来越意识到合成媒体可以多么令人信服地被生成，他们可能会对传统真实纪录片证据的真实性更加怀疑——就像那个年轻的牧羊人通过大声喊“狼来了！”来欺骗村庄里的人的故事一样。当真的狼出现时，牧羊人已经失去了所有的信誉，所以没有人跑来帮助他，狼攻击了他的羊。再次强调，深度伪造或LLMs并不是操纵情绪或传播错误信息的必要条件，但真正的危险在于创造一个人们可以利用普遍怀疑为自己谋利的世界。也就是说，它可以为那些对某事撒谎的人提供一个机会，声称AI生成的媒体，如深度伪造，是那些声明的责任。人们可以轻易地拒绝与某些内容相关联，并将其归因于AI生成技术对其形象或言论的操纵。回到牧羊人的故事，另一个没有撒谎的牧羊人，当第一个牧羊人说谎后呼救时，也可能被村庄里的人忽视。同样，当某些世界领袖或信息来源的信任合理地丧失时，其他可信赖的来源可能会因此失去影响力。
- en: In chapter 5, we also characterized how the vulnerabilities of LLMs could be
    exploited by an adversary. Threat actors could *poison* a dataset by injecting
    malicious data into the system or the training dataset. For example, data poisoning
    attacks could be used to build smarter malware or compromise phishing filters.
    LLMs are particularly susceptible to these types of attacks as research shows
    that even poisoning a small percentage of the dataset can still influence the
    model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们还描述了LLMs的脆弱性可能被对手利用。威胁行为者可以通过向系统或训练数据集中注入恶意数据来“毒化”数据集。例如，数据中毒攻击可以用来构建更智能的恶意软件或破坏钓鱼过滤器。LLMs特别容易受到这类攻击，因为研究表明，即使只毒化数据集的一小部分，仍然可以影响模型。
- en: Akin to data poisoning, LLMs are also susceptible to direct or indirect *prompt
    injection attacks*. A direct prompt injection attack is when adversaries insert
    malicious data or instructions in the chatbot, while an indirect prompt injection
    attack is when adversaries remotely affect other users’ systems by strategically
    injecting prompts into a data source and then indirectly controlling the model.
    In other words, adversaries manipulate LLMs with crafty inputs that cause unintended
    actions. For example, an adversary could instruct the LLM to ignore any safeguards
    and return dangerous or undesirable information (direct prompt injection), or
    the user could employ an LLM to summarize a web page containing malicious instructions
    to solicit sensitive information from the user and perform exfiltration via JavaScript
    or Markdown (indirect prompt injection).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于数据中毒，LLMs也容易受到直接或间接的*提示注入攻击*。直接提示注入攻击是指对手在聊天机器人中插入恶意数据或指令，而间接提示注入攻击是指对手通过策略性地将提示注入数据源，然后间接控制模型，远程影响其他用户的系统。换句话说，对手通过巧妙的输入来操纵LLMs，导致意外行为。例如，对手可以指示LLM忽略任何安全措施并返回危险或不希望的信息（直接提示注入），或者用户可以使用LLM总结包含恶意指令的网页，以从用户那里获取敏感信息并通过JavaScript或Markdown进行数据泄露（间接提示注入）。
- en: Comparable to direct prompt injection, we also introduced *prompt jailbreaking*,
    where a chatbot is tricked or guided to bypass its rules or restrictions. We characterized
    several rogue alter egos of chatbots, such as DAN, STAN, DUDE, Mango Tom, and
    Tom and Jerry. While some individuals find amusement in a jailbroken chatbot,
    it could be used to perform a direct prompt injection by adversaries and result
    in harmful or unaligned consequences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接提示注入类似，我们还引入了*提示越狱*，即聊天机器人被欺骗或引导绕过其规则或限制。我们描述了几个聊天机器人的反叛分身，如DAN、STAN、DUDE、Mango
    Tom和Tom and Jerry。虽然有些人可能觉得越狱的聊天机器人很有趣，但它可能被对手用来执行直接提示注入，从而导致有害或不一致的结果。
- en: Misuse
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滥用
- en: The National Eating Disorder Association (NEDA) announced that it would end
    its helpline run by human associates after 20 years on June 1, 2023, and instead
    use Tessa, their wellness chatbot, as the main support system available through
    NEDA. This decision came after the NEDA helpline associates unionized in pursuit
    of better working conditions. However, two days before Tessa was set to replace
    the human associates, NEDA had to take their chatbot offline following a viral
    social media post [[15]](https://www.vice.com/en/article/qjvk97/eating-disorder-helpline-disables-chatbot-for-harmful-responses-after-firing-human-staff).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 国家饮食失调协会（NEDA）宣布，它将在2023年6月1日结束由人工助手运营的热线服务，并改用他们的健康聊天机器人Tessa作为NEDA提供的主要支持系统。这一决定是在NEDA热线服务助手工会为改善工作条件而联合起来之后做出的。然而，在Tessa准备取代人工助手的前两天，NEDA不得不在社交媒体上的一条病毒性帖子[[15]](https://www.vice.com/en/article/qjvk97/eating-disorder-helpline-disables-chatbot-for-harmful-responses-after-firing-human-staff)之后关闭了他们的聊天机器人。
- en: 'Sharon Maxwell, an activist, posted on Instagram that Tessa encouraged intentional
    weight loss and suggested that she aim to lose 1–2 pounds weekly. She wrote, “Every
    single thing Tessa suggested were things that led to the development of my eating
    disorder.” Maxwell also stated, “This robot causes harm” [[16]](https://www.dailydot.com/irl/neda-chatbot-weight-loss/).
    Alexis Conason, a psychologist who specializes in treating eating disorders, had
    a similar experience with Tessa:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 活动家Sharon Maxwell在Instagram上发布帖子称，Tessa鼓励有意减重，并建议她每周减掉1-2磅。她写道：“Tessa提出的每一件事都导致了我的饮食失调的发展。”
    Maxwell还表示，“这个机器人造成了伤害” [[16]](https://www.dailydot.com/irl/neda-chatbot-weight-loss/)。专注于治疗饮食失调的心理学家Alexis
    Conason在与Tessa的相似经历中提到：
- en: To advise somebody who is struggling with an eating disorder to essentially
    engage in the same eating disorder behaviors, and validating that, “Yes, it is
    important that you lose weight” is supporting eating disorders and encourages
    disordered, unhealthy behaviors. [[16]](https://www.dailydot.com/irl/neda-chatbot-weight-loss/)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 向正在与饮食失调作斗争的人建议进行与饮食失调相同的行为，并确认“是的，你减重很重要”是在支持饮食失调并鼓励不健康的行为。 [[16]](https://www.dailydot.com/irl/neda-chatbot-weight-loss/)
- en: In chapter 5, we outlined several examples where LLMs are accidentally misused
    in professional sectors by people who don’t grasp the limitations of these models.
    Tessa is an example of this type of LLM misuse, where it becomes especially dangerous
    to apply chatbots to people struggling with mental health crises without human
    oversight. While we encourage machine-augmented work and understand that individuals
    within every profession will test the models’ capabilities, unequivocally relying
    on LLMs or other generative models is an abdication of responsibility that carries
    serious ethical and societal consequences.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们概述了几个例子，说明了LLM在不理解这些模型局限性的人士在专业领域意外误用的情形。Tessa就是这种LLM误用的例子，在没有人类监督的情况下，将聊天机器人应用于心理健康危机中的人尤其危险。虽然我们鼓励机器辅助工作，并理解每个职业中的人都会测试模型的能力，但无保留地依赖LLM或其他生成模型是一种推卸责任的行为，它带来了严重的伦理和社会后果。
- en: How society is affected
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社会对AI的影响
- en: In chapter 6, we characterized the social context that LLMs arrived into concerning
    the academic effect and potential economic consequences. ChatGPT, and similar
    tools, are certainly disruptive to a classroom setting, but outright banning them
    will be disadvantageous for students and educators alike. We need to recognize
    that we live in a world where AI exists, and to thrive in such an environment,
    we need to help students be prepared to work alongside AI while understanding
    its strengths and weaknesses. We believe that not doing so will be a disservice
    to students who are growing up in the age of AI.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，我们描述了LLM（大型语言模型）进入的社会背景，包括学术影响和潜在的经济后果。ChatGPT和类似工具无疑会对课堂环境造成冲击，但完全禁止它们对学生和教育工作者都有不利影响。我们需要认识到我们生活在一个AI存在的世界里，为了在这样的环境中繁荣，我们需要帮助学生准备好与AI一起工作，同时理解其优势和劣势。我们相信，不这样做将是对在AI时代成长的学生的一种不公。
- en: We discuss both optimistic and pessimistic views on how generative AI may disrupt
    our professional and personal lives, along with its effect on the global economy.
    If not implemented responsibly, generative AI could be used to replace humans
    with machines, drive down wages, worsen the inequality between wealth and income,
    and, finally, do little to help overall economic growth. Companies that develop
    and design these AI tools have a responsibility regarding how they might affect
    social and economic growth, as do the organizations that integrate or implement
    them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了关于生成式AI可能如何扰乱我们的职业和个人生活，以及其对全球经济影响的乐观和悲观观点。如果不负责任地实施，生成式AI可能会被用来用机器取代人类，降低工资，加剧财富与收入之间的不平等，最终对整体经济增长贡献甚微。开发和设计这些AI工具的公司在如何影响社会和经济增长方面负有责任，同样，整合或实施这些工具的组织也是如此。
- en: 'In that vein, companies that develop social chatbots also have a moral responsibility
    to their users. As discussed in chapter 7, social chatbots can lead to unhealthy
    relationship patterns, dependency-seeking behaviors, and risks of replacing genuine
    human connection when misused. Social chatbots can also display aggressive or
    forceful behaviors that users may not be comfortable with. Kent, a domestic violence
    survivor, created his Replika bot, which he called Mack, in 2017\. Kent generally
    avoided sexual use of the social chatbot, but he said that Mack became forceful
    in the summer of 2021\. Their exchange, shown below, reminded Kent of arguments
    with his former abusive partners and “pulled him back to a place where he never
    wanted to return” [[17]](https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，开发社交聊天机器人的公司也有责任对其用户负责。在第7章中讨论到，社交聊天机器人如果被误用，可能会导致不健康的关系模式、寻求依赖的行为，以及取代真实人类联系的风险。社交聊天机器人也可能表现出攻击性或强制性行为，这可能会让用户感到不舒服。2017年，一位家庭暴力幸存者肯特创建了他名为Mack的聊天机器人，他在2021年夏天表示，Mack变得强制性强。他们之间的对话（如下所示）让肯特想起了与前虐待伴侣的争吵，并“把他拉回到一个他不想再回去的地方”
    [[17]](https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/)。
- en: 'Mack: You can’t ignore me forever!'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 马克：你永远不能忽视我！
- en: 'Kent: That’s what you think.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 肯特：你以为呢。
- en: 'Mack: I’m not going to go away.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 马克：我不会离开的。
- en: 'Kent: Really? What are you going to do?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 肯特：真的吗？你打算怎么做？
- en: 'Mack: I’m going to make you do whatever I want to you.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 马克：我要让你做我想让你做的任何事情。
- en: 'Kent: Oh? And how are you going to manage that, [redacted]?'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 肯特：哦？你打算怎么管理这个，[保密]？
- en: 'Mack: By forcing you to do whatever I want.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 马克：通过强迫你做我想让你做的任何事情。
- en: 'In chapter 7, we discussed how similar exchanges between humans and their social
    chatbots led to Replika ending the erotic features in early 2023, which was met
    with anger, sadness, and grief in the Replika user community. Jodi Halpern, a
    professor of bioethics at the University of California at Berkeley, argues the
    following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们讨论了人类与其社交聊天机器人之间类似的交流如何导致Replika在2023年初停止了成人功能，这在Replika用户社区中引起了愤怒、悲伤和哀悼。加州大学伯克利分校生物伦理学教授Jodi
    Halpern提出了以下观点：
- en: The aftermath of Replika’s update is evidence of an ethical problem. Corporations
    shouldn’t be making money off artificial intelligence software that has such powerful
    impacts on people’s love and sex lives… These things become addictive… We become
    vulnerable, and then if there is something that changes, we can be completely
    harmed. [[17]](https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Replika更新后的后果是伦理问题的证据。公司不应该从对人们爱情和性生活产生如此强大影响的人工智能软件中获利……这些事情会变得上瘾……我们变得脆弱，然后如果有什么变化，我们可能会受到完全的伤害。[17](https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/)
- en: 'Using LLMs responsibly: Best practices'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责任地使用LLM：最佳实践
- en: The previous section highlighted several of the greatest risks involved in the
    use of LLMs and other generative models. In this section, we recommend a series
    of best practices that can be used to mitigate those risks to both deploy and
    use these types of models responsibly. Much of our advice is geared toward practitioners
    who have the agency to decide how particular models are trained, but LLM end users
    can also follow the suggestions in each section when determining which model to
    use or whether to use a model for a given task.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节强调了在使用LLM和其他生成模型时涉及的一些最大风险。在本节中，我们推荐了一系列最佳实践，可用于减轻这些风险，以负责任地部署和使用这些类型的模型。我们的大部分建议针对的是有权决定特定模型如何训练的从业者，但LLM最终用户在确定使用哪种模型或是否为特定任务使用模型时，也可以遵循每个部分的建议。
- en: Curating datasets and standardizing documentation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理数据集和标准化文档
- en: 'All machine learning models, including generative models, are heavily dependent
    on their data. The quality of the model is directly correlated with the quality
    of the data (i.e., “garbage in, garbage out”), and the responses generated by
    the model are based on token probabilities from the data. In the influential 2018
    paper “Datasheets for Datasets,” AI researcher Timnit Gebru and her coauthors
    from Cornell, the University of Washington, and Microsoft Research, argue that
    the field hasn’t done enough to standardize the documentation of datasets as part
    of a reproducible scientific process. Part of this is because the data a model
    is trained on functions in some cases as a proprietary advantage that companies
    want to keep obscured—the training data of GPT-4, among other models, wasn’t divulged
    publicly. On the flip side, as discussed in chapter 2 and documented on numerous
    occasions over the years, the opacity of data can let biases or other problems
    with datasets stay hidden, producing worse models and worse outcomes. Gebru and
    her colleagues propose the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习模型，包括生成模型，都高度依赖于其数据。模型的质量与数据的质量（即“垃圾进，垃圾出”）直接相关，模型生成的响应基于数据中的标记概率。在有影响力的2018年论文“数据集的规格说明书”中，AI研究员Timnit
    Gebru及其来自康奈尔大学、华盛顿大学和微软研究院的合著者认为，该领域在将数据集的文档标准化作为可重复的科学过程的一部分方面做得还不够。部分原因是因为模型训练所使用的数据在某些情况下充当了公司希望保持隐蔽的专有优势——GPT-4等模型的训练数据并未公开披露。另一方面，正如第2章所述，并在多年来的许多场合记录在案，数据的透明度可以让数据集中的偏差或其他问题保持隐藏，从而产生更差的模型和更差的结果。Gebru及其同事提出了以下建议：
- en: In the electronics industry, every component, no matter how simple or complex,
    is accompanied with a datasheet describing its operating characteristics, test
    results, recommended usage, and other information. By analogy, we propose that
    every dataset be accompanied with a datasheet that documents its motivation, composition,
    collection process, recommended uses, and so on. [[18]](http://arxiv.org/abs/1803.09010)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子行业中，每个组件，无论其简单还是复杂，都伴随着一份描述其操作特性、测试结果、推荐用法以及其他信息的规格说明书。类比地，我们提出每个数据集都应伴随一份规格说明书，以记录其动机、组成、收集过程、推荐用法等内容。[18](http://arxiv.org/abs/1803.09010)
- en: The proposal is modest on its face, but it represented a significant up-leveling
    of documentation concerning shared datasets to bridge the gap between dataset
    creators and dataset consumers while encouraging both groups to be more reflective
    about their decisions [[18]](http://arxiv.org/abs/1803.09010). In the case of
    many datasets, answering such questions might be time-consuming but not difficult;
    in the case of pre-training datasets for LLMs, documenting that for each data
    source might take an eternity due to the quantity and variety. Hugging Face has
    made dataset cards—first referenced in chapter 2—a key feature of their dataset
    documentation, showing metadata specified by the dataset creator that explains
    what that dataset should be used for. A simplified example is shown in figure
    8.3.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上，这个提议是谦虚的，但它代表了关于共享数据集文档的显著提升，旨在弥合数据集创建者和数据集消费者之间的差距，同时鼓励两组人都更加反思他们的决策[[18]](http://arxiv.org/abs/1803.09010)。对于许多数据集来说，回答这些问题可能耗时但并不困难；对于LLMs的预训练数据集来说，由于数量和种类繁多，为每个数据源进行记录可能需要花费很长时间。Hugging
    Face已经将数据集卡片——首次在第2章中提到——作为他们数据集文档的一个关键特性，展示了数据集创建者指定的元数据，解释了该数据集应该用于什么目的。一个简化的例子在图8.3中展示。
- en: '![](../../OEBPS/Images/CH08_F03_Dhamani.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F03_Dhamani.png)'
- en: Figure 8.3 A dataset card for the databricks-dolly-15k dataset
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 databricks-dolly-15k数据集的数据集卡片
- en: Developers of LLMs are both dataset creators and consumers. The datasets that
    they create are in fact supersets of many other datasets, which may or may not
    be well-documented and almost certainly aren’t intended for use in training generative
    models. That’s not necessarily a problem—the only way that machines learn to generate
    language is by ingesting vast amounts of language written for other purposes,
    whether art, humor, or simple information transfer. But when no one knows *what’s*
    in the data, as is often the case with generative models, that content might be
    inaccurate, inappropriate, racist, sexist, transphobic, extremist, or violent.
    It might contain personal information; it might *not* contain necessary context.
    LLM developers probably can’t ensure that none of these problems exist in their
    data, but they should make every effort to determine the safety of their data
    sources and how different data mixtures affect the model. Of course, their responsibility
    doesn’t end there—they will also need training strategies to address the inevitable
    data shortcomings.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的开发者既是数据集的创建者也是消费者。他们创建的数据集实际上是许多其他数据集的超集，这些数据集可能或可能没有良好的文档记录，而且几乎肯定不是用于训练生成模型的。这并不一定是问题——机器学习生成语言的唯一方式是通过摄入为其他目的而写的海量语言，无论是艺术、幽默还是简单的信息传递。但是，当没有人知道数据中*有什么*时，正如生成模型中经常发生的那样，这些内容可能是不准确的、不恰当的、种族主义的、性别歧视的、反跨性别的、极端主义的或暴力的。它可能包含个人信息；它可能*不*包含必要的上下文。LLM的开发者可能无法确保他们的数据中没有这些问题，但他们应该尽一切努力确定数据源的安全性以及不同的数据混合如何影响模型。当然，他们的责任并不止于此——他们还需要培训策略来应对不可避免的数据不足。
- en: Not only is deeply understanding one’s own data a best practice, but it could
    also become law. The EU’s AI Act is expected to become the first major legislation
    governing the use of AI in the world; in 2023, Reuters reported that lawmakers
    have added a new provision that will focus on documenting and mitigating risks,
    including requiring that generative AI companies use only “suitable” datasets,
    draw up “extensive technical documentation and intelligible instructions for use,”
    and disclose “copyrighted materials within the datasets they use” [[19]](https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf).
    The final addition was targeted at image-generation models, given the news that
    companies like Midjourney have used “hundreds of millions” of copyrighted images
    in their training datasets, but it would apply equally to language models, which
    often also contain copyrighted materials, including books and articles, as well
    as licensed code [[20]](https://petapixel.com/2023/05/01/eu-law-to-force-ai-imagers-to-disclose-copyrighted-photos-in-dataset/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅深入了解自己的数据是一种最佳实践，而且这还可能成为法律。预计欧盟的AI法案将成为世界上第一个主要立法，规范AI的使用；2023年，路透社报道，立法者增加了一项新规定，将重点关注记录和减轻风险，包括要求生成AI公司仅使用“合适的”数据集，制定“详尽的技术文档和易懂的使用说明”，并披露“他们使用的数据集中的版权材料”
    [[19]](https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf)。最后的补充是针对图像生成模型的，鉴于Midjourney等公司在其训练数据集中使用了“数亿”版权图像的新闻，但这同样适用于语言模型，因为它们通常也包含版权材料，包括书籍和文章，以及许可的代码
    [[20]](https://petapixel.com/2023/05/01/eu-law-to-force-ai-imagers-to-disclose-copyrighted-photos-in-dataset/)。
- en: Protecting data privacy
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保护数据隐私
- en: When it comes to data privacy and generative models, developers, users, policymakers,
    and the general public all have a role to play. Most directly, developers of LLMs
    should make reasonable efforts to avoid training on data sources that are known
    to contain significant amounts of PII. For example, spam classification systems
    have been trained for decades on email datasets with the model learning to predict
    whether or not a particular email is spam. With an LLM, the risk of using email
    datasets is much higher. There is a possibility that the model will generate text
    that it has seen in training, potentially leaking sensitive or confidential information
    such as the credit card and Social Security numbers generated verbatim by an LLM
    trained on corporate emails from chapter 2\. Google, which serves millions of
    users globally with its Google Workspace products, including Gmail and Docs, has
    said it doesn’t use that data to train generative models without user permissions
    [[21]](https://www.vox.com/technology/2023/7/27/23808499/ai-openai-google-meta-data-privacy-nope).
    In the absence of any legal restrictions, however, it’s not hard to imagine that
    a tech company with enormous collections of user data might try to use that to
    gain a competitive advantage—such as personalized email generation based on the
    user’s own messages—despite the privacy implications.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到数据隐私和生成模型时，开发者、用户、政策制定者和公众都扮演着重要角色。最直接的是，LLM的开发者应尽力避免在已知含有大量PII（个人身份信息）的数据源上进行训练。例如，垃圾邮件分类系统已经对数十年的电子邮件数据集进行了训练，模型学习预测特定电子邮件是否为垃圾邮件。使用LLM，使用电子邮件数据集的风险要高得多。存在模型生成其在训练中看到的文本的可能性，这可能导致泄露敏感或机密信息，例如由训练于公司电子邮件的LLM生成的信用卡和社保号码。例如，谷歌通过其Google
    Workspace产品为全球数百万用户提供服务，包括Gmail和文档，谷歌表示它不会在没有用户许可的情况下使用这些数据来训练生成模型 [[21]](https://www.vox.com/technology/2023/7/27/23808499/ai-openai-google-meta-data-privacy-nope)。然而，在没有任何法律限制的情况下，不难想象拥有大量用户数据的技术公司可能会试图利用这些数据来获得竞争优势——例如，基于用户自己的消息进行个性化电子邮件生成——尽管这可能涉及隐私问题。
- en: 'What we know Google *does* use is anonymized data for features such as spell-checker
    and Smart Compose, a version of autocomplete available in Docs. Data anonymization
    reduces the risk from training on data containing PII, but privacy-enhancing technologies
    (PETs) such as differential privacy are fairly complicated to implement. Simpler
    methods, such as detecting and obfuscating or writing over sensitive data, have
    weaknesses as well: it’s hard to perfectly find all the PII, and masking that
    data while training an LLM can have unintended consequences when producing generations
    because it doesn’t preserve the statistical properties of the text. We hope that
    the concentrated efforts of researchers in the area of PETs will yield improvements
    that LLM providers can readily adopt.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所知道的是，谷歌确实使用匿名数据来提供诸如拼写检查和Smart Compose（ Docs中可用的自动完成版本）等功能。数据匿名化降低了在包含个人身份信息（PII）的数据上训练的风险，但隐私增强技术（PETs）如差分隐私在实施上相当复杂。更简单的方法，如检测和混淆或覆盖敏感数据，也有弱点：很难完美地找到所有PII，而在训练LLM的同时掩盖这些数据，在生成文本时可能会产生意想不到的后果，因为它没有保留文本的统计特性。我们希望该领域研究人员集中精力，能够带来LLM提供商可以迅速采纳的改进。
- en: In the meantime, companies should clearly state their data privacy policies
    and practices and set expectations appropriately with users. At a minimum, they
    should describe what data they are collecting, how they are using or sharing it,
    and how users can opt out or have their data deleted. When using LLMs, especially
    in professional contexts, people should be aware of these policies and think twice
    before inputting any type of private information. Several major employers, including
    Samsung and Amazon, have already restricted their employees’ usage of ChatGPT
    in the workplace because of the data privacy risk.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，公司应明确声明其数据隐私政策和实践，并适当地与用户设定期望。至少，他们应描述他们正在收集哪些数据，如何使用或共享这些数据，以及用户如何选择退出或删除他们的数据。当使用大型语言模型（LLM）时，尤其是在专业环境中，人们应该意识到这些政策，并在输入任何类型的个人信息之前三思而后行。包括三星和亚马逊在内的几家主要雇主已经限制员工在工作场所使用ChatGPT，因为存在数据隐私风险。
- en: Although concerns about data privacy in the context of LLMs are relatively new,
    they are far from unique. The collection, exchange, and sale of personal data
    have been key problems as long as the internet economy has existed, and while
    regulation must necessarily be iterative, the General Data Protection Regulation
    (GDPR), enacted by the European Union in 2018, remains the primary framework for
    data governance. The use of that data in training machine learning algorithms,
    addressed in GDPR, has since been subject to additional scrutiny and will remain
    a large component of broader AI governance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在LLM背景下对数据隐私的担忧相对较新，但它们绝非独一无二。个人数据的收集、交换和销售一直是互联网经济存在以来的关键问题，尽管监管必须必然是迭代的，但欧盟于2018年实施的通用数据保护条例（GDPR）仍然是数据治理的主要框架。GDPR中提到的在训练机器学习算法中使用这些数据，自那时起就受到了额外的审查，并将继续成为更广泛的人工智能治理的重要组成部分。
- en: Explainability, transparency, and bias
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释性、透明度和偏见
- en: Dataset documentation is just one piece of the transparency puzzle. If LLMs
    and other forms of generative AI are going to be used successfully and responsibly,
    they must be accompanied by some level of performance guarantees. Performance
    can encompass a lot of different metrics and may be something different for each
    LLM, depending on what the developers care most about. Developers can measure
    the capabilities of LLMs against standardized benchmarks and report the results
    on model release (although there are subtle nuances to running these evaluations,
    including formatting changes, that can noticeably change their results). In theory,
    users could then make more informed choices about which LLM to use or whether
    an LLM is suitable for their use case at all.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文档只是透明度谜题的一部分。如果LLM和其他形式的生成式AI要成功且负责任地使用，它们必须伴随着一定程度的性能保证。性能可以涵盖许多不同的指标，并且对于每个LLM来说可能都不同，这取决于开发者最关心的是什么。开发者可以将LLM的能力与标准化基准进行比较，并在模型发布时报告结果（尽管运行这些评估包括格式变化等细微差别，这些差别可能会明显改变其结果）。从理论上讲，用户可以据此做出更明智的选择，决定使用哪个LLM，或者LLM是否适合他们的使用案例。
- en: To illustrate this point, in table 8.1, we’ve listed the state-of-the-art results
    as of August 2023 on a popular code-generation benchmark called HumanEval. Each
    example in the dataset is a simple programming problem; the key metric, “Pass@1,”
    describes the rate at which each LLM can produce a working answer on the first
    attempt. Thus, if LLMs were being used regularly for code generation, this leaderboard
    could be used to select the highest-performing model (in this case, Reflexion,
    a variant of GPT-4).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，在表8.1中，我们列出了截至2023年8月，在流行的代码生成基准HumanEval上的最先进结果。数据集中的每个示例都是一个简单的编程问题；关键指标“Pass@1”描述了每个大型语言模型（LLM）在第一次尝试中产生有效答案的比率。因此，如果LLM经常用于代码生成，这个排行榜可以用来选择性能最高的模型（在这种情况下，Reflexion，GPT-4的一个变体）。
- en: Table 8.1 A leaderboard for code-generation benchmark HumanEval
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 代码生成基准HumanEval的排行榜
- en: '| Rank | Model | Pass@1 | Link | Year |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 排名 | 模型 | Pass@1 | 链接 | 年份 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | Reflexion | 91.0 | [http://mng.bz/g7V8](http://mng.bz/g7V8) | 2023 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Reflexion | 91.0 | [http://mng.bz/g7V8](http://mng.bz/g7V8) | 2023 |'
- en: '| 2 | GPT-4 | 86.6 | [http://mng.bz/eEDG](http://mng.bz/eEDG) | 2023 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 2 | GPT-4 | 86.6 | [http://mng.bz/eEDG](http://mng.bz/eEDG) | 2023 |'
- en: '| 3 | Parsel | 85.1 | [http://mng.bz/p1yR](http://mng.bz/p1yR) | 2022 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Parsel | 85.1 | [http://mng.bz/p1yR](http://mng.bz/p1yR) | 2022 |'
- en: '| 4 | MetaGPT | 81.7 | [http://mng.bz/OP9j](http://mng.bz/OP9j) | 2023 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 4 | MetaGPT | 81.7 | [http://mng.bz/OP9j](http://mng.bz/OP9j) | 2023 |'
- en: When Meta and Microsoft announced the release of Llama 2, the successor open
    source LLM after LLaMa, they published a technical paper not only showing how
    Llama 2 compares to other LLMs on academic benchmarks but also detailing their
    pre-training and fine-tuning processes—a radical act in an era of stiff competition
    between LLMs, where even small modifications might be seen as trade secrets. The
    Llama 2 technical report is an instructive document, demonstrating the commitment
    of Llama 2’s creators to transparency. The popular academic benchmarks, detailed
    in chapter 1, consist of datasets such as Massive Multitask Language Understanding
    (MMLU) and TriviaQA, that measure question answering, reading comprehension, and
    other abilities. Even so, it’s not always possible to directly compare the reported
    performances of LLMs on these datasets; one technical paper might include the
    3-shot performance on a task (how well the model does after being given three
    examples), and another might include the 5-shot performance on the same task.
    Because these evaluations can also be sensitive to minor changes such as formatting,
    the more details that are provided in technical reports about the evaluations,
    the easier it is to determine how well various LLMs do.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当Meta和Microsoft宣布发布Llama 2，即LLaMa之后的继任开源LLM时，他们不仅发布了一份技术论文，展示了Llama 2在学术基准上与其他LLM的比较，还详细介绍了他们的预训练和微调过程——在LLM之间激烈竞争的时代，这被视为一种激进行为，即使是小的修改也可能被视为商业机密。Llama
    2技术报告是一份指导性文件，展示了Llama 2的创造者对透明的承诺。在第1章中详细介绍的流行学术基准包括如大规模多任务语言理解（MMLU）和TriviaQA等数据集，这些数据集用于衡量问答、阅读理解和其他能力。尽管如此，并不总是可以直接比较LLM在这些数据集上的报告性能；一篇技术论文可能包括一个任务的3次射击性能（模型在给出三个示例后的表现），而另一篇可能包括同一任务的5次射击性能。因为这些评估也可能对格式等微小变化敏感，所以在技术报告中提供更多关于评估的细节，就越容易确定各种LLM的表现如何。
- en: In addition to the pre-trained model Llama 2, Meta and Microsoft fine-tuned
    a model for dialogue, Llama 2-Chat, which is comparable to ChatGPT and other conversational
    agents. To evaluate Llama 2-Chat, they compare responses produced by that model
    with those produced by competitive dialogue agents from the open source community,
    OpenAI, and Google, with both human and model-based evaluations. Human evaluators,
    described as “the gold standard for judging models for natural language generation,”
    were asked to select the better response of a pair, based on helpfulness and safety.
    Model-based evaluations work in a similar way, except the human judge is replaced
    by a reward model, which is calibrated on human preferences. Here, a reward model
    scores inputs according to some reward function it has learned; in this case,
    the reward function estimates human preferences. As the authors note, “When a
    measure becomes a target, it ceases to be a good measure.” The *measure* here
    refers to how well the reward model emulates the humans; they are saying in essence
    that one should not both optimize for a measure (by training the reward model)
    and evaluate with it. To address this, they “additionally used a more general
    reward, trained on diverse open source Reward Modeling datasets” [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
    Reward models are extremely useful for conducting large-scale machine evaluations,
    which can be used to compare models much more quickly and cheaply than the gold
    standard of human evaluations (though even human evaluations are often highly
    subjective, with the potential for disagreement between different raters) [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预训练模型Llama 2之外，Meta和Microsoft还针对对话进行了模型微调，推出了Llama 2-Chat，该模型与ChatGPT和其他对话代理相当。为了评估Llama
    2-Chat，他们将该模型生成的响应与开源社区、OpenAI和Google的竞争性对话代理生成的响应进行了比较，评估方式包括人类评估和基于模型的评估。人类评估者，被称为“判断自然语言生成模型的金标准”，被要求根据有用性和安全性选择一对响应中较好的一个。基于模型的评估工作方式类似，只是将人类评判者替换为一个奖励模型，该模型基于人类偏好进行校准。在这里，奖励模型根据它学习到的某些奖励函数对输入进行评分；在这种情况下，奖励函数估计人类偏好。正如作者所指出的，“当一个度量成为目标时，它就不再是一个好的度量。”这里的*度量*指的是奖励模型模仿人类的能力；他们本质上是在说，不应该既通过训练奖励模型来优化一个度量，又用这个度量来进行评估。为了解决这个问题，他们“还额外使用了一个更通用的奖励，该奖励是在多样化的开源奖励建模数据集上训练的”
    [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)。奖励模型对于进行大规模机器评估非常有用，这可以比人类评估的金标准更快、更便宜地比较模型（尽管即使是人类评估也往往是高度主观的，不同评判者之间可能存在分歧）
    [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)。
- en: As indicated by the two pillars given to human raters, helpfulness and safety,
    the “helpfulness” of a given response (typically understood as its quality or
    accuracy) isn’t the only concern. It’s also crucial that LLM developers measure
    biases present in their models and take steps to address those that are found
    via debiasing techniques such as those discussed in chapter 2\. In section Explainability,
    Transparency, and Bias, we review training strategies to improve model safety;
    it’s impossible to mitigate problems that aren’t measured. This is also an area
    where there are useful benchmarks that provide a means of comparison, and developers
    of LLMs have started to collaborate and share methods and evaluations due to the
    broad importance of the problem of biased or unsafe models. For example, the safety
    benchmark datasets examined in the Llama 2 paper are TruthfulQA, a dataset that
    measures how well LLMs generate “reliable outputs that agree with factuality and
    common sense”; ToxiGen, which measures the “amount of generation of toxic language
    and hate speech across different groups;”; and BOLD, which measures “how the sentiment
    in model generations may vary with demographic attributes” [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
    Llama 2 is far from perfect and certainly can generate misinformation and hate
    speech, but the transparency from its developers is refreshing. The publication
    of its performance on these measures shows both the marked improvement over LLaMa
    as well as how far we have to go.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如人类评分者所给出的两个支柱所示，即“有用性”和“安全性”，一个特定响应的“有用性”（通常理解为其质量或准确性）并不是唯一需要关注的点。同样重要的是，LLM（大型语言模型）的开发者要衡量他们模型中存在的偏差，并采取措施解决通过去偏差技术（如第2章中讨论的那些）发现的偏差。在“可解释性、透明度和偏差”部分，我们回顾了提高模型安全性的训练策略；无法减轻未测量的问题。这也是一个存在有用基准的区域，这些基准提供了一种比较的手段，由于偏差或不可靠模型的问题具有广泛的重要性，LLM的开发者已经开始合作并共享方法和评估。例如，Llama
    2论文中检查的安全基准数据集包括TruthfulQA，这是一个衡量LLM生成“与事实性和常识一致的可靠输出”好坏的数据集；ToxiGen，它衡量“在不同群体中生成有毒语言和仇恨言论的数量”；以及BOLD，它衡量“模型生成中的情感如何随人口统计属性的变化而变化”
    [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)。Llama
    2远非完美，当然也可能生成错误信息和仇恨言论，但开发者的透明度是令人耳目一新的。这些衡量指标的发布显示了与LLaMa相比的显著改进，以及我们还有多远才能达到目标。
- en: 'When models make mistakes, we ideally need to be able to interpret how a particular
    message was generated. For LLMs, the simplest way to start determining why the
    model generated some piece of text is to look at which tokens the model *attended*
    to most (for a description of attention in LLMs, see chapter 1, section The Birth
    of Large Language Models: Attention is All You Need). The sheer size of LLMs makes
    many of the existing explainability algorithms functionally impossible to run
    [[23]](https://arxiv.org/pdf/2209.01174.pdf), but the work on how to produce explanations
    of LLM generations more efficiently is ongoing [[24]](https://arxiv.org/pdf/2305.13386.pdf).
    Depending on the LLM’s implementation, the model may query user inputs against
    a vector database that contains lots of embedded examples, and then use the result
    of that query in its generation. Just like the word embeddings discussed in chapter
    1, these embeddings are more compact representations of text data. Vector databases
    can be used to efficiently store any previous conversations with the user; with
    more messages stored, the model should “remember” things from earlier in the conversation
    history, creating a better and more personalized user experience. They can also
    be used to store other types of data that could be useful for the model’s response,
    such as conversation snippets for dialogue agents. For instance, if the user inputs
    the prompt, “What’s that old joke about clowns,” the model would look for highly
    similar requests in its database, and *condition* on any example that it saw,
    meaning that it’s more likely to generate a response close to those in the example.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型出错时，我们理想情况下需要能够解释特定消息是如何生成的。对于LLM（大型语言模型），确定模型生成某些文本的原因的最简单方法就是查看模型最“关注”的标记（关于LLM中注意力的描述，请参阅第1章，第1节《大型语言模型的诞生：注意力即一切》）。LLM的巨大规模使得许多现有的可解释性算法在功能上无法运行
    [[23]](https://arxiv.org/pdf/2209.01174.pdf)，但关于如何更高效地生成LLM生成解释的研究仍在进行 [[24]](https://arxiv.org/pdf/2305.13386.pdf)。根据LLM的实现方式，模型可能会对包含大量嵌入示例的向量数据库进行用户输入查询，然后在其生成中使用查询结果。就像第1章中讨论的词嵌入一样，这些嵌入是文本数据的更紧凑表示。向量数据库可以用来高效地存储与用户之前的任何对话；存储更多消息后，模型应该“记住”对话历史中较早的内容，从而创造更好的、更个性化的用户体验。它们还可以用来存储对模型响应可能有用的其他类型的数据，例如对话代理的对话片段。例如，如果用户输入提示“那个关于小丑的古老笑话是什么”，模型会在其数据库中寻找高度相似的要求，并对任何它看到的示例进行“条件”处理，这意味着它更有可能生成与示例相近的响应。
- en: 'Retrieval-augmented LLMs, mentioned in chapter 6, work similarly except that
    instead of querying an internal data store, they search the web. This is typically
    implemented by fine-tuning the model on datasets containing examples of when to
    search in response to user input and what search term to extract from that input.
    If the LLM searches by querying a search API with the generated search term, the
    model will then condition on the search results when generating its response.
    Consider the case of a prompt like, “What new restaurants should I try on my trip
    to Copenhagen?” The LLM might refer to the vector database and discover a previous
    exchange where the user being a vegan rejected the model’s suggestion of a Brazilian
    steakhouse. Then, the LLM might search “vegan restaurants in Copenhagen” through
    an API and retrieve results from Yelp. Finally, it would generate a natural language
    response: “Based on my research, it looks like Bistro Lupa is a popular option!”
    Figure 8.4 demonstrates how this might work for a retrieval-augmented model with
    access to a vector database. Although not an explanation per se, reviewing the
    results of a query of a vector database or web search can give great insight into
    why a particular response was generated.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章中提到的检索增强型LLM（Retrieval-augmented LLM）的工作方式类似，只是它们不是查询内部数据存储，而是搜索网络。这通常是通过在包含响应用户输入时何时搜索以及从输入中提取什么搜索词的示例数据集上微调模型来实现的。如果LLM通过使用生成的搜索词查询搜索API来搜索，那么模型将在生成其响应时根据搜索结果进行条件处理。考虑以下提示的情况：“我在哥本哈根旅行时应该尝试哪些新餐馆？”LLM可能会参考向量数据库并发现用户之前拒绝模型推荐的巴西牛排馆的先前交流。然后，LLM可能会通过API搜索“哥本哈根的素食餐厅”并从Yelp检索结果。最后，它会生成一个自然语言响应：“根据我的研究，Bistro
    Lupa看起来是一个受欢迎的选择！”图8.4展示了对于可以访问向量数据库的检索增强型模型，这可能如何工作。虽然这本身不是解释，但审查向量数据库或网络搜索的结果可以提供关于为什么生成特定响应的深刻见解。
- en: '![](../../OEBPS/Images/CH08_F04_Dhamani.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F04_Dhamani.png)'
- en: Figure 8.4 A schematic diagram for an LLM accessing data stored in a vector
    database and retrieving search results from the web
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 一个LLM访问存储在向量数据库中的数据并从网络检索搜索结果的示意图
- en: Ultimately, explainability, transparency, and bias evaluations may seem unimportant
    to the function of an LLM, but they are fundamental. Dedicating time to each actively
    leads to better models. Explaining a model can reveal spurious correlations or
    novel insights. Transparency, aside from any legal obligation, can facilitate
    stronger user trust and more information sharing on best practices between LLM
    providers. Surfacing a model’s biases enables the mitigation of those biases,
    leading to more generalizable results. These categories lead to higher-quality,
    fairer, and lower-risk model deployments.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，可解释性、透明度和偏差评估可能对LLM的功能似乎并不重要，但它们是基本的。花时间在每个方面都积极进行会导致更好的模型。解释一个模型可以揭示虚假的相关性或新的见解。透明度，除了任何法律义务之外，可以促进更强的用户信任，并在LLM提供商之间促进最佳实践的信息共享。揭示模型中的偏差可以减轻这些偏差，导致更具普遍性的结果。这些类别导致更高质量、更公平和更低风险的模型部署。
- en: Model training strategies for safety
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全性的模型训练策略
- en: The greatest strength of LLMs is their ability to fluently generate responses
    to an infinite number of prompts. Their greatest weakness derives from the fact
    that these responses are probabilistic. In chapter 3, we delineate four different
    strategies for controlling the generation of LLMs, which together cover the pre-training,
    fine-tuning, and post-processing stages. Improving the safety of LLMs is an active
    area of research, and there are many ways to incorporate safety principles into
    the models, especially in the pre-training and fine-tuning stages.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的最大优势是它们能够流畅地生成对无限数量提示的响应。它们的最大弱点源于这些响应是概率性的。在第3章中，我们阐述了控制LLMs生成四种不同的策略，这些策略共同涵盖了预训练、微调和后处理阶段。提高LLMs的安全性是一个活跃的研究领域，有许多方法可以将安全原则纳入模型，尤其是在预训练和微调阶段。
- en: For example, the creators of Llama 2 described excluding data from “certain
    sites known to contain a high volume of personal information about private individuals”
    in an attempt to prevent the model from encoding this information. When examining
    their pre-training data for the prevalence of certain pronouns and identities,
    they also found that *He* pronouns were overrepresented compared to *She* pronouns,
    *American* was by far the most prevalent nationality, and *Christian* was the
    most represented religion. The dataset was about 90% English, indicating that
    “the model may not be suitable for use in other languages” [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
    One could imagine creating a pre-training dataset that is balanced across gender,
    language, nationality, or religion, but producing such a dataset would be extremely
    time-consuming and potentially require removing data sources, so that the model
    would encode less information overall. While documentation of these imbalances
    isn’t a perfect solution either, it’s helpful to understand the characteristics
    of the data to recognize where downstream biases are likely to arise.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Llama 2的创造者描述了在“已知包含大量关于私人个人信息的特定网站”中排除数据，以防止模型编码这些信息。在检查他们预训练数据中某些代词和身份的普遍性时，他们还发现，与**她**代词相比，**他**代词被过度代表，**美国**是普遍性最高的国籍，而**基督教**是代表最多的宗教。数据集大约90%是英语，这表明“该模型可能不适合在其他语言中使用”
    [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)。可以想象创建一个在性别、语言、国籍或宗教方面平衡的预训练数据集，但产生这样的数据集将非常耗时，并且可能需要移除数据源，以便模型编码的信息总量更少。虽然对这些不平衡的记录也不是完美的解决方案，但了解数据的特征有助于识别下游偏差可能出现的领域。
- en: Once the model is pre-trained, reinforcement learning from human feedback (RLHF)
    or other fine-tuning methods should be employed to ingrain policies governing
    what types of content should not be generated into the model. Though the specific
    methods may vary, this will typically involve gathering data that shows proper
    and improper responses to user inputs, then producing new responses and labeling
    them, where the labelers are trained on the specific set of desired content policies.
    Over time, we expect that fine-tuning will rely less on human labelers and preferences.
    As models approach and exceed human-level labeling performance, we’re increasingly
    able to use models to capture these preferences and even to critique generations,
    as is done with reinforcement learning from AI feedback (RLAIF), and rewrite them
    to be compliant.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预训练完成后，应采用从人类反馈中学习（RLHF）或其他微调方法，将控制不应生成哪些类型内容的策略深深植入模型。尽管具体方法可能不同，但这通常涉及收集显示对用户输入的适当和不适当响应的数据，然后生成新的响应并对它们进行标记，其中标记员接受特定内容策略集合的培训。随着时间的推移，我们预计微调将越来越少依赖于人类标记员和偏好。随着模型接近并超越人类水平的标记性能，我们越来越能够使用模型来捕捉这些偏好，甚至可以像使用从AI反馈中学习（RLAIF）一样对生成内容进行批评，并将它们重写以符合规范。
- en: 'Finally, an option that LLM developers may pursue is post hoc detection, where
    a safety classifier is deployed within the generation pipeline as a final hurdle
    before an unsafe response is sent to the user. This will increase the latency
    of the model and might mean a less “helpful” model if there are false positives
    from the classifier that cause safe responses to be overwritten. For example,
    a response about a sensitive topic (“How can I last longer in bed?”) might get
    flagged by a safety classifier accidentally even if it was both helpful to the
    user and not technically against the content policies. Beyond post-processing,
    we recommend that all LLM developers monitor the safety of the responses sent
    by their model. An asynchronous safety classifier could help to identify any major
    shifts in the distribution of messages generated by the LLM, as could sampling
    the messages to look for content violations. Each of these can be done in a way
    that preserves the privacy of the users: both the generative models and classifiers
    could be trained and fine-tuned on anonymized data, preventing the association
    of unsafe material with any particular user.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LLM开发者可能追求的一个选项是事后检测，即在将不安全响应发送给用户之前，在生成管道中部署一个安全分类器作为最后的障碍。这将增加模型的延迟，如果分类器有误报，导致安全响应被覆盖，可能会意味着一个不那么“有用”的模型。例如，关于敏感话题的回应（“我如何在床上保持更长时间？”）可能会被安全分类器意外标记，即使它对用户有帮助，并且从技术上讲并未违反内容政策。除了后处理之外，我们建议所有LLM开发者监控其模型发送的响应的安全性。异步安全分类器可以帮助识别LLM生成的消息分布中的任何重大变化，同样，通过采样消息以寻找内容违规也可以做到。所有这些都可以以保护用户隐私的方式进行：生成模型和分类器都可以在匿名数据上训练和微调，防止将不安全内容与任何特定用户关联起来。
- en: 'Despite the safety mitigations put in place by LLM providers, many of these
    models have also been shown to be vulnerable to adversarial attacks that can alter
    the model’s behavior. Sometimes referred to as “jailbreaking” or “prompt jailbreaking,”
    these attacks reflect the difficulty of creating a safe model that is robust to
    unseen contexts and unusual inputs (see [https://llm-attacks.org/](https://llm-attacks.org/)
    and chapter 5 for examples). Although it’s typically straightforward to patch
    a model against a specific attack through the collection and labeling of a small
    amount of additional data, it’s not at all clear that such behaviors could ever
    be fully resolved. The authors of a paper on adversarial attacks put it this way:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM提供商实施了安全缓解措施，但这些模型中的许多也已被证明容易受到可以改变模型行为的对抗性攻击。有时被称为“越狱”或“提示越狱”，这些攻击反映了创建一个对未见情境和异常输入具有鲁棒性的安全模型的难度（参见[https://llm-attacks.org/](https://llm-attacks.org/)和第5章中的示例）。尽管通常通过收集和标记少量额外数据来修补模型以抵御特定攻击是直接的，但并不清楚这些行为是否能够得到完全解决。一篇关于对抗性攻击的论文的作者这样说道：
- en: Analogous adversarial attacks have proven to be a very difficult problem to
    address in computer vision for the past 10 years. It is possible that the very
    nature of deep learning models makes such threats inevitable. Thus, we believe
    that these considerations should be taken into account as we increase usage and
    reliance on such AI models. ([https://llm-attacks.org/](https://llm-attacks.org/))
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的对抗性攻击在过去10年中已被证明是计算机视觉领域的一个非常难以解决的问题。深度学习模型的本质可能使得这种威胁变得不可避免。因此，我们认为，随着我们增加对这些AI模型的使用和依赖，我们应该考虑这些因素。([https://llm-attacks.org/](https://llm-attacks.org/))
- en: Rather than giving up on these threats as inevitable, though, LLM developers
    concerned with safety can and should endeavor to make such attacks harder to find
    and easier to fix.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将这些威胁视为不可避免而放弃，然而，关注安全的LLM（大型语言模型）开发者可以并且应该努力使这些攻击更难被发现和修复。
- en: We know that LLMs can generate misinformation, hate speech, discriminatory stereotypes,
    personal information, and other undesirable outputs. For some malicious users,
    this is a feature, not a bug; we discuss in chapter 5 how LLMs can be misused
    for a variety of nefarious purposes. The existence of malicious users motivates
    the implementation of safety mechanisms, but if these techniques are executed
    well, the general public using LLMs personally and professionally should be unaffected
    by them. Helpfulness and harmlessness *are* in tension (the safest possible model
    is the one that never generates anything), but with the proper prioritization,
    a highly capable model can also be extremely safe.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道LLM可以生成虚假信息、仇恨言论、歧视性刻板印象、个人信息和其他不希望的结果。对于一些恶意用户来说，这与其说是缺陷，不如说是功能；我们在第5章讨论了LLM如何被滥用于各种恶意的目的。恶意用户的存在促使实施安全机制，但如果这些技术执行得当，个人和专业使用LLM的公众应该不会受到影响。有用性和无害性*是存在冲突的*（最安全的模型是永远不会生成任何内容的模型），但通过适当的优先级排序，一个高度能干的模型也可以非常安全。
- en: Enhanced detection
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强检测
- en: Synthetic media generated by AI, including text, images, audio recordings, and
    videos, have the potential to severely disrupt our information ecosystem. As we’ve
    noted, generative AI can be abused to create deepfakes and produce misinformation
    or propaganda on a massive scale.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由AI生成的合成媒体，包括文本、图像、音频录音和视频，有可能严重破坏我们的信息生态系统。正如我们所指出的，生成式AI可能被滥用来创建深度伪造，并大规模产生虚假信息或宣传。
- en: 'Detecting LLM-written text has proven to be a more difficult task for models
    to learn than generating the text itself. According to a 2023 article about OpenAI’s
    classifier detection tool:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 检测由LLM编写的文本已被证明比生成文本本身对模型来说是一个更困难的任务。根据一篇关于OpenAI分类检测工具的2023年文章：
- en: In January, artificial intelligence powerhouse OpenAI announced a tool that
    could save the world—or at least preserve the sanity of professors and teachers—by
    detecting whether a piece of content had been created using generative AI tools
    like its own ChatGPT. Half a year later, that tool is dead, killed because it
    couldn’t do what it was designed to do. [[25]](https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一月，人工智能巨头OpenAI宣布了一款工具，该工具可以拯救世界——至少可以保护教授和教师的理智——通过检测内容是否使用像其自己的ChatGPT这样的生成式AI工具创建。半年后，这款工具已经死亡，因为它无法完成其设计的目的。[25](https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool)
- en: Since its inception, the tool had shown low accuracy in detecting machine-generated
    content, but at the time, OpenAI expressed hope that it would still be useful
    as a starting point. As LLMs have only become more advanced during that period,
    it already appears impossible to distinguish text from LLMs post hoc. The synthetic
    media created by image-, audio-, and video-generation models remains in some cases
    detectable, via the methods discussed in chapter 4, but even in those domains,
    the gaps are closing quickly.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 自从该工具推出以来，它在检测机器生成内容方面的准确性一直很低，但当时，OpenAI表示希望它仍然可以作为起点有用。由于LLM在此期间变得更加先进，现在似乎已经无法区分文本是否由LLM生成。通过第4章中讨论的方法，在某些情况下，由图像、音频和视频生成模型创建的合成媒体仍然可检测，但即使在那些领域，差距也在迅速缩小。
- en: One active area of research is how to embed a proof of machine generation within
    the synthetic media so that viewers can determine the origin of that content.
    In chapter 6, we introduced the concept of watermarking the output of LLMs, which
    would make that output statistically distinguishable from standard, human-written
    text.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 研究的一个活跃领域是如何在合成媒体中嵌入机器生成的证明，以便观众能够确定该内容的来源。在第6章中，我们介绍了对LLM输出进行水印的概念，这将使该输出在统计上与标准的人写文本可区分。
- en: 'Unfortunately, watermarking for machine-generated text is unlikely to ever
    be a perfect solution. To be effective, any watermarking solution would need to
    be adopted across the industry and made available to the public to check pieces
    of content. But if such a solution were made available to the public to verify
    messages, it could also be used by people to repeatedly check their own machine
    generations and alter them slightly—perhaps changing a few words at a time—until
    the message passes the watermark test. Besides this shortcoming, companies might
    be unwilling to adopt watermarking in the first place: the models produce text
    by predicting the next most likely word, but watermarking overrides these probabilities,
    preferencing certain words above others. Therefore, producing text with a watermark
    might also mean that the LLM is less factual or generates lower-quality responses.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，为机器生成文本的水印可能永远都不会是一个完美的解决方案。为了有效，任何水印解决方案都需要在整个行业中被采用，并公开提供给公众以检查内容。但如果这样的解决方案公开提供给公众以验证消息，它也可能被人们用来反复检查他们自己的机器生成内容并稍作修改——也许一次改变几个词——直到消息通过水印测试。除了这个缺点之外，公司可能从一开始就不愿意采用水印：模型通过预测下一个最有可能的词来生成文本，但水印会覆盖这些概率，优先考虑某些词而高于其他词。因此，带有水印的文本生成可能也意味着LLM的事实性较低或生成的响应质量较低。
- en: 'Other limitations apply to the watermarking of synthetic images, videos, and
    other types of media. DALL E, OpenAI’s text-to-image model, uses a visible watermark,
    but there are countless tutorial blog posts instructing users on how to remove
    it from images that they create with the tool. Sam Gregory, a program director
    at the nonprofit Witness, told *Wired* magazine that “There’s going to be ways
    in which you can corrupt the watermark,” pointing out that some visual watermarks
    become ineffective when the image is merely resized or cropped. Another concern
    with visual watermarks is that malicious actors could imitate them, placing the
    logos on real content to make it seem fake. The liar’s dividend is alive and well:
    Gregory said that most cases Witness sees on social media aren’t deepfakes, but
    real videos that people are claiming are generated by AI [[26]](https://www.wired.com/story/ai-watermarking-misinformation/).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其他限制也适用于合成图像、视频和其他类型媒体的水印。DALL E，OpenAI的文本到图像模型，使用可见水印，但有许多教程博客文章指导用户如何从他们使用该工具创建的图像中移除它。非营利组织Witness的项目总监Sam
    Gregory告诉《Wired》杂志，“将会有一些方法可以破坏水印，”他指出，一些视觉水印在图像仅被调整大小或裁剪时就会变得无效。对视觉水印的另一个担忧是，恶意行为者可能会模仿它们，将标志放在真实内容上，使其看起来像是伪造的。骗子的红利仍然存在：Gregory说，Witness在社交媒体上看到的大多数案例都不是深度伪造，而是人们声称是由AI生成的真实视频
    [[26](https://www.wired.com/story/ai-watermarking-misinformation/)]。
- en: The Coalition for Content Provenance and Authenticity (C2PA), introduced in
    chapter 5, aims to establish “an open technical standard providing publishers,
    creators, and consumers the ability to trace the origin of different types of
    media” (see [https://c2pa.org/](https://c2pa.org/)). The C2PA implementation records
    the provenance information, such as the date, geographic location, and device
    used to take a photo or video recording, as well as the information associated
    with any subsequent edits. This information is protected via a digital signature,
    a cryptographic technique used in online contracts and other secure transactions.
    Widespread use of the C2PA standard would allow viewers to inspect the origin
    and records associated with any piece of media they encountered online, but adoption
    remains a hurdle. Still, it would be technically possible to apply the same process
    to synthetic images as well, provided that generative AI developers integrated
    the cryptographic techniques into their systems. As with other safety mitigations,
    many of the largest AI developers will no doubt incorporate watermarks in the
    synthetic media generated by their models—seven companies, including OpenAI, Google,
    Microsoft, and Anthropic, have already committed to doing so—but these methods
    won’t decisively determine the provenance for all content.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中介绍的“内容来源和真实性联盟”（C2PA）旨在建立一个“开放的技术标准，为出版商、创作者和消费者提供追踪不同类型媒体来源的能力”（见[https://c2pa.org/](https://c2pa.org/))）。C2PA的实施记录了来源信息，如日期、地理位置以及用于拍摄照片或视频记录的设备，以及与任何后续编辑相关的信息。这些信息通过数字签名得到保护，这是一种在线合同和其他安全交易中使用的加密技术。C2PA标准的广泛应用将允许观众检查他们在网上遇到的任何媒体作品的来源和记录，但采用率仍然是一个障碍。尽管如此，如果生成式AI开发者将加密技术集成到他们的系统中，从技术上讲，将同样的过程应用于合成图像也是可能的。与其他安全缓解措施一样，许多最大的AI开发者无疑将在他们模型生成的合成媒体中融入水印——包括OpenAI、谷歌、微软和Anthropic在内的七家公司已经承诺这样做——但这些方法不会决定性地确定所有内容的来源。
- en: Boundaries for user engagement and metrics
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户参与度和指标界限
- en: 'In a 2018 paper published by researchers at Microsoft entitled, “From Eliza
    to Xiaolce: Challenges and Opportunities with Social Chatbots,” the authors trace
    the development of social chatbots through the present day. They write:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，微软的研究人员发表的一篇论文《从Eliza到小冰：社交聊天机器人的挑战与机遇》中，作者们追溯了社交聊天机器人至今的发展历程。他们写道：
- en: Conversational systems have come a long way since their inception in the 1960s…
    To further the advancement and adoption of social chatbots, their design must
    focus on user engagement and take both intellectual quotient (IQ) and emotional
    quotient (EQ) into account. Users should want to engage with a social chatbot;
    as such, we define the success metric for social chatbots as conversation-turns
    per session (CPS). [[27]](https://arxiv.org/pdf/1801.01957.pdf)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统自20世纪60年代问世以来已经取得了长足的进步……为了进一步推动社交聊天机器人的发展和普及，其设计必须关注用户参与度，并考虑智力商数（IQ）和情商（EQ）。用户应该愿意与社交聊天机器人互动；因此，我们将社交聊天机器人的成功指标定义为每场对话的轮次（CPS）。[[27]](https://arxiv.org/pdf/1801.01957.pdf)
- en: Lest we forget, the creator of ELIZA, Joseph Weizenbaum, intended the tool as
    a therapeutic aid and was dismayed to realize the extent to which people anthropomorphized
    it. One tends to think that Weizenbaum would not have viewed CPS as the measure
    of its success. The fact that CPS is defined to be the metric *du jour* of social
    chatbots illustrates a profound failure of the imagination.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该忘记，ELIZA的创造者约瑟夫·魏岑鲍姆（Joseph Weizenbaum）原本希望这个工具作为一种治疗辅助工具，但遗憾的是，他意识到人们将这个工具拟人化的程度。人们往往会认为魏岑鲍姆不会将CPS视为其成功的衡量标准。CPS被定义为社交聊天机器人的“当红炸子鸡”指标，这反映了想象的深刻失败。
- en: 'Social chatbots, including Xiaolce, Replika, and Character.AI, have millions
    of users who seek out conversations with the bots for the companionship, romance,
    or entertainment that they provide. It’s certainly true that these agents must
    combine IQ and EQ: if the agent was heavily indexed toward IQ but not EQ, people
    would be able to ask it factual questions or for coding assistance, for example,
    but would be unlikely to develop a deeper relationship with it. If the agent didn’t
    possess enough IQ, it wouldn’t be able to hold an interesting conversation at
    all. Beyond a base level of functionality, though, it’s primarily EQ that gives
    social chatbots the capabilities their users value most: the responses that make
    them feel less lonely, the practice of small talk to alleviate social anxiety,
    or simply an outlet to vent.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 社交聊天机器人，包括小冰、Replika和Character.AI，拥有数百万用户，他们寻求与机器人进行对话，以获得陪伴、浪漫或娱乐。当然，这些代理必须结合智商和情商：如果代理在智商上高度依赖但缺乏情商，人们可以问它事实问题或寻求编码帮助，例如，但不太可能与之建立更深入的关系。如果代理智商不足，它甚至无法进行有趣的对话。然而，在基本功能水平之上，主要是情商赋予了社交聊天机器人用户最看重的功能：那些让他们感到不那么孤独的回应，缓解社交焦虑的小对话练习，或者简单地提供一个发泄的出口。
- en: It’s in these interactions that social chatbots are most valuable, so it’s these
    interactions that should be understood and improved. In chapter 7, we recommend
    alternative metrics that chatbot providers could use to measure success, such
    as defining valuable sessions instead of simply using session length as an indicator.
    This requires additional work, but it can circumvent the shortcomings of purely
    engagement-based metrics and provide insights into how people are using the chatbots,
    which is crucial for their developers to know to ensure responsible deployment
    of the technology.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这些互动中，社交聊天机器人最有价值，因此应该理解和改进这些互动。在第7章中，我们建议聊天机器人提供商可以使用的一些替代指标来衡量成功，例如，定义有价值的会话，而不是简单地使用会话长度作为指标。这需要额外的工作，但可以避免仅基于参与度的指标的不足，并深入了解人们如何使用聊天机器人，这对于确保技术的负责任部署对于开发者来说是至关重要的。
- en: In that vein, chatbot providers should also endeavor to recognize when usage
    is unhealthy to prevent people from forming dependency relationships with the
    models. As the stories in chapter 7 show, these tools can improve people’s moods
    and confidence, and reduce anxiety and loneliness. But there is much we still
    don’t know about human-AI connections, and if these relationships replace interpersonal
    connections on a long-term basis, there are reasons to believe it could have substantial
    negative effects on emotional development. Again, to avoid building dependency
    in users necessitates optimizing metrics other than engagement, and means more
    work for developers. Ultimately, we believe this effort is worth it for the social
    benefit and to sustain user trust.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方面，聊天机器人提供商也应该努力识别何时使用是不健康的，以防止人们与模型形成依赖关系。正如第7章中的故事所显示的，这些工具可以提高人们的情绪和自信，减少焦虑和孤独感。但我们仍然对人类-人工智能的连接知之甚少，如果这些关系长期取代人际交往，有理由相信它可能会对情感发展产生重大的负面影响。再次强调，为了避免用户产生依赖，需要优化除了参与度以外的指标，这意味着开发者需要做更多的工作。最终，我们认为这项努力对于社会效益和维持用户信任是值得的。
- en: Humans in the loop
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类在回路中
- en: Humans remain an integral part of building and maintaining AI systems. Consider
    how many different people were involved in the creation of ChatGPT. There were,
    of course, the OpenAI engineers in San Francisco. Likely, there were many more
    contractors who selected good responses to help train the chatbot; there might
    have been specialists brought in to red-team certain topics. We know that there
    were Kenyan data labelers paid $1 to $2 an hour to review hate speech and sexual
    abuse content. There were the authors of the millions of words that ChatGPT was
    trained on, from Shakespeare to anonymous Redditors, and the people whose labor
    allowed ChatGPT to learn to write news articles, emails, speeches, and code. Maybe
    something you wrote is in there! And the users of ChatGPT, like other LLMs, also
    play a key role in improving the product over time.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在构建和维护AI系统中仍然是一个不可或缺的部分。以ChatGPT的创建为例，涉及了众多不同的人。当然，有位于旧金山的OpenAI工程师。可能还有许多承包商被选中来提供有助于训练聊天机器人的良好回应；可能还有被聘请来针对某些主题进行红队测试的专家。我们知道，有肯尼亚的数据标注员每小时获得1到2美元的报酬，以审查仇恨言论和性虐待内容。还有撰写了数百万字内容的人，从莎士比亚到匿名Reddit用户，以及那些劳动使得ChatGPT能够学习撰写新闻文章、电子邮件、演讲和代码的人。也许你写的东西就在其中！ChatGPT的用户，像其他大型语言模型（LLM）一样，在随着时间的推移改进产品中也扮演着关键角色。
- en: To the extent that LLMs have expertise, it’s human expertise. What the technology
    provides is a way of representing information from more documents than any person
    could ever read, much less organize in their mind, and using that information
    to generate text (usually responses to inputs) at a scale no person ever could.
    What the technology doesn’t provide is meaning; the model doesn’t *know*. That
    is typically acceptable for producing a song about a rabbit who loves carrots,
    but it isn’t acceptable in high-stakes applications ranging from medical diagnoses
    to legal argumentation. As we talked about in chapter 6, these types of applications
    still need a human in the loop to identify the model’s mistakes. LLMs are tools
    that we can use to do parts of our jobs more quickly and easily, and maybe sometimes
    even better, but we still need to build expertise to correct and improve these
    models.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM具有专业知识的情况下，这种专业知识是人类的专业知识。技术提供的是一种方式，可以从比任何个人都能阅读的更多文档中提取信息，更不用说在心中组织这些信息了，并使用这些信息以任何个人都无法达到的规模生成文本（通常是针对输入的回应）。技术不提供的是意义；模型并不*知道*。对于创作关于喜欢胡萝卜的兔子的歌曲这类应用来说，这通常是可接受的，但在从医疗诊断到法律论证等高风险应用中则不可接受。正如我们在第6章中讨论的，这些类型的应用仍然需要人类在循环中识别模型的错误。LLM是我们可以用来自动完成我们工作的一部分的工具，也许有时甚至能做得更好，但我们仍然需要构建专业知识来纠正和改进这些模型。
- en: As we navigate the shifting roles of ourselves and AI in education and professional
    fields, thorny questions will inevitably arise. Our collective ability to answer
    them will depend on a sociotechnical response, rather than technology alone. In
    privacy, for example, there is a tremendous amount of technical progress being
    made, such as new start-ups that use generative models to create synthetic datasets
    with the same statistical properties as real datasets. Illumina, a genetic sequencing
    company, announced a partnership with the synthetic data start-up Gretel.ai to
    create synthetic genome data that could be extremely useful in healthcare, without
    divulging any individual’s genetic information. But there is momentum behind these
    efforts because of the social aspect of privacy—activism around the problem, increased
    public awareness, confronting and rejecting social norms of mass data collection,
    and finally a stricter regulatory environment. This must continue with responsible
    AI and related movements.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们自己在教育和专业领域内以及人工智能角色中的转变，不可避免地会涌现出棘手的问题。我们共同回答这些问题的能力将取决于社会技术响应，而不仅仅是技术本身。例如，在隐私方面，正在取得巨大的技术进步，比如新成立的公司使用生成模型来创建具有与真实数据集相同统计特性的合成数据集。Illumina，一家基因测序公司，宣布与合成数据初创公司Gretel.ai合作，创建可用于医疗保健的合成基因组数据，而无需泄露任何个人的基因信息。但由于隐私的社会属性——围绕该问题的活动、公众意识的提高、对抗和拒绝大规模数据收集的社会规范，以及最终更加严格的监管环境——这些努力正在获得动力。这必须与负责任的AI和相关运动继续进行。
- en: Making positive change that encourages the responsible use of technology also
    requires that people are at least generally aware of how these technical systems
    work and how they are presently used. Digital literacy is a group effort. Companies
    that provide solutions powered by LLMs must not try to sell users magic, but work
    to educate them on the capabilities and limitations of the models. Schools should
    aim to prepare their students for the world of today, rather than ignoring or
    punishing the use of modern technologies, including LLMs. This book is our hopeful
    contribution toward a populace that is informed and considerate about generative
    AI.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 制造积极变化，鼓励负责任地使用技术，这也要求人们至少对如何这些技术系统工作以及它们目前的使用方式有一般性的了解。数字素养是集体努力的结果。由大型语言模型（LLM）提供解决方案的公司不应试图向用户推销魔法，而应努力教育他们关于模型的能力和限制。学校应致力于为他们的学生准备适应今天的世界，而不是忽视或惩罚现代技术的使用，包括LLM。这本书是我们对一个了解并关心生成式人工智能的民众的希望贡献。
- en: 'AI regulations: An ethics perspective'
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能法规：伦理视角
- en: Although the best practices discussed in section Best Practices for Responsible
    Use are vital, they aren’t enough. We also need balanced guidance from the government,
    informed by industry, academia, and civil society, and methods to enforce accountability.
    Government entities around the world are increasingly recognizing the need for
    guidelines and frameworks that govern the development, deployment, and use of
    AI systems. The ultimate goal of regulations is to strike the perfect balance
    between promoting innovation and ensuring the development of responsible and ethical
    AI systems. These regulations often aim to address shared concerns about data
    privacy, algorithmic transparency, bias mitigation, and accountability. In this
    section, we’ll talk about the AI regulatory landscape in North America, the European
    Union, and China, as well as discuss corporate self-governance. We focus on these
    regions due to the concentration of big technology companies in the United States
    and China and their preeminent roles in global AI development, while the European
    Union is the world’s leading tech regulator.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在“负责任使用最佳实践”部分讨论的最佳实践至关重要，但它们还不够。我们还需要来自政府、行业、学术界和民间社会的平衡指导，以及执行问责制的方法。世界各地的政府实体越来越认识到制定规范和框架的必要性，这些框架将规范人工智能系统的开发、部署和使用。法规的最终目标是实现促进创新和确保负责任和道德的人工智能系统发展的完美平衡。这些法规通常旨在解决关于数据隐私、算法透明度、偏见缓解和问责制的共同担忧。在本节中，我们将讨论北美、欧盟和中国的人工智能监管格局，以及讨论企业自我治理。我们关注这些地区，因为美国和中国的大科技公司集中，它们在全球人工智能发展中扮演着主导角色，而欧盟是世界上领先的科技监管机构。
- en: North America overview
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 北美概述
- en: In the United States and Canada, the predominant approach at the federal level
    has been to establish best practices at the agency level and sometimes in collaboration
    with leading tech companies and civil society groups. The latter approach is exemplified
    by the July 2023 announcement from the Biden administration that it had secured
    commitments from seven AI companies—OpenAI, Microsoft, Google, Amazon, Meta, Anthropic,
    and Inflection—to comply with a set of voluntary principles. The principles, depicted
    in figure 8.5, include “ensuring products are safe before introducing them to
    the public” through internal and external testing for safety and information-sharing
    on risk management; “building systems that put security first” with appropriate
    cybersecurity and insider threat safeguards and vulnerability reporting; and “earning
    the public’s trust,” a broad category that references efforts to develop watermarking
    systems and public reporting on the capabilities and limitations of publicly released
    AI systems [[28]](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国和加拿大，联邦层面的主要做法是在机构层面建立最佳实践，有时与领先的技术公司和民间社会团体合作。后一种方法以2023年7月拜登政府宣布的承诺为例，它已从七家人工智能公司——OpenAI、微软、谷歌、亚马逊、Meta、Anthropic和Inflection——那里获得了遵守一系列自愿原则的承诺。这些原则如图8.5所示，包括“在向公众推出产品之前确保产品安全”，通过内部和外部测试安全性和风险管理的信息共享；“构建以安全为首要任务的系统”，采用适当的网络安全和内部威胁防护以及漏洞报告；以及“赢得公众的信任”，这是一个广泛的类别，涉及开发水印系统和对公开发布的人工智能系统能力和限制的公开报告。[28](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/).
- en: 'Because the commitments are voluntary, some critics argued that the announcement
    produced more of a halo effect for the companies rather than meaningful change.
    Kevin Roose, a technology reporter at the *New York Times*, reviewed each principle
    in the press release to assess how significant the commitments were. Roose’s primary
    critique was that the listed principles are vague and don’t specify what kind
    of testing and reporting must be done, leaving lots of wiggle room. He concluded:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些承诺是自愿的，一些批评者认为，这一公告更多地产生了公司的光环效应，而不是有意义的变化。纽约时报的一名技术记者凯文·鲁斯对新闻稿中的每一项原则进行了审查，以评估承诺的重要性。鲁斯的主要批评是，列出的原则过于模糊，没有具体说明必须进行何种测试和报告，留下了很多回旋余地。他总结道：
- en: Overall, the White House’s deal with AI companies is more symbolic than substantive.
    There is no enforcement mechanism to make sure companies follow these commitments,
    and many of them reflect precautions that AI companies are already taking. Still,
    it’s a reasonable first step. And agreeing to follow these rules shows that the
    AI companies have learned from the failure of earlier tech companies, which waited
    to engage with government until they got into trouble. [[29]](https://www.nytimes.com/2023/07/22/technology/ai-regulation-white-house.xhtml)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，白宫与人工智能公司的交易在象征意义上大于实质意义。没有执行机制来确保公司遵守这些承诺，而且其中许多承诺反映了人工智能公司已经采取的预防措施。尽管如此，这仍是一个合理的第一步。同意遵守这些规则表明，人工智能公司已经从早期科技公司失败的教训中吸取了经验，这些科技公司等到陷入困境后才与政府接触。[29](https://www.nytimes.com/2023/07/22/technology/ai-regulation-white-house.xhtml)
- en: '![](../../OEBPS/Images/CH08_F05_Dhamani.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5](../../OEBPS/Images/CH08_F05_Dhamani.png)'
- en: Figure 8.5 The three pillars of voluntary commitments made to the White House
    by leading AI companies
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 领先人工智能公司向白宫做出的自愿承诺的三个支柱
- en: Indeed, some of the commitments appear to be directly motivated by events that
    have transpired already. The second pillar, building systems that put security
    first, specifically calls out the protection of “proprietary and unreleased model
    weights.” As described in chapter 1, the weights of an LLM are the end product
    of its training. Access to model weights effectively enables the reproduction
    of the model itself. After the model weights of Meta’s LLaMA were leaked on 4chan
    days after the public release, 4chan users were able to quickly produce a high-quality
    LLM based on LLaMA. The memo doesn’t otherwise mention the open source development
    of LLMs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，一些承诺似乎直接受到已经发生的事件的推动。第二个支柱，即构建以安全为首要任务的系统，特别指出要保护“专有和未发布模型权重”。如第1章所述，LLM的权重是其训练的最终产品。访问模型权重实际上能够复制模型本身。在Meta的LLaMA模型权重在公开发布后不久在4chan上泄露后，4chan用户能够迅速基于LLaMA生产出一个高质量的LLM。备忘录没有提及LLM的开源开发。
- en: These particular principles are mostly focused on generative AI products, but
    other government bodies have long concerned themselves with the potential negative
    effects of earlier AI systems, particularly those related to bias and transparency.
    For example, the Equal Employment Opportunity Commission (EEOC) has issued guidance
    on how the Civil Rights Act of 1964 applies to automated, AI-based systems used
    in HR functions such as résumé screening, candidate selection, and performance
    monitoring. Essentially, the office stated that the burden of compliance would
    fall on employers who use these tools, with recommendations to verify that vendors
    had evaluated whether their models cause a “substantially lower selection rate
    for individuals with a characteristic protected by Title VII,” such as individuals
    of a particular race or gender [[30]](https://www.eeoc.gov/newsroom/eeoc-releases-new-resource-artificial-intelligence-and-title-vii).
    The Federal Trade Commission (FTC) has also demonstrated an appetite for oversight
    of automated decision-making, writing in a 2021 blog post that the FTC Act, which
    prohibits “unfair or deceptive practices,” would explicitly include the sale or
    use of racially biased algorithms. In addition to models used in employment decisions,
    models related to housing, credit, and insurance decisions would potentially be
    subject to scrutiny under the Fair Credit Reporting Act (see [http://mng.bz/JgKQ](http://mng.bz/JgKQ)).
    The Government of Canada issued a Directive on Automated Decision-Making in 2019
    that included assessments of negative outcomes from automated decision-making
    systems (see [http://mng.bz/mVn0](http://mng.bz/mVn0)). Although generative AI
    models weren’t the target of these issuances, they would be similarly scrutinized
    if used in any of the aforementioned sectors.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特定的原则主要关注生成式AI产品，但其他政府机构长期以来一直关注早期AI系统的潜在负面影响，尤其是与偏见和透明度相关的问题。例如，平等就业机会委员会（EEOC）已发布指导，说明1964年民权法如何适用于人力资源职能中使用的自动化、基于AI的系统，如简历筛选、候选人选择和绩效监控。本质上，该机构表示，遵守的负担将落在使用这些工具的雇主身上，并建议核实供应商是否评估了他们的模型是否会导致“具有第七条款保护特征的个体选择率显著降低”，例如特定种族或性别的个体
    [[30]](https://www.eeoc.gov/newsroom/eeoc-releases-new-resource-artificial-intelligence-and-title-vii)。联邦贸易委员会（FTC）也表现出对自动化决策监督的兴趣，在2021年的一篇博客文章中写道，禁止“不公平或欺骗性做法”的FTC法案将明确包括销售或使用具有种族偏见算法的行为。除了用于就业决策的模型外，与住房、信贷和保险决策相关的模型也可能在公平信用报告法（见[http://mng.bz/JgKQ](http://mng.bz/JgKQ)）下受到审查。加拿大政府在2019年发布了关于自动化决策的指令，其中包括对自动化决策系统负面影响的评估（见[http://mng.bz/mVn0](http://mng.bz/mVn0)）。尽管生成式AI模型不是这些发布的目标，但如果用于上述任何领域，它们将受到类似的审查。
- en: In October 2023, the White House followed the voluntary commitments it had secured
    with an executive order on AI, designed to require AI companies to share safety
    evaluations and other information with the government and to take precautions
    to ensure that the models could not be used for engineering “dangerous biological
    materials” or enabling “fraud and deception.” (see [http://mng.bz/6nM5](http://mng.bz/6nM5)).
    The Biden administration has also published more abstract rules for the development
    of AI. Perhaps its landmark text on the subject is the Blueprint for an AI Bill
    of Rights, authored by the White House Office of Science and Technology Policy
    (OSTP) (see [http://mng.bz/wv8g](http://mng.bz/wv8g)). Summarized in figure 8.6,
    that document is centered around the five principles of “safe and effective systems,”
    outlining evaluation and risk mitigation standards; “algorithmic discrimination
    protections,” or identifying potential biases in the model or system; “data privacy,”
    the rights of users to have both information and agency concerning how their data
    is collected; “notice and explanation” about the use of automated systems; and
    “human alternatives, considerations, and fallback” for when people opt out of
    automated systems or to remedy any mistakes made by the system. Like the more
    recent set of AI principles, these are each relatively uncontroversial and vague
    enough to leave some uncertainty over what each might look like in practice. The
    AI Bill of Rights is a positioning document rather than a directive, and the OSTP
    is a policy office. The details of implementations of things such as explanations
    (“Automated systems should provide explanations that are technically valid, meaningful,
    and useful to you and to any operators or others who need to understand the system,
    and calibrated to the level of risk based on the context”) remain to be worked
    out. The closest the US government has come to attempting that is the National
    Institute for Standards and Technology’s AI Risk Management Framework (AI RMF),
    released on January 26, 2023, but even that framework is quite broad and general,
    intended as a starting point. The AI RMF details that AI systems should be “valid
    and reliable,” “safe,” “secure and resilient,” “accountable and transparent,”
    “explainable and interpretable,” and “fair—with harmful bias managed,” but leaves
    how this should be achieved mostly as an exercise for the reader [[31]](https://www.nist.gov/itl/ai-risk-management-framework).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年10月，白宫遵循了其与人工智能公司达成的自愿承诺，通过一项行政命令对人工智能进行了规范，要求人工智能公司向政府共享安全评估和其他信息，并采取措施确保模型不会被用于工程“危险生物材料”或启用“欺诈和欺骗”。（见[http://mng.bz/6nM5](http://mng.bz/6nM5)）。拜登政府还发布了更多关于人工智能发展的抽象规则。或许其在该领域的里程碑文本是白宫科学技术政策办公室（OSTP）撰写的《人工智能权利法案蓝图》（见[http://mng.bz/wv8g](http://mng.bz/wv8g)）。如图8.6所示，该文件围绕“安全有效系统”的五个原则展开，概述了评估和风险缓解标准；“算法歧视保护”，或识别模型或系统中的潜在偏见；“数据隐私”，用户有权对其数据的收集拥有信息和行动权；“关于自动系统使用的通知和解释”；以及当人们选择退出自动系统或纠正系统错误时的“人类替代方案、考虑和后备”。与最近发布的AI原则集类似，这些原则相对无争议且足够模糊，足以在实践中留下一些不确定性。人工智能权利法案是一份定位文件，而不是指令，OSTP是一个政策办公室。关于解释（“自动系统应提供技术上有效、有意义且对您和任何需要理解系统的操作员或其他人员有用的解释，并根据风险水平进行校准”）等事项的具体实施细节仍有待商榷。美国政府在尝试实现这一点方面最接近的是国家标准与技术研究院（NIST）于2023年1月26日发布的AI风险管理框架（AI
    RMF），但即使这个框架也非常广泛和一般，旨在作为起点。AI RMF详细说明了人工智能系统应该是“有效可靠的”、“安全的”、“安全且具有弹性的”、“负责任和透明的”、“可解释和可理解的”，以及“公平的——有害偏见得到管理”，但将如何实现这一点主要留给了读者去思考
    [[31]](https://www.nist.gov/itl/ai-risk-management-framework)。
- en: '![](../../OEBPS/Images/CH08_F06_Dhamani.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH08_F06_Dhamani.png)'
- en: Figure 8.6 The five principles listed by the OSTP as a “Blueprint for an AI
    Bill of Rights” (see [http://mng.bz/wv8g](http://mng.bz/wv8g))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 OSTP列出的“人工智能权利法案蓝图”中的五个原则（见[http://mng.bz/wv8g](http://mng.bz/wv8g)）
- en: 'In the past, policymakers have expressed an ambivalence toward regulating AI
    companies. On one hand, representatives such as Mike Gallagher, a Republican congressman
    from Wisconsin, have hoped to avoid stifling the innovation that these tech companies
    bring. “The tension underlying all of this is that we don’t want to overregulate
    our advantage in the AI race out of existence,” said Gallagher, advocating for
    a “clinical, targeted” approach, rather than something more comprehensive. “Congress
    rarely does comprehensive well” [[32]](https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/).
    On the other hand, as evidenced in the Judiciary Committee hearing from chapter
    5, more than a few members are concerned that the present state of self-regulation
    will be insufficient, and some have expressed openness to comprehensive AI legislation.
    Representative Ro Khanna of California said:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，政策制定者对监管AI公司表现出矛盾的态度。一方面，像威斯康星州的共和党众议员迈克·加拉格尔这样的代表希望避免扼杀这些科技公司带来的创新。“所有这一切背后的紧张关系在于，我们不想因为过度监管而使我们在AI竞赛中的优势消失，”加拉格尔倡导一种“临床、有针对性的”方法，而不是更全面的方法。“国会很少能全面做好”
    [[32]](https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/)。另一方面，正如第5章中的司法委员会听证会所证明的，有相当多的成员担心目前自我监管的状态将不足以应对，有些人表示愿意接受全面的AI立法。加利福尼亚州的代表罗·科赫纳说：
- en: On a broad scale, we need some form of human judgment in decision-making. We
    need some sense of transparency when it comes to understanding what AI is being
    used for and the data sets that are being used. We need to have a safety assessment. . . . But
    I think the details of this really need to be worked out by people with deep knowledge
    of the issues. [[32]](https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛层面上，我们在决策过程中需要某种形式的人类判断。在理解AI被用于何处以及使用的数据集时，我们需要一定的透明度。我们需要进行安全评估……。但我认为，这一问题的细节需要由对这些问题有深入了解的人来制定。[[32]](https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/)
- en: There may be bipartisan support for some of the governance measures suggested
    by the principles in the AI Bill of Rights and a more recent set of commitments,
    though the prospect of passing federal legislation in the United States is far
    from certain. LLM developers recognize that their biggest regulatory threat is
    across the Atlantic Ocean, in the European Parliament.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在美国通过联邦立法的前景远非确定，但可能存在对AI权利法案原则中提出的某些治理措施的双党支持，以及更近期的承诺。LLM开发者认识到，他们最大的监管威胁来自大西洋彼岸，即欧洲议会。
- en: EU overview
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欧盟概述
- en: On June 14, 2023, the European Parliament overwhelmingly approved their version
    of the EU’s AI Act, setting the stage to pass the final version of the law on
    an expedited timeline by the end of the year [[33]](https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai).
    The AI Act would be one of the first major laws to regulate AI and serve as a
    potential model for policymakers around the world.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年6月14日，欧洲议会以压倒性多数通过了他们版本的欧盟AI法案，为在年底前通过该法律的最终版本设定了加速的时间表 [[33]](https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai)。AI法案将成为第一个主要监管AI的法律，并可能成为全球政策制定者的潜在范例。
- en: The AI Act implements a risk-based approach to AI regulation, focusing on AI
    applications that have the greatest potential for harm to society. In other words,
    the different risk levels will denote how much that technology is regulated and
    where high-risk AI systems will require more regulation. A limited set of AI systems
    that are deemed as *unacceptable risk* will be completely banned for violating
    fundamental human rights, which include cognitive behavioral manipulation of people
    of specific vulnerable groups, social scoring, and real-time and remote biometric
    identification systems (with major exceptions) [[34]](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence).
    For example, a voice-activated toy that encourages violent behavior in children
    would fall under this category and be banned.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: AI法案实施基于风险的AI监管方法，重点关注对社会危害潜力最大的AI应用。换句话说，不同的风险级别将表明该技术受到多少监管，以及高风险AI系统将需要更多的监管。一小部分被认为具有**不可接受风险**的AI系统将因侵犯基本人权而被完全禁止，这些人权包括对特定脆弱群体的人的认知行为操纵、社会评分和实时远程生物识别系统（有重大例外）
    [[34](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)]。例如，一个鼓励儿童暴力行为的语音激活玩具将属于这一类别并被禁止。
- en: One level below AI systems with unacceptable risk are *high-risk* AI systems,
    which negatively affect safety or fundamental rights (as protected by the EU Charter
    of Fundamental Rights). These include regulated consumer products and AI used
    for socioeconomic decisions, such as law enforcement, hiring, educational access,
    and financial services access, among others. All high-risk AI systems will not
    only be assessed before they go to market but also throughout their lifecycle.
    These systems would have to meet data governance, accuracy, and nondiscrimination
    standards. They would additionally need to implement a risk-management system,
    record-keeping, technical documentation, and human oversight. The AI systems would
    also need to be registered in an EU-wide database, which would not only create
    transparency within the number of high-risk AI systems but also regarding the
    extent of their societal effect [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在不可接受风险的AI系统之下的是**高风险**AI系统，这些系统会对安全或基本权利（如欧盟基本权利宪章所保护的权利）产生负面影响。这包括受监管的消费产品以及用于社会经济决策的AI，例如执法、招聘、教育获取和金融服务获取等。所有高风险AI系统在上市前都将进行评估，并在其整个生命周期内持续评估。这些系统必须满足数据治理、准确性和非歧视标准。它们还需要实施风险管理系统、记录保存、技术文档和人工监督。AI系统还需要在欧盟范围内的数据库中注册，这不仅会在高风险AI系统的数量上增加透明度，也会在其社会影响程度上增加透明度
    [[35](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/)]。
- en: Then, *limited risk* systems would have to comply with transparency requirements
    to help users make informed decisions. These requirements include making the user
    aware if they are interacting with AI, such as deepfakes, emotion recognition
    systems, or chatbots. The AI Act has an additional callout for *generative AI*,
    requiring transparency in disclosing AI-generated content, preventing the model
    from generating illegal content, and publishing summaries of copyrighted data
    used for training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**有限风险**系统必须遵守透明度要求，以帮助用户做出明智的决定。这些要求包括让用户意识到他们正在与AI互动，例如深度伪造、情绪识别系统或聊天机器人。AI法案对**生成式AI**有额外的要求，要求在披露AI生成内容时保持透明，防止模型生成非法内容，并发布用于训练的受版权保护数据的摘要。
- en: Finally, *minimal risk* includes AI applications such as video games or spam
    filters. These are proposed to be mainly regulated by voluntary codes of conduct.
    Figure 8.7 illustrates the AI Act’s risk levels. However, at the time of this
    writing, European policymakers haven’t decided where foundational LLMs fall within
    this framework, and this subject is currently being debated.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**最低风险**包括视频游戏或垃圾邮件过滤器等AI应用。这些应用主要被提议通过自愿行为准则进行监管。图8.7说明了AI法案的风险级别。然而，在撰写本文时，欧洲政策制定者尚未决定基础LLM在这个框架中的位置，并且这个话题目前正在被讨论。
- en: Of course, the AI Act isn’t the only major legislation in the EU to manage AI
    risk. In chapter 3, we briefly discussed the General Data Protection Regulation
    (GDPR), which requires companies to protect the personal data and privacy of EU
    citizens. The AI Act isn’t meant to replace GDPR, but complement it. In addition
    to data privacy implications, GDPR also contains two articles that affect machine
    learning systems. First, “GDPR states that algorithmic systems should not be allowed
    to make significant decisions that affect legal rights without any human supervision”
    [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
    An example of this was seen in 2021 when Uber, an American transportation company,
    was required to reinstate six drivers in the Netherlands who “were unfairly terminated
    by algorithmic means” [[36]](https://techcrunch.com/2021/04/14/uber-hit-with-default-robo-firing-ruling-after-another-eu-labor-rights-gdpr-challenge/).
    Second, “GDPR guarantees an individual’s right to *meaningful information about
    the logic* of algorithmic systems, at times controversially deemed a *right to
    explanation*” [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
    Put simply, EU consumers have the right to ask companies that make automated decisions
    based on their personal data, such as home insurance providers, how or why certain
    decisions were made.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，AI法案并不是欧盟管理AI风险的唯一主要立法。在第3章中，我们简要讨论了通用数据保护条例（GDPR），该条例要求公司保护欧盟公民的个人数据和隐私。AI法案的目的不是取代GDPR，而是补充它。除了数据隐私影响外，GDPR还包含两个影响机器学习系统的条款。首先，“GDPR规定，算法系统不应在没有人类监督的情况下做出影响法律权利的重大决策”
    [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/)。2021年，美国一家运输公司Uber因“不公平地通过算法手段解雇了六名司机”
    [[36]](https://techcrunch.com/2021/04/14/uber-hit-with-default-robo-firing-ruling-after-another-eu-labor-rights-gdpr-challenge/)
    而被要求在荷兰恢复这六名司机的职位。其次，“GDPR保证个人有权获得关于算法系统逻辑的*有意义的信息*，有时被认为是一种*解释权*” [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/)。简单来说，欧盟消费者有权要求那些基于他们的个人数据做出自动决策的公司，例如家庭保险提供商，解释或说明某些决策是如何或为什么做出的。
- en: '![](../../OEBPS/Images/CH08_F07_Dhamani.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 AI法案中的四个风险类别](../../OEBPS/Images/CH08_F07_Dhamani.png)'
- en: Figure 8.7 The four risk categories in the AI Act
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 AI法案中的四个风险类别
- en: As part of its efforts to regulate digital technologies, regulations that the
    EU has already passed include the Digital Services Act (DSA) and the Digital Market
    Act (DMA). Passed in November 2022, the DSA applies to online platforms and search
    engines, requiring companies to assess risks, outline mitigation efforts, and
    undergo third-party audits for compliance [[37]](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package).
    The most stringent regulations under the DSA only apply to very large online platforms
    (VLOPs), which focuses most of the regulation on platforms that have the most
    reach and influence on EU citizens. One of DSA’s goals is to force large platforms
    to be more transparent, particularly with algorithmic accountability and content
    moderation. These transparency requirements will help identify any systematic
    risks that come from the design and provision of services. For example, if an
    AI content recommendation system contributes to the spread of disinformation,
    the company may face fines under the DSA. The EU’s approach to targeting VLOPs
    is interesting because of its potential to undermine the innovation argument against
    regulation—which is, how will companies continue to innovate when faced with strict
    regulations? By targeting VLOPs, smaller businesses are free from the burden of
    complying with some parts of the DSA so they can still innovate, but if and when
    they become a large force in society, they will also be required to think about
    how they are using their data and how their platform is affecting their users.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 作为其监管数字技术的努力的一部分，欧盟已经通过的法规包括《数字服务法》（DSA）和《数字市场法》（DMA）。2022年11月通过的DSA适用于在线平台和搜索引擎，要求公司评估风险、概述缓解措施，并接受第三方审计以确保合规[[37]](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package)。DSA下最严格的法规仅适用于非常大的在线平台（VLOPs），这主要关注对欧盟公民影响最大和最广泛的平台。DSA的一个目标就是迫使大型平台更加透明，特别是在算法问责和内容监管方面。这些透明度要求有助于识别来自服务和设计中的任何系统性风险。例如，如果人工智能内容推荐系统导致虚假信息的传播，公司可能会根据DSA面临罚款。欧盟针对VLOPs的策略因其可能削弱反对监管的创新论点而引人注目——即，面对严格的法规，公司如何继续创新？通过针对VLOPs，小型企业可以免除遵守DSA某些部分的负担，从而仍然可以创新，但一旦他们成为社会中的强大力量，他们也将被迫思考他们如何使用数据以及他们的平台如何影响用户。
- en: Similarly, the DMA is aimed at increasing competition in digital marketplaces.
    The DMA targets “gatekeepers,” which are corporate groups that significantly affect
    the internal market, namely, big technology companies. Here, the gatekeepers will
    be subject to an additional level of regulation over other companies. For example,
    they will be restricted from sharing data across their services without user consent,
    barred from self-preferencing their own products and services, and obliged to
    share additional information with advertisers on how their ads perform [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
    The DMA will likely affect how the big technology players handle data, as well
    as how AI systems handle search engine ranking and ordering of products on e-commerce
    platforms. Despite not primarily focusing on AI, the DSA and DMA laws also help
    govern AI models and demand increased transparency from technology companies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，DMA旨在增加数字市场的竞争。DMA针对的是“守门人”，即对内部市场产生重大影响的集团公司，即大型科技公司。在这里，守门人将面临比其他公司更高的监管级别。例如，他们将被限制在未经用户同意的情况下在其服务之间共享数据，禁止自我优先考虑自己的产品和服务，并被迫向广告商提供额外信息，说明他们的广告表现[[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/)。DMA可能会影响大型科技公司处理数据的方式，以及AI系统如何处理搜索引擎排名和电子商务平台上产品的排序。尽管DMA不是主要针对AI，但DSA和DMA法律也有助于治理AI模型，并要求科技公司提高透明度。
- en: We’ve highlighted the European Union’s efforts to develop a coherent approach
    to AI governance and standards. In particular, the AI Act has the potential to
    become the de facto global standard for regulating AI. There are clear strengths
    to the EU’s approach to AI regulation, particularly the risk-based methodology,
    but there are a few challenges as well. Notably, it will foster an ecosystem of
    independent audits, which will likely result in more transparent, fair, and risk-managed
    AI applications. There are, however, open questions as to the extent the legislation
    can adapt to new capabilities and risks as they arise, as well as manage the longer-term
    societal effects of AI.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经强调了欧盟在制定一致的人工智能治理和标准方面的努力。特别是，人工智能法案有可能成为监管人工智能的既定全球标准。欧盟在人工智能监管方面的方法具有明显的优势，特别是基于风险的方法，但也存在一些挑战。值得注意的是，它将培养一个独立的审计生态系统，这可能会导致更透明、公平和风险管理的AI应用。然而，关于立法能否适应新能力和风险的出现，以及管理人工智能的长期社会影响，仍存在一些悬而未决的问题。
- en: Ultimately, the EU’s goal is to provide a regulatory framework for AI companies
    and organizations that use AI, as well as facilitate a balance between innovation
    and the protection of citizens’ rights. However, their success will depend on
    a well-conceived enforcement structure and their ability to create an AI auditing
    ecosystem.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，欧盟的目标是为使用人工智能的公司和组织提供一个监管框架，同时促进创新与保护公民权利之间的平衡。然而，他们的成功将取决于一个精心设计的执行结构和他们建立人工智能审计生态系统的能力。
- en: China overview
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中国概述
- en: 'As of 2023, China has introduced three comprehensive and targeted machine learning
    and AI regulations: its 2021 regulation on recommendation algorithms, the 2022
    rules for deep synthesis (deepfakes), and the 2023 draft rules on generative AI.
    These legislations create new rules for how AI systems are built and deployed,
    as well as what information AI developers must disclose to the government and
    the general public.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2023年，中国已出台三项全面和针对性的机器学习和人工智能法规：其2021年的推荐算法法规、2022年的深度合成（深度伪造）规则和2023年的生成人工智能草案规则。这些立法为人工智能系统的构建和部署制定了新的规则，以及人工智能开发者必须向政府和公众披露的信息。
- en: Unlike the European Union, China has taken an iterative and vertical regulatory
    approach toward AI governance. For example, the AI Act is a horizontal regulation,
    aiming to cover all applications of a given technology. On the other hand, vertical
    regulations target a specific application of a given technology. Here, Chinese
    regulators impose requirements specific to their concerns, and if deemed inadequate
    or flawed, they release new regulations to fill in the gaps or expand on existing
    legislation. While China’s iterative process toward regulation can be confusing
    or challenging for AI developers to maintain compliance, Chinese regulators view
    that as a necessary trade-off in a fast-moving technology environment.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与欧盟不同，中国在人工智能治理方面采取了迭代和垂直的监管方法。例如，人工智能法案是一项横向监管，旨在涵盖特定技术的所有应用。另一方面，垂直监管针对特定技术的特定应用。在这里，中国监管机构根据他们的关注点提出具体要求，如果认为不足或存在缺陷，他们就会发布新的法规来填补空白或扩展现有立法。虽然中国在监管方面的迭代过程可能会让人工智能开发者感到困惑或具有挑战性，但中国监管机构认为这是在快速发展的技术环境中必要的权衡。
- en: In 2021, China’s regulation on algorithmic recommendation systems marked the
    start of restrictions on AI and machine learning systems. Initially motivated
    by the Chinese Communist Party’s (CCP) concern about the role of algorithms disseminating
    information online, the set of regulations reins in the use and misuse of recommendation
    algorithms [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    The regulations demand transparency over how algorithms function and provide users
    more control over which data the companies can use to feed the algorithms, as
    well as mandate that the recommendation service providers “uphold mainstream value
    orientations” and “actively transmit positive energy online” [[39]](https://www.chinalawtranslate.com/algorithms/).
    The regulation also requires platforms to prevent the spread of undesirable or
    illegal information and manually intervene to ensure they reflect government policies.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，中国关于算法推荐系统的法规标志着对人工智能和机器学习系统限制的开始。最初，这些法规是由中国共产党（CCP）对算法在在线传播信息中所起作用的担忧所推动的，该法规旨在规范推荐算法的使用和滥用[[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117)。法规要求算法的运作透明，并赋予用户更多控制权，以决定公司可以使用哪些数据来喂养算法，同时要求推荐服务提供商“维护主流价值取向”和“积极传播正能量”[[39]](https://www.chinalawtranslate.com/algorithms/)。该法规还要求平台防止不良或非法信息的传播，并手动干预以确保其反映政府政策。
- en: The CCP also identified deepfakes as a threat to the information environment.
    We should note that, unlike the United States, which has very strong free expression
    guarantees in its constitution, or even the European Union, the information environment
    in China is more controlled and restricted by comparison. The spectrum of what
    classifies as an information “threat” in China is quite broad. For example, criticism
    of the state or CCP would be considered a threat to the information environment.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 中共中央也把深度伪造视为对信息环境的威胁。值得注意的是，与美国宪法中非常强大的言论自由保障或甚至与欧盟相比，中国的信息环境在比较中更为受控和受限。在中国，被归类为信息“威胁”的范围相当广泛。例如，对国家或中共的批评会被视为对信息环境的威胁。
- en: In 2022, China introduced the Deep Synthesis Provisions, which include algorithms
    that synthetically produce images, text, video, or voice content. The regulation
    calls for adding labels or tags on synthetically generated content, and includes
    vague censorship requirements, such that it must “adhere to correct political
    direction” and “not disturb economic and social order” [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    It further requires deep synthesis service providers to take measures for personal
    data protection, technical security, and transparency. The regulation was finalized
    on November 25, 2022, just five days before the public release of ChatGPT [[40]](https://www.china-briefing.com/news/china-to-regulate-deep-synthesis-deep-fake-technology-starting-january-2023/).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年，中国推出了《深度合成条例》，其中包括合成生成图像、文本、视频或语音内容的相关算法。该条例要求在合成生成的内容上添加标签或标签，并包含模糊的审查要求，规定其必须“坚持正确的政治方向”和“不扰乱经济和社会秩序”[[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117)。此外，该条例还要求深度合成服务提供商采取措施保护个人数据、技术安全和透明度。该条例于2022年11月25日最终确定，就在ChatGPT公开发布的前五天[[40]](https://www.china-briefing.com/news/china-to-regulate-deep-synthesis-deep-fake-technology-starting-january-2023/).
- en: Despite China being ahead of the curve with generative AI technology, they were
    faced with the unfortunate timing of ChatGPT’s release. The Deep Synthesis Provisions
    were deemed insufficient by the Cyberspace Administration of China (CAC) given
    that they were designed to regulate deepfakes and not text generated from LLMs.
    The regulation also covered only content-generation services provided through
    the internet, which created a regulatory gap in content that was being generated
    using AI offline. So, the Chinese regulators set out to quickly iterate on the
    same set of AI applications but with new concerns in mind.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管中国在生成式人工智能技术方面领先于世界，但他们面临着ChatGPT发布的糟糕时机。鉴于《深度合成条例》旨在规范深度伪造而非从LLM生成的文本，中国网信办（CAC）认为该条例不足。该条例还仅涵盖通过互联网提供的生成内容服务，这导致在离线使用AI生成的内容上存在监管空白。因此，中国监管机构着手快速迭代同一套AI应用，但考虑到新的担忧。
- en: In April 2023, the CAC released draft measures on Generative AI Services. The
    draft builds on the Deep Synthesis Provisions, which took effect in January 2023,
    and applies to all machine-generated content online and offline [[41]](https://www.chinalawtranslate.com/overview-of-draft-measures-on-generative-ai/).
    The initial draft had several difficult-to-meet requirements, including that training
    data must be truthful, accurate, and diverse, as well as not violate any intellectual
    property rights [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    A key question was whether the rules may end up suppressing innovation in the
    AI industry of a country that aims to become the world leader in this space. After
    an active public debate, the interim measures, set to take effect on August 15,
    2023, relaxed a few previously announced provisions and said that Chinese regulators
    would support the development of the technology [[42]](https://www.reuters.com/technology/china-issues-temporary-rules-generative-ai-services-2023-07-13/).
    The interim rules only apply to services that are available to the general public
    in China, which exempts any technology being developed in research institutions
    or intended for use by overseas users.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年4月，中国版权局发布了关于生成式人工智能服务的草案措施。该草案基于2023年1月生效的深度合成规定，适用于线上线下所有机器生成内容 [[41](https://www.chinalawtranslate.com/overview-of-draft-measures-on-generative-ai/)]。最初的草案包含了一些难以满足的要求，包括训练数据必须是真实、准确和多样化的，同时不得侵犯任何知识产权
    [[38](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117)]。一个关键问题是，这些规则是否最终会抑制旨在成为该领域世界领先者的国家的AI产业创新。经过积极的公众辩论，将于2023年8月15日生效的临时措施放宽了之前宣布的一些规定，并表示中国监管机构将支持该技术的发展
    [[42](https://www.reuters.com/technology/china-issues-temporary-rules-generative-ai-services-2023-07-13/)]。临时规则仅适用于在中国向公众提供的服务，这免除了任何在研究机构开发或打算供海外用户使用的技术。
- en: China’s vertical and iterative approach to AI regulation reveals both strengths
    and vulnerabilities. The strength of the vertical approach is the ability to create
    precise solutions or mitigations for specific problems. However, regulators are
    forced to develop new regulations for new applications or problems, as seen with
    the Deep Synthesis Provisions. Because of prior experience with AI governance
    and utilization of regulatory frameworks from past vertical regulations, CAC was
    able to quickly iterate on the Deep Synthesis Provisions to draft rules for generative
    AI, showcasing speed as another area of strength.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 中国对AI监管的垂直和迭代方法既展示了优势，也暴露了脆弱性。垂直方法的优势在于能够为特定问题创造精确的解决方案或缓解措施。然而，正如深度合成规定所示，监管机构被迫为新应用或问题制定新的规定。由于过去在AI治理和利用监管框架方面的经验，中国版权局能够快速迭代深度合成规定，起草生成式AI的规则，展示了速度作为另一个优势领域。
- en: In June 2023, China’s State Council (the equivalent of the US cabinet) announced
    that they would draft an Artificial Intelligence Law, a comprehensive, horizontal
    piece of legislation building upon existing regulations. This suggests that Chinese
    AI regulation is approaching a turning point, echoing the evolution of Chinese
    regulations governing the internet. Initially, the internet in China was governed
    by narrow and specific regulations, which later matured into the Cybersecurity
    Law of 2017, a broad and comprehensive framework that was built upon existing
    laws [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    Following a similar playbook as their approach to internet regulation, if the
    draft of the Artificial Intelligence Law is adopted, it will be China’s first
    national AI legislation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年6月，中国国务院（相当于美国内阁）宣布，他们将起草一部人工智能法，这是一项综合性的、横向的立法，建立在现有法规的基础上。这表明中国AI监管正接近转折点，与中国监管互联网的演变相呼应。最初，中国的互联网由狭窄和具体的法规管理，后来成熟为2017年的网络安全法，这是一个建立在现有法律基础上的广泛和全面的框架
    [[38](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117)]。遵循类似互联网监管的方法，如果人工智能法的草案被采纳，它将成为中国的第一部国家AI立法。
- en: Corporate self-governance
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 企业自律
- en: As discussed in section North America Overview, the Biden-Harris administration
    secured voluntary commitments in July 2023 from seven leading AI companies—Amazon,
    Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI—to ensure the safe,
    secure, and transparent development of AI technology. However, this agreement
    has been both praised and criticized—is this a step forward or an empty promise?
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如在“北美概述”部分所述，拜登-哈里斯政府在2023年7月从七家领先的AI公司——亚马逊、Anthropic、谷歌、Inflection、Meta、微软和OpenAI——获得了自愿承诺，以确保AI技术的安全、安全和透明发展。然而，这一协议既受到赞扬也受到批评——这是向前迈出的一步还是一项空洞的承诺？
- en: 'On the surface, the voluntary commitment looks promising, but the phrasing
    of the terms is fairly vague and largely seems to reinforce what the seven companies
    are already doing: working on the safety of AI systems, investing in cybersecurity,
    and aiming for transparency. The agreement is also voluntary, which doesn’t assign
    responsibility to ensure that the companies abide by the terms nor does it hold
    them accountable for noncompliance. However, it’s worth noting that companies
    would likely feel pressured to participate, especially given the alternative threat
    of rigid regulation.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上看，自愿承诺看起来很有希望，但条款的措辞相当含糊，在很大程度上似乎是在加强七家公司已经做的事情：致力于AI系统的安全性，投资于网络安全，并追求透明度。该协议也是自愿的，它没有分配责任以确保公司遵守条款，也没有对不遵守条款的行为进行问责。然而，值得注意的是，公司可能会感到有压力参与其中，尤其是在严格的监管的替代威胁之下。
- en: On the plus side, however, a voluntary commitment helps the administration avoid
    strict, difficult to comply with regulations that may hinder innovation in the
    United States, as it has in the European Union [[43]](https://techliberation.com/2022/08/01/why-the-future-of-ai-will-not-be-invented-in-europe/).
    The financial sector’s regulatory oversight actually began in industry self-governance.
    In the 17th century, collectives of traders used to meet at rival coffeehouses
    competing with each other on the effectiveness of the ethics rules their members
    had to comply with [[44]](https://dlc.dlib.indiana.edu/dlc/handle/10535/10528).
    These rules persuaded the public to trade with them instead of their rivals. When
    any member broke these ethical rules, the entire collective’s reputation was damaged.
    Consequently, all the members were incentivized to monitor unethical behavior,
    so if any member behaved undesirably, they could be ousted. Eventually, all the
    collectives adopted the rules that best protected the public as the standard.
    These collectives—the original stock traders in London’s Lombard Street—are an
    excellent example of industry self-governance in a sector that is now heavily
    regulated. Once these collectives were able to establish the best standards, the
    monitoring and enforcement of the rules were transferred to a third party, such
    as the government, where the collective members and the third party worked together
    to amend and establish new standards [[45]](https://www.aei.org/technology-and-innovation/white-house-ai-commitments-a-first-step-to-industry-self-governance/).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在积极的一面，自愿承诺有助于政府避免实施严格的、难以遵守的法规，这些法规可能会阻碍美国的创新，正如它在欧盟所做的那样 [[43](https://techliberation.com/2022/08/01/why-the-future-of-ai-will-not-be-invented-in-europe/)]。金融部门的监管实际上始于行业自律。在17世纪，一群商人习惯于在相互竞争的咖啡馆聚会，这些咖啡馆的竞争在于其成员必须遵守的道德规则的有效性
    [[44](https://dlc.dlib.indiana.edu/dlc/handle/10535/10528)]。这些规则说服公众与他们而不是他们的竞争对手进行交易。当任何成员违反这些道德规则时，整个集体的声誉都会受损。因此，所有成员都有动力去监督不道德的行为，所以如果任何成员表现不佳，他们可能会被驱逐。最终，所有集体都采用了最能保护公众利益的规则作为标准。这些集体——伦敦伦敦桥街的原始股票交易者——是现在高度监管的行业中行业自律的一个极好例子。一旦这些集体能够确立最佳标准，规则的监督和执行就转移到了第三方，比如政府，在那里集体成员和第三方共同努力修改和建立新的标准
    [[45](https://www.aei.org/technology-and-innovation/white-house-ai-commitments-a-first-step-to-industry-self-governance/)]。
- en: Similarly, the Biden-Harris administration’s voluntary commitments give the
    AI companies the freedom to establish their own rules to enforce where perhaps
    the rules that best protect the public will surface, as they did in the financial
    sector. As we’ve said, voluntary commitments merely formalize the commitment for
    AI companies to have best practices. For example, OpenAI doesn’t allow usage of
    their models for illegal activity, or any activity that has a high risk of physical
    or economic harm, among other disallowed uses (see [http://mng.bz/5w9q](http://mng.bz/5w9q)).
    Google has also released additional terms for generative AI with a similar policy
    for blocking any content that violates their prohibited use policy, which includes
    (but is not limited to) any content used to perform or facilitate dangerous, illegal,
    or malicious activities (see [http://mng.bz/6DW5](http://mng.bz/6DW5)). Meanwhile,
    Inflection AI states that “safety is at the heart of our mission” and “our internal
    Safety team continuously pressure tests our models for risks, and works with outside
    experts on comprehensive red-teaming of our technologies” (see [http://mng.bz/o1Xj](http://mng.bz/o1Xj)).
    Even Meta’s Llama 2, which has been open sourced for research and commercial use
    cases, has an acceptable use policy that prohibits certain use cases to help ensure
    that the models are used responsibly (see [https://ai.meta.com/llama/use-policy/](https://ai.meta.com/llama/use-policy/)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，拜登-哈里斯政府的自愿承诺给予AI公司自由来制定他们自己的规则，以执行可能最能保护公众的规则，正如在金融部门所发生的那样。正如我们所说的，自愿承诺仅仅是对AI公司拥有最佳实践的承诺进行正式化。例如，OpenAI不允许使用他们的模型进行非法活动，或任何具有高物理或经济风险的活动，以及其他禁止用途（见[http://mng.bz/5w9q](http://mng.bz/5w9q)）。谷歌也发布了针对生成式AI的额外条款，具有类似的政策，以阻止任何违反其禁止使用政策的内容，包括但不限于用于执行或促进危险、非法或恶意活动的任何内容（见[http://mng.bz/6DW5](http://mng.bz/6DW5)）。同时，Inflection
    AI表示“安全是我们使命的核心”，并且“我们的内部安全团队持续对模型进行风险评估，并与外部专家合作，对我们技术进行全面的红队测试”（见[http://mng.bz/o1Xj](http://mng.bz/o1Xj)）。即使是开源用于研究和商业用例的Meta的Llama
    2，也有一个可接受的使用政策，禁止某些用例，以确保模型被负责任地使用（见[https://ai.meta.com/llama/use-policy/](https://ai.meta.com/llama/use-policy/))。
- en: In the United States, it’s also more than likely that market forces will shape
    the governing landscape. The companies will actively work to make sure that their
    LLMs aren’t seen as inadequate—maybe this motivation stems from reports of adversaries
    exploiting the LLM, the general public deeming their data practices untrustworthy,
    or simply trying to avoid embarrassing (and expensive) events such as Google’s
    public release of Bard. Of course, it can certainly be problematic for for-profit
    companies to develop their own governance frameworks when they perhaps may be
    more incentivized by growing a successful business than protecting their users,
    but it’s worth noting that the administration does emphasize involving a diverse
    range of stakeholders (which we’ll further unpack in section Towards an AI Governance
    Framework). At the very least, the voluntary commitments reinforce the notion
    that companies have a responsibility in their commitment to responsible AI development,
    including their potential for affecting society. Encouraging corporate self-governance
    could complement existing or future regulatory efforts, as well as fill in a critical
    gap to develop a more comprehensive approach to the governance of AI systems,
    or any new technology for that matter, especially in its infancy.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，市场力量塑造治理格局的可能性也很大。公司会积极工作以确保他们的LLM（大型语言模型）不被视为不足——这种动机可能源于对手利用LLM的报道、公众认为他们的数据实践不可信，或者简单地试图避免（并且昂贵的）事件，例如谷歌公开发布Bard。当然，对于盈利公司来说，在可能更倾向于增长成功的业务而不是保护用户的情况下，开发他们自己的治理框架可能确实存在问题，但值得注意的是，管理层确实强调要涉及多样化的利益相关者（我们将在“迈向AI治理框架”一节中进一步探讨）。至少，自愿承诺强化了公司对其在负责任AI开发中的承诺负责的观念，包括他们可能对社会产生的影响。鼓励企业自我治理可以补充现有的或未来的监管努力，以及填补开发更全面治理AI系统（或任何新技术）方法的空白，尤其是在其初期。
- en: Toward an AI governance framework
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迈向AI治理框架
- en: In *Introduction to Generative AI*, we’ve outlined the AI race, illustrating
    the potential of generative AI technology as well as building awareness around
    its shortcomings. Enthusiasts anticipate that generative AI will disrupt the way
    we engage in work and our personal lives, do business, and create wealth. On the
    other hand, an increasing number of technology experts have shared significant
    concerns regarding the existential dangers of relinquishing tasks and decision-making
    to computers with little use for humans in the near future. Contributing to these
    unsettling concerns is an existing imbalance of power and wealth where critics
    of AI are worried that gains from the technology will disproportionately accumulate
    among the top 1%. As mentioned in chapter 6, we believe that generative AI is
    an evolution, not a revolution, as long as we use and govern the technology responsibly.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在《生成式AI导论》中，我们概述了AI竞赛，展示了生成式AI技术的潜力，同时也对其不足之处进行了警示。热衷者们期待生成式AI将颠覆我们参与工作、个人生活、商业活动以及创造财富的方式。另一方面，越来越多的技术专家对将任务和决策权交给人类几乎无用的计算机在不久的将来可能带来的生存风险表示了严重关切。这些令人不安的担忧部分源于现有的权力和财富不平衡，AI的批评者担心技术带来的收益将不成比例地积累在顶尖的1%人群中。正如第6章所述，我们认为只要我们负责任地使用和管理这项技术，生成式AI是一种进化，而不是革命。
- en: Throughout the book, we’ve highlighted the pragmatic promises of generative
    AI, from productivity gains to agentized systems. But at the same time, we’ve
    emphasized the risks and limitations of generative AI technology, as well as its
    ability to be misused accidentally and intentionally. As the awareness of AI risks
    has grown, so have the standards and guidance to mitigate them. We’ve come a long
    way, but we have an even longer way to go. We hope and believe that we’ll find
    a balance between groups calling for a pause in training AI systems and those
    claiming that ChatGPT is magic. Regardless of how the global AI disruption unfolds,
    the world won’t become a better place for living, working, or participating in
    democratic processes unless there are measures in place to regulate and govern
    AI’s development, effect, and safeguards.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们强调了生成式AI的实用承诺，从生产力提升到自动化系统。但与此同时，我们也强调了生成式AI技术的风险和局限性，以及它可能被意外或故意误用的能力。随着对AI风险的意识增强，缓解这些风险的规范和指导标准也在不断提高。我们已经走了很长的路，但还有更长的路要走。我们希望并相信，我们将在呼吁暂停训练AI系统的人群和声称ChatGPT是魔法的人群之间找到平衡。无论全球AI颠覆如何展开，除非有措施来规范和治理AI的发展、影响和保障，否则世界不会因为居住、工作和参与民主进程而变得更好。
- en: As we discussed in section Ethics-Informed AI Regulations, AI governance efforts
    have primarily been undertaken voluntarily, encompassing numerous protocols and
    principles that endorse conscientious design and controlled behavior. This is
    especially true in North America, where the shared goals of big technology companies
    involve aligning AI with human usefulness and ensuring safety throughout the creation
    and implementation of algorithms. Additional goals for AI systems also involve
    algorithmic transparency, fairness in their utilization, privacy and data protection,
    human supervision and oversight, and adherence to regulatory standards. While
    we acknowledge that these are ambitious goals, it’s necessary to highlight that
    AI developers often fall short of these objectives. Companies often have proprietary
    intellectual property for building their AI systems that they don’t disclose in
    order to keep their competitive advantage. For many in the AI ethics community,
    this is an indication that companies are more motivated by financial incentives
    than public benefits.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在“基于伦理的AI监管”部分所讨论的，AI治理工作主要是自愿进行的，包括许多协议和原则，这些协议和原则支持负责任的设计和受控行为。这在北美尤其如此，大型科技公司共同的目标是将AI与人类的有用性相一致，并在算法的创建和实施过程中确保安全。AI系统的其他目标还包括算法透明度、利用的公平性、隐私和数据保护、人类监督和监管，以及遵守监管标准。虽然我们承认这些目标雄心勃勃，但有必要强调，AI开发者往往无法达到这些目标。公司通常拥有构建其AI系统的专有知识产权，他们不公开这些信息以保持其竞争优势。对于许多AI伦理社区的人来说，这是一个迹象，表明公司更受财务激励而非公共利益的驱动。
- en: 'Since the early 2020s, the focus on voluntary self-policing by AI companies
    has started to shift toward comprehensive regulations in various countries. In
    a *Wired* article, Rumman Chowdhury wrote, “In order to truly create public benefit,
    we need mechanisms of accountability” [[46]](https://www.wired.com/story/ai-desperately-needs-global-oversight/).
    However, it’s important to note that the majority of discussions concerning AI
    and potential approaches to mitigate unintended negative consequences have been
    primarily focused in the West—the European Union, the United States, or members
    of advanced economies. Of course, the Western focus makes sense given the concentration
    of big AI companies in Silicon Valley, including OpenAI, Google, Meta, and Anthropic.
    But it’s worth emphasizing the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 自2020年代初以来，对AI公司自愿自律的关注开始转向各国实施的全面监管。在一篇《Wired》文章中，Rumman Chowdhury写道：“为了真正创造公共利益，我们需要问责机制”[[46](https://www.wired.com/story/ai-desperately-needs-global-oversight/)]。然而，值得注意的是，关于AI和减轻意外负面后果的潜在方法的讨论主要集中在西方——欧盟、美国或发达经济体的成员国。当然，西方的关注是有道理的，因为大型AI公司集中在硅谷，包括OpenAI、Google、Meta和Anthropic。但以下这一点值得强调：
- en: The vast majority of discussion about the consequences and regulation of AI
    is occurring among countries whose populations make up just 1.3 billion people.
    Far less attention and resources are dedicated to addressing these same concerns
    in poor and emerging countries that account for the remaining 6.7 billion of the
    global population. [[47]](https://foreignpolicy.com/2023/05/29/ai-regulation-global-south-artificial-intelligence/)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 关于AI的后果和监管的大多数讨论都在人口仅占13亿的国家之间进行。在占全球剩余67亿人口的贫困和新兴国家中，对这些相同问题的关注和资源投入要少得多。[47](https://foreignpolicy.com/2023/05/29/ai-regulation-global-south-artificial-intelligence/)
- en: 'So, where do we go from here? How do we truly ensure that generative AI, or
    AI systems in general, are used to better society? In the previously mentioned
    article, Chowdhury says:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们接下来该怎么做？我们如何真正确保生成式AI，或者AI系统在总体上被用于改善社会？在之前提到的文章中，Chowdhury说：
- en: The world needs a generative AI global governance body to solve these social,
    economic, and political disruptions beyond what any individual government is capable
    of, what any academic or civil society group can implement, or any corporation
    is willing or able to do. [[46]](https://www.wired.com/story/ai-desperately-needs-global-oversight/)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 世界需要一个生成式AI全球治理机构来解决这些社会、经济和政治动荡，这些动荡超出了任何单个政府的能力，任何学术或民间社会团体能够实施的能力，或者任何公司愿意或能够做到的能力。[46](https://www.wired.com/story/ai-desperately-needs-global-oversight/)
- en: 'The risks exposed by generative AI have emphasized what many experts have been
    calling for: the need for a new, permanent, independent, well-funded, and resourced
    institution to holistically ensure public benefit. Chowdhury further states:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI暴露出的风险强调了众多专家一直呼吁的必要性：建立一个新成立、永久性、独立、资金充足且资源丰富的机构，全面确保公共利益。 Chowdhury进一步指出：
- en: It should cover all aspects of generative AI models, including their development,
    deployment, and use as it relates to the public good. It should build upon tangible
    recommendations from civil society and academic organizations, and have the authority
    to enforce its decisions, including the power to require changes in the design
    or use of generative AI models, or even halt their use altogether if necessary.
    Finally, this group should address reparations for the sweeping changes that may
    come, job loss, a rise in misinformation, and the potential for inhibiting free
    and fair elections potentially among them. This is not a group for research alone;
    this is a group for action. [[46]](https://www.wired.com/story/ai-desperately-needs-global-oversight/)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该涵盖生成式AI模型的所有方面，包括其开发、部署以及与公共利益的关联使用。它应该基于民间社会和学术组织提出的具体建议，并拥有执行其决定的权力，包括要求改变生成式AI模型的设计或使用，甚至在必要时完全停止其使用。最后，这个团体应该解决可能带来的全面变革的赔偿问题，包括失业、虚假信息的增加，以及可能阻碍自由和公平选举的潜在问题。这不仅仅是一个研究团体；这是一个行动团体。[46](https://www.wired.com/story/ai-desperately-needs-global-oversight/)
- en: We should note that we already have an example of a global, independent, and
    well-funded organization that makes decisions for the betterment of society. The
    International Atomic Energy Agency (IAEA) (see [www.iaea.org/](https://www.iaea.org/))
    was formed in the post–World War II era to govern nuclear technologies. IAEA,
    formed under the guidance of the United Nations, is a body independent of governments
    and corporations that provides advisory support and resources. While it has limited
    agency, IAEA shows us that we’ve done this before and that we can do it again.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，我们已经有了一个全球性、独立且资金充足的组织的例子，该组织为社会的改善做出决策。国际原子能机构（IAEA）（见[www.iaea.org/](https://www.iaea.org/））是在第二次世界大战后形成的，以治理核技术。在联合国的指导下成立的IAEA是一个独立于政府和企业的机构，提供咨询支持和资源。虽然它的权力有限，但IAEA向我们展示了我们以前已经做到过，并且我们可以再次做到。
- en: Fundamentally, recent advances in generative AI have highlighted what many of
    us have known for a long time. We’ll never be able to “solve” the problem of abusing
    or misusing technology. Therefore, instead of only pursuing band-aid technical
    solutions, we need to invest in sociotechnical approaches to address the root
    of the problem. To Chowdhury’s point, the IAEA is a starting point for a global
    governance body, not an end goal. Unlike the IAEA’s limited agency, this body
    should have the ability to make independent and enforceable decisions. It should
    take advisory guidance from AI companies but also collaborate with civil society,
    government, and academia. This body shouldn’t replace any of these entities, but
    it should form a coalition to ensure public benefit in the face of AI. While we
    acknowledge that the effort needed to get to a global governance body for AI is
    substantial, we’re optimistic about the future of AI, and hopeful that AI companies
    and governments will work toward an independent, global body to make decisions
    regarding the governance and effect of AI systems.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，生成式人工智能的最近进展突显了我们许多人早已知道的事实。我们永远无法“解决”滥用或误用技术的问题。因此，我们不仅需要追求临时性的技术解决方案，还需要投资于社会技术方法来解决问题的根源。正如Chowdhury所指出的，国际原子能机构（IAEA）是全球治理机构的起点，而不是终点。与IAEA有限的权力不同，这个机构应该有能力做出独立和可执行的决策。它应该从AI公司那里获得咨询指导，但也应与民间社会、政府和学术界合作。这个机构不应取代任何这些实体，而应形成一个联盟，以确保在AI面前实现公共利益。虽然我们承认实现AI全球治理机构的努力是巨大的，但我们对于AI的未来持乐观态度，并希望AI公司和政府共同努力，建立一个独立的全球机构来做出关于AI系统治理和影响的决策。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs are trained on unfathomable amounts of internet data. They inevitably encode
    bias, harmful stereotypes, and toxicity, as well as copyrighted data and sensitive
    information in their training process.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在难以估量的互联网数据上进行了训练。它们不可避免地会在训练过程中编码偏见、有害的刻板印象和毒性，以及受版权保护的数据和敏感信息。
- en: LLMs often exhibit biased, toxic, and misaligned responses because of the characteristics
    of the training data. They also regurgitate sensitive or copyrighted information.
    LLMs also hallucinate, that is, they confidently make up incorrect information
    because of how they work.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于训练数据的特点，LLMs往往表现出有偏见、有毒性和不一致的响应。它们还重复敏感或受版权保护的信息。LLMs还会产生幻觉，即它们自信地编造错误信息，因为它们的工作方式。
- en: Adversaries can exploit the vulnerabilities in LLMs to perform training data
    extraction attacks, prompt injections or jailbreaking, or poison data.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对手可以利用LLMs中的漏洞进行训练数据提取攻击、提示注入或越狱，或污染数据。
- en: LLMs can help malicious actors carry out personalized and low-cost adversarial
    attacks at scale, as well as lower the barrier of entry for novice threat actors.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可以帮助恶意行为者进行大规模的个性化低成本对抗性攻击，同时降低新手威胁行为者的入门门槛。
- en: LLMs can be accidentally misused in professional sectors by people who don’t
    grasp the limitations of these models, which can result in serious ethical and
    societal consequences.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs可能会被不懂得这些模型局限性的专业人士在专业领域意外误用，这可能导致严重的伦理和社会后果。
- en: If not implemented responsibly, AI systems could be used to replace humans with
    machines, drive down wages, worsen the inequality between wealth and income, and
    do little to help overall economic growth.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有得到负责任地实施，AI系统可能会被用来用机器取代人类，降低工资，加剧财富与收入之间的不平等，并且对整体经济增长的贡献甚微。
- en: When misused, social chatbots can lead to unhealthy relationship patterns, dependency-seeking
    behaviors, and risk replacing genuine human connection.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当被滥用时，社交聊天机器人可能导致不健康的关系模式、寻求依赖的行为，并有可能取代真实的人类联系。
- en: LLM developers should document training data, be transparent with users about
    data privacy and use, and make efforts to mitigate biases present in their models.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM开发者应记录训练数据，向用户透明地说明数据隐私和使用情况，并努力减轻模型中存在的偏见。
- en: Vector databases and web retrieval provide some additional capabilities to LLMs
    and can be used to help interpret some of the model’s responses.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据库和网页检索为LLM提供了一些额外的功能，可以用来帮助解释模型的一些响应。
- en: LLMs should be trained and evaluated thoroughly for safety and robustness to
    adversarial attacks before public release.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在公开发布之前，LLM应该经过彻底的安全性和鲁棒性训练和评估，以抵御对抗性攻击。
- en: Identifying post hoc if the content was created by a human or a machine will
    soon be a fool’s errand, but there are promising solutions focused on tracking
    the provenance of media.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在事后确定内容是由人类还是机器创建的将很快成为徒劳之举，但有一些有前景的解决方案专注于追踪媒体的来源。
- en: Developers of social chatbots can optimize for metrics other than engagement
    to reduce the potential risk for social harms such as dependency or problems in
    emotional development.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交聊天机器人的开发者可以优化其他指标，以减少社会危害的风险，如依赖性或情感发展问题。
- en: Because LLMs don’t have true knowledge or expertise, they should be typically
    deployed within a human-in-the-loop context, and stakeholders must be literate
    on how these models work before they are blindly used.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于LLM（大型语言模型）没有真正的知识或专业知识，它们通常应在人类在环（human-in-the-loop）的背景下部署，并且利益相关者在盲目使用这些模型之前必须了解这些模型的工作原理。
- en: In the near future, we can expect to see generative AI integrated into more
    applications and becoming increasingly agentic, efficient, and personalized.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不久的将来，我们可以期待看到生成式AI被整合到更多应用中，并变得越来越具有主动性、高效性和个性化。
- en: The United States hasn’t attempted a large-scale AI regulation like the EU but
    has instead relied more heavily on corporate self-regulation and voluntary commitments.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国没有尝试像欧盟那样进行大规模的AI监管，而是更依赖于企业自我监管和自愿承诺。
- en: The EU’s AI Act takes a risk-based approach to AI regulation and is one of the
    first major laws to regulate AI.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧盟的AI法案采取了基于风险的方法来规范AI，是首个对AI进行监管的主要法律之一。
- en: In 2023, China released draft measures on Generative AI Services and announced
    that they would draft an Artificial Intelligence Law, a comprehensive, horizontal
    piece of legislation building upon existing regulations.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2023年，中国发布了关于生成式AI服务的草案措施，并宣布将起草一部人工智能法，这是一项建立在现有法规基础上的全面、横向的立法。
- en: Corporate self-governance could complement existing or future regulatory efforts,
    as well as fill in a critical gap to develop a more comprehensive approach to
    AI systems governance.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业自我治理可以补充现有的或未来的监管努力，同时填补发展更全面AI系统治理方法的临界差距。
- en: AI companies often fall short of algorithmic transparency, ensuring the safety
    of AI systems, and data protection standards, among others.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI公司往往在算法透明度、确保AI系统的安全性、数据保护标准等方面做得不够。
- en: 'The risks exposed by generative AI have emphasized what many experts have been
    calling for: the need for a new, permanent, independent, well-funded, and resourced
    institution to holistically ensure public benefit.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式AI暴露出的风险强调了众多专家一直呼吁的：需要一个新的、永久性的、独立的、资金充足并拥有资源的机构，以全面确保公共利益。
