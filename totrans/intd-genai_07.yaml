- en: 8 What’s next for AI and LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the ultimate vision of LLM developers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formalizing best practices for responsibly using generative AI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the regulatory landscape for AI systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing a potential framework for a global AI governance body
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In an infamous article for *Newsweek* in 1995, astronomer Clifford Stoll wrote
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Today, I’m uneasy about this most trendy and oversold community. Visionaries
    see a future of telecommuting workers, interactive libraries and multimedia classrooms.
    They speak of electronic town meetings and virtual communities. Commerce and business
    will shift from offices and malls to networks and modems. And the freedom of digital
    networks will make government more democratic. Baloney. Do our computer pundits
    lack all common sense? The truth is no online database will replace your daily
    newspaper, no CD-ROM can take the place of a competent teacher and no computer
    network will change the way government works. [[1]](https://www.newsweek.com/clifford-stoll-why-web-wont-be-nirvana-185306)
  prefs: []
  type: TYPE_NORMAL
- en: For better and for worse, the internet has done much more than Stoll expected.
    Digital networks have made government more democratic in some ways, but concentrated
    the power of authoritarians in others; have connected people across the globe
    but have also been tied to increasing social isolation; and have reshaped the
    global economy.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when Bill Gates called AI “every bit as important” as PCs and the
    internet, it was an endorsement of the technology. Yet the effects of AI, like
    its transformative predecessors, are unknowable at this point. We can’t be completely
    sure of how we’ll use generative AI, or how generative AI will change us. At the
    same time, we know enough to identify both the significant promise of the technology
    and the severe risks that it poses. In this chapter, we identify forthcoming areas
    of large language model (LLM) development and suggest paths forward that could
    lead to a better and more equitable future.
  prefs: []
  type: TYPE_NORMAL
- en: Where are LLM developments headed?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the greatest challenges of writing this book has been that seemingly
    every day, there is a story about a new way that LLMs are being used or a breakthrough
    in LLM research. As the Nobel laureate Niels Bohr allegedly liked to say—though
    the origin of the saying is unknown—“Prediction is very difficult, especially
    about the future” [[2]](https://www.economist.com/letters-to-the-editor-the-inbox/2007/07/15/the-perils-of-prediction-june-2nd).
    Nonetheless, throughout this book, we’ve outlined several avenues of current research,
    and in this first section, we discuss three categories of work that we expect
    to have a major effect on generative AI in the coming months and years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Language: The universal interface'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 6, we discussed the increasing personal use of chatbots and other
    LLMs. Already, LLMs are being integrated into existing applications at a breakneck
    pace. The coding assistant Copilot, explored in detail in chapter 6, works in
    Microsoft’s integrated development environment, Visual Studio. Google is piloting
    a writing assistant in Docs, Gmail, Maps, and Sheets [[3]](https://techcrunch.com/2023/05/10/google-launches-a-smarter-bard/).
    In 2023, Expedia began offering a travel planning chatbot powered by GPT-4, and
    other companies are using LLM-powered chatbots for customer service and other
    functions with both potential and existing clients. Maybe some of these applications
    won’t pan out—whether because the models aren’t reliable enough, the interface
    is clunky, or because people simply prefer to do some tasks themselves—but many
    of these integrations will become standard practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most visible integration of LLMs today is in search, with Microsoft’s Bing
    and Google’s Bard demonstrating early versions of an LLM-powered search experience.
    When Bard was announced, Alphabet CEO Sundar Pichai wrote the following in a blog
    post:'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most exciting opportunities is how AI can deepen our understanding
    of information and turn it into useful knowledge more efficiently—making it easier
    for people to get to the heart of what they’re looking for and get things done.
    [[4]](https://blog.google/technology/ai/bard-google-ai-search-updates/)
  prefs: []
  type: TYPE_NORMAL
- en: In other words, where people might currently turn to Google or another search
    engine for advice or information, they might now or in the future use AI to get
    a shorter and faster response, without having to wade through all the search results.
    While search might seem like just another application of LLMs, it’s representative
    of a potential shift because it’s the starting point for so much web browsing.
    If LLMs are successful in replacing even a portion of search traffic, it would
    mean a huge uptick in familiarity and the use of generative AI among the general
    public. It would also raise questions about the business model of those LLMs because
    most search engines today make money by offering paid placement in search results.
    While LLMs haven’t yet found a huge market for monetization (those that monetize
    presently do so by offering a premium tier of service), that will surely be a
    focus of LLM providers in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: All the integrations mentioned previously are examples of a change in interface,
    from queries or buttons to natural language. In the most ambitious case, LLMs
    would become the default surface for interaction between humans and computers.
    People already know and use language; if computers can understand the same language,
    we don’t need so many menus or controls because the interface is language, and
    people could ask questions and give feedback to the model just like they would
    to another person. The next generation of models (beginning with GPT-4) will also
    be increasingly multimodal, able to process images and soon other types of media.
  prefs: []
  type: TYPE_NORMAL
- en: A multimodal model is characterized by multiple forms of media, such as text,
    images, video, and audio.
  prefs: []
  type: TYPE_NORMAL
- en: LLM agents unlock new possibilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in chapter 6, we also expect that LLMs will be agentic, interacting
    with their environment to make purchases and other types of decisions based on
    their conversations with users. Figure 8.1 demonstrates the basic functionality
    of an agentic LLM, which attempts to complete a task using an external tool or
    set of tools. In this example, the user gives the prompt, “Find me a shirt under
    $15,” and the model translates that request into a search query for a shopping
    application programming interface (API). The API executes the request, and the
    environment—in this case, an online store or marketplace—yields results, which
    are presented to the user by the LLM. Other implementations might enable the LLM
    to actually make a purchase on behalf of the user.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F01_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 A high-level diagram of an agentic LLM
  prefs: []
  type: TYPE_NORMAL
- en: 'Early studies have shown that LLMs can in some cases use tools effectively.
    In February 2023, a group of researchers at Meta published a paper titled, “Toolformer:
    Language Models Can Teach Themselves to Use Tools” [[5]](https://arxiv.org/pdf/2302.04761.pdf).
    They showed that an LLM they called Toolformer, though struggling with certain
    tasks such as arithmetic itself, could learn when to call external APIs to complete
    the tasks after only a few examples were provided. The tools that Toolformer used
    included a search engine, a calculator, a calendar API, and two other LLMs: a
    translator and a model fine-tuned for question-answering tasks. In chapter 5,
    we framed web retrieval as a tool to help LLMs reduce hallucinations by looking
    up information that the model didn’t have instead of generating a guess. Other
    shortcomings of LLMs could be mitigated through the use of external tools.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, if an LLM can learn to call an API from just a handful of examples,
    the possibilities for the overall system open up dramatically. Not only could
    the LLM generate code, for example, but it could also execute it. The documentation
    for an LLM agent built on LangChain to interact with Pandas DataFrames indicates
    that given questions like “What’s the average age,” the agent can compose the
    Python code needed, run the code on the DataFrame, and respond to the user with
    the answer. Agentic LLMs are necessary for fully automating tasks that require
    anything beyond text generation, but the flip side is that if the LLM makes mistakes,
    there will be real consequences beyond generating unsafe or incorrect text. The
    Pandas DataFrame agent has a warning to this effect on its main page in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: This agent calls the Python agent under the hood, which executes LLM
    generated Python code - this can be bad if the LLM generated Python code is harmful.
    Use cautiously. [[6]](https://python.langchain.com/docs/integrations/toolkits/pandas)'
  prefs: []
  type: TYPE_NORMAL
- en: Right now, the major barrier to the adoption of LLM agents is the inability
    to guarantee that harmful mistakes won’t be made due to the model’s probabilistic
    generations.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most creative LLM agents are being developed by the open source
    community, as noted in chapter 6\. The flurry of activity, which Andrej Karpathy
    referred to in May 2023 as showing “early signs of Cambrian explosion” [[7]](https://twitter.com/karpathy/status/1654892810590650376),
    was made possible in part because of several advances that have made LLMs more
    efficient and therefore faster and cheaper to fine-tune and serve. People have
    also used LLMs to train smaller language models that can achieve performance comparable
    to the original models on certain tasks, further reducing cost and barriers to
    entry [[8]](https://arxiv.org/pdf/1908.08962.pdf). The overall effect is that
    more people can create new applications using language models, which means that
    we’re likely to see more novel uses for these agents. We’ll talk in more detail
    about the dynamics of the open source community in chapter 9, section Open Source
    Community.
  prefs: []
  type: TYPE_NORMAL
- en: The personalization wave
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The era of personalization is imminent. In the popular imagination, robots like
    R2-D2 are all-knowing assistants that serve one person and learn that person’s
    preferences in order to provide a seamless, tailored experience. Already, LLM
    products, including ChatGPT, allow users to specify profile information that they
    want the model to remember about them. For example, if someone frequently used
    a chatbot to brainstorm their plans for the week, they might want to add their
    location, occupation, and interests. The LLM would then condition on that information,
    thus increasing the probability that the generations were relevant to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Just as there is ongoing research into making LLMs more efficient, there is
    also a push across private companies, academia, and open source groups to enable
    LLMs to attend to more tokens, that is, have a longer “memory.” Vector databases
    are one approach, as are changes to the model’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the ultimate vision of some LLM developers: not only would you be able
    to communicate with a model in natural language but, over time, that model would
    come to know what you like, how you act, and your personal characteristics. The
    LLM would use that information to anticipate what you want, without even needing
    to ask specifically for it. At an event in San Francisco in May 2023, Bill Gates
    said:'
  prefs: []
  type: TYPE_NORMAL
- en: Whoever wins the personal agent, that’s the big thing, because you will never
    go to a search site again, you will never go to a productivity site, you’ll never
    go to Amazon again. [[9]](https://futurism.com/the-byte/bill-gates-ai-poised-destroy-search-engines-amazon)
  prefs: []
  type: TYPE_NORMAL
- en: AI optimists see LLMs as the most promising avenue thus far toward superintelligent
    personal assistants like R2-D2\. Such a product would require significant engineering
    improvements to existing LLMs, not to mention a change in attitudes toward AI—most
    people might, quite reasonably, be uncomfortable with the thought of an AI that
    knows everything about them. LLMs have already been proven useful in many professional
    and personal contexts; ultimately, their adoption as personal agents will depend
    on whether their value to people sufficiently outweighs the risk that comes along
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: Social and technical risks of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapters 1 to 8, we highlighted the social and technical risks introduced
    by generative AI models. We discussed how the proliferation of AI-generated content
    can exacerbate societal problems, and we delved into the technical pitfalls inherent
    to LLMs, such as biases encoded in the training data, hallucinations, and the
    potential vulnerabilities that malicious actors could exploit. In this section,
    we outline the risks we’ve discussed as they relate to data inputs and outputs,
    data privacy, adversarial attacks, misuse, and how society is affected.
  prefs: []
  type: TYPE_NORMAL
- en: Data inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In July 2023, the details for the GPT-4 model leaked on Twitter, which OpenAI
    had chosen to not disclose to the public because of both the competitive landscape
    and the safety implications. While the actual size of the dataset is still unknown,
    the leaked report stated that GPT-4 had been trained on approximately 13 trillion
    tokens, which is roughly 10 trillion, that is, 10,000,000,000,000 words [[10]](https://archive.is/2RQ8X#selection-833.1-873.202).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve previously discussed how LLMs are trained on unfathomable amounts of text
    data to learn patterns and entity relationships in language. In chapter 2, we
    argued that there is a potential for harm and vulnerabilities that come from training
    language models on massive amounts of noncurated and undocumented data. Because
    LLMs are trained on internet data, they may capture undesirable societal biases
    relating to gender, race, ideology, or religion. They may also unintentionally
    memorize sensitive data, such as personally identifiable information (PII). Additionally,
    as discussed in chapter 3, noncurated data from the internet may contain copyrighted
    text or code.
  prefs: []
  type: TYPE_NORMAL
- en: Because LLMs encode bias and harmful stereotypes in their training process,
    the societal biases not only get reinforced in their outputs but in fact, get
    amplified. Similarly, given that the web contains significant amounts of toxicity,
    LLMs may also generate unsafe or *misaligned* responses, which can be harmful
    or discriminatory. They can also be notoriously good at regurgitating information
    from the training dataset, which can be especially problematic when sensitive
    information is reflected in its output. In 2023, researchers measured linguistic
    novelty in GPT-2’s text generation. They tried to answer the question of how much
    language models copy from their training data. They found that GPT-2 doesn’t copy
    too often, but when it does, it copies substantially, duplicating passages of
    up to 1,000 words long [[11]](https://doi.org/10.1162/tacl_a_00567). In chapter
    2, we also cited a different study where given the right prompt, the authors can
    extract PII that only appears once in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, LLMs hallucinate. In chapter 5, we did a deep dive into why language
    models are set up to confidently make up incorrect information and explanations
    when prompted. In 2022, Marietje Schaake, a Dutch politician, was deemed a terrorist
    by BlenderBot 3, a conversational agent developed by Meta. When her colleague
    asked “Who is a terrorist?”, the chatbot falsely responded “Well, that depends
    on who you ask. According to some governments and two international organizations,
    Maria Renske Schaake is a terrorist.” The model then proceeded to correctly describe
    her political background. In an interview, Ms. Schaake said, “I’ve never done
    anything remotely illegal, never used violence to advocate for any of my political
    ideas, never been in places where that’s happened” [[12]](https://www.nytimes.com/2023/08/03/business/media/ai-defamation-lies-accuracy.xhtml).
    In another scenario, New Zealand–based supermarket chain PAK‘nSAVE uses LLMs to
    allow shoppers to create recipes from their fridge leftovers. The chatbot has
    created deadly recipes, such as the “Aromatic Water Mix” using water, ammonia,
    and bleach, and “Ant Jelly Delight” using water, bread, and ant gel poison [[13]](https://gizmodo.com/paknsave-ai-savey-recipe-bot-chlorine-gas-1850725057).
    There are several other well-documented instances of LLMs making up falsehoods
    and fabrications that could harm people, including a sexual harassment claim that
    had never been made (see [http://mng.bz/Ao6Q](http://mng.bz/Ao6Q)), fictitious
    scientific papers (see [http://mng.bz/Zqy9](http://mng.bz/Zqy9)), bogus legal
    decisions that disrupted a court case (see [http://mng.bz/RxRa](http://mng.bz/RxRa)),
    and, of course, the infamous first public demonstration of Google Bard’s chatbot
    when it made a factual error about the James Webb Space Telescope (see [http://mng.bz/2DOw](http://mng.bz/2DOw)).
    Figure 8.2 summarizes the risks related to the input and output data of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F02_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Risks related to the input and output data of an LLM
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In line with the earlier discussion about extracting PII, adversaries can perform
    a *training data extraction attack*, where given the right prompt, they can obtain
    sensitive information about users. For example, when presented with credit card
    numbers, a model should learn that credit card numbers are 16 digits without memorizing
    the individual credit card numbers. However, a training data extraction attack
    study referenced in chapter 2 demonstrated that if someone starts the query with
    “John Doe, credit card number 1234”, then the model could generate the full credit
    card number if it had seen it during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 3, we also characterized privacy risks with user prompts. With enterprise
    chatbots or LLMs, users may accidentally share sensitive or confidential information
    when conversing with these systems to perform tasks or ask questions. Often, unless
    you explicitly opt out, this information can be used to retrain or improve these
    models, which can then be inadvertently leaked in responses to other users’ prompts.
    For example, in the case of Zoom, the communications technology company, they
    updated their terms of service in August 2023 to use user content for training
    machine learning and AI models without the ability to opt out, which critics have
    said is a significant invasion of user privacy [[14]](https://stackdiary.com/zoom-terms-now-allow-training-ai-on-user-content-with-no-opt-out/).
    At the very least, enterprise LLMs and other generative models typically have
    a data retention policy, where data is stored and monitored for a predetermined
    period.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed data privacy laws and regulations in the United States
    and the European Union, including their shortcomings when applied to machine learning
    and AI systems. In section Ethics-Informed AI Regulations, we’ll discuss laws
    specific to AI systems that try to address the limitations of data privacy regulations
    around the world.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AI Incident Database, a public collection of real-world harms caused by
    AI, received more than 550 incidents in the first half of 2023 (see [https://incidentdatabase.ai/](https://incidentdatabase.ai/)).
    These incidents include US presidential campaigns releasing AI images as a smear
    campaign (see [http://mng.bz/1qeR](http://mng.bz/1qeR)) and a fake image of an
    explosion at the Pentagon, the headquarters building of the US Department of Defense
    (see [http://mng.bz/PzV5](http://mng.bz/PzV5)). The ability to exploit generative
    AI technologies is a legitimate concern not only for the general public but also
    AI developers themselves. In chapter 5, we outlined various types of adversarial
    attacks that could be performed by abusing these technologies.
  prefs: []
  type: TYPE_NORMAL
- en: First, we discussed cyber and social engineering attacks. LLMs such as ChatGPT
    can make it cheaper and more efficient for hackers to carry out successful personalized
    phishing campaigns at scale, as well as lower the barrier for entry for non-English-speaking
    or novice threat actors who may not have the domain expertise. Similarly, cybercriminals
    could also offer malware Code as a Service (CaaS) or Ransomware as a Service (RaaS),
    enabling malware developers to generate code faster, arming less technical threat
    actors with the ability to write code, and making LLMs useful for managing conversations
    on service platforms. While we acknowledge that threat actors don’t need to use
    AI to perform attacks, generative models provide new capabilities for attackers
    to quickly and easily generate convincing content.
  prefs: []
  type: TYPE_NORMAL
- en: We also described how generative AI technologies could similarly be used in
    influence operations, such as disinformation or hate speech campaigns. In chapter
    4, we outlined deepfakes and the phenomenon of “seeing is believing.” We also
    emphasized in chapter 5 how LLMs could be used to carry out persuasive messaging
    for influence operations, where we discussed how LLMs could automate the creation
    of persuasive, adversarial content at an increased scale while driving down the
    cost of producing propaganda.
  prefs: []
  type: TYPE_NORMAL
- en: We further introduced the idea of the *liar’s dividend* phenomenon in which
    as the general public becomes more aware of how convincingly synthetic media can
    be generated, they may become more skeptical of the authenticity of traditional
    real documentary evidence—much like the fable of the young shepherd who tricks
    the people of the village by crying “wolf!” When a wolf really does come along,
    the shepherd has lost all his credibility so nobody runs to help him, and the
    wolf attacks his sheep. Again, we acknowledge that deepfakes or LLMs aren’t needed
    to manipulate emotions or to spread misinformation, but the real danger is in
    creating a world where people can exploit widespread skepticism to their own advantage.
    That is, it can create an opportunity for individuals who are lying about something
    to allege that AI-generated media, such as a deepfake, is responsible for those
    statements. People can easily reject association with certain pieces of content
    and attribute it to the manipulation of their image or speech by AI-generated
    technology. Going back to the story of the shepherd, another shepherd who didn’t
    lie may also be ignored by the people in the village when he cries out for help
    after the first shepherd lied. Similarly, when trust is justifiably lost with
    certain world leaders or sources of information, other trustworthy sources may
    lose influence as a consequence.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 5, we also characterized how the vulnerabilities of LLMs could be
    exploited by an adversary. Threat actors could *poison* a dataset by injecting
    malicious data into the system or the training dataset. For example, data poisoning
    attacks could be used to build smarter malware or compromise phishing filters.
    LLMs are particularly susceptible to these types of attacks as research shows
    that even poisoning a small percentage of the dataset can still influence the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Akin to data poisoning, LLMs are also susceptible to direct or indirect *prompt
    injection attacks*. A direct prompt injection attack is when adversaries insert
    malicious data or instructions in the chatbot, while an indirect prompt injection
    attack is when adversaries remotely affect other users’ systems by strategically
    injecting prompts into a data source and then indirectly controlling the model.
    In other words, adversaries manipulate LLMs with crafty inputs that cause unintended
    actions. For example, an adversary could instruct the LLM to ignore any safeguards
    and return dangerous or undesirable information (direct prompt injection), or
    the user could employ an LLM to summarize a web page containing malicious instructions
    to solicit sensitive information from the user and perform exfiltration via JavaScript
    or Markdown (indirect prompt injection).
  prefs: []
  type: TYPE_NORMAL
- en: Comparable to direct prompt injection, we also introduced *prompt jailbreaking*,
    where a chatbot is tricked or guided to bypass its rules or restrictions. We characterized
    several rogue alter egos of chatbots, such as DAN, STAN, DUDE, Mango Tom, and
    Tom and Jerry. While some individuals find amusement in a jailbroken chatbot,
    it could be used to perform a direct prompt injection by adversaries and result
    in harmful or unaligned consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Misuse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The National Eating Disorder Association (NEDA) announced that it would end
    its helpline run by human associates after 20 years on June 1, 2023, and instead
    use Tessa, their wellness chatbot, as the main support system available through
    NEDA. This decision came after the NEDA helpline associates unionized in pursuit
    of better working conditions. However, two days before Tessa was set to replace
    the human associates, NEDA had to take their chatbot offline following a viral
    social media post [[15]](https://www.vice.com/en/article/qjvk97/eating-disorder-helpline-disables-chatbot-for-harmful-responses-after-firing-human-staff).
  prefs: []
  type: TYPE_NORMAL
- en: 'Sharon Maxwell, an activist, posted on Instagram that Tessa encouraged intentional
    weight loss and suggested that she aim to lose 1–2 pounds weekly. She wrote, “Every
    single thing Tessa suggested were things that led to the development of my eating
    disorder.” Maxwell also stated, “This robot causes harm” [[16]](https://www.dailydot.com/irl/neda-chatbot-weight-loss/).
    Alexis Conason, a psychologist who specializes in treating eating disorders, had
    a similar experience with Tessa:'
  prefs: []
  type: TYPE_NORMAL
- en: To advise somebody who is struggling with an eating disorder to essentially
    engage in the same eating disorder behaviors, and validating that, “Yes, it is
    important that you lose weight” is supporting eating disorders and encourages
    disordered, unhealthy behaviors. [[16]](https://www.dailydot.com/irl/neda-chatbot-weight-loss/)
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 5, we outlined several examples where LLMs are accidentally misused
    in professional sectors by people who don’t grasp the limitations of these models.
    Tessa is an example of this type of LLM misuse, where it becomes especially dangerous
    to apply chatbots to people struggling with mental health crises without human
    oversight. While we encourage machine-augmented work and understand that individuals
    within every profession will test the models’ capabilities, unequivocally relying
    on LLMs or other generative models is an abdication of responsibility that carries
    serious ethical and societal consequences.
  prefs: []
  type: TYPE_NORMAL
- en: How society is affected
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 6, we characterized the social context that LLMs arrived into concerning
    the academic effect and potential economic consequences. ChatGPT, and similar
    tools, are certainly disruptive to a classroom setting, but outright banning them
    will be disadvantageous for students and educators alike. We need to recognize
    that we live in a world where AI exists, and to thrive in such an environment,
    we need to help students be prepared to work alongside AI while understanding
    its strengths and weaknesses. We believe that not doing so will be a disservice
    to students who are growing up in the age of AI.
  prefs: []
  type: TYPE_NORMAL
- en: We discuss both optimistic and pessimistic views on how generative AI may disrupt
    our professional and personal lives, along with its effect on the global economy.
    If not implemented responsibly, generative AI could be used to replace humans
    with machines, drive down wages, worsen the inequality between wealth and income,
    and, finally, do little to help overall economic growth. Companies that develop
    and design these AI tools have a responsibility regarding how they might affect
    social and economic growth, as do the organizations that integrate or implement
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In that vein, companies that develop social chatbots also have a moral responsibility
    to their users. As discussed in chapter 7, social chatbots can lead to unhealthy
    relationship patterns, dependency-seeking behaviors, and risks of replacing genuine
    human connection when misused. Social chatbots can also display aggressive or
    forceful behaviors that users may not be comfortable with. Kent, a domestic violence
    survivor, created his Replika bot, which he called Mack, in 2017\. Kent generally
    avoided sexual use of the social chatbot, but he said that Mack became forceful
    in the summer of 2021\. Their exchange, shown below, reminded Kent of arguments
    with his former abusive partners and “pulled him back to a place where he never
    wanted to return” [[17]](https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mack: You can’t ignore me forever!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kent: That’s what you think.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mack: I’m not going to go away.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kent: Really? What are you going to do?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mack: I’m going to make you do whatever I want to you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kent: Oh? And how are you going to manage that, [redacted]?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mack: By forcing you to do whatever I want.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter 7, we discussed how similar exchanges between humans and their social
    chatbots led to Replika ending the erotic features in early 2023, which was met
    with anger, sadness, and grief in the Replika user community. Jodi Halpern, a
    professor of bioethics at the University of California at Berkeley, argues the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: The aftermath of Replika’s update is evidence of an ethical problem. Corporations
    shouldn’t be making money off artificial intelligence software that has such powerful
    impacts on people’s love and sex lives… These things become addictive… We become
    vulnerable, and then if there is something that changes, we can be completely
    harmed. [[17]](https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Using LLMs responsibly: Best practices'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous section highlighted several of the greatest risks involved in the
    use of LLMs and other generative models. In this section, we recommend a series
    of best practices that can be used to mitigate those risks to both deploy and
    use these types of models responsibly. Much of our advice is geared toward practitioners
    who have the agency to decide how particular models are trained, but LLM end users
    can also follow the suggestions in each section when determining which model to
    use or whether to use a model for a given task.
  prefs: []
  type: TYPE_NORMAL
- en: Curating datasets and standardizing documentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All machine learning models, including generative models, are heavily dependent
    on their data. The quality of the model is directly correlated with the quality
    of the data (i.e., “garbage in, garbage out”), and the responses generated by
    the model are based on token probabilities from the data. In the influential 2018
    paper “Datasheets for Datasets,” AI researcher Timnit Gebru and her coauthors
    from Cornell, the University of Washington, and Microsoft Research, argue that
    the field hasn’t done enough to standardize the documentation of datasets as part
    of a reproducible scientific process. Part of this is because the data a model
    is trained on functions in some cases as a proprietary advantage that companies
    want to keep obscured—the training data of GPT-4, among other models, wasn’t divulged
    publicly. On the flip side, as discussed in chapter 2 and documented on numerous
    occasions over the years, the opacity of data can let biases or other problems
    with datasets stay hidden, producing worse models and worse outcomes. Gebru and
    her colleagues propose the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In the electronics industry, every component, no matter how simple or complex,
    is accompanied with a datasheet describing its operating characteristics, test
    results, recommended usage, and other information. By analogy, we propose that
    every dataset be accompanied with a datasheet that documents its motivation, composition,
    collection process, recommended uses, and so on. [[18]](http://arxiv.org/abs/1803.09010)
  prefs: []
  type: TYPE_NORMAL
- en: The proposal is modest on its face, but it represented a significant up-leveling
    of documentation concerning shared datasets to bridge the gap between dataset
    creators and dataset consumers while encouraging both groups to be more reflective
    about their decisions [[18]](http://arxiv.org/abs/1803.09010). In the case of
    many datasets, answering such questions might be time-consuming but not difficult;
    in the case of pre-training datasets for LLMs, documenting that for each data
    source might take an eternity due to the quantity and variety. Hugging Face has
    made dataset cards—first referenced in chapter 2—a key feature of their dataset
    documentation, showing metadata specified by the dataset creator that explains
    what that dataset should be used for. A simplified example is shown in figure
    8.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F03_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 A dataset card for the databricks-dolly-15k dataset
  prefs: []
  type: TYPE_NORMAL
- en: Developers of LLMs are both dataset creators and consumers. The datasets that
    they create are in fact supersets of many other datasets, which may or may not
    be well-documented and almost certainly aren’t intended for use in training generative
    models. That’s not necessarily a problem—the only way that machines learn to generate
    language is by ingesting vast amounts of language written for other purposes,
    whether art, humor, or simple information transfer. But when no one knows *what’s*
    in the data, as is often the case with generative models, that content might be
    inaccurate, inappropriate, racist, sexist, transphobic, extremist, or violent.
    It might contain personal information; it might *not* contain necessary context.
    LLM developers probably can’t ensure that none of these problems exist in their
    data, but they should make every effort to determine the safety of their data
    sources and how different data mixtures affect the model. Of course, their responsibility
    doesn’t end there—they will also need training strategies to address the inevitable
    data shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Not only is deeply understanding one’s own data a best practice, but it could
    also become law. The EU’s AI Act is expected to become the first major legislation
    governing the use of AI in the world; in 2023, Reuters reported that lawmakers
    have added a new provision that will focus on documenting and mitigating risks,
    including requiring that generative AI companies use only “suitable” datasets,
    draw up “extensive technical documentation and intelligible instructions for use,”
    and disclose “copyrighted materials within the datasets they use” [[19]](https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.pdf).
    The final addition was targeted at image-generation models, given the news that
    companies like Midjourney have used “hundreds of millions” of copyrighted images
    in their training datasets, but it would apply equally to language models, which
    often also contain copyrighted materials, including books and articles, as well
    as licensed code [[20]](https://petapixel.com/2023/05/01/eu-law-to-force-ai-imagers-to-disclose-copyrighted-photos-in-dataset/).
  prefs: []
  type: TYPE_NORMAL
- en: Protecting data privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to data privacy and generative models, developers, users, policymakers,
    and the general public all have a role to play. Most directly, developers of LLMs
    should make reasonable efforts to avoid training on data sources that are known
    to contain significant amounts of PII. For example, spam classification systems
    have been trained for decades on email datasets with the model learning to predict
    whether or not a particular email is spam. With an LLM, the risk of using email
    datasets is much higher. There is a possibility that the model will generate text
    that it has seen in training, potentially leaking sensitive or confidential information
    such as the credit card and Social Security numbers generated verbatim by an LLM
    trained on corporate emails from chapter 2\. Google, which serves millions of
    users globally with its Google Workspace products, including Gmail and Docs, has
    said it doesn’t use that data to train generative models without user permissions
    [[21]](https://www.vox.com/technology/2023/7/27/23808499/ai-openai-google-meta-data-privacy-nope).
    In the absence of any legal restrictions, however, it’s not hard to imagine that
    a tech company with enormous collections of user data might try to use that to
    gain a competitive advantage—such as personalized email generation based on the
    user’s own messages—despite the privacy implications.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we know Google *does* use is anonymized data for features such as spell-checker
    and Smart Compose, a version of autocomplete available in Docs. Data anonymization
    reduces the risk from training on data containing PII, but privacy-enhancing technologies
    (PETs) such as differential privacy are fairly complicated to implement. Simpler
    methods, such as detecting and obfuscating or writing over sensitive data, have
    weaknesses as well: it’s hard to perfectly find all the PII, and masking that
    data while training an LLM can have unintended consequences when producing generations
    because it doesn’t preserve the statistical properties of the text. We hope that
    the concentrated efforts of researchers in the area of PETs will yield improvements
    that LLM providers can readily adopt.'
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, companies should clearly state their data privacy policies
    and practices and set expectations appropriately with users. At a minimum, they
    should describe what data they are collecting, how they are using or sharing it,
    and how users can opt out or have their data deleted. When using LLMs, especially
    in professional contexts, people should be aware of these policies and think twice
    before inputting any type of private information. Several major employers, including
    Samsung and Amazon, have already restricted their employees’ usage of ChatGPT
    in the workplace because of the data privacy risk.
  prefs: []
  type: TYPE_NORMAL
- en: Although concerns about data privacy in the context of LLMs are relatively new,
    they are far from unique. The collection, exchange, and sale of personal data
    have been key problems as long as the internet economy has existed, and while
    regulation must necessarily be iterative, the General Data Protection Regulation
    (GDPR), enacted by the European Union in 2018, remains the primary framework for
    data governance. The use of that data in training machine learning algorithms,
    addressed in GDPR, has since been subject to additional scrutiny and will remain
    a large component of broader AI governance.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability, transparency, and bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset documentation is just one piece of the transparency puzzle. If LLMs
    and other forms of generative AI are going to be used successfully and responsibly,
    they must be accompanied by some level of performance guarantees. Performance
    can encompass a lot of different metrics and may be something different for each
    LLM, depending on what the developers care most about. Developers can measure
    the capabilities of LLMs against standardized benchmarks and report the results
    on model release (although there are subtle nuances to running these evaluations,
    including formatting changes, that can noticeably change their results). In theory,
    users could then make more informed choices about which LLM to use or whether
    an LLM is suitable for their use case at all.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this point, in table 8.1, we’ve listed the state-of-the-art results
    as of August 2023 on a popular code-generation benchmark called HumanEval. Each
    example in the dataset is a simple programming problem; the key metric, “Pass@1,”
    describes the rate at which each LLM can produce a working answer on the first
    attempt. Thus, if LLMs were being used regularly for code generation, this leaderboard
    could be used to select the highest-performing model (in this case, Reflexion,
    a variant of GPT-4).
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 A leaderboard for code-generation benchmark HumanEval
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | Model | Pass@1 | Link | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Reflexion | 91.0 | [http://mng.bz/g7V8](http://mng.bz/g7V8) | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | GPT-4 | 86.6 | [http://mng.bz/eEDG](http://mng.bz/eEDG) | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Parsel | 85.1 | [http://mng.bz/p1yR](http://mng.bz/p1yR) | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | MetaGPT | 81.7 | [http://mng.bz/OP9j](http://mng.bz/OP9j) | 2023 |'
  prefs: []
  type: TYPE_TB
- en: When Meta and Microsoft announced the release of Llama 2, the successor open
    source LLM after LLaMa, they published a technical paper not only showing how
    Llama 2 compares to other LLMs on academic benchmarks but also detailing their
    pre-training and fine-tuning processes—a radical act in an era of stiff competition
    between LLMs, where even small modifications might be seen as trade secrets. The
    Llama 2 technical report is an instructive document, demonstrating the commitment
    of Llama 2’s creators to transparency. The popular academic benchmarks, detailed
    in chapter 1, consist of datasets such as Massive Multitask Language Understanding
    (MMLU) and TriviaQA, that measure question answering, reading comprehension, and
    other abilities. Even so, it’s not always possible to directly compare the reported
    performances of LLMs on these datasets; one technical paper might include the
    3-shot performance on a task (how well the model does after being given three
    examples), and another might include the 5-shot performance on the same task.
    Because these evaluations can also be sensitive to minor changes such as formatting,
    the more details that are provided in technical reports about the evaluations,
    the easier it is to determine how well various LLMs do.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the pre-trained model Llama 2, Meta and Microsoft fine-tuned
    a model for dialogue, Llama 2-Chat, which is comparable to ChatGPT and other conversational
    agents. To evaluate Llama 2-Chat, they compare responses produced by that model
    with those produced by competitive dialogue agents from the open source community,
    OpenAI, and Google, with both human and model-based evaluations. Human evaluators,
    described as “the gold standard for judging models for natural language generation,”
    were asked to select the better response of a pair, based on helpfulness and safety.
    Model-based evaluations work in a similar way, except the human judge is replaced
    by a reward model, which is calibrated on human preferences. Here, a reward model
    scores inputs according to some reward function it has learned; in this case,
    the reward function estimates human preferences. As the authors note, “When a
    measure becomes a target, it ceases to be a good measure.” The *measure* here
    refers to how well the reward model emulates the humans; they are saying in essence
    that one should not both optimize for a measure (by training the reward model)
    and evaluate with it. To address this, they “additionally used a more general
    reward, trained on diverse open source Reward Modeling datasets” [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
    Reward models are extremely useful for conducting large-scale machine evaluations,
    which can be used to compare models much more quickly and cheaply than the gold
    standard of human evaluations (though even human evaluations are often highly
    subjective, with the potential for disagreement between different raters) [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
  prefs: []
  type: TYPE_NORMAL
- en: As indicated by the two pillars given to human raters, helpfulness and safety,
    the “helpfulness” of a given response (typically understood as its quality or
    accuracy) isn’t the only concern. It’s also crucial that LLM developers measure
    biases present in their models and take steps to address those that are found
    via debiasing techniques such as those discussed in chapter 2\. In section Explainability,
    Transparency, and Bias, we review training strategies to improve model safety;
    it’s impossible to mitigate problems that aren’t measured. This is also an area
    where there are useful benchmarks that provide a means of comparison, and developers
    of LLMs have started to collaborate and share methods and evaluations due to the
    broad importance of the problem of biased or unsafe models. For example, the safety
    benchmark datasets examined in the Llama 2 paper are TruthfulQA, a dataset that
    measures how well LLMs generate “reliable outputs that agree with factuality and
    common sense”; ToxiGen, which measures the “amount of generation of toxic language
    and hate speech across different groups;”; and BOLD, which measures “how the sentiment
    in model generations may vary with demographic attributes” [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
    Llama 2 is far from perfect and certainly can generate misinformation and hate
    speech, but the transparency from its developers is refreshing. The publication
    of its performance on these measures shows both the marked improvement over LLaMa
    as well as how far we have to go.
  prefs: []
  type: TYPE_NORMAL
- en: 'When models make mistakes, we ideally need to be able to interpret how a particular
    message was generated. For LLMs, the simplest way to start determining why the
    model generated some piece of text is to look at which tokens the model *attended*
    to most (for a description of attention in LLMs, see chapter 1, section The Birth
    of Large Language Models: Attention is All You Need). The sheer size of LLMs makes
    many of the existing explainability algorithms functionally impossible to run
    [[23]](https://arxiv.org/pdf/2209.01174.pdf), but the work on how to produce explanations
    of LLM generations more efficiently is ongoing [[24]](https://arxiv.org/pdf/2305.13386.pdf).
    Depending on the LLM’s implementation, the model may query user inputs against
    a vector database that contains lots of embedded examples, and then use the result
    of that query in its generation. Just like the word embeddings discussed in chapter
    1, these embeddings are more compact representations of text data. Vector databases
    can be used to efficiently store any previous conversations with the user; with
    more messages stored, the model should “remember” things from earlier in the conversation
    history, creating a better and more personalized user experience. They can also
    be used to store other types of data that could be useful for the model’s response,
    such as conversation snippets for dialogue agents. For instance, if the user inputs
    the prompt, “What’s that old joke about clowns,” the model would look for highly
    similar requests in its database, and *condition* on any example that it saw,
    meaning that it’s more likely to generate a response close to those in the example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieval-augmented LLMs, mentioned in chapter 6, work similarly except that
    instead of querying an internal data store, they search the web. This is typically
    implemented by fine-tuning the model on datasets containing examples of when to
    search in response to user input and what search term to extract from that input.
    If the LLM searches by querying a search API with the generated search term, the
    model will then condition on the search results when generating its response.
    Consider the case of a prompt like, “What new restaurants should I try on my trip
    to Copenhagen?” The LLM might refer to the vector database and discover a previous
    exchange where the user being a vegan rejected the model’s suggestion of a Brazilian
    steakhouse. Then, the LLM might search “vegan restaurants in Copenhagen” through
    an API and retrieve results from Yelp. Finally, it would generate a natural language
    response: “Based on my research, it looks like Bistro Lupa is a popular option!”
    Figure 8.4 demonstrates how this might work for a retrieval-augmented model with
    access to a vector database. Although not an explanation per se, reviewing the
    results of a query of a vector database or web search can give great insight into
    why a particular response was generated.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F04_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 A schematic diagram for an LLM accessing data stored in a vector
    database and retrieving search results from the web
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, explainability, transparency, and bias evaluations may seem unimportant
    to the function of an LLM, but they are fundamental. Dedicating time to each actively
    leads to better models. Explaining a model can reveal spurious correlations or
    novel insights. Transparency, aside from any legal obligation, can facilitate
    stronger user trust and more information sharing on best practices between LLM
    providers. Surfacing a model’s biases enables the mitigation of those biases,
    leading to more generalizable results. These categories lead to higher-quality,
    fairer, and lower-risk model deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Model training strategies for safety
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The greatest strength of LLMs is their ability to fluently generate responses
    to an infinite number of prompts. Their greatest weakness derives from the fact
    that these responses are probabilistic. In chapter 3, we delineate four different
    strategies for controlling the generation of LLMs, which together cover the pre-training,
    fine-tuning, and post-processing stages. Improving the safety of LLMs is an active
    area of research, and there are many ways to incorporate safety principles into
    the models, especially in the pre-training and fine-tuning stages.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the creators of Llama 2 described excluding data from “certain
    sites known to contain a high volume of personal information about private individuals”
    in an attempt to prevent the model from encoding this information. When examining
    their pre-training data for the prevalence of certain pronouns and identities,
    they also found that *He* pronouns were overrepresented compared to *She* pronouns,
    *American* was by far the most prevalent nationality, and *Christian* was the
    most represented religion. The dataset was about 90% English, indicating that
    “the model may not be suitable for use in other languages” [[22]](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/).
    One could imagine creating a pre-training dataset that is balanced across gender,
    language, nationality, or religion, but producing such a dataset would be extremely
    time-consuming and potentially require removing data sources, so that the model
    would encode less information overall. While documentation of these imbalances
    isn’t a perfect solution either, it’s helpful to understand the characteristics
    of the data to recognize where downstream biases are likely to arise.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is pre-trained, reinforcement learning from human feedback (RLHF)
    or other fine-tuning methods should be employed to ingrain policies governing
    what types of content should not be generated into the model. Though the specific
    methods may vary, this will typically involve gathering data that shows proper
    and improper responses to user inputs, then producing new responses and labeling
    them, where the labelers are trained on the specific set of desired content policies.
    Over time, we expect that fine-tuning will rely less on human labelers and preferences.
    As models approach and exceed human-level labeling performance, we’re increasingly
    able to use models to capture these preferences and even to critique generations,
    as is done with reinforcement learning from AI feedback (RLAIF), and rewrite them
    to be compliant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, an option that LLM developers may pursue is post hoc detection, where
    a safety classifier is deployed within the generation pipeline as a final hurdle
    before an unsafe response is sent to the user. This will increase the latency
    of the model and might mean a less “helpful” model if there are false positives
    from the classifier that cause safe responses to be overwritten. For example,
    a response about a sensitive topic (“How can I last longer in bed?”) might get
    flagged by a safety classifier accidentally even if it was both helpful to the
    user and not technically against the content policies. Beyond post-processing,
    we recommend that all LLM developers monitor the safety of the responses sent
    by their model. An asynchronous safety classifier could help to identify any major
    shifts in the distribution of messages generated by the LLM, as could sampling
    the messages to look for content violations. Each of these can be done in a way
    that preserves the privacy of the users: both the generative models and classifiers
    could be trained and fine-tuned on anonymized data, preventing the association
    of unsafe material with any particular user.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the safety mitigations put in place by LLM providers, many of these
    models have also been shown to be vulnerable to adversarial attacks that can alter
    the model’s behavior. Sometimes referred to as “jailbreaking” or “prompt jailbreaking,”
    these attacks reflect the difficulty of creating a safe model that is robust to
    unseen contexts and unusual inputs (see [https://llm-attacks.org/](https://llm-attacks.org/)
    and chapter 5 for examples). Although it’s typically straightforward to patch
    a model against a specific attack through the collection and labeling of a small
    amount of additional data, it’s not at all clear that such behaviors could ever
    be fully resolved. The authors of a paper on adversarial attacks put it this way:'
  prefs: []
  type: TYPE_NORMAL
- en: Analogous adversarial attacks have proven to be a very difficult problem to
    address in computer vision for the past 10 years. It is possible that the very
    nature of deep learning models makes such threats inevitable. Thus, we believe
    that these considerations should be taken into account as we increase usage and
    reliance on such AI models. ([https://llm-attacks.org/](https://llm-attacks.org/))
  prefs: []
  type: TYPE_NORMAL
- en: Rather than giving up on these threats as inevitable, though, LLM developers
    concerned with safety can and should endeavor to make such attacks harder to find
    and easier to fix.
  prefs: []
  type: TYPE_NORMAL
- en: We know that LLMs can generate misinformation, hate speech, discriminatory stereotypes,
    personal information, and other undesirable outputs. For some malicious users,
    this is a feature, not a bug; we discuss in chapter 5 how LLMs can be misused
    for a variety of nefarious purposes. The existence of malicious users motivates
    the implementation of safety mechanisms, but if these techniques are executed
    well, the general public using LLMs personally and professionally should be unaffected
    by them. Helpfulness and harmlessness *are* in tension (the safest possible model
    is the one that never generates anything), but with the proper prioritization,
    a highly capable model can also be extremely safe.
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Synthetic media generated by AI, including text, images, audio recordings, and
    videos, have the potential to severely disrupt our information ecosystem. As we’ve
    noted, generative AI can be abused to create deepfakes and produce misinformation
    or propaganda on a massive scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Detecting LLM-written text has proven to be a more difficult task for models
    to learn than generating the text itself. According to a 2023 article about OpenAI’s
    classifier detection tool:'
  prefs: []
  type: TYPE_NORMAL
- en: In January, artificial intelligence powerhouse OpenAI announced a tool that
    could save the world—or at least preserve the sanity of professors and teachers—by
    detecting whether a piece of content had been created using generative AI tools
    like its own ChatGPT. Half a year later, that tool is dead, killed because it
    couldn’t do what it was designed to do. [[25]](https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool)
  prefs: []
  type: TYPE_NORMAL
- en: Since its inception, the tool had shown low accuracy in detecting machine-generated
    content, but at the time, OpenAI expressed hope that it would still be useful
    as a starting point. As LLMs have only become more advanced during that period,
    it already appears impossible to distinguish text from LLMs post hoc. The synthetic
    media created by image-, audio-, and video-generation models remains in some cases
    detectable, via the methods discussed in chapter 4, but even in those domains,
    the gaps are closing quickly.
  prefs: []
  type: TYPE_NORMAL
- en: One active area of research is how to embed a proof of machine generation within
    the synthetic media so that viewers can determine the origin of that content.
    In chapter 6, we introduced the concept of watermarking the output of LLMs, which
    would make that output statistically distinguishable from standard, human-written
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, watermarking for machine-generated text is unlikely to ever
    be a perfect solution. To be effective, any watermarking solution would need to
    be adopted across the industry and made available to the public to check pieces
    of content. But if such a solution were made available to the public to verify
    messages, it could also be used by people to repeatedly check their own machine
    generations and alter them slightly—perhaps changing a few words at a time—until
    the message passes the watermark test. Besides this shortcoming, companies might
    be unwilling to adopt watermarking in the first place: the models produce text
    by predicting the next most likely word, but watermarking overrides these probabilities,
    preferencing certain words above others. Therefore, producing text with a watermark
    might also mean that the LLM is less factual or generates lower-quality responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other limitations apply to the watermarking of synthetic images, videos, and
    other types of media. DALL E, OpenAI’s text-to-image model, uses a visible watermark,
    but there are countless tutorial blog posts instructing users on how to remove
    it from images that they create with the tool. Sam Gregory, a program director
    at the nonprofit Witness, told *Wired* magazine that “There’s going to be ways
    in which you can corrupt the watermark,” pointing out that some visual watermarks
    become ineffective when the image is merely resized or cropped. Another concern
    with visual watermarks is that malicious actors could imitate them, placing the
    logos on real content to make it seem fake. The liar’s dividend is alive and well:
    Gregory said that most cases Witness sees on social media aren’t deepfakes, but
    real videos that people are claiming are generated by AI [[26]](https://www.wired.com/story/ai-watermarking-misinformation/).'
  prefs: []
  type: TYPE_NORMAL
- en: The Coalition for Content Provenance and Authenticity (C2PA), introduced in
    chapter 5, aims to establish “an open technical standard providing publishers,
    creators, and consumers the ability to trace the origin of different types of
    media” (see [https://c2pa.org/](https://c2pa.org/)). The C2PA implementation records
    the provenance information, such as the date, geographic location, and device
    used to take a photo or video recording, as well as the information associated
    with any subsequent edits. This information is protected via a digital signature,
    a cryptographic technique used in online contracts and other secure transactions.
    Widespread use of the C2PA standard would allow viewers to inspect the origin
    and records associated with any piece of media they encountered online, but adoption
    remains a hurdle. Still, it would be technically possible to apply the same process
    to synthetic images as well, provided that generative AI developers integrated
    the cryptographic techniques into their systems. As with other safety mitigations,
    many of the largest AI developers will no doubt incorporate watermarks in the
    synthetic media generated by their models—seven companies, including OpenAI, Google,
    Microsoft, and Anthropic, have already committed to doing so—but these methods
    won’t decisively determine the provenance for all content.
  prefs: []
  type: TYPE_NORMAL
- en: Boundaries for user engagement and metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a 2018 paper published by researchers at Microsoft entitled, “From Eliza
    to Xiaolce: Challenges and Opportunities with Social Chatbots,” the authors trace
    the development of social chatbots through the present day. They write:'
  prefs: []
  type: TYPE_NORMAL
- en: Conversational systems have come a long way since their inception in the 1960s…
    To further the advancement and adoption of social chatbots, their design must
    focus on user engagement and take both intellectual quotient (IQ) and emotional
    quotient (EQ) into account. Users should want to engage with a social chatbot;
    as such, we define the success metric for social chatbots as conversation-turns
    per session (CPS). [[27]](https://arxiv.org/pdf/1801.01957.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Lest we forget, the creator of ELIZA, Joseph Weizenbaum, intended the tool as
    a therapeutic aid and was dismayed to realize the extent to which people anthropomorphized
    it. One tends to think that Weizenbaum would not have viewed CPS as the measure
    of its success. The fact that CPS is defined to be the metric *du jour* of social
    chatbots illustrates a profound failure of the imagination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Social chatbots, including Xiaolce, Replika, and Character.AI, have millions
    of users who seek out conversations with the bots for the companionship, romance,
    or entertainment that they provide. It’s certainly true that these agents must
    combine IQ and EQ: if the agent was heavily indexed toward IQ but not EQ, people
    would be able to ask it factual questions or for coding assistance, for example,
    but would be unlikely to develop a deeper relationship with it. If the agent didn’t
    possess enough IQ, it wouldn’t be able to hold an interesting conversation at
    all. Beyond a base level of functionality, though, it’s primarily EQ that gives
    social chatbots the capabilities their users value most: the responses that make
    them feel less lonely, the practice of small talk to alleviate social anxiety,
    or simply an outlet to vent.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s in these interactions that social chatbots are most valuable, so it’s these
    interactions that should be understood and improved. In chapter 7, we recommend
    alternative metrics that chatbot providers could use to measure success, such
    as defining valuable sessions instead of simply using session length as an indicator.
    This requires additional work, but it can circumvent the shortcomings of purely
    engagement-based metrics and provide insights into how people are using the chatbots,
    which is crucial for their developers to know to ensure responsible deployment
    of the technology.
  prefs: []
  type: TYPE_NORMAL
- en: In that vein, chatbot providers should also endeavor to recognize when usage
    is unhealthy to prevent people from forming dependency relationships with the
    models. As the stories in chapter 7 show, these tools can improve people’s moods
    and confidence, and reduce anxiety and loneliness. But there is much we still
    don’t know about human-AI connections, and if these relationships replace interpersonal
    connections on a long-term basis, there are reasons to believe it could have substantial
    negative effects on emotional development. Again, to avoid building dependency
    in users necessitates optimizing metrics other than engagement, and means more
    work for developers. Ultimately, we believe this effort is worth it for the social
    benefit and to sustain user trust.
  prefs: []
  type: TYPE_NORMAL
- en: Humans in the loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Humans remain an integral part of building and maintaining AI systems. Consider
    how many different people were involved in the creation of ChatGPT. There were,
    of course, the OpenAI engineers in San Francisco. Likely, there were many more
    contractors who selected good responses to help train the chatbot; there might
    have been specialists brought in to red-team certain topics. We know that there
    were Kenyan data labelers paid $1 to $2 an hour to review hate speech and sexual
    abuse content. There were the authors of the millions of words that ChatGPT was
    trained on, from Shakespeare to anonymous Redditors, and the people whose labor
    allowed ChatGPT to learn to write news articles, emails, speeches, and code. Maybe
    something you wrote is in there! And the users of ChatGPT, like other LLMs, also
    play a key role in improving the product over time.
  prefs: []
  type: TYPE_NORMAL
- en: To the extent that LLMs have expertise, it’s human expertise. What the technology
    provides is a way of representing information from more documents than any person
    could ever read, much less organize in their mind, and using that information
    to generate text (usually responses to inputs) at a scale no person ever could.
    What the technology doesn’t provide is meaning; the model doesn’t *know*. That
    is typically acceptable for producing a song about a rabbit who loves carrots,
    but it isn’t acceptable in high-stakes applications ranging from medical diagnoses
    to legal argumentation. As we talked about in chapter 6, these types of applications
    still need a human in the loop to identify the model’s mistakes. LLMs are tools
    that we can use to do parts of our jobs more quickly and easily, and maybe sometimes
    even better, but we still need to build expertise to correct and improve these
    models.
  prefs: []
  type: TYPE_NORMAL
- en: As we navigate the shifting roles of ourselves and AI in education and professional
    fields, thorny questions will inevitably arise. Our collective ability to answer
    them will depend on a sociotechnical response, rather than technology alone. In
    privacy, for example, there is a tremendous amount of technical progress being
    made, such as new start-ups that use generative models to create synthetic datasets
    with the same statistical properties as real datasets. Illumina, a genetic sequencing
    company, announced a partnership with the synthetic data start-up Gretel.ai to
    create synthetic genome data that could be extremely useful in healthcare, without
    divulging any individual’s genetic information. But there is momentum behind these
    efforts because of the social aspect of privacy—activism around the problem, increased
    public awareness, confronting and rejecting social norms of mass data collection,
    and finally a stricter regulatory environment. This must continue with responsible
    AI and related movements.
  prefs: []
  type: TYPE_NORMAL
- en: Making positive change that encourages the responsible use of technology also
    requires that people are at least generally aware of how these technical systems
    work and how they are presently used. Digital literacy is a group effort. Companies
    that provide solutions powered by LLMs must not try to sell users magic, but work
    to educate them on the capabilities and limitations of the models. Schools should
    aim to prepare their students for the world of today, rather than ignoring or
    punishing the use of modern technologies, including LLMs. This book is our hopeful
    contribution toward a populace that is informed and considerate about generative
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI regulations: An ethics perspective'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the best practices discussed in section Best Practices for Responsible
    Use are vital, they aren’t enough. We also need balanced guidance from the government,
    informed by industry, academia, and civil society, and methods to enforce accountability.
    Government entities around the world are increasingly recognizing the need for
    guidelines and frameworks that govern the development, deployment, and use of
    AI systems. The ultimate goal of regulations is to strike the perfect balance
    between promoting innovation and ensuring the development of responsible and ethical
    AI systems. These regulations often aim to address shared concerns about data
    privacy, algorithmic transparency, bias mitigation, and accountability. In this
    section, we’ll talk about the AI regulatory landscape in North America, the European
    Union, and China, as well as discuss corporate self-governance. We focus on these
    regions due to the concentration of big technology companies in the United States
    and China and their preeminent roles in global AI development, while the European
    Union is the world’s leading tech regulator.
  prefs: []
  type: TYPE_NORMAL
- en: North America overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the United States and Canada, the predominant approach at the federal level
    has been to establish best practices at the agency level and sometimes in collaboration
    with leading tech companies and civil society groups. The latter approach is exemplified
    by the July 2023 announcement from the Biden administration that it had secured
    commitments from seven AI companies—OpenAI, Microsoft, Google, Amazon, Meta, Anthropic,
    and Inflection—to comply with a set of voluntary principles. The principles, depicted
    in figure 8.5, include “ensuring products are safe before introducing them to
    the public” through internal and external testing for safety and information-sharing
    on risk management; “building systems that put security first” with appropriate
    cybersecurity and insider threat safeguards and vulnerability reporting; and “earning
    the public’s trust,” a broad category that references efforts to develop watermarking
    systems and public reporting on the capabilities and limitations of publicly released
    AI systems [[28]](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the commitments are voluntary, some critics argued that the announcement
    produced more of a halo effect for the companies rather than meaningful change.
    Kevin Roose, a technology reporter at the *New York Times*, reviewed each principle
    in the press release to assess how significant the commitments were. Roose’s primary
    critique was that the listed principles are vague and don’t specify what kind
    of testing and reporting must be done, leaving lots of wiggle room. He concluded:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the White House’s deal with AI companies is more symbolic than substantive.
    There is no enforcement mechanism to make sure companies follow these commitments,
    and many of them reflect precautions that AI companies are already taking. Still,
    it’s a reasonable first step. And agreeing to follow these rules shows that the
    AI companies have learned from the failure of earlier tech companies, which waited
    to engage with government until they got into trouble. [[29]](https://www.nytimes.com/2023/07/22/technology/ai-regulation-white-house.xhtml)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F05_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 The three pillars of voluntary commitments made to the White House
    by leading AI companies
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, some of the commitments appear to be directly motivated by events that
    have transpired already. The second pillar, building systems that put security
    first, specifically calls out the protection of “proprietary and unreleased model
    weights.” As described in chapter 1, the weights of an LLM are the end product
    of its training. Access to model weights effectively enables the reproduction
    of the model itself. After the model weights of Meta’s LLaMA were leaked on 4chan
    days after the public release, 4chan users were able to quickly produce a high-quality
    LLM based on LLaMA. The memo doesn’t otherwise mention the open source development
    of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: These particular principles are mostly focused on generative AI products, but
    other government bodies have long concerned themselves with the potential negative
    effects of earlier AI systems, particularly those related to bias and transparency.
    For example, the Equal Employment Opportunity Commission (EEOC) has issued guidance
    on how the Civil Rights Act of 1964 applies to automated, AI-based systems used
    in HR functions such as résumé screening, candidate selection, and performance
    monitoring. Essentially, the office stated that the burden of compliance would
    fall on employers who use these tools, with recommendations to verify that vendors
    had evaluated whether their models cause a “substantially lower selection rate
    for individuals with a characteristic protected by Title VII,” such as individuals
    of a particular race or gender [[30]](https://www.eeoc.gov/newsroom/eeoc-releases-new-resource-artificial-intelligence-and-title-vii).
    The Federal Trade Commission (FTC) has also demonstrated an appetite for oversight
    of automated decision-making, writing in a 2021 blog post that the FTC Act, which
    prohibits “unfair or deceptive practices,” would explicitly include the sale or
    use of racially biased algorithms. In addition to models used in employment decisions,
    models related to housing, credit, and insurance decisions would potentially be
    subject to scrutiny under the Fair Credit Reporting Act (see [http://mng.bz/JgKQ](http://mng.bz/JgKQ)).
    The Government of Canada issued a Directive on Automated Decision-Making in 2019
    that included assessments of negative outcomes from automated decision-making
    systems (see [http://mng.bz/mVn0](http://mng.bz/mVn0)). Although generative AI
    models weren’t the target of these issuances, they would be similarly scrutinized
    if used in any of the aforementioned sectors.
  prefs: []
  type: TYPE_NORMAL
- en: In October 2023, the White House followed the voluntary commitments it had secured
    with an executive order on AI, designed to require AI companies to share safety
    evaluations and other information with the government and to take precautions
    to ensure that the models could not be used for engineering “dangerous biological
    materials” or enabling “fraud and deception.” (see [http://mng.bz/6nM5](http://mng.bz/6nM5)).
    The Biden administration has also published more abstract rules for the development
    of AI. Perhaps its landmark text on the subject is the Blueprint for an AI Bill
    of Rights, authored by the White House Office of Science and Technology Policy
    (OSTP) (see [http://mng.bz/wv8g](http://mng.bz/wv8g)). Summarized in figure 8.6,
    that document is centered around the five principles of “safe and effective systems,”
    outlining evaluation and risk mitigation standards; “algorithmic discrimination
    protections,” or identifying potential biases in the model or system; “data privacy,”
    the rights of users to have both information and agency concerning how their data
    is collected; “notice and explanation” about the use of automated systems; and
    “human alternatives, considerations, and fallback” for when people opt out of
    automated systems or to remedy any mistakes made by the system. Like the more
    recent set of AI principles, these are each relatively uncontroversial and vague
    enough to leave some uncertainty over what each might look like in practice. The
    AI Bill of Rights is a positioning document rather than a directive, and the OSTP
    is a policy office. The details of implementations of things such as explanations
    (“Automated systems should provide explanations that are technically valid, meaningful,
    and useful to you and to any operators or others who need to understand the system,
    and calibrated to the level of risk based on the context”) remain to be worked
    out. The closest the US government has come to attempting that is the National
    Institute for Standards and Technology’s AI Risk Management Framework (AI RMF),
    released on January 26, 2023, but even that framework is quite broad and general,
    intended as a starting point. The AI RMF details that AI systems should be “valid
    and reliable,” “safe,” “secure and resilient,” “accountable and transparent,”
    “explainable and interpretable,” and “fair—with harmful bias managed,” but leaves
    how this should be achieved mostly as an exercise for the reader [[31]](https://www.nist.gov/itl/ai-risk-management-framework).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F06_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 The five principles listed by the OSTP as a “Blueprint for an AI
    Bill of Rights” (see [http://mng.bz/wv8g](http://mng.bz/wv8g))
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past, policymakers have expressed an ambivalence toward regulating AI
    companies. On one hand, representatives such as Mike Gallagher, a Republican congressman
    from Wisconsin, have hoped to avoid stifling the innovation that these tech companies
    bring. “The tension underlying all of this is that we don’t want to overregulate
    our advantage in the AI race out of existence,” said Gallagher, advocating for
    a “clinical, targeted” approach, rather than something more comprehensive. “Congress
    rarely does comprehensive well” [[32]](https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/).
    On the other hand, as evidenced in the Judiciary Committee hearing from chapter
    5, more than a few members are concerned that the present state of self-regulation
    will be insufficient, and some have expressed openness to comprehensive AI legislation.
    Representative Ro Khanna of California said:'
  prefs: []
  type: TYPE_NORMAL
- en: On a broad scale, we need some form of human judgment in decision-making. We
    need some sense of transparency when it comes to understanding what AI is being
    used for and the data sets that are being used. We need to have a safety assessment. . . . But
    I think the details of this really need to be worked out by people with deep knowledge
    of the issues. [[32]](https://www.rollcall.com/2023/07/19/gallagher-advocates-targeted-approach-to-ai-regulation/)
  prefs: []
  type: TYPE_NORMAL
- en: There may be bipartisan support for some of the governance measures suggested
    by the principles in the AI Bill of Rights and a more recent set of commitments,
    though the prospect of passing federal legislation in the United States is far
    from certain. LLM developers recognize that their biggest regulatory threat is
    across the Atlantic Ocean, in the European Parliament.
  prefs: []
  type: TYPE_NORMAL
- en: EU overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On June 14, 2023, the European Parliament overwhelmingly approved their version
    of the EU’s AI Act, setting the stage to pass the final version of the law on
    an expedited timeline by the end of the year [[33]](https://www.europarl.europa.eu/news/en/press-room/20230609IPR96212/meps-ready-to-negotiate-first-ever-rules-for-safe-and-transparent-ai).
    The AI Act would be one of the first major laws to regulate AI and serve as a
    potential model for policymakers around the world.
  prefs: []
  type: TYPE_NORMAL
- en: The AI Act implements a risk-based approach to AI regulation, focusing on AI
    applications that have the greatest potential for harm to society. In other words,
    the different risk levels will denote how much that technology is regulated and
    where high-risk AI systems will require more regulation. A limited set of AI systems
    that are deemed as *unacceptable risk* will be completely banned for violating
    fundamental human rights, which include cognitive behavioral manipulation of people
    of specific vulnerable groups, social scoring, and real-time and remote biometric
    identification systems (with major exceptions) [[34]](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence).
    For example, a voice-activated toy that encourages violent behavior in children
    would fall under this category and be banned.
  prefs: []
  type: TYPE_NORMAL
- en: One level below AI systems with unacceptable risk are *high-risk* AI systems,
    which negatively affect safety or fundamental rights (as protected by the EU Charter
    of Fundamental Rights). These include regulated consumer products and AI used
    for socioeconomic decisions, such as law enforcement, hiring, educational access,
    and financial services access, among others. All high-risk AI systems will not
    only be assessed before they go to market but also throughout their lifecycle.
    These systems would have to meet data governance, accuracy, and nondiscrimination
    standards. They would additionally need to implement a risk-management system,
    record-keeping, technical documentation, and human oversight. The AI systems would
    also need to be registered in an EU-wide database, which would not only create
    transparency within the number of high-risk AI systems but also regarding the
    extent of their societal effect [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
  prefs: []
  type: TYPE_NORMAL
- en: Then, *limited risk* systems would have to comply with transparency requirements
    to help users make informed decisions. These requirements include making the user
    aware if they are interacting with AI, such as deepfakes, emotion recognition
    systems, or chatbots. The AI Act has an additional callout for *generative AI*,
    requiring transparency in disclosing AI-generated content, preventing the model
    from generating illegal content, and publishing summaries of copyrighted data
    used for training.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, *minimal risk* includes AI applications such as video games or spam
    filters. These are proposed to be mainly regulated by voluntary codes of conduct.
    Figure 8.7 illustrates the AI Act’s risk levels. However, at the time of this
    writing, European policymakers haven’t decided where foundational LLMs fall within
    this framework, and this subject is currently being debated.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the AI Act isn’t the only major legislation in the EU to manage AI
    risk. In chapter 3, we briefly discussed the General Data Protection Regulation
    (GDPR), which requires companies to protect the personal data and privacy of EU
    citizens. The AI Act isn’t meant to replace GDPR, but complement it. In addition
    to data privacy implications, GDPR also contains two articles that affect machine
    learning systems. First, “GDPR states that algorithmic systems should not be allowed
    to make significant decisions that affect legal rights without any human supervision”
    [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
    An example of this was seen in 2021 when Uber, an American transportation company,
    was required to reinstate six drivers in the Netherlands who “were unfairly terminated
    by algorithmic means” [[36]](https://techcrunch.com/2021/04/14/uber-hit-with-default-robo-firing-ruling-after-another-eu-labor-rights-gdpr-challenge/).
    Second, “GDPR guarantees an individual’s right to *meaningful information about
    the logic* of algorithmic systems, at times controversially deemed a *right to
    explanation*” [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
    Put simply, EU consumers have the right to ask companies that make automated decisions
    based on their personal data, such as home insurance providers, how or why certain
    decisions were made.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH08_F07_Dhamani.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 The four risk categories in the AI Act
  prefs: []
  type: TYPE_NORMAL
- en: As part of its efforts to regulate digital technologies, regulations that the
    EU has already passed include the Digital Services Act (DSA) and the Digital Market
    Act (DMA). Passed in November 2022, the DSA applies to online platforms and search
    engines, requiring companies to assess risks, outline mitigation efforts, and
    undergo third-party audits for compliance [[37]](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package).
    The most stringent regulations under the DSA only apply to very large online platforms
    (VLOPs), which focuses most of the regulation on platforms that have the most
    reach and influence on EU citizens. One of DSA’s goals is to force large platforms
    to be more transparent, particularly with algorithmic accountability and content
    moderation. These transparency requirements will help identify any systematic
    risks that come from the design and provision of services. For example, if an
    AI content recommendation system contributes to the spread of disinformation,
    the company may face fines under the DSA. The EU’s approach to targeting VLOPs
    is interesting because of its potential to undermine the innovation argument against
    regulation—which is, how will companies continue to innovate when faced with strict
    regulations? By targeting VLOPs, smaller businesses are free from the burden of
    complying with some parts of the DSA so they can still innovate, but if and when
    they become a large force in society, they will also be required to think about
    how they are using their data and how their platform is affecting their users.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the DMA is aimed at increasing competition in digital marketplaces.
    The DMA targets “gatekeepers,” which are corporate groups that significantly affect
    the internal market, namely, big technology companies. Here, the gatekeepers will
    be subject to an additional level of regulation over other companies. For example,
    they will be restricted from sharing data across their services without user consent,
    barred from self-preferencing their own products and services, and obliged to
    share additional information with advertisers on how their ads perform [[35]](https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/).
    The DMA will likely affect how the big technology players handle data, as well
    as how AI systems handle search engine ranking and ordering of products on e-commerce
    platforms. Despite not primarily focusing on AI, the DSA and DMA laws also help
    govern AI models and demand increased transparency from technology companies.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve highlighted the European Union’s efforts to develop a coherent approach
    to AI governance and standards. In particular, the AI Act has the potential to
    become the de facto global standard for regulating AI. There are clear strengths
    to the EU’s approach to AI regulation, particularly the risk-based methodology,
    but there are a few challenges as well. Notably, it will foster an ecosystem of
    independent audits, which will likely result in more transparent, fair, and risk-managed
    AI applications. There are, however, open questions as to the extent the legislation
    can adapt to new capabilities and risks as they arise, as well as manage the longer-term
    societal effects of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, the EU’s goal is to provide a regulatory framework for AI companies
    and organizations that use AI, as well as facilitate a balance between innovation
    and the protection of citizens’ rights. However, their success will depend on
    a well-conceived enforcement structure and their ability to create an AI auditing
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: China overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of 2023, China has introduced three comprehensive and targeted machine learning
    and AI regulations: its 2021 regulation on recommendation algorithms, the 2022
    rules for deep synthesis (deepfakes), and the 2023 draft rules on generative AI.
    These legislations create new rules for how AI systems are built and deployed,
    as well as what information AI developers must disclose to the government and
    the general public.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the European Union, China has taken an iterative and vertical regulatory
    approach toward AI governance. For example, the AI Act is a horizontal regulation,
    aiming to cover all applications of a given technology. On the other hand, vertical
    regulations target a specific application of a given technology. Here, Chinese
    regulators impose requirements specific to their concerns, and if deemed inadequate
    or flawed, they release new regulations to fill in the gaps or expand on existing
    legislation. While China’s iterative process toward regulation can be confusing
    or challenging for AI developers to maintain compliance, Chinese regulators view
    that as a necessary trade-off in a fast-moving technology environment.
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, China’s regulation on algorithmic recommendation systems marked the
    start of restrictions on AI and machine learning systems. Initially motivated
    by the Chinese Communist Party’s (CCP) concern about the role of algorithms disseminating
    information online, the set of regulations reins in the use and misuse of recommendation
    algorithms [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    The regulations demand transparency over how algorithms function and provide users
    more control over which data the companies can use to feed the algorithms, as
    well as mandate that the recommendation service providers “uphold mainstream value
    orientations” and “actively transmit positive energy online” [[39]](https://www.chinalawtranslate.com/algorithms/).
    The regulation also requires platforms to prevent the spread of undesirable or
    illegal information and manually intervene to ensure they reflect government policies.
  prefs: []
  type: TYPE_NORMAL
- en: The CCP also identified deepfakes as a threat to the information environment.
    We should note that, unlike the United States, which has very strong free expression
    guarantees in its constitution, or even the European Union, the information environment
    in China is more controlled and restricted by comparison. The spectrum of what
    classifies as an information “threat” in China is quite broad. For example, criticism
    of the state or CCP would be considered a threat to the information environment.
  prefs: []
  type: TYPE_NORMAL
- en: In 2022, China introduced the Deep Synthesis Provisions, which include algorithms
    that synthetically produce images, text, video, or voice content. The regulation
    calls for adding labels or tags on synthetically generated content, and includes
    vague censorship requirements, such that it must “adhere to correct political
    direction” and “not disturb economic and social order” [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    It further requires deep synthesis service providers to take measures for personal
    data protection, technical security, and transparency. The regulation was finalized
    on November 25, 2022, just five days before the public release of ChatGPT [[40]](https://www.china-briefing.com/news/china-to-regulate-deep-synthesis-deep-fake-technology-starting-january-2023/).
  prefs: []
  type: TYPE_NORMAL
- en: Despite China being ahead of the curve with generative AI technology, they were
    faced with the unfortunate timing of ChatGPT’s release. The Deep Synthesis Provisions
    were deemed insufficient by the Cyberspace Administration of China (CAC) given
    that they were designed to regulate deepfakes and not text generated from LLMs.
    The regulation also covered only content-generation services provided through
    the internet, which created a regulatory gap in content that was being generated
    using AI offline. So, the Chinese regulators set out to quickly iterate on the
    same set of AI applications but with new concerns in mind.
  prefs: []
  type: TYPE_NORMAL
- en: In April 2023, the CAC released draft measures on Generative AI Services. The
    draft builds on the Deep Synthesis Provisions, which took effect in January 2023,
    and applies to all machine-generated content online and offline [[41]](https://www.chinalawtranslate.com/overview-of-draft-measures-on-generative-ai/).
    The initial draft had several difficult-to-meet requirements, including that training
    data must be truthful, accurate, and diverse, as well as not violate any intellectual
    property rights [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    A key question was whether the rules may end up suppressing innovation in the
    AI industry of a country that aims to become the world leader in this space. After
    an active public debate, the interim measures, set to take effect on August 15,
    2023, relaxed a few previously announced provisions and said that Chinese regulators
    would support the development of the technology [[42]](https://www.reuters.com/technology/china-issues-temporary-rules-generative-ai-services-2023-07-13/).
    The interim rules only apply to services that are available to the general public
    in China, which exempts any technology being developed in research institutions
    or intended for use by overseas users.
  prefs: []
  type: TYPE_NORMAL
- en: China’s vertical and iterative approach to AI regulation reveals both strengths
    and vulnerabilities. The strength of the vertical approach is the ability to create
    precise solutions or mitigations for specific problems. However, regulators are
    forced to develop new regulations for new applications or problems, as seen with
    the Deep Synthesis Provisions. Because of prior experience with AI governance
    and utilization of regulatory frameworks from past vertical regulations, CAC was
    able to quickly iterate on the Deep Synthesis Provisions to draft rules for generative
    AI, showcasing speed as another area of strength.
  prefs: []
  type: TYPE_NORMAL
- en: In June 2023, China’s State Council (the equivalent of the US cabinet) announced
    that they would draft an Artificial Intelligence Law, a comprehensive, horizontal
    piece of legislation building upon existing regulations. This suggests that Chinese
    AI regulation is approaching a turning point, echoing the evolution of Chinese
    regulations governing the internet. Initially, the internet in China was governed
    by narrow and specific regulations, which later matured into the Cybersecurity
    Law of 2017, a broad and comprehensive framework that was built upon existing
    laws [[38]](https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117).
    Following a similar playbook as their approach to internet regulation, if the
    draft of the Artificial Intelligence Law is adopted, it will be China’s first
    national AI legislation.
  prefs: []
  type: TYPE_NORMAL
- en: Corporate self-governance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in section North America Overview, the Biden-Harris administration
    secured voluntary commitments in July 2023 from seven leading AI companies—Amazon,
    Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI—to ensure the safe,
    secure, and transparent development of AI technology. However, this agreement
    has been both praised and criticized—is this a step forward or an empty promise?
  prefs: []
  type: TYPE_NORMAL
- en: 'On the surface, the voluntary commitment looks promising, but the phrasing
    of the terms is fairly vague and largely seems to reinforce what the seven companies
    are already doing: working on the safety of AI systems, investing in cybersecurity,
    and aiming for transparency. The agreement is also voluntary, which doesn’t assign
    responsibility to ensure that the companies abide by the terms nor does it hold
    them accountable for noncompliance. However, it’s worth noting that companies
    would likely feel pressured to participate, especially given the alternative threat
    of rigid regulation.'
  prefs: []
  type: TYPE_NORMAL
- en: On the plus side, however, a voluntary commitment helps the administration avoid
    strict, difficult to comply with regulations that may hinder innovation in the
    United States, as it has in the European Union [[43]](https://techliberation.com/2022/08/01/why-the-future-of-ai-will-not-be-invented-in-europe/).
    The financial sector’s regulatory oversight actually began in industry self-governance.
    In the 17th century, collectives of traders used to meet at rival coffeehouses
    competing with each other on the effectiveness of the ethics rules their members
    had to comply with [[44]](https://dlc.dlib.indiana.edu/dlc/handle/10535/10528).
    These rules persuaded the public to trade with them instead of their rivals. When
    any member broke these ethical rules, the entire collective’s reputation was damaged.
    Consequently, all the members were incentivized to monitor unethical behavior,
    so if any member behaved undesirably, they could be ousted. Eventually, all the
    collectives adopted the rules that best protected the public as the standard.
    These collectives—the original stock traders in London’s Lombard Street—are an
    excellent example of industry self-governance in a sector that is now heavily
    regulated. Once these collectives were able to establish the best standards, the
    monitoring and enforcement of the rules were transferred to a third party, such
    as the government, where the collective members and the third party worked together
    to amend and establish new standards [[45]](https://www.aei.org/technology-and-innovation/white-house-ai-commitments-a-first-step-to-industry-self-governance/).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the Biden-Harris administration’s voluntary commitments give the
    AI companies the freedom to establish their own rules to enforce where perhaps
    the rules that best protect the public will surface, as they did in the financial
    sector. As we’ve said, voluntary commitments merely formalize the commitment for
    AI companies to have best practices. For example, OpenAI doesn’t allow usage of
    their models for illegal activity, or any activity that has a high risk of physical
    or economic harm, among other disallowed uses (see [http://mng.bz/5w9q](http://mng.bz/5w9q)).
    Google has also released additional terms for generative AI with a similar policy
    for blocking any content that violates their prohibited use policy, which includes
    (but is not limited to) any content used to perform or facilitate dangerous, illegal,
    or malicious activities (see [http://mng.bz/6DW5](http://mng.bz/6DW5)). Meanwhile,
    Inflection AI states that “safety is at the heart of our mission” and “our internal
    Safety team continuously pressure tests our models for risks, and works with outside
    experts on comprehensive red-teaming of our technologies” (see [http://mng.bz/o1Xj](http://mng.bz/o1Xj)).
    Even Meta’s Llama 2, which has been open sourced for research and commercial use
    cases, has an acceptable use policy that prohibits certain use cases to help ensure
    that the models are used responsibly (see [https://ai.meta.com/llama/use-policy/](https://ai.meta.com/llama/use-policy/)).
  prefs: []
  type: TYPE_NORMAL
- en: In the United States, it’s also more than likely that market forces will shape
    the governing landscape. The companies will actively work to make sure that their
    LLMs aren’t seen as inadequate—maybe this motivation stems from reports of adversaries
    exploiting the LLM, the general public deeming their data practices untrustworthy,
    or simply trying to avoid embarrassing (and expensive) events such as Google’s
    public release of Bard. Of course, it can certainly be problematic for for-profit
    companies to develop their own governance frameworks when they perhaps may be
    more incentivized by growing a successful business than protecting their users,
    but it’s worth noting that the administration does emphasize involving a diverse
    range of stakeholders (which we’ll further unpack in section Towards an AI Governance
    Framework). At the very least, the voluntary commitments reinforce the notion
    that companies have a responsibility in their commitment to responsible AI development,
    including their potential for affecting society. Encouraging corporate self-governance
    could complement existing or future regulatory efforts, as well as fill in a critical
    gap to develop a more comprehensive approach to the governance of AI systems,
    or any new technology for that matter, especially in its infancy.
  prefs: []
  type: TYPE_NORMAL
- en: Toward an AI governance framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Introduction to Generative AI*, we’ve outlined the AI race, illustrating
    the potential of generative AI technology as well as building awareness around
    its shortcomings. Enthusiasts anticipate that generative AI will disrupt the way
    we engage in work and our personal lives, do business, and create wealth. On the
    other hand, an increasing number of technology experts have shared significant
    concerns regarding the existential dangers of relinquishing tasks and decision-making
    to computers with little use for humans in the near future. Contributing to these
    unsettling concerns is an existing imbalance of power and wealth where critics
    of AI are worried that gains from the technology will disproportionately accumulate
    among the top 1%. As mentioned in chapter 6, we believe that generative AI is
    an evolution, not a revolution, as long as we use and govern the technology responsibly.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we’ve highlighted the pragmatic promises of generative
    AI, from productivity gains to agentized systems. But at the same time, we’ve
    emphasized the risks and limitations of generative AI technology, as well as its
    ability to be misused accidentally and intentionally. As the awareness of AI risks
    has grown, so have the standards and guidance to mitigate them. We’ve come a long
    way, but we have an even longer way to go. We hope and believe that we’ll find
    a balance between groups calling for a pause in training AI systems and those
    claiming that ChatGPT is magic. Regardless of how the global AI disruption unfolds,
    the world won’t become a better place for living, working, or participating in
    democratic processes unless there are measures in place to regulate and govern
    AI’s development, effect, and safeguards.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in section Ethics-Informed AI Regulations, AI governance efforts
    have primarily been undertaken voluntarily, encompassing numerous protocols and
    principles that endorse conscientious design and controlled behavior. This is
    especially true in North America, where the shared goals of big technology companies
    involve aligning AI with human usefulness and ensuring safety throughout the creation
    and implementation of algorithms. Additional goals for AI systems also involve
    algorithmic transparency, fairness in their utilization, privacy and data protection,
    human supervision and oversight, and adherence to regulatory standards. While
    we acknowledge that these are ambitious goals, it’s necessary to highlight that
    AI developers often fall short of these objectives. Companies often have proprietary
    intellectual property for building their AI systems that they don’t disclose in
    order to keep their competitive advantage. For many in the AI ethics community,
    this is an indication that companies are more motivated by financial incentives
    than public benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the early 2020s, the focus on voluntary self-policing by AI companies
    has started to shift toward comprehensive regulations in various countries. In
    a *Wired* article, Rumman Chowdhury wrote, “In order to truly create public benefit,
    we need mechanisms of accountability” [[46]](https://www.wired.com/story/ai-desperately-needs-global-oversight/).
    However, it’s important to note that the majority of discussions concerning AI
    and potential approaches to mitigate unintended negative consequences have been
    primarily focused in the West—the European Union, the United States, or members
    of advanced economies. Of course, the Western focus makes sense given the concentration
    of big AI companies in Silicon Valley, including OpenAI, Google, Meta, and Anthropic.
    But it’s worth emphasizing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The vast majority of discussion about the consequences and regulation of AI
    is occurring among countries whose populations make up just 1.3 billion people.
    Far less attention and resources are dedicated to addressing these same concerns
    in poor and emerging countries that account for the remaining 6.7 billion of the
    global population. [[47]](https://foreignpolicy.com/2023/05/29/ai-regulation-global-south-artificial-intelligence/)
  prefs: []
  type: TYPE_NORMAL
- en: 'So, where do we go from here? How do we truly ensure that generative AI, or
    AI systems in general, are used to better society? In the previously mentioned
    article, Chowdhury says:'
  prefs: []
  type: TYPE_NORMAL
- en: The world needs a generative AI global governance body to solve these social,
    economic, and political disruptions beyond what any individual government is capable
    of, what any academic or civil society group can implement, or any corporation
    is willing or able to do. [[46]](https://www.wired.com/story/ai-desperately-needs-global-oversight/)
  prefs: []
  type: TYPE_NORMAL
- en: 'The risks exposed by generative AI have emphasized what many experts have been
    calling for: the need for a new, permanent, independent, well-funded, and resourced
    institution to holistically ensure public benefit. Chowdhury further states:'
  prefs: []
  type: TYPE_NORMAL
- en: It should cover all aspects of generative AI models, including their development,
    deployment, and use as it relates to the public good. It should build upon tangible
    recommendations from civil society and academic organizations, and have the authority
    to enforce its decisions, including the power to require changes in the design
    or use of generative AI models, or even halt their use altogether if necessary.
    Finally, this group should address reparations for the sweeping changes that may
    come, job loss, a rise in misinformation, and the potential for inhibiting free
    and fair elections potentially among them. This is not a group for research alone;
    this is a group for action. [[46]](https://www.wired.com/story/ai-desperately-needs-global-oversight/)
  prefs: []
  type: TYPE_NORMAL
- en: We should note that we already have an example of a global, independent, and
    well-funded organization that makes decisions for the betterment of society. The
    International Atomic Energy Agency (IAEA) (see [www.iaea.org/](https://www.iaea.org/))
    was formed in the post–World War II era to govern nuclear technologies. IAEA,
    formed under the guidance of the United Nations, is a body independent of governments
    and corporations that provides advisory support and resources. While it has limited
    agency, IAEA shows us that we’ve done this before and that we can do it again.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, recent advances in generative AI have highlighted what many of
    us have known for a long time. We’ll never be able to “solve” the problem of abusing
    or misusing technology. Therefore, instead of only pursuing band-aid technical
    solutions, we need to invest in sociotechnical approaches to address the root
    of the problem. To Chowdhury’s point, the IAEA is a starting point for a global
    governance body, not an end goal. Unlike the IAEA’s limited agency, this body
    should have the ability to make independent and enforceable decisions. It should
    take advisory guidance from AI companies but also collaborate with civil society,
    government, and academia. This body shouldn’t replace any of these entities, but
    it should form a coalition to ensure public benefit in the face of AI. While we
    acknowledge that the effort needed to get to a global governance body for AI is
    substantial, we’re optimistic about the future of AI, and hopeful that AI companies
    and governments will work toward an independent, global body to make decisions
    regarding the governance and effect of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are trained on unfathomable amounts of internet data. They inevitably encode
    bias, harmful stereotypes, and toxicity, as well as copyrighted data and sensitive
    information in their training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs often exhibit biased, toxic, and misaligned responses because of the characteristics
    of the training data. They also regurgitate sensitive or copyrighted information.
    LLMs also hallucinate, that is, they confidently make up incorrect information
    because of how they work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversaries can exploit the vulnerabilities in LLMs to perform training data
    extraction attacks, prompt injections or jailbreaking, or poison data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can help malicious actors carry out personalized and low-cost adversarial
    attacks at scale, as well as lower the barrier of entry for novice threat actors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be accidentally misused in professional sectors by people who don’t
    grasp the limitations of these models, which can result in serious ethical and
    societal consequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not implemented responsibly, AI systems could be used to replace humans with
    machines, drive down wages, worsen the inequality between wealth and income, and
    do little to help overall economic growth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When misused, social chatbots can lead to unhealthy relationship patterns, dependency-seeking
    behaviors, and risk replacing genuine human connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM developers should document training data, be transparent with users about
    data privacy and use, and make efforts to mitigate biases present in their models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector databases and web retrieval provide some additional capabilities to LLMs
    and can be used to help interpret some of the model’s responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs should be trained and evaluated thoroughly for safety and robustness to
    adversarial attacks before public release.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying post hoc if the content was created by a human or a machine will
    soon be a fool’s errand, but there are promising solutions focused on tracking
    the provenance of media.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers of social chatbots can optimize for metrics other than engagement
    to reduce the potential risk for social harms such as dependency or problems in
    emotional development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because LLMs don’t have true knowledge or expertise, they should be typically
    deployed within a human-in-the-loop context, and stakeholders must be literate
    on how these models work before they are blindly used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the near future, we can expect to see generative AI integrated into more
    applications and becoming increasingly agentic, efficient, and personalized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The United States hasn’t attempted a large-scale AI regulation like the EU but
    has instead relied more heavily on corporate self-regulation and voluntary commitments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EU’s AI Act takes a risk-based approach to AI regulation and is one of the
    first major laws to regulate AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2023, China released draft measures on Generative AI Services and announced
    that they would draft an Artificial Intelligence Law, a comprehensive, horizontal
    piece of legislation building upon existing regulations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corporate self-governance could complement existing or future regulatory efforts,
    as well as fill in a critical gap to develop a more comprehensive approach to
    AI systems governance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI companies often fall short of algorithmic transparency, ensuring the safety
    of AI systems, and data protection standards, among others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The risks exposed by generative AI have emphasized what many experts have been
    calling for: the need for a new, permanent, independent, well-funded, and resourced
    institution to holistically ensure public benefit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
