- en: 6 Fine-tuning for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing different LLM fine-tuning approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a dataset for text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying a pretrained LLM for fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning an LLM to identify spam messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the accuracy of a fine-tuned LLM classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a fine-tuned LLM to classify new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we have coded the LLM architecture, pretrained it, and learned how
    to import pretrained weights from an external source, such as OpenAI, into our
    model. Now we will reap the fruits of our labor by fine-tuning the LLM on a specific
    target task, such as classifying text. The concrete example we examine is classifying
    text messages as “spam” or “not spam.” Figure 6.1 highlights the two main ways
    of fine-tuning an LLM: fine-tuning for classification (step 8) and fine-tuning
    to follow instructions (step 9).'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1 The three main stages of coding an LLM. This chapter focus on stage
    3 (step 8): fine-tuning a pretrained LLM as a classifier.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.1 Different categories of fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common ways to fine-tune language models are *instruction fine-tuning*
    and *classification fine-tuning*. Instruction fine-tuning involves training a
    language model on a set of tasks using specific instructions to improve its ability
    to understand and execute tasks described in natural language prompts, as illustrated
    in figure 6.2\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Two different instruction fine-tuning scenarios. At the top, the
    model is tasked with determining whether a given text is spam. At the bottom,
    the model is given an instruction on how to translate an English sentence into
    German.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In classification fine-tuning, a concept you might already be acquainted with
    if you have a background in machine learning, the model is trained to recognize
    a specific set of class labels, such as “spam” and “not spam.” Examples of classification
    tasks extend beyond LLMs and email filtering: they include identifying different
    species of plants from images; categorizing news articles into topics like sports,
    politics, and technology; and distinguishing between benign and malignant tumors
    in medical imaging.'
  prefs: []
  type: TYPE_NORMAL
- en: The key point is that a classification fine-tuned model is restricted to predicting
    classes it has encountered during its training. For instance, it can determine
    whether something is “spam” or “not spam,” as illustrated in figure 6.3, but it
    can’t say anything else about the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 A text classification scenario using an LLM. A model fine-tuned for
    spam classification does not require further instruction alongside the input.
    In contrast to an instruction fine-tuned model, it can only respond with “spam”
    or “not spam.”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In contrast to the classification fine-tuned model depicted in figure 6.3, an
    instruction fine-tuned model typically can undertake a broader range of tasks.
    We can view a classification fine-tuned model as highly specialized, and generally,
    it is easier to develop a specialized model than a generalist model that works
    well across various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Instruction fine-tuning improves a model’s ability to understand and generate
    responses based on specific user instructions. Instruction fine-tuning is best
    suited for models that need to handle a variety of tasks based on complex user
    instructions, improving flexibility and interaction quality. Classification fine-tuning
    is ideal for projects requiring precise categorization of data into predefined
    classes, such as sentiment analysis or spam detection.
  prefs: []
  type: TYPE_NORMAL
- en: While instruction fine-tuning is more versatile, it demands larger datasets
    and greater computational resources to develop models proficient in various tasks.
    In contrast, classification fine-tuning requires less data and compute power,
    but its use is confined to the specific classes on which the model has been trained.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Preparing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will modify and classification fine-tune the GPT model we previously implemented
    and pretrained. We begin by downloading and preparing the dataset, as highlighted
    in figure 6.4\. To provide an intuitive and useful example of classification fine-tuning,
    we will work with a text message dataset that consists of spam and non-spam messages.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 The three-stage process for classification fine-tuning an LLM. Stage
    1 involves dataset preparation. Stage 2 focuses on model setup. Stage 3 covers
    fine-tuning and evaluating the model.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  Text messages typically sent via phone, not email. However, the same steps
    also apply to email classification, and interested readers can find links to email
    spam classification datasets in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to download the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Downloading and unzipping the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Downloads the file'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Unzips the file'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds a .tsv file extension'
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the preceding code, the dataset is saved as a tab-separated
    text file, `SMSSpamCollection.tsv`, in the `sms_spam_collection` folder. We can
    load it into a pandas `DataFrame` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Renders the data frame in a Jupyter notebook. Alternatively, use print(df).'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 shows the resulting data frame of the spam dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 Preview of the `SMSSpamCollection` dataset in a pandas `DataFrame`,
    showing class labels (“ham” or “spam”) and corresponding text messages. The dataset
    consists of 5,572 rows (text messages and labels).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s examine the class label distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the previous code, we find that the data contains “ham” (i.e., not
    spam) far more frequently than “spam”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For simplicity, and because we prefer a small dataset (which will facilitate
    faster fine-tuning of the LLM), we choose to undersample the dataset to include
    747 instances from each class.
  prefs: []
  type: TYPE_NORMAL
- en: Note  There are several other methods to handle class imbalances, but these
    are beyond the scope of this book. Readers interested in exploring methods for
    dealing with imbalanced data can find additional information in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the code in the following listing to undersample and create a balanced
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Creating a balanced dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Counts the instances of “spam”'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Randomly samples “ham” instances to match the number of “spam” instances'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Combines ham subset with “spam”'
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the previous code to balance the dataset, we can see that we
    now have equal amounts of spam and non-spam messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert the “string” class labels `"ham"` and `"spam"` into integer
    class labels 0 and 1, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This process is similar to converting text into token IDs. However, instead
    of using the GPT vocabulary, which consists of more than 50,000 words, we are
    dealing with just two token IDs: 0 and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a `random_split` function to split the dataset into three parts:
    70% for training, 10% for validation, and 20% for testing. (These ratios are common
    in machine learning to train, adjust, and evaluate models.)'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Splitting the dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Shuffles the entire DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculates split indices'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Splits the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Test size is implied to be 0.2 as the remainder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s save the dataset as CSV (comma-separated value) files so we can reuse
    it later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Thus far, we have downloaded the dataset, balanced it, and split it into training
    and evaluation subsets. Now we will set up the PyTorch data loaders that will
    be used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Creating data loaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will develop PyTorch data loaders conceptually similar to those we implemented
    while working with text data. Previously, we utilized a sliding window technique
    to generate uniformly sized text chunks, which we then grouped into batches for
    more efficient model training. Each chunk functioned as an individual training
    instance. However, we are now working with a spam dataset that contains text messages
    of varying lengths. To batch these messages as we did with the text chunks, we
    have two primary options:'
  prefs: []
  type: TYPE_NORMAL
- en: Truncate all messages to the length of the shortest message in the dataset or
    batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pad all messages to the length of the longest message in the dataset or batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first option is computationally cheaper, but it may result in significant
    information loss if shorter messages are much smaller than the average or longest
    messages, potentially reducing model performance. So, we opt for the second option,
    which preserves the entire content of all messages.
  prefs: []
  type: TYPE_NORMAL
- en: To implement batching, where all messages are padded to the length of the longest
    message in the dataset, we add padding tokens to all shorter messages. For this
    purpose, we use `"<|endoftext|>"` as a padding token.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, instead of appending the string `"<|endoftext|>"` to each of the text
    messages directly, we can add the token ID corresponding to `"<|endoftext|>"`
    to the encoded text messages, as illustrated in figure 6.6\. `50256` is the token
    ID of the padding token `"<|endoftext|>"`. We can double-check whether the token
    ID is correct by encoding the `"<|endoftext|>"` using the *GPT-2 tokenizer* from
    the `tiktoken` package that we used previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/6-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 The input text preparation process. First, each input text message
    is converted into a sequence of token IDs. Then, to ensure uniform sequence lengths,
    shorter sequences are padded with a padding token (in this case, token ID 50256)
    to match the length of the longest sequence.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Indeed, executing the preceding code returns `[50256]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to implement a PyTorch `Dataset`, which specifies how the data
    is loaded and processed before we can instantiate the data loaders. For this purpose,
    we define the `SpamDataset` class, which implements the concepts in figure 6.6\.
    This `SpamDataset` class handles several key tasks: it encodes the text messages
    into token sequences, identifies the longest sequence in the training dataset,
    and ensures that all other sequences are padded with a *padding token* to match
    the length of the longest sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Setting up a Pytorch `Dataset` class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Pretokenizes texts'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Truncates sequences if they are longer than max_length'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Pads sequences to the longest sequence'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SpamDataset` class loads data from the CSV files we created earlier, tokenizes
    the text using the GPT-2 tokenizer from `tiktoken`, and allows us to *pad* or
    *truncate* the sequences to a uniform length determined by either the longest
    sequence or a predefined maximum length. This ensures each input tensor is of
    the same size, which is necessary to create the batches in the training data loader
    we implement next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The longest sequence length is stored in the dataset’s `max_length` attribute.
    If you are curious to see the number of tokens in the longest sequence, you can
    use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The code outputs `120`, showing that the longest sequence contains no more than
    120 tokens, a common length for text messages. The model can handle sequences
    of up to 1,024 tokens, given its context length limit. If your dataset includes
    longer texts, you can pass `max_length=1024` when creating the training dataset
    in the preceding code to ensure that the data does not exceed the model’s supported
    input (context) length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we pad the validation and test sets to match the length of the longest
    training sequence. Importantly, any validation and test set samples exceeding
    the length of the longest training example are truncated using `encoded_text[:self.max_length]`
    in the `SpamDataset` code we defined earlier. This truncation is optional; you
    can set `max_length=None` for both validation and test sets, provided there are
    no sequences exceeding 1,024 tokens in these sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 6.1 Increasing the context length
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Pad the inputs to the maximum number of tokens the model supports and observe
    how it affects the predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: Using the datasets as inputs, we can instantiate the data loaders similarly
    to when we were working with text data. However, in this case, the targets represent
    class labels rather than the next tokens in the text. For instance, if we choose
    a batch size of 8, each batch will consist of eight training examples of length
    120 and the corresponding class label of each example, as illustrated in figure
    6.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 A single training batch consisting of eight text messages represented
    as token IDs. Each text message consists of 120 token IDs. A class label array
    stores the eight class labels corresponding to the text messages, which can be
    either `0` (“not spam”) or `1` (“spam”).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The code in the following listing creates the training, validation, and test
    set data loaders that load the text messages and labels in batches of size 8.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Creating PyTorch data loaders
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This setting ensures compatibility with most computers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that the data loaders are working and are, indeed, returning batches
    of the expected size, we iterate over the training loader and then print the tensor
    dimensions of the last batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the input batches consist of eight training examples with 120
    tokens each, as expected. The label tensor stores the class labels corresponding
    to the eight training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, to get an idea of the dataset size, let’s print the total number of
    batches in each dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The number of batches in each dataset are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve prepared the data, we need to prepare the model for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Initializing a model with pretrained weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We must prepare the model for classification fine-tuning to identify spam messages.
    We start by initializing our pretrained model, as highlighted in figure 6.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 The three-stage process for classification fine-tuning the LLM. Having
    completed stage 1, preparing the dataset, we now must initialize the LLM, which
    we will then fine-tune to classify spam messages.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To begin the model preparation process, we employ the same configurations we
    used to pretrain unlabeled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Vocabulary size'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Context length'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Dropout rate'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Query-key-value bias'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we import the `download_and_load_gpt2` function from the `gpt_download.py`
    file and reuse the `GPTModel` class and `load_weights_into_gpt` function from
    pretraining (see chapter 5) to load the downloaded weights into the GPT model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Loading a pretrained GPT model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the model weights into the `GPTModel`, we reuse the text generation
    utility function from chapters 4 and 5 to ensure that the model generates coherent
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the model generates coherent text, which is indicates
    that the model weights have been loaded correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start fine-tuning the model as a spam classifier, let’s see whether
    the model already classifies spam messages by prompting it with instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The model output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Based on the output, it’s apparent that the model is struggling to follow instructions.
    This result is expected, as it has only undergone pretraining and lacks instruction
    fine-tuning. So, let’s prepare the model for classification fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Adding a classification head
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must modify the pretrained LLM to prepare it for classification fine-tuning.
    To do so, we replace the original output layer, which maps the hidden representation
    to a vocabulary of 50,257, with a smaller output layer that maps to two classes:
    `0` (“not spam”) and `1` (“spam”), as shown in figure 6.9\. We use the same model
    as before, except we replace the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 Adapting a GPT model for spam classification by altering its architecture.
    Initially, the model’s linear output layer mapped 768 hidden units to a vocabulary
    of 50,257 tokens. To detect spam, we replace this layer with a new output layer
    that maps the same 768 hidden units to just two classes, representing “spam” and
    “not spam.”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Output layer nodes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We could technically use a single output node since we are dealing with a binary
    classification task. However, it would require modifying the loss function, as
    I discuss in “Losses Learned—Optimizing Negative Log-Likelihood and Cross-Entropy
    in PyTorch” ([https://mng.bz/NRZ2](https://mng.bz/NRZ2)). Therefore, we choose
    a more general approach, where the number of output nodes matches the number of
    classes. For example, for a three-class problem, such as classifying news articles
    as “Technology,” “Sports,” or “Politics,” we would use three output nodes, and
    so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we attempt the modification shown in figure 6.9, let’s print the model
    architecture via `print(model`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This output neatly lays out the architecture we laid out in chapter 4\. As previously
    discussed, the `GPTModel` consists of embedding layers followed by 12 identical
    *transformer blocks* (only the last block is shown for brevity), followed by a
    final `LayerNorm` and the output layer, `out_head`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we replace the `out_head` with a new output layer (see figure 6.9) that
    we will fine-tune.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning selected layers vs. all layers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since we start with a pretrained model, it’s not necessary to fine-tune all
    model layers. In neural network-based language models, the lower layers generally
    capture basic language structures and semantics applicable across a wide range
    of tasks and datasets. So, fine-tuning only the last layers (i.e., layers near
    the output), which are more specific to nuanced linguistic patterns and task-specific
    features, is often sufficient to adapt the model to new tasks. A nice side effect
    is that it is computationally more efficient to fine-tune only a small number
    of layers. Interested readers can find more information, including experiments,
    on which layers to fine-tune in appendix B.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the model ready for classification fine-tuning, we first *freeze* the
    model, meaning that we make all layers nontrainable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Then, we replace the output layer (`model.out_head`), which originally maps
    the layer inputs to 50,257 dimensions, the size of the vocabulary (see figure
    6.9).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Adding a classification layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: To keep the code more general, we use `BASE_CONFIG["emb_dim"]`, which is equal
    to 768 in the `"gpt2-small` `(124M)"` model. Thus, we can also use the same code
    to work with the larger GPT-2 model variants.
  prefs: []
  type: TYPE_NORMAL
- en: This new `model.out_head` output layer has its `requires_grad` attribute set
    to `True` by default, which means that it’s the only layer in the model that will
    be updated during training. Technically, training the output layer we just added
    is sufficient. However, as I found in experiments, fine-tuning additional layers
    can noticeably improve the predictive performance of the model. (For more details,
    refer to appendix B.) We also configure the last transformer block and the final
    `LayerNorm` module, which connects this block to the output layer, to be trainable,
    as depicted in figure 6.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 The GPT model includes 12 repeated transformer blocks. Alongside
    the output layer, we set the final `LayerNorm` and the last transformer block
    as trainable. The remaining 11 transformer blocks and the embedding layers are
    kept nontrainable.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To make the final `LayerNorm` and last transformer block trainable, we set
    their respective `requires_grad` to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 6.2 Fine-tuning the whole model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Instead of fine-tuning just the final transformer block, fine-tune the entire
    model and assess the effect on predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we added a new output layer and marked certain layers as trainable
    or nontrainable, we can still use this model similarly to how we have previously.
    For instance, we can feed it an example text identical to our previously used
    example text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#1 shape: (batch_size, num_tokens)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The print output shows that the preceding code encodes the inputs into a tensor
    consisting of four input tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can pass the encoded token IDs to the model as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output tensor looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: A similar input would have previously produced an output tensor of `[1,` `4,`
    `50257]`, where `50257` represents the vocabulary size. The number of output rows
    corresponds to the number of input tokens (in this case, four). However, each
    output’s embedding dimension (the number of columns) is now 2 instead of 50,257
    since we replaced the output layer of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we are interested in fine-tuning this model to return a class
    label indicating whether a model input is “spam” or “not spam.” We don’t need
    to fine-tune all four output rows; instead, we can focus on a single output token.
    In particular, we will focus on the last row corresponding to the last output
    token, as shown in figure 6.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 The GPT model with a four-token example input and output. The output
    tensor consists of two columns due to the modified output layer. We are only interested
    in the last row corresponding to the last token when fine-tuning the model for
    spam classification.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To extract the last output token from the output tensor, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We still need to convert the values into a class-label prediction. But first,
    let’s understand why we are particularly interested in the last output token only.
  prefs: []
  type: TYPE_NORMAL
- en: We have already explored the attention mechanism, which establishes a relationship
    between each input token and every other input token, and the concept of a *causal
    attention mask*, commonly used in GPT-like models (see chapter 3). This mask restricts
    a token’s focus to its current position and the those before it, ensuring that
    each token can only be influenced by itself and the preceding tokens, as illustrated
    in figure 6.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 The causal attention mechanism, where the attention scores between
    input tokens are displayed in a matrix format. The empty cells indicate masked
    positions due to the causal attention mask, preventing tokens from attending to
    future tokens. The values in the cells represent attention scores; the last token,
    `time`, is the only one that computes attention scores for all preceding tokens.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Given the causal attention mask setup in figure 6.12, the last token in a sequence
    accumulates the most information since it is the only token with access to data
    from all the previous tokens. Therefore, in our spam classification task, we focus
    on this last token during the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to transform the last token into class label predictions and
    calculate the model’s initial prediction accuracy. Subsequently, we will fine-tune
    the model for the spam classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 6.3 Fine-tuning the first vs. last token
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Try fine-tuning the first output token. Notice the changes in predictive performance
    compared to fine-tuning the last output token.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Calculating the classification loss and accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Only one small task remains before we fine-tune the model: we must implement
    the model evaluation functions used during fine-tuning, as illustrated in figure
    6.13.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13 The three-stage process for classification fine-tuning the LLM.
    We''ve completed the first six steps. We are now ready to undertake the last step
    of stage 2: implementing the functions to evaluate the model’s performance to
    classify spam messages before, during, and after the fine-tuning.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Before implementing the evaluation utilities, let’s briefly discuss how we convert
    the model outputs into class label predictions. We previously computed the token
    ID of the next token generated by the LLM by converting the 50,257 outputs into
    probabilities via the `softmax` function and then returning the position of the
    highest probability via the `argmax` function. We take the same approach here
    to calculate whether the model outputs a “spam” or “not spam” prediction for a
    given input, as shown in figure 6.14\. The only difference is that we work with
    2-dimensional instead of 50,257-dimensional outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 The model outputs corresponding to the last token are converted
    into probability scores for each input text. The class labels are obtained by
    looking up the index position of the highest probability score. The model predicts
    the spam labels incorrectly because it has not yet been trained.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s consider the last token output using a concrete example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The values of the tensor corresponding to the last token are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can obtain the class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the code returns `1`, meaning the model predicts that the input
    text is “spam.” Using the `softmax` function here is optional because the largest
    outputs directly correspond to the highest probability scores. Hence, we can simplify
    the code without using softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This concept can be used to compute the classification accuracy, which measures
    the percentage of correct predictions across a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the classification accuracy, we apply the `argmax`-based prediction
    code to all examples in the dataset and calculate the proportion of correct predictions
    by defining a `calc_accuracy_loader` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 Calculating the classification accuracy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Logits of last output token'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the function to determine the classification accuracies across various
    datasets estimated from 10 batches for efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Via the `device` setting, the model automatically runs on a GPU if a GPU with
    Nvidia CUDA support is available and otherwise runs on a CPU. The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the prediction accuracies are near a random prediction, which
    would be 50% in this case. To improve the prediction accuracies, we need to fine-tune
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before we begin fine-tuning the model, we must define the loss function
    we will optimize during training. Our objective is to maximize the spam classification
    accuracy of the model, which means that the preceding code should output the correct
    class labels: `0` for non-spam and `1` for spam.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because classification accuracy is not a differentiable function, we use cross-entropy
    loss as a proxy to maximize accuracy. Accordingly, the `calc_loss_batch` function
    remains the same, with one adjustment: we focus on optimizing only the last token,
    `model(input_batch)[:,` `-1,` `:]`, rather than all tokens, `model(input_batch)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Logits of last output token'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `calc_loss_batch` function to compute the loss for a single batch
    obtained from the previously defined data loaders. To calculate the loss for all
    batches in a data loader, we define the `calc_loss_loader` function as before.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 Calculating the classification loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Ensures number of batches doesn’t exceed batches in data loader'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to calculating the training accuracy, we now compute the initial loss
    for each data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Disables gradient tracking for efficiency because we are not training yet'
  prefs: []
  type: TYPE_NORMAL
- en: The initial loss values are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will implement a training function to fine-tune the model, which means
    adjusting the model to minimize the training set loss. Minimizing the training
    set loss will help increase the classification accuracy, which is our overall
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Fine-tuning the model on supervised data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We must define and use the training function to fine-tune the pretrained LLM
    and improve its spam classification accuracy. The training loop, illustrated in
    figure 6.15, is the same overall training loop we used for pretraining; the only
    difference is that we calculate the classification accuracy instead of generating
    a sample text to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 A typical training loop for training deep neural networks in PyTorch
    consists of several steps, iterating over the batches in the training set for
    several epochs. In each loop, we calculate the loss for each training set batch
    to determine loss gradients, which we use to update the model weights to minimize
    the training set loss.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The training function implementing the concepts shown in figure 6.15 also closely
    mirrors the `train_model_simple` function used for pretraining the model. The
    only two distinctions are that we now track the number of training examples seen
    (`examples_seen`) instead of the number of tokens, and we calculate the accuracy
    after each epoch instead of printing a sample text.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 Fine-tuning the model to classify spam
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initialize lists to track losses and examples seen'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Main training loop'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets model to training mode'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Resets loss gradients from the previous batch iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Calculates loss gradients'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Updates model weights using loss gradients'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 New: tracks examples instead of tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Optional evaluation step'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Calculates accuracy after each epoch'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `evaluate_model` function is identical to the one we used for pretraining:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we initialize the optimizer, set the number of training epochs, and initiate
    the training using the `train_classifier_simple` function. The training takes
    about 6 minutes on an M3 MacBook Air laptop computer and less than half a minute
    on a V100 or A100 GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we see during the training is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We then use Matplotlib to plot the loss function for the training and validation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Plotting the classification loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Plots training and validation loss against epochs'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates a second x-axis for examples seen'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Invisible plot for aligning ticks'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Adjusts layout to make room'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 plots the resulting loss curves.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 The model’s training and validation loss over the five training
    epochs. Both the training loss, represented by the solid line, and the validation
    loss, represented by the dashed line, sharply decline in the first epoch and gradually
    stabilize toward the fifth epoch. This pattern indicates good learning progress
    and suggests that the model learned from the training data while generalizing
    well to the unseen validation data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As we can see based on the sharp downward slope in figure 6.16, the model is
    learning well from the training data, and there is little to no indication of
    overfitting; that is, there is no noticeable gap between the training and validation
    set losses.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the number of epochs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Earlier, when we initiated the training, we set the number of epochs to five.
    The number of epochs depends on the dataset and the task’s difficulty, and there
    is no universal solution or recommendation, although an epoch number of five is
    usually a good starting point. If the model overfits after the first few epochs
    as a loss plot (see figure 6.16), you may need to reduce the number of epochs.
    Conversely, if the trendline suggests that the validation loss could improve with
    further training, you should increase the number of epochs. In this concrete case,
    five epochs is a reasonable number as there are no signs of early overfitting,
    and the validation loss is close to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same `plot_values` function, let’s now plot the classification accuracies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Figure 6.17 graphs the resulting accuracy. The model achieves a relatively high
    training and validation accuracy after epochs 4 and 5. Importantly, we previously
    set `eval_iter=5` when using the `train_classifier_simple` function, which means
    our estimations of training and validation performance are based on only five
    batches for efficiency during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 Both the training accuracy (solid line) and the validation accuracy
    (dashed line) increase substantially in the early epochs and then plateau, achieving
    almost perfect accuracy scores of 1.0\. The close proximity of the two lines throughout
    the epochs suggests that the model does not overfit the training data very much.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now we must calculate the performance metrics for the training, validation,
    and test sets across the entire dataset by running the following code, this time
    without defining the `eval_iter` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The resulting accuracy values are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The training and test set performances are almost identical. The slight discrepancy
    between the training and test set accuracies suggests minimal overfitting of the
    training data. Typically, the validation set accuracy is somewhat higher than
    the test set accuracy because the model development often involves tuning hyperparameters
    to perform well on the validation set, which might not generalize as effectively
    to the test set. This situation is common, but the gap could potentially be minimized
    by adjusting the model’s settings, such as increasing the dropout rate (`drop_rate`)
    or the `weight_ decay` parameter in the optimizer configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8 Using the LLM as a spam classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having fine-tuned and evaluated the model, we are now ready to classify spam
    messages (see figure 6.18). Let’s use our fine-tuned GPT-based spam classification
    model. The following `classify_review` function follows data preprocessing steps
    similar to those we used in the `SpamDataset` implemented earlier. Then, after
    processing text into token IDs, the function uses the model to predict an integer
    class label, similar to what we implemented in section 6.6, and then returns the
    corresponding class name.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 The three-stage process for classification fine-tuning our LLM.
    Step 10 is the final step of stage 3—using the fine-tuned model to classify new
    spam messages.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 6.12 Using the model to classify new texts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Prepares inputs to the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Truncates sequences if they are too long'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Pads sequences to the longest sequence'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Adds batch dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Models inference without gradient tracking'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Logits of the last output token'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Returns the classified result'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try this `classify_review` function on an example text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting model correctly predicts `"spam"`. Let’s try another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The model again makes a correct prediction and returns a “not spam” label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s save the model in case we want to reuse the model later without
    having to train it again. We can use the `torch.save` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Once saved, the model can be loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different strategies for fine-tuning LLMs, including classification
    fine-tuning and instruction fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification fine-tuning involves replacing the output layer of an LLM via
    a small classification layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of classifying text messages as “spam” or “not spam,” the new classification
    layer consists of only two output nodes. Previously, we used the number of output
    nodes equal to the number of unique tokens in the vocabulary (i.e., 50,256).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of predicting the next token in the text as in pretraining, classification
    fine-tuning trains the model to output a correct class label—for example, “spam”
    or “not spam.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model input for fine-tuning is text converted into token IDs, similar to
    pretraining.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before fine-tuning an LLM, we load the pretrained model as a base model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a classification model involves calculating the classification accuracy
    (the fraction or percentage of correct predictions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a classification model uses the same cross entropy loss function
    as when pretraining the LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
