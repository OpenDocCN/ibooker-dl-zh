<html><head></head><body>

  <div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span> </span> <span class="chapter-title-text">Descriptive analysis and statistical inference supported by generative AI</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Using generative AI to design analytics</li> 
    <li class="readable-text" id="p3">Descriptive analytics of collected data using generative AI</li> 
    <li class="readable-text" id="p4">Utilizing generative AI to select appropriate inference analytics methods</li> 
    <li class="readable-text" id="p5">Using generative AI to obtain complete code solutions for data transformation, visualization, and modeling</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>The work described in the previous chapter was challenging and strenuous but absolutely necessary. You ensured that the data is of acceptable quality and that you understand its meaning. Your understanding of the data may still change as analysis uncovers nuances and intricacies of the underlying process, but you should have a solid foundation by now. </p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>In this chapter, we will walk through the first part of an in-depth business analysis, the end-to-end descriptive analysis, learning how generative AI can help us at each step. And it can help a lot, from answering questions related to available data to providing real business insights. We will start with high-level analysis planning, touch on even more data preparation (surprise!), and then describe and visualize the data, searching for business-relevant information. We will also apply more advanced statistical modeling tools to our data, hoping to infer more insight from it.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_36"><span class="num-string">3.1</span> Research questions</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>In the first chapter, we explained how you can use generative AI to help specify the business goal of your analysis. As we said then, your goals should usually be determined by the business stakeholders. In the second chapter, you did a primary cleanup of your data and investigated the contents of available data sources. </p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>Let’s now make millions for your company! That’s your role, no? Well, actually, no. At least, it <em>shouldn’t </em>be. Your role is to inform decisions made by business stakeholders. But to do that, you need to translate the business questions into research questions. The plural here is not by accident. Usually, you’ll discover that answering a single business question requires answering multiple research questions. Take a simple question: “How do we make more money?” To answer that, you may go down the following path: “Who spends the most money with us?”, followed by “Can they buy more of our product?”, or “Where do we find similar people?” Or, as the dollar saved is worth ten dollars earned: “What step in our production produces the most scrap?”, followed by “What production parameters could we optimize to reduce this?”</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>The possibilities are endless, and you’re limited only by your imagination and data availability. That’s why knowing your data is so important! Business stakeholders usually think in terms of business processes. A person managing a supply chain would love to lay their hands on a prediction of product demand across different regions to minimize overstocking and understocking issues. A person responsible for alloy casting would love to have a tool that calculates optimal ratios of different substrate materials to get the final product. They understand the supply chain or metal mixing process; they might not appreciate the complexity of integrating regional sales trends and global economic indicators, or the need for confirmed quality and trace element analysis that are crucial for creating robust models. They don’t necessarily know the limitations and possibilities inherent to your data sources. </p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>The key point here is that you will rarely answer the business question you’ve been presented with. More often than not, you will need to break it down and translate it into a series of research questions, and only after their analysis will you synthetize the final answer. In the first chapter, we showed you an iterative way of defining the analysis scope. Even if it seems you defined your scope just a short while ago, the moment you’ve finished cleaning and assessing your data inventory is a good time to look at the problem with fresh eyes. Maybe some questions you thought were answerable aren’t, and perhaps new opportunities have arisen. </p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>As we were writing this book, we asked generative AI what information it would want from us to help us formulate research questions about the dataset we have at hand (think of a situation when a business stakeholder drops data on your lap with a short request “do something about it”; it happens). What we got back was a handy checklist, which should help you structure your thinking about what questions are and aren’t answerable in the context of your data environment: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p14"> <em>Data dictionary</em><em> </em>—A brief description of each variable in your dataset, including its type (e.g., continuous, ordinal, or nominal), measurement scale (e.g., interval, ratio), and any potential coding or transformation applied. </li> 
   <li class="readable-text" id="p15"> <em>Data sources and context</em><em> </em>—The origin of the data, the context in which it was collected, and any relevant background information. </li> 
   <li class="readable-text" id="p16"> <em>Data collection and sampling methods</em><em> </em>—How the data was collected, any sampling methods applied, and any potential biases or limitations in the data. </li> 
   <li class="readable-text" id="p17"> <em>Domain knowledge</em><em> </em>—Any specific domain knowledge or expertise you have about the subject matter of the dataset. </li> 
   <li class="readable-text" id="p18"> <em>Goals and interests</em><em> </em>—Goals and interests for this analysis, such as identifying patterns, making predictions, evaluating a policy, understanding relationships between variables, and so on. </li> 
   <li class="readable-text" id="p19"> <em>Stakeholders</em><em> </em>—Stakeholders who will benefit from the insights generated from your analysis. </li> 
   <li class="readable-text" id="p20"> <em>Temporal and spatial aspects</em><em> </em>—Temporal or spatial components that may affect the research questions. </li> 
   <li class="readable-text" id="p21"> <em>Previous research</em><em> </em>—Any knowledge of prior research or studies conducted using similar data or in the same domain. </li> 
   <li class="readable-text" id="p22"> <em>Limitations and constraints</em><em> </em>—Any limitations or constraints in the data or the analysis process that might affect the research questions you can explore. </li> 
  </ul> 
  <div class="readable-text" id="p23"> 
   <p>Collecting all this information at this stage will allow you to design an analytics process that won’t suddenly stop midway when you realize you are missing a critical bit of required data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>With all this information at hand, you may come up with a bunch of research questions, answers to which you will be able to synthesize into the answer to your business question. But are they really all relevant questions? Fortunately, you have access to a tool with hundreds of combined years of analytical experience. With its help, you can make sure you didn’t omit any relevant issues and check that all the research questions you are thinking of asking are indeed relevant to the analyzed business issue. Such help will be of exceptional value if your boss sends you an email with a pile of files and asks you to extract all possible insights from them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p25"> 
   <p>Let’s see how a generative AI advisor tackles the problem of overly general business problems in the context of available data. We will use our example dataset of Brazilian e-commerce, specifically the data description, and ask generative AI about possible related research questions. Please note that, due to the size of the request, it’s presented in full as appendix A. </p> 
  </div> 
  <div class="readable-text prompt" id="p26"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Propose research questions for the analysis of the following set of data:</p> 
  </div> 
  <div class="readable-text prompt" id="p27"> 
   <p>1. Data description and dictionaries: Data is composed of 8 DataFrames described in detail below:</p> 
  </div> 
  <div class="readable-text prompt" id="p28"> 
   <p># BORING DETAILS RELATED TO POINTS 1–5 (see appendix A) </p> 
  </div> 
  <div class="readable-text prompt" id="p29"> 
   <p>6. Goals and interests: I would like to increase the business efficiency of the e-commerce shop which produced this data.</p> 
  </div> 
  <div class="readable-text prompt" id="p30"> 
   <p>7. Temporal and spatial aspects: Data comes from Brazil and describes orders from 2016 to 2018.</p> 
  </div> 
  <div class="readable-text prompt" id="p31"> 
   <p>8. Previous research: Unknown. </p> 
  </div> 
  <div class="readable-text prompt" id="p32"> 
   <p>9. Limitations and constraints: We would like to focus on descriptive analytics and avoid Machine Learning methods beyond linear or logistic regression. </p> 
  </div> 
  <div class="readable-text response" id="p33"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Based on the provided information, here are some potential research questions for your analysis:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p34"> What are the most popular product categories in terms of revenue, and how do their sales performance vary across different regions in Brazil? </li> 
   <li class="readable-text" id="p35"> How does the average product price, freight value, and payment value differ across different product categories and regions? </li> 
   <li class="readable-text" id="p36"> What is the distribution of review scores across different product categories? </li> 
   <li class="readable-text" id="p37"> How does the order status (e.g., delivered, shipped, etc.) vary across different regions, product categories, and sellers? </li> 
   <li class="readable-text" id="p38"> What is the relationship between product attributes (e.g., weight, dimensions) and overhead costs (e.g., shipping costs)? </li> 
   <li class="readable-text" id="p39"> What factors (e.g., delivery time, price, product attributes) contribute to differences in the average review score? </li> 
   <li class="readable-text" id="p40"> How do different payment methods and the number of payment installments affect order completion rates and customer satisfaction? </li> 
   <li class="readable-text" id="p41"> What is the impact of delivery time (actual vs. estimated) on customer satisfaction and review scores? </li> 
   <li class="readable-text" id="p42"> Are there any significant differences in sales performance and customer satisfaction between sellers from different cities or states? </li> 
  </ol> 
  <div class="readable-text response" id="p43"> 
   <p>These research questions focus on descriptive analytics and should help you gain insights into the factors that influence sales performance, customer satisfaction, and overall business efficiency for the e-commerce shop.</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>As you can see, you can identify multiple areas of interest within seconds! While our initial inquiry didn’t specify objectives beyond enhancing business value, performing the suggested analytics should provide business stakeholders with information, allowing them to make significant steps toward the overarching goal of increasing business efficiency. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p45"> 
   <p><span class="print-book-callout-head">I found 1,000 ways of not doing something</span>  It would be best to talk to the business at this stage. They should think about whether such an insight will help them and if they want to prioritize it. Most importantly, this list will hopefully spark some new ideas about what kind of analysis they could immediately use to generate value.</p> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>This list is good enough for us to use as a base for our analysis. Of course, we might expect more of a typical analysis, like the Net Promoter Score (the most common method of measuring client satisfaction), but the result is not bad, not bad at all! </p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_37"><span class="num-string">3.2</span> Analysis design</h2> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>Once you have a clear goal, you’ll probably feel energized and may be tempted to jump into creating pivot tables and charts . . . Sit down, sip a coffee, and wait until that feeling passes. In chapter 1, we advised you to <em>first </em>prepare a protocol of what your analysis will look like, what the possible results are, and what the <em>meaning</em> of each of these possible results would be. This element of experiment design is often overlooked, even in the academic research world, but you don’t want to learn from the worst. Getting some results and trying to fit a theory to them may result in worthless science fiction research papers (not to be mistaken with a respectable literature genre in which we’ve authored some titles ourselves); in business reality, this may lead to significant financial losses. Always plan ahead. No result should come as a surprise. Prepare a complete list of possible results and their meaning in the context of your business case. For example, if the current price is higher than the purchase price, sell; if they are equal, hold; otherwise buy.</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>Of course, no protocol is set in stone. You will likely revise it after preprocessing your data (you may find that not all planned analyses are possible, or new ideas might strike you while scrolling through the data). Your stakeholders may change their priorities or focus areas when you’re halfway through. Even in this book we ignore our own advice—the speed of GenAI development outpaced our typing abilities, so we had to revisit parts of this book a couple of times. Pretending that we followed the assumed path would be a straight-up lie.</p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>Unfortunately, cases where playing analysis by ear is justified are less common than the popularity of this approach. The sad fact is that as rare as proper experiment design is in an academic environment, it’s even rarer outside of it. You won’t find many good materials to guide you through the process. Even in the data (or text) corpus on which most generative AIs are trained, the cases where analytics design includes prior result analysis are most likely rare and far between, as none of the tested AI advisors considered it worth mentioning. </p> 
  </div> 
  <div class="readable-text prompt" id="p51"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you propose a design of the analysis that would answer all these questions based on the provided data? </p> 
  </div> 
  <div class="readable-text response" id="p52"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To answer the proposed research questions, you can follow this general design for your analysis:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text buletless-item" id="p53"> Data Preparation: 
    <ol style="list-style: lower-alpha"> 
     <li class=" buletless-item"> Merge relevant DataFrames to create a comprehensive dataset: 
      <ol style="list-style: lower-roman"> 
       <li> Merge <code>df_product_category_translation</code> with <code>df_products</code> to get product category names in English. </li> 
       <li> Merge <code>df_orders</code>, <code>df_customers</code>, and <code>df_order_items</code> using <code>customer_ id</code> and <code>order_id</code>. </li> 
       <li> Merge the resulting DataFrame with <code>df_products</code> using <code>product_id</code>. </li> 
       <li> Merge the DataFrame with <code>df_order_payments</code> using <code>order_id</code>. </li> 
       <li> Merge the DataFrame with <code>df_order_reviews</code> using <code>order_id</code>. </li> 
       <li> Merge the DataFrame with <code>df_sellers</code> using <code>seller_id</code>. </li> 
      </ol></li> 
     <li class=" buletless-item"> Clean the dataset: 
      <ol style="list-style: lower-roman"> 
       <li> Handle missing values (e.g., impute, drop, or leave as is, depending on the context). </li> 
       <li> Convert data types if necessary (e.g., timestamps, floats, and integers). </li> 
       <li> Create new variables, such as delivery time difference (actual vs. estimated), if needed. </li> 
      </ol></li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p54"> Descriptive Analysis: 
    <ol style="list-style: lower-alpha"> 
     <li> Compute summary statistics for key variables (e.g., sales volume, revenue, product attributes, and review scores). </li> 
     <li> Create visualizations to explore patterns and trends in the data (e.g., bar charts, line charts, scatter plots, and heatmaps). </li> 
     <li> Conduct cross-tabulations and comparisons across different categories (e.g., regions, product categories, and sellers). </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p55"> Inferential Analysis: 
    <ol style="list-style: lower-alpha"> 
     <li> Test for statistically significant differences between groups (e.g., using t-tests, ANOVA, or chi-square tests). </li> 
     <li> Perform correlation analyses to identify relationships between variables (e.g., product attributes and sales performance). </li> 
     <li> Conduct linear or logistic regression analyses to identify predictors of key outcomes (e.g., sales volume, revenue, and customer satisfaction). </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p56"> Interpretation and Discussion: 
    <ol style="list-style: lower-alpha"> 
     <li> Summarize key findings from the descriptive and inferential analyses. </li> 
     <li> Discuss potential explanations for the observed patterns and relationships. </li> 
     <li> Identify potential limitations of the analysis and areas for future research. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p57"> Conclusion and Recommendations: 
    <ol style="list-style: lower-alpha"> 
     <li> Draw conclusions based on the analysis findings. </li> 
     <li> Provide actionable recommendations to improve the e-commerce shop’s business efficiency. </li> 
    </ol></li> 
  </ol> 
  <div class="readable-text response" id="p58"> 
   <p>Following this general design for your analysis will help you systematically answer the proposed research questions using the provided data.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>Let us share a little story, or actually more than a dozen we encountered, fitting the same scenario: The analyst performs a regression analysis to explore the relationship between social media advertising spend and sales. The results indicate a statistically significant correlation between the amount spent on social media ads and increased sales. The company decides to increase social media spending, only to find out their return on investment (RoI) is in the negative. To make things worse, the budget diverted to social media campaigns weakened other business functions. That is a typical example of a poor analytics design with a perfectly avoidable outcome. </p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>It is not our goal to follow in the footsteps of Don Quixote, pursuing lofty ideals that we will never obtain, but you should spend some time inquiring about the possible results of different types of analytics and their meanings. In some cases, predicting all possible outcomes is not feasible. For example, you don’t want to come up with all possible variants of a chart you’re expected to generate. But you should have a general idea of expected shapes or traces. We touched upon this previously when we discussed data cleaning. You need to understand what’s expected to notice what’s relevant that’s unexpected. In some analyses, you should spend more time thinking about the meaning of the results. A good example here would be correlation analysis. </p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>What does correlation mean from the business perspective? How big a correlation is actually <em>meaningfu</em><em>l</em>?<em> </em>(Do not mistake this for statistical significance, as did the protagonists of the earlier story.) When calculating regression, what is the predictive power you require? There are no ready-to-use templates to be filled out. Your domain knowledge and understanding of the business will allow you to distinguish between valuable and misleading insights. Generative AI can help you, but don’t expect it to do all the heavy lifting for you. You need to talk to your business people to be able to deeply integrate domain knowledge into your analytical pipeline.</p> 
  </div> 
  <div class="readable-text intended-text" id="p62"> 
   <p>In the generative AI’s proposed design, there is one popularity-bias-driven element—the content of point 3.a, the methods for statistical testing. We’ll explain our gripe with that particular point in the introduction to section 3.5. </p> 
  </div> 
  <div class="readable-text intended-text" id="p63"> 
   <p>Other than that, the execution of this plan may indeed extract some insight from the data we meticulously cleaned.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_38"><span class="num-string">3.3</span> Descriptive data analysis</h2> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Many people approach data analysis (both stakeholders and naive newcomers to the job), with quite high expectations. Wouldn’t it be great to just drop all your company’s data into a magical box and receive back a set of simple instructions, like “Fire Sue from the third floor, as her micromanagement style kills the soul and effectiveness of her team. While you’re firing people, don’t forget Joe from packaging, as he’s the reason behind 20% of your returns”? Or, “Add the pink <em>Buy now!</em> button in the left-right corner of your landing page, and you’ll get 61.73% more sales in the first week”?</p> 
  </div> 
  <div class="readable-text intended-text" id="p66"> 
   <p>Going from data to information to insight is a process. It can be somewhat automated or sped up by the tools, but a lot of data analysis is just that—<em>a</em><em>nalyzin</em><em>g</em>: a detailed examination of the elements or structure of the data. Despite the best efforts of AI-focused companies, the part where someone needs to describe data elements and structure is not going anywhere, primarily because it offers an overview of business performance. Various statistical measures and visualization techniques enable you to identify patterns, relationships, and anomalies within datasets. You can use descriptive analytics to understand your business operations, customer behavior, and market dynamics better, laying a solid foundation for informed decision-making and future analysis. </p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>It doesn’t matter if you work for an organization, big or small. Analyzing data from a restaurant, you may use this approach to identify the most ordered items versus those rarely ordered. While working in the corporate office of the restaurant chain, you may use it to determine the most popular items by location and time of day. This insight could then guide decisions around stock management, staff scheduling, and targeted promotions, ultimately enhancing operational efficiency and customer satisfaction. Despite its simplicity, descriptive analytics is crucial in paving your way toward more advanced analytics methods, such as predictive and prescriptive analytics.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>You should describe your data using <em>nomen omen</em> descriptive statistics, including mean, median, mode, standard deviation, and percentile values. You should also prepare visualizations like bar charts, histograms, and scatter plots to further enhance your ability to identify and examine patterns and trends within the data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p69"> 
   <p>Four of the nine research questions initially proposed by generative AI can be addressed using these methods. We will do just that in the following subsections. The other five will require a somewhat different toolbox, presented in section 3.4.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_39"><span class="num-string">3.3.1</span> Popularity of product categories</h3> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>Recall the question provided by ChatGPT: “What are the most popular product categories in terms of revenue, and how do their sales performance vary across different regions in Brazil?” If working with e-commerce data makes you stifle your yawn, don’t worry. The same methodology is helpful in many business areas. Let’s explore some examples where this approach can provide you with valuable insights:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p72"> <em>Retail, e-commerce, and manufacturing</em><em> </em>—Beyond product categories, this method can be applied to understand customer segments’ buying behavior across different regions or time periods, such as analyzing seasonal trends in purchasing or the effect of marketing campaigns on various demographics. </li> 
   <li class="readable-text" id="p73"> <em>Healthcare</em><em> </em>—In healthcare, similar techniques can be used to analyze the distribution of disease incidence or treatment outcomes across different geographical areas and demographic groups. This can help in resource allocation, understanding disease spread patterns, and tailoring public health interventions. </li> 
   <li class="readable-text" id="p74"> <em>Financial services</em><em> </em>—For banks and financial institutions, analyzing the distribution of loan or credit card defaults by product type and geographic region can help in risk management and in designing targeted financial products. </li> 
   <li class="readable-text" id="p75"> <em>Supply chain and logistics</em><em> </em>—Here, the method can be used to analyze the performance of different logistics routes or transportation methods, categorizing them by cost, efficiency, or environmental impact across various regions. This can aid in optimizing routes, reducing costs, and improving sustainability. </li> 
   <li class="readable-text" id="p76"> <em>Telecommunications</em><em> </em>—Analyzing the usage patterns of different service plans or products (e.g., data plans, voice plans) across regions can help in tailoring offerings, improving network infrastructure, and identifying new market opportunities. </li> 
   <li class="readable-text" id="p77"> <em>Utilities and energy</em><em> </em>—In this sector, analyzing consumption patterns of different energy sources (e.g., electricity, gas) across regions can inform infrastructure development, energy conservation efforts, and pricing strategies. </li> 
   <li class="readable-text" id="p78"> <em>Tourism and hospitality</em><em> </em>—Understanding the preferences for different types of accommodations or travel experiences across various tourist demographics can help businesses tailor their services, optimize pricing strategies, and improve customer satisfaction. </li> 
   <li class="readable-text" id="p79"> <em>Real estate</em><em> </em>—Analyzing the performance of different types of properties (e.g., residential versus commercial) across regions can inform investment strategies, property development, and marketing efforts. </li> 
   <li class="readable-text" id="p80"> <em>Education</em><em> </em>—Evaluating the performance of different educational programs or courses across demographic groups can help educational institutions tailor their offerings, improve student outcomes, and optimize resource allocations. </li> 
   <li class="readable-text" id="p81"> <em>Government and public policy</em><em> </em>—Governments can use this method to analyze various policy impacts across regions, such as the effectiveness of economic incentives, public health initiatives, or infrastructure projects, enabling more informed decision-making. </li> 
  </ul> 
  <div class="readable-text" id="p82"> 
   <p>For each of these examples, the method we’ll look at next can help you uncover patterns and insights that might not be immediately apparent. You can really support your businesses by categorizing data across meaningful dimensions (like product categories, service types, or demographic segments) and analyzing their performance across different regions or groups.</p> 
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>Let’s get back to work. We will start with the simple task of testing which product categories bring the most revenue. It’s a fundamental analysis required for product portfolio optimization. We can apply the same methodology to evaluate the performance of different groups quickly.</p> 
  </div> 
  <div class="readable-text prompt" id="p84"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Provide me with a complete Python code solution required to answer this question on the most common product categories in the dataset. </p> 
  </div> 
  <div class="readable-text response" id="p85"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To answer the question about the most common product categories in the dataset, you can follow these steps in Python:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p86"> Import necessary libraries </li> 
   <li class="readable-text" id="p87"> Read and merge the required DataFrames </li> 
   <li class="readable-text" id="p88"> Perform the analysis to find the most common product categories </li> 
   <li class="readable-text" id="p89"> Display the results </li> 
  </ol> 
  <div class="readable-text response" id="p90"> 
   <p>Here’s the complete Python code solution:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p91"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

<em># Read DataFrames</em>
df_products = pd.read_csv('df_products.csv')
df_product_category_translation = 
<span class="">↪</span>pd.read_csv('df_product_category_translation.csv')

<em># Merge DataFrames</em>
df_products_merged = df_products.merge(
<span class="">↪</span>df_product_category_translation, on='product_category_name', 
<span class="">↪</span>how='left')

<em># Analyze the most common product categories</em>
category_counts = 
<span class="">↪</span>df_products_merged['product_category_name_english'].value_counts()

<em># Display the results</em>
print("The most common product categories are:")
print(category_counts.head(10))</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p92"> 
   <p>This code will read the data, merge the DataFrames, count the occurrences of each product category, and display the top 10 most common categories.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Generative AI failed to understand what question we were referring to. We hoped it would recall the question it itself proposed just two prompts earlier: “What are the most popular product categories in terms of sales volume and revenue, and how do their sales performance vary across different regions in Brazil?” Instead, it provided us with unhelpful output related to identifying the most common product categories instead of their sales performance. Let this be a warning that you shouldn’t always rely on generative AI to recall your conversation correctly if your prompt is vague. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p94"> 
   <p><span class="print-book-callout-head">Tomayto, tomahto</span>  Sometimes, the difference between the question you asked and the question generative AI answered might be minute and hard to spot, but inferring any insight from the answer to a question that’s different than you asked may have far-reaching implications. </p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Let’s try to be more specific. </p> 
  </div> 
  <div class="readable-text prompt" id="p96"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Provide me with a complete Python code solution required to answer the question, “What are the most popular product categories in terms of sales volume and revenue, and how do their sales performance vary across different regions in Brazil?” </p> 
  </div> 
  <div class="readable-text response" id="p97"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To answer this question, you need to follow these steps in Python:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p98"> Import necessary libraries </li> 
   <li class="readable-text" id="p99"> Read and merge the required DataFrames </li> 
   <li class="readable-text" id="p100"> Perform the analysis to find the most popular product categories in terms of sales volume and revenue and how their sales performance varies across different regions in Brazil </li> 
   <li class="readable-text" id="p101"> Display the results </li> 
  </ol> 
  <div class="readable-text response" id="p102"> 
   <p>Here’s the complete Python code solution:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p103"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

<em># Read DataFrames</em>
df_products = pd.read_csv('df_products.csv')
df_product_category_translation = 
<span class="">↪</span>pd.read_csv('df_product_category_translation.csv')
df_order_items = pd.read_csv('df_order_items.csv')
df_orders = pd.read_csv('df_orders.csv')
df_customers = pd.read_csv('df_customers.csv')

<em># Merge DataFrames</em>
df_products_merged = df_products.merge(
<span class="">↪</span>df_product_category_translation, on='product_category_name', 
<span class="">↪</span> how='left')
df_order_items_merged = df_order_items.merge(
<span class="">↪</span>df_products_merged, on='product_id', how='left')
df_orders_merged = df_orders.merge(
<span class="">↪</span>df_customers, on='customer_id', how='left')
df_merged = df_order_items_merged.merge(
<span class="">↪</span>df_orders_merged, on='order_id', how='left')

<em># Calculate sales volume and revenue per category</em>
category_sales = df_merged.groupby(
<span class="">↪</span>['product_category_name_english', 'customer_state']).agg(
<span class="">↪</span>{'order_id': 'count', 'price': 'sum'}).reset_index()
category_sales.columns = 
<span class="">↪</span>['product_category', 'customer_state', 'sales_volume', 'revenue']

<em># Sort by sales_volume and revenue</em>
category_sales_sorted = category_sales.sort_values(
<span class="">↪</span>by=['sales_volume', 'revenue'], ascending=False)

<em># Display the results</em>
print("The most popular product categories 
<span class="">↪</span>in terms of sales volume and revenue are:")
print(category_sales_sorted.head(10))

print("\nSales performance across different regions in Brazil:")
sales_by_region = 
<span class="">↪</span>category_sales_sorted.pivot_table(
<span class="">↪</span>index='product_category', 
<span class="">↪</span>columns='customer_state', 
<span class="">↪</span>values=['sales_volume', 'revenue'], fill_value=0)
print(sales_by_region)</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p104"> 
   <p>Replace the file names <code>df_products.csv</code>, <code>df_product_category_translation .csv</code>, <code>df_order_items.csv</code>, <code>df_orders.csv</code>, and <code>df_customers.csv</code> with the appropriate file names for your dataset. This code will read the data, merge the DataFrames, analyze the sales volume and revenue for each product category across different regions in Brazil, and display the results.</p> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>The interesting fact is that, following the language of the conversation, ChatGPT assumed we would be interested in English names of categories rather than Portuguese, even though it itself is fluent in both languages.</p> 
  </div> 
  <div class="readable-text intended-text" id="p106"> 
   <p>The resulting DataFrame has 54 columns. If only generative AI would suggest a way to make interpreting the results easier, right? Well, yes and no. It is important to remember that we don’t leave our own analytical skills at the front door when we let generative AI help us out. Any experienced data analyst could tell you that a large DataFrame such as this will benefit from the visualization of the results. So before we go further with the steps proposed by generative AI, we’ll take a moment to get some visuals. </p> 
  </div> 
  <div class="readable-text prompt" id="p107"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you propose visualization methods for <code>sales_by_region</code> DataFrame, which would allow answering the posed research question? </p> 
  </div> 
  <div class="readable-text response" id="p108"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Certainly! To visualize the <code>sales_by_region</code> DataFrame and answer the research question effectively, you can use the following visualization methods:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p109"> Bar plot: You can create a grouped bar plot to compare sales volume and revenue for each product category across different regions in Brazil. The x-axis will represent the regions, and the y-axis will represent the sales volume and revenue. You can use different colors for each product category to distinguish them. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p110"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.barplot(data=sales_by_region, x='customer_state', y='revenue', 
<span class="">↪</span>hue='product_category_name_english', ci=None)
plt.title('Revenue by Product Category and Region')
plt.show()

plt.figure(figsize=(12, 8))
sns.barplot(data=sales_by_region, x='customer_state', y='sales_volume',
<span class="">↪</span>hue='product_category_name_english', ci=None)
plt.title('Sales Volume by Product Category and Region')
plt.show()</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p111"><span class="faux-ol-li-counter">2. </span> Heatmap: Another way to visualize the sales performance across different regions in Brazil is by using a heatmap. You can create a pivot table with regions as rows, product categories as columns, and sales volume and revenue as values. Then, you can use a heatmap to represent the sales performance using a color gradient. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p112"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import seaborn as sns
import matplotlib.pyplot as plt

<em># Create pivot tables for sales_volume and revenue</em>
sales_volume_pivot = sales_by_region.pivot_table(
<span class="">↪</span>values='sales_volume', index='customer_state', 
<span class="">↪</span>columns='product_category_name_english')
revenue_pivot = sales_by_region.pivot_table(
<span class="">↪</span>values='revenue', index='customer_state', 
<span class="">↪</span>columns='product_category_name_english')

<em># Plot heatmaps</em>
plt.figure(figsize=(12, 8))
sns.heatmap(sales_volume_pivot, annot=True, cmap='viridis', fmt='.0f')
plt.title('Sales Volume by Product Category and Region')
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(revenue_pivot, annot=True, cmap='viridis', fmt='.0f')
plt.title('Revenue by Product Category and Region')
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p113"> 
   <p>These visualization methods will help you better understand the sales performance of different product categories across various regions in Brazil.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>This code, instead of producing graphs, resulted in a warning and the following error message: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p115"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><strong>FutureWarning:</strong> The `ci` parameter is deprecated. 
<span class="">↪</span>Use `errorbar=None` for the same effect.
<strong>ValueError:</strong> Could not interpret input 'customer_state'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>We present this transcript as a warning. Bugs and problems <em>are</em> to be expected. Getting generative AI to debug the code can take some effort, as appendix B shows. However, this exercise resulted in two conclusions worth presenting here:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p117"> Generative AIs may provide you with incorrect code, and more than once at that. We can still use them to debug the code (with more or less success, depending on the language), as it may take less time than learning all the intricacies of syntax from the ground up yourself. </li> 
   <li class="readable-text" id="p118"> You should not leave the debugging process to generative AI completely, as it doesn’t have access to the results. Looking for oddities in the output and informing your AI advisor about them significantly speeds up the process. </li> 
  </ul> 
  <div class="readable-text" id="p119"> 
   <p>Thanks to the provided code, we were able to generate four figures: two bar plots and two heatmaps. </p> 
  </div> 
  <div class="readable-text intended-text" id="p120"> 
   <p>The bar plots may not be suitable for adding to a presentation destined for an SD projector. In figure 3.1 you can find one of them; the second one is almost identical. To make it fit the page, we dropped the legend containing 71 categories (they are still present on the figure generated in the accompanying Jupyter notebook). It’s still unreadable, but that’s hardly generative AI’s fault. We are trying to visualize almost six thousand data points, after all!</p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>To make this figure more consumable, we can ask our AI advisor to break it down by state or product group, but we’ll end up with dozens of charts. Let’s check if using heatmaps will save us this trouble. A visualization of the same data is presented in figure 3.2. Both heatmaps are available in the Jupyter Notebook in the book’s GitHub repository.</p> 
  </div> 
  <div class="readable-text intended-text" id="p122"> 
   <p>Unfortunately, the generated heatmap is not much more readable, is it? We printed this indecipherable image in this book as valuable learning material.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p123">  
   <img alt="figure" src="../Images/CH03_F01_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.1</span> Bar plot of sales volume by product category and region</h5>
  </div> 
  <div class="browsable-container figure-container" id="p124">  
   <img alt="figure" src="../Images/CH03_F02_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.2</span> Heatmap of sales revenue by product category and region</h5>
  </div> 
  <div class="readable-text" id="p125"> 
   <p>You will see unreadable graphics in your work every now and then. If you’re asked to prepare “something that will fit a PowerPoint slide,” it can really ruin your day. In such a case, you need to consider whether you face an inherent issue, like the number of bars in figure 3.1, or if the visual clutter can be remedied with a simple reformatting. If you’re unsure how to tweak your visualization software’s settings, generative AI is there for you! You should get the solution and an explanation to help you in your future work.</p> 
  </div> 
  <div class="readable-text prompt" id="p126"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you modify the heatmap code so the numbers in the matrix are printed in a smaller font? </p> 
  </div> 
  <div class="readable-text response" id="p127"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Sure! You can adjust the font size of the numbers in the heatmap by using the <code>annot_kws</code> parameter within the <code>sns.heatmap</code> function. You can set the font size to a specific value as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p128"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">plt.figure(figsize=(12, 12))
sns.heatmap(revenue_df.set_index('product_category'), 
<span class="">↪</span>cmap="YlGnBu", annot=True, fmt=".0f", linewidths=0.5, 
<span class="">↪</span>annot_kws={"size": 6})
plt.title("Revenue by Product Category and State")
plt.show()

plt.figure(figsize=(12, 12))
sns.heatmap(sales_volume_df.set_index('product_category'), 
<span class="">↪</span>cmap="YlGnBu", annot=True, fmt=".0f", linewidths=0.5, 
<span class="">↪</span>annot_kws={"size": 6})
plt.title("Sales Volume by Product Category and State")
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p129"> 
   <p>In this example, I set the font size to 6. You can change this value as needed to achieve the desired appearance.</p> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>Before we proceed to present the result, we need to note that the <code>revenue_df</code> DataFrame mentioned in the code was defined in the previously mentioned appendix B, where we show a long and winding road to the correct code. This time, however, things progressed as expected. The result is presented in figure 3.3.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p131">  
   <img alt="figure" src="../Images/CH03_F03_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.3</span> Heatmap of sales revenue by product category by state, reformatted</h5>
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>The overall result is now possible to analyze. Maybe not so much in terms of reading individual numbers, but that’s precisely the purpose of the heatmap. It’s meant to pinpoint outliers in the dimensional distribution. </p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>The other visualization proposed is stacked bar plots, which are included in the book’s GitHub repository.</p> 
  </div> 
  <div class="readable-text intended-text" id="p134"> 
   <p>At this stage, we are well equipped with visualizations for drawing business conclusions on the most popular product categories. Following our AI advisor’s guidance, we will now move on to research their performance.</p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_40"><span class="num-string">3.3.2</span> Performance of products in their categories and regions</h3> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>As a reminder, based on our data availability, our generative AI advisor proposed the following approach to measuring performance: “How does the average product price, freight value, and payment value differ across different product categories and regions?” The information on the performance and behavior of products across categories and different locations is a trove of information for the business. Using this methodology, you may support decisions driving sales and marketing efforts. For example, best practices from best-performing regions could be used to boost the efficiency of weaker ones, or some products could be dropped and replaced by more profitable ones. <span class="aframe-location"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>However, this one application wouldn’t be worth reading about unless you’re genuinely excited by marketing. Fortunately, by applying this analytical framework, you can explore a lot of different variables to extract insights across various industries. Here are some examples:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p138"> <em>Customer lifetime value </em><em>(CLV)</em><em> and acquisition cost</em><em> </em>—In industries like e-commerce, telecommunications, or financial services, analyzing the relationship between the CLV and the cost of acquiring customers across different products or services and regions can help optimize marketing spend and target customer segments more effectively. </li> 
   <li class="readable-text" id="p139"> <em>Service response time and customer satisfaction</em><em> </em>—For service-oriented sectors like healthcare, logistics, or customer support, examining how response times relate to customer satisfaction across different service categories and regions can pinpoint areas for improvement and guide resource allocation. </li> 
   <li class="readable-text" id="p140"> <em>Inventory turnover and sales performance</em><em> </em>—In retail and manufacturing, analyzing the relationship between inventory turnover rates and sales performance across product categories and regions can help manage inventory more efficiently, reduce holding costs, and identify demand patterns. </li> 
   <li class="readable-text" id="p141"> <em>Renewable energy production and consumption</em><em> </em>—In the utilities and energy sector, examining the relationship between renewable energy production (e.g., solar, wind) and consumption across different regions can inform infrastructure investments and sustainability initiatives. </li> 
   <li class="readable-text" id="p142"> <em>Occupancy rates and rental yields</em><em> </em>—In real estate, particularly within the hospitality and residential sectors, analyzing the relationship between occupancy rates and rental yields across different property types and locations can aid investment decisions and pricing strategies. </li> 
   <li class="readable-text" id="p143"> <em>Utilization rates and operational costs</em><em> </em>—For transportation and logistics, looking at how vehicle or asset utilization rates relate to operational costs across different regions or routes can optimize operations and improve profitability. </li> 
   <li class="readable-text" id="p144"> <em>R&amp;D spend and innovation outcomes</em><em> </em>—In technology and pharmaceutical industries, analyzing the relationship between R&amp;D spending and outcomes such as patent filings or product launches across different categories can guide investment strategies and innovation focus. </li> 
   <li class="readable-text" id="p145"> <em>Digital marketing spend and engagement metrics</em><em> </em>—For businesses investing in digital marketing, examining how spending relates to engagement metrics (e.g., click-through rates, conversion rates) across different campaigns or regions can optimize marketing strategies and budget allocation. </li> 
   <li class="readable-text" id="p146"> <em>Workforce training investment and productivity gains</em><em> </em>—In sectors with significant human capital, such as education, healthcare, or manufacturing, analyzing the relationship between investment in workforce training and productivity gains across different departments or locations can inform talent development strategies. </li> 
   <li class="readable-text" id="p147"> <em>Sustainability initiatives and corporate performance</em><em> </em>—Across industries, examining the relationship between investments in sustainability initiatives (e.g., carbon footprint reduction, waste management) and corporate performance metrics (e.g., profitability, brand perception) can guide corporate social responsibility efforts and investment. </li> 
  </ul> 
  <div class="readable-text" id="p148"> 
   <p>Each of these examples applies the analytical approach of exploring relationships and variations across categories and regions to different variables. </p> 
  </div> 
  <div class="readable-text intended-text" id="p149"> 
   <p>Let’s look at how we can capture performance differences between categories, as they may be valuable for business decision-making. </p> 
  </div> 
  <div class="readable-text prompt" id="p150"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Provide me with a complete Python code solution required to answer the question, “What are the most popular product categories in terms of sales volume and revenue, and how do their sales performance vary across different regions in Brazil?” </p> 
  </div> 
  <div class="readable-text response" id="p151"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To answer this question, you can use the merged DataFrame that you created earlier and perform a <code>groupby</code> operation on product categories and regions. You will then calculate the average product price, freight value, and payment value for each group.</p> 
  </div> 
  <div class="readable-text response" id="p152"> 
   <p>Here’s the Python code to achieve this:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p153"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd
<em># Assuming you have already merged the DataFrames and named it </em>
<em><span class="">↪</span>'merged_data'</em>

<em># Group the data by product category and region (state)</em>
grouped_data = merged_data.groupby(
<span class="">↪</span>['product_category_name_english', 'customer_state'])

<em># Calculate the average product price, freight value, </em>
<em><span class="">↪</span>and payment value for each group</em>
average_values = grouped_data[
<span class="">↪</span>['price', 'freight_value', 'payment_value']].mean()

<em># Reset the index to have a flat DataFrame</em>
average_values = average_values.reset_index()

<em># Rename the columns for clarity</em>
average_values.columns = ['product_category', 'state', 'avg_product_price', 
<span class="">↪</span>'avg_freight_value', 'avg_payment_value']

<em># You can now explore the 'average_values' DataFrame </em>
<em><span class="">↪</span>to analyze the differences across product categories and regions</em></pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p154"> 
   <p>The <code>average_values</code> DataFrame will contain the average product price, freight value, and payment value for each product category and region (state). You can further analyze this DataFrame using various visualization techniques, such as bar charts or heatmaps, to better understand the differences across product categories and regions.</p> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>We presented the received answer in its entirety, as there are three noteworthy elements: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p156"> The first is generative AI’s ability to predict our next steps and suggest visualization techniques. </li> 
   <li class="readable-text" id="p157"> The second is that it considers the readability of the column names. </li> 
   <li class="readable-text" id="p158"> The last one is, unfortunately, that it used the wrong name of the DataFrame. We never defined a <code>merged_data</code> DataFrame. </li> 
  </ul> 
  <div class="readable-text" id="p159"> 
   <p>We want you to pay attention now. You could assume that the <code>merged_data</code> DataFrame is just <code>df_merge</code> from the previous example. But, as we repeat over and over, cultivate your paranoia! If there is any doubt, you should double- and triple-check to make sure what you are analyzing is indeed what you intend to analyze!</p> 
  </div> 
  <div class="readable-text prompt" id="p160"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you write the columns that should be in the <code>merged_data</code> DataFrame you used? </p> 
  </div> 
  <div class="readable-text response" id="p161"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Certainly! Based on the data we discussed earlier, the <code>merged_data</code> DataFrame should contain the following columns:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p162"> <code>product_category_name_english</code> (<code>str</code>) - category name in English </li> 
   <li class="readable-text" id="p163"> <code>customer_state</code> (<code>str</code>) - customer state </li> 
   <li class="readable-text" id="p164"> <code>price</code> (<code>float64</code>) - item price </li> 
   <li class="readable-text" id="p165"> <code>freight_value</code> (<code>float64</code>) - item freight value (if an order has more than one item, the freight value is split between items) </li> 
   <li class="readable-text" id="p166"> <code>payment_value</code> (<code>float64</code>) - transaction value </li> 
  </ol> 
  <div class="readable-text response" id="p167"> 
   <p>To create the <code>merged_data</code> DataFrame, you would need to merge the necessary DataFrames, as discussed earlier in the conversation. Here’s a quick recap of how to create the <code>merged_data</code> DataFrame:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p168"> Merge <code>df_order_items</code> with <code>df_orders</code> on <code>order_id</code> </li> 
   <li class="readable-text" id="p169"> Merge the result with <code>df_product_category_translation</code> on <code>product_category_name</code> </li> 
   <li class="readable-text" id="p170"> Merge the result with <code>df_customers</code> on <code>customer_id</code> </li> 
   <li class="readable-text" id="p171"> Merge the result with <code>df_order_payments</code> on <code>order_id</code> </li> 
  </ol> 
  <div class="readable-text response" id="p172"> 
   <p>The resulting <code>merged_data</code> DataFrame should contain the columns listed above, which can be used to answer the question related to average product price, freight value, and payment value across different product categories and regions.</p> 
  </div> 
  <div class="readable-text" id="p173"> 
   <p>Interestingly, <code>merged_data</code>, hallucinated by ChatGPT, and <code>df_merged</code>, which we actually have, do differ, and significantly! You would get an error if you tried to run the original code simply by replacing <code>merged_data</code> with <code>df_merged</code>.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p174"> 
   <p><span class="print-book-callout-head">Keeping the bells ringing</span>  The longer the conversation, the more likely generative AI is to lose track of nuances of the conversation. To ensure the proper contextual awareness of your AI advisor, when you notice it hallucinating about provided data structures, paste it a reminder with a detailed description of the data in question, or move on to a new conversation session.</p> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>Remember, that’s not the worst-case scenario. Errors that break the code are simple to spot. Errors where things go smoothly, but the results are based on wrong data, are a different kettle of fish. Consider this fair warning.</p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>The resulting DataFrame has 1,356 rows. Yet again, we should probably think of visualizing the result. </p> 
  </div> 
  <div class="readable-text prompt" id="p177"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Could you propose a visualization method suitable for data in the generated <code>average_values</code> DataFrame?</p> 
  </div> 
  <div class="readable-text response" id="p178"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Given the structure of the <code>average_values</code> DataFrame, you can create:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p179"> A scatterplot matrix (pair plot) to visualize the relationships between <code>average_ product_price</code>, <code>average_freight_value</code>, and <code>average_payment_value</code> across different product categories and states. You can use seaborn’s <code>pairplot</code> function for this purpose. </li> 
  </ol> 
  <div class="readable-text response" id="p180"> 
   <p>Here’s the code to create a scatterplot matrix:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p181"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import seaborn as sns

<em># Create a pair plot with different colors for each state</em>
g = sns.pairplot(data=average_values, hue='state', markers='.')

<em># Set the title and labels for each plot</em>
g.fig.suptitle(
<span class="">↪</span>"Scatterplot Matrix of Average Values by State and Product Category",
<span class="">↪</span>y=1.02)
g.set_axis_labels(
<span class="">↪</span>"Average Product Price", "Average Freight Value", 
<span class="">↪</span>"Average Payment Value")

<em># Adjust legend</em>
g.add_legend(title="State")

<em># Show the plot</em>
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p182"> 
   <p>This will give you a scatterplot matrix that shows the relationships between the average product price, average freight value, and average payment value for each product category in each state. The points in the scatterplots will be colored according to the state.</p> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p183"><span class="faux-ol-li-counter">2. </span> Three separate bar plots - one for each value of interest (average product price, average freight value, and average payment value). These bar plots will display the product categories on the x-axis and the corresponding values on the y-axis, with each bar representing a state. </li> 
  </ol> 
  <div class="readable-text response" id="p184"> 
   <p>Here’s Python code to create these bar plots using the seaborn library:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p185"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import seaborn as sns
import matplotlib.pyplot as plt

<em># Function to create bar plots for the given value</em>
def create_bar_plot(data, value_col, title, ylabel):
    plt.figure(figsize=(15, 6))
    sns.barplot(x='product_category', y=value_col, hue='state',
                <span class="">↪</span>data=data)
    plt.title(title)
    plt.xlabel('Product Category')
    plt.ylabel(ylabel)
    plt.xticks(rotation=90)
    plt.legend(title='State', bbox_to_anchor=(1, 1))
    plt.show()

<em># Create bar plots for average product price, average freight value, </em>
<em><span class="">↪</span>and average payment value</em>
create_bar_plot(average_values, 'avg_product_price', 
<span class="">↪</span>'Average Product Price by State and Product Category', 
<span class="">↪</span>'Average Product Price')
create_bar_plot(average_values, 'avg_freight_value', 
<span class="">↪</span>'Average Freight Value by State and Product Category', 
<span class="">↪</span>'Average Freight Value')
create_bar_plot(average_values, 'avg_payment_value', 
<span class="">↪</span>'Average Payment Value by State and Product Category', 
<span class="">↪</span>'Average Payment Value')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p186"> 
   <p>The resulting scatterplot is presented as figure 3.4, and one of the bar charts is presented as figure 3.5. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p187">  
   <img alt="figure" src="../Images/CH03_F04_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.4</span> Scatterplot of relationships between average payment values, average freight values, and average product prices</h5>
  </div> 
  <div class="browsable-container figure-container" id="p188">  
   <img alt="figure" src="../Images/CH03_F05_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.5</span> Grouped bar plot showing the relation of average product price per category per region<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p189"> 
   <p>The piece of code responsible for explicitly naming the chart axes didn’t work correctly. Fortunately, the DataFrame column names used by matplotlib as a default are informative enough. We left it as is to drive home a more general point: Sometimes, in your work, especially under time pressure, you’ll be tempted to “save time” on naming conventions. Quickly, all variables will become <code>x,</code> <code>tmp</code>, or <code>var1</code>—it’s a terrible habit. You’ll agree with us after you open a file you wrote two months ago. You’re saving seconds now to lose literally hours later. Using informative variable names is a time saver. Of course, being too verbose won’t do much for your code readability either. A variable named <code>second_attempt_to_average_the_sales_in_Joe_Smiths_analysis_ of_his_shop</code> may be overkill. </p> 
  </div> 
  <div class="readable-text intended-text" id="p190"> 
   <p>The resulting figure 3.5 may take some time (and a magnifying glass or two) to analyze, but it should suffice to provide some business insight into product performance. If you lack a magnifying glass of sufficient magnitude, we will discuss utilizing generative AI to cluster product categories in the next section, where we’ll analyze product perception.</p> 
  </div> 
  <div class="readable-text" id="p191"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_41"><span class="num-string">3.3.3</span> Review scores distribution</h3> 
  </div> 
  <div class="readable-text" id="p192"> 
   <p>The value of knowing the distribution of review scores is relatively apparent. It’s critical feedback about how the products are perceived by the people paying for them. As we mentioned before, there are certain methods and measures for representing overall client satisfaction as a single number. Still, we’ll forget about them for the time being (pretending we are working on unknown data) and try to answer the question proposed by our AI advisor: “What is the distribution of review scores across different product categories?”</p> 
  </div> 
  <div class="readable-text intended-text" id="p193"> 
   <p>If you feel like skipping this exercise because you know how to make a bar chart of a single variable in Excel, you can skip it. We’re not rediscovering the wheel here. However, if you feel like spending a minute learning how generative AI can help you work on the granularity of your results, read on! </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p194"> 
   <p><span class="print-book-callout-head">Repetition is a mother of learning</span>  We previously wrote about the issues with how far back generative AI can reach in its conversation context analysis. Every now and then, you should clarify whether it remembers all the details correctly. If there is a problem, you can remind it of the full data description instead of correcting one variable after another.</p> 
  </div> 
  <div class="readable-text" id="p195"> 
   <p>Let’s see if generative AI will provide any helpful twists to this simple exercise. After jogging generative AI’s memory on the structure of the involved DataFrames (as presented in appendix A), we received the following answer to our question.</p> 
  </div> 
  <div class="readable-text prompt" id="p196"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How should I extract information on the distribution of review scores, and how does it vary across different product categories and sellers? </p> 
  </div> 
  <div class="readable-text response" id="p197"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To analyze the distribution of review scores across different product categories and sellers, you can follow these steps:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p198"> Merge the required DataFrames: You may need to merge the <code>reviews</code>, <code>orders</code>, <code>order_items</code>, and <code>products</code> DataFrames to obtain a combined DataFrame containing review scores, product categories, and seller information. </li> 
   <li class="readable-text" id="p199"> Group the combined DataFrame by product categories and sellers: Use the <code>groupby()</code> function to group the data by <code>product_category_name_english</code> and <code>seller_id</code>. </li> 
   <li class="readable-text" id="p200"> Calculate the distribution of review scores: Use the <code>value_counts()</code> function to compute the count of each review score within each group. </li> 
  </ol> 
  <div class="readable-text response" id="p201"> 
   <p>Here’s the code to perform the above steps:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p202"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em><code><em># Merge the DataFrames</em></code></em>
merged_data = (
    df_orders.merge(df_order_items, on='order_id')
    .merge(df_products, on='product_id')
    .merge(df_order_reviews, on='order_id')
    .merge(df_product_category_translation, on='product_category_name')
)

<em><code><em># Group by product_category_name_english, seller_id, and review_score</em></code></em>
review_score_distribution = 
<span class="">↪</span>merged_data.groupby(['product_category_name_english', 'seller_id', 
<span class="">↪</span>'review_score']).size().reset_index(name='count')</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p203"> 
   <p>Now, the <code>review_score_distribution</code> DataFrame contains the distribution of review scores for each combination of product category and seller. To visualize this data, you could use a stacked bar chart, heatmap, or other suitable visualization method, depending on your preference.</p> 
  </div> 
  <div class="readable-text" id="p204"> 
   <p>The code worked as advertised, but the problem is that we again beat our own record, and now we have a DataFrame with 14,503 rows. We’re going to need visualizations again if we’re to make any sense of this. </p> 
  </div> 
  <div class="readable-text prompt" id="p205"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How should I visualize the <code>review_score_distribution</code> DataFrame to get to meaningful conclusions? </p> 
  </div> 
  <div class="readable-text response" id="p206"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>For visualizing the <code>review_score_distribution</code> DataFrame, you can use a grouped bar chart, where the x-axis represents the product categories, the y-axis represents the count of reviews, and different colors or patterns represent the different review scores. This will allow you to see how review scores are distributed across product categories and compare their performance in terms of customer satisfaction.</p> 
  </div> 
  <div class="readable-text response" id="p207"> 
   <p>Here’s a code snippet using Seaborn to create a grouped bar chart:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p208"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import seaborn as sns
import matplotlib.pyplot as plt

<em># Set a larger figure size</em>
plt.figure(figsize=(20, 10))

<em># Create the grouped bar chart</em>
sns.barplot(data=review_score_distribution,
    x='product_category_name_english',
    y='count',
    hue='review_score',
    ci=None)

<em># Rotate the x-axis labels for better readability</em>
plt.xticks(rotation=90)

<em># Set labels for the axes and the title</em>
plt.xlabel('Product Categories')
plt.ylabel('Review Count')
plt.title('Review Score Distribution by Product Category')

<em># Show the plot</em>
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p209"> 
   <p>This plot will give you an idea of how different product categories perform in terms of customer reviews. Keep in mind that this plot might be quite large and hard to read depending on the number of product categories in your dataset. You may want to filter or aggregate the data further to focus on specific categories or review score ranges.</p> 
  </div> 
  <div class="readable-text" id="p210"> 
   <p>It looks like generative AI was able to estimate that the size of the results will be bigger than the previous ones. Figure 3.6 shows that it wasn’t wrong. At this time, the advised course of action would be a trip to a stakeholder, who will receive this analysis and a solid heart-to-heart. They could decide to accept the oversized chart and promise never to print it, or they could propose an aggregation meaningful to them. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p211">  
   <img alt="figure" src="../Images/CH03_F06_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.6</span> Grouped bar plot showing review score counts by product category</h5>
  </div> 
  <div class="readable-text" id="p212"> 
   <p>For the sake of argument, we will assume that this analysis is being prepared for an elevator pitch and, as such, cannot be approved in advance. Let’s divide the reviews into positive (4 or 5 points) and negative (1, 2, 3 points), with us arbitrarily deciding what scores are considered positive (and negative). Getting the code from generative AI is straightforward.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p213"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.1</span> Generating a grouped bar plot</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import seaborn as sns
import matplotlib.pyplot as plt

def categorize_review_score(score):
    if score &gt;= 4:
        return 'Positive'
    else:
        return 'Negative'

review_score_distribution['review_category'] = 
<span class="">↪</span>review_score_distribution['review_score'].apply(categorize_review_score)

<em># Set a larger figure size</em>
plt.figure(figsize=(20, 10))

<em># Create the grouped bar chart</em>
sns.barplot(data=review_score_distribution,
    x='product_category_name_english',
    y='count',
    hue='review_category',
    ci=None)

<em># Rotate the x-axis labels for better readability</em>
plt.xticks(rotation=90)

<em># Set labels for the axes and the title</em>
plt.xlabel('Product Categories')
plt.ylabel('Review Count')
plt.title('Simplified Review Score Distribution by Product Category')

<em># Show the plot</em>
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p214"> 
   <p>As shown in figure 3.7, the result is a bit more readable than figure 3.6. With “bit” being the key word. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p215">  
   <img alt="figure" src="../Images/CH03_F07_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.7</span> Grouped bar plot showing aggregated review score counts by product category<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p216"> 
   <p>If it’s genuinely going to serve as an elevator pitch, we need to do better. That means we’ll need to aggregate product categories. However, our dataset doesn’t have an ontology categorizing products into even broader categories. Fortunately, generative AI is really good at providing first approximations in this type of task.</p> 
  </div> 
  <div class="readable-text prompt" id="p217"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I would like to add the aggregation by product category. Propose the grouping and explanation of the grouping decision for the following list of categories:</p> 
  </div> 
  <div class="readable-text prompt" id="p218"> 
   <p>[&lt;HERE WE PROVIDED A FULL LIST OF CATEGORIES&gt;]</p> 
  </div> 
  <div class="readable-text response" id="p219"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>One way to group the product categories is by considering their main industry or area of interest. This can make the visualization more manageable and reveal trends at a higher level. Here’s a proposed grouping for the given list of categories:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p220"> Home &amp; Furniture: <code>['bed_bath_table',</code> <code>'furniture_bedroom',</code> <code>'furniture_ decor',</code> <code>'furniture_living_room',</code> <code>'furniture_mattress_and_upholstery',</code> <code>'kitchen_dining_laundry_garden_furniture',</code> <code>'home_confort', 'home_ comfort_2', 'home_construction', 'housewares']</code> </li> 
   <li class="readable-text" id="p221"> Electronics &amp; Computers: <code>['audio', 'computers', 'computers_accessories', 'electronics', 'telephony', 'tablets_printing_image']</code> </li> 
   <li class="readable-text" id="p222"> Fashion: <code>['fashio_female_clothing',</code> <code>'fashion_bags_accessories', 'fashion_childrens_clothes',</code> <code>'fashion_male_clothing',</code> <code>'fashion_ shoes',</code> <code>'fashion_sport',</code> <code>'fashion_underwear_beach']</code> </li> 
   <li class="readable-text" id="p223"> Entertainment: <code>['art',</code> <code>'arts_and_craftmanship',</code> <code>'books_general_ interest', 'books_imported',</code> <code>'books_technical',</code> <code>'cds_dvds_musicals',</code> <code>'christmas_supplies',</code> <code>'consoles_games',</code> <code>'cool_stuff', 'cine_photo',</code> <code>'dvds_blu_ray',</code> <code>'music',</code> <code>'musical_instruments', 'toys',</code> <code>'watches_ gifts']</code> </li> 
   <li class="readable-text" id="p224"> Sports &amp; Leisure: <code>['sports_leisure',</code> <code>'garden_tools']</code> </li> 
   <li class="readable-text" id="p225"> Health &amp; Beauty: <code>['health_beauty',</code> <code>'perfumery']</code> </li> 
   <li class="readable-text" id="p226"> Baby &amp; Kids: <code>['baby',</code> <code>'diapers_and_hygiene',</code> <code>'fashion_childrens_ clothes']</code> </li> 
   <li class="readable-text" id="p227"> Office &amp; Industry: <code>['agro_industry_and_commerce',</code> <code>'industry_commerce_ and_business',</code> <code>'office_furniture',</code> <code>'stationery']</code> </li> 
   <li class="readable-text" id="p228"> Home Appliances: <code>['air_conditioning',</code> <code>'home_appliances',</code> <code>'home_ appliances_2',</code> <code>'small_appliances',</code> <code>'small_appliances_home_oven_and_ coffee']</code> </li> 
   <li class="readable-text" id="p229"> Tools &amp; Construction: <code>['construction_tools_construction',</code> <code>'construction_ tools_lights',</code> <code>'construction_tools_safety',</code> <code>'costruction_tools_ garden', 'costruction_tools_tools']</code> </li> 
   <li class="readable-text" id="p230"> Auto &amp; Accessories: <code>['auto',</code> <code>'fixed_telephony']</code> </li> 
   <li class="readable-text" id="p231"> Food &amp; Drink: <code>['food',</code> <code>'food_drink',</code> <code>'drinks']</code> </li> 
   <li class="readable-text" id="p232"> Miscellaneous: <code>['flowers',</code> <code>'la_cuisine',</code> <code>'luggage_accessories',</code> <code>'market_place', 'party_supplies', 'pet_shop', 'security_and_services', 'signaling_and_security']</code> </li> 
  </ol> 
  <div class="readable-text response" id="p233"> 
   <p>You can create a new column in the <code>review_score_distribution</code> DataFrame to represent these groupings:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p234"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">def categorize_product_category(category):
    category_mapping = {
        'Home &amp; Furniture': ['bed_bath_table', 'furniture_bedroom', 
        <span class="">↪</span>'furniture_decor', 'furniture_living_room', 
        <span class="">↪</span>'furniture_mattress_and_upholstery', 
        <span class="">↪</span>'kitchen_dining_laundry_garden_furniture', 'home_confort', 
        <span class="">↪</span>'home_comfort_2', 'home_construction', 'housewares'],
<em>        # ... Include other mappings here ...</em>
    }
    for key, values in category_mapping.items():
        if category in values:
            return key
    return 'Unknown'

review_score_distribution['product_category_group'] = 
<span class="">↪</span>review_score_distribution['product_category_name_english'].apply(
<span class="">↪</span>categorize_product_category)</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p235"> 
   <p>Now you can visualize the aggregated review score distribution for these groupings using a grouped bar chart or a stacked bar chart, as discussed earlier.</p> 
  </div> 
  <div class="readable-text" id="p236"> 
   <p>Our generative AI advisor presented an impressive level of understanding of the task. If you are interested in our discussion with ChatGPT on placing <code>fixed_telephony</code> in “Auto &amp; Accessories” and making it a complete <code>category_mapping</code> dictionary, we propose reading appendix C.</p> 
  </div> 
  <div class="readable-text intended-text" id="p237"> 
   <p>As seen in figure 3.8, the result is much more comprehensible than previous versions.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p238">  
   <img alt="figure" src="../Images/CH03_F08_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.8</span> Grouped bar plot showing aggregated review score counts by product categories aggregated into product groups</h5>
  </div> 
  <div class="readable-text intended-text" id="p239"> 
   <p>Having secured analyzable results of the review score distribution, we can now move on to the last of the descriptive analyses—the description of order statuses and their distributions.</p> 
  </div> 
  <div class="readable-text" id="p240"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_42"><span class="num-string">3.3.4</span> Order status</h3> 
  </div> 
  <div class="readable-text" id="p241"> 
   <p>There are multiple reasons to research order statuses, from identifying problems with logging systems to improving the user experience. To answer the question, “How does the order status (e.g., delivered, shipped, etc.) vary across different regions, product categories, and sellers?” we will extract information about types of order statuses across categories and regions. Later, in section 3.4, we will do a more advanced regression analysis on delivery times.</p> 
  </div> 
  <div class="readable-text intended-text" id="p242"> 
   <p>This will be a recurring theme in your work. You will take a first glance at the data when cleaning it, a second during descriptive analytics, and sometimes then decide what data is rich enough to drive data modeling. This may seem inconsistent with our previous remarks about having a strict protocol, but that’s not the case. Your protocol should be flexible enough to allow for evaluation and decisions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p243"> 
   <p>Generative AI will readily provide the code needed to merge the necessary DataFrames to get the required information in one place, and then to group and analyze the data.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p244"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.2</span> Preparing DataFrame with order statuses</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

orders_items = pd.merge(df_orders, df_order_items, on='order_id')

<em># Merge orders_items with df_products on 'product_id'</em>
orders_items_products = pd.merge(
<span class="">↪</span>orders_items, df_products, on='product_id')

<em># Merge orders_items_products with df_product_category_translation </em>
<em><span class="">↪</span>on 'product_category_name'</em>
merged_data = pd.merge(orders_items_products, 
<span class="">↪</span>df_product_category_translation, on='product_category_name')

<em># Group by 'order_status', 'product_category_name_english', </em>
<em><span class="">↪</span>and 'seller_id', then count the occurrences</em>
status_distribution = merged_data.groupby(['order_status', 
<span class="">↪</span>'product_category_name_english', 'seller_id']).size().reset_index(
<span class="">↪</span>name='count')

<em># Sort the result by 'order_status', 'product_category_name_english', </em>
<em><span class="">↪</span>and 'count' in descending order</em>
status_distribution_sorted = 
<span class="">↪</span>status_distribution.sort_values(['order_status', 
<span class="">↪</span>'product_category_name_english', 'count'], ascending=[True, True, 
<span class="">↪</span>False])

print(status_distribution_sorted)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p245"> 
   <p>The code is simple enough to work without a hitch. A glance at the resulting DataFrame shows that some orders have the status of <em>approved</em> but have apparently never shipped. This could be investigated further. </p> 
  </div> 
  <div class="readable-text intended-text" id="p246"> 
   <p>However, with a result including 7,524 rows, we need something worth a thousand words to analyze these results. Fortunately, in data analytics, as we prove over and over again, the power of generative AI goes way beyond code generation.</p> 
  </div> 
  <div class="readable-text prompt" id="p247"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>What type of visualization would you propose for the previously generated <code>status_ distribution_sorted</code> DataFrame? </p> 
  </div> 
  <div class="readable-text response" id="p248"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>For the <code>status_distribution_sorted</code> DataFrame, a stacked bar chart would be an appropriate visualization. This will allow you to display the distribution of order statuses for each product category while also making it easy to compare the total number of orders among product categories.</p> 
  </div> 
  <div class="readable-text response" id="p249"> 
   <p>To create a stacked bar chart, we need to use the pandas <code>plot</code> function with the <code>kind='bar'</code>, <code>vstacked=True</code> parameters. First, we need to reshape the <code>status_ distribution_sorted</code> DataFrame into a format suitable for creating a stacked bar chart.</p> 
  </div> 
  <div class="readable-text response" id="p250"> 
   <p>Here’s the code to create the stacked bar chart for the <code>status_distribution_ sorted</code> DataFrame:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p251"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd
import matplotlib.pyplot as plt

<em># Pivot the DataFrame for the stacked bar chart</em>
status_pivot = 
<span class="">↪</span>status_distribution_sorted.pivot_table(
<span class="">↪</span>index='product_category_name_english',
    columns='order_status',
    values='count',
    aggfunc='sum',
    fill_value=0)

<em># Plot the stacked bar chart</em>
plt.figure(figsize=(20, 10))
status_pivot.plot(kind='bar', stacked=True, figsize=(20, 10))

plt.xticks(rotation=90, fontsize=10)
plt.yticks(fontsize=12)
plt.xlabel('Product Category', fontsize=12)
plt.ylabel('Count of Orders', fontsize=12)
plt.title('Order Status Distribution by Product Category', fontsize=14)
plt.legend(title='Order Status', title_fontsize=12, fontsize=10, 
<span class="">↪</span>loc='upper right')

plt.tight_layout()
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p252"> 
   <p>This code pivots the <code>status_distribution_sorted</code> DataFrame, creating a new DataFrame with the product categories as the index and order statuses as columns. It then creates a stacked bar chart using the plot function with <code>kind='bar'</code> and <code>stacked=True</code> parameters. The remaining lines adjust the figure size, font size, and rotation of the x-axis labels for better readability.</p> 
  </div> 
  <div class="readable-text" id="p253"> 
   <p>The result of this code is presented in figure 3.9. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p254">  
   <img alt="figure" src="../Images/CH03_F09_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.9</span> Stacked bar plot showing order status delivery counts per product category</h5>
  </div> 
  <div class="readable-text" id="p255"> 
   <p>Though it might be challenging to discern from the chart, due to its scale, the lion’s share of statuses are marked as “delivered,” irrespective of the product category. The occurrences of other statuses are minimal in comparison.</p> 
  </div> 
  <div class="readable-text intended-text" id="p256"> 
   <p>If we were to just going to generate another bar chart, we wouldn’t bother you with reading this section. Let’s try to go beyond the obvious and add our own twist to the question proposed by our AI advisor.</p> 
  </div> 
  <div class="readable-text prompt" id="p257"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Are there any patterns in order delivery time based on the customer’s and seller’s location?</p> 
  </div> 
  <div class="readable-text response" id="p258"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To analyze patterns in order delivery time based on customer’s and seller’s location, we can first calculate the delivery time for each order and then visualize the relationship between delivery time, customer’s location, and seller’s location.</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p259"> Calculate delivery time: </li> 
  </ol> 
  <div class="readable-text response" id="p260"> 
   <p>We will use the <code>df_orders</code> DataFrame and calculate the delivery time as the difference between <code>order_delivered_customer_date</code> and <code>order_purchase_ timestamp</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p261"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

df_orders['order_purchase_timestamp'] = 
<span class="">↪</span>pd.to_datetime(df_orders['order_purchase_timestamp'])
df_orders['order_delivered_customer_date'] = 
<span class="">↪</span>pd.to_datetime(df_orders['order_delivered_customer_date'])
df_orders['delivery_time'] = (df_orders['order_delivered_customer_date'] –
<span class="">↪</span>df_orders['order_purchase_timestamp']).dt.days</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p262"><span class="faux-ol-li-counter">2. </span> Merge necessary columns from <code>df_customers</code> and <code>df_sellers</code> DataFrames with <code>df_orders</code>: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p263"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">df_orders_customers = df_orders.merge(df_customers[
<span class="">↪</span>['customer_id', 'customer_city', 'customer_state']], on='customer_id')
df_orders_customers_sellers = 
<span class="">↪</span>df_orders_customers.merge(df_order_items[
<span class="">↪</span>['order_id', 'seller_id']], on='order_id')
df_orders_customers_sellers = 
<span class="">↪</span>df_orders_customers_sellers.merge(
<span class="">↪</span>df_sellers[['seller_id', 'seller_city', 'seller_state']], 
<span class="">↪</span>on='seller_id')</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p264"><span class="faux-ol-li-counter">3. </span> Visualize the relationship between delivery time, customer’s location, and seller’s location: </li> 
  </ol> 
  <div class="readable-text response" id="p265"> 
   <p>A suitable visualization method for this purpose is a scatter plot, where we can use the customer’s state and seller’s state on the x and y axes, respectively, and color the points based on the delivery time.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p266"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(16, 12))
sns.scatterplot(data=df_orders_customers_sellers, x='customer_state', 
<span class="">↪</span>y='seller_state', hue='delivery_time', palette='coolwarm', alpha=0.6)
plt.xlabel("Customer State")
plt.ylabel("Seller State")
plt.title("Delivery Time by Customer and Seller State")
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p267"> 
   <p>This visualization will help us identify any patterns or trends in delivery time based on the customer’s and seller’s locations. Keep in mind that this is a high-level overview, and further analysis may be required to draw more specific conclusions.</p> 
  </div> 
  <div class="readable-text" id="p268"> 
   <p>Interestingly, this time we received the end-to-end solution. From merging required DataFrames, to feature engineering (creating a completely new column), to visualization! That’s impressive. </p> 
  </div> 
  <div class="readable-text intended-text" id="p269"> 
   <p>In the notebook in our GitHub repository, you’ll notice that the initial visualization is less than impressive. The data distribution is clearly skewed toward low values, while high-value outliers “hijacked” the upper part of the color range, leaving the vast majority of results barely distinguishable. In addition, the order of regions on both axes is random. However, it wasn’t much work to get code to deal with these problems.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p270"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.3</span> Creating a log-scaled heatmap</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import matplotlib.colors as mcolors
import matplotlib.pyplot as plt

<em># Sort the data by customer_state and seller_state</em>
df_sorted = df_orders_customers_sellers.sort_values(
<span class="">↪</span>by=['customer_state', 'seller_state'])

<em># Create the scatter plot</em>
plt.figure(figsize=(16, 12))

<em># Use LogNorm for color normalization</em>
norm = mcolors.LogNorm(
<span class="">↪</span>vmin=df_orders_customers_sellers['delivery_time'].min() + 1, 
<span class="">↪</span>vmax=df_orders_customers_sellers['delivery_time'].max())

plt.scatter(x=df_sorted['customer_state'], y=df_sorted['seller_state'], 
<span class="">↪</span>c=df_sorted['delivery_time'], cmap='coolwarm', alpha=0.6, norm=norm)

plt.xlabel("Customer State")
plt.ylabel("Seller State")
plt.title("Log-scaled Delivery Time by Customer and Seller State")
plt.colorbar(label="Delivery Time (log-scaled)")

<em># Get the unique state values and sort them</em>
sorted_states = sorted(df_sorted['customer_state'].unique())

# Set the same order for both x and y axes
ax = plt.gca()
ax.set_xticks(range(len(sorted_states)))
ax.set_xticklabels(sorted_states)
ax.set_yticks(range(len(sorted_states)))
ax.set_yticklabels(sorted_states)

plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p271"> 
   <p>Applying generative AI’s suggestion to use the logarithmic scale solved the “blueiness” problem we encountered, and the alphabetical order of regions makes finding relevant pairs much easier. As you can see in figure 3.10, the result is a perfectly analyzable image; we will conduct the analysis itself in chapter 4. For now, it’s enough to see that there are indeed differences in delivery times depending on the region.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p272">  
   <img alt="figure" src="../Images/CH03_F10_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.10</span> Scatterplot of delivery times from one state to another, colored by a log-norm scale, with identical order of elements on both axes<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text print-book-callout" id="p273"> 
   <p><span class="print-book-callout-head">“when we talk about tastes and colors, there is nothing to be disputed” </span> Certain elements of human perception may be hard for a computer program to notice. If done properly, things like the distribution of colors or order of elements will enhance the human ability to absorb the information presented. You can try to get some theoretical background and sound advice from generative AI, but the responsibility for the clarity and readability of the final results is on you. </p> 
  </div> 
  <div class="readable-text" id="p274"> 
   <p>In this section, you learned that generative AI can provide you with the code required to perform all the necessary elements of descriptive analytics. It’s not error-free, but you must remember that LLMs do not run actual calculations or transformations, despite being computer programs. They generate answers based on their understanding of the language patterns involved in mathematical reasoning rather than by performing a numerical calculation. The accuracy of the provided answers depends on how well-represented related mathematical patterns are in the data they were trained on. However, descriptive analysis is relatively easy. Let’s see how well our AI advisor fares when facing more demanding tasks. </p> 
  </div> 
  <div class="readable-text" id="p275"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_43"><span class="num-string">3.4</span> Inferential analysis</h2> 
  </div> 
  <div class="readable-text" id="p276"> 
   <p>In the previous section, you mainly visualized data, grouping it this way or another along the way. Luckily (from the business decision-making perspective), or unluckily (from your workload perspective), there is more to descriptive analysis than bar and line charts. More even than scatterplots. </p> 
  </div> 
  <div class="readable-text intended-text" id="p277"> 
   <p>Imagine that your organization has recently implemented a novel marketing strategy aimed at increasing product sales. The plan was rolled out across several regions with distinct market dynamics and consumer behaviors. To evaluate the strategy’s effectiveness, you need to determine not only whether sales have increased post-implementation but also if the observed changes can be attributed directly to the marketing efforts rather than external factors or mere chance. Furthermore, you’re tasked with identifying which specific strategy elements were most effective in different regions and among various demographic groups.</p> 
  </div> 
  <div class="readable-text intended-text" id="p278"> 
   <p>The toolbox you need to open for more advanced problems is called inferential or inference statistics. <em>Inference analysis</em> is a branch of statistics that focuses on drawing conclusions about a population based on data obtained from a sample (inferring things about the population based on what you find in the sample—it does what it says on the tin, really). It is grounded in probability theory and employs various statistical tests and methods to estimate population parameters, test hypotheses, and assess the strength and direction of relationships between variables. </p> 
  </div> 
  <div class="readable-text intended-text" id="p279"> 
   <p>The following sections will test generative AI’s ability to guide us through this statistics-laced minefield.</p> 
  </div> 
  <div class="readable-text" id="p280"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_44"><span class="num-string">3.4.1</span> Before you begin</h3> 
  </div> 
  <div class="readable-text" id="p281"> 
   <p>Unfortunately, we need to put on our Don Quixote cap (or helmet) for a moment. The methods we will show you are not very popular, and you may rightfully ask yourself, why don’t we use methods we’ll find in most textbooks or websites? There are a couple of reasons for that.</p> 
  </div> 
  <div class="readable-text intended-text" id="p282"> 
   <p>First, we perceive our role as pointing you to ways of doing things right, where right means analyzing your data in a way that allows you to draw correct conclusions. </p> 
  </div> 
  <div class="readable-text intended-text" id="p283"> 
   <p>The second reason results from the availability of generative AI. Where previously you’d need to spend a considerable amount of time learning the intricacies of statistics to even begin to do things right, you now have, at your fingertips, all the required knowledge and a tool to parse and explain it to you in a way you’ll feel comfortable with. Plus, this tool will provide the complete code to perform the analysis as a bonus. </p> 
  </div> 
  <div class="readable-text" id="p284"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">The importance of using confidence intervals in inference analysis</h4> 
  </div> 
  <div class="readable-text" id="p285"> 
   <p>In section 3.1, we mentioned that we disagree with generative AI’s popularity-based approach to selecting methods for evaluating the results of statistical analyses. We realize that for the majority of our readers, the concept on which we base the following analyses may be new, so we’ll present an overview here.</p> 
  </div> 
  <div class="readable-text intended-text" id="p286"> 
   <p>We already mentioned that the role of inference analysis is to draw conclusions about the population from the sample. Imagine we wanted to know the average monthly spending of our company’s customers on some product category (one they may also buy outside of our company). Since it’s not feasible to survey all our customers, we may decide to randomly select 100 customers and calculate their average spending. Let’s assume the result turned out to be $200.</p> 
  </div> 
  <div class="readable-text intended-text" id="p287"> 
   <p>The sample might not perfectly represent our entire customer base. We would then calculate the <em>confidence interval </em>(CI) to account for potential variability. A 95% confidence interval for average monthly spending might be between $180 and $220. This means that we can be 95% confident that the <em>true </em>average spending of all customers on this product category lies within that range. In other words, if we repeat the sampling and CI calculation 100 times, 95 of the calculated confidence intervals will contain the true mean.</p> 
  </div> 
  <div class="readable-text intended-text" id="p288"> 
   <p>Confidence intervals offer certain advantages over more widely used null hypothesis significance testing (NHST), or p-value, as they provide a range of plausible values for the population parameter, allowing for a more comprehensive understanding of the effect size and its uncertainty. </p> 
  </div> 
  <div class="readable-text intended-text" id="p289"> 
   <p>Therefore, confidence intervals contribute to a better interpretation of the results and more informed decision-making. This is why, despite the general, popularity-driven consensus approach proposed initially by ChatGPT, we modified the proposed analysis design for the questions we will discuss in following subsections and explicitly asked our AI advisor to focus on methods utilizing confidence intervals.</p> 
  </div> 
  <div class="readable-text" id="p290"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">An important note on the completeness of the data</h4> 
  </div> 
  <div class="readable-text" id="p291"> 
   <p>We use inference statistics here because we know that the data we have is a sample of a bigger set. If you work with a full dataset, such as all the customers you have and all the transactions made, there is no need to calculate confidence intervals for the purpose of historical data analysis, as your sample <em>is</em> your population. However, inference statistics will come in handy if you want to use your data for any kind of predictive analytics. </p> 
  </div> 
  <div class="readable-text intended-text" id="p292"> 
   <p>We are now equipped to plunge back into analyzing our e-commerce data. </p> 
  </div> 
  <div class="readable-text" id="p293"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_45"><span class="num-string">3.4.2</span> Relationship between product attributes and shipping costs</h3> 
  </div> 
  <div class="readable-text" id="p294"> 
   <p>The question, “What is the relationship between product attributes (e.g., weight, dimensions) and overhead costs (e.g., shipping costs)?” posed by our AI advisor may seem a little naive, but it will help us build an intuition about the power of confidence-interval–based linear regression. The importance of this tool is impossible to overestimate. You will encounter it when working in various fields:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p295"> <em>Economics</em><em> </em>—Economists use linear regression to understand the relationship between economic indicators, such as GDP growth and unemployment rates, or inflation and interest rates. Confidence intervals in these regressions help policymakers assess the reliability of these relationships when making fiscal or monetary policy decisions. </li> 
   <li class="readable-text" id="p296"> <em>Finance</em><em> </em>—In finance, linear regression is used to model the relationship between the risk and return of assets, as described by the capital asset pricing model (CAPM). Analysts use confidence intervals to gauge the precision of beta coefficients, which measure the sensitivity of asset returns to market movements, thereby informing investment strategies. </li> 
   <li class="readable-text" id="p297"> <em>Healthcare</em><em> </em>—Researchers employ linear regression to study the relationship between various risk factors (e.g., smoking, diet, physical activity) and health outcomes (e.g., heart disease, diabetes). Confidence intervals around the regression coefficients provide insights into the significance and reliability of these factors, guiding public health recommendations and patient treatment plans. </li> 
   <li class="readable-text" id="p298"> <em>Marketing</em><em> </em><em>—</em>Marketing analysts use linear regression to understand how different aspects of marketing strategies (e.g., advertising spend, promotional activities) influence sales or customer behavior. Confidence intervals help in assessing the strength of these relationships, optimizing marketing budgets, and tailoring strategies to maximize ROI. </li> 
   <li class="readable-text" id="p299"> <em>Real estate</em><em> </em>—Linear regression models are used to predict property prices based on features such as size, location, and amenities. Confidence intervals around the predictions provide potential buyers and sellers with a range of expected prices, aiding in negotiation and decision-making processes. </li> 
   <li class="readable-text" id="p300"> <em>Environmental science</em><em> </em>—Scientists apply linear regression to model the impact of human activities on environmental indicators, such as CO<sub>2</sub> emissions and global temperature rise, or the effect of environmental variables on biodiversity. Confidence intervals help quantify the uncertainty in these models, crucial for environmental policy and conservation strategies. </li> 
   <li class="readable-text" id="p301"> <em>Manufacturing</em><em> </em>—In the manufacturing sector, linear regression is used to model the relationship between production processes (e.g., temperature, pressure, material properties) and product quality or yield. Confidence intervals allow engineers to understand the variability in these processes and to optimize them for better outcomes. </li> 
   <li class="readable-text" id="p302"> <em>Education</em><em> </em>—Researchers use linear regression to explore the impact of educational interventions (e.g., class size, teaching methods, technology use) on student performance. Confidence intervals provide a measure of the effectiveness and reliability of these interventions, informing educational policy and practice. </li> 
   <li class="readable-text" id="p303"> <em>Sports analytics</em><em> </em>—Linear regression is applied in sports analytics to evaluate the performance of players based on various metrics (e.g., scoring, defensive actions) and their impact on game outcomes. Confidence intervals around these estimates help teams make strategic decisions about player training and game tactics. </li> 
   <li class="readable-text" id="p304"> <em>Technology and innovation</em><em> </em>—In technology companies, linear regression can analyze the relationship between features of a product or service and user engagement or satisfaction. Confidence intervals around these relationships guide product development, feature prioritization, and customer experience strategies. </li> 
  </ul> 
  <div class="readable-text" id="p305"> 
   <p>In our opinion, quantitatively checking how much one thing depends on the other is irreplaceable in any decision-making process. Fortunately, generative AI can help you go through the whole process quickly and efficiently.</p> 
  </div> 
  <div class="readable-text prompt" id="p306"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How can I check if there is a relationship between product attributes (e.g., weight, dimensions) and shipping costs?</p> 
  </div> 
  <div class="readable-text response" id="p307"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To investigate the relationship between product attributes (e.g., weight, dimensions) and shipping costs, you can use a linear regression model with confidence intervals. First, let’s combine the necessary data from the <code>df_products</code> and <code>df_order_items</code> DataFrames:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p308"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import summary_table

<em># Merge the DataFrames</em>
merged_df = df_order_items.merge(df_products, on="product_id", <span class="">↪</span>how="left")

<em># Calculate product volume</em>
merged_df["product_volume"] = merged_df["product_length_cm"] * 
<span class="">↪</span>merged_df["product_height_cm"] * merged_df["product_width_cm"]

<em># Filter out the relevant columns</em>
analysis_df = merged_df[
<span class="">↪</span>["freight_value", "product_weight_g", "product_volume"]]

<em># Remove rows with NaN values</em>
analysis_df = analysis_df.dropna()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p309"> 
   <p>Next, let’s build a linear regression model for each of the product attributes (weight and volume) against the shipping cost (freight value):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p310"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Linear regression for weight</em>
X_weight = sm.add_constant(analysis_df["product_weight_g"])
y = analysis_df["freight_value"]
model_weight = sm.OLS(y, X_weight).fit()

<em># Linear regression for volume</em>
X_volume = sm.add_constant(analysis_df["product_volume"])
y = analysis_df["freight_value"]
model_volume = sm.OLS(y, X_volume).fit()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p311"> 
   <p>Now, you can visualize the relationships and their confidence intervals:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p312"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Function to plot regression line with confidence interval</em>
def plot_regression_with_ci(X, y, model, xlabel, ylabel):
    fig, ax = plt.subplots()
    sns.regplot(x=X.iloc[:, 1], y=y, ax=ax, scatter_kws={'alpha': 0.3})
    st, data, ss2 = summary_table(model, alpha=0.05)
    predicted = data[:, 2]
    upper = data[:, 4]
    lower = data[:, 5]
    ax.plot(X.iloc[:, 1], predicted, '-', lw=2, color='red', 
    <span class="">↪</span>label='Fitted Line')
    ax.fill_between(X.iloc[:, 1], lower, upper, color='red', alpha=0.15, 
    <span class="">↪</span>label='95% CI')
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.legend()
    plt.show()

<em># Plot the relationship between weight and freight value</em>
plot_regression_with_ci(X_weight, y, model_weight, 
<span class="">↪</span>xlabel="Product Weight (g)", ylabel="Freight Value")

<em># Plot the relationship between volume and freight value</em>
plot_regression_with_ci(X_volume, y, model_volume, 
<span class="">↪</span>xlabel="Product Volume (cm³)", ylabel="Freight Value")</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p313"> 
   <p>By examining the plots and their 95% confidence intervals, you can determine whether there’s a relationship between the product attributes and shipping costs. A narrow confidence interval around the fitted line indicates a stronger relationship between the variables, while a wider interval suggests more uncertainty in the relationship.</p> 
  </div> 
  <div class="readable-text" id="p314"> 
   <p>Our AI advisor proposed a complete solution. It engineered the required DataFrame, built regression models, and proposed a visualization of results. The latter turned out to be way more cluttered than we could hope, but it is presented in figure 3.11 nevertheless.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p315">  
   <img alt="figure" src="../Images/CH03_F11_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.11</span> Scatterplot of the relationship between (a) freight value and product weight, and (b) freight value and volume</h5>
  </div> 
  <div class="readable-text" id="p316"> 
   <p>There are interesting vertical lines of data points at 30,000 g and 25,000 cm<sup>3</sup> in figure 3.11. We guess it’s caused by some standard package sizes used by delivery companies across Brazil. </p> 
  </div> 
  <div class="readable-text intended-text" id="p317"> 
   <p>The other thing worth discussing is that the confidence interval is so narrow that it’s not even visible in the picture. That means, from the statistical point of view, that the line most likely is where we drew it—we have so much data that we can be sure about our estimation’s precision. That doesn’t mean this regression is immediately useful, though. </p> 
  </div> 
  <div class="readable-text intended-text" id="p318"> 
   <p>The results are spread wide around the regression line. Its predictive power seems to be low, to say the least. If we have a package of 20 kg, for example, we can estimate its freight value to be anywhere between 0 and 200 BRL (it’s our assumption that that’s the monetary unit the data provider uses). </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p319"> 
   <p><span class="print-book-callout-head">How much wood would the woodchuck chuck?</span>  The main value of regressions is their ability to predict dependent variables based on a set of independent variables. To assess the performance of a regression model, we may employ various metrics, such as mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and R<sup>2</sup> (coefficient of determination). These metrics provide insights into the model’s accuracy, the magnitude of the errors, and the proportion of the variance in the dependent variable explained by the independent variables. Additionally, residual analysis and diagnostic plots can be used to examine the model’s assumptions, such as linearity, homoscedasticity, and the normality of residuals.</p> 
  </div> 
  <div class="readable-text" id="p320"> 
   <p>We promised you a quantitative analysis of the dependency, and we’re not going to stop at drawing a chart this time. These are several commonly used metrics that you’ll want to look at when analyzing regression data (especially in the context of its predictive power): </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p321"> <em>Mean absolute error </em><em>(MAE)</em> —The average of the absolute differences between predicted and actual values. It gives an idea of how wrong the predictions are. A smaller MAE indicates better predictive performance. </li> 
   <li class="readable-text" id="p322"> <em>Mean squared error </em><em>(MSE)</em> —The average of the squared differences between predicted and actual values. It tends to penalize large errors more than small ones. A smaller MSE indicates better predictive performance. </li> 
   <li class="readable-text" id="p323"> <em>Root mean squared error </em><em>(RMSE)</em> —The square root of the MSE. It has the same units as the dependent variable and is more interpretable than the MSE. A smaller RMSE indicates better predictive performance. </li> 
   <li class="readable-text" id="p324"> <em>R</em><sup>2</sup>—R<sup>2</sup> measures the proportion of variance in the dependent variable that can be explained by the independent variable(s). A higher R<sup>2</sup> value indicates better predictive performance. </li> 
  </ul> 
  <div class="readable-text" id="p325"> 
   <p>As usual, accepting the help of generative AI is the quickest way to get code that calculates all of these values.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p326"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.4</span> Predicting product weight or volume influence on freight value</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from sklearn.metrics import mean_absolute_error, mean_squared_error, 
<span class="">↪</span>r2_score

<em># Split the data into features (X) and target (y)</em>
X_weight = analysis_df["product_weight_g"].values.reshape(-1, 1)
X_weight = sm.add_constant(X_weight)  <em># Add a constant for the intercept</em>
X_volume = analysis_df["product_volume"].values.reshape(-1, 1)
X_volume = sm.add_constant(X_volume)  <em># Add a constant for the intercept</em>

y = analysis_df["freight_value"].values

<em># Predict the target variable using the models</em>
y_pred_weight = model_weight.predict(X_weight)
y_pred_volume = model_volume.predict(X_volume)

<em># Calculate the evaluation metrics for both models</em>
mae_weight = mean_absolute_error(y, y_pred_weight)
mse_weight = mean_squared_error(y, y_pred_weight)
rmse_weight = np.sqrt(mse_weight)
r2_weight = r2_score(y, y_pred_weight)

mae_volume = mean_absolute_error(y, y_pred_volume)
mse_volume = mean_squared_error(y, y_pred_volume)
rmse_volume = np.sqrt(mse_volume)
r2_volume = r2_score(y, y_pred_volume)

<em># Print the evaluation metrics</em>
print(f"Product Weight Model: MAE={mae_weight:.2f}, MSE={mse_weight:.2f}, 
<span class="">↪</span>RMSE={rmse_weight:.2f}, R2={r2_weight:.2f}")
print(f"Product Volume Model: MAE={mae_volume:.2f}, MSE={mse_volume:.2f}, 
<span class="">↪</span>RMSE={rmse_volume:.2f}, R2={r2_volume:.2f}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p327"> 
   <p>The proposed code should provide us with the following result:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p328"> 
   <div class="code-area-container"> 
    <pre class="code-area">Product Weight Model: MAE=6.89, MSE=156.77, RMSE=12.52, R2=0.37
Product Volume Model: MAE=7.41, MSE=163.70, RMSE=12.79, R2=0.34</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p329"> 
   <p>The results show that just about 35 percent of the variability in the freight value is explained by product weight or volume. It may be interesting to see if combining the two would increase the available predictive power.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p330"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.5</span> Predicting product weight and volume influence on freight value</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd
import numpy as np
import statsmodels.api as sm

<em># Merge the DataFrames</em>
merged_df = df_order_items.merge(df_products, on="product_id", how="left")

<em># Calculate product volume</em>
merged_df["product_volume"] = merged_df["product_length_cm"] * 
<span class="">↪</span>merged_df["product_height_cm"] * merged_df["product_width_cm"]

<em># Filter out the relevant columns</em>
analysis_df = merged_df[
<span class="">↪</span>["freight_value", "product_weight_g", "product_volume"]]

<em># Remove rows with NaN values</em>
analysis_df = analysis_df.dropna()

<em># Define the independent variables (weight and volume) </em>
<em><span class="">↪</span>and the dependent variable (freight value)</em>
X = analysis_df[['product_weight_g', 'product_volume']]
y = analysis_df['freight_value']

<em># Add a constant for the intercept</em>
X = sm.add_constant(X)

<em># Fit the model</em>
model = sm.OLS(y, X).fit()

<em># Print the summary</em>
print(model.summary())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p331"> 
   <p>The results may seem a bit daunting, but if you feel daring, generative AI should help you understand all of the following parameters. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p332"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"> OLS Regression Results
===========================================================================
Dep. Variable:       freight_value   R-squared:                       0.399
Model:                         OLS   Adj. R-squared:                  0.399
Method:              Least Squares   F-statistic:                 3.744e+04
Date:             Sun, 09 Apr 2023   Prob (F-statistic):               0.00
Time:                     15:38:56   Log-Likelihood:            -4.4203e+05
No. Observations:           112632   AIC:                         8.841e+05
Df Residuals:               112629   BIC:                         8.841e+05
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
===========================================================================
                     coef     std err         t    P&gt;|t|    [0.025   0.975]
---------------------------------------------------------------------------
const             13.7268    0.044      314.740    0.000    13.641  13.812
product_weight_g   0.0016    1.63e-05   101.023    0.000     0.002   0.002
product_volume     0.0002    2.61e-06    70.759    0.000     0.000   0.000
===========================================================================
Omnibus:                105287.730   Durbin-Watson:                   1.815
Prob(Omnibus):               0.000   Jarque-Bera (JB):         12678300.032
Skew:                        4.145   Prob(JB):                         0.00
Kurtosis:                   54.311   Cond. No.                     3.37e+04
===========================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors 
    <span class="">↪</span>is correctly specified.
[2] The condition number is large, 3.37e+04. This might indicate 
    <span class="">↪</span>that there are strong multicollinearity or other numerical problems.</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p333"> 
   <p><span class="print-book-callout-head">Note</span>  You can learn a lot about the measurements and parameters used in statistics from the perspective of someone dealing with money, rather than basic science, from <em>Mostly Harmless Econometrics</em> by Joshua Angrist and Jörn-Steffen Pischke (Princeton University Press, 2009).</p> 
  </div> 
  <div class="readable-text" id="p334"> 
   <p>The critical information here should be R<sup>2</sup>, which tells you that almost 40% of the variability in freight value can be explained by product weight and volume variability. This means that by controlling and optimizing these two variables, you can impact the freight value. If the freight value is an essential factor of the overall cost of the product, this result could lead to the conclusion that it would be worth trying to reduce the weight or size of products without reducing their functionality or quality. This could lead to freight cost savings.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p335"> 
   <p><span class="print-book-callout-head">Beggars can’t be choosers</span>  As we explained in chapter 2, the quality of results depends heavily on the quality and accessibility of data. Given a choice, you should avoid proposing business solutions based on data with the spread seen in figure 3.11. The observed effects are also not too significant. However, the key phrase is “given a choice.” Sometimes you will not have any better data, and your business may require any support it can get. Relying on even imperfect results like these may mean the difference between letting the business sink and giving it a fighting chance.</p> 
  </div> 
  <div class="readable-text" id="p336"> 
   <p>We will spend a bit more time drawing conclusions from this result in chapter 4. For now, we’ll turn our attention back to the review score.</p> 
  </div> 
  <div class="readable-text" id="p337"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_46"><span class="num-string">3.4.3</span> Relationship between product, transaction, shipping attributes, and the review score</h3> 
  </div> 
  <div class="readable-text" id="p338"> 
   <p>Your boss would be proud of you if they saw you doing what we will do in this section— merge the three research questions proposed by our AI advisor into one: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p339"> “What factors (e.g., delivery time, price, product attributes) contribute to differences in the average review score?” </li> 
   <li class="readable-text" id="p340"> “How do different payment methods and the number of payment installments affect order completion rates and customer satisfaction?” </li> 
   <li class="readable-text" id="p341"> “What is the impact of delivery time (actual vs. estimated) on customer satisfaction and review scores?” </li> 
  </ul> 
  <div class="readable-text" id="p342"> 
   <p>The method of answering them is identical, so why waste time (which is money) doing the same work three times? Well, that would only work if you don’t care too much about the reliability and accuracy of the results. That’s an important lesson: even if you technically can do something, it doesn’t mean you should. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p343"> 
   <p><span class="print-book-callout-head">Less is more</span>  In regression analysis, you could hope that the more independent variables you drop into the model, the better the result, but that is not necessarily the case. Too many variables may muddy the water, so to speak, as they can result in either overfitting the model or causing issues related to multicollinearity. Overfitting means your model starts to fit the noise, rather than the underlying patterns, and it usually occurs when you have too many parameters in relation to the number of observations. Multicollinearity occurs when two or more predictor variables are highly correlated—the estimated coefficients may then become unstable and sensitive to minor changes in the data. This instability makes it difficult to reliably interpret which variables are genuinely impactful and how much they contribute to explaining the dependent variable.</p> 
  </div> 
  <div class="readable-text" id="p344"> 
   <p>We have only limited information on the product and transaction process. We also know already that the majority of transactions are finished when they are delivered. This should prepare us not to expect great success in finding a correlation between these factors and the level of happiness expressed by the customers in their reviews. However, we think the techniques for investigating such relationships are worth learning about. </p> 
  </div> 
  <div class="readable-text prompt" id="p345"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>What inferential statistics should I use to research how the average review score varies across different product categories, and what factors (e.g., delivery time and its difference vs. estimation, price, product, and payment attributes) contribute to these differences?</p> 
  </div> 
  <div class="readable-text response" id="p346"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To use inferential statistics to research how the average review score varies across different product categories and how factors like delivery time, price, and product attributes contribute to these differences, you can follow these steps:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text buletless-item" id="p347"> Prepare the data: 
    <ol style="list-style: lower-alpha"> 
     <li> Create a DataFrame containing the necessary columns: product category, review score, delivery time, price, and product attributes (e.g., weight, dimensions). </li> 
     <li> Merge the necessary DataFrames to obtain this information. </li> 
     <li> Ensure there are no missing values and convert categorical variables to numerical format if required. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p348"> Perform ordinal logistic regression: 
    <ol style="list-style: lower-alpha"> 
     <li> Since the review score is an ordinal variable (1-5), you can use ordinal logistic regression to model the relationship between the review score and the factors of interest (product category, delivery time, price, and product attributes). </li> 
     <li> To do this, you can use the <code>mord.LogisticAT</code> model from the mord package. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p349"> Fit the model: 
    <ol style="list-style: lower-alpha"> 
     <li> Split the data into training and testing sets. </li> 
     <li> Fit the ordinal logistic regression model on the training data. </li> 
    </ol></li> 
   <li class="readable-text buletless-item" id="p350"> Evaluate the model: 
    <ol style="list-style: lower-alpha"> 
     <li> Use the model to predict the review scores on the test data. </li> 
     <li> Calculate the accuracy of the predictions using a confusion matrix and other metrics like precision, recall, and F1-score. </li> 
     <li> Analyze the model’s coefficients to determine the importance of each factor in predicting the review score. </li> 
    </ol></li> 
  </ol> 
  <div class="readable-text response" id="p351"> 
   <p>Here’s a sample code to help you get started:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p352"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd
import mord
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

<em># Prepare the data (assuming you have already merged necessary </em>
<em><span class="">↪</span>DataFrames into analysis_df)</em>

<em># Convert categorical variables to numerical format if required</em>
<em># (e.g., using pd.get_dummies() for product categories)</em>

<em># Split the data into training and testing sets</em>
X = analysis_df.drop("review_score", axis=1)
y = analysis_df["review_score"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, 
<span class="">↪</span>random_state=42)

<em># Fit the ordinal logistic regression model</em>
model = mord.LogisticAT(alpha=0)
model.fit(X_train, y_train)

<em># Predict the review scores on the test data</em>
y_pred = model.predict(X_test)

<em># Calculate the accuracy of the predictions</em>
print("Accuracy:", accuracy_score(y_test, y_pred))

<em># Display the confusion matrix</em>
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

<em># Analyze the model's coefficients</em>
print("Coefficients:\n", model.coef_)</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p353"> 
   <p>With this approach, you can explore the relationship between the average review score and various factors using ordinal logistic regression.</p> 
  </div> 
  <div class="readable-text" id="p354"> 
   <p>If the comment in the code saying “<code>assuming you have already merged necessary DataFrames into analysis_df</code>,” was not a red flag for you, we’ve done a poor job so far. Whenever you see any assumptions on the part of the generative AI, you need to shiver, curse under your breath, and double-check them. We won’t include the whole conversation we had, trying to specify what generative AI hallucinated <code>analysis_df</code> to be. You can see the final code in listing 3.6.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p355"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.6</span> Preparation of the <code>analysis_df</code> DataFrame</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Merge df_products with df_product_category_translation</em>
products_merged = pd.merge(
<span class="">↪</span>df_products, df_product_category_translation, on='product_category_name')

<em># Merge df_orders with df_order_items</em>
order_items_merged = pd.merge(df_orders, df_order_items, on='order_id')

<em># Merge the resulting DataFrame with df_customers</em>
order_customer_merged = pd.merge(
<span class="">↪</span>order_items_merged, df_customers, on='customer_id')

<em># Merge the resulting DataFrame with products_merged</em>
order_product_merged = pd.merge(
<span class="">↪</span>order_customer_merged, products_merged, on='product_id')

<em># Merge the resulting DataFrame with df_sellers</em>
order_seller_merged = pd.merge(
<span class="">↪</span>order_product_merged, df_sellers, on='seller_id')

<em># Merge the resulting DataFrame with df_order_reviews</em>
order_review_merged = pd.merge(
<span class="">↪</span>order_seller_merged, df_order_reviews, on='order_id')

<em># Merge the resulting DataFrame with df_order_payments</em>
analysis_df = pd.merge(
<span class="">↪</span>order_review_merged, df_order_payments, on='order_id')

<em># Calculate delivery time in days</em>
analysis_df['delivery_time'] = 
<span class="">↪</span>(analysis_df['order_delivered_customer_date'] – 
<span class="">↪</span>analysis_df['order_purchase_timestamp']).dt.days

<em># Calculate the difference between actual vs. estimated delivery time</em>
analysis_df['delivery_time_misestimation'] = 
<span class="">↪</span>(pd.to_datetime(analysis_df['order_estimated_delivery_date']) – 
<span class="">↪</span>pd.to_datetime(analysis_df['order_delivered_customer_date'])).dt.days

<em># Calculate product volume</em>
analysis_df['product_volume'] = analysis_df['product_length_cm'] * 
<span class="">↪</span>analysis_df['product_height_cm'] * analysis_df['product_width_cm']

<em># Apply one-hot encoding to the payment_type column</em>
payment_type_dummies = pd.get_dummies(
<span class="">↪</span>analysis_df['payment_type'], prefix='payment_type')

<em># Concatenate the one-hot encoded columns with analysis_df</em>
analysis_df = pd.concat([analysis_df, payment_type_dummies], axis=1)

<em># Drop unnecessary columns</em>
analysis_df.drop(columns=['product_category_name', 'order_approved_at', 
<span class="">↪</span>'order_delivered_carrier_date', 'order_estimated_delivery_date', 
<span class="">↪</span>'shipping_limit_date', 'review_creation_date', 
<span class="">↪</span>'review_answer_timestamp', 'order_id', 'customer_id', 'order_status', 
<span class="">↪</span>'order_purchase_timestamp', 'order_delivered_customer_date', 
<span class="">↪</span>'order_item_id','product_id', 'seller_id', 
<span class="">↪</span>'product_category_name_english', 'seller_zip_code_prefix', 
<span class="">↪</span>'seller_city', 'seller_state', 'review_comment_message', 
<span class="">↪</span>'review_comment_title', 'review_id', 'product_length_cm', 
<span class="">↪</span>'product_height_cm', 'product_width_cm', 'customer_unique_id', 
<span class="">↪</span>'customer_zip_code_prefix', 'customer_city', 'customer_state', 
<span class="">↪</span>'payment_type'], inplace=True)

analysis_df = analysis_df.dropna()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p356"> 
   <p>The DataFrame resulting from the preceding code will be devoid of all unnecessary information about identifiers, geographic locations, and timestamps, as they will not be used in this particular analysis and would be unnecessary noise. We want to focus on variables of interest and their impact on review score.</p> 
  </div> 
  <div class="readable-text intended-text" id="p357"> 
   <p>The result of the logistic regression analysis is relatively easy to read.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p358"> 
   <div class="code-area-container"> 
    <pre class="code-area">Accuracy: 0.43035067573535907
Confusion Matrix:
 [[  385    70   401  1637  1302]
 [   38    13    71   563   513]
 [   55    16   109  1342  1365]
 [   49    18   100  2909  3501]
 [   54    23   208  8021 11200]]
Coefficients:
 [-4.92618413e-02  1.63964377e-03  3.08760598e-03  1.38901936e-02
  1.93889038e-05  4.21370114e-01 -8.75650964e-06  3.59787432e-01
  2.93638614e-02 -1.72296023e-03  2.82130024e-02  9.50451240e-07
  1.32936583e-01  2.23209948e-01  1.31639907e-02 -5.95202359e-03]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p359"> 
   <p>This accuracy means that you can explain 43% of the variability in review scores. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p360"> 
   <p><span class="print-book-callout-head">Right on the money?</span>  Misinterpretation or overestimation of a model’s accuracy can be very costly. A confusion matrix is a table layout that visualizes the performance of a classification algorithm, typically used in supervised learning. In binary prediction, it displays the number of correct and incorrect predictions made by the model, comparing them to the actual outcomes. The matrix is organized into four quadrants representing true positives (TP), true negatives (TN), false positives (FP; also known as type I errors), and false negatives (FN; also known as type II errors). By assessing the matrix, we can estimate the rate of type I errors, which occur when the model incorrectly classifies a positive instance as negative, and type II errors, which occur when the model incorrectly classifies a negative instance as positive. You’ll find more information on error types in chapter 5.</p> 
  </div> 
  <div class="readable-text" id="p361"> 
   <p>In a multivariate confusion matrix, such as the preceding one, each row represents the actual class, while each column represents the predicted class. The elements on the main diagonal of the matrix correspond to the correctly classified instances for each class, while the off-diagonal elements indicate misclassifications. To get a better intuition, let’s take a look at our classification of reviews and focus on the third column in the matrix, representing predictions of score three. In 109 cases, the classifier correctly assigned the score. It also misclassified 71 actual threes as twos, 401 as ones, 100 as fours, and 208 as fives.</p> 
  </div> 
  <div class="readable-text intended-text" id="p362"> 
   <p>To make analysis easier, let’s ask generative AI for the code linking the received result with the remaining variables.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p363"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.7</span> Connecting prediction results with variable names</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Assuming you have already fitted the model and have the `model` variable</em>
<em><span class="">↪</span>as the instance of the fitted model, e.g.,  </em>
model = mord.LogisticAT().fit(X_train, y_train)

<em># Get the feature names from the analysis_df DataFrame</em>
<em># Replace 'X' with the actual variable name containing your</em> 
<span class="">↪</span><em>independent variables</em>
feature_names = X.columns

<em># Get the coefficients from the model</em>
coefficients = model.coef_

<em># Create a dictionary to link the coefficients to the column names</em>
coefficient_dict = dict(zip(feature_names, coefficients))

<em># Print the coefficients along with the corresponding column names</em>
for feature, coef in coefficient_dict.items():
    print(f"{feature}: {coef}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p364"> 
   <p>The preceding code snippet should work as advertised and return us this simple answer:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p365"> 
   <div class="code-area-container"> 
    <pre class="code-area">delivery_time: -0.04926184129232652
price: 0.0016396437688542174
freight_value: 0.003087605979194856
product_name_lenght: 0.013890193611842357
product_description_lenght: 1.938890383573905e-05
product_photos_qty: 0.4213701136980763
product_weight_g: -8.756509636657266e-06
payment_sequential: 0.35978743241417466
payment_installments: 0.02936386138061978
payment_value: -0.001722960231783981
delivery_time_misestimation: 0.02821300240733595
product_volume: 9.504512398147115e-07
payment_type_boleto: 0.1329365830670677
payment_type_credit_card: 0.22320994812520034
payment_type_debit_card: 0.013163990732791755
payment_type_voucher: -0.005952023594428278</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p366"> 
   <p>The interpretation of coefficients resulting from the ordinal regression is similar to the interpretation of results we obtained from the linear regression, if not so straightforward. The resulting numbers represent the change in log-odds of being in a higher category of the outcome variable for a one-unit increase in the corresponding predictor variable, holding other variables constant. In simpler terms, when we’re predicting something with a specific order, like “low,” “medium,” and “high,” the coefficients give us information about how our prediction changes with input changes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p367"> 
   <p>For example, let’s say we want to predict a happiness level based on how many hours you sleep. The coefficient would tell us how much the odds of being “more happy” (moving up a level from “low” to “medium” or “medium” to “high”) change for each extra hour of sleep you get. All of this is assuming that nothing else in your environment has changed. So, if the number (coefficient) is positive, that means getting more sleep increases the odds of being “more happy.” If it’s negative, more sleep decreases the odds. The bigger the number, the stronger the effect.</p> 
  </div> 
  <div class="readable-text intended-text" id="p368"> 
   <p>From the research perspective, it’s not ideal to evaluate sleep’s influence on your happiness if the increased sleep results from holidays from stressful work. Still, we’re not always given perfect testing conditions. The method described here could allow you to identify the influence of sleep if you provide multiple conditions and your mood record as input. </p> 
  </div> 
  <div class="readable-text intended-text" id="p369"> 
   <p>We will discuss how our coefficients translate to business insights in chapter 4.</p> 
  </div> 
  <div class="readable-text" id="p370"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_47"><span class="num-string">3.4.4</span> Differences in sales performance and customer satisfaction between sellers</h3> 
  </div> 
  <div class="readable-text" id="p371"> 
   <p>In this section, our focus will be on the differences between groups of sellers. We’ll try to answer the question, “are there any significant differences in sales performance and customer satisfaction between sellers from different cities or states?” </p> 
  </div> 
  <div class="readable-text intended-text" id="p372"> 
   <p>As it’s later in the chapter and you may be eager to move on to the next one, let’s take a look at where this approach could be successfully applied. Who knows, maybe something here will be of interest to you: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p373"> <em>Healthcare performance analysis</em><em> </em>—Comparing patient outcomes (e.g., recovery rates, satisfaction scores) between different hospitals or healthcare providers to identify quality disparities and areas for improvement. </li> 
   <li class="readable-text" id="p374"> <em>Educational achievement gaps</em><em> </em>—Analyzing differences in student performance (e.g., test scores, graduation rates) across schools or districts to uncover educational inequalities and target interventions. </li> 
   <li class="readable-text" id="p375"> <em>Marketing campaign effectiveness</em><em> </em>—Evaluating the impact of different marketing campaigns across regions or demographics to determine which strategies are most effective in enhancing brand awareness or sales. </li> 
   <li class="readable-text" id="p376"> <em>Human resource management</em><em> </em>—Comparing employee satisfaction and performance metrics across different company locations or departments to identify best practices and areas needing attention. </li> 
   <li class="readable-text" id="p377"> <em>Financial services comparison</em><em> </em>—Analyzing loan approval rates or default rates across different branches to assess risk management effectiveness and customer service quality. </li> 
   <li class="readable-text" id="p378"> <em>Supply chain efficiency</em><em> </em>—Comparing delivery times or inventory turnover rates across different distribution centers to optimize logistics and supply chain management. </li> 
   <li class="readable-text" id="p379"> <em>Product quality control</em><em> </em>—Analyzing return rates or customer complaints for products manufactured at different facilities to identify quality control issues. </li> 
   <li class="readable-text" id="p380"> <em>Environmental policy impact</em><em> </em>—Comparing pollution levels or conservation outcomes across regions before and after the implementation of environmental policies to assess their effectiveness. </li> 
   <li class="readable-text" id="p381"> <em>Retail chain performance</em><em> </em>—Evaluating sales volumes or customer loyalty scores across different store locations to identify high-performing stores and understand factors contributing to their success. </li> 
   <li class="readable-text" id="p382"> <em>Public policy and social programs</em><em> </em>—Assessing the effectiveness of social programs (e.g., unemployment aid, public health initiatives) in different communities by comparing outcome metrics like employment rates or health indicators. </li> 
  </ul> 
  <div class="readable-text" id="p383"> 
   <p>Before we jump into the analysis, we need to expose you to the method called bootstrapping. </p> 
  </div> 
  <div class="readable-text intended-text" id="p384"> 
   <p>Bootstrap methods are a powerful resampling technique you can use to estimate confidence intervals, especially when the underlying data distribution is unknown or difficult to determine analytically. The bootstrap approach involves generating multiple random samples with replacements from the original dataset, each of the same size as the original sample. This means that while constructing each sample, every data point from the original dataset can be selected multiple times, and the number of data points in each new sample matches the number in the original dataset. A statistic of interest (e.g., mean, median, or regression coefficient) is calculated for each generated sample. These calculated statistics form a bootstrap distribution for estimating the population parameter’s confidence interval. A bootstrap distribution is essentially a frequency distribution of a statistic—like the mean or a regression coefficient—computed across multiple resampled datasets.</p> 
  </div> 
  <div class="readable-text intended-text" id="p385"> 
   <p>What does this mean in practice? Imagine you work for a company manufacturing extremely expensive equipment, such as quantum electronics. They need to make destruction tests to estimate safety parameters, such as the temperature the product can withstand. Destroying too many samples would literally bankrupt them, but let’s assume they can afford to overheat 50 pieces of equipment. The engineers run the test 50 times and come to you with 50 breakpoint temperatures. The reliability of a simple average calculation isn’t sufficient to satisfy the needs of clients, who want to know exactly how critical the cooling requirements are. Remember, we’re talking about huge costs for the products, but also huge costs for ensuring safe operational requirements. Safety margins are good and nice, but widening them too much can make your product unfeasible for clients. </p> 
  </div> 
  <div class="readable-text intended-text" id="p386"> 
   <p>Enter bootstrapping. You create “new” test batches by randomly selecting 50 scores from your original test results, with replacement. In other words, some scores may be selected multiple times and others not at all, simulating a new batch of tests under identical conditions. You repeat this resampling process many times—let’s say 10,000 times—each time calculating the average score of the resampled batch. This extensive repetition produces a wide range of possible breakpoint temperatures, reflecting the natural variability in your equipment’s durability. The collection of these 10,000 breakpoint temperatures forms your bootstrap distribution. This distribution gives you a statistical model of what the average durability score for your quantum electronics equipment might look like, based on your actual testing data. Given this bootstrap distribution, you can determine the middle 95% (or 99%, or 99.9%) of the average breakpoint temperatures as your confidence interval. This interval provides a statistically sound estimate of where the true average breakpoint temperature of your equipment lies, offering insights into its performance and how it might hold up in real-world use.</p> 
  </div> 
  <div class="readable-text intended-text" id="p387"> 
   <p>The bootstrap method shines in situations where traditional assumptions (like the normal distribution of the data) may not hold, or where sample sizes are too small to rely on the asymptotic properties of estimators (e.g., the simple calculation of a mean). Here are a few examples:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p388"> <em>Startup performance metrics</em><em> </em>—Early-stage startups often have limited data on customer behavior, revenues, or engagement metrics. Bootstrap methods can help estimate the variability and confidence intervals of key performance indicators (KPIs) like average revenue per user (ARPU) or customer lifetime value (CLV), providing valuable insights for decision-making despite small sample sizes. </li> 
   <li class="readable-text" id="p389"> <em>Drug efficacy in clinical trials</em><em> </em>—In clinical trials, especially early-phase trials with small sample sizes, the distribution of treatment effects can be skewed or multimodal due to heterogeneous responses among participants. Bootstrap methods allow for the estimation of confidence intervals around the median treatment effect or other non-normally distributed outcomes, offering a more robust understanding of the drug’s efficacy. </li> 
   <li class="readable-text" id="p390"> <em>Environmental impact studies</em><em> </em>—Environmental data, such as pollution levels or species counts, often exhibit skewed distributions due to the presence of extreme values (e.g., very high pollution levels on certain days). Bootstrap methods can provide reliable confidence intervals for median pollution levels or mean species counts, which are more informative and robust for policy-making than simple means. </li> 
   <li class="readable-text" id="p391"> <em>Economic policy evaluation</em><em> </em>—Evaluating the impact of economic policies on outcomes such as employment rates or GDP growth often involves dealing with complex, non-normal distributions of effects across different regions or industries. Bootstrap confidence intervals can offer a more nuanced understanding of policy impacts, helping policymakers identify the range of potential outcomes and the reliability of estimated effects. </li> 
  </ul> 
  <div class="readable-text" id="p392"> 
   <p>In each of these scenarios, the bootstrap method’s ability to provide reliable confidence intervals without relying on strict assumptions about the data’s distribution is invaluable. It allows researchers and practitioners to draw more confident conclusions from their data, even in challenging analytical contexts.</p> 
  </div> 
  <div class="readable-text intended-text" id="p393"> 
   <p>Let’s move from quantum electronics and economic policies into more commonly encountered customer satisfaction issues. In the case of customer satisfaction, you will often operate on opinions collected from just a sample of the total number of customers. Only some customers will leave a review or answer a survey. Businesses should know, reliably, their customers’ opinions across different categories.</p> 
  </div> 
  <div class="readable-text intended-text" id="p394"> 
   <p>Chapters 5 and 6 will discuss analyzing the reviews’ texts. Here, we’ll ask generative AI to prepare bootstrap-based confidence intervals for two numerical values important from the business perspective: average price and average customer review. </p> 
  </div> 
  <div class="readable-text intended-text" id="p395"> 
   <p>We will limit our attention to a single category of products. From the computational perspective, it doesn’t matter if we drop in the whole DataFrame or a filtered selection. We would like, however, to point your attention to the importance of the business meaning of the analysis. In a real-life scenario, even selecting a single category—in our case, “health and beauty”—wouldn’t be granular enough. Luxury and commodity products differ greatly in both prices and expectations. Mixing them does not offer sufficient insight into the market situation. Mixing them all together with car sales would muddy the waters even more, don’t you think? </p> 
  </div> 
  <div class="readable-text intended-text" id="p396"> 
   <p>Let’s get to the code.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p397"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 3.8</span> Calculating confidence intervals for sales and review scores by state</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Prepare datasets</em>
<em># Merge df_products with df_product_category_translation</em>
products_merged = pd.merge(
<span class="">↪</span>df_products, df_product_category_translation, on='product_category_name')

<em># Merge df_orders with df_order_items</em>
order_items_merged = pd.merge(df_orders, df_order_items, on='order_id')

<em># Merge the resulting DataFrame with df_customers</em>
order_customer_merged = pd.merge(
<span class="">↪</span>order_items_merged, df_customers, on='customer_id')

<em># Merge the resulting DataFrame with products_merged</em>
order_product_merged = pd.merge(
<span class="">↪</span>order_customer_merged, products_merged, on='product_id')

<em># Merge the resulting DataFrame with df_sellers</em>
order_seller_merged = pd.merge(
<span class="">↪</span>order_product_merged, df_sellers, on='seller_id')

<em># Merge the resulting DataFrame with df_order_reviews</em>
order_review_merged = pd.merge(
<span class="">↪</span>order_seller_merged, df_order_reviews, on='order_id')

<em># Merge the resulting DataFrame with df_order_payments</em>
analysis_df = pd.merge(
<span class="">↪</span>order_review_merged, df_order_payments, on='order_id')

<em># Choose a specific product category to analyze</em>
product_category = "health_beauty"
filtered_df = analysis_df[
<span class="">↪</span>analysis_df['product_category_name_english'] == product_category]

<em># Group the data by seller state or city, and calculate </em>
<span class="">↪</span><em>the metrics of interest</em>
grouped_data = filtered_df.groupby('seller_state')
group_metrics = grouped_data.agg(
<span class="">↪</span>{'price': ['sum', 'mean', 'count'], 'review_score': 'mean'})

<em># Calculate the standard deviations</em>
group_metrics['price', 'std'] = grouped_data['price'].std()
group_metrics['review_score', 'std'] = grouped_data['review_score'].std()

<em># Use bootstrapping to estimate the 95% confidence intervals for </em>
<span class="">↪</span><em>the average sales revenue and average review score for each city or state.</em>

import numpy as np
<em># Bootstrap the confidence intervals</em>
def bootstrap_CI(data, func, n_bootstraps=1000, ci=95, axis=0):
    """
    Generate a confidence interval for a given statistic using 
    <span class="">↪</span>bootstrapping.

    Parameters:
    data (numpy.ndarray or pandas.DataFrame): 
    <span class="">↪</span>The data to calculate the statistic from.
    func (callable): The function used to calculate the statistic.
    n_bootstraps (int): The number of bootstrap samples to generate.
    ci (float): The confidence interval percentage (e.g., 95 for a 95% CI).
    axis (int): 
    <span class="">↪</span>The axis along which to apply the statistic function 
    <span class="">↪</span>(0 for columns, 1 for rows).

    Returns:
    tuple: The lower and upper bounds of the confidence interval.
    """
    bootstrapped_statistics = []
    for _ in range(n_bootstraps):
        bootstrap_sample = np.random.choice(
        <span class="">↪</span>data, size=len(data), replace=True)
        bootstrapped_statistic = func(bootstrap_sample, axis=axis)
        bootstrapped_statistics.append(bootstrapped_statistic)

    lower_bound = np.percentile(bootstrapped_statistics, (100 - ci) / 2)
    upper_bound = np.percentile(
    <span class="">↪</span>bootstrapped_statistics, 100 - (100 - ci) / 2)

    return lower_bound, upper_bound

<em># Create a new DataFrame to store the confidence intervals</em>
def calculate_ci(group):
    return pd.Series({
        'price_ci_lower': bootstrap_CI(group['price'], np.mean, 
        <span class="">↪</span>n_bootstraps=1000, ci=95)[0],
        'price_ci_upper': bootstrap_CI(group['price'], np.mean, 
        <span class="">↪</span>n_bootstraps=1000, ci=95)[1],
        'review_score_ci_lower': bootstrap_CI(group['review_score'], 
        <span class="">↪</span>np.mean, n_bootstraps=1000, ci=95)[0],
        'review_score_ci_upper': bootstrap_CI(group['review_score'], 
        <span class="">↪</span>np.mean, n_bootstraps=1000, ci=95)[1]
    })

ci_df = grouped_data.apply(calculate_ci)

<em># Create visualizations with error bars for the average sales revenue and </em>
<span class="">↪</span><em>average review score by seller state.</em>

def plot_error_bars_only(df, metric_name, ylabel='', title='', 
<span class="">↪</span>figsize=(10, 6)):
    fig, ax = plt.subplots(figsize=figsize)

    x = np.arange(len(df.index))
    y = df[metric_name + '_ci_lower'] + (df[metric_name + '_ci_upper'] – 
    <span class="">↪</span>df[metric_name + '_ci_lower']) / 2
    yerr = (df[metric_name + '_ci_upper'] – 
    <span class="">↪</span>df[metric_name + '_ci_lower']) / 2

    plt.errorbar(x, y, yerr=yerr, fmt='o', capsize=5, capthick=2, 
    <span class="">↪</span>ecolor='black', elinewidth=2)

    plt.xticks(x, df.index, rotation=90)
    plt.xlabel('Seller State')
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(axis='y')

    plt.show()

<em># Plot the confidence intervals</em>
plot_error_bars_only(ci_df, 'price', ylabel='Average Sales')
plot_error_bars_only(ci_df, 'review_score', ylabel='Average Review Score')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p398"> 
   <p>As you can see, going through data harmonization, statistical modeling, and visualization results in quite a substantial amount of code. Fortunately, generative AIs are capable of not only providing the code, but also commenting it in an informative way. </p> 
  </div> 
  <div class="readable-text intended-text" id="p399"> 
   <p>The confidence intervals calculated for our examples are presented in figure 3.12.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p400">  
   <img alt="figure" src="../Images/CH03_F12_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 3.12</span> Confidence intervals of (a) the average sales price and (b) the review score, both per state</h5>
  </div> 
  <div class="readable-text" id="p401"> 
   <p>We would like to repeat that, while doing the full-scale analysis, you should prepare separate charts for each product category, or dig even deeper, to avoid comparing apples with oranges or visual clutter.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p402"> 
   <p><span class="print-book-callout-head">A Word on software efficiency</span>  There are two main elements of increasing program efficiency. One is to optimize code execution, and we write about that at length in chapter 7. The other one, oftentimes no less important, is the parameterization of the algorithm used. Bootstrap methods balance two main parameters: the number of resamplings and the accuracy of the confidence interval. If you analyze really big data, before you jump into the technical aspects of optimization, you should try to understand whether the parameters of the algorithm you are using don’t offer ways of getting to your goal faster. Or at least closer to your goal. That’s another bonus of learning about parameters—you can usually understand the results better if you know what the assumptions behind each of them are.</p> 
  </div> 
  <div class="readable-text" id="p403"> 
   <p>We can see some interesting differences in the results, but we’ll leave interpreting the results for chapter 4. For now, we would like to leave you with a riddle: Why, when working with our example, does it make sense to use bootstrapping and calculate confidence intervals for our prices, and why would it most likely not make sense if you worked with your business data?</p> 
  </div> 
  <div class="readable-text intended-text" id="p404"> 
   <p>Having read this section, you should now be able to use inferential statistics and its different methods in your analytical work. In this chapter, you broke down the data and identified multiple relationships between different parameters. You used different visualization tools, like heatmaps and scatter plots, as well as regression analysis, to see how some parameters correlate with others. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p405"> 
   <p><span class="print-book-callout-head">Correlation does not imply causation</span>  Sleeping with shoes on correlates with headaches in the morning, ice cream sales correlate with violent crimes, hair color correlates with ice-skating skills. These examples demonstrate that seeing a correlation is just a first stop in explaining observed phenomena. Often the correlation is caused by a third factor influencing your observed parameters. Sleeping with shoes on and headaches go hand in hand with alcohol overuse, ice cream sales and violent crimes result from heat waves, while blond hair is more often found in countries where you can find ice outside every now and then. <em>Never</em> assume causation on the basis of observed correlation without identifying a logical link between the observations.</p> 
  </div> 
  <div class="readable-text" id="p406"> 
   <p>There are many more methods in descriptive analytics, but after reading this chapter, you should be able to use generative AI to quickly identify and implement the best analytical tools for your dataset. All you need to do is to honestly and precisely describe the data you have available, and as honestly and as precisely as you can (which could mean not at all!), describe the goal of your analytics. </p> 
  </div> 
  <div class="readable-text intended-text" id="p407"> 
   <p>You should also be aware of generative AI’s weaknesses. The most notable one is its tendency to hallucinate when asked about topics and data defined too long before the prompt. Also, the provided code always needs to be tested. Applying this “trust, but verify” approach, generative AI can substantially improve and accelerate your analysis process.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p408"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Things to ask generative AI</h5> 
   </div> 
   <ul> 
    <li class="readable-text" id="p409"> What research questions related to the provided data can be asked? </li> 
    <li class="readable-text" id="p410"> What analysis design would lead to answering the following question? </li> 
    <li class="readable-text" id="p411"> What statistical method is best to analyze &lt;available data&gt;? </li> 
    <li class="readable-text" id="p412"> Provide me with code to &lt;data transformation&gt;. </li> 
    <li class="readable-text" id="p413"> How should I visualize &lt;data structure&gt;. </li> 
    <li class="readable-text" id="p414"> Provide me with a solution answering the following question: . . . </li> 
    <li class="readable-text" id="p415"> Modify the code to change the visualization format. </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p416"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_48">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p417"> Generative AI may be excellent at suggesting proper analytical tools and providing end-to-end analytical strategies. </li> 
   <li class="readable-text" id="p418"> Generative AI will first suggest the most popular methods, but they can provide competent support in the implementation of more niche ones too. </li> 
   <li class="readable-text" id="p419"> Generative AI may supplement analysis with market-specific information. </li> 
   <li class="readable-text" id="p420"> Feeding generative AI with results to get insights requires caution and sometimes re-engineering of the output. </li> 
   <li class="readable-text" id="p421"> Generative AI can offer sound business insights. </li> 
   <li class="readable-text" id="p422"> You need to control the context awareness of generative AIs and be wary of their hallucinations. Jog their memories from time to time. </li> 
   <li class="readable-text" id="p423"> The code provided by generative AI needs careful testing. </li> 
  </ul>
 </body></html>