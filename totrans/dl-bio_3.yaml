- en: Chapter 3\. Learning the Logic of DNA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll build a deep learning model to predict whether a DNA
    sequence is bound by a class of proteins called *transcription factors* (TFs).
    Transcription factors play a central role in gene regulation: they bind to specific
    DNA sequences and influence whether nearby genes are turned on or off. By recognizing
    these sequence patterns, we can begin to decode the regulatory logic embedded
    in the genome.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the previous chapter—where we used an off-the-shelf protein model from
    Hugging Face—here we’ll start defining and training our own models from scratch.
    This gives us more control and helps us better understand how deep learning works
    on biological data. We’ll explore both convolutional and transformer-based architectures
    and introduce interpretation techniques to help us understand how our models make
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will tackle this problem in stages, gradually increasing the complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Start simple
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll train a basic convolutional network to predict whether a DNA sequence
    binds a single transcription factor called CTCF. Its binding behavior is relatively
    easy to predict, making it a great first target. We’ll build the full pipeline:
    loading data, training the model, and checking whether it captures meaningful
    biological signals.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Increase complexity
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll scale up to predicting whether a sequence binds any of 10 different
    TFs. We’ll introduce regularization and normalization, improve our evaluation
    metrics, and begin inspecting individual predictions. We’ll also use mutation
    experiments and input gradients to highlight which parts of the sequence the model
    relies on—offering a first step toward interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Incorporate advanced techniques
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll try adding transformer layers to explore whether they improve
    performance and continue dissecting model behavior to understand how different
    architectural choices influence learning.
  prefs: []
  type: TYPE_NORMAL
- en: This staged approach—building up from simple to more complex—is one we recommend
    in general. It helps keep models interpretable, makes debugging easier, and builds
    confidence along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving in, we’ll do a quick refresher on the biological and machine learning
    concepts that underpin this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To get the most out of this chapter, open the companion Colab notebook and run
    the code cells as you follow along. Executing the code interactively will deepen
    your understanding and give you space to experiment with the concepts in real
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Biology Primer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s astonishing that all the instructions for building an entire human body
    are encoded in the DNA of a single cell. Every human starts as one tiny cell—about
    100 micrometers wide—with its DNA tightly packed into a nucleus just 6 micrometers
    across. This DNA acts as the blueprint for processes like cell division and differentiation,
    eventually giving rise to the diverse tissues and cell types that make up an entire
    human body.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A few terminology clarifications: The *genome* refers to the complete set of
    DNA in an organism, including all of its genes and other genetic material. While
    *genetics* typically studies individual genes or small gene sets, *genomics* takes
    a broader view, often analyzing entire genomes across individuals or even species.'
  prefs: []
  type: TYPE_NORMAL
- en: The human genome is vast—more than 3 billion base pairs long—and carries with
    it billions of years of evolutionary history. But what is this molecule, really,
    and how does it encode biological function?
  prefs: []
  type: TYPE_NORMAL
- en: What Exactly Is DNA?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DNA is the molecule of inheritance—the fundamental code of life. Its double-helix
    structure was first revealed in 1953 by Watson, Crick, and Franklin, marking a
    pivotal moment in the biological sciences. Nearly half a century later, the first
    complete draft of the human genome was published in 2001,^([1](ch03.html#id616))
    laying the foundation for modern genetics and genomics. But these milestones are
    relatively recent, and while we now know a great deal about *what* is in the genome,
    we still understand surprisingly little about *how* it actually works.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do know that DNA is built from four chemical letters, or nucleotide bases:
    `A` (adenine), `C` (cytosine), `G` (guanine), and `T` (thymine). These bases form
    long sequences that carry genetic instructions. The full human genome contains
    around 3.2 billion of these letters, packed into 23 pairs of chromosomes. To fit
    inside the tiny nucleus of a cell, this DNA wraps around proteins and coils into
    compact, highly organized structures known as *chromatin*, as shown in [Figure 3-1](#dna-organization).'
  prefs: []
  type: TYPE_NORMAL
- en: Despite decades of research, the genome remains full of unanswered questions.
    Only about 2% of it directly codes for proteins—what is the rest doing? How can
    all the cells in your body share the same DNA, yet behave so differently? What
    controls when a gene is used, and how do changes in the environment or during
    development affect this process?
  prefs: []
  type: TYPE_NORMAL
- en: These mysteries lie at the heart of gene regulation—and increasingly, deep learning
    is helping us explore them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1\. DNA is packed into the nucleus in multiple layers of structure.
    Starting with the double helix, it first wraps around histone proteins to form
    nucleosomes (beads on a string), which fold into chromatin fibers, and ultimately
    into chromosomes. Source: [National Institute of Environmental Health Sciences](https://oreil.ly/h8M44).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Coding and Noncoding Regions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The human genome contains around 20,000 protein-coding genes. These make up
    the *coding* regions—stretches of DNA that are *transcribed* into RNA and then
    *translated* into proteins. Each protein carries out specific tasks, from building
    cellular structures to catalyzing chemical reactions. Together, they perform most
    of the cell’s essential functions.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, protein-coding genes account for only about 2% of the genome. The remaining
    98% is noncoding DNA. While it doesn’t produce proteins, noncoding DNA plays critical
    regulatory roles, helping to control when and where genes are used. In fact, most
    genetic variants associated with human disease fall in noncoding regions—though
    we still have limited understanding of how most of them exert their effects.
  prefs: []
  type: TYPE_NORMAL
- en: Some noncoding DNA produces RNAs that regulate gene expression, while other
    regions help organize the 3D structure of the genome or serve as docking sites
    for regulatory proteins. One especially important category of noncoding region
    is transcription factor binding sites. These are short DNA sequences where *transcription
    factors* (TFs) attach to help regulate gene activity—and they’re the central focus
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How Transcription Factors Orchestrate Gene Activity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TFs are proteins that control which genes are used, when, and in what context.
    They do this by binding to short, specific DNA sequences called *motifs*, often
    located near genes. By binding these motifs, TFs can activate or repress transcription.
    You can think of them as conductors of a genomic orchestra—directing the performance
    by determining which regions get “played” and when.
  prefs: []
  type: TYPE_NORMAL
- en: TFs are involved in nearly every biological process, from guiding development
    to coordinating how cells respond to stress or infection. Humans have around 1,600
    different TFs, each of which has evolved to recognize specific DNA motifs. These
    motifs are short sequence patterns—typically 6 to 15 base pairs long—that TFs
    preferentially bind to. For example, the well-studied transcription factor CTCF
    binds a core motif with the central pattern `CCCTC`. TFs don’t bind randomly across
    the genome—they search for these preferred sequences.
  prefs: []
  type: TYPE_NORMAL
- en: These motifs form specific 3D shapes in the DNA helix, and the protein-binding
    domains of TFs are shaped to complement them—much like a key fitting into a lock.
    In reality, the interaction is often more flexible and dynamic than the analogy
    suggests, but the idea of a physical match still holds.
  prefs: []
  type: TYPE_NORMAL
- en: To make this interaction more concrete, [Figure 3-2](#fig3-2) shows crystallographic
    structures of different TFs bound to DNA.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2\. Crystallographic structures showing how different types of TF
    binding domains interact with DNA. Each gray structure represents a different
    TF binding domain—zinc finger, homeodomain, helix-loop-helix, and forkhead—bound
    to a double-stranded DNA molecule. These protein segments recognize specific DNA
    motifs by the physical shape those sequences form. Like a key fitting into a lock,
    the structure of the protein complements the shape of the DNA at its binding site.
    Shown here are actual resolved structures from the Protein Data Bank (PDB: 6ML2,
    6KKS, 1NKP, 3G73). Source: [Wikipedia](https://oreil.ly/Nd4Tv).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'However, not every matching motif is actually bound in a living cell. In fact,
    the genome contains far more motif matches than actual binding events. That’s
    because binding depends on many additional factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Chromatin accessibility
  prefs: []
  type: TYPE_NORMAL
- en: DNA that’s tightly packed into chromatin is harder for proteins to access. TFs
    are more likely to bind in regions of open chromatin.
  prefs: []
  type: TYPE_NORMAL
- en: DNA methylation
  prefs: []
  type: TYPE_NORMAL
- en: Certain TFs, like CTCF, are sensitive to methylation (a certain chemical modification
    of DNA bases) at their binding sites, which can block binding even if the motif
    is present.
  prefs: []
  type: TYPE_NORMAL
- en: Cellular signals
  prefs: []
  type: TYPE_NORMAL
- en: Signals inside the cell—such as hormones or stress responses—can activate or
    deactivate a TF’s ability to bind.
  prefs: []
  type: TYPE_NORMAL
- en: Other proteins
  prefs: []
  type: TYPE_NORMAL
- en: Helper or blocking proteins in the local environment can facilitate or inhibit
    binding.
  prefs: []
  type: TYPE_NORMAL
- en: Cooperative binding
  prefs: []
  type: TYPE_NORMAL
- en: Many TFs work in complexes or recruit others to stabilize binding and control
    gene activity.
  prefs: []
  type: TYPE_NORMAL
- en: And perhaps most importantly, real cells are dynamic. Molecules move, concentrations
    change, and TF binding events happen on short timescales. Most genomic datasets,
    by contrast, are static snapshots—freeze-frames of a constantly changing scene.
    That’s worth keeping in mind when interpreting binding data in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Where Transcription Factors Bind
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In deep learning, we’re incredibly reliant on experimental data—we need something
    to learn from. When it comes to studying TFs, that data typically comes from laboratory
    experiments that measure where in the genome a particular TF binds.
  prefs: []
  type: TYPE_NORMAL
- en: The cornerstone wet lab method is [ChIP-seq](https://oreil.ly/IovSO), or chromatin
    immunoprecipitation followed by sequencing. TFs don’t permanently stick to DNA—they
    bind and unbind constantly. ChIP-seq captures a snapshot of this dynamic process
    by chemically cross-linking (gluing) proteins to DNA, essentially freezing them
    in place. The DNA fragments bound by a specific TF can then be isolated, sequenced,
    and mapped back to the genome to determine where the protein was bound.
  prefs: []
  type: TYPE_NORMAL
- en: ChIP-seq data is typically visualized as peaks over the DNA sequence—regions
    of the genome where TF binding was enriched. The height of a peak reflects how
    strong or frequent the binding was at that site, while a flat zero signal means
    no detectable binding occurred there.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To simplify the data for modeling, ChIP-seq peaks can be binarized: instead
    of retaining the full quantitative signal, we apply a threshold and record only
    whether the TF was bound in a given region. This reduces the task to a binary
    classification problem—does the TF bind to this DNA sequence or not—which is the
    setup we’ll use throughout this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Primer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the biology background knowledge in place, we now review some foundational
    machine learning concepts that we’ll use in this chapter. If you’re already familiar
    with deep learning, feel free to skim this section as a refresher. We’ll briefly
    cover *convolutional* and *transformer*-based architectures, how they can be applied
    to biological sequence data like DNA, and what kinds of insights they can provide.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Convolutional neural networks* (CNNs) are one of the most widely used deep
    learning architectures. Their core strength lies in their ability to automatically
    learn useful patterns from raw, grid-like input data—whether that’s pixels in
    an image or bases in a DNA sequence—without the need for hand-engineered features.'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs were originally developed for image recognition tasks. In images, low-level
    features like edges and textures appear in small local patches, while higher-level
    concepts like shapes or objects are formed by combining these local features.
    CNNs mirror this structure by using small, learnable *filters* that slide across
    the input, extracting local patterns at each position. As we stack more layers,
    the model combines local features into more abstract and global representations.
  prefs: []
  type: TYPE_NORMAL
- en: This ability to model hierarchical structure turns out to be useful far beyond
    images. Whenever there are meaningful local patterns in data—like substructures
    in molecules, motifs in DNA, or phonemes in speech—CNNs often perform well.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We cover CNNs in more detail in Chapter 5, where we build a skin cancer classifier
    and explore common model design patterns. For now, we’ll provide a short overview
    of the key components you’ll need for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s briefly walk through the key components of a typical CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs: []
  type: TYPE_NORMAL
- en: These are the heart of the CNN. A convolutional layer contains multiple filters
    (also called *kernels*)—small weight matrices that slide across the input and
    compute dot products. Each filter acts like a pattern detector, lighting up when
    it finds a good match in the input. The result of applying a convolution is a
    *feature map* showing where the pattern occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs: []
  type: TYPE_NORMAL
- en: These downsample the feature maps to reduce dimensionality and computation.
    *Max pooling*, for example, keeps only the strongest signal in each region, helping
    the model focus on the most salient features.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs: []
  type: TYPE_NORMAL
- en: Layers like *batch normalization* rescale activations to make training more
    stable and efficient. They reduce internal *covariate shift*—the tendency for
    activations to drift during training—and often speed up convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs: []
  type: TYPE_NORMAL
- en: These sit at the end of the network and use the features extracted by earlier
    layers to make final predictions—such as whether a DNA sequence is bound by a
    transcription factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'One key property of CNNs is that they are *translation equivariant*, meaning
    that a pattern can be recognized regardless of where it appears in the input.
    This is especially useful for DNA: a binding motif is still a binding motif whether
    it’s at position 10 or position 90 of the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions for DNA Sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although CNNs were originally developed for images (which are 2D grids of pixels),
    the architecture can easily be adapted to 1D data like DNA sequences. In genomics,
    DNA is commonly represented as a one-hot encoded matrix, where each base (`A`,
    `C`, `G`, `T`) is turned into a binary vector. A sequence of 100 bases would thus
    become a 100×4 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then apply *1D convolutions*—filters that slide across the sequence in one
    dimension, looking for patterns in short windows of DNA bases. These filters often
    end up learning to identify the presence of DNA motifs: the short sequence patterns
    that have biological meaning we mentioned before, such as transcription factor
    binding sites. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Shallow layers might learn to detect low-level DNA features such as simple GC-rich
    or AT-rich regions in DNA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mid-level filters may identify known TF motifs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeper layers might learn higher-order combinations—such as co-occurring motifs
    or long-range dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importantly, the model learns all of this automatically from labeled data. It
    doesn’t need to be told what motif to look for—it discovers useful patterns by
    optimizing for the task at hand, such as predicting TF binding.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'CNNs have become a standard architecture in sequence-based biology tasks because
    they offer a good balance between power, speed, and interpretability. Compared
    to other deep learning architectures, CNNs are relatively lightweight, easy to
    train, and often easier to interpret: you can visualize which motifs a filter
    has learned and where they appear in a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Their main limitation, however, is that they operate on fixed windows of sequence
    and struggle to model interactions between distant bases. For problems involving
    long-range dependencies—relationships between elements far apart in a sequence—we
    often turn to a different class of models.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While CNNs are excellent at detecting local patterns, transformers are particularly
    powerful for modeling relationships across long distances in a sequence. Their
    core mechanism—*self-attention*—allows the model to dynamically determine which
    parts of the input are relevant for predicting any given output, regardless of
    how far apart those parts are.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based models have revolutionized deep learning since their debut
    in 2017.^([2](ch03.html#id638)) Originally designed for natural language processing
    tasks like translation and summarization, transformers have since become the state
    of the art in a wide range of domains, including genomics and protein modeling.
    Earlier architectures like RNNs and CNNs can also handle sequences, but transformers
    have largely surpassed them for tasks that require understanding global sequence
    context.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key innovation is *self-attention*: a mechanism that lets each token in
    the sequence “attend to” every other token, computing how much influence they
    should have. This is useful for:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Language: Where a word’s meaning depends on faraway context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Genomics: Where a regulatory DNA motif located thousands of bases away might
    affect gene activity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This flexibility allows transformers to learn arbitrary and complex dependencies—something
    CNNs struggle with. The main drawback of transformers is scalability: self-attention
    requires computations that grow quadratically with sequence length. For very long
    inputs like whole-genome sequences, this can become a bottleneck. However, many
    efficient transformer variants (e.g., Linformer, Longformer, Performer) have been
    developed to partially address this weakness.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’d like to dive deeper into transformers, we recommend the excellent blog
    post [“The Illustrated Transformer”](https://oreil.ly/WxzGl) by Jay Alammar or
    the [3Blue1Brown video](https://oreil.ly/dLW40) on attention. We’ll only touch
    on the basics here.
  prefs: []
  type: TYPE_NORMAL
- en: The foundation of transformers lies in their ability to assign *attention*—a
    mechanism that lets each position in a sequence dynamically focus on other positions.
    This is what enables them to model long-range dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, attention is a process that enriches the embedding of each
    token by incorporating information from every other token in the sequence. This
    makes each token more context aware, as illustrated in [Figure 3-3](#attention-mechanism).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. High-level visualization of the attention mechanism transforming
    input token embeddings into context-aware output embeddings. Each of the four
    tokens in the sequence (“the cat is black”) attends to every other token, allowing
    the model to capture relationships and dependencies across the entire sequence.
    The output is also four tokens, each enriched with contextual information from
    the others.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Briefly, here’s how it works. The model first creates three versions of the
    input embeddings—queries (Q), keys (K), and values (V)—via learned linear transformations.
    Each query is compared to all keys using a dot product, producing a score that
    reflects how relevant one token is to another. These scores are then normalized
    via a softmax function, producing the attention weights. Finally, each token’s
    new embedding is computed as a weighted sum of the value vectors, with the attention
    weights determining how much each value contributes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through a simple example. Suppose we have the input sequence `the`,
    `cat`, `is`, `black`. In a transformer model, each token doesn’t just pass through
    the network on its own; it decides how much attention to pay to every other token.
    For example, when processing the word `cat`, the model might assign a high attention
    weight to `the`, recognizing that articles and nouns are often linked. This helps
    the model understand grammatical relationships and contextual meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In genomics, attention can serve a similar purpose. Imagine a model processing
    a DNA sequence to predict TF binding. An attention mechanism allows the model
    to ask: how relevant is this motif to another element upstream or downstream—perhaps
    thousands of bases away? Just as a word’s meaning is shaped by the words around
    it, the function of a sequence motif may depend on other elements scattered across
    the genome.'
  prefs: []
  type: TYPE_NORMAL
- en: Once attention has enriched the token embeddings with contextual information,
    the result is passed through a *feedforward network*—typically a small, multilayer
    perceptron applied independently to each token. This network introduces nonlinearity
    and helps the model capture more complex patterns. The output is then passed through
    *residual connections* (which help with gradient flow) and *layer normalization*
    (which stabilizes training).
  prefs: []
  type: TYPE_NORMAL
- en: All together, this sequence—attention, feedforward layers, residual connections,
    and normalization—forms one transformer block. A full transformer model is typically
    built by stacking many of these blocks, allowing information to flow and be refined
    across layers. As tokens pass through successive layers, their representations
    become increasingly rich, capturing everything from local patterns to global structure.
  prefs: []
  type: TYPE_NORMAL
- en: Query, Key, and Value Intuition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might wonder: why bother transforming the input into queries, keys, and
    values at all? The key idea is that Q, K, and V aren’t just redundant copies of
    the original token embeddings. They’re learned projections that allow the model
    to look at the same sequence from different perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: The *query* is like a lens that each token uses to express what it wants to
    pay attention to. In our sentence example, the word `black` might use its query
    to find the noun it modifies—`cat`. In DNA, a regulatory region might “look for”
    compatible regulatory motifs elsewhere in the sequence. The query says, “I’m looking
    for X.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *key* is how each token presents itself to others: it encodes what kind
    of information it offers. Continuing the analogy, each word or DNA element “advertises”
    what kind of content it has, saying: “I contain Y.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *value* is the actual content that gets passed along if the query decides
    the key is relevant. In other words, the query compares itself to all keys to
    compute attention weights and then uses those weights to pull a mixture of values
    from across the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This separation of roles allows the model to reason more flexibly. Instead of
    treating all parts of the sequence as equally relevant, each token decides what
    matters to it right now, based on its query and the other tokens’ keys. The values
    then supply the useful content.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This design makes attention work like a smart lookup system: tokens advertise
    what they contain (keys), queries look for matches, and then the actual content
    (values) is pulled in weighted proportion to how well the key matched the query.'
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, the Q, K, and V projections are all learned from data. They start
    out random, and the model figures out—through training—how best to shape these
    representations for the task at hand, whether that’s learning grammar, predicting
    regulatory activity, or modeling protein interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Multiheaded Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Multiheaded attention* (MHA) enhances the attention mechanism by running several
    attention operations—called *heads*—in parallel. Each head learns to focus on
    different parts or patterns in the input sequence. For instance, one head might
    focus on local motifs, while another might detect longer-range interactions or
    subtle contextual cues.'
  prefs: []
  type: TYPE_NORMAL
- en: By combining these multiple perspectives, MHA allows the model to capture a
    richer and more diverse set of relationships within the data, beyond what a single
    attention operation could learn.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While often the different heads learn somewhat redundant patterns, this parallel
    structure increases the model’s expressive power and flexibility. The outputs
    from all heads are concatenated and linearly transformed to produce the final
    representation, which then feeds into subsequent model layers.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, MHA lets transformers attend to multiple types of interactions simultaneously,
    which is particularly useful for complex biological sequences where various signals
    and dependencies exist at different scales.
  prefs: []
  type: TYPE_NORMAL
- en: Representing Positional Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One final point: basic self-attention is *position invariant*, meaning it does
    not inherently capture the order or position of tokens in a sequence. To address
    this, transformer models include positional encodings or other mechanisms that
    inject information about the relative or absolute positions of tokens, enabling
    the model to understand sequence order. In the original transformer paper, sinusoidal
    functions of the token index were added to the token embeddings to provide this
    positional information.'
  prefs: []
  type: TYPE_NORMAL
- en: With this brief overview of transformers complete, let’s move on to some model
    interpretation techniques we’ll apply in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Model Interpretation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common criticism of deep learning models is that they act as a *black box*—they
    may produce accurate predictions, but it’s often unclear how exactly those predictions
    are made or what internal reasoning the model uses. While deep learning models
    are generally less interpretable than simpler methods like linear models or decision
    trees, there are several techniques to probe their inner workings. These techniques
    fall under the umbrella of *model interpretation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model interpretation for deep learning is a vast and active research area,
    so here we provide a brief overview of the most commonly used techniques in the
    DNA modeling space:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutagenesis
  prefs: []
  type: TYPE_NORMAL
- en: To understand which input features the model relies on, we systematically alter
    (or *mutate*) parts of the input and observe how the model’s predictions change.
    For example, when predicting gene expression from DNA sequence, we can shuffle,
    replace, or delete certain bases and see if the prediction shifts significantly.
    Large changes indicate that the mutated region is important for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pro: This is direct and intuitive. It provides rich, localized insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Con: It is computationally expensive, since each mutation requires a separate
    forward pass through the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input gradients
  prefs: []
  type: TYPE_NORMAL
- en: A faster but approximate method involves computing the gradient of the model’s
    output with respect to each input feature. This gradient shows how sensitive the
    prediction is to small changes in each input element.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pro: It is efficient, as it requires only one backward pass to generate an
    importance map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Con: It can be noisy and less precise, making it harder to distinguish signal
    from noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: For models that include attention, we can inspect the attention weights to see
    where the model is focusing when making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pro: This provides a naturally interpretable visualization of model focus and
    interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Con: Attention scales quadratically with input sequence length, meaning that
    in practice, we can’t use attention to model very long DNA strings without first
    condensing them down in some way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many extensions and refinements of these methods, and model interpretation
    is increasingly standard in deep learning biology research papers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll dive deeper into the two interpretation approaches that we will
    implement in this chapter: *in silico* mutagenesis and input gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: In Silico Saturation Mutagenesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*In silico saturation mutagenesis* (ISM) may sound complex, but it essentially
    involves systematically making every possible alteration (or mutation) to a biological
    sequence—such as DNA or protein—and generating a separate model prediction for
    each mutated sequence. Because this requires many forward passes through the model,
    it is computationally expensive. However, the detailed insights it provides into
    how each possible variation affects the output often justify the cost.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Terminology breakdown: it’s called *mutagenesis* because we induce mutations,
    *saturation* because every possible mutation is tested, and *in silico* since
    these predictions are done computationally rather than experimentally in the lab.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-4](#saturation-mutagenesis) shows an example plot we will generate
    later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Example of an in silico saturation mutagenesis plot showing predicted
    probabilities of transcription factor binding across a 200-base DNA sequence.
    The x-axis represents the DNA sequence positions, while the y-axis shows the three
    possible mutations at each position (including the mutation to the original base,
    which has no effect and is set to zero). In this example, most mutation effects
    are negative (darker color), indicating that changing bases generally reduces
    the predicted binding probability. This approach is computationally expensive
    due to the large number of forward passes required to generate this output matrix
    (here, 3 mutations × 200 positions = 600 model predictions).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ISM plots help to quickly highlight which parts of the sequence are most important
    for the model’s predictions. Because they visualize the most salient or impactful
    input regions, they are often referred to as *saliency maps*. In this example,
    the plot suggests that mutating any of the central bases likely disrupts a motif
    that the protein binds to, as these mutations generally lead to lower predicted
    binding probabilities (indicated by the negative values).
  prefs: []
  type: TYPE_NORMAL
- en: Input Gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Input gradients provide a faster way to generate a saliency map, summarizing
    which parts of the sequence most influence the model’s predictions. Conceptually,
    they are the derivatives of the model’s output with respect to its input features.
    If you’ve trained neural networks before, you’ve already encountered gradients—typically
    computed with respect to model parameters to guide weight updates.
  prefs: []
  type: TYPE_NORMAL
- en: Input gradients follow the same principle, but they shift the focus from parameters
    to the input itself. By calculating how the output changes in response to tiny
    perturbations at each input position, we can assess the model’s sensitivity. For
    DNA sequences, this means identifying which bases have the greatest influence
    on predictions like TF binding. A large gradient at a given base suggests that
    altering it would significantly impact the model’s output—signaling that the base
    is important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, for a TF binding prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: A *large negative gradient* at a base suggests that changing it would significantly
    lower the binding probability, perhaps disrupting the motif the TF needs to fit
    properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *large positive gradient* suggests that changing it would significantly increase
    the binding probability, maybe by strengthening an existing motif or creating
    a new one, thus improving the physical binding affinity between the DNA and the
    TF.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What does “making a small change to the input” mean when the input is a one-hot
    encoded DNA sequence, rather than a continuous scalar value? Each DNA sequence,
    each base (A, T, C, G) is generally represented as a binary vector (e.g., `[1,
    0, 0, 0]` for `A`). During gradient calculation, however, we treat these vectors
    as if they could vary continuously—allowing fractional values like `[0.9, 0.1,
    0, 0]`. While such fractional bases aren’t biologically meaningful, this mathematical
    abstraction lets us compute gradients and gain insights into which positions are
    influential for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of input gradients as a faster but more approximate alternative
    to in silico saturation mutagenesis. Input gradients provide a general idea of
    important regions, whereas saturation mutagenesis directly tests every possible
    mutation’s effect but is computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the biology and machine learning primers. Now let’s dive into
    exploring and modeling the data to predict transcription factor binding in DNA
    sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Simple Prototype
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The modeling task we’ll tackle in this chapter is a *binary classification
    problem*: given a 200-base DNA sequence, can we predict whether it binds to a
    specific TF called CTCF? Among the 1,500+ TFs in humans, CTCF stands out for its
    role in organizing the genome’s 3D architecture—folding DNA into loops and domains
    that regulate gene activity.'
  prefs: []
  type: TYPE_NORMAL
- en: CTCF is also a great first target because its binding behavior is relatively
    easy to model. It recognizes a well-characterized, highly conserved motif with
    strong sequence specificity, meaning its binding sites are more predictable than
    those of many other TFs. That makes CTCF an ideal entry point for building and
    interpreting sequence-based models of TF binding. Later, we will expand our scope
    to predict the binding of 10 different TFs.
  prefs: []
  type: TYPE_NORMAL
- en: As with other chapters in this book, we’ll begin by exploring the dataset, building
    a simple prototype model, and then iteratively extending and improving it.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset we will use looks like [Figure 3-5](#dna-data-task).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. The input dataset consists of DNA sequences, each 200 bases long,
    with an associated binary label indicating whether the protein CTCF binds to it.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This task is inspired by one of the evaluation challenges presented in a recent
    2024 paper preprint,^([3](ch03.html#id670)) which sourced the dataset from a 2023
    genomics interpretation study.^([4](ch03.html#id671))
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Labeled Sequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start by examining the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The two classes appear fairly balanced (equally represented) in the training
    dataset, so we won’t have to do any rebalancing by downsampling the majority class
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the DNA sequences numerically, we need to convert them into one-hot
    format. The function `dna_to_one_hot` performs this mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what the one-hot encoding looks like on a sample DNA sequence, “AAACGT”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can apply this converter to the entire training dataset to generate the
    numerical training data `x_train` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `y_train` contains the binary target labels: `0` means the sequence does
    not bind CTCF, and `1` means the sequence does bind CTCF.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset loading code for this problem is fairly straightforward, but we
    can wrap it into a convenient function called `load_dataset` for cleaner use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Converting the Data to a TensorFlow Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we convert the training data into a TensorFlow dataset. This format makes
    it easy to efficiently iterate over batches during model training, especially
    when shuffling and repeating the data for multiple epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we created the training dataset `train_ds` with batching,
    shuffling, and repetition enabled. Let’s sanity-check by pulling one batch from
    the dataset and inspecting its shape and contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This looks sensible. We see the data has shape (`32`, `200`, `4`), indicating
    a batch size of `32`, sequence length of 200, and 4 channels per DNA base as expected.
    The labels have shape (`32`, `1`) since each label is a simple binary `0` or `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For validation, since the dataset is smaller, we can structure it as one single
    big batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With our data now in the correct format, we are ready to build and train our
    first simple model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Simple Convolutional Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we define a simple CNN composed of two 1D convolutional layers, followed
    by flattening and fully connected (dense) layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This model architecture is fairly “no frills” but should already be able to
    capture local patterns in DNA sequences. We can instantiate our model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To initialize model parameters in JAX, we need to provide a dummy input tensor
    that matches the expected shape of the model’s input. Although we could use an
    actual batch from our dataset, it is common and simpler to use a tensor of ones
    with batch size 1 and the same shape as a single-encoded DNA sequence. Importantly,
    the batch size used for this dummy input does not affect the model initialization—JAX
    initializes parameters based on the shape of an individual input sample (excluding
    the batch dimension). This means the model can later be trained or used for inference
    with any batch size.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This dummy input allows JAX to infer the shapes of all model parameters, and
    the random key `rng_init` seeds any stochastic initialization, ensuring reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Examining Model Tensor Shapes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding and keeping track of tensor shapes is a crucial part of machine
    learning. Always make it a habit to verify the shapes of your data as it flows
    through the model.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a practical exercise, try adding `print(x.shape)` statements at various points
    inside the model’s `__call__` method and then rerun the `model.init` step to observe
    how the shapes change as data flows through the model layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key operations in the model that change tensor shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional layers (`nn.Conv`): These layers can modify the channel dimension.
    For example, our input starts with four channels (DNA bases), but the first convolution
    increases this to 64 channels, effectively learning 64 different sequence features
    or motifs. Additionally, the convolution’s `padding` option affects the sequence
    length dimension:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adding=''SAME''` preserves the sequence length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding=''VALID''` reduces it depending on the kernel size. Try switching
    between these to see the impact on the sequence length.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Max pooling layers (`nn.max_pool`): These reduce the sequence length by downsampling.
    In our model, each max pooling layer halves the length, from 200 → 100 → 50 bases.
    To reduce the spatial axis more aggressively (e.g., by a factor of 5 each time),
    adjust the `window_shape` and `strides` arguments accordingly (typically, these
    are set to the same value to avoid overlapping windows and simplify downsampling).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flattening (`reshape`): Before passing data to dense layers, the tensor is
    reshaped from `(batch_size, sequence_length, channels)` to `(batch_size, flattened_features)`.
    This collapses the spatial and channel dimensions into one long vector per example,
    preparing it for fully connected layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the model parameters are initialized, you can inspect them to confirm
    the layer structure and shapes. Check the parameter keys (layer names) with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, inspect each layer’s kernel shape to verify the expected parameter dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Some notes on these shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: For convolutional layers, the parameter shape is `(kernel_size, input_channels,
    output_channels)`. For example, `Conv_0` has kernel size 10, input channels 4
    (DNA bases), and outputs 64 feature maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense layers have shapes (`input_features`, `output_units`). For instance, `Dense_0`
    maps 3200 flattened features (50 sequence length * 64) to 128 units.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the model initialized and the parameter shapes explored, let’s start setting
    up our training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions with the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can actually already obtain predictions from the model using the randomly
    initialized parameters, though the predictions will be random. We make predictions
    by calling `model.apply` on a batch of sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Since the model is untrained, predicted probabilities will be around around
    0.5, reflecting pure guesswork from our randomly-parameterized model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s now define a binary cross-entropy loss function using the `optax` library
    that we can use to train our model parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use it to compute the loss for the initial batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This gives a baseline loss value before training. As the model learns, this
    loss should decrease substantially.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Why use binary cross-entropy loss? In this chapter, we’re predicting whether
    a TF binds to a given DNA sequence—a classic binary classification task. Binary
    cross-entropy is the standard loss function for this setting: it measures how
    well the model’s predicted probabilities align with the true binary labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It penalizes confident but incorrect predictions more heavily, encouraging
    well-calibrated outputs near 0 or 1\. You can also think of it as a signal reconstruction
    problem: the model tries to approximate a hidden binary signal, and cross-entropy
    imposes a sharp cost for noisy or off-target estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the TrainState
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start training the model, we first need to define the optimizer. We’ll use
    Adam with a learning rate of 1e-3, which is a common default value that tends
    to work well across diverse problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, we now have all the components to initialize the training state.
    For this, we’ll use Flax’s `TrainState` class, which is a container that bundles
    all the important objects for training (the model, parameters, and optimizer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For convenience, let’s define a function to create the train state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Defining a single training step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, putting everything together, we can write a function to run one training
    iteration, which performs these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Forward pass
  prefs: []
  type: TYPE_NORMAL
- en: Takes a batch of data, makes model predictions, and computes loss based on the
    current parameters
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Backwards pass:'
  prefs: []
  type: TYPE_NORMAL
- en: Computes gradients of the loss with respect to the parameters
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Update
  prefs: []
  type: TYPE_NORMAL
- en: Using the computed gradients, updates the parameters to minimize the model’s
    loss
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps happen in the `train_step` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run one training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If our setup is working well, the training loss should already be lower on
    this batch. Let’s check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: And indeed, the loss is lower than it was before a `train_step` was run. With
    the model and training loop ready, let’s set up a full training run.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Simple Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s now run the preceding `train_step` many times in order to optimize the
    model. Here, we train for 500 steps and periodically evaluate on the validation
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In [Figure 3-6](#training-simplest-model), we can plot the training and validation
    loss curves resulting from this run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Training and validation loss over learning steps. Both decrease,
    indicating the model is learning signal in the data over time. Note that training
    loss curve is comparatively noisier due to it being computed on a small batch
    of the data at each step (rather than the full training set), which introduces
    variability.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reaching this stage—where you have a working model, a dataset, and training
    with decreasing loss—is a major milestone. The rest of this chapter focuses on
    improving and extending this foundation.
  prefs: []
  type: TYPE_NORMAL
- en: Sanity-checking the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we do anything more complicated, we should first check that the model
    has learned something sensible. One simple check is verifying that the trained
    model behaves as expected on known DNA motifs. For example, from an online search,
    we can see that the CTCF transcription factor is known to prefer binding DNA sequences
    containing motifs similar `CCACCAGGGGGCGC`. Let’s construct the 200-base-long
    DNA string containing repeats of this motif and convert it to the one-hot encoded
    format that our model expects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We expect that the model will predict a very high probability of CTCF binding
    this sequence, since it’s packed with the relevant motif. Let’s check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: And this is indeed the case—the predicted probability of binding is very close
    to 1\. This means the model has learned to identify this motif in the DNA sequence
    and associate it with CTCF binding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, if we construct some pseudorandom DNA strings, the model should
    predict a relatively low probability of CTCF binding them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As expected, these probabilities look relatively low, meaning that the CTCF
    protein is not likely to bind these sequences of random DNA. This completes a
    basic sanity check of our approach, but let’s dig a bit deeper into what this
    model has already learned about DNA sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we introduce two important extensions to our modeling approach.
  prefs: []
  type: TYPE_NORMAL
- en: First, we focus on *model interpretation*. We’ll apply two techniques from the
    earlier machine learning primer—in silico mutagenesis (ISM) and input gradients—to
    better understand what the model has learned. These methods produce *contribution
    scores* (or *saliency maps*) that assign an importance value to each base in the
    DNA sequence, indicating how much that base influences the model’s prediction
    of TF binding.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we’ll expand the scope of our prediction task. Instead of predicting
    binding for just one transcription factor (CTCF), we train models for *all 10
    transcription factors* in the dataset. This allows us to explore how model performance
    varies across TFs and how motif preferences differ between them.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these steps deepen both our understanding of the model’s behavior
    and the complexity of the biological task it’s modeling. After this, we’ll turn
    our attention to improving the model architecture itself.
  prefs: []
  type: TYPE_NORMAL
- en: Conducting In Silico Mutagenesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from the introduction that ISM is a technique in which each base in a
    DNA sequence is systematically mutated to all possible alternative bases one at
    a time, and the effect of each mutation on a given output (in this example, CTCF
    binding probability) is quantified. This allows us to identify which regions are
    important to the output—unimportant regions can be freely mutated without impacting
    predictions, whereas important regions significantly affect the probability of
    TF binding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before making all possible mutations, let’s first check the effect of making
    just a single mutation. We’ll start by identifying a DNA sequence in the validation
    set that binds the CTCF protein (i.e., has a label of `1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s examine what the model predicts for this unmodified sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The original sequence has a predicted binding probability of 95.8%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s create a single mutation at position 100\. The original base is a
    `G` (encoded as `[0, 0, 1, 0]`), and we change it to a `C` (encoded as `[0, 1,
    0, 0]`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll now run the model again to see if this mutation has a measurable effect
    on the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: After the mutation, the model’s predicted binding probability drops to 93.4%,
    a decrease of over 2.4%. This shows that even a single-base change at a sensitive
    position can substantially impact the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve observed how a single mutation can influence the prediction,
    let’s extend this approach to systematically mutate every position in the sequence.
    This will give us a more global view of which bases matter most for the model’s
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing in silico saturation mutagenesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s the basic plan for our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutate
  prefs: []
  type: TYPE_NORMAL
- en: Start with the original sequence. At each position, change the base to each
    of the other three possible DNA bases.
  prefs: []
  type: TYPE_NORMAL
- en: Predict
  prefs: []
  type: TYPE_NORMAL
- en: Run the model on each mutated sequence and record the predicted binding probability.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate
  prefs: []
  type: TYPE_NORMAL
- en: Collect all results to identify which mutations cause meaningful changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by generating all possible single-base mutations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have 800 sequences: four variants for each of the 200 positions. Although
    only three of those per position are true mutations (since mutating, say, `A`
    to an `A` is a no-op), we include all four to simplify downstream logic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run predictions on all these mutated sequences in a single batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s visualize the predicted binding probabilities for every mutated sequence
    in [Figure 3-7](#mutation-plot-saturated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. All possible variations of a 200-base pair DNA region and their
    corresponding probabilities of TF binding, represented as a heatmap. The x-axis
    indicates a position in the DNA sequence, and the y-axis represents each possible
    DNA base at that position. The value represents the predicted probability of CTCF
    binding.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This shows us that:'
  prefs: []
  type: TYPE_NORMAL
- en: Most mutations don’t change the prediction much. The model predicts close to
    the original 95.8% probability of CTCF binding for sequences containing most mutations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A central region does appear to significantly affect the binding prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But what we really care about is how much each mutation changes the prediction.
    Let’s subtract the original (unmutated) predicted probability and center the color
    map at zero in [Figure 3-8](#mutation-plot-difference):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. In silico mutagenesis results showing the change in predicted CTCF
    binding probability for each possible mutation. Light and dark shades indicate
    a deviation from the original prediction, with lighter colors showing mutations
    that increase predicted binding and darker colors showing those that decrease
    binding.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now the heatmap is clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: Most positions are lighter, meaning their mutations have little effect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few bases in the center are dark (decreased binding) or light (increased binding),
    showing meaningful influence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can quantify mutation effects using a helper function, `describe_change`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It allows us to have a look at the impact of mutating position 100 for all
    bases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also summarize the overall importance of each position in the DNA sequence
    by summing the absolute changes across all possible base mutations at each position.
    The result is visualized in [Figure 3-9](#mutation-plot-stacked), highlighting
    which regions the model considers most influential for its prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Positional importance of the TF binding motif. The bottom panel
    is the same as in [Figure 3-8](#mutation-plot-difference) with a linegraph superimposed.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can identify the most impactful mutations by ranking the values. In this
    case, the largest *increase* comes from mutating the base at position 92 with
    G→C (3.11% increase), and the biggest decrease comes from mutating the base at
    position 102 with G→T (-20.24% decrease).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a great moment to reflect on the biological meaning of these plots.
    The central region of the sequence likely contains the CTCF binding motif—mutations
    here have a strong impact on the model’s prediction. In contrast, flanking regions
    show little effect, indicating they contribute less to binding. Interestingly,
    most impactful mutations in the core motif tend to reduce predicted binding, suggesting
    the original sequence already contains a fairly strong CTCF motif that’s difficult
    to strengthen with single-base changes.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying motif presence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To validate whether the region identified by our model corresponds to a known
    CTCF binding motif, we can use an external bioinformatics tool. But first, we
    need to convert the one-hot encoded sequence back into the standard DNA string
    format. This is done using the `one_hot_to_dna` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'which we can use on our sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: After converting the sequence, we paste it into the FIMO (Find Individual Motif
    Occurrences) tool, which is part of the MEME suite of motif discovery and search
    tools. FIMO allows for fuzzy matching of motifs—meaning the match doesn’t need
    to be exact—which better reflects biological reality, as transcription factors
    often tolerate some variability in their binding sites.
  prefs: []
  type: TYPE_NORMAL
- en: For CTCF, the known binding motif we used was CCACCAGGGGGCGC. When we submitted
    our sequence, FIMO reported a match to the CTCF motif (specifically, the subsequence
    GCCTCTGGGGGCGC, spanning positions 93 to 106) with a highly significant p-value
    of 2.6e-05, as shown in [Figure 3-10](#dlfb_0310).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. How we used FIMO to search for the known CTCF binding motif within
    a 200 bp DNA sequence labeled as positive. Although transcription factor motifs
    are usually represented more flexibly using position weight matrices (PWMs), a
    simple string-based search for the canonical motif `CCACCAGGGGGCGC` offers a quick
    sanity check to confirm that the expected pattern is present in the sequence.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To further verify this, we can overlay the motif region found by FIMO on our
    earlier saliency map of mutation impact. This helps us visually compare where
    mutations have the strongest effect on the model’s prediction versus where the
    detected motif actually occurs (see [Figure 3-11](#mutation-plot-stacked-highlighted)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Highlight of the TF binding site overlapping neatly with the region
    where mutations have the biggest influence.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the motif returned by FIMO lines up almost exactly with the
    most mutation-sensitive region identified by our model through in silico mutagenesis.
    This alignment gives us confidence that the model has learned to recognize meaningful
    biological signal—in this case, a known CTCF binding motif—directly from sequence
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing input gradients
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While *in silico* mutagenesis provides highly interpretable insights, it can
    be computationally expensive—requiring multiple forward passes through the model
    for each position in the input sequence. An alternative approach that is much
    cheaper to compute is *input gradients*. This technique, introduced earlier in
    the chapter, relies on a simple idea: how much does the model output change if
    we nudge each input base slightly?'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, this corresponds to computing the gradient of the model’s prediction
    with respect to its input sequence. If a small change in a particular base results
    in a large change in the output, that base must be important.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation is straightforward. We use `jax.grad` to take the derivative
    of the model’s predicted binding probability with respect to the one-hot input
    sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s apply it to the same sequence we used for ISM and inspect the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The result is a matrix of the same shape as the input sequence, where each value
    indicates how sensitive the model’s prediction is to a small change in a particular
    base at a particular position.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize this as a heatmap in [Figure 3-12](#mutation-plot-input-gradients):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like with ISM, we see that most positions have little influence on the
    prediction, while a central region—roughly positions 90 to 110—exhibits strong
    gradients. This reflects the bases that the model is most sensitive to for making
    its prediction. However, input gradients differ from ISM in key ways: the values
    are not bounded to represent discrete mutations, and even the base currently present
    at each position can have a nonzero gradient. This is because the gradient describes
    how the model’s output would change with an infinitesimal increase in that base’s
    activation—not an actual mutation. This makes gradients a bit more abstract but
    also more flexible and computationally efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Input gradient saliency map for a CTCF-binding sequence, indicating
    the contribution of each DNA base to the model’s prediction of CTCF binding. The
    bottom heatmap shows input gradients per base (A/C/G/T), while the top line shows
    aggregated importance across positions. A clear central region stands out as most
    influential for the model’s decision.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To examine this region more closely, we can zoom in and label the actual bases
    in the 90:110 base region in [Figure 3-13](#mutation-plot-input-gradients-zoomed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This plot confirms what we saw using ISM and the FIMO tool: the model is placing
    high importance on the central region of the sequence, which corresponds to a
    canonical CTCF binding motif.'
  prefs: []
  type: TYPE_NORMAL
- en: While input gradients are computationally much cheaper than ISM, they can be
    somewhat noisier or harder to interpret due to model nonlinearities or saturation
    effects. Still, they offer a practical way to rapidly inspect what a model is
    attending to—especially when analyzing many sequences at once.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. Zoomed input gradients over the CTCF motif. In this close-up view
    of the region identified as important by input gradients, each cell shows how
    the model’s predicted CTCF binding probability would change in response to a small
    increase in a specific base at a given position. Unlike in silico mutagenesis,
    even the base that is already present (e.g., G→G) can have a high gradient—indicating
    that the model is highly sensitive to that base and relies on it for its prediction.
    Strongly positive or negative values reflect positions where the model has learned
    a motif and is using it to assess CTCF binding.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So far, we have been analyzing a single sequence example (the first validation
    example). In [Figure 3-14](#mutation-plot-ten-input-gratient-panels) we extend
    this to the first 10 sequences in the validation set that are labeled 1 (i.e.,
    sequences that are known to bind CTCF), and visualize their input gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. Input gradients for 10 CTCF-bound sequences show consistent central
    regions of high importance, suggesting a shared motif-like structure driving model
    predictions. Each cell shows the gradient value for mutating a specific base at
    a given position. Strongly positive or negative gradients indicate high sensitivity
    to mutations.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These gradients show a consistent central pattern across the examples, indicating
    that the model has likely learned a motif centered within the sequence. While
    the width of the important region varies slightly, the signal is strong and localized.
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, [Figure 3-15](#dlfb_0315) shows the input gradients for 10 negative
    examples—sequences that the model predicts do not bind CTCF. Here, the picture
    is far more heterogeneous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15\. Input gradients for 10 nonbinding sequences reveal more diffuse
    or noisy importance patterns, but often retain a weak central focus—likely reflecting
    the peak-centered sampling strategy used during dataset construction.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this plot, some sequences show a weaker but still present centered signal.
    Others display no clear motif or have diffuse scores spread across the entire
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: This variation makes sense given the way the dataset was constructed. While
    these sequences are labeled negative, they were carefully matched to positive
    examples in terms of chromatin accessibility. That means they also come from open
    chromatin regions and may still contain weak or partial motifs—or even motifs
    for other TFs. As a result, the model might still “attend” to the center of the
    sequence, even if it ultimately predicts no binding.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These findings serve as a good reminder that contribution scores like input
    gradients don’t just reflect the presence or absence of strong motifs. They also
    reveal where the model is looking for evidence and can surface subtle or confounding
    biological signals introduced by dataset design choices. In this case, the weak
    central patterns likely reflect a bias introduced by sampling from peak-centered
    open chromatin, even in negative (class 0) examples.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored two complementary interpretation tools—ISM and input
    gradients—we’re in a better position to trust (and debug) what the model is learning.
    Let’s return to the modeling task and increase both the problem complexity and
    model capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling Multiple Transcription Factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve focused on a single transcription factor: CTCF. Let’s now increase
    the biological scope of our modeling by predicting binding for all 10 TFs in the
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a multi-TF dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The dataset includes binding labels for the following 10 transcription factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of these TFs has its own distinct DNA-binding preference. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: CTCF, as we’ve seen, binds motifs like `CCACCAGGGGGCGC`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAX prefers the E-box motif `CACGTG`, important in regulating cell proliferation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SRF binds to `CCW6GG` (where W = A or T), a motif involved in muscle-specific
    gene expression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These preferences are often conserved across species, and the role of each TF
    can be deeply rooted in specific cell type identity or developmental processes.
    If you’re curious, it’s worth looking up some of these proteins—for example, REST
    is a key repressor in neurons, while MAX has a known role in oncogenesis through
    interaction with MYC.
  prefs: []
  type: TYPE_NORMAL
- en: Our task now is to train models that can automatically discover these motif
    patterns and accurately associate them with TF binding.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common strategies for tackling this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Multitask learning
  prefs: []
  type: TYPE_NORMAL
- en: Train a single model to output one prediction per TF, potentially learning shared
    representations across tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Single-task learning
  prefs: []
  type: TYPE_NORMAL
- en: Train separate binary classification models for each TF independently.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the 10 TFs are not particularly closely related, and their binding
    preferences are quite distinct—ranging from insulator proteins like CTCF to general
    transcriptional regulators like MAX and neuron-specific factors like REST. Additionally,
    the dataset provides separate training sets for each TF, and the original paper
    trained them independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these factors, we’ll follow a single-task learning approach: train 10
    individual binary classification models, one per TF. This also contrasts nicely
    with the multitask approach you saw in [Chapter 2](ch02.html#learning-the-language-of-proteins).'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a more complex model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’re training on 10 separate TF binding tasks, it’s a good opportunity
    to improve the training stability and generalization of our model.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll retain the same core convolutional architecture from earlier, but introduce
    three standard deep learning techniques—batch normalization, dropout, and learning
    rate scheduling—that are widely used in CNNs and often help even for relatively
    shallow models and simple datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs: []
  type: TYPE_NORMAL
- en: 'We add batch normalization after each convolutional layer to improve training
    stability:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch norm normalizes the activations across the batch, which helps smooth the
    optimization landscape and accelerates convergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though our network is relatively shallow, batch norm can still improve
    performance and robustness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few implementation details:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch norm behaves differently during training and inference. During training,
    it computes mean and variance from the current batch and updates running averages;
    at inference time, it uses the learned averages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Flax, you control this behavior with the `is_training` flag using the `use_running_average=not
    is_training` pattern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout regularization
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce overfitting, we add dropout after the dense (fully connected) layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout randomly sets a portion of activations to zero during training, encouraging
    the model to learn redundant and robust features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is typically used after dense layers, rather than convolutions, because spatially
    shared convolutional filters already generalize well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Flax, dropout requires passing a PRNG (pseudorandom number generator) key
    to the forward pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a moderate dropout rate of 0.2, which adds some regularization without
    significantly reducing model capacity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate scheduling
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than using a fixed learning rate, we adopt a learning rate schedule
    to guide training:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic learning rates usually start high (encouraging fast exploration) and
    decrease over time to help convergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular options include exponential decay, step decay, and cosine annealing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implemented via `optax.cosine_decay_schedule`, we use a cosine decay schedule
    which gradually reduces the learning rate over the course of training. The shape
    of this learning rate schedule is visualized in [Figure 3-16](#dlfb_0316):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](assets/dlfb_0316.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3-16\. Cosine decay learning rate schedule used during training. The
    learning rate starts at 0.001 and gradually decreases over 1,000 steps, helping
    the model explore early on and converge smoothly toward the end.
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Even though our dataset and architecture are fairly simple, these additions
    should help improve training stability and model generalization across the expanded
    set of transcription factor tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now implement the updated model definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Our `ConvModelV2` implementation is starting to get a bit long and repetitive.
    Later in the chapter, we’ll address this by refactoring out the repeated logic
    into `ConvBlock` and `DenseBlock` components to make the model definition more
    concise and modular.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we have all the pieces needed to create the training state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The training step looks fairly similar to what we’ve seen before, but now it
    needs to handle both dropout and batch normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'One small additional change: the loss function is defined directly inside `train_step`.
    This is often done for readability and encapsulation, especially when the loss
    depends on extra mutable model state (like `batch_stats`) or dropout, both of
    which are now required in the forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that everything is wired up correctly, we can overfit to a single
    batch—that is, run a few steps on the same batch and check that the loss decreases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Looks good—the loss decreases quickly. This indicates the model is capable of
    fitting the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, loss alone isn’t the best way to evaluate classification models. We
    want to monitor additional metrics like accuracy and the area under the ROC curve
    (auROC). We compute these in an evaluation step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Note that since `scikit-learn` functions are not JAX compatible, the evaluation
    step is not decorated with `@jax.jit`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the output of running `eval_step`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve implemented the training and evaluation steps, let’s define
    a full training loop. The train function takes an initialized model state, training
    and validation datasets, and a few configuration parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The training loop is quite similar to before, but as a quick recap, here is
    what it does:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterates through training steps using a progress bar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs `train_step` to update model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Periodically evaluates on the validation set using `eval_step`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs and stores metrics at each step using a custom MetricsLogger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we’re training one model per TF, we’ll use a utility function to load
    datasets split by TF name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: This function loads training, validation, and test splits for the given `transcription_factor`
    and converts them into TensorFlow datasets ready for batching.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have everything in place to train one model per TF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'With training complete, let’s take a look at just CTCF transcription factor
    performance in [Figure 3-17](#dlfb_0317):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0317.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. Learning curves for the CTCF transcription factor. The left panel
    shows the training and validation loss over time. The right panel tracks the model’s
    classification performance using validation set accuracy and auROC. Performance
    improves steadily during training and converges smoothly.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our model performs well on predicting whether CTCF binds a given DNA sequence,
    with training and validation losses decreasing and validation auROC approaching
    a perfect score of 1.0—consistent with the fact that CTCF is relatively easy to
    predict compared to some of the other TFs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now visualize the training curves across all 10 TFs. We first process
    the logged metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then visualize their learning curves in [Figure 3-18](#learning-curves-10-tfs)
    sorted by auROC performance (best-performing TFs are first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0318.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. Learning curves for all 10 TFs. Each panel shows the training
    and validation loss (solid lines), validation accuracy (dashed lines), and validation
    auROC (dotted lines) over training steps. TFs are ordered by peak auROC performance.
    While some TFs, like CTCF and ATF2, reach near-perfect performance quickly, others,
    such as ZNF24 and BACH1, prove more challenging to model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Are these different TF binding models doing a good job? It can be hard to know
    what constitutes a “good enough” auROC—especially in biological settings where
    label noise and dataset complexity can vary widely. Fortunately, since our dataset
    comes from a published benchmark, we can directly compare our model’s results
    to those reported in the original paper.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 3 of the source paper](https://oreil.ly/QciKO), we see that auROC
    scores vary considerably across TFs, and some are challenging to predict well—even
    when using information from more advanced architectures like genomic language
    models (gLMs) trained with large-scale pretraining. The paper’s figure helps establish
    a performance ceiling for baseline CNNs trained on one-hot encoded DNA. Interestingly,
    the paper notes that pretrained gLM representations do not consistently outperform
    conventional approaches using one-hot inputs, suggesting that simple models can
    still be competitive on these TF binding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are key takeaways from the paper’s figure:'
  prefs: []
  type: TYPE_NORMAL
- en: CTCF and ATF2 are the most predictable TFs, with both one-hot and pretrained
    models achieving auROC scores above 0.95\. These TFs have strong, conserved binding
    motifs that are easy for models to learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: REST, MAX, and ELK1 show intermediate difficulty, with auROCs around 0.83 to
    0.85.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZNF24, BACH1, ARID3, and GABPA tend to be more difficult, with auROCs hovering
    in the 0.75 to 0.80 range.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now see how our own models compare. The following prints the peak validation
    auROC achieved by our CNNs trained independently for each TF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Our results closely mirror the rankings and scores from the paper—especially
    in terms of which TFs are easier or harder to predict. This consistency offers
    reassuring external validation that our training setup is functioning correctly
    and our models are learning something meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore how we might push our TF-binding prediction results further
    through more expressive architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we introduce more complex model components, let’s first improve the clarity
    and modularity of our model architecture by refactoring it into reusable building
    blocks. This makes our code easier to read, extend, and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will modularize our convolutional and MLP layers by creating two helper
    modules, `ConvBlock` and `MLPBlock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'With these reusable blocks in place, we can now define a more compact and configurable
    model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: This architecture supports optional transformer blocks inserted between the
    convolutional feature extractors and the MLP classifier. You’ll also notice the
    introduction of `TransformerBlock`, which we’ll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Self-attention and Transformer Blocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our final architecture combines convolutional layers with transformer blocks.
    While convolutional layers are excellent at extracting local motif-like patterns
    from DNA, transformer blocks can capture long-range dependencies—patterns that
    span larger regions of the sequence. These two types of layers are highly complementary.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In our case, the sequences are relatively short (just 200 base pairs), so the
    benefits of long-range attention may be modest. However, attention mechanisms
    become increasingly useful as input length increases. For longer genomic contexts—such
    as full gene bodies, or interactions between potentially distant regulatory elements
    such as promoter and enhancer sequences—transformers can integrate signals across
    a broader range than is possible with fixed-size convolutional kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flax includes a built-in `SelfAttention` module, which we can use as the foundation
    for our `TransformerBlock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'A few notes on this design:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention
  prefs: []
  type: TYPE_NORMAL
- en: The core operation is `nn.SelfAttention`, which enables each position in the
    sequence to attend to every other position, capturing dependencies across the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: Residual connections
  prefs: []
  type: TYPE_NORMAL
- en: The skip connections help stabilize training by allowing gradients to flow more
    easily through deep architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization
  prefs: []
  type: TYPE_NORMAL
- en: Transformers typically use layer norm rather than batch norm, as it tends to
    be more stable for sequence-based models and is invariant to batch size.
  prefs: []
  type: TYPE_NORMAL
- en: Position information
  prefs: []
  type: TYPE_NORMAL
- en: In our current setup, we omit explicit positional encodings—this is a simplification
    that may be acceptable for short sequences (e.g., 200 bp), where the relative
    arrangement of motifs can still be learned through the convolutional layers preceding
    attention. For longer inputs or if attention is applied very early, consider adding
    learned or sinusoidal positional encodings.
  prefs: []
  type: TYPE_NORMAL
- en: This modular `TransformerBlock` can now be optionally included between convolutional
    and MLP layers in our `ConvTransformerModel`.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Various Model Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that our model is modular, we can experiment with different architectural
    settings to better understand their impact on performance. This kind of architectural
    exploration is common when tuning deep learning models—there’s rarely a single
    obvious “best” model, and trying multiple variants can reveal helpful insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we define several models with different settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'To test these variants, we’ll focus on the most difficult TF from our earlier
    results: ZNF24, which achieved a peak validation auROC of 0.76 in our initial
    model. This makes it a great candidate to explore whether architectural improvements
    can lead to better predictive performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Sweeping Over the Different Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With all our model components now modularized, we can easily run systematic
    experiments to compare architectural choices.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll train several model variants—ranging from simpler ablations to transformer-enhanced
    architectures—on the most challenging TF in our benchmark: ZNF24\. This allows
    us to explore which components help or hurt performance on a relatively difficult
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With all of this in place, we can train the different models in the `model`
    dict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'After training each model, we extract logged metrics over training time and
    visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code, we can plot our learning curves and metrics over time in [Figure 3-19](#learning-curve-panels-model-variants-znf24):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0319.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-19\. Learning curves for different model architectures on ZNF24 binding
    prediction. Each panel shows training and validation loss, accuracy, and auROC.
    We see that adding transformer blocks slightly improves performance, while removing
    capacity (e.g., fewer conv filters) hurts it.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To better isolate model differences, we can plot just the validation auROC
    over time in [Figure 3-20](#comparison-of-model-variants-znf24):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0320.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-20\. Validation auROC across training steps for each model variant.
    Models with transformer blocks outperform simpler baselines, while reducing convolutional
    capacity or removing MLPs tends to hurt performance.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'And here are the max auROC values for each model (by step 1,000):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Some observations and hypotheses based on these results:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions are critical
  prefs: []
  type: TYPE_NORMAL
- en: The `single_conv_only` model underperforms relative to the baseline, and reducing
    the number of convolutional filters degrades performance further. This suggests
    that having multiple convolutional layers with sufficient capacity is important
    for learning meaningful sequence features. It may be worth exploring deeper convolutional
    stacks or wider kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer blocks help
  prefs: []
  type: TYPE_NORMAL
- en: Adding one or two self-attention blocks gives a modest but consistent improvement
    in auROC, despite the input sequences being only 200 bp long. This supports the
    idea that even short DNA windows can benefit from modeling long-range dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: MLPs may be unnecessary
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, removing the MLP blocks doesn’t drastically hurt performance.
    This suggests that most of the representational power is coming from earlier convolutional
    layers, and the additional MLP layers may be redundant.
  prefs: []
  type: TYPE_NORMAL
- en: Training dynamics
  prefs: []
  type: TYPE_NORMAL
- en: Models with transformer blocks exhibit noisier validation loss during early
    training. This instability might be reduced with a smaller initial learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: While we won’t exhaustively optimize hyperparameters here, these results demonstrate
    the value of modular model exploration and raise intriguing questions for further
    study.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating on the Test Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final step is to evaluate our best-performing model on the held-out test
    set. This ensures that the model’s performance generalizes to completely unseen
    data and was not overfit to the training or validation distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reload the checkpoint for the top model—selected based on the highest validation
    auROC—and evaluate it on the test split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Success! Our best model achieves a comparable AUC on the test set to what we
    saw on the validation set, demonstrating good generalization to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions and Improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many ways to extend this work—in terms of both analyzing model behavior
    and exploring new modeling directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few analysis ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Failure analysis
  prefs: []
  type: TYPE_NORMAL
- en: Inspect misclassified sequences. Do they correspond to originally noisy or lower-magnitude
    ChIP-seq peaks? Are there weak motif matches within them? Trace these sequences
    back to the raw data to understand the source of prediction errors.
  prefs: []
  type: TYPE_NORMAL
- en: Motif discovery
  prefs: []
  type: TYPE_NORMAL
- en: Use saliency maps to extract high-signal regions and align them to known motifs
    from databases like JASPAR or HOCOMOCO. You can also explore tools like TF-MoDISco
    for automatic motif discovery based on contribution scores.
  prefs: []
  type: TYPE_NORMAL
- en: Attribution comparisons
  prefs: []
  type: TYPE_NORMAL
- en: Compare in silico mutagenesis and input gradients across TFs. Do easier tasks
    yield more consistent or interpretable saliency patterns?
  prefs: []
  type: TYPE_NORMAL
- en: Cross-TF generalization
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate a model trained on one TF against the validation data for another.
    Which TFs generalize well to others? This could reveal shared motif features or
    similarities in binding logic.
  prefs: []
  type: TYPE_NORMAL
- en: Saliency reproducibility
  prefs: []
  type: TYPE_NORMAL
- en: Compare saliency maps across different training runs or architectures to assess
    how reliably motif patterns are captured.
  prefs: []
  type: TYPE_NORMAL
- en: 'And some potential modeling extensions:'
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization
  prefs: []
  type: TYPE_NORMAL
- en: Tune architectural hyperparameters such as the number of convolutional filters,
    kernel width, dropout rate, or transformer head count and MLP size.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encodings
  prefs: []
  type: TYPE_NORMAL
- en: Add sinusoidal or learned positional embeddings to improve transformer modeling
    of sequence order—especially valuable for longer DNA sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Multitask setup
  prefs: []
  type: TYPE_NORMAL
- en: Build a multitask model that predicts binding for all TFs simultaneously. This
    allows shared representations across tasks and may improve performance on lower-data
    TFs with less data.
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative binding prediction
  prefs: []
  type: TYPE_NORMAL
- en: Instead of binary classification, predict continuous binding intensity (e.g.,
    ChIP-seq signal). This would require adapting the model for sequence-to-sequence
    or dense regression output.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune pretrained DNA models such as DNABERT or Nucleotide Transformer to
    leverage prior genomic knowledge and improve performance with less data.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: Improve generalization with reverse complements, shifted windows (jittering),
    or synthetic motif insertions.
  prefs: []
  type: TYPE_NORMAL
- en: These directions offer exciting possibilities for improving accuracy, interpretability,
    and generalizability in TF binding prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the fascinating world of gene regulation through
    transcription factor binding. Starting with simple convolutional models, we built
    a foundation for understanding how neural networks can learn to recognize sequence
    motifs in raw DNA sequences. From there, we incrementally increased model complexity—adding
    batch normalization, dropout, and even transformer blocks—to investigate how architectural
    changes impact performance.
  prefs: []
  type: TYPE_NORMAL
- en: This gradual, modular approach to model development not only helped clarify
    the strengths of each component, but also made debugging and evaluation more manageable.
    Along the way, we touched on model interpretability, performance benchmarking,
    and ideas for deeper analysis and extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to this point, we’ve focused on biological data with an inherent sequential
    structure—first with protein sequences in [Chapter 2, “Learning the Language of
    Proteins”](ch02.html#learning-the-language-of-proteins) and now with DNA. In the
    next chapter, we shift gears to a very different kind of data: *graphs*. We’ll
    explore how graph neural networks can help us reason about relationships between
    entities—in particular, interactions between different drugs.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch03.html#id616-marker)) International Human Genome Sequencing Consortium,
    “Initial Sequencing and Analysis of the Human Genome.” Nature 409 (2001): 860–921.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch03.html#id638-marker)) Vaswani, Ashish, Noam Shazeer, Niki Parmar,
    Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
    2017\. [“Attention Is All You Need”](https://oreil.ly/-Qa6G). *arXiv (Cornell
    University)* 30 (June): 5998–6008.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#id670-marker)) Tang, Z., Somia, N., Yu, Y., & Koo, P. K. (2024).
    [Evaluating the representational power of pre-trained DNA language models for
    regulatory genomics](https://doi.org/10.1101/2024.02.29.582810). bioRxiv (Cold
    Spring Harbor Laboratory).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#id671-marker)) Majdandzic, A., Rajesh, C., & Koo, P. K. (2023).
    [Correcting gradient-based interpretations of deep neural networks for genomics](https://doi.org/10.1186/s13059-023-02956-3).
    Genome Biology, 24(1).
  prefs: []
  type: TYPE_NORMAL
