- en: 'Chapter 6\. Maximizing Speed and Performance of TensorFlow: A Handy Checklist'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。最大化TensorFlow的速度和性能：一个便捷清单
- en: Life is all about making do with what we have, and optimization is the name
    of the game.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生活就是要用手头的资源做到最好，优化就是游戏的名字。
- en: It’s not about having everything—it’s about using your resources wisely. Maybe
    we really want to buy that Ferrari, but our budget allows for a Toyota. You know
    what, though? With the right kinds of performance tuning, we can make that bad
    boy race at NASCAR!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 关键不在于拥有一切，而在于明智地利用你的资源。也许我们真的想买那辆法拉利，但我们的预算只够买一辆丰田。不过你知道吗？通过正确的性能调优，我们可以让那家伙在纳斯卡比赛中飞驰！
- en: Let’s look at this in terms of the deep learning world. Google, with its engineering
    might and TPU pods capable of boiling the ocean, set a speed record by training
    ImageNet in just about 30 minutes! And yet, just a few months later, a ragtag
    team of three researchers (Andrew Shaw, Yaroslav Bulatov, and Jeremy Howard),
    with $40 in their pockets using a public cloud, were able to train ImageNet in
    only 18 minutes!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从深度学习世界来看这个问题。谷歌凭借其工程实力和能够煮沸海洋的TPU机架，在约30分钟内训练ImageNet创下了速度纪录！然而，仅仅几个月后，三名研究人员（Andrew
    Shaw、Yaroslav Bulatov和Jeremy Howard）带着口袋里的40美元，在公共云上只用了18分钟就训练完了ImageNet！
- en: The lesson we can draw from these examples is that the amount of resources that
    you have is not nearly as important as using them to their maximum potential.
    It’s all about doing more with less. In that spirit, this chapter is meant to
    serve as a handy checklist of potential performance optimizations that we can
    make when building all stages of the deep learning pipelines, and will be useful
    throughout the book. Specifically, we will discuss optimizations related to data
    preparation, data reading, data augmentation, training, and finally inference.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些例子中得出的教训是，你拥有的资源量并不像你充分利用它们那样重要。关键在于用最大潜力来充分利用资源。这一章旨在作为一个潜在性能优化的便捷清单，我们在构建深度学习流水线的所有阶段都可以使用，并且在整本书中都会很有用。具体来说，我们将讨论与数据准备、数据读取、数据增强、训练以及最终推断相关的优化。
- en: And the story starts and ends with two words...
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 而这个故事始于两个词，也终于两个词...
- en: GPU Starvation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU饥饿
- en: A commonly asked question by AI practitioners is, “Why is my training so slow?”
    The answer more often than not is GPU starvation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能从业者经常问的一个问题是，“为什么我的训练速度这么慢？”答案往往是GPU饥饿。
- en: GPUs are the lifelines of deep learning. They can also be the most expensive
    component in a computer system. In light of that, we want to fully utilize them.
    This means that a GPU should not need to wait for data to be available from other
    components for processing. Rather, when the GPU is ready to process, the preprocessed
    data should already be available at its doorstep and ready to go. Yet, the reality
    is that the CPU, memory, and storage are frequently the performance bottlenecks,
    resulting in suboptimal utilization of the GPU. In other words, we want the GPU
    to be the bottleneck, not the other way round.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: GPU是深度学习的生命线。它们也可能是计算机系统中最昂贵的组件。鉴于此，我们希望充分利用它们。这意味着GPU不应该等待来自其他组件的数据以进行处理。相反，当GPU准备好处理时，预处理的数据应该已经准备就绪并等待使用。然而，现实是CPU、内存和存储通常是性能瓶颈，导致GPU的利用率不佳。换句话说，我们希望GPU成为瓶颈，而不是反过来。
- en: Buying expensive GPUs for thousands of dollars can be worthwhile, but only if
    the GPU is the bottleneck to begin with. Otherwise, we might as well burn the
    cash.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为数千美元购买昂贵的GPU可能是值得的，但前提是GPU本身就是瓶颈。否则，我们可能还不如把钱烧了。
- en: To illustrate this better, consider [Figure 6-1](part0008.html#gpu_starvationcomma_while_waiting_for_cp).
    In a deep learning pipeline, the CPU and GPU work in collaboration, passing data
    to each other. The CPU reads the data, performs preprocessing steps including
    augmentations, and then passes it on to the GPU for training. Their collaboration
    is like a relay race, except one of the relay runners is an Olympic athlete, waiting
    for a high school track runner to pass the baton. The more time the GPU stays
    idle, the more wasted resources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明这一点，考虑[图6-1](part0008.html#gpu_starvationcomma_while_waiting_for_cp)。在深度学习流水线中，CPU和GPU协作工作，彼此传递数据。CPU读取数据，执行包括增强在内的预处理步骤，然后将其传递给GPU进行训练。它们的合作就像接力比赛，只不过其中一个接力选手是奥运会运动员，等待着一个高中田径选手传递接力棒。GPU空闲的时间越长，资源浪费就越多。
- en: '![GPU starvation, while waiting for CPU to finish preparing the data](../images/00112.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![GPU饥饿，等待CPU完成数据准备](../images/00112.jpeg)'
- en: Figure 6-1\. GPU starvation, while waiting for CPU to finish preparing the data
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. GPU饥饿，等待CPU完成数据准备
- en: A large portion of this chapter is devoted to reducing the idle time of the
    GPU and the CPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大部分内容致力于减少GPU和CPU的空闲时间。
- en: 'A logical question to ask is: how do we know whether the GPU is starving? Two
    handy tools can help us answer this question:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个合理的问题是：我们如何知道GPU是否饥饿？两个方便的工具可以帮助我们回答这个问题：
- en: '`nvidia-smi`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`'
- en: This command shows GPU statistics including utilization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令显示GPU的统计信息，包括利用率。
- en: TensorFlow Profiler + TensorBoard
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Profiler + TensorBoard
- en: This visualizes program execution interactively in a timeline within TensorBoard.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这在TensorBoard中以时间线的形式交互式地可视化程序执行。
- en: nvidia-smi
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: nvidia-smi
- en: Short for NVIDIA System Management Interface program, `nvidia-smi` provides
    detailed statistics about our precious GPUs, including memory, utilization, temperature,
    power wattage, and more. It’s a geek’s dream come true.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`的全称是NVIDIA系统管理接口程序，提供了关于我们珍贵GPU的详细统计信息，包括内存、利用率、温度、功耗等。这对于极客来说是一个梦想成真。'
- en: 'Let’s take it for a test drive:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来试一试：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Figure 6-2](part0008.html#terminal_output_of_nvidia-smi_highlighti) shows
    the result.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-2](part0008.html#terminal_output_of_nvidia-smi_highlighti)展示了结果。'
- en: '![Terminal output of nvidia-smi highlighting the GPU utilization](../images/00217.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![nvidia-smi的终端输出，突出显示GPU利用率](../images/00217.jpeg)'
- en: Figure 6-2\. Terminal output of `nvidia-smi` highlighting the GPU utilization
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。`nvidia-smi`的终端输出，突出显示GPU利用率
- en: 'While training a network, the key figure we are interested in is the GPU utilization,
    defined in the documentation as the percent of time over the past second during
    which *one or more* kernels was executing on the GPU. Fifty-one percent is frankly
    not that great. But this is utilization at the moment in time when `nvidia-smi`
    is called. How do we continuously monitor these numbers? To better understand
    the GPU usage, we can refresh the utilization metrics every half a second with
    the `watch` command (it’s worth memorizing this command):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络时，我们感兴趣的关键数字是GPU利用率，文档中定义为过去一秒钟内GPU上执行*一个或多个*内核的时间百分比。51%实际上并不那么好。但这是在调用`nvidia-smi`时的瞬间利用率。我们如何持续监控这些数字？为了更好地了解GPU使用情况，我们可以使用`watch`命令每半秒刷新一次利用率指标（值得记住这个命令）：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although GPU utilization is a good proxy for measuring the efficiency of our
    pipeline, it does not alone measure how well we’re using the GPU, because the
    work could still be using a small fraction of the GPU’s resources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPU利用率是衡量我们流水线效率的一个很好的代理，但它本身并不能衡量我们如何充分利用GPU，因为工作仍可能只使用GPU资源的一小部分。
- en: 'Because staring at the terminal screen with the number jumping around is not
    the most optimal way to analyze, we can instead poll the GPU utilization every
    second and dump that into a file. Run this for about 30 seconds while any GPU-related
    process is running on our system and stop it by pressing Ctrl+C:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因为盯着终端屏幕看数字跳动并不是分析的最佳方式，我们可以每秒轮询一次GPU利用率并将其转储到文件中。在我们的系统上运行任何与GPU相关的进程时运行大约30秒，然后通过按Ctrl+C停止它：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, calculate the median GPU utilization from the file generated:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从生成的文件中计算中位数GPU利用率：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Datamash is a handy command-line tool that performs basic numeric, textual,
    and statistical operations on textual data files. You can find instructions to
    install it at [*https://www.gnu.org/software/datamash/*](https://www.gnu.org/software/datamash/).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Datamash是一个方便的命令行工具，可对文本数据文件执行基本的数字、文本和统计操作。您可以在[*https://www.gnu.org/software/datamash/*](https://www.gnu.org/software/datamash/)找到安装说明。
- en: '`nvidia-smi` is the most convenient way to check our GPU utilization on the
    command line. Could we get a deeper analysis? It turns out, for advanced users,
    TensorFlow provides a powerful set of tools.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`nvidia-smi`是在命令行上检查GPU利用率的最便捷方式。我们能否进行更深入的分析？原来，对于高级用户，TensorFlow提供了一套强大的工具。'
- en: TensorFlow Profiler + TensorBoard
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow分析器 + TensorBoard
- en: TensorFlow ships with `tfprof` ([Figure 6-3](part0008.html#profilerapostrophes_timeline_in_tensorbo)),
    the TensorFlow profiler to help analyze and understand the training process at
    a much deeper level, such as generating a detailed model analysis report for each
    operation in our model. But the command line can be a bit daunting to navigate.
    Luckily, TensorBoard, a suite of browser-based visualization tools for TensorFlow,
    includes a plugin for the profiler that lets us interactively debug the network
    with a few mouse clicks. This includes Trace Viewer, a feature that shows events
    in a timeline. It helps investigate how resources are being used precisely at
    a given period of time and spot inefficiencies.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow附带了`tfprof`（[图6-3](part0008.html#profilerapostrophes_timeline_in_tensorbo)），TensorFlow分析器，帮助分析和理解训练过程，例如为模型中的每个操作生成详细的模型分析报告。但是命令行可能有点难以导航。幸运的是，TensorBoard是一个基于浏览器的TensorFlow可视化工具套件，包括一个用于分析器的插件，让我们可以通过几次鼠标点击与网络进行交互式调试。其中包括Trace
    Viewer，一个显示时间轴上事件的功能。它有助于调查资源在特定时间段内的精确使用情况并发现效率低下的地方。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As of this writing, TensorBoard is fully supported only in Google Chrome and
    might not show the profile view in other browsers, like Firefox.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前为止，TensorBoard仅在Google Chrome中得到完全支持，可能不会在其他浏览器（如Firefox）中显示分析视图。
- en: '![Profiler’s timeline in TensorBoard shows an idle GPU while the CPU is processing
    as well as CPU idling while the GPU is processing](../images/00276.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![TensorBoard中分析器的时间轴显示GPU处于空闲状态，而CPU正在处理，以及CPU处于空闲状态而GPU正在处理](../images/00276.jpeg)'
- en: Figure 6-3\. Profiler’s timeline in TensorBoard shows an idle GPU while the
    CPU is processing as well as CPU idling while the GPU is processing
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3。TensorBoard中分析器的时间轴显示GPU处于空闲状态，而CPU正在处理，以及CPU处于空闲状态而GPU正在处理
- en: 'TensorBoard, by default, has the profiler enabled. Activating TensorBoard involves
    a simple callback function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TensorBoard启用了分析器。激活TensorBoard涉及一个简单的回调函数：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When initializing the callback, unless `profile_batch` is explicitly specified,
    it profiles the second batch. Why the second batch? Because the first batch is
    usually slower than the rest due to some initialization overhead.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化回调时，除非显式指定`profile_batch`，否则会对第二批进行分析。为什么是第二批？因为第一批通常比其余批次慢，这是由于一些初始化开销。
- en: Note
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It bears reiterating that profiling using TensorBoard is best suited for power
    users of TensorFlow. If you are just starting out, you are better off using `nvidia-smi`.
    (Although `nvidia-smi` is a far more capable than just providing GPU utilization
    info, which is typically how most practitioners use it.) For users wanting even
    deeper access to their hardware utilization metrics, NVIDIA Nsight is a great
    tool.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 需要重申的是，使用TensorBoard进行分析最适合TensorFlow的高级用户。如果您刚开始使用，最好使用`nvidia-smi`。（尽管`nvidia-smi`不仅提供GPU利用率信息，而且通常是大多数实践者使用的方式。）对于希望更深入了解硬件利用率指标的用户，NVIDIA
    Nsight是一个很好的工具。
- en: Alright. With these tools at our disposal, we know that our program needs some
    tuning and has room for efficiency improvements. We look at those areas one by
    one in the next few sections.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好了。有了这些工具，我们知道我们的程序需要一些调整，并有提高效率的空间。我们将在接下来的几节中逐个查看这些领域。
- en: How to Use This Checklist
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用此检查表
- en: In business, an oft-quoted piece of advice is “You can’t improve what you can’t
    measure.” This applies to deep learning pipelines, as well. Tuning performance
    is like a science experiment. You set up a baseline run, tune a knob, measure
    the effect, and iterate in the direction of improvement. The items on the following
    checklist are our knobs—some are quick and easy, whereas others are more involved.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业中，一个经常引用的建议是“无法衡量的东西无法改进”。这也适用于深度学习流水线。调整性能就像进行科学实验。您设置一个基准运行，调整一个旋钮，测量效果，并朝着改进的方向迭代。以下清单上的项目是我们的旋钮——有些快速简单，而其他一些更复杂。
- en: 'To use this checklist effectively, do the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效使用此清单，请执行以下操作：
- en: Isolate the part of the pipeline that you want to improve.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隔离要改进的流水线部分。
- en: Find a relevant point on the checklist.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在清单上找到相关的要点。
- en: Implement, experiment, and observe if runtime is reduced. If not reduced, ignore
    change.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实施、实验，并观察运行时间是否减少。如果没有减少，则忽略更改。
- en: Repeat steps 1 through 3 until the checklist is exhausted.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1至3，直到清单耗尽。
- en: Some of the improvements might be minute, some more drastic. But the cumulative
    effect of all these changes should hopefully result in faster, more efficient
    execution and best of all, more bang for the buck for your hardware. Let’s look
    at each area of the deep learning pipeline step by step, including data preparation,
    data reading, data augmentation, training, and, finally, inference.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一些改进可能微小，一些可能更为重大。但所有这些变化的累积效应希望能够实现更快、更高效的执行，最重要的是，为您的硬件带来更大的回报。让我们逐步查看深度学习流水线的每个领域，包括数据准备、数据读取、数据增强、训练，最后是推理。
- en: Performance Checklist
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能清单
- en: Data Preparation
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: '[“Store as TFRecords”](part0008.html#7K4PE-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“存储为TFRecords”](part0008.html#7K4PE-13fa565533764549a6f0ab7f11eed62b)'
- en: '[“Reduce Size of Input Data”](part0008.html#reduce_size_of_input_data)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“减少输入数据的大小”](part0008.html#reduce_size_of_input_data)'
- en: '[“Use TensorFlow Datasets”](part0008.html#use_tensorflow_datasets)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用TensorFlow数据集”](part0008.html#use_tensorflow_datasets)'
- en: Data Reading
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据读取
- en: '[“Use tf.data”](part0008.html#use_tfdotdata)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用tf.data”](part0008.html#use_tfdotdata)'
- en: '[“Prefetch Data”](part0008.html#prefetch_data)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“预取数据”](part0008.html#prefetch_data)'
- en: '[“Parallelize CPU Processing”](part0008.html#parallelize_cpu_processing)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“并行化CPU处理”](part0008.html#parallelize_cpu_processing)'
- en: '[“Parallelize I/O and Processing”](part0008.html#parallelize_isoliduso_and_processing)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“并行化I/O和处理”](part0008.html#parallelize_isoliduso_and_processing)'
- en: '[“Enable Nondeterministic Ordering”](part0008.html#enable_nondeterministic_ordering)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“启用非确定性排序”](part0008.html#enable_nondeterministic_ordering)'
- en: '[“Cache Data”](part0008.html#cache_data)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“缓存数据”](part0008.html#cache_data)'
- en: '[“Turn on Experimental Optimizations”](part0008.html#7K5GA-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“打开实验性优化”](part0008.html#7K5GA-13fa565533764549a6f0ab7f11eed62b)'
- en: '[“Autotune Parameter Values”](part0008.html#7K5KJ-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“自动调整参数值”](part0008.html#7K5KJ-13fa565533764549a6f0ab7f11eed62b)'
- en: Data Augmentation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: '[“Use GPU for Augmentation”](part0008.html#use_gpu_for_augmentation)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用GPU进行增强”](part0008.html#use_gpu_for_augmentation)'
- en: Training
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: '[“Use Automatic Mixed Precision”](part0008.html#use_automatic_mixed_precision)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用自动混合精度”](part0008.html#use_automatic_mixed_precision)'
- en: '[“Use Larger Batch Size”](part0008.html#use_larger_batch_size)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用更大的批量大小”](part0008.html#use_larger_batch_size)'
- en: '[“Use Multiples of Eight”](part0008.html#use_multiples_of_eight)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用八的倍数”](part0008.html#use_multiples_of_eight)'
- en: '[“Find the Optimal Learning Rate”](part0008.html#find_the_optimal_learning_rate)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“找到最佳学习率”](part0008.html#find_the_optimal_learning_rate)'
- en: '[“Use tf.function”](part0008.html#use_tfdotfunction)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用tf.function”](part0008.html#use_tfdotfunction)'
- en: '[“Overtrain, and Then Generalize”](part0008.html#overtraincomma_and_then_generalize)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“过度训练，然后泛化”](part0008.html#overtraincomma_and_then_generalize)'
- en: '[“Use progressive sampling”](part0008.html#use_progressive_sampling)'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用渐进采样”](part0008.html#use_progressive_sampling)'
- en: '[“Use progressive augmentation”](part0008.html#use_progressive_augmentation)'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用渐进增强”](part0008.html#use_progressive_augmentation)'
- en: '[“Use progressive resizing”](part0008.html#use_progressive_resizing)'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用渐进调整大小”](part0008.html#use_progressive_resizing)'
- en: '[“Install an Optimized Stack for the Hardware”](part0008.html#install_an_optimized_stack_for_the_hardw)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“为硬件安装优化堆栈”](part0008.html#install_an_optimized_stack_for_the_hardw)'
- en: '[“Optimize the Number of Parallel CPU Threads”](part0008.html#optimize_the_number_of_parallel_cpu_thre)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“优化并行CPU线程数量”](part0008.html#optimize_the_number_of_parallel_cpu_thre)'
- en: '[“Use Better Hardware”](part0008.html#use_better_hardware)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用更好的硬件”](part0008.html#use_better_hardware)'
- en: '[“Distribute Training”](part0008.html#distribute_training)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“分布式训练”](part0008.html#distribute_training)'
- en: '[“Examine Industry Benchmarks”](part0008.html#7K6CP-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“检查行业基准”](part0008.html#7K6CP-13fa565533764549a6f0ab7f11eed62b)'
- en: Inference
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: '[“Use an Efficient Model”](part0008.html#use_an_efficient_model)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用高效模型”](part0008.html#use_an_efficient_model)'
- en: '[“Quantize the Model”](part0008.html#7K6EM-13fa565533764549a6f0ab7f11eed62b)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“量化模型”](part0008.html#7K6EM-13fa565533764549a6f0ab7f11eed62b)'
- en: '[“Prune the Model”](part0008.html#prune_the_model)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“修剪模型”](part0008.html#prune_the_model)'
- en: '[“Use Fused Operations”](part0008.html#use_fused_operations)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“使用融合操作”](part0008.html#use_fused_operations)'
- en: '[“Enable GPU Persistence”](part0008.html#enable_gpu_persistence)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“启用GPU持久性”](part0008.html#enable_gpu_persistence)'
- en: Note
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A printable version of this checklist is available at [http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai).
    Feel free to use it as a reference next time you train or deploy a model. Or even
    better, spread the cheer by sharing with your friends, colleagues, and more importantly,
    your manager.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此清单的可打印版本可在[http://PracticalDeepLearning.ai](http://PracticalDeepLearning.ai)上找到。下次训练或部署模型时，可以将其用作参考。或者更好的是，通过与朋友、同事以及更重要的是您的经理分享，传播快乐。
- en: Data Preparation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: There are a few optimizations that we can make even before we do any kind of
    training, and they have to do with how we prepare our data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行任何训练之前，我们可以进行一些优化，这些优化与我们如何准备数据有关。
- en: Store as TFRecords
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储为TFRecords
- en: Image datasets typically consist of thousands of tiny files, each file measuring
    a few kilobytes. And our training pipeline must read each file individually. Doing
    this thousands of times has significant overhead, causing a slowdown of the training
    process. That problem is even more severe in the case of spinning hard drives,
    for which the magnetic head needs to seek to the beginning of each file. This
    problem is further exacerbated when the files are stored on a remote storage service
    like the cloud. And there lies our first hurdle!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据集通常由成千上万个小文件组成，每个文件大小几千字节。我们的训练管道必须逐个读取每个文件。这样做成千上万次会产生显著的开销，导致训练过程变慢。在使用旋转硬盘时，这个问题更加严重，因为磁头需要寻找每个文件的开头。当文件存储在像云这样的远程存储服务上时，这个问题会进一步恶化。这就是我们的第一个障碍所在！
- en: 'To speed up the reads, one idea is to combine thousands of files into a handful
    of larger files. And that’s exactly what TFRecord does. It stores data in efficient
    Protocol Buffer (protobuf) objects, making them quicker to read. Let’s see how
    to create TFRecord files:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快读取速度，一个想法是将成千上万个文件合并成少数几个较大的文件。这正是TFRecord所做的。它将数据存储在高效的Protocol Buffer（protobuf）对象中，使其更快速读取。让我们看看如何创建TFRecord文件：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s take a look at reading these TFRecord files:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何读取这些TFRecord文件：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So, why not join all of the data in a single file, like say for ImageNet? Although
    reading thousands of tiny files harms performance due to the overhead involved,
    reading gigantic files is an equally bad idea. They reduce our ability to make
    parallel reads and parallel network calls. The sweet spot to shard (divide) a
    large dataset in TFRecord files lies at around 100 MB.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不将所有数据合并到一个文件中，比如说ImageNet？尽管读取成千上万个小文件会因涉及的开销而影响性能，但读取巨大文件同样不是一个好主意。它们降低了我们进行并行读取和并行网络调用的能力。将大型数据集分片（划分）为TFRecord文件的甜蜜点在大约100
    MB左右。
- en: Reduce Size of Input Data
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减小输入数据的大小
- en: 'Image datasets with large images need to be resized before passing through
    to the GPU. This means the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 具有大图像的图像数据集在传递到GPU之前需要调整大小。这意味着以下内容：
- en: Repeated CPU cycles at every iteration
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次迭代中重复使用CPU周期
- en: Repeated I/O bandwidth being consumed at a larger rate than needed in our data
    pipeline
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的数据管道中消耗的I/O带宽以比所需更大的速率被重复使用
- en: One good strategy to save compute cycles is to perform common preprocessing
    steps once on the entire dataset (like resizing) and then saving the results in
    TFRecord files for all future runs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 节省计算周期的一个好策略是在整个数据集上执行常见的预处理步骤一次（如调整大小），然后将结果保存在TFRecord文件中，以供所有未来运行使用。
- en: Use TensorFlow Datasets
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow数据集
- en: For commonly used public datasets, from MNIST (11 MB) to CIFAR-100 (160 MB)
    all the way to MS COCO (38 GB) and Google Open Images (565 GB), it’s quite an
    effort to download the data (often spread across multiple zipped files). Imagine
    your frustration if after downloading 95% of the file slowly, the connection becomes
    spotty and breaks. This is not unusual because these files are typically hosted
    on university servers, or are downloaded from various sources like Flickr (as
    is the case with ImageNet 2012, which gives us the URLs from which to download
    150 GB-plus of images). A broken connection might mean having to start all over
    again.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于常用的公共数据集，从MNIST（11 MB）到CIFAR-100（160 MB），再到MS COCO（38 GB）和Google Open Images（565
    GB），下载数据是相当费力的（通常分布在多个压缩文件中）。想象一下，如果在慢慢下载文件的过程中，下载了95%，然后连接变得不稳定并中断，你会感到多么沮丧。这并不罕见，因为这些文件通常托管在大学服务器上，或者从各种来源（如Flickr）下载（就像ImageNet
    2012一样，它提供了下载150 GB以上图像的URL）。连接中断可能意味着需要重新开始。
- en: If you think that was tedious, the real challenge actually begins only after
    you successfully download the data. For every new dataset, we now need to hunt
    through the documentation to determine how the data is formatted and organized,
    so we can begin reading and processing appropriately. Then, we need to split the
    data into training, validation, and test sets (preferably converting to TFRecords).
    And when the data is so large as to not fit in memory, we will need to do some
    manual jiu-jitsu to read it and feed it efficiently to the training pipeline.
    We never said it was easy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为这很繁琐，那么真正的挑战实际上只有在成功下载数据之后才开始。对于每个新数据集，我们现在需要查阅文档，确定数据的格式和组织方式，以便适当地开始读取和处理。然后，我们需要将数据分割为训练、验证和测试集（最好转换为TFRecords）。当数据太大而无法放入内存时，我们需要做一些手动操作来高效地读取并将其有效地提供给训练管道。我们从未说过这很容易。
- en: Alternately, we could skip all the pain by consuming the high-performance, ready-to-use
    TensorFlow Datasets package. With several famous datasets available, it downloads,
    splits, and feeds our training pipeline using best practices in a few lines.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过使用高性能、即用即用的TensorFlow数据集包来避免所有痛苦。有几个著名的数据集可用，它会下载、拆分并使用最佳实践来喂养我们的训练管道，只需几行代码。
- en: Let’s look at which datasets are available.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看有哪些数据集可用。
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There are more than 100 datasets as of this writing, and that number is steadily
    increasing. Now, let’s download, extract, and make an efficient pipeline using
    the training set of CIFAR-10:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，已有100多个数据集，这个数字还在稳步增长。现在，让我们下载、提取并使用CIFAR-10的训练集创建一个高效的管道：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That’s it! The first time we execute the code, it will download and cache the
    dataset on our machine. For every future run, it will skip the network download
    and directly read from the cache.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！第一次执行代码时，它将在我们的机器上下载并缓存数据集。对于以后的每次运行，它将跳过网络下载，直接从缓存中读取。
- en: Data Reading
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据读取
- en: Now that the data is prepared, let’s look for opportunities to maximize the
    throughput of the data reading pipeline.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据准备好了，让我们寻找最大化数据读取管道吞吐量的机会。
- en: Use tf.data
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用tf.data
- en: We could choose to manually read every file from our dataset with Python’s built-in
    I/O library. We could simply call `open` for each file and we’d be good to go,
    right? The main downside in this approach is that our GPU would be bottlenecked
    by our file reads. Every time we read a file, the GPU needs to wait. Every time
    the GPU starts processing its input, we wait before we read the next file from
    disk. Seems rather wasteful, doesn’t it?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择使用Python的内置I/O库手动读取数据集中的每个文件。我们只需为每个文件调用`open`，然后就可以开始了，对吧？这种方法的主要缺点是我们的GPU将受到文件读取的限制。每次读取一个文件时，GPU都需要等待。每次GPU开始处理其输入时，我们都需要等待下一个文件从磁盘中读取。看起来相当浪费，不是吗？
- en: 'If there’s only one thing you can take away from this chapter, let it be this:
    `tf.data` is the way to go for building a high-performance training pipeline.
    In the next few sections, we explore several aspects of `tf.data` that you can
    exploit to improve training speed.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只能从本章中学到一件事，那就是：`tf.data`是构建高性能训练管道的方法。在接下来的几节中，我们将探讨几个可以利用来提高训练速度的`tf.data`方面。
- en: 'Let’s set up a base pipeline for reading data:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为读取数据设置一个基本管道：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Prefetch Data
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预取数据
- en: In the pipeline we discussed earlier, the GPU waits for the CPU to generate
    data, and then the CPU waits for the GPU to finish computation before generating
    data for the next cycle. This circular dependency causes idle time for both CPU
    and GPU, which is inefficient.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前讨论的管道中，GPU等待CPU生成数据，然后CPU等待GPU完成计算，然后再生成下一个周期的数据。这种循环依赖会导致CPU和GPU的空闲时间，这是低效的。
- en: The `prefetch` function helps us here by delinking the production of the data
    (by the CPU) from the consumption of the data (by the GPU). Using a background
    thread, it allows data to be passed *asynchronously* into an intermediate buffer,
    where it is readily available for a GPU to consume. The CPU now carries on with
    the next computation instead of waiting for the GPU. Similarly, as soon as the
    GPU is finished with its previous computation, and there’s data readily available
    in the buffer, it starts processing.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`prefetch`函数通过将数据的生成（由CPU）与数据的消耗（由GPU）分离，帮助我们。使用一个后台线程，它允许数据被*异步*传递到一个中间缓冲区，其中数据可以立即供GPU消耗。CPU现在继续进行下一个计算，而不是等待GPU。同样，一旦GPU完成了其先前的计算，并且缓冲区中有数据可用，它就开始处理。'
- en: 'To use it, we can simply call `prefetch` on our dataset at the very end of
    our pipeline along with a `buffer_size` parameter (which is the maximum amount
    of data that can be stored). Usually `buffer_size` is a small number; `1` is good
    enough in many cases:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，我们只需在管道的最后调用`prefetch`，并附加一个`buffer_size`参数（即可以存储的最大数据量）。通常`buffer_size`是一个小数字；在许多情况下，`1`就足够了：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In just a few pages, we show you how to find an optimal value for this parameter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在短短几页中，我们将向您展示如何找到这个参数的最佳值。
- en: In summary, if there’s an opportunity to overlap CPU and GPU computations, `prefetch`
    will automatically exploit it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果有机会重叠CPU和GPU计算，`prefetch`将自动利用它。
- en: Parallelize CPU Processing
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化CPU处理
- en: 'It would be a waste to have a CPU with multiple cores but doing all of our
    processing on only one of them. Why not take advantage of the rest? This is exactly
    where the `num_parallel_calls` argument in the `map` function comes in handy:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有多个核心的CPU，但只使用其中一个核心进行所有处理，那将是一种浪费。为什么不利用其他核心呢？这正是`map`函数中的`num_parallel_calls`参数派上用场的地方：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This starts multiple threads to parallelize processing of the `map()` function.
    Assuming that there is no heavy application running in the background, we will
    want to set `num_parallel_calls` to the number of CPU cores on our system. Anything
    more will potentially degrade the performance due to the overhead of context switching.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动多个线程来并行处理`map()`函数。假设后台没有运行重型应用程序，我们将希望将`num_parallel_calls`设置为系统上的CPU核心数。任何更多的设置可能会由于上下文切换的开销而降低性能。
- en: Parallelize I/O and Processing
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化I/O和处理
- en: Reading files from disk or worse, over a network, is a huge cause of bottlenecks.
    We might possess the best CPU and GPU in the world, but if we don’t optimize our
    file reads, it would all be for naught. One solution that addresses this problem
    is to parallelize both I/O and subsequent processing (also known as *interleaving*).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从磁盘或更糟的是从网络中读取文件是瓶颈的主要原因。我们可能拥有世界上最好的CPU和GPU，但如果我们不优化文件读取，那一切都将是徒劳的。解决这个问题的一个方法是并行化I/O和后续处理（也称为*交错处理）。
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this command, two things are happening:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，发生了两件事：
- en: The input data is acquired in parallel (by default equal to the number of cores
    on the system).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据是并行获取的（默认情况下等于系统上的核心数）。
- en: On the acquired data, setting the `num_parallel_calls` parameter allows the
    `map_func` function to execute on multiple parallel threads and read from the
    incoming data asynchronously.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在获取的数据上，设置`num_parallel_calls`参数允许`map_func`函数在多个并行线程上执行，并异步从传入的数据中读取。
- en: If `num_parallel_calls` was not specified, even if the data were read in parallel,
    `map_func` would run synchronously on a single thread. As long as `map_func` runs
    faster than the rate at which the input data is coming in, there will not be a
    problem. We definitely want to set `num_parallel_calls` higher if `map_func` becomes
    a bottleneck.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有指定`num_parallel_calls`，即使数据是并行读取的，`map_func`也会在单个线程上同步运行。只要`map_func`的运行速度快于输入数据到达的速度，就不会有问题。如果`map_func`成为瓶颈，我们肯定希望将`num_parallel_calls`设置得更高。
- en: Enable Nondeterministic Ordering
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用非确定性排序
- en: 'For many datasets, the reading order is not important. After all, we might
    be randomizing their ordering anyway. By default, when reading files in parallel,
    `tf.data` still attempts to produce their outputs in a *fixed round-robin order*.
    The disadvantage is that we might encounter a “straggler” along the way (i.e.,
    an operation that takes a lot longer than others, such as a slow file read, and
    holds up all other operations). It’s like a grocery store line where the person
    in front of us insists on using cash with the exact change, whereas everyone else
    uses a credit card. So instead of blocking all the subsequent operations that
    are ready to give output, we skip over the stragglers until they are done with
    their processing. This breaks the ordering while reducing wasted cycles waiting
    for the handful of slower operations:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多数据集，读取顺序并不重要。毕竟，我们可能会随机排列它们的顺序。默认情况下，在并行读取文件时，`tf.data`仍然尝试以*固定的轮流顺序*产生它们的输出。缺点是我们可能会在途中遇到“拖延者”（即，一个操作比其他操作花费更长时间，例如慢速文件读取，并阻止所有其他操作）。这就像在杂货店排队时，我们前面的人坚持使用现金并找零，而其他人都使用信用卡。因此，我们跳过拖延者，直到他们完成处理，而不是阻塞所有准备输出的后续操作。这打破了顺序，同时减少了等待少数较慢操作的浪费周期：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Cache Data
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存数据
- en: 'The `Dataset.cache()` function allows us to make a copy of data either in memory
    or as a file on disk. There are two reasons why you might want to cache a dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset.cache()`函数允许我们将数据复制到内存或磁盘文件中。有两个原因可能需要缓存数据集：'
- en: To avoid repeatedly reading from disk after the first epoch. This is obviously
    effective only when the cache is in memory and can fit in the available RAM.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免在第一个epoch之后重复从磁盘读取。这显然只在缓存在内存中且可以适应可用RAM时有效。
- en: To avoid having to repeatedly perform expensive CPU operations on data (e.g.,
    resizing large images to a smaller size).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免重复执行昂贵的CPU操作（例如，将大图像调整为较小尺寸）。
- en: Tip
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Cache is best used for data that is not going to change. It is recommended to
    place `cache()` before any random augmentations and shuffling; otherwise, caching
    at the end will result in exactly the same data and order in every run.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存最适用于不会更改的数据。建议在任何随机增强和洗牌之前放置`cache()`；否则，在最后进行缓存将导致每次运行中数据和顺序完全相同。
- en: 'Depending on our scenario, we can use one of the two following lines:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的情况，我们可以使用以下两行中的一行：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It’s worth noting that in-memory cache is volatile and hence only shows performance
    improvements in the second epoch of every run. On the other hand, file-based cache
    will make every run faster (beyond the very first epoch of the first run).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，内存中的缓存是易失性的，因此只在每次运行的第二个epoch中显示性能改进。另一方面，基于文件的缓存将使每次运行更快（超出第一次运行的第一个epoch）。
- en: Tip
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In the [“Reduce Size of Input Data”](part0008.html#reduce_size_of_input_data),
    we mentioned preprocessing the data and saving it as TFRecord files as input to
    future data pipelines. Using the **`cache()`** function directly after the preprocessing
    step in your pipeline would give a similar performance with a single word change
    in code.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“减少输入数据的大小”](part0008.html#reduce_size_of_input_data)中，我们提到预处理数据并将其保存为TFRecord文件作为未来数据流水线的输入。在流水线中的预处理步骤之后直接使用**`cache()`**函数，只需在代码中进行一个单词的更改，就可以获得类似的性能。
- en: Turn on Experimental Optimizations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打开实验性优化
- en: TensorFlow has many built-in optimizations, often initially experimental and
    turned off by default. Depending on your use case, you might want to turn on some
    of them to squeeze out just a little more performance from your pipeline. Many
    of these optimizations are detailed in the documentation for `tf.data.experimental.OptimizationOptions`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow有许多内置的优化，通常最初是实验性的，并默认关闭。根据您的用例，您可能希望打开其中一些以从流水线中挤出更多性能。这些优化中的许多细节在`tf.data.experimental.OptimizationOptions`的文档中有详细说明。
- en: Note
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Here’s a quick refresher on filter and map operations:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关于过滤和映射操作的快速复习：
- en: Filter
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤
- en: A filter operation goes through a list element by element and grabs those that
    match a given condition. The condition is supplied as a lambda operation that
    returns a boolean value.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤操作逐个元素遍历列表，并抓取符合给定条件的元素。条件以lambda操作的形式提供，返回布尔值。
- en: Map
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 映射
- en: A map operation simply takes in an element, performs a computation, and returns
    an output. For example, resizing an image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 映射操作只是接受一个元素，执行计算，并返回一个输出。例如，调整图像大小。
- en: Let’s look at a few experimental optimizations that are available to us, including
    examples of two consecutive operations that could benefit from being fused together
    as one single operation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些可用的实验性优化，包括两个连续操作的示例，这些操作合并在一起作为一个单一操作可能会受益。
- en: Filter fusion
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤融合
- en: 'Sometimes, we might want to filter based on multiple attributes. Maybe we want
    to use only images that have both a dog and a cat. Or, in a census dataset, only
    look at families above a certain income threshold who also live within a certain
    distance to the city center. `filter_fusion` can help speed up such scenarios.
    Consider the following example:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能想根据多个属性进行过滤。也许我们只想使用同时有狗和猫的图像。或者在人口普查数据集中，只查看收入超过一定门槛且距离市中心一定距离的家庭。`filter_fusion`可以帮助加快这种情况的速度。考虑以下示例：
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first filter performs a full pass over the entire dataset and returns elements
    that are less than 1,000\. On this output, the second filter does another pass
    to further remove elements not divisible by three. Instead of doing two passes
    over many of the same elements, we could instead combine both the filter operations
    into one pass using an `AND` operation. That is precisely what the `filter_fusion`
    option enables—combining multiple filter operations into one pass. By default,
    it is turned off. You can enable it by using the following statement:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个过滤器对整个数据集执行完整传递，并返回小于1,000的元素。在此输出上，第二个过滤器执行另一个传递以进一步删除不能被三整除的元素。我们可以将两个过滤操作合并为一个传递，而不是对许多相同元素执行两次传递，方法是使用`AND`操作。这正是`filter_fusion`选项所能实现的——将多个过滤操作合并为一个传递。默认情况下，它是关闭的。您可以使用以下语句启用它：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Map and filter fusion
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射和过滤融合
- en: 'Consider the following example:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this example, the `map` function does a full pass on the entire dataset
    to calculate the square of every element. Then, the `filter` function discards
    the odd elements. Rather than doing two passes (more so in this particularly wasteful
    example), we could simply fuse the map and filter operations together by turning
    on the `map_and_filter_fusion` option so that they operate as a single unit:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，`map`函数对整个数据集执行完整传递，计算每个元素的平方。然后，`filter`函数丢弃奇数元素。与执行两次传递（尤其是在这个特别浪费的示例中）相比，我们可以通过打开`map_and_filter_fusion`选项将map和filter操作合并在一起，使它们作为一个单元操作：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Map fusion
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射融合
- en: 'Similar to the aforementioned two examples, fusing two or more map operations
    prevents multiple passes from being performed on the same data and instead combines
    them in a single pass:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面两个示例类似，合并两个或多个映射操作可以防止对相同数据执行多次传递，而是将它们合并为单次传递：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Autotune Parameter Values
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动调整参数值
- en: You might have noticed that many of the code examples in this section have hardcoded
    values for some of the parameters. For the combination of the problem and hardware
    at hand, you can tune them for maximum efficiency. How to tune them? One obvious
    way is to manually tweak the parameters one at a time and isolate and observe
    the impact of each of them on the overall performance until we get the precise
    parameter set. But the number of knobs to tune quickly gets out of hand due to
    the combinatorial explosion. If this wasn’t enough, our finely tuned script wouldn’t
    necessarily be as efficient on another machine due to differences in hardware
    such as the number of CPU cores, GPU availability, and so on. And even on the
    same system, depending on resource usage by other programs, these knobs might
    need to be adjusted over different runs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，本节中的许多代码示例对一些参数具有硬编码值。针对手头问题和硬件的组合，您可以调整它们以实现最大效率。如何调整它们？一个明显的方法是逐个手动调整参数并隔离并观察每个参数对整体性能的影响，直到我们获得精确的参数集。但由于组合爆炸，要调整的旋钮数量很快就会失控。如果这还不够，我们精心调整的脚本在另一台机器上不一定会像在另一台机器上那样高效，因为硬件的差异，如CPU核心数量、GPU可用性等。甚至在同一系统上，根据其他程序的资源使用情况，这些旋钮可能需要在不同运行中进行调整。
- en: 'How do we solve this? We do the opposite of manual tuning: autotuning. Using
    hill-climbing optimization algorithms (which are a type of heuristic-driven search
    algorithms), this option automatically finds the ideal parameter combination for
    many of the `tf.data` function parameters. Simply use `tf.data.experimental.AUTOTUNE`
    instead of manually assigning numbers. It’s the one parameter to rule them all.
    Consider the following example:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个问题？我们做与手动调整相反的事情：自动调整。使用爬山优化算法（这是一种启发式搜索算法），此选项会自动找到许多`tf.data`函数参数的理想参数组合。只需使用`tf.data.experimental.AUTOTUNE`而不是手动分配数字。这是一个参数来统治它们所有。考虑以下示例：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Isn’t that an elegant solution? We can do that for several other function calls
    in the `tf.data` pipeline. The following is an example of combining together several
    optimizations from the section “Data Reading” to make a high-performance data
    pipeline:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个优雅的解决方案吗？我们可以对`tf.data`管道中的几个其他函数调用执行相同的操作。以下是一个示例，结合了“数据读取”部分中的几个优化，以创建高性能数据管道：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Data Augmentation
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Sometimes, we might not have sufficient data to run our training pipeline. Even
    if we did, we might still want to manipulate the images to improve the robustness
    of our model—with the help of data augmentation. Let’s see whether we can make
    this step any faster.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能没有足够的数据来运行我们的训练管道。即使有，我们可能仍然希望操纵图像以提高模型的鲁棒性—借助数据增强。让我们看看是否可以使这一步更快。
- en: Use GPU for Augmentation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GPU进行增强
- en: Data preprocessing pipelines can be elaborate enough that you could write an
    entire book about them. Image transformation operations such as resizing, cropping,
    color transformations, blurring, and so on are commonly performed on the data
    immediately after it’s read from disk into memory. Given that these are all matrix
    transformation operations, they might do well on a GPU.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理管道可能足够复杂，以至于你可以写一本关于它们的整本书。图像转换操作，如调整大小、裁剪、颜色转换、模糊等通常在数据从磁盘读入内存后立即执行。鉴于这些都是矩阵转换操作，它们可能在GPU上表现良好。
- en: OpenCV, Pillow, and the built-in Keras augmentation functionality are the most
    commonly used libraries in computer vision for working on images. There’s one
    major limitation here, though. Their image processing is primarily CPU based (although
    you can compile OpenCV to work with CUDA), which means that the pipeline might
    not be fully utilizing the underlying hardware to its true potential.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV、Pillow和内置的Keras增强功能是计算机视觉中最常用的用于处理图像的库。然而，这里有一个主要限制。它们的图像处理主要基于CPU（尽管你可以编译OpenCV以与CUDA一起使用），这意味着管道可能没有充分利用底层硬件的真正潜力。
- en: Note
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As of August 2019, there are efforts underway to convert Keras image augmentation
    to be GPU accelerated, as well.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2019年8月，正在努力将Keras图像增强转换为GPU加速。
- en: There are a few different GPU-bound options that we can explore.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: tf.image built-in augmentations
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tf.image` provides some handy augmentation functions that we can seamlessly
    plug into a `tf.data` pipeline. Some of the methods include image flipping, color
    augmentations (hue, saturation, brightness, contrast), zooming, and rotation.
    Consider the following example, which changes the hue of an image:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The downside to relying on `tf.image` is that the functionality is much more
    limited compared to OpenCV, Pillow, and even Keras. For example, the built-in
    function for image rotation in `tf.image` only supports rotating images by 90
    degrees counter-clockwise. If we need to be able to rotate by an arbitrary amount,
    such as 10 degrees, we’d need to manually build that functionality. Keras, on
    the other hand, provides that functionality out of the box.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: As another alternative to the `tf.data` pipeline, the NVIDIA Data Loading Library
    (DALI) offers a fast data loading and preprocessing pipeline accelerated by GPU
    processing. As shown in [Figure 6-4](part0008.html#the_nvidia_dali_pipeline),
    DALI implements several common steps including resizing an image and augmenting
    an image in the GPU, immediately before the training. DALI works with multiple
    deep learning frameworks including TensorFlow, PyTorch, MXNet, and others, offering
    portability of the preprocessing pipelines.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA DALI
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![The NVIDIA DALI pipeline](../images/00256.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. The NVIDIA DALI pipeline
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Additionally, even JPEG decoding (a relatively heavy task) can partially make
    use of the GPU, giving it an additional boost. This is done using nvJPEG, a GPU-accelerated
    library for JPEG decoding. For multi-GPU tasks, this scales near linearly as the
    number of GPUs increases.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIAs efforts culminated in a record-breaking MLPerf entry (which benchmarks
    machine learning hardware, software, and services), training a ResNet-50 model
    in 80 seconds.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Training
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For those beginning their performance optimization journey, the quickest wins
    come from improving the data pipelines, which is relatively easy. For a training
    pipeline that is already being fed data fast, let’s investigate optimizations
    for our actual training step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Use Automatic Mixed Precision
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “*One line to make your training two to three times faster!*”
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Weights in deep learning models are typically stored in single-precision; that
    is, 32-bit floating point, or as it’s more commonly referenced: FP32\. Putting
    these models in memory-constrained devices such as mobile phones can be challenging
    to accommodate. A simple trick to make models smaller is to convert them from
    single-precision (FP32) to half-precision (FP16). Sure, the representative power
    of these weights goes down, but as we demonstrate later in this chapter ([“Quantize
    the Model”](part0008.html#7K6EM-13fa565533764549a6f0ab7f11eed62b)), neural networks
    are resilient to small changes, much like they are resilient to noise in images.
    Hence, we get the benefits of a more efficient model without sacrificing much
    accuracy. In fact, we can even reduce the representation to 8-bit integers (INT8)
    without a significant loss in accuracy, as we will see in some upcoming chapters.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: So, if we can use reduced-precision representation during inference, could we
    do the same during training, as well? Going from 32-bit to 16-bit representation
    would effectively mean double the memory bandwidth available, double the model
    size, or double the batch size can be accommodated. Unfortunately, it turns out
    that using FP16 naïvely *during training* can potentially lead to a significant
    loss in model accuracy and might not even converge to an optimal solution. This
    happens because of FP16’s limited range for representing numbers. Due to a lack
    of adequate precision, any updates to the model during training, if sufficiently
    small, will cause an update to not even register. Imagine adding 0.00006 to a
    weight value of 1.1\. With FP32, the weight would be correctly updated to 1.10006\.
    With FP16, however, the weight would remain 1.1\. Conversely, any activations
    from layers such as Rectified Linear Unit (ReLU) could be high enough for FP16
    to overflow and hit infinity (`NaN` in Python).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The easy answer to these challenges is to use automatic mixed-precision training.
    In this method, we store the model in FP32 as a master copy and perform the forward/backward
    passes of training in FP16\. After each training step is performed, the final
    update from that step is then scaled back up to FP32 before it is applied to the
    master copy. This helps avoid the pitfalls of FP16 arithmetic and results in a
    lower memory footprint, and faster training (experiments have shown increases
    in speed by two to three times), while achieving similar accuracy levels as training
    solely in FP32\. It is noteworthy that newer GPU architectures like the NVIDIA
    Volta and Turing especially optimize FP16 operations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable mixed precision during training, we simply need to add the following
    line to the beginning of our Python script:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Use Larger Batch Size
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of using the entire dataset for training in one batch, we train with
    several minibatches of data. This is done for two reasons:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Our full data (single batch) might not fit in the GPU RAM.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can achieve similar training accuracy by feeding many smaller batches, just
    as you would by feeding fewer larger batches.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having smaller minibatches might not fully utilize the available GPU memory,
    so it’s vital to experiment with this parameter, see its effect on the GPU utilization
    (using the `nvidia-smi` command), and choose the batch size that maximizes the
    utilization. Consumer GPUs like the NVIDIA 2080 Ti ship with 11 GB of GPU memory,
    which is plenty for efficient models like MobileNet family.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: For example on hardware with the 2080 Ti graphics card, using 224 x 224 resolution
    images and MobileNetV2 model, the GPU can accommodate a batch size up to 864\.
    [Figure 6-5](part0008.html#effect_of_varying_batch_size_on_time_per) shows the
    effect of varying batch sizes from 4 to 864, on both the GPU utilization (solid
    line) as well as the time per epoch (dashed line). As we can see in the figure,
    the higher the batch size, the higher the GPU utilization, leading to a shorter
    training time per epoch.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Even at our max batch size of 864 (before running out of memory allocation),
    the GPU utilization does not cross 85%. This means that the GPU was fast enough
    to handle the computations of our otherwise very efficient data pipeline. Replacing
    MobileNetV2 with a heavier ResNet-50 model immediately increased GPU to 95%.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of varying batch size on time per epoch (seconds) as well as on percentage
    GPU utilization (Log scales have been used for both X- and Y-axes.)](../images/00065.jpeg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Effect of varying batch size on time per epoch (seconds) as well
    as on percentage GPU utilization (Log scales have been used for both X- and Y-axes.)
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even though we showcased batch sizes up to a few hundreds, large industrial
    training loads distributed across multiple nodes often use much larger batch sizes
    with the help of a technique called Layer-wise Adaptive Rate Scaling (LARS). For
    example, Fujitsu Research trained a ResNet-50 network to 75% Top-1 accuracy on
    ImageNet in a mere 75 seconds. Their ammunition? 2048 Tesla V100 GPUs and a whopping
    batch size of 81,920!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Use Multiples of Eight
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the computations in deep learning are in the form of “matrix multiply
    and add.” Although it’s an expensive operation, specialized hardware has increasingly
    been built in the past few years to optimize for its performance. Examples include
    Google’s TPUs and NVIDIA’s Tensor Cores (which can be found in the Turing and
    Volta architectures). Turing GPUs provide both Tensor Cores (for FP16 and INT8
    operations) as well as CUDA cores (for FP32 operations), with the Tensor Cores
    delivering significantly higher throughput. Due to their specialized nature, Tensor
    Cores require that certain parameters within the data supplied to them be divisible
    by eight. Here are just three such parameters:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The number of channels in a convolutional filter
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of neurons in a fully connected layer and the inputs to this layer
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of minibatches
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these parameters are not divisible by eight, the GPU CUDA cores will be used
    as the fallback accelerator instead. In an [experiment](https://oreil.ly/KoEkM)
    reported by NVIDIA, simply changing the batch size from 4,095 to 4,096 resulted
    in an increase in throughput of five times. Keep in mind that using multiples
    of eight (or 16 in the case of INT8 operations), in addition to using automatic
    mixed precision, is the bare minimum requirement to activate the Tensor Cores.
    For higher efficiency, the recommended values are in fact multiples of 64 or 256\.
    Similarly, Google recommends multiples of 128 when using TPUs for maximum efficiency.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Find the Optimal Learning Rate
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One hyperparameter that greatly affects our speed of convergence (and accuracy)
    is the learning rate. The ideal result of training is the global minimum; that
    is, the point of least loss. Too high a learning rate can cause our model to overshoot
    the global minimum (like a wildly swinging pendulum) and potentially never converge.
    Too low a learning rate can cause convergence to take too long because the learning
    algorithm will take very small steps toward the minimum. Finding the right initial
    learning rate can make a world of difference.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The naive way to find the ideal initial learning rate is to try a few different
    learning rates (such as 0.00001, 0.0001, 0.001, 0.01, 0.1) and find one that starts
    converging quicker than others. Or, even better, perform grid search over a range
    of values. This approach has two problems: 1) depending on the granularity, it
    might find a decent value, but it might not be the most optimal value; and 2)
    we need to train multiple times, which can be time consuming.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'In Leslie N. Smith’s 2015 paper, “Cyclical Learning Rates for Training Neural
    Networks,” he describes a much better strategy to find this optimal learning rate.
    In summary:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Start with a really low learning rate and gradually increase it until reaching
    a prespecified maximum value.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each learning rate, observe the loss—first it will be stagnant, then it will
    begin going down and then eventually go back up.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the rate of decrease of loss (first derivative) at each learning rate.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the point with the highest rate of decrease of loss.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It sounds like a lot of steps, but thankfully we don’t need to write code for
    it. The [keras_lr_finder](https://oreil.ly/il_BI) library by Pavel Surmenok gives
    us a handy function to find it:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Figure 6-6](part0008.html#a_graph_showing_the_change_in_loss_as_th) shows
    the plot of loss versus learning rate. It becomes evident that a learning rate
    of 10^(–4) or 10^(–3) might be too low (owing to barely any drop in loss), and
    similarly, above 1 might be too high (because of the rapid increase in loss).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing the change in loss as the learning rate is increased](../images/00175.jpeg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. A graph showing the change in loss as the learning rate is increased
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'What we are most interested in is the point of the greatest decrease in loss.
    After all, we want to minimize the time we spend in getting to the least loss
    during training. In [Figure 6-7](part0008.html#a_graph_showing_the_rate_of_change_in_lo),
    we plot the *rate of change* of loss—the derivative of the loss with regard to
    the learning rate:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![A graph showing the rate of change in loss as the learning rate is increased](../images/00314.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. A graph showing the rate of change in loss as the learning rate
    is increased
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These figures show that values around 0.1 would lead to the fastest decrease
    in loss, and hence we would choose it as our optimal learning rate.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Use tf.function
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Eager execution mode, which is turned on by default in TensorFlow 2.0, allows
    users to execute code line by line and immediately see the results. This is immensely
    helpful in development and debugging. This is in contrast to TensorFlow 1.x, for
    which the user had to build all operations as a graph and then execute them in
    one go to see the results. This made debugging a nightmare!
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Does the added flexibility from eager execution come at a cost? Yes, a tiny
    one, typically in the order of microseconds, which can essentially be ignored
    for large compute-intensive operations, like training ResNet-50\. But where there
    are many small operations, eager execution can have a sizable impact.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'We can overcome this by two approaches:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Disabling eager execution
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: For TensorFlow 1.x, not enabling eager execution will let the system optimize
    the program flow as a graph and run it faster.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Use `tf.function`
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow 2.x, you cannot disable eager execution (there is a compatibility
    API, but we shouldn’t be using that for anything other than migration from TensorFlow
    1.x). Instead, any function that could benefit from a speedup by executing in
    graph mode can simply be annotated with `@tf.function`. It’s worth noting that
    any function that is called within an annotated function will also run in graph
    mode. This gives us the advantage of speedup from graph-based execution without
    sacrificing the debugging capabilities of eager execution. Typically, the best
    speedup is observed on short computationally intensive tasks:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As we can see in our contrived example, simply attributing a function with `@tf.function`
    has given us a speedup of 10 times, from 7.2 seconds to 0.7 seconds.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Overtrain, and Then Generalize
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, overtraining on a dataset is considered to be harmful.
    However, we will demonstrate that we can use overtraining in a controlled fashion
    to our advantage to make training faster.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: As the saying goes, “The perfect is the enemy of the good.” We don’t want our
    network to be perfect right off the bat. In fact, we wouldn’t even want it to
    be any good initially. What we really want instead is for it to be learning *something*
    quickly, even if imperfectly. Because then we have a good baseline that we can
    fine tune to its highest potential. And experiments have shown that we can get
    to the end of the journey faster than training conventionally.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To further clarify the idea of overtraining and then generalizing, let’s look
    at an imperfect analogy of language learning. Suppose that you want to learn French.
    One way is to throw a book of vocabulary and grammar at you and expect you to
    memorize everything. Sure, you might go through the book every day and maybe in
    a few years, you might be able to speak some French. But this would not be the
    optimal way to learn.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we could look at how language learning programs approach this
    process. These programs introduce you to only a small set of words and grammatical
    rules initially. After you have learned them, you will be able to speak some broken
    French. Maybe you could ask for a cup of coffee at a restaurant or ask for directions
    at a bus stop. At this point, you will be introduced constantly to a larger set
    of words and rules, and this will help you to improve over time.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: This process is similar to how our model would learn gradually with more and
    more data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: How do we force a network to learn quickly and imperfectly? Make it overtrain
    on our data. The following three strategies can help.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Use progressive sampling
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One approach to overtrain and then generalize is to progressively show more
    and more of the original training set to the model. Here’s a simple implementation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Take a sample of the dataset (say, roughly 10%).
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the network until it converges; in other words, until it begins to perform
    well on the training set.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on a larger sample (or even the entire training set).
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By repeatedly showing a smaller sample of the dataset, the network will learn
    features much more quickly, but only related to the sample shown. Hence, it would
    tend to overtrain, usually performing better on the training set compared to the
    test set. When that happens, exposing the training process to the entire dataset
    will tend to generalize its learning, and eventually the test set performance
    would increase.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Use progressive augmentation
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach is to train on the entire dataset with little to no data augmentation
    at first, and then progressively increase the degree of augmentation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: By showing the unaugmented images repeatedly, the network would learn patterns
    faster, and by progressively increasing the degree of augmentation, it would become
    more robust.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Use progressive resizing
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach, made famous by Jeremy Howard from fast.ai (which offers free
    courses on AI), is progressive resizing. The key idea behind this approach is
    to train first on images scaled down to smaller pixel size, and then progressively
    fine tune on larger and larger sizes until the original image size is reached.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Images resized by half along both the width and height have a 75% reduction
    in pixels, and theoretically could lead to an increase in training speed of four
    times over the original images. Similarly, resizing to a quarter of the original
    height and width can in the best case lead to 16-times reduction (at a lower accuracy).
    Smaller images have fewer details visible, forcing the network to instead learn
    higher-level features including broad shapes and colors. Then, training with larger
    images will help the network learn the finer details, progressively increasing
    the test accuracy, as well. Just like a child is taught the high-level concepts
    first and then progressively exposed to more details in later years, the same
    concept is applied here to CNNs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can experiment with a combination of any of these methods or even build
    your own creative methods such as training on a subset of classes and then generalizing
    to all the classes later.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Install an Optimized Stack for the Hardware
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hosted binaries for open source packages are usually built to run on a variety
    of hardware and software configurations. These packages try to appeal to the least
    common denominator. When we do `pip install` on a package, we end up downloading
    and installing this general-purpose, works-for-everyone binary. This convenience
    comes at the expense of not being able to take advantage of the specific features
    offered by a particular hardware stack. This issue is one of the big reasons to
    avoid installing prebuilt binaries and instead opt for building packages from
    source.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: As an example, Google has a single TensorFlow package on `pip` that can run
    on an old Sandy Bridge (second-generation Core i3) laptop as well as a powerful
    16-core Intel Xeon server. Although convenient, the downside of this is that this
    package does not take advantage of the highly powerful hardware of the Xeon server.
    Hence, for CPU-based training and inference, Google recommends compiling TensorFlow
    from source to best optimize for the hardware at hand.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to do this manually is by setting the configuration flags for the hardware
    before building the source code. For example, to enable support for AVX2 and SSE
    4.2 instruction sets, we can simply execute the following build command (note
    the extra `m` character ahead of each instruction set in the command):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'How do you check which CPU features are available? Use the following command
    (Linux only):'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Building TensorFlow from source with the appropriate instruction set specified
    as build flags should result in a substantial increase in speed. The downside
    here is that building from source can take quite some time, at least a couple
    of hours. Alternatively, we can use Anaconda to download and install a highly
    optimized variant of TensorFlow, built by Intel on top of their Math Kernel Library
    for Deep Neural Networks (MKL-DNN). The installation process is pretty straightforward.
    First, we install the [Anaconda](https://anaconda.com) package manager. Then,
    we run the following command:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: On Xeon CPUs, MKL-DNN often provides upward of two-times speedup in inference.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: How about optimization for GPUs? Because NVIDIA abstracts away the differences
    between the various GPU internals with the CUDA library, there is usually no need
    to build from source. Instead, we could simply install a GPU variant of TensorFlow
    from `pip` (`tensorflow-gpu` package). We recommend the [Lambda Stack](https://oreil.ly/4AUxp)
    one-liner installer for convenience (along with NVIDIA drivers, CUDA, and cuDNN).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: For training and inference on the cloud, AWS, Microsoft Azure, and GCP all provide
    GPU machine images of TensorFlow optimized for their hardware. It’s quick to spin
    up multiple instances and get started. Additionally, NVIDIA offers GPU-accelerated
    containers for on-premises and cloud setups.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Optimize the Number of Parallel CPU Threads
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compare the following two examples:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are a couple of areas in these examples where we can exploit inherent
    parallelism:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Between operations
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: In example 1, the calculation of Y does not depend on the calculation of X.
    This is because there is no shared data between those two operations, and thus
    both of them can execute in parallel on two separate threads.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, in example 2, the calculation of Y depends on the outcome of the
    first operation (X), and so the second statement cannot execute until the first
    statement completes execution.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration for the maximum number of threads that can be used for interoperation
    parallelism is set using the following statement:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The recommended number of threads is equal to the number of CPUs (sockets) on
    the machine. This value can be obtained by using the `lscpu` command (Linux only).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Per-operation level
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: We can also exploit the parallelism within a single operation. Operations such
    as matrix multiplications are inherently parallelizable.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-8](part0008.html#a_matrix_multiplication_for_a_x_b_operat) demonstrates
    a simple matrix multiplication operation. It’s clear that the overall product
    can be split into four independent calculations. After all, the product between
    one row of a matrix and one column of another matrix does not depend on the calculations
    for the other rows and columns. Each of those splits could potentially get its
    own thread and all four of them could execute at the same time.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![A matrix multiplication for A x B operation with one of the multiplications
    highlighted](../images/00088.jpeg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. A matrix multiplication for A x B operation with one of the multiplications
    highlighted
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The configuration for the number of threads that can be used for intraoperation
    parallelism is set using the following statement:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The recommended number of threads is equal to the number of cores per CPU. You
    can obtain this value by using the `lscpu` command on Linux.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Use Better Hardware
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have already maximized performance optimizations and still need faster
    training, you might be ready for some new hardware. Replacing spinning hard drives
    with SSDs can go a long way, as can adding one or more better GPUs. And let’s
    not forget, sometimes the CPU can be the culprit.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, you might not need to spend much money: public clouds like AWS, Azure,
    and GCP all provide the ability to rent powerful configurations for a few dollars
    per hour. Best of all, they come with optimized TensorFlow stacks preinstalled.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if you have the cash to spend or have a rather generous expense account,
    you could just skip this entire chapter and buy the 2-petaFLOPS NVIDIA DGX-2\.
    Weighing in at 163 kgs (360 pounds), its 16 V100 GPUs (with a total of 81,920
    CUDA cores) consume 10 kW of power—the equivalent of seven large window air conditioners.
    And all it costs is $400,000!
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![The $400,000 NVIDIA DGX-2 deep learning system](../images/00303.jpeg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. The $400,000 NVIDIA DGX-2 deep learning system
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Distribute Training
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “*Two lines to scale training horizontally!*”
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: On a single machine with a single GPU, there’s only so far that we can go. Even
    the beefiest GPUs have an upper limit in compute power. Vertical scaling can take
    us only so far. Instead, we look to scale horizontally—distribute computation
    across processors. We can do this across multiple GPUs, TPUs, or even multiple
    machines. In fact, that is exactly what researchers at Google Brain did back in
    2012, using 16,000 processors to run a neural network built to look at cats on
    YouTube.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'In the dark days of the early 2010s, training on ImageNet used to take anywhere
    from several weeks to months. Multiple GPUs would speed things up, but few people
    had the technical know-how to configure such a setup. It was practically out of
    reach for beginners. Luckily, we live in the day of TensorFlow 2.0, in which setting
    up distributed training is a matter of introducing two lines of code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Training speed increases nearly proportionally (90–95%) in relation to the number
    of GPUs added. As an example, if we added four GPUs (of similar compute power),
    we would notice an increase of >3.6 times speedup ideally.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Still, a single system can only support a limited number of GPUs. How about
    multiple nodes, each with multiple GPUs? Similar to `MirroredStrategy`, we can
    use `Multi``Worker``MirroredStrategy`. This is quite useful when building a cluster
    on the cloud. [Table 6-1](part0008.html#recommended_distribution_strategies) presents
    a couple of distribution strategies for different use cases.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Recommended distribution strategies
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strategy** | **Use case** |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| `MirroredStrategy` | Single node with two or more GPUs |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| `MultiWorkerMirroredStrategy` | Multiple nodes with one or more GPUs each
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: To get the cluster nodes to communicate with one another for `MultiWorkerMirroredStrategy`,
    we need to configure the `TF_CONFIG` environment variable on every single host.
    This requires setting up a JSON object that contains the IP addresses and ports
    of all other hosts in the cluster. Manually managing this can be error prone,
    and this is where orchestration frameworks like Kubernetes really shine.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The open source Horovod library from Uber is another high-performance and easy-to-use
    distribution framework. Many of the record benchmark performances seen in the
    next section require distributed training on several nodes, and Horovod’s performance
    helped them get the edge. It is worth noting that the majority of the industry
    uses Horovod particularly because distributed training on earlier versions of
    TensorFlow was a much more involved process. Additionally, Horovod works with
    all major deep learning libraries with minimal amount of code change or expertise.
    Often configured through the command line, running a distributed program on four
    nodes, each with four GPUs, can be done in a single command line:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Examine Industry Benchmarks
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Three things were universally popular in the 1980s—long hair, the Walkman, and
    database benchmarks. Much like the current hype of deep learning, database software
    was similarly going through a phase of making bold promises, some of which were
    marketing hype. To put these companies to the test, a few benchmarks were introduced,
    more famously among them was the Transaction Processing Council (TPC) benchmark.
    When someone needed to buy database software, they could rely on this public benchmark
    to decide where to spend their company’s budget. This competition fueled rapid
    innovation, increasing speed and performance per dollar, moving the industry ahead
    faster than anticipated.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by TPC and other benchmarks, a few system benchmarks were created to
    standardize performance reporting in machine learning.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: DAWNBench
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Stanford’s DAWNBench benchmarks time and cost to get a model to 93% Top-5 accuracy
    on ImageNet. Additionally, it also does a time and cost leaderboard on inference
    time. It’s worth appreciating the rapid pace of performance improvement for training
    such a massive network. When DAWNBench originally started in September 2017, the
    reference entry trained in 13 days at a cost of $2,323.39\. In just one and a
    half years since then, although the cheapest training costs as low as $12, the
    fastest training time is 2 minutes 43 seconds. Best of all, most entries contain
    the training source code and optimizations that can be studied and replicated
    by us. This gives further guidance on the effects of hyperparameters and how we
    can use the cloud for cheap and fast training without breaking the bank.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Entries on DAWNBench as of August 2019, sorted by the least cost
    for training a model to 93% Top-5 accuracy
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cost (USD)** | **Training time** | **Model** | **Hardware** | **Framework**
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| $12.60 | 2:44:31 | ResNet-50Google Cloud TPU | GCP n1-standard-2, Cloud TPU
    | TensorFlow 1.11 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| $20.89 | 1:42:23 | ResNet-50Setu Chokshi(MS AI MVP) | Azure ND40s_v2 | PyTorch
    1.0 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| $42.66 | 1:44:34 | ResNet-50 v1GE Healthcare(Min Zhang) | 8*V100 (single
    p3.16x large) | TensorFlow 1.11 + Horovod |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| $48.48 | 0:29:43 | ResNet-50Andrew Shaw, Yaroslav Bulatov, Jeremy Howard
    | 32 * V100(4x - AWS p3.16x large) | Ncluster + PyTorch 0.5 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: MLPerf
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to DAWNBench, MLPerf is aimed at repeatable and fair testing of AI
    system performance. Although newer than DAWNBench, this is an industry consortium
    with much wider support, especially on the hardware side. It runs challenges for
    both training and inference in two divisions: open and closed. The closed division
    trains the same model with the same optimizers, so the raw hardware performance
    can be compared apples-to-apples. The open division, on the other hand, allows
    using faster models and optimizers to allow for more rapid progress. Compared
    to the more cost-effective entries in DAWNBench in [Table 6-2](part0008.html#entries_on_dawnbench_as_of_august_2019co),
    the top performers on MLPerf as shown in [Table 6-3](part0008.html#key_closed-division_entries_on_dawnbench)
    might be a bit out of reach for most of us. The top-performing NVIDIA DGX SuperPod,
    composed of 96 DGX-2H with a total of 1,536 V100 GPUs, costs in the $35 to $40
    million range. Even though 1,024 Google TPUs might themselves cost in the several
    millions, they are each available to rent on the cloud at $8/hour on-demand pricing
    (as of August 2019), resulting in a net cost of under $275 for the less-than two
    minutes of training time.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-3\. Key closed-division entries on DAWNBench as of August 2019, showing
    training time for a ResNet-50 model to get to 75.9% Top-1 accuracy
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time (minutes)** | **Submitter** | **Hardware** | **Accelerator** | **#
    of accelerators** |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| 1.28 | Google | TPUv3 | TPUv3 | 1,024 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| 1.33 | NVIDIA | 96x DGX-2H | Tesla V100 | 1,536 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| 8,831.3 | Reference | Pascal P100 | Pascal P100 | 1 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: 'Although both the aforementioned benchmarks highlight training as well as inference
    (usually on more powerful devices), there are other inference-specific competitions
    on low-power devices, with the aim to maximize accuracy and speed while reducing
    power consumption. Held at annual conferences, here are some of these competitions:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'LPIRC: Low-Power Image Recognition Challenge'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EDLDC: Embedded Deep Learning Design Contest'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System Design Contest at Design Automation Conference (DAC)
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training our model is only half the game. We eventually need to serve the predictions
    to our users. The following points guide you to making your serving side more
    performant.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Use an Efficient Model
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning competitions have traditionally been a race to come up with the
    highest accuracy model, get on top of the leaderboard, and get the bragging rights.
    But practitioners live in a different world—the world of serving their users quickly
    and efficiently. With devices like smartphones, edge devices, and servers with
    thousands of calls per second, being efficient on all fronts (model size and computation)
    is critically needed. After all, many machines would not be capable of serving
    a half gigabyte VGG-16 model, which happens to need 30 billion operations to execute,
    for not even that high of accuracy. Among the wide variety of pretrained architectures
    available, some are on the higher end of accuracy but large and resource intensive,
    whereas others provide modest accuracy but are much lighter. Our goal is to pick
    the architecture that can deliver the highest accuracy for the available computational
    power and memory budget of our inference device. In [Figure 6-10](part0008.html#comparing_different_models_for_sizecomma),
    we want to pick models in the upper-left zone.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing different models for size, accuracy, and operations per second
    (adapted from “An Analysis of Deep Neural Network Models for Practical Applications”
    by Alfredo Canziani, Adam Paszke and Eugenio Culurciello)](../images/00008.jpeg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. Comparing different models for size, accuracy, and operations
    per second (adapted from “An Analysis of Deep Neural Network Models for Practical
    Applications” by Alfredo Canziani, Adam Paszke, and Eugenio Culurciello)
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Usually, the approximately 15 MB MobileNet family is the go-to model for efficient
    smartphone runtimes, with more recent versions like MobileNetV2 and MobileNetV3
    being better than their predecessors. Additionally, by varying the hyperparameters
    of the MobileNet models like depth multiplier, the number of computations can
    be further reduced, making it ideal for real-time applications. Since 2017, the
    task of generating the most optimal architecture to maximize accuracy has also
    been automated with NAS. It has helped discover new (rather obfuscated looking)
    architectures that have broken the ImageNet accuracy metric multiple times. For
    example, FixResNeXt (based on PNASNet architecture at 829 MB) reaches a whopping
    86.4% Top-1 accuracy on ImageNet. So, it was natural for the research community
    to ask whether NAS helps find architecture that’s tuned for mobile, maximizing
    accuracy while minimizing computations. The answer is a resounding yes—resulting
    in faster and better models, optimized for the hardware at hand. As an example,
    MixNet (July 2019) outperforms many state-of-the-art models. Note how we went
    from billions of floating-point operations to millions ([Figure 6-10](part0008.html#comparing_different_models_for_sizecomma)
    and [Figure 6-11](part0008.html#comparison_of_several_mobile-friendly_mo)).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison of several mobile-friendly models in the paper “MixNet: Mixed
    Depthwise Convolution Kernels” by Mingxing Tan and Quoc V. Le](../images/00172.jpeg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-11\. Comparison of several mobile-friendly models in the paper “MixNet:
    Mixed Depthwise Convolution Kernels” by Mingxing Tan and Quoc V. Le'
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As practitioners, where can we find current state-of-the-art models? *PapersWithCode.com/SOTA*
    showcases leaderboards on several AI problems, comparing paper results over time,
    along with the model code. Of particular interest would be the models with a low
    number of parameters that achieve high accuracies. For example, EfficientNet gets
    an amazing Top-1 84.4% accuracy with 66 million parameters, so it could be an
    ideal candidate for running on servers. Additionally, the ImageNet test metrics
    are on 1,000 classes, whereas our case might just require classification on a
    few classes. For those cases, a much smaller model would suffice. Models listed
    in Keras Application (*tf.keras.applications*), TensorFlow Hub, and TensorFlow
    Models usually carry many variations (input image sizes, depth multipliers, quantizations,
    etc.).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-368
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Shortly after Google AI researchers publish a paper, they release the model
    used in the paper on the [TensorFlow Models](https://oreil.ly/Piq40) repository.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Quantize the Model
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “*Represent 32-bit weights to 8-bit integer, get 2x faster, 4x smaller models*”
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are driven primarily by matrix–matrix multiplications. The arithmetic
    involved tends to be rather forgiving in that small deviations in values do not
    cause a significant swing in output. This makes neural networks fairly robust
    to noise. After all, we want to be able to recognize an apple in a picture, even
    in less-than-perfect lighting. When we quantize, we essentially take advantage
    of this “forgiving” nature of neural networks.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at the different quantization techniques, let’s first try to
    build an intuition for it. To illustrate quantized representations with a simple
    example, we’ll convert 32-bit floating-point weights to INT8 (8-bit integer) using
    *linear quantization*. Obviously, FP32 represents 2^(32) values (hence, 4 bytes
    to store), whereas INT8 represents 2⁸ = 256 values (1 byte). To quantize:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Find the minimum and maximum values represented by FP32 weights in the neural
    network.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide this range into 256 intervals, each corresponding to an INT8 value.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate a scaling factor that converts an INT8 (integer) back to a FP32\.
    For example, if our original range is from 0 to 1, and INT8 numbers are 0 to 255,
    the scaling factor will be 1/256.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace FP32 numbers in each interval with the INT8 value. Additionally, store
    the scaling factor for the inference stage where we convert INT8 values back to
    FP32 values. This scaling factor only needs to be stored once for the entire group
    of quantized values.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During inference calculations, multiply the INT8 values by the scaling factor
    to convert it back to a floating-point representation. [Figure 6-12](part0008.html#quantizing_from_a_0_to_1_32-bit_floating)
    illustrates an example of linear quantization for the interval [0, 1].
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Quantizing from a 0 to 1 32-bit floating-point range down to 8-bit integer
    range for reduced storage space](../images/00254.jpeg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. Quantizing from a 0 to 1 32-bit floating-point range down to an
    8-bit integer range for reduced storage space
  id: totrans-380
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a few different ways to quantize our models, the simplest one being
    reducing the bit representation of the weights from 32-bit to 16-bit or lower.
    As might be evident, converting 32-bit to 16-bit means half the memory size is
    needed to store a model. Similarly, converting to 8-bit would require a quarter
    of the size. So why not convert it to 1-bit and save 32x the size? Well, although
    the models are forgiving up to a certain extent, with each reduction, we will
    notice a drop in accuracy. This reduction in accuracy grows exponentially beyond
    a certain threshold (especially below 8 bits). To go below and still have a useful
    working model (like a 1-bit representation), we’d need to follow a special conversion
    process to convert them to binarized neural networks. XNOR.ai, a deep learning
    startup, has famously been able to bring this technique to production. The Microsoft
    Embedded Learning Library (ELL) similarly provides such tools, which have a lot
    of value for edge devices like the Raspberry Pi.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'There are numerous benefits to quantization:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Improved memory usage
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: By quantizing to 8-bit integer representation (INT8), we typically get a 75%
    reduction in model size. This makes it more convenient to store and load the model
    in memory.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Improved performance
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Integer operations are faster than floating-point operations. Additionally,
    the savings in memory usage reduces the likelihood of having to unload the model
    from RAM during execution, which also has the added benefit of decreased power
    consumption.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Portability
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Edge devices such as Internet of Things devices might not support floating-point
    arithmetic, so it would be untenable to keep the model as a floating-point in
    such situations.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Most inference frameworks provide a way to quantize, including Core ML Tools
    from Apple, TensorRT from NVIDIA (for servers), and TensorFlow Lite, as well as
    the TensorFlow Model Optimization Toolkit from Google. With TensorFlow Lite, models
    can be quantized after training during conversion (called post-training quantization).
    To minimize accuracy losses even further, we can use the TensorFlow Model Optimization
    Toolkit during training. This process is called *quantization-aware training*.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: It would be useful to measure the benefit provided by quantization. Metrics
    from the [TensorFlow Lite Model optimization](https://oreil.ly/me4-I) benchmarks
    (shown in [Table 6-4](part0008.html#effects_of_different_quantization_strate))
    give us a hint, comparing 1) unquantized, 2) post-training quantized, and 3) quantization-aware
    trained models. The performance was measured on a Google Pixel 2 device.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6-4\. Effects of different quantization strategies (8-bit) on models
    (source: TensorFlow Lite model optimization documentation)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **MobileNet** | **MobileNetV2** | **InceptionV3** |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| **Top-1 accuracy** | **Original** | 0.709 | 0.719 | 0.78 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| **Post-training quantized** | 0.657 | 0.637 | 0.772 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| **Quantization-aware training** | 0.7 | 0.709 | 0.775 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| **Latency (ms)** | **Original** | 124 | 89 | 1130 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| **Post-training quantized** | 112 | 98 | 845 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| **Quantization-aware training** | 64 | 54 | 543 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| **Size (MB)** | **Original** | 16.9 | 14 | 95.7 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| **Optimized** | 4.3 | 3.6 | 23.9 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: So, what do these numbers indicate? After quantization using TensorFlow Lite
    to INT8, we see roughly a four-times reduction in size, approximately two-times
    speedup in run time, and less than 1% change in accuracy. Not bad!
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: More extreme form of quantization, like 1-bit binarized neural networks (like
    XNOR-Net), claim a whopping 58-times speedup with roughly 32-times smaller size
    when tested on AlexNet, with a 22% loss in accuracy.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Prune the Model
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pick a number. Multiply it by 0\. What do we get? Zero. Multiply your pick again
    by a small value neighboring 0, like 10–⁶, and we’ll still get an insignificant
    value. If we replace such tiny weights (→ 0) in a model with 0 itself, it should
    have little effect on the model’s predictions. This is called *magnitude-based
    weight pruning,* or simply pruning, and is a form of *model compression*. Logically,
    putting a weight of 0 between two nodes in a fully connected layer is equivalent
    to deleting the edge between them. This makes a model with dense connections sparser.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: As it happens, a large chunk of the weights in a model are close to 0\. Pruning
    the model will result in many of those weights being set to 0\. This happens with
    little impact to accuracy. Although this does not save any space by itself, it
    introduces a ton of redundancy that can be exploited when it comes time to save
    the model to disk in a compressed format such as ZIP. (It is worth noting that
    compression algorithms thrive on repeating patterns. The more the repetition,
    the higher the compressibility.) The end result is that our model can often be
    compressed by four times. Of course, when we finally need to use the model, it
    would need to be uncompressed before loading in memory for inference.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow team observed the accuracy loss shown in [Table 6-5](part0008.html#model_accuracy_loss_versus_pruning_perce)
    while pruning the models. As expected, more efficient models like MobileNet observe
    higher (though still small) accuracy loss when compared with comparatively bigger
    models like InceptionV3.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-5\. Model accuracy loss versus pruning percentage
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Sparsity** | **Accuracy loss against original accuracy** |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| InceptionV3 | 50% | 0.1% |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| InceptionV3 | 75% | 2.5% |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| InceptionV3 | 87.5% | 4.5% |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| MobileNet | 50% | 2% |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: Keras provides APIs to prune our model. This process can be done iteratively
    during training. Train a model normally or pick a pretrained model. Then, periodically
    prune the model and continue training. Having enough epochs between the periodic
    prunes allows the model to recover from any damage due to introducing so much
    sparsity. The amount of sparsity and number of epochs between prunes can be treated
    as hyperparameters to be tuned.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Another way of implementing this is by using [Tencent’s PocketFlow](https://oreil.ly/JJms2)
    tool, a one-line command that provides several other pruning strategies implemented
    in recent research papers.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Use Fused Operations
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In any serious CNN, the convolutional layer and batch normalization layer frequently
    appear together. They are kind of the Laurel and Hardy of CNN layers. Fundamentally,
    they are both linear operations. Basic linear algebra tells us that combining
    two or more linear operations will also result in a linear operation. By combining
    convolutional and batch normalization layers, we not only reduce the number of
    computations, but also decrease the amount of time spent in data transfer, both
    between main memory and GPU, and main memory and CPU registers/cache. Making them
    one operation prevents an extra roundtrip. Luckily, for inference purposes, most
    inference frameworks either automatically do this fusing step or provide model
    converters (like TensorFlow Lite) to make this optimization while converting the
    model to the inference format.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Enable GPU Persistence
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loading and initializing the GPU drivers take time. You might have noticed
    a delay every time a training or inference job was initiated. For frequent, short
    jobs, the overhead can become relatively expensive quickly. Imagine an image classification
    program for which the classification takes 10 seconds, 9.9 of which were spent
    in loading the driver. What we need is for the GPU driver to stay preinitialized
    in the background, and be ready for whenever our training jobs start. And that’s
    where the NVIDIA GPU Persistence Daemon comes to the rescue:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Our GPUs will use a bit more wattage during idle time, but they will be ready
    and available the next time a program is launched.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored different avenues for improving the speed and performance
    of our deep learning pipeline, from storing and reading the data to inference.
    A slow data pipeline often leads to a GPU starving for data, resulting in idle
    cycles. With several of the simple optimizations we discussed, our hardware can
    be put to its maximum efficiency. The handy checklist can serve as a ready reference.
    Feel free to make a copy for your desk (or your refrigerator). With these learnings,
    we hope to see your entry among the top performers of the MLPerf benchmark list.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
