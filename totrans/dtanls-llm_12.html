<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__frameworks"> <span class="chapter-title-numbering"><span class="num-string">10</span></span> <span class="title-text"> Software frameworks</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">Building applications with LangChain</li> 
    <li id="p3">Solving complex tasks with agents</li> 
    <li id="p4">Querying data with LlamaIndex</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>Up to now, we’ve mostly been using OpenAI’s Python library to interact with language models. This library offers basic functionality for sending prompts and retrieving answers from GPT and other OpenAI models (as well as options for tuning and fine-tuning). The libraries from other providers, such as Anthropic and Cohere, offer similar functionality. As long as your data-analysis tasks are simple, this is probably all you need. However, what if your data analysis requires a complex multistep pipeline, possibly integrating many different data formats?</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>At that point, you may want to switch to a more powerful software framework. Several higher-level frameworks for building complex applications on top of language models are currently emerging. In this chapter, we’ll discuss two of the most popular contenders: LangChain and LlamaIndex. The former is a general framework for building applications using large language models. What’s more, it comes with various useful built-in components that implement popular use cases for language models. LlamaIndex, on the other hand, specifically supports use cases where language models need to interact with large data sets.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>To get the hang of it, we’ll first write a simple text-classification pipeline using LangChain. Then we’ll explore some of the advanced features of LangChain. More precisely, we’ll see how LangChain supports <em>agents</em> on top of language models. Creating an agent means putting the language model itself into the driver’s seat, giving it lots of freedom on how to accomplish a given task while using a collection of tools provided by the user. We will use such agents to solve complex data-analysis tasks independently, using a mix of tools to access different data sources. Next, we’ll see how LlamaIndex easily ingests large amounts of data in diverse data formats and makes them usable for language models. Internally, it uses cheap language models to map data snippets and analysis tasks to vector representations, after which it maps tasks to data based on the similarity between those vectors. Finally, we’ll compare the two frameworks and discuss tradeoffs between those frameworks and the libraries offered by OpenAI and other language model providers.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class=" readable-text-h2" id="langchain"><span class="num-string browsable-reference-id">10.1</span> LangChain</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>If you want to create a complex application based on language models, you should probably check out LangChain. The framework launched in October 2022 and has been gaining popularity quickly (leading to the creation of a corresponding startup in April 2023). At the time of writing, LangChain is still developing rapidly. Be sure to run the code in this section with the right LangChain version (because future versions may change the interfaces).</p> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>As the name suggests, LangChain relates to language models (Lang) and chains. In LangChain terminology, a <em>chain</em> is simply a sequence of steps. Each step may correspond to the invocation of a language model, a data-processing step, or the invocation of an arbitrary tool. The important point here is that we no longer assume that a single call to a language model will solve our problem (which was the case for most of the scenarios we have discussed in this book). Instead, we assume that we need a complex network of connected components. That’s the scenario where LangChain shines!</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>To use LangChain, you first need to install it. Go to a terminal, and run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p12"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install langchain==0.1.13</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>As we mentioned, you need to install the right LangChain version if you want to run the following code samples! LangChain is currently changing so quickly that the code may not work with a different version.</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>Beyond the LangChain core, you may want to install libraries that support language models from specific providers. In the following sections, we’ll be using OpenAI’s models. Run the following command in the terminal (and, again, make sure to use the version specified):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p15"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install langchain-openai==0.1.1</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Support for other providers, such as Anthropic and Cohere, is equally available.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>Okay, that’s it! After running these commands, you’re ready to run the sample projects discussed next.</p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <h2 class=" readable-text-h2" id="classifying-reviews-with-langchain"><span class="num-string browsable-reference-id">10.2</span> Classifying reviews with LangChain</h2> 
  </div> 
  <div class="readable-text" id="p19"> 
   <p>One of the first projects we did was text analysis with language models. Remember chapter 4? We used language models to classify reviews based on the underlying sentiment (is this a recommendation or a warning?). We’ll do the same here; we will just use LangChain in our code. Comparing the LangChain code with the original should give you a first impression of how LangChain can help simplify building applications with language models.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <h3 class=" readable-text-h3" id="overview"><span class="num-string browsable-reference-id">10.2.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>We will create a chain to classify text documents. A LangChain chain may involve many steps, each implemented by invoking a language model or a generic Python function (e.g., to parse the results of language model invocations into a standardized format). The term <em>chain</em> is actually slightly misleading. Although you may imagine a chain as a sequence of consecutive steps, the chains in LangChain are much more powerful. For instance, they may involve parallel steps as well as conditional execution. However, for the simple text-classification application, we won’t need such advanced features. Instead, we will restrict ourselves to a simple chain with just a few steps.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>Our chain will integrate several standard components offered by LangChain. The first component in our chain is a prompt template. As in chapter 4, this template describes the classification task and the expected output format. You may wonder what has changed compared to the previous code version. After all, we have been discussing prompt templates all along. The difference is that LangChain introduces a dedicated class to represent prompt templates. This class offers various convenience functions for prompt templates: for example, for creating and instantiating them. At the same time, LangChain offers a hub allowing users to upload and download prompt templates (as well as many other components). In our simple scenario, we won’t need any of these advanced features. Instead, we just need to instantiate our prompt template by passing a single parameter (the text to classify) as input.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>The second step in our chain is a language model. Again, we have been using language models throughout this book, but LangChain adds several helpful functions on top of the language model object. For instance, it is easy to automatically log all language model invocations, and LangChain offers convenience functions for different invocation scenarios (e.g., batch and stream processing). Again, we won’t use those advanced features here. Instead, we will pass the prompt (the first step in our chain) to the language model to generate a reply.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>The third step in our chain is a parser, extracting the answer string from the reply generated by the language model. You may remember from chapter 3 that OpenAI’s language models generate detailed replies, integrating one or multiple answers as well as various types of metadata (e.g., information about token usage). The parser automatically extracts the answer string we’re looking for from the result object (which works for OpenAI models as well as for all other providers). The result of the pipeline is a single token indicating whether the input review is a recommendation. Figure <a href="#fig__LCclassification">10.1</a> illustrates the three steps of this pipeline.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p25">  
   <img alt="figure" src="../Images/CH10_F01_Trummer.png" width="600" height="607"/> 
   <h5 class=" figure-container-h5" id="fig__LCclassification"><span class="num-string">Figure <span class="browsable-reference-id">10.1</span></span> Components in our LangChain classification chain</h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class=" readable-text-h3" id="creating-a-classification-chain"><span class="num-string browsable-reference-id">10.2.2</span> Creating a classification chain</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>Time to implement our chain in Python! First we need a prompt template. We use the same template as in chapter 4, but this time, we use LangChain’s <code>ChatPromptTemplate</code> class:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p28"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_core.prompts.chat import ChatPromptTemplate
prompt = ChatPromptTemplate.from_template(
    '{text}\n'                         #1
    'Is the sentiment positive or negative?\n'
    'Answer ("Positive"/"Negative")\n')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Text placeholder
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>You may notice the reference to chat models (after all, the class we’re instantiating is called <code>ChatPromptTemplate</code>). As discussed in chapter 3, chat models process a history of prior messages rather than a single input message. Many of the most recently released models are chat models. In LangChain, chat models require a specialized prompt template (which instantiates into a sequence of messages rather than a single text). This is the type of template we’re creating here. The template is the same as we used in chapter 4. It contains a placeholder (<strong class="cueball">1</strong>) for the input text to classify. We generally use curly braces (<code>{}</code>) to mark placeholders in prompt templates; they are replaced with concrete values when instantiating the prompt.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Second, we need a language model to process prompts. The following code instantiates the GPT-4o model from OpenAI:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p31"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model='gpt-4o', temperature=0, 
    max_tokens=1)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>The <code>ChatOpenAI</code> class covers all chat models by OpenAI. It is imported from the <code>langchain_openai</code> package, featuring functionality to support the use of OpenAI models in LangChain. Other providers, such as Anthropic and Cohere, have their own associated packages offering comparable functionality for their models (note that you need to install those packages separately via pip). The parameters in the constructor of <code>ChatOpenAI</code> may seem familiar: we choose the model (<code>gpt-4o</code>), set <code>temperature</code> to 0 (to reduce the degree of randomness in the output), and limit the maximum number of output tokens to one (because both possible class labels, <code>Positive</code> and <code>Negative</code>, consist of a single token).</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>Third, we need to extract the answer string from the (more detailed) reply of our language model. That’s easy to do with <code>StrOutputParser</code>. LangChain output parsers implement a wide range of transformations on the output of a model invocation. In this case, we only need a very simple transformation, extracting the desired answer string. The following piece of code creates a corresponding parser:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p34"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_core.output_parsers.string import StrOutputParser
parser = StrOutputParser()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Finally, we’ll put all the components together in a chain. To do so, we can use the LangChain Expression Language (LCEL). If you’re a Linux user, the following syntax should look familiar to you:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p36"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_core.runnables.passthrough import RunnablePassthrough
chain = ({'text':RunnablePassthrough()} | prompt | llm | parser)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>To use the output of an operation as input for the following step, we connect them with the pipe symbol (<code/>|). The command creates a chain that connects the previously mentioned components. In addition, it specifies the input that the chain expects. In our case, the prompt template has a placeholder for the text to classify.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>At the start of the chain, we mark this parameter as <code>RunnablePassthrough</code>. This gives us a lot of flexibility in terms of how we pass inputs to the chain. For instance, the following code illustrates how to process a list of inputs using the previously created chain:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <div class="code-area-container"> 
    <pre class="code-area">inputs = ['This movie is great!', 'This movie is bad!']
outputs = chain.batch(inputs)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <h3 class=" readable-text-h3" id="putting-it-together"><span class="num-string browsable-reference-id">10.2.3</span> Putting it together</h3> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>Time to finalize our code for text classification! The code in listing <a href="#code__LangChainClassification">10.1</a> takes as input a path to a .csv file containing a <code>text</code> column. Executing the code generates a result file containing an additional column called <code>class</code> with the classification result. In other words, the code does exactly the same thing as that from chapter 4, but this time using LangChain.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p42"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__LangChainClassification"><span class="num-string">Listing <span class="browsable-reference-id">10.1</span></span> Sentiment classification using LangChain</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_openai import ChatOpenAI
from langchain_core.prompts.chat import ChatPromptTemplate
from langchain_core.output_parsers.string import StrOutputParser
from langchain_core.runnables.passthrough import RunnablePassthrough

import argparse
import pandas as pd

def create_chain():                         #1
    """ Creates chain for text classification.
    
    Returns:
        a chain for text classification.
    """
    prompt = ChatPromptTemplate.from_template(  #2
        '{text}\n'
        'Is the sentiment positive or negative?\n'
        'Answer ("Positive"/"Negative")\n')
    llm = ChatOpenAI(              #3
        model='gpt-4o', temperature=0, 
        max_tokens=1)
    parser = StrOutputParser()  #4
     #5
    chain = ({'text':RunnablePassthrough()} | prompt | llm | parser)
    return chain

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument('file_path', type=str, help='Path to input file')
    args = parser.parse_args()
    
    df = pd.read_csv(args.file_path)  #6

    chain = create_chain()            #7
    results = chain.batch(list(df['text']))  #8

    df['class'] = results  #9
    df.to_csv('result.csv')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates a chain
     <br/>#2 Creates a prompt template
     <br/>#3 Creates an LLM object
     <br/>#4 Creates an output parser
     <br/>#5 Creates a chain
     <br/>#6 Reads the data
     <br/>#7 Creates a chain
     <br/>#8 Uses it
     <br/>#9 Stores the output
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>The <code>create_chain</code> function (<strong class="cueball">1</strong>) implements the steps discussed in the last section. It generates a prompt template for classification (<strong class="cueball">2</strong>), then a chat model (<strong class="cueball">3</strong>), and finally an output parser (<strong class="cueball">4</strong>). The result is a chain connecting all those components (<strong class="cueball">5</strong>).</p> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>After reading the command-line parameters, the code reads the input data (<strong class="cueball">6</strong>), creates a corresponding chain (<strong class="cueball">7</strong>), and finally applies the chain to the list of input texts (<strong class="cueball">8</strong>). The classification results are added to the input data and stored on disk (<strong class="cueball">9</strong>).</p> 
  </div> 
  <div class="readable-text" id="p45"> 
   <h3 class=" readable-text-h3" id="trying-it-out"><span class="num-string browsable-reference-id">10.2.4</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>Time to try it! As usual, you will find the code for listing <a href="#code__LangChainClassification">10.1</a> on the book’s companion website in the chapter 10 section. Download the code (the listing1.py file) and, optionally, a file containing reviews to classify (such as reviews.csv from chapter 4). Open the terminal, and switch to the folder containing the code. Assuming that reviews.csv is located in the same folder, run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p47"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing1.py reviews.csv</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>Check the folder containing the code. You should see a new file, result.csv, with the desired classification results. So far, we have only verified that we can do the same things using LangChain that we can do with OpenAI’s libraries directly (even though, arguably, the LangChain code is cleaner). In the next section, we’ll see that LangChain enables us to do much more than that.</p> 
  </div> 
  <div class="readable-text" id="p49"> 
   <h2 class=" readable-text-h2" id="agents-putting-the-large-language-model-into-the-drivers-seat"><span class="num-string browsable-reference-id">10.3</span> Agents: Putting the large language model into the driver’s seat</h2> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>So far, you may have considered language models as (highly sophisticated) tools. Based on your input, the language model produces output. If data processing requires more than the language model can accomplish, it is up to you, the developer, to add the necessary infrastructure. For instance, assume that you’re building a question-answering system for math questions. Realizing that language models are bad at calculating things (which, ironically for a computer program, they are), you may consider the following approach: based on the user question, the language model translates the input into a mathematical formula. Then that formula is parsed and evaluated by a simple calculator tool. The output of that tool is sent to the user.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>So far, so good. It gets more complicated in situations where you have not one but multiple math tools. Perhaps one tool solves differential equations, and another evaluates simple arithmetic equations. In such cases, you can expand your approach with a classification stage, mapping the user input to the most suitable tool. However, this approach breaks down in situations where answering the user question may require not applying a single tool but multiple invocations of different tools, possibly using the output of one tool as input for the next invocations. In such cases, manually covering each possible sequence of required tool invocations is simply not feasible.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>This is the type of use case where <em>agents</em> become useful. Agents are a fairly novel way of using large language models. At the core of this approach is a change in perspective. Instead of considering the language model a tool used as a step within a pipeline designed by the developer, we make the language model an independent agent. Rather than trying to orchestrate the order in which the language model and other processing steps are applied (which we did in the last section), we leave it up to the language model to decide which processing steps are applied in which order. The advantage of this approach is that it is much more flexible, freeing us as developers from having to foresee each possible development in advance to create an associated branch in our processing logic.</p> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>Agents can be useful for complex data-analysis tasks where it is unclear, a priori, which data sources or processing methods may be required to satisfy a user’s request. Two terms are central to the agent approach, and we will look at them next: the <em>agent</em> and its <em>tools</em>.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Let’s start by discussing tools. A tool can encapsulate arbitrary functionality. It is a function that the language model can use if it deems it necessary. When we use LangChain or similar frameworks to implement agents, a tool is typically implemented as a Python function. Each tool must be associated with a description in natural language. This description is shown to the language model as part of the prompts. Based on this description, the language model can decide whether a tool seems helpful in a given context. To use a tool, the language model requires a description of the input parameters and the output semantics. Similar to human programmers, choosing meaningful parameter names and writing precise documentation helps language models use tools effectively. Because agents are implemented via language models, a full description of all available tools is typically provided as part of the input prompt.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>Agents use tools whenever they are required to solve a complex task specified by the user. Agents are implemented via language models. Although fine-tuning can improve the performance of language models as agents, generic models should work in principle. The secret behind turning language models into agents lies less within the model itself but rather in the way it is prompted. At a minimum, corresponding prompts integrate the following components:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p56">A description of a high-level task the agent should solve. This description is provided by the user.</li> 
   <li class="readable-text" id="p57">A list of available tools, together with a description of their functionality and their input and output parameters.</li> 
   <li class="readable-text" id="p58">A description of the expected output format. This enables mapping the output of the language model to tool invocations.</li> 
  </ul> 
  <div class="readable-text" id="p59"> 
   <p>Given such a prompt, the language model can produce output requesting specific tool invocations. The infrastructure implementing the agent approach parses the output, maps it to corresponding tools and input parameter values, and obtains the invocation result. In the next iteration, the result of the tool invocation is added to the input prompt. In this way, the language model can essentially <em>access</em> the results of tool invocations. Based on that, the language model can choose to apply more tools (possibly using the results of prior tool invocations as inputs) or terminate if a final answer is available.</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Figure <a href="#fig__AgentOverview">10.2</a> summarizes this process. The user-specified task, together with a detailed description of all tools, forms the input to the language model. The output of the language model is parsed and mapped to an action. Either this action represents the invocation of a tool (in that case, the invocation command contains values for all input parameters of the tool), or it represents termination (in this case, the termination command contains what the language model believes is an answer to the input task). If the action is a tool invocation, the corresponding call is executed. The result is added to the prompt used in the next iteration. Iterations continue until the language model decides to terminate (or until a user-specified limit on the number of iterations is reached).</p> 
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/CH10_F02_Trummer.png" width="1100" height="489"/> 
   <h5 class=" figure-container-h5" id="fig__AgentOverview"><span class="num-string">Figure <span class="browsable-reference-id">10.2</span></span> Using language models as agents. Given a prompt describing the task and available tools, the language model decides on termination and tool invocations. Results of tool invocations are added to the prompt used for the next iteration.</h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <p>At this point, you may be curious what the corresponding prompts look like. Let’s examine the standard prompt template used for agents in LangChain. You can download the prompt template from LangChain’s hub. If you want to do so, install the hub first using the following command in the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p63"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install langchainhub==0.1.15</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Then run the following code in Python to print out the standard template for one of the most popular agent types:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p65"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain import hub
prompt = hub.pull('hwchase17/react')
print(prompt.template)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>You should see the following output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p67"> 
   <div class="code-area-container"> 
    <pre class="code-area"> #1
Answer the following questions as best you can. 
You have access to the following tools:

{tools}  #2

Use the following format:  #3

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}      #4
Thought:{agent_scratchpad}  #5</pre> 
    <div class="code-annotations-overlay-container">
     #1 General scenario
     <br/>#2 Tool descriptions
     <br/>#3 Format description
     <br/>#4 User input
     <br/>#5 Prior results
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>This prompt template describes the general scenario (<strong class="cueball">1</strong>) (there is a question that needs answering), available tools (<strong class="cueball">2</strong>), and the process to solve the task (<strong class="cueball">3</strong>). The prompt template contains multiple placeholders representing tool descriptions (<strong class="cueball">2</strong>), the input from the user (<strong class="cueball">4</strong>), and the results of prior iterations (<strong class="cueball">5</strong>). As we will see in the following sections, LangChain offers various convenience functions to create and execute agents based on this and similar prompt templates.</p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <h2 class=" readable-text-h2" id="building-an-agent-for-data-analysis"><span class="num-string browsable-reference-id">10.4</span> Building an agent for data analysis</h2> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>In this section, we will use LangChain to build an agent for data analysis. This agent will be able to access different data sources with structured and unstructured data. What’s more, the agent will decide which of those sources to access and in which order. It may even use information obtained from one source to query a second source (e.g., to access a structured database about video game sales to identify the most sold game in a specific year and then use the game title to query the web for further information).</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h3 class=" readable-text-h3" id="overview-1"><span class="num-string browsable-reference-id">10.4.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Our data-analysis agent implements the approach we discussed in the previous section. It uses a language model to decide which tools to invoke in which order and with what input parameters. In our example scenario, we will provide the agent with tools to access a relational database (as well as obtain information about its structure, such as the names of available tables). We also provide the agent with a tool that enables web search (exploiting existing search engines in the background). Taken together, we get an agent that can query a relational database and use the web to obtain information that relates to the database content.</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>Let’s start our discussion with a more detailed description of the tools we will provide to the agent. In total, the agent will have access to the following five tools:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p74"><code>sql_db_list_tables</code> lists all tables in the relational database.</li> 
   <li class="readable-text" id="p75"><code>sql_db_schema</code> returns the SQL schema of a table, given the table name.</li> 
   <li class="readable-text" id="p76"><code>sql_db_query_checker</code> enables the agent to validate an SQL query.</li> 
   <li class="readable-text" id="p77"><code>sql_db_query</code> evaluates an SQL query and returns the query result.</li> 
   <li class="readable-text" id="p78"><code>search</code> enables the agent to search the web via keywords, returning web text.</li> 
  </ul> 
  <div class="readable-text" id="p79"> 
   <p>The first four tools help the agent access a relational database. The last tool enables the agent to retrieve information from the web. Given a user-specified task, the agent decides (using the underlying language model) which of these tools to invoke and in which order. Figure <a href="#fig__DataAgent">10.3</a> illustrates this scenario.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/CH10_F03_Trummer.png" width="1100" height="528"/> 
   <h5 class=" figure-container-h5" id="fig__DataAgent"><span class="num-string">Figure <span class="browsable-reference-id">10.3</span></span> The data agent uses multiple tools to explore the structure and query a relational data- base. In addition, the agent can retrieve web text via the web search tool.</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <h3 class=" readable-text-h3" id="creating-an-agent-with-langchain"><span class="num-string browsable-reference-id">10.4.2</span> Creating an agent with LangChain</h3> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>Creating an agent with LangChain is fast! LangChain even offers specialized constructors for agents that access a structured database. We will use those features in the following code.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Agents are implemented via language models. To create an agent, we first have to create a language model object:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p84"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    temperature=0, model='gpt-4o')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p85"> 
   <p>We’re creating an OpenAI language model of type chat. More precisely, we refer to the GPT-4o model again.</p> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Next, we create an object representing our relational database. We will query an SQLite database stored on disk. Assume that <code>dbpath</code> stores the path of the corresponding database file (typically, such files have the .db suffix). We can create a database object using the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_community.utilities.sql_database import SQLDatabase
db = SQLDatabase.from_uri(f'sqlite:///{dbpath}')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>We mentioned four tools for accessing the relational database. Fortunately, all of these tools will be automatically created from the database object. However, we still need to create a tool for web search.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>We will use a built-in component of LangChain, the SerpAPI tool. To use this tool, you first need to register for an account on the SerpAPI website. Open your browser, go to <a href="https://serpapi.com/">https://serpapi.com/</a>, click the Register button, and create a corresponding account. To execute the code presented next, you will need to retrieve your API access key (available at <a href="https://serpapi.com/dashboard">https://serpapi.com/dashboard</a>). You also need to install a LangChain extension to enable the web search tool. Go to the terminal, and run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p90"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install google-search-results==2.4.2</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>After that, all it takes is the following snippet of Python code to generate a tool for web search (assuming that <code>llm</code> contains the previously created language model object and <code>serpaikey</code> the SerpAPI access key):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain.agents.load_tools import load_tools
extra_tools = load_tools(
    ['serpapi'], serpapi_api_key=serpaikey, llm=llm)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>The <code>load_tools</code> function is used for standard tools by passing the names of the desired tools as parameters. In this case, we only need the web search tool, and we pass only a single entry in the list of tool names (<code>serpapi</code>). After the call to <code>load_tools</code>, we store the result in <code>extra_tools</code>: a list of tools with a single entry (the web search tool). We now have all the components we need to create an agent using LangChain.</p> 
  </div> 
  <div class="readable-text" id="p94"> 
   <p>Assume that <code>db</code> contains the database object, created previously, and <code>llm</code> the language model generated before. We initialize an agent for SQL-based data access using the following code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p95"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_community.agent_toolkits.sql.base import create_sql_agent
agent = create_sql_agent(
    llm=llm, db=db, verbose=True,
    agent_type='openai-tools',
    extra_tools=extra_tools)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>The <code>create_sql_agent</code> command is a convenience function offered by LangChain to create agents for SQL-based data access. The four previously mentioned tools for relational database access (useful for retrieving table names, showing table schemata, validating SQL queries, and, ultimately, issuing them) are added automatically without us having to add them explicitly. There is only one more tool we want in addition to the SQL-focused tools: the web search capability. Such tools are specified in a list via the <code>extra_tools</code> input parameter. Setting the <code>verbose</code> flag to <code>True</code> enables us to follow the “thought process” leading the agent to call specific tools (we will see some example output later). The agent type, <code>openai-tools</code> in this case, determines the precise prompt to use as well as which parsers to use to map the output of the language model to tool invocations.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>After creating the agent, we use the following code to apply the agent to a specific task (we assume that the variable <code>task</code> stores a natural language description of the task we want to solve):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p98"> 
   <div class="code-area-container"> 
    <pre class="code-area">agent.invoke({'input':task})</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p99"> 
   <h3 class=" readable-text-h3" id="complete-code-for-data-analysis-agent"><span class="num-string browsable-reference-id">10.4.3</span> Complete code for data-analysis agent</h3> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Listing <a href="#code__DataAgent">10.2</a> brings all of this together: after reading the SerpAPI API access key, as well as the path to the database file and a question from the command line, it creates a language model object (<strong class="cueball">1</strong>), then a database (<strong class="cueball">2</strong>), the web search tool (<strong class="cueball">3</strong>), and, finally, the agent (<strong class="cueball">4</strong>). It invokes the agent (<strong class="cueball">5</strong>) on the input question. The output produced by the agent terminates with an answer to that question (or with a failure message if the agent is unable to find an answer).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__DataAgent"><span class="num-string">Listing <span class="browsable-reference-id">10.2</span></span> Agent for data analysis with web search capability</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse

from langchain.agents.load_tools import load_tools
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain_openai import ChatOpenAI

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('serpaikey', type=str, help='SERP API access key')
    parser.add_argument('dbpath', type=str, help='Path to SQLite database')
    parser.add_argument('question', type=str, help='A question to answer')
    args = parser.parse_args()
    
    llm = ChatOpenAI(              #1
        temperature=0, model='gpt-4o')
     #2
    db = SQLDatabase.from_uri(f'sqlite:///{args.dbpath}')
    extra_tools = load_tools(                         #3
        ['serpapi'], serpapi_api_key=args.serpaikey, llm=llm)
    agent = create_sql_agent(     #4
        llm=llm, db=db, verbose=True,
        agent_type='openai-tools',
        extra_tools=extra_tools)
    agent.invoke({'input':args.question})  #5</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates an LLM client
     <br/>#2 Creates a database object
     <br/>#3 Adds a web search tool
     <br/>#4 Creates the agent
     <br/>#5 Invokes the agent with input
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h3 class=" readable-text-h3" id="trying-it-out-1"><span class="num-string browsable-reference-id">10.4.4</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>Let’s see how that works in practice! Download the code for listing <a href="#code__DataAgent">10.2</a> from the book’s companion website. Besides the code, you will need an SQLite database to try the data agent. We will use the SQLite database from chapter 5, storing information about video games (you can find the corresponding file on the book’s companion website under the Games SQLite link).</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>Open the terminal, and switch to the directory containing the code. We will assume that the database file, games.db, is located in the same directory. Run the following code (replace <code>[SerpAPI key]</code> with your search key, available at <a href="https://serpapi.com/dashboard">https://</a> <a href="https://serpapi.com/dashboard">serpapi.com/dashboard</a>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">python listing2.py [SerpAPI key] games.db
<span class="">↪</span> 'What was the most sold game in 2016, and how is it played?'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>You should see output like the following (the output you see may differ slightly due to changing web content, small changes to the GPT-4o model, and a few other factors):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p107"> 
   <div class="code-area-container"> 
    <pre class="code-area">[1m&gt; Entering new SQL Agent Executor chain...[0m
[32;1m[1;3m 
 #1
Invoking: `sql_db_list_tables` with `{'tool_input': "}`
[0m[38;5;200m[1;3mgames[0m[32;1m[1;3m
 #2
Invoking: `sql_db_schema` with `{'table_names': 'games'}`

[0m[33;1m[1;3m
CREATE TABLE games (
    rank INTEGER, 
    name TEXT, 
    platform TEXT, 
    year INTEGER, 
    genre TEXT, 
    publisher TEXT, 
    americasales NUMERIC, 
    eusales NUMERIC, 
    japansales NUMERIC, 
    othersales NUMERIC, 
    globalsales NUMERIC
)

/*
3 rows from the games table:
rank    name    platform    year    genre    publisher    
americasales    eusales    japansales    othersales    globalsales
1    Wii Sports    Wii    2006    Sports    Nintendo    41.4900000000    
29.0200000000    3.7700000000    8.4600000000    82.7400000000
2    Super Mario Bros.   NES   1985   Platform   Nintendo   29.0800000000    
3.5800000000    6.8100000000    0.7700000000    40.2400000000
3    Mario Kart Wii    Wii    2008    Racing    Nintendo    15.8500000000    
12.8800000000    3.7900000000    3.3100000000    35.8200000000
*/[0m[32;1m[1;3m
 #3
Invoking: `sql_db_query_checker` with `{'query': 'SELECT name 
FROM games WHERE year = 2016 ORDER BY globalsales DESC LIMIT 1'}`
responded: The games table contains the information we need. 
I will query for the game with the highest global sales in 2016.
[0m[36;1m[1;3mSELECT name FROM games WHERE year = 2016 
ORDER BY globalsales DESC LIMIT 1[0m[32;1m[1;3m
 #4
Invoking: `sql_db_query` with `{'query': 'SELECT name 
FROM games WHERE year = 2016 ORDER BY globalsales DESC LIMIT 1'}`
[0m[36;1m[1;3m[('FIFA 17',)][0m[32;1m[1;3m
 #5
Invoking: `Search` with `How to play FIFA 17`
[0m[33;1m[1;3m["A Beginner's Guide To Complete FIFA 17 Domination ... The 
main steps you should take are to jump right in with a quick play game. ...
EA Sports FIFA ...", 'Play FIFA 17 up to 5 days before launch for a full 
10 hours when you join EA Access on Xbox One and Origin Access on PC.', 
"1. Shield the ball in 360 degrees · 2. Use Driven Shots and Driven Headers · 
3. Use set piece upgrades to score with style · 4. Make Fifa 17's ...", 
'Play FIFA 17 as much as you want with EA Access or Origin Access 
for only $4.99 per month. Now available in The Vault.', 
'Cautiously Start An Online Match. Score Early After Some Self-Proclaimed 
Beautiful Build Up Play. Concede 4 Goals In A Row And Convince ...', 
'FIFA 17 TUTORIALS &amp; ULTIMATE TEAM ➞ 
Twitter: https://twitter.com/KrasiFIFA ➞ 
Instagram: http://instagram.com/KrasiFIFA How I record my ...', 
"Draft mode is another way to play FIFA Ultimate Team, 
giving you the ability to play with Players you don't own. 
You'll have the opportunity to draft a random ..."]
 #6
[0m[32;1m[1;3mThe most sold game in 2016 was FIFA 17. 

To play FIFA 17, you can follow these steps:

1. Jump right in with a quick play game.
2. Shield the ball in 360 degrees.
3. Use Driven Shots and Driven Headers.
4. Use set piece upgrades to score with style.
5. Start an online match cautiously.
6. Score early after some self-proclaimed beautiful build-up play.
7. Draft mode is another way to play FIFA Ultimate Team, 
giving you the ability to play with players you don't own. 
You'll have the opportunity to draft a random team.

Remember, practice makes perfect![0m

[1m&gt; Finished chain.[0m</pre> 
    <div class="code-annotations-overlay-container">
     #1 The agent retrieves the list of tables.
     <br/>#2 The agent retrieves the table schema.
     <br/>#3 The agent verifies the SQL query.
     <br/>#4 The agent queries for the top game.
     <br/>#5 The agent searches the web for FIFA 17.
     <br/>#6 The agent formulates the final answer.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p108"> 
   <p>Remember that we switched the agent’s output to verbose mode. That means the output contains a full log of tools invoked by the agent, as well as the agent’s reasoning process. Let’s take a closer look at the output to see what happened.</p> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>First, the agent retrieves a list of the tables available in the relational database (using <code>sql_db_list_tables</code>) (<strong class="cueball">1</strong>). Clearly, that’s a reasonable step when confronted with a new database. The result of the tool invocation reveals that the database contains only a single table (called <code>games</code>). The agent becomes “curious” about the table contents. It invokes the <code>sql_db_schema</code> tool to get further information about the <code>games</code> table (<strong class="cueball">2</strong>). Note that this tool consumes input parameters, specifically the name of the table to investigate. The log shows the values of all input parameters for each tool invocation.</p> 
  </div> 
  <div class="readable-text" id="p110"> 
   <p>The invocation of the <code>sql_db_schema</code> tool returns the SQL command that was used to create the <code>games</code> table, together with a small sample of the table’s content. Next, the agent considers an SQL query to retrieve relevant information about the input question (“What was the most sold game in 2016 and how is it played?”). In the first step, it validates that the following query is syntactically correct by invoking the <code>sql_db_query_checker</code> tool (<strong class="cueball">3</strong>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <div class="code-area-container"> 
    <pre class="code-area">SELECT name FROM games WHERE year = 2016 ORDER BY globalsales DESC LIMIT 1</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>At the same time, the agent uses the opportunity to “reflect” on the usefulness of the query under consideration, as evidenced by the output “The games table contains the information we need. I will query for the game with the highest global sales in 2016.” It may seem strange that a language model can benefit from this type of monologue instead of writing out tool invocations directly. Yet it has been shown that enabling agents to explicitly reason about the problem at hand and the steps they are taking to solve it can improve their performance [1]. That’s what’s happening here as well.</p> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>Finally, the agent decides to use the previously validated query to retrieve information from the database, using the <code>sql_db_query</code> tool (<strong class="cueball">4</strong>). The SQL query returns the game that generated the most revenue in 2016: FIFA 17, a soccer simulation produced by Electronic Arts. But the input question asks for more than that: “What was the most sold game in 2016 and <em>how is it played</em>?” The second part of the question cannot be answered from database content. To its credit, the agent realizes that and tries to access the web instead: it issues a web search request using the <code>Search</code> tool for the search string “How to play FIFA 17” (<strong class="cueball">5</strong>). Note that the agent was able to automatically formulate a suitable search string from the result of the SQL query and the input question. The result of the web search is a collection of text snippets (shown in the output) that contain information about how to play FIFA 2017.</p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Finally, the agent uses the information returned by the web search (in combination with information from the SQL database) to formulate a final answer (<strong class="cueball">6</strong>). The final answer identifies FIFA 17 as the most popular game in 2016 and contains detailed instructions for how to play it well. We have seen that the agent can perform a complex sequence of tool invocations to find the desired answer without having to specify the process to follow by hand. If you’re interested, try querying the agent with a few more, possibly more complicated, questions and see whether it can answer them as well.</p> 
  </div> 
  <div class="readable-text" id="p115"> 
   <h2 class=" readable-text-h2" id="adding-custom-tools"><span class="num-string browsable-reference-id">10.5</span> Adding custom tools</h2> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>So far, we have used standard tools offered by LangChain for the most common use cases. What happens if we have specialized requirements? For example, say you want to make a data source accessible via a custom API, or you have specialized analysis functions that an agent can apply to your data. In those cases, you can define your own custom tools and make them accessible to a LangChain agent.</p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <h3 class=" readable-text-h3" id="the-currency-converter"><span class="num-string browsable-reference-id">10.5.1</span> The currency converter</h3> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>In the last section, we analyzed a data set about video game sales. The original data reports sales values in US dollars. What about other currencies? To enable agents to reason about game sales using multiple currencies, we will add a currency-converter tool. Given an amount in US dollars as input, together with the name of a target currency, this tool returns the equivalent value in the target currency.</p> 
  </div> 
  <div class="readable-text" id="p119"> 
   <p>Listing <a href="#code__CustomAgent">10.3</a> shows how to add the currency-converter tool to our data agent. At its core, a tool is nothing but a Python function. Our currency converter is implemented by the <code>convert_currency</code> function (<strong class="cueball">2</strong>). How does LangChain know that we want to turn the function into a tool? That’s done by the <code>@tool</code> decorator (<strong class="cueball">1</strong>), which needs to directly precede the function name. Typically, we do not have to specify types for parameters and return values of Python functions (even though it does improve the readability of your code). If you plan to turn a function into a tool, however, you should specify all these types. The reason is as follows: to use your function properly as a tool, the agent needs to invoke it with parameters of the right type. All types you specify in the function header will be made accessible to the agent as part of the description of your tool. Hence, associating parameters with types helps your agent avoid unnecessary invocation errors.</p> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Besides parameter types, the agent should know a little about what your tool can accomplish. The first important piece of information is the name of your function. By default, your tool will be named after your function. Don’t call your function <code>XYZ</code>, because that will make it very hard to understand what’s going on! The name of the function in listing <a href="#code__CustomAgent">10.3</a>, <code>convert_currency</code>, should make it pretty clear what the function does. Similarly, the names of the input parameters, <code>USD_amount</code> (of type <code>float</code>) and <code>currency</code> (of type <code>str</code>), are pretty self-explanatory (which is good!). The function output is a converted amount in the target currency or an error message if the requested target currency is not supported (that’s why the output type is a <code>Union</code> of string and float values). As a rule of thumb, if you plan to use a Python function as a tool, write it the same way you would to enable human coders to understand your function without reading its code in detail.</p> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>In addition to the names of the function and its parameters, the agent “sees” the function documentation (<strong class="cueball">3</strong>). Again, make sure your documentation is well structured, and explain the semantics of your tool and associated parameters. In this case, the documentation describes the function of the tool, the semantics of the input parameters (even with an example of an admissible value for the second parameter), and the output semantics.</p> 
  </div> 
  <div class="readable-text" id="p122"> 
   <p>The <code>convert_currency</code> function uses a small database of currencies with associated conversion factors. For instance, it contains conversion factors for euros and yen but not many other currencies. If you’re creating a tool for your agent, take into account cases in which the agent does not use the tool properly. This may happen if the tool description is incomplete or if the language model makes a mistake (which happens even to state-of-the-art language models). In this case, we’re adding specialized handling for the case that the target currency is not supported (i.e., a corresponding conversion factor is missing) (<strong class="cueball">4</strong>). If the target currency is not supported, the function returns a helpful error message that contains the full set of supported currencies. This helps the agent to restrict the parameter to the set of admissible options for the following invocations. If the target currency is supported, the function returns the converted amount (<strong class="cueball">5</strong>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p123"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__CustomAgent"><span class="num-string">Listing <span class="browsable-reference-id">10.3</span></span> Data-analysis agent with currency-converter tool</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse

from langchain.agents.load_tools import load_tools
from langchain.tools import tool
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_community.agent_toolkits.sql.base import create_sql_agent
from langchain_openai import ChatOpenAI
from typing import Union

@tool  #1
 #2
def convert_currency(USD_amount: float, currency: str)-&gt;Union[float, str]:
     #3
    """ Converts an amount in US dollars to another currency.
    
    Args:
        USD_amount: amount in US dollars.
        currency: name of target currency (e.g., "Yen").
    
    Returns:
        input amount in target currency.
    """
    conversion_factors = {
        'Euro':0.93, 'Yen':151.28, 'Yun':0.14, 
        'Pound':1.26, 'Won':0.00074, 'Rupee':0.012}
    
    if currency not in conversion_factors:
         #4
        error_message = (
            f'Unknown currency: {currency}!'
            f'Use one of {conversion_factors.keys()}')
        return error_message

     #5
    conversion_factor = conversion_factors[currency]
    converted_amount = USD_amount * conversion_factor
    return converted_amount

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('serpaikey', type=str, help='SERP API access key')
    parser.add_argument('dbpath', type=str, help='Path to SQLite database')
    parser.add_argument('question', type=str, help='A question to answer')
    args = parser.parse_args()
    
    llm = ChatOpenAI(
        temperature=0, model='gpt-4o')
    db = SQLDatabase.from_uri(f'sqlite:///{args.dbpath}')
    extra_tools = load_tools(
        ['serpapi'], serpapi_api_key=args.serpaikey, llm=llm)
     #6
    extra_tools.append(convert_currency)

    agent = create_sql_agent(
        llm=llm, db=db, verbose=True,
        agent_type='openai-tools',
        extra_tools=extra_tools)
    agent.invoke({'input':args.question})</pre> 
    <div class="code-annotations-overlay-container">
     #1 Turns the function into a tool
     <br/>#2 Function signature with types
     <br/>#3 Function documentation
     <br/>#4 Helpful error message for the agent
     <br/>#5 Converts and returns the result
     <br/>#6 Adds the currency-converter tool
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p124"> 
   <p>After creating a tool based on a Python function, we just need to make the tool available to our agent. Listing <a href="#code__CustomAgent">10.3</a> creates almost the same agent as listing <a href="#code__DataAgent">10.2</a>, with the only difference being that we add the currency-converter tool (<strong class="cueball">6</strong>). Because we’re using the SQL agent again, the converter tool and the web search tool are inserted into the list of extra tools (added on top of the standard tools for SQL access that are automatically provided to the agent). By default, the tool name equals the name of the function it is based on. Hence, we’re simply adding <code>convert_currency</code> to the list of extra tools (<strong class="cueball">6</strong>) to enhance the agent with currency conversion abilities.</p> 
  </div> 
  <div class="readable-text" id="p125"> 
   <h3 class=" readable-text-h3" id="trying-it-out-2"><span class="num-string browsable-reference-id">10.5.2</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>Let’s see whether our agent is able to use our newly added tool! Download the code for listing <a href="#code__CustomAgent">10.3</a> from the book’s companion website. You can use the same database file as before (and assume that the games.db file is located in the same folder as the code). Then, open the terminal and execute the following code (substituting your SerpAPI access key for <code>[SerpAPI key]</code>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <div class="code-area-container"> 
    <pre class="code-area">python listing3.py [SerpAPI key] games.db  'What revenue was generated by
computer games in 2015?  How much is it in Yen?'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>Clearly, answering that question requires the currency-converter tool. When running the code, you will see output like the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <div class="code-area-container"> 
    <pre class="code-area">[1m&gt; Entering new SQL Agent Executor chain...[0m
[32;1m[1;3m
 #1
Invoking: `sql_db_list_tables` with `'tool_input': "`
[0m[38;5;200m[1;3mgames[0m[32;1m[1;3m
 #2
Invoking: `sql_db_schema` with `'table_names': 'games'`
[0m[33;1m[1;3m
CREATE TABLE games (
    rank INTEGER, 
    name TEXT, 
    platform TEXT, 
    year INTEGER, 
    genre TEXT, 
    publisher TEXT, 
    americasales NUMERIC, 
    eusales NUMERIC, 
    japansales NUMERIC, 
    othersales NUMERIC, 
    globalsales NUMERIC
)

/*
3 rows from the games table:
rank    name    platform    year    genre    publisher    americasales
eusales    japansales    othersales    globalsales
1    Wii Sports    Wii    2006    Sports    Nintendo    41.4900000000    
29.0200000000    3.7700000000    8.4600000000    82.7400000000
2    Super Mario Bros.   NES   1985   Platform   Nintendo   29.0800000000    
3.5800000000    6.8100000000    0.7700000000    40.2400000000
3    Mario Kart Wii    Wii    2008    Racing    Nintendo    15.8500000000    
12.8800000000    3.7900000000    3.3100000000    35.8200000000
*/[0m[32;1m[1;3m
 #3
Invoking: `sql_db_query_checker` with `{'query': 'SELECT SUM(globalsales) as 
total_revenue FROM games WHERE year = 2015'}`
responded: The "games" table contains the information we need. 
The "globalsales" column represents the global revenue generated by 
each game. We can sum this column for the games released in 2015 to 
get the total revenue. Let's write and check the SQL query.

[0m[36;1m[1;3mSELECT SUM(globalsales) as total_revenue 
FROM games WHERE year = 2015[0m[32;1m[1;3m
 #4
Invoking: `sql_db_query` with `{'query': 'SELECT SUM(globalsales) as 
total_revenue FROM games WHERE year = 2015'}`

[0m[36;1m[1;3m[(264.43999999999795,)][0m[32;1m[1;3m
 #5
Invoking: `convert_currency` with `{'USD_amount': 
264.43999999999795, 'currency': 'Yen'}`

[0m[38;5;200m[1;3m40004.48319999969[0m[32;1m[1;3m
 #6
The total revenue generated by computer games in 2015 
was approximately $264.44 million. 
In Japanese Yen, this is approximately ¥40,004,483,200.[0m

[1m&gt; Finished chain.[0m</pre> 
    <div class="code-annotations-overlay-container">
     #1 The agent retrieves the database tables.
     <br/>#2 The agent queries for the table schema.
     <br/>#3 The agent verifies the SQL query.
     <br/>#4 The agent queries for sales in 2015.
     <br/>#5 The agent converts the currencies.
     <br/>#6 The agent formulates the final answer.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>Similarly to before, the agent first explores the database by retrieving the set of tables (<strong class="cueball">1</strong>) and then, after finding out that the database only contains a single table, retrieving the schema for that table (<strong class="cueball">2</strong>). Correctly, the agent infers that the database contains useful information about the input question and first validates (<strong class="cueball">3</strong>) and then executes (<strong class="cueball">4</strong>) a corresponding SQL query. The SQL query returns the total value of computer game sales in 2015, expressed in US dollars. To answer the final part of the question (“How much is it in Yen?”), the agent then applies the currency-converter tool (<strong class="cueball">5</strong>). Note that the agent chooses appropriate values for the two input parameters based on the function description and types. Finally, the agent formulates the answer to the input question (<strong class="cueball">6</strong>).</p> 
  </div> 
  <div class="readable-text" id="p131"> 
   <h2 class=" readable-text-h2" id="indexing-multimodal-data-with-llamaindex"><span class="num-string browsable-reference-id">10.6</span> Indexing multimodal data with LlamaIndex</h2> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>LangChain is by no means the only framework that makes it easier to use language models for data analysis. In this section, we discuss another framework that has recently appeared and is quickly gaining popularity: LlamaIndex.</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h3 class=" readable-text-h3" id="overview-2"><span class="num-string browsable-reference-id">10.6.1</span> Overview</h3> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>LlamaIndex shines for use cases where language models need to access large collections of data, possibly integrating various data types. In such cases, it is generally not advisable (or even possible) to directly feed all the data into the language model. Instead, we need a mechanism that quickly identifies relevant data for a given task, passing only relevant data to the language model. As the name suggests, LlamaIndex indexes data to quickly identify relevant subsets. More precisely, LlamaIndex associates pieces of data (e.g., chunks of text) with embedding vectors. We briefly discussed embedding vectors in chapter 4. In short, an embedding vector represents the semantics of text as a vector calculated by a language model. If two documents have similar embedding vectors (the distance between the vectors is small), we assume that they discuss similar topics.</p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>A typical LlamaIndex data-processing pipeline entails the following steps. First, it loads data, possibly in various formats, and performs preprocessing. For example, preprocessing may entail dividing long text documents into smaller chunks that are more convenient to handle. Next, LlamaIndex indexes the data. As discussed before, this means associating data chunks with embedding vectors. By default, LlamaIndex uses fairly small language models (e.g., OpenAI’s ada models) to calculate embedding vectors. This makes the indexing step cheap. Furthermore, LlamaIndex can store the generated index (the embedding vectors) on disk to avoid having to regenerate them for each new task.</p> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>LlamaIndex offers support for various use cases based on the generated index. For instance, it can use indexed data to answer natural language questions. Given a question as input, it first calculates an embedding vector for the question text. Then it compares the vector representing the question to precalculated vectors representing data chunks. It identifies the data items with the most similar vectors. The associated data is included in the prompt, together with the input question. The goal is to generate an answer to the question, exploiting relevant data as context. Whereas small models are used for indexing, we typically use larger models to generate the final reply. Figure <a href="#fig__LlamaIndex">10.4</a> illustrates this data-processing pipeline.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p137">  
   <img alt="figure" src="../Images/CH10_F04_Trummer.png" width="812" height="729"/> 
   <h5 class=" figure-container-h5" id="fig__LlamaIndex"><span class="num-string">Figure <span class="browsable-reference-id">10.4</span></span> Primary steps of a typical LlamaIndex data-processing pipeline. LlamaIndex loads and indexes data to enable fast retrieval. Given a question, LlamaIndex identifies relevant data items and submits them, together with the input question, to a language model to generate an answer.</h5>
  </div> 
  <div class="readable-text" id="p138"> 
   <h3 class=" readable-text-h3" id="installing-llamaindex"><span class="num-string browsable-reference-id">10.6.2</span> Installing LlamaIndex</h3> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>Let’s implement the pipeline from the last section in Python. To use LlamaIndex, we first have to install a few packages. Go to your terminal, and run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p140"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install llama-index==0.10.25</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p141"> 
   <p>That will set you up with LlamaIndex’s core packages. However, you will use LlamaIndex to analyze a diverse collection of data formats. To enable LlamaIndex to properly access and parse all of them, you need to install a few additional packages. Run the following commands in your terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p142"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install torch==2.1.2
pip install transformers==4.36.0
pip install python-pptx==0.6.23
pip install Pillow==10.2.0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>These libraries are necessary to analyze .pdf documents and PowerPoint presentations, all of which we will need for the following project.</p> 
  </div> 
  <div class="readable-text" id="p144"> 
   <h3 class=" readable-text-h3" id="implementing-a-simple-question-answering-system"><span class="num-string browsable-reference-id">10.6.3</span> Implementing a simple question-answering system</h3> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>You’re back at Banana and confronted with a challenging problem: being a global company, Banana has many different units. Your boss wants you to analyze data from different units, for example, to compare their performance. However, different units have widely varying preferences in terms of data formats. Some units publish their results as simple text documents, whereas others regularly turn out elaborate PowerPoint presentations. How do you integrate all those different data formats? Fortunately, LlamaIndex makes that easy.</p> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>Look at listing <a href="#code__LlamaIndex">10.4</a>: in just a few lines of Python code, it handles the task. The code accepts the following input parameters:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p147">A link to a data repository. This repository may contain files of various types.</li> 
   <li class="readable-text" id="p148">A question to answer. LlamaIndex will use the data in the repository to answer it.</li> 
  </ul> 
  <div class="readable-text" id="p149"> 
   <p>After parsing those parameters from the command line (<strong class="cueball">1</strong>), we load data from the input repository (<strong class="cueball">2</strong>). Fortunately, LlamaIndex makes this step very straightforward: no need to add handling for different file types and so on. Instead, passing the directory path is sufficient. Next, we index the data we just loaded (<strong class="cueball">3</strong>). By default, LlamaIndex uses OpenAI’s ada models to calculate embedding vectors. Data conversions and chunking (e.g., splitting large text documents into pieces small enough to be processed by OpenAI’s ada models) are all handled automatically. Now we create a query engine on top of the index (<strong class="cueball">4</strong>). This engine will automatically retrieve data related to an input question using the index. Finally, we use the previously generated engine to answer the input question (<strong class="cueball">5</strong>) and print the result.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p150"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__LlamaIndex"><span class="num-string">Listing <span class="browsable-reference-id">10.4</span></span> A simple question-answering system with LlamaIndex</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import openai

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

if __name__ == '__main__':
     #1
    parser = argparse.ArgumentParser()
    parser.add_argument('datadir', type=str, help='Path to data directory')
    parser.add_argument('question', type=str, help='A question to answer')
    args = parser.parse_args()
     #2
    documents = SimpleDirectoryReader(args.datadir).load_data()
     #3
    index = VectorStoreIndex.from_documents(documents)
     #4
    engine = index.as_query_engine()
     #5
    answer = engine.query(args.question)
    print(answer)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Parses the command-line parameters
     <br/>#2 Loads data from the directory
     <br/>#3 Indexes the data
     <br/>#4 Enables querying on the index
     <br/>#5 Generates the answer
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p151"> 
   <p>Although LlamaIndex offers various ways to configure and specialize each step of this pipeline (and to create other pipelines), using the default settings in each step leads to particularly concise code.</p> 
  </div> 
  <div class="readable-text" id="p152"> 
   <h3 class=" readable-text-h3" id="trying-it-out-3"><span class="num-string browsable-reference-id">10.6.4</span> Trying it out</h3> 
  </div> 
  <div class="readable-text" id="p153"> 
   <p>Let’s try our pipeline using some example data. You can download listing <a href="#code__LlamaIndex">10.4</a> from the book’s companion website. Also download the bananareports.zip file from the website, and unzip it in the same folder as the code (use the Banana Reports link). Look inside the folder: you will find (short) business reports in text, .pdf documents, and PowerPoint presentations. Time to answer a few questions! Open your terminal, and change to the directory containing the code and the bananareports folder (after unzipping). Now run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p154"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">python listing4.py bananareports 'How much did the Plantain unit make
<span class="">↪</span> in 2023?'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>You should see output like the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p156"> 
   <div class="code-area-container"> 
    <pre class="code-area">The Plantain unit made 30 million USD in 2023.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p157"> 
   <p>Have a look at the corresponding file. You will find that the Plantain unit did indeed make 30 million USD. Try asking for the other units (Pisang Raja and Cavendish); you should see the correct results. You can even try more complex questions—such as “Which unit made the most in 2023?”—requiring a comparison of different files. Again, the system should be able to find an accurate answer.</p> 
  </div> 
  <div class="readable-text" id="p158"> 
   <h2 class=" readable-text-h2" id="concluding-remarks"><span class="num-string browsable-reference-id">10.7</span> Concluding remarks</h2> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>LangChain and LlamaIndex are two popular frameworks for using language models. You have seen that implementing even complex applications, like those based on agents, is fairly quick when using those frameworks. LangChain and LlamaIndex have overlapping functionality: for example, both frameworks provide support for implementing agents based on language models. LlamaIndex particularly shines in scenarios that follow the high-level template we saw in the previous section (providing functionality on top of data indexes). LangChain has a more general focus on supporting developers in building complex applications using language models.</p> 
  </div> 
  <div class="readable-text" id="p160"> 
   <p>Both frameworks are relatively young at the time of writing, so the previous characterizations may not hold for future versions. In addition, their interfaces are evolving quickly, and running with the newest framework versions may require code changes. If your application does not require complex logic, you may consider using lower-level libraries such as the one by OpenAI.</p> 
  </div> 
  <div class="readable-text" id="p161"> 
   <p>Of course, this chapter does not cover these two frameworks in much detail. The intent is to give you an impression of what the frameworks can do for you, enabling you to make informed choices about which frameworks to study in more depth.</p> 
  </div> 
  <div class="readable-text" id="p162"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p163">LangChain and LlamaIndex enable complex applications on top of language models.</li> 
   <li class="readable-text" id="p164">LangChain and LlamaIndex make it easy to create agents. Agents use language models to control invocations of various tools (standard tools as well as custom tools). They can solve complex tasks if given access to suitable tools.</li> 
   <li class="readable-text" id="p165">In LangChain, use the <code>@tool</code> decorator to turn functions into tools.</li> 
   <li class="readable-text" id="p166">LlamaIndex indexes various data types by creating embedding vectors.</li> 
   <li class="readable-text" id="p167">LlamaIndex makes it easy to explore and query indexed data.</li> 
  </ul> 
  <div class="readable-text" id="p168"> 
   <h2 class=" readable-text-h2" id="references"><span class="num-string browsable-reference-id">10.9</span> References</h2> 
  </div> 
  <ol> 
   <li class="readable-text" id="p169">Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>Advances in Neural Information Processing Systems 35</em>, 24824–24837.</li> 
  </ol>
 </div></div></body></html>