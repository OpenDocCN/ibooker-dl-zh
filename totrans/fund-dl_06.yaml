- en: Chapter 6\. Beyond Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。超越梯度下降
- en: The Challenges with Gradient Descent
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降的挑战
- en: The fundamental ideas behind neural networks have existed for decades, but it
    wasn’t until recently that neural network-based learning models have become mainstream.
    Our fascination with neural networks has everything to do with their expressiveness,
    a quality we’ve unlocked by creating networks with many layers. As we have discussed
    in previous chapters, deep neural networks are able to crack problems that were
    previously deemed intractable. Training deep neural networks end to end, however,
    is fraught with difficult challenges that took many technological innovations
    to unravel, including massive labeled datasets (ImageNet, CIFAR-10, etc.), better
    hardware in the form of GPU acceleration, and several algorithmic discoveries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络背后的基本思想已经存在几十年了，但直到最近，基于神经网络的学习模型才变得主流。我们对神经网络的着迷与它们的表现力有关，这是我们通过创建具有许多层的网络解锁的质量。正如我们在之前的章节中讨论的那样，深度神经网络能够解决以前被认为是棘手的问题。然而，端到端训练深度神经网络充满了困难挑战，需要许多技术创新来解决，包括大规模标记数据集（ImageNet、CIFAR-10等）、GPU加速等更好的硬件以及几项算法发现。
- en: For several years, researchers resorted to layer-wise greedy pretraining to
    grapple with the complex error surfaces presented by deep learning models.^([1](ch06.xhtml#idm45934168902672))
    These time-intensive strategies would try to find more accurate initializations
    for the model’s parameters one layer at a time before using minibatch gradient
    descent to converge to the optimal parameter settings. More recently, however,
    breakthroughs in optimization methods have enabled us to train models directly
    in an end-to-end fashion.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，研究人员采用逐层贪婪预训练来应对深度学习模型呈现的复杂误差表面。这些耗时的策略会尝试逐层找到模型参数更准确的初始化，然后使用小批量梯度下降收敛到最佳参数设置。然而，最近，优化方法的突破使我们能够直接以端到端的方式训练模型。
- en: In this chapter, we will discuss several of these breakthroughs. The next couple
    of sections will focus primarily on local minima and whether they pose hurdles
    for successfully training deep models. Then we will further explore the nonconvex
    error surfaces induced by deep models, why vanilla minibatch gradient descent
    falls short, and how modern nonconvex optimizers overcome these pitfalls.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论几项突破性的成果。接下来的几节将主要关注局部最小值以及它们是否对成功训练深度模型构成障碍。然后我们将进一步探讨深度模型引起的非凸误差表面，为什么普通的小批量梯度下降不足以应对，以及现代非凸优化器如何克服这些困难。
- en: Local Minima in the Error Surfaces of Deep Networks
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度网络的误差表面中的局部最小值
- en: The primary challenge in optimizing deep learning models is that we are forced
    to use minimal local information to infer the global structure of the error surface.
    This is difficult because there is usually very little correspondence between
    local and global structure. Take the following analogy as an example.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 优化深度学习模型的主要挑战在于我们被迫使用最少的局部信息来推断误差表面的全局结构。这很困难，因为局部和全局结构之间通常几乎没有对应关系。以以下类比为例。
- en: Let’s assume you’re an insect on the continental United States. You’re dropped
    randomly on the map, and your goal is to find the lowest point on this surface.
    How do you do it? If all you can observe is your immediate surroundings, this
    seems like an intractable problem. If the surface of the US were bowl-shaped (or
    mathematically speaking, convex) and we were smart about our learning rate, we
    could use the gradient descent algorithm to eventually find the bottom of the
    bowl. But the surface of the US is extremely complex, that is to say, is a nonconvex
    surface, which means that even if we find a valley (a local minimum), we have
    no idea if it’s the lowest valley on the map (the global minimum). In [Chapter 4](ch04.xhtml#training_feed_forward),
    we talked about how a minibatch version of gradient descent can help navigate
    a troublesome error surface when there are spurious regions of magnitude zero
    gradients. But as we can see in [Figure 6-1](#mini_batch_gradient_descent), even
    a stochastic error surface won’t save us from a deep local minimum.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是美国大陆上的一只昆虫。你被随机放在地图上，你的目标是找到这个表面上的最低点。你该怎么做？如果你所能观察到的只是你周围的环境，这似乎是一个棘手的问题。如果美国的表面是碗状的（或者在数学上说是凸的），并且我们对学习率很聪明，我们可以使用梯度下降算法最终找到碗底。但美国的表面非常复杂，也就是说，是一个非凸表面，这意味着即使我们找到了一个山谷（一个局部最小值），我们也不知道它是否是地图上最低的山谷（全局最小值）。在[第4章](ch04.xhtml#training_feed_forward)中，我们讨论了梯度下降的小批量版本如何帮助在存在零梯度幅度的虚假区域时导航棘手的误差表面。但正如我们在[图6-1](#mini_batch_gradient_descent)中所看到的，即使是随机的误差表面也无法拯救我们脱离深层局部最小值。
- en: '![](Images/fdl2_0601.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0601.png)'
- en: Figure 6-1\. Minibatch gradient descent may aid in escaping shallow local minima,
    but often fails when dealing with deep local minima, as shown
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。小批量梯度下降可能有助于逃离浅层局部最小值，但在处理深层局部最小值时通常会失败，如所示
- en: Now comes the critical question. Theoretically, local minima pose a significant
    issue. But in practice, how common are local minima in the error surfaces of deep
    networks? And in which scenarios are they actually problematic for training? In
    the following two sections, we’ll pick apart common misconceptions about local
    minima.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现了一个关键问题。从理论上讲，局部最小值构成了一个重要问题。但在实践中，深度网络的误差表面中局部最小值有多常见？在哪些情况下它们实际上对训练有问题？在接下来的两节中，我们将剖析关于局部最小值的常见误解。
- en: Model Identifiability
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型可辨识性
- en: The first source of local minima is tied to a concept commonly referred to as
    *model identifiability*. One observation about deep neural networks is that their
    error surfaces are guaranteed to have a large—and in some cases, an infinite—number
    of local minima. There are two major reasons this observation is true.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 局部最小值的第一个来源与一个常被称为*模型可辨识性*的概念有关。关于深度神经网络的一个观察是，它们的误差表面保证有大量的局部最小值，有时甚至是无限多个。这个观察是真实的有两个主要原因。
- en: The first is that within a layer of a fully connected feed-forward neural network,
    any rearrangement of neurons will still give you the same final output at the
    end of the network. We illustrate this using a simple three-neuron layer in [Figure 6-2](#rearranging_neurons_in_a_layer).
    As a result, within a layer with  <math alttext="n"><mi>n</mi></math>  neurons,
    there are  <math alttext="n factorial"><mrow><mi>n</mi> <mo>!</mo></mrow></math>
     ways to rearrange parameters. And for a deep network with  <math alttext="l"><mi>l</mi></math>
     layers, each with  <math alttext="n"><mi>n</mi></math>  neurons, we have a total
    of  <math alttext="n factorial Superscript l Baseline"><mrow><mi>n</mi> <msup><mo>!</mo>
    <mi>l</mi></msup></mrow></math>  equivalent configurations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0602.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Rearranging neurons in a layer of a neural network results in equivalent
    configurations due to symmetry
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the symmetries of neuron rearrangements, nonidentifiability is
    present in other forms in certain kinds of neural networks. For example, there
    is an infinite number of equivalent configurations that for an individual ReLU
    neuron result in equivalent networks. Because an ReLU uses a piecewise linear
    function, we are free to multiply all of the incoming weights by any nonzero constant 
    <math alttext="k"><mi>k</mi></math>  while scaling all of the outgoing weights
    by  <math alttext="StartFraction 1 Over k EndFraction"><mfrac><mn>1</mn> <mi>k</mi></mfrac></math>
     without changing the behavior of the network. We leave the justification for
    this statement as an exercise for you.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, however, local minima that arise because of the nonidentifiability
    of deep neural networks are not inherently problematic. This is because all nonidentifiable
    configurations behave in an indistinguishable fashion no matter what input values
    they are fed. This means they will achieve the same error on the training, validation,
    and testing datasets. In other words, all of these models will have learned equally
    from the training data and will have identical behavior during generalization
    to unseen examples.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Instead, local minima are only problematic when they are *spurious*. A spurious
    local minimum corresponds to a configuration of weights in a neural network that
    incurs a higher error than the configuration at the global minimum. If these kinds
    of local minima are common, we quickly run into significant problems while using
    gradient-based optimization methods because we can take only local structure into
    account.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: How Pesky Are Spurious Local Minima in Deep Networks?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many years, deep learning practitioners blamed all of their troubles in
    training deep networks on spurious local minima, albeit with little evidence.
    Today, it remains an open question whether spurious local minima with a high error
    rate relative to the global minimum are common in practical deep networks. However,
    many recent studies seem to indicate that most local minima have error rates and
    generalization characteristics that are very similar to global minima.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: One way we might try to naively tackle this problem is by plotting the value
    of the error function over time as we train a deep neural network. This strategy,
    however, doesn’t give us enough information about the error surface because it
    is difficult to tell whether the error surface is “bumpy,” or whether we merely
    have a difficult time figuring out which direction we should be moving in.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: To more effectively analyze this problem, Goodfellow et al. (a team of researchers
    collaborating between Google and Stanford) published a paper in 2014 that attempted
    to separate these two potential confounding factors.^([2](ch06.xhtml#idm45934168861872))
    Instead of analyzing the error function over time, they cleverly investigated
    what happens on the error surface between a randomly initialized parameter vector
    and a successful final solution by using linear interpolation. So, given a randomly
    initialized parameter vector  <math alttext="theta Subscript i"><msub><mi>θ</mi>
    <mi>i</mi></msub></math>  and stochastic gradient descent (SGD) solution  <math
    alttext="theta Subscript f"><msub><mi>θ</mi> <mi>f</mi></msub></math> , we aim
    to compute the error function at every point along the linear interpolation  <math
    alttext="theta Subscript alpha Baseline equals alpha dot theta Subscript f Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis dot theta Subscript i"><mrow><msub><mi>θ</mi>
    <mi>α</mi></msub> <mo>=</mo> <mi>α</mi> <mo>·</mo> <msub><mi>θ</mi> <mi>f</mi></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow>
    <mo>·</mo> <msub><mi>θ</mi> <mi>i</mi></msub></mrow></math> .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更有效地分析这个问题，Goodfellow等人（谷歌和斯坦福大学的研究人员合作组成的团队）在2014年发表了一篇论文，试图分离这两个潜在的混淆因素。[^2]
    他们巧妙地研究了在随机初始化参数向量和成功的最终解之间的误差曲面上发生了什么，而不是随时间分析误差函数。因此，给定一个随机初始化的参数向量θi和随机梯度下降（SGD）解θf，我们的目标是计算沿着线性插值θα的每个点处的误差函数。
- en: They wanted to investigate whether local minima would hinder our gradient-based
    search method even if we knew which direction to move in. They showed that for
    a wide variety of practical networks with different types of neurons, the direct
    path between a randomly initialized point in the parameter space and a stochastic
    gradient descent solution isn’t plagued with troublesome local minima.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 他们想要调查即使我们知道要移动的方向，局部最小值是否会阻碍基于梯度的搜索方法。他们表明，对于各种具有不同类型神经元的实际网络，参数空间中随机初始化点和随机梯度下降解之间的直接路径并不受困扰。
- en: 'We can even demonstrate this ourselves using the feed-forward ReLU network
    we built in [Chapter 5](ch05.xhtml#neural_networks_in_pytorch). Using a checkpoint
    file that we saved while training our original feed-forward network, we can reinstantiate
    the model using `load_state_dict` and `torch.load`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用我们在[第5章](ch05.xhtml#neural_networks_in_pytorch)中构建的前馈ReLU网络来演示这一点。使用我们在训练原始前馈网络时保存的检查点文件，我们可以使用`load_state_dict`和`torch.load`重新实例化模型：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In PyTorch, we cannot access a model’s parameters directly since the `model.parameters()`
    method returns a generator that provides only a *copy* of the parameters. To modify
    a model’s parameters, we use `torch.load` to read the state dictionary containing
    the parameter values from the file, and then use `load_state_dict` to set the
    model’s parameters with these values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们无法直接访问模型的参数，因为`model.parameters()`方法返回一个仅提供参数*副本*的生成器。要修改模型的参数，我们使用`torch.load`从文件中读取包含参数值的状态字典，然后使用`load_state_dict`将模型的参数设置为这些值。
- en: 'Instead of using `torch.load` to load the state dictionary from a file, we
    can also access the state dictionary from a model itself using the `state_dict`
    method:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`state_dict`方法从模型本身访问状态字典，而不是使用`torch.load`从文件加载状态字典：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we need to use the `copy.deepcopy` method to copy a dictionary with
    its values. Just setting `opt_state_dict = model.state_dict()` would result in
    a shallow copy, and `opt_state_dict` would be changed when we load our model with
    interpolated parameters later.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要使用`copy.deepcopy`方法来复制带有其值的字典。只设置`opt_state_dict = model.state_dict()`会导致浅复制，并且当我们稍后加载具有插值参数的模型时，`opt_state_dict`会发生更改。
- en: 'Next, we instantiate a new model with randomly initialized parameters and save
    those parameters as `rand_state_dict`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化一个具有随机初始化参数的新模型，并将这些参数保存为`rand_state_dict`：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With these two networks appropriately initialized, we can now construct the
    linear interpolation using the mixing parameters `alpha` and `beta`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这两个网络适当初始化后，我们现在可以使用混合参数`alpha`和`beta`构建线性插值：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will compute the average loss over the entire test dataset using the
    model with the interpolated parameters.  For convenience, let’s create a function
    for inference:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用具有插值参数的模型计算整个测试数据集上的平均损失。为了方便起见，让我们创建一个推理函数：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally,  we can vary the value of `alpha` to understand how the error surface
    changes as we traverse the line between the randomly initialized point and the
    final SGD solution:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以改变`alpha`的值，以了解在穿过随机初始化点和最终SGD解之间的线路时，误差曲面如何变化：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This creates [Figure 6-3](#cost_function_of_a_three_layer_feedforward_network),
    which we can inspect ourselves. In fact, if we run this experiment over and over
    again, we find that there are no truly troublesome local minima that would get
    us stuck. It seems that the true struggle of gradient descent isn’t the existence
    of troublesome local minima, but instead is that we have a tough time finding
    the appropriate direction to move in. We’ll return to this thought a little later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了[图6-3](#cost_function_of_a_three_layer_feedforward_network)，我们可以自己检查。实际上，如果我们一遍又一遍地运行这个实验，我们会发现没有真正困扰的局部最小值会让我们陷入困境。看来梯度下降的真正挑战不是存在困扰的局部最小值，而是我们很难找到适当的移动方向。我们稍后会回到这个想法。
- en: '![](Images/fdl2_0603.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0603.png)'
- en: Figure 6-3\. The cost function of a three-layer feed-forward network as we linearly
    interpolate on the line connecting a randomly initialized parameter vector and
    an SGD solution
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 三层前馈网络的成本函数，当我们在线性插值连接随机初始化的参数向量和随机梯度下降解时
- en: Flat Regions in the Error Surface
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 误差表面中的平坦区域
- en: Although  it seems that our analysis is devoid of troublesome local minimum,
    we do notice a peculiar flat region where the gradient approaches zero when we
    get to approximately `alpha=1`. This point is not a local minima, so it is unlikely
    to get us completely stuck, but it seems like the zero gradient might slow down
    learning if we are unlucky enough to encounter it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的分析似乎没有令人头疼的局部最小值，但我们注意到一个奇怪的平坦区域，在大约`alpha=1`时梯度接近零。这个点不是局部最小值，所以不太可能完全卡住我们，但如果我们不走运遇到它，零梯度可能会减慢学习速度。
- en: More generally, given an arbitrary function, a point at which the gradient is
    the zero vector is called a *critical point*. Critical points come in various
    flavors. We’ve already talked about local minima. It’s also not hard to imagine
    their counterparts, the *local maxima*, which don’t really pose much of an issue
    for SGD. But then there are these strange critical points that lie somewhere in
    between. These “flat” regions that are potentially pesky but not necessarily deadly
    are called *saddle points*. It turns out that as our function has more and more
    dimensions (i.e., we have more and more parameters in our model), saddle points
    are exponentially more likely than local minima. Let’s try to intuit why.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，对于任意函数，梯度为零向量的点被称为*临界点*。临界点有各种不同的类型。我们已经讨论过局部最小值。很容易想象它们的对应物，即*局部最大值*，对于随机梯度下降并不构成太大问题。但是还有这些奇怪的临界点，它们位于两者之间。这些“平坦”区域可能会让人头疼，但不一定致命，被称为*鞍点*。事实证明，随着我们的函数具有越来越多的维度（即我们的模型中有越来越多的参数），鞍点比局部最小值更有可能。让我们试着直观地理解为什么。
- en: For a 1D cost function, a critical point can take one of three forms, as shown
    in [Figure 6-4](#analyzing_a_critical_point).  Loosely, let’s assume each of these
    three configurations is equally likely. This means that given a random critical
    point in a random 1D function, it has one-third probability of being a local minimum.
    This means that if we have a total of  <math alttext="k"><mi>k</mi></math>  critical
    points, we can expect to have a total of  <math alttext="StartFraction k Over
    3 EndFraction"><mfrac><mi>k</mi> <mn>3</mn></mfrac></math>  local minima.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个一维成本函数，临界点可以采取三种形式，如[图6-4](#analyzing_a_critical_point)所示。粗略地说，让我们假设这三种配置是等可能的。这意味着在一个随机的一维函数中，给定一个随机的临界点，它有三分之一的概率是局部最小值。这意味着如果我们总共有
    <math alttext="k"><mi>k</mi></math> 个临界点，我们可以期望总共有 <math alttext="StartFraction
    k Over 3 EndFraction"><mfrac><mi>k</mi> <mn>3</mn></mfrac></math> 个局部最小值。
- en: '![](Images/fdl2_0604.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0604.png)'
- en: Figure 6-4\. Analyzing a critical point along a single dimension
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 沿单个维度分析临界点
- en: We can also extend this to higher dimensional functions. Consider a cost function
    operating in a <math alttext="d"><mi>d</mi></math> -dimensional space. Let’s take
    an arbitrary critical point. It turns out that figuring out if this point is a
    local minimum, local maximum, or a saddle point is a little bit trickier than
    in the one-dimensional case. Consider the error surface in [Figure 6-5](#saddle_point_over_a_two_dimensional_error_surface).
    Depending on how you slice the surface (from A to B or from C to D), the critical
    point looks like either a minimum or a maximum. In reality, it’s neither. It’s
    a more complex type of saddle point.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将这个扩展到更高维度的函数。考虑一个在 <math alttext="d"><mi>d</mi></math> 维空间中运行的成本函数。让我们取一个任意的临界点。事实证明，确定这一点是局部最小值、局部最大值还是鞍点比一维情况要棘手一些。考虑[图6-5](#saddle_point_over_a_two_dimensional_error_surface)中的误差表面。根据你如何切割表面（从A到B还是从C到D），临界点看起来像最小值或最大值。实际上，它既不是最小值也不是最大值。它是一种更复杂的鞍点。
- en: '![](Images/fdl2_0605.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0605.png)'
- en: Figure 6-5\. A saddle point over a 2D error surface
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5\. 二维误差表面上的一个鞍点
- en: In general, in a  <math alttext="d"><mi>d</mi></math> -dimensional parameter
    space, we can slice through a critical point on  <math alttext="d"><mi>d</mi></math>
     different axes. A critical point can be a local minimum only if it appears as
    a local minimum in every single one of the  <math alttext="d"><mi>d</mi></math>
     1D subspaces. Using the fact that a critical point can come in one of three different
    flavors in a one-dimensional subspace, we realize that the probability that a
    random critical point is in a random function is  <math alttext="StartFraction
    1 Over 3 Superscript d Baseline EndFraction"><mfrac><mn>1</mn> <msup><mn>3</mn>
    <mi>d</mi></msup></mfrac></math> . This means that a random function with  <math
    alttext="k"><mi>k</mi></math>  critical points has an expected number of  <math
    alttext="StartFraction k Over 3 Superscript d Baseline EndFraction"><mfrac><mi>k</mi>
    <msup><mn>3</mn> <mi>d</mi></msup></mfrac></math>  local minima. In other words,
    as the dimensionality of our parameter space increases, local minima become exponentially
    more rare. A more rigorous treatment of this topic is outside the scope of this
    book, but was explored more extensively by Dauphin et al. in 2014.^([3](ch06.xhtml#idm45934164995408))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在一个 <math alttext="d"><mi>d</mi></math> 维参数空间中，我们可以通过 <math alttext="d"><mi>d</mi></math>
    个不同的轴切过一个临界点。一个临界点只有在每一个 <math alttext="d"><mi>d</mi></math> 个一维子空间中都出现为局部最小值时才能成为局部最小值。利用一个临界点在一维子空间中有三种不同类型的事实，我们意识到一个随机临界点在随机函数中的概率是
    <math alttext="StartFraction 1 Over 3 Superscript d Baseline EndFraction"><mfrac><mn>1</mn>
    <msup><mn>3</mn> <mi>d</mi></msup></mfrac></math> 。这意味着一个具有 <math alttext="k"><mi>k</mi></math>
    个临界点的随机函数有望有 <math alttext="StartFraction k Over 3 Superscript d Baseline EndFraction"><mfrac><mi>k</mi>
    <msup><mn>3</mn> <mi>d</mi></msup></mfrac></math> 个局部最小值。换句话说，随着参数空间的维度增加，局部最小值变得越来越稀有。对这个主题的更严格的处理超出了本书的范围，但在2014年由Dauphin等人进行了更深入的探讨。^([3](ch06.xhtml#idm45934164995408))
- en: So what does this mean for optimizing deep learning models? For stochastic gradient
    descent, it’s still unclear. It seems like these flat segments of the error surface
    are pesky but ultimately don’t prevent stochastic gradient descent from converging
    to a good answer. However, it does pose serious problems for methods that attempt
    to directly solve for a point where the gradient is zero. This has been a major
    hindrance to the usefulness of certain second-order optimization methods for deep
    learning models, which we will discuss later.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这对于优化深度学习模型意味着什么？对于随机梯度下降，目前还不清楚。看起来这些错误表面的平坦段是棘手的，但最终并不会阻止随机梯度下降收敛到一个好的答案。然而，对于试图直接解决梯度为零点的方法，这对于某些二阶优化方法在深度学习模型中的有用性构成了严重问题，我们将在后面讨论。
- en: When the Gradient Points in the Wrong Direction
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当梯度指向错误方向时
- en: Upon analyzing the error surfaces of deep networks, it seems like the most critical
    challenge to optimizing deep networks is finding the correct trajectory to move
    in. It’s no surprise, however, that this is a major challenge when we look at
    what happens to the error surface around a local minimum. As an example, we consider
    an error surface defined over a 2D parameter space, as shown in [Figure 6-6](#local_information_encoded_by_the_gradient).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析深度网络的错误表面时，似乎优化深度网络最关键的挑战是找到正确的移动轨迹。然而，当我们看看局部最小值周围的错误表面发生了什么时，这并不令人惊讶。例如，我们考虑在2D参数空间上定义的错误表面，如[图6-6](#local_information_encoded_by_the_gradient)所示。
- en: '![](Images/fdl2_0606.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0606.png)'
- en: Figure 6-6\. Local information encoded by the gradient usually does not corroborate
    the global structure of the error surface
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6。梯度编码的局部信息通常不符合错误表面的全局结构
- en: Revisiting the contour diagrams we explored in [Chapter 4](ch04.xhtml#training_feed_forward),
    notice that the gradient isn’t usually a very good indicator of the good trajectory.
    Specifically, only when the contours are perfectly circular does the gradient
    always point in the direction of the local minimum. However, if the contours are
    extremely elliptical (as is usually the case for the error surfaces of deep networks),
    the gradient can be as inaccurate as 90 degrees away from the correct direction.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 重新审视我们在[第4章](ch04.xhtml#training_feed_forward)中探索的等高线图，注意梯度通常不是一个很好的良好轨迹指示器。具体来说，只有当等高线是完全圆形时，梯度才总是指向局部最小值的方向。然而，如果等高线极端椭圆形（通常是深度网络的错误表面的情况），梯度可能与正确方向相差90度。
- en: We extend this analysis to an arbitrary number of dimensions using some mathematical
    formalism. For every weight <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
     in the parameter space, the gradient computes the value of  <math alttext="StartFraction
    normal partial-differential upper E Over normal partial-differential w Subscript
    i Baseline EndFraction"><mfrac><mrow><mi>∂</mi><mi>E</mi></mrow> <mrow><mi>∂</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></mfrac></math> , or how the value of the error changes
    as we change the value of  <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    . Taken together over all weights in the parameter space, the gradient gives us
    the direction of steepest descent. The general problem with taking a significant
    step in this direction, however, is that the gradient could be changing under
    our feet as we move! We demonstrate this simple fact in [Figure 6-7](#direction_of_gradient_changes).
    Going back to the 2D example, if our contours are perfectly circular and we take
    a big step in the direction of the steepest descent, the gradient doesn’t change
    direction as we move. However, this is not the case for highly elliptical contours.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一些数学形式将这种分析扩展到任意维度。对于参数空间中的每个权重<math alttext="w下标i"><msub><mi>w</mi> <mi>i</mi></msub></math>，梯度计算<math
    alttext="StartFraction normal partial-differential upper E Over normal partial-differential
    w下标i Baseline EndFraction"><mfrac><mrow><mi>∂</mi><mi>E</mi></mrow> <mrow><mi>∂</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></mfrac></math>的值，或者当我们改变<math alttext="w下标i"><msub><mi>w</mi>
    <mi>i</mi></msub></math>的值时，错误值的变化。在参数空间中的所有权重上综合考虑，梯度给出了最陡下降的方向。然而，在这个方向上迈出重要步骤的一般问题是，当我们移动时，梯度可能在我们脚下改变！我们在[图6-7](#direction_of_gradient_changes)中演示了这个简单事实。回到2D示例，如果我们的等高线是完全圆形的，并且我们朝最陡下降的方向迈出一大步，梯度在我们移动时不会改变方向。然而，对于高度椭圆形的等高线，情况并非如此。
- en: '![](Images/fdl2_0607.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0607.png)'
- en: Figure 6-7\. The direction of the gradient changes as we move along the direction
    of steepest descent, as determined from a starting point; the gradient vectors
    are normalized to identical length to emphasize the change in direction of the
    gradient vector
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7。当我们沿着最陡下降的方向移动时，梯度的方向会改变，这是从一个起始点确定的；梯度向量被归一化为相同长度，以强调梯度向量方向的变化
- en: More generally, we can quantify how the gradient changes under our feet as we
    move in a certain direction by computing second derivatives. Specifically, we
    want to measure  <math alttext="StartFraction normal partial-differential left-parenthesis
    normal partial-differential upper E slash normal partial-differential w Subscript
    j Baseline right-parenthesis Over normal partial-differential w Subscript i Baseline
    EndFraction"><mfrac><mrow><mi>∂</mi><mfenced separators="" open="(" close=")"><mi>∂</mi><mi>E</mi><mo>/</mo><mi>∂</mi><msub><mi>w</mi>
    <mi>j</mi></msub></mfenced></mrow> <mrow><mi>∂</mi><msub><mi>w</mi> <mi>i</mi></msub></mrow></mfrac></math>
    , which tells us how the gradient component for  <math alttext="w Subscript j"><msub><mi>w</mi>
    <mi>j</mi></msub></math>  changes as we change the value of  <math alttext="w
    Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> . We can compile this information
    into a special matrix known as the *Hessian matrix* (***H***). And when describing
    an error surface where the gradient changes underneath our feet as we move in
    the direction of steepest descent, this matrix is said to be *ill-conditioned*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，我们可以通过计算二阶导数来量化梯度在我们移动某个方向时的变化。具体来说，我们想要测量 <math alttext="StartFraction
    normal partial-differential left-parenthesis normal partial-differential upper
    E slash normal partial-differential w Subscript j Baseline right-parenthesis Over
    normal partial-differential w Subscript i Baseline EndFraction"> <mfrac> <mrow>
    <mi>∂</mi> <mfenced separators="" open="(" close=")"> <mi>∂</mi> <mi>E</mi> <mo>/</mo>
    <mi>∂</mi> <msub> <mi>w</mi> <mi>j</mi> </msub> </mfenced> </mrow> <mrow> <mi>∂</mi>
    <msub> <mi>w</mi> <mi>i</mi> </msub> </mrow> </mfrac> </math>，这告诉我们当我们改变 <math
    alttext="w Subscript i"> <msub> <mi>w</mi> <mi>i</mi> </msub> </math> 的值时， <math
    alttext="w Subscript j"> <msub> <mi>w</mi> <mi>j</mi> </msub> </math> 的梯度分量如何变化。我们可以将这些信息编译成一个特殊的矩阵，称为*Hessian矩阵*（***H***）。当描述一个错误表面，在这个表面上，当我们沿着最陡降方向移动时，梯度在我们脚下发生变化时，这个矩阵被称为*病态*。
- en: Momentum-Based Optimization
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于动量的优化
- en: Fundamentally, the problem of an ill-conditioned Hessian matrix manifests itself
    in the form of gradients that fluctuate wildly. As a result, one popular mechanism
    for dealing with ill-conditioning bypasses the computation of the Hessian, and
    instead, focuses on how to cancel out these fluctuations over the duration of
    training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，病态Hessian矩阵的问题表现为梯度剧烈波动。因此，处理病态的一种流行机制是绕过Hessian的计算，而是专注于如何在训练期间抵消这些波动。
- en: One way to think about how we might tackle this problem is by investigating
    how a ball rolls down a hilly surface. Driven by gravity, the ball eventually
    settles into a minimum on the surface, but for some reason, it doesn’t suffer
    from the wild fluctuations and divergences that happen during gradient descent.
    Why is this the case? Unlike in stochastic gradient descent (which uses only the
    gradient), there are two major components that determine how a ball rolls down
    an error surface. The first, which we already model in SGD as the gradient, is
    what we commonly refer to as acceleration. But acceleration does not single-handedly
    determine the ball’s movements. Instead, its motion is more directly determined
    by its velocity. Acceleration indirectly changes the ball’s position only by modifying
    its velocity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过研究球如何沿着山坡滚动来思考如何解决这个问题。受重力驱动，球最终会落入表面上的最小值，但出于某种原因，在梯度下降过程中不会出现野蛮波动和发散。为什么会这样呢？与随机梯度下降不同（仅使用梯度），有两个主要组成部分决定了球如何沿着误差表面滚动。第一个部分，我们在随机梯度下降中已经建模为梯度，通常称为加速度。但加速度并不能单独决定球的运动。相反，它的运动更直接地由其速度决定。加速度只通过修改速度间接改变球的位置。
- en: 'Velocity-driven motion is desirable because it counteracts the effects of a
    wildly fluctuating gradient by smoothing the ball’s trajectory over its history.
    Velocity serves as a form of memory, and this allows us to more effectively accumulate
    movement in the direction of the minimum while canceling out oscillating accelerations
    in orthogonal directions. Our goal, then, is to somehow generate an analog for
    velocity in our optimization algorithm. We can do this by keeping track of an
    *exponentially weighted decay* of past gradients. The premise is simple: every
    update is computed by combining the update in the last iteration with the current
    gradient. Concretely, we compute the change in the parameter vector as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于速度的运动是可取的，因为它通过在历史上平滑球的轨迹来抵消梯度的剧烈波动。速度作为一种记忆形式，这使我们能够更有效地在最小值方向上积累运动，同时抵消正交方向上的振荡加速度。因此，我们的目标是在我们的优化算法中生成速度的类似物。我们可以通过跟踪过去梯度的*指数加权衰减*来实现这一点。这个前提很简单：每次更新都是通过将上一次迭代的更新与当前梯度相结合来计算的。具体来说，我们计算参数向量的变化如下：
- en: <math alttext="bold v Subscript i Baseline equals m bold v Subscript i minus
    1 Baseline minus epsilon bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>m</mi> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mi>ϵ</mi> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals m bold v Subscript i minus
    1 Baseline minus epsilon bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>m</mi> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mi>ϵ</mi> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    plus bold v Subscript i"><mrow><msub><mi>θ</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>θ</mi>
    <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    plus bold v Subscript i"><mrow><msub><mi>θ</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>θ</mi>
    <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub></mrow></math>
- en: We use the momentum hyperparameter <math alttext="m"><mi>m</mi></math> to determine
    what fraction of the previous velocity to retain in the new update, and add this
    “memory” of past gradients to our current gradient. This approach is commonly
    referred to as *momentum*.^([4](ch06.xhtml#idm45934167589056)) Because the momentum
    term increases the step size we take, using momentum may require a reduced learning
    rate compared to vanilla stochastic gradient descent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用动量超参数<m> m </m>来确定新更新中保留多少比例的先前速度，并将过去梯度的“记忆”添加到当前梯度中。这种方法通常称为*动量*。因为动量项增加了我们采取的步长，使用动量可能需要比普通随机梯度下降更低的学习率。
- en: 'To better visualize how momentum works, we’ll explore a toy example. Specifically,
    we’ll investigate how momentum affects updates during a *random walk*. A random
    walk is a succession of randomly chosen steps. In our example, we’ll imagine a
    particle on a line that, at every time interval, randomly picks a step size between
    –10 and 10 and takes a moves in that direction. This is simply expressed as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解动量的工作原理，我们将探讨一个玩具示例。具体来说，我们将研究动量如何影响*随机漫步*中的更新。随机漫步是一系列随机选择的步骤。在我们的示例中，我们将想象一条线上的一个粒子，在每个时间间隔内，随机选择一个步长在-10到10之间，并朝着那个方向移动。这可以简单地表示为：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We’ll then simulate what happens when we use a slight modification of momentum
    (i.e., the standard exponentially weighted moving average algorithm) to smooth
    our choice of step at every time interval. Again, we can concisely express this
    as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将模拟当我们使用动量的轻微修改（即标准的指数加权移动平均算法）来在每个时间间隔平滑我们的步长选择时会发生什么。同样，我们可以简洁地表示为：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results, as we vary the momentum from 0 to 1, are quite staggering. Momentum
    significantly reduces the volatility of updates. The larger the momentum, the
    less responsive we are to new updates (e.g., a large inaccuracy on the first estimation
    of trajectory propagates for a significant period of time). We summarize the results
    of our toy experiment in [Figure 6-8](#momentum_smooths_volatility).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将动量从0变化到1时，结果令人震惊。动量显著减少了更新的波动性。动量越大，我们对新的更新越不敏感（例如，在轨迹第一次估计时的大误差会在相当长的时间内传播）。我们总结了我们玩具实验的结果在[图6-8](#momentum_smooths_volatility)中。
- en: '![](Images/fdl2_0608.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0608.png)'
- en: Figure 6-8\. Momentum smooths volatility in the step sizes during a random walk
    using an exponentially weighted moving average
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8。动量通过指数加权移动平均值平滑随机漫步中的步长波动
- en: 'To investigate how momentum actually affects the training of feed-forward neural
    networks, we can retrain our trusty MNIST feed-forward network with a PyTorch
    momentum optimizer. In this case, we can get away with using the same learning
    rate (0.01) with a typical momentum of 0.9:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 研究动量如何实际影响前馈神经网络的训练，我们可以使用 PyTorch 动量优化器重新训练我们可靠的 MNIST 前馈网络。在这种情况下，我们可以使用相同的学习率（0.01）和典型的动量0.9：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that when we create a PyTorch optimizer, we need to pass in `model.parameters()`.
    The resulting speedup is staggering. We display how the cost function changes
    over time by comparing the visualizations in [Figure 6-9](#comparing_training_a_feedforward_network).
    The figure demonstrates that to achieve a cost of 0.1 without momentum (right)
    requires nearly 18,000 steps (minibatches), whereas with momentum (left), we require
    just over 2,000.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当我们创建一个 PyTorch 优化器时，我们需要传入 `model.parameters()`。结果的加速效果令人震惊。我们通过比较[图6-9](#comparing_training_a_feedforward_network)中的可视化来展示成本函数随时间的变化。该图表明，要在没有动量的情况下（右侧）实现成本为0.1，需要近18,000步（小批量），而使用动量（左侧）则只需要超过2,000步。
- en: '![](Images/fdl2_0609.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0609.png)'
- en: Figure 6-9\. Comparing training a feed-forward network with (right) and without
    (left) momentum demonstrates a massive decrease in training time
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9。比较使用（右侧）和不使用（左侧）动量训练前馈网络，显示了训练时间的大幅减少
- en: Recently, more work has explored how the classical momentum technique can be
    improved. Sutskever et al. in 2013 proposed an alternative called Nesterov momentum,
    which computes the gradient on the error surface at <math alttext="theta plus
    bold v Subscript i minus 1"><mrow><mi>θ</mi> <mo>+</mo> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
     during the velocity update instead of at <math alttext="theta"><mi>θ</mi></math>
    .^([5](ch06.xhtml#idm45934167566352)) This subtle difference seems to allow Nesterov
    momentum to change its velocity in a more responsive way. It’s been shown that
    this method has clear benefits in batch gradient descent (convergence guarantees
    and the ability to use a higher momentum for a given learning rate as compared
    to classical momentum), but it’s not entirely clear whether this is true for the
    more stochastic minibatch gradient descent used in most deep learning optimization
    approaches.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，有更多的工作探讨了如何改进经典的动量技术。2013年，Sutskever等人提出了一种称为 Nesterov 动量的替代方法，它在速度更新期间计算误差表面上的梯度，而不是在
    θ 处。这种微妙的差异似乎使 Nesterov 动量能够以更具响应性的方式改变其速度。已经证明，这种方法在批量梯度下降中具有明显的好处（收敛保证和相对于经典动量可以使用更高的动量来实现给定学习率），但对于大多数深度学习优化方法中使用的更随机的小批量梯度下降是否也适用尚不清楚。
- en: 'Nerestov momentum is supported in PyTorch out-of-the-box by setting the `nesterov`
    argument:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 `nesterov` 参数，PyTorch 默认支持 Nerestov 动量：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A Brief View of Second-Order Methods
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二阶方法的简要概述
- en: As we discussed, computing the Hessian is a computationally difficult task,
    and momentum afforded us significant speedup without having to worry about it
    altogether. Several second-order methods, however, have been researched over the
    past several years that attempt to approximate the Hessian directly. For completeness,
    we give a broad overview of these methods, but a detailed treatment is beyond
    the scope of this text.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，计算海森矩阵是一个计算上困难的任务，而动量使我们在不必担心它的情况下获得了显著的加速。然而，在过去几年中，已经研究了几种试图直接逼近海森矩阵的二阶方法。为了完整起见，我们对这些方法进行了广泛概述，但详细讨论超出了本文的范围。
- en: The first is *conjugate gradient descent*, which arises out of attempting to
    improve on a naive method of steepest descent. In steepest descent, we compute
    the direction of the gradient and then line search to find the minimum along that
    direction. We jump to the minimum and then recompute the gradient to determine
    the direction of the next line search. It turns out that this method ends up zigzagging
    a significant amount, as shown in [Figure 6-10](#fig0610), because each time we
    move in the direction of steepest descent, we undo a little bit of progress in
    another direction. A remedy to this problem is moving in a *conjugate direction*
    relative to the previous choice instead of the direction of steepest descent.
    The conjugate direction is chosen by using an indirect approximation of the Hessian
    to linearly combine the gradient and our previous direction. With a slight modification,
    this method generalizes to the nonconvex error surfaces we find in deep networks.^([6](ch06.xhtml#idm45934167542112))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度网络优化的一个重大突破是学习率调整的出现。学习率调整背后的基本概念是，在学习的过程中适当地修改最佳学习率，以实现良好的收敛性质。在接下来的几节中，我们将讨论AdaGrad、RMSProp和Adam，这三种最流行的自适应学习率算法之一。
- en: '![](Images/fdl2_0610.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0610.png)'
- en: Figure 6-10\. The method of steepest descent often zigzags; conjugate descent
    attempts to remedy this issue
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 在其原始形式中，BFGS具有显著的内存占用，但最近的工作已经产生了一个更节省内存的版本，称为*L-BFGS*。[8]
- en: An alternative optimization algorithm known as the *Broyden–Fletcher–Goldfarb–Shanno*
    (BFGS) algorithm attempts to compute the inverse of the Hessian matrix iteratively
    and use the inverse Hessian to more effectively optimize the parameter vector.^([7](ch06.xhtml#idm45934167533568))
    In its original form, BFGS has a significant memory footprint, but recent work
    has produced a more memory-efficient version known as *L-BFGS*.^([8](ch06.xhtml#idm45934167530400))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代优化算法被称为*Broyden–Fletcher–Goldfarb–Shanno*（BFGS）算法，试图迭代计算Hessian矩阵的逆，并使用逆Hessian更有效地优化参数向量。[7]
- en: In general, while these methods hold some promise, second-order methods are
    still an area of active research and are unpopular among practitioners. PyTorch
    does, however, support L-BFGS as well as other second-order methods, such as Averaged
    Stochastic Gradient Descent, for your own experimentation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，虽然这些方法有一些希望，但二阶方法仍然是一个活跃研究领域，且在实践者中不受欢迎。然而，PyTorch支持L-BFGS以及其他二阶方法，如平均随机梯度下降，供您自行尝试。
- en: Learning Rate Adaptation
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个算法是AdaGrad，它试图通过历史梯度的累积随时间调整全局学习率，由Duchi等人在2011年首次提出。具体来说，我们跟踪每个参数的学习率。这个学习率与所有参数历史梯度的平方和的平方根成反比地缩放。
- en: As we have discussed previously, another major challenge for training deep networks
    is appropriately selecting the learning rate. Choosing the correct learning rate
    has long been one of the most troublesome aspects of training deep networks because
    it has a major impact on a network’s performance. A learning rate that is too
    small doesn’t learn quickly enough, but a learning rate that is too large may
    have difficulty converging as we approach a local minimum or region that is ill-conditioned.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是*共轭梯度下降*，它起源于试图改进最陡下降的朴素方法。在最陡下降中，我们计算梯度的方向，然后进行线性搜索以找到沿该方向的最小值。我们跳到最小值，然后重新计算梯度以确定下一次线性搜索的方向。事实证明，这种方法最终会出现相当多的曲折，如[图6-10](#fig0610)所示，因为每次我们沿最陡下降的方向移动时，我们会在另一个方向上撤销一点进展。解决这个问题的方法是相对于先前选择的方向移动*共轭方向*，而不是最陡下降的方向。通过使用Hessian的间接近似来选择共轭方向，将梯度和我们先前的方向线性组合。稍作修改，这种方法可以推广到我们在深度网络中找到的非凸误差表面。[6]
- en: One of the major breakthroughs in modern deep network optimization was the advent
    of learning rate adaption. The basic concept behind learning rate adaptation is
    that the optimal learning rate is appropriately modified over the span of learning
    to achieve good convergence properties. Over the next several sections, we’ll
    discuss AdaGrad, RMSProp, and Adam, three of the most popular adaptive learning
    rate algorithms.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调整
- en: AdaGrad—Accumulating Historical Gradients
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaGrad-累积历史梯度
- en: The first algorithm we’ll discuss is AdaGrad, which attempts to adapt the global
    learning rate over time using an accumulation of the historical gradients, first
    proposed by Duchi et al. in 2011.^([9](ch06.xhtml#idm45934166863168)) Specifically,
    we keep track of a learning rate for each parameter. This learning rate is inversely
    scaled with respect to the square root of the sum of the squares (root mean square)
    of all the parameter’s historical gradients.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-10。最陡下降法经常曲折；共轭下降试图解决这个问题
- en: 'We can express this mathematically. We initialize a gradient accumulation vector 
    <math alttext="bold r 0 equals bold 0"><mrow><msub><mi>𝐫</mi> <mn>0</mn></msub>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> . At every step, we accumulate
    the square of all the gradient parameters as follows (where the <math alttext="circled-dot"><mo>⊙</mo></math>
     operation is element-wise tensor multiplication):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们之前讨论过的，训练深度网络的另一个主要挑战是适当选择学习率。选择正确的学习率长期以来一直是训练深度网络中最棘手的方面之一，因为它对网络的性能有重大影响。学习率太小则学习速度不够快，但学习率太大可能在接近局部最小值或病态区域时难以收敛。我们可以用数学方式表达这一点。我们初始化一个梯度累积向量<math
    alttext="bold r 0 equals bold 0"><mrow><msub><mi>𝐫</mi> <mn>0</mn></msub> <mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow> </math>。在每一步中，我们按如下方式累积所有梯度参数的平方（其中<math
    alttext="circled-dot"><mo>⊙</mo></math>操作是逐元素张量乘法）:'
- en: <math alttext="bold r Subscript i Baseline equals bold r Subscript i minus 1
    Baseline plus bold g Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐫</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>𝐫</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold r Subscript i Baseline equals bold r Subscript i minus 1
    Baseline plus bold g Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐫</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>𝐫</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
- en: 'Then we compute the update as usual, except our global learning rate <math
    alttext="epsilon"><mi>ϵ</mi></math>  is divided by the square root of the gradient
    accumulation vector:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们像往常一样计算更新，只是我们的全局学习率<math alttext="epsilon"><mi>ϵ</mi></math>被梯度累积向量的平方根除以：
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold r Subscript
    i Baseline EndRoot EndFraction circled-dot bold g"><mrow><msub><mi>θ</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>ϵ</mi> <mrow><mi>δ</mi><mo>⊕</mo><msqrt><msub><mi>𝐫</mi>
    <mi>i</mi></msub></msqrt></mrow></mfrac> <mo>⊙</mo> <mi>𝐠</mi></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold r Subscript
    i Baseline EndRoot EndFraction circled-dot bold g"><mrow><msub><mi>θ</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>ϵ</mi> <mrow><mi>δ</mi><mo>⊕</mo><msqrt><msub><mi>𝐫</mi>
    <mi>i</mi></msub></msqrt></mrow></mfrac> <mo>⊙</mo> <mi>𝐠</mi></mrow></math>
- en: 'Note that we add a tiny number  <math alttext="delta"><mi>δ</mi></math>  (~
    <math alttext="10 Superscript negative 7"><msup><mn>10</mn> <mrow><mo>-</mo><mn>7</mn></mrow></msup></math>
    ) to the denominator to prevent division by zero. Also, the division and addition
    operations are broadcast to the size of the gradient accumulation vector and applied
    element-wise. In PyTorch, a built-in optimizer allows for easily utilizing AdaGrad
    as a learning algorithm:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在分母中添加一个微小数<math alttext="delta"><mi>δ</mi></math>（~ <math alttext="10
    Superscript negative 7"><msup><mn>10</mn> <mrow><mo>-</mo><mn>7</mn></mrow></msup></math>）以防止除以零。此外，除法和加法操作被广播到梯度累积向量的大小，并逐元素应用。在PyTorch中，内置优化器允许轻松地将AdaGrad用作学习算法：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The only hitch is that in PyTorch, the  <math alttext="delta"><mi>δ</mi></math>
     and initial gradient accumulation vector are rolled together into the `initial_accumulator_value` argument.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的问题是在PyTorch中，<math alttext="delta"><mi>δ</mi></math>和初始梯度累积向量被合并到`initial_accumulator_value`参数中。
- en: On a functional level, this update mechanism means that the parameters with
    the largest gradients experience a rapid decrease in their learning rates, while
    parameters with smaller gradients observe only a small decrease in their learning
    rates. The ultimate effect is that AdaGrad forces more progress in the more gently
    sloped directions on the error surface, which can help overcome ill-conditioned
    surfaces. This results in some good theoretical properties, but in practice, training
    deep learning models with AdaGrad can be somewhat problematic. Empirically, AdaGrad
    has a tendency to cause a premature drop in learning rate, and as a result doesn’t
    work particularly well for some deep models. In the next section, we’ll describe
    RMSProp, which attempts to remedy this shortcoming.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在功能级别上，这种更新机制意味着具有最大梯度的参数会快速降低其学习率，而具有较小梯度的参数只会略微降低其学习率。最终效果是AdaGrad在误差表面上更温和倾斜的方向上取得更多进展，这有助于克服病态表面。这导致了一些良好的理论性质，但在实践中，使用AdaGrad训练深度学习模型可能会有些问题。经验上，AdaGrad有导致学习率过早下降的倾向，因此对于一些深度模型效果并不特别好。在下一节中，我们将描述RMSProp，试图弥补这个缺点。
- en: RMSProp—Exponentially Weighted Moving Average of Gradients
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMSProp—梯度的指数加权移动平均
- en: While AdaGrad works well for simple convex functions, it isn’t designed to navigate
    the complex error surfaces of deep networks. Flat regions may force AdaGrad to
    decrease the learning rate before it reaches a minimum. The conclusion is that
    simply using a naive accumulation of gradients isn’t sufficient.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AdaGrad对简单的凸函数效果很好，但它并不适用于深度网络复杂的误差表面。平坦区域可能会导致AdaGrad在达到最小值之前降低学习率。结论是简单地累积梯度是不够的。
- en: 'Our solution is to bring back a concept we introduced earlier while discussing
    momentum to dampen fluctuations in the gradient. Compared to naive accumulation,
    exponentially weighted moving averages also enables us to “toss out” measurements
    that we made a long time ago. More specifically, our update to the gradient accumulation
    vector is now as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案是重新引入我们在讨论动量时介绍的一个概念，以减弱梯度中的波动。与简单累积相比，指数加权移动平均值还使我们能够“丢弃”很久以前的测量。更具体地说，我们对梯度累积向量的更新现在如下所示：
- en: <math alttext="bold r Subscript i Baseline equals rho bold r Subscript i minus
    1 Baseline plus left-parenthesis 1 minus rho right-parenthesis bold g Subscript
    i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐫</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>ρ</mi> <msub><mi>𝐫</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>ρ</mi></mfenced>
    <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold r Subscript i Baseline equals rho bold r Subscript i minus
    1 Baseline plus left-parenthesis 1 minus rho right-parenthesis bold g Subscript
    i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐫</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>ρ</mi> <msub><mi>𝐫</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>ρ</mi></mfenced>
    <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
- en: The decay factor <math alttext="rho"><mi>ρ</mi></math>  determines how long
    we keep old gradients. The smaller the decay factor, the shorter the effective
    window. Plugging this modification into AdaGrad gives rise to the RMSProp learning
    algorithm, first proposed by Geoffrey Hinton.^([10](ch06.xhtml#idm45934166835152))
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减因子<math alttext="rho"><mi>ρ</mi></math>决定我们保留旧梯度的时间。衰减因子越小，有效窗口就越短。将这种修改插入AdaGrad中产生了由Geoffrey
    Hinton首次提出的RMSProp学习算法。
- en: 'In PyTorch, we can instantiate the RMSProp optimizer with the following code.
    Note that in this case, unlike in AdaGrad, we pass in  <math alttext="delta"><mi>δ</mi></math>
     separately as the `epsilon` argument to the constructor:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们可以使用以下代码实例化RMSProp优化器。请注意，在这种情况下，与AdaGrad不同，我们将<math alttext="delta"><mi>δ</mi></math>单独作为构造函数的`epsilon`参数传入：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As the template suggests, we can utilize RMSProp with momentum (specifically
    Nerestov momentum). Overall, RMSProp has been shown to be a highly effective optimizer
    for deep neural networks, and is a default choice for many seasoned practitioners.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如模板所示，我们可以使用带有动量的RMSProp（具体来说是Nerestov动量）。总的来说，RMSProp已被证明是深度神经网络的高效优化器，并且是许多经验丰富的从业者的默认选择。
- en: Adam—Combining Momentum and RMSProp
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adam—结合动量和RMSProp
- en: Before concluding our discussion of modern optimizers, we discuss one final
    algorithm—Adam.^([11](ch06.xhtml#idm45934166819376)) Spiritually, we can think
    about Adam as a variant combination of RMSProp and momentum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束我们对现代优化器的讨论之前，我们讨论最后一个算法—Adam。从精神上讲，我们可以将Adam看作是RMSProp和动量的变体组合。
- en: 'The basic idea is as follows. We want to keep track of an exponentially weighted
    moving average of the gradient (essentially the concept of velocity in classical
    momentum), which we can express as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想如下。我们希望跟踪梯度的指数加权移动平均（基本上是经典动量中的速度概念），我们可以表示如下：
- en: '**m**[*i*] = β[1]**m**[*i* – 1] + (1 – β[1])**g**[i]'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**m**[*i*] = β[1]**m**[*i* – 1] + (1 – β[1])**g**[i]'
- en: 'This is our approximation of what we call the *first moment* of the gradient,
    or <math alttext="double-struck upper E left-bracket bold g Subscript i Baseline
    right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub>
    <mo>]</mo></mrow></math> . And similarly to RMSProp, we can maintain an exponentially
    weighted moving average of the historical gradients. This is our estimation of
    what we call the *second moment* of the gradient, or <math alttext="double-struck
    upper E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript
    i Baseline right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> :'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们对我们称之为梯度的 *第一矩* 或 <math alttext="double-struck upper E left-bracket bold
    g Subscript i Baseline right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow></math> 的近似。类似于RMSProp，我们可以维护历史梯度的指数加权移动平均值。这是我们对我们称之为梯度的
    *第二矩* 或 <math alttext="double-struck upper E left-bracket bold g Subscript i Baseline
    circled-dot bold g Subscript i Baseline right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo>
    <msub><mi>𝐠</mi> <mi>i</msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></math>
    的估计：
- en: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
- en: However, it turns out these estimations are biased relative to the real moments
    because we start off by initializing both vectors to the zero vector. In order
    to remedy this bias, we derive a correction factor for both estimations. Here,
    we describe the derivation for the estimation of the second moment. The derivation
    for the first moment, which is analogous to the derivation here, is left as an
    exercise for you.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事实证明这些估计相对于真实矩是有偏差的，因为我们从将两个向量初始化为零向量开始。为了纠正这种偏差，我们为这两个估计推导了一个校正因子。在这里，我们描述了对二阶矩的估计的推导。对于第一矩的推导，与此处类似的推导留作练习。
- en: 'We begin by expressing the estimation of the second moment in terms of all
    past gradients. This is done by simply expanding the recurrence relationship:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过所有过去梯度的估计来表达二阶矩的估计。这是通过简单地展开递归关系来完成的：
- en: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
- en: <math alttext="bold v Subscript i Baseline equals beta 2 Superscript i minus
    1 Baseline left-parenthesis 1 minus beta 2 right-parenthesis bold g 1 circled-dot
    bold g 1 plus beta 2 Superscript i minus 2 Baseline left-parenthesis 1 minus beta
    2 right-parenthesis bold g 2 circled-dot bold g 2 plus ellipsis plus left-parenthesis
    1 minus beta 2 right-parenthesis bold g Subscript i Baseline circled-dot bold
    g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>=</mo> <msubsup><mi>β</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>𝐠</mi>
    <mn>1</mn></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>β</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>2</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>𝐠</mi>
    <mn>2</mn></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mn>2</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals beta 2 Superscript i minus
    1 Baseline left-parenthesis 1 minus beta 2 right-parenthesis bold g 1 circled-dot
    bold g 1 plus beta 2 Superscript i minus 2 Baseline left-parenthesis 1 minus beta
    2 right-parenthesis bold g 2 circled-dot bold g 2 plus ellipsis plus left-parenthesis
    1 minus beta 2 right-parenthesis bold g Subscript i Baseline circled-dot bold
    g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>=</mo> <msubsup><mi>β</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>𝐠</mi>
    <mn>1</mn></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>β</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>2</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>𝐠</mi>
    <mn>2</mn></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mn>2</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
- en: <math alttext="bold v Subscript i Baseline equals left-parenthesis 1 minus beta
    2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msubsup><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup> <msup><mi>β</mi>
    <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup> <msub><mi>𝐠</mi> <mi>k</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals left-parenthesis 1 minus beta
    2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msubsup><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup> <msup><mi>β</mi>
    <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup> <msub><mi>𝐠</mi> <mi>k</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub></mrow></math>
- en: 'We can then take the expected value of both sides to determine how our estimation
    <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>]</mo></mrow></math>  compares to the real value of <math alttext="double-struck
    upper E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript
    i Baseline right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> :'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以取两边的期望值，以确定我们的估计 <math alttext="double-struck upper E left-bracket bold
    v Subscript i Baseline right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow></math> 如何与 <math alttext="double-struck upper
    E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript i Baseline
    right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi> <mi>i</msub> <mo>⊙</mo>
    <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> 的真实值相比：
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket equals double-struck upper E left-bracket left-parenthesis 1 minus
    beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k Baseline right-bracket"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mfenced separators=""
    open="[" close="]"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>i</mi></msubsup> <msup><mi>β</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup>
    <msub><mi>𝐠</mi> <mi>k</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub></mfenced></mrow></math>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket equals double-struck upper E left-bracket left-parenthesis 1 minus
    beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k Baseline right-bracket"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mfenced separators=""
    open="[" close="]"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>i</mi></msubsup> <msup><mi>β</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup>
    <msub><mi>𝐠</mi> <mi>k</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub></mfenced></mrow></math>
- en: 'We can also assume that <math alttext="double-struck upper E left-bracket bold
    g Subscript k Baseline circled-dot bold g Subscript k Baseline right-bracket almost-equals
    double-struck upper E left-bracket bold g Subscript i Baseline almost-equals bold
    g Subscript i Baseline right-bracket"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi>
    <mi>k</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub> <mo>]</mo></mrow>
    <mo>≈</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>≈</mo>
    <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></mrow></math> because even
    if the second moment of the gradient has changed since a historical value, <math
    alttext="beta 2"><msub><mi>β</mi> <mn>2</mn></msub></math> should be chosen so
    that the old second moments of the gradients are essentially decayed out of relevancy.
    As a result, we can make the following simplification:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以假设 <math alttext="double-struck upper E left-bracket bold g Subscript k
    Baseline circled-dot bold g Subscript k Baseline right-bracket almost-equals double-struck
    upper E left-bracket bold g Subscript i Baseline almost-equals bold g Subscript
    i Baseline right-bracket"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub> <mo>]</mo></mrow> <mo>≈</mo> <mi>𝔼</mi>
    <mrow><mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>≈</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow></mrow></math> 因为即使梯度的二阶矩相对于历史值发生了变化，<math
    alttext="beta 2"><msub><mi>β</mi> <mn>2</mn></msub></math> 应该被选择，以便旧的梯度二阶矩基本上衰减到不相关。因此，我们可以进行以下简化：
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript
    i Endscripts beta Superscript i minus k"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>≈</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup>
    <msup><mi>β</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></math>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript
    i Endscripts beta Superscript i minus k"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>≈</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup>
    <msup><mi>β</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></math>
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta Subscript 2 Sub Superscript i Subscript Baseline right-parenthesis"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>]</mo></mrow> <mo>≈</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi>
    <msup><mn>2</mn> <mi>i</mi></msup></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta Subscript 2 Sub Superscript i Subscript Baseline right-parenthesis"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>]</mo></mrow> <mo>≈</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi>
    <msup><mn>2</mn> <mi>i</mi></msup></msub> <mo>)</mo></mrow></mrow></math>
- en: 'Note that we make the final simplification using the elementary algebraic identity 
    <math alttext="1 minus x Superscript n Baseline equals left-parenthesis 1 minus
    x right-parenthesis left-parenthesis 1 plus x plus ellipsis plus x Superscript
    n minus 1 Baseline right-parenthesis"><mrow><mn>1</mn> <mo>-</mo> <msup><mi>x</mi>
    <mi>n</mi></msup> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>+</mo>
    <mi>x</mi> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mfenced></mrow></math>
    . The results of this derivation and the analogous derivation for the first moment
    are the following correction schemes to account for the initialization bias:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用基本代数恒等式进行最终简化 <math alttext="1 minus x Superscript n Baseline equals
    left-parenthesis 1 minus x right-parenthesis left-parenthesis 1 plus x plus ellipsis
    plus x Superscript n minus 1 Baseline right-parenthesis"><mrow><mn>1</mn> <mo>-</mo>
    <msup><mi>x</mi> <mi>n</mi></msup> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mfenced separators="" open="(" close=")"><mn>1</mn>
    <mo>+</mo> <mi>x</mi> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mfenced></mrow></math>
    。这个推导的结果以及第一矩的类似推导得到以下校正方案，以解决初始化偏差：
- en: '**m̃**[*i*] = <math alttext="StartFraction m Subscript i Baseline Over 1 minus
    beta 1 Superscript i Baseline EndFraction"><mfrac><msub><mi>m</mi> <mi>i</mi></msub>
    <mrow><mn>1</mn><mo>-</mo><msubsup><mi>β</mi> <mn>1</mn> <mi>i</mi></msubsup></mrow></mfrac></math>'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**m̃**[*i*] = <math alttext="StartFraction m Subscript i Baseline Over 1 minus
    beta 1 Superscript i Baseline EndFraction"><mfrac><msub><mi>m</mi> <mi>i</mi></msub>
    <mrow><mn>1</mn><mo>-</mo><msubsup><mi>β</mi> <mn>1</mn> <mi>i</mi></msubsup></mrow></mfrac></math>'
- en: <math alttext="bold v overTilde Subscript i Baseline equals StartFraction bold
    v overTilde Subscript i Baseline Over 1 minus beta 2 Superscript i Baseline EndFraction"><mrow><msub><mover
    accent="true"><mi>𝐯</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mfrac><msub><mover
    accent="true"><mi>𝐯</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mrow><mn>1</mn><mo>-</mo><msubsup><mi>β</mi>
    <mn>2</mn> <mi>i</mi></msubsup></mrow></mfrac></mrow></math>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v overTilde Subscript i Baseline equals StartFraction bold
    v overTilde Subscript i Baseline Over 1 minus beta 2 Superscript i Baseline EndFraction"><mrow><msub><mover
    accent="true"><mi>𝐯</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mfrac><msub><mover
    accent="true"><mi>𝐯</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mrow><mn>1</mn><mo>-</mo><msubsup><mi>β</mi>
    <mn>2</mn> <mi>i</mi></msubsup></mrow></mfrac></mrow></math>
- en: 'We can then use these corrected moments to update the parameter vector, resulting
    in the final Adam update:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这些校正后的矩来更新参数向量，得到最终的Adam更新：
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold v overTilde
    Subscript i Baseline EndRoot EndFraction"><mrow><msub><mi>θ</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>ϵ</mi> <mrow><mi>δ</mi><mo>⊕</mo><msqrt><msub><mover accent="true"><mi>𝐯</mi>
    <mo>˜</mo></mover> <mi>i</mi></msub></msqrt></mrow></mfrac></mrow></math> **m̃**[*i*]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Adam has gained popularity because of its corrective measures against
    the zero initialization bias (a weakness of RMSProp) and its ability to combine
    the core concepts behind RMSProp with momentum more effectively. PyTorch exposes
    the Adam optimizer through the following constructor:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The default hyperparameter settings for Adam for PyTorch generally perform quite
    well, but Adam is also generally robust to choices in hyperparameters. The only
    exception is that the learning rate may need to be modified in certain cases from
    the default value of 0.001.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The Philosophy Behind Optimizer Selection
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve discussed several strategies that are used to make navigating
    the complex error surfaces of deep networks more tractable. These strategies have
    culminated in several optimization algorithms, each with its own benefits and
    shortcomings.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: While it would be awfully nice to know when to use which algorithm, there is
    very little consensus among expert practitioners. Currently, the most popular
    algorithms are minibatch gradient descent,   minibatch gradient  with  momentum,  RMSProp,  RMSProp  with  momentum,  Adam,
    and  AdaDelta (which we haven’t discussed here, but is also supported by PyTorch). We
    encourage you to experiment with these optimization algorithms on the feed-forward
    network model we built.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: One important point, however, is that for most deep learning practitioners,
    the best way to push the cutting edge of deep learning is not by building more
    advanced optimizers. Instead, the vast majority of breakthroughs in deep learning
    over the past several decades have been obtained by discovering architectures
    that are easier to train instead of trying to wrangle with nasty error surfaces.
    We’ll begin focusing on how to leverage architecture to more effectively train
    neural networks in the rest of this book.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed several challenges that arise when trying to train
    deep networks with complex error surfaces. We discussed how while the challenges
    of spurious local minima are likely exaggerated, saddle points and ill-conditioning
    do pose a serious threat to the success of vanilla minibatch gradient descent.
    We described how momentum can be used to overcome ill-conditioning, and briefly
    discussed recent research in second-order methods to approximate the Hessian matrix.
    We also described the evolution of adaptive learning rate optimizers, which tune
    the learning rate during the training process for better convergence.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll begin tackling the larger issue of network architecture and design.
    We’ll explore computer vision and how we might design deep networks that learn
    effectively from complex images.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch06.xhtml#idm45934168902672-marker)) Bengio, Yoshua, et al. “Greedy
    Layer-Wise Training of Deep Networks.” *Advances in Neural Information Processing
    Systems* 19 (2007): 153.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.xhtml#idm45934168861872-marker)) Goodfellow, Ian J., Oriol Vinyals,
    and Andrew M. Saxe. “Qualitatively characterizing neural network optimization
    problems.” *arXiv preprint arXiv*:1412.6544 (2014).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.xhtml#idm45934164995408-marker)) Dauphin, Yann N., et al. “Identifying
    and attacking the saddle point problem in high-dimensional non-convex optimization.”
    *Advances in Neural Information Processing Systems*. 2014.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch06.xhtml#idm45934167589056-marker)) Polyak, Boris T. “Some methods
    of speeding up the convergence of iteration methods.” *USSR Computational Mathematics
    and Mathematical Physics* 4.5 (1964): 1-17.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch06.xhtml#idm45934167589056-marker)) Polyak, Boris T. “一些加速迭代方法收敛的方法。”
    *苏联计算数学和数学物理* 4.5 (1964): 1-17。'
- en: '^([5](ch06.xhtml#idm45934167566352-marker)) Sutskever, Ilya, et al. “On the
    importance of initialization and momentum in deep learning.” *ICML* (3) 28 (2013):
    1139-1147.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch06.xhtml#idm45934167566352-marker)) Sutskever, Ilya, 等. “关于深度学习中初始化和动量的重要性。”
    *ICML* (3) 28 (2013): 1139-1147。'
- en: '^([6](ch06.xhtml#idm45934167542112-marker)) Møller, Martin Fodslette. “A Scaled
    Conjugate Gradient Algorithm for Fast Supervised Learning.” *Neural Networks*
    6.4 (1993): 525-533.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch06.xhtml#idm45934167542112-marker)) Møller, Martin Fodslette. “一种用于快速监督学习的缩放共轭梯度算法。”
    *神经网络* 6.4 (1993): 525-533。'
- en: '^([7](ch06.xhtml#idm45934167533568-marker)) Broyden, C. G. “A New Method of
    Solving Nonlinear Simultaneous Equations.” *The Computer Journal* 12.1 (1969):
    94-99.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '^([7](ch06.xhtml#idm45934167533568-marker)) Broyden, C. G. “解非线性联立方程的一种新方法。”
    *计算机杂志* 12.1 (1969): 94-99。'
- en: '^([8](ch06.xhtml#idm45934167530400-marker)) Bonnans, Joseph-Frédéric, et al.
    *Numerical Optimization: Theoretical and Practical Aspects*. Springer Science
    & Business Media, 2006.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.xhtml#idm45934167530400-marker)) Bonnans, Joseph-Frédéric, 等. *数值优化：理论和实践方面*。Springer
    Science & Business Media, 2006。
- en: '^([9](ch06.xhtml#idm45934166863168-marker)) Duchi, John, Elad Hazan, and Yoram
    Singer. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.”
    *Journal of Machine Learning Research* 12.Jul (2011): 2121-2159.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch06.xhtml#idm45934166863168-marker)) Duchi, John, Elad Hazan, 和 Yoram
    Singer. “用于在线学习和随机优化的自适应次梯度方法。” *机器学习研究杂志* 12.Jul (2011): 2121-2159。'
- en: '^([10](ch06.xhtml#idm45934166835152-marker)) Tieleman, Tijmen, and Geoffrey
    Hinton. “Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its
    Recent Magnitude.” *COURSERA: Neural Networks for Machine Learning* 4.2 (2012).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.xhtml#idm45934166835152-marker)) Tieleman, Tijmen, 和 Geoffrey Hinton.
    “讲座6.5-rmsprop：将梯度除以最近幅度的运行平均值。” *COURSERA：神经网络机器学习* 4.2 (2012)。
- en: '^([11](ch06.xhtml#idm45934166819376-marker)) Kingma, Diederik, and Jimmy Ba.
    “Adam: A Method for Stochastic Optimization.” *arXiv preprint arXiv*:1412.6980
    (2014).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.xhtml#idm45934166819376-marker)) Kingma, Diederik, 和 Jimmy Ba. “Adam：一种用于随机优化的方法。”
    *arXiv预印本 arXiv*:1412.6980 (2014)。
