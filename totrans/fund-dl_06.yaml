- en: Chapter 6\. Beyond Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Challenges with Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental ideas behind neural networks have existed for decades, but it
    wasn’t until recently that neural network-based learning models have become mainstream.
    Our fascination with neural networks has everything to do with their expressiveness,
    a quality we’ve unlocked by creating networks with many layers. As we have discussed
    in previous chapters, deep neural networks are able to crack problems that were
    previously deemed intractable. Training deep neural networks end to end, however,
    is fraught with difficult challenges that took many technological innovations
    to unravel, including massive labeled datasets (ImageNet, CIFAR-10, etc.), better
    hardware in the form of GPU acceleration, and several algorithmic discoveries.
  prefs: []
  type: TYPE_NORMAL
- en: For several years, researchers resorted to layer-wise greedy pretraining to
    grapple with the complex error surfaces presented by deep learning models.^([1](ch06.xhtml#idm45934168902672))
    These time-intensive strategies would try to find more accurate initializations
    for the model’s parameters one layer at a time before using minibatch gradient
    descent to converge to the optimal parameter settings. More recently, however,
    breakthroughs in optimization methods have enabled us to train models directly
    in an end-to-end fashion.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss several of these breakthroughs. The next couple
    of sections will focus primarily on local minima and whether they pose hurdles
    for successfully training deep models. Then we will further explore the nonconvex
    error surfaces induced by deep models, why vanilla minibatch gradient descent
    falls short, and how modern nonconvex optimizers overcome these pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Local Minima in the Error Surfaces of Deep Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary challenge in optimizing deep learning models is that we are forced
    to use minimal local information to infer the global structure of the error surface.
    This is difficult because there is usually very little correspondence between
    local and global structure. Take the following analogy as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume you’re an insect on the continental United States. You’re dropped
    randomly on the map, and your goal is to find the lowest point on this surface.
    How do you do it? If all you can observe is your immediate surroundings, this
    seems like an intractable problem. If the surface of the US were bowl-shaped (or
    mathematically speaking, convex) and we were smart about our learning rate, we
    could use the gradient descent algorithm to eventually find the bottom of the
    bowl. But the surface of the US is extremely complex, that is to say, is a nonconvex
    surface, which means that even if we find a valley (a local minimum), we have
    no idea if it’s the lowest valley on the map (the global minimum). In [Chapter 4](ch04.xhtml#training_feed_forward),
    we talked about how a minibatch version of gradient descent can help navigate
    a troublesome error surface when there are spurious regions of magnitude zero
    gradients. But as we can see in [Figure 6-1](#mini_batch_gradient_descent), even
    a stochastic error surface won’t save us from a deep local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Minibatch gradient descent may aid in escaping shallow local minima,
    but often fails when dealing with deep local minima, as shown
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now comes the critical question. Theoretically, local minima pose a significant
    issue. But in practice, how common are local minima in the error surfaces of deep
    networks? And in which scenarios are they actually problematic for training? In
    the following two sections, we’ll pick apart common misconceptions about local
    minima.
  prefs: []
  type: TYPE_NORMAL
- en: Model Identifiability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first source of local minima is tied to a concept commonly referred to as
    *model identifiability*. One observation about deep neural networks is that their
    error surfaces are guaranteed to have a large—and in some cases, an infinite—number
    of local minima. There are two major reasons this observation is true.
  prefs: []
  type: TYPE_NORMAL
- en: The first is that within a layer of a fully connected feed-forward neural network,
    any rearrangement of neurons will still give you the same final output at the
    end of the network. We illustrate this using a simple three-neuron layer in [Figure 6-2](#rearranging_neurons_in_a_layer).
    As a result, within a layer with  <math alttext="n"><mi>n</mi></math>  neurons,
    there are  <math alttext="n factorial"><mrow><mi>n</mi> <mo>!</mo></mrow></math>
     ways to rearrange parameters. And for a deep network with  <math alttext="l"><mi>l</mi></math>
     layers, each with  <math alttext="n"><mi>n</mi></math>  neurons, we have a total
    of  <math alttext="n factorial Superscript l Baseline"><mrow><mi>n</mi> <msup><mo>!</mo>
    <mi>l</mi></msup></mrow></math>  equivalent configurations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Rearranging neurons in a layer of a neural network results in equivalent
    configurations due to symmetry
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the symmetries of neuron rearrangements, nonidentifiability is
    present in other forms in certain kinds of neural networks. For example, there
    is an infinite number of equivalent configurations that for an individual ReLU
    neuron result in equivalent networks. Because an ReLU uses a piecewise linear
    function, we are free to multiply all of the incoming weights by any nonzero constant 
    <math alttext="k"><mi>k</mi></math>  while scaling all of the outgoing weights
    by  <math alttext="StartFraction 1 Over k EndFraction"><mfrac><mn>1</mn> <mi>k</mi></mfrac></math>
     without changing the behavior of the network. We leave the justification for
    this statement as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, however, local minima that arise because of the nonidentifiability
    of deep neural networks are not inherently problematic. This is because all nonidentifiable
    configurations behave in an indistinguishable fashion no matter what input values
    they are fed. This means they will achieve the same error on the training, validation,
    and testing datasets. In other words, all of these models will have learned equally
    from the training data and will have identical behavior during generalization
    to unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, local minima are only problematic when they are *spurious*. A spurious
    local minimum corresponds to a configuration of weights in a neural network that
    incurs a higher error than the configuration at the global minimum. If these kinds
    of local minima are common, we quickly run into significant problems while using
    gradient-based optimization methods because we can take only local structure into
    account.
  prefs: []
  type: TYPE_NORMAL
- en: How Pesky Are Spurious Local Minima in Deep Networks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many years, deep learning practitioners blamed all of their troubles in
    training deep networks on spurious local minima, albeit with little evidence.
    Today, it remains an open question whether spurious local minima with a high error
    rate relative to the global minimum are common in practical deep networks. However,
    many recent studies seem to indicate that most local minima have error rates and
    generalization characteristics that are very similar to global minima.
  prefs: []
  type: TYPE_NORMAL
- en: One way we might try to naively tackle this problem is by plotting the value
    of the error function over time as we train a deep neural network. This strategy,
    however, doesn’t give us enough information about the error surface because it
    is difficult to tell whether the error surface is “bumpy,” or whether we merely
    have a difficult time figuring out which direction we should be moving in.
  prefs: []
  type: TYPE_NORMAL
- en: To more effectively analyze this problem, Goodfellow et al. (a team of researchers
    collaborating between Google and Stanford) published a paper in 2014 that attempted
    to separate these two potential confounding factors.^([2](ch06.xhtml#idm45934168861872))
    Instead of analyzing the error function over time, they cleverly investigated
    what happens on the error surface between a randomly initialized parameter vector
    and a successful final solution by using linear interpolation. So, given a randomly
    initialized parameter vector  <math alttext="theta Subscript i"><msub><mi>θ</mi>
    <mi>i</mi></msub></math>  and stochastic gradient descent (SGD) solution  <math
    alttext="theta Subscript f"><msub><mi>θ</mi> <mi>f</mi></msub></math> , we aim
    to compute the error function at every point along the linear interpolation  <math
    alttext="theta Subscript alpha Baseline equals alpha dot theta Subscript f Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis dot theta Subscript i"><mrow><msub><mi>θ</mi>
    <mi>α</mi></msub> <mo>=</mo> <mi>α</mi> <mo>·</mo> <msub><mi>θ</mi> <mi>f</mi></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow>
    <mo>·</mo> <msub><mi>θ</mi> <mi>i</mi></msub></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: They wanted to investigate whether local minima would hinder our gradient-based
    search method even if we knew which direction to move in. They showed that for
    a wide variety of practical networks with different types of neurons, the direct
    path between a randomly initialized point in the parameter space and a stochastic
    gradient descent solution isn’t plagued with troublesome local minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even demonstrate this ourselves using the feed-forward ReLU network
    we built in [Chapter 5](ch05.xhtml#neural_networks_in_pytorch). Using a checkpoint
    file that we saved while training our original feed-forward network, we can reinstantiate
    the model using `load_state_dict` and `torch.load`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In PyTorch, we cannot access a model’s parameters directly since the `model.parameters()`
    method returns a generator that provides only a *copy* of the parameters. To modify
    a model’s parameters, we use `torch.load` to read the state dictionary containing
    the parameter values from the file, and then use `load_state_dict` to set the
    model’s parameters with these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using `torch.load` to load the state dictionary from a file, we
    can also access the state dictionary from a model itself using the `state_dict`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that we need to use the `copy.deepcopy` method to copy a dictionary with
    its values. Just setting `opt_state_dict = model.state_dict()` would result in
    a shallow copy, and `opt_state_dict` would be changed when we load our model with
    interpolated parameters later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we instantiate a new model with randomly initialized parameters and save
    those parameters as `rand_state_dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With these two networks appropriately initialized, we can now construct the
    linear interpolation using the mixing parameters `alpha` and `beta`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will compute the average loss over the entire test dataset using the
    model with the interpolated parameters.  For convenience, let’s create a function
    for inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally,  we can vary the value of `alpha` to understand how the error surface
    changes as we traverse the line between the randomly initialized point and the
    final SGD solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This creates [Figure 6-3](#cost_function_of_a_three_layer_feedforward_network),
    which we can inspect ourselves. In fact, if we run this experiment over and over
    again, we find that there are no truly troublesome local minima that would get
    us stuck. It seems that the true struggle of gradient descent isn’t the existence
    of troublesome local minima, but instead is that we have a tough time finding
    the appropriate direction to move in. We’ll return to this thought a little later.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The cost function of a three-layer feed-forward network as we linearly
    interpolate on the line connecting a randomly initialized parameter vector and
    an SGD solution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Flat Regions in the Error Surface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although  it seems that our analysis is devoid of troublesome local minimum,
    we do notice a peculiar flat region where the gradient approaches zero when we
    get to approximately `alpha=1`. This point is not a local minima, so it is unlikely
    to get us completely stuck, but it seems like the zero gradient might slow down
    learning if we are unlucky enough to encounter it.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, given an arbitrary function, a point at which the gradient is
    the zero vector is called a *critical point*. Critical points come in various
    flavors. We’ve already talked about local minima. It’s also not hard to imagine
    their counterparts, the *local maxima*, which don’t really pose much of an issue
    for SGD. But then there are these strange critical points that lie somewhere in
    between. These “flat” regions that are potentially pesky but not necessarily deadly
    are called *saddle points*. It turns out that as our function has more and more
    dimensions (i.e., we have more and more parameters in our model), saddle points
    are exponentially more likely than local minima. Let’s try to intuit why.
  prefs: []
  type: TYPE_NORMAL
- en: For a 1D cost function, a critical point can take one of three forms, as shown
    in [Figure 6-4](#analyzing_a_critical_point).  Loosely, let’s assume each of these
    three configurations is equally likely. This means that given a random critical
    point in a random 1D function, it has one-third probability of being a local minimum.
    This means that if we have a total of  <math alttext="k"><mi>k</mi></math>  critical
    points, we can expect to have a total of  <math alttext="StartFraction k Over
    3 EndFraction"><mfrac><mi>k</mi> <mn>3</mn></mfrac></math>  local minima.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Analyzing a critical point along a single dimension
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can also extend this to higher dimensional functions. Consider a cost function
    operating in a <math alttext="d"><mi>d</mi></math> -dimensional space. Let’s take
    an arbitrary critical point. It turns out that figuring out if this point is a
    local minimum, local maximum, or a saddle point is a little bit trickier than
    in the one-dimensional case. Consider the error surface in [Figure 6-5](#saddle_point_over_a_two_dimensional_error_surface).
    Depending on how you slice the surface (from A to B or from C to D), the critical
    point looks like either a minimum or a maximum. In reality, it’s neither. It’s
    a more complex type of saddle point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. A saddle point over a 2D error surface
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general, in a  <math alttext="d"><mi>d</mi></math> -dimensional parameter
    space, we can slice through a critical point on  <math alttext="d"><mi>d</mi></math>
     different axes. A critical point can be a local minimum only if it appears as
    a local minimum in every single one of the  <math alttext="d"><mi>d</mi></math>
     1D subspaces. Using the fact that a critical point can come in one of three different
    flavors in a one-dimensional subspace, we realize that the probability that a
    random critical point is in a random function is  <math alttext="StartFraction
    1 Over 3 Superscript d Baseline EndFraction"><mfrac><mn>1</mn> <msup><mn>3</mn>
    <mi>d</mi></msup></mfrac></math> . This means that a random function with  <math
    alttext="k"><mi>k</mi></math>  critical points has an expected number of  <math
    alttext="StartFraction k Over 3 Superscript d Baseline EndFraction"><mfrac><mi>k</mi>
    <msup><mn>3</mn> <mi>d</mi></msup></mfrac></math>  local minima. In other words,
    as the dimensionality of our parameter space increases, local minima become exponentially
    more rare. A more rigorous treatment of this topic is outside the scope of this
    book, but was explored more extensively by Dauphin et al. in 2014.^([3](ch06.xhtml#idm45934164995408))
  prefs: []
  type: TYPE_NORMAL
- en: So what does this mean for optimizing deep learning models? For stochastic gradient
    descent, it’s still unclear. It seems like these flat segments of the error surface
    are pesky but ultimately don’t prevent stochastic gradient descent from converging
    to a good answer. However, it does pose serious problems for methods that attempt
    to directly solve for a point where the gradient is zero. This has been a major
    hindrance to the usefulness of certain second-order optimization methods for deep
    learning models, which we will discuss later.
  prefs: []
  type: TYPE_NORMAL
- en: When the Gradient Points in the Wrong Direction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Upon analyzing the error surfaces of deep networks, it seems like the most critical
    challenge to optimizing deep networks is finding the correct trajectory to move
    in. It’s no surprise, however, that this is a major challenge when we look at
    what happens to the error surface around a local minimum. As an example, we consider
    an error surface defined over a 2D parameter space, as shown in [Figure 6-6](#local_information_encoded_by_the_gradient).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Local information encoded by the gradient usually does not corroborate
    the global structure of the error surface
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Revisiting the contour diagrams we explored in [Chapter 4](ch04.xhtml#training_feed_forward),
    notice that the gradient isn’t usually a very good indicator of the good trajectory.
    Specifically, only when the contours are perfectly circular does the gradient
    always point in the direction of the local minimum. However, if the contours are
    extremely elliptical (as is usually the case for the error surfaces of deep networks),
    the gradient can be as inaccurate as 90 degrees away from the correct direction.
  prefs: []
  type: TYPE_NORMAL
- en: We extend this analysis to an arbitrary number of dimensions using some mathematical
    formalism. For every weight <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
     in the parameter space, the gradient computes the value of  <math alttext="StartFraction
    normal partial-differential upper E Over normal partial-differential w Subscript
    i Baseline EndFraction"><mfrac><mrow><mi>∂</mi><mi>E</mi></mrow> <mrow><mi>∂</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></mfrac></math> , or how the value of the error changes
    as we change the value of  <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    . Taken together over all weights in the parameter space, the gradient gives us
    the direction of steepest descent. The general problem with taking a significant
    step in this direction, however, is that the gradient could be changing under
    our feet as we move! We demonstrate this simple fact in [Figure 6-7](#direction_of_gradient_changes).
    Going back to the 2D example, if our contours are perfectly circular and we take
    a big step in the direction of the steepest descent, the gradient doesn’t change
    direction as we move. However, this is not the case for highly elliptical contours.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. The direction of the gradient changes as we move along the direction
    of steepest descent, as determined from a starting point; the gradient vectors
    are normalized to identical length to emphasize the change in direction of the
    gradient vector
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: More generally, we can quantify how the gradient changes under our feet as we
    move in a certain direction by computing second derivatives. Specifically, we
    want to measure  <math alttext="StartFraction normal partial-differential left-parenthesis
    normal partial-differential upper E slash normal partial-differential w Subscript
    j Baseline right-parenthesis Over normal partial-differential w Subscript i Baseline
    EndFraction"><mfrac><mrow><mi>∂</mi><mfenced separators="" open="(" close=")"><mi>∂</mi><mi>E</mi><mo>/</mo><mi>∂</mi><msub><mi>w</mi>
    <mi>j</mi></msub></mfenced></mrow> <mrow><mi>∂</mi><msub><mi>w</mi> <mi>i</mi></msub></mrow></mfrac></math>
    , which tells us how the gradient component for  <math alttext="w Subscript j"><msub><mi>w</mi>
    <mi>j</mi></msub></math>  changes as we change the value of  <math alttext="w
    Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> . We can compile this information
    into a special matrix known as the *Hessian matrix* (***H***). And when describing
    an error surface where the gradient changes underneath our feet as we move in
    the direction of steepest descent, this matrix is said to be *ill-conditioned*.
  prefs: []
  type: TYPE_NORMAL
- en: Momentum-Based Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentally, the problem of an ill-conditioned Hessian matrix manifests itself
    in the form of gradients that fluctuate wildly. As a result, one popular mechanism
    for dealing with ill-conditioning bypasses the computation of the Hessian, and
    instead, focuses on how to cancel out these fluctuations over the duration of
    training.
  prefs: []
  type: TYPE_NORMAL
- en: One way to think about how we might tackle this problem is by investigating
    how a ball rolls down a hilly surface. Driven by gravity, the ball eventually
    settles into a minimum on the surface, but for some reason, it doesn’t suffer
    from the wild fluctuations and divergences that happen during gradient descent.
    Why is this the case? Unlike in stochastic gradient descent (which uses only the
    gradient), there are two major components that determine how a ball rolls down
    an error surface. The first, which we already model in SGD as the gradient, is
    what we commonly refer to as acceleration. But acceleration does not single-handedly
    determine the ball’s movements. Instead, its motion is more directly determined
    by its velocity. Acceleration indirectly changes the ball’s position only by modifying
    its velocity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Velocity-driven motion is desirable because it counteracts the effects of a
    wildly fluctuating gradient by smoothing the ball’s trajectory over its history.
    Velocity serves as a form of memory, and this allows us to more effectively accumulate
    movement in the direction of the minimum while canceling out oscillating accelerations
    in orthogonal directions. Our goal, then, is to somehow generate an analog for
    velocity in our optimization algorithm. We can do this by keeping track of an
    *exponentially weighted decay* of past gradients. The premise is simple: every
    update is computed by combining the update in the last iteration with the current
    gradient. Concretely, we compute the change in the parameter vector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold v Subscript i Baseline equals m bold v Subscript i minus
    1 Baseline minus epsilon bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>m</mi> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mi>ϵ</mi> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    plus bold v Subscript i"><mrow><msub><mi>θ</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>θ</mi>
    <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We use the momentum hyperparameter <math alttext="m"><mi>m</mi></math> to determine
    what fraction of the previous velocity to retain in the new update, and add this
    “memory” of past gradients to our current gradient. This approach is commonly
    referred to as *momentum*.^([4](ch06.xhtml#idm45934167589056)) Because the momentum
    term increases the step size we take, using momentum may require a reduced learning
    rate compared to vanilla stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better visualize how momentum works, we’ll explore a toy example. Specifically,
    we’ll investigate how momentum affects updates during a *random walk*. A random
    walk is a succession of randomly chosen steps. In our example, we’ll imagine a
    particle on a line that, at every time interval, randomly picks a step size between
    –10 and 10 and takes a moves in that direction. This is simply expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll then simulate what happens when we use a slight modification of momentum
    (i.e., the standard exponentially weighted moving average algorithm) to smooth
    our choice of step at every time interval. Again, we can concisely express this
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The results, as we vary the momentum from 0 to 1, are quite staggering. Momentum
    significantly reduces the volatility of updates. The larger the momentum, the
    less responsive we are to new updates (e.g., a large inaccuracy on the first estimation
    of trajectory propagates for a significant period of time). We summarize the results
    of our toy experiment in [Figure 6-8](#momentum_smooths_volatility).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Momentum smooths volatility in the step sizes during a random walk
    using an exponentially weighted moving average
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To investigate how momentum actually affects the training of feed-forward neural
    networks, we can retrain our trusty MNIST feed-forward network with a PyTorch
    momentum optimizer. In this case, we can get away with using the same learning
    rate (0.01) with a typical momentum of 0.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Notice that when we create a PyTorch optimizer, we need to pass in `model.parameters()`.
    The resulting speedup is staggering. We display how the cost function changes
    over time by comparing the visualizations in [Figure 6-9](#comparing_training_a_feedforward_network).
    The figure demonstrates that to achieve a cost of 0.1 without momentum (right)
    requires nearly 18,000 steps (minibatches), whereas with momentum (left), we require
    just over 2,000.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Comparing training a feed-forward network with (right) and without
    (left) momentum demonstrates a massive decrease in training time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, more work has explored how the classical momentum technique can be
    improved. Sutskever et al. in 2013 proposed an alternative called Nesterov momentum,
    which computes the gradient on the error surface at <math alttext="theta plus
    bold v Subscript i minus 1"><mrow><mi>θ</mi> <mo>+</mo> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
     during the velocity update instead of at <math alttext="theta"><mi>θ</mi></math>
    .^([5](ch06.xhtml#idm45934167566352)) This subtle difference seems to allow Nesterov
    momentum to change its velocity in a more responsive way. It’s been shown that
    this method has clear benefits in batch gradient descent (convergence guarantees
    and the ability to use a higher momentum for a given learning rate as compared
    to classical momentum), but it’s not entirely clear whether this is true for the
    more stochastic minibatch gradient descent used in most deep learning optimization
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nerestov momentum is supported in PyTorch out-of-the-box by setting the `nesterov`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A Brief View of Second-Order Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed, computing the Hessian is a computationally difficult task,
    and momentum afforded us significant speedup without having to worry about it
    altogether. Several second-order methods, however, have been researched over the
    past several years that attempt to approximate the Hessian directly. For completeness,
    we give a broad overview of these methods, but a detailed treatment is beyond
    the scope of this text.
  prefs: []
  type: TYPE_NORMAL
- en: The first is *conjugate gradient descent*, which arises out of attempting to
    improve on a naive method of steepest descent. In steepest descent, we compute
    the direction of the gradient and then line search to find the minimum along that
    direction. We jump to the minimum and then recompute the gradient to determine
    the direction of the next line search. It turns out that this method ends up zigzagging
    a significant amount, as shown in [Figure 6-10](#fig0610), because each time we
    move in the direction of steepest descent, we undo a little bit of progress in
    another direction. A remedy to this problem is moving in a *conjugate direction*
    relative to the previous choice instead of the direction of steepest descent.
    The conjugate direction is chosen by using an indirect approximation of the Hessian
    to linearly combine the gradient and our previous direction. With a slight modification,
    this method generalizes to the nonconvex error surfaces we find in deep networks.^([6](ch06.xhtml#idm45934167542112))
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The method of steepest descent often zigzags; conjugate descent
    attempts to remedy this issue
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An alternative optimization algorithm known as the *Broyden–Fletcher–Goldfarb–Shanno*
    (BFGS) algorithm attempts to compute the inverse of the Hessian matrix iteratively
    and use the inverse Hessian to more effectively optimize the parameter vector.^([7](ch06.xhtml#idm45934167533568))
    In its original form, BFGS has a significant memory footprint, but recent work
    has produced a more memory-efficient version known as *L-BFGS*.^([8](ch06.xhtml#idm45934167530400))
  prefs: []
  type: TYPE_NORMAL
- en: In general, while these methods hold some promise, second-order methods are
    still an area of active research and are unpopular among practitioners. PyTorch
    does, however, support L-BFGS as well as other second-order methods, such as Averaged
    Stochastic Gradient Descent, for your own experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate Adaptation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have discussed previously, another major challenge for training deep networks
    is appropriately selecting the learning rate. Choosing the correct learning rate
    has long been one of the most troublesome aspects of training deep networks because
    it has a major impact on a network’s performance. A learning rate that is too
    small doesn’t learn quickly enough, but a learning rate that is too large may
    have difficulty converging as we approach a local minimum or region that is ill-conditioned.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major breakthroughs in modern deep network optimization was the advent
    of learning rate adaption. The basic concept behind learning rate adaptation is
    that the optimal learning rate is appropriately modified over the span of learning
    to achieve good convergence properties. Over the next several sections, we’ll
    discuss AdaGrad, RMSProp, and Adam, three of the most popular adaptive learning
    rate algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad—Accumulating Historical Gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first algorithm we’ll discuss is AdaGrad, which attempts to adapt the global
    learning rate over time using an accumulation of the historical gradients, first
    proposed by Duchi et al. in 2011.^([9](ch06.xhtml#idm45934166863168)) Specifically,
    we keep track of a learning rate for each parameter. This learning rate is inversely
    scaled with respect to the square root of the sum of the squares (root mean square)
    of all the parameter’s historical gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can express this mathematically. We initialize a gradient accumulation vector 
    <math alttext="bold r 0 equals bold 0"><mrow><msub><mi>𝐫</mi> <mn>0</mn></msub>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> . At every step, we accumulate
    the square of all the gradient parameters as follows (where the <math alttext="circled-dot"><mo>⊙</mo></math>
     operation is element-wise tensor multiplication):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold r Subscript i Baseline equals bold r Subscript i minus 1
    Baseline plus bold g Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐫</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>𝐫</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we compute the update as usual, except our global learning rate <math
    alttext="epsilon"><mi>ϵ</mi></math>  is divided by the square root of the gradient
    accumulation vector:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold r Subscript
    i Baseline EndRoot EndFraction circled-dot bold g"><mrow><msub><mi>θ</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>ϵ</mi> <mrow><mi>δ</mi><mo>⊕</mo><msqrt><msub><mi>𝐫</mi>
    <mi>i</mi></msub></msqrt></mrow></mfrac> <mo>⊙</mo> <mi>𝐠</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we add a tiny number  <math alttext="delta"><mi>δ</mi></math>  (~
    <math alttext="10 Superscript negative 7"><msup><mn>10</mn> <mrow><mo>-</mo><mn>7</mn></mrow></msup></math>
    ) to the denominator to prevent division by zero. Also, the division and addition
    operations are broadcast to the size of the gradient accumulation vector and applied
    element-wise. In PyTorch, a built-in optimizer allows for easily utilizing AdaGrad
    as a learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The only hitch is that in PyTorch, the  <math alttext="delta"><mi>δ</mi></math>
     and initial gradient accumulation vector are rolled together into the `initial_accumulator_value` argument.
  prefs: []
  type: TYPE_NORMAL
- en: On a functional level, this update mechanism means that the parameters with
    the largest gradients experience a rapid decrease in their learning rates, while
    parameters with smaller gradients observe only a small decrease in their learning
    rates. The ultimate effect is that AdaGrad forces more progress in the more gently
    sloped directions on the error surface, which can help overcome ill-conditioned
    surfaces. This results in some good theoretical properties, but in practice, training
    deep learning models with AdaGrad can be somewhat problematic. Empirically, AdaGrad
    has a tendency to cause a premature drop in learning rate, and as a result doesn’t
    work particularly well for some deep models. In the next section, we’ll describe
    RMSProp, which attempts to remedy this shortcoming.
  prefs: []
  type: TYPE_NORMAL
- en: RMSProp—Exponentially Weighted Moving Average of Gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While AdaGrad works well for simple convex functions, it isn’t designed to navigate
    the complex error surfaces of deep networks. Flat regions may force AdaGrad to
    decrease the learning rate before it reaches a minimum. The conclusion is that
    simply using a naive accumulation of gradients isn’t sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our solution is to bring back a concept we introduced earlier while discussing
    momentum to dampen fluctuations in the gradient. Compared to naive accumulation,
    exponentially weighted moving averages also enables us to “toss out” measurements
    that we made a long time ago. More specifically, our update to the gradient accumulation
    vector is now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold r Subscript i Baseline equals rho bold r Subscript i minus
    1 Baseline plus left-parenthesis 1 minus rho right-parenthesis bold g Subscript
    i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐫</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>ρ</mi> <msub><mi>𝐫</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>ρ</mi></mfenced>
    <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The decay factor <math alttext="rho"><mi>ρ</mi></math>  determines how long
    we keep old gradients. The smaller the decay factor, the shorter the effective
    window. Plugging this modification into AdaGrad gives rise to the RMSProp learning
    algorithm, first proposed by Geoffrey Hinton.^([10](ch06.xhtml#idm45934166835152))
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyTorch, we can instantiate the RMSProp optimizer with the following code.
    Note that in this case, unlike in AdaGrad, we pass in  <math alttext="delta"><mi>δ</mi></math>
     separately as the `epsilon` argument to the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As the template suggests, we can utilize RMSProp with momentum (specifically
    Nerestov momentum). Overall, RMSProp has been shown to be a highly effective optimizer
    for deep neural networks, and is a default choice for many seasoned practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Adam—Combining Momentum and RMSProp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before concluding our discussion of modern optimizers, we discuss one final
    algorithm—Adam.^([11](ch06.xhtml#idm45934166819376)) Spiritually, we can think
    about Adam as a variant combination of RMSProp and momentum.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea is as follows. We want to keep track of an exponentially weighted
    moving average of the gradient (essentially the concept of velocity in classical
    momentum), which we can express as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**m**[*i*] = β[1]**m**[*i* – 1] + (1 – β[1])**g**[i]'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is our approximation of what we call the *first moment* of the gradient,
    or <math alttext="double-struck upper E left-bracket bold g Subscript i Baseline
    right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub>
    <mo>]</mo></mrow></math> . And similarly to RMSProp, we can maintain an exponentially
    weighted moving average of the historical gradients. This is our estimation of
    what we call the *second moment* of the gradient, or <math alttext="double-struck
    upper E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript
    i Baseline right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: However, it turns out these estimations are biased relative to the real moments
    because we start off by initializing both vectors to the zero vector. In order
    to remedy this bias, we derive a correction factor for both estimations. Here,
    we describe the derivation for the estimation of the second moment. The derivation
    for the first moment, which is analogous to the derivation here, is left as an
    exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by expressing the estimation of the second moment in terms of all
    past gradients. This is done by simply expanding the recurrence relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold v Subscript i Baseline equals beta 2 Superscript i minus
    1 Baseline left-parenthesis 1 minus beta 2 right-parenthesis bold g 1 circled-dot
    bold g 1 plus beta 2 Superscript i minus 2 Baseline left-parenthesis 1 minus beta
    2 right-parenthesis bold g 2 circled-dot bold g 2 plus ellipsis plus left-parenthesis
    1 minus beta 2 right-parenthesis bold g Subscript i Baseline circled-dot bold
    g Subscript i"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>=</mo> <msubsup><mi>β</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>𝐠</mi>
    <mn>1</mn></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>β</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>2</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>𝐠</mi>
    <mn>2</mn></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mn>2</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold v Subscript i Baseline equals left-parenthesis 1 minus beta
    2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msubsup><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup> <msup><mi>β</mi>
    <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup> <msub><mi>𝐠</mi> <mi>k</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then take the expected value of both sides to determine how our estimation
    <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐯</mi> <mi>i</mi></msub>
    <mo>]</mo></mrow></math>  compares to the real value of <math alttext="double-struck
    upper E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript
    i Baseline right-bracket"><mrow><mi>𝔼</mi> <mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub>
    <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket equals double-struck upper E left-bracket left-parenthesis 1 minus
    beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k Baseline right-bracket"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <mi>𝔼</mi> <mfenced separators=""
    open="[" close="]"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>i</mi></msubsup> <msup><mi>β</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup>
    <msub><mi>𝐠</mi> <mi>k</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also assume that <math alttext="double-struck upper E left-bracket bold
    g Subscript k Baseline circled-dot bold g Subscript k Baseline right-bracket almost-equals
    double-struck upper E left-bracket bold g Subscript i Baseline almost-equals bold
    g Subscript i Baseline right-bracket"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi>
    <mi>k</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>k</mi></msub> <mo>]</mo></mrow>
    <mo>≈</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>≈</mo>
    <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow></mrow></math> because even
    if the second moment of the gradient has changed since a historical value, <math
    alttext="beta 2"><msub><mi>β</mi> <mn>2</mn></msub></math> should be chosen so
    that the old second moments of the gradients are essentially decayed out of relevancy.
    As a result, we can make the following simplification:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript
    i Endscripts beta Superscript i minus k"><mrow><mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>≈</mo> <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>]</mo></mrow>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup>
    <msup><mi>β</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta Subscript 2 Sub Superscript i Subscript Baseline right-parenthesis"><mrow><mi>𝔼</mi>
    <mrow><mo>[</mo> <msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>]</mo></mrow> <mo>≈</mo>
    <mi>𝔼</mi> <mrow><mo>[</mo> <msub><mi>𝐠</mi> <mi>i</mi></msub> <mo>⊙</mo> <msub><mi>𝐠</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>β</mi>
    <msup><mn>2</mn> <mi>i</mi></msup></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we make the final simplification using the elementary algebraic identity 
    <math alttext="1 minus x Superscript n Baseline equals left-parenthesis 1 minus
    x right-parenthesis left-parenthesis 1 plus x plus ellipsis plus x Superscript
    n minus 1 Baseline right-parenthesis"><mrow><mn>1</mn> <mo>-</mo> <msup><mi>x</mi>
    <mi>n</mi></msup> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>+</mo>
    <mi>x</mi> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mfenced></mrow></math>
    . The results of this derivation and the analogous derivation for the first moment
    are the following correction schemes to account for the initialization bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '**m̃**[*i*] = <math alttext="StartFraction m Subscript i Baseline Over 1 minus
    beta 1 Superscript i Baseline EndFraction"><mfrac><msub><mi>m</mi> <mi>i</mi></msub>
    <mrow><mn>1</mn><mo>-</mo><msubsup><mi>β</mi> <mn>1</mn> <mi>i</mi></msubsup></mrow></mfrac></math>'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold v overTilde Subscript i Baseline equals StartFraction bold
    v overTilde Subscript i Baseline Over 1 minus beta 2 Superscript i Baseline EndFraction"><mrow><msub><mover
    accent="true"><mi>𝐯</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mfrac><msub><mover
    accent="true"><mi>𝐯</mi> <mo>˜</mo></mover> <mi>i</mi></msub> <mrow><mn>1</mn><mo>-</mo><msubsup><mi>β</mi>
    <mn>2</mn> <mi>i</mi></msubsup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then use these corrected moments to update the parameter vector, resulting
    in the final Adam update:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold v overTilde
    Subscript i Baseline EndRoot EndFraction"><mrow><msub><mi>θ</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>θ</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>ϵ</mi> <mrow><mi>δ</mi><mo>⊕</mo><msqrt><msub><mover accent="true"><mi>𝐯</mi>
    <mo>˜</mo></mover> <mi>i</mi></msub></msqrt></mrow></mfrac></mrow></math> **m̃**[*i*]
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Adam has gained popularity because of its corrective measures against
    the zero initialization bias (a weakness of RMSProp) and its ability to combine
    the core concepts behind RMSProp with momentum more effectively. PyTorch exposes
    the Adam optimizer through the following constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The default hyperparameter settings for Adam for PyTorch generally perform quite
    well, but Adam is also generally robust to choices in hyperparameters. The only
    exception is that the learning rate may need to be modified in certain cases from
    the default value of 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: The Philosophy Behind Optimizer Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve discussed several strategies that are used to make navigating
    the complex error surfaces of deep networks more tractable. These strategies have
    culminated in several optimization algorithms, each with its own benefits and
    shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: While it would be awfully nice to know when to use which algorithm, there is
    very little consensus among expert practitioners. Currently, the most popular
    algorithms are minibatch gradient descent,   minibatch gradient  with  momentum,  RMSProp,  RMSProp  with  momentum,  Adam,
    and  AdaDelta (which we haven’t discussed here, but is also supported by PyTorch). We
    encourage you to experiment with these optimization algorithms on the feed-forward
    network model we built.
  prefs: []
  type: TYPE_NORMAL
- en: One important point, however, is that for most deep learning practitioners,
    the best way to push the cutting edge of deep learning is not by building more
    advanced optimizers. Instead, the vast majority of breakthroughs in deep learning
    over the past several decades have been obtained by discovering architectures
    that are easier to train instead of trying to wrangle with nasty error surfaces.
    We’ll begin focusing on how to leverage architecture to more effectively train
    neural networks in the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed several challenges that arise when trying to train
    deep networks with complex error surfaces. We discussed how while the challenges
    of spurious local minima are likely exaggerated, saddle points and ill-conditioning
    do pose a serious threat to the success of vanilla minibatch gradient descent.
    We described how momentum can be used to overcome ill-conditioning, and briefly
    discussed recent research in second-order methods to approximate the Hessian matrix.
    We also described the evolution of adaptive learning rate optimizers, which tune
    the learning rate during the training process for better convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll begin tackling the larger issue of network architecture and design.
    We’ll explore computer vision and how we might design deep networks that learn
    effectively from complex images.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch06.xhtml#idm45934168902672-marker)) Bengio, Yoshua, et al. “Greedy
    Layer-Wise Training of Deep Networks.” *Advances in Neural Information Processing
    Systems* 19 (2007): 153.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.xhtml#idm45934168861872-marker)) Goodfellow, Ian J., Oriol Vinyals,
    and Andrew M. Saxe. “Qualitatively characterizing neural network optimization
    problems.” *arXiv preprint arXiv*:1412.6544 (2014).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.xhtml#idm45934164995408-marker)) Dauphin, Yann N., et al. “Identifying
    and attacking the saddle point problem in high-dimensional non-convex optimization.”
    *Advances in Neural Information Processing Systems*. 2014.
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch06.xhtml#idm45934167589056-marker)) Polyak, Boris T. “Some methods
    of speeding up the convergence of iteration methods.” *USSR Computational Mathematics
    and Mathematical Physics* 4.5 (1964): 1-17.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch06.xhtml#idm45934167566352-marker)) Sutskever, Ilya, et al. “On the
    importance of initialization and momentum in deep learning.” *ICML* (3) 28 (2013):
    1139-1147.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch06.xhtml#idm45934167542112-marker)) Møller, Martin Fodslette. “A Scaled
    Conjugate Gradient Algorithm for Fast Supervised Learning.” *Neural Networks*
    6.4 (1993): 525-533.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch06.xhtml#idm45934167533568-marker)) Broyden, C. G. “A New Method of
    Solving Nonlinear Simultaneous Equations.” *The Computer Journal* 12.1 (1969):
    94-99.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch06.xhtml#idm45934167530400-marker)) Bonnans, Joseph-Frédéric, et al.
    *Numerical Optimization: Theoretical and Practical Aspects*. Springer Science
    & Business Media, 2006.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch06.xhtml#idm45934166863168-marker)) Duchi, John, Elad Hazan, and Yoram
    Singer. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.”
    *Journal of Machine Learning Research* 12.Jul (2011): 2121-2159.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch06.xhtml#idm45934166835152-marker)) Tieleman, Tijmen, and Geoffrey
    Hinton. “Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its
    Recent Magnitude.” *COURSERA: Neural Networks for Machine Learning* 4.2 (2012).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([11](ch06.xhtml#idm45934166819376-marker)) Kingma, Diederik, and Jimmy Ba.
    “Adam: A Method for Stochastic Optimization.” *arXiv preprint arXiv*:1412.6980
    (2014).'
  prefs: []
  type: TYPE_NORMAL
