- en: Chapter 6\. Beyond Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬6ç« ã€‚è¶…è¶Šæ¢¯åº¦ä¸‹é™
- en: The Challenges with Gradient Descent
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™çš„æŒ‘æˆ˜
- en: The fundamental ideas behind neural networks have existed for decades, but it
    wasnâ€™t until recently that neural network-based learning models have become mainstream.
    Our fascination with neural networks has everything to do with their expressiveness,
    a quality weâ€™ve unlocked by creating networks with many layers. As we have discussed
    in previous chapters, deep neural networks are able to crack problems that were
    previously deemed intractable. Training deep neural networks end to end, however,
    is fraught with difficult challenges that took many technological innovations
    to unravel, including massive labeled datasets (ImageNet, CIFAR-10, etc.), better
    hardware in the form of GPU acceleration, and several algorithmic discoveries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œèƒŒåçš„åŸºæœ¬æ€æƒ³å·²ç»å­˜åœ¨å‡ åå¹´äº†ï¼Œä½†ç›´åˆ°æœ€è¿‘ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„å­¦ä¹ æ¨¡å‹æ‰å˜å¾—ä¸»æµã€‚æˆ‘ä»¬å¯¹ç¥ç»ç½‘ç»œçš„ç€è¿·ä¸å®ƒä»¬çš„è¡¨ç°åŠ›æœ‰å…³ï¼Œè¿™æ˜¯æˆ‘ä»¬é€šè¿‡åˆ›å»ºå…·æœ‰è®¸å¤šå±‚çš„ç½‘ç»œè§£é”çš„è´¨é‡ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­è®¨è®ºçš„é‚£æ ·ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œèƒ½å¤Ÿè§£å†³ä»¥å‰è¢«è®¤ä¸ºæ˜¯æ£˜æ‰‹çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç«¯åˆ°ç«¯è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œå……æ»¡äº†å›°éš¾æŒ‘æˆ˜ï¼Œéœ€è¦è®¸å¤šæŠ€æœ¯åˆ›æ–°æ¥è§£å†³ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†ï¼ˆImageNetã€CIFAR-10ç­‰ï¼‰ã€GPUåŠ é€Ÿç­‰æ›´å¥½çš„ç¡¬ä»¶ä»¥åŠå‡ é¡¹ç®—æ³•å‘ç°ã€‚
- en: For several years, researchers resorted to layer-wise greedy pretraining to
    grapple with the complex error surfaces presented by deep learning models.^([1](ch06.xhtml#idm45934168902672))
    These time-intensive strategies would try to find more accurate initializations
    for the modelâ€™s parameters one layer at a time before using minibatch gradient
    descent to converge to the optimal parameter settings. More recently, however,
    breakthroughs in optimization methods have enabled us to train models directly
    in an end-to-end fashion.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¹´æ¥ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨é€å±‚è´ªå©ªé¢„è®­ç»ƒæ¥åº”å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹å‘ˆç°çš„å¤æ‚è¯¯å·®è¡¨é¢ã€‚è¿™äº›è€—æ—¶çš„ç­–ç•¥ä¼šå°è¯•é€å±‚æ‰¾åˆ°æ¨¡å‹å‚æ•°æ›´å‡†ç¡®çš„åˆå§‹åŒ–ï¼Œç„¶åä½¿ç”¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°æœ€ä½³å‚æ•°è®¾ç½®ã€‚ç„¶è€Œï¼Œæœ€è¿‘ï¼Œä¼˜åŒ–æ–¹æ³•çš„çªç ´ä½¿æˆ‘ä»¬èƒ½å¤Ÿç›´æ¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒæ¨¡å‹ã€‚
- en: In this chapter, we will discuss several of these breakthroughs. The next couple
    of sections will focus primarily on local minima and whether they pose hurdles
    for successfully training deep models. Then we will further explore the nonconvex
    error surfaces induced by deep models, why vanilla minibatch gradient descent
    falls short, and how modern nonconvex optimizers overcome these pitfalls.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå‡ é¡¹çªç ´æ€§çš„æˆæœã€‚æ¥ä¸‹æ¥çš„å‡ èŠ‚å°†ä¸»è¦å…³æ³¨å±€éƒ¨æœ€å°å€¼ä»¥åŠå®ƒä»¬æ˜¯å¦å¯¹æˆåŠŸè®­ç»ƒæ·±åº¦æ¨¡å‹æ„æˆéšœç¢ã€‚ç„¶åæˆ‘ä»¬å°†è¿›ä¸€æ­¥æ¢è®¨æ·±åº¦æ¨¡å‹å¼•èµ·çš„éå‡¸è¯¯å·®è¡¨é¢ï¼Œä¸ºä»€ä¹ˆæ™®é€šçš„å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ä¸è¶³ä»¥åº”å¯¹ï¼Œä»¥åŠç°ä»£éå‡¸ä¼˜åŒ–å™¨å¦‚ä½•å…‹æœè¿™äº›å›°éš¾ã€‚
- en: Local Minima in the Error Surfaces of Deep Networks
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦ç½‘ç»œçš„è¯¯å·®è¡¨é¢ä¸­çš„å±€éƒ¨æœ€å°å€¼
- en: The primary challenge in optimizing deep learning models is that we are forced
    to use minimal local information to infer the global structure of the error surface.
    This is difficult because there is usually very little correspondence between
    local and global structure. Take the following analogy as an example.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæˆ‘ä»¬è¢«è¿«ä½¿ç”¨æœ€å°‘çš„å±€éƒ¨ä¿¡æ¯æ¥æ¨æ–­è¯¯å·®è¡¨é¢çš„å…¨å±€ç»“æ„ã€‚è¿™å¾ˆå›°éš¾ï¼Œå› ä¸ºå±€éƒ¨å’Œå…¨å±€ç»“æ„ä¹‹é—´é€šå¸¸å‡ ä¹æ²¡æœ‰å¯¹åº”å…³ç³»ã€‚ä»¥ä»¥ä¸‹ç±»æ¯”ä¸ºä¾‹ã€‚
- en: Letâ€™s assume youâ€™re an insect on the continental United States. Youâ€™re dropped
    randomly on the map, and your goal is to find the lowest point on this surface.
    How do you do it? If all you can observe is your immediate surroundings, this
    seems like an intractable problem. If the surface of the US were bowl-shaped (or
    mathematically speaking, convex) and we were smart about our learning rate, we
    could use the gradient descent algorithm to eventually find the bottom of the
    bowl. But the surface of the US is extremely complex, that is to say, is a nonconvex
    surface, which means that even if we find a valley (a local minimum), we have
    no idea if itâ€™s the lowest valley on the map (the global minimum). In [ChapterÂ 4](ch04.xhtml#training_feed_forward),
    we talked about how a minibatch version of gradient descent can help navigate
    a troublesome error surface when there are spurious regions of magnitude zero
    gradients. But as we can see inÂ [FigureÂ 6-1](#mini_batch_gradient_descent), even
    a stochastic error surface wonâ€™t save us from a deep local minimum.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æ˜¯ç¾å›½å¤§é™†ä¸Šçš„ä¸€åªæ˜†è™«ã€‚ä½ è¢«éšæœºæ”¾åœ¨åœ°å›¾ä¸Šï¼Œä½ çš„ç›®æ ‡æ˜¯æ‰¾åˆ°è¿™ä¸ªè¡¨é¢ä¸Šçš„æœ€ä½ç‚¹ã€‚ä½ è¯¥æ€ä¹ˆåšï¼Ÿå¦‚æœä½ æ‰€èƒ½è§‚å¯Ÿåˆ°çš„åªæ˜¯ä½ å‘¨å›´çš„ç¯å¢ƒï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªæ£˜æ‰‹çš„é—®é¢˜ã€‚å¦‚æœç¾å›½çš„è¡¨é¢æ˜¯ç¢—çŠ¶çš„ï¼ˆæˆ–è€…åœ¨æ•°å­¦ä¸Šè¯´æ˜¯å‡¸çš„ï¼‰ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯¹å­¦ä¹ ç‡å¾ˆèªæ˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•æœ€ç»ˆæ‰¾åˆ°ç¢—åº•ã€‚ä½†ç¾å›½çš„è¡¨é¢éå¸¸å¤æ‚ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ˜¯ä¸€ä¸ªéå‡¸è¡¨é¢ï¼Œè¿™æ„å‘³ç€å³ä½¿æˆ‘ä»¬æ‰¾åˆ°äº†ä¸€ä¸ªå±±è°·ï¼ˆä¸€ä¸ªå±€éƒ¨æœ€å°å€¼ï¼‰ï¼Œæˆ‘ä»¬ä¹Ÿä¸çŸ¥é“å®ƒæ˜¯å¦æ˜¯åœ°å›¾ä¸Šæœ€ä½çš„å±±è°·ï¼ˆå…¨å±€æœ€å°å€¼ï¼‰ã€‚åœ¨[ç¬¬4ç« ](ch04.xhtml#training_feed_forward)ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ¢¯åº¦ä¸‹é™çš„å°æ‰¹é‡ç‰ˆæœ¬å¦‚ä½•å¸®åŠ©åœ¨å­˜åœ¨é›¶æ¢¯åº¦å¹…åº¦çš„è™šå‡åŒºåŸŸæ—¶å¯¼èˆªæ£˜æ‰‹çš„è¯¯å·®è¡¨é¢ã€‚ä½†æ­£å¦‚æˆ‘ä»¬åœ¨[å›¾6-1](#mini_batch_gradient_descent)ä¸­æ‰€çœ‹åˆ°çš„ï¼Œå³ä½¿æ˜¯éšæœºçš„è¯¯å·®è¡¨é¢ä¹Ÿæ— æ³•æ‹¯æ•‘æˆ‘ä»¬è„±ç¦»æ·±å±‚å±€éƒ¨æœ€å°å€¼ã€‚
- en: '![](Images/fdl2_0601.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0601.png)'
- en: Figure 6-1\. Minibatch gradient descent may aid in escaping shallow local minima,
    but often fails when dealing with deep local minima, as shown
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-1ã€‚å°æ‰¹é‡æ¢¯åº¦ä¸‹é™å¯èƒ½æœ‰åŠ©äºé€ƒç¦»æµ…å±‚å±€éƒ¨æœ€å°å€¼ï¼Œä½†åœ¨å¤„ç†æ·±å±‚å±€éƒ¨æœ€å°å€¼æ—¶é€šå¸¸ä¼šå¤±è´¥ï¼Œå¦‚æ‰€ç¤º
- en: Now comes the critical question. Theoretically, local minima pose a significant
    issue. But in practice, how common are local minima in the error surfaces of deep
    networks? And in which scenarios are they actually problematic for training? In
    the following two sections, weâ€™ll pick apart common misconceptions about local
    minima.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å‡ºç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ä»ç†è®ºä¸Šè®²ï¼Œå±€éƒ¨æœ€å°å€¼æ„æˆäº†ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚ä½†åœ¨å®è·µä¸­ï¼Œæ·±åº¦ç½‘ç»œçš„è¯¯å·®è¡¨é¢ä¸­å±€éƒ¨æœ€å°å€¼æœ‰å¤šå¸¸è§ï¼Ÿåœ¨å“ªäº›æƒ…å†µä¸‹å®ƒä»¬å®é™…ä¸Šå¯¹è®­ç»ƒæœ‰é—®é¢˜ï¼Ÿåœ¨æ¥ä¸‹æ¥çš„ä¸¤èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‰–æå…³äºå±€éƒ¨æœ€å°å€¼çš„å¸¸è§è¯¯è§£ã€‚
- en: Model Identifiability
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¯è¾¨è¯†æ€§
- en: The first source of local minima is tied to a concept commonly referred to as
    *model identifiability*. One observation about deep neural networks is that their
    error surfaces are guaranteed to have a largeâ€”and in some cases, an infiniteâ€”number
    of local minima. There are two major reasons this observation is true.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å±€éƒ¨æœ€å°å€¼çš„ç¬¬ä¸€ä¸ªæ¥æºä¸ä¸€ä¸ªå¸¸è¢«ç§°ä¸º*æ¨¡å‹å¯è¾¨è¯†æ€§*çš„æ¦‚å¿µæœ‰å…³ã€‚å…³äºæ·±åº¦ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªè§‚å¯Ÿæ˜¯ï¼Œå®ƒä»¬çš„è¯¯å·®è¡¨é¢ä¿è¯æœ‰å¤§é‡çš„å±€éƒ¨æœ€å°å€¼ï¼Œæœ‰æ—¶ç”šè‡³æ˜¯æ— é™å¤šä¸ªã€‚è¿™ä¸ªè§‚å¯Ÿæ˜¯çœŸå®çš„æœ‰ä¸¤ä¸ªä¸»è¦åŸå› ã€‚
- en: The first is that within a layer of a fully connected feed-forward neural network,
    any rearrangement of neurons will still give you the same final output at the
    end of the network. We illustrate this using a simple three-neuron layer in [FigureÂ 6-2](#rearranging_neurons_in_a_layer).
    As a result, within a layer withÂ  <math alttext="n"><mi>n</mi></math> Â neurons,
    there areÂ  <math alttext="n factorial"><mrow><mi>n</mi> <mo>!</mo></mrow></math>
    Â ways to rearrange parameters. And for a deep network withÂ  <math alttext="l"><mi>l</mi></math>
    Â layers, each withÂ  <math alttext="n"><mi>n</mi></math> Â neurons, we have a total
    ofÂ  <math alttext="n factorial Superscript l Baseline"><mrow><mi>n</mi> <msup><mo>!</mo>
    <mi>l</mi></msup></mrow></math> Â equivalent configurations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0602.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Rearranging neurons in a layer of a neural network results in equivalent
    configurations due to symmetry
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the symmetries of neuron rearrangements, nonidentifiability is
    present in other forms in certain kinds of neural networks. For example, there
    is an infinite number of equivalent configurations that for an individual ReLU
    neuron result in equivalent networks. Because an ReLU uses a piecewise linear
    function, we are free to multiply all of the incoming weights by any nonzero constantÂ 
    <math alttext="k"><mi>k</mi></math> Â while scaling all of the outgoing weights
    byÂ  <math alttext="StartFraction 1 Over k EndFraction"><mfrac><mn>1</mn> <mi>k</mi></mfrac></math>
    Â without changing the behavior of the network. We leave the justification for
    this statement as an exercise for you.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, however, local minima that arise because of the nonidentifiability
    of deep neural networks are not inherently problematic. This is because all nonidentifiable
    configurations behave in an indistinguishable fashion no matter what input values
    they are fed. This means they will achieve the same error on the training, validation,
    and testing datasets. In other words, all of these models will have learned equally
    from the training data and will have identical behavior during generalization
    to unseen examples.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Instead, local minima are only problematic when they areÂ *spurious*. A spurious
    local minimum corresponds to a configuration of weights in a neural network that
    incurs a higher error than the configuration at the global minimum. If these kinds
    of local minima are common, we quickly run into significant problems while using
    gradient-based optimization methods because we can take only local structure into
    account.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: How Pesky Are Spurious Local Minima in Deep Networks?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many years, deep learning practitioners blamed all of their troubles in
    training deep networks on spurious local minima, albeit with little evidence.
    Today, it remains an open question whether spurious local minima with a high error
    rate relative to the global minimum are common in practical deep networks. However,
    many recent studies seem to indicate that most local minima have error rates and
    generalization characteristics that are very similar to global minima.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: One way we might try to naively tackle this problem is by plotting the value
    of the error function over time as we train a deep neural network. This strategy,
    however, doesnâ€™t give us enough information about the error surface because it
    is difficult to tell whether the error surface is â€œbumpy,â€ or whether we merely
    have a difficult time figuring out which direction we should be moving in.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: To more effectively analyze this problem, Goodfellow et al. (a team of researchers
    collaborating between Google and Stanford) published a paper in 2014 that attempted
    to separate these two potential confounding factors.^([2](ch06.xhtml#idm45934168861872))
    Instead of analyzing the error function over time, they cleverly investigated
    what happens on the error surface between a randomly initialized parameter vector
    and a successful final solution by using linear interpolation. So, given a randomly
    initialized parameter vectorÂ  <math alttext="theta Subscript i"><msub><mi>Î¸</mi>
    <mi>i</mi></msub></math> Â and stochastic gradient descent (SGD) solutionÂ  <math
    alttext="theta Subscript f"><msub><mi>Î¸</mi> <mi>f</mi></msub></math> , we aim
    to compute the error function at every point along the linear interpolationÂ  <math
    alttext="theta Subscript alpha Baseline equals alpha dot theta Subscript f Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis dot theta Subscript i"><mrow><msub><mi>Î¸</mi>
    <mi>Î±</mi></msub> <mo>=</mo> <mi>Î±</mi> <mo>Â·</mo> <msub><mi>Î¸</mi> <mi>f</mi></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>Î±</mi> <mo>)</mo></mrow>
    <mo>Â·</mo> <msub><mi>Î¸</mi> <mi>i</mi></msub></mrow></math> .
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´æœ‰æ•ˆåœ°åˆ†æè¿™ä¸ªé—®é¢˜ï¼ŒGoodfellowç­‰äººï¼ˆè°·æ­Œå’Œæ–¯å¦ç¦å¤§å­¦çš„ç ”ç©¶äººå‘˜åˆä½œç»„æˆçš„å›¢é˜Ÿï¼‰åœ¨2014å¹´å‘è¡¨äº†ä¸€ç¯‡è®ºæ–‡ï¼Œè¯•å›¾åˆ†ç¦»è¿™ä¸¤ä¸ªæ½œåœ¨çš„æ··æ·†å› ç´ ã€‚[^2]
    ä»–ä»¬å·§å¦™åœ°ç ”ç©¶äº†åœ¨éšæœºåˆå§‹åŒ–å‚æ•°å‘é‡å’ŒæˆåŠŸçš„æœ€ç»ˆè§£ä¹‹é—´çš„è¯¯å·®æ›²é¢ä¸Šå‘ç”Ÿäº†ä»€ä¹ˆï¼Œè€Œä¸æ˜¯éšæ—¶é—´åˆ†æè¯¯å·®å‡½æ•°ã€‚å› æ­¤ï¼Œç»™å®šä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å‚æ•°å‘é‡Î¸iå’Œéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰è§£Î¸fï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¡ç®—æ²¿ç€çº¿æ€§æ’å€¼Î¸Î±çš„æ¯ä¸ªç‚¹å¤„çš„è¯¯å·®å‡½æ•°ã€‚
- en: They wanted to investigate whether local minima would hinder our gradient-based
    search method even if we knew which direction to move in. They showed that for
    a wide variety of practical networks with different types of neurons, the direct
    path between a randomly initialized point in the parameter space and a stochastic
    gradient descent solution isnâ€™t plagued with troublesome local minima.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æƒ³è¦è°ƒæŸ¥å³ä½¿æˆ‘ä»¬çŸ¥é“è¦ç§»åŠ¨çš„æ–¹å‘ï¼Œå±€éƒ¨æœ€å°å€¼æ˜¯å¦ä¼šé˜»ç¢åŸºäºæ¢¯åº¦çš„æœç´¢æ–¹æ³•ã€‚ä»–ä»¬è¡¨æ˜ï¼Œå¯¹äºå„ç§å…·æœ‰ä¸åŒç±»å‹ç¥ç»å…ƒçš„å®é™…ç½‘ç»œï¼Œå‚æ•°ç©ºé—´ä¸­éšæœºåˆå§‹åŒ–ç‚¹å’Œéšæœºæ¢¯åº¦ä¸‹é™è§£ä¹‹é—´çš„ç›´æ¥è·¯å¾„å¹¶ä¸å—å›°æ‰°ã€‚
- en: 'We can even demonstrate this ourselves using the feed-forward ReLU network
    we built in [ChapterÂ 5](ch05.xhtml#neural_networks_in_pytorch). Using a checkpoint
    file that we saved while training our original feed-forward network, we can reinstantiate
    the model using `load_state_dict` and `torch.load`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”šè‡³å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨[ç¬¬5ç« ](ch05.xhtml#neural_networks_in_pytorch)ä¸­æ„å»ºçš„å‰é¦ˆReLUç½‘ç»œæ¥æ¼”ç¤ºè¿™ä¸€ç‚¹ã€‚ä½¿ç”¨æˆ‘ä»¬åœ¨è®­ç»ƒåŸå§‹å‰é¦ˆç½‘ç»œæ—¶ä¿å­˜çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`load_state_dict`å’Œ`torch.load`é‡æ–°å®ä¾‹åŒ–æ¨¡å‹ï¼š
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In PyTorch, we cannot access a modelâ€™s parameters directly since the `model.parameters()`
    method returns a generator that provides only a *copy* of the parameters. To modify
    a modelâ€™s parameters, we use `torch.load` to read the state dictionary containing
    the parameter values from the file, and then use `load_state_dict` to set the
    modelâ€™s parameters with these values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyTorchä¸­ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¿é—®æ¨¡å‹çš„å‚æ•°ï¼Œå› ä¸º`model.parameters()`æ–¹æ³•è¿”å›ä¸€ä¸ªä»…æä¾›å‚æ•°*å‰¯æœ¬*çš„ç”Ÿæˆå™¨ã€‚è¦ä¿®æ”¹æ¨¡å‹çš„å‚æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨`torch.load`ä»æ–‡ä»¶ä¸­è¯»å–åŒ…å«å‚æ•°å€¼çš„çŠ¶æ€å­—å…¸ï¼Œç„¶åä½¿ç”¨`load_state_dict`å°†æ¨¡å‹çš„å‚æ•°è®¾ç½®ä¸ºè¿™äº›å€¼ã€‚
- en: 'Instead of using `torch.load` to load the state dictionary from a file, we
    can also access the state dictionary from a model itself using the `state_dict`
    method:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨`state_dict`æ–¹æ³•ä»æ¨¡å‹æœ¬èº«è®¿é—®çŠ¶æ€å­—å…¸ï¼Œè€Œä¸æ˜¯ä½¿ç”¨`torch.load`ä»æ–‡ä»¶åŠ è½½çŠ¶æ€å­—å…¸ï¼š
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we need to use the `copy.deepcopy` method to copy a dictionary with
    its values. Just setting `opt_state_dict = model.state_dict()`Â would result in
    a shallow copy, and `opt_state_dict` would be changed when we load our model with
    interpolated parameters later.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`copy.deepcopy`æ–¹æ³•æ¥å¤åˆ¶å¸¦æœ‰å…¶å€¼çš„å­—å…¸ã€‚åªè®¾ç½®`opt_state_dict = model.state_dict()`ä¼šå¯¼è‡´æµ…å¤åˆ¶ï¼Œå¹¶ä¸”å½“æˆ‘ä»¬ç¨ååŠ è½½å…·æœ‰æ’å€¼å‚æ•°çš„æ¨¡å‹æ—¶ï¼Œ`opt_state_dict`ä¼šå‘ç”Ÿæ›´æ”¹ã€‚
- en: 'Next, we instantiate a new model with randomly initialized parameters and save
    those parameters as `rand_state_dict`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ªå…·æœ‰éšæœºåˆå§‹åŒ–å‚æ•°çš„æ–°æ¨¡å‹ï¼Œå¹¶å°†è¿™äº›å‚æ•°ä¿å­˜ä¸º`rand_state_dict`ï¼š
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With these two networks appropriately initialized, we can now construct the
    linear interpolation using the mixing parametersÂ `alpha`Â and `beta`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™ä¸¤ä¸ªç½‘ç»œé€‚å½“åˆå§‹åŒ–åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨æ··åˆå‚æ•°`alpha`å’Œ`beta`æ„å»ºçº¿æ€§æ’å€¼ï¼š
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will compute the average loss over the entire test dataset using the
    model with the interpolated parameters.Â  For convenience,Â letâ€™s create a function
    for inference:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å…·æœ‰æ’å€¼å‚æ•°çš„æ¨¡å‹è®¡ç®—æ•´ä¸ªæµ‹è¯•æ•°æ®é›†ä¸Šçš„å¹³å‡æŸå¤±ã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨ç†å‡½æ•°ï¼š
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally,Â Â we can vary the value ofÂ `alpha`Â to understand how the error surface
    changes as we traverse the line between the randomly initialized point and the
    final SGD solution:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜`alpha`çš„å€¼ï¼Œä»¥äº†è§£åœ¨ç©¿è¿‡éšæœºåˆå§‹åŒ–ç‚¹å’Œæœ€ç»ˆSGDè§£ä¹‹é—´çš„çº¿è·¯æ—¶ï¼Œè¯¯å·®æ›²é¢å¦‚ä½•å˜åŒ–ï¼š
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This creates [FigureÂ 6-3](#cost_function_of_a_three_layer_feedforward_network),
    which we can inspect ourselves. In fact, if we run this experiment over and over
    again, we find that there are no truly troublesome local minima that would get
    us stuck. It seems that the true struggle of gradient descent isnâ€™t the existence
    of troublesome local minima, but instead is that we have a tough time finding
    the appropriate direction to move in. Weâ€™ll return to this thought a little later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åˆ›å»ºäº†[å›¾6-3](#cost_function_of_a_three_layer_feedforward_network)ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªå·±æ£€æŸ¥ã€‚å®é™…ä¸Šï¼Œå¦‚æœæˆ‘ä»¬ä¸€éåˆä¸€éåœ°è¿è¡Œè¿™ä¸ªå®éªŒï¼Œæˆ‘ä»¬ä¼šå‘ç°æ²¡æœ‰çœŸæ­£å›°æ‰°çš„å±€éƒ¨æœ€å°å€¼ä¼šè®©æˆ‘ä»¬é™·å…¥å›°å¢ƒã€‚çœ‹æ¥æ¢¯åº¦ä¸‹é™çš„çœŸæ­£æŒ‘æˆ˜ä¸æ˜¯å­˜åœ¨å›°æ‰°çš„å±€éƒ¨æœ€å°å€¼ï¼Œè€Œæ˜¯æˆ‘ä»¬å¾ˆéš¾æ‰¾åˆ°é€‚å½“çš„ç§»åŠ¨æ–¹å‘ã€‚æˆ‘ä»¬ç¨åä¼šå›åˆ°è¿™ä¸ªæƒ³æ³•ã€‚
- en: '![](Images/fdl2_0603.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0603.png)'
- en: Figure 6-3\. The cost function of a three-layer feed-forward network as we linearly
    interpolate on the line connecting a randomly initialized parameter vector and
    an SGD solution
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-3\. ä¸‰å±‚å‰é¦ˆç½‘ç»œçš„æˆæœ¬å‡½æ•°ï¼Œå½“æˆ‘ä»¬åœ¨çº¿æ€§æ’å€¼è¿æ¥éšæœºåˆå§‹åŒ–çš„å‚æ•°å‘é‡å’Œéšæœºæ¢¯åº¦ä¸‹é™è§£æ—¶
- en: Flat Regions in the Error Surface
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¯å·®è¡¨é¢ä¸­çš„å¹³å¦åŒºåŸŸ
- en: Although Â it seems that our analysis is devoid of troublesome local minimum,
    we do notice a peculiar flat region where the gradient approaches zero when we
    get to approximatelyÂ `alpha=1`. This point is not a local minima, so it is unlikely
    to get us completely stuck, but it seems like the zero gradient might slow down
    learning if we are unlucky enough to encounter it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬çš„åˆ†æä¼¼ä¹æ²¡æœ‰ä»¤äººå¤´ç–¼çš„å±€éƒ¨æœ€å°å€¼ï¼Œä½†æˆ‘ä»¬æ³¨æ„åˆ°ä¸€ä¸ªå¥‡æ€ªçš„å¹³å¦åŒºåŸŸï¼Œåœ¨å¤§çº¦`alpha=1`æ—¶æ¢¯åº¦æ¥è¿‘é›¶ã€‚è¿™ä¸ªç‚¹ä¸æ˜¯å±€éƒ¨æœ€å°å€¼ï¼Œæ‰€ä»¥ä¸å¤ªå¯èƒ½å®Œå…¨å¡ä½æˆ‘ä»¬ï¼Œä½†å¦‚æœæˆ‘ä»¬ä¸èµ°è¿é‡åˆ°å®ƒï¼Œé›¶æ¢¯åº¦å¯èƒ½ä¼šå‡æ…¢å­¦ä¹ é€Ÿåº¦ã€‚
- en: More generally, given an arbitrary function, a point at which the gradient is
    the zero vector is called a *critical point*. Critical points come in various
    flavors. Weâ€™ve already talked about local minima. Itâ€™s also not hard to imagine
    their counterparts, theÂ *local maxima*, which donâ€™t really pose much of an issue
    for SGD. But then there are these strange critical points that lie somewhere in
    between. These â€œflatâ€ regions that are potentially pesky but not necessarily deadly
    are calledÂ *saddle points*. It turns out that as our function has more and more
    dimensions (i.e., we have more and more parameters in our model), saddle points
    are exponentially more likely than local minima. Letâ€™s try to intuit why.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸€èˆ¬åœ°ï¼Œå¯¹äºä»»æ„å‡½æ•°ï¼Œæ¢¯åº¦ä¸ºé›¶å‘é‡çš„ç‚¹è¢«ç§°ä¸º*ä¸´ç•Œç‚¹*ã€‚ä¸´ç•Œç‚¹æœ‰å„ç§ä¸åŒçš„ç±»å‹ã€‚æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡å±€éƒ¨æœ€å°å€¼ã€‚å¾ˆå®¹æ˜“æƒ³è±¡å®ƒä»¬çš„å¯¹åº”ç‰©ï¼Œå³*å±€éƒ¨æœ€å¤§å€¼*ï¼Œå¯¹äºéšæœºæ¢¯åº¦ä¸‹é™å¹¶ä¸æ„æˆå¤ªå¤§é—®é¢˜ã€‚ä½†æ˜¯è¿˜æœ‰è¿™äº›å¥‡æ€ªçš„ä¸´ç•Œç‚¹ï¼Œå®ƒä»¬ä½äºä¸¤è€…ä¹‹é—´ã€‚è¿™äº›â€œå¹³å¦â€åŒºåŸŸå¯èƒ½ä¼šè®©äººå¤´ç–¼ï¼Œä½†ä¸ä¸€å®šè‡´å‘½ï¼Œè¢«ç§°ä¸º*éç‚¹*ã€‚äº‹å®è¯æ˜ï¼Œéšç€æˆ‘ä»¬çš„å‡½æ•°å…·æœ‰è¶Šæ¥è¶Šå¤šçš„ç»´åº¦ï¼ˆå³æˆ‘ä»¬çš„æ¨¡å‹ä¸­æœ‰è¶Šæ¥è¶Šå¤šçš„å‚æ•°ï¼‰ï¼Œéç‚¹æ¯”å±€éƒ¨æœ€å°å€¼æ›´æœ‰å¯èƒ½ã€‚è®©æˆ‘ä»¬è¯•ç€ç›´è§‚åœ°ç†è§£ä¸ºä»€ä¹ˆã€‚
- en: For a 1D cost function, a critical point can take one of three forms, as shown
    inÂ [FigureÂ 6-4](#analyzing_a_critical_point). Â Loosely, letâ€™s assume each of these
    three configurations is equally likely. This means that given a random critical
    point in a random 1D function, it has one-third probability of being a local minimum.
    This means that if we have a total ofÂ  <math alttext="k"><mi>k</mi></math> Â critical
    points, we can expect to have a total ofÂ  <math alttext="StartFraction k Over
    3 EndFraction"><mfrac><mi>k</mi> <mn>3</mn></mfrac></math> Â local minima.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªä¸€ç»´æˆæœ¬å‡½æ•°ï¼Œä¸´ç•Œç‚¹å¯ä»¥é‡‡å–ä¸‰ç§å½¢å¼ï¼Œå¦‚[å›¾6-4](#analyzing_a_critical_point)æ‰€ç¤ºã€‚ç²—ç•¥åœ°è¯´ï¼Œè®©æˆ‘ä»¬å‡è®¾è¿™ä¸‰ç§é…ç½®æ˜¯ç­‰å¯èƒ½çš„ã€‚è¿™æ„å‘³ç€åœ¨ä¸€ä¸ªéšæœºçš„ä¸€ç»´å‡½æ•°ä¸­ï¼Œç»™å®šä¸€ä¸ªéšæœºçš„ä¸´ç•Œç‚¹ï¼Œå®ƒæœ‰ä¸‰åˆ†ä¹‹ä¸€çš„æ¦‚ç‡æ˜¯å±€éƒ¨æœ€å°å€¼ã€‚è¿™æ„å‘³ç€å¦‚æœæˆ‘ä»¬æ€»å…±æœ‰
    <math alttext="k"><mi>k</mi></math> ä¸ªä¸´ç•Œç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›æ€»å…±æœ‰ <math alttext="StartFraction
    k Over 3 EndFraction"><mfrac><mi>k</mi> <mn>3</mn></mfrac></math> ä¸ªå±€éƒ¨æœ€å°å€¼ã€‚
- en: '![](Images/fdl2_0604.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0604.png)'
- en: Figure 6-4\. Analyzing a critical point along a single dimension
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-4\. æ²¿å•ä¸ªç»´åº¦åˆ†æä¸´ç•Œç‚¹
- en: We can also extend this to higher dimensional functions. Consider a cost function
    operating in a <math alttext="d"><mi>d</mi></math> -dimensional space. Letâ€™s take
    an arbitrary critical point. It turns out that figuring out if this point is a
    local minimum, local maximum, or a saddle point is a little bit trickier than
    in the one-dimensional case. Consider the error surface inÂ [FigureÂ 6-5](#saddle_point_over_a_two_dimensional_error_surface).
    Depending on how you slice the surface (from A to B or from C to D), the critical
    point looks like either a minimum or a maximum. In reality, itâ€™s neither. Itâ€™s
    a more complex type of saddle point.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†è¿™ä¸ªæ‰©å±•åˆ°æ›´é«˜ç»´åº¦çš„å‡½æ•°ã€‚è€ƒè™‘ä¸€ä¸ªåœ¨ <math alttext="d"><mi>d</mi></math> ç»´ç©ºé—´ä¸­è¿è¡Œçš„æˆæœ¬å‡½æ•°ã€‚è®©æˆ‘ä»¬å–ä¸€ä¸ªä»»æ„çš„ä¸´ç•Œç‚¹ã€‚äº‹å®è¯æ˜ï¼Œç¡®å®šè¿™ä¸€ç‚¹æ˜¯å±€éƒ¨æœ€å°å€¼ã€å±€éƒ¨æœ€å¤§å€¼è¿˜æ˜¯éç‚¹æ¯”ä¸€ç»´æƒ…å†µè¦æ£˜æ‰‹ä¸€äº›ã€‚è€ƒè™‘[å›¾6-5](#saddle_point_over_a_two_dimensional_error_surface)ä¸­çš„è¯¯å·®è¡¨é¢ã€‚æ ¹æ®ä½ å¦‚ä½•åˆ‡å‰²è¡¨é¢ï¼ˆä»Aåˆ°Bè¿˜æ˜¯ä»Cåˆ°Dï¼‰ï¼Œä¸´ç•Œç‚¹çœ‹èµ·æ¥åƒæœ€å°å€¼æˆ–æœ€å¤§å€¼ã€‚å®é™…ä¸Šï¼Œå®ƒæ—¢ä¸æ˜¯æœ€å°å€¼ä¹Ÿä¸æ˜¯æœ€å¤§å€¼ã€‚å®ƒæ˜¯ä¸€ç§æ›´å¤æ‚çš„éç‚¹ã€‚
- en: '![](Images/fdl2_0605.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0605.png)'
- en: Figure 6-5\. A saddle point over a 2D error surface
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-5\. äºŒç»´è¯¯å·®è¡¨é¢ä¸Šçš„ä¸€ä¸ªéç‚¹
- en: In general, in aÂ  <math alttext="d"><mi>d</mi></math> -dimensional parameter
    space, we can slice through a critical point onÂ  <math alttext="d"><mi>d</mi></math>
    Â different axes. A critical point can be a local minimum only if it appears as
    a local minimum in every single one of theÂ  <math alttext="d"><mi>d</mi></math>
    Â 1D subspaces. Using the fact that a critical point can come in one of three different
    flavors in a one-dimensional subspace, we realize that the probability that a
    random critical point is in a random function isÂ  <math alttext="StartFraction
    1 Over 3 Superscript d Baseline EndFraction"><mfrac><mn>1</mn> <msup><mn>3</mn>
    <mi>d</mi></msup></mfrac></math> . This means that a random function withÂ  <math
    alttext="k"><mi>k</mi></math> Â critical points has an expected number ofÂ  <math
    alttext="StartFraction k Over 3 Superscript d Baseline EndFraction"><mfrac><mi>k</mi>
    <msup><mn>3</mn> <mi>d</mi></msup></mfrac></math> Â local minima. In other words,
    as the dimensionality of our parameter space increases, local minima become exponentially
    more rare. A more rigorous treatment of this topic is outside the scope of this
    book, but was explored more extensively by Dauphin et al. in 2014.^([3](ch06.xhtml#idm45934164995408))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨ä¸€ä¸ª <math alttext="d"><mi>d</mi></math> ç»´å‚æ•°ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ <math alttext="d"><mi>d</mi></math>
    ä¸ªä¸åŒçš„è½´åˆ‡è¿‡ä¸€ä¸ªä¸´ç•Œç‚¹ã€‚ä¸€ä¸ªä¸´ç•Œç‚¹åªæœ‰åœ¨æ¯ä¸€ä¸ª <math alttext="d"><mi>d</mi></math> ä¸ªä¸€ç»´å­ç©ºé—´ä¸­éƒ½å‡ºç°ä¸ºå±€éƒ¨æœ€å°å€¼æ—¶æ‰èƒ½æˆä¸ºå±€éƒ¨æœ€å°å€¼ã€‚åˆ©ç”¨ä¸€ä¸ªä¸´ç•Œç‚¹åœ¨ä¸€ç»´å­ç©ºé—´ä¸­æœ‰ä¸‰ç§ä¸åŒç±»å‹çš„äº‹å®ï¼Œæˆ‘ä»¬æ„è¯†åˆ°ä¸€ä¸ªéšæœºä¸´ç•Œç‚¹åœ¨éšæœºå‡½æ•°ä¸­çš„æ¦‚ç‡æ˜¯
    <math alttext="StartFraction 1 Over 3 Superscript d Baseline EndFraction"><mfrac><mn>1</mn>
    <msup><mn>3</mn> <mi>d</mi></msup></mfrac></math> ã€‚è¿™æ„å‘³ç€ä¸€ä¸ªå…·æœ‰ <math alttext="k"><mi>k</mi></math>
    ä¸ªä¸´ç•Œç‚¹çš„éšæœºå‡½æ•°æœ‰æœ›æœ‰ <math alttext="StartFraction k Over 3 Superscript d Baseline EndFraction"><mfrac><mi>k</mi>
    <msup><mn>3</mn> <mi>d</mi></msup></mfrac></math> ä¸ªå±€éƒ¨æœ€å°å€¼ã€‚æ¢å¥è¯è¯´ï¼Œéšç€å‚æ•°ç©ºé—´çš„ç»´åº¦å¢åŠ ï¼Œå±€éƒ¨æœ€å°å€¼å˜å¾—è¶Šæ¥è¶Šç¨€æœ‰ã€‚å¯¹è¿™ä¸ªä¸»é¢˜çš„æ›´ä¸¥æ ¼çš„å¤„ç†è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ï¼Œä½†åœ¨2014å¹´ç”±Dauphinç­‰äººè¿›è¡Œäº†æ›´æ·±å…¥çš„æ¢è®¨ã€‚^([3](ch06.xhtml#idm45934164995408))
- en: So what does this mean for optimizing deep learning models? For stochastic gradient
    descent, itâ€™s still unclear. It seems like these flat segments of the error surface
    are pesky but ultimately donâ€™t prevent stochastic gradient descent from converging
    to a good answer. However, it does pose serious problems for methods that attempt
    to directly solve for a point where the gradient is zero. This has been a major
    hindrance to the usefulness of certain second-order optimization methods for deep
    learning models, which we will discuss later.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè¿™å¯¹äºä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹æ„å‘³ç€ä»€ä¹ˆï¼Ÿå¯¹äºéšæœºæ¢¯åº¦ä¸‹é™ï¼Œç›®å‰è¿˜ä¸æ¸…æ¥šã€‚çœ‹èµ·æ¥è¿™äº›é”™è¯¯è¡¨é¢çš„å¹³å¦æ®µæ˜¯æ£˜æ‰‹çš„ï¼Œä½†æœ€ç»ˆå¹¶ä¸ä¼šé˜»æ­¢éšæœºæ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°ä¸€ä¸ªå¥½çš„ç­”æ¡ˆã€‚ç„¶è€Œï¼Œå¯¹äºè¯•å›¾ç›´æ¥è§£å†³æ¢¯åº¦ä¸ºé›¶ç‚¹çš„æ–¹æ³•ï¼Œè¿™å¯¹äºæŸäº›äºŒé˜¶ä¼˜åŒ–æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„æœ‰ç”¨æ€§æ„æˆäº†ä¸¥é‡é—®é¢˜ï¼Œæˆ‘ä»¬å°†åœ¨åé¢è®¨è®ºã€‚
- en: When the Gradient Points in the Wrong Direction
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å½“æ¢¯åº¦æŒ‡å‘é”™è¯¯æ–¹å‘æ—¶
- en: Upon analyzing the error surfaces of deep networks, it seems like the most critical
    challenge to optimizing deep networks is finding the correct trajectory to move
    in. Itâ€™s no surprise, however, that this is a major challenge when we look at
    what happens to the error surface around a local minimum. As an example, we consider
    an error surface defined over a 2D parameter space, as shown in [FigureÂ 6-6](#local_information_encoded_by_the_gradient).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†ææ·±åº¦ç½‘ç»œçš„é”™è¯¯è¡¨é¢æ—¶ï¼Œä¼¼ä¹ä¼˜åŒ–æ·±åº¦ç½‘ç»œæœ€å…³é”®çš„æŒ‘æˆ˜æ˜¯æ‰¾åˆ°æ­£ç¡®çš„ç§»åŠ¨è½¨è¿¹ã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬çœ‹çœ‹å±€éƒ¨æœ€å°å€¼å‘¨å›´çš„é”™è¯¯è¡¨é¢å‘ç”Ÿäº†ä»€ä¹ˆæ—¶ï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬è€ƒè™‘åœ¨2Då‚æ•°ç©ºé—´ä¸Šå®šä¹‰çš„é”™è¯¯è¡¨é¢ï¼Œå¦‚[å›¾6-6](#local_information_encoded_by_the_gradient)æ‰€ç¤ºã€‚
- en: '![](Images/fdl2_0606.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0606.png)'
- en: Figure 6-6\. Local information encoded by the gradient usually does not corroborate
    the global structure of the error surface
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-6ã€‚æ¢¯åº¦ç¼–ç çš„å±€éƒ¨ä¿¡æ¯é€šå¸¸ä¸ç¬¦åˆé”™è¯¯è¡¨é¢çš„å…¨å±€ç»“æ„
- en: Revisiting the contour diagrams we explored in [ChapterÂ 4](ch04.xhtml#training_feed_forward),
    notice that the gradient isnâ€™t usually a very good indicator of the good trajectory.
    Specifically, only when the contours are perfectly circular does the gradient
    always point in the direction of the local minimum. However, if the contours are
    extremely elliptical (as is usually the case for the error surfaces of deep networks),
    the gradient can be as inaccurate as 90 degrees away from the correct direction.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ–°å®¡è§†æˆ‘ä»¬åœ¨[ç¬¬4ç« ](ch04.xhtml#training_feed_forward)ä¸­æ¢ç´¢çš„ç­‰é«˜çº¿å›¾ï¼Œæ³¨æ„æ¢¯åº¦é€šå¸¸ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è‰¯å¥½è½¨è¿¹æŒ‡ç¤ºå™¨ã€‚å…·ä½“æ¥è¯´ï¼Œåªæœ‰å½“ç­‰é«˜çº¿æ˜¯å®Œå…¨åœ†å½¢æ—¶ï¼Œæ¢¯åº¦æ‰æ€»æ˜¯æŒ‡å‘å±€éƒ¨æœ€å°å€¼çš„æ–¹å‘ã€‚ç„¶è€Œï¼Œå¦‚æœç­‰é«˜çº¿æç«¯æ¤­åœ†å½¢ï¼ˆé€šå¸¸æ˜¯æ·±åº¦ç½‘ç»œçš„é”™è¯¯è¡¨é¢çš„æƒ…å†µï¼‰ï¼Œæ¢¯åº¦å¯èƒ½ä¸æ­£ç¡®æ–¹å‘ç›¸å·®90åº¦ã€‚
- en: We extend this analysis to an arbitrary number of dimensions using some mathematical
    formalism. For every weight <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    Â in the parameter space, the gradient computes the value ofÂ  <math alttext="StartFraction
    normal partial-differential upper E Over normal partial-differential w Subscript
    i Baseline EndFraction"><mfrac><mrow><mi>âˆ‚</mi><mi>E</mi></mrow> <mrow><mi>âˆ‚</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></mfrac></math> , or how the value of the error changes
    as we change the value ofÂ  <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    . Taken together over all weights in the parameter space, the gradient gives us
    the direction of steepest descent. The general problem with taking a significant
    step in this direction, however, is that the gradient could be changing under
    our feet as we move! We demonstrate this simple fact in [FigureÂ 6-7](#direction_of_gradient_changes).
    Going back to the 2D example, if our contours are perfectly circular and we take
    a big step in the direction of the steepest descent, the gradient doesnâ€™t change
    direction as we move. However, this is not the case for highly elliptical contours.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä¸€äº›æ•°å­¦å½¢å¼å°†è¿™ç§åˆ†ææ‰©å±•åˆ°ä»»æ„ç»´åº¦ã€‚å¯¹äºå‚æ•°ç©ºé—´ä¸­çš„æ¯ä¸ªæƒé‡<math alttext="wä¸‹æ ‡i"><msub><mi>w</mi> <mi>i</mi></msub></math>ï¼Œæ¢¯åº¦è®¡ç®—<math
    alttext="StartFraction normal partial-differential upper E Over normal partial-differential
    wä¸‹æ ‡i Baseline EndFraction"><mfrac><mrow><mi>âˆ‚</mi><mi>E</mi></mrow> <mrow><mi>âˆ‚</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></mfrac></math>çš„å€¼ï¼Œæˆ–è€…å½“æˆ‘ä»¬æ”¹å˜<math alttext="wä¸‹æ ‡i"><msub><mi>w</mi>
    <mi>i</mi></msub></math>çš„å€¼æ—¶ï¼Œé”™è¯¯å€¼çš„å˜åŒ–ã€‚åœ¨å‚æ•°ç©ºé—´ä¸­çš„æ‰€æœ‰æƒé‡ä¸Šç»¼åˆè€ƒè™‘ï¼Œæ¢¯åº¦ç»™å‡ºäº†æœ€é™¡ä¸‹é™çš„æ–¹å‘ã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸ªæ–¹å‘ä¸Šè¿ˆå‡ºé‡è¦æ­¥éª¤çš„ä¸€èˆ¬é—®é¢˜æ˜¯ï¼Œå½“æˆ‘ä»¬ç§»åŠ¨æ—¶ï¼Œæ¢¯åº¦å¯èƒ½åœ¨æˆ‘ä»¬è„šä¸‹æ”¹å˜ï¼æˆ‘ä»¬åœ¨[å›¾6-7](#direction_of_gradient_changes)ä¸­æ¼”ç¤ºäº†è¿™ä¸ªç®€å•äº‹å®ã€‚å›åˆ°2Dç¤ºä¾‹ï¼Œå¦‚æœæˆ‘ä»¬çš„ç­‰é«˜çº¿æ˜¯å®Œå…¨åœ†å½¢çš„ï¼Œå¹¶ä¸”æˆ‘ä»¬æœæœ€é™¡ä¸‹é™çš„æ–¹å‘è¿ˆå‡ºä¸€å¤§æ­¥ï¼Œæ¢¯åº¦åœ¨æˆ‘ä»¬ç§»åŠ¨æ—¶ä¸ä¼šæ”¹å˜æ–¹å‘ã€‚ç„¶è€Œï¼Œå¯¹äºé«˜åº¦æ¤­åœ†å½¢çš„ç­‰é«˜çº¿ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ã€‚
- en: '![](Images/fdl2_0607.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0607.png)'
- en: Figure 6-7\. The direction of the gradient changes as we move along the direction
    of steepest descent, as determined from a starting point; the gradient vectors
    are normalized to identical length to emphasize the change in direction of the
    gradient vector
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-7ã€‚å½“æˆ‘ä»¬æ²¿ç€æœ€é™¡ä¸‹é™çš„æ–¹å‘ç§»åŠ¨æ—¶ï¼Œæ¢¯åº¦çš„æ–¹å‘ä¼šæ”¹å˜ï¼Œè¿™æ˜¯ä»ä¸€ä¸ªèµ·å§‹ç‚¹ç¡®å®šçš„ï¼›æ¢¯åº¦å‘é‡è¢«å½’ä¸€åŒ–ä¸ºç›¸åŒé•¿åº¦ï¼Œä»¥å¼ºè°ƒæ¢¯åº¦å‘é‡æ–¹å‘çš„å˜åŒ–
- en: More generally, we can quantify how the gradient changes under our feet as we
    move in a certain direction by computing second derivatives. Specifically, we
    want to measureÂ  <math alttext="StartFraction normal partial-differential left-parenthesis
    normal partial-differential upper E slash normal partial-differential w Subscript
    j Baseline right-parenthesis Over normal partial-differential w Subscript i Baseline
    EndFraction"><mfrac><mrow><mi>âˆ‚</mi><mfenced separators="" open="(" close=")"><mi>âˆ‚</mi><mi>E</mi><mo>/</mo><mi>âˆ‚</mi><msub><mi>w</mi>
    <mi>j</mi></msub></mfenced></mrow> <mrow><mi>âˆ‚</mi><msub><mi>w</mi> <mi>i</mi></msub></mrow></mfrac></math>
    , which tells us how the gradient component forÂ  <math alttext="w Subscript j"><msub><mi>w</mi>
    <mi>j</mi></msub></math> Â changes as we change the value ofÂ  <math alttext="w
    Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> . We can compile this information
    into a special matrix known as the *Hessian matrix* (***H***). And when describing
    an error surface where the gradient changes underneath our feet as we move in
    the direction of steepest descent, this matrix is said to beÂ *ill-conditioned*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸€èˆ¬åœ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—äºŒé˜¶å¯¼æ•°æ¥é‡åŒ–æ¢¯åº¦åœ¨æˆ‘ä»¬ç§»åŠ¨æŸä¸ªæ–¹å‘æ—¶çš„å˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æƒ³è¦æµ‹é‡ <math alttext="StartFraction
    normal partial-differential left-parenthesis normal partial-differential upper
    E slash normal partial-differential w Subscript j Baseline right-parenthesis Over
    normal partial-differential w Subscript i Baseline EndFraction"> <mfrac> <mrow>
    <mi>âˆ‚</mi> <mfenced separators="" open="(" close=")"> <mi>âˆ‚</mi> <mi>E</mi> <mo>/</mo>
    <mi>âˆ‚</mi> <msub> <mi>w</mi> <mi>j</mi> </msub> </mfenced> </mrow> <mrow> <mi>âˆ‚</mi>
    <msub> <mi>w</mi> <mi>i</mi> </msub> </mrow> </mfrac> </math>ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬å½“æˆ‘ä»¬æ”¹å˜ <math
    alttext="w Subscript i"> <msub> <mi>w</mi> <mi>i</mi> </msub> </math> çš„å€¼æ—¶ï¼Œ <math
    alttext="w Subscript j"> <msub> <mi>w</mi> <mi>j</mi> </msub> </math> çš„æ¢¯åº¦åˆ†é‡å¦‚ä½•å˜åŒ–ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›ä¿¡æ¯ç¼–è¯‘æˆä¸€ä¸ªç‰¹æ®Šçš„çŸ©é˜µï¼Œç§°ä¸º*HessiançŸ©é˜µ*ï¼ˆ***H***ï¼‰ã€‚å½“æè¿°ä¸€ä¸ªé”™è¯¯è¡¨é¢ï¼Œåœ¨è¿™ä¸ªè¡¨é¢ä¸Šï¼Œå½“æˆ‘ä»¬æ²¿ç€æœ€é™¡é™æ–¹å‘ç§»åŠ¨æ—¶ï¼Œæ¢¯åº¦åœ¨æˆ‘ä»¬è„šä¸‹å‘ç”Ÿå˜åŒ–æ—¶ï¼Œè¿™ä¸ªçŸ©é˜µè¢«ç§°ä¸º*ç—…æ€*ã€‚
- en: Momentum-Based Optimization
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºåŠ¨é‡çš„ä¼˜åŒ–
- en: Fundamentally, the problem of an ill-conditioned Hessian matrix manifests itself
    in the form of gradients that fluctuate wildly. As a result, one popular mechanism
    for dealing with ill-conditioning bypasses the computation of the Hessian, and
    instead, focuses on how to cancel out these fluctuations over the duration of
    training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ ¹æœ¬ä¸Šè®²ï¼Œç—…æ€HessiançŸ©é˜µçš„é—®é¢˜è¡¨ç°ä¸ºæ¢¯åº¦å‰§çƒˆæ³¢åŠ¨ã€‚å› æ­¤ï¼Œå¤„ç†ç—…æ€çš„ä¸€ç§æµè¡Œæœºåˆ¶æ˜¯ç»•è¿‡Hessiançš„è®¡ç®—ï¼Œè€Œæ˜¯ä¸“æ³¨äºå¦‚ä½•åœ¨è®­ç»ƒæœŸé—´æŠµæ¶ˆè¿™äº›æ³¢åŠ¨ã€‚
- en: One way to think about how we might tackle this problem is by investigating
    how a ball rolls down a hilly surface. Driven by gravity, the ball eventually
    settles into a minimum on the surface, but for some reason, it doesnâ€™t suffer
    from the wild fluctuations and divergences that happen during gradient descent.
    Why is this the case? Unlike in stochastic gradient descent (which uses only the
    gradient), there are two major components that determine how a ball rolls down
    an error surface. The first, which we already model in SGD as the gradient, is
    what we commonly refer to as acceleration. But acceleration does not single-handedly
    determine the ballâ€™s movements. Instead, its motion is more directly determined
    by its velocity. Acceleration indirectly changes the ballâ€™s position only by modifying
    its velocity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ç ”ç©¶çƒå¦‚ä½•æ²¿ç€å±±å¡æ»šåŠ¨æ¥æ€è€ƒå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å—é‡åŠ›é©±åŠ¨ï¼Œçƒæœ€ç»ˆä¼šè½å…¥è¡¨é¢ä¸Šçš„æœ€å°å€¼ï¼Œä½†å‡ºäºæŸç§åŸå› ï¼Œåœ¨æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­ä¸ä¼šå‡ºç°é‡è›®æ³¢åŠ¨å’Œå‘æ•£ã€‚ä¸ºä»€ä¹ˆä¼šè¿™æ ·å‘¢ï¼Ÿä¸éšæœºæ¢¯åº¦ä¸‹é™ä¸åŒï¼ˆä»…ä½¿ç”¨æ¢¯åº¦ï¼‰ï¼Œæœ‰ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†å†³å®šäº†çƒå¦‚ä½•æ²¿ç€è¯¯å·®è¡¨é¢æ»šåŠ¨ã€‚ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬åœ¨éšæœºæ¢¯åº¦ä¸‹é™ä¸­å·²ç»å»ºæ¨¡ä¸ºæ¢¯åº¦ï¼Œé€šå¸¸ç§°ä¸ºåŠ é€Ÿåº¦ã€‚ä½†åŠ é€Ÿåº¦å¹¶ä¸èƒ½å•ç‹¬å†³å®šçƒçš„è¿åŠ¨ã€‚ç›¸åï¼Œå®ƒçš„è¿åŠ¨æ›´ç›´æ¥åœ°ç”±å…¶é€Ÿåº¦å†³å®šã€‚åŠ é€Ÿåº¦åªé€šè¿‡ä¿®æ”¹é€Ÿåº¦é—´æ¥æ”¹å˜çƒçš„ä½ç½®ã€‚
- en: 'Velocity-driven motion is desirable because it counteracts the effects of a
    wildly fluctuating gradient by smoothing the ballâ€™s trajectory over its history.
    Velocity serves as a form of memory, and this allows us to more effectively accumulate
    movement in the direction of the minimum while canceling out oscillating accelerations
    in orthogonal directions. Our goal, then, is to somehow generate an analog for
    velocity in our optimization algorithm. We can do this by keeping track of an
    *exponentially weighted decay* of past gradients. The premise is simple: every
    update is computed by combining the update in the last iteration with the current
    gradient. Concretely, we compute the change in the parameter vector as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºé€Ÿåº¦çš„è¿åŠ¨æ˜¯å¯å–çš„ï¼Œå› ä¸ºå®ƒé€šè¿‡åœ¨å†å²ä¸Šå¹³æ»‘çƒçš„è½¨è¿¹æ¥æŠµæ¶ˆæ¢¯åº¦çš„å‰§çƒˆæ³¢åŠ¨ã€‚é€Ÿåº¦ä½œä¸ºä¸€ç§è®°å¿†å½¢å¼ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åœ¨æœ€å°å€¼æ–¹å‘ä¸Šç§¯ç´¯è¿åŠ¨ï¼ŒåŒæ—¶æŠµæ¶ˆæ­£äº¤æ–¹å‘ä¸Šçš„æŒ¯è¡åŠ é€Ÿåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨æˆ‘ä»¬çš„ä¼˜åŒ–ç®—æ³•ä¸­ç”Ÿæˆé€Ÿåº¦çš„ç±»ä¼¼ç‰©ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è·Ÿè¸ªè¿‡å»æ¢¯åº¦çš„*æŒ‡æ•°åŠ æƒè¡°å‡*æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™ä¸ªå‰æå¾ˆç®€å•ï¼šæ¯æ¬¡æ›´æ–°éƒ½æ˜¯é€šè¿‡å°†ä¸Šä¸€æ¬¡è¿­ä»£çš„æ›´æ–°ä¸å½“å‰æ¢¯åº¦ç›¸ç»“åˆæ¥è®¡ç®—çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¡ç®—å‚æ•°å‘é‡çš„å˜åŒ–å¦‚ä¸‹ï¼š
- en: <math alttext="bold v Subscript i Baseline equals m bold v Subscript i minus
    1 Baseline minus epsilon bold g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>m</mi> <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mi>Ïµ</mi> <msub><mi>ğ </mi> <mi>i</mi></msub></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals m bold v Subscript i minus
    1 Baseline minus epsilon bold g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>m</mi> <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mi>Ïµ</mi> <msub><mi>ğ </mi> <mi>i</mi></msub></mrow></math>
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    plus bold v Subscript i"><mrow><msub><mi>Î¸</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>Î¸</mi>
    <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msub><mi>ğ¯</mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    plus bold v Subscript i"><mrow><msub><mi>Î¸</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>Î¸</mi>
    <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msub><mi>ğ¯</mi>
    <mi>i</mi></msub></mrow></math>
- en: We use the momentum hyperparameter <math alttext="m"><mi>m</mi></math> to determine
    what fraction of the previous velocity to retain in the new update, and add this
    â€œmemoryâ€ of past gradients to our current gradient. This approach is commonly
    referred to as *momentum*.^([4](ch06.xhtml#idm45934167589056)) Because the momentum
    term increases the step size we take, using momentum may require a reduced learning
    rate compared to vanilla stochastic gradient descent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨åŠ¨é‡è¶…å‚æ•°<m> m </m>æ¥ç¡®å®šæ–°æ›´æ–°ä¸­ä¿ç•™å¤šå°‘æ¯”ä¾‹çš„å…ˆå‰é€Ÿåº¦ï¼Œå¹¶å°†è¿‡å»æ¢¯åº¦çš„â€œè®°å¿†â€æ·»åŠ åˆ°å½“å‰æ¢¯åº¦ä¸­ã€‚è¿™ç§æ–¹æ³•é€šå¸¸ç§°ä¸º*åŠ¨é‡*ã€‚å› ä¸ºåŠ¨é‡é¡¹å¢åŠ äº†æˆ‘ä»¬é‡‡å–çš„æ­¥é•¿ï¼Œä½¿ç”¨åŠ¨é‡å¯èƒ½éœ€è¦æ¯”æ™®é€šéšæœºæ¢¯åº¦ä¸‹é™æ›´ä½çš„å­¦ä¹ ç‡ã€‚
- en: 'To better visualize how momentum works, weâ€™ll explore a toy example. Specifically,
    weâ€™ll investigate how momentum affects updates during a *random walk*. A random
    walk is a succession of randomly chosen steps. In our example, weâ€™ll imagine a
    particle on a line that, at every time interval, randomly picks a step size between
    â€“10 and 10 and takes a moves in that direction. This is simply expressed as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£åŠ¨é‡çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä¸€ä¸ªç©å…·ç¤ºä¾‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç ”ç©¶åŠ¨é‡å¦‚ä½•å½±å“*éšæœºæ¼«æ­¥*ä¸­çš„æ›´æ–°ã€‚éšæœºæ¼«æ­¥æ˜¯ä¸€ç³»åˆ—éšæœºé€‰æ‹©çš„æ­¥éª¤ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æƒ³è±¡ä¸€æ¡çº¿ä¸Šçš„ä¸€ä¸ªç²’å­ï¼Œåœ¨æ¯ä¸ªæ—¶é—´é—´éš”å†…ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªæ­¥é•¿åœ¨-10åˆ°10ä¹‹é—´ï¼Œå¹¶æœç€é‚£ä¸ªæ–¹å‘ç§»åŠ¨ã€‚è¿™å¯ä»¥ç®€å•åœ°è¡¨ç¤ºä¸ºï¼š
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Weâ€™ll then simulate what happens when we use a slight modification of momentum
    (i.e., the standard exponentially weighted moving average algorithm) to smooth
    our choice of step at every time interval. Again, we can concisely express this
    as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†æ¨¡æ‹Ÿå½“æˆ‘ä»¬ä½¿ç”¨åŠ¨é‡çš„è½»å¾®ä¿®æ”¹ï¼ˆå³æ ‡å‡†çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ç®—æ³•ï¼‰æ¥åœ¨æ¯ä¸ªæ—¶é—´é—´éš”å¹³æ»‘æˆ‘ä»¬çš„æ­¥é•¿é€‰æ‹©æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥ç®€æ´åœ°è¡¨ç¤ºä¸ºï¼š
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results, as we vary the momentum from 0 to 1, are quite staggering. Momentum
    significantly reduces the volatility of updates. The larger the momentum, the
    less responsive we are to new updates (e.g., a large inaccuracy on the first estimation
    of trajectory propagates for a significant period of time). We summarize the results
    of our toy experiment in [FigureÂ 6-8](#momentum_smooths_volatility).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬å°†åŠ¨é‡ä»0å˜åŒ–åˆ°1æ—¶ï¼Œç»“æœä»¤äººéœ‡æƒŠã€‚åŠ¨é‡æ˜¾è‘—å‡å°‘äº†æ›´æ–°çš„æ³¢åŠ¨æ€§ã€‚åŠ¨é‡è¶Šå¤§ï¼Œæˆ‘ä»¬å¯¹æ–°çš„æ›´æ–°è¶Šä¸æ•æ„Ÿï¼ˆä¾‹å¦‚ï¼Œåœ¨è½¨è¿¹ç¬¬ä¸€æ¬¡ä¼°è®¡æ—¶çš„å¤§è¯¯å·®ä¼šåœ¨ç›¸å½“é•¿çš„æ—¶é—´å†…ä¼ æ’­ï¼‰ã€‚æˆ‘ä»¬æ€»ç»“äº†æˆ‘ä»¬ç©å…·å®éªŒçš„ç»“æœåœ¨[å›¾6-8](#momentum_smooths_volatility)ä¸­ã€‚
- en: '![](Images/fdl2_0608.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0608.png)'
- en: Figure 6-8\. Momentum smooths volatility in the step sizes during a random walk
    using an exponentially weighted moving average
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-8ã€‚åŠ¨é‡é€šè¿‡æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡å€¼å¹³æ»‘éšæœºæ¼«æ­¥ä¸­çš„æ­¥é•¿æ³¢åŠ¨
- en: 'To investigate how momentum actually affects the training of feed-forward neural
    networks, we can retrain our trusty MNIST feed-forward network with a PyTorch
    momentum optimizer. In this case, we can get away with using the same learning
    rate (0.01) with a typical momentum of 0.9:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶åŠ¨é‡å¦‚ä½•å®é™…å½±å“å‰é¦ˆç¥ç»ç½‘ç»œçš„è®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ PyTorch åŠ¨é‡ä¼˜åŒ–å™¨é‡æ–°è®­ç»ƒæˆ‘ä»¬å¯é çš„ MNIST å‰é¦ˆç½‘ç»œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ï¼ˆ0.01ï¼‰å’Œå…¸å‹çš„åŠ¨é‡0.9ï¼š
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that when we create a PyTorch optimizer, we need to pass inÂ `model.parameters()`.
    The resulting speedup is staggering. We display how the cost function changes
    over time by comparing the visualizations inÂ [FigureÂ 6-9](#comparing_training_a_feedforward_network).
    The figure demonstrates that to achieve a cost of 0.1 without momentum (right)
    requires nearly 18,000 steps (minibatches), whereas with momentum (left), we require
    just over 2,000.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª PyTorch ä¼˜åŒ–å™¨æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä¼ å…¥ `model.parameters()`ã€‚ç»“æœçš„åŠ é€Ÿæ•ˆæœä»¤äººéœ‡æƒŠã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒ[å›¾6-9](#comparing_training_a_feedforward_network)ä¸­çš„å¯è§†åŒ–æ¥å±•ç¤ºæˆæœ¬å‡½æ•°éšæ—¶é—´çš„å˜åŒ–ã€‚è¯¥å›¾è¡¨æ˜ï¼Œè¦åœ¨æ²¡æœ‰åŠ¨é‡çš„æƒ…å†µä¸‹ï¼ˆå³ä¾§ï¼‰å®ç°æˆæœ¬ä¸º0.1ï¼Œéœ€è¦è¿‘18,000æ­¥ï¼ˆå°æ‰¹é‡ï¼‰ï¼Œè€Œä½¿ç”¨åŠ¨é‡ï¼ˆå·¦ä¾§ï¼‰åˆ™åªéœ€è¦è¶…è¿‡2,000æ­¥ã€‚
- en: '![](Images/fdl2_0609.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0609.png)'
- en: Figure 6-9\. Comparing training a feed-forward network with (right) and without
    (left) momentum demonstrates a massive decrease in training time
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-9ã€‚æ¯”è¾ƒä½¿ç”¨ï¼ˆå³ä¾§ï¼‰å’Œä¸ä½¿ç”¨ï¼ˆå·¦ä¾§ï¼‰åŠ¨é‡è®­ç»ƒå‰é¦ˆç½‘ç»œï¼Œæ˜¾ç¤ºäº†è®­ç»ƒæ—¶é—´çš„å¤§å¹…å‡å°‘
- en: Recently, more work has explored how the classical momentum technique can be
    improved. Sutskever et al. in 2013 proposed an alternative called Nesterov momentum,
    which computes the gradient on the error surface at <math alttext="theta plus
    bold v Subscript i minus 1"><mrow><mi>Î¸</mi> <mo>+</mo> <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    Â during the velocity update instead of at <math alttext="theta"><mi>Î¸</mi></math>
    .^([5](ch06.xhtml#idm45934167566352)) This subtle difference seems to allow Nesterov
    momentum to change its velocity in a more responsive way. Itâ€™s been shown that
    this method has clear benefits in batch gradient descent (convergence guarantees
    and the ability to use a higher momentum for a given learning rate as compared
    to classical momentum), but itâ€™s not entirely clear whether this is true for the
    more stochastic minibatch gradient descent used in most deep learning optimization
    approaches.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œæœ‰æ›´å¤šçš„å·¥ä½œæ¢è®¨äº†å¦‚ä½•æ”¹è¿›ç»å…¸çš„åŠ¨é‡æŠ€æœ¯ã€‚2013å¹´ï¼ŒSutskeverç­‰äººæå‡ºäº†ä¸€ç§ç§°ä¸º Nesterov åŠ¨é‡çš„æ›¿ä»£æ–¹æ³•ï¼Œå®ƒåœ¨é€Ÿåº¦æ›´æ–°æœŸé—´è®¡ç®—è¯¯å·®è¡¨é¢ä¸Šçš„æ¢¯åº¦ï¼Œè€Œä¸æ˜¯åœ¨
    Î¸ å¤„ã€‚è¿™ç§å¾®å¦™çš„å·®å¼‚ä¼¼ä¹ä½¿ Nesterov åŠ¨é‡èƒ½å¤Ÿä»¥æ›´å…·å“åº”æ€§çš„æ–¹å¼æ”¹å˜å…¶é€Ÿåº¦ã€‚å·²ç»è¯æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ä¸­å…·æœ‰æ˜æ˜¾çš„å¥½å¤„ï¼ˆæ”¶æ•›ä¿è¯å’Œç›¸å¯¹äºç»å…¸åŠ¨é‡å¯ä»¥ä½¿ç”¨æ›´é«˜çš„åŠ¨é‡æ¥å®ç°ç»™å®šå­¦ä¹ ç‡ï¼‰ï¼Œä½†å¯¹äºå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ä¸­ä½¿ç”¨çš„æ›´éšæœºçš„å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ˜¯å¦ä¹Ÿé€‚ç”¨å°šä¸æ¸…æ¥šã€‚
- en: 'Nerestov momentum is supported in PyTorch out-of-the-box by setting the `nesterov`
    argument:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¾ç½® `nesterov` å‚æ•°ï¼ŒPyTorch é»˜è®¤æ”¯æŒ Nerestov åŠ¨é‡ï¼š
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A Brief View of Second-Order Methods
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äºŒé˜¶æ–¹æ³•çš„ç®€è¦æ¦‚è¿°
- en: AsÂ we discussed, computing the Hessian is a computationally difficult task,
    and momentum afforded us significant speedup without having to worry about it
    altogether. Several second-order methods, however, have been researched over the
    past several years that attempt to approximate the Hessian directly. For completeness,
    we give a broad overview of these methods, but a detailed treatment is beyond
    the scope of this text.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬è®¨è®ºè¿‡çš„ï¼Œè®¡ç®—æµ·æ£®çŸ©é˜µæ˜¯ä¸€ä¸ªè®¡ç®—ä¸Šå›°éš¾çš„ä»»åŠ¡ï¼Œè€ŒåŠ¨é‡ä½¿æˆ‘ä»¬åœ¨ä¸å¿…æ‹…å¿ƒå®ƒçš„æƒ…å†µä¸‹è·å¾—äº†æ˜¾è‘—çš„åŠ é€Ÿã€‚ç„¶è€Œï¼Œåœ¨è¿‡å»å‡ å¹´ä¸­ï¼Œå·²ç»ç ”ç©¶äº†å‡ ç§è¯•å›¾ç›´æ¥é€¼è¿‘æµ·æ£®çŸ©é˜µçš„äºŒé˜¶æ–¹æ³•ã€‚ä¸ºäº†å®Œæ•´èµ·è§ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›æ¦‚è¿°ï¼Œä½†è¯¦ç»†è®¨è®ºè¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚
- en: The first is *conjugate gradient descent*, which arises out of attempting to
    improve on a naive method of steepest descent. In steepest descent, we compute
    the direction of the gradient and then line search to find the minimum along that
    direction. We jump to the minimum and then recompute the gradient to determine
    the direction of the next line search. It turns out that this method ends up zigzagging
    a significant amount, as shown inÂ [FigureÂ 6-10](#fig0610), because each time we
    move in the direction of steepest descent, we undo a little bit of progress in
    another direction. A remedy to this problem is moving in a *conjugate direction*
    relative to the previous choice instead of the direction of steepest descent.
    The conjugate direction is chosen by using an indirect approximation of the Hessian
    to linearly combine the gradient and our previous direction. With a slight modification,
    this method generalizes to the nonconvex error surfaces we find in deep networks.^([6](ch06.xhtml#idm45934167542112))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æ·±åº¦ç½‘ç»œä¼˜åŒ–çš„ä¸€ä¸ªé‡å¤§çªç ´æ˜¯å­¦ä¹ ç‡è°ƒæ•´çš„å‡ºç°ã€‚å­¦ä¹ ç‡è°ƒæ•´èƒŒåçš„åŸºæœ¬æ¦‚å¿µæ˜¯ï¼Œåœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­é€‚å½“åœ°ä¿®æ”¹æœ€ä½³å­¦ä¹ ç‡ï¼Œä»¥å®ç°è‰¯å¥½çš„æ”¶æ•›æ€§è´¨ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºAdaGradã€RMSPropå’ŒAdamï¼Œè¿™ä¸‰ç§æœ€æµè¡Œçš„è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•ä¹‹ä¸€ã€‚
- en: '![](Images/fdl2_0610.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0610.png)'
- en: Figure 6-10\. The method of steepest descent often zigzags; conjugate descent
    attempts to remedy this issue
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: åœ¨å…¶åŸå§‹å½¢å¼ä¸­ï¼ŒBFGSå…·æœ‰æ˜¾è‘—çš„å†…å­˜å ç”¨ï¼Œä½†æœ€è¿‘çš„å·¥ä½œå·²ç»äº§ç”Ÿäº†ä¸€ä¸ªæ›´èŠ‚çœå†…å­˜çš„ç‰ˆæœ¬ï¼Œç§°ä¸º*L-BFGS*ã€‚[8]
- en: An alternative optimization algorithm known as the *Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno*
    (BFGS) algorithm attempts to compute the inverse of the Hessian matrix iteratively
    and use the inverse Hessian to more effectively optimize the parameter vector.^([7](ch06.xhtml#idm45934167533568))
    In its original form, BFGS has a significant memory footprint, but recent work
    has produced a more memory-efficient version known as *L-BFGS*.^([8](ch06.xhtml#idm45934167530400))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ›¿ä»£ä¼˜åŒ–ç®—æ³•è¢«ç§°ä¸º*Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno*ï¼ˆBFGSï¼‰ç®—æ³•ï¼Œè¯•å›¾è¿­ä»£è®¡ç®—HessiançŸ©é˜µçš„é€†ï¼Œå¹¶ä½¿ç”¨é€†Hessianæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å‚æ•°å‘é‡ã€‚[7]
- en: In general, while these methods hold some promise, second-order methods are
    still an area of active research and are unpopular among practitioners. PyTorch
    does, however, support L-BFGS as well as other second-order methods, such as Averaged
    Stochastic Gradient Descent, for your own experimentation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œè™½ç„¶è¿™äº›æ–¹æ³•æœ‰ä¸€äº›å¸Œæœ›ï¼Œä½†äºŒé˜¶æ–¹æ³•ä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒç ”ç©¶é¢†åŸŸï¼Œä¸”åœ¨å®è·µè€…ä¸­ä¸å—æ¬¢è¿ã€‚ç„¶è€Œï¼ŒPyTorchæ”¯æŒL-BFGSä»¥åŠå…¶ä»–äºŒé˜¶æ–¹æ³•ï¼Œå¦‚å¹³å‡éšæœºæ¢¯åº¦ä¸‹é™ï¼Œä¾›æ‚¨è‡ªè¡Œå°è¯•ã€‚
- en: Learning Rate Adaptation
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®¨è®ºçš„ç¬¬ä¸€ä¸ªç®—æ³•æ˜¯AdaGradï¼Œå®ƒè¯•å›¾é€šè¿‡å†å²æ¢¯åº¦çš„ç´¯ç§¯éšæ—¶é—´è°ƒæ•´å…¨å±€å­¦ä¹ ç‡ï¼Œç”±Duchiç­‰äººåœ¨2011å¹´é¦–æ¬¡æå‡ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è·Ÿè¸ªæ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚è¿™ä¸ªå­¦ä¹ ç‡ä¸æ‰€æœ‰å‚æ•°å†å²æ¢¯åº¦çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹æˆåæ¯”åœ°ç¼©æ”¾ã€‚
- en: As we have discussed previously, another major challenge for training deep networks
    is appropriately selecting the learning rate. Choosing the correct learning rate
    has long been one of the most troublesome aspects of training deep networks because
    it has a major impact on a networkâ€™s performance. A learning rate that is too
    small doesnâ€™t learn quickly enough, but a learning rate that is too large may
    have difficulty converging as we approach a local minimum or region that is ill-conditioned.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ˜¯*å…±è½­æ¢¯åº¦ä¸‹é™*ï¼Œå®ƒèµ·æºäºè¯•å›¾æ”¹è¿›æœ€é™¡ä¸‹é™çš„æœ´ç´ æ–¹æ³•ã€‚åœ¨æœ€é™¡ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æ¢¯åº¦çš„æ–¹å‘ï¼Œç„¶åè¿›è¡Œçº¿æ€§æœç´¢ä»¥æ‰¾åˆ°æ²¿è¯¥æ–¹å‘çš„æœ€å°å€¼ã€‚æˆ‘ä»¬è·³åˆ°æœ€å°å€¼ï¼Œç„¶åé‡æ–°è®¡ç®—æ¢¯åº¦ä»¥ç¡®å®šä¸‹ä¸€æ¬¡çº¿æ€§æœç´¢çš„æ–¹å‘ã€‚äº‹å®è¯æ˜ï¼Œè¿™ç§æ–¹æ³•æœ€ç»ˆä¼šå‡ºç°ç›¸å½“å¤šçš„æ›²æŠ˜ï¼Œå¦‚[å›¾6-10](#fig0610)æ‰€ç¤ºï¼Œå› ä¸ºæ¯æ¬¡æˆ‘ä»¬æ²¿æœ€é™¡ä¸‹é™çš„æ–¹å‘ç§»åŠ¨æ—¶ï¼Œæˆ‘ä»¬ä¼šåœ¨å¦ä¸€ä¸ªæ–¹å‘ä¸Šæ’¤é”€ä¸€ç‚¹è¿›å±•ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯ç›¸å¯¹äºå…ˆå‰é€‰æ‹©çš„æ–¹å‘ç§»åŠ¨*å…±è½­æ–¹å‘*ï¼Œè€Œä¸æ˜¯æœ€é™¡ä¸‹é™çš„æ–¹å‘ã€‚é€šè¿‡ä½¿ç”¨Hessiançš„é—´æ¥è¿‘ä¼¼æ¥é€‰æ‹©å…±è½­æ–¹å‘ï¼Œå°†æ¢¯åº¦å’Œæˆ‘ä»¬å…ˆå‰çš„æ–¹å‘çº¿æ€§ç»„åˆã€‚ç¨ä½œä¿®æ”¹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°æˆ‘ä»¬åœ¨æ·±åº¦ç½‘ç»œä¸­æ‰¾åˆ°çš„éå‡¸è¯¯å·®è¡¨é¢ã€‚[6]
- en: One of the major breakthroughs in modern deep network optimization was the advent
    of learning rate adaption. The basic concept behind learning rate adaptation is
    that the optimal learning rate is appropriately modified over the span of learning
    to achieve good convergence properties. Over the next several sections, weâ€™ll
    discuss AdaGrad, RMSProp, and Adam, three of the most popular adaptive learning
    rate algorithms.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡è°ƒæ•´
- en: AdaGradâ€”Accumulating Historical Gradients
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaGrad-ç´¯ç§¯å†å²æ¢¯åº¦
- en: The first algorithm weâ€™ll discuss is AdaGrad, which attempts to adapt the global
    learning rate over time using an accumulation of the historical gradients, first
    proposed by Duchi et al. in 2011.^([9](ch06.xhtml#idm45934166863168))Â Specifically,
    we keep track of a learning rate for each parameter. This learning rate is inversely
    scaled with respect to the square root of the sum of the squares (root mean square)
    of all the parameterâ€™s historical gradients.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6-10ã€‚æœ€é™¡ä¸‹é™æ³•ç»å¸¸æ›²æŠ˜ï¼›å…±è½­ä¸‹é™è¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜
- en: 'We can express this mathematically. We initialize a gradient accumulation vectorÂ 
    <math alttext="bold r 0 equals bold 0"><mrow><msub><mi>ğ«</mi> <mn>0</mn></msub>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> . At every step, we accumulate
    the square of all the gradient parameters as follows (where the <math alttext="circled-dot"><mo>âŠ™</mo></math>
    Â operation is element-wise tensor multiplication):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ï¼Œè®­ç»ƒæ·±åº¦ç½‘ç»œçš„å¦ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯é€‚å½“é€‰æ‹©å­¦ä¹ ç‡ã€‚é€‰æ‹©æ­£ç¡®çš„å­¦ä¹ ç‡é•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯è®­ç»ƒæ·±åº¦ç½‘ç»œä¸­æœ€æ£˜æ‰‹çš„æ–¹é¢ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒå¯¹ç½‘ç»œçš„æ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚å­¦ä¹ ç‡å¤ªå°åˆ™å­¦ä¹ é€Ÿåº¦ä¸å¤Ÿå¿«ï¼Œä½†å­¦ä¹ ç‡å¤ªå¤§å¯èƒ½åœ¨æ¥è¿‘å±€éƒ¨æœ€å°å€¼æˆ–ç—…æ€åŒºåŸŸæ—¶éš¾ä»¥æ”¶æ•›ã€‚æˆ‘ä»¬å¯ä»¥ç”¨æ•°å­¦æ–¹å¼è¡¨è¾¾è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯å‘é‡<math
    alttext="bold r 0 equals bold 0"><mrow><msub><mi>ğ«</mi> <mn>0</mn></msub> <mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow> </math>ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬æŒ‰å¦‚ä¸‹æ–¹å¼ç´¯ç§¯æ‰€æœ‰æ¢¯åº¦å‚æ•°çš„å¹³æ–¹ï¼ˆå…¶ä¸­<math
    alttext="circled-dot"><mo>âŠ™</mo></math>æ“ä½œæ˜¯é€å…ƒç´ å¼ é‡ä¹˜æ³•ï¼‰:'
- en: <math alttext="bold r Subscript i Baseline equals bold r Subscript i minus 1
    Baseline plus bold g Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ«</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>ğ«</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub></mrow></math>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold r Subscript i Baseline equals bold r Subscript i minus 1
    Baseline plus bold g Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ«</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>ğ«</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub></mrow></math>
- en: 'Then we compute the update as usual, except our global learning rate <math
    alttext="epsilon"><mi>Ïµ</mi></math> Â is divided by the square root of the gradient
    accumulation vector:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·è®¡ç®—æ›´æ–°ï¼Œåªæ˜¯æˆ‘ä»¬çš„å…¨å±€å­¦ä¹ ç‡<math alttext="epsilon"><mi>Ïµ</mi></math>è¢«æ¢¯åº¦ç´¯ç§¯å‘é‡çš„å¹³æ–¹æ ¹é™¤ä»¥ï¼š
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold r Subscript
    i Baseline EndRoot EndFraction circled-dot bold g"><mrow><msub><mi>Î¸</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î¸</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>Ïµ</mi> <mrow><mi>Î´</mi><mo>âŠ•</mo><msqrt><msub><mi>ğ«</mi>
    <mi>i</mi></msub></msqrt></mrow></mfrac> <mo>âŠ™</mo> <mi>ğ </mi></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold r Subscript
    i Baseline EndRoot EndFraction circled-dot bold g"><mrow><msub><mi>Î¸</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î¸</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>Ïµ</mi> <mrow><mi>Î´</mi><mo>âŠ•</mo><msqrt><msub><mi>ğ«</mi>
    <mi>i</mi></msub></msqrt></mrow></mfrac> <mo>âŠ™</mo> <mi>ğ </mi></mrow></math>
- en: 'Note that we add a tiny numberÂ  <math alttext="delta"><mi>Î´</mi></math> Â (~
    <math alttext="10 Superscript negative 7"><msup><mn>10</mn> <mrow><mo>-</mo><mn>7</mn></mrow></msup></math>
    ) to the denominator to prevent division by zero. Also,Â the division and addition
    operations are broadcast to the size of the gradient accumulation vector and applied
    element-wise. In PyTorch, a built-in optimizer allows for easily utilizing AdaGrad
    as a learning algorithm:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨åˆ†æ¯ä¸­æ·»åŠ ä¸€ä¸ªå¾®å°æ•°<math alttext="delta"><mi>Î´</mi></math>ï¼ˆ~ <math alttext="10
    Superscript negative 7"><msup><mn>10</mn> <mrow><mo>-</mo><mn>7</mn></mrow></msup></math>ï¼‰ä»¥é˜²æ­¢é™¤ä»¥é›¶ã€‚æ­¤å¤–ï¼Œé™¤æ³•å’ŒåŠ æ³•æ“ä½œè¢«å¹¿æ’­åˆ°æ¢¯åº¦ç´¯ç§¯å‘é‡çš„å¤§å°ï¼Œå¹¶é€å…ƒç´ åº”ç”¨ã€‚åœ¨PyTorchä¸­ï¼Œå†…ç½®ä¼˜åŒ–å™¨å…è®¸è½»æ¾åœ°å°†AdaGradç”¨ä½œå­¦ä¹ ç®—æ³•ï¼š
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The only hitch is that in PyTorch, theÂ  <math alttext="delta"><mi>Î´</mi></math>
    Â and initial gradient accumulation vector are rolled together into the `initial_accumulator_value`Â argument.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€çš„é—®é¢˜æ˜¯åœ¨PyTorchä¸­ï¼Œ<math alttext="delta"><mi>Î´</mi></math>å’Œåˆå§‹æ¢¯åº¦ç´¯ç§¯å‘é‡è¢«åˆå¹¶åˆ°`initial_accumulator_value`å‚æ•°ä¸­ã€‚
- en: On a functional level, this update mechanism means that the parameters with
    the largest gradients experience a rapid decrease in their learning rates, while
    parameters with smaller gradients observe only a small decrease in their learning
    rates. The ultimate effect is that AdaGrad forces more progress in the more gently
    sloped directions on the error surface, which can help overcome ill-conditioned
    surfaces. This results in some good theoretical properties, but in practice, training
    deep learning models with AdaGrad can be somewhat problematic. Empirically, AdaGrad
    has a tendency to cause a premature drop in learning rate, and as a result doesnâ€™t
    work particularly well for some deep models. In the next section, weâ€™ll describe
    RMSProp, which attempts to remedy this shortcoming.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠŸèƒ½çº§åˆ«ä¸Šï¼Œè¿™ç§æ›´æ–°æœºåˆ¶æ„å‘³ç€å…·æœ‰æœ€å¤§æ¢¯åº¦çš„å‚æ•°ä¼šå¿«é€Ÿé™ä½å…¶å­¦ä¹ ç‡ï¼Œè€Œå…·æœ‰è¾ƒå°æ¢¯åº¦çš„å‚æ•°åªä¼šç•¥å¾®é™ä½å…¶å­¦ä¹ ç‡ã€‚æœ€ç»ˆæ•ˆæœæ˜¯AdaGradåœ¨è¯¯å·®è¡¨é¢ä¸Šæ›´æ¸©å’Œå€¾æ–œçš„æ–¹å‘ä¸Šå–å¾—æ›´å¤šè¿›å±•ï¼Œè¿™æœ‰åŠ©äºå…‹æœç—…æ€è¡¨é¢ã€‚è¿™å¯¼è‡´äº†ä¸€äº›è‰¯å¥½çš„ç†è®ºæ€§è´¨ï¼Œä½†åœ¨å®è·µä¸­ï¼Œä½¿ç”¨AdaGradè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹å¯èƒ½ä¼šæœ‰äº›é—®é¢˜ã€‚ç»éªŒä¸Šï¼ŒAdaGradæœ‰å¯¼è‡´å­¦ä¹ ç‡è¿‡æ—©ä¸‹é™çš„å€¾å‘ï¼Œå› æ­¤å¯¹äºä¸€äº›æ·±åº¦æ¨¡å‹æ•ˆæœå¹¶ä¸ç‰¹åˆ«å¥½ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æè¿°RMSPropï¼Œè¯•å›¾å¼¥è¡¥è¿™ä¸ªç¼ºç‚¹ã€‚
- en: RMSPropâ€”Exponentially Weighted Moving Average of Gradients
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMSPropâ€”æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡
- en: While AdaGrad works well for simple convex functions, it isnâ€™t designed to navigate
    the complex error surfaces of deep networks. Flat regions may force AdaGrad to
    decrease the learning rate before it reaches a minimum. The conclusion is that
    simply using a naive accumulation of gradients isnâ€™t sufficient.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶AdaGradå¯¹ç®€å•çš„å‡¸å‡½æ•°æ•ˆæœå¾ˆå¥½ï¼Œä½†å®ƒå¹¶ä¸é€‚ç”¨äºæ·±åº¦ç½‘ç»œå¤æ‚çš„è¯¯å·®è¡¨é¢ã€‚å¹³å¦åŒºåŸŸå¯èƒ½ä¼šå¯¼è‡´AdaGradåœ¨è¾¾åˆ°æœ€å°å€¼ä¹‹å‰é™ä½å­¦ä¹ ç‡ã€‚ç»“è®ºæ˜¯ç®€å•åœ°ç´¯ç§¯æ¢¯åº¦æ˜¯ä¸å¤Ÿçš„ã€‚
- en: 'Our solution is to bring back a concept we introduced earlier while discussing
    momentum to dampen fluctuations in the gradient. Compared to naive accumulation,
    exponentially weighted moving averages also enables us to â€œtoss outâ€ measurements
    that we made a long time ago. More specifically, our update to the gradient accumulation
    vector is now as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯é‡æ–°å¼•å…¥æˆ‘ä»¬åœ¨è®¨è®ºåŠ¨é‡æ—¶ä»‹ç»çš„ä¸€ä¸ªæ¦‚å¿µï¼Œä»¥å‡å¼±æ¢¯åº¦ä¸­çš„æ³¢åŠ¨ã€‚ä¸ç®€å•ç´¯ç§¯ç›¸æ¯”ï¼ŒæŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡å€¼è¿˜ä½¿æˆ‘ä»¬èƒ½å¤Ÿâ€œä¸¢å¼ƒâ€å¾ˆä¹…ä»¥å‰çš„æµ‹é‡ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¯¹æ¢¯åº¦ç´¯ç§¯å‘é‡çš„æ›´æ–°ç°åœ¨å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: <math alttext="bold r Subscript i Baseline equals rho bold r Subscript i minus
    1 Baseline plus left-parenthesis 1 minus rho right-parenthesis bold g Subscript
    i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ«</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>Ï</mi> <msub><mi>ğ«</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>Ï</mi></mfenced>
    <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold r Subscript i Baseline equals rho bold r Subscript i minus
    1 Baseline plus left-parenthesis 1 minus rho right-parenthesis bold g Subscript
    i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ«</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>Ï</mi> <msub><mi>ğ«</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>Ï</mi></mfenced>
    <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub></mrow></math>
- en: The decay factor <math alttext="rho"><mi>Ï</mi></math> Â determines how long
    we keep old gradients. The smaller the decay factor, the shorter the effective
    window. Plugging this modification into AdaGrad gives rise to the RMSProp learning
    algorithm, first proposed by Geoffrey Hinton.^([10](ch06.xhtml#idm45934166835152))
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¡°å‡å› å­<math alttext="rho"><mi>Ï</mi></math>å†³å®šæˆ‘ä»¬ä¿ç•™æ—§æ¢¯åº¦çš„æ—¶é—´ã€‚è¡°å‡å› å­è¶Šå°ï¼Œæœ‰æ•ˆçª—å£å°±è¶ŠçŸ­ã€‚å°†è¿™ç§ä¿®æ”¹æ’å…¥AdaGradä¸­äº§ç”Ÿäº†ç”±Geoffrey
    Hintoné¦–æ¬¡æå‡ºçš„RMSPropå­¦ä¹ ç®—æ³•ã€‚
- en: 'In PyTorch, we can instantiate the RMSProp optimizer with the following code.
    Note that in this case, unlike in AdaGrad, we pass inÂ  <math alttext="delta"><mi>Î´</mi></math>
    Â separately as the `epsilon` argument to the constructor:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyTorchä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç å®ä¾‹åŒ–RMSPropä¼˜åŒ–å™¨ã€‚è¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸AdaGradä¸åŒï¼Œæˆ‘ä»¬å°†<math alttext="delta"><mi>Î´</mi></math>å•ç‹¬ä½œä¸ºæ„é€ å‡½æ•°çš„`epsilon`å‚æ•°ä¼ å…¥ï¼š
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As the template suggests, we can utilize RMSProp with momentum (specifically
    Nerestov momentum). Overall, RMSProp has been shown to be a highly effective optimizer
    for deep neural networks, and is a default choice for many seasoned practitioners.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ¨¡æ¿æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¸¦æœ‰åŠ¨é‡çš„RMSPropï¼ˆå…·ä½“æ¥è¯´æ˜¯NerestovåŠ¨é‡ï¼‰ã€‚æ€»çš„æ¥è¯´ï¼ŒRMSPropå·²è¢«è¯æ˜æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œçš„é«˜æ•ˆä¼˜åŒ–å™¨ï¼Œå¹¶ä¸”æ˜¯è®¸å¤šç»éªŒä¸°å¯Œçš„ä»ä¸šè€…çš„é»˜è®¤é€‰æ‹©ã€‚
- en: Adamâ€”Combining Momentum and RMSProp
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adamâ€”ç»“åˆåŠ¨é‡å’ŒRMSProp
- en: Before concluding our discussion of modern optimizers, we discuss one final
    algorithmâ€”Adam.^([11](ch06.xhtml#idm45934166819376)) Spiritually, we can think
    about Adam as a variant combination of RMSProp and momentum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»“æŸæˆ‘ä»¬å¯¹ç°ä»£ä¼˜åŒ–å™¨çš„è®¨è®ºä¹‹å‰ï¼Œæˆ‘ä»¬è®¨è®ºæœ€åä¸€ä¸ªç®—æ³•â€”Adamã€‚ä»ç²¾ç¥ä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥å°†Adamçœ‹ä½œæ˜¯RMSPropå’ŒåŠ¨é‡çš„å˜ä½“ç»„åˆã€‚
- en: 'The basic idea is as follows. We want to keep track of an exponentially weighted
    moving average of the gradient (essentially the concept of velocity in classical
    momentum), which we can express as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬æ€æƒ³å¦‚ä¸‹ã€‚æˆ‘ä»¬å¸Œæœ›è·Ÿè¸ªæ¢¯åº¦çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼ˆåŸºæœ¬ä¸Šæ˜¯ç»å…¸åŠ¨é‡ä¸­çš„é€Ÿåº¦æ¦‚å¿µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '**m**[*i*] = Î²[1]**m**[*i* â€“ 1] + (1 â€“ Î²[1])**g**[i]'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**m**[*i*] = Î²[1]**m**[*i* â€“ 1] + (1 â€“ Î²[1])**g**[i]'
- en: 'This is our approximation of what we call the *first moment* of the gradient,
    or <math alttext="double-struck upper E left-bracket bold g Subscript i Baseline
    right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo> <msub><mi>ğ </mi> <mi>i</mi></msub>
    <mo>]</mo></mrow></math> . And similarly to RMSProp, we can maintain an exponentially
    weighted moving average of the historical gradients. This is our estimation of
    what we call the *second moment* of the gradient, or <math alttext="double-struck
    upper E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript
    i Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo> <msub><mi>ğ </mi> <mi>i</mi></msub>
    <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> :'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å¯¹æˆ‘ä»¬ç§°ä¹‹ä¸ºæ¢¯åº¦çš„ *ç¬¬ä¸€çŸ©* æˆ– <math alttext="double-struck upper E left-bracket bold
    g Subscript i Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub> <mo>]</mo></mrow></math> çš„è¿‘ä¼¼ã€‚ç±»ä¼¼äºRMSPropï¼Œæˆ‘ä»¬å¯ä»¥ç»´æŠ¤å†å²æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡å€¼ã€‚è¿™æ˜¯æˆ‘ä»¬å¯¹æˆ‘ä»¬ç§°ä¹‹ä¸ºæ¢¯åº¦çš„
    *ç¬¬äºŒçŸ©* æˆ– <math alttext="double-struck upper E left-bracket bold g Subscript i Baseline
    circled-dot bold g Subscript i Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo>
    <msub><mi>ğ </mi> <mi>i</msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>]</mo></mrow></math>
    çš„ä¼°è®¡ï¼š
- en: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub></mrow></math>
- en: However, it turns out these estimations are biased relative to the real moments
    because we start off by initializing both vectors to the zero vector. In order
    to remedy this bias, we derive a correction factor for both estimations. Here,
    we describe the derivation for the estimation of the second moment. The derivation
    for the first moment, which is analogous to the derivation here, is left as an
    exercise for you.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œäº‹å®è¯æ˜è¿™äº›ä¼°è®¡ç›¸å¯¹äºçœŸå®çŸ©æ˜¯æœ‰åå·®çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä»å°†ä¸¤ä¸ªå‘é‡åˆå§‹åŒ–ä¸ºé›¶å‘é‡å¼€å§‹ã€‚ä¸ºäº†çº æ­£è¿™ç§åå·®ï¼Œæˆ‘ä»¬ä¸ºè¿™ä¸¤ä¸ªä¼°è®¡æ¨å¯¼äº†ä¸€ä¸ªæ ¡æ­£å› å­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æè¿°äº†å¯¹äºŒé˜¶çŸ©çš„ä¼°è®¡çš„æ¨å¯¼ã€‚å¯¹äºç¬¬ä¸€çŸ©çš„æ¨å¯¼ï¼Œä¸æ­¤å¤„ç±»ä¼¼çš„æ¨å¯¼ç•™ä½œç»ƒä¹ ã€‚
- en: 'We begin by expressing the estimation of the second moment in terms of all
    past gradients. This is done by simply expanding the recurrence relationship:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆé€šè¿‡æ‰€æœ‰è¿‡å»æ¢¯åº¦çš„ä¼°è®¡æ¥è¡¨è¾¾äºŒé˜¶çŸ©çš„ä¼°è®¡ã€‚è¿™æ˜¯é€šè¿‡ç®€å•åœ°å±•å¼€é€’å½’å…³ç³»æ¥å®Œæˆçš„ï¼š
- en: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals beta 2 bold v Subscript i
    minus 1 Baseline plus left-parenthesis 1 minus beta 2 right-parenthesis bold g
    Subscript i Baseline circled-dot bold g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub></mrow></math>
- en: <math alttext="bold v Subscript i Baseline equals beta 2 Superscript i minus
    1 Baseline left-parenthesis 1 minus beta 2 right-parenthesis bold g 1 circled-dot
    bold g 1 plus beta 2 Superscript i minus 2 Baseline left-parenthesis 1 minus beta
    2 right-parenthesis bold g 2 circled-dot bold g 2 plus ellipsis plus left-parenthesis
    1 minus beta 2 right-parenthesis bold g Subscript i Baseline circled-dot bold
    g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>=</mo> <msubsup><mi>Î²</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>ğ </mi>
    <mn>1</mn></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>Î²</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>2</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>ğ </mi>
    <mn>2</mn></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mn>2</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals beta 2 Superscript i minus
    1 Baseline left-parenthesis 1 minus beta 2 right-parenthesis bold g 1 circled-dot
    bold g 1 plus beta 2 Superscript i minus 2 Baseline left-parenthesis 1 minus beta
    2 right-parenthesis bold g 2 circled-dot bold g 2 plus ellipsis plus left-parenthesis
    1 minus beta 2 right-parenthesis bold g Subscript i Baseline circled-dot bold
    g Subscript i"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>=</mo> <msubsup><mi>Î²</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>ğ </mi>
    <mn>1</mn></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>Î²</mi>
    <mn>2</mn> <mrow><mi>i</mi><mo>-</mo><mn>2</mn></mrow></msubsup> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msub><mi>ğ </mi>
    <mn>2</mn></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mn>2</mn></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub></mrow></math>
- en: <math alttext="bold v Subscript i Baseline equals left-parenthesis 1 minus beta
    2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msubsup><mo>âˆ‘</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup> <msup><mi>Î²</mi>
    <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup> <msub><mi>ğ </mi> <mi>k</mi></msub>
    <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>k</mi></msub></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline equals left-parenthesis 1 minus beta
    2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>=</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msubsup><mo>âˆ‘</mo>
    <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup> <msup><mi>Î²</mi>
    <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup> <msub><mi>ğ </mi> <mi>k</mi></msub>
    <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>k</mi></msub></mrow></math>
- en: 'We can then take the expected value of both sides to determine how our estimation
    <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo> <msub><mi>ğ¯</mi> <mi>i</mi></msub>
    <mo>]</mo></mrow></math> Â compares to the real value of <math alttext="double-struck
    upper E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript
    i Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo> <msub><mi>ğ </mi> <mi>i</mi></msub>
    <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> :'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å–ä¸¤è¾¹çš„æœŸæœ›å€¼ï¼Œä»¥ç¡®å®šæˆ‘ä»¬çš„ä¼°è®¡ <math alttext="double-struck upper E left-bracket bold
    v Subscript i Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo> <msub><mi>ğ¯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow></math> å¦‚ä½•ä¸ <math alttext="double-struck upper
    E left-bracket bold g Subscript i Baseline circled-dot bold g Subscript i Baseline
    right-bracket"><mrow><mi>ğ”¼</mi> <mo>[</mo> <msub><mi>ğ </mi> <mi>i</msub> <mo>âŠ™</mo>
    <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>]</mo></mrow></math> çš„çœŸå®å€¼ç›¸æ¯”ï¼š
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket equals double-struck upper E left-bracket left-parenthesis 1 minus
    beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ¯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <mi>ğ”¼</mi> <mfenced separators=""
    open="[" close="]"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mo>âˆ‘</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>i</mi></msubsup> <msup><mi>Î²</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup>
    <msub><mi>ğ </mi> <mi>k</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>k</mi></msub></mfenced></mrow></math>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket equals double-struck upper E left-bracket left-parenthesis 1 minus
    beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript i Endscripts
    beta Superscript i minus k Baseline bold g Subscript k Baseline circled-dot bold
    g Subscript k Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ¯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>=</mo> <mi>ğ”¼</mi> <mfenced separators=""
    open="[" close="]"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mo>âˆ‘</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>i</mi></msubsup> <msup><mi>Î²</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup>
    <msub><mi>ğ </mi> <mi>k</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>k</mi></msub></mfenced></mrow></math>
- en: 'We can also assume that <math alttext="double-struck upper E left-bracket bold
    g Subscript k Baseline circled-dot bold g Subscript k Baseline right-bracket almost-equals
    double-struck upper E left-bracket bold g Subscript i Baseline almost-equals bold
    g Subscript i Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ </mi>
    <mi>k</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>k</mi></msub> <mo>]</mo></mrow>
    <mo>â‰ˆ</mo> <mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>â‰ˆ</mo>
    <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>]</mo></mrow></mrow></math> because even
    if the second moment of the gradient has changed since a historical value, <math
    alttext="beta 2"><msub><mi>Î²</mi> <mn>2</mn></msub></math> should be chosen so
    that the old second moments of the gradients are essentially decayed out of relevancy.
    As a result, we can make the following simplification:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥å‡è®¾ <math alttext="double-struck upper E left-bracket bold g Subscript k
    Baseline circled-dot bold g Subscript k Baseline right-bracket almost-equals double-struck
    upper E left-bracket bold g Subscript i Baseline almost-equals bold g Subscript
    i Baseline right-bracket"><mrow><mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ </mi> <mi>k</mi></msub>
    <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>k</mi></msub> <mo>]</mo></mrow> <mo>â‰ˆ</mo> <mi>ğ”¼</mi>
    <mrow><mo>[</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>â‰ˆ</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub> <mo>]</mo></mrow></mrow></math> å› ä¸ºå³ä½¿æ¢¯åº¦çš„äºŒé˜¶çŸ©ç›¸å¯¹äºå†å²å€¼å‘ç”Ÿäº†å˜åŒ–ï¼Œ<math
    alttext="beta 2"><msub><mi>Î²</mi> <mn>2</mn></msub></math> åº”è¯¥è¢«é€‰æ‹©ï¼Œä»¥ä¾¿æ—§çš„æ¢¯åº¦äºŒé˜¶çŸ©åŸºæœ¬ä¸Šè¡°å‡åˆ°ä¸ç›¸å…³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œä»¥ä¸‹ç®€åŒ–ï¼š
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript
    i Endscripts beta Superscript i minus k"><mrow><mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ¯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>â‰ˆ</mo> <mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>]</mo></mrow>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <msubsup><mo>âˆ‘</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup>
    <msup><mi>Î²</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></math>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta 2 right-parenthesis sigma-summation Underscript k equals 1 Overscript
    i Endscripts beta Superscript i minus k"><mrow><mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ¯</mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mo>â‰ˆ</mo> <mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>]</mo></mrow>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <msubsup><mo>âˆ‘</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>i</mi></msubsup>
    <msup><mi>Î²</mi> <mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></math>
- en: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta Subscript 2 Sub Superscript i Subscript Baseline right-parenthesis"><mrow><mi>ğ”¼</mi>
    <mrow><mo>[</mo> <msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>]</mo></mrow> <mo>â‰ˆ</mo>
    <mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi>
    <msup><mn>2</mn> <mi>i</mi></msup></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket bold v Subscript i Baseline
    right-bracket almost-equals double-struck upper E left-bracket bold g Subscript
    i Baseline circled-dot bold g Subscript i Baseline right-bracket left-parenthesis
    1 minus beta Subscript 2 Sub Superscript i Subscript Baseline right-parenthesis"><mrow><mi>ğ”¼</mi>
    <mrow><mo>[</mo> <msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>]</mo></mrow> <mo>â‰ˆ</mo>
    <mi>ğ”¼</mi> <mrow><mo>[</mo> <msub><mi>ğ </mi> <mi>i</mi></msub> <mo>âŠ™</mo> <msub><mi>ğ </mi>
    <mi>i</mi></msub> <mo>]</mo></mrow> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>Î²</mi>
    <msup><mn>2</mn> <mi>i</mi></msup></msub> <mo>)</mo></mrow></mrow></math>
- en: 'Note that we make the final simplification using the elementary algebraic identityÂ 
    <math alttext="1 minus x Superscript n Baseline equals left-parenthesis 1 minus
    x right-parenthesis left-parenthesis 1 plus x plus ellipsis plus x Superscript
    n minus 1 Baseline right-parenthesis"><mrow><mn>1</mn> <mo>-</mo> <msup><mi>x</mi>
    <mi>n</mi></msup> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>+</mo>
    <mi>x</mi> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mfenced></mrow></math>
    . The results of this derivation and the analogous derivation for the first moment
    are the following correction schemes to account for the initialization bias:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºæœ¬ä»£æ•°æ’ç­‰å¼è¿›è¡Œæœ€ç»ˆç®€åŒ– <math alttext="1 minus x Superscript n Baseline equals
    left-parenthesis 1 minus x right-parenthesis left-parenthesis 1 plus x plus ellipsis
    plus x Superscript n minus 1 Baseline right-parenthesis"><mrow><mn>1</mn> <mo>-</mo>
    <msup><mi>x</mi> <mi>n</mi></msup> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mfenced separators="" open="(" close=")"><mn>1</mn>
    <mo>+</mo> <mi>x</mi> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msup><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mfenced></mrow></math>
    ã€‚è¿™ä¸ªæ¨å¯¼çš„ç»“æœä»¥åŠç¬¬ä¸€çŸ©çš„ç±»ä¼¼æ¨å¯¼å¾—åˆ°ä»¥ä¸‹æ ¡æ­£æ–¹æ¡ˆï¼Œä»¥è§£å†³åˆå§‹åŒ–åå·®ï¼š
- en: '**mÌƒ**[*i*] = <math alttext="StartFraction m Subscript i Baseline Over 1 minus
    beta 1 Superscript i Baseline EndFraction"><mfrac><msub><mi>m</mi> <mi>i</mi></msub>
    <mrow><mn>1</mn><mo>-</mo><msubsup><mi>Î²</mi> <mn>1</mn> <mi>i</mi></msubsup></mrow></mfrac></math>'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**mÌƒ**[*i*] = <math alttext="StartFraction m Subscript i Baseline Over 1 minus
    beta 1 Superscript i Baseline EndFraction"><mfrac><msub><mi>m</mi> <mi>i</mi></msub>
    <mrow><mn>1</mn><mo>-</mo><msubsup><mi>Î²</mi> <mn>1</mn> <mi>i</mi></msubsup></mrow></mfrac></math>'
- en: <math alttext="bold v overTilde Subscript i Baseline equals StartFraction bold
    v overTilde Subscript i Baseline Over 1 minus beta 2 Superscript i Baseline EndFraction"><mrow><msub><mover
    accent="true"><mi>ğ¯</mi> <mo>Ëœ</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mfrac><msub><mover
    accent="true"><mi>ğ¯</mi> <mo>Ëœ</mo></mover> <mi>i</mi></msub> <mrow><mn>1</mn><mo>-</mo><msubsup><mi>Î²</mi>
    <mn>2</mn> <mi>i</mi></msubsup></mrow></mfrac></mrow></math>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v overTilde Subscript i Baseline equals StartFraction bold
    v overTilde Subscript i Baseline Over 1 minus beta 2 Superscript i Baseline EndFraction"><mrow><msub><mover
    accent="true"><mi>ğ¯</mi> <mo>Ëœ</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mfrac><msub><mover
    accent="true"><mi>ğ¯</mi> <mo>Ëœ</mo></mover> <mi>i</mi></msub> <mrow><mn>1</mn><mo>-</mo><msubsup><mi>Î²</mi>
    <mn>2</mn> <mi>i</mi></msubsup></mrow></mfrac></mrow></math>
- en: 'We can then use these corrected moments to update the parameter vector, resulting
    in the final Adam update:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ ¡æ­£åçš„çŸ©æ¥æ›´æ–°å‚æ•°å‘é‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„Adamæ›´æ–°ï¼š
- en: <math alttext="theta Subscript i Baseline equals theta Subscript i minus 1 Baseline
    minus StartFraction epsilon Over delta circled-plus StartRoot bold v overTilde
    Subscript i Baseline EndRoot EndFraction"><mrow><msub><mi>Î¸</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î¸</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <mfrac><mi>Ïµ</mi> <mrow><mi>Î´</mi><mo>âŠ•</mo><msqrt><msub><mover accent="true"><mi>ğ¯</mi>
    <mo>Ëœ</mo></mover> <mi>i</mi></msub></msqrt></mrow></mfrac></mrow></math> **mÌƒ**[*i*]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Adam has gained popularity because of its corrective measures against
    the zero initialization bias (a weakness of RMSProp) and its ability to combine
    the core concepts behind RMSProp with momentum more effectively. PyTorch exposes
    the Adam optimizer through the following constructor:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The default hyperparameter settings for Adam for PyTorch generally perform quite
    well, but Adam is also generally robust to choices in hyperparameters. The only
    exception is that the learning rate may need to be modified in certain cases from
    the default value of 0.001.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The Philosophy Behind Optimizer Selection
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, weâ€™ve discussed several strategies that are used to make navigating
    the complex error surfaces of deep networks more tractable. These strategies have
    culminated in several optimization algorithms, each with its own benefits and
    shortcomings.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: While it would be awfully nice to know when to use which algorithm, there is
    very little consensus among expert practitioners. Currently, the most popular
    algorithms are minibatch gradient descent,  Â minibatch gradient  with  momentum,  RMSProp,  RMSProp  with  momentum,  Adam,
    and  AdaDelta (which we havenâ€™t discussed here, but is also supported by PyTorch).Â We
    encourage you to experiment with these optimization algorithms on the feed-forward
    network model we built.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: One important point, however, is that for most deep learning practitioners,
    the best way to push the cutting edge of deep learning is not by building more
    advanced optimizers. Instead, the vast majority of breakthroughs in deep learning
    over the past several decades have been obtained by discovering architectures
    that are easier to train instead of trying to wrangle with nasty error surfaces.
    Weâ€™ll begin focusing on how to leverage architecture to more effectively train
    neural networks in the rest of this book.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed several challenges that arise when trying to train
    deep networks with complex error surfaces. We discussed how while the challenges
    of spurious local minima are likely exaggerated, saddle points and ill-conditioning
    do pose a serious threat to the success of vanilla minibatch gradient descent.
    We described how momentum can be used to overcome ill-conditioning, and briefly
    discussed recent research in second-order methods to approximate the Hessian matrix.
    We also described the evolution of adaptive learning rate optimizers, which tune
    the learning rate during the training process for better convergence.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Next, weâ€™ll begin tackling the larger issue of network architecture and design.
    Weâ€™ll explore computer vision and how we might design deep networks that learn
    effectively from complex images.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch06.xhtml#idm45934168902672-marker)) Bengio, Yoshua, et al. â€œGreedy
    Layer-Wise Training of Deep Networks.â€ *Advances in Neural Information Processing
    Systems* 19 (2007): 153.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.xhtml#idm45934168861872-marker)) Goodfellow, Ian J., Oriol Vinyals,
    and Andrew M. Saxe. â€œQualitatively characterizing neural network optimization
    problems.â€ *arXiv preprint arXiv*:1412.6544 (2014).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.xhtml#idm45934164995408-marker)) Dauphin, Yann N., et al. â€œIdentifying
    and attacking the saddle point problem in high-dimensional non-convex optimization.â€
    *Advances in Neural Information Processing Systems*. 2014.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch06.xhtml#idm45934167589056-marker)) Polyak, Boris T. â€œSome methods
    of speeding up the convergence of iteration methods.â€ *USSR Computational Mathematics
    and Mathematical Physics* 4.5 (1964): 1-17.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch06.xhtml#idm45934167589056-marker)) Polyak, Boris T. â€œä¸€äº›åŠ é€Ÿè¿­ä»£æ–¹æ³•æ”¶æ•›çš„æ–¹æ³•ã€‚â€
    *è‹è”è®¡ç®—æ•°å­¦å’Œæ•°å­¦ç‰©ç†* 4.5 (1964): 1-17ã€‚'
- en: '^([5](ch06.xhtml#idm45934167566352-marker)) Sutskever, Ilya, et al. â€œOn the
    importance of initialization and momentum in deep learning.â€ *ICML* (3) 28 (2013):
    1139-1147.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch06.xhtml#idm45934167566352-marker)) Sutskever, Ilya, ç­‰. â€œå…³äºæ·±åº¦å­¦ä¹ ä¸­åˆå§‹åŒ–å’ŒåŠ¨é‡çš„é‡è¦æ€§ã€‚â€
    *ICML* (3) 28 (2013): 1139-1147ã€‚'
- en: '^([6](ch06.xhtml#idm45934167542112-marker)) MÃ¸ller, Martin Fodslette. â€œA Scaled
    Conjugate Gradient Algorithm for Fast Supervised Learning.â€ *Neural Networks*
    6.4 (1993): 525-533.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch06.xhtml#idm45934167542112-marker)) MÃ¸ller, Martin Fodslette. â€œä¸€ç§ç”¨äºå¿«é€Ÿç›‘ç£å­¦ä¹ çš„ç¼©æ”¾å…±è½­æ¢¯åº¦ç®—æ³•ã€‚â€
    *ç¥ç»ç½‘ç»œ* 6.4 (1993): 525-533ã€‚'
- en: '^([7](ch06.xhtml#idm45934167533568-marker)) Broyden, C. G. â€œA New Method of
    Solving Nonlinear Simultaneous Equations.â€ *The Computer Journal* 12.1 (1969):
    94-99.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '^([7](ch06.xhtml#idm45934167533568-marker)) Broyden, C. G. â€œè§£éçº¿æ€§è”ç«‹æ–¹ç¨‹çš„ä¸€ç§æ–°æ–¹æ³•ã€‚â€
    *è®¡ç®—æœºæ‚å¿—* 12.1 (1969): 94-99ã€‚'
- en: '^([8](ch06.xhtml#idm45934167530400-marker)) Bonnans, Joseph-FrÃ©dÃ©ric, et al.
    *Numerical Optimization: Theoretical and Practical Aspects*. Springer Science
    & Business Media, 2006.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.xhtml#idm45934167530400-marker)) Bonnans, Joseph-FrÃ©dÃ©ric, ç­‰. *æ•°å€¼ä¼˜åŒ–ï¼šç†è®ºå’Œå®è·µæ–¹é¢*ã€‚Springer
    Science & Business Media, 2006ã€‚
- en: '^([9](ch06.xhtml#idm45934166863168-marker)) Duchi, John, Elad Hazan, and Yoram
    Singer. â€œAdaptive Subgradient Methods for Online Learning and Stochastic Optimization.â€
    *Journal of Machine Learning Research* 12.Jul (2011): 2121-2159.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch06.xhtml#idm45934166863168-marker)) Duchi, John, Elad Hazan, å’Œ Yoram
    Singer. â€œç”¨äºåœ¨çº¿å­¦ä¹ å’Œéšæœºä¼˜åŒ–çš„è‡ªé€‚åº”æ¬¡æ¢¯åº¦æ–¹æ³•ã€‚â€ *æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—* 12.Jul (2011): 2121-2159ã€‚'
- en: '^([10](ch06.xhtml#idm45934166835152-marker)) Tieleman, Tijmen, and Geoffrey
    Hinton. â€œLecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its
    Recent Magnitude.â€ *COURSERA: Neural Networks for Machine Learning* 4.2 (2012).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.xhtml#idm45934166835152-marker)) Tieleman, Tijmen, å’Œ Geoffrey Hinton.
    â€œè®²åº§6.5-rmspropï¼šå°†æ¢¯åº¦é™¤ä»¥æœ€è¿‘å¹…åº¦çš„è¿è¡Œå¹³å‡å€¼ã€‚â€ *COURSERAï¼šç¥ç»ç½‘ç»œæœºå™¨å­¦ä¹ * 4.2 (2012)ã€‚
- en: '^([11](ch06.xhtml#idm45934166819376-marker)) Kingma, Diederik, and Jimmy Ba.
    â€œAdam: A Method for Stochastic Optimization.â€ *arXiv preprint arXiv*:1412.6980
    (2014).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.xhtml#idm45934166819376-marker)) Kingma, Diederik, å’Œ Jimmy Ba. â€œAdamï¼šä¸€ç§ç”¨äºéšæœºä¼˜åŒ–çš„æ–¹æ³•ã€‚â€
    *arXivé¢„å°æœ¬ arXiv*:1412.6980 (2014)ã€‚
