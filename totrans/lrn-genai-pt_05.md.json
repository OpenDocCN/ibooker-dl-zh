["```py\nimport torch\nimport torch.nn as nn\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nD=nn.Sequential(\n    nn.Linear(784, 1024),          ①\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(1024, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(256, 1),    \n    nn.Sigmoid()).to(device)       ②\n```", "```py\nG=nn.Sequential(\n    nn.Linear(100, 256),         ①\n    nn.ReLU(),\n    nn.Linear(256, 512),         ②\n    nn.ReLU(),\n    nn.Linear(512, 1024),        ③\n    nn.ReLU(),\n    nn.Linear(1024, 784),        ④\n    nn.Tanh()).to(device)        ⑤\n```", "```py\nloss_fn=nn.BCELoss()\nlr=0.0001\noptimD=torch.optim.Adam(D.parameters(),lr=lr)\noptimG=torch.optim.Adam(G.parameters(),lr=lr)  \n```", "```py\nimport matplotlib.pyplot as plt\n\ndef see_output():\n    noise=torch.randn(32,100).to(device=device)\n    fake_samples=G(noise).cpu().detach()                 ①\n    plt.figure(dpi=100,figsize=(20,10))\n    for i in range(32):\n        ax=plt.subplot(4, 8, i + 1)                      ②\n        img=(fake_samples[i]/2+0.5).reshape(28, 28)\n        plt.imshow(img)                                  ③\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()\n\nsee_output()                                             ④\n```", "```py\nfor i in range(50):    \n    gloss=0\n    dloss=0\n    for n, (real_samples,_) in enumerate(train_loader):\n        loss_D=train_D_on_real(real_samples)            ①\n        dloss+=loss_D\n        loss_D=train_D_on_fake()                        ②\n        dloss+=loss_D\n        loss_G=train_G()                                ③\n        gloss+=loss_G\n    gloss=gloss/n\n    dloss=dloss/n    \n    if i % 10 == 9:\n        print(f\"at epoch {i+1}, dloss: {dloss}, gloss {gloss}\")\n        see_output()                                    ④\n```", "```py\nscripted = torch.jit.script(G) \nscripted.save('files/fashion_gen.pt') \n```", "```py\nnew_G=torch.jit.load('files/fashion_gen.pt',\n                     map_location=device)\nnew_G.eval()\n```", "```py\nnoise=torch.randn(32,100).to(device=device)\nfake_samples=new_G(noise).cpu().detach()\nfor i in range(32):\n    ax = plt.subplot(4, 8, i + 1)\n    plt.imshow((fake_samples[i]/2+0.5).reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(hspace=-0.6)\nplt.show() \n```", "```py\nimg = torch.Tensor([[1,1,1],\n                    [0,1,2],\n                    [8,7,6]]).reshape(1,1,3,3)    ①\n```", "```py\nconv=nn.Conv2d(in_channels=1,\n            out_channels=1,\n            kernel_size=2, \n            stride=1)                             ①\nsd=conv.state_dict()                              ②\nprint(sd)\n```", "```py\nOrderedDict([('weight', tensor([[[[ 0.3823,  0.4150],\n          [-0.1171,  0.4593]]]])), ('bias', tensor([-0.1096]))])\n```", "```py\nweights={'weight':torch.tensor([[[[1,2],\n   [3,4]]]]), 'bias':torch.tensor([0])}          ①\nfor k in sd:\n    with torch.no_grad():\n        sd[k].copy_(weights[k])                  ②\nprint(conv.state_dict())                         ③\n```", "```py\nOrderedDict([('weight', tensor([[[[1., 2.],\n          [3., 4.]]]])), ('bias', tensor([0.]))])\n```", "```py\noutput = conv(img)\nprint(output)\n```", "```py\ntensor([[[[ 7., 14.],\n          [54., 50.]]]], grad_fn=<ConvolutionBackward0>)\n```", "```py\nconv=nn.Conv2d(in_channels=1,\n            out_channels=1,\n            kernel_size=2, \n            stride=2,                           ①\n            padding=1)                          ②\nsd=conv.state_dict()\nfor k in sd:\n    with torch.no_grad():\n        sd[k].copy_(weights[k])\noutput = conv(img)\nprint(output)\n```", "```py\ntensor([[[[ 4.,  7.],\n          [32., 50.]]]], grad_fn=<ConvolutionBackward0>)\n```", "```py\nimg = torch.Tensor([[1,0],\n                    [2,3]]).reshape(1,1,2,2)\n```", "```py\ntransconv=nn.ConvTranspose2d(in_channels=1,\n            out_channels=1,\n            kernel_size=2, \n            stride=2)                            ①\nsd=transconv.state_dict()\nweights={'weight':torch.tensor([[[[2,3],\n   [4,5]]]]), 'bias':torch.tensor([0])}\nfor k in sd:\n    with torch.no_grad():\n        sd[k].copy_(weights[k])                  ②\n```", "```py\ntransoutput = transconv(img)\nprint(transoutput)\n```", "```py\ntensor([[[[ 2.,  3.,  0.,  0.],\n          [ 4.,  5.,  0.,  0.],\n          [ 4.,  6.,  6.,  9.],\n          [ 8., 10., 12., 15.]]]], grad_fn=<ConvolutionBackward0>)\n```", "```py\ntorch.manual_seed(42)                          ①\nimg = torch.rand(1,3,64,64)                    ②\nconv = nn.Conv2d(in_channels=3,\n            out_channels=3,\n            kernel_size=3, \n            stride=1,\n            padding=1)                         ③\nout=conv(img)                                  ④\nprint(out.shape)\n```", "```py\ntorch.Size([1, 3, 64, 64])\n```", "```py\nfor i in range(3):\n    print(f\"mean in channel {i} is\", out[:,i,:,:].mean().item())\n    print(f\"std in channel {i} is\", out[:,i,:,:].std().item())\n```", "```py\nmean in channel 0 is -0.3766776919364929\nstd in channel 0 is 0.17841289937496185\nmean in channel 1 is -0.3910464942455292\nstd in channel 1 is 0.16061744093894958\nmean in channel 2 is 0.39275866746902466\nstd in channel 2 is 0.18207983672618866\n```", "```py\nnorm=nn.BatchNorm2d(3)\nout2=norm(out)\nprint(out2.shape)\nfor i in range(3):\n    print(f\"mean in channel {i} is\", out2[:,i,:,:].mean().item())\n    print(f\"std in channel {i} is\", out2[:,i,:,:].std().item())\n```", "```py\ntorch.Size([1, 3, 64, 64])\nmean in channel 0 is 6.984919309616089e-09\nstd in channel 0 is 0.9999650120735168\nmean in channel 1 is -5.3085386753082275e-08\nstd in channel 1 is 0.9999282956123352\nmean in channel 2 is 9.872019290924072e-08\nstd in channel 2 is 0.9999712705612183\n```", "```py\nanime_path = r\"files/anime\"\n```", "```py\nfrom torchvision import transforms as T\nfrom torchvision.datasets import ImageFolder\n\ntransform = T.Compose([T.Resize((64, 64)),               ①\n    T.ToTensor(),                                        ②\n    T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])      ③\ntrain_data = ImageFolder(root=anime_path,\n                         transform=transform)            ④\n```", "```py\nfrom torch.utils.data import DataLoader\n\nbatch_size = 128\ntrain_loader = DataLoader(dataset=train_data, \n               batch_size=batch_size, shuffle=True)\n```", "```py\nimage0, _ = train_data[0]\nprint(image0.shape)\n```", "```py\ntorch.Size([3, 64, 64])\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.imshow(image0.permute(1,2,0)*0.5+0.5)\nplt.show()\n```", "```py\ndef plot_images(imgs):                              ①\n    for i in range(32):\n        ax = plt.subplot(4, 8, i + 1)               ②\n        plt.imshow(imgs[i].permute(1,2,0)/2+0.5)\n        plt.xticks([])\n        plt.yticks([])\n    plt.subplots_adjust(hspace=-0.6)\n    plt.show()    \n\nimgs, _ = next(iter(train_loader))                  ③\nplot_images(imgs)                                   ④\n```", "```py\nimport torch.nn as nn\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nD = nn.Sequential(\n    nn.Conv2d(3, 64, 4, 2, 1, bias=False),           ①\n    nn.LeakyReLU(0.2, inplace=True),                 ②\n    nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(128),                             ③\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n    nn.Sigmoid(),\n    nn.Flatten()).to(device)                         ④\n```", "```py\nG=nn.Sequential(\n    nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),    ①\n    nn.BatchNorm2d(512),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    ②\n    nn.BatchNorm2d(256),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),       ③\n    nn.Tanh()).to(device)                                 ④\n```", "```py\nloss_fn=nn.BCELoss()\nlr = 0.0002\noptimG = torch.optim.Adam(G.parameters(), \n                         lr = lr, betas=(0.5, 0.999))\noptimD = torch.optim.Adam(D.parameters(), \n                         lr = lr, betas=(0.5, 0.999))\n```", "```py\ndef test_epoch():\n    noise=torch.randn(32,100,1,1).\\\n        to(device=device)                             ①\n    fake_samples=G(noise).cpu().detach()              ②\n    for i in range(32):                               ③\n        ax = plt.subplot(4, 8, i + 1)\n        img=(fake_samples.cpu().detach()[i]/2+0.5).\\\n            permute(1,2,0)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n    plt.subplots_adjust(hspace=-0.6)\n    plt.show()\ntest_epoch()                                          ④\n```", "```py\nfor i in range(20):\n    gloss=0\n    dloss=0\n    for n, (real_samples,_) in enumerate(train_loader):\n        loss_D=train_D_on_real(real_samples)\n        dloss+=loss_D\n        loss_D=train_D_on_fake()\n        dloss+=loss_D\n        loss_G=train_G()\n        gloss+=loss_G\n    gloss=gloss/n\n    dloss=dloss/n\n    print(f\"epoch {i+1}, dloss: {dloss}, gloss {gloss}\")\n    test_epoch()\n```", "```py\nscripted = torch.jit.script(G) \nscripted.save('files/anime_gen.pt') \n```", "```py\nnew_G=torch.jit.load('files/anime_gen.pt',\n                     map_location=device)\nnew_G.eval()\nnoise=torch.randn(32,100,1,1).to(device)\nfake_samples=new_G(noise).cpu().detach()\nfor i in range(32):\n    ax = plt.subplot(4, 8, i + 1)\n    img=(fake_samples.cpu().detach()[i]/2+0.5).permute(1,2,0)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(hspace=-0.6)\nplt.show() \n```"]