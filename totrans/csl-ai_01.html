<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">1</span> </span> <span class="chapter-title-text">Why causal AI</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Defining causal AI and its benefits</li>
<li class="readable-text" id="p3">Incorporating causality into machine learning models</li>
<li class="readable-text" id="p4">A simple example of applying causality to a machine learning model</li>
</ul>
</div>
<div class="readable-text" id="p5">
<p>Subscription streaming platforms like Netflix are always looking for ways to optimize various indicators of performance. One of these is their <em>churn rate</em>, meaning the rate at which they lose subscribers. Imagine that you are a machine learning engineer or data scientist at Netflix tasked with finding ways of reducing churn. What are the types of <em>causal questions</em> (questions that require causal thinking) you might ask with respect to this task? </p>
</div>
<ul>
<li class="readable-text" id="p6"> <em>Causal discovery</em><em> </em>—Given detailed data on who churned and who did not, can you analyze that data to find causes of the churn? <em>Causal discovery</em> investigates what causes what. </li>
<li class="readable-text" id="p7"> <em>Estimating average treatment effects</em> (ATEs)—Suppose the algorithm that recommends content to the user is a cause of the churn; a better choice of algorithm might reduce churn, but by how much? The task of quantifying how much, on average, a cause drives an effect is the <em>ATE estimation</em>. For example, some users could be exposed to a new version of the algorithm, and you could measure how much this affects churn, relative to the baseline algorithm. </li>
</ul>
<div class="readable-text" id="p8">
<p>Let’s go a bit deeper. The mockumentary <em>The Office </em>(the American version) was one of the most popular shows on Netflix. Later, Netflix learned that NBCUniversal was planning to stop licensing the show to Netflix to stream in the US, so that US streaming of The Office would be exclusive to NBCUniversal’s rival streaming platform, Peacock. Given the popularity of the show, churn was certainly affected, but by how much? </p>
</div>
<ul>
<li class="readable-text" id="p9"> <em>Estimating conditional average treatment effects </em>(CATEs) —The effect of losing <em>The Office</em> would be more pronounced for some subscriber segments than others, but what attributes define these segments? One attribute is certainly having watched the show, but there are others (demographics, other content watched, etc.). <em>CATE estimation</em> is the task of quantifying how much a cause drives an effect for a particular segment of the population. Indeed, there are likely multiple segments we could define, each with a different within-segment ATE. Part of the task of CATE estimation is finding distinct segments of interest. </li>
</ul>
<div class="readable-text" id="p10">
<p>Suppose you had reliable data on subscribers who quit Netflix and signed up for Peacock to continue watching <em>The Office</em>. For some of these users, the recommendation algorithm failed to show them possible substitutes for <em>The Office</em>, like the mockumentary <em>Parks and Recreation</em>. That may lead to a different type of question. </p>
</div>
<ul>
<li class="readable-text" id="p11"> <em>Counterfactual reasoning and attribution</em><em> </em>—If the algorithm had placed <em>Parks and Recreation</em> more prominently in those users’ dashboards, would they have stayed on with Netflix? These <em>counterfactual questions</em> (“counter” to the “fact” that the show wasn’t prominent in their dashboard) are essential for <em>attribution</em> (assigning a root cause and credit/blame for an outcome). </li>
</ul>
<div class="readable-text" id="p12">
<p>Netflix worked with Steve Carrel (star of <em>The Office</em>) and Greg Daniels (writer, director, and producer of <em>The Office</em>) to create the show <em>Space Force </em>as Netflix original content. The show was released just months before <em>The Office </em>moved to Peacock. Suppose that this show was Netflix’s attempt to create content to retain subscribers who were fans of <em>The Office</em>. Consider the decisions that would go into the creation of such a show:</p>
</div>
<ul>
<li class="readable-text" id="p13"> <em>Causal decision theory</em><em> </em>—What actors/directors/writers would tempt <em>The Office</em> fans to stay subscribed? What themes and content? </li>
<li class="readable-text" id="p14"> <em>Causal machine learning</em><em> </em>—How could we use generative AI, such as large language models to create scripts and pilots for the show in such a way that optimizes for the objective of reducing churn amongst fans of <em>The Office</em>? </li>
</ul>
<div class="readable-text" id="p15">
<p><em>Causal</em> <em>inference</em> is about breaking down a problem into these types of specific <em>causal queries</em>, and then using data to answer these queries. <em>Causal AI </em>is about building algorithms that automate this analysis. We’ll tackle both of these problem areas in this book.</p>
</div>
<div class="readable-text" id="p16">
<h2 class="readable-text-h2" id="sigil_toc_id_6"><span class="num-string">1.1</span> What is causal AI?</h2>
</div>
<div class="readable-text" id="p17">
<p>To understand what causal AI is, we’ll start with the basic ideas of causality and causal inference, and work our way up. Then we’ll review the kinds of problems we can solve with causal AI.</p>
</div>
<div class="readable-text intended-text" id="p18">
<p><em>Causal reasoning</em> is a crucial element of how humans understand, explain, and make decisions about the world. Anytime we think about cause (“Why did that happen?”) or effect (“What will happen if I do this?”), we are practicing causal reasoning. </p>
</div>
<div class="readable-text intended-text" id="p19">
<p>In statistics and machine learning, we use data to lend statistical rigor to our causal reasoning. But while cause-and-effect relationships drive the data, statistical correlation alone is insufficient to draw causal conclusions from data. For this, we must turn to <em>causal inference</em>.</p>
</div>
<div class="readable-text intended-text" id="p20">
<p>Statistical (non-causal) inference relies on statistical assumptions. This is true even in deep learning, where assumptions are often called “inductive bias.” Similarly, causal inference relies on causal assumptions; causal inference refers to a body of theory and practical methods that constrain statistical analysis with causal assumptions.</p>
</div>
<div class="readable-text intended-text" id="p21">
<p><em>Causal AI</em> refers to the automation of causal inference. We can leverage machine learning algorithms, which have developed robust approaches to automating statistical analyses and scale up to large amounts of data of different modalities.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>The goal of AI is automating reasoning tasks that until now have required human intelligence to solve. Humans rely heavily on causal reasoning to navigate the world, and while we are better at causal reasoning than statistical reasoning, our cognitive biases still make our causal reasoning highly error prone. Improving our ability to answer causal questions has been the work of millennia of philosophers, centuries of scientists, and decades of statisticians. But now, a convergence of statistical and computational advances has shifted the focus from discourse to algorithms that we can train on data and deploy to software. It is a fascinating time to learn how to build causal AI.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p23">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Key definitions underpinning causal AI</h5>
</div>
<ul>
<li class="readable-text" id="p24"> <em>Inference</em>—Drawing conclusions from observations and data </li>
<li class="readable-text" id="p25"> <em>Assumptions</em>—Constraints that guide inferences </li>
<li class="readable-text" id="p26"> <em>Inductive biases</em>—Another word for <em>assumptions</em>, often used to refer to assumptions implicit in the choice of machine learning algorithm </li>
<li class="readable-text" id="p27"> <em>Statistical model</em>—A framework using statistical assumptions to analyze data </li>
<li class="readable-text" id="p28"> <em>Data science</em>—An interdisciplinary field that uses statistical models along with other algorithms and techniques to extract insights and knowledge from structured and unstructured data </li>
<li class="readable-text" id="p29"> <em>Causal inference</em>—Techniques that use causal assumptions to guide conclusions </li>
<li class="readable-text" id="p30"> <em>Causal model</em>—A statistical model built on causal assumptions about data generation </li>
</ul>
<ul>
<li class="readable-text" id="p31"> <em>Causal data science</em>—Data science that employs causal models to extract causal insights </li>
<li class="readable-text" id="p32"> <em>Causal AI</em>—Algorithms that automate causal inference tasks using causal models </li>
</ul>
</div>
<div class="readable-text" id="p33">
<h2 class="readable-text-h2" id="sigil_toc_id_7"><span class="num-string">1.2</span> How this book approaches causal inference</h2>
</div>
<div class="readable-text" id="p34">
<p>The goal of this book is the fusion of two powerful domains: causality and AI. By the end of this journey, you’ll be equipped with the skills to</p>
</div>
<ul>
<li class="readable-text" id="p35"> <em>Design AI systems with causal capabilities</em><em> </em>—Harness the power of AI, but with an added layer of causal reasoning. </li>
<li class="readable-text" id="p36"> <em>Use machine learning frameworks for causal inference</em><em> </em>—Utilize tools like PyTorch and other Python libraries to seamlessly integrate causal modeling into your projects. </li>
<li class="readable-text" id="p37"> <em>Build tools for automated causal decision-making</em><em> </em>—Implement causal decision-making algorithms, including causal reinforcement learning algorithms. </li>
</ul>
<div class="readable-text" id="p38">
<p>Historically, causality and AI evolved from different bodies of research, they have been applied to different problems, and they have led to experts with different skill sets, books that use different languages, and libraries with different abstractions. This book is for anyone who wants to connect these domains into one comprehensive skill set. </p>
</div>
<div class="readable-text intended-text" id="p39">
<p>There are many books on causal inference, including books that focus on causal inference in Python. The following subsections discuss some features that make this book unique.</p>
</div>
<div class="readable-text" id="p40">
<h3 class="readable-text-h3" id="sigil_toc_id_8"><span class="num-string">1.2.1</span> Emphasis on AI</h3>
</div>
<div class="readable-text" id="p41">
<p>This book focuses on causal AI. We’ll cover not just the relevance of causal inference to AI, or how machine learning can scale up causal inference, but also focus on implementation. Specifically, we’ll integrate causal models with conventional models and training procedures from probabilistic machine learning.</p>
</div>
<div class="readable-text" id="p42">
<h3 class="readable-text-h3" id="sigil_toc_id_9"><span class="num-string">1.2.2</span> Focus on tech, retail, and business</h3>
</div>
<div class="readable-text" id="p43">
<p>Practical causal inference methods have developed from econometrics, public health, social sciences, and other domains where it is difficult to run randomized experiments. As a result, examples in most books tend to come from those domains. In contrast, this book leans heavily into examples from tech, retail, and business. </p>
</div>
<div class="readable-text" id="p44">
<h3 class="readable-text-h3" id="sigil_toc_id_10"><span class="num-string">1.2.3</span> Parallel world counterfactuals and other queries beyond causal effects</h3>
</div>
<div class="readable-text" id="p45">
<p>When many think of “causal inference,” they think of estimating causal effects, namely average treatment effects (ATEs) and conditional average treatment effects (CATEs). These are certainly important queries, but there are other kinds of causal queries as well. This book gives due attention to these other types.</p>
</div>
<div class="readable-text intended-text" id="p46">
<p>For example, this book provides in-depth coverage of the <em>parallel worlds </em>account of counterfactuals. In this approach, when some cause and some effect occur, we imagine a parallel universe where the causal event was different. For example, suppose you asked, “I married for money and now I’m sad. Would I have been happier had I married for love?” With our parallel worlds approach, you’d use your experience of marrying for money and being sad as inputs to a causal model-based probabilistic simulation of your happiness in a parallel universe where you married for love. This type of reasoning is useful in decision-making. For example, it might help you choose a better spouse next time. </p>
</div>
<div class="readable-text intended-text" id="p47">
<p>Hopefully this example of love and regret illustrates how fundamental this kind of “what could have been” thinking is to human cognition (we’ll see more applied examples in chapters 8 and 9). It therefore makes sense to learn how to build AI with the same capabilities. But although they’re useful, some counterfactual inferences are hard or impossible to verify (you <em>can’t</em> prove you would have been happier if you had married for love). Most causal inference books only focus on the narrow set of counterfactuals we can verify with data and experiments, which misses many interesting, cognitive science-aligned, and practical use cases of counterfactual reasoning. This book leans into those use cases.</p>
</div>
<div class="readable-text" id="p48">
<h3 class="readable-text-h3" id="sigil_toc_id_11"><span class="num-string">1.2.4</span> An assumption of commodification of inference</h3>
</div>
<div class="readable-text" id="p49">
<p>Many causal inference books go deep into the statistical inference nuts and bolts of various causal effect estimators. But a major trend in the last decade of developing deep learning frameworks is the <em>commodification of inference</em>. This refers to how libraries like PyTorch abstract away the difficult aspects of estimation and inference—if you can define your estimation/inference problem in terms minimizing a differentiable loss function, PyTorch will handle the rest. The commodification of inference frees up the user to focus on creating ever more nuanced and powerful models, such as models that represent the causal structure of the data-generating process. </p>
</div>
<div class="readable-text intended-text" id="p50">
<p>In this book, we’ll focus on leveraging frameworks for inference so that you can learn a universal view of modeling techniques. Once you find the right modeling approach for your domain, you can use other resources to go deep into any statistical algorithm of interest.</p>
</div>
<div class="readable-text" id="p51">
<h3 class="readable-text-h3" id="sigil_toc_id_12"><span class="num-string">1.2.5</span> Breaking down theory with code</h3>
</div>
<div class="readable-text" id="p52">
<p>One of the standout features of this book is its approach to advanced topics in causal inference theory. Many introductory texts shy away from subjects like identification, the do-calculus, and the causal hierarchy theorem because they are difficult. The problem is that if you want to create causal-capable AI algorithms, you need an intuition for these concepts. </p>
</div>
<div class="readable-text intended-text" id="p53">
<p>In this book, we’ll make these topics accessible by relying on Python libraries that implement their basic abstractions and algorithms. We’ll build intuition for these advanced topics by working with these primitives in code.</p>
</div>
<div class="readable-text" id="p54">
<h2 class="readable-text-h2" id="sigil_toc_id_13"><span class="num-string">1.3</span> Causality’s role in modern AI workflows</h2>
</div>
<div class="readable-text" id="p55">
<p>There is great value in positioning ourselves to build future versions of AI with causal capabilities, but the topics covered in this book will also have an impact on applications common today. In this section, we’ll review how causality can enhance some of these applications. </p>
</div>
<div class="readable-text" id="p56">
<h3 class="readable-text-h3" id="sigil_toc_id_14"><span class="num-string">1.3.1</span> Better data science</h3>
</div>
<div class="readable-text" id="p57">
<p>Big tech and tech-powered retail organizations have recognized the significance of causal inference, offering premium salaries to those proficient in it. This is because the essence of data science—deriving actionable insights from data—is inherently causal. </p>
</div>
<div class="readable-text intended-text" id="p58">
<p>When a data scientist examines the correlation between a feature on an e-commerce site and sales, they do so because they want to know whether the feature causally drives sales. Causal inference can help answer this question in several ways. First, it can help them design an experiment that will quantify the causal effect of the feature on sales, especially in the case where a perfect randomized experiment is not possible. Second, if a proposed experiment is not feasible, the data scientist can use past observational data and data from related but different past experiments to infer the value of the causal effect that would result from the proposed experiment without actually running it. Finally, even if the data scientist has complete freedom in running experiments, causal inference can help select which experiment to run and what variables to measure, minimizing the opportunity cost of running wasteful or uninformative experiments.</p>
</div>
<div class="readable-text" id="p59">
<h3 class="readable-text-h3" id="sigil_toc_id_15"><span class="num-string">1.3.2</span> Better attribution, credit assignment, and root cause analysis</h3>
</div>
<div class="readable-text" id="p60">
<p>Causal inference also supports attribution. The “attribution problem” in marketing is perhaps best articulated by a quote credited to advertising pioneer John Wanamaker:</p>
</div>
<div class="readable-text" id="p61">
<blockquote>
<div>
     Half the money I spend on advertising is wasted; the trouble is I don’t know which half.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p62">
<p>In other words, it is difficult to know what advertisement, promotion, or other action <em>caused </em>a specific customer behavior, sales number, or other key business outcome. Even in online marketing, where the data has gotten much richer and more granular than in Wanamaker’s time, attribution remains a challenge. For example, a user may have clicked after seeing an ad, but was it that single ad view that led to the click? Or were they going to click anyway? Perhaps there was a cumulative effect of all the nudges to click that they received over multiple channels. Causal modeling addresses the attribution problem by using formal causal logic to answer “why” questions, such as “why did this user click?”</p>
</div>
<div class="readable-text intended-text" id="p63">
<p>Attribution goes by other names in other domains, such as “credit assignment” and “root cause analysis.” The core meaning is the same; we want to understand why a particular event outcome happened. We know what the causes are in general, but we want to know how much a particular cause is to blame in a given instance.</p>
</div>
<div class="readable-text" id="p64">
<h3 class="readable-text-h3" id="sigil_toc_id_16"><span class="num-string">1.3.3</span> More robust, decomposable, and explainable models</h3>
</div>
<div class="readable-text" id="p65">
<p>For organizations that use machine learning to build software, incorporating causal modelling can improve both the process and the product. In particular, causality adds value by making machine learning more robust, decomposable, and explainable. </p>
</div>
<div class="readable-text" id="p66">
<h4 class="readable-text-h4 sigil_not_in_toc">More robust machine learning</h4>
</div>
<div class="readable-text" id="p67">
<p>Machine learning models lack robustness when differences between the environment where the model was trained and the environment where the model is deployed cause the model to break down. Causality can address the lack of robustness in the following ways:</p>
</div>
<ul>
<li class="readable-text" id="p68"> <em>Overfitting</em><em> </em>—Overfitting occurs when learning algorithms place too much weight on spurious statistical patterns in the training data. Causal approaches can orient machine learning models toward learning statistical patterns that are rooted in causal relationships. </li>
<li class="readable-text" id="p69"> <em>Underspecification</em><em> </em>—Underspecification occurs when there are many equivalent configurations of a model that perform equivalently on test data but perform differently in the deployment environment. One sign of underspecification is sensitivity to arbitrary elements of the model’s configuration, such as a random seed. Causal inference can tell you when a causal prediction is “identified” (i.e., not “underspecified”), meaning a unique answer exists given the assumptions and the data.  </li>
<li class="readable-text" id="p70"> <em>Data drift</em><em> </em>—As time passes, the characteristics of the data in the environment where you deploy the model differ or “drift” from the characteristics of the training data. Causal modeling addresses this by capturing causal invariance underlying the data. For example, suppose you train a model that uses elevation to predict average temperature. If you train with data only from high-elevation cities, it should still work well in low-elevation cities if the model successfully fit the underlying physics-based causal relationship between altitude and temperature. </li>
</ul>
<div class="readable-text" id="p71">
<p>This is why leading tech companies deploy causal machine learning techniques—they can make their machine learning services more robust. It is also why notable deep learning researchers are pursuing research that combines deep learning with causal reasoning.</p>
</div>
<div class="readable-text" id="p72">
<h4 class="readable-text-h4 sigil_not_in_toc">More decomposable machine learning</h4>
</div>
<div class="readable-text" id="p73">
<p>Causal models decompose into components, specifically tuples of effects and their direct causes, which I’ll define formally in chapter 3. To illustrate, let’s consider a simple machine learning problem of predicting whether an individual who sees a digital ad will go on to make a purchase. </p>
</div>
<div class="readable-text intended-text" id="p74">
<p>We could use various characteristics of the ad impression (e.g., the number of times the ad was seen, the duration of the view, the ad category, the time of day, etc.) as the feature vector, and predict the purchase using a neural network, as depicted in figure 1.1. The weights in the hidden layers of the model are mutually dependent, so the model cannot be reduced to smaller independent components. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p75">
<img alt="figure" height="357" src="../Images/CH01_F01_Ness.png" width="678"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.1</span> A simple multilayer perceptron neural network that uses features associated with ad impressions to predict whether a purchase will result</h5>
</div>
<div class="readable-text" id="p76">
<p>On the other hand, if we take a causal view of the problem, we might reason that an ad impression drives engagement, and that the engagement drives whether an individual makes a purchase. Using engagement metrics as another feature vector, we could instead train the model shown in figure 1.2. This model aligns with the causal structure of the domain (i.e., ad impressions causing engagement, and engagement causing purchases). As such, it decomposes into two components: {ad impression, engagement} and {engagement, purchase}.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p77">
<img alt="figure" height="441" src="../Images/CH01_F02_Ness.png" width="900"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.2</span> A model that captures how ad impressions drive engagement, which in turn drives purchases. This model decomposes into {ad impression, engagement} and {engagement, purchase}.</h5>
</div>
<div class="readable-text intended-text" id="p78">
<p>There are several benefits of this decomposability:</p>
</div>
<ul>
<li class="readable-text" id="p79"> Components of the model can be tested and validated independently. </li>
<li class="readable-text" id="p80"> Components of the model can be executed separately, enabling more efficient use of modern cloud computing infrastructure and enabling edge computing. </li>
<li class="readable-text" id="p81"> When additional training data is available, only the components relevant to the data need retraining. </li>
<li class="readable-text" id="p82"> Components of old models can be reused in new models targeting new problems. </li>
<li class="readable-text" id="p83"> There is less sensitivity to suboptimal model configuration and hyperparameter settings, because components can be optimized separately. </li>
</ul>
<div class="readable-text" id="p84">
<p>The components of the causal model correspond to concepts in the domain that you are modeling. This leads to the next benefit, explainability.</p>
</div>
<div class="readable-text" id="p85">
<h4 class="readable-text-h4 sigil_not_in_toc">More explainable machine learning</h4>
</div>
<div class="readable-text" id="p86">
<p>Many machine learning algorithms, particularly deep learning algorithms, can be quite “black box,” meaning the internal workings are not easily interpretable, and the process by which the model produces an output for a given input is not easily explainable.</p>
</div>
<div class="readable-text intended-text" id="p87">
<p>In contrast, causal models are eminently explainable because they directly encode easy-to-understand causal relationships in the modeling domain. Indeed, causality is the core of explanation; explaining an event means describing the event’s causes and how they led to the event occurring. Causal models provide explanations in the language of the domain you are modeling (semantic explanations) rather than in terms of the model’s architecture (such as syntactic explanations of “nodes” and “activations”).</p>
</div>
<div class="readable-text intended-text" id="p88">
<p>Consider the examples in figures 1.1 and 1.2. In figure 1.1, only the input features and output are interpretable in terms of the domain; the internal workings of the hidden layers are not. Thus, given a particular ad impression, it is difficult to explain how the model arrives at a particular purchase outcome. In contrast, the example in figure 1.2 explicitly provides engagement to explain how we get from an ad impression to a purchase outcome. </p>
</div>
<div class="readable-text intended-text" id="p89">
<p>The connections between engagement and ad impression, and between purchase and engagement, are still black boxes, but if we need to, we can make additional variables in those black boxes explicit. We just need to make sure we do so in a way that is aligned with our assumptions about the causal structure of the problem.</p>
</div>
<div class="readable-text" id="p90">
<h3 class="readable-text-h3" id="sigil_toc_id_17"><span class="num-string">1.3.4</span> Fairer AI</h3>
</div>
<div class="readable-text" id="p91">
<p>Suppose Bob applies for a business loan. A machine learning algorithm predicts that Bob would be a bad loan candidate, so Bob is rejected. Bob is a man, and he got ahold of the bank’s loan data, which shows that men are less likely to have their loan applications approved. Was this an “unfair” outcome?</p>
</div>
<div class="readable-text intended-text" id="p92">
<p>We might say the outcome is “unfair” if, for example, the algorithm made that prediction <em>because</em> Bob is a man. To be a “fair” prediction, it would need to be formulated from factors relevant to Bob’s ability to pay back the loan, such as his credit history, his line of business, or his available collateral. Bob’s dilemma is another example of why we’d like machine learning to be explainable: so that we can analyze what factors in Bob’s application led to the algorithm’s decision.</p>
</div>
<div class="readable-text intended-text" id="p93">
<p>Suppose the training data came from a history of decisions from loan officers, some of whom harbored a gender prejudice that hurt men. For example, they might have read studies that show men are more likely to default in times of financial difficulty. Based on those studies, they decided to deduct points from their rating if the applicant was a man.</p>
</div>
<div class="readable-text intended-text" id="p94">
<p>Furthermore, suppose that when the data was collected, the bank advertised the loan program on social media. When we look at the campaign results, we notice that the men who responded to the ad were, on average, less qualified than the women who clicked on the ad. This discrepancy might have been because the campaign was better targeted toward women, or because the average bid price in online ad auctions was lower when the ad audience was composed of less-qualified men. Figure 1.3 plots various factors that might influence the loan approval process, and it distinguishes fair from unfair causes. The factors are plotted in a directed acyclic graph (DAG), a popular and effective way to represent causal relationships. We’ll use DAGs as our workhorse for causal reasoning throughout the book.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p95">
<img alt="figure" height="524" src="../Images/CH01_F03_Ness.png" width="574"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.3</span> A causal directed acyclic graph (DAG) showing how statistical bias against a particular gender could come from an algorithm directly penalizing that gender (unfair) and indirectly through gender discrepancies in applicants targeted by digital advertising algorithms (fair). Causal inference can parse the bias into fair and unfair sources.</h5>
</div>
<div class="readable-text" id="p96">
<p>Thus, we have two possible sources of statistical bias against men in the data. One source of bias is from the online ad that attracted men who were, on average, less qualified, leading to a higher rejection rate for men. The other source of statistical bias comes from the prejudice of loan officers. One of these sources of bias is arguably “fair” (it’s hard to blame the bank for the targeting behavior of digital advertising algorithms), and one of the sources is “unfair” (we <em>can</em> blame the bank for sexist loan policies). But when we only look at the training data without this causal context, all we see is statistical bias against men. The learning algorithm reproduced this bias when it made its decision about Bob.</p>
</div>
<div class="readable-text intended-text" id="p97">
<p>One naive solution to this problem is simply to remove gender labels from the training data. But even if those sexist loan officers didn’t see an explicit indication of the person’s gender, they could infer it from elements of the application, such as the person’s name. Those loan officers encode their prejudicial views in the form of a statistical correlation between those proxy variables for gender and loan outcome. The machine learning algorithm would discover this statistical pattern and use it to make predictions. As a result, you could have a situation where the algorithm produces two different predictions for two individuals who had the same repayment risk but differed in gender, even if gender wasn’t a direct input to the prediction. Deploying this algorithm would effectively scale up the harm caused by those loan officers’ prejudicial views. </p>
</div>
<div class="readable-text intended-text" id="p98">
<p>For these reasons, we can see how many fears about the widespread deployment of machine learning algorithms are justified. Without corrections, these algorithms could adversely impact our society by magnifying the unfair outcomes captured in the data that our society produces. </p>
</div>
<div class="readable-text intended-text" id="p99">
<p>Causal analysis is instrumental in parsing these kinds of algorithmic fairness issues. In this example, we could use causal analysis to parse the statistical bias into “unfair” bias due to sexism and bias due to external factors like how the digital advertising service targets ads. Ultimately, we could use causal modeling to build a model that only considers variables <em>causally relevant</em> to whether an individual can repay a loan.</p>
</div>
<div class="readable-text intended-text" id="p100">
<p>It is important to note that causal inference alone is insufficient to solve algorithmic fairness. Causal inference can help parse statistical bias into what is fair and what’s not. And yet, even that depends on all parties involved agreeing on definitions of concepts and outcomes, which is often a tall order. To illustrate, suppose that the social media ad campaign served the loan ad to more men because the cost of serving an ad to men is cheaper. Thus, an ad campaign can win the online ad spot auctions with lower bids when the impression is coming from a man, and, as a result, more men see the ad, though many of these men are not good matches for the loan program. Was this process unfair? Is the result unfair? What is the fairness tradeoff between balanced outcomes across genders and pricing fairness to advertisers? Should some advertisers have to pay more due to pricing mechanisms designed to encourage balanced outcomes? Causal analysis can’t solve these questions, but it can help understand them in technical detail.</p>
</div>
<div class="readable-text" id="p101">
<h2 class="readable-text-h2" id="sigil_toc_id_18"><span class="num-string">1.4</span> How causality is driving the next AI wave</h2>
</div>
<div class="readable-text" id="p102">
<p>Incorporating causal logic into machine learning is leading to new advances in AI. Three trending areas of AI highlighted in this book are representation learning, reinforcement learning, and large language models. These trends in causal AI are reminiscent of the early days of deep learning. People already working with neural networks when the deep learning wave was gaining momentum enjoyed first dibs on new opportunities in this space, and access to opportunities begets access to more opportunities. The next wave of AI is still taking shape, but it is clear it will fundamentally incorporate some representation of causality. The goal of this book is to help you ride that wave.</p>
</div>
<div class="readable-text" id="p103">
<h3 class="readable-text-h3" id="sigil_toc_id_19"><span class="num-string">1.4.1</span> Causal representation learning</h3>
</div>
<div class="readable-text" id="p104">
<p>Many state-of-the-art deep learning methods attempt to learn geometric representations of the objects being modeled. However, these methods struggle with learning causally meaningful representations. For example, consider a video of a child holding a helium-filled balloon on a string. Suppose we had a corresponding vector representation of that image. If the vector representation were causally meaningful, then manipulating the vector to remove the child and converting the manipulated vector to a new video would result in a depiction of the balloon rising upwards. Causal representation learning is a promising area of deep representation learning that’s still in its early stages. This book provides several examples in different chapters of causal models built upon deep learning architectures, providing an introduction to the fundamental ideas used in this exciting new growth area of causal AI.</p>
</div>
<div class="readable-text" id="p105">
<h3 class="readable-text-h3" id="sigil_toc_id_20"><span class="num-string">1.4.2</span> Causal reinforcement learning</h3>
</div>
<div class="readable-text" id="p106">
<p>In canonical reinforcement learning, learning agents ingest large amounts of data and learn like Pavlov’s dog; they learn actions that correlate positively with good outcomes and negatively with bad outcomes. However, as we all know, correlation does not imply causation. Causal reinforcement learning can highlight cases where the action that causes a higher reward differs from the action that correlates most strongly with high rewards. Further, it addresses the problem of credit assignment (correctly attributing rewards to actions) with counterfactual reasoning (i.e., asking questions like “how much reward would the agent have received had they been using a different policy?”). Chapter 12 is devoted to causal reinforcement learning and other areas of causal decision-making.</p>
</div>
<div class="readable-text" id="p107">
<h3 class="readable-text-h3" id="sigil_toc_id_21"><span class="num-string">1.4.3</span> Large language models and foundation models</h3>
</div>
<div class="readable-text" id="p108">
<p>Large language models (LLMs) such as OpenAI’s GPT, Google’s Gemini, and Meta’s Llama are deep neural language models with many billions of parameters trained on vast amounts of text and other data. These models can generate highly coherent natural language, code, and content of other modalities. They are foundation models, meaning they provide a foundation for building more domain-specific machine learning models and products. These products, such as Microsoft 365 Copilot, are already having a tremendous business impact.</p>
</div>
<div class="readable-text intended-text" id="p109">
<p>A new area of investigation and product development investigates LLMs’ ability to answer causal questions and perform causal analysis. Another line of investigation is using causal methods to design and train new LLMs with optimized causal capabilities. In chapter 13, we’ll explore the intersection of LLMs and causality.</p>
</div>
<div class="readable-text" id="p110">
<h2 class="readable-text-h2" id="sigil_toc_id_22"><span class="num-string">1.5</span> A machine learning-themed primer on causality</h2>
</div>
<div class="readable-text" id="p111">
<p>Now that you’ve seen the many ways that causal inference can improve machine learning, let’s look at the process of incorporating causality into AI models. To do this, we will use a popular benchmark dataset often used in machine learning: the MNIST dataset of images of handwritten digits, each labeled with the actual digit represented in the image. Figure 1.4 illustrates multiple examples of the digits in MNIST.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p112">
<img alt="figure" height="655" src="../Images/CH01_F04_Ness.png" width="1035"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.4</span> Each image in the MNIST dataset is an image of a written digit, and each image is labeled with the digit it represents.</h5>
</div>
<div class="readable-text" id="p113">
<p>MNIST is essentially the “Hello World” of machine learning. It is primarily used to experiment with different machine learning algorithms and to compare their relative strengths. The basic prediction task is to take the matrix of pixels representing each image as input and return the correct image label as output. Let’s start the process of incorporating causal thinking into a probabilistic machine learning model applied to MNIST images. </p>
</div>
<div class="readable-text" id="p114">
<h3 class="readable-text-h3" id="sigil_toc_id_23"><span class="num-string">1.5.1</span> Queries, probabilities, and statistics</h3>
</div>
<div class="readable-text" id="p115">
<p>First, we’ll look at the basic process without including causal inference. Machine learning can use probability in analyses about quantities of interest. To do so, a probabilistic machine learning model learns a probabilistic representation of all the variables in that system. We can make predictions and decisions with probabilistic machine learning models using a three-step process.</p>
</div>
<ol>
<li class="readable-text" id="p116"> <em>Pose the question</em><em> </em>—What is the question you want to answer? </li>
<li class="readable-text" id="p117"> <em>Write down the math</em><em> </em>—What probability (or probability-related quantity) will answer the question, given the evidence or data? </li>
<li class="readable-text" id="p118"> <em>Do the statistical inference</em><em> </em>—What statistical analysis will give you (or will <em>estimate</em>) that quantity? </li>
</ol>
<div class="readable-text" id="p119">
<p>There is more formal terminology for these steps (<em>query</em>, <em>estimand</em>, and <em>estimator</em>) but we’ll avoid the jargon for now. Instead, we’ll start with a simple statistical example problem. Your step 1 might be “How tall are Bostonians?” For step 2, you might decide that knowing the <em>mean</em> height (in probability terms, the “expected value”) of everyone who lives in Boston will answer your question. Step 3 might involve randomly selecting 100 Bostonians and taking their average height; statistical theorems guarantee that this sample average is a close estimate of the true population mean.</p>
</div>
<div class="readable-text intended-text" id="p120">
<p>Let’s extend that workflow to modeling MNIST images.</p>
</div>
<div class="readable-text" id="p121">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 1: Pose the question</h4>
</div>
<div class="readable-text" id="p122">
<p>Suppose we are looking at the MNIST image in figure 1.5, which could be a “4” or could be a “9”. In step 1, we articulate a question, such as “given this image, what is the digit represented in this image?”<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p123">
<img alt="figure" height="520" src="../Images/CH01_F05_Ness.png" width="420"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.5</span> Is this an image of the digit 4 or 9? The canonical task of the MNIST dataset is to classify the digit label given the image.</h5>
</div>
<div class="readable-text" id="p124">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 2: Write down the math</h4>
</div>
<div class="readable-text" id="p125">
<p>In step 2, we want to find some probabilistic quantity that answers the question, given the evidence or data. In other words, we want to find something we can write down in probability math notation that can answer the question from step 1. For our example with figure 1.5, the “evidence” or “data” is the image. Is the image a 4 or a 9? Let the variable <em>I</em> represent the image and <em>D</em> represent the digit. In probability notation, we can write the probability that the digit is a 4, given the image, as <em>P</em>(<em>D</em>=4|<em>I</em>=<img alt="figure" height="41px" src="../Images/CH01_F05_Ness.png" width="33px"/>)<em>, </em>where <em>I</em>=<img alt="figure" height="41px" src="../Images/CH01_F05_Ness.png" width="33px"/> is shorthand for <em>I</em> being equal to some vector representation of the image. We can compare this probability to <em>P</em>(<em>D</em>=9|<em>I</em>=<img alt="figure" height="41px" src="../Images/CH01_F05_Ness.png" width="33px"/>),<em> </em>and choose the value of <em>D</em> that has the higher probability. Generalizing to all ten digits, the mathematical quantity we want in step 2 is shown in figure 1.6.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p126">
<img alt="figure" height="61" src="../Images/CH01_F06_Ness.png" width="301"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.6</span> Choose the digit with the highest probability, given the image.</h5>
</div>
<div class="readable-text" id="p127">
<p>In plain English, this is “the value <em>d</em> that maximizes the probability that <em>D</em> equals <em>d,</em> given the image,” where <em>d</em> is one of the ten digits (0–9).</p>
</div>
<div class="readable-text" id="p128">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 3: Do the statistical inference</h4>
</div>
<div class="readable-text" id="p129">
<p>Step 3 uses statistical analysis to assign a number to the quantity we identified in step 2. There are any number of ways we can do this. For example, we could train a deep neural network that takes in the image as an input and predicts the digit as an output; we could design the neural net to assign a probability to <em>D</em>=<em>d</em> for every value <em>d</em>.</p>
</div>
<div class="readable-text" id="p130">
<h3 class="readable-text-h3" id="sigil_toc_id_24"><span class="num-string">1.5.2</span> Causality and MNIST</h3>
</div>
<div class="readable-text" id="p131">
<p>So how could causality feature in the previous section’s three-step analysis? Yann LeCun is a Turing Award winner (computer science’s equivalent of the Nobel prize) for his work on deep learning, and he’s director of AI research at Meta. He is also one of the three researchers behind the creation of MNIST. He discusses the <em>causal</em> backstory of the MNIST data on his personal website, <a href="https://yann.lecun.com/exdb/mnist/index.xhtml">https://yann.lecun.com/exdb/mnist/index.xhtml</a>:</p>
</div>
<div class="readable-text" id="p132">
<blockquote>
<div>
     The MNIST database was constructed from NIST’s Special Database 3 and Special Database 1 which contain binary images of handwritten digits. NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore, it was necessary to build a new database by mixing NIST’s datasets.
    </div>
</blockquote>
</div>
<div class="readable-text" id="p133">
<p>In other words, the authors mixed the two datasets because they argue that if they trained a machine learning model solely on digits drawn by high schoolers, it would underperform when applied to digits drawn by bureaucrats. However, in real-world settings, we want robust models that can learn in one scenario and predict in another, even when those scenarios differ. For example, we want a spam filter to keep working when the spammers switch from Nigerian princes to Bhutanese princesses. We want our self-driving cars to stop even when there is graffiti on the stop sign. Shuffling the data like a deck of cards is a luxury not easily afforded in real-world settings.</p>
</div>
<div class="readable-text intended-text" id="p134">
<p>Causal modeling leverages knowledge about the causal mechanisms underlying how the digits are drawn that will help models generalize beyond high school students and bureaucrats in the training data to high schoolers in the test data. Figure 1.7 illustrates a causal DAG representing this system.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p135">
<img alt="figure" height="370" src="../Images/CH01_F07_Ness.png" width="487"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.7</span> An example causal DAG representing the generation of MNIST images. The nodes represent objects in the data generating process, and edges correspond to causal relationships between those objects.</h5>
</div>
<div class="readable-text intended-text" id="p136">
<p>This particular DAG imagines that the writer determines the thickness and curviness of the drawn digits, and that high schoolers tend to have a different handwriting style than bureaucrats. The graph also assumes that the writer’s classification is a cause of what digits they draw. Perhaps bureaucrats write more 1s, 0s, and 5s, as these numbers occur more frequently in census work, while high schoolers draw other digits more often because they do more long division in math classes (this is a similar idea to how, in topic models, “topics” <em>cause</em> the frequency of words in a document). Finally, the DAG assumes that age is a common cause of writer type and image; you have to be below a certain age to be in high school and above a certain age to be a census official.</p>
</div>
<div class="readable-text intended-text" id="p137">
<p>A causal modeling approach would use this causal knowledge to train a predictive model that could extrapolate from the high school training data to the bureaucrat test data. Such a model would generalize better to new situations where the distributions of writer type and other variables are different than in the training data. </p>
</div>
<div class="readable-text" id="p138">
<h3 class="readable-text-h3" id="sigil_toc_id_25"><span class="num-string">1.5.3</span> Causal queries, probabilities, and statistics</h3>
</div>
<div class="readable-text" id="p139">
<p>At the beginning of this chapter, I discussed various types of causal questions we can pose, such as causal discovery, quantifying causal effects, and causal decision-making. We can answer these and various other questions with a causal variation on our previous three-step analysis (pose the question, write down the math, do the statistical inference):</p>
</div>
<ol>
<li class="readable-text" id="p140"> <em>Pose the causal question</em><em> </em>—What is the question you want to answer? </li>
<li class="readable-text" id="p141"> <em>Write down the causal math</em><em> </em>—What probability (or expectation) will answer the causal question, given the evidence or data? </li>
<li class="readable-text" id="p142"> <em>Do the statistical inference</em><em> </em>—What statistical analysis will give you (or “estimate”) that causal quantity? </li>
</ol>
<div class="readable-text" id="p143">
<p>Note that the third step is the same as in the original three steps. The causal nuance occurs in the first and second steps.</p>
</div>
<div class="readable-text" id="p144">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 1: Pose the causal question</h4>
</div>
<div class="readable-text" id="p145">
<p>These are examples of some causal questions we could ask about our causal MNIST model:</p>
</div>
<ul>
<li class="readable-text" id="p146"> “How much does the writer’s type (high schooler vs. bureaucrat) affect the look of an image of the digit 4 with level 3 thickness?”<em> </em>(<em>Conditional average treatment effect estimation</em> is discussed in chapter 11). </li>
<li class="readable-text" id="p147"> Assuming that stroke thickness is a cause of the image, we might ask, “What would a 2 look like if it were as curvy as possible?” (This is <em>intervention prediction</em>, discussed in chapter 7). </li>
<li class="readable-text" id="p148"> “Given an image, how would it have turned out differently if the stroke curviness were heavier?” (See <em>counterfactual reasoning</em>, discussed in chapters 8 and 9). </li>
<li class="readable-text" id="p149"> “What should the stroke curviness be to get an aesthetically ideal image?” (<em>Causal decision-making</em> is discussed in chapter 12). </li>
</ul>
<div class="readable-text" id="p150">
<p>Let’s consider the CATE in the first item. CATE estimation is a common causal inference question applied to ordinary tabular data, but rarely do we see it in the applied in the context of an AI computer vision problem.</p>
</div>
<div class="readable-text" id="p151">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 2: Write down the causal math</h4>
</div>
<div class="readable-text" id="p152">
<p>Causal inference theory tells us how to mathematically formalize our causal question. Using special causal notation, we can mathematically formalize our CATE query as follows:</p>
</div>
<div class="browsable-container figure-container" id="p153">
<img alt="figure" height="27" src="../Images/ness-ch1-eqs-0x.png" width="664"/>
</div>
<div class="readable-text" id="p154">
<p>where <em>E</em>(.) is an expectation operator. We’ll review expectation in the next chapter, but for now we can think of it as an averaging of pixels across images.</p>
</div>
<div class="readable-text intended-text" id="p155">
<p>The preceding use of subscripts is a special notation called “counterfactual notation” that represents an <em>intervention</em>. A random assignment in an experiment is a real-world intervention, but there are many experiments we can’t run in the real world. For example, it wouldn’t be feasible to run a trial where you randomly assign participants to either be a high school student or be a census bureau official. Nonetheless, we want to know how the writer type causally impacts the images, and thus we rely on a causal model and its ability to represent interventions.</p>
</div>
<div class="readable-text intended-text" id="p156">
<p>To illustrate, figure 1.8 visualizes what CATE might look like. The challenge is deriving the differential image at the right of figure 1.8. Causal inference theory helps us address potential age-related “confounding” bias in quantifying how much writer type drives the image. For example, the <em>do-calculus</em> (chapter 10) is a set of graph-based rules that allows us to take this DAG and algorithmically derive the following equation:<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p157">
<img alt="figure" height="52" src="../Images/ness-ch1-eqs-1x.png" width="885"/>
</div>
<div class="readable-text" id="p158">
<p>The left side of this equation defines the expectations used in the CATE definition in the second step—it is a theoretical construct that captures the hypothetical condition “if writer type were set to ‘w’”. But the right side is actionable; it is composed entirely of terms we could estimate using machine learning methods on a hypothetical version of NIST image data labeled with the writers’ ages.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p159">
<img alt="figure" height="374" src="../Images/CH01_F08_Ness.png" width="808"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 1.8</span> Visualization of an example CATE of writer type on an image. It is the pixel-by-pixel difference of the expected image under one intervention (<em>W=</em><sub>"high school"</sub>) minus the expected image under another intervention (<em>W=</em><sub>"bureaucrat"</sub>), with both expectations conditional on being images of the digit 4 with a certain level of thickness.</h5>
</div>
<div class="readable-text" id="p160">
<h4 class="readable-text-h4 sigil_not_in_toc">Step 3: Do the statistical inference</h4>
</div>
<div class="readable-text" id="p161">
<p>Step 3 does the statistical estimation, and there are several ways we could estimate the quantities on the right side of that equation. For example, we could use a convolutional neural network to model <em>E</em><em> </em>(<em>I</em><em> </em>|<em>W</em><em> </em>=<em> </em><em>w</em>, <em>A</em><em> </em>=<em> </em><em>a</em>, <em>D</em><em> </em>=<em> </em><em>d</em>, <em>T</em><em> </em>=<em> </em><em>t</em>), and build a probability model of the joint distribution <em>P</em><em> </em>(<em>A</em>, <em>D</em>, <em>T</em>). The choice of statistical modeling approach involves the usual statistical trade-offs, such as ease-of-use, bias and variance, scalability to large data, and parallelizability. </p>
</div>
<div class="readable-text intended-text" id="p162">
<p>Other books go into great detail on preferred statistical methods for step 3. I take the strongly opinionated view that we should rely on the “commodification of inference” trend in statistical modeling and machine learning frameworks to handle step 3, and instead focus on honing our skills on steps 1 and 2: figuring out the right questions to ask, and representing the possible causes mathematically.</p>
</div>
<div class="readable-text intended-text" id="p163">
<p>As you’ve seen in this section, our journey into causal AI is scaffolded by a three-step process, and the essence of causal thinking emerges prominently in the first two steps. Step 1 invites us to frame the right causal questions, while step 2 illuminates the mathematics behind these questions. Step 3 leverages patterns we’re well-accustomed to in traditional statistical prediction and inference. </p>
</div>
<div class="readable-text intended-text" id="p164">
<p>Using this structured approach, we’ll transition in the coming chapters from purely predictive machine learning models—like the deep latent variable models you might be familiar with from MNIST—to causal machine learning models that offer deeper insights into and answers to our causal questions. First, we will review the underlying mathematics and machine learning foundations. Then, in part 2 of the book, we’ll delve into crafting the right questions and articulating them mathematically for steps 1 and 2. For step 3, we’ll harness the power of contemporary tools like PyTorch and other advanced libraries to bridge the causal concepts with cutting-edge statistical learning algorithms.</p>
</div>
<div class="readable-text" id="p165">
<h2 class="readable-text-h2" id="sigil_toc_id_26">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p166"> Causal AI seeks to augment statistical learning and probabilistic reasoning with causal logic. </li>
<li class="readable-text" id="p167"> Causal inference helps data scientists extract more causal insights from observational data (the vast majority of data in the world) and experimental data. </li>
<li class="readable-text" id="p168"> When data scientists can’t run experiments, causal models can simulate experiments from observational data. </li>
<li class="readable-text" id="p169"> They can use these simulations to make causal inferences, such as estimating causal effects, and even to prioritize interesting experiments to run in real life. </li>
<li class="readable-text" id="p170"> Causal inference also helps data scientists improve decision-making in their organizations through algorithmic counterfactual reasoning and attribution. </li>
<li class="readable-text" id="p171"> Causal inference also makes machine learning more <em>robust</em>, <em>decomposable</em>, and <em>explainable</em>. </li>
<li class="readable-text" id="p172"> Causal analysis is useful for formally analyzing <em>fairness</em> in predictive algorithms and for building fairer algorithms by parsing ordinary statistical bias into its causal sources. </li>
<li class="readable-text" id="p173"> The <em>commodification of inference</em> is a trend in machine learning that refers to how universal modeling frameworks like PyTorch continuously automate the nuts and bolts of statistical learning and probabilistic inference. The trend reduces the need for the modeler to be an expert at the formal and statistical details of causal inference and allows them to focus on turning domain expertise into better causal models of their problem domain. </li>
<li class="readable-text" id="p174"> Types of causal inference tasks include <em>causal discovery</em>, <em>intervention prediction</em>,<em> causal effect estimation</em>, c<em>ounterfactual reasoning</em>, <em>explanation</em>, and <em>attribution</em>. </li>
<li class="readable-text" id="p175"> The way we build and work with probabilistic machine learning models can be extended to causal generative models implemented in probabilistic machine learning tools such as PyTorch. </li>
</ul>
</div></body></html>