["```py\n08.1 Sequence to sequence mapping\n08.2 Import Gutenberg\n08.3 Subword tokenizing\n```", "```py\ndef create_seq2seq(num_nodes, num_layers):\n    question = Input(shape=(max_question_len, len(chars),\n                     name='question'))\n    repeat = RepeatVector(max_expected_len)(question)\n    prev = input\n    for _ in range(num_layers)::\n        lstm = LSTM(num_nodes, return_sequences=True,\n                    name='lstm_layer_%d' % (i + 1))(prev)\n        prev = lstm\n    dense = TimeDistributed(Dense(num_chars, name='dense',\n                            activation='softmax'))(prev)\n    model = Model(inputs=[input], outputs=[dense])\n    optimizer = RMSprop(lr=0.01)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    return model\n```", "```py\nwith open('data/gutenberg_index.json') as fin:\n    authors = json.load(fin)\nrecent = [x for x in authors\n          if 'birthdate' in x and x['birthdate'] > 1830]\n[(x['name'], x['birthdate'], x['english_books']) for x in recent[:5]]\n```", "```py\n[('Twain, Mark', 1835, 210),\n ('Ebers, Georg', 1837, 164),\n ('Parker, Gilbert', 1862, 135),\n ('Fenn, George Manville', 1831, 128),\n ('Jacobs, W. W. (William Wymark)', 1863, 112)]\n```", "```py\ndef extract_conversations(text, quote='\"'):\n    paragraphs = PARAGRAPH_SPLIT_RE.split(text.strip())\n    conversations = [['']]\n    for paragraph in paragraphs:\n        chunks = paragraph.replace('\\n', ' ').split(quote)\n        for i in range((len(chunks) + 1) // 2):\n            if (len(chunks[i * 2]) > 100\n                or len(chunks) == 1) and conversations[-1] != ['']:\n                if conversations[-1][-1] == '':\n                    del conversations[-1][-1]\n                conversations.append([''])\n            if i * 2 + 1 < len(chunks):\n                chunk = chunks[i * 2 + 1]\n                if chunk:\n                    if conversations[-1][-1]:\n                        if chunk[0] >= 'A' and chunk[0] <= 'Z':\n                            if conversations[-1][-1].endswith(','):\n                                conversations[-1][-1] = \\\n                                     conversations[-1][-1][:-1]\n                            conversations[-1][-1] += '.'\n                        conversations[-1][-1] += ' '\n                    conversations[-1][-1] += chunk\n        if conversations[-1][-1]:\n            conversations[-1].append('')\n\n    return [x for x in conversations if len(x) > 1]\n```", "```py\nfor author in recent[:1000]:\n    for book in author['books']:\n        txt = strip_headers(load_etext(int(book[0]))).strip()\n        conversations += extract_conversations(txt)\n```", "```py\nwith open('gutenberg.txt', 'w') as fout:\n    for conv in conversations:\n        fout.write('\\n'.join(conv) + '\\n\\n')\n```", "```py\nRE_TOKEN = re.compile('(\\w+|\\?)', re.UNICODE)\ntoken_counter = Counter()\nwith open('gutenberg.txt') as fin:\n    for line in fin:\n        line = line.lower().replace('_', ' ')\n        token_counter.update(RE_TOKEN.findall(line))\nwith open('gutenberg.tok', 'w') as fout:\n    for token, count in token_counter.items():\n        fout.write('%s\\t%d\\n' % (token, count))\n```", "```py\n./learn_bpe.py -s 25000 < gutenberg.tok > gutenberg.bpe\n```", "```py\n./apply_bpe.py -c gutenberg.bpe < some_text.txt > some_text.bpe.txt\n```", "```py\nRE_TOKEN = re.compile('(\\w+|\\?)', re.UNICODE)\ndef tokenize(st):\n    st = st.lower().replace('_', ' ')\n    return ' '.join(RE_TOKEN.findall(st))\n\npairs = []\nprev = None\nwith open('data/gutenberg.txt') as fin:\n    for line in fin:\n        line = line.strip()\n        if line:\n            sentences = nltk.sent_tokenize(line)\n            if prev:\n                pairs.append((prev, tokenize(sentences[0])))\n            prev = tokenize(sentences[-1])\n        else:\n            prev = None\n```", "```py\nrandom.shuffle(pairs)\nss = len(pairs) // 20\n\ndata = {'dev': pairs[:ss],\n        'test': pairs[ss:ss * 2],\n        'train': pairs[ss * 2:]}\n```", "```py\nfor tag, pairs2 in data.items():\n    path = 'seq2seq/%s' % tag\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    with open(path + '/sources.txt', 'wt') as sources:\n        with open(path + '/targets.txt', 'wt') as targets:\n            for source, target in pairs2:\n                sources.write(source + '\\n')\n                targets.write(target + '\\n')\n```", "```py\ngit clone https://github.com/google/seq2seq.git\ncd seq2seq\npip install -e .\n```", "```py\nExport SEQ2SEQROOT=/path/to/data/seq2seq\n```", "```py\npython -m bin.train \\                                                                                                               --config_paths=\"                                                                                                                                                ./example_configs/nmt_large.yml,\n      ./example_configs/train_seq2seq.yml\" \\\n  --model_params \"\n      vocab_source: $SEQ2SEQROOT/gutenberg.tok\n      vocab_target: $SEQ2SEQROOT/gutenberg.tok\" \\\n  --input_pipeline_train \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - $SEQ2SEQROOT/train/sources.txt\n      target_files:\n        - $SEQ2SEQROOT/train/targets.txt\" \\\n  --input_pipeline_dev \"\n    class: ParallelTextInputPipeline\n    params:\n       source_files:\n        - $SEQ2SEQROOT/dev/sources.txt\n       target_files:\n        - $SEQ2SEQROOT/dev/targets.txt\" \\\n  --batch_size 1024  --eval_every_n_steps 5000 \\\n  --train_steps 5000000 \\\n  --output_dir $SEQ2SEQROOT/model_large\n```", "```py\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\" \\\n  --model_dir $SEQ2SEQROOT/model_large \\\n  --input_pipeline \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - '/tmp/test_questions.txt'\"\n```", "```py\n> hi\nhi\n> what is your name ?\nsam barker\n> how do you feel ?\nFine\n> good night\ngood night\n```"]