- en: Image generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像生成
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter17_image-generation](https://deeplearningwithpython.io/chapters/chapter17_image-generation)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://deeplearningwithpython.io/chapters/chapter17_image-generation](https://deeplearningwithpython.io/chapters/chapter17_image-generation)'
- en: 'The most popular and successful application of creative AI today is image generation:
    learning latent visual spaces and sampling from them to create entirely new pictures,
    interpolated from real ones — pictures of imaginary people, imaginary places,
    imaginary cats and dogs, and so on.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 目前创意AI最受欢迎和最成功的应用是图像生成：学习潜在视觉空间并从中采样以创建全新的图片，这些图片是从真实图片中插值出来的——比如虚构人物、虚构地点、虚构的猫狗等等。
- en: Deep learning for image generation
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像生成的深度学习
- en: 'In this section and the next, we’ll review some high-level concepts pertaining
    to image generation, alongside implementation details relative to two of the main
    techniques in this domain: *variational autoencoders* (VAEs) and *diffusion models*.
    Do note that the techniques we present here aren’t specific to images — you could
    develop latent spaces of sound or music using similar models — but in practice,
    the most interesting results so far have been obtained with pictures, and that’s
    what we focus on here.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节和下一节中，我们将回顾一些与图像生成相关的高级概念，以及与该领域两大主要技术相关的实现细节：*变分自编码器*（VAEs）和*扩散模型*。请注意，我们在这里介绍的技术并不仅限于图像——你可以使用类似的模型来开发声音或音乐的潜在空间——但在实践中，迄今为止最有趣的结果都是通过图片获得的，这正是我们在这里关注的重点。
- en: Sampling from latent spaces of images
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从图像的潜在空间中进行采样
- en: 'The key idea of image generation is to develop a low-dimensional *latent space*
    of representations (which, like everything else in deep learning, is a vector
    space) where any point can be mapped to a “valid” image: an image that looks like
    the real thing. The module capable of realizing this mapping, taking as input
    a latent point and outputting an image (a grid of pixels), is usually called a
    *generator*, or sometimes a *decoder*. Once such a latent space has been learned,
    you can sample points from it, and, by mapping them back to image space, generate
    images that have never been seen before (see figure 17.1) — the in-betweens of
    the training images.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成的关键思想是开发一个低维的*潜在空间*，用于表示（这就像深度学习中的所有其他事物一样，是一个向量空间），其中任何一点都可以映射到一个“有效”的图像：一个看起来像真实事物的图像。能够实现这种映射的模块，以潜在点为输入，输出图像（像素网格），通常被称为*生成器*，有时也称为*解码器*。一旦学习到这样的潜在空间，你就可以从中采样点，并通过将它们映射回图像空间，生成以前从未见过的图像（见图17.1）——训练图像之间的中间状态。
- en: '![](../Images/9e23d16af989a4b5dcff3bd4b3e2ca7a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e23d16af989a4b5dcff3bd4b3e2ca7a.png)'
- en: '[Figure 17.1](#figure-17-1): Using a latent vector space to sample new images'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.1](#figure-17-1)：使用潜在向量空间采样新图像'
- en: Further, *text-conditioning* makes it possible to map a space of prompts in
    natural language to the latent space (see figure 17.2), making it possible to
    do *language-guided image generation* — generating pictures that correspond to
    a text description. This category of models is called *text-to-image* models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*文本条件化*使得将自然语言中的提示空间映射到潜在空间成为可能（见图17.2），从而使得进行*语言引导的图像生成*成为可能——生成与文本描述相对应的图片。这类模型被称为*文本到图像*模型。
- en: Interpolating between many training images in the latent space enables such
    models to generate infinite combinations of visual concepts, including many that
    no one had explicitly come up with before. A horse riding a bike on the moon?
    You got it. This makes image generation a powerful brush for creative-minded people
    to play with.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜在空间中在许多训练图像之间进行插值，使得这些模型能够生成无限多的视觉概念组合，包括许多以前没有人明确提出过的。比如在月球上骑自行车的马？你做到了。这使得图像生成成为创意人士进行创作的强大画笔。
- en: '![](../Images/7dfdbf6716859d9d3c6913ee78fa1687.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dfdbf6716859d9d3c6913ee78fa1687.png)'
- en: '[Figure 17.2](#figure-17-2): Language-guided image generation'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.2](#figure-17-2)：语言引导的图像生成'
- en: Of course, there are still challenges to overcome. Like with all deep learning
    models, the latent space doesn’t encode a consistent model of the physical world,
    so you might occasionally see hands with extra fingers, incoherent lighting, or
    garbled objects. The coherence of generated images is still an area of active
    research. In the case of figure 17.2, despite having seen tens of thousands of
    images of people riding bikes, the model doesn’t understand in a human sense what
    it means to ride a bike — concepts like pedaling, steering, or maintaining upright
    balance. That’s why your bike-riding horse is unlikely to get depicted pedaling
    with its hind legs in a believable manner, the way a human artist would draw it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，仍然存在需要克服的挑战。与所有深度学习模型一样，潜在空间并没有编码一个一致的物理世界模型，所以你可能会偶尔看到多指的手、不连贯的照明或混乱的物体。生成的图像的连贯性仍然是一个活跃的研究领域。在图17.2的情况下，尽管已经看到了成千上万张人们骑自行车的图片，但模型并没有以人类的方式理解骑自行车的含义——比如踩踏、转向或保持直立平衡等概念。这就是为什么你的骑自行车的马不太可能以可信的方式描绘出用后腿踩踏，就像人类艺术家会画的那样。
- en: There’s a range of different strategies for learning such latent spaces of image
    representations, each with its own characteristics. The most common types of image
    generation models are
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 学习图像表示的潜在空间有多种不同的策略，每种策略都有其自身的特点。最常见的图像生成模型类型包括
- en: Diffusion models
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型
- en: Variational autoencoders (VAEs)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自编码器 (VAEs)
- en: Generative adversarial networks (GANs)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络 (GANs)
- en: While previous editions of this book covered GANs, they have gradually fallen
    out of fashion in recent years and have been all but replaced by diffusion models.
    In this edition, we’ll cover both VAEs and diffusion models and we will skip GANs.
    In the models we’ll build ourselves, we’ll focus on unconditioned image generation
    — sampling images from a latent space without text conditioning. However, you
    will also learn how to use a pretrained text-to-image model and how to explore
    its latent space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然此书的先前版本涵盖了生成对抗网络（GANs），但近年来它们已经逐渐过时，几乎被扩散模型所取代。在本版中，我们将涵盖VAEs和扩散模型，并跳过GANs。在我们自己构建的模型中，我们将专注于无条件的图像生成——从潜在空间中采样图像而不需要文本条件。然而，你还将学习如何使用预训练的文本到图像模型以及如何探索其潜在空间。
- en: Variational autoencoders
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: VAEs, simultaneously discovered by Kingma and Welling in December 2013^([[1]](#footnote-1))
    and Rezende, Mohamed, and Wierstra in January 2014,^([[2]](#footnote-2)) are a
    kind of generative model that’s especially appropriate for the task of image editing
    via concept vectors. They’re a kind of *autoencoder* — a type of network that
    aims to encode an input to a low dimensional latent space and then decode it back
    — that mixes ideas from deep learning with Bayesian inference.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs（变分自编码器），由Kingma和Welling于2013年12月^([[1]](#footnote-1))和Rezende、Mohamed和Wierstra于2014年1月^([[2]](#footnote-2))同时发现，是一种特别适合通过概念向量进行图像编辑的生成模型。它们是一种*自编码器*——一种旨在将输入编码到低维潜在空间并解码回原始输入的网络——它结合了深度学习和贝叶斯推理的思想。
- en: VAEs have been around for over a decade, but they remain relevant to this day
    and continue to be used in recent research. While VAEs will never be the first
    choice for generating high-fidelity images — where diffusion models excel — they
    remain an important tool in the deep learning toolbox, particularly when interpretability,
    control over the latent space, and data reconstruction capabilities are crucial.
    It’s also your first contact with the concept of the autoencoder, which is useful
    to know about. VAEs beautifully illustrate the core idea behind this class of
    models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs已经存在十多年了，但它们至今仍然相关，并继续在最近的研究中被使用。虽然VAEs永远不会是生成高保真图像的首选——在这方面扩散模型表现更佳——但它们仍然是深度学习工具箱中的一个重要工具，尤其是在可解释性、对潜在空间的控制和数据重建能力至关重要的场合。它也是你第一次接触自编码器的概念，了解这一点是有用的。VAEs完美地展示了这类模型背后的核心思想。
- en: A classical image autoencoder takes an image, maps it to a latent vector space
    via an encoder module, and then decodes it back to an output with the same dimensions
    as the original image, via a decoder module (see figure 17.3). It’s then trained
    by using as target data the *same images* as the input images, meaning the autoencoder
    learns to reconstruct the original inputs. By imposing various constraints on
    the code (the output of the encoder), you can get the autoencoder to learn more
    or less interesting latent representations of the data. Most commonly, you’ll
    constrain the code to be low-dimensional and sparse (mostly zeros), in which case
    the encoder acts as a way to compress the input data into fewer bits of information.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的图像自编码器通过编码器模块将图像映射到潜在向量空间，然后通过解码器模块将其解码回与原始图像相同维度的输出（见图17.3）。它通过使用与输入图像相同的图像作为目标数据进行训练，这意味着自编码器学习重建原始输入。通过对代码（编码器的输出）施加各种约束，可以使自编码器学习到数据更有趣或更少的潜在表示。最常见的是，将代码约束为低维和稀疏（主要是零），在这种情况下，编码器充当将输入数据压缩成更少信息位的方式。
- en: '![](../Images/bb897f734939d070b5d3c8dc78cbf12d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/bb897f734939d070b5d3c8dc78cbf12d.png)'
- en: '[Figure 17.3](#figure-17-3): An autoencoder: mapping an input `x` to a compressed
    representation and then decoding it back as `x''`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.3](#figure-17-3)：自动编码器：将输入`x`映射到压缩表示，然后解码回`x''`'
- en: In practice, such classical autoencoders don’t lead to particularly useful or
    nicely structured latent spaces. They’re not much good at compression either.
    For these reasons, they have largely fallen out of fashion. VAEs, however, augment
    autoencoders with a little bit of statistical magic that forces them to learn
    continuous, highly structured latent spaces. They have turned out to be a powerful
    tool for image generation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这种经典的自动编码器并没有导致特别有用或结构良好的潜在空间。它们在压缩方面也不太有用。因此，它们在很大程度上已经过时。然而，VAE通过一点统计魔法增强了自动编码器，迫使它们学习连续、高度结构的潜在空间。它们已经证明是图像生成的一个强大工具。
- en: 'A VAE, instead of compressing its input image into a fixed code in the latent
    space, turns the image into the parameters of a statistical distribution: a mean
    and a variance. Essentially, this means we’re assuming the input image has been
    generated by a statistical process, and that the randomness of this process should
    be taken into account during encoding and decoding. The VAE then uses the mean
    and variance parameters to randomly sample one element of the distribution, and
    decodes that element back to the original input (see figure 17.4). The stochasticity
    of this process improves robustness and forces the latent space to encode meaningful
    representations everywhere: every point sampled in the latent space is decoded
    to a valid output.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，变分自编码器（VAE）不是将其输入图像压缩到潜在空间中的固定代码，而是将图像转换为统计分布的参数：均值和方差。本质上，这意味着我们假设输入图像是由统计过程生成的，并且在这个过程中应该考虑随机性。VAE然后使用均值和方差参数随机采样分布中的一个元素，并将该元素解码回原始输入（见图17.4）。这个过程的不确定性提高了鲁棒性，并迫使潜在空间在各个地方编码有意义的表示：潜在空间中采样的每个点都被解码为有效的输出。
- en: '![](../Images/865a82d8137e658bc20fb4f1848c4267.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/865a82d8137e658bc20fb4f1848c4267.png)'
- en: '[Figure 17.4](#figure-17-4): A VAE maps an image to two vectors, `z_mean` and
    `z_log_sigma`, which define a probability distribution over the latent space,
    used to sample a latent point to decode.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.4](#figure-17-4)：VAE将图像映射到两个向量`z_mean`和`z_log_sigma`，这些向量定义了潜在空间上的概率分布，用于采样一个潜在点进行解码。'
- en: 'In technical terms, here’s how a VAE works:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术术语上，以下是VAE的工作原理：
- en: An encoder module turns the input sample `input_img` into two parameters in
    a latent space of representations, `z_mean` and `z_log_variance`.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器模块将输入样本`input_img`转换到表示的潜在空间中的两个参数，`z_mean`和`z_log_variance`。
- en: You randomly sample a point `z` from the latent normal distribution that’s assumed
    to generate the input image, via `z = z_mean + exp(z_log_variance) * epsilon`,
    where `epsilon` is a random tensor of small values.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您从假设生成输入图像的潜在正态分布中随机采样一个点`z`，通过`z = z_mean + exp(z_log_variance) * epsilon`，其中`epsilon`是一个包含小值的随机张量。
- en: A decoder module maps this point in the latent space back to the original input
    image.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器模块将潜在空间中的这个点映射回原始输入图像。
- en: Because `epsilon` is random, the process ensures that every point that’s close
    to the latent location where you encoded `input_img` (`z-mean`) can be decoded
    to something similar to `input_img`, thus forcing the latent space to be continuously
    meaningful. Any two close points in the latent space will decode to highly similar
    images. Continuity, combined with the low dimensionality of the latent space,
    forces every direction in the latent space to encode a meaningful axis of variation
    of the data, making the latent space very structured and thus highly suitable
    to manipulation via concept vectors.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`epsilon`是随机的，这个过程确保了每个接近你编码`input_img`（`z-mean`）的潜在位置的点都可以解码成类似于`input_img`的东西，从而迫使潜在空间具有连续的有意义性。潜在空间中任何两个接近的点都会解码成高度相似的画面。连续性，加上潜在空间低维度的特性，迫使潜在空间中的每个方向都编码了数据的有意义变化轴，使得潜在空间非常结构化，因此非常适合通过概念向量进行操作。
- en: 'The parameters of a VAE are trained via two loss functions: a *reconstruction
    loss* that forces the decoded samples to match the initial inputs, and a *regularization
    loss* that helps learn well-rounded latent distributions and reduces overfitting
    to the training data. Schematically, the process looks like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的参数通过两个损失函数进行训练：一个*重建损失*，它迫使解码样本与初始输入相匹配，以及一个*正则化损失*，它有助于学习均匀的潜在分布并减少对训练数据的过度拟合。从示意图上看，这个过程看起来是这样的：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can then train the model using the reconstruction loss and the regularization
    loss. For the regularization loss, we typically use an expression (the Kullback–Leibler
    divergence) meant to nudge the distribution of the encoder output toward a well-rounded
    normal distribution centered around 0. This provides the encoder with a sensible
    assumption about the structure of the latent space it’s modeling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用重建损失和正则化损失来训练模型。对于正则化损失，我们通常使用一个表达式（Kullback–Leibler散度），其目的是将编码器输出的分布推向一个以0为中心的均匀分布。这为编码器提供了对其建模的潜在空间结构的合理假设。
- en: Now let’s see what implementing a VAE looks like in practice!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下在实践中实现VAE是什么样的！
- en: Implementing a VAE with Keras
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Keras实现VAE
- en: 'We’re going to be implementing a VAE that can generate MNIST digits. It’s going
    to have three parts:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个可以生成MNIST数字的VAE。它将包含三个部分：
- en: An encoder network that turns a real image into a mean and a variance in the
    latent space
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将真实图像转换为潜在空间中的均值和方差的编码网络
- en: A sampling layer that takes such a mean and variance and uses them to sample
    a random point from the latent space
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个采样层，它接受这样的均值和方差，并使用它们从潜在空间中采样一个随机点
- en: A decoder network that turns points from the latent space back into images
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个解码器网络，它将潜在空间中的点转换回图像
- en: The following listing shows the encoder network you’ll use, mapping images to
    the parameters of a probability distribution over the latent space. It’s a simple
    ConvNet that maps the input image `x` to two vectors, `z_mean` and `z_log_var`.
    One important detail is that we use strides for downsampling feature maps, instead
    of max pooling. The last time we did this was in the image segmentation example
    of chapter 11\. Recall that, in general, strides are preferable to max pooling
    for any model that cares about *information location* — that is, *where* stuff
    is in the image — and this one does, since it will have to produce an image encoding
    that can be used to reconstruct a valid image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了您将使用的编码器网络，它将图像映射到潜在空间上概率分布的参数。它是一个简单的卷积神经网络，将输入图像`x`映射到两个向量，`z_mean`和`z_log_var`。一个重要的细节是我们使用步长进行特征图的下采样，而不是最大池化。我们上次这样做是在第11章的图像分割示例中。回想一下，通常来说，对于任何关心*信息位置*（即图像中的*哪里*有东西）的模型，步长比最大池化更可取，因为这个模型确实需要产生一个可以用来重建有效图像的图像编码。
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 17.1](#listing-17-1): VAE encoder network'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表17.1](#listing-17-1)：VAE编码器网络'
- en: 'Its summary looks like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它的总结看起来是这样的：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next is the code for using `z_mean` and `z_log_var`, the parameters of the statistical
    distribution assumed to have produced `input_img`, to generate a latent space
    point `z`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是使用`z_mean`和`z_log_var`，即假设产生`input_img`的统计分布的参数，来生成一个潜在空间点`z`的代码。
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 17.2](#listing-17-2): Latent space sampling layer'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表17.2](#listing-17-2)：潜在空间采样层'
- en: The following listing shows the decoder implementation. We reshape the vector
    `z` to the dimensions of an image and then use a few convolution layers to obtain
    a final image output that has the same dimensions as the original `input_img`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了解码器的实现。我们将向量 `z` 调整为图像的维度，然后使用几个卷积层来获得最终图像输出，其维度与原始 `input_img` 相同。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 17.3](#listing-17-3): VAE decoder network, mapping latent space points
    to images'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 17.3](#listing-17-3)：VAE 解码器网络，将潜在空间点映射到图像'
- en: 'Its summary looks like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其摘要如下：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, let’s create the VAE model itself. This is your first example of a model
    that isn’t doing supervised learning (an autencoder is an example of *self-supervised*
    learning because it uses its inputs as targets). Whenever you depart from classic
    supervised learning, it’s common to subclass the `Model` class and implement a
    custom `train_step()` to specify the new training logic, a workflow you’ve learned
    about in chapter 7\. We could easily do that here, but a downside of this technique
    is that the `train_step()` contents must be backend specific — you’d use `GradientTape`
    with TensorFlow, you’d use `loss.backward()` with PyTorch, and so on. A simpler
    way to customize your training logic is to just implement the `compute_loss()`
    method instead and keep the default `train_step()`. `compute_loss()` is the key
    bit of differentiable logic called by the built-in `train_step()`. Since it doesn’t
    involve direct manipulation of gradients, it’s easy to keep it backend agnostic.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建 VAE 模型本身。这是您第一个不是进行监督学习的模型示例（自编码器是 *自监督* 学习的一个例子，因为它使用其输入作为目标）。每当您偏离经典监督学习时，通常都会子类化
    `Model` 类并实现自定义的 `train_step()` 来指定新的训练逻辑，这是您在第 7 章中学到的。我们在这里可以轻松做到这一点，但这种方法的一个缺点是
    `train_step()` 的内容必须是后端特定的——您会使用 TensorFlow 的 `GradientTape`，您会使用 PyTorch 的 `loss.backward()`，等等。自定义训练逻辑的一个更简单的方法是仅实现
    `compute_loss()` 方法，并保留默认的 `train_step()`。`compute_loss()` 是内置 `train_step()`
    调用的关键可微分逻辑。由于它不涉及直接操作梯度，因此很容易保持其与后端无关。
- en: 'Its signature is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其签名如下：
- en: '`compute_loss(x, y, y_pred, sample_weight=None, training=True)`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_loss(x, y, y_pred, sample_weight=None, training=True)`'
- en: where `x` is the model’s input; `y` is the model’s target (in our case, it is
    `None` since the dataset we use only has inputs, no targets); and `y_pred` is
    the output of `call()` — the model’s predictions. In any supervised training workflow,
    you’d compute the loss based on `y` and `y_pred`. In our case, since `y` is `None`
    and `y_pred` contains the latent parameters, we’ll compute the loss using `x`
    (the original input) and the `reconstruction` derived from `y_pred`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `x` 是模型的输入；`y` 是模型的输出（在我们的案例中，它是 `None`，因为我们使用的数据集只有输入，没有目标）；而 `y_pred` 是
    `call()` 的输出——即模型的预测。在任何监督训练流程中，您都会基于 `y` 和 `y_pred` 计算损失。在我们的案例中，由于 `y` 是 `None`
    且 `y_pred` 包含潜在参数，我们将使用 `x`（原始输入）和从 `y_pred` 导出的 `reconstruction` 来计算损失。
- en: The method must return a scalar, the loss value to be minimized. You can also
    use `compute_loss()` to update the state of your metrics, which is something we’ll
    want to do in our case.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法必须返回一个标量，即要最小化的损失值。您还可以使用 `compute_loss()` 来更新您的指标状态，这是我们在这个案例中想要做的。
- en: Now, let’s write our VAE with a custom `compute_loss()` method. It works with
    all backends with no code changes!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用自定义的 `compute_loss()` 方法来编写我们的 VAE。它适用于所有后端，无需更改代码！
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 17.4](#listing-17-4): VAE model with custom `compute_loss()` method'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 17.4](#listing-17-4)：具有自定义 `compute_loss()` 方法的 VAE 模型'
- en: Finally, you’re ready to instantiate and train the model on MNIST digits. Because
    `compute_loss()` already takes care of the loss, you don’t specify an external
    loss at compile time (`loss=None`), which, in turn, means you won’t pass target
    data during training (as you can see, you only pass `x_train` to the model in
    `fit`).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您已经准备好实例化模型并在 MNIST 数字上对其进行训练。因为 `compute_loss()` 已经处理了损失，所以在编译时不需要指定外部损失（`loss=None`），这反过来意味着您在训练期间不会传递目标数据（如您所见，在
    `fit` 中您只向模型传递 `x_train`）。
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 17.5](#listing-17-5): Training the VAE'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 17.5](#listing-17-5)：训练 VAE'
- en: Once the model is trained, you can use the `decoder` network to turn arbitrary
    latent space vectors into images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，您就可以使用 `decoder` 网络将任意的潜在空间向量转换为图像。
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 17.6](#listing-17-6): Sampling a grid of points from the 2D latent
    space and decoding them to images'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 17.6](#listing-17-6)：从 2D 潜在空间中采样点网格并将它们解码为图像'
- en: 'The grid of sampled digits (see figure 17.5) shows a completely continuous
    distribution of the different digit classes, with one digit morphing into another
    as you follow a path through latent space. Specific directions in this space have
    a meaning: for example, there’s a direction for “four-ness,” “one-ness,” and so
    on.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 样本数字的网格（见图17.5）显示了不同数字类别的完全连续分布，当你沿着潜在空间中的路径移动时，一个数字会逐渐变成另一个数字。这个空间中的特定方向具有意义：例如，有一个“四”的方向，“一”的方向，等等。
- en: '![](../Images/f4749282cfa2ae236719506aa783ba13.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/f4749282cfa2ae236719506aa783ba13.png)'
- en: '[Figure 17.5](#figure-17-5): Grid of digits decoded from the latent space'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.5](#figure-17-5)：从潜在空间解码的数字网格'
- en: 'In the next section, we’ll cover in detail another major tool for generating
    images: diffusion models, the architecture behind nearly all commercial image
    generation services today.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细介绍生成图像的另一项主要工具：扩散模型，这是今天几乎所有商业图像生成服务背后的架构。
- en: Diffusion models
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩散模型
- en: 'A long-standing application of autoencoders has been *denoising*: feeding into
    a model an input that features a small amount of noise — for instance, a low-quality
    JPEG image — and getting back a cleaned-up version of the same input. This is
    the one task that autoencoders excel at. In the late 2010s, this idea gave rise
    to very successful *image super-resolution* models, capable of taking in low-resolution,
    potentially noisy images and outputting high-quality, high-resolution versions
    of them (see figure 17.6). Such models have been shipped as part of every major
    smartphone camera app for the past few years.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的一个长期应用是*去噪*：将包含少量噪声的输入（例如，低质量的JPEG图像）输入到模型中，并得到相同输入的清理版本。这是自动编码器擅长的一项任务。在2010年代后期，这个想法催生了非常成功的*图像超分辨率*模型，能够接受低分辨率、可能包含噪声的图像，并输出高质量、高分辨率的版本（见图17.6）。这些模型在过去几年中已成为每个主要智能手机相机应用的一部分。
- en: '![](../Images/b91235f55349752d57dc992c17fcfe23.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/b91235f55349752d57dc992c17fcfe23.png)'
- en: '[Figure 17.6](#figure-17-6): Image super-resolution'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.6](#figure-17-6)：图像超分辨率'
- en: Of course, these models aren’t magically recovering lost details hidden in the
    input, like in the “enhance” scene from *Blade Runner* (1982). Rather, they’re
    making educated guesses about what the image should look like — they’re *hallucinating*
    a cleaned-up, higher-resolution version of what you give them. This can potentially
    lead to funny mishaps. For instance, with some AI-enhanced cameras, you can take
    a picture of something that looks vaguely moon-like (such as a printout of a severely
    blurred moon image), and you will get in your camera roll a crisp picture of the
    moon’s craters. A lot of detail that simply wasn’t present in the printout gets
    straight-up hallucinated by the camera, because the super-resolution model it
    uses is overfitted to moon photography images. So, unlike Rick Deckard, definitely
    don’t use this technique for forensics!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些模型并不是像《银翼杀手》（1982年）中的“增强”场景那样神奇地恢复输入中隐藏的丢失细节。相反，它们是在对图像应该看起来像什么做出有根据的猜测——它们正在*幻觉*出一个清理过的、更高分辨率的版本。这可能会导致一些有趣的意外。例如，使用一些AI增强相机，你可以拍摄一个看起来有点像月亮的东西（例如严重模糊的月亮图像的打印件），你会在你的相机胶卷中得到一张清晰的月亮陨石坑的图片。许多在打印件中根本不存在的细节被相机直接幻觉出来，因为使用的超分辨率模型过度拟合了月亮摄影图像。所以，绝对不要像Rick
    Deckard那样使用这项技术进行法医鉴定！
- en: 'Early successes in image denoising led researchers to an arresting idea: since
    you can use an autoencoder to remove a small amount of noise from an image, surely
    it would be possible to repeat the process multiple times in a loop to remove
    a large amount of noise. Ultimately, could you denoise an image made out of *pure
    noise*?'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图像去噪的早期成功使研究人员产生了令人震惊的想法：既然你可以使用自动编码器从图像中去除少量噪声，那么重复这个过程多次，循环去除大量噪声当然也是可能的。最终，你能去除由*纯噪声*组成的图像的噪声吗？
- en: As it turns out, yes, you can. By doing this, you can effectively hallucinate
    brand new images out of nothing, like in figure 17.7. This is the key insight
    behind diffusion models, which should more accurately be called *reverse diffusion*
    models, since “diffusion” refers to the process of gradually adding noise to an
    image until it disperses into nothing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，你可以做到这一点。通过这样做，你可以有效地从无中生有地创造出全新的图像，就像图17.7所示的那样。这是扩散模型背后的关键洞察，这些模型更准确地应该被称为*逆向扩散*模型，因为“扩散”指的是逐渐向图像添加噪声直到其消散成无的过程。
- en: '![](../Images/773711fe867a7d43244aa21bbc8e3dff.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/773711fe867a7d43244aa21bbc8e3dff.png)'
- en: '[Figure 17.7](#figure-17-7): Reverse diffusion: turning pure noise into an
    image via repeated denoising'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.7](#figure-17-7)：反向扩散：通过重复去噪将纯噪声转换为图像'
- en: A diffusion model is essentially a denoising autoencoder in a loop, capable
    of turning pure noise into sharp, realistic imagery. You may know this poetic
    quote from Michelangelo, “Every block of stone has a statue inside it and it is
    the task of the sculptor to discover it” — well, every square of white noise has
    an image inside it, and it is the task of the diffusion model to discover it.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型本质上是一个循环中的去噪自动编码器，能够将纯噪声转换为清晰、逼真的图像。你可能知道米开朗基罗的这句诗意名言：“每一块石头里都有一尊雕像，雕塑家的任务就是发现它”——同样，每一块白色噪声中都有一个图像，扩散模型的任务就是发现它。
- en: Now, let’s build one with Keras.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用Keras构建一个模型。
- en: The Oxford Flowers dataset
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 牛津花卉数据集
- en: The dataset we’re going to use is the Oxford Flowers dataset ([https://www.robots.ox.ac.uk/~vgg/data/flowers/102/](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/)),
    a collection of 8,189 images of flowers that belong to 102 different species.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的数据集是牛津花卉数据集（[https://www.robots.ox.ac.uk/~vgg/data/flowers/102/](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/))，这是一个包含102个不同物种的8,189张花卉图片的集合。
- en: 'Let’s get the dataset archive and extract it:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取数据集存档并提取它：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`fpath` is now the local path to the extracted directory. The images are contained
    in the `jpg` subdirectory there. Let’s turn them into an iterable dataset using
    `image_dataset_from_directory()`.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`fpath`现在是提取目录的本地路径。图像包含在该目录下的`jpg`子目录中。让我们使用`image_dataset_from_directory()`将它们转换为可迭代的数据集。'
- en: 'We need to resize our images to a fixed size, but we don’t want to distort
    their aspect ratio since this would negatively affect the quality of our generated
    images, so we use the `crop_to_aspect_ratio` option to extract maximally large
    undistorted crops of the right size (128 × 128):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将图像调整到固定大小，但不想扭曲它们的纵横比，因为这会负面影响我们生成图像的质量，所以我们使用`crop_to_aspect_ratio`选项来提取最大尺寸且未扭曲的裁剪图像，尺寸为（128
    × 128）：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here’s an example image (figure 17.8):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例图像（图17.8）：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/310b8269e172e1a1f277640ea66bfc9c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/310b8269e172e1a1f277640ea66bfc9c.png)'
- en: '[Figure 17.8](#figure-17-8): An example image from the Oxford Flowers dataset'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.8](#figure-17-8)：牛津花卉数据集的一个示例图像'
- en: A U-Net denoising autoencoder
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个U-Net去噪自动编码器
- en: The same denoising model gets reused across each iteration of the diffusion
    denoising process, erasing a little bit of noise each time. To make the job of
    the model easier, we tell it how much noise it is supposed to extract for a given
    input image — that’s the `noise_rates` input. Rather than outputting a denoised
    image, we make our model output a predicted noise mask, which we can subtract
    from the input to denoise it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的去噪模型在扩散去噪过程的每次迭代中都会被重复使用，每次消除一点噪声。为了使模型的工作更容易，我们告诉它对于给定的输入图像应该提取多少噪声——这就是`noise_rates`输入。我们让模型输出一个预测的噪声掩码，我们可以从输入中减去它来实现去噪。
- en: For our denoising model, we’re going to use a U-Net — a kind of ConvNet originally
    developed for image segmentation. It looks like figure 17.9.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的去噪模型，我们将使用U-Net——一种最初为图像分割而开发的卷积神经网络。它看起来像图17.9。
- en: '![](../Images/7b15c50bb2c1e461998862527e14c74d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7b15c50bb2c1e461998862527e14c74d.png)'
- en: '[Figure 17.9](#figure-17-9): Our U-Net-style denoising autoencoder architecture'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.9](#figure-17-9)：我们的U-Net风格的去噪自动编码器架构'
- en: 'This architecture features three stages:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构有三个阶段：
- en: A *downsampling stage*, made of several blocks of convolution layers, where
    the inputs get downsampled from their original 128 × 128 size down to a much smaller
    size (in our case, 16 × 16).
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个*下采样阶段*，由几个卷积层块组成，其中输入从原始的128 × 128大小下采样到一个更小的尺寸（在我们的例子中，是16 × 16）。
- en: A *middle stage*, where the feature map has a constant size.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个*中间阶段*，其中特征图具有恒定的尺寸。
- en: An *upsampling stage*, where the feature map get upsampled back to 128 × 128.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个*上采样阶段*，其中特征图被上采样回128 × 128。
- en: 'There is a 1:1 mapping between the blocks of the downsampling and upsampling
    stages: each upsampling block is the inverse of a downsampling block. Importantly,
    the model features concatenative residual connections going from each downsampling
    block to the corresponding upsampling block. These connections help avoid loss
    of image detail information across the successive downsampling and upsampling
    operations.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样阶段和上采样阶段的块之间存在 1:1 的映射：每个上采样块是下采样块的逆。重要的是，该模型具有从每个下采样块到相应上采样块的连接性，这些连接性有助于避免在连续的下采样和上采样操作中丢失图像细节信息。
- en: 'Let’s assemble the model using the Functional API:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Functional API 来组装模型：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You would instantiate the model with something like `get_model(image_size=128,
    widths=[32, 64, 96, 128], block_depth=2)`. The `widths` argument is a list containing
    the `Conv2D` layer sizes for each successive downsampling or upsampling stage.
    We typically want the layers to get bigger as we downsample the inputs (going
    from 32 to 128 units here) and then get smaller as as upsample (from 128 back
    to 32 here).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用类似 `get_model(image_size=128, widths=[32, 64, 96, 128], block_depth=2)`
    的方式实例化模型。`widths` 参数是一个列表，包含每个连续下采样或上采样阶段的 `Conv2D` 层大小。我们通常希望随着输入的下采样（从 32 到
    128 个单位）而层的大小增加，然后在上采样时减小（从 128 回到 32 个单位）。
- en: The concepts of diffusion time and diffusion schedule
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩散时间和扩散计划的概念
- en: The diffusion process is a series of steps in which we apply our denoising autoencoder
    to erase a small amount of noise from an image, starting with a pure-noise image,
    and ending with a pure-signal image. The index of the current step in the loop
    is called the *diffusion time* (see figure 17.7). In our case, we’ll use a continuous
    value between 1 and 0 for this index — a value of 1 indicates the start of the
    process, where the amount of noise is maximal and the amount of signal is minimal,
    and a value of 0 indicates the end of the process, where the image is almost all
    signal and no noise.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散过程是一系列步骤，其中我们将我们的去噪自动编码器应用于从图像中消除少量噪声，从纯噪声图像开始，以纯信号图像结束。循环中当前步骤的索引称为 *扩散时间*（见图
    17.7）。在我们的情况下，我们将使用介于 1 和 0 之间的连续值作为此索引的值——1 表示过程的开始，此时噪声量最大，信号量最小，而 0 表示过程的结束，此时图像几乎全部是信号而没有噪声。
- en: The relationship between the current diffusion time and the amount of noise
    and signal present in the image is called the *diffusion schedule*. In our experiment,
    we’re going to use a cosine schedule to smoothly transition from a high signal
    rate (low noise) at the beginning to a low signal rate (high noise) at the end
    of the diffusion process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当前扩散时间与图像中存在的噪声和信号量之间的关系称为 *扩散计划*。在我们的实验中，我们将使用余弦计划来平滑地从扩散过程开始的高信号率（低噪声）过渡到结束时的低信号率（高噪声）。
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 17.7](#listing-17-7): The diffusion schedule'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 17.7](#listing-17-7)：扩散计划'
- en: This `diffusion_schedule()` function takes as input a `diffusion_times` tensor,
    which represents the progression of the diffusion process and returns the corresponding
    `noise_rates` and `signal_rates` tensors. These rates will be used to guide the
    denoising process. The logic behind using a cosine schedule is to maintain the
    relationship `noise_rates ** 2 + signal_rates ** 2 == 1` (see figure 17.10).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `diffusion_schedule()` 函数接受一个 `diffusion_times` 张量作为输入，它表示扩散过程的进展，并返回相应的
    `noise_rates` 和 `signal_rates` 张量。这些比率将被用来指导去噪过程。使用余弦计划的逻辑是保持关系 `noise_rates **
    2 + signal_rates ** 2 == 1`（见图 17.10）。
- en: '![](../Images/a696630163a531e9ca732856013c0290.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a696630163a531e9ca732856013c0290.png)'
- en: '[Figure 17.10](#figure-17-10): Cosine relationship between noise rates and
    signal rates'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17.10](#figure-17-10)：噪声率和信号率之间的余弦关系'
- en: 'Let’s plot how this function maps diffusion times (between 0 and 1) to specific
    noise rates and signal rates (see figure 17.11):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这个函数如何将扩散时间（介于 0 和 1 之间）映射到特定的噪声率和信号率（见图 17.11）：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/caa853e9765e1abf244f1f70f18b90cc.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/caa853e9765e1abf244f1f70f18b90cc.png)'
- en: '[Figure 17.11](#figure-17-11): Our cosine diffusion schedule'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17.11](#figure-17-11)：我们的余弦扩散计划'
- en: The training process
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练过程
- en: 'Let’s create a `DiffusionModel` class to implement the training procedure.
    It’s going to have our denoising autoencoder as one of its attributes. We’re also
    going to need a couple more things:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个 `DiffusionModel` 类来实现训练过程。它将包含我们的去噪自动编码器作为其属性之一。我们还需要一些其他的东西：
- en: '*A loss function* — We’ll use mean absolute error as our loss, that is to say
    `mean(abs(real_noise_mask - predicted_noise_mask))`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数* — 我们将使用平均绝对误差作为我们的损失，也就是说 `mean(abs(real_noise_mask - predicted_noise_mask))`。'
- en: '*An image normalization layer* — The noise we’ll add to the images will have
    unit variance and zero mean, so we’d like our images to be normalized as such
    too, for the value range of the noise to match the value range of the images.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像归一化层* — 我们将添加到图像中的噪声将具有单位方差和零均值，因此我们希望我们的图像也以这种方式归一化，以便噪声的值范围与图像的值范围相匹配。'
- en: 'Let’s start by writing the model constructor:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先编写模型构造函数：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first method we’re going to need is the denoising method. It simply calls
    the denoising model to retrieve a predicted noise mask, and it uses it to reconstruct
    a denoised image:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要的方法是去噪方法。它简单地调用去噪模型以检索一个预测的噪声掩码，并使用它来重建一个去噪图像：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next comes the training logic. This is the most important part! Like in the
    VAE example, we’re going to implement a custom `compute_loss()` method to keep
    our model backend agnostic. Of course, if you are set on using one specific backend,
    you could also write a custom `train_step()` with the exact same logic in it,
    plus the backend-specific logic for gradient computation and weight updates.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练逻辑。这是最重要的部分！就像在VAE示例中一样，我们将实现一个自定义的`compute_loss()`方法来保持我们的模型后端无关。当然，如果你坚持使用一个特定的后端，你也可以编写一个带有相同逻辑的自定义`train_step()`，以及后端特定的梯度计算和权重更新逻辑。
- en: 'Since `compute_loss()` receives as input the output of `call()`, we’re going
    to put the denoising forward pass in `call()`. Our `call()` takes a batch of clean
    input images and applies the following steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`compute_loss()`接收`call()`的输出作为输入，我们将去噪前向传递放在`call()`中。我们的`call()`接受一批干净的输入图像，并执行以下步骤：
- en: Normalizes the images
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行归一化
- en: Samples random diffusion times (the denoising model needs to be trained on the
    full spectrum of diffusion times)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本随机的扩散时间（去噪模型需要在扩散时间的全谱上训练）
- en: Computes corresponding noise rates and signal rates (using the diffusion schedule)
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相应的噪声率和信号率（使用扩散计划）
- en: Adds random noise to the clean images (based on the computed noise rates and
    signal rates)
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在干净的图像上添加随机噪声（基于计算出的噪声率和信号率）
- en: Denoises the images
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行去噪
- en: It returns
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回
- en: The predicted denoised images
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的去噪图像
- en: The predicted noise masks
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的噪声掩码
- en: The actual noise masks it applied
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际应用的噪声掩码
- en: 'These last two quantities are then used in `compute_loss()` to compute the
    loss of the model on the noise mask prediction task:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个量随后在`compute_loss()`中使用，以计算模型在噪声掩码预测任务上的损失：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The generation process
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成过程
- en: Finally, let’s implement the image generation process. We start from pure random
    noise, and we repeatedly apply the `denoise()` method until we get high-signal,
    low-noise images.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实现图像生成过程。我们从纯随机噪声开始，并反复应用`denoise()`方法，直到我们得到高信号、低噪声的图像。
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Visualizing results with a custom callback
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义回调可视化结果
- en: 'We don’t have a proper metric to judge the quality of our generated images,
    so you’re going to want to visualize the generated images yourself over the course
    of training to judge if your model is getting somewhere. An easy way to do this
    is with a custom callback. The following callback uses the `generate()` method
    at the end of each epoch to display a 3 × 6 grid of generated images:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有合适的度量标准来评判我们生成的图像质量，所以你将需要在训练过程中自己可视化生成的图像来判断你的模型是否有所进展。一个简单的方法是使用自定义回调。以下回调在每一个epoch结束时使用`generate()`方法来显示一个3
    × 6的生成图像网格：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It’s go time!
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 是时候开始了！
- en: 'It’s finally time to train our diffusion model on the Oxford Flowers dataset.
    Let’s instantiate the model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 终于到了在牛津花卉数据集上训练我们的扩散模型的时候了。让我们实例化模型：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We’re going to use `AdamW` as our optimizer, with a few neat options enabled
    to help stabilize training and improve the quality of the generated images:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`AdamW`作为我们的优化器，并启用一些实用的选项来帮助稳定训练并提高生成图像的质量：
- en: '*Learning rate decay* — We gradually reduce the learning rate during training,
    via an `InverseTimeDecay` schedule.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习率衰减* — 我们在训练过程中通过`InverseTimeDecay`计划逐渐降低学习率。'
- en: '*Exponential moving average of model weights* — Also known as Polyak averaging.
    This technique maintains a running average of the model’s weights during training.
    Every 100 batches, we overwrite the model’s weights with this averaged set of
    weights. This helps stabilize the model’s representations in scenarios where the
    loss landscape is noisy.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型权重的指数移动平均*——也称为Polyak平均。这种技术在整个训练过程中维护模型权重的运行平均值。每100个批次，我们用这个平均权重量覆模型的权重。这有助于在损失景观嘈杂的情况下稳定模型的表示。'
- en: The code is
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s fit the model. We’ll use our `VisualizationCallback` callback to plot
    examples of generated images after each epoch, and we’ll save the model’s weights
    with the `ModelCheckpoint` callback:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合模型。我们将使用`VisualizationCallback`回调在每个epoch后绘制生成图像的示例，并使用`ModelCheckpoint`回调保存模型的权重：
- en: '[PRE22]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you’re running on Colab, you might run into the error, “Buffered data was
    truncated after reaching the output size limit.” This happens because the logs
    of `fit()` include images, which take up a lot of space, whereas the allowed output
    for a single notebook cell is limited. To get around the problem, you can simply
    chain five `model.fit(..., epochs=20)` calls, in five successive cells. This is
    equivalent to a single `fit(..., epochs=100)` call.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在Colab上运行，你可能会遇到错误：“缓冲数据在达到输出大小限制后截断。” 这是因为`fit()`的日志包括图像，它们占用了大量空间，而单个笔记本单元允许的输出是有限的。为了解决这个问题，你可以在五个连续的单元中简单地链式调用五个`model.fit(...,
    epochs=20)`，这相当于一个单独的`fit(..., epochs=100)`调用。
- en: After 100 epochs (which takes about 90 minutes on a T4, the free Colab GPU),
    we get pretty generative flowers like these (see figure 17.12).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 经过100个epoch（在T4上大约需要90分钟，免费的Colab GPU），我们得到了像这样的相当生成性的花朵（见图17.12）。
- en: '![](../Images/c291aa0a0eb3da4b07c4f02d9212c102.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c291aa0a0eb3da4b07c4f02d9212c102.png)'
- en: '[Figure 17.12](#figure-17-12): Examples of generated flowers'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.12](#figure-17-12)：生成的花朵示例'
- en: You can keep training for even longer and get increasingly realistic results.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续训练更长的时间，并得到越来越逼真的结果。
- en: So that’s how image generation with diffusion works! Now, the next step to unlock
    their potential is to add *text conditioning*, which would result in a text-to-image
    model, capable of producing images that match a given text caption.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是扩散图像生成的工作原理！现在，下一步是解锁它们的潜力，添加*文本条件*，这将导致一个文本到图像模型，能够生成与给定文本标题匹配的图像。
- en: Text-to-image models
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到图像模型
- en: We can use the same basic diffusion process to create a model that maps text
    input to image output. To do this we need to take a pretrained text encoder (think
    a transformer encoder like RoBERTa from chapter 15) that can map text to vectors
    in a continuous embedding space. Then we can train a diffusion model on `(prompt,
    image)` pairs, where each prompt is a short, textual description of the input
    image.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的基本扩散过程来创建一个将文本输入映射到图像输出的模型。为此，我们需要一个预训练的文本编码器（例如第15章中的RoBERTa这样的transformer编码器），它可以映射文本到连续嵌入空间中的向量。然后我们可以在`(prompt,
    image)`对上训练扩散模型，其中每个提示是输入图像的简短文本描述。
- en: 'We can handle the image input in the same way as we did previously, mapping
    noisy input to a denoised output that progressively approaches our input image.
    Critically, we can extend this setup by also passing the embedded text prompt
    to the denoising model. So rather than our denoising model simply taking in a
    `noisy_images` input, our model will take two inputs: `noisy_images` and `text_embeddings`.
    This gives a leg up on the flower denoiser we trained previously. Instead of learning
    to remove noise from an image without any additional information, the model gets
    to use a textual representation of the final image to help guide the denoising
    process.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以前同样的方式处理图像输入，将噪声输入映射到去噪输出，逐渐接近我们的输入图像。关键的是，我们可以通过也将嵌入的文本提示传递给去噪模型来扩展这个设置。因此，我们的去噪模型不仅接受`noisy_images`输入，还将接受两个输入：`noisy_images`和`text_embeddings`。这使我们训练的先前花朵去噪器有了优势。模型不是在没有额外信息的情况下学习从图像中去除噪声，而是可以使用最终图像的文本表示来帮助指导去噪过程。
- en: After training is when things get a bit more fun. Because we have trained a
    model that can map pure noise to images *conditioned on* a vector representation
    of some text, we can now pass in pure noise and a never-before-seen prompt and
    denoise it into an image for our prompt.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，事情会变得更有趣。因为我们已经训练了一个可以将纯噪声映射到某些文本的向量表示的图像的条件上的模型，现在我们可以输入纯噪声和从未见过的提示，并将其去噪成图像。
- en: 'Let’s try this out. We won’t actually train one of these models from scratch
    in this book — you have all the ingredients you need, but it’s quite expensive
    and time consuming to train a text-to-image diffusion model that works well. Instead,
    we will play with a popular pretrained model in KerasHub called Stable Diffusion
    (figure 17.13). Stable Diffusion is made by a company named Stability AI that
    specializes in making open models for image and video generation. We can use the
    third version of their image generation model in KerasHub with just a couple of
    lines of code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试这个。在这本书中，我们实际上不会从头开始训练这样的模型——你已经有了所有需要的成分，但训练一个效果良好的文本到图像扩散模型既昂贵又耗时。相反，我们将使用
    KerasHub 中一个流行的预训练模型 Stable Diffusion（图 17.13）。Stable Diffusion 是由一家名为 Stability
    AI 的公司制作的，该公司专门制作图像和视频生成的开源模型。我们只需几行代码就可以在 KerasHub 中使用他们图像生成模型的第三个版本：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Listing 17.8](#listing-17-8): Creating a Stable Diffusion text-to-image model'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 17.8](#listing-17-8)：创建一个稳定扩散文本到图像模型'
- en: '![](../Images/f8ec280f2a2008a60812cf66aa7a89c8.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8ec280f2a2008a60812cf66aa7a89c8.png)'
- en: '[Figure 17.13](#figure-17-13): An example output from our Stable Diffusion
    model'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17.13](#figure-17-13)：我们的稳定扩散模型的一个示例输出'
- en: Like the `CausalLM` task we covered last chapter, the `TextToImage` task is
    a high-level class for performing image generation conditioned on text input.
    It wraps tokenization and the diffusion process into a high-level generate call.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章中介绍的 `CausalLM` 任务类似，`TextToImage` 任务是一个高级类，用于根据文本输入进行图像生成。它将标记化和扩散过程封装成一个高级的生成调用。
- en: 'The Stable Diffusion model actually adds a second “negative prompt” to its
    model, which can be used to steer the diffusion process away from certain text
    inputs. There’s nothing magic here. To add a negative prompt, you could simply
    train a model on triplets: `(image, positive_prompt, negative_prompt)`, where
    the positive prompt is a description of the image, and the negative prompt is
    a series of words that do not describe the image. By feeding the positive and
    negative text embedding to the denoiser, the denoiser will learn to steer the
    noise toward images that match the positive prompt and away from images that match
    the negative prompt (figure 17.14). Let’s try removing the color blue from our
    input:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散模型实际上为其模型添加了一个第二个“负提示”，可以用来将扩散过程引导远离某些文本输入。这里没有什么魔法。要添加一个负提示，你可以简单地训练一个三元组模型：`(image,
    positive_prompt, negative_prompt)`，其中正提示是对图像的描述，而负提示是一系列不描述图像的词语。通过将正负文本嵌入输入到去噪器中，去噪器将学习引导噪声向与正提示匹配的图像，并远离与负提示匹配的图像（图
    17.14）。让我们尝试从我们的输入中移除颜色蓝色：
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/fbd61fbad1abb2a0e1862fae44563a25.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbd61fbad1abb2a0e1862fae44563a25.png)'
- en: '[Figure 17.14](#figure-17-14): Using a negative prompt to steer the model away
    from the color blue'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17.14](#figure-17-14)：使用负提示将模型引导远离蓝色'
- en: 'Like the `generate()` method for text models we used in the last chapter, we
    have a few additional parameters we can pass to control the generation process.
    Let’s try passing a variable number of diffusion steps to our model to see the
    denoising process in action (figure 17.15):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章中使用的文本模型的 `generate()` 方法类似，我们有一些额外的参数可以传递给模型以控制生成过程。让我们尝试向我们的模型传递一个可变的扩散步骤数，以查看去噪过程（图
    17.15）的实际操作：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/aba6739b002e9fa379c845107a1d25a8.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba6739b002e9fa379c845107a1d25a8.png)'
- en: '[Figure 17.15](#figure-17-15): Controlling the number of diffusion steps'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17.15](#figure-17-15)：控制扩散步骤的数量'
- en: Exploring the latent space of a text-to-image model
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索文本到图像模型的潜在空间
- en: There is probably no better way to see the interpolative nature of deep neural
    networks than text diffusion models. The text encoder used by our model will learn
    a smooth, low-dimensional manifold to represent our input prompts. It’s continuous,
    meaning we have learned a space where we can walk from the text representation
    of one prompt to another, and each intermediate point will have semantic meaning.
    We can couple that with our diffusion process to morph between two images by simply
    describing each end state with a text prompt.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 没有比文本扩散模型更好地看到深度神经网络插值性质的方法了。我们模型使用的文本编码器将学习一个平滑、低维流形来表示我们的输入提示。它是连续的，这意味着我们学习了一个空间，我们可以从一个提示的文本表示走到另一个提示的文本表示，每个中间点都将具有语义意义。我们可以将这一点与我们的扩散过程相结合，通过简单地用文本提示描述每个端状态来在两个图像之间进行形态变化。
- en: Before we can do this, we will need to break up our high-level `generate()`
    function into its constituent parts. Let’s try that out.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够做到这一点之前，我们需要将我们的高级`generate()`函数分解成其组成部分。让我们试试看。
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 17.9](#listing-17-9): Breaking down the `generate()` function'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表17.9](#listing-17-9)：分解`generate()`函数'
- en: 'Our generation process has three distinct steps:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成过程有三个不同的步骤：
- en: First, we take our prompts, tokenize them, and embed them with our text encoder.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们获取提示，对它们进行分词，并使用我们的文本编码器将它们嵌入。
- en: Second, we take our text embeddings and pure noise and progressively “denoise”
    the noise into an image. This is the same as the flower model we just built.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步，我们获取文本嵌入和纯噪声，并逐步将噪声“去噪”成图像。这与我们刚刚构建的花朵模型相同。
- en: Lastly, we map our model outputs, which are from `[-1, 1]` back to `[0, 255]`
    so we can render the image.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将模型输出从`[-1, 1]`映射回`[0, 255]`，这样我们就可以渲染图像。
- en: 'One thing to note here is that our text embeddings actually contain four separate
    tensors:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点需要注意，我们的文本嵌入实际上包含四个不同的张量：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Rather than only passing the final, embedded text vector to the denoising model,
    the Stable Diffusion authors chose to pass both the final output vector and the
    last representation of the entire token sequence learned by the text encoder.
    This effectively gives our denoising model more information to work with. The
    authors do this for both the positive and negative prompts, so we have a total
    of four tensors here:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅将最终的嵌入文本向量传递给去噪模型不同，Stable Diffusion的作者选择传递最终的输出向量和文本编码器学习到的整个标记序列的最后表示。这实际上为我们去噪模型提供了更多的工作信息。作者对正向和负向提示都这样做，所以我们这里总共有四个张量：
- en: The positive prompt’s encoder sequence
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正向提示的编码序列
- en: The negative prompt’s encoder sequence
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负向提示的编码序列
- en: The positive prompt’s encoder vector
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正向提示的编码向量
- en: The negative prompt’s encoder vector
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负向提示的编码向量
- en: With our `generate()` function decomposed, we can now try walking the latent
    space between two text prompts. To do so, let’s build a function to interpolate
    between the text embeddings outputted by the model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的`generate()`函数分解后，我们现在可以尝试在两个文本提示之间的潜在空间中行走。为了做到这一点，让我们构建一个在模型输出的文本嵌入之间进行插值的函数。
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Listing 17.10](#listing-17-10): A function to interpolate text embeddings'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表17.10](#listing-17-10)：一个用于插值文本嵌入的函数'
- en: You’ll notice we use a special interpolation function called `slerp` to walk
    between our text embeddings. This is short for *spherical linear interpolation*
    — it’s a function that has been used in computer graphics for decades to interpolate
    points on a sphere.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们使用了一个特殊的插值函数，称为`slerp`，在文本嵌入之间行走。这代表*球面线性插值*——这是一个在计算机图形学中使用了数十年的函数，用于在球体上插值点。
- en: Don’t worry too much about the math; it’s not important for our example, but
    it is important to understand the motivation. If we imagine our text manifold
    as a sphere and our two prompts as random points on that sphere, directly linearly
    interpolating between these two points would land us inside the sphere. We would
    no longer be on its surface. We would like to stay on the surface of the smooth
    manifold learned by our text embedding — that’s where embedding points have meaning
    for our denoising model. See figure 17.16.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过于担心数学问题；对于我们这个例子来说，它并不重要，但理解动机是很重要的。如果我们想象我们的文本流形是一个球体，我们的两个提示是这个球体上的随机点，那么直接在这两点之间进行线性插值将会让我们落在球体内部。我们不再处于其表面。我们希望保持在由我们的文本嵌入学习到的光滑流形的表面上——这就是嵌入点对我们去噪模型有意义的所在。见图17.16。
- en: '![](../Images/75fa06a333d70c863b3fe4fa6dddbd00.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/75fa06a333d70c863b3fe4fa6dddbd00.png)'
- en: '[Figure 17.16](#figure-17-16): Spherical interpolation keeps us close to the
    surface of our manifold.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17.16](#figure-17-16)：球面插值使我们接近流形的表面。'
- en: Of course, the manifold learned by our text embedding model is not actually
    spherical. But it’s a smooth surface of numbers all with the same rough magnitude
    — it is *sphere-like*, and interpolating as if we were on a sphere is a better
    approximation than interpolating as if we were on a line.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们文本嵌入模型学习到的流形实际上并不是球形的。但它是一个具有相同粗糙大小的数字的平滑表面——它是*球形的*，如果我们假设我们处于一个球体上，那么通过球面插值比如果我们假设我们处于一条线上进行插值要更好。
- en: 'With our interpolation defined, let’s try walking between the text embeddings
    for two prompts and generating an image at each interpolated output. We will run
    our slerp function from 0.5 to 0.6 (out of 0 to 1) to zoom in on the middle of
    the interpolation right when the “morph” becomes visually obvious (figure 17.17):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了我们的插值后，让我们尝试在两个提示之间的文本嵌入之间行走，并在每个插值输出处生成一个图像。我们将从 0.5 运行到 0.6（在 0 到 1 之间），以便在“形态”在视觉上变得明显时放大插值的中间部分（图
    17.17）：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/91328f66e3e42c215b75d6b46066b2ec.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91328f66e3e42c215b75d6b46066b2ec.png)'
- en: '[Figure 17.17](#figure-17-17): Interpolating between two prompts and generating
    outputs'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17.17](#figure-17-17)：在两个提示之间进行插值并生成输出'
- en: This might feel like magic the first time you try it, but there’s nothing magic
    about it — interpolation is fundamental to the way deep neural networks learn.
    This will be the last substantive model we work with in the book, and it’s a great
    visual metaphor to end with. Deep neural networks are interpolation machines;
    they map complex, real-world probability distributions to low-dimensional manifolds.
    We can exploit this fact even for input as complex as human language and output
    as complex as natural images.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次尝试时，这可能会感觉像魔法，但其中并没有什么魔法——插值是深度神经网络学习方式的基础。这将是我们在书中最后要讨论的实质性模型，它是一个很好的视觉隐喻来结束。深度神经网络是插值机器；它们将复杂、现实世界的概率分布映射到低维流形上。我们可以利用这一事实，即使对于像人类语言这样复杂的输入和像自然图像这样复杂的输出也是如此。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Image generation with deep learning is done by learning latent spaces that
    capture statistical information about a dataset of images. By sampling and decoding
    points from the latent space, you can generate never-before-seen images. There
    are three major tools to do this: VAEs, diffusion models, and GANs.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习进行图像生成是通过学习捕获图像数据集统计信息的潜在空间来完成的。通过从潜在空间中采样和解码点，你可以生成从未见过的图像。为此有三个主要工具：VAEs、扩散模型和
    GANs。
- en: 'VAEs result in highly structured, continuous latent representations. For this
    reason, they work well for doing all sorts of image editing in latent space: face
    swapping, turning a frowning face into a smiling face, and so on. They also work
    nicely for doing latent space–based animations, such as animating a walk along
    a cross section of the latent space, showing a starting image slowly morphing
    into different images in a continuous way.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VAEs 导致高度结构化的、连续的潜在表示。因此，它们非常适合在潜在空间中进行各种图像编辑：人脸交换、将皱眉的脸变成微笑的脸等等。它们也适合进行基于潜在空间的动画，例如在潜在空间的横截面上进行行走动画，显示起始图像以连续的方式缓慢地变成不同的图像。
- en: Diffusion models result in very realistic outputs and are the dominant method
    of image generation today. They work by repeatedly denoising an image, starting
    from pure noise. They can easily be conditioned on text captions to create text-to-image
    models.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散模型产生非常逼真的输出，并且是目前图像生成的主要方法。它们通过从纯噪声开始反复去噪图像来工作。它们可以很容易地根据文本标题进行条件化，以创建文本到图像模型。
- en: Stable Diffusion 3 is a state-of-the-art pretrained text-to-image model that
    you can use to create highly realistic images of your own.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stable Diffusion 3 是一个最先进的预训练文本到图像模型，你可以用它来创建你自己的高度逼真的图像。
- en: The visual latent space learned by such text-to-image diffusion models is fundamentally
    interpolative. You can see this by interpolating between the text embeddings used
    as inputs to the diffusion process and achieving a smooth interpolation between
    images as output.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种文本到图像扩散模型学习的视觉潜在空间本质上是插值的。你可以通过在用于扩散过程的文本嵌入之间进行插值，并实现输出图像之间的平滑插值来看到这一点。
- en: Footnotes
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脚注
- en: Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes,” arXiv
    (2013), [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114). [[↩]](#footnote-link-1)
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Diederik P. Kingma 和 Max Welling, “Auto-Encoding Variational Bayes,” arXiv (2013),
    [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114). [[↩]](#footnote-link-1)
- en: Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra, “Stochastic Backpropagation
    and Approximate Inference in Deep Generative Models,” arXiv (2014), [https://arxiv.org/abs/1401.4082](https://arxiv.org/abs/1401.4082).
    [[↩]](#footnote-link-2)
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Danilo Jimenez Rezende, Shakir Mohamed, 和 Daan Wierstra, “Stochastic Backpropagation
    and Approximate Inference in Deep Generative Models,” arXiv (2014), [https://arxiv.org/abs/1401.4082](https://arxiv.org/abs/1401.4082).
    [[↩]](#footnote-link-2)
