- en: Chapter 7\. Training a State-of-the-Art Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。训练一个最先进的模型
- en: This chapter introduces more advanced techniques for training an image classification
    model and getting state-of-the-art results. You can skip it if you want to learn
    more about other applications of deep learning and come back to it later—knowledge
    of this material will not be assumed in later chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了更高级的技术，用于训练图像分类模型并获得最先进的结果。如果您想了解更多关于深度学习的其他应用，并稍后回来，您可以跳过它——后续章节不会假设您已掌握这些材料。
- en: We will look at what normalization is, a powerful data augmentation technique
    called Mixup, the progressive resizing approach, and test time augmentation. To
    show all of this, we are going to train a model from scratch (not using transfer
    learning) by using a subset of ImageNet called [Imagenette](https://oreil.ly/1uj3x).
    It contains a subset of 10 very different categories from the original ImageNet
    dataset, making for quicker training when we want to experiment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看一下什么是归一化，一种强大的数据增强技术叫做Mixup，渐进式调整大小方法，以及测试时间增强。为了展示所有这些，我们将从头开始训练一个模型（不使用迁移学习），使用一个名为Imagenette的ImageNet子集。它包含了原始ImageNet数据集中10个非常不同的类别的子集，使得在我们想要进行实验时训练更快。
- en: This is going to be much harder to do well than with our previous datasets because
    we’re using full-size, full-color images, which are photos of objects of different
    sizes, in different orientations, in different lighting, and so forth. So, in
    this chapter we’re going to introduce important techniques for getting the most
    out of your dataset, especially when you’re training from scratch, or using transfer
    learning to train a model on a very different kind of dataset than the pretrained
    model used.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这将比我们之前的数据集更难做得好，因为我们使用全尺寸、全彩色的图像，这些图像是不同大小、不同方向、不同光照等对象的照片。因此，在本章中，我们将介绍一些重要的技术，以便充分利用您的数据集，特别是当您从头开始训练，或者使用迁移学习在一个与预训练模型使用的非常不同类型的数据集上训练模型时。
- en: Imagenette
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Imagenette
- en: 'When fast.ai first started, people used three main datasets for building and
    testing computer vision models:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当fast.ai刚开始时，人们主要使用三个主要数据集来构建和测试计算机视觉模型：
- en: ImageNet
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet
- en: 1.3 million images of various sizes, around 500 pixels across, in 1,000 categories,
    which took a few days to train
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3百万张各种尺寸的图像，大约500像素宽，分为1,000个类别，需要几天时间来训练
- en: MNIST
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST
- en: 50,000 28×28-pixel grayscale handwritten digits
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 50,000个28×28像素的灰度手写数字
- en: CIFAR10
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR10
- en: 60,000 32×32-pixel color images in 10 classes
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 60,000个32×32像素的彩色图像，分为10类
- en: The problem was that the smaller datasets didn’t generalize effectively to the
    large ImageNet dataset. The approaches that worked well on ImageNet generally
    had to be developed and trained on ImageNet. This led to many people believing
    that only researchers with access to giant computing resources could effectively
    contribute to developing image classification algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于较小的数据集无法有效地泛化到大型ImageNet数据集。在ImageNet上表现良好的方法通常必须在ImageNet上开发和训练。这导致许多人认为，只有拥有巨大计算资源的研究人员才能有效地为发展图像分类算法做出贡献。
- en: We thought that seemed very unlikely to be true. We had never seen a study that
    showed that ImageNet happens to be exactly the right size, and that other datasets
    could not be developed that would provide useful insights. So we wanted to create
    a new dataset that researchers could test their algorithms on quickly and cheaply,
    but that would also provide insights likely to work on the full ImageNet dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为这似乎是不太可能成立的。我们从未见过一项研究表明ImageNet恰好是正确的大小，其他数据集无法提供有用的见解。因此，我们希望创建一个新的数据集，研究人员可以快速、廉价地测试他们的算法，但也能提供可能在完整的ImageNet数据集上起作用的见解。
- en: About three hours later, we had created Imagenette. We selected 10 classes from
    the full ImageNet that looked very different from one another. As we had hoped,
    we were able to quickly and cheaply create a classifier capable of recognizing
    these classes. We then tried out a few algorithmic tweaks to see how they impacted
    Imagenette. We found some that worked pretty well, and tested them on ImageNet
    as well—and we were pleased to find that our tweaks worked well on ImageNet too!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大约三个小时后，我们创建了Imagenette。我们从完整的ImageNet中选择了10个看起来非常不同的类别。正如我们所希望的那样，我们能够快速、廉价地创建一个能够识别这些类别的分类器。然后我们尝试了一些算法调整，看它们如何影响Imagenette。我们发现一些效果不错的，并在ImageNet上进行了测试，我们很高兴地发现我们的调整在ImageNet上也效果很好！
- en: 'There is an important message here: the dataset you are given is not necessarily
    the dataset you want. It’s particularly unlikely to be the dataset that you want
    to do your development and prototyping in. You should aim to have an iteration
    speed of no more than a couple of minutes—that is, when you come up with a new
    idea you want to try out, you should be able to train a model and see how it goes
    within a couple of minutes. If it’s taking longer to do an experiment, think about
    how you could cut down your dataset, or simplify your model, to improve your experimentation
    speed. The more experiments you can do, the better!'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个重要的信息：您得到的数据集不一定是您想要的数据集。特别是不太可能是您想要进行开发和原型设计的数据集。您应该力求迭代速度不超过几分钟——也就是说，当您想尝试一个新想法时，您应该能够在几分钟内训练一个模型并查看其效果。如果做一个实验花费的时间更长，考虑如何减少数据集的规模，或简化模型，以提高实验速度。您做的实验越多，效果就越好！
- en: 'Let’s get started with this dataset:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这个数据集开始：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First we’ll get our dataset into a `DataLoaders` object, using the *presizing*
    trick introduced in [Chapter 5](ch05.xhtml#chapter_pet_breeds):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用在第5章中介绍的*预调整*技巧将我们的数据集放入`DataLoaders`对象中：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then we’ll do a training run that will serve as a baseline:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行一个作为基线的训练运行：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.583403 | 2.064317 | 0.401792 | 01:03 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.583403 | 2.064317 | 0.401792 | 01:03 |'
- en: '| 1 | 1.208877 | 1.260106 | 0.601568 | 01:02 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.208877 | 1.260106 | 0.601568 | 01:02 |'
- en: '| 2 | 0.925265 | 1.036154 | 0.664302 | 01:03 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.730190 | 0.700906 | 0.777819 | 01:03 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.585707 | 0.541810 | 0.825243 | 01:03 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: That’s a good baseline, since we are not using a pretrained model, but we can
    do better. When working with models that are being trained from scratch, or fine-tuned
    to a very different dataset from the one used for the pretraining, some additional
    techniques are really important. In the rest of the chapter, we’ll consider some
    key approaches you’ll want to be familiar with. The first one is *normalizing*
    your data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training a model, it helps if your input data is *normalized*—that is,
    has a mean of 0 and a standard deviation of 1\. But most images and computer vision
    libraries use values between 0 and 255 for pixels, or between 0 and 1; in either
    case, your data is not going to have a mean of 0 and a standard deviation of 1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s grab a batch of our data and look at those values, by averaging over
    all axes except for the channel axis, which is axis 1:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we expected, the mean and standard deviation are not very close to the desired
    values. Fortunately, normalizing the data is easy to do in fastai by adding the
    `Normalize` transform. This acts on a whole mini-batch at once, so you can add
    it to the `batch_tfms` section of your data block. You need to pass to this transform
    the mean and standard deviation that you want to use; fastai comes with the standard
    ImageNet mean and standard deviation already defined. (If you do not pass any
    statistics to the `Normalize` transform, fastai will automatically calculate them
    from a single batch of your data.)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add this transform (using `imagenet_stats`, as Imagenette is a subset
    of ImageNet) and take a look at one batch now:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s check what effect this had on training our model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.632865 | 2.250024 | 0.391337 | 01:02 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.294041 | 1.579932 | 0.517177 | 01:02 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.960535 | 1.069164 | 0.657207 | 01:04 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.730220 | 0.767433 | 0.771845 | 01:05 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.577889 | 0.550673 | 0.824496 | 01:06 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: Although it helped only a little here, normalization becomes especially important
    when using pretrained models. The pretrained model knows how to work with only
    data of the type that it has seen before. If the average pixel value was 0 in
    the data it was trained with, but your data has 0 as the minimum possible value
    of a pixel, then the model is going to be seeing something very different from
    what is intended!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: This means that when you distribute a model, you need to also distribute the
    statistics used for normalization, since anyone using it for inference or transfer
    learning will need to use the same statistics. By the same token, if you’re using
    a model that someone else has trained, make sure you find out what normalization
    statistics they used, and match them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: We didn’t have to handle normalization in previous chapters because when using
    a pretrained model through `cnn_learner`, the fastai library automatically adds
    the proper `Normalize` transform; the model has been pretrained with certain statistics
    in `Normalize` (usually coming from the ImageNet dataset), so the library can
    fill those in for you. Note that this applies to only pretrained models, which
    is why we need to add this information manually here, when training from scratch.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: All our training up until now has been done at size 224\. We could have begun
    training at a smaller size before going to that. This is called *progressive resizing*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Progressive Resizing
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When fast.ai and its team of students [won the DAWNBench competition](https://oreil.ly/16tar)
    in 2018, one of the most important innovations was something very simple: start
    training using small images, and end training using large images. Spending most
    of the epochs training with small images helps training complete much faster.
    Completing training using large images makes the final accuracy much higher. We
    call this approach *progressive resizing*.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当fast.ai及其学生团队在2018年赢得DAWNBench比赛时，其中最重要的创新之一是非常简单的事情：使用小图像开始训练，然后使用大图像结束训练。在大部分时期使用小图像进行训练有助于训练完成得更快。使用大图像完成训练使最终准确率更高。我们称这种方法为*渐进式调整大小*。
- en: 'Jargon: Progressive Resizing'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：渐进式调整大小
- en: Gradually using larger and larger images as you train.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中逐渐使用越来越大的图像。
- en: As we have seen, the kinds of features that are learned by convolutional neural
    networks are not in any way specific to the size of the image—early layers find
    things like edges and gradients, and later layers may find things like noses and
    sunsets. So, when we change image size in the middle of training, it doesn’t mean
    that we have to find totally different parameters for our model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，卷积神经网络学习的特征类型与图像的大小无关——早期层发现边缘和梯度等内容，而后期层可能发现鼻子和日落等内容。因此，当我们在训练中途更改图像大小时，并不意味着我们必须为我们的模型找到完全不同的参数。
- en: But clearly there are some differences between small images and big ones, so
    we shouldn’t expect our model to continue working exactly as well, with no changes
    at all. Does this remind you of something? When we developed this idea, it reminded
    us of transfer learning! We are trying to get our model to learn to do something
    a little bit different from what it has learned to do before. Therefore, we should
    be able to use the `fine_tune` method after we resize our images.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但显然小图像和大图像之间存在一些差异，因此我们不应该期望我们的模型继续完全不变地工作得很好。这让你想起了什么吗？当我们开发这个想法时，它让我们想起了迁移学习！我们试图让我们的模型学会做一些与以前学会的有点不同的事情。因此，在调整图像大小后，我们应该能够使用`fine_tune`方法。
- en: 'Progressive resizing has an additional benefit: it is another form of data
    augmentation. Therefore, you should expect to see better generalization of your
    models that are trained with progressive resizing.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进式调整大小还有一个额外的好处：它是另一种数据增强形式。因此，您应该期望看到使用渐进式调整大小训练的模型具有更好的泛化能力。
- en: To implement progressive resizing, it is most convenient if you first create
    a `get_dls` function that takes an image size and a batch size, as we did in the
    previous section, and returns your `DataLoaders`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现渐进式调整大小，最方便的方法是首先创建一个`get_dls`函数，该函数接受图像大小和批量大小，就像我们在前一节中所做的那样，并返回您的`DataLoaders`。
- en: 'Now you can create your `DataLoaders` with a small size and use and `fit_one_cycle`
    in the usual way, training for fewer epochs than you might otherwise do:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用小尺寸创建您的`DataLoaders`，并以通常的方式使用`fit_one_cycle`，训练的时期比您可能以其他方式做的要少：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1.902943 | 2.447006 | 0.401419 | 00:30 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.902943 | 2.447006 | 0.401419 | 00:30 |'
- en: '| 1 | 1.315203 | 1.572992 | 0.525765 | 00:30 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.315203 | 1.572992 | 0.525765 | 00:30 |'
- en: '| 2 | 1.001199 | 0.767886 | 0.759149 | 00:30 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.001199 | 0.767886 | 0.759149 | 00:30 |'
- en: '| 3 | 0.765864 | 0.665562 | 0.797984 | 00:30 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.765864 | 0.665562 | 0.797984 | 00:30 |'
- en: 'Then you can replace the `DataLoaders` inside the `Learner`, and fine-tune:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以在`Learner`内部替换`DataLoaders`，并进行微调：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.985213 | 1.654063 | 0.565721 | 01:06 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.985213 | 1.654063 | 0.565721 | 01:06 |'
- en: '| epoch | train_loss | valid_loss | accuracy | time |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| epoch | train_loss | valid_loss | accuracy | time |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0.706869 | 0.689622 | 0.784541 | 01:07 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.706869 | 0.689622 | 0.784541 | 01:07 |'
- en: '| 1 | 0.739217 | 0.928541 | 0.712472 | 01:07 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.739217 | 0.928541 | 0.712472 | 01:07 |'
- en: '| 2 | 0.629462 | 0.788906 | 0.764003 | 01:07 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.629462 | 0.788906 | 0.764003 | 01:07 |'
- en: '| 3 | 0.491912 | 0.502622 | 0.836445 | 01:06 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.491912 | 0.502622 | 0.836445 | 01:06 |'
- en: '| 4 | 0.414880 | 0.431332 | 0.863331 | 01:06 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.414880 | 0.431332 | 0.863331 | 01:06 |'
- en: As you can see, we’re getting much better performance, and the initial training
    on small images was much faster on each epoch.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们的性能要好得多，而在每个时期的小图像上的初始训练速度要快得多。
- en: You can repeat the process of increasing size and training more epochs as many
    times as you like, for as big an image as you wish—but of course, you will not
    get any benefit by using an image size larger than the size of your images on
    disk.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据需要重复增加大小并训练更多时期的过程，为您希望的图像大小——但当然，如果使用大于磁盘上图像大小的图像大小，您将不会获得任何好处。
- en: Note that for transfer learning, progressive resizing may actually hurt performance.
    This is most likely to happen if your pretrained model was quite similar to your
    transfer learning task and the dataset and was trained on similar-sized images,
    so the weights don’t need to be changed much. In that case, training on smaller
    images may damage the pretrained weights.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于迁移学习，渐进式调整大小实际上可能会损害性能。如果您的预训练模型与您的迁移学习任务和数据集非常相似，并且是在类似大小的图像上训练的，那么权重不需要进行太多更改。在这种情况下，使用较小的图像进行训练可能会损坏预训练权重。
- en: On the other hand, if the transfer learning task is going to use images that
    are of different sizes, shapes, or styles than those used in the pretraining task,
    progressive resizing will probably help. As always, the answer to “Will it help?”
    is “Try it!”
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果迁移学习任务将使用与预训练任务中使用的图像大小、形状或风格不同的图像，渐进式调整大小可能会有所帮助。像往常一样，“它会有帮助吗？”的答案是“试试看！”
- en: Another thing we could try is applying data augmentation to the validation set.
    Up until now, we have applied it only on the training set; the validation set
    always gets the same images. But maybe we could try to make predictions for a
    few augmented versions of the validation set and average them. We’ll consider
    this approach next.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试将数据增强应用于验证集。到目前为止，我们只在训练集上应用了数据增强；验证集始终获得相同的图像。但也许我们可以尝试为验证集的几个增强版本进行预测并取平均值。我们将在下一步考虑这种方法。
- en: Test Time Augmentation
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试时间增强
- en: We have been using random cropping as a way to get some useful data augmentation,
    which leads to better generalization, and results in a need for less training
    data. When we use random cropping, fastai will automatically use center-cropping
    for the validation set—that is, it will select the largest square area it can
    in the center of the image, without going past the image’s edges.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用随机裁剪作为一种获取一些有用数据增强的方法，这导致更好的泛化，并且需要更少的训练数据。当我们使用随机裁剪时，fastai将自动为验证集使用中心裁剪——也就是说，它将选择图像中心的最大正方形区域，而不会超出图像的边缘。
- en: This can often be problematic. For instance, in a multi-label dataset, sometimes
    there are small objects toward the edges of an image; these could be entirely
    cropped out by center cropping. Even for problems such as our pet breed classification
    example, it’s possible that a critical feature necessary for identifying the correct
    breed, such as the color of the nose, could be cropped out.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常会带来问题。例如，在多标签数据集中，有时图像边缘会有小物体；这些物体可能会被中心裁剪完全裁剪掉。即使对于像我们的宠物品种分类示例这样的问题，也有可能关键特征，例如鼻子的颜色，可能会被裁剪掉。
- en: One solution to this problem is to avoid random cropping entirely. Instead,
    we could simply squish or stretch the rectangular images to fit into a square
    space. But then we miss out on a very useful data augmentation, and we also make
    the image recognition more difficult for our model, because it has to learn how
    to recognize squished and squeezed images, rather than just correctly proportioned
    images.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个方法是完全避免随机裁剪。相反，我们可以简单地压缩或拉伸矩形图像以适应正方形空间。但是这样我们会错过一个非常有用的数据增强，并且还会使图像识别对我们的模型更加困难，因为它必须学会识别被压缩和拉伸的图像，而不仅仅是正确比例的图像。
- en: Another solution is to not center crop for validation, but instead to select
    a number of areas to crop from the original rectangular image, pass each of them
    through our model, and take the maximum or average of the predictions. In fact,
    we could do this not just for different crops, but for different values across
    all of our test time augmentation parameters. This is known as *test time augmentation*
    (TTA).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决方案是在验证时不进行中心裁剪，而是从原始矩形图像中选择若干区域进行裁剪，将每个区域通过我们的模型，然后取预测的最大值或平均值。事实上，我们不仅可以对不同裁剪进行此操作，还可以对所有测试时间增强参数的不同值进行操作。这被称为*测试时间增强*（TTA）。
- en: 'Jargon: Test Time Augmentation (TTA)'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语：测试时间增强（TTA）
- en: During inference or validation, creating multiple versions of each image using
    data augmentation, and then taking the average or maximum of the predictions for
    each augmented version of the image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断或验证期间，使用数据增强创建每个图像的多个版本，然后取每个增强版本的预测的平均值或最大值。
- en: Depending on the dataset, test time augmentation can result in dramatic improvements
    in accuracy. It does not change the time required to train at all, but will increase
    the amount of time required for validation or inference by the number of test-time-augmented
    images requested. By default, fastai will use the unaugmented center crop image
    plus four randomly augmented images.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集的不同，测试时间增强可以显著提高准确性。它不会改变训练所需的时间，但会增加验证或推断所需的时间，数量取决于请求的测试时间增强图像数量。默认情况下，fastai将使用未增强的中心裁剪图像加上四个随机增强的图像。
- en: 'You can pass any `DataLoader` to fastai’s `tta` method; by default, it will
    use your validation set:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将任何`DataLoader`传递给fastai的`tta`方法；默认情况下，它将使用您的验证集：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, using TTA gives us good a boost in performance, with no additional
    training required. However, it does make inference slower—if you’re averaging
    five images for TTA, inference will be five times slower.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，使用TTA可以显著提高性能，而无需额外的训练。但是，它会使推断变慢——如果你对TTA平均了五张图像，推断将变慢五倍。
- en: We’ve seen a few examples of how data augmentation helps train better models.
    Let’s now focus on a new data augmentation technique called *Mixup*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一些数据增强如何帮助训练更好的模型。现在让我们专注于一种名为*混合*的新数据增强技术。
- en: Mixup
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合
- en: 'Mixup, introduced in the 2017 paper ["*mixup*: Beyond Empirical Risk Minimization”](https://oreil.ly/UvIkN)
    by Hongyi Zhang et al., is a powerful data augmentation technique that can provide
    dramatically higher accuracy, especially when you don’t have much data and don’t
    have a pretrained model that was trained on data similar to your dataset. The
    paper explains: “While data augmentation consistently leads to improved generalization,
    the procedure is dataset-dependent, and thus requires the use of expert knowledge.”
    For instance, it’s common to flip images as part of data augmentation, but should
    you flip only horizontally or also vertically? The answer is that it depends on
    your dataset. In addition, if flipping (for instance) doesn’t provide enough data
    augmentation for you, you can’t “flip more.” It’s helpful to have data augmentation
    techniques that “dial up” or “dial down” the amount of change, to see what works
    best for you.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 混合（Mixup）是在2017年张宏毅等人的论文《*混合：超越经验风险最小化*》中引入的一种强大的数据增强技术，可以提供极高的准确性，特别是当你没有太多数据，也没有经过预训练的模型，该模型是在与你的数据集相似的数据上训练的。该论文解释道：“虽然数据增强始终会导致改进的泛化，但该过程取决于数据集，并因此需要专业知识的使用。”例如，将图像翻转作为数据增强的一部分是很常见的，但是你应该只水平翻转还是同时垂直翻转呢？答案是取决于你的数据集。此外，如果（例如）翻转对你来说提供的数据增强不够，你不能“多翻转”。有助于拥有数据增强技术，可以“调高”或“调低”变化的程度，以找到最适合你的方法。
- en: 'Mixup works as follows, for each image:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个图像，Mixup的工作方式如下：
- en: Select another image from your dataset at random.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机从数据集中选择另一个图像。
- en: Pick a weight at random.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个权重。
- en: Take a weighted average (using the weight from step 2) of the selected image
    with your image; this will be your independent variable.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤2中的权重对所选图像和您的图像进行加权平均；这将是您的自变量。
- en: Take a weighted average (with the same weight) of this image’s labels with your
    image’s labels; this will be your dependent variable.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个图像的标签与您的图像的标签进行加权平均（使用相同的权重）；这将是您的因变量。
- en: 'In pseudocode, we’re doing this (where `t` is the weight for our weighted average):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在伪代码中，我们这样做（其中`t`是我们加权平均值的权重）：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For this to work, our targets need to be one-hot encoded. The paper describes
    this using the equations in [Figure 7-1](#mixup) (where <math alttext="lamda"><mi>λ</mi></math>
    is the same as `t` in our pseudocode).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其正常工作，我们的目标需要进行独热编码。该论文使用[图7-1](#mixup)中的方程式描述了这一点（其中<math alttext="lamda"><mi>λ</mi></math>与我们伪代码中的`t`相同）。
- en: '![An excerpt from the Mixup paper](Images/dlcf_0701.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![Mixup论文摘录](Images/dlcf_0701.png)'
- en: Figure 7-1\. An excerpt from the Mixup paper
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1。Mixup论文摘录
- en: '[Figure 7-2](#mixup_example) shows what it looks like when we take a *linear
    combination* of images, as done in Mixup.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-2](#mixup_example)展示了在Mixup中进行图像*线性组合*的样子。'
- en: '![An image of a church, a gas station and the two mixed up.](Images/dlcf_0702.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![一座教堂、一个加油站和两者混合的图像。](Images/dlcf_0702.png)'
- en: Figure 7-2\. Mixing a church and a gas station
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2。混合教堂和加油站
- en: 'The third image is built by adding 0.3 times the first one and 0.7 times the
    second. In this example, should the model predict “church” or “gas station”? The
    right answer is 30% church and 70% gas station, since that’s what we’ll get if
    we take the linear combination of the one-hot-encoded targets. For instance, suppose
    we have 10 classes, and “church” is represented by the index 2 and “gas station”
    by the index 7\. The one-hot-encoded representations are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个图像是通过将第一个图像的0.3倍和第二个图像的0.7倍相加而构建的。在这个例子中，模型应该预测“教堂”还是“加油站”？正确答案是30%的教堂和70%的加油站，因为如果我们采用独热编码目标的线性组合，那就是我们将得到的结果。例如，假设我们有10个类别，“教堂”由索引2表示，“加油站”由索引7表示。独热编码表示如下：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'So here is our final target:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的最终目标：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This all done for us inside fastai by adding a *callback* to our `Learner`.
    `Callback`s are what is used inside fastai to inject custom behavior in the training
    loop (like a learning rate schedule, or training in mixed precision). You’ll be
    learning all about callbacks, including how to make your own, in [Chapter 16](ch16.xhtml#chapter_accel_sgd).
    For now, all you need to know is that you use the `cbs` parameter to `Learner`
    to pass callbacks.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: fastai通过向我们的`Learner`添加一个*callback*来完成所有这些操作。`Callback`是fastai中用于在训练循环中注入自定义行为的内容（如学习率调度或混合精度训练）。您将在[第16章](ch16.xhtml#chapter_accel_sgd)中学习有关回调的所有内容，包括如何制作自己的回调。目前，您只需要知道使用`cbs`参数将回调传递给`Learner`。
- en: 'Here is how we train a model with Mixup:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何使用Mixup训练模型的方式：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: What happens when we train a model with data that’s “mixed up” in this way?
    Clearly, it’s going to be harder to train, because it’s harder to see what’s in
    each image. And the model has to predict two labels per image, rather than just
    one, as well as figuring out how much each one is weighted. Overfitting seems
    less likely to be a problem, however, because we’re not showing the same image
    in each epoch, but are instead showing a random combination of two images.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用这种方式“混合”的数据训练模型时会发生什么？显然，训练会更加困难，因为很难看清每个图像中的内容。模型必须为每个图像预测两个标签，而不仅仅是一个，并且还必须弄清楚每个标签的权重。然而，过拟合似乎不太可能成为问题，因为我们不会在每个时代中显示相同的图像，而是显示两个图像的随机组合。
- en: Mixup requires far more epochs to train to get better accuracy, compared to
    other augmentation approaches we’ve seen. You can try training Imagenette with
    and without Mixup by using the *examples/train_imagenette.py* script in the [fastai
    repo](https://oreil.ly/lrGXE). At the time of writing, the leaderboard in the
    [Imagenette repo](https://oreil.ly/3Gt56) is showing that Mixup is used for all
    leading results for trainings of >80 epochs, and for fewer epochs Mixup is not
    being used. This is in line with our experience of using Mixup too.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们看到的其他增强方法相比，Mixup需要更多的时代来训练以获得更好的准确性。您可以尝试使用[fastai repo](https://oreil.ly/lrGXE)中的*examples/train_imagenette.py*脚本来训练Imagenette，使用Mixup和不使用Mixup。在撰写本文时，[Imagenette
    repo](https://oreil.ly/3Gt56)中的排行榜显示，Mixup用于训练超过80个时代的所有领先结果，而对于更少的时代，不使用Mixup。这与我们使用Mixup的经验一致。
- en: One of the reasons that Mixup is so exciting is that it can be applied to types
    of data other than photos. In fact, some people have even shown good results by
    using Mixup on activations *inside* their models, not just on inputs—this allows
    Mixup to be used for NLP and other data types too.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Mixup如此令人兴奋的原因之一是它可以应用于除照片之外的数据类型。事实上，有些人甚至已经展示了通过在模型内部的激活上使用Mixup而获得良好结果，而不仅仅是在输入上使用Mixup——这使得Mixup也可以用于NLP和其他数据类型。
- en: There’s another subtle issue that Mixup deals with for us, which is that it’s
    not actually possible with the models we’ve seen before for our loss to ever be
    perfect. The problem is that our labels are 1s and 0s, but the outputs of softmax
    and sigmoid can never equal 1 or 0\. This means training our model pushes our
    activations ever closer to those values, such that the more epochs we do, the
    more extreme our activations become.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Mixup为我们处理的另一个微妙问题是，我们之前看到的模型实际上永远无法完美。问题在于我们的标签是1和0，但softmax和sigmoid的输出永远无法等于1或0。这意味着训练我们的模型会使我们的激活值越来越接近这些值，这样我们做的时代越多，我们的激活值就会变得越极端。
- en: With Mixup, we no longer have that problem, because our labels will be exactly
    1 or 0 only if we happen to “mix” with another image of the same class. The rest
    of the time, our labels will be a linear combination, such as the 0.7 and 0.3
    we got in the church and gas station example earlier.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Mixup，我们不再有这个问题，因为我们的标签只有在我们碰巧与同一类别的另一幅图像“混合”时才会完全是1或0。其余时间，我们的标签将是一个线性组合，比如我们在之前的教堂和加油站示例中得到的0.7和0.3。
- en: One issue with this, however, is that Mixup is “accidentally” making the labels
    bigger than 0 or smaller than 1\. That is to say, we’re not *explicitly* telling
    our model that we want to change the labels in this way. So, if we want to change
    to make the labels closer to or further away from 0 and 1, we have to change the
    amount of Mixup—which also changes the amount of data augmentation, which might
    not be what we want. There is, however, a way to handle this more directly, which
    is to use *label smoothing*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的一个问题是Mixup“意外地”使标签大于0或小于1。也就是说，我们并没有*明确*告诉我们的模型我们想以这种方式改变标签。因此，如果我们想要使标签更接近或远离0和1，我们必须改变Mixup的数量，这也会改变数据增强的数量，这可能不是我们想要的。然而，有一种更直接处理的方法，那就是使用*标签平滑*。
- en: Label Smoothing
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签平滑
- en: 'In the theoretical expression of loss, in classification problems, our targets
    are one-hot encoded (in practice, we tend to avoid doing this to save memory,
    but what we compute is the same loss as if we had used one-hot encoding). That
    means the model is trained to return 0 for all categories but one, for which it
    is trained to return 1\. Even 0.999 is not “good enough”; the model will get gradients
    and learn to predict activations with even higher confidence. This encourages
    overfitting and gives you at inference time a model that is not going to give
    meaningful probabilities: it will always say 1 for the predicted category even
    if it’s not too sure, just because it was trained this way.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在损失的理论表达中，在分类问题中，我们的目标是独热编码的（在实践中，我们倾向于避免这样做以节省内存，但我们计算的损失与使用独热编码时相同）。这意味着模型被训练为对所有类别返回0，只有一个类别返回1。即使是0.999也不是“足够好”；模型将获得梯度并学会以更高的信心预测激活。这会鼓励过拟合，并在推理时给出一个不会给出有意义概率的模型：即使不太确定，它总是会为预测的类别说1，只是因为它是这样训练的。
- en: This can become very harmful if your data is not perfectly labeled. In the bear
    classifier we studied in [Chapter 2](ch02.xhtml#chapter_production), we saw that
    some of the images were mislabeled, or contained two different kinds of bears.
    In general, your data will never be perfect. Even if the labels were manually
    produced by humans, they could make mistakes, or have differences of opinions
    on images that are harder to label.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据不完全标记，这可能会变得非常有害。在我们在[第2章](ch02.xhtml#chapter_production)中研究的熊分类器中，我们看到一些图像被错误标记，或包含两种不同种类的熊。一般来说，您的数据永远不会是完美的。即使标签是人工制作的，也可能出现错误，或者在难以标记的图像上存在不同意见。
- en: Instead, we could replace all our 1s with a number a bit less than 1, and our
    0s with a number a bit more than 0, and then train. This is called *label smoothing*.
    By encouraging your model to be less confident, label smoothing will make your
    training more robust, even if there is mislabeled data. The result will be a model
    that generalizes better at inference.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以用一个比1稍微小一点的数字替换所有的1，用一个比0稍微大一点的数字替换所有的0，然后进行训练。这就是*标签平滑*。通过鼓励模型变得不那么自信，标签平滑将使您的训练更加健壮，即使存在错误标记的数据。结果将是一个在推理时更好泛化的模型。
- en: 'This is how label smoothing works in practice: we start with one-hot-encoded
    labels, then replace all 0s with <math alttext="StartFraction epsilon Over upper
    N EndFraction"><mfrac><mi>ϵ</mi> <mi>N</mi></mfrac></math> (that’s the Greek letter
    *epsilon*, which is what was used in the [paper that introduced label smoothing](https://oreil.ly/L3ypf)
    and is used in the fastai code), where <math alttext="upper N"><mi>N</mi></math>
    is the number of classes and <math alttext="epsilon"><mi>ϵ</mi></math> is a parameter
    (usually 0.1, which would mean we are 10% unsure of our labels). Since we want
    the labels to add up to 1, we also replace the 1s with <math alttext="1 minus
    epsilon plus StartFraction epsilon Over upper N EndFraction"><mrow><mn>1</mn>
    <mo>-</mo> <mi>ϵ</mi> <mo>+</mo> <mfrac><mi>ϵ</mi> <mi>N</mi></mfrac></mrow></math>
    . This way, we don’t encourage the model to predict something overconfidently.
    In our Imagenette example that has 10 classes, the targets become something like
    this (here for a target that corresponds to the index 3):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是标签平滑在实践中的工作方式：我们从独热编码的标签开始，然后用<math alttext="StartFraction epsilon Over upper
    N EndFraction"><mfrac><mi>ϵ</mi> <mi>N</mi></mfrac></math>（这是希腊字母*epsilon*，在介绍标签平滑的[论文](https://oreil.ly/L3ypf)和fastai代码中使用）替换所有的0，其中<math
    alttext="upper N"><mi>N</mi></math>是类别数，<math alttext="epsilon"><mi>ϵ</mi></math>是一个参数（通常为0.1，这意味着我们对标签有10%的不确定性）。由于我们希望标签总和为1，我们还用<math
    alttext="1 minus epsilon plus StartFraction epsilon Over upper N EndFraction"><mrow><mn>1</mn>
    <mo>-</mo> <mi>ϵ</mi> <mo>+</mo> <mfrac><mi>ϵ</mi> <mi>N</mi></mfrac></mrow></math>替换1。这样，我们不会鼓励模型过于自信地预测。在我们的Imagenette示例中有10个类别，目标变成了这样（这里是对应于索引3的目标）：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In practice, we don’t want to one-hot encode the labels, and fortunately we
    won’t need to (the one-hot encoding is just good to explain label smoothing and
    visualize it).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们不想对标签进行独热编码，幸运的是我们也不需要（独热编码只是用来解释标签平滑和可视化的）。
- en: 'To use this in practice, we just have to change the loss function in our call
    to `Learner`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中使用这个方法，我们只需要在调用`Learner`时改变损失函数：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As with Mixup, you won’t generally see significant improvements from label
    smoothing until you train more epochs. Try it yourself and see: how many epochs
    do you have to train before label smoothing shows an improvement?'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与Mixup一样，您通常在训练更多时期后才会看到标签平滑带来的显著改进。自己尝试一下：在标签平滑显示改进之前，您需要训练多少个时期？
- en: Conclusion
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You have now seen everything you need to train a state-of-the-art model in computer
    vision, whether from scratch or using transfer learning. Now all you have to do
    is experiment on your own problems! See if training longer with Mixup and/or label
    smoothing avoids overfitting and gives you better results. Try progressive resizing
    and test time augmentation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经看到了训练计算机视觉中最先进模型所需的一切，无论是从头开始还是使用迁移学习。现在您只需要在自己的问题上进行实验！看看使用Mixup和/或标签平滑进行更长时间的训练是否可以避免过拟合并给出更好的结果。尝试渐进式调整大小和测试时间增强。
- en: Most importantly, remember that if your dataset is big, there is no point prototyping
    on the whole thing. Find a small subset that is representative of the whole, as
    we did with Imagenette, and experiment on it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，记住，如果您的数据集很大，那么在整个数据集上进行原型设计是没有意义的。找到一个代表整体的小子集，就像我们在Imagenette上所做的那样，并在其上进行实验。
- en: 'In the next three chapters, we will look at the other applications directly
    supported by fastai: collaborative filtering, tabular modeling, and working with
    text. We will go back to computer vision in the next section of the book, with
    a deep dive into convolutional neural networks in [Chapter 13](ch13.xhtml#chapter_convolutions).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，我们将看到fastai直接支持的其他应用程序：协同过滤、表格建模和处理文本。在本书的下一部分中，我们将回到计算机视觉，深入研究卷积神经网络在[第13章](ch13.xhtml#chapter_convolutions)中。
- en: Questionnaire
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问卷
- en: What is the difference between ImageNet and Imagenette? When is it better to
    experiment on one versus the other?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ImageNet和Imagenette之间有什么区别？在什么情况下最好在其中一个上进行实验而不是另一个？
- en: What is normalization?
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是归一化？
- en: Why didn’t we have to care about normalization when using a pretrained model?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在使用预训练模型时我们不需要关心归一化？
- en: What is progressive resizing?
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是渐进式调整大小？
- en: Implement progressive resizing in your own project. Did it help?
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在自己的项目中实现渐进式调整大小。有帮助吗？
- en: What is test time augmentation? How do you use it in fastai?
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是测试时间增强？如何在fastai中使用它？
- en: Is using TTA at inference slower or faster than regular inference? Why?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在推理中使用TTA比常规推理更慢还是更快？为什么？
- en: What is Mixup? How do you use it in fastai?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是Mixup？如何在fastai中使用它？
- en: Why does Mixup prevent the model from being too confident?
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么Mixup可以防止模型过于自信？
- en: Why does training with Mixup for five epochs end up worse than training without
    Mixup?
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么使用Mixup进行五个时期的训练最终比不使用Mixup训练更糟糕？
- en: What is the idea behind label smoothing?
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签平滑背后的理念是什么？
- en: What problems in your data can label smoothing help with?
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的数据中有哪些问题可以通过标签平滑来解决？
- en: When using label smoothing with five categories, what is the target associated
    with the index 1?
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用五个类别的标签平滑时，与索引1相关联的目标是什么？
- en: What is the first step to take when you want to prototype quick experiments
    on a new dataset?
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您想在新数据集上快速进行原型实验时，应该采取的第一步是什么？
- en: Further Research
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步研究
- en: Use the fastai documentation to build a function that crops an image to a square
    in each of the four corners; then implement a TTA method that averages the predictions
    on a center crop and those four crops. Did it help? Is it better than the TTA
    method of fastai?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用fastai文档构建一个函数，将图像裁剪为每个角落的正方形；然后实现一种TTA方法，该方法对中心裁剪和这四个裁剪的预测进行平均。有帮助吗？比fastai的TTA方法更好吗？
- en: Find the Mixup paper on arXiv and read it. Pick one or two more recent articles
    introducing variants of Mixup and read them; then try to implement them on your
    problem.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在arXiv上找到Mixup论文并阅读。选择一两篇介绍Mixup变体的较新文章并阅读它们；然后尝试在您的问题上实现它们。
- en: Find the script training Imagenette using Mixup and use it as an example to
    build a script for a long training on your own project. Execute it and see if
    it helps.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到使用Mixup训练Imagenette的脚本，并将其用作在自己项目上进行长时间训练的示例。执行它并查看是否有帮助。
- en: Read the sidebar [“Label Smoothing, the Paper”](#label_smoothing); then look
    at the relevant section of the original paper and see if you can follow it. Don’t
    be afraid to ask for help!
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读侧边栏["标签平滑，论文"](＃label_smoothing）；然后查看原始论文的相关部分，看看您是否能够理解。不要害怕寻求帮助！
