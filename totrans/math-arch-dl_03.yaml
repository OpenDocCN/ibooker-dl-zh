- en: 4 Linear algebraic tools in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Quadratic forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying principal component analysis (PCA) in data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving documents with a machine learning application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding patterns in large volumes of high-dimensional data is the name of the
    game in machine learning and data science. Data often appears in the form of large
    matrices (a toy example of this is shown in section [2.3](02.xhtml#sec-matrices)
    and also in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)). The rows
    of the data matrix represent feature vectors for individual input instances. Hence,
    the number of rows matches the count of observed input instances, and the number
    of columns matches the size of the feature vector—that is, the number of dimensions
    in the feature space. Geometrically speaking, each feature vector (that is, row
    of the data matrix) represents a point in feature space. These points are not
    distributed uniformly over the space. Rather, the set of points belonging to a
    specific class occupies a small subregion of that space. This leads to certain
    structures in the data matrices. Linear algebra provides us the tools needed to
    study matrix structures.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we study linear algebraic tools to analyze matrix structures.
    The chapter presents some intricate mathematics, and we encourage you to persevere
    through it, including the theorem proofs. An intuitive understanding the proofs
    will give you significantly better insights into the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng.bz/aoYz](http://mng.bz/aoYz)
    in the form of fully functional and executable Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Distribution of feature data points and true dimensionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For instance, consider the problem of determining the similarity between documents.
    This is an important problem for document search companies like Google. Given
    a query document, the system needs to retrieve from an archive—in ranked order
    of similarity—documents that match the query document. To do this, we typically
    create a vector representation of each document. Then the dot product of the vectors
    representing a pair of documents can be used as a quantitative estimate of the
    similarity between the documents. Thus, each document is represented by a document
    descriptor vector in which every word in the vocabulary is associated with a fixed
    index in the vector. The value stored in that index position is the frequency
    (number of occurrences) of that word in the document.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Prepositions and conjunctions are typically excluded and singular; plural
    and other variants of words originating from the same stem are usually collapsed
    into one word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every word in the vocabulary gets its own dimension in the document space.
    If a word does not occur in a document, we put a zero at that word’s index location
    in the descriptor vector for that document. We store one descriptor vector for
    every document in the archive. In theory, the document descriptor is an extremely
    long vector: its length matches the size of the vocabulary of the documents in
    the system. But this vector only exists notionally. In practice, we do not explicitly
    store descriptor vectors in their entirety. We store a <word, frequency> pair
    for every unique word that occurs in a document—*but we do not explicitly store
    words that do not occur*. This is a *sparse representation* of a document vector.
    The corresponding *full representation* can be constructed from the sparse one
    whenever necessary. In documents, certain words often occur together (for example,
    *Michael* and *Jackson*, or *gun* and *violence*). For example, in a given set
    of documents, the number of occurrences of *gun* will more or less match the number
    of occurrences of *violence*: if one appears, the other also appears most of the
    time. For a descriptor vector or, equivalently, a point in a feature space representing
    a document, the value at the index position corresponding to the word *gun* will
    be more or less equal to that for the word *violence*. If we project those points
    on the hyperplane formed by the axes for these correlated words, all the points
    fall around a 45-degree straight line (whose equation is *x* = *y*), as shown
    in figure [4.1](#fig-lsa-dimred).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F01_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 Document descriptor space. Each word in the vocabulary corresponds
    to a separate dimension. Dots show projections of document feature vectors on
    the plane formed by the axes corresponding to the terms *gun* and *violence*.
  prefs: []
  type: TYPE_NORMAL
- en: In figure [4.1](#fig-lsa-dimred), all the points representing documents are
    concentrated near the 45-degree line, and the rest of the plane is unpopulated.
    Can we collapse the two axes defining that plane and replace them with the single
    line around which most data is concentrated? It turns out that yes, we can do
    this. Doing so reduces the number of dimensions in the data representation—we
    are replacing a pair of correlated dimensions with a single one—thereby simplifying
    the representation. This leads to lower storage costs and, more importantly, provides
    additional insights. We have effectively discovered a new topic called *gun-violence*
    from the documents.
  prefs: []
  type: TYPE_NORMAL
- en: as another example, consider a set of points in 3D, represented by coordinates
    *X*, *Y*, *Z*. If the *Z* coordinate is near zero for all the points, the data
    is concentrated around the *X*, *Y* plane. We can (and should) represent these
    points in two dimensions by projecting them onto the *Z* = 0 plane. Doing so approximates
    the positions of the points only slightly (they are projected onto a plane that
    they were close to in the first place). In a more realistic example, the data
    points may be clustered around an arbitrary plane in the 3D space (as opposed
    to the *Z* = 0 plane). We can still reduce the dimensionality of these data points
    to 2D by projecting on the plane they are close to.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, if a set of data points is distributed in a space so that the points
    are clustered around a lower-dimensional subspace within that space (such as a
    plane or line), we can project the points onto the subspace and perform a *dimensionality
    reduction* on the data. We effectively approximate the distances from the subspace
    with zero: since these distances are small by definition, the approximation is
    not too bad. Viewed another way, we eliminate smaller *from-subspace* variations
    and retain the larger *in-subspace* variations. The resulting representation is
    more compressed and also lends itself more easily to better analysis and insights
    as we have eliminated unimportant perturbations and are focusing on the main pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: These ideas form the basis of the technique called *principal component analysis*
    (PCA). It is one of the most important tools in the repertoire of a data scientist
    and machine learning practitioner. These ideas also underlie the *latent semantic
    analysis* (LSA) technique for document retrieval—a fundamental approach for solving
    natural language processing (NLP) problems in machine learning. This chapter is
    dedicated to studying a set of methods leading to PCA and LSA. We examine a basic
    document retrieval system along with Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Quadratic forms and their minimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a square symmetric matrix *A*, the scalar quantity *Q* = ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    is called a *quadratic form*. These are seen in various situations in machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, recall the equation for a circle that we learned in high school
  prefs: []
  type: TYPE_NORMAL
- en: (*x*[0]−*α*[0])² + (*x*[1]−*α*[1])² = *r*²
  prefs: []
  type: TYPE_NORMAL
- en: where the center of the circle is (*α*[0], *α*[1]) and the radius is *r*. This
    equation can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-00-a.png)'
  prefs: []
  type: TYPE_IMG
- en: If we denote the position vector ![](../../OEBPS/Images/eq_04-00-b.png) as ![](../../OEBPS/Images/AR_x.png)
    and the center of the circle as ![](../../OEBPS/Images/eq_04-00-c.png) as ![](../../OEBPS/Images/AR_alpha.png),
    the previous equation can be written compactly as
  prefs: []
  type: TYPE_NORMAL
- en: (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_alpha.png))*^T***I**(![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_alpha.png))
    = *r*²
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that left hand side of this equation is a quadratic form. The original
    *x*[0], *x*[1]-based equation only works for two dimensions. The matrix based
    equation is dimension agnostic: it represents a hypersphere in an arbitrary-dimensional
    space. For a two-dimensional space, the two equations become identical.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider the equation for an ellipse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-00-d.png)'
  prefs: []
  type: TYPE_IMG
- en: You can verify that this can be written compactly in matrix form as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-00-e.png)'
  prefs: []
  type: TYPE_IMG
- en: or, equivalently,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.1
  prefs: []
  type: TYPE_NORMAL
- en: 'where ![](../../OEBPS/Images/eq_04-01-a.png). Once again, the matrix representation
    is dimension independent. In other words, equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again)
    represents a hyperellipsoid. Note that if the ellipse axes are aligned with the
    coordinate axes, matrix *A* is diagonal. If we rotate the coordinate system, each
    position vector is rotated by an orthogonal matrix *R*. Equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again)
    is transformed as follows (we have used the rules for transposing the products
    of matrices from equation [2.10](02.xhtml#eq-mat-prod-transpose)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-01-b.png)'
  prefs: []
  type: TYPE_IMG
- en: Replacing *R^TAR* with *A*, we get the same equation as equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again),
    but *A* is no longer a diagonal matrix.
  prefs: []
  type: TYPE_NORMAL
- en: For a generic ellipsoid with arbitrary axes, *A* has nonzero off-diagonal terms
    but is still symmetric. Thus, the multidimensional hyperellipsoid is represented
    by a quadratic form. The hypersphere is a special case of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quadratic forms are also found in the second term of the multidimensional Taylor
    expansion shown in equation [3.8](../Text/03.xhtml#eq-taylor-multidim): ![](../../OEBPS/Images/eq_04-01-c.png)
    is a quadratic form in the Hessian matrix. Another huge application of quadratic
    forms is PCA, which is so important that we devote a whole section to it section
    [4.4](../Text/04.xhtml#sec-pca)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Minimizing quadratic forms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An important question is, what choice of ![](../../OEBPS/Images/AR_x.png) maximizes
    or minimizes the quadratic form? For instance, because the quadratic form is part
    of the multidimensional Taylor series, we need to minimize quadratic forms when
    we want to determine the best direction to move in to minimize the loss *L*(![](../../OEBPS/Images/AR_x.png)).
    Later, we will see that this question also lies at the heart of PCA computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If ![](../../OEBPS/Images/AR_x.png) is a vector with arbitrary length, we can
    make *Q* arbitrarily big or small by simply changing the length of ![](../../OEBPS/Images/AR_x.png).
    Consequently, optimizing *Q* with arbitrary length ![](../../OEBPS/Images/AR_x.png)
    is not a very interesting problem: rather, we want to know which *direction* of
    ![](../../OEBPS/Images/AR_x.png) optimizes *Q*. For the rest of this section,
    we discuss quadratic forms with unit vectors *Q* = *x̂^TAx̂* recall that *x̂*
    denotes a unit-length vector satisfying *x̂^Tx̂* = ||*x̂*||² = 1). Equivalently,
    we could use a different flavor, *Q* = *![](../../OEBPS/Images/AR_x.png)^TA*![](../../OEBPS/Images/AR_x.png)/![](../../OEBPS/Images/AR_x.png)^T![](../../OEBPS/Images/AR_x.png),
    but we will use the former expression here. We are essentially searching over
    all possible directions *x̂*, examining which direction minimizes *Q* = *x̂^TAx̂*.'
  prefs: []
  type: TYPE_NORMAL
- en: Using matrix diagonalization (section [2.15](02.xhtml#sec-mat-diagonalization)),
  prefs: []
  type: TYPE_NORMAL
- en: '*Q* = *x̂^TAx̂* = *x̂^TSΛS^Tx̂*'
  prefs: []
  type: TYPE_NORMAL
- en: where *S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]
     …  *![](../../OEBPS/Images/AR_e.png)[n]*] is the matrix with eigenvectors of
    *A* as its columns and Λ is a diagonal matrix with the eigenvalues of *A* on the
    diagonal and 0 everywhere else. Substituting
  prefs: []
  type: TYPE_NORMAL
- en: '*ŷ* = *S^Tx̂*'
  prefs: []
  type: TYPE_NORMAL
- en: we get
  prefs: []
  type: TYPE_NORMAL
- en: '*Q* = *x̂^TAx̂* = *x̂^TS*Λ*S^Tx̂* = *ŷ^T*Λ*ŷ*'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4.2
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that since *A* is symmetric, its eigenvectors are orthogonal. This implies
    that *S* is an orthogonal matrix: that is, *S^TS* = *SS^T* = **I**. Recall from
    section [2.14.2.1](02.xhtml#orth-len-preserv) that for an orthogonal matrix *S*,
    the transformation *S^Tx̂* is length preserving. Consequently, *ŷ* = *S^Tx̂* is
    a unit-length vector. To be precise,'
  prefs: []
  type: TYPE_NORMAL
- en: '||*ŷ*||² = ||*S^Tx̂*||² = (*S^Tx̂*)*^T*(*S^Tx̂*) = *x̂^TSS^Tx̂* = *x̂^Tx̂*
    = 1 since *SS^T* = **I**'
  prefs: []
  type: TYPE_NORMAL
- en: So, expanding the right-hand side of equation [4.2](#eq-quad-form), we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.3
  prefs: []
  type: TYPE_NORMAL
- en: We can assume that the eigenvalues are sorted in decreasing order of magnitude
    (if not, we can always renumber them).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this *lemma* (small proof): The quantity Σ*[i]^n*[= 1] *λ[i]* *y[i]*²,
    where Σ*[i]^n*[= 1] *y[i]*² = 1 and *λ*[1] ≥ *λ*[2] ≥ ⋯ *λ[n]*, attains its maximum
    value when *y*[1] = 1, *y*[2] = ⋯ *y[n]* = 0.'
  prefs: []
  type: TYPE_NORMAL
- en: An *intuitive proof* follows. If possible, let that the maximum value occur
    at some other value of *ŷ*. We are constrained by the fact that *ŷ* is an unit
    vector, so we must maintain Σ*[i]^n*[= 1] *y[i]*² = 1.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, none of the elements of *ŷ* can exceed 1. If we reduce the first
    term from 1 to a smaller value, say √1-*ϵ*, some other element(s) must go up by
    an equivalent amount to compensate (i.e., maintain the unit length property).
    Accordingly, suppose the hypothesized *ŷ* maximizing *Q* is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-03-a.png)'
  prefs: []
  type: TYPE_IMG
- en: where *δ* > 0.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we transfer the entire mass from the later term to the first
    term so that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-03-b.png)'
  prefs: []
  type: TYPE_IMG
- en: Doing this does not alter the length of *ŷ* as the sum of the squares of the
    first and the other term remains 1 − *ϵ* + *δ*. But the value of *Q* = Σ*[i]^n*[=
    1] *λ[i]*² is higher in the second case (where ![](../../OEBPS/Images/AR_y.png)[1]
    has been beefed up at the expense of another term), since *λ*[1](1−*ϵ* + *δ*)
    > *λ*[1](1−*ϵ*) + *λ[j]δ* for any *j* > 1 (since, *λ*[1] > *λ*[2]⋯ by assumption).
    Thus, whenever we have less than 1 in the first term and greater than zero in
    some other term, we can increase *Q* without losing the unit length property of
    *ŷ* by transferring the entire mass to the first term.
  prefs: []
  type: TYPE_NORMAL
- en: This means to maximize the right hand side of equation [4.3](#quad-form-trans),
    we must have 1 as the first element (corresponding to the largest eigenvalue)
    of the unit vector *ŷ* and zeros everywhere else. Anything else violates the condition
    that the corresponding quadratic form *Q* = Σ*[i]^n*[= 1] *λ[i]*² is a maximum.
  prefs: []
  type: TYPE_NORMAL
- en: Thus we have established that the maximum of *Q* occurs at ![](../../OEBPS/Images/eq_04-03-c.png).
    The corresponding *x̂* = *Sŷ* = ![](../../OEBPS/Images/AR_e.png)[1] - the eigenvector
    corresponding to the largest eigenvalue of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the quadratic form *Q* = *x̂^TAx̂* attains its maximum when *x̂* is along
    the eigenvector corresponding to the largest eigenvalue of *A*. The corresponding
    maximum *Q* is equal to the largest eigenvalue of *A*. Similarly, the minimum
    of the quadratic form occurs when *x̂* is along the eigenvector corresponding
    to the smallest eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: As stated above, many machine learning problems boil down to minimizing a quadratic
    form. We will study a few of them in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Symmetric positive (semi)definite matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A square symmetric *n* × *n* matrix *A* is positive semidefinite if and only
    if
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) ≥ 0 ∀![](../../OEBPS/Images/AR_x.png)
    ∈ ℝ*^n*'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, a positive semidefinite matrix yields a non-negative quadratic
    form with all *n* × 1 vectors ![](../../OEBPS/Images/AR_x.png). If we disallow
    the equality, we get symmetric positive definite matrices. Thus a square symmetric
    *n* × *n* matrix *A* is positive definite if and only if
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) > 0 ∀![](../../OEBPS/Images/AR_x.png)
    ∈ ℝ*^n*'
  prefs: []
  type: TYPE_NORMAL
- en: From equations [4.2](#eq-quad-form) and [4.3](#quad-form-trans), *Q* is positive
    or zero if all *λ[i]*s are positive or zero (since the ![](../../OEBPS/Images/AR_y.png)*[i]*²s
    are non-negative). Hence, symmetric positive (semi)definiteness is equivalent
    to the condition that all eigenvalues of the matrix are greater than (or equal
    to) zero.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Spectral and Frobenius norms of a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vector is an entity with a magnitude and direction. The norm ||![](../../OEBPS/Images/AR_x.png)||
    of a vector ![](../../OEBPS/Images/AR_x.png) represents its magnitude. Is there
    an equivalent notion for matrices? The answer is yes, and we will study two such
    ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Spectral norms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In section [2.5.4](02.xhtml#subsection-vector_length), we saw that the length
    (aka magnitude) of a vector ![](../../OEBPS/Images/AR_x.png) is ||![](../../OEBPS/Images/AR_x.png)||
    = ![](../../OEBPS/Images/AR_x.png)*^T*![](../../OEBPS/Images/AR_x.png). Is there
    an equivalent notion of magnitude for a matrix *A*?
  prefs: []
  type: TYPE_NORMAL
- en: Well, a matrix can be viewed as an amplifier of a vector. The matrix *A* amplifies
    the vector ![](../../OEBPS/Images/AR_x.png) to ![](../../OEBPS/Images/AR_b.png)
    = *A*![](../../OEBPS/Images/AR_x.png). So we can take the maximum possible value
    of ||*A*![](../../OEBPS/Images/AR_x.png)|| over all possible ![](../../OEBPS/Images/AR_x.png);
    that is a measure for the magnitude of *A*. Of course, if we consider arbitrary-length
    vectors, we can make ![](../../OEBPS/Images/AR_b.png) arbitrarily large by simply
    scaling ![](../../OEBPS/Images/AR_x.png) for any *A*. That is uninteresting. Rather,
    we want to examine which direction of ![](../../OEBPS/Images/AR_x.png) is amplified
    most and by how much.
  prefs: []
  type: TYPE_NORMAL
- en: 'We examine this question with unit vectors *x̂*: what is the maximum (or minimum)
    value of ||*Ax̂*||, and what direction *x̂* materializes it? The quantity'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-03-d.png)'
  prefs: []
  type: TYPE_IMG
- en: is known as the *spectral norm* of the matrix *A*. Note that *A*![](../../OEBPS/Images/AR_x.png)
    is a vector and ||*A*![](../../OEBPS/Images/AR_x.png)||[2] is its length. (We
    will sometimes drop the subscript 2 and denote the spectral norm as ||*A*||.)
  prefs: []
  type: TYPE_NORMAL
- en: Now consider the vector *Ax̂*. Its magnitude is
  prefs: []
  type: TYPE_NORMAL
- en: '||*Ax̂*|| = (*Ax̂*)*^T*(*Ax̂*) = *x̂^TA^TAx̂*'
  prefs: []
  type: TYPE_NORMAL
- en: This is a quadratic form. From section [4.2](../Text/04.xhtml#sec-quadratic-form),
    we know it will be maximized (minimized) when *x̂* s aligned with the largest
    smallest) eigenvalue of *A^TA*. Thus the spectral norm is given by the largest
    eigenvalue of *A^TA*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.4
  prefs: []
  type: TYPE_NORMAL
- en: where *σ*[1] is the largest eigenvalue of *A^TA*. It is also (the square of)
    the largest singular value of *A*. We will see *σ*[1] again in section [4.5](../Text/04.xhtml#sec-svd),
    when we study singular value decomposition (SVD).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Frobenius norms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative measure for the magnitude of a matrix is the Frobenius norm,
    defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.5
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it is the root mean square of all the matrix elements.
  prefs: []
  type: TYPE_NORMAL
- en: It can be proved that the Frobenius norm is equal to the root mean square of
    the sum of all the singular values (eigenvalues of *A^TA*) of the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.6
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Principal component analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a set of numbers, *X* = {*x*^((0)), *x*^((1)),⋯, *x*^((*n*))}.
    We want to get a sense of how tightly packed these points are. In other words,
    we want to measure the *spread* of these numbers. Figure [4.2](#fig-pca-1d) shows
    such a distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F02_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2 A 1D distribution of points. The distance between extreme points
    is *not* a fair representation of the spread of points: the distribution is not
    uniform, and the extreme points are far from the others. Most points are within
    a more tightly packed region.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the points need not be uniformly distributed. In particular, the
    extreme points (*x[max]*, *x[min]*) may be far from most other points (as in figure
    [4.2](#fig-pca-1d)). Thus, (*x[max]*– *x[min]*)/(*n*+1) is not a fair representation
    of the average spread of points here. Most points are within a more tightly packed
    region. The statistically sensible way to obtain the spread is to first obtain
    the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then obtain the average distance of the numbers from the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06-b.png)'
  prefs: []
  type: TYPE_IMG
- en: (If we want to, we can take the square root and use *σ*, but it is often not
    necessary to incur that extra computational burden). This scalar quantity, *σ*,
    is a good measure of the mean packing density or spread of the points in 1D. You
    may recognize that the previous equation is nothing but the famous variance formula
    from statistics. Can we extend the notion to higher-dimensional data?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first examine the idea in two dimensions. As usual, we name our coordinate
    axes *X*[0], *X*[1], and so on, instead of *X*, *Y*, to facilitate the extension
    to multiple dimensions. An individual 2D data point is denoted ![](../../OEBPS/Images/eq_04-06-c.png).
    The dataset is {![](../../OEBPS/Images/AR_x.png)^((0)), ![](../../OEBPS/Images/AR_x.png)^((1)),
    ⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean is straightforward. Instead of one means, we have two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06-d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus we now have a mean *vector*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06-e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s do the variance. The immediate problem we face is that there are
    infinite possible directions in the 2D plane. We can measure variance along any
    of them, and it will be different for each choice. We can, of course, find the
    variance along the *X*[0] and *X*[1] axes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06-f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*σ*[00] and *σ*[11] tells us the variance along *only one* of the axes *X*[0]
    and *X*[1], respectively. But in general, there will be joint variation along
    both axes. To deal with joint variation, let’s introduce a cross term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06-g.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These equations can be written compactly in matrix vector notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-06-h.png)'
  prefs: []
  type: TYPE_IMG
- en: NOTE In the expression for *C*, we are *not* taking the dot product of the vectors
    (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))
    and (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png)).
    The dot product would be (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))*^T*(![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png)).
    Here, the second element of the product is transposed, not the first. Consequently,
    the result is a matrix. The dot product would yield a scalar.)
  prefs: []
  type: TYPE_NORMAL
- en: The previous equations are general, meaning they can be extended to any dimension.
    To be precise, given a set of *n* multidimensional data points *X* = {![](../../OEBPS/Images/AR_x.png)^((0)),
    ![](../../OEBPS/Images/AR_x.png)^((1)),⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))},
    we can define
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.8
  prefs: []
  type: TYPE_NORMAL
- en: Note how the mean has become a vector (it was a scalar for 1D data) and the
    scalar variance of 1D, *σ*, has become a matrix *C*. This matrix is called the
    *covariance matrix*. The (*n*+1)-dimensional mean and covariance matrix can also
    be defined a
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.9
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.10
  prefs: []
  type: TYPE_NORMAL
- en: For *i* = *j*, *σ[ii]* is essentially the variance of the data along the *i*th
    dimension. Thus the diagonal elements of matrix *C* contain the variance along
    the coordinate axes. Off-diagonal elements correspond to cross-covariances.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Equations [4.8](#eq-covar-mat-0) and [4.9](#eq-covar-mat) are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Direction of maximum spread
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is the direction of maximum spread/variance? Let’s first consider an arbitrary
    direction specified by the unit vector *l̂*. Recalling that the component of any
    vector along a direction is yielded by the dot product of the vector with the
    unit direction vector, the components of the data points along *l̂* are given
    by
  prefs: []
  type: TYPE_NORMAL
- en: '*X* ^′ = {*l̂* ^T![](../../OEBPS/Images/AR_x.png)^((0)), *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((1)),⋯,
    *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((*n*))}'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Remember figure [2.8b](02.xhtml#fig-multi-dim-lineeq), which showed that
    the component of one vector along another is given by the dot product between
    them? *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((*i*)) are dot products and hence
    scalar values.
  prefs: []
  type: TYPE_NORMAL
- en: The spread along direction *l̂* is given by the variance of the scalar values
    in *X* ^′. The mean of the values in *X* ^′ is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-10-b.png)'
  prefs: []
  type: TYPE_IMG
- en: and the variance is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-10-c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that *C* ^′ = *l̂* ^T*Cl̂* is the variance of the data components along
    the direction *l̂*. As such, it represents the spread of the data along that direction.
    What is the direction *l̂* along which this spread *l̂* ^T*Cl̂* is maximal? It
    is the direction *l̂* that maximizes *C* ^′ = *l̂* ^T*Cl̂*. This maximizing direction
    can be identified using the quadratic form optimization technique we discussed
    in [4.2](../Text/04.xhtml#sec-quadratic-form). Applying that, we have the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: Variance is maximal when *l̂* is along the eigenvector corresponding to the
    largest eigenvalue of the covariance matrix *C*. This direction is called the
    *first principal axis* of the multidimensional data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of the data vectors along the principal axis are known as *first
    principal components*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of the variance along the first principal axis, given by the corresponding
    eigenvalue of the covariance matrix, is called the *first principal value*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second principal axis is the eigenvector of the covariance matrix corresponding
    to the second largest eigenvalue of the covariance matrix. Second principal components
    and values are defined likewise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The principal axes are orthogonal to each other because they are eigenvectors
    of the symmetric covariance matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the practical significance of PCA? Why would we like to know the direction
    along which the spread is maximum for a point distribution? Sections [4.4.2](#subsec-pca-app-dimred)
    through [4.4.5](#subsec-pca-app-datacompress) are devoted to answering this question.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 PCA and dimensionality reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In section [4.1](#sec-truedim), we saw that when data points are clustered around
    a lower-dimensional subspace, it is beneficial to project them onto the subspace
    and reduce the dimensionality of the data representation. The dimensionally reduced
    data is more compactly representable and more amenable to deriving insights and
    analysis. In the particular case where the data points are clustered around a
    straight line or hyperplane, PCA can be used to generate a lower-dimensional data
    representation by getting rid of the principal components corresponding to relatively
    small principal values. The technique is agnostic to the dimensionality of the
    data. The line or hyperplane can be anywhere in the space, with arbitrary orientation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F03a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Dimensionality reduction from 2D to 1D
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F03b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Dimensionality reduction from 3D to 2D
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 Dimensionality reduction via PCA. Original data points are shown
    with filled little circles, and hollow circles represent lower-dimensional representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider the 2D distribution shown in figure [4.3a](#fig-pca-3and2d).
    Here, the data is 2D and plotted on a plane, but the main spread of the data is
    along a 1D line shown by the thick two-arrowed line in the figure). There is very
    little spread in the direction orthogonal to that line (indicated by the little
    perpendiculars from the data points to the line in the figure). PCA reveals this
    internal structure. There are two principal values because the data is 2D), but
    one of them is much smaller than the other: this reveals that dimensionality reduction
    is possible. The principal axis corresponding to the larger principal value is
    along the line of maximum spread. The small perturbations along the other principal
    axis can be eliminated with little loss of information. Replacing each data point
    with its projection on the first principal axis converts the 2D dataset into a
    1D dataset, brings out the true underlying pattern in the data (straight line),
    eliminates noise (little perpendiculars), and reduces storage costs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure [4.3b](#fig-pca-3and2d), the data is 3D, but the data points are
    clustered around a plane in 3D space (shown as the rectangle in the figure). The
    main spread of the data is *along* the plane, while the spread in the direction
    normal to that plane (shown with little perpendiculars from data points to the
    plane) is small. PCA reveals this: there are three principal values (because the
    data is 3D), but one of them is much smaller than the other two, revealing that
    dimensionality reduction is possible. The principal axis corresponding to the
    small principal value is normal to the plane. We can ignore these perturbations
    (perpendiculars in figure [4.3b](#fig-pca-3and2d)) with little loss of information.
    This is equivalent to projecting the data onto the plane formed by the first two
    principal axes. Doing so brings out the underlying data pattern (plane), eliminates
    noise (little perpendiculars), and reduces storage costs.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.3 PyTorch code: PCA and dimensionality reduction'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide a PyTorch code sample for PCA computation in listing
    [4.1](#code-pca-computation). Then we provide PyTorch code for applying PCA on
    a correlated dataset and an uncorrelated dataset in listings [4.2](#code-pca-correlated)
    and [4.3](#code-pca-uncorrelated), respectively. The results are plotted in figure
    [4.4](#fig-pca-results).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F04a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PCA on correlated data
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F04b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) PCA on uncorrelated data
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 PCA results. In (a), the data points are around the straight line
    **y** = 2*x*. Consequently, one principal value is much larger than the other,
    indicating that dimensionality reduction will work. In (b), both principal values
    are large. Dimensionality reduction will not work.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The complete PyTorch code for this section is available at [http://mng.bz/aoYz](http://mng.bz/aoYz)
    in the form of fully functional and executable Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.1 PCA computation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Returns principal values and vectors
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for the PCA computation in listing 4.1 is available
    at [http://mng.bz/DRYR](http://mng.bz/DRYR).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.2 PCA on synthetic correlated data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Random feature vector
  prefs: []
  type: TYPE_NORMAL
- en: ② Correlated feature vector + minor noise
  prefs: []
  type: TYPE_NORMAL
- en: ③ Data matrix spread mostly along y = 2x
  prefs: []
  type: TYPE_NORMAL
- en: ④ One large principal value and one small
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ First principal vector along y = 2x
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Dimensionality reduction by projecting on the first principal vector
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: NOTE Fully functional code for the PCA computation in listing 4.2 is available
    at [http://mng.bz/gojl](http://mng.bz/gojl).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.3 PCA on synthetic uncorrelated data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Random uncorrelated feature-vector pair
  prefs: []
  type: TYPE_NORMAL
- en: ② Principal values close to each other. The spread of the data points is comparable
    in both directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: NOTE Fully functional code for the PCA computation in listing 4.3 is available
    at [http://mng.bz/e5Kz](http://mng.bz/e5Kz).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 Limitations of PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PCA assumes that the underlying pattern is linear in nature. Where this is not
    true, PCA will not capture the correct underlying pattern. This is illustrated
    schematically in figure [4.5a](#fig-non-linear-pca) and via experimental results
    from listing [4.3](#code-pca-uncorrelated). Figure [4.5b](#fig-non-linear-pca)
    shows the results of running listing [4.4](#code-synthetic-nonlin-correlated),
    where we synthetically generate non-linearly correlated data and perform PCA.
    The straight line at the base shows the first principal axis. Projecting data
    on this axis results in a large error in the data positions (loss of information).
    Linear PCA will not do well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F05a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Schematic 2D data distribution with a curved underlying pattern
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F05b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) PCA results on synthetic (computer generated) non-linearly correlated data.
    The line at the base shows the first principal axis.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 Non-linearly correlated data. The points are distributed around a
    curve as opposed to a straight line. It is impossible to find a straight line
    such that all points are near it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.4 PCA on synthetic nonlinearly correlated data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Principal vectors fail to capture the underlying distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 4.4.5 PCA and data compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we want to represent a large multidimensional dataset within a fixed byte
    budget, what information can we can get rid of with the least loss of accuracy?
    Clearly, the answer is the principal components along the smaller principal values—getting
    rid of them actually helps, as described in section [4.4.2](#subsec-pca-app-dimred).
    To compress data, we often perform PCA and then replace the data points with their
    projections on first few principal axes; doing so reduces the number of data components
    to store. This is the underlying principle in JPEG 98 image compression techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Singular value decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Singular value decomposition (SVD) may be the most important linear algebraic
    tool in machine learning. Among other things, PCA and LSA implementations are
    built based on SVD. We illustrate the basic idea in this section.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE There are several slightly different forms of SVD. We have chosen the one
    that seems intuitively simplest.
  prefs: []
  type: TYPE_NORMAL
- en: The SVD theorem states that any matrix *A*, singular or nonsingular, rectangular
    or square, can be decomposed as the product of three matrices
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *U*Σ*V^T*'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4.11
  prefs: []
  type: TYPE_NORMAL
- en: where (assuming that the matrix *A* is *m* × *n*)
  prefs: []
  type: TYPE_NORMAL
- en: Σ is an *m* × *n* diagonal matrix. Its diagonal elements contain the square
    roots of the eigenvalues of *A^TA*. These are also known as the singular values
    of *A*. The singular values appear in decreasing order in the diagonal of Σ.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V* is an *n* × *n* orthogonal matrix containing eigenvectors of *A^TA* in
    its columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* is an *m* × *m* orthogonal matrix containing eigenvectors of *AA^T* in
    its columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.5.1 Informal proof of the SVD theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will provide an informal proof of the SVD theorem through a series of lemmas.
    Going through these will provide additional insights.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 1
  prefs: []
  type: TYPE_NORMAL
- en: '*A^TA* is symmetric positive semidefinite. Its eigenvalues—aka singular values—are
    non-negative. Its eigenvectors—aka singular vectors—are orthogonal.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of lemma 1
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say *A* has *m* rows and *n* columns. Then *A^TA* is an *n* × *n* square
    matrix
  prefs: []
  type: TYPE_NORMAL
- en: (*A^TA*)*^T* = *A^T*(*A^T*)*^T* = *A^TA*
  prefs: []
  type: TYPE_NORMAL
- en: which proves that *A^TA* is symmetric. Also, for any ![](../../OEBPS/Images/AR_x.png),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TA^TA*![](../../OEBPS/Images/AR_x.png) =
    (*A*![](../../OEBPS/Images/AR_x.png))*^T*(*A*![](../../OEBPS/Images/AR_x.png))
    = ||*A*![](../../OEBPS/Images/AR_x.png)||² > 0'
  prefs: []
  type: TYPE_NORMAL
- en: which, as per section [4.2.2](#subsec-sym-pos-sd), proves that the matrix *A^TA*
    is symmetric and positive semidefinite. Hence, its eigenvalues are all positive
    or zero.
  prefs: []
  type: TYPE_NORMAL
- en: We proved in section [2.13](02.xhtml#sec-eigen-values-vectors) that symmetric
    matrices have orthogonal eigenvectors. That proves that singular vectors are orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: Let (*λ[i], v̂*[1]), for *i* ∈ [1, *n*] be the set of eigenvalue, eigenvector
    pairs of *A^TA*—aka the singular value, singular vector pair of *A*. Note that
    without loss of generality, we can assume *λ*[1] ≥ *λ*[2] ≥ ⋯ *λ[n]* because if
    not, we can always renumber the eigenvalues and eigenvectors).
  prefs: []
  type: TYPE_NORMAL
- en: Now, by definition,
  prefs: []
  type: TYPE_NORMAL
- en: '*A^TAv̂[i]* = *λ[i]v̂[i]* ∀*i* ∈ [1, *n*]'
  prefs: []
  type: TYPE_NORMAL
- en: From lemma 1, singular vectors are orthogonal, and hence
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.12
  prefs: []
  type: TYPE_NORMAL
- en: Note that *v̂[i]*s are unit vectors (that is why we are using the hat sign as
    opposed to the overhead arrow). As described in section [2.13](02.xhtml#sec-eigen-values-vectors),
    eigenvectors remain eigenvectors if we change their length. We are free to choose
    any length for eigenvectors as long as we choose it consistently. We are choosing
    unit-length eigenvectors here.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 2
  prefs: []
  type: TYPE_NORMAL
- en: '*AA^T* is symmetric positive semidefinite. Its eigenvalues are non-negative
    and eigenvectors are orthogonal.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of lemma 2
  prefs: []
  type: TYPE_NORMAL
- en: (*AA^T*)*^T* = (*A^T*)*^TA^T* = *AA^T*
  prefs: []
  type: TYPE_NORMAL
- en: Also,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png)*^TAA^T*![](../../OEBPS/Images/AR_x.png) =
    (*A^T*![](../../OEBPS/Images/AR_x.png))*^T*(*A^T*![](../../OEBPS/Images/AR_x.png))
    = ||(*A^T*![](../../OEBPS/Images/AR_x.png))|| ≥ 0'
  prefs: []
  type: TYPE_NORMAL
- en: and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 3
  prefs: []
  type: TYPE_NORMAL
- en: 1/√*λ[i] ⋅ Av̂*[1], ∀*i* ∈ [1, *n*] is a set of orthogonal unit vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of lemma 3
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the dot product of a pair of these vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Since *λ[j]*, *v̂[j]* are eigenvalue, eigenvector pairs of *A^TA*, the previous
    equation can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-b.png)'
  prefs: []
  type: TYPE_IMG
- en: which, using equation [4.12](#eq-ortho-eigvec), can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Lemma 4
  prefs: []
  type: TYPE_NORMAL
- en: If (*λ[i]*, *v̂[i]*) is an eigenvalue, eigenvector pair of *A^TA*, then *λ[i],
    û[i]* = 1/√*λ[i]* ⋅ *Av̂[i]* is an eigenvalue, eigenvector pair of *AA^T*.
  prefs: []
  type: TYPE_NORMAL
- en: Proof of lemma 4
  prefs: []
  type: TYPE_NORMAL
- en: Given
  prefs: []
  type: TYPE_NORMAL
- en: '*A^TAv̂[i]* = *λ[i]v̂[i]*'
  prefs: []
  type: TYPE_NORMAL
- en: left-multiplying both sides of the equation by *A*, we get
  prefs: []
  type: TYPE_NORMAL
- en: '*AA^T**Av̂[i]* = *λ[i]* *Av̂[i]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*AA^T*(*Av̂[i]*) = *λ[i]* (*Av̂[i]*)'
  prefs: []
  type: TYPE_NORMAL
- en: Substituting *![](../../OEBPS/Images/AR_f.png)[i]* = *Av̂[i]* in the last equation,
    we get
  prefs: []
  type: TYPE_NORMAL
- en: '*AA^T![](../../OEBPS/Images/AR_f.png)[i]* = *λ[i]![](../../OEBPS/Images/AR_f.png)[i]*'
  prefs: []
  type: TYPE_NORMAL
- en: which proves that *![](../../OEBPS/Images/AR_f.png)[i]* = *Av̂[i]* is an eigenvector
    of *AA^T* with *λ[i]* as a corresponding eigenvalue. Multiplying by 1/√*λ[i]*
    converts it into a unit vector as per lemma 3\. This completes the proof of the
    lemma.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Proof of the SVD theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we are ready to examine the proof of the SVD theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: More rows than columns in A'
  prefs: []
  type: TYPE_NORMAL
- en: If *m*, the number of rows in *A*, is greater than or equal to *n*, the number
    of columns in *A*, we define
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: From lemma 1, we know that the eigenvalues of *A^TA* are positive. This makes
    the square roots, √*λ[i]s*, real.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U* is an *m* × *m* orthogonal matrix whose columns are the eigenvectors of
    *AA^T*. Since, *AA^T* is *m* × *m*, it has *m* eigenvalues and eigenvectors. The
    first *n* of them are *û*[1] = 1/√*λ*[1] ⋅ *Av̂*[1], *û*[2] = 1/√*λ*[2] ⋅ *Av̂*[2],
    … , *û[n]* = 1/√*λ[i]* ⋅ *Av̂[n]* from lemma 4, we know these are eigenvectors
    of *AA^T*). In this case, by our initial assumption, *n* < *m*. Thus *AA^T* has
    (*m* − *n*) more eigenvectors, *û*[*n* + 1], ⋯ *û[m]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V* is an *n* × *n* orthogonal matrix with the eigenvectors of *A^TA* that
    is, *v̂*[1], *v̂*[2], ⋯, *v̂[n]*) as its columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the matrix product *U*Σ. From basic matrix multiplication rules (section
    [2.5](02.xhtml#sec-misc_mat-vec), we can see that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-e.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the last columns of *U*, *û*[*n* + 1], ⋯, *û[m]*, are multiplied by
    all zeros in Σ and vanishing. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-f.png)'
  prefs: []
  type: TYPE_IMG
- en: The later columns of *U*—those named with *u*s—fail to survive because they
    are multiplied by the zeros at the bottom of Σ.
  prefs: []
  type: TYPE_NORMAL
- en: Thus we have proved that
  prefs: []
  type: TYPE_NORMAL
- en: '*AV* = *U*Σ'
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: '*AVV^T* = *U*Σ*V^T*'
  prefs: []
  type: TYPE_NORMAL
- en: Since *V* is orthogonal, *VV^T* = **I**. Hence
  prefs: []
  type: TYPE_NORMAL
- en: '*A* = *U*Σ*V^T*'
  prefs: []
  type: TYPE_NORMAL
- en: which completes the proof of the singular value theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 2: Fewer rows than columns in A'
  prefs: []
  type: TYPE_NORMAL
- en: If *m*, the number of rows in *A*, is less than or equal to *n*, the number
    of columns in *A*, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-g.png)'
  prefs: []
  type: TYPE_IMG
- en: The proof follows along similar lines.
  prefs: []
  type: TYPE_NORMAL
- en: '4.5.3 Applying SVD: PCA computation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will illustrate the idea first with a toy dataset. Consider a 3D dataset
    with five points. We use a superscript to denote the index of the data instance
    and a subscript to denote the component. Thus the *i*th data instance vector is
    denoted as [*x*[0]^((*i*))   *x*[1]^((*i*))   *x*[2]^((*i*))]. We denote the entire
    data set with a matrix in which each feature instance appears as a row vector.
    The data matrix is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-h.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will assume that the data is already mean-subtracted. Now examine the matrix
    product *X^TX*, using ordinary rules of matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-i.png)'
  prefs: []
  type: TYPE_IMG
- en: From equations [4.10](#eq-varij) and [4.9](#eq-covar-mat),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-j.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus *X^TX* is the covariance matrix of the dataset *X*. This holds for arbitrary
    dimensions and arbitrary feature instance counts.
  prefs: []
  type: TYPE_NORMAL
- en: If we create a data matrix *X* with each data instance forming a row, *X^TX*
    ields the covariance matrix of the dataset. The eigenvalues and eigenvectors of
    this matrix are the principal components. Hence, performing SVD on *X* yields
    PCA of the data (assuming prior mean subtraction).
  prefs: []
  type: TYPE_NORMAL
- en: '4.5.4 Applying SVD: Solving arbitrary linear systems'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A linear system is a system of simultaneous linear equations
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first encountered a linear system in section [2.12](02.xhtml#sec-lin_systems).
    It is possible to use matrix inversion to solve such a system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png) = *A*^(-1)![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, solving a linear system with matrix inversion is undesirable for many
    reasons. To begin with, it is numerically unstable. The matrix inverse contains
    the determinant of the matrix in its denominator. If the determinant is near zero,
    the inverse will contain very large numbers. Minor noise in ![](../../OEBPS/Images/AR_b.png)
    will be multiplied by these large numbers and cause large errors in the computed
    solution ![](../../OEBPS/Images/AR_x.png). In this case, the inverse-based solution
    can be very inaccurate. Furthermore, the determinant can be zero: this can happen
    when one row of the matrix is a linear combination of others, indicating that
    we have fewer equations than we think. And what if the matrix is not square to
    begin with? This can happen when we have more equations than unknowns overdetermined
    system) or fewer equations than unknowns underdetermined system). In these cases,
    the inverse is not computable, and the system cannot be solved fully.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even in these cases, we would like to obtain a solution that is the best approximation
    in some sense; and in the case of a square matrix, we would like to get the exact
    solution. How do we do this? Answer: we use SVD. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) can
    be rewritten as *U*(Σ*V^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_b.png).
    We then solve *U*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png).
    This can be easily done using orthogonality of *U*, as ![](../../OEBPS/Images/AR_y.png)[1]
    = *U^T*![](../../OEBPS/Images/AR_b.png).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have Σ(*V^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_y.png)[1]
    Solve Σ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1].
    This can be easily done because for any diagonal matrix ![](../../OEBPS/Images/eq_04-12-k.png)
    we can easily compute ![](../../OEBPS/Images/eq_04-12-l.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hence, ![](../../OEBPS/Images/AR_y.png)[2] = Σ^(−1)![](../../OEBPS/Images/AR_y.png)[1].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we have *V^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2].
    This too can be solved easily using the orthogonality of *V*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/AR_x.png) = *V*![](../../OEBPS/Images/AR_y.png)[2]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Thus we have solved for ![](../../OEBPS/Images/AR_x.png) without inverting
    the matrix *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: For invertible square matrices *A*, this method yields the same solution as
    the matrix-inverse-based method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For nonsquare matrices, this boils down to the Moore-Penrose inverse and yields
    the best-effort solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.5.5 Rank of a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In section [2.12](02.xhtml#sec-lin_systems), we studied linear systems of equations.
    Such a system can be represented in matrix-vector form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Each row of *A* and ![](../../OEBPS/Images/AR_b.png) contributes one equation.
    If we have as many independent equations as unknowns, the system is solvable.
    This is the simplest case; matrix *A* is square and invertible. *det*(*A*) is
    nonzero, and *A*^(−1) exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes the situation is misleading. Consider the following system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-m.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although there are three rows and apparently three equations, the equations
    are not independent. For instance, the third equation can be obtained by adding
    the first two. We really have only two equations, not three. We say this linear
    system is *degenerate*. All of the following statements are true for such a system
    *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png):'
  prefs: []
  type: TYPE_NORMAL
- en: The linear system is degenerate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*det*(*A*) = 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A*^(−1) cannot be computed, and *A* is not invertible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rows of *A* are linearly dependent. There exists a linear combination of the
    rows that sum to zero. For example, in the previous example, ![](../../OEBPS/Images/AR_r.png)[0]
    + ![](../../OEBPS/Images/AR_r.png)[1] − ![](../../OEBPS/Images/AR_r.png)[2] =
    0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least one of the singular values of *A* (eigenvalues of *A^TA*) is zero.
    The number of linearly independent rows is equal to the number of nonzero eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of linearly independent rows in a matrix is called its *rank*. It
    can be proved that a matrix has as many nonzero singular values as its rank. It
    can also be proved that the number of linearly independent columns in a matrix
    matches the number of linearly independent rows. Hence, rank can also be defined
    as the number of linearly independent columns in a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: A nonsquare rectangular matrix with *m* rows and *n* columns has a rank *r*
    = *min*(*m*, *n*). Such matrices are never invertible. We usually resort to SVD
    to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: A square matrix with *n* rows and *n* columns is invertible nonzero determinant)
    if and only if it has rank *n*. Such a matrix is said to have *full rank*. Full-rank
    matrices are invertible. They can be solved via matrix inverse computation, but
    inverse computation is not always numerically stable. SVD can be applied here
    as well, with good numerical properties.
  prefs: []
  type: TYPE_NORMAL
- en: Non-full-rank matrices are degenerate. So, rank is a measure of the non-degeneracy
    of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.6 PyTorch code for solving linear systems with SVD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The listings in this section show a PyTorch-based implementation of SVD and
    demonstrate an application that solves a linear system via SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.5 Solving an invertible linear system with matrix inversion and SVD
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① Simple test linear system of equations
  prefs: []
  type: TYPE_NORMAL
- en: ② Matrix inversion is numerically unstable; SVD is better.
  prefs: []
  type: TYPE_NORMAL
- en: ③ *A* = *USV^T* ⟹ *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    ≜ *USV^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
  prefs: []
  type: TYPE_NORMAL
- en: ④ Solves *U*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png).
    Remember *U*^(−1) = *U^T* as U is orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Solves *S*![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1].
     Remember *S*^(−1) is easy as *S* is diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Solves *V^T* ![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2].
    Remember *V^(−T)* = *V* as *V* is orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ The two solutions are the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Listing 4.6 Solving an overdetermined linear system by pseudo-inverse and SVD
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '① Cat-brain dataset: nonsquare matrix'
  prefs: []
  type: TYPE_NORMAL
- en: ② Solution via pseudo-inverse
  prefs: []
  type: TYPE_NORMAL
- en: ③ Solution via SVD
  prefs: []
  type: TYPE_NORMAL
- en: ④ The two solutions are the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Fully functional code for solving the SVD-based linear system can be found at
    [http://mng.bz/OERn](http://mng.bz/OERn).
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.7 PyTorch code for PCA computation via SVD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing demonstrates PCA computations using SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.7 Computing PCA directly and using SVD
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Eigenvalues of the covariance matrix yield principal values.
  prefs: []
  type: TYPE_NORMAL
- en: ② Eigenvectors of the covariance matrix yield principal vectors.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Direct PCA computation from a covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: ④ Data matrix
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Diagonal elements of matrix *S* yield principal values.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ PCA from SVD
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Columns of matrix *V* yield principal vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '4.5.8 Applying SVD: Best low-rank approximation of a matrix'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a matrix *A* of some rank *p*, we sometimes want to approximate it with
    a matrix of lower rank *r*, where *r* < *p*. How do we obtain the best rank *r*
    approximation of *A*?
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs: []
  type: TYPE_NORMAL
- en: Why would we want to do this? Well, consider a data matrix *X* as shown in section
    [4.5.3](#subsec-svd-app-pca-computation). As explained in section [4.4.2](#subsec-pca-app-dimred),
    we often want to eliminate small variances in the data (likely due to noise) and
    get the pattern underlying large variations. Replacing the data matrix with a
    lower-rank matrix often achieves this. However, we must bear in mind that this
    does not work when the underlying pattern is nonlinear (such as in figure [4.5a](#fig-non-linear-pca)).
  prefs: []
  type: TYPE_NORMAL
- en: Approximation error
  prefs: []
  type: TYPE_NORMAL
- en: What do we mean by *best approximation*? The Frobenius norm can be taken as
    the magnitude of the matrix. Accordingly, given a matrix *A* and its rank *r*
    approximation *A[r]*, the approximation error is *e* = ||*A* − *A[r]*||*[F]*.
  prefs: []
  type: TYPE_NORMAL
- en: Method
  prefs: []
  type: TYPE_NORMAL
- en: To solidify our ideas, let’s consider an *m* × *n* matrix *A*. From section
    [4.5](../Text/04.xhtml#sec-svd), we know it will have *min*(*m*, *n*) singular
    values. Let its rank be *p* ≤ *min*(*m*, *n*). We want to approximate this matrix
    with a rank *r*(<*p*) matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s rewrite the SVD expression. We will assume *m* > *n*. Also, as usual,
    we have the singular values sorted in decreasing order: *λ*[1] ≥ *λ*[2] ≥ *λ[n]*.
    We will partition *U*, Σ, *V*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-12-n.png)'
  prefs: []
  type: TYPE_IMG
- en: It can be proved that *U*[1]Σ[1]*V*[1]*^T* is a rank *r* matrix. Furthermore,
    it is the best rank *r* approximation of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: '4.6 Machine learning application: Document retrieval'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now bring together several of the concepts we have discussed in this
    chapter with an illustrative toy example: the document retrieval problem we first
    encountered in section [2.1](02.xhtml#sec-vectors). Briefly recapping, we have
    a set of documents {*d*[0],⋯, *d*[6]}. Given an incoming query phrase, we have
    to retrieve documents that match the query phrase. We will use the *bag of words*
    model: that is, our matching approach does not pay attention to *where* a word
    appears in a document; it simply pays attention to *how many times* the word appears
    in the document. Although this technique is not the most sophisticated, it is
    popular because of its conceptual simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our documents are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**d*[0]*: Roses are lovely. Nobody hates roses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d*[1]*: *Gun* *violence* has reached epidemic proportions in America.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d*[2]*: The issue of *gun* *violence* is really over-hyped. One can find
    many instances of *violence* where no *guns* were involved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d*[3]*: *Guns* are for *violence* prone people. *Violence* begets *guns*.
    *Guns* beget *violence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d*[4]*: I like *guns* but I hate *violence*. I have never been involved in
    *violence*. But I own many *guns*. *Gun violence* is incomprehensible to me. I
    do believe *gun* owners are the most anti *violence* people on the planet. He
    who never uses a *gun* will be prone to senseless *violence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d*[5]*: *Guns* were used in an armed robbery in San Francisco last night.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d*[6]*: Acts of *violence* usually involve a weapon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.6.1 Using TF-IDF and cosine similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before discussing PCA, let’s look at some more elementary techniques for document
    retrieval. These are based on term frequency-inverse document frequency (TF-IDF)
    and cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency
  prefs: []
  type: TYPE_NORMAL
- en: '*Term frequency* (TF) is defined as the number of occurrences of a particular
    term in a document. (In this context, note that in this book, we use *term* and
    *word* somewhat interchangeably.) In a slightly looser definition, any quantity
    proportional to the number of occurrences of the term is also known as term frequency.
    For example, the TF of the word *gun* in *d*[0], *d*[6] is 0, in *d*[1] is 1,
    in *d*[3] is 3, and so on. Note that we are being case independent. Also, singular/plural
    (*gun* and *guns*) and various flavors of the words originating from the same
    stem (such as *violence* and *violent*) are typically mapped to the same term.'
  prefs: []
  type: TYPE_NORMAL
- en: Inverse document frequency
  prefs: []
  type: TYPE_NORMAL
- en: Certain terms, such as *the*, appear in pretty much all documents. These should
    be ignored during document retrieval. How do we down-weight them?
  prefs: []
  type: TYPE_NORMAL
- en: The IDF is obtained by inverting and then taking the absolute value of the logarithm
    of the fraction of all documents in which the term occurs. For terms that occur
    in most documents, the IDF weight is very low. It is high for relatively esoteric
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: Document feature vectors
  prefs: []
  type: TYPE_NORMAL
- en: Each document is represented by a document feature vector. It has as many elements
    as the size of the vocabulary (that is, the number of distinct words over all
    the documents). Every word has a fixed index position in the vector. Given a specific
    document, the value at the index position corresponding to a specific word contains
    the TF of the corresponding word multiplied by that word’s IDF. Thus, every document
    is a point in a space that has as many dimensions as the vocabulary size. The
    coordinate value along a specific dimension is proportional to the number of times
    the word is repeated in the document, with a weigh-down factor for common words.
  prefs: []
  type: TYPE_NORMAL
- en: 'For real-life document retrieval systems like Google, this vector is extremely
    long. But not to worry: this vector is notional—it is never explicitly stored
    in the computer’s memory. We store a sparse version of the document feature vector:
    a list of unique words along with their TF×IDF scores.'
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs: []
  type: TYPE_NORMAL
- en: 'In section [2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement), we saw that
    the dot product between two vectors measures the agreement between them. Given
    two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    we know ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png) =
    ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*),
    where the operator || ⋅ || implies the length of a vector and *θ* is the angle
    between the two vectors (see figure [2.7b](02.xhtml#ch2fig-vec_component)). The
    cosine is at its maximum possible value, 1, when the vectors are pointing in the
    same direction and the angle between them is zero. It becomes progressively smaller
    as the angle between the vectors increases until the two vectors are perpendicular
    to each other and the cosine is zero, implying no correlation: the vectors are
    independent of each other.'
  prefs: []
  type: TYPE_NORMAL
- en: The magnitude of the dot product is also proportional to the length of the two
    vectors. We do not want to use the full dot product as a measure of similarity
    between the vectors because two long vectors would have a high similarity score
    even if they were not aligned in direction. Rather, we want to use the cosine,
    defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4.13
  prefs: []
  type: TYPE_NORMAL
- en: The cosine similarity between document vectors is a principled way of measuring
    the degree of term sharing between the documents. It is higher if many repeated
    words are shared between the two documents.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2 Latent semantic analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cosine similarity and similar techniques suffer from a significant drawback.
    To see this, examine the cosine similarity between *d*[5] and *d*[6]. It is zero.
    But it is obvious to a human that the documents are similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'What went wrong? Answer: we are measuring only the direct overlap between terms
    in documents. The words *gun* and *violence* occur together in many of the other
    documents, indicating some degree of similarity between them. Hence, documents
    containing only *gun* have some similarity with documents containing only *violence*—but
    cosine similarity between document vectors does not look at such secondary evidence.
    This is the blind spot that LSA tries to overcome.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Words are known by the company they keep*. That is, if terms appear together
    in many documents (like *gun* and *violence* in the previous examples), they are
    likely to share some semantic similarity. Such terms should be grouped together
    into a common pool of semantically similar terms. Such a pool is called a *topic*.
    Document similarity should be measured in terms of common topics rather than explicit
    common terms. We are particularly interested in topics that discriminate the documents
    in our corpus: that is, there should be a high variation in the degree to which
    different documents subscribe to the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometrically, a topic is a subspace in the document feature space. In classical
    latent semantic analysis, we only look at linear subspaces, and a topic can be
    visualized as a direction or linear combination of directions (hyperplane) in
    the document feature space. In particular, any direction line in the space is
    a topic: it is a subspace representing a weighted combination of the coordinate
    axis directions, which means it is a weighted combination of vocabulary terms.
    We are, of course, interested in topics with high variance. These correspond to
    a direction along which the document vectors are well spread, which means the
    document vectors are well discriminated over this topic. We typically prune the
    set of topics, eliminating those with insufficient variance.'
  prefs: []
  type: TYPE_NORMAL
- en: From this discussion, a mathematical definition of *topic* begins to emerge.
    Topics are principal components of the matrix of document vectors with individual
    document descriptor vectors along its rows. Measuring document similarity in terms
    of topic has the advantage that two documents may not have many exact words in
    common but may still have a common topic. This happens when they share words belonging
    to the same topic. Essentially, they share a lot of words that occur together
    in other documents. So even if the number of common words is low, we can have
    high document similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F06_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 Document vectors from our toy dataset *d*[0], ⋯ *d*[6]. Each word
    in the vocabulary corresponds to a separate dimension. Dots show projections of
    document feature vectors on the plane formed by the axes corresponding to the
    terms *gun(s)* and *violence*.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in our toy document corpus, *gun* and *violence* are very correlated
    (both or neither is likely to occur in a document). *Gun-violence* emerges as
    a topic. If we express the document vector in terms of this topic instead of the
    individual words, we see similarities that otherwise would have escaped us. That
    is, we see *latent semantic* similarities. For instance, the cosine similarity
    between *d*[5] and *d*[6] is nonzero. This is the core idea of latent semantic
    analysis and is illustrated in figure [4.6](#fig-lsa).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s revisit our example document-retrieval problem in light of topic extraction.
    The document matrix (with document vectors as rows) looks like table [4.1](#tab-doc_vector_dataset).
    Rows correspond to documents, and columns correspond to terms. Each cell contains
    the term frequency. The terms *gun* and *violence* occur an equal number of times
    in most documents, indicating clear correlation. Hence *gun-violence* is a topic.
    The principal components right eigenvectors) identify topics. As usual, we have
    omitted prepositions, conjunctions, commas, and so on. The overall steps are as
    follows (see listing [4.8](#list-svd-lsa-toy) for the Python code):'
  prefs: []
  type: TYPE_NORMAL
- en: Create a document term *m* × *n*. Its rows correspond to documents (*m* documents),
    and its columns correspond to terms (*n* terms).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform SVD on the matrix. This yields *U*, *S*, and *V* matrices. *V* is an
    *n* × *n* orthogonal matrix, and *S* is a diagonal matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The columns of matrix *V* yield topics. These are principal vectors for the
    rows of *X*: that is, eigenvectors of *X^TX* or, equivalently, the covariance
    matrix of *X*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The successive elements of each topic vector (column in matrix *V*) tell us
    the contribution of corresponding terms to that topic. Each column is *n* × 1,
    depicting the contributions of the *n* terms in the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The diagonal elements of *S* tell us the weights (importance) of corresponding
    topics. These are the eigenvalues of *X^TX*: that is, principal values of the
    row vectors of *X*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspect the weights, and choose a cutoff. All topics below that weight are discarded—the
    corresponding columns of *V* are thrown away. This yields a matrix *V* with fewer
    columns but the same number of rows); these are the topic vectors of interest
    to us. We have reduced the dimensionality of the problem. If the number of retained
    topics is *t*, the reduced *V* is *m* × *t*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By projecting (multiplying) the original matrix *X* of document terms to this
    new matrix *V*, we get an *m* × *t* matrix of document topics (it has same number
    of rows as *X* but fewer columns). This is the projection of *X* to the topic
    space: that is, a topic-based representation of the document vectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rows of the document topic matrix will henceforth be taken as document representations.
    Document similarities will be computed by taking the cosine similarity of these
    rows rather than the rows of the original document term matrix. This cosine similarity,
    in the topic space, will capture many indirect connections that were not visible
    in the original input space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Table 4.1 Document matrix for the toy example dataset
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Violence | Gun | America | ⋯ | Roses |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **d**[0] | 0 | 0 | 0 | ⋯ | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **d**[1] | 1 | 1 | 1 | ⋯ | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **d**[2] | 2 | 2 | 0 | ⋯ | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **d**[3] | 3 | 3 | 0 | ⋯ | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **d**[4] | 5 | 5 | 0 | ⋯ | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **d**[5] | 0 | 1 | 0 | ⋯ | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **d**[6] | 1 | 0 | 0 | ⋯ | 0 |'
  prefs: []
  type: TYPE_TB
- en: 4.6.3 PyTorch code to perform LSA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following listing demonstrates how to compute the LSA for our toy dataset
    from table [4.1](#tab-doc_vector_dataset). Fully functional code for this section
    can be found at [http://mng.bz/E2Gd](http://mng.bz/E2Gd).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.8 Computing LSA
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Considers only four terms for simplicity
  prefs: []
  type: TYPE_NORMAL
- en: ② Document term matrix. Each row describes a document. Each column contains
    TF scores for one term. IDF is ignored for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: '③ Performs SVD on the doc-term matrix. Columns of the resulting matrix *V*
    correspond to topics. These are eigenvectors of *X^TX*: principal vectors of the
    doc-term matrix. A topic corresponds to the direction of maximum variance in the
    doc feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: ④ *S* indicates the diagonal matrix of principal values. These signify topic
    weights (importance). We choose a cut-off and discard all topics below that weight
    (dimensionality reduction). Only the first few columns of *V* are retained. Principal
    values (topic weights) for this dataset are shown in the output. Only one topic
    is retained in this example.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Elements of the topic vector show the contributions of corresponding terms
    to the topic.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Cosine similarity in the feature space fails to capture d, d6 similarity.
    LSA succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 4.6.4 PyTorch code to compute LSA and SVD on a large dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we have a set of 500 documents over a vocabulary of 3 terms. This is
    an unrealistically short vocabulary, but it allows us to easily visualize the
    space of document vectors. Each document vector is a 3 × 1 vector, and there are
    500 such vectors. Together they form a 500 × 3 data matrix *X*. In this dataset,
    the terms *x*0 and *x*1 are correlated: *x*0 occurs randomly between 0 and 100
    times in a document, and *x*1 occurs twice as many times as *x*0 except for small
    random fluctuations. The third term’s frequency varies between 0 and 5. From section
    [4.6](../Text/04.xhtml#sec-lsa), we know that *x*0, *x*1 together form a single
    topic, while *x*2 by itself forms another topic. We expect a principal component
    along each topic.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.9 creates the dataset, computes the SVD, plots the dataset, and shows
    the first two principal components. The third singular value is small compared
    to the first. We can ignore that dimension—it corresponds to the small random
    variation within the *x*0 − *x*1 topic. The singular values are printed out and
    also shown graphically along with the data points in figure [4.7](#fig-svd-lsa).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH04_F07_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 Latent semantic analysis. Note that the vertical axis line is actually
    much smaller than it appears to be in the diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 4.9 LSA using SVD
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '① 3D dataset: the first two axes are linearly correlated; the third axis has
    small near-zero random values.'
  prefs: []
  type: TYPE_NORMAL
- en: ② The third singular value is relatively small; we ignore it
  prefs: []
  type: TYPE_NORMAL
- en: ③ The first two principal vectors represent topics. Projecting data points on
    them yields document descriptors in terms of the two topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we studied several linear algebraic tools used in machine
    learning and data science:'
  prefs: []
  type: TYPE_NORMAL
- en: The direction (unit vector) that maximizes (minimizes) the quadratic form *x̂^TAx̂*
    is the eigenvector corresponding to the largest (smallest) eigenvalue of matrix
    *A*. The magnitude of the quadratic form when *x̂* is along those directions is
    the largest (smallest) eigenvalue of *A*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a set of points *X* = {![](../../OEBPS/Images/AR_x.png)^((0)), ![](../../OEBPS/Images/AR_x.png)^((1)),
    ![](../../OEBPS/Images/AR_x.png)^((2)), ⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}
    in an *n* + 1-dimensional space, we can define the mean vector and covariance
    matrix as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_04-13-a.png)'
  prefs: []
  type: TYPE_IMG
- en: The variance along an arbitrary direction (unit vector) *l̂* is *l̂* ^T*Cl̂*.
    This is a quadratic form. Consequently, the maximum (minimum) variance of a set
    of data points in multidimensional space occurs along the eigenvector corresponding
    to the largest (smallest) eigenvalue of the covariance matrix. This direction
    is called the first principal axis of the data. The subsequent eigenvectors, sorted
    in order of decreasing eigenvalues, are mutually orthogonal (perpendicular) and
    yield the subsequent direction of maximum variance. This technique is known as
    principal component analysis (PCA).
  prefs: []
  type: TYPE_NORMAL
- en: In many real-life cases, larger variances correspond to the true underlying
    pattern of the data, while smaller variances correspond to noise (such as measurement
    error). Projecting the data on the principal axes corresponding to the larger
    eigenvalues yields lower-dimensional data that is relatively noise-free. The projected
    data points also match the true underlying pattern more closely, yielding better
    insights. This is known as dimensionality reduction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singular value decomposition (SVD) allows us to decompose an arbitrary *m*
    × *n* matrix *A* as a product of three matrices: *A* = *U*Σ*V^T*, where *U*, *V*
    are orthogonal and Σ is diagonal. Matrix *V* has the eigenvectors of *A^TA* as
    its columns. *U* has eigenvectors of *AA^T* as columns. Σ has the eigenvalues
    of *A^TA* sorted in decreasing order) in its diagonal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SVD provides a numerically stable way to solve the linear system of equations
    *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png). In particular,
    for nonsquare matrices, it provides the closest approximations: that is, ![](../../OEBPS/Images/AR_x.png)
    that minimizes ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)||.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a dataset *X* whose rows are data vectors corresponding to individual
    instances and columns correspond to feature values, *X^TX* ields the covariance
    matrix. Thus eigenvectors of *X^TX* ield the data’s principal components. Since
    the SVD of *X* has eigenvectors of *X^TX* as columns of the matrix *V*, SVD is
    an effective way to compute PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using machine learning data science for document retrieval, the bag-of-words
    model represents documents with document vectors that contain the term frequency
    (number of occurrences) of each term in the document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF is a cosine similarity technique for document matching and retrieval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Latent semantic analysis (LSA) does topic modeling: we perform PCA on the document
    vectors to identify topics. Projecting document vectors onto topic axes allows
    LSA to see latent (indirect) similarities beyond the direct overlapping of terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
