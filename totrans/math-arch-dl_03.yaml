- en: 4 Linear algebraic tools in machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 机器学习中的线性代数工具
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Quadratic forms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二次型
- en: Applying principal component analysis (PCA) in data science
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据科学中应用主成分分析（PCA）
- en: Retrieving documents with a machine learning application
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习应用检索文档
- en: Finding patterns in large volumes of high-dimensional data is the name of the
    game in machine learning and data science. Data often appears in the form of large
    matrices (a toy example of this is shown in section [2.3](02.xhtml#sec-matrices)
    and also in equation [2.1](02.xhtml#eq-cat-brain-toy-training-dataset)). The rows
    of the data matrix represent feature vectors for individual input instances. Hence,
    the number of rows matches the count of observed input instances, and the number
    of columns matches the size of the feature vector—that is, the number of dimensions
    in the feature space. Geometrically speaking, each feature vector (that is, row
    of the data matrix) represents a point in feature space. These points are not
    distributed uniformly over the space. Rather, the set of points belonging to a
    specific class occupies a small subregion of that space. This leads to certain
    structures in the data matrices. Linear algebra provides us the tools needed to
    study matrix structures.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量高维数据中寻找模式是机器学习和数据科学中的关键。数据通常以大型矩阵的形式出现（例如，在[2.3](02.xhtml#sec-matrices)节中展示的一个玩具示例，以及方程[2.1](02.xhtml#eq-cat-brain-toy-training-dataset)中），数据矩阵的行代表单个输入实例的特征向量。因此，行数与观察到的输入实例数量相匹配，列数与特征向量的大小相匹配——即特征空间中的维度数。从几何学的角度来看，每个特征向量（即数据矩阵的行）代表特征空间中的一个点。这些点在空间中不是均匀分布的。相反，属于特定类别的点集占据该空间的一个小子区域。这导致数据矩阵中存在某些结构。线性代数为我们提供了研究矩阵结构所需工具。
- en: In this chapter, we study linear algebraic tools to analyze matrix structures.
    The chapter presents some intricate mathematics, and we encourage you to persevere
    through it, including the theorem proofs. An intuitive understanding the proofs
    will give you significantly better insights into the rest of the chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究线性代数工具来分析矩阵结构。本章展示了某些复杂的数学，我们鼓励您坚持不懈地学习，包括定理证明。对证明的直观理解将使您对本章的其余部分有更深刻的洞察。
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng.bz/aoYz](http://mng.bz/aoYz)
    in the form of fully functional and executable Jupyter notebooks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章节的完整PyTorch代码以完全功能化和可执行的Jupyter笔记本形式，可在[http://mng.bz/aoYz](http://mng.bz/aoYz)找到。
- en: 4.1 Distribution of feature data points and true dimensionality
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 特征数据点的分布和真实维度
- en: For instance, consider the problem of determining the similarity between documents.
    This is an important problem for document search companies like Google. Given
    a query document, the system needs to retrieve from an archive—in ranked order
    of similarity—documents that match the query document. To do this, we typically
    create a vector representation of each document. Then the dot product of the vectors
    representing a pair of documents can be used as a quantitative estimate of the
    similarity between the documents. Thus, each document is represented by a document
    descriptor vector in which every word in the vocabulary is associated with a fixed
    index in the vector. The value stored in that index position is the frequency
    (number of occurrences) of that word in the document.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑确定文档之间相似度的问题。这对于像谷歌这样的文档搜索公司来说是一个重要问题。给定一个查询文档，系统需要从存档中检索与查询文档匹配的文档，并按相似度排序。为此，我们通常为每个文档创建一个向量表示。然后，一对文档表示的向量的点积可以用作文档之间相似度的定量估计。因此，每个文档都由一个文档描述符向量表示，其中词汇表中的每个词都与向量中的一个固定索引相关联。该索引位置存储的值是该词在文档中的频率（出现次数）。
- en: NOTE Prepositions and conjunctions are typically excluded and singular; plural
    and other variants of words originating from the same stem are usually collapsed
    into one word.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通常省略介词和连词，使用单数形式；来自同一词根的复数和其他变体通常合并为一个词。
- en: 'Every word in the vocabulary gets its own dimension in the document space.
    If a word does not occur in a document, we put a zero at that word’s index location
    in the descriptor vector for that document. We store one descriptor vector for
    every document in the archive. In theory, the document descriptor is an extremely
    long vector: its length matches the size of the vocabulary of the documents in
    the system. But this vector only exists notionally. In practice, we do not explicitly
    store descriptor vectors in their entirety. We store a <word, frequency> pair
    for every unique word that occurs in a document—*but we do not explicitly store
    words that do not occur*. This is a *sparse representation* of a document vector.
    The corresponding *full representation* can be constructed from the sparse one
    whenever necessary. In documents, certain words often occur together (for example,
    *Michael* and *Jackson*, or *gun* and *violence*). For example, in a given set
    of documents, the number of occurrences of *gun* will more or less match the number
    of occurrences of *violence*: if one appears, the other also appears most of the
    time. For a descriptor vector or, equivalently, a point in a feature space representing
    a document, the value at the index position corresponding to the word *gun* will
    be more or less equal to that for the word *violence*. If we project those points
    on the hyperplane formed by the axes for these correlated words, all the points
    fall around a 45-degree straight line (whose equation is *x* = *y*), as shown
    in figure [4.1](#fig-lsa-dimred).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表中的每个词在文档空间中都有自己的维度。如果一个词在文档中未出现，我们在该文档描述向量中该词的索引位置放置一个零。我们为存档中的每个文档存储一个描述向量。理论上，文档描述向量是一个极其长的向量：其长度与系统中文档词汇表的大小相匹配。但这个向量只存在于概念上。在实践中，我们并不明确存储描述向量的全部内容。我们为文档中出现的每个唯一词存储一个<词，频率>对——*但我们不明确存储未出现的词*。这是文档向量的*稀疏表示*。相应的*完整表示*可以在必要时从稀疏表示中构建。在文档中，某些词经常一起出现（例如，*迈克尔*和*杰克逊*，或*枪*和*暴力*）。例如，在给定的一组文档中，*枪*出现的次数与*暴力*出现的次数大致相等：如果一个出现，另一个也大多数时候出现。对于一个描述向量或，等价地，表示文档的特征空间中的一个点，对应于词*枪*的索引位置上的值将大致等于对应于词*暴力*的值。如果我们将这些点投影到由这些相关词的轴形成的超平面上，所有点都围绕一个45度直线（其方程为*x*
    = *y*）分布，如图[4.1](#fig-lsa-dimred)所示。
- en: '![](../../OEBPS/Images/CH04_F01_Chaudhury.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1](../../OEBPS/Images/CH04_F01_Chaudhury.png)'
- en: Figure 4.1 Document descriptor space. Each word in the vocabulary corresponds
    to a separate dimension. Dots show projections of document feature vectors on
    the plane formed by the axes corresponding to the terms *gun* and *violence*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 文档描述空间。词汇表中的每个词对应一个独立的维度。点表示文档特征向量在由术语*枪*和*暴力*对应的轴形成的平面上的投影。
- en: In figure [4.1](#fig-lsa-dimred), all the points representing documents are
    concentrated near the 45-degree line, and the rest of the plane is unpopulated.
    Can we collapse the two axes defining that plane and replace them with the single
    line around which most data is concentrated? It turns out that yes, we can do
    this. Doing so reduces the number of dimensions in the data representation—we
    are replacing a pair of correlated dimensions with a single one—thereby simplifying
    the representation. This leads to lower storage costs and, more importantly, provides
    additional insights. We have effectively discovered a new topic called *gun-violence*
    from the documents.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[4.1](#fig-lsa-dimred)中，代表文档的所有点都集中在45度线附近，其余的平面都是空白的。我们能否将定义该平面的两个轴合并，并用围绕大多数数据集中的单一线替换它们？结果是，我们可以这样做。这样做减少了数据表示中的维度数——我们正在用一个维度替换一对相关维度——从而简化了表示。这导致存储成本降低，更重要的是，提供了额外的见解。我们实际上从文档中发现了一个新的主题，称为*枪支暴力*。
- en: as another example, consider a set of points in 3D, represented by coordinates
    *X*, *Y*, *Z*. If the *Z* coordinate is near zero for all the points, the data
    is concentrated around the *X*, *Y* plane. We can (and should) represent these
    points in two dimensions by projecting them onto the *Z* = 0 plane. Doing so approximates
    the positions of the points only slightly (they are projected onto a plane that
    they were close to in the first place). In a more realistic example, the data
    points may be clustered around an arbitrary plane in the 3D space (as opposed
    to the *Z* = 0 plane). We can still reduce the dimensionality of these data points
    to 2D by projecting on the plane they are close to.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，考虑一个由坐标 *X*，*Y*，*Z* 表示的 3D 点集。如果所有点的 *Z* 坐标都接近零，则数据集中在 *X*，*Y* 平面周围。我们可以（并且应该）通过将它们投影到
    *Z* = 0 平面上来用二维表示这些点。这样做只会略微近似点的位置（它们被投影到一个它们最初就接近的平面上）。在更现实的例子中，数据点可能聚集在 3D 空间中的任意平面上（而不是
    *Z* = 0 平面）。我们仍然可以通过在它们接近的平面上投影来将这些数据点的维度降低到 2D。
- en: 'In general, if a set of data points is distributed in a space so that the points
    are clustered around a lower-dimensional subspace within that space (such as a
    plane or line), we can project the points onto the subspace and perform a *dimensionality
    reduction* on the data. We effectively approximate the distances from the subspace
    with zero: since these distances are small by definition, the approximation is
    not too bad. Viewed another way, we eliminate smaller *from-subspace* variations
    and retain the larger *in-subspace* variations. The resulting representation is
    more compressed and also lends itself more easily to better analysis and insights
    as we have eliminated unimportant perturbations and are focusing on the main pattern.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果一组数据点在空间中分布，使得这些点围绕该空间中的一个低维子空间（如平面或直线）聚集，我们可以将这些点投影到子空间上，并对数据进行**降维**处理。我们有效地将子空间中的距离近似为零：因为这些距离按定义很小，所以近似并不太差。从另一个角度来看，我们消除了较小的**子空间内**的变异，并保留了较大的**子空间内**的变异。这种表示方式更加紧凑，并且更容易进行更好的分析和洞察，因为我们消除了不重要的扰动，并专注于主要模式。
- en: These ideas form the basis of the technique called *principal component analysis*
    (PCA). It is one of the most important tools in the repertoire of a data scientist
    and machine learning practitioner. These ideas also underlie the *latent semantic
    analysis* (LSA) technique for document retrieval—a fundamental approach for solving
    natural language processing (NLP) problems in machine learning. This chapter is
    dedicated to studying a set of methods leading to PCA and LSA. We examine a basic
    document retrieval system along with Python code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些思想构成了称为**主成分分析**（PCA）的技术的基础。它是数据科学家和机器学习实践者工具箱中最重要工具之一。这些思想也是文档检索技术**潜在语义分析**（LSA）的基础——这是解决机器学习中自然语言处理（NLP）问题的基本方法。本章致力于研究导致
    PCA 和 LSA 的一系列方法。我们检查了一个基本的文档检索系统以及相应的 Python 代码。
- en: 4.2 Quadratic forms and their minimization
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 二次型及其最小化
- en: Given a square symmetric matrix *A*, the scalar quantity *Q* = ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    is called a *quadratic form*. These are seen in various situations in machine
    learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个平方对称矩阵 *A*，标量量 *Q* = ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    被称为**二次型**。这些在机器学习的各种情况下都可以看到。
- en: For instance, recall the equation for a circle that we learned in high school
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，回想一下我们在高中学习的圆的方程
- en: (*x*[0]−*α*[0])² + (*x*[1]−*α*[1])² = *r*²
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (*x*[0]−*α*[0])² + (*x*[1]−*α*[1])² = *r*²
- en: where the center of the circle is (*α*[0], *α*[1]) and the radius is *r*. This
    equation can be rewritten as
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中圆心为 (*α*[0]，*α*[1])，半径为 *r*。这个方程可以重写为
- en: '![](../../OEBPS/Images/eq_04-00-a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_04-00-a.png)'
- en: If we denote the position vector ![](../../OEBPS/Images/eq_04-00-b.png) as ![](../../OEBPS/Images/AR_x.png)
    and the center of the circle as ![](../../OEBPS/Images/eq_04-00-c.png) as ![](../../OEBPS/Images/AR_alpha.png),
    the previous equation can be written compactly as
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将位置向量![图片](../../OEBPS/Images/eq_04-00-b.png)表示为![图片](../../OEBPS/Images/AR_x.png)，并将圆心![图片](../../OEBPS/Images/eq_04-00-c.png)表示为![图片](../../OEBPS/Images/AR_alpha.png)，则前面的方程可以紧凑地表示为
- en: (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_alpha.png))*^T***I**(![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_alpha.png))
    = *r*²
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_alpha.png))*^T***I**(![](../../OEBPS/Images/AR_x.png)−![](../../OEBPS/Images/AR_alpha.png))
    = *r*²
- en: 'Note that left hand side of this equation is a quadratic form. The original
    *x*[0], *x*[1]-based equation only works for two dimensions. The matrix based
    equation is dimension agnostic: it represents a hypersphere in an arbitrary-dimensional
    space. For a two-dimensional space, the two equations become identical.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个方程的左边是一个二次型。基于 *x*[0]，*x*[1] 的原始方程仅适用于二维。基于矩阵的方程是维度无关的：它表示任意维空间中的超球面。对于二维空间，两个方程变为相同。
- en: 'Now, consider the equation for an ellipse:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑椭圆的方程：
- en: '![](../../OEBPS/Images/eq_04-00-d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_04-00-d.png)'
- en: You can verify that this can be written compactly in matrix form as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以验证这可以紧凑地写成矩阵形式
- en: '![](../../OEBPS/Images/eq_04-00-e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_04-00-e.png)'
- en: or, equivalently,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，等价地，
- en: '![](../../OEBPS/Images/eq_04-01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_04-01.png)'
- en: Equation 4.1
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4.1
- en: 'where ![](../../OEBPS/Images/eq_04-01-a.png). Once again, the matrix representation
    is dimension independent. In other words, equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again)
    represents a hyperellipsoid. Note that if the ellipse axes are aligned with the
    coordinate axes, matrix *A* is diagonal. If we rotate the coordinate system, each
    position vector is rotated by an orthogonal matrix *R*. Equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again)
    is transformed as follows (we have used the rules for transposing the products
    of matrices from equation [2.10](02.xhtml#eq-mat-prod-transpose)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![图片](../../OEBPS/Images/eq_04-01-a.png)。再次，矩阵表示是维度无关的。换句话说，方程 [4.1](../Text/04.xhtml#eq-hyper-ellipse-again)
    表示一个超椭球体。注意，如果椭圆轴与坐标轴对齐，矩阵 *A* 是对角的。如果我们旋转坐标系，每个位置向量都由正交矩阵 *R* 旋转。方程 [4.1](../Text/04.xhtml#eq-hyper-ellipse-again)
    的变换如下（我们使用了方程 [2.10](02.xhtml#eq-mat-prod-transpose) 中的矩阵乘积转置规则）：
- en: '![](../../OEBPS/Images/eq_04-01-b.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_04-01-b.png)'
- en: Replacing *R^TAR* with *A*, we get the same equation as equation [4.1](../Text/04.xhtml#eq-hyper-ellipse-again),
    but *A* is no longer a diagonal matrix.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *R^TAR* 替换为 *A*，我们得到与方程 [4.1](../Text/04.xhtml#eq-hyper-ellipse-again) 相同的方程，但
    *A* 不再是对角矩阵。
- en: For a generic ellipsoid with arbitrary axes, *A* has nonzero off-diagonal terms
    but is still symmetric. Thus, the multidimensional hyperellipsoid is represented
    by a quadratic form. The hypersphere is a special case of this.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有任意轴的通用椭球体，矩阵 *A* 具有非零的非对角项，但仍然是对称的。因此，多维超椭球体由一个二次型表示。超球面是这个特殊情况之一。
- en: 'Quadratic forms are also found in the second term of the multidimensional Taylor
    expansion shown in equation [3.8](../Text/03.xhtml#eq-taylor-multidim): ![](../../OEBPS/Images/eq_04-01-c.png)
    is a quadratic form in the Hessian matrix. Another huge application of quadratic
    forms is PCA, which is so important that we devote a whole section to it section
    [4.4](../Text/04.xhtml#sec-pca)).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 二次型也出现在方程 [3.8](../Text/03.xhtml#eq-taylor-multidim) 所示的多维泰勒展开的第二项中：![图片](../../OEBPS/Images/eq_04-01-c.png)
    是Hessian矩阵中的二次型。二次型还有另一个巨大的应用，即PCA，它如此重要，以至于我们专门用一节来介绍它 [4.4](../Text/04.xhtml#sec-pca))。
- en: 4.2.1 Minimizing quadratic forms
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 最小化二次型
- en: An important question is, what choice of ![](../../OEBPS/Images/AR_x.png) maximizes
    or minimizes the quadratic form? For instance, because the quadratic form is part
    of the multidimensional Taylor series, we need to minimize quadratic forms when
    we want to determine the best direction to move in to minimize the loss *L*(![](../../OEBPS/Images/AR_x.png)).
    Later, we will see that this question also lies at the heart of PCA computation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的问题是，什么选择 ![图片](../../OEBPS/Images/AR_x.png) 可以最大化或最小化二次型？例如，因为二次型是多维泰勒级数的一部分，当我们想要确定最佳移动方向以最小化损失
    *L*(![图片](../../OEBPS/Images/AR_x.png)) 时，我们需要最小化二次型。稍后，我们将看到这个问题也是PCA计算的核心。
- en: 'If ![](../../OEBPS/Images/AR_x.png) is a vector with arbitrary length, we can
    make *Q* arbitrarily big or small by simply changing the length of ![](../../OEBPS/Images/AR_x.png).
    Consequently, optimizing *Q* with arbitrary length ![](../../OEBPS/Images/AR_x.png)
    is not a very interesting problem: rather, we want to know which *direction* of
    ![](../../OEBPS/Images/AR_x.png) optimizes *Q*. For the rest of this section,
    we discuss quadratic forms with unit vectors *Q* = *x̂^TAx̂* recall that *x̂*
    denotes a unit-length vector satisfying *x̂^Tx̂* = ||*x̂*||² = 1). Equivalently,
    we could use a different flavor, *Q* = *![](../../OEBPS/Images/AR_x.png)^TA*![](../../OEBPS/Images/AR_x.png)/![](../../OEBPS/Images/AR_x.png)^T![](../../OEBPS/Images/AR_x.png),
    but we will use the former expression here. We are essentially searching over
    all possible directions *x̂*, examining which direction minimizes *Q* = *x̂^TAx̂*.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](../../OEBPS/Images/AR_x.png) 是一个任意长度的向量，我们可以通过简单地改变 ![](../../OEBPS/Images/AR_x.png)
    的长度来使 *Q* 随意变大或变小。因此，使用任意长度 ![](../../OEBPS/Images/AR_x.png) 优化 *Q* 并不是一个很有趣的问题：相反，我们想知道
    ![](../../OEBPS/Images/AR_x.png) 的哪个 *方向* 优化 *Q*。在本节的其余部分，我们将讨论具有单位向量 *Q* = *x̂^TAx̂*
    的二次型（记住 *x̂* 表示一个单位长度的向量，满足 *x̂^Tx̂* = ||*x̂*||² = 1）。等价地，我们可以使用不同的形式，*Q* = *![](../../OEBPS/Images/AR_x.png)^TA*![](../../OEBPS/Images/AR_x.png)/![](../../OEBPS/Images/AR_x.png)^T![](../../OEBPS/Images/AR_x.png)，但我们将在这里使用前者。我们实际上是在搜索所有可能的方向
    *x̂*，检查哪个方向最小化 *Q* = *x̂^TAx̂*。
- en: Using matrix diagonalization (section [2.15](02.xhtml#sec-mat-diagonalization)),
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵对角化（第 [2.15](02.xhtml#sec-mat-diagonalization) 节），
- en: '*Q* = *x̂^TAx̂* = *x̂^TSΛS^Tx̂*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q* = *x̂^TAx̂* = *x̂^TSΛS^Tx̂*'
- en: where *S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]
     …  *![](../../OEBPS/Images/AR_e.png)[n]*] is the matrix with eigenvectors of
    *A* as its columns and Λ is a diagonal matrix with the eigenvalues of *A* on the
    diagonal and 0 everywhere else. Substituting
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *S* = [![](../../OEBPS/Images/AR_e.png)[1]   ![](../../OEBPS/Images/AR_e.png)[2]
     …  *![](../../OEBPS/Images/AR_e.png)[n]*] 是以 *A* 的特征向量为列的矩阵，Λ 是一个对角矩阵，其对角线上的元素是
    *A* 的特征值，其余位置都是 0。代入
- en: '*ŷ* = *S^Tx̂*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *S^Tx̂*'
- en: we get
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到
- en: '*Q* = *x̂^TAx̂* = *x̂^TS*Λ*S^Tx̂* = *ŷ^T*Λ*ŷ*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q* = *x̂^TAx̂* = *x̂^TS*Λ*S^Tx̂* = *ŷ^T*Λ*ŷ*'
- en: Equation 4.2
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4.2
- en: 'Note that since *A* is symmetric, its eigenvectors are orthogonal. This implies
    that *S* is an orthogonal matrix: that is, *S^TS* = *SS^T* = **I**. Recall from
    section [2.14.2.1](02.xhtml#orth-len-preserv) that for an orthogonal matrix *S*,
    the transformation *S^Tx̂* is length preserving. Consequently, *ŷ* = *S^Tx̂* is
    a unit-length vector. To be precise,'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于 *A* 是对称的，其特征向量是正交的。这表明 *S* 是一个正交矩阵：即，*S^TS* = *SS^T* = **I**。回顾第 [2.14.2.1](02.xhtml#orth-len-preserv)
    节，对于正交矩阵 *S*，变换 *S^Tx̂* 保持长度。因此，*ŷ* = *S^Tx̂* 是一个单位长度的向量。为了更精确，
- en: '||*ŷ*||² = ||*S^Tx̂*||² = (*S^Tx̂*)*^T*(*S^Tx̂*) = *x̂^TSS^Tx̂* = *x̂^Tx̂*
    = 1 since *SS^T* = **I**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '||*ŷ*||² = ||*S^Tx̂*||² = (*S^Tx̂*)*^T*(*S^Tx̂*) = *x̂^TSS^Tx̂* = *x̂^Tx̂*
    = 1 因为 *SS^T* = **I**'
- en: So, expanding the right-hand side of equation [4.2](#eq-quad-form), we get
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，展开方程 [4.2](#eq-quad-form) 的右侧，我们得到
- en: '![](../../OEBPS/Images/eq_04-03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-03.png)'
- en: Equation 4.3
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4.3
- en: We can assume that the eigenvalues are sorted in decreasing order of magnitude
    (if not, we can always renumber them).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设特征值按大小递减的顺序排列（如果不是，我们可以重新编号它们）。
- en: 'Consider this *lemma* (small proof): The quantity Σ*[i]^n*[= 1] *λ[i]* *y[i]*²,
    where Σ*[i]^n*[= 1] *y[i]*² = 1 and *λ*[1] ≥ *λ*[2] ≥ ⋯ *λ[n]*, attains its maximum
    value when *y*[1] = 1, *y*[2] = ⋯ *y[n]* = 0.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个 *引理*（小证明）：当 Σ*[i]^n*[= 1] *y[i]*² = 1 且 *λ*[1] ≥ *λ*[2] ≥ ⋯ *λ[n]* 时，量
    Σ*[i]^n*[= 1] *λ[i]* *y[i]*² 达到其最大值，其中 *y*[1] = 1，*y*[2] = ⋯ *y[n]* = 0。
- en: An *intuitive proof* follows. If possible, let that the maximum value occur
    at some other value of *ŷ*. We are constrained by the fact that *ŷ* is an unit
    vector, so we must maintain Σ*[i]^n*[= 1] *y[i]*² = 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 *直观的证明*。如果可能，假设最大值出现在 *ŷ* 的某个其他值。我们受限于 *ŷ* 是一个单位向量，因此我们必须保持 Σ*[i]^n*[=
    1] *y[i]*² = 1。
- en: In particular, none of the elements of *ŷ* can exceed 1. If we reduce the first
    term from 1 to a smaller value, say √1-*ϵ*, some other element(s) must go up by
    an equivalent amount to compensate (i.e., maintain the unit length property).
    Accordingly, suppose the hypothesized *ŷ* maximizing *Q* is given by
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，*ŷ* 的任何元素都不能超过 1。如果我们把第一个元素从 1 减少到一个更小的值，比如说 √1-*ϵ*，其他某个元素必须相应增加以补偿（即，保持单位长度属性）。相应地，假设最大化
    *Q* 的假设 *ŷ* 是
- en: '![](../../OEBPS/Images/eq_04-03-a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-03-a.png)'
- en: where *δ* > 0.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *δ* > 0。
- en: What happens if we transfer the entire mass from the later term to the first
    term so that
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将整个质量从后一项转移到第一项，会发生什么？
- en: '![](../../OEBPS/Images/eq_04-03-b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-03-b.png)'
- en: Doing this does not alter the length of *ŷ* as the sum of the squares of the
    first and the other term remains 1 − *ϵ* + *δ*. But the value of *Q* = Σ*[i]^n*[=
    1] *λ[i]*² is higher in the second case (where ![](../../OEBPS/Images/AR_y.png)[1]
    has been beefed up at the expense of another term), since *λ*[1](1−*ϵ* + *δ*)
    > *λ*[1](1−*ϵ*) + *λ[j]δ* for any *j* > 1 (since, *λ*[1] > *λ*[2]⋯ by assumption).
    Thus, whenever we have less than 1 in the first term and greater than zero in
    some other term, we can increase *Q* without losing the unit length property of
    *ŷ* by transferring the entire mass to the first term.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事不会改变 *ŷ* 的长度，因为第一项和其他项的平方和仍然是 1 − *ϵ* + *δ*。但是，*Q* = Σ*[i]^n*[= 1] *λ[i]*²
    的值在第二种情况下更高（其中 ![](../../OEBPS/Images/AR_y.png)[1] 以牺牲另一个项为代价而增强），因为对于任何 *j* >
    1（由于，*λ*[1] > *λ*[2]⋯ 的假设），*λ*[1](1−*ϵ* + *δ*) > *λ*[1](1−*ϵ*) + *λ[j]δ*。因此，每当第一项小于
    1 而其他某些项大于零时，我们可以通过将全部质量转移到第一项来增加 *Q*，而不会失去 *ŷ* 的单位长度属性。
- en: This means to maximize the right hand side of equation [4.3](#quad-form-trans),
    we must have 1 as the first element (corresponding to the largest eigenvalue)
    of the unit vector *ŷ* and zeros everywhere else. Anything else violates the condition
    that the corresponding quadratic form *Q* = Σ*[i]^n*[= 1] *λ[i]*² is a maximum.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着为了最大化方程 [4.3](#quad-form-trans) 的右侧，我们必须让单位向量 *ŷ* 的第一个元素（对应于最大特征值）为 1，其他地方都为
    0。任何其他情况都会违反条件，即相应的二次型 *Q* = Σ*[i]^n*[= 1] *λ[i]*² 是一个最大值。
- en: Thus we have established that the maximum of *Q* occurs at ![](../../OEBPS/Images/eq_04-03-c.png).
    The corresponding *x̂* = *Sŷ* = ![](../../OEBPS/Images/AR_e.png)[1] - the eigenvector
    corresponding to the largest eigenvalue of *A*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经证明了 *Q* 的最大值出现在 ![](../../OEBPS/Images/eq_04-03-c.png)。相应的 *x̂* = *Sŷ*
    = ![](../../OEBPS/Images/AR_e.png)[1] - 对应于 *A* 的最大特征值的特征向量。
- en: Thus, the quadratic form *Q* = *x̂^TAx̂* attains its maximum when *x̂* is along
    the eigenvector corresponding to the largest eigenvalue of *A*. The corresponding
    maximum *Q* is equal to the largest eigenvalue of *A*. Similarly, the minimum
    of the quadratic form occurs when *x̂* is along the eigenvector corresponding
    to the smallest eigenvalue.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 *x̂* 沿着对应于 *A* 的最大特征值的特征向量时，二次型 *Q* = *x̂^TAx̂* 达到其最大值。相应的最大 *Q* 等于 *A*
    的最大特征值。类似地，当 *x̂* 沿着对应于最小特征值的特征向量时，二次型的最小值出现。
- en: As stated above, many machine learning problems boil down to minimizing a quadratic
    form. We will study a few of them in later sections.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，许多机器学习问题可以归结为最小化一个二次型。我们将在后面的章节中研究其中的一些。
- en: 4.2.2 Symmetric positive (semi)definite matrices
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 对称正（半）定矩阵
- en: A square symmetric *n* × *n* matrix *A* is positive semidefinite if and only
    if
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对称的 *n* × *n* 矩阵 *A* 是正半定的，当且仅当
- en: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) ≥ 0 ∀![](../../OEBPS/Images/AR_x.png)
    ∈ ℝ*^n*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) ≥ 0 ∀![](../../OEBPS/Images/AR_x.png)
    ∈ ℝ*^n*'
- en: In other words, a positive semidefinite matrix yields a non-negative quadratic
    form with all *n* × 1 vectors ![](../../OEBPS/Images/AR_x.png). If we disallow
    the equality, we get symmetric positive definite matrices. Thus a square symmetric
    *n* × *n* matrix *A* is positive definite if and only if
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，一个正半定矩阵产生一个非负的二次型，其中所有 *n* × 1 向量 ![](../../OEBPS/Images/AR_x.png) 都是非负的。如果我们不允许等式成立，我们得到对称正定矩阵。因此，一个对称的
    *n* × *n* 矩阵 *A* 是正定的，当且仅当
- en: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) > 0 ∀![](../../OEBPS/Images/AR_x.png)
    ∈ ℝ*^n*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png) > 0 ∀![](../../OEBPS/Images/AR_x.png)
    ∈ ℝ*^n*'
- en: From equations [4.2](#eq-quad-form) and [4.3](#quad-form-trans), *Q* is positive
    or zero if all *λ[i]*s are positive or zero (since the ![](../../OEBPS/Images/AR_y.png)*[i]*²s
    are non-negative). Hence, symmetric positive (semi)definiteness is equivalent
    to the condition that all eigenvalues of the matrix are greater than (or equal
    to) zero.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 [4.2](#eq-quad-form) 和 [4.3](#quad-form-trans) 可以看出，如果所有 *λ[i]* 都是正的或零，则
    *Q* 是正的或零（因为 ![](../../OEBPS/Images/AR_y.png)*[i]*²s 都是非负的）。因此，对称正（半）定性与矩阵的所有特征值都大于（或等于）零的条件等价。
- en: 4.3 Spectral and Frobenius norms of a matrix
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 矩阵的谱范数和Frobenius范数
- en: A vector is an entity with a magnitude and direction. The norm ||![](../../OEBPS/Images/AR_x.png)||
    of a vector ![](../../OEBPS/Images/AR_x.png) represents its magnitude. Is there
    an equivalent notion for matrices? The answer is yes, and we will study two such
    ideas.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是一个具有模和方向的实体。向量 ![](../../OEBPS/Images/AR_x.png) 的范数 ||![](../../OEBPS/Images/AR_x.png)||
    表示其模。对于矩阵是否有等价的概念？答案是肯定的，我们将研究两个这样的想法。
- en: 4.3.1 Spectral norms
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 特征范数
- en: In section [2.5.4](02.xhtml#subsection-vector_length), we saw that the length
    (aka magnitude) of a vector ![](../../OEBPS/Images/AR_x.png) is ||![](../../OEBPS/Images/AR_x.png)||
    = ![](../../OEBPS/Images/AR_x.png)*^T*![](../../OEBPS/Images/AR_x.png). Is there
    an equivalent notion of magnitude for a matrix *A*?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [2.5.4](02.xhtml#subsection-vector_length) 节中，我们看到了向量的长度（也称为模）![](../../OEBPS/Images/AR_x.png)
    是 ||![](../../OEBPS/Images/AR_x.png)|| = ![](../../OEBPS/Images/AR_x.png)*^T*![](../../OEBPS/Images/AR_x.png).
    对于矩阵 *A* 是否存在一个等价的模的概念？
- en: Well, a matrix can be viewed as an amplifier of a vector. The matrix *A* amplifies
    the vector ![](../../OEBPS/Images/AR_x.png) to ![](../../OEBPS/Images/AR_b.png)
    = *A*![](../../OEBPS/Images/AR_x.png). So we can take the maximum possible value
    of ||*A*![](../../OEBPS/Images/AR_x.png)|| over all possible ![](../../OEBPS/Images/AR_x.png);
    that is a measure for the magnitude of *A*. Of course, if we consider arbitrary-length
    vectors, we can make ![](../../OEBPS/Images/AR_b.png) arbitrarily large by simply
    scaling ![](../../OEBPS/Images/AR_x.png) for any *A*. That is uninteresting. Rather,
    we want to examine which direction of ![](../../OEBPS/Images/AR_x.png) is amplified
    most and by how much.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，一个矩阵可以被看作是一个向量的放大器。矩阵 *A* 将向量 ![](../../OEBPS/Images/AR_x.png) 放大到 ![](../../OEBPS/Images/AR_b.png)
    = *A*![](../../OEBPS/Images/AR_x.png)。因此，我们可以取所有可能的 ![](../../OEBPS/Images/AR_x.png)
    中 ||*A*![](../../OEBPS/Images/AR_x.png)|| 的最大可能值；这是 *A* 的模的一个度量。当然，如果我们考虑任意长度的向量，我们可以通过简单地缩放
    ![](../../OEBPS/Images/AR_x.png) 来使 ![](../../OEBPS/Images/AR_b.png) 随意增大，对于任何
    *A* 来说这都是无趣的。相反，我们想要检查 ![](../../OEBPS/Images/AR_x.png) 的哪个方向被放大得最多，以及放大了多少。
- en: 'We examine this question with unit vectors *x̂*: what is the maximum (or minimum)
    value of ||*Ax̂*||, and what direction *x̂* materializes it? The quantity'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用单位向量 *x̂* 来考察这个问题：||*Ax̂*|| 的最大（或最小）值是多少，以及 *x̂* 的哪个方向实现了它？这个量
- en: '![](../../OEBPS/Images/eq_04-03-d.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-03-d.png)'
- en: is known as the *spectral norm* of the matrix *A*. Note that *A*![](../../OEBPS/Images/AR_x.png)
    is a vector and ||*A*![](../../OEBPS/Images/AR_x.png)||[2] is its length. (We
    will sometimes drop the subscript 2 and denote the spectral norm as ||*A*||.)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为矩阵 *A* 的 *谱范数*。请注意，*A*![](../../OEBPS/Images/AR_x.png) 是一个向量，||*A*![](../../OEBPS/Images/AR_x.png)||[2]
    是它的长度。（我们有时会省略下标 2 并将谱范数表示为 ||*A*||。）
- en: Now consider the vector *Ax̂*. Its magnitude is
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑向量 *Ax̂*。它的模是
- en: '||*Ax̂*|| = (*Ax̂*)*^T*(*Ax̂*) = *x̂^TA^TAx̂*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '||*Ax̂*|| = (*Ax̂*)*^T*(*Ax̂*) = *x̂^TA^TAx̂*'
- en: This is a quadratic form. From section [4.2](../Text/04.xhtml#sec-quadratic-form),
    we know it will be maximized (minimized) when *x̂* s aligned with the largest
    smallest) eigenvalue of *A^TA*. Thus the spectral norm is given by the largest
    eigenvalue of *A^TA*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二次型。从第 [4.2](../Text/04.xhtml#sec-quadratic-form) 节，我们知道当 *x̂* 与 *A^TA*
    的最大（最小）特征值对齐时，它将被最大化（最小化）。因此，谱范数由 *A^TA* 的最大特征值给出
- en: '![](../../OEBPS/Images/eq_04-04.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-04.png)'
- en: Equation 4.4
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4.4
- en: where *σ*[1] is the largest eigenvalue of *A^TA*. It is also (the square of)
    the largest singular value of *A*. We will see *σ*[1] again in section [4.5](../Text/04.xhtml#sec-svd),
    when we study singular value decomposition (SVD).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *σ*[1] 是 *A^TA* 的最大特征值。它也是 *A* 的最大奇异值（平方）。我们将在第 [4.5](../Text/04.xhtml#sec-svd)
    节再次看到 *σ*[1]，当我们研究奇异值分解（SVD）时。
- en: 4.3.2 Frobenius norms
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 Frobenius 范数
- en: An alternative measure for the magnitude of a matrix is the Frobenius norm,
    defined as
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵模的一个替代度量是 Frobenius 范数，定义为
- en: '![](../../OEBPS/Images/eq_04-05.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-05.png)'
- en: Equation 4.5
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4.5
- en: In other words, it is the root mean square of all the matrix elements.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它是所有矩阵元素的均方根。
- en: It can be proved that the Frobenius norm is equal to the root mean square of
    the sum of all the singular values (eigenvalues of *A^TA*) of the matrix
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明 Frobenius 范数等于矩阵所有奇异值（*A^TA* 的特征值）之和的均方根
- en: '![](../../OEBPS/Images/eq_04-06.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06.png)'
- en: Equation 4.6
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4.6
- en: 4.4 Principal component analysis
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 主成分分析
- en: Suppose we have a set of numbers, *X* = {*x*^((0)), *x*^((1)),⋯, *x*^((*n*))}.
    We want to get a sense of how tightly packed these points are. In other words,
    we want to measure the *spread* of these numbers. Figure [4.2](#fig-pca-1d) shows
    such a distribution.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组数字集合，*X* = {*x*^((0)), *x*^((1)),⋯, *x*^((*n*))}。我们想要了解这些点是如何紧密排列的。换句话说，我们想要测量这些数字的*分布范围*。图
    [4.2](#fig-pca-1d) 展示了这样的分布。
- en: '![](../../OEBPS/Images/CH04_F02_Chaudhury.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F02_Chaudhury.png)'
- en: 'Figure 4.2 A 1D distribution of points. The distance between extreme points
    is *not* a fair representation of the spread of points: the distribution is not
    uniform, and the extreme points are far from the others. Most points are within
    a more tightly packed region.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 点的 1D 分布。极端点之间的距离*不是*点分布范围的公平表示：分布不是均匀的，极端点与其他点相距甚远。大多数点都在一个更紧密排列的区域中。
- en: 'Note that the points need not be uniformly distributed. In particular, the
    extreme points (*x[max]*, *x[min]*) may be far from most other points (as in figure
    [4.2](#fig-pca-1d)). Thus, (*x[max]*– *x[min]*)/(*n*+1) is not a fair representation
    of the average spread of points here. Most points are within a more tightly packed
    region. The statistically sensible way to obtain the spread is to first obtain
    the mean:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，点不必均匀分布。特别是，极端点 (*x[max]*, *x[min]*) 可能与其他大多数点相距甚远（如图 [4.2](#fig-pca-1d)）。因此，(*x[max]*–
    *x[min]*)/(*n*+1) 并不是这里点平均分布范围的公平表示。大多数点都在一个更紧密排列的区域中。获取分布的统计合理方法是首先获得均值：
- en: '![](../../OEBPS/Images/eq_04-06-a.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06-a.png)'
- en: 'Then obtain the average distance of the numbers from the mean:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后获得数字与均值的平均距离：
- en: '![](../../OEBPS/Images/eq_04-06-b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06-b.png)'
- en: (If we want to, we can take the square root and use *σ*, but it is often not
    necessary to incur that extra computational burden). This scalar quantity, *σ*,
    is a good measure of the mean packing density or spread of the points in 1D. You
    may recognize that the previous equation is nothing but the famous variance formula
    from statistics. Can we extend the notion to higher-dimensional data?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （如果我们愿意，我们可以取平方根并使用*σ*，但通常没有必要承担额外的计算负担）。这个标量量*σ*是衡量1D中点的平均排列密度或分布范围的好方法。你可能已经注意到，前面的方程不过是统计学中著名的方差公式。我们能否将这个概念扩展到高维数据？
- en: Let’s first examine the idea in two dimensions. As usual, we name our coordinate
    axes *X*[0], *X*[1], and so on, instead of *X*, *Y*, to facilitate the extension
    to multiple dimensions. An individual 2D data point is denoted ![](../../OEBPS/Images/eq_04-06-c.png).
    The dataset is {![](../../OEBPS/Images/AR_x.png)^((0)), ![](../../OEBPS/Images/AR_x.png)^((1)),
    ⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先在二维中考察这个想法。像往常一样，我们用 *X*[0]，*X*[1]，等等来命名坐标轴，而不是 *X*，*Y*，以便于扩展到多维度。一个单独的
    2D 数据点表示为 ![](../../OEBPS/Images/eq_04-06-c.png)。数据集是 {![](../../OEBPS/Images/AR_x.png)^((0)),
    ![](../../OEBPS/Images/AR_x.png)^((1)), ⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}。
- en: 'The mean is straightforward. Instead of one means, we have two:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 均值是直接的。我们有两个均值，而不是一个：
- en: '![](../../OEBPS/Images/eq_04-06-d.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06-d.png)'
- en: 'Thus we now have a mean *vector*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在有一个均值*向量*：
- en: '![](../../OEBPS/Images/eq_04-06-e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06-e.png)'
- en: 'Now let’s do the variance. The immediate problem we face is that there are
    infinite possible directions in the 2D plane. We can measure variance along any
    of them, and it will be different for each choice. We can, of course, find the
    variance along the *X*[0] and *X*[1] axes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来做方差。我们面临的一个直接问题是，在二维平面上有无限多个可能的方向。我们可以沿着其中的任何一个测量方差，并且对于每个选择都会不同。当然，我们可以沿着
    *X*[0] 和 *X*[1] 轴测量方差：
- en: '![](../../OEBPS/Images/eq_04-06-f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06-f.png)'
- en: '*σ*[00] and *σ*[11] tells us the variance along *only one* of the axes *X*[0]
    and *X*[1], respectively. But in general, there will be joint variation along
    both axes. To deal with joint variation, let’s introduce a cross term:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*σ*[00] 和 *σ*[11] 告诉我们沿着 *仅一个* 轴 *X*[0] 和 *X*[1] 的方差。但在一般情况下，两个轴上都会存在联合变化。为了处理联合变化，让我们引入一个交叉项：'
- en: '![](../../OEBPS/Images/eq_04-06-g.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06-g.png)'
- en: 'These equations can be written compactly in matrix vector notation:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程可以用矩阵向量表示法简洁地写出：
- en: '![](../../OEBPS/Images/eq_04-06-h.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-06-h.png)'
- en: NOTE In the expression for *C*, we are *not* taking the dot product of the vectors
    (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))
    and (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png)).
    The dot product would be (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))*^T*(![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png)).
    Here, the second element of the product is transposed, not the first. Consequently,
    the result is a matrix. The dot product would yield a scalar.)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在 *C* 的表达式中，我们并没有取向量的点积 (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))
    和 (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))。点积将是
    (![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))*^T*(![](../../OEBPS/Images/AR_x.png)^((*i*))−![](../../OEBPS/Images/AR_micro.png))。在这里，乘积的第二元素被转置，而不是第一元素。因此，结果是矩阵。点积将产生一个标量。）
- en: The previous equations are general, meaning they can be extended to any dimension.
    To be precise, given a set of *n* multidimensional data points *X* = {![](../../OEBPS/Images/AR_x.png)^((0)),
    ![](../../OEBPS/Images/AR_x.png)^((1)),⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))},
    we can define
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程式是通用的，这意味着它们可以扩展到任何维度。更准确地说，给定一组 *n* 维多维数据点 *X* = {![](../../OEBPS/Images/AR_x.png)^((0)),
    ![](../../OEBPS/Images/AR_x.png)^((1)),⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*)))，我们可以定义
- en: '![](../../OEBPS/Images/eq_04-07.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.7](../../OEBPS/Images/eq_04-07.png)'
- en: Equation 4.7
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 4.7
- en: '![](../../OEBPS/Images/eq_04-08.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.8](../../OEBPS/Images/eq_04-08.png)'
- en: Equation 4.8
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 4.8
- en: Note how the mean has become a vector (it was a scalar for 1D data) and the
    scalar variance of 1D, *σ*, has become a matrix *C*. This matrix is called the
    *covariance matrix*. The (*n*+1)-dimensional mean and covariance matrix can also
    be defined a
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到均值已经变成了一个向量（对于一维数据，它是一个标量）并且一维的标量方差 *σ* 已经变成了一个矩阵 *C*。这个矩阵被称为 *协方差矩阵*。(*n*+1)-维均值和协方差矩阵也可以定义为
- en: '![](../../OEBPS/Images/eq_04-09.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.8](../../OEBPS/Images/eq_04-09.png)'
- en: Equation 4.9
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 4.9
- en: where
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../../OEBPS/Images/eq_04-10.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.10](../../OEBPS/Images/eq_04-10.png)'
- en: Equation 4.10
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 4.10
- en: For *i* = *j*, *σ[ii]* is essentially the variance of the data along the *i*th
    dimension. Thus the diagonal elements of matrix *C* contain the variance along
    the coordinate axes. Off-diagonal elements correspond to cross-covariances.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *i* = *j*，*σ[ii]* 实际上是数据沿 *i* 维的方差。因此，矩阵 *C* 的对角元素包含了沿坐标轴的方差。非对角元素对应于交叉协方差。
- en: NOTE Equations [4.8](#eq-covar-mat-0) and [4.9](#eq-covar-mat) are equivalent.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：方程式 [4.8](#eq-covar-mat-0) 和 [4.9](#eq-covar-mat) 是等价的。
- en: 4.4.1 Direction of maximum spread
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 最大扩展方向
- en: What is the direction of maximum spread/variance? Let’s first consider an arbitrary
    direction specified by the unit vector *l̂*. Recalling that the component of any
    vector along a direction is yielded by the dot product of the vector with the
    unit direction vector, the components of the data points along *l̂* are given
    by
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最大扩展/方差的指向是什么？首先考虑一个由单位向量 *l̂* 指定的任意方向。回忆起任何向量沿一个方向的分量是由向量与单位方向向量的点积给出的，*l̂*
    沿 *l̂* 的数据点的分量由
- en: '*X* ^′ = {*l̂* ^T![](../../OEBPS/Images/AR_x.png)^((0)), *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((1)),⋯,
    *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((*n*))}'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* ^′ = {*l̂* ^T![](../../OEBPS/Images/AR_x.png)^((0)), *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((1)),⋯,
    *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((*n*))}'
- en: NOTE Remember figure [2.8b](02.xhtml#fig-multi-dim-lineeq), which showed that
    the component of one vector along another is given by the dot product between
    them? *l̂* ^T![](../../OEBPS/Images/AR_x.png)^((*i*)) are dot products and hence
    scalar values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：记住图 [2.8b](02.xhtml#fig-multi-dim-lineeq)，它显示了另一个向量中一个向量的分量是由它们之间的点积给出的？*l̂*
    ^T![](../../OEBPS/Images/AR_x.png)^((*i*)) 是点积，因此是标量值。
- en: The spread along direction *l̂* is given by the variance of the scalar values
    in *X* ^′. The mean of the values in *X* ^′ is given by
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 沿方向 *l̂* 的扩展由 *X* ^′ 中标量值的方差给出。*X* ^′ 中值的均值由
- en: '![](../../OEBPS/Images/eq_04-10-b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.10-b](../../OEBPS/Images/eq_04-10-b.png)'
- en: and the variance is
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以及方差为
- en: '![](../../OEBPS/Images/eq_04-10-c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.10-c](../../OEBPS/Images/eq_04-10-c.png)'
- en: 'Note that *C* ^′ = *l̂* ^T*Cl̂* is the variance of the data components along
    the direction *l̂*. As such, it represents the spread of the data along that direction.
    What is the direction *l̂* along which this spread *l̂* ^T*Cl̂* is maximal? It
    is the direction *l̂* that maximizes *C* ^′ = *l̂* ^T*Cl̂*. This maximizing direction
    can be identified using the quadratic form optimization technique we discussed
    in [4.2](../Text/04.xhtml#sec-quadratic-form). Applying that, we have the following
    results:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 *C* ^′ = *l̂* ^T*Cl̂* 是数据成分沿方向 *l̂* 的方差。因此，它代表了数据沿该方向的分布范围。沿着哪个方向 *l̂*，这种分布
    *l̂* ^T*Cl̂* 达到最大？这是最大化 *C* ^′ = *l̂* ^T*Cl̂* 的方向 *l̂*。这个最大化方向可以通过我们讨论过的二次型优化技术来识别，即[4.2](../Text/04.xhtml#sec-quadratic-form)。应用该技术，我们得到以下结果：
- en: Variance is maximal when *l̂* is along the eigenvector corresponding to the
    largest eigenvalue of the covariance matrix *C*. This direction is called the
    *first principal axis* of the multidimensional data.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *l̂* 沿着协方差矩阵 *C* 的最大特征值对应的特征向量时，方差达到最大。这个方向被称为多维数据的*第一主轴*。
- en: The components of the data vectors along the principal axis are known as *first
    principal components*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着主轴的数据向量的分量被称为*第一主成分*。
- en: The value of the variance along the first principal axis, given by the corresponding
    eigenvalue of the covariance matrix, is called the *first principal value*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着第一主轴的方差值，由协方差矩阵的对应特征值给出，被称为*第一主值*。
- en: The second principal axis is the eigenvector of the covariance matrix corresponding
    to the second largest eigenvalue of the covariance matrix. Second principal components
    and values are defined likewise.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二主轴是协方差矩阵对应于第二大的特征值的特征向量。第二主成分和值被定义为类似。
- en: The principal axes are orthogonal to each other because they are eigenvectors
    of the symmetric covariance matrix.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主轴彼此正交，因为它们是对称协方差矩阵的特征向量。
- en: What is the practical significance of PCA? Why would we like to know the direction
    along which the spread is maximum for a point distribution? Sections [4.4.2](#subsec-pca-app-dimred)
    through [4.4.5](#subsec-pca-app-datacompress) are devoted to answering this question.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的实际意义是什么？为什么我们想知道点分布沿最大分布方向的方向？从[4.4.2](#subsec-pca-app-dimred)到[4.4.5](#subsec-pca-app-datacompress)的章节致力于回答这个问题。
- en: 4.4.2 PCA and dimensionality reduction
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 PCA与降维
- en: In section [4.1](#sec-truedim), we saw that when data points are clustered around
    a lower-dimensional subspace, it is beneficial to project them onto the subspace
    and reduce the dimensionality of the data representation. The dimensionally reduced
    data is more compactly representable and more amenable to deriving insights and
    analysis. In the particular case where the data points are clustered around a
    straight line or hyperplane, PCA can be used to generate a lower-dimensional data
    representation by getting rid of the principal components corresponding to relatively
    small principal values. The technique is agnostic to the dimensionality of the
    data. The line or hyperplane can be anywhere in the space, with arbitrary orientation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在[4.1](#sec-truedim)节中，我们看到了当数据点围绕一个低维子空间聚集时，将它们投影到子空间并降低数据表示的维度是有益的。降维后的数据可以更紧凑地表示，并且更容易得出见解和分析。在数据点围绕直线或超平面聚集的特定情况下，可以通过去除对应于相对较小的主成分值的主成分来使用PCA生成一个低维数据表示。这项技术对数据的维度没有限制。直线或超平面可以在空间中的任何位置，具有任意方向。
- en: '![](../../OEBPS/Images/CH04_F03a_Chaudhury.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F03a_Chaudhury.png)'
- en: (a) Dimensionality reduction from 2D to 1D
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 从2D到1D的降维
- en: '![](../../OEBPS/Images/CH04_F03b_Chaudhury.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F03b_Chaudhury.png)'
- en: (b) Dimensionality reduction from 3D to 2D
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 从3D到2D的降维
- en: Figure 4.3 Dimensionality reduction via PCA. Original data points are shown
    with filled little circles, and hollow circles represent lower-dimensional representations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 通过PCA进行降维。原始数据点用实心小圆圈表示，空心圆圈表示低维表示。
- en: 'For instance, consider the 2D distribution shown in figure [4.3a](#fig-pca-3and2d).
    Here, the data is 2D and plotted on a plane, but the main spread of the data is
    along a 1D line shown by the thick two-arrowed line in the figure). There is very
    little spread in the direction orthogonal to that line (indicated by the little
    perpendiculars from the data points to the line in the figure). PCA reveals this
    internal structure. There are two principal values because the data is 2D), but
    one of them is much smaller than the other: this reveals that dimensionality reduction
    is possible. The principal axis corresponding to the larger principal value is
    along the line of maximum spread. The small perturbations along the other principal
    axis can be eliminated with little loss of information. Replacing each data point
    with its projection on the first principal axis converts the 2D dataset into a
    1D dataset, brings out the true underlying pattern in the data (straight line),
    eliminates noise (little perpendiculars), and reduces storage costs.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑图[4.3a](#fig-pca-3and2d)中显示的二维分布。在这里，数据是二维的，并绘制在平面上，但数据的主要分布是沿着图中粗双箭头线所示的1D线。垂直于该线的方向（如图中数据点到线的短垂线所示）的分布非常小。主成分分析（PCA）揭示了这种内部结构。由于数据是二维的，有两个主成分值），但其中之一比另一个小得多：这表明降维是可能的。对应于较大主成分值的主轴沿着最大分布的线。沿着其他主轴的小扰动可以消除，而损失的信息很少。用每个数据点在第一个主成分轴上的投影来替换每个数据点，将二维数据集转换为1D数据集，揭示数据中的真实基本模式（直线），消除噪声（短垂线），并降低存储成本。
- en: 'In figure [4.3b](#fig-pca-3and2d), the data is 3D, but the data points are
    clustered around a plane in 3D space (shown as the rectangle in the figure). The
    main spread of the data is *along* the plane, while the spread in the direction
    normal to that plane (shown with little perpendiculars from data points to the
    plane) is small. PCA reveals this: there are three principal values (because the
    data is 3D), but one of them is much smaller than the other two, revealing that
    dimensionality reduction is possible. The principal axis corresponding to the
    small principal value is normal to the plane. We can ignore these perturbations
    (perpendiculars in figure [4.3b](#fig-pca-3and2d)) with little loss of information.
    This is equivalent to projecting the data onto the plane formed by the first two
    principal axes. Doing so brings out the underlying data pattern (plane), eliminates
    noise (little perpendiculars), and reduces storage costs.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[4.3b](#fig-pca-3and2d)中，数据是三维的，但数据点在三维空间中的一个平面上聚集（如图中的矩形所示）。数据的主要分布是*沿着*这个平面，而垂直于该平面的方向（如图中从数据点到平面的短垂线所示）的分布很小。主成分分析（PCA）揭示了这一点：有三个主成分值（因为数据是三维的），但其中之一比其他两个小得多，表明降维是可能的。对应于小主成分值的主轴垂直于该平面。我们可以忽略这些扰动（图[4.3b](#fig-pca-3and2d)中的垂线）而损失的信息很少。这相当于将数据投影到由前两个主成分轴形成的平面上。这样做可以揭示数据的基本模式（平面），消除噪声（短垂线），并降低存储成本。
- en: '4.4.3 PyTorch code: PCA and dimensionality reduction'
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 PyTorch代码：PCA和降维
- en: In this section, we provide a PyTorch code sample for PCA computation in listing
    [4.1](#code-pca-computation). Then we provide PyTorch code for applying PCA on
    a correlated dataset and an uncorrelated dataset in listings [4.2](#code-pca-correlated)
    and [4.3](#code-pca-uncorrelated), respectively. The results are plotted in figure
    [4.4](#fig-pca-results).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一个PyTorch代码示例，用于列表[4.1](#code-pca-computation)中的PCA计算。然后我们分别提供了应用于相关数据集和无相关数据集的PyTorch代码，列表[4.2](#code-pca-correlated)和[4.3](#code-pca-uncorrelated)。结果如图[4.4](#fig-pca-results)所示。
- en: '![](../../OEBPS/Images/CH04_F04a_Chaudhury.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F04a_Chaudhury.png)'
- en: (a) PCA on correlated data
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 相关数据的PCA
- en: '![](../../OEBPS/Images/CH04_F04b_Chaudhury.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F04b_Chaudhury.png)'
- en: (b) PCA on uncorrelated data
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 无相关数据的PCA
- en: Figure 4.4 PCA results. In (a), the data points are around the straight line
    **y** = 2*x*. Consequently, one principal value is much larger than the other,
    indicating that dimensionality reduction will work. In (b), both principal values
    are large. Dimensionality reduction will not work.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 PCA结果。在(a)中，数据点围绕直线**y** = 2*x*分布。因此，一个主成分值比另一个大得多，表明降维将有效。在(b)中，两个主成分值都很大。降维将不会有效。
- en: NOTE The complete PyTorch code for this section is available at [http://mng.bz/aoYz](http://mng.bz/aoYz)
    in the form of fully functional and executable Jupyter notebooks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节完整的PyTorch代码以完全功能性和可执行的Jupyter笔记本形式，可在[http://mng.bz/aoYz](http://mng.bz/aoYz)找到。
- en: Listing 4.1 PCA computation
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 PCA计算
- en: '[PRE0]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Returns principal values and vectors
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ① 返回主值和向量
- en: NOTE Fully functional code for the PCA computation in listing 4.1 is available
    at [http://mng.bz/DRYR](http://mng.bz/DRYR).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表4.1中PCA计算的完全功能代码可在[http://mng.bz/DRYR](http://mng.bz/DRYR)找到。
- en: Listing 4.2 PCA on synthetic correlated data
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 对合成相关数据的PCA
- en: '[PRE1]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Random feature vector
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ① 随机特征向量
- en: ② Correlated feature vector + minor noise
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ② 相关特征向量加少量噪声
- en: ③ Data matrix spread mostly along y = 2x
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 数据矩阵主要沿y = 2x方向扩展
- en: ④ One large principal value and one small
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 一个大主值和一个小主值
- en: ⑤ First principal vector along y = 2x
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 沿y = 2x的第一主向量
- en: ⑥ Dimensionality reduction by projecting on the first principal vector
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 通过投影到第一个主向量进行降维
- en: 'The output is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE2]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: NOTE Fully functional code for the PCA computation in listing 4.2 is available
    at [http://mng.bz/gojl](http://mng.bz/gojl).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表4.2中PCA计算的完全功能代码可在[http://mng.bz/gojl](http://mng.bz/gojl)找到。
- en: Listing 4.3 PCA on synthetic uncorrelated data
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 对合成非相关数据的PCA
- en: '[PRE3]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Random uncorrelated feature-vector pair
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ① 随机非相关特征向量对
- en: ② Principal values close to each other. The spread of the data points is comparable
    in both directions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ② 主值彼此接近。数据点的分布在这两个方向上相当。
- en: 'Here is the output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE4]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: NOTE Fully functional code for the PCA computation in listing 4.3 is available
    at [http://mng.bz/e5Kz](http://mng.bz/e5Kz).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表4.3中PCA计算的完全功能代码可在[http://mng.bz/e5Kz](http://mng.bz/e5Kz)找到。
- en: 4.4.4 Limitations of PCA
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.4 PCA的局限性
- en: PCA assumes that the underlying pattern is linear in nature. Where this is not
    true, PCA will not capture the correct underlying pattern. This is illustrated
    schematically in figure [4.5a](#fig-non-linear-pca) and via experimental results
    from listing [4.3](#code-pca-uncorrelated). Figure [4.5b](#fig-non-linear-pca)
    shows the results of running listing [4.4](#code-synthetic-nonlin-correlated),
    where we synthetically generate non-linearly correlated data and perform PCA.
    The straight line at the base shows the first principal axis. Projecting data
    on this axis results in a large error in the data positions (loss of information).
    Linear PCA will not do well.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: PCA假设潜在模式在本质上呈线性。当这并不成立时，PCA将无法捕捉到正确的潜在模式。这通过图[4.5a](#fig-non-linear-pca)中的示意图和列表[4.3](#code-pca-uncorrelated)中的实验结果来展示。图[4.5b](#fig-non-linear-pca)显示了运行列表[4.4](#code-synthetic-nonlin-correlated)的结果，其中我们合成了非线性相关数据并执行了PCA。底部直线表示第一个主轴。将数据投影到这个轴上会导致数据位置（信息损失）出现较大误差。线性PCA表现不佳。
- en: '![](../../OEBPS/Images/CH04_F05a_Chaudhury.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F05a_Chaudhury.png)'
- en: (a) Schematic 2D data distribution with a curved underlying pattern
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 带有弯曲潜在模式的2D数据分布示意图
- en: '![](../../OEBPS/Images/CH04_F05b_Chaudhury.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH04_F05b_Chaudhury.png)'
- en: (b) PCA results on synthetic (computer generated) non-linearly correlated data.
    The line at the base shows the first principal axis.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 对合成（计算机生成）非线性相关数据的PCA结果。底部直线表示第一个主轴。
- en: Figure 4.5 Non-linearly correlated data. The points are distributed around a
    curve as opposed to a straight line. It is impossible to find a straight line
    such that all points are near it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 非线性相关数据。点分布在曲线周围，而不是直线周围。不可能找到一条直线，使得所有点都靠近它。
- en: Listing 4.4 PCA on synthetic nonlinearly correlated data
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.4 对合成非线性相关数据的PCA
- en: '[PRE5]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Principal vectors fail to capture the underlying distribution.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ① 主向量未能捕捉到潜在分布
- en: 'The output is as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE6]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 4.4.5 PCA and data compression
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.5 PCA与数据压缩
- en: If we want to represent a large multidimensional dataset within a fixed byte
    budget, what information can we can get rid of with the least loss of accuracy?
    Clearly, the answer is the principal components along the smaller principal values—getting
    rid of them actually helps, as described in section [4.4.2](#subsec-pca-app-dimred).
    To compress data, we often perform PCA and then replace the data points with their
    projections on first few principal axes; doing so reduces the number of data components
    to store. This is the underlying principle in JPEG 98 image compression techniques.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在固定的字节数据预算内表示一个大型多维数据集，我们可以通过最小损失精度来丢弃哪些信息？显然，答案是较小的主值方向上的主成分——丢弃它们实际上是有帮助的，如第
    [4.4.2](#subsec-pca-app-dimred) 节所述。为了压缩数据，我们通常执行PCA，然后用数据点在第一个几个主轴上的投影来替换它们；这样做可以减少需要存储的数据分量数量。这是JPEG
    98图像压缩技术背后的基本原理。
- en: 4.5 Singular value decomposition
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 奇异值分解
- en: Singular value decomposition (SVD) may be the most important linear algebraic
    tool in machine learning. Among other things, PCA and LSA implementations are
    built based on SVD. We illustrate the basic idea in this section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）可能是机器学习中最重要的线性代数工具。在许多其他方面，PCA和LSA的实现都是基于SVD构建的。我们将在本节中阐述基本思想。
- en: NOTE There are several slightly different forms of SVD. We have chosen the one
    that seems intuitively simplest.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：SVD有几种稍微不同的形式。我们选择了看起来最直观的一种。
- en: The SVD theorem states that any matrix *A*, singular or nonsingular, rectangular
    or square, can be decomposed as the product of three matrices
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: SVD定理表明，任何矩阵 *A*，无论是奇异的还是非奇异的，矩形的还是方阵的，都可以分解为三个矩阵的乘积
- en: '*A* = *U*Σ*V^T*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = *U*Σ*V^T*'
- en: Equation 4.11
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 4.11
- en: where (assuming that the matrix *A* is *m* × *n*)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其中（假设矩阵 *A* 是 *m* × *n*）
- en: Σ is an *m* × *n* diagonal matrix. Its diagonal elements contain the square
    roots of the eigenvalues of *A^TA*. These are also known as the singular values
    of *A*. The singular values appear in decreasing order in the diagonal of Σ.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Σ 是一个 *m* × *n* 的对角矩阵。其对角元素包含 *A^TA* 的特征值的平方根。这些也被称为 *A* 的奇异值。奇异值按降序出现在Σ的对角线上。
- en: '*V* is an *n* × *n* orthogonal matrix containing eigenvectors of *A^TA* in
    its columns.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V* 是一个 *n* × *n* 的正交矩阵，其列包含 *A^TA* 的特征向量。'
- en: '*U* is an *m* × *m* orthogonal matrix containing eigenvectors of *AA^T* in
    its columns.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U* 是一个 *m* × *m* 的正交矩阵，其列包含 *AA^T* 的特征向量。'
- en: 4.5.1 Informal proof of the SVD theorem
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 SVD定理的非正式证明
- en: We will provide an informal proof of the SVD theorem through a series of lemmas.
    Going through these will provide additional insights.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一系列引理非正式地证明SVD定理。通过这些引理，我们将获得额外的见解。
- en: Lemma 1
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 引理1
- en: '*A^TA* is symmetric positive semidefinite. Its eigenvalues—aka singular values—are
    non-negative. Its eigenvectors—aka singular vectors—are orthogonal.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*A^TA* 是对称正定矩阵。它的特征值（也称为奇异值）是非负的。它的特征向量（也称为奇异向量）是正交的。'
- en: Proof of lemma 1
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 引理1的证明
- en: Let’s say *A* has *m* rows and *n* columns. Then *A^TA* is an *n* × *n* square
    matrix
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *A* 有 *m* 行和 *n* 列。那么 *A^TA* 是一个 *n* × *n* 的方阵
- en: (*A^TA*)*^T* = *A^T*(*A^T*)*^T* = *A^TA*
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: (*A^TA*)*^T* = *A^T*(*A^T*)*^T* = *A^TA*
- en: which proves that *A^TA* is symmetric. Also, for any ![](../../OEBPS/Images/AR_x.png),
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了 *A^TA* 是对称的。对于任何 ![](../../OEBPS/Images/AR_x.png)，
- en: '![](../../OEBPS/Images/AR_x.png)*^TA^TA*![](../../OEBPS/Images/AR_x.png) =
    (*A*![](../../OEBPS/Images/AR_x.png))*^T*(*A*![](../../OEBPS/Images/AR_x.png))
    = ||*A*![](../../OEBPS/Images/AR_x.png)||² > 0'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png)*^TA^TA*![](../../OEBPS/Images/AR_x.png) =
    (*A*![](../../OEBPS/Images/AR_x.png))*^T*(*A*![](../../OEBPS/Images/AR_x.png))
    = ||*A*![](../../OEBPS/Images/AR_x.png)||² > 0'
- en: which, as per section [4.2.2](#subsec-sym-pos-sd), proves that the matrix *A^TA*
    is symmetric and positive semidefinite. Hence, its eigenvalues are all positive
    or zero.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这，根据第 [4.2.2](#subsec-sym-pos-sd) 节，证明了矩阵 *A^TA* 是对称和正半定的。因此，其特征值都是正的或零。
- en: We proved in section [2.13](02.xhtml#sec-eigen-values-vectors) that symmetric
    matrices have orthogonal eigenvectors. That proves that singular vectors are orthogonal.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 [2.13](02.xhtml#sec-eigen-values-vectors) 节中证明了对称矩阵有正交的特征向量。这证明了奇异向量是正交的。
- en: Let (*λ[i], v̂*[1]), for *i* ∈ [1, *n*] be the set of eigenvalue, eigenvector
    pairs of *A^TA*—aka the singular value, singular vector pair of *A*. Note that
    without loss of generality, we can assume *λ*[1] ≥ *λ*[2] ≥ ⋯ *λ[n]* because if
    not, we can always renumber the eigenvalues and eigenvectors).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 设 (*λ[i], v̂*[1])，对于 *i* ∈ [1, *n*] 是 *A^TA* 的特征值、特征向量对——也称为 *A* 的奇异值、奇异向量对。注意，不失一般性，我们可以假设
    *λ*[1] ≥ *λ*[2] ≥ ⋯ *λ[n]*，因为如果不是这样，我们总是可以重新编号特征值和特征向量）。
- en: Now, by definition,
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据定义，
- en: '*A^TAv̂[i]* = *λ[i]v̂[i]* ∀*i* ∈ [1, *n*]'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*A^TAv̂[i]* = *λ[i]v̂[i]* 对于所有*i* ∈ [1, *n*]'
- en: From lemma 1, singular vectors are orthogonal, and hence
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从引理1，单向量是正交的，因此
- en: '![](../../OEBPS/Images/eq_04-12.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12.png)'
- en: Equation 4.12
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 方程4.12
- en: Note that *v̂[i]*s are unit vectors (that is why we are using the hat sign as
    opposed to the overhead arrow). As described in section [2.13](02.xhtml#sec-eigen-values-vectors),
    eigenvectors remain eigenvectors if we change their length. We are free to choose
    any length for eigenvectors as long as we choose it consistently. We are choosing
    unit-length eigenvectors here.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*v̂[i]*s是单位向量（这就是为什么我们使用帽子符号而不是箭头）。如第[2.13](02.xhtml#sec-eigen-values-vectors)节所述，如果改变特征向量的长度，特征向量仍然是特征向量。我们可以自由选择任何长度的特征向量，只要我们一致选择。我们在这里选择单位长度的特征向量。
- en: Lemma 2
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 引理2
- en: '*AA^T* is symmetric positive semidefinite. Its eigenvalues are non-negative
    and eigenvectors are orthogonal.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*AA^T*是对称正半定的。它的特征值是非负的，特征向量是正交的。'
- en: Proof of lemma 2
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 引理2的证明
- en: (*AA^T*)*^T* = (*A^T*)*^TA^T* = *AA^T*
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: (*AA^T*)*^T* = (*A^T*)*^TA^T* = *AA^T*
- en: Also,
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，
- en: '![](../../OEBPS/Images/AR_x.png)*^TAA^T*![](../../OEBPS/Images/AR_x.png) =
    (*A^T*![](../../OEBPS/Images/AR_x.png))*^T*(*A^T*![](../../OEBPS/Images/AR_x.png))
    = ||(*A^T*![](../../OEBPS/Images/AR_x.png))|| ≥ 0'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png)*^TAA^T*![](../../OEBPS/Images/AR_x.png) =
    (*A^T*![](../../OEBPS/Images/AR_x.png))*^T*(*A^T*![](../../OEBPS/Images/AR_x.png))
    = ||(*A^T*![](../../OEBPS/Images/AR_x.png))|| ≥ 0'
- en: and so on.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推。
- en: Lemma 3
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 引理3
- en: 1/√*λ[i] ⋅ Av̂*[1], ∀*i* ∈ [1, *n*] is a set of orthogonal unit vectors.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 1/√*λ[i] ⋅ Av̂*[1]，对于所有*i* ∈ [1, *n*]是一个正交的单位向量集合。
- en: Proof of lemma 3
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 引理3的证明
- en: 'Let’s take the dot product of a pair of these vectors:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取一对这些向量的点积：
- en: '![](../../OEBPS/Images/eq_04-12-a.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-a.png)'
- en: Since *λ[j]*, *v̂[j]* are eigenvalue, eigenvector pairs of *A^TA*, the previous
    equation can be rewritten as
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*λ[j]*, *v̂[j]*是*A^TA*的特征值，特征向量对，前面的方程可以重写为
- en: '![](../../OEBPS/Images/eq_04-12-b.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-b.png)'
- en: which, using equation [4.12](#eq-ortho-eigvec), can be rewritten as
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这，使用方程[4.12](#eq-ortho-eigvec)，可以重写为
- en: '![](../../OEBPS/Images/eq_04-12-c.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-c.png)'
- en: Lemma 4
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 引理4
- en: If (*λ[i]*, *v̂[i]*) is an eigenvalue, eigenvector pair of *A^TA*, then *λ[i],
    û[i]* = 1/√*λ[i]* ⋅ *Av̂[i]* is an eigenvalue, eigenvector pair of *AA^T*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(*λ[i]*, *v̂[i]*)是*A^TA*的特征值，特征向量对，那么*λ[i], û[i]* = 1/√*λ[i]* ⋅ *Av̂[i]*是*AA^T*的特征值，特征向量对。
- en: Proof of lemma 4
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 引理4的证明
- en: Given
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 给定
- en: '*A^TAv̂[i]* = *λ[i]v̂[i]*'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*A^TAv̂[i]* = *λ[i]v̂[i]*'
- en: left-multiplying both sides of the equation by *A*, we get
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 将等式的两边左乘以*A*，我们得到
- en: '*AA^T**Av̂[i]* = *λ[i]* *Av̂[i]*'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*AA^T**Av̂[i]* = *λ[i]* *Av̂[i]*'
- en: '*AA^T*(*Av̂[i]*) = *λ[i]* (*Av̂[i]*)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*AA^T*(*Av̂[i]*) = *λ[i]* (*Av̂[i]*)'
- en: Substituting *![](../../OEBPS/Images/AR_f.png)[i]* = *Av̂[i]* in the last equation,
    we get
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 将*![](../../OEBPS/Images/AR_f.png)[i]* = *Av̂[i]*代入最后一个方程，我们得到
- en: '*AA^T![](../../OEBPS/Images/AR_f.png)[i]* = *λ[i]![](../../OEBPS/Images/AR_f.png)[i]*'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*AA^T![](../../OEBPS/Images/AR_f.png)[i]* = *λ[i]![](../../OEBPS/Images/AR_f.png)[i]*'
- en: which proves that *![](../../OEBPS/Images/AR_f.png)[i]* = *Av̂[i]* is an eigenvector
    of *AA^T* with *λ[i]* as a corresponding eigenvalue. Multiplying by 1/√*λ[i]*
    converts it into a unit vector as per lemma 3\. This completes the proof of the
    lemma.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了*![](../../OEBPS/Images/AR_f.png)[i]* = *Av̂[i]*是*AA^T*的一个特征向量，其对应的特征值为*λ[i]*。乘以1/√*λ[i]*将其转换为根据引理3的单位向量。这完成了引理的证明。
- en: 4.5.2 Proof of the SVD theorem
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 SVD定理的证明
- en: Now we are ready to examine the proof of the SVD theorem.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好检查SVD定理的证明。
- en: 'Case 1: More rows than columns in A'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 情况1：A的行数多于列数
- en: If *m*, the number of rows in *A*, is greater than or equal to *n*, the number
    of columns in *A*, we define
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*m*，*A*的行数大于或等于*n*，*A*的列数，我们定义
- en: '![](../../OEBPS/Images/eq_04-12-d.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-d.png)'
- en: 'Note the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下内容：
- en: From lemma 1, we know that the eigenvalues of *A^TA* are positive. This makes
    the square roots, √*λ[i]s*, real.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从引理1，我们知道*A^TA*的特征值是正的。这使得平方根，√*λ[i]*s，是实数。
- en: '*U* is an *m* × *m* orthogonal matrix whose columns are the eigenvectors of
    *AA^T*. Since, *AA^T* is *m* × *m*, it has *m* eigenvalues and eigenvectors. The
    first *n* of them are *û*[1] = 1/√*λ*[1] ⋅ *Av̂*[1], *û*[2] = 1/√*λ*[2] ⋅ *Av̂*[2],
    … , *û[n]* = 1/√*λ[i]* ⋅ *Av̂[n]* from lemma 4, we know these are eigenvectors
    of *AA^T*). In this case, by our initial assumption, *n* < *m*. Thus *AA^T* has
    (*m* − *n*) more eigenvectors, *û*[*n* + 1], ⋯ *û[m]*.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U* 是一个 *m* × *m* 的正交矩阵，其列是 *AA^T* 的特征向量。由于 *AA^T* 是 *m* × *m* 的，它有 *m* 个特征值和特征向量。其中前
    *n* 个是 *û*[1] = 1/√*λ*[1] ⋅ *Av̂*[1]，*û*[2] = 1/√*λ*[2] ⋅ *Av̂*[2]，… ，*û[n]* =
    1/√*λ[i]* ⋅ *Av̂[n]*（根据引理 4，我们知道这些是 *AA^T* 的特征向量）。在这种情况下，根据我们的初始假设，*n* < *m*。因此
    *AA^T* 有 (*m* − *n*) 个更多的特征向量，*û*[*n* + 1]，⋯ *û[m]*。'
- en: '*V* is an *n* × *n* orthogonal matrix with the eigenvectors of *A^TA* that
    is, *v̂*[1], *v̂*[2], ⋯, *v̂[n]*) as its columns.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V* 是一个 *n* × *n* 的正交矩阵，其列是 *A^TA* 的特征向量，即 *v̂*[1]，*v̂*[2]，⋯ ，*v̂[n]*)。 '
- en: Consider the matrix product *U*Σ. From basic matrix multiplication rules (section
    [2.5](02.xhtml#sec-misc_mat-vec), we can see that
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑矩阵乘积 *U*Σ。根据基本的矩阵乘法规则（第 [2.5](02.xhtml#sec-misc_mat-vec) 节），我们可以看到
- en: '![](../../OEBPS/Images/eq_04-12-e.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-e.png)'
- en: Note that the last columns of *U*, *û*[*n* + 1], ⋯, *û[m]*, are multiplied by
    all zeros in Σ and vanishing. Thus,
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 *U* 的最后几列，*û*[*n* + 1]，⋯ ，*û[m]*，在 Σ 中乘以所有零并消失。因此，
- en: '![](../../OEBPS/Images/eq_04-12-f.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-f.png)'
- en: The later columns of *U*—those named with *u*s—fail to survive because they
    are multiplied by the zeros at the bottom of Σ.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*U* 的后列——那些用 *u* 命名的列——未能幸存，因为它们乘以 Σ 底部的零。'
- en: Thus we have proved that
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经证明了
- en: '*AV* = *U*Σ'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*AV* = *U*Σ'
- en: Then
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '*AVV^T* = *U*Σ*V^T*'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*AVV^T* = *U*Σ*V^T*'
- en: Since *V* is orthogonal, *VV^T* = **I**. Hence
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *V* 是正交的，*VV^T* = **I**。因此
- en: '*A* = *U*Σ*V^T*'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* = *U*Σ*V^T*'
- en: which completes the proof of the singular value theorem.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了奇异值定理的证明。
- en: 'Case 2: Fewer rows than columns in A'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 情况 2：A 的行数少于列数
- en: If *m*, the number of rows in *A*, is less than or equal to *n*, the number
    of columns in *A*, we have
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *m*，矩阵 *A* 的行数小于或等于 *n*，矩阵 *A* 的列数，我们有
- en: '![](../../OEBPS/Images/eq_04-12-g.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-g.png)'
- en: The proof follows along similar lines.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 证明遵循类似的路线。
- en: '4.5.3 Applying SVD: PCA computation'
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.3 应用 SVD：PCA 计算
- en: We will illustrate the idea first with a toy dataset. Consider a 3D dataset
    with five points. We use a superscript to denote the index of the data instance
    and a subscript to denote the component. Thus the *i*th data instance vector is
    denoted as [*x*[0]^((*i*))   *x*[1]^((*i*))   *x*[2]^((*i*))]. We denote the entire
    data set with a matrix in which each feature instance appears as a row vector.
    The data matrix is
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先用一个玩具数据集来说明这个想法。考虑一个包含五个点的 3D 数据集。我们使用上标来表示数据实例的索引，下标来表示分量。因此，第 *i* 个数据实例向量表示为
    [*x*[0]^((*i*))   *x*[1]^((*i*))   *x*[2]^((*i*))]。我们用矩阵表示整个数据集，其中每个特征实例作为一个行向量出现。数据矩阵是
- en: '![](../../OEBPS/Images/eq_04-12-h.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-h.png)'
- en: 'We will assume that the data is already mean-subtracted. Now examine the matrix
    product *X^TX*, using ordinary rules of matrix multiplication:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设数据已经减去了均值。现在检查矩阵乘积 *X^TX*，使用普通的矩阵乘法规则：
- en: '![](../../OEBPS/Images/eq_04-12-i.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-i.png)'
- en: From equations [4.10](#eq-varij) and [4.9](#eq-covar-mat),
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 [4.10](#eq-varij) 和 [4.9](#eq-covar-mat)，
- en: '![](../../OEBPS/Images/eq_04-12-j.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-j.png)'
- en: Thus *X^TX* is the covariance matrix of the dataset *X*. This holds for arbitrary
    dimensions and arbitrary feature instance counts.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 *X^TX* 是数据集 *X* 的协方差矩阵。这对于任意维度和任意特征实例数量都成立。
- en: If we create a data matrix *X* with each data instance forming a row, *X^TX*
    ields the covariance matrix of the dataset. The eigenvalues and eigenvectors of
    this matrix are the principal components. Hence, performing SVD on *X* yields
    PCA of the data (assuming prior mean subtraction).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建一个数据矩阵 *X*，其中每个数据实例形成一个行，*X^TX* 得到数据集的协方差矩阵。这个矩阵的特征值和特征向量是主成分。因此，对 *X*
    进行 SVD 得到数据的 PCA（假设先前的均值减法）。
- en: '4.5.4 Applying SVD: Solving arbitrary linear systems'
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.4 应用 SVD：求解任意线性系统
- en: A linear system is a system of simultaneous linear equations
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 线性系统是一组联立线性方程
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
- en: 'We first encountered a linear system in section [2.12](02.xhtml#sec-lin_systems).
    It is possible to use matrix inversion to solve such a system:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次在 [2.12](02.xhtml#sec-lin_systems) 节中遇到了线性系统。可以使用矩阵求逆来解决这样的系统：
- en: '![](../../OEBPS/Images/AR_x.png) = *A*^(-1)![](../../OEBPS/Images/AR_b.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/AR_x.png) = *A*^(-1)![](../../OEBPS/Images/AR_b.png)'
- en: 'However, solving a linear system with matrix inversion is undesirable for many
    reasons. To begin with, it is numerically unstable. The matrix inverse contains
    the determinant of the matrix in its denominator. If the determinant is near zero,
    the inverse will contain very large numbers. Minor noise in ![](../../OEBPS/Images/AR_b.png)
    will be multiplied by these large numbers and cause large errors in the computed
    solution ![](../../OEBPS/Images/AR_x.png). In this case, the inverse-based solution
    can be very inaccurate. Furthermore, the determinant can be zero: this can happen
    when one row of the matrix is a linear combination of others, indicating that
    we have fewer equations than we think. And what if the matrix is not square to
    begin with? This can happen when we have more equations than unknowns overdetermined
    system) or fewer equations than unknowns underdetermined system). In these cases,
    the inverse is not computable, and the system cannot be solved fully.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，用矩阵求逆来解决线性系统是不理想的，原因有很多。首先，它是数值不稳定的。矩阵逆在其分母中包含矩阵的行列式。如果行列式接近零，逆矩阵将包含非常大的数。![](../../OEBPS/Images/AR_b.png)中的微小噪声将被这些大数相乘，导致计算解![](../../OEBPS/Images/AR_x.png)中的大误差。在这种情况下，基于逆的解可能非常不准确。此外，行列式可能为零：这发生在矩阵的一行是其他行的线性组合时，表明我们拥有的方程比我们想象的要少。那么，如果矩阵一开始就不是方阵呢？这发生在我们拥有的方程比未知数多（过定系统）或比未知数少（欠定系统）时。在这些情况下，逆是不可计算的，系统不能完全求解。
- en: 'Even in these cases, we would like to obtain a solution that is the best approximation
    in some sense; and in the case of a square matrix, we would like to get the exact
    solution. How do we do this? Answer: we use SVD. The steps are as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这些情况下，我们也希望得到某种意义上的最佳近似解；在方阵的情况下，我们希望得到精确解。我们如何做到这一点？答案：我们使用奇异值分解（SVD）。步骤如下：
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png) can
    be rewritten as *U*(Σ*V^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_b.png).
    We then solve *U*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png).
    This can be easily done using orthogonality of *U*, as ![](../../OEBPS/Images/AR_y.png)[1]
    = *U^T*![](../../OEBPS/Images/AR_b.png).'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)可以重写为*U*(Σ*V^T*![](../../OEBPS/Images/AR_x.png))
    = ![](../../OEBPS/Images/AR_b.png)。然后我们解*U*![](../../OEBPS/Images/AR_y.png)[1]
    = ![](../../OEBPS/Images/AR_b.png)。这可以通过使用*U*的正交性轻松完成，因为![](../../OEBPS/Images/AR_y.png)[1]
    = *U^T*![](../../OEBPS/Images/AR_b.png)。'
- en: Now we have Σ(*V^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_y.png)[1]
    Solve Σ![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1].
    This can be easily done because for any diagonal matrix ![](../../OEBPS/Images/eq_04-12-k.png)
    we can easily compute ![](../../OEBPS/Images/eq_04-12-l.png)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们有Σ(*V^T*![](../../OEBPS/Images/AR_x.png)) = ![](../../OEBPS/Images/AR_y.png)[1]。求解Σ![](../../OEBPS/Images/AR_y.png)[2]
    = ![](../../OEBPS/Images/AR_y.png)[1]。这可以很容易地完成，因为对于任何对角矩阵![](../../OEBPS/Images/eq_04-12-k.png)，我们可以轻松地计算![](../../OEBPS/Images/eq_04-12-l.png)
- en: Hence, ![](../../OEBPS/Images/AR_y.png)[2] = Σ^(−1)![](../../OEBPS/Images/AR_y.png)[1].
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，![](../../OEBPS/Images/AR_y.png)[2] = Σ^(−1)![](../../OEBPS/Images/AR_y.png)[1]。
- en: 'Now we have *V^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2].
    This too can be solved easily using the orthogonality of *V*:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们有*V^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]。这也可以通过使用*V*的正交性轻松解决：
- en: '![](../../OEBPS/Images/AR_x.png) = *V*![](../../OEBPS/Images/AR_y.png)[2]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_x.png) = *V*![](../../OEBPS/Images/AR_y.png)[2]'
- en: 'Thus we have solved for ![](../../OEBPS/Images/AR_x.png) without inverting
    the matrix *A*:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们求解了![](../../OEBPS/Images/AR_x.png)而没有对矩阵*A*进行求逆：
- en: For invertible square matrices *A*, this method yields the same solution as
    the matrix-inverse-based method.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于可逆的方阵*A*，这种方法得到的解与基于矩阵逆的方法相同。
- en: For nonsquare matrices, this boils down to the Moore-Penrose inverse and yields
    the best-effort solution.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非方阵，这归结为摩尔-彭罗斯逆矩阵，并得到最佳努力解。
- en: 4.5.5 Rank of a matrix
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.5 矩阵的秩
- en: 'In section [2.12](02.xhtml#sec-lin_systems), we studied linear systems of equations.
    Such a system can be represented in matrix-vector form:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2.12](02.xhtml#sec-lin_systems)节中，我们研究了线性方程组。这样的系统可以用矩阵-向量形式表示：
- en: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)'
- en: Each row of *A* and ![](../../OEBPS/Images/AR_b.png) contributes one equation.
    If we have as many independent equations as unknowns, the system is solvable.
    This is the simplest case; matrix *A* is square and invertible. *det*(*A*) is
    nonzero, and *A*^(−1) exists.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* 和 ![](../../OEBPS/Images/AR_b.png) 的每一行都贡献一个方程。如果我们有与未知数一样多的独立方程，则该方程组是可解的。这是最简单的情况；矩阵
    *A* 是方阵且可逆。*det*(*A*) 非零，且 *A*^(−1) 存在。'
- en: 'Sometimes the situation is misleading. Consider the following system:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 有时情况会误导。考虑以下系统：
- en: '![](../../OEBPS/Images/eq_04-12-m.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-m.png)'
- en: 'Although there are three rows and apparently three equations, the equations
    are not independent. For instance, the third equation can be obtained by adding
    the first two. We really have only two equations, not three. We say this linear
    system is *degenerate*. All of the following statements are true for such a system
    *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有三个行和显然有三个方程，但这些方程并不独立。例如，第三个方程可以通过将前两个方程相加得到。我们实际上只有两个方程，而不是三个。我们说这个线性系统是
    *退化的*。对于这样的系统 *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    的以下所有陈述都是正确的：
- en: The linear system is degenerate.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性方程组是退化的。
- en: '*det*(*A*) = 0.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*det*(*A*) = 0。'
- en: '*A*^(−1) cannot be computed, and *A* is not invertible.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A*^(−1) 不能计算，且 *A* 不可逆。'
- en: Rows of *A* are linearly dependent. There exists a linear combination of the
    rows that sum to zero. For example, in the previous example, ![](../../OEBPS/Images/AR_r.png)[0]
    + ![](../../OEBPS/Images/AR_r.png)[1] − ![](../../OEBPS/Images/AR_r.png)[2] =
    0.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A* 的行线性相关。存在一个行的线性组合，其和为零。例如，在前一个例子中，![](../../OEBPS/Images/AR_r.png)[0] +
    ![](../../OEBPS/Images/AR_r.png)[1] − ![](../../OEBPS/Images/AR_r.png)[2] = 0。'
- en: At least one of the singular values of *A* (eigenvalues of *A^TA*) is zero.
    The number of linearly independent rows is equal to the number of nonzero eigenvalues.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少 *A* 的一个奇异值（*A^TA* 的特征值）是零。线性无关行的数量等于非零特征值的数量。
- en: The number of linearly independent rows in a matrix is called its *rank*. It
    can be proved that a matrix has as many nonzero singular values as its rank. It
    can also be proved that the number of linearly independent columns in a matrix
    matches the number of linearly independent rows. Hence, rank can also be defined
    as the number of linearly independent columns in a matrix.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中线性无关行的数量被称为其 *秩*。可以证明矩阵的非零奇异值与其秩一样多。也可以证明矩阵中线性无关列的数量与线性无关行的数量相匹配。因此，秩也可以定义为矩阵中线性无关列的数量。
- en: A nonsquare rectangular matrix with *m* rows and *n* columns has a rank *r*
    = *min*(*m*, *n*). Such matrices are never invertible. We usually resort to SVD
    to solve them.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有 *m* 行和 *n* 列的非方矩形矩阵的秩 *r* = *min*(*m*, *n*)。这样的矩阵永远不可逆。我们通常求助于 SVD 来解它们。
- en: A square matrix with *n* rows and *n* columns is invertible nonzero determinant)
    if and only if it has rank *n*. Such a matrix is said to have *full rank*. Full-rank
    matrices are invertible. They can be solved via matrix inverse computation, but
    inverse computation is not always numerically stable. SVD can be applied here
    as well, with good numerical properties.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有 *n* 行和 *n* 列的方阵是可逆的（非零行列式）当且仅当它具有秩 *n*。这样的矩阵被称为满秩矩阵。满秩矩阵是可逆的。它们可以通过矩阵逆计算来求解，但逆计算并不总是数值稳定的。SVD
    也可以在这里应用，具有良好的数值特性。
- en: Non-full-rank matrices are degenerate. So, rank is a measure of the non-degeneracy
    of the matrix.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 非满秩矩阵是退化的。因此，秩是矩阵非退化的度量。
- en: 4.5.6 PyTorch code for solving linear systems with SVD
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.6 使用 SVD 解线性方程组的 PyTorch 代码
- en: The listings in this section show a PyTorch-based implementation of SVD and
    demonstrate an application that solves a linear system via SVD.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的列表展示了基于 PyTorch 的 SVD 实现，并演示了一个通过 SVD 解线性方程组的应用。
- en: Listing 4.5 Solving an invertible linear system with matrix inversion and SVD
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 使用矩阵求逆和 SVD 解可逆线性方程组
- en: '[PRE7]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Simple test linear system of equations
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ① 简单的测试线性方程组
- en: ② Matrix inversion is numerically unstable; SVD is better.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ② 矩阵求逆在数值上是不稳定的；SVD 更好。
- en: ③ *A* = *USV^T* ⟹ *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    ≜ *USV^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ③ *A* = *USV^T* ⟹ *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    ≜ *USV^T*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
- en: ④ Solves *U*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png).
    Remember *U*^(−1) = *U^T* as U is orthogonal.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 解 *U*![](../../OEBPS/Images/AR_y.png)[1] = ![](../../OEBPS/Images/AR_b.png).
    记住 *U*^(−1) = *U^T* 因为 U 是正交的。
- en: ⑤ Solves *S*![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1].
     Remember *S*^(−1) is easy as *S* is diagonal.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 解 *S*![](../../OEBPS/Images/AR_y.png)[2] = ![](../../OEBPS/Images/AR_y.png)[1]。记住
    *S*^(−1) 很容易，因为 *S* 是对角矩阵。
- en: ⑥ Solves *V^T* ![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2].
    Remember *V^(−T)* = *V* as *V* is orthogonal.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 解 *V^T* ![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_y.png)[2]。记住
    *V^(−T)* = *V*，因为*V*是正交的。
- en: ⑦ The two solutions are the same.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 这两种解是相同的。
- en: 'Here is the output:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出：
- en: '[PRE8]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Listing 4.6 Solving an overdetermined linear system by pseudo-inverse and SVD
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 通过伪逆和SVD求解超定线性系统
- en: '[PRE9]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '① Cat-brain dataset: nonsquare matrix'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ① 猫脑数据集：非方阵
- en: ② Solution via pseudo-inverse
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过伪逆求解
- en: ③ Solution via SVD
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过SVD求解
- en: ④ The two solutions are the same.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 这两种解是相同的。
- en: 'The output is as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Fully functional code for solving the SVD-based linear system can be found at
    [http://mng.bz/OERn](http://mng.bz/OERn).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 解决基于SVD的线性系统的完整代码可以在[http://mng.bz/OERn](http://mng.bz/OERn)找到。
- en: 4.5.7 PyTorch code for PCA computation via SVD
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.7 使用SVD进行PCA计算的PyTorch代码
- en: The following listing demonstrates PCA computations using SVD.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了使用SVD进行PCA计算。
- en: Listing 4.7 Computing PCA directly and using SVD
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.7 直接计算PCA和使用SVD
- en: '[PRE11]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Eigenvalues of the covariance matrix yield principal values.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ① 协方差矩阵的特征值产生主值。
- en: ② Eigenvectors of the covariance matrix yield principal vectors.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ② 协方差矩阵的特征向量产生主向量。
- en: ③ Direct PCA computation from a covariance matrix
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从协方差矩阵直接计算PCA
- en: ④ Data matrix
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 数据矩阵
- en: ⑤ Diagonal elements of matrix *S* yield principal values.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 矩阵*S*的对角元素产生主值。
- en: ⑥ PCA from SVD
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 从SVD进行PCA
- en: ⑦ Columns of matrix *V* yield principal vectors.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 矩阵*V*的列产生主向量。
- en: 'The output is as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE12]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '4.5.8 Applying SVD: Best low-rank approximation of a matrix'
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.8 应用SVD：矩阵的最佳低秩逼近
- en: Given a matrix *A* of some rank *p*, we sometimes want to approximate it with
    a matrix of lower rank *r*, where *r* < *p*. How do we obtain the best rank *r*
    approximation of *A*?
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个秩为*p*的矩阵*A*，我们有时想用秩*r*的矩阵来逼近它，其中*r* < *p*。我们如何获得*A*的最佳秩*r*逼近？
- en: Motivation
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 动机
- en: Why would we want to do this? Well, consider a data matrix *X* as shown in section
    [4.5.3](#subsec-svd-app-pca-computation). As explained in section [4.4.2](#subsec-pca-app-dimred),
    we often want to eliminate small variances in the data (likely due to noise) and
    get the pattern underlying large variations. Replacing the data matrix with a
    lower-rank matrix often achieves this. However, we must bear in mind that this
    does not work when the underlying pattern is nonlinear (such as in figure [4.5a](#fig-non-linear-pca)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要这样做呢？好吧，考虑一个如[4.5.3](#subsec-svd-app-pca-computation)节中所示的数据矩阵*X*。如[4.4.2](#subsec-pca-app-dimred)节中解释的那样，我们通常希望消除数据中的小方差（可能是由于噪声引起的），并获取底层的大变化模式。用低秩矩阵替换数据矩阵通常可以达到这个目的。然而，我们必须记住，当底层模式是非线性时（如图[4.5a](#fig-non-linear-pca)所示），这并不适用。
- en: Approximation error
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 逼近误差
- en: What do we mean by *best approximation*? The Frobenius norm can be taken as
    the magnitude of the matrix. Accordingly, given a matrix *A* and its rank *r*
    approximation *A[r]*, the approximation error is *e* = ||*A* − *A[r]*||*[F]*.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的*最佳逼近*是什么意思？我们可以将Frobenius范数视为矩阵的模。因此，给定一个矩阵*A*及其秩*r*的逼近*A[r]*，逼近误差是*e*
    = ||*A* − *A[r]*||*[F]*。
- en: Method
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 方法
- en: To solidify our ideas, let’s consider an *m* × *n* matrix *A*. From section
    [4.5](../Text/04.xhtml#sec-svd), we know it will have *min*(*m*, *n*) singular
    values. Let its rank be *p* ≤ *min*(*m*, *n*). We want to approximate this matrix
    with a rank *r*(<*p*) matrix.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固我们的想法，让我们考虑一个*m* × *n*的矩阵*A*。从[4.5](../Text/04.xhtml#sec-svd)节中，我们知道它将会有*min*(*m*,
    *n*)个奇异值。设其秩为*p* ≤ *min*(*m*, *n*)。我们想要用秩*r*(<*p*)的矩阵来逼近这个矩阵。
- en: 'Let’s rewrite the SVD expression. We will assume *m* > *n*. Also, as usual,
    we have the singular values sorted in decreasing order: *λ*[1] ≥ *λ*[2] ≥ *λ[n]*.
    We will partition *U*, Σ, *V*:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重写SVD表达式。我们将假设*m* > *n*。同样，通常情况下，我们按降序排列奇异值：*λ*[1] ≥ *λ*[2] ≥ *λ[n]*。我们将划分*U*，Σ，*V*：
- en: '![](../../OEBPS/Images/eq_04-12-n.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_04-12-n.png)'
- en: It can be proved that *U*[1]Σ[1]*V*[1]*^T* is a rank *r* matrix. Furthermore,
    it is the best rank *r* approximation of *A*.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明 *U*[1]Σ[1]*V*[1]*^T* 是一个秩*r*的矩阵。此外，它是*A*的最佳秩*r*逼近。
- en: '4.6 Machine learning application: Document retrieval'
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 机器学习应用：文档检索
- en: 'We will now bring together several of the concepts we have discussed in this
    chapter with an illustrative toy example: the document retrieval problem we first
    encountered in section [2.1](02.xhtml#sec-vectors). Briefly recapping, we have
    a set of documents {*d*[0],⋯, *d*[6]}. Given an incoming query phrase, we have
    to retrieve documents that match the query phrase. We will use the *bag of words*
    model: that is, our matching approach does not pay attention to *where* a word
    appears in a document; it simply pays attention to *how many times* the word appears
    in the document. Although this technique is not the most sophisticated, it is
    popular because of its conceptual simplicity.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将本章中讨论的几个概念结合到一个说明性的玩具示例中：我们在第[2.1](02.xhtml#sec-vectors)节首次遇到的文档检索问题。简要回顾一下，我们有一组文档{*d*[0],⋯,
    *d*[6]*}。给定一个查询短语，我们必须检索与查询短语匹配的文档。我们将使用*词袋模型*：也就是说，我们的匹配方法不关注一个词在文档中的*位置*；它只关注词在文档中出现的*次数*。尽管这种技术不是最复杂的，但由于其概念简单，它很受欢迎。
- en: 'Our documents are as follows:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文档如下：
- en: '**d*[0]*: Roses are lovely. Nobody hates roses.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d*[0]*: 玫瑰很美丽。没有人讨厌玫瑰。'
- en: '**d*[1]*: *Gun* *violence* has reached epidemic proportions in America.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d*[1]*: 在美国，*枪* *暴力*已经达到流行病的规模。'
- en: '**d*[2]*: The issue of *gun* *violence* is really over-hyped. One can find
    many instances of *violence* where no *guns* were involved.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d*[2]*: *枪* *暴力*问题实际上被过度炒作。可以找到许多没有涉及*枪*的*暴力*实例。'
- en: '**d*[3]*: *Guns* are for *violence* prone people. *Violence* begets *guns*.
    *Guns* beget *violence*.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d*[3]*: *枪*是倾向*暴力*的人用的。*暴力*滋生*枪*。*枪*滋生*暴力*。'
- en: '**d*[4]*: I like *guns* but I hate *violence*. I have never been involved in
    *violence*. But I own many *guns*. *Gun violence* is incomprehensible to me. I
    do believe *gun* owners are the most anti *violence* people on the planet. He
    who never uses a *gun* will be prone to senseless *violence*.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d*[4]*: 我喜欢*枪*但讨厌*暴力*。我从未参与过*暴力*。但我拥有很多*枪*。对我来说，*枪支暴力*是无法理解的。我相信*枪*主是地球上最反*暴力*的人。从不使用*枪*的人容易陷入无意义的*暴力*。'
- en: '**d*[5]*: *Guns* were used in an armed robbery in San Francisco last night.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d*[5]*: 昨晚旧金山发生了一起武装抢劫，使用了*枪*。'
- en: '**d*[6]*: Acts of *violence* usually involve a weapon.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**d*[6]*: *暴力*行为通常涉及武器。'
- en: 4.6.1 Using TF-IDF and cosine similarity
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.1 使用TF-IDF和余弦相似度
- en: Before discussing PCA, let’s look at some more elementary techniques for document
    retrieval. These are based on term frequency-inverse document frequency (TF-IDF)
    and cosine similarity.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论PCA之前，让我们看看一些更基础的文档检索技术。这些技术基于词频-逆文档频率（TF-IDF）和余弦相似度。
- en: Term frequency
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 词频
- en: '*Term frequency* (TF) is defined as the number of occurrences of a particular
    term in a document. (In this context, note that in this book, we use *term* and
    *word* somewhat interchangeably.) In a slightly looser definition, any quantity
    proportional to the number of occurrences of the term is also known as term frequency.
    For example, the TF of the word *gun* in *d*[0], *d*[6] is 0, in *d*[1] is 1,
    in *d*[3] is 3, and so on. Note that we are being case independent. Also, singular/plural
    (*gun* and *guns*) and various flavors of the words originating from the same
    stem (such as *violence* and *violent*) are typically mapped to the same term.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '*词频*（TF）定义为文档中特定词的出现次数。（在此上下文中，请注意，在这本书中，我们使用*词*和*词*有些互换。）在略微宽松的定义中，任何与词出现次数成比例的量也称为词频。例如，*枪*在*d*[0]，*d*[6]中的TF为0，在*d*[1]中为1，在*d*[3]中为3，等等。请注意，我们是不区分大小写的。此外，单复数（*枪*和*guns*）以及来自同一词根的各种词形（如*violence*和*violent*）通常映射到相同的词。'
- en: Inverse document frequency
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 逆文档频率
- en: Certain terms, such as *the*, appear in pretty much all documents. These should
    be ignored during document retrieval. How do we down-weight them?
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 某些词，如*the*，几乎出现在所有文档中。在文档检索过程中应该忽略这些词。我们如何降低它们的权重？
- en: The IDF is obtained by inverting and then taking the absolute value of the logarithm
    of the fraction of all documents in which the term occurs. For terms that occur
    in most documents, the IDF weight is very low. It is high for relatively esoteric
    terms.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: IDF是通过取所有包含该词的文档的分数的倒数，然后取绝对值得到的。对于在大多数文档中出现的词，IDF权重非常低。对于相对冷门的专业术语，IDF权重很高。
- en: Document feature vectors
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 文档特征向量
- en: Each document is represented by a document feature vector. It has as many elements
    as the size of the vocabulary (that is, the number of distinct words over all
    the documents). Every word has a fixed index position in the vector. Given a specific
    document, the value at the index position corresponding to a specific word contains
    the TF of the corresponding word multiplied by that word’s IDF. Thus, every document
    is a point in a space that has as many dimensions as the vocabulary size. The
    coordinate value along a specific dimension is proportional to the number of times
    the word is repeated in the document, with a weigh-down factor for common words.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档都由一个文档特征向量表示。它有与词汇表大小一样多的元素（即，所有文档中不同单词的数量）。每个单词在向量中都有一个固定的索引位置。给定一个特定的文档，对应特定单词的索引位置上的值包含该单词的TF乘以该单词的IDF。因此，每个文档都是在一个具有与词汇表大小一样多维度的空间中的一个点。沿着特定维度的坐标值与单词在文档中重复的次数成正比，对于常见单词有一个降权因子。
- en: 'For real-life document retrieval systems like Google, this vector is extremely
    long. But not to worry: this vector is notional—it is never explicitly stored
    in the computer’s memory. We store a sparse version of the document feature vector:
    a list of unique words along with their TF×IDF scores.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像Google这样的现实生活中的文档检索系统，这个向量非常长。但不用担心：这个向量是概念性的——它永远不会明确存储在计算机的内存中。我们存储文档特征向量的稀疏版本：一个包含唯一单词及其TF×IDF得分的列表。
- en: Cosine similarity
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: 'In section [2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement), we saw that
    the dot product between two vectors measures the agreement between them. Given
    two vectors ![](../../OEBPS/Images/AR_a.png) and ![](../../OEBPS/Images/AR_b.png),
    we know ![](../../OEBPS/Images/AR_a.png) ⋅ ![](../../OEBPS/Images/AR_b.png) =
    ||![](../../OEBPS/Images/AR_a.png)|| ||![](../../OEBPS/Images/AR_b.png)||*cos*(*θ*),
    where the operator || ⋅ || implies the length of a vector and *θ* is the angle
    between the two vectors (see figure [2.7b](02.xhtml#ch2fig-vec_component)). The
    cosine is at its maximum possible value, 1, when the vectors are pointing in the
    same direction and the angle between them is zero. It becomes progressively smaller
    as the angle between the vectors increases until the two vectors are perpendicular
    to each other and the cosine is zero, implying no correlation: the vectors are
    independent of each other.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在[2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement)节中，我们看到了两个向量之间的点积衡量了它们之间的协议。给定两个向量![AR_a](../../OEBPS/Images/AR_a.png)和![AR_b](../../OEBPS/Images/AR_b.png)，我们知道![AR_a](../../OEBPS/Images/AR_a.png)
    ⋅ ![AR_b](../../OEBPS/Images/AR_b.png) = ||![AR_a](../../OEBPS/Images/AR_a.png)||
    ||![AR_b](../../OEBPS/Images/AR_b.png)||*cos*(*θ*)，其中运算符|| ⋅ ||表示向量的长度，*θ*是两个向量之间的角度（见图[2.7b](02.xhtml#ch2fig-vec_component))。当向量指向同一方向且它们之间的角度为零时，余弦值达到最大可能值，即1。随着向量之间角度的增加，余弦值逐渐减小，直到两个向量相互垂直且余弦值为零，这意味着没有相关性：向量相互独立。
- en: The magnitude of the dot product is also proportional to the length of the two
    vectors. We do not want to use the full dot product as a measure of similarity
    between the vectors because two long vectors would have a high similarity score
    even if they were not aligned in direction. Rather, we want to use the cosine,
    defined as
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 点积的大小也与两个向量的长度成正比。我们不希望使用完整的点积作为衡量向量之间相似度的标准，因为即使两个长向量方向不一致，它们也会有一个很高的相似度得分。相反，我们希望使用余弦值，其定义为
- en: '![](../../OEBPS/Images/eq_04-13.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4.13](../../OEBPS/Images/eq_04-13.png)'
- en: Equation 4.13
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式4.13
- en: The cosine similarity between document vectors is a principled way of measuring
    the degree of term sharing between the documents. It is higher if many repeated
    words are shared between the two documents.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 文档向量之间的余弦相似度是衡量文档之间术语共享程度的一种原则性方法。如果两个文档之间共享许多重复的单词，则余弦相似度更高。
- en: 4.6.2 Latent semantic analysis
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.2 隐含语义分析
- en: Cosine similarity and similar techniques suffer from a significant drawback.
    To see this, examine the cosine similarity between *d*[5] and *d*[6]. It is zero.
    But it is obvious to a human that the documents are similar.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度以及类似的技术存在一个显著的缺点。为了看到这一点，检查*d*[5]和*d*[6]之间的余弦相似度。它是零。但人类显然可以看出这两个文档是相似的。
- en: 'What went wrong? Answer: we are measuring only the direct overlap between terms
    in documents. The words *gun* and *violence* occur together in many of the other
    documents, indicating some degree of similarity between them. Hence, documents
    containing only *gun* have some similarity with documents containing only *violence*—but
    cosine similarity between document vectors does not look at such secondary evidence.
    This is the blind spot that LSA tries to overcome.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么问题？答案：我们只测量文档中术语之间的直接重叠。*枪*和*暴力*这两个词在许多其他文档中也一起出现，表明它们之间存在某种程度的相似性。因此，只包含*枪*的文档与只包含*暴力*的文档有一些相似性——但文档向量的余弦相似度并没有考虑这种次级证据。这是LSA试图克服的盲点。
- en: '*Words are known by the company they keep*. That is, if terms appear together
    in many documents (like *gun* and *violence* in the previous examples), they are
    likely to share some semantic similarity. Such terms should be grouped together
    into a common pool of semantically similar terms. Such a pool is called a *topic*.
    Document similarity should be measured in terms of common topics rather than explicit
    common terms. We are particularly interested in topics that discriminate the documents
    in our corpus: that is, there should be a high variation in the degree to which
    different documents subscribe to the topic.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '*词语的意义是通过它们所伴随的词语来认识的*。也就是说，如果某些术语在许多文档中一起出现（如前例中的*枪*和*暴力*），它们很可能具有一些语义相似性。这些术语应该被分组到一个共同的语义相似术语库中。这样的库被称为*主题*。文档相似度应该从共同主题的角度来衡量，而不是从显式的共同术语来衡量。我们特别感兴趣的是那些能够区分我们语料库中文档的主题：也就是说，不同文档对主题的订阅程度应该有很大的差异。'
- en: 'Geometrically, a topic is a subspace in the document feature space. In classical
    latent semantic analysis, we only look at linear subspaces, and a topic can be
    visualized as a direction or linear combination of directions (hyperplane) in
    the document feature space. In particular, any direction line in the space is
    a topic: it is a subspace representing a weighted combination of the coordinate
    axis directions, which means it is a weighted combination of vocabulary terms.
    We are, of course, interested in topics with high variance. These correspond to
    a direction along which the document vectors are well spread, which means the
    document vectors are well discriminated over this topic. We typically prune the
    set of topics, eliminating those with insufficient variance.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 几何上，一个主题是文档特征空间中的一个子空间。在经典潜在语义分析中，我们只考虑线性子空间，一个主题可以被视为文档特征空间中的一个方向或方向的线性组合（超平面）。特别是，空间中的任何方向线都是一个主题：它是一个表示坐标轴方向加权组合的子空间，这意味着它是词汇术语的加权组合。我们当然对具有高变异性的主题感兴趣。这些对应于文档向量分布良好的方向，这意味着文档向量在这个主题上得到了很好的区分。我们通常会对主题集进行剪枝，消除那些变异不足的主题。
- en: From this discussion, a mathematical definition of *topic* begins to emerge.
    Topics are principal components of the matrix of document vectors with individual
    document descriptor vectors along its rows. Measuring document similarity in terms
    of topic has the advantage that two documents may not have many exact words in
    common but may still have a common topic. This happens when they share words belonging
    to the same topic. Essentially, they share a lot of words that occur together
    in other documents. So even if the number of common words is low, we can have
    high document similarity.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次讨论，*主题*的数学定义开始显现。主题是文档向量矩阵的主成分，其中每个文档描述向量沿着其行排列。以主题来衡量文档相似度的优点是，两个文档可能没有许多共同的精确单词，但它们可能仍然有一个共同的主题。这种情况发生在它们共享属于同一主题的单词时。本质上，它们共享了许多在其他文档中一起出现的单词。因此，即使共同单词的数量很少，我们也可以有很高的文档相似度。
- en: '![](../../OEBPS/Images/CH04_F06_Chaudhury.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F06_Chaudhury.png)'
- en: Figure 4.6 Document vectors from our toy dataset *d*[0], ⋯ *d*[6]. Each word
    in the vocabulary corresponds to a separate dimension. Dots show projections of
    document feature vectors on the plane formed by the axes corresponding to the
    terms *gun(s)* and *violence*.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6展示了我们的玩具数据集*d*[0]，⋯ *d*[6]的文档向量。词汇表中的每个词对应一个单独的维度。点表示文档特征向量在由术语*枪(s)*和*暴力*对应的轴形成的平面上的投影。
- en: For instance, in our toy document corpus, *gun* and *violence* are very correlated
    (both or neither is likely to occur in a document). *Gun-violence* emerges as
    a topic. If we express the document vector in terms of this topic instead of the
    individual words, we see similarities that otherwise would have escaped us. That
    is, we see *latent semantic* similarities. For instance, the cosine similarity
    between *d*[5] and *d*[6] is nonzero. This is the core idea of latent semantic
    analysis and is illustrated in figure [4.6](#fig-lsa).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的玩具文档语料库中，*枪* 和 *暴力* 非常相关（两者或两者都不太可能在文档中出现）。*枪-暴力* 作为主题出现。如果我们用这个主题而不是单个单词来表达文档向量，我们会看到其他情况下可能忽略的相似性。也就是说，我们看到了
    *潜在语义* 相似性。例如，*d*[5] 和 *d*[6] 之间的余弦相似度不为零。这是潜在语义分析的核心思想，如图 [4.6](#fig-lsa) 所示。
- en: 'Let’s revisit our example document-retrieval problem in light of topic extraction.
    The document matrix (with document vectors as rows) looks like table [4.1](#tab-doc_vector_dataset).
    Rows correspond to documents, and columns correspond to terms. Each cell contains
    the term frequency. The terms *gun* and *violence* occur an equal number of times
    in most documents, indicating clear correlation. Hence *gun-violence* is a topic.
    The principal components right eigenvectors) identify topics. As usual, we have
    omitted prepositions, conjunctions, commas, and so on. The overall steps are as
    follows (see listing [4.8](#list-svd-lsa-toy) for the Python code):'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 基于主题提取，让我们重新审视我们的文档检索示例。文档矩阵（以文档向量为行）看起来像表 [4.1](#tab-doc_vector_dataset)。行对应文档，列对应术语。每个单元格包含术语频率。术语
    *枪* 和 *暴力* 在大多数文档中出现的次数相等，表明存在明显的相关性。因此，*枪-暴力* 是一个主题。主成分（右特征向量）识别主题。通常，我们省略了介词、连词、逗号等。整体步骤如下（Python
    代码请见列表 [4.8](#list-svd-lsa-toy)）：
- en: Create a document term *m* × *n*. Its rows correspond to documents (*m* documents),
    and its columns correspond to terms (*n* terms).
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文档-术语 *m* × *n* 矩阵。其行对应文档（*m* 个文档），其列对应术语（*n* 个术语）。
- en: Perform SVD on the matrix. This yields *U*, *S*, and *V* matrices. *V* is an
    *n* × *n* orthogonal matrix, and *S* is a diagonal matrix.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对矩阵执行奇异值分解。这会产生 *U*、*S* 和 *V* 矩阵。*V* 是一个 *n* × *n* 的正交矩阵，而 *S* 是一个对角矩阵。
- en: 'The columns of matrix *V* yield topics. These are principal vectors for the
    rows of *X*: that is, eigenvectors of *X^TX* or, equivalently, the covariance
    matrix of *X*.'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵 *V* 的列产生主题。这些是 *X* 的行的主向量：即 *X^TX* 的特征向量，或者等价地，*X* 的协方差矩阵。
- en: The successive elements of each topic vector (column in matrix *V*) tell us
    the contribution of corresponding terms to that topic. Each column is *n* × 1,
    depicting the contributions of the *n* terms in the system.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个主题向量（矩阵 *V* 中的列）的连续元素告诉我们对应术语对该主题的贡献。每个列是 *n* × 1，描述了系统中 *n* 个术语的贡献。
- en: 'The diagonal elements of *S* tell us the weights (importance) of corresponding
    topics. These are the eigenvalues of *X^TX*: that is, principal values of the
    row vectors of *X*.'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*S* 的对角元素告诉我们对应主题的权重（重要性）。这些是 *X^TX* 的特征值：即 *X* 的行向量的主值。'
- en: Inspect the weights, and choose a cutoff. All topics below that weight are discarded—the
    corresponding columns of *V* are thrown away. This yields a matrix *V* with fewer
    columns but the same number of rows); these are the topic vectors of interest
    to us. We have reduced the dimensionality of the problem. If the number of retained
    topics is *t*, the reduced *V* is *m* × *t*.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查权重，并选择一个截止值。所有低于该权重的主题都被丢弃——相应的 *V* 的列被丢弃。这产生了一个具有较少列但行数相同的矩阵 *V*；这些是我们感兴趣的主题向量。我们已经降低了问题的维度。如果保留的主题数量是
    *t*，则减少的 *V* 是 *m* × *t*。
- en: 'By projecting (multiplying) the original matrix *X* of document terms to this
    new matrix *V*, we get an *m* × *t* matrix of document topics (it has same number
    of rows as *X* but fewer columns). This is the projection of *X* to the topic
    space: that is, a topic-based representation of the document vectors.'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过（乘以）原始文档术语矩阵 *X* 到这个新矩阵 *V*，我们得到一个 *m* × *t* 的文档主题矩阵（它具有与 *X* 相同的行数但较少的列）。这是
    *X* 到主题空间的投影：即文档向量的基于主题的表示。
- en: Rows of the document topic matrix will henceforth be taken as document representations.
    Document similarities will be computed by taking the cosine similarity of these
    rows rather than the rows of the original document term matrix. This cosine similarity,
    in the topic space, will capture many indirect connections that were not visible
    in the original input space.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从此以后，文档主题矩阵的行将被视为文档表示。文档相似度将通过计算这些行的余弦相似度来计算，而不是原始文档词矩阵的行。这种在主题空间中的余弦相似度将捕获许多在原始输入空间中不可见的间接连接。
- en: Table 4.1 Document matrix for the toy example dataset
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 玩具示例数据集的文档矩阵
- en: '|  | Violence | Gun | America | ⋯ | Roses |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | 暴力 | 枪 | 美国 | ⋯ | 玫瑰 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **d**[0] | 0 | 0 | 0 | ⋯ | 2 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| **d**[0] | 0 | 0 | 0 | ⋯ | 2 |'
- en: '| **d**[1] | 1 | 1 | 1 | ⋯ | 0 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| **d**[1] | 1 | 1 | 1 | ⋯ | 0 |'
- en: '| **d**[2] | 2 | 2 | 0 | ⋯ | 0 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| **d**[2] | 2 | 2 | 0 | ⋯ | 0 |'
- en: '| **d**[3] | 3 | 3 | 0 | ⋯ | 0 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| **d**[3] | 3 | 3 | 0 | ⋯ | 0 |'
- en: '| **d**[4] | 5 | 5 | 0 | ⋯ | 0 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| **d**[4] | 5 | 5 | 0 | ⋯ | 0 |'
- en: '| **d**[5] | 0 | 1 | 0 | ⋯ | 0 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| **d**[5] | 0 | 1 | 0 | ⋯ | 0 |'
- en: '| **d**[6] | 1 | 0 | 0 | ⋯ | 0 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| **d**[6] | 1 | 0 | 0 | ⋯ | 0 |'
- en: 4.6.3 PyTorch code to perform LSA
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.3 执行LSA的PyTorch代码
- en: The following listing demonstrates how to compute the LSA for our toy dataset
    from table [4.1](#tab-doc_vector_dataset). Fully functional code for this section
    can be found at [http://mng.bz/E2Gd](http://mng.bz/E2Gd).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表演示了如何从表 [4.1](#tab-doc_vector_dataset) 计算我们的玩具数据集的LSA。本节的完整功能代码可在 [http://mng.bz/E2Gd](http://mng.bz/E2Gd)
    找到。
- en: Listing 4.8 Computing LSA
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.8 计算LSA
- en: '[PRE13]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Considers only four terms for simplicity
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为了简单起见，仅考虑四个术语
- en: ② Document term matrix. Each row describes a document. Each column contains
    TF scores for one term. IDF is ignored for simplicity.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ② 文档词矩阵。每一行描述一个文档。每一列包含一个术语的TF分数。为了简单起见，忽略了IDF。
- en: '③ Performs SVD on the doc-term matrix. Columns of the resulting matrix *V*
    correspond to topics. These are eigenvectors of *X^TX*: principal vectors of the
    doc-term matrix. A topic corresponds to the direction of maximum variance in the
    doc feature space.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 对文档词矩阵执行SVD。结果矩阵 *V* 的列对应于主题。这些是 *X^TX* 的特征向量：文档词矩阵的主向量。一个主题对应于文档特征空间中最大方差的方向。
- en: ④ *S* indicates the diagonal matrix of principal values. These signify topic
    weights (importance). We choose a cut-off and discard all topics below that weight
    (dimensionality reduction). Only the first few columns of *V* are retained. Principal
    values (topic weights) for this dataset are shown in the output. Only one topic
    is retained in this example.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ④ *S* 表示主值对角矩阵。这些表示主题权重（重要性）。我们选择一个截止值并丢弃所有低于该权重的主题（降维）。仅保留 *V* 的前几列。此数据集的主值（主题权重）显示在输出中。本例中仅保留一个主题。
- en: ⑤ Elements of the topic vector show the contributions of corresponding terms
    to the topic.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 主题向量的元素表示对应术语对主题的贡献。
- en: ⑥ Cosine similarity in the feature space fails to capture d, d6 similarity.
    LSA succeeds.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 在特征空间中的余弦相似度无法捕获 d, d6 相似度。LSA 成功。
- en: 'The output is as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 4.6.4 PyTorch code to compute LSA and SVD on a large dataset
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.4 在大型数据集上计算LSA和SVD的PyTorch代码
- en: 'Suppose we have a set of 500 documents over a vocabulary of 3 terms. This is
    an unrealistically short vocabulary, but it allows us to easily visualize the
    space of document vectors. Each document vector is a 3 × 1 vector, and there are
    500 such vectors. Together they form a 500 × 3 data matrix *X*. In this dataset,
    the terms *x*0 and *x*1 are correlated: *x*0 occurs randomly between 0 and 100
    times in a document, and *x*1 occurs twice as many times as *x*0 except for small
    random fluctuations. The third term’s frequency varies between 0 and 5. From section
    [4.6](../Text/04.xhtml#sec-lsa), we know that *x*0, *x*1 together form a single
    topic, while *x*2 by itself forms another topic. We expect a principal component
    along each topic.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含 500 个文档的词汇表，词汇表包含 3 个术语。这是一个不切实际的短词汇表，但它使我们能够轻松地可视化文档向量空间。每个文档向量是一个
    3 × 1 向量，共有 500 个这样的向量。它们共同构成一个 500 × 3 的数据矩阵 *X*。在此数据集中，术语 *x*0 和 *x*1 是相关的：*x*0
    在文档中随机出现 0 到 100 次，而 *x*1 的出现次数是 *x*0 的两倍，除了小的随机波动。第三个术语的频率在 0 到 5 之间。从第 [4.6](../Text/04.xhtml#sec-lsa)
    节中，我们知道 *x*0 和 *x*1 一起形成一个主题，而 *x*2 单独形成另一个主题。我们期望每个主题都有一个主成分。
- en: Listing 4.9 creates the dataset, computes the SVD, plots the dataset, and shows
    the first two principal components. The third singular value is small compared
    to the first. We can ignore that dimension—it corresponds to the small random
    variation within the *x*0 − *x*1 topic. The singular values are printed out and
    also shown graphically along with the data points in figure [4.7](#fig-svd-lsa).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.9创建数据集，计算奇异值分解（SVD），绘制数据集，并显示前两个主成分。第三个奇异值相对于第一个来说很小。我们可以忽略这个维度——它对应于*x*0
    − *x*1主题中的小随机变化。奇异值被打印出来，并在图[4.7](#fig-svd-lsa)中与数据点一起以图形方式展示。
- en: '![](../../OEBPS/Images/CH04_F07_Chaudhury.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH04_F07_Chaudhury.png)'
- en: Figure 4.7 Latent semantic analysis. Note that the vertical axis line is actually
    much smaller than it appears to be in the diagram.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7潜在语义分析。请注意，垂直轴线实际上比图中看起来要小得多。
- en: Listing 4.9 LSA using SVD
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.9使用SVD的LSA
- en: '[PRE15]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '① 3D dataset: the first two axes are linearly correlated; the third axis has
    small near-zero random values.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ① 3D数据集：前两个轴是线性相关的；第三个轴有小的接近零的随机值。
- en: ② The third singular value is relatively small; we ignore it
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ② 第三个奇异值相对较小；我们忽略它
- en: ③ The first two principal vectors represent topics. Projecting data points on
    them yields document descriptors in terms of the two topics.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 前两个主向量代表主题。将数据点投影到它们上，可以得到两个主题的文档描述符。
- en: 'Here is the output:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE16]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Summary
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we studied several linear algebraic tools used in machine
    learning and data science:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了机器学习和数据科学中使用的几个线性代数工具：
- en: The direction (unit vector) that maximizes (minimizes) the quadratic form *x̂^TAx̂*
    is the eigenvector corresponding to the largest (smallest) eigenvalue of matrix
    *A*. The magnitude of the quadratic form when *x̂* is along those directions is
    the largest (smallest) eigenvalue of *A*.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使二次型*x̂^TAx̂*最大（最小）的方向（单位向量）是矩阵*A*对应于最大（最小）特征值的特征向量。当*x̂*沿着这些方向时，二次型的幅度是*A*的最大（最小）特征值。
- en: Given a set of points *X* = {![](../../OEBPS/Images/AR_x.png)^((0)), ![](../../OEBPS/Images/AR_x.png)^((1)),
    ![](../../OEBPS/Images/AR_x.png)^((2)), ⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}
    in an *n* + 1-dimensional space, we can define the mean vector and covariance
    matrix as
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个在*n* + 1维空间中的点集*X* = {![](../../OEBPS/Images/AR_x.png)^((0)), ![](../../OEBPS/Images/AR_x.png)^((1)),
    ![](../../OEBPS/Images/AR_x.png)^((2)), ⋯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}，我们可以定义均值向量和协方差矩阵为
- en: '![](../../OEBPS/Images/eq_04-13-a.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_04-13-a.png)'
- en: The variance along an arbitrary direction (unit vector) *l̂* is *l̂* ^T*Cl̂*.
    This is a quadratic form. Consequently, the maximum (minimum) variance of a set
    of data points in multidimensional space occurs along the eigenvector corresponding
    to the largest (smallest) eigenvalue of the covariance matrix. This direction
    is called the first principal axis of the data. The subsequent eigenvectors, sorted
    in order of decreasing eigenvalues, are mutually orthogonal (perpendicular) and
    yield the subsequent direction of maximum variance. This technique is known as
    principal component analysis (PCA).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 沿任意方向（单位向量）*l̂*的方差是*l̂* ^T*Cl̂*。这是一个二次型。因此，多维空间中一组数据点的最大（最小）方差发生在协方差矩阵对应于最大（最小）特征值的特征向量上。这个方向被称为数据的第一主轴。后续的特征向量，按特征值递减的顺序排序，是相互正交（垂直）的，并产生后续的最大方差方向。这种技术被称为主成分分析（PCA）。
- en: In many real-life cases, larger variances correspond to the true underlying
    pattern of the data, while smaller variances correspond to noise (such as measurement
    error). Projecting the data on the principal axes corresponding to the larger
    eigenvalues yields lower-dimensional data that is relatively noise-free. The projected
    data points also match the true underlying pattern more closely, yielding better
    insights. This is known as dimensionality reduction.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多实际情况下，较大的方差对应于数据的真实潜在模式，而较小的方差对应于噪声（如测量误差）。将数据投影到对应于较大特征值的协方差矩阵的特征轴上，可以得到相对无噪声的较低维数据。投影的数据点也与真实潜在模式更接近，从而获得更好的洞察。这被称为降维。
- en: 'Singular value decomposition (SVD) allows us to decompose an arbitrary *m*
    × *n* matrix *A* as a product of three matrices: *A* = *U*Σ*V^T*, where *U*, *V*
    are orthogonal and Σ is diagonal. Matrix *V* has the eigenvectors of *A^TA* as
    its columns. *U* has eigenvectors of *AA^T* as columns. Σ has the eigenvalues
    of *A^TA* sorted in decreasing order) in its diagonal.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单值分解（SVD）允许我们将任意 *m* × *n* 矩阵 *A* 分解为三个矩阵的乘积：*A* = *U*Σ*V^T*，其中 *U* 和 *V* 是正交矩阵，Σ
    是对角矩阵。矩阵 *V* 的列是 *A^TA* 的特征向量。*U* 的列是 *AA^T* 的特征向量。Σ 的对角线元素是按降序排列的 *A^TA* 的特征值。
- en: 'SVD provides a numerically stable way to solve the linear system of equations
    *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png). In particular,
    for nonsquare matrices, it provides the closest approximations: that is, ![](../../OEBPS/Images/AR_x.png)
    that minimizes ||*A*![](../../OEBPS/Images/AR_x.png) − ![](../../OEBPS/Images/AR_b.png)||.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD 提供了一种数值稳定的线性方程组 *A*![](../../OEBPS/Images/AR_x.png) = ![](../../OEBPS/Images/AR_b.png)
    的解法。特别是，对于非方阵，它提供了最接近的近似：即 *A*![](../../OEBPS/Images/AR_x.png)，它最小化了 ||*A*![](../../OEBPS/Images/AR_x.png)
    − ![](../../OEBPS/Images/AR_b.png)||。
- en: Given a dataset *X* whose rows are data vectors corresponding to individual
    instances and columns correspond to feature values, *X^TX* ields the covariance
    matrix. Thus eigenvectors of *X^TX* ield the data’s principal components. Since
    the SVD of *X* has eigenvectors of *X^TX* as columns of the matrix *V*, SVD is
    an effective way to compute PCA.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个数据集 *X*，其中行对应于个体实例的数据向量，列对应于特征值，*X^TX* 得到协方差矩阵。因此，*X^TX* 的特征向量是数据的主成分。由于
    *X* 的奇异值分解的列向量是 *X^TX* 的特征向量，因此奇异值分解是计算主成分分析（PCA）的有效方法。
- en: When using machine learning data science for document retrieval, the bag-of-words
    model represents documents with document vectors that contain the term frequency
    (number of occurrences) of each term in the document.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用机器学习数据科学进行文档检索时，词袋模型用包含文档中每个术语的词频（出现次数）的文档向量来表示文档。
- en: TF-IDF is a cosine similarity technique for document matching and retrieval.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF 是一种用于文档匹配和检索的余弦相似度技术。
- en: 'Latent semantic analysis (LSA) does topic modeling: we perform PCA on the document
    vectors to identify topics. Projecting document vectors onto topic axes allows
    LSA to see latent (indirect) similarities beyond the direct overlapping of terms.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在语义分析（LSA）进行主题建模：我们对文档向量进行主成分分析以识别主题。将文档向量投影到主题轴上，使得 LSA 能够看到超越直接术语重叠的潜在（间接）相似性。
