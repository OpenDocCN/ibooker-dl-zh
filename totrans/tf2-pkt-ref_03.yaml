- en: Chapter 3\. Data Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to prepare and set up data for training. Some
    of the most common data formats for ML work are tables, images, and text. There
    are commonly practiced techniques associated with each, though how you set up
    your data engineering pipeline will, of course, depend on what your problem statement
    is and what you are trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll look at all three formats in detail, using specific examples to walk you
    through the techniques. All the data can be read directly into your Python runtime
    memory; however, this isn’t the most efficient way to use your compute resources.
    When I discuss text data, I’ll give particular attention to tokenization and dictionaries.
    By the end of this chapter, you’ll have learned how to prepare table, image, and
    text data for training.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Tabular Data for Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a tabular dataset, it is important to identify which columns are considered
    categorical, because you have to encode their value as a class or a binary representation
    of the class (one-hot encoding), rather than a numerical value. Another aspect
    of tabular datasets is the potential for interactions among multiple features.
    This section will also look at the API that TensorFlow provides to make it easier
    to model column interactions.
  prefs: []
  type: TYPE_NORMAL
- en: It’s common to encounter tabular datasets as CSV files or simply as structured
    output from a database query. For this example, we’ll start with a dataset that’s
    already in a pandas DataFrame and then learn how to transform it and set it up
    for model training. We’ll use the *Titanic* dataset, an open source, tabular dataset
    that is often used for teaching because of its manageable size and availability.
    This dataset contains attributes for each passenger, such as age, gender, cabin
    grade, and whether or not they survived. We are going to try to predict each passenger’s
    probability of survival based on their attributes or features. Be aware that this
    is a small dataset for teaching and learning purposes only. In reality, your dataset
    will likely be much larger. You may make different decisions and choose different
    default values for some of these input parameters, so don’t take this example
    too literally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with loading all the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data from Google’s public storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now take a look at `train_file_path`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This file path points to a CSV file, which we’ll read as a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-1](#titanic_dataset_as_a_pandas_dataframe) shows what `titanic_df`
    looks like as a pandas DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Titanic dataset as a pandas DataFrame](Images/t2pr_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. *Titanic* dataset as a pandas DataFrame
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Marking Columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see in [Figure 3-1](#titanic_dataset_as_a_pandas_dataframe), there
    are numeric as well as categorical columns in this data. The target column, or
    the column for prediction, is the “survived” column. You’ll need to mark it as
    the target and mark the rest of the columns as features.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A best practice in TensorFlow is to convert your table into a streaming dataset.
    This practice ensures that the data’s size does not affect memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do exactly that, TensorFlow provides the function `tf.data.experimental.make_csv_dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function signature, you specify the file path for which you
    wish to generate a dataset object. The `batch_size` is arbitrarily set to something
    small (3 in this case) for convenience in inspecting the data. We also set `label_name`
    to the “survived” column. For data quality, if a question mark (?) is specified
    in any cell, you want it to be interpreted as “NA” (not applicable). For training,
    set `num_epochs` to iterate over the dataset once. You can ignore any parsing
    errors or empty lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, inspect the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It will appear similar to [Figure 3-2](#a_batch_of_the_titanic_dataset).
  prefs: []
  type: TYPE_NORMAL
- en: '![A batch of the Titanic dataset](Images/t2pr_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A batch of the *Titanic* dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the major steps for training a paradigm to consume your training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Designate columns by feature types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide whether or not to embed or cross columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the columns of interest, possibly as an experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a “feature layer” for consumption by the training paradigm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you have set up the data as datasets, you can designate each column
    by its feature type, such as numeric or categorical, bucketized (by binning) if
    necessary. You can also embed the column if there are too many unique categories
    and dimension reduction would be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go ahead with step 1\. There are four numeric columns: `age`, `n_siblings_spouses`,
    `parch`, and `fare`. Five columns are categorical: `sex`, `class`, `deck`, `embark_town`,
    and `alone`. You will create a `feature_columns` list to hold all the feature
    columns once you are done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to designate numeric columns based strictly on the actual numeric
    values, without any transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in addition to using `age` as is, you could also bin `age` into a
    bucket, such as by quantile of age distribution. But what are the bin boundaries
    (quantiles)? You can inspect the general statistics of numeric columns in a pandas
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-3](#statistics_for_numeric_columns_in_the_ti) shows the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistics for numeric columns in the Titanic dataset](Images/t2pr_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Statistics for numeric columns in the *Titanic* dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s try three bin boundaries for age: 23, 28, and 35\. This means passenger
    age will be grouped into first quantile, second quantile, and third quantile (as
    shown in [Figure 3-3](#statistics_for_numeric_columns_in_the_ti)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, in addition to “age,” you have generated another column, “age_bucket.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the nature of each categorical column, it would be helpful to
    know the distinct values in them. You’ll need to encode the vocabulary list with
    the unique entries in each column. For categorical columns, this means you need
    to determine which entries are unique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in [Figure 3-4](#unique_values_in_each_categorical_column).
  prefs: []
  type: TYPE_NORMAL
- en: '![Unique values in each categorical column of the Titanic dataset](Images/t2pr_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Unique values in each categorical column of the dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You need to keep track of these unique values in a dictionary format for the
    model to do the mapping and lookup. Therefore, you’ll encode unique categorical
    values in the “sex” column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if the list is long, it becomes inconvenient to write it out. Instead,
    as you iterate through the categorical columns, you can save each column’s unique
    values in a Python dictionary data structure `h` for future lookup. Then you can
    pass the unique value as a list into these vocabulary lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also embed the “deck” column, since there are eight unique values,
    more than any other categorical column. Reduce its dimension to 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way to reduce the dimensions of categorical columns is by using a *hashed
    feature column*. This method calculates a hashed value based on the input data.
    It then designates a hashed bucket for the data. The following code reduces the
    dimension of the “class” column to 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Encoding Column Interactions as Possible Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now comes the most interesting part: you’re going to find interactions between
    different features (this is referred to as *crossing columns*) and encode those
    interactions as possible features. This is also where your intuition and domain
    knowledge can benefit your feature engineering endeavor. For example, a question
    that comes to mind based on the historical background of the *Titanic* disaster
    is this: were women in first-class cabins more likely to survive than women in
    second- or third-class cabins? To rephrase this as a data science question, you’ll
    need to consider interactions between the gender and cabin class of the passengers.
    Then you’ll need to pick a starting dimension size to represent the data variability.
    Let’s say you arbitrarily decide to bin the variability into five dimensions (`hash_bucket_size`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have created all the features, you need to put them together—and
    perhaps experiment to decide which to include in the training process. For that,
    you’ll first create a list to hold all the features you want to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you’ll append each feature of interest to the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create a feature layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This layer will serve as the first (input) layer in the model you are about
    to build and train. This is how you’ll provide all the feature engineering frameworks
    for the model’s training process.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Cross-Validation Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you start training, you have to create a small dataset for cross-validation
    purposes. Since there are only two partitions (training and testing) to begin
    with, one way to generate a cross-validation dataset is to simply subdivide one
    of the partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, 40% of the original `test_df` partition was randomly reserved as `test_df`,
    and the remaining 60% is now `val_df`. Usually, test datasets are the smallest
    of the three (training, validation, testing), since they are used only for final
    evaluation, and not during model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have taken care of feature engineering and data partitioning,
    there is one last thing to do: stream the data into the training process with
    the dataset. You’ll convert each of the three DataFrames (training, validation,
    and testing) into its own dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, first you’ll arbitrarily decide how many samples
    to include in a batch (`batch_size`). Then you need to set aside a label designation
    (`survived`). The `tf.data.Dataset.from_tensor_slices` method takes a tuple as
    an argument. In this tuple, there are two elements: feature columns and the label
    column.'
  prefs: []
  type: TYPE_NORMAL
- en: The first element is `dict(train_df)`. This `dict` operation essentially transforms
    the DataFrame into a key-value pair, where each key represents a column name and
    the corresponding value is an array of the values in the column. The other element
    is `labels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we shuffle and batch the dataset. Since this conversion will be applied
    to all three datasets, it would be convenient to combine these steps into a helper
    function to reduce repetition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can apply this function to both validation and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Starting the Model Training Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now you’re ready to start the model training process. Technically this isn’t
    a part of preprocessing, but running through this short section will allow you
    to see how the work you have done fits into the model training process itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll start by building a model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: For demonstration purposes, you’ll build a simple two-layer, deep-learning perceptron
    model, which is a basic configuration of a feedforward neural network. (For more
    on this, see Aurélien Géron’s [*Neural Networks and Deep Learning*](https://oreil.ly/1wOQj)
    (O’Reilly)). Notice that since this is a multilayer perceptron model, you’ll use
    the sequential API. Inside this API, the first layer is `feature_layer`, which
    represents all the feature engineering logic and derived features, such as age
    bins and crosses, that are used to model the feature interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model and set up the loss function for binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can start the training. You’ll only train it for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You can expect an outcome similar to that pictured in [Figure 3-5](#example_training_outcome_from_survival_p).
  prefs: []
  type: TYPE_NORMAL
- en: '![Example training outcome from survival prediction in the Titanic dataset](Images/t2pr_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Example training outcome from survival prediction in the *Titanic*
    dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you saw how to deal with tabular data that consists of multiple
    data types. You also saw that TensorFlow provides a `feature_column` API, which
    enables proper casting of data types, handling of categorical data, and feature
    crossing for potential interactions. This API is very helpful in simplifying data
    and feature engineering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Image Data for Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For images, you need to reshape or resample all the images into the same pixel
    count; this is known as *standardization*. You also need to ensure that all pixel
    values are within the same color range so that they fall within the finite range
    of RGB values of each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Image data comes with different file extensions, such as *.jpg*, *.tiff*, and
    *.bmp*. These are not really problematic, as there are APIs in Python and TensorFlow
    that can read and parse images with any file extension. The tricky part about
    image data is capturing its dimensions—height, width, and depth—as measured by
    pixel counts. (If it is a color image encoded with RGB, these appear as three
    separate channels.)
  prefs: []
  type: TYPE_NORMAL
- en: If all the images in your dataset (including training, validation, and all the
    images during testing or deployment time) are expected to have the same dimensions
    *and* you are going to build your own model, then processing image data is not
    too much of a problem. However, if you wish to leverage prebuilt models such as
    ResNet or Inception, then you have to conform to their image requirements. As
    an example, ResNet requires each input image to be 224 × 224 × 3 pixels and be
    presented as a NumPy multidimensional array. This means that, in the preprocessing
    routine, you have to resample your images to conform to those dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Another situation for resampling arises when you cannot reasonably expect all
    the images, especially during deployment, to have the same size. In this case,
    you need to consider a proper image dimension as you build the model, then set
    up the preprocessing routine to ensure that resampling is done properly.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you are going to use the flower dataset provided by TensorFlow.
    It consists of five types of flowers and diverse image dimensions. This is a convenient
    dataset to use, since all images are already in JPEG format. You are going to
    process this image data to train a model to parse each image and classify it as
    one of the five classes of flowers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, import all necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now download the flower dataset from the URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This file is a compressed TAR archive file. Therefore, you need to set `untar=True`.
  prefs: []
  type: TYPE_NORMAL
- en: When using `tf.keras.utils.get_file`, by default you will find the downloaded
    data in the `~/.keras/datasets` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Mac or Linux system’s Jupyter Notebook cell, execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You will find the flower dataset as shown in [Figure 3-6](#flower_dataset_folders).
  prefs: []
  type: TYPE_NORMAL
- en: '![Flower dataset folders](Images/t2pr_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Flower dataset folders
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let’s take a look at one of the flower types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You should see the first nine images, as shown in [Figure 3-7](#ten_example_image_files_in_the_rose_dire).
  prefs: []
  type: TYPE_NORMAL
- en: '![Ten example image files in the rose directory](Images/t2pr_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Nine example image files in the rose directory
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These images are all different sizes. You can verify this by examining a couple
    of images. Here is a helper function you can leverage to display the image in
    its original size:^([1](ch03.xhtml#idm45772916304408))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use it to display an image (as shown in [Figure 3-8](#rose_image_sample_one)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Rose image sample 1](Images/t2pr_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Rose image sample 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now try a different image (as shown in [Figure 3-9](#rose_image_sample_two)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Rose image sample 2](Images/t2pr_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Rose image sample 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, the dimensions and aspect ratios of these images are different.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming Images to a Fixed Specification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now you’re ready to transform these images to a fixed specification. In this
    particular example, you’ll use the ResNet input image spec, which is 224 × 224
    with three color channels (RGB). Also, it is a best practice to use data streaming
    whenever possible. Therefore, your goal here is to transform these color images
    into the shape of 224 × 224 pixels and build a dataset from them for streaming
    into the training paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this you’ll use the `ImageDataGenerator` class and the `flow_from_directory`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '`ImageDataGenerator` is responsible for creating a generator object, which
    generates streaming data from the directory as specified by `flow_from_directory`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the coding pattern is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In both cases, keyword argument options, or `kwargs`, give your code great
    flexibility. (Keyword arguments are frequently seen in Python.) These arguments
    enable you to pass optional parameters to the function. As it turns out, in `ImageDataGenerator`,
    there are two parameters relevant to your needs: `rescale` and `validation_split`.
    The `rescale` parameter is used for normalizing pixel values into a finite range,
    and `validation_split` lets you subdivide a partition of data, such as for cross
    validation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `flow_from_directory`, there are three parameters that are useful for this
    example: `target_size`, `batch_size`, and `interpolation`. The `target_size` parameter
    helps you specify the desired dimension of each image, and `batch_size` is for
    specifying the number of samples in a batch of images. As for `interpolation`,
    remember how you need to interpolate, or resample, each image to a prescribed
    dimension specified with `target_size`? Supported methods for interpolation are
    `nearest`, `bilinear`, and `bicubic`. For this example, first try `bilinear`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can define these keyword arguments as follows. Later you’ll pass them into
    their function calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a generator object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can specify the source directory from which this generator will stream
    the data. This generator will only stream 20% of the data, and this is designated
    as a validation dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the same generator object for training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the output of the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The output is represented as NumPy arrays. For a batch of images, the sample
    size is 32, with 224 pixels in height and width and three channels representing
    RGB color space. For the label batch, there are likewise 32 samples. Each row
    is one-hot encoded to represent which of the five classes it belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important thing to do is to retrieve the lookup dictionary of labels.
    During inferencing, the model will output the probability for each of the five
    classes. The only way to decode which class has the highest probability is with
    a prediction lookup dictionary of labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'A typical output from our classification model would be a NumPy array similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The position with the highest probability value is the first element. Map this
    index to the first key in `idx_labels`—in this case, `daisy`. This is how you
    capture the results of the prediction. Save the `idx_labels` dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how to load it back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Training the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, for training you’ll use a model built from a pretrained ResNet feature
    vector. This technique is known as *transfer learning*. TensorFlow Hub hosts many
    pretrained models for free. This is how to access it during your model construction
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The first layer is `InputLayer`. Remember that the expected input is 224 ×
    224 × 3 pixels. You’ll use the tuple addition trick to append an extra dimension
    to `IMAGE_SIZE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now you have (224, 224, 3), which is a tuple that represents the dimension of
    an image as a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: The next layer is the pretrained ResNet feature vector referenced by the URL
    to TensorFlow Hub. Let’s use it as is so that we don’t have to retrain it.
  prefs: []
  type: TYPE_NORMAL
- en: Next is the `Dense` layer with five nodes of output. Each output is the probability
    of the image belonging to that class. Then you’ll build the model skeleton, with
    `None` as the first dimension. This means the first dimension, which represents
    the sample size of a batch, is not decided until runtime. This is how to handle
    batch input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspect the model summary to make sure it’s what you expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The output is shown in [Figure 3-10](#image_classification_model_summary).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image classification model summary](Images/t2pr_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Image classification model summary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compile the model with `optimizers` and the corresponding `losses` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'And then train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: You may see output similar to that in [Figure 3-11](#output_from_training_the_image_classific).
  prefs: []
  type: TYPE_NORMAL
- en: '![Output from training the image classification model](Images/t2pr_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Output from training the image classification model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you learned how to process image files. Specifically, it is
    necessary to make sure you have a predetermined image size requirement set before
    you design the model. Once that standard is accepted, the next step is to resample
    images into that size and normalize the pixel’s value into a smaller dynamic range.
    These routines are nearly universal. Also, streaming images into the training
    workflow is the most efficient method and a best practice, especially in cases
    where your working sample size approaches your Python runtime’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Text Data for Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For text data, each word or character needs to be represented as a numerical
    integer. This process is known as *tokenization*. Further, if the goal is classification,
    then the target needs to be encoded as *classes*. If the goal is something more
    complicated, such as translation, then the target language in the training data
    (such as French in an English-to-French translation) also requires its own tokenization
    process. This is because the target is essentially a long string of text, just
    like the input text. Likewise, you also need to think about whether to tokenize
    the target at the word or character level.
  prefs: []
  type: TYPE_NORMAL
- en: Text data can be presented in many different formats. From a content organization
    perspective, it may be stored and organized as a table, with one column containing
    the body or string of text and another column containing labels, such as a binary
    sentiment indicator. It may be a free-form file, with lines of different lengths
    and a carriage return at the end of each line. It may be a manuscript in which
    blocks of text are defined by paragraphs or sections.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to determine the processing techniques and logic to use
    as you set up a natural language processing (NLP) machine learning problem; this
    section will cover some of the most frequently used techniques.
  prefs: []
  type: TYPE_NORMAL
- en: This example will use text from William Shakespeare’s tragedy *Coriolanus,*
    which is a simple public-domain example hosted on Google. You will build a *text
    generation model* that will learn how to write in Shakespeare’s style.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text is represented by strings of characters. These characters need to be converted
    to integers for modeling tasks. This example is a raw text string from *Coriolanus*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the necessary libraries and download the text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Open it and output a few lines of sample text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect this text by printing the first 400 characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The output is shown in [Figure 3-12](#sample_of_william_shakespeareapostrophes).
  prefs: []
  type: TYPE_NORMAL
- en: 'To tokenize each character in this file, a simple `set` operation will suffice.
    This operation will create a unique set of characters found in the text string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: A glimpse of the list `vocabulary` is shown in [Figure 3-13](#part_of_the_vocabulary_list_from_coriola).
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample of William Shakespeare’s Coriolanus](Images/t2pr_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Sample of William Shakespeare’s Coriolanus
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Part of the vocabulary list from Coriolanus](Images/t2pr_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. Part of the vocabulary list from Coriolanus
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These tokens include punctuation, as well as both upper- and lowercase characters.
    It is not always necessary to include both upper- and lowercase characters; if
    you don’t want to, you can convert every character to lowercase before performing
    the `set` operation. Since you sorted the token list, you can see that special
    characters are also being tokenized. In some cases, this is not necessary; these
    tokens can be removed manually. The following code will convert all characters
    to lowercase and then perform the `set` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'You might be wondering if it would be reasonable to tokenize text at the word
    level instead of the character level. After all, the word is the fundamental unit
    of semantic understanding of a text string. Although this reasoning is sound and
    has some logic to it, in reality it creates more work and problems, while not
    really adding value to the training process or accuracy to the model. To illustrate
    why, let’s try to tokenize the text string by words. The first thing to recognize
    is that words are separated by spaces. So you need to split the text string on
    spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Inspect the list `vocabulary_word`, shown in [Figure 3-14](#sample_of_tokenized_words).
  prefs: []
  type: TYPE_NORMAL
- en: With special characters and carriage returns embedded in each word token, this
    list is nearly unusable. It would require considerable work to clean it up with
    regular expressions or more sophisticated logic. In some cases, punctuation marks
    are attached to words. Further, the list of word tokens is much larger than the
    character-level token list. This makes it much more complicated for the model
    to learn the patterns in the text. For these reasons and the lack of proven benefit,
    it is not a common practice to tokenize text at the word level. If you wish to
    use word-level tokenization, then it is common to perform a word-embedding operation
    to reduce the variability and dimensions of the utterance representations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample of tokenized words](Images/t2pr_0314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. Sample of tokenized words
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Creating a Dictionary and Reverse Dictionary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have the list of tokens that contains the chosen characters, you’ll
    need to map each token to an integer. This is known as the *dictionary*. Likewise,
    you’ll need to create a *reverse dictionary* that maps the integers back to the
    tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating an integer is easy with the `enumerate` function. This function
    takes a list as input and returns an integer corresponding to each unique element
    in the list. In this case, the list contains tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: You can see a sample of this result in [Figure 3-15](#sample_enumerated_output_of_a_token_list).
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample enumerated output of a token list](Images/t2pr_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15\. Sample enumerated output of a token list
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next you need to make this into a dictionary. A dictionary is really a collection
    of key-value pairs used as a lookup table: when you give it a key, it returns
    the value corresponding to that key. The notation to build a dictionary, with
    the key being the token and the value being the integer, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The output will look like [Figure 3-16](#sample_of_character-to-index_dictionary).
  prefs: []
  type: TYPE_NORMAL
- en: 'This dictionary is used to convert text into integers. At inference time, the
    model output is also in the format of integers. Therefore, if you want the output
    as text, then you’ll need a reverse dictionary to map the integers back to characters.
    To do this, simply reverse the order of `i` and `u`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![Sample of character-to-index dictionary](Images/t2pr_0316.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. Sample of character-to-index dictionary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tokenization is the most basic and necessary step in most NLP problems. A text
    generation model will not generate plain text as the output; it generates the
    output in a series of integers. In order for this series of indices to map to
    letters (tokens), you need a lookup table. `index_to_char` is specifically built
    for this purpose. Using `index_to_char`, you can look up each character (token)
    by key, where the key is the index from the model’s output. Without `index_to_char`,
    you will not be able to map model outputs back to a readable, plain-text format.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to handle some of the most common data structures:
    tables, images, and text. Tabular datasets (the structured, CSV-style data) are
    very common, are returned from a typical database query, and are frequently used
    as training data. You learned how to deal with columns of different data types
    in such structures, as well as how to model feature interactions by crossing columns
    of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: For image data, you learned that you need to standardize image size and pixel
    values before using the image collection as a whole to train a model, and that
    you need to keep track of image labels.
  prefs: []
  type: TYPE_NORMAL
- en: Text data is by far the most diverse data type, in terms of both format and
    use. Nevertheless, whether the data is for text classification, translation, or
    question-and-answer models, tokenization and dictionary construction processes
    are very common. The methods and approaches described in this chapter are by no
    means exhaustive or comprehensive; rather, they represent “table stakes” when
    dealing with these data types.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.xhtml#idm45772916304408-marker)) From an answer on [StackOverflow](https://oreil.ly/1iVv1)
    by user Joe Kington, January 13, 2016, accessed October 23, 2020.
  prefs: []
  type: TYPE_NORMAL
