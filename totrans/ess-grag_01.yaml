- en: 2 Vector similarity search and hybrid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introduction to embeddings, embedding models, vector space, and vector similarity
    search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How vector similarity fits in RAG applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A practical walkthrough of a RAG application using vector similarity search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding full-text search to the RAG application to see how enabling a hybrid
    search approach can improve results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a knowledge graph can be an iterative process where you start with
    unstructured data and then add structure to it. This is often the case when you
    have a lot of unstructured data and you want to start using it to answer questions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will look at how we can use RAG to answer questions using unstructured
    data. We’ll look at how to use vector similarity search and hybrid search to find
    relevant information and how to use that information to generate an answer. In
    later chapters, we’ll look at what techniques we can use to improve the retriever
    and generator to get better results when there’s some structure to the data.
  prefs: []
  type: TYPE_NORMAL
- en: In data science and machine learning, embedding models and vector similarity
    search are important tools for handling complex data. This chapter looks at how
    these technologies turn complicated data, like text and images, into uniform formats
    called embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the basics of embedding models and vector similarity
    search, explaining why they are useful, how they are used, and the challenges
    they help solve in RAG applications. To follow along, you’ll need access to a
    running, blank Neo4j instance. This can be a local installation or a cloud-hosted
    instance; just make sure it’s empty. You can follow the implementation directly
    in the accompanying Jupyter notebook available here: [https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb](https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch02.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Components of a RAG architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a RAG application, there are two main components: a *retriever* and a *generator*.
    The retriever finds relevant information, and the generator uses that information
    to create a response. Vector similarity search is used in the retriever to find
    relevant information; this is explained in more detail later. Let’s dig into both
    these components.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 The retriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The retriever is the first component of a RAG application. Its purpose is to
    find relevant information and pass that information to the generator. How the
    retriever finds the relevant information is not implied in the RAG framework,
    but the most common way is to use vector similarity search. Let’s look at what’s
    needed to prepare data for the retriever to be successful using vector similarity
    search.
  prefs: []
  type: TYPE_NORMAL
- en: Vector index
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While a vector index isn’t strictly required for vector similarity search, it’s
    highly recommended. A vector index is a data structure (like a map) that stores
    vectors in a way that makes it easy to search for similar vectors. When using
    a vector index, the retriever method is often referred to as an *approximate nearest
    neighbor search*. This is because the vector index doesn’t find the exact nearest
    neighbors, but it finds vectors that are very close to the nearest neighbor. This
    is a tradeoff between speed and accuracy. The vector index is much faster than
    a brute-force search, but it’s not as accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Vector similarity search function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *vector similarity search* function is a function that takes a vector as input
    and returns a list of similar vectors. This function might use a vector index
    to find similar vectors, or it might use some other (brute-force) method. The
    important thing is that it returns a list of similar vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The two most common vector similarity search functions are cosine similarity
    and Euclidean distance. *Euclidean distance* represents the content and intensity
    of the text, which is not as important in most cases covered in this book. *Cosine
    similarity* is a measure of the angle between two vectors. In our text-embedding
    case, this angle represents how similar two texts are in their meaning. The cosine
    similarity function takes two vectors as input and returns a number between 0
    and 1; 0 means the vectors are completely different, and 1 means they are identical.
    Cosine similarity is considered the best fit for text chatbots, and it’s the one
    we’ll use in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The result from a semantic classification of text is called an *embedding*.
    Any text you want to match using vector similarity search must be converted into
    an embedding. This is done using an embedding model, and it’s important that the
    embedding model stays the same throughout the RAG application. If you want to
    change the embedding model, you must repopulate the vector index.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are lists of numbers, and the length of the list is called the embedding
    dimension. The embedding dimension is important because it determines how much
    information the embedding can hold. The higher the embedding dimension, the more
    computationally expensive it is to work with the embedding, both when generating
    the embedding as well as when performing vector similarity search.
  prefs: []
  type: TYPE_NORMAL
- en: An *embedding* is a way to represent complex data as a set of numbers in a simpler,
    lower-dimensional space. Think of it as translating data into a format that a
    computer can easily understand and work with.
  prefs: []
  type: TYPE_NORMAL
- en: '*Embedding models* provide a uniform way to represent different types of data.
    Input to an embedding model can be any complex data, and the output is a vector.
    For instance, in dealing with text, an embedding model will take words or sentences
    and turn them into vectors, which are lists of numbers. The model is trained to
    ensure that these number lists capture essential aspects of the original words,
    such as their meaning or context.'
  prefs: []
  type: TYPE_NORMAL
- en: Text chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Text chunking* is the process of splitting up text into smaller pieces. This
    is done to improve the accuracy of the retriever. The presence of smaller pieces
    of text means that the embedding is narrower and more specific; thus the retriever
    will find more relevant information when searching.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text chunking is very important and not easy to get right. You need to think
    about how to split up the text: Should it be sentences, paragraphs, semantic meaning,
    or something else? Should you use a sliding window, or should you use a fixed
    size? How big should the chunks be?'
  prefs: []
  type: TYPE_NORMAL
- en: There are no right answers to these questions, and it depends on the use case,
    data, and domain. But it’s important to think about these questions and try different
    approaches to find the best solution for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Retriever pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once all pieces are in place, the retriever pipeline is quite simple. It takes
    a query as input, converts it into an embedding using the embedding model, and
    then uses the vector similarity search function to find similar embeddings. In
    the naive case, the retriever pipeline then just returns the source chunks, which
    then are passed to the generator. But in most cases, the retriever pipeline needs
    to do some postprocessing to find the best chunks to pass to the generator. We’ll
    get to more advanced strategies in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 The generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *generator* is the second component of a RAG application. It uses the information
    found by the retriever to generate a response. The generator is often an LLM,
    but one benefit of RAG over finetuning or relying on a model’s base knowledge
    is that the models don’t need to be as large. This is because the retriever finds
    relevant information, so the generator doesn’t need to know everything. It does
    need to know how to use the information found by the retriever to create a response.
    This is a much smaller task than knowing everything.
  prefs: []
  type: TYPE_NORMAL
- en: So we’re using the language model for its ability to generate text, not for
    its knowledge. This means we can use smaller language models, which are faster
    and cheaper to run. It also means that we can trust that the language model will
    base its response on the information found by the retriever and therefore make
    fewer things up and hallucinate less.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 RAG using vector similarity search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few pieces needed to implement a RAG application using vector similarity
    search. We’ll go through each of them in this chapter. The goal is to show how
    to implement a RAG application using vector similarity search and how to use the
    information found by the retriever to generate a response. Figure 2.1 illustrates
    the data flow for the finished RAG application.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 The data flow for this RAG application using vector similarity search
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We need to separate the application into two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Data setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll start by looking at the data setup, and then we’ll look at what the application
    will do at query time.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Application data setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From earlier sections, we know that we need to process the data a bit to be
    able to place it in the embedding model vector space to perform vector similarity
    search at run time. The pieces needed are
  prefs: []
  type: TYPE_NORMAL
- en: A text corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-chunking function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database with vector similarity search ability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will go through these pieces one by one and show how they contribute to the
    application data setup.
  prefs: []
  type: TYPE_NORMAL
- en: The data will be stored in text chunks in a database, and the vector index will
    be populated with the embeddings of the text chunks. Later, at run time, when
    a user asks a question, the question will be embedded using the same embedding
    model as the text chunks, and then the vector index will be used to find similar
    text chunks. Figure 2.2 shows the data flow for the application data setup.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 The pieces in the pipeline for the application data setup
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 2.2.2 The text corpus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The text we will be using in this example is a paper titled “Einstein’s Patents
    and Inventions” (Caudhuri, 2017). Even though LLMs are well aware of Albert Einstein,
    we show that RAG works by asking very specific questions and comparing them with
    the answers we get from the paper versus answers we get from an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Text chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With an LLM having a large enough context window, we can use the whole paper
    as a single chunk. But to get better results, we’ll split the paper into smaller
    chunks and use every few hundred characters as a chunk. The chunk size that yields
    the best results varies on a case-by-case basis, so make sure to experiment with
    different chunk sizes.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we also want to have some overlap between the chunks. This is
    because we want to be able to find answers that span multiple chunks. So we’ll
    use a sliding window with a size of 500 characters and an overlap of 40 characters.
    This will make the index a bit bigger, but it will also make the retriever more
    accurate.
  prefs: []
  type: TYPE_NORMAL
- en: To help the embedding model better classify the semantics of each chunk, we
    will only chunk at spaces, so we don’t have broken words at the start and end
    of each chunk. This function takes a text, chunk size (number of characters),
    overlap (number of characters), and an optional argument whether to split on any
    character or on whitespaces only and returns a list of chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 The text-chunking function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines the function to chunk text'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calls the function and get chunks back'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prints the length of the chunks list. The majority of the function is just
    to make sure that we don’t split individual words but only split on spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Embedding model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When choosing an embedding model, it’s important to think about what kind of
    data you want to match. In this case, we want to match text, so we’ll use a text-embedding
    model. Throughout this book, we will use both embedding models and LLMs from OpenAI,
    but there are many alternatives out there. `all-MiniLM-L12-v2` via Sentence Transformers
    ([https://mng.bz/nZZ2](https://mng.bz/nZZ2)) from Hugging Face is a great alternative
    to OpenAI’s embedding models, and it’s very easy to use and can run on your local
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have decided on a embedding model, we need to make sure that we use
    the same model throughout the RAG application. This is because the vector index
    is populated with vectors from the embedding model, so if we change the embedding
    model, we need to repopulate the vector index. To embed the chunks using OpenAI’s
    embedding models, we’ll use the following code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Embedding chunks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines the function to embed chunks'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calls the function and get embeddings back'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Prints the length of the embeddings list'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Prints the length of the first embedding'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 Database with vector similarity search function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have the embeddings, we need to store them so we can perform a similarity
    search later. In this book, we will use Neo4j as our database, since it has a
    built-in vector index and it’s easy to use; later in the book we will use Neo4j
    for its graph capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data model we’ll use at this stage is quite simple. We’ll have a single
    node type `Chunk` with two properties: `text` and `embedding`. The `text` property
    will hold the text of the chunk, and the `embedding` property will hold the embedding
    of the chunk.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 The data model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 2.3 shows the simplistic data model that will be used to demonstrate
    how to implement a RAG application using vector similarity search.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s create a vector index. One thing to keep in mind is that when we
    create the vector index, we need to define the number of dimensions the vectors
    will have. If you at any point in the future change the embedding model that outputs
    a different number of dimensions, you need to recreate the vector index.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the code listing 2.2, the embedding model we used outputs vectors
    with 1,536 dimensions, so we’ll use that as the number of dimensions when we create
    the vector index.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Creating a vector index in Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will name the vector index `pdf` and it will be used to index nodes of type
    `Chunk` on the property `embedding` using the cosine similarity search function.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a vector index, we can populate it with the embeddings. We
    will do this using Cypher, where we first create a node for each chunk and then
    set the `text` and `embedding` properties on the node using a Cypher loop. We’re
    also storing an index on each `:Chunk` node, so we can easily find the chunk later.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Storing chunks and populating the vector index in Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To check what’s in the database, we can run this Cypher query to get the `:Chunk`
    node with index 0\.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Getting data from a chunk node in Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 2.2.6 Performing vector search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have the vector index populated with the embeddings, we can perform
    a vector similarity search. First, we need to embed the question that we want
    to answer. We’ll use the same embedding model as we used for the chunks, and we’ll
    use the same function as we used to embed the chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Embedding user question
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the question embedded, we can perform a vector similarity search
    using Cypher.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 Performing vector search in Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The query returns the top two most similar chunks, and we can print the results
    to see what we got back. The code will print the following text chunks and their
    similarity scores.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 Printing results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: From the print, we can see the matched chunks, their similarity score, and their
    index. The next step is to use the chunks to generate an answer using an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.7 Generating an answer using an LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When communicating with an LLM, we have the ability to pass in what’s called
    a “system message,” where we can pass in instructions for the LLM to follow. We
    also pass in a “user message,” which holds the original question and, in our case,
    the answer to the question.
  prefs: []
  type: TYPE_NORMAL
- en: In the user message, we pass in the chunks that we want the LLM to use to generate
    the answer. We do this by passing in the `text` property of the similar chunks
    we found in the similar search in listing 2.8\.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 The LLM context
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now use the LLM to generate an answer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.10 Generating an answer using an LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will stream the result from the LLM as it’s generated, and we can see the
    result as it’s generated.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.11 Answer from LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Wow, look at that! The LLM was able to generate an answer based on the information
    found by the retriever.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Adding full-text search to the RAG application to enable hybrid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we saw how to implement a RAG application using vector
    similarity search. While pure vector similarity search can take you a long way
    and is a great improvement over plain full-text search, it’s often not enough
    to produce high enough quality, accuracy, and performance for production use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll look at how to improve the retriever to get better results.
    We’ll consider how to add full-text search to the RAG application to enable hybrid
    search.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Full-text search index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Full-text search*, a text search method in databases, has existed for a long
    time. It searches for matches in the data via keywords and not by similarity in
    a vector space. To find a match in a full-text search, the search term must be
    an exact match to a word in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: To enable hybrid search, we need to add a full-text search index to the database.
    Most databases have some kind of full-text search index, and in this book we’ll
    use Neo4j’s full-text search index.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.12 Creating a full-text index in Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here we create a full-text index named `PdfChunkFulltext` on the `text` property
    of the `:Chunk` nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Performing hybrid search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea with the hybrid search is that we perform a vector similarity search
    and a full-text search and then combine the results. To be able to compare the
    scores for the two different matches, we need to normalize the scores. We do this
    by dividing the scores by the highest score for each search.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.13 Performing hybrid search in Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We write a union Cypher query where we first perform a vector similarity search
    and then a full-text search. We then deduplicate the results and return the top
    `k` results.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.14 Calling hybrid search in Neo4j
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Listing 2.15 Answer from hybrid search
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here we can see that the top result got a score of 1.0 because of the normalization.
    This means that the top result is the same as the top result from the vector similarity
    search. But we can also see that the second result is different. This is because
    the full-text search found a better match than the vector similarity search.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we looked at what vector similarity search is, what components
    it consists of, and how it fits into RAG applications. We then added full-text
    search to improve the performance of the retriever.
  prefs: []
  type: TYPE_NORMAL
- en: By using both vector similarity search and full-text search, we can get better
    results than by using only one of them. While this approach might work well in
    certain situations, its quality, accuracy, and performance when using hybrid search
    is still quite limited since we’re using unstructured data to retrieve information.
    References in the text are not always captured, and the surrounding context is
    not always enough to understand the meaning of the text for the LLMs to generate
    good answers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll look at how to improve the retriever to get better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A RAG application consists of a retriever and a generator. The retriever finds
    relevant information, and the generator uses that information to create a response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text embeddings capture the meaning of text in a vector space, which allows
    us to use vector similarity search to find similar text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By adding full-text search to the RAG application, we can enable hybrid search
    to improve the performance of the retriever.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector similarity search and hybrid search can work well in certain situations,
    but their quality, accuracy, and performance are still quite limited as the data
    complexity grows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
