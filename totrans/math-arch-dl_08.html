<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-loss-optim-reg">9 Loss, optimization, and regularization</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Geometrical and algebraic introductions to loss functions</li>
<li class="co-summary-bullet">Geometrical intuitions for softmax</li>
<li class="co-summary-bullet">Optimization techniques including momentum, Nesterov, AdaGrad, Adam, and SGD</li>
<li class="co-summary-bullet">Regularization and its relationship to Bayesian approaches</li>
<li class="co-summary-bullet">Overfitting while training, and dropout</li>
</ul>
<p class="body"><a id="marker-300"/>By now, it should be etched in your mind that neural networks are essentially function approximators. In particular, neural network classifiers model the decision boundaries between the classes in the feature space (a space where every input feature combination is a specific point). Supervised classifiers mark sample training data inputs in this space with a‚Äîperhaps manually generated‚Äîclass label (ground truth). The training process iteratively learns a function that essentially creates decision boundaries separating the sampled training data points into individual classes. If the training data set is a reasonable representative of the true distribution of possible inputs, the network (the learned function that models the class boundaries) will classify never-before-seen inputs with good accuracy.</p>
<p class="body">When we select a specific neural network architecture (with a fixed set of layers, each with a fixed set of perceptrons with specific connections), we have essentially frozen the <i class="fm-italics">family</i> of functions we use as a function approximator. We still have to ‚Äúlearn‚Äù the exact weights of the connectors between various <i class="fm-italics">perceptrons</i> aka <i class="fm-italics">neurons</i>). The training process iteratively sets these weights so as to best classify the training data points. To do this, we design a loss function that measures the departure of the network output from the desired result. The network continually tries to minimize this loss. There are a variety of loss functions to choose from.</p>
<p class="body">The iterative process through which loss is minimized is called <i class="fm-italics">optimization</i>. We also have a multitude of optimization algorithms to choose from. In this chapter, we study loss functions, optimization algorithms, and associated topics like L1 and L2 regularization and dropout. We also learn about overfitting, a potential pitfall to avoid while training a neural network.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The complete PyTorch code for this chapter is available at <a class="url" href="http://mng.bz/aZv9">http://mng.bz/aZv9</a> in the form of fully functional and executable Jupyter notebooks.</p>
<h2 class="fm-head" id="sec-regression-loss">9.1 Loss functions</h2>
<p class="body"><a id="marker-301"/>A loss function essentially measures the badness of the neural network output. In the case of a supervised network, the loss for an individual training data instance is the distance of the actual output of the neural network (aka prediction) from the desired ideal outputs known or manually labeled ground truth [GT]) on that particular training input instance. Total training loss is obtained by summing the losses from all training data instances. Training is essentially an iterative optimization process that minimizes the total training loss.</p>
<h3 class="fm-head1" id="quantification-and-geometrical-view-of-loss">9.1.1 Quantification and geometrical view of loss</h3>
<p class="body">Loss surfaces and their minimization are described in detail in section <a class="url" href="../Text/08.xhtml#sec-loss-surface-graddesc">8.4.2</a>. Here we only do a quick review.</p>
<p class="body">A full neural network can be described by the equation</p><!--<p class="FM-Equation"><span class="times"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span> = <i class="fm-italics">f</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span>|<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_w.png" /></span>, <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_b.png" /></span>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_09-01.png" width="107"/></p>
</div>
<p class="fm-equation-caption">Equation 9.1 <span class="calibre" id="eq-nn-forward-pass-overall"/></p>
<p class="body">Equation <a class="url" href="#eq-nn-forward-pass-overall">9.1</a> says that given an input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, the neural network with weights <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and biases <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> emits the <i class="fm-italics">output vector</i> or <i class="fm-italics">prediction vector</i> <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>. The weights and biases may be organized into layers; this equation does not care. The vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>, respectively, denote the sets of <i class="fm-italics">all</i> weights and biases from all layers aggregated. Evaluating the function <span class="math"><i class="fm-italics">f</i>(‚ãÖ)</span> is equivalent to performing one forward pass on the network. In particular, given a training input instance <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>, the neural network emits <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = <i class="fm-italics">f</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span>. We refer to <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> as the output of the <i class="timesitalic">i</i>th training data instance.</p>
<p class="body">During supervised training, for each training input instance <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>, we have the GT (the known output), <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>. We refer to <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> as the <i class="fm-italics">GT vector</i> (as usual, we use superscript indices for training data instances).</p>
<p class="body">Ideally, the output vector <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> should match the GT vector <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>. The mismatch between them is the loss for that training data instance <span class="math"><span class="segoe">ùïÉ</span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>(<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span>, which we sometimes denote as <span class="math"><span class="segoe">ùïÉ</span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>(<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span>. The overall training loss (to be minimized by the optimization process) is the sum of losses over all training data instances:</p><!--<p class="Body"><span class="times">$$\mathbb{L} \left( \vec{w}, \vec{b}
\right)
= \sum_{ i = 0 }^{ n - 1 } \mathbb{L}^{ \left( i \right) } \left( f\left( \vec{x}^{ \left( i  \right) } \middle\vert  \vec{w}, \vec{b}
\right), \bar{y}^{ \left( i \right) }
\right)
= \sum_{ i = 0 }^{ n - 1 } \mathbb{L}^{ \left( i \right) } \left(
\vec{y}^{ \left( i \right) }, \bar{y}^{ \left( i \right) }
\middle\vert  \vec{w}, \vec{b}  \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="68" src="../../OEBPS/Images/eq_09-02.png" width="484"/></p>
</div>
<p class="fm-equation-caption">Equation 9.2 <span class="calibre" id="eq-loss"/></p>
<p class="body">where the summation is over all training data instances, and <i class="timesitalic">n</i> is the size of the training data set. Note that this summation over all training data points is needed to compute the loss for each training data instance. Thus an <i class="fm-italics">epoch</i>, a single training loop over all training data instances, costs <span class="math"><i class="fm-italics">O</i>(<i class="fm-italics">n</i><sup class="fm-superscript">2</sup>)</span>, where <i class="timesitalic">n</i> is the number of training data points. Training usually requires many epochs. This makes the training process very expensive. In section <a class="url" href="../Text/09.xhtml#sec-SGD">9.2.2</a>, we study ways of mitigating this.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="416" id="fig-grand-canyon" src="../../OEBPS/Images/CH09_F01_Chaudhury.png" width="792"/></p>
<p class="figurecaption">Figure 9.1 The loss surface can be viewed as a canyon.</p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> In this chapter, we use <i class="timesitalic">n</i> to indicate the number of training data points and <i class="timesitalic">N</i> to indicate the dimensionality of the output vector. For classifiers, the dimensionality of the output vector, <i class="timesitalic">N</i>, matches the number of classes. We also use superscript <span class="math">(<i class="fm-italics">i</i>)</span> to index training data points and subscript <i class="timesitalic">j</i> to index output vector dimensions. For classifiers, <i class="timesitalic">j</i> indicates the class.</p>
<p class="body"><a id="marker-302"/>We can visualize <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span> as a hypersurface in high-dimensional space. Figures <a class="url" href="../Text/08.xhtml#fig-gradient-descent-3d">8.8</a> and <a class="url" href="../Text/08.xhtml#fig-non-convex-local-minima">8.9</a> show some low-dimensional examples of loss surfaces. These are illustrative examples. In reality, the loss surface is typically high dimensional and very complex. One good mental picture is that of a canyon (see figure <a class="url" href="#fig-grand-canyon">9.1</a>). Traveling ‚Äúdownhill‚Äù at any point effectively follows the negative direction of the local gradient the gradient of a loss surface was introduced in section <a class="url" href="../Text/08.xhtml#sec-loss-surface-graddesc">8.4.2</a>). Traveling downhill along the gradient does not always lead to the global minimum. For instance, going downhill following the dashed arrow will take us to a local minimum, whereas the global minimum is where the water is going, indicated by the solid arrow. (See also section <a class="url" href="../Text/08.xhtml#sec-graddesc_local_minima">8.4.4</a> and figure <a class="url" href="../Text/08.xhtml#fig-non-convex-local-minima">8.9</a>.)</p>
<p class="body">Many loss formulations are possible, quantifying the mismatch <span class="math"><span class="segoe">ùïÉ</span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>(<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span>; some of them are described in the following subsections.</p>
<h3 class="fm-head1" id="regression-loss">9.1.2 Regression loss</h3>
<p class="body"><a id="marker-303"/>Regression loss is the simplest loss formulation. It is the L2 norm of the difference between the output and GT vectors. This loss was introduced in equation <a class="url" href="../Text/08.xhtml#eq-mse-loss">8.11</a>. We restate it here: the loss on the <i class="timesitalic">i</i>th training data instance is</p><!--<p class="Body"><span class="times">$$\mathbb{L}^{ \left ( i \right)
} \left( \vec{y}^{ \left ( i \right) }, \bar{y}^{ \left ( i \right) }
\right) = \| \vec{y}^{ \left ( i \right) } - \bar{y}^{ \left ( i \right)
}  \|^{2} = \sum_{j=0}^{N-1} \left( y_{j}^{ \left ( i \right) }  -
\bar{y}_{j}^{ \left ( i \right) }  \right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="63" src="../../OEBPS/Images/eq_09-02-a.png" width="374"/></p>
</div>
<p class="body">where the summation is over the components of the output vector. <i class="timesitalic">N</i> is the number of classes. The GT vector and output vector are both <i class="timesitalic">N</i>-dimensional.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for regression loss, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/g1a8">http://mng.bz/g1a8</a>.</p>
<p class="fm-code-listing-caption" id="listing-9.1-pytorch-code-for-regression-loss">Listing 9.1 PyTorch code for regression loss</p>
<pre class="programlisting">from torch.nn.functional import mse_loss                   <span class="fm-combinumeral">‚ë†</span>

y_pred = torch.tensor([-0.10, -0.24,  1.43, -0.14, -0.59]) <span class="fm-combinumeral">‚ë°</span>

y_gt = torch.tensor([ 0.59, -1.92, -1.27, -0.40,  0.50])   <span class="fm-combinumeral">‚ë¢</span>

loss = mse_loss(y_pred, y_gt, reduction='sum')             <span class="fm-combinumeral">‚ë£</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Imports the regression loss (mean squared error loss)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> N-d prediction vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> N-d ground truth vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Computes the regression loss</p>
<h3 class="fm-head1" id="sec-ce-loss-again">9.1.3 Cross-entropy loss</h3>
<p class="body">Cross-entropy loss was discussed in the context of entropy in section <a class="url" href="../Text/06.xhtml#sec-cross-entropy">6.3</a>. If necessary, this would be a great time to reread that. Here we review the idea quickly.</p>
<p class="body">Cross-entropy loss is typically used to measure the mismatch between a classifier neural network output and the corresponding GT in a classification problem. Here, the GT is a one-hot vector whose length equals the number of classes. All but one of its elements are 0. The single nonzero element is 1, and it occurs at the index corresponding to the correct class for that training data instance.</p>
<p class="body">Thus the GT vector looks like <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = [0,‚Ä¶,0,1,0,‚Ä¶,0]</span>. The prediction vector should have elements with values between <span class="math">0</span> and <span class="math">1</span>. Each element of the prediction vector <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> indicates the probability of a specific class. In other words, <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = [<i class="fm-italics">p</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">p</i><sub class="fm-subscript">1</sub>,‚Ä¶, <i class="fm-italics">p</i><sub class="fm-subscript"><i class="fm-italics1">N</i>‚àí1</sub>]</span>, where <i class="timesitalic">p<sub class="fm-subscript">j</sub></i> is the probability of the input <i class="timesitalic">i</i> belonging to the <i class="timesitalic">j</i>th class. In section <a class="url" href="../Text/06.xhtml#sec-cross-entropy">6.3</a>, we illustrated with an example image classifier that predicts whether an image contains a cat class <span class="math">0</span>), a dog (class <span class="math">1</span>), an airplane (class <span class="math">2</span>), or an automobile (class <span class="math">3</span>). One of the four is always assumed to be present in the image. If, for the <i class="timesitalic">i</i>th training data instance, the GT vector is an image of cat, we have <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = [1,0,0,0]</span>. A prediction vector <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = [0.8,0.15,0.04,0.01]</span> is good, while <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = [0.25,0.25,0.25,0.25]</span> is bad. Note that sum of the elements of the GT as well as the prediction vector is always <span class="math">1</span> since they are probabilities. Mathematically, given a training dataset <i class="timesitalic">X</i>,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\sum_{j=0}^{N-1} \bar{y}_{j}^{ \left ( i \right) } = 1
&amp;\sum_{j=0}^{N-1} y_{j}^{ \left ( i \right) } = 1
&amp;\forall i \in X\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_09-02-b.png" width="284"/></p>
</div>
<p class="body">Given such GT and prediction vectors, the cross-entropy loss (CE loss) is</p><!--<p class="Body"><span class="times">$$\mathbb{L}^{ \left ( i \right) } \left( \vec{y}^{
\left ( i \right) }, \bar{y}^{ \left ( i \right) } \right) =
-\sum_{j=0}^{N-1} \bar{y}_{j}^{ \left ( i \right) } \;\; log \left( _{j}^{ \left ( i \right) } \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="82" src="../../OEBPS/Images/eq_09-03.png" width="284"/></p>
</div>
<p class="fm-equation-caption">Equation 9.3 <span class="calibre" id="eq-ce-loss"/></p>
<p class="body">where the summation is over the elements of the prediction vector and <i class="timesitalic">N</i> is the number of classes.</p>
<p class="fm-head2" id="intuitions-behind-cross-entropy-loss">Intuitions behind cross-entropy loss</p>
<p class="body"><a id="marker-304"/>Note that only one element‚Äîthe one corresponding to the GT class‚Äîsurvives in the summation of equation <a class="url" href="#eq-ce-loss">9.3</a>. The other elements vanish because they are multiplied by the <span class="math">0</span> GT value. The (logarithm of) the predicted probability of the correct GT class is multiplied by <span class="math">1</span>. Hence, the CE loss always boils down to <span class="math">‚àí<i class="fm-italics">log</i>(<i class="fm-italics">y</i><sub class="fm-subscript"><i class="fm-italics1">j</i><sup class="fm-superscript">*</sup></sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span>, where <span class="math"><i class="fm-italics">j</i><sup class="fm-superscript">*</sup></span> is the GT class. If this probability is <span class="math">1</span>, the CE loss becomes <span class="math">0</span>, rightly so, as the correct class is being predicted with a probability of <span class="math">1</span>. If the predicted probability of the correct class is <span class="math">0</span>, the CE loss is <span class="math">‚àí<i class="fm-italics">log</i>(0) = ‚àû</span>, again rightly so, since this is the worst possible prediction. The closer the prediction for the correct class is to <span class="math">1</span>, the smaller the loss.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for cross-entropy loss, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/g1a8">http://mng.bz/g1a8</a>.</p>
<p class="fm-code-listing-caption" id="listing-9.2-pytorch-code-for-cross-entropy-loss">Listing 9.2 PyTorch code for cross-entropy loss</p>
<pre class="programlisting">import torch

y_pred = torch.tensor([0.8, 0.15, 0.04, 0.01]) <span class="fm-combinumeral">‚ë†</span>

y_gt = torch.tensor([1., 0., 0., 0.])          <span class="fm-combinumeral">‚ë°</span>

loss = -1 * torch.dot(y_gt, torch.log(y_pred)) <span class="fm-combinumeral">‚ë†</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> N-d prediction vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> N-d one-hot ground truth vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Computes the cross-entropy loss</p>
<p class="fm-head2" id="special-case-of-two-classes">Special case of two classes</p>
<p class="body">What happens if <span class="math"><i class="fm-italics">N</i> = 2</span> (that is, we have only two classes)? Let‚Äôs denote the predicted probability of class <span class="math">0</span>, for the <i class="timesitalic">i</i>th training input, as <span class="math"><i class="fm-italics">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>: that is, <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">0</sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = <i class="fm-italics">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>. Then, since these are probabilities, the prediction on the other class <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sub class="fm-subscript">1</sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = 1 ‚àí <i class="timesitalic">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>. Also, let <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> denote the GT probability for class <span class="math">0</span> on this <i class="timesitalic">i</i>th training input. Then <span class="math">1 ‚àí <i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> is the GT probability for class <span class="math">1</span>. (We have slightly abused notations‚Äîup to this point, <i class="timesitalic">»≥</i> has denoted a vector, but here it denotes a scalar.)</p>
<p class="body">Then, following equation <a class="url" href="#eq-ce-loss">9.3</a>, the CE loss on the <i class="timesitalic">i</i>th training data instance becomes</p>
<p class="fm-equation"><span class="math"><span class="segoe">ùïÉ</span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>(<i class="fm-italics">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>) = ‚àí<i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> <i class="fm-italics">log</i>(<i class="fm-italics">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>) ‚àí (1‚àí<i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>) <i class="fm-italics">log</i>(1‚àí<i class="fm-italics">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span></p>
<p class="fm-equation-caption">Equation 9.4 <span class="calibre" id="eq-binary-ce-loss"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for binary cross-entropy loss, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/g1a8">http://mng.bz/g1a8</a>.</p>
<p class="fm-code-listing-caption" id="listing-9.3-pytorch-code-for-binary-cross-entropy-loss">Listing 9.3 PyTorch code for binary cross-entropy loss</p>
<pre class="programlisting">from torch.nn.functional import binary_cross_entropy <span class="fm-combinumeral">‚ë†</span>

y_pred = torch.tensor([0.8])                         <span class="fm-combinumeral">‚ë°</span>

y_gt = torch.tensor([1.])                            <span class="fm-combinumeral">‚ë¢</span>

loss = binary_cross_entropy(y_pred, y_gt)            <span class="fm-combinumeral">‚ë£</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Imports the binary cross-entropy loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Outputs the probability of class 0 - <span class="math"><i class="fm-italics">y</i><sub class="fm-subscript">0</sub></span>. A single value is sufficient because <span class="math"><i class="fm-italics">y</i><sub class="fm-subscript">1</sub> = 1 ‚àí <i class="fm-italics">y</i><sub class="fm-subscript">0</sub></span>.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> The ground truth is either 0 or 1.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Computes the cross-entropy loss</p>
<h3 class="fm-head1" id="binary-cross-entropy-loss-for-image-and-vector-mismatches">9.1.4 Binary cross-entropy loss for image and vector mismatches</h3>
<p class="body"><a id="marker-305"/>Given a pair of normalized tensors (such as images or vectors) whose elements all have values between <span class="math">0</span> and <span class="math">1</span>, a variant of the two-class CE loss can be used to estimate the mismatch between the tensors. Note that an image with pixel-intensity values between <span class="math">0</span> and <span class="math">255</span> can always be normalized by dividing each pixel-intensity value by <span class="math">255</span>, thereby converting it to the 0 to 1 range. Such a comparison of two images is used in image autoencoders, for example. We study autoencoders later; here, we provide a brief overview in the following sidebar.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Autoencoders</p>
<p class="fm-sidebar-text">Autoencoders take an image as input, create a low-dimensional descriptor from the image‚Äîthis descriptor is often referred to as an <i class="fm-italics">embedding</i> of the image‚Äîand try to reconstruct the input image from the embedding. The image embedding is a compressed representation of the image. Reconstruction is a lossy process: the small, subtle variations in the signal are lost, and only the essential part is retained. The loss is the mismatch between the input image and the reconstructed image. By minimizing this loss, we incentivize the system to retain the essence of the input as much as possible within the embedding-size budget.</p>
</div>
<p class="body">Let <i class="timesitalic">»≥</i> denote the input image. Let <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> denote the reconstructed image outputted by the autoencoder. The binary cross-entropy loss is defined as</p><!--<p class="Body"><span class="times">$$\mathbb{L}^{ \left ( i \right) } \left(
\bar{y}^{ \left ( i \right) }, \vec{y}^{ \left ( i \right) }  \right) =
-\sum_{j=0}^{N-1}
\left(
\bar{y}_{j}^{ \left ( i \right) } \; log \left( y_{j}^{ \left ( i
\right) } \right) + \left(1 - \bar{y}_{j}^{ \left ( i \right) } \right) log \left( 1 - y_{j}^{ \left ( i \right) } \right)
\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="69" src="../../OEBPS/Images/eq_09-05.png" width="479"/></p>
</div>
<p class="fm-equation-caption">Equation 9.5 <span class="calibre" id="eq-pixel-ce-loss"/></p>
<p class="body">Note that here <i class="timesitalic">N</i> is the number of pixels in the image, not the number of classes, as before. The summation is over the pixels in the image. Also, the GT vector <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> is not a one-hot vector; rather, it is the input image. These differences aside, equation <a class="url" href="#eq-pixel-ce-loss">9.5</a> is based on the same idea as equation <a class="url" href="#eq-binary-ce-loss">9.4</a>.</p>
<p class="fm-head2" id="why-does-it-work">Why does it work?</p>
<p class="body">Binary cross-entropy loss attains its minimum when the input matches the GT. We outline the proof next. (Note that we drop the superscripts and subscripts for simplicity.) We have</p>
<p class="fm-equation"><span class="math">‚àí<span class="segoe">ùïÉ</span> = <i class="fm-italics">»≥log</i>(<i class="fm-italics">y</i>) + (1 ‚àí¬†<i class="fm-italics">»≥</i>)<i class="fm-italics">log</i>(1 ‚àí¬†<i class="fm-italics">y</i>)</span></p>
<p class="body">At the minimum, <span class="math"><i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><b class="fm-bold">/</b><i class="fm-italics">‚àÇy</i> = 0</span>.</p><!--<p class="Body">This implies <span class="times">$$-\frac{\partial \mathbb{L}}{\partial y} = \bar{y}
\frac{1}{y} - \left( 1 - \bar{y} \right) \frac{1}{1 - y} =
\frac{\bar{y}}{y} - \frac{ 1- \bar{y}}{1 - y} = 0 \implies y =
\bar{y}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_09-05-a.png" width="382"/></p>
</div>
<p class="body">Thus, the minimum of the binary cross-entropy loss occurs when the network output matches the GT. However, this does not mean this loss becomes zero when the output matches the GT.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Binary cross-entropy loss is not necessarily zero even in the ideal case of the output matching the input (although the loss is indeed <i class="fm-italics">minimal</i> in the ideal case, meaning the loss is higher for non-ideal cases with mismatched input and output).</p>
<p class="body">Examining equation <a class="url" href="#eq-pixel-ce-loss">9.5</a>, when the two inputs match, we have <span class="math">‚àí<span class="segoe">ùïÉ</span>(<i class="fm-italics">»≥</i>, <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span>)|<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics1">»≥</i></sub></span>. We intuitively expect this loss to be zero since the output is ideal. But it isn‚Äôt. For example, if, for <span class="math"><i class="fm-italics">»≥<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = <i class="fm-italics">y<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = 0.25</span></p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\left. \mathbb{L}^{ \left ( i \right) } \left( \bar{y}^{ \left ( i
\right) }_{j}, \vec{y}^{ \left ( i \right) }_{j}  \right)
\right|_{\bar{y}_{j} = \vec{y}_{j}^{ \left ( i \right) } = 0.25}
= -
\bar{y}_{j}^{ \left ( i \right) } log \left( y_{j}^{ \left ( i \right) }
\right) - \left(1 - \bar{y}_{j}^{ \left ( i \right) } \right) log \left( 1 - y_{j}^{ \left ( i \right) } \right)
\\
&amp;\quad =
- 0.25 log\left( 0.25 \right) - 0.75 log\left( 0.75 \right) = 0.56 \neq 0\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_09-05-b.png" width="861"/></p>
</div>
<p class="body">In fact, the binary cross-entropy loss is zero only in special cases, like <span class="math"><i class="fm-italics">y<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = <i class="fm-italics">»≥<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = 1</span>.<a id="marker-306"/></p>
<h3 class="fm-head1" id="softmax">9.1.5 Softmax</h3>
<p class="body">Suppose we are building a classifier: for instance, the image classifier we illustrated in section <a class="url" href="../Text/06.xhtml#sec-cross-entropy">6.3</a>, which predicts whether an image contains a cat (class <span class="math">0</span>), a dog (class <span class="math">1</span>), and airplane (class <span class="math">2</span>), or an automobile (class <span class="math">3</span>). Our classifier can emit a score vector <span class="times"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> corresponding to an input image. Element <i class="timesitalic">j</i> of the score vector corresponds to the <i class="timesitalic">j<sub class="fm-subscript">th</sub></i> class. We take the max of the score vector and call that the neural network-predicted label for the image. For instance, in the example image classifier, a score vector may be <span class="math">[9.99 ¬†¬†10 ¬†¬†0.01 ¬†¬†-10]</span>. Since the highest score occurs at index <span class="math">1</span>, we conclude that the image contains a dog class <span class="math">1</span>).</p>
<p class="body">The scores are unbounded; they can be any real number in the range <span class="math">[‚àí‚àû,‚àû]</span>. In general, however, neural networks behave better when the loss function involves a bounded set of numbers in the same range. The training converges faster and to better minima, and the inferencing is more accurate. Consequently, it is desirable to convert the example scores to probabilities. These will be numbers in the range <span class="math">[0,1]</span> (and the elements of the vector will sum to <span class="math">1</span>).</p>
<p class="body">The softmax function converts unbounded scores to probabilities. Given a score vector <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [<i class="fm-italics">s</i><sub class="fm-subscript">0</sub> ¬†¬†<i class="fm-italics">s</i><sub class="fm-subscript">1</sub> ¬†¬†<i class="fm-italics">s</i><sub class="fm-subscript">2</sub>¬†¬†‚Ä¶¬†¬†<i class="fm-italics">s</i><sub class="fm-subscript"><i class="fm-italics1">N</i>-1</sub>]</span>, the corresponding softmax vector is</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\textit{softmax} \left( \vec{s} \right) =
\begin{bmatrix}
\frac{e^{s_{0}}}{ S }\\
\frac{e^{s_{1}}}{ S }\\
\frac{e^{s_{2}}}{ S }\\
\cdots\\
\frac{e^{s_{N-1}}}{ S }
\end{bmatrix} \nonumber \\
&amp;\text{where   }S = \sum_{k=0}^{N-1} e^{s_{k}}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="168" src="../../OEBPS/Images/eq_09-06.png" width="346"/></p>
</div>
<p class="fm-equation-caption">Equation 9.6 <span class="calibre" id="eq-softmax"/></p>
<p class="body">A few noteworthy points:<a id="marker-307"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The vector has as many elements as possible classes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The sum of the elements in the previous vector is <span class="math">1</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The <i class="timesitalic">j</i>th element of the vector represents the predicted probability of class <i class="timesitalic">j</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The formulation can handle arbitrary scores, including negative ones.</p>
</li>
</ul>
<p class="body">So in our example classification problem with the four classes (cat, dog, airplane, automobile), the score vector</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span> = [9.99 ¬†¬†10 ¬†¬†0.01 ¬†¬†‚Äì10]</span></p>
<p class="body">will yield the softmax vector</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">softmax</i> (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span>) = [0.497 ¬†¬†0.502 ¬†¬†2.30e‚Äì5 ¬†¬†1.04e‚Äì9].</span></p>
<p class="body">The probability of cat is <span class="math">0.497</span>, and the probability of dog is slightly higher, <span class="math">0.502</span>. The probabilities of airplane and automobile are much lower: the neural network predicts that the image is that of a dog, but it is not very confident; it could also be a cat.</p>
<p class="fm-head2" id="why-the-name-softmax">Why the name softmax?</p>
<p class="body">The softmax function is a smooth (differentiable) approximation to the argmaxonehot function, which emits a one-hot vector corresponding to the index of the max score. The argmaxonehot function is <i class="fm-italics">discontinuous</i>. To see this, consider a pair of two class score vectors:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{p} = \begin{bmatrix} 9.99  &amp;10
\end{bmatrix}\\
&amp;\vec{q} = \begin{bmatrix} 10  &amp;9.99
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="77" src="../../OEBPS/Images/eq_09-06-a.png" width="121"/></p>
</div>
<p class="body">Performing an argmaxonehot operation on them will yield the following one-hot vectors, respectively:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;argmaxonehot \left( \vec{p} \right) = \begin{bmatrix} 0  &amp;1
\end{bmatrix}\\
&amp;argmaxonehot \left( \vec{q} \right)  = \begin{bmatrix} 1 &amp;0
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="78" src="../../OEBPS/Images/eq_09-06-b.png" width="214"/></p>
</div>
<p class="body">Thus we see that the vectors <span class="math"><i class="fm-italics">argmaxonehot</i>(<span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span>)</span> and <span class="math"><i class="fm-italics">argmaxonehot</i>(<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_q.png" width="14"/></span>)</span> are significantly far from each other, even though the points <span class="times"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_p.png" width="16"/></span> and <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_q.png" width="14"/></span> are very close to each other. On the other hand, the corresponding softmax vectors are</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\textit{softmax} \left( \vec{p} \right) = \begin{bmatrix} 0.4975  &amp;0.5025
\end{bmatrix}\\[6pt]
&amp;\textit{softmax} \left( \vec{q} \right)  = \begin{bmatrix}
&amp;0.5025 &amp;0.4975
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="86" src="../../OEBPS/Images/eq_09-06-c.png" width="260"/></p>
</div>
<p class="body">Although the predicted classes still match those from the argmaxonehot vector, the softmax vectors are very close to each other. The closer the scores, the closer the softmax probabilities. In other words, the softmax is continuous.</p>
<p class="body"><a id="marker-308"/>Figure <a class="url" href="#fig-twoclass-softmax-argmaxonehot">9.2</a> depicts this geometrically. The argmaxonehot functions as a function of the score vector <span class="math">[<i class="fm-italics">s</i>0, <i class="fm-italics">s</i>1]</span> (for selecting classes <span class="math">0</span> and <span class="math">1</span>, respectively) are shown in figures <a class="url" href="#fig-argmaxonehot-class0">9.2a</a> and <a class="url" href="#fig-argmaxonehot-class1">9.2c</a>. These are step functions on the <span class="math">(<i class="fm-italics">s</i>0, <i class="fm-italics">s</i>1)</span> plane, with <span class="math"><i class="fm-italics">s</i>0 = <i class="fm-italics">s</i>1</span> being the decision boundary. Their softmax approximations are shown in figures <a class="url" href="#fig-softmax-argmaxonehot-class0">9.2b</a> and <a class="url" href="#fig-softmax-argmaxonehot-class1">9.2d</a>. In section <a class="url" href="../Text/08.xhtml#sec-sigmoid-etc">8.1</a>, we introduced the <span class="math">1</span>D sigmoid function (see figure <a class="url" href="../Text/08.xhtml#fig-sigmoid1d">8.1</a>), which approximates the <span class="math">1</span>D step function. Here we see the higher-dimensional analog of that.</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="559" id="fig-argmaxonehot-class0" src="../../OEBPS/Images/CH09_F02a_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(a) Step function: <span class="math"><i class="fm-italics">z</i> = 1</span> if <span class="math"><i class="fm-italics">s</i>0 &gt; = <i class="fm-italics">s</i>1</span>, else <span class="math"><i class="fm-italics">z</i> = 0</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre31" height="498" id="fig-softmax-argmaxonehot-class0" src="../../OEBPS/Images/CH09_F02b_Chaudhury.png" width="543"/></p>
<p class="figurecaption">(b) Softmax: differential approximation of the step function</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="558" id="fig-argmaxonehot-class1" src="../../OEBPS/Images/CH09_F02c_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(c) Step function: <span class="math"><i class="fm-italics">z</i> = 1</span> if <span class="math"><i class="fm-italics">s</i>1 &gt; = <i class="fm-italics">s</i>0</span>, else <span class="math"><i class="fm-italics">z</i> = 0</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="568" id="fig-softmax-argmaxonehot-class1" src="../../OEBPS/Images/CH09_F02d_Chaudhury.png" width="543"/></p>
<p class="figurecaption">(d) Softmax: differential approximation of the step function<a id="marker-309"/></p>
</div>
<p class="fm-table-caption" id="fig-twoclass-softmax-argmaxonehot">Figure 9.2 Two-class argmaxonehot and softmax (function of score vector <span class="math">[<i class="fm-italics">s</i>0, <i class="fm-italics">s</i>1]</span>) on the <span class="math">(<i class="fm-italics">s</i>0, <i class="fm-italics">s</i>1)</span> plane. The decision boundary is the <span class="math">45<i class="fm-italics"><sub class="fm-subscript">o</sub></i></span> line <span class="math"><i class="fm-italics">s</i>0 = <i class="fm-italics">s</i>1</span>.</p>
<p class="fm-code-listing-caption" id="listing-9.4-pytorch-code-for-softmax">Listing 9.4 PyTorch code for softmax</p>
<pre class="programlisting">from torch.nn.functional import softmax      <span class="fm-combinumeral">‚ë†</span>

scores = torch.tensor([9.99, 10, 0.01, -10]) <span class="fm-combinumeral">‚ë°</span>

output = softmax(scores, dim=0)              <span class="fm-combinumeral">‚ë¢</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Imports the softmax function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Scores are typically raw, un-normalized outputs of a neural network.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Computes the softmax</p>
<h3 class="fm-head1" id="softmax-cross-entropy-loss">9.1.6 Softmax cross-entropy loss</h3>
<p class="body">From the preceding discussion, it should be clear that it is desirable to make the last layer of a classifier neural network a softmax layer. Then, given an input, the network will emit probabilities for each class. During training, we can evaluate the loss on these probabilities with regard to the known GT probabilities. This can be done via the CE loss (see section <a class="url" href="#sec-ce-loss-again">9.1.3</a>). Thus the softmax is often followed by the CE loss during classifier training. Consequently, the combination (softmax CE loss) is available as a single operation in many deep learning packages, such as PyTorch. This is convenient because we do not need to call softmax and then CE loss. But the deeper reason for combining them is that the combination tends to be numerically better.</p>
<p class="body">Let‚Äôs look at an example to see how the softmax CE loss changes as the output prediction changes. Consider the image classification problem again, where we‚Äôd like to classify whether an image contains one of four categories: cat (class <span class="math">0</span>), dog (class <span class="math">1</span>), airplane (class <span class="math">2</span>), or automobile (class <span class="math">3</span>). Figure <a class="url" href="#fig-softmax-good-bad-visual">9.3</a> represents this visually. Suppose the image we‚Äôre classifying actually contains a dog class <span class="math">1</span>). The GT is represented as a one-hot vector <span class="math">[0 1 0 0]</span>. If our classifier predicts the vector <span class="math">[0.498 0.502 0 0]</span>, it‚Äôs predicting both cat and dog with almost equal probability. This is a bad prediction because we would ideally expect it to confidently predict a dog (class <span class="math">1</span>). Consequently, the CE loss is high (0.688). On the other hand, if our classifier predicts <span class="math">[0.003 0.997 0 0]</span>, it is highly certain (with a probability of 0.997) that the image contains a dog. This is a good prediction, and hence the CE loss is low (0.0032). Softmax CE loss is probably the most popular loss method used for training in classifiers at the moment.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="706" id="fig-softmax-good-bad-visual" src="../../OEBPS/Images/CH09_F03_Chaudhury.png" width="990"/></p>
<p class="figurecaption">Figure 9.3 Softmax output and cross-entropy loss for good and bad output predictions</p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for softmax CE loss, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/g1a8">http://mng.bz/g1a8</a>.<a id="marker-310"/></p>
<p class="fm-code-listing-caption" id="listing-9.5-pytorch-code-for-softmax-cross-entropy-loss">Listing 9.5 PyTorch code for softmax cross-entropy loss</p>
<pre class="programlisting">from torch.nn.functional import cross_entropy

scores = torch.tensor([[9.99, 10, 0.01, -10]])

y_gt = torch.tensor([1])            <span class="fm-combinumeral">‚ë†</span>

loss = cross_entropy(scores, y_gt)  <span class="fm-combinumeral">‚ë°</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Ground truth class index Ranges from 0 to _ <span class="math">‚Äì1</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Computes the softmax cross-entropy loss</p>
<h3 class="fm-head1" id="focal-loss">9.1.7 Focal loss</h3>
<p class="body">As training progresses, where should we focus our attention? This question becomes especially significant when there is <i class="fm-italics">data imbalance</i>, meaning the number of training data instances available for some classes is significantly smaller than others. In such cases, not all training data is equally important. We have to use our training data instances wisely.</p>
<p class="body">Intuitively, the greater bang for the buck is trying to improve the training data instances that are not doing well. In other words, instead of trying to squeeze out every bit of juice from the examples in which the network is doing well (the so-called ‚Äúeasy‚Äù examples), it is better to focus on the examples where the network is not doing as well (‚Äúhard‚Äù examples).</p>
<p class="body"><a id="marker-311"/>To stop focusing on easy examples and focus instead on hard examples, we can put more weight on the loss from training data instances that are far from the GT, and vice versa: that is, put less weight on the loss from training data instances that are close to GT. Consider the binary CE loss of equation <a class="url" href="#eq-binary-ce-loss">9.4</a> one more time. The loss for the <i class="timesitalic">i</i>th training instance can be rewritten as follows:</p><!--<p class="Body"><span class="times">$$\mathbb{L}^{ \left ( i \right) } \left( ^{ \left ( i \right) }, \bar{y}^{ \left ( i \right) } \right) =
\begin{cases}
-log \left( y^{ \left ( i \right) } \right) \;\;\; \text{if GT is class 1: that is, $\bar{y}^{ \left ( i \right) } = 1$ }\\[6pt]
-log \left( 1 - y^{ \left ( i \right) } \right) \;\;\; \text{if GT is class 0: that is, $\bar{y}^{ \left ( i \right) } = 0$}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="86" src="../../OEBPS/Images/eq_09-06-d.png" width="510"/></p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Going forward in this subsection, we drop the superscript <span class="math">(<i class="fm-italics">i</i>)</span> for the sake of notational simplification, although it remains implied.</p>
<p class="body">Now, when the GT is class <span class="math">1</span> (that is, <span class="math"><i class="fm-italics">»≥</i> = 1</span>), the entity <span class="math">(1‚àí<i class="timesitalic">y</i>)</span> measures the departure of the prediction from GT. We can multiply the loss by this to weigh down the losses from good predictions and weigh up the losses from bad predictions. In practice, we multiply by <span class="math">(1‚àí<i class="timesitalic">y</i>)<sup class="superscript-italic">Œ≥</sup></span> for some value of <i class="timesitalic">Œ≥</i> (such as <span class="math"><i class="fm-italics">Œ≥</i> = 2</span>). Similarly, when the GT is class <span class="math">0</span> (that is, <span class="math"><i class="fm-italics">»≥</i> = 0</span>), the entity <i class="timesitalic">y</i> measures the departure of the prediction from the GT. In this case, we multiply the loss by <i class="timesitalic">y<sup class="fm-superscript">Œ≥</sup></i>. The overall loss then becomes</p><!--<p class="Body"><span class="times">$$\mathbb{L} \left( \vec{y}, \bar{y}
\right) =
\begin{cases}
-\left( 1 - y \right)^{\gamma} log \left( y \right) \;\;\; \text{if the GT is class 1: $\bar{y} = 1$ }\\
-y^{\gamma} log \left( 1 - y \right) \;\;\; \text{if the GT is class 0:
$\bar{y} = 0$}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="67" src="../../OEBPS/Images/eq_09-06-e.png" width="416"/></p>
</div>
<p class="body">We can have a somewhat simplified expression</p><!--<p class="Body"><span class="times"><span class="segoe">ùïÉ</span>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span>, <i class="fm-italics">»≥</em>) = ‚àí(1‚àí<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>)<i class="fm-italics"><sub class="FM-Subscript">Œ≥</sub></em>  <i class="fm-italics">log</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/eq_09-07.png" width="230"/></p>
</div>
<p class="fm-equation-caption">Equation 9.7 <span class="calibre" id="eq-focal-loss"/></p>
<p class="body">where</p><!--<p class="Body"><span class="times">$$y_{t} =
\begin{cases}  \;\;\; \text{if the GT is class 1: $\bar{y} = 1$ }\\ 1 - y \;\;\; \text{if the GT is class 0: $\bar{y} = 0$}
\end{cases}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_09-07-a.png" width="326"/></p>
</div>
<p class="body">Equation <a class="url" href="#eq-focal-loss">9.7</a> is the popular expression of focal loss. Its graph at various values of <i class="timesitalic">Œ≥</i> is shown in figure <a class="url" href="#fig-focal-loss">9.4</a>. Note how the loss becomes more and more subdued as the probability of the GT increases toward the right until it flattens out at the bottom.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for focal loss, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/g1a8">http://mng.bz/g1a8</a>.</p>
<p class="fm-code-listing-caption" id="listing-9.6-pytorch-code-for-focal-loss">Listing 9.6 PyTorch code for focal loss</p>
<pre class="programlisting">def focal_loss(y, y_gt, gamma):
    y_t = (y_gt * y) + ((1 - y_gt) * (1 - y)) <span class="fm-combinumeral">‚ë†</span>

    loss = -1 * torch.pow((1 - y_t), gamma) * torch.log(y_t)
    return loss</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> <span class="math"><i class="fm-italics">y<sub class="fm-subscript">t</sub></i> = <i class="fm-italics">y</i></span> if <i class="timesitalic">y<sub class="fm-subscript">gt</sub></i> is <span class="math">1</span><br class="calibre20"/>
<span class="math"><i class="fm-italics">y<sub class="fm-subscript">t</sub></i> = 1 ‚àí <i class="fm-italics">y</i></span> if <i class="timesitalic">y<sub class="fm-subscript">gt</sub></i> is <span class="math">0</span></p>
<h3 class="fm-head1" id="hinge-loss">9.1.8 Hinge loss</h3>
<p class="body"><a id="marker-312"/>The softmax CE loss becomes zero only under the ideal condition: the correct class has a finite score, and other classes have a score of negative infinity. Hence, that loss will continue to push the network toward improvement until the ideal is achieved (which never happens in practice). Sometimes we prefer to stop changing the network when the correct class has the maximum score, and we do not care about increasing the distance between correct and incorrect scores any further. This is where hinge loss comes in.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="916" id="fig-focal-loss" src="../../OEBPS/Images/CH09_F04_Chaudhury.jpg" width="1384"/></p>
<p class="figurecaption">Figure 9.4 Focal loss graph (various <i class="timesitalic">Œ≥</i> values)</p>
</div>
<p class="body">A hinged door opens in one direction but not in the other direction. Similarly, a hinge loss function increases if a certain goodness criterion is <i class="fm-italics">not</i> satisfied but becomes zero (and does not reduce any further) if the criterion is satisfied. This is akin to saying, ‚ÄúIf you are not my friend, the distance between us can vary from small to large (unboundedly), but I don‚Äôt distinguish between friends. All my friends are at a distance of zero from me.‚Äù</p>
<p class="fm-head2" id="multiclass-support-vector-machine-loss-hinge-loss-for-classification">Multiclass support vector machine loss: Hinge loss for classification</p>
<p class="body"><a id="marker-313"/>Consider again our old friend, the classifier that predicts whether an image contains a cat (class <span class="math">0</span>), dog class <span class="math">1</span>), airplane (class <span class="math">2</span>), or automobile (class <span class="math">3</span>). Our classifier emits an output vector <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> corresponding to an input image. Here, the outputs are scores: <i class="timesitalic">y<sub class="fm-subscript">j</sub></i> is the score corresponding to the <i class="timesitalic">j</i>th class. (In this subsection, we have dropped the superscripts indicating the training data index to simplify notations.)</p>
<p class="body">Given a (training data instance, GT label) pair <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">c</i>)</span> (that is, the GT class corresponding to the input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is <i class="timesitalic">c</i>), the multiclass support vector machine (SVM) loss is</p><!--<p class="Body"><span class="times">$$\sum_{j=0, j \neq c}^{N-1}  max\left( 0, y_{j} - y_{c} + m \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="63" src="../../OEBPS/Images/eq_09-08.png" width="196"/></p>
</div>
<p class="fm-equation-caption">Equation 9.8 <span class="calibre" id="eq-muticlass-svm-loss"/></p>
<p class="body">where <i class="timesitalic">m</i> is a margin usually <span class="math"><i class="fm-italics">m</i> = 1</span>).</p>
<p class="body">To understand this, consider the equation without the margin first:</p><!--<p class="Body"><span class="times">$$\sum_{j=0, j \neq c}^{N-1}  max\left( 0, _{j} - y_{c}  \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="65" src="../../OEBPS/Images/eq_09-08-a.png" width="202"/></p>
</div>
<p class="body">In equation <a class="url" href="#eq-muticlass-svm-loss">9.8</a>, we are summing over all the classes except the one that matches the GT. In other words, we are summing over only the incorrect classes. For these, we want the score <i class="timesitalic">y<sub class="fm-subscript">j</sub></i> to be smaller than the score <i class="timesitalic">y<sub class="fm-subscript">c</sub></i> for the correct class. There are two possibilities:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Good output</i>‚ÄîIncorrect class score less than correct class score:<br class="calibre20"/>
<br class="calibre20"/>
<span class="math"><i class="fm-italics">y<sub class="fm-subscript">j</sub></i> ‚àí <i class="fm-italics">y<sub class="fm-subscript">c</sub></i> &lt; 0 <span class="cambria">‚üπ</span> <i class="fm-italics">max</i>(0, <i class="fm-italics">y<sub class="fm-subscript">j</sub></i> ‚àí <i class="fm-italics">y<sub class="fm-subscript">c</sub></i>) = 0</span><br class="calibre20"/>
<br class="calibre20"/>
      The contribution to the loss is zero (we do not distinguish between correct scores: all friends are at zero distance).</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Bad output</i>‚ÄîIncorrect class score more than correct class score:<br class="calibre20"/>
<br class="calibre20"/>
<span class="math"><i class="fm-italics">y<sub class="fm-subscript">j</sub></i> &gt; <i class="fm-italics">y<sub class="fm-subscript">c</sub></i> <span class="cambria">‚üπ</span> <i class="fm-italics">max</i>(0, <i class="fm-italics">y<sub class="fm-subscript">j</sub></i> ‚Äì <i class="fm-italics">y<sub class="fm-subscript">c</sub></i>) = <i class="fm-italics">y<sub class="fm-subscript">j</sub></i> ‚àí <i class="fm-italics">y<sub class="fm-subscript">c</sub></i></span><br class="calibre20"/>
<br class="calibre20"/>
      The contribution to the loss is positive (non-friends are at a positive distance that varies with the degree of non-friendness).</p>
</li>
</ul>
<p class="body">In practical settings, the margin is set to a positive number usually 1) to penalize predictions where the score of the correct class is marginally greater than that of the incorrect classes. This forces the classifier to learn to predict the correct class with high confidence. Figure <a class="url" href="#fig-hinge-loss-good-bad-visual">9.8</a> shows how hinge loss differs for good and bad output predictions.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="566" id="fig-hinge-loss-good-bad-visual" src="../../OEBPS/Images/CH09_F05_Chaudhury.png" width="1022"/></p>
<p class="figurecaption">Figure 9.5 Hinge loss for good and bad output predictions</p>
</div>
<p class="body">One mental model to have about the multiclass SVM loss is that it is lazy. It stops changing as soon as the correct class score exceeds the incorrect scores by the margin <i class="timesitalic">m</i>. The loss does not change if the correct class score goes still higher, which means it does not push the machine to improve beyond that point. This behavior is different from the softmax CE loss, which tries to push the machine to achieve an infinite score for the correct class.</p>
<h2 class="fm-head" id="optimization">9.2 Optimization</h2>
<p class="body"><a id="marker-314"/>Neural network models define a loss function that estimates the badness of the network‚Äôs output. During supervised training, the output on a particular training instance input is compared to a known output GT) for that particular training instance. The difference between the GT and the network-generated output is called loss. We can sum up the losses from individual training data instances and compute the total loss over all the training data.</p>
<p class="body">These losses are, of course, functions of the network parameters, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. We can imagine a space whose dimensionality is <span class="math"><i class="fm-italics">dim</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>) + <i class="fm-italics">dim</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span>. At each point in this space, we have a value for the total training loss. Thus, we can imagine a loss surface‚Äîa surface whose height represents the loss value‚Äîdefined over the high-dimensional domain of network parameters (weights and biases).</p>
<p class="body">Optimization is nothing but finding the lowest point on this surface. During training, we start with random values of network parameters: this is akin to starting at a random point on the surface. Then we constantly move locally downhill on the loss surface in the direction of the negative gradient. We hope this eventually takes us to the minimum or a sufficiently low point. Continuing our analogy of the loss surface as a ravine, the minimum is at sea level. This minimum provides us with the network parameter values (weights and biases) that will yield the least loss on the training data. If the training data set adequately represents the problem, the trained model will perform well on unseen data.</p>
<p class="body">This process of traveling toward the minimum, the iterative updating of weights and biases to have minimal loss over the training dataset, is called <i class="fm-italics">optimization</i>. The basic math was introduced in section <a class="url" href="../Text/08.xhtml#sec-loss-surface-graddesc">8.4.2</a> (equation <a class="url" href="../Text/08.xhtml#eq-wtsbiases-update-vector">8.12</a>). Here we study many practical nuances and variants.</p>
<p class="body">At every iteration, we update the weights and biases, so if <i class="timesitalic">t</i> denotes the iteration number, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">t</sub></i></span> denotes the weight values at iteration <i class="timesitalic">t</i>, <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">t</sub></i></span> denotes the weight updates at iteration <i class="timesitalic">t</i>, and so on:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{w}_{t+1} = \vec{w}_{t} -\delta \vec{w}_{t} \nonumber
\nonumber\\
&amp;\vec{b}_{t+1} = \vec{b}_{t} - \delta
\vec{b}_{t}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_09-09.png" width="116"/></p>
</div>
<p class="fm-equation-caption">Equation 9.9 <span class="calibre" id="eq-optim-updates"/></p>
<p class="body">The basic update is along the direction of the negative gradient (see equation <a class="url" href="../Text/08.xhtml#eq-wtsbiases-update-vector">8.12</a>):</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\delta \vec{w}_{t} =  \eta \nabla_{\vec{w}} \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t}  \right) \nonumber \\
&amp;\delta \vec{b}_{t} =  \eta \nabla_{\vec{b}} \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t}  \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="79" src="../../OEBPS/Images/eq_09-10.png" width="155"/></p>
</div>
<p class="fm-equation-caption">Equation 9.10 <span class="calibre" id="eq-optim-vanilla"/></p>
<p class="body"><a id="marker-315"/>Here, <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">t</sub></i>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">t</sub></i>)</span> denotes the loss at iteration <i class="timesitalic">t</i>. Ideally, we should evaluate the loss on every training data instance and average them. But that would imply that we must process every training data instance for every iteration, which is prohibitively expensive. Instead, we use sampling see section <a class="url" href="../Text/09.xhtml#sec-SGD">9.2.2</a>):</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The constant <i class="timesitalic">Œ∑</i> is called the <i class="fm-italics">learning rate</i> (LR). A larger LR results in bigger steps (bigger adjustments to weights and biases per update) and vice versa. We use larger values of LR in the beginning: when the network is completely untrained, we want to take large steps toward the minimum. When we are close to the minimum, on the other hand, we want to take smaller steps, lest we overshoot it. The LR is typically a small number, like <span class="math"><i class="fm-italics">Œ∑</i> = 0.01</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In stochastic gradient descent (SGD; a popular approach), the LR <i class="timesitalic">Œ∑</i> is typically held constant during an epoch (an <i class="fm-italics">epoch</i> is a single pass over all the training data). Then the LR is decreased after one or more epochs. This process is called <i class="fm-italics">learning rate decay</i>. So, the LR is not exactly a constant. We could have written it <i class="timesitalic">Œ∑<sub class="fm-subscript">t</sub></i> to indicate the temporal nature, but we chose to keep it simple because (in SGD, at least) it changes infrequently.</p>
</li>
</ul>
<p class="body">We have to reevaluate the loss and its gradients in each iteration since their values will change in every iteration, because the weights and biases of the underlying neural network are changing.</p>
<p class="body">How many iterations are necessary? Typically, this is a large number. We iterate <i class="fm-italics">multiple times over the entire training dataset</i>. A typical training session has multiple epochs. In this context, note that for proper convergence, it is <i class="fm-italics">extremely important</i> to randomly shuffle the order of occurrence of the training data after every epoch. In the following sections, we look at some practical nuances of the process.</p>
<h3 class="fm-head1" id="geometrical-view-of-optimization">9.2.1 Geometrical view of optimization</h3>
<p class="body">This topic is described in detail in section <a class="url" href="../Text/08.xhtml#sec-loss-surface-graddesc">8.4.2</a>. You are encouraged to revisit that discussion if necessary.</p>
<p class="body">Overall, neural network optimization is an iterative process. Ideally, in each iteration, we compute the gradient of the loss with respect to the current parameters (weights and biases) and obtain improved values for them by moving in the direction of the negative gradient.</p>
<h3 class="fm-head1" id="sec-SGD">9.2.2 Stochastic gradient descent and minibatches</h3>
<p class="body"><a id="marker-316"/>How do we compute the gradient of the loss function? The loss is different for every training data instance. The sensible thing to do is to average them out. But as we mentioned earlier, that leads to a practical problem: we would have to process the entire training dataset on every iteration. If the size of the training dataset is <i class="timesitalic">n</i>, an epoch is an <span class="math"><i class="fm-italics">O</i>(<i class="fm-italics">n</i><sup class="fm-superscript">2</sup>)</span> operation every iteration, we have to process all of the <i class="timesitalic">n</i> training data instances to compute the gradient, and an epoch has <i class="timesitalic">n</i> iterations). Since <i class="timesitalic">n</i> is a large number, often in the millions, <span class="math"><i class="fm-italics">O</i>(<i class="fm-italics">n</i><sup class="fm-superscript">2</sup>)</span> is prohibitively expensive.</p>
<p class="body">In SGD, we do not average over the entire training data set to produce the gradient. Instead, we average over a random sampled subset of the training data. This randomly sampled subset of training data is called a <i class="fm-italics">minibatch</i>. The gradient is computed by averaging the loss over the minibatch (as opposed to the entire training dataset). This gradient is used to update the weight and bias parameters.</p>
<h3 class="fm-head1" id="sec-pytorch-sgd">9.2.3 PyTorch code for SGD</h3>
<p class="body">Now, let‚Äôs implement SGD in PyTorch.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for SGD, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/ePyG">http://mng.bz/ePyG</a>.</p>
<p class="body">Let‚Äôs consider the example discussed in section <a class="url" href="../Text/06.xhtml#sec-GMM">6.9</a>. Our goal is to build a model that can predict whether a Statsville resident is a man, woman, or child using height and weight as input data. For this purpose, let‚Äôs assume we have a large dataset <i class="timesitalic">X</i> containing the heights and weights of various Statsville residents. <i class="timesitalic">X</i> is of shape <span class="math">(<i class="fm-italics">num</i>_<i class="fm-italics">samples</i>,2)</span>, where each row represents the <span class="math">(<i class="fm-italics">height</i>, <i class="fm-italics">weight</i>)</span> pair of a single resident. The corresponding labels are stored in <span class="times1">y<sub class="fm-subscript">gt</sub></span>, which contains <span class="math"><i class="fm-italics">num</i>_<i class="fm-italics">samples</i></span> elements. Each row of <span class="times1">y<sub class="fm-subscript">gt</sub></span> can be 0, 1, or 2, depending on whether the resident is a man, woman, or child. Figure <a class="url" href="#fig-sgd-example-gt-dist">9.6</a> shows an example distribution of <i class="timesitalic">X</i>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="518" id="fig-sgd-example-gt-dist" src="../../OEBPS/Images/CH09_F06_Chaudhury.png" width="779"/></p>
<p class="figurecaption">Figure 9.6 Height and weight of various Statsville residents. Class <span class="math">0</span> (man) is represented by the right-most cluster, class <span class="math">1</span> (woman) by the middle cluster, and class <span class="math">2</span> (child) by the left-most cluster.</p>
</div>
<p class="body">Before training a model, we must first convert the data into a training-friendly format. We subclass <code class="fm-code-in-text">torch.utils.data.Dataset</code> to do so and implement the <code class="fm-code-in-text">__len__</code> and <code class="fm-code-in-text">__getitem__</code> methods. The <i class="timesitalic">i</i>th training data instance can be accessed by calling <code class="fm-code-in-text">data_set[i]</code>. Remember that in SGD, we feed in minibatches that contain <code class="fm-code-in-text">batch_size</code> elements in every iteration. This can be achieved by calling the <code class="fm-code-in-text">__getitem__</code> method <code class="fm-code-in-text">batch_size</code> times. However, instead of doing this ourselves, we use PyTorch‚Äôs <code class="fm-code-in-text">DataLoader</code>, which gives us a convenient wrapper. Using <code class="fm-code-in-text">DataLoader</code> is recommended in production settings because it provides a simple API through which we can (1) create minibatches, 2) speed up data-loading times via multiprocessing, and (3) randomly shuffle data in every epoch to prevent overfitting. The following code creates a custom PyTorch data set.<a id="marker-317"/></p>
<p class="fm-code-listing-caption" id="code-create-dataset">Listing 9.7 PyTorch code to create a custom dataset</p>
<pre class="programlisting">from torch.utils.data import Dataset, DataLoader

class StatsvilleDataset(Dataset):                <span class="fm-combinumeral">‚ë†</span>
    def __init__(self, X, y_gt):
        self.X = X
        self.y_gt = y_gt

    def __len__(self):                           <span class="fm-combinumeral">‚ë°</span>
        return len(self.X)

    def __getitem__(self, i):                    <span class="fm-combinumeral">‚ë¢</span>
        return self.X[i], self.y_gt[i]

dataset = StatsvilleDataset(X, y_gt)             <span class="fm-combinumeral">‚ë£</span>

data_loader = DataLoader(dataset, batch_size=10, <span class="fm-combinumeral">‚ë§</span>
                       shuffle=True)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Subclasses torch.utils.data.Dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Returns the size of the data set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Returns the <i class="timesitalic">i</i>th training data element</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Instantiates the data set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> Instantiates the data loader with a batch size of 10 and shuffle on</p>
<p class="body">Our next step is to create a classifier model that can take the height and weight data (<i class="timesitalic">X</i>) as input and predict the output class. Here, we create a simple neural network model that consists of two linear layers followed by a softmax layer. The output of the softmax layer has three values representing the probability of each of the three classes (man, woman, and child), respectively. Note that in the forward pass, we don‚Äôt call the softmax layer because our loss function, PyTorch‚Äôs CE loss, expects raw, un-normalized scores as input. Hence we pass the output of the second linear layer to the loss function. However, during prediction, we pass the scores to the softmax layer to get a probability vector and then take an argmax to get the predicted class. Notice that we have a function to initialize the weights of the linear layers: this is important because the starting value of the weights can often affect convergence. If the model starts too far from the minimum, it may never converge.<a id="marker-318"/></p>
<p class="fm-code-listing-caption" id="code-create-nn-model">Listing 9.8 PyTorch code to create a custom neural network model</p>
<pre class="programlisting">class Model(torch.nn.Module):                                     <span class="fm-combinumeral">‚ë†</span>
    def __init__(self, input_size, hidden_size, output_size):
        super(Model, self).__init__()
        self.linear1 = torch.nn.Linear(input_size, hidden_size)   <span class="fm-combinumeral">‚ë°</span>
        self.linear2 = torch.nn.Linear(hidden_size, output_size)
        self.softmax = torch.nn.Softmax(dim=1)

    def forward(self, X):                                         <span class="fm-combinumeral">‚ë¢</span>
        scores = self.linear2(self.linear1(X))
        return scores

    def predict(self, X):                                         <span class="fm-combinumeral">‚ë£</span>
        scores = self.forward(X)
        y_pred = torch.argmax(self.softmax(scores), dim=1)
        return y_pred


def initialize_weights(m):
    if isinstance(m, torch.nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight.data)              <span class="fm-combinumeral">‚ë§</span>
        torch.nn.init.constant_(m.bias.data, 0)

model = Model(input_size=2, hidden_size=100, output_size=3)
model.apply(initialize_weights)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Subclasses torch.nn.Module</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Instantiates the linear layers and the softmax layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Feeds forward the input through the two linear layers</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Predicts the output class index</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> Initializes the weights to help the model converge better</p>
<p class="body">Now that we have our data set and model, let‚Äôs define our loss function and instantiate our SGD optimizer.</p>
<p class="fm-code-listing-caption" id="code-create-ce-loss-sgd-optim">Listing 9.9 PyTorch code for a loss function and SGD optimizer</p>
<pre class="programlisting">loss_fn = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.02) <span class="fm-combinumeral">‚ë†</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Instantiates the SGD optimizer with learning rate <span class="math">=</span> 0.02</p>
<p class="body">Now we define the training loop, which is essentially one pass over the entire dataset. We iterate over the dataset in batches of size <code class="fm-code-in-text">batch_size</code>, run the forward pass, compute the gradients, and update the weights in the direction of the negative gradient. Note that we call <code class="fm-code-in-text">optimizer.zero_grad()</code> in every iteration to prevent the accumulation of gradients from the previous steps.</p>
<p class="fm-code-listing-caption" id="code-train-loop">Listing 9.10 PyTorch code for one training loop</p>
<pre class="programlisting">def train_loop(epoch, data_loader, model, loss_fn, optimizer):
    for X_batch, y_gt_batch in data_loader:                   <span class="fm-combinumeral">‚ë†</span>

        scores = model(X_batch)                               <span class="fm-combinumeral">‚ë°</span>

        loss = loss_fn(scores, y_gt_batch)                    <span class="fm-combinumeral">‚ë¢</span>

        optimizer.zero_grad()                                 <span class="fm-combinumeral">‚ë£</span>

        loss.backward()                                       <span class="fm-combinumeral">‚ë§</span>

        optimizer.step()                                      <span class="fm-combinumeral">‚ë•</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Iterates through the data set in batches</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Feeds forward the model to compute scores</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Computes the cross-entropy loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Clears the gradients accumulated from the previous step</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> Runs backpropagation and computes the gradients</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë•</span> Updates the weights</p>
<p class="body">With this, we are ready to train our model. The following code shows how to do so. Figure <a class="url" href="#fig-sgd-examples-model-pred">9.7</a> shows the output predictions and loss at the end of every epoch.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="655" id="fig-sgd-examples-model-pred" src="../../OEBPS/Images/CH09_F07_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 9.7 Model predictions at the end of every epoch. Notice how the loss is reduced with every epoch. In the beginning, all training data points are wrongly classified as class 1. After the end of epoch 1, most of the training data is classified correctly, and the distribution of the classifier‚Äôs output has become close to the ground truth. The loss continues to decrease until epoch 4 although the improvements are harder to see visually).<a id="marker-319"/></p>
</div>
<p class="fm-code-listing-caption" id="code-run-train-loop">Listing 9.11 PyTorch code to run the training loop <code class="fm-code-in-text">num_epochs</code> times</p>
<pre class="programlisting">num_epochs = 2
for epoch in range(num_epochs):
    train_loop(epoch, data_loader, model, loss_fn, optimizer)</pre>
<h3 class="fm-head1" id="momentum">9.2.4 Momentum</h3>
<p class="body">For real-life loss surfaces in high dimensions, the ravine analogy is quite appropriate. A loss surface is hardly like a porcelain cup with smooth walls; it is much more like the walls of the Grand Canyon (see figure <a class="url" href="#fig-grand-canyon-momentum">9.8</a>). Furthermore, the gradient estimate (usually done over a minibatch) is noisy. Consequently, gradient estimates are never aligned in one direction‚Äîthey tend to go hither and thither. Nonetheless, most of them tend to have a good downhill component. The other (non-downhill component) is somewhat random (see figure <a class="url" href="#fig-grand-canyon-momentum">9.8</a>). So if we average them, the downhill components reinforce each other and are strengthened, while the non-downhill components cancel each other and are weakened.<a id="marker-320"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="537" id="fig-grand-canyon-momentum" src="../../OEBPS/Images/CH09_F08_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 9.8 Momentum. Noisy stochastic gradient estimates at different points on the loss surface (thick solid arrows) are not aligned in direction, but they all have a significant downhill component (thin solid arrows). The non-downhill components thin dashed arrows) point in random directions. Hence, averaging tends to strengthen the downhill component and cancel out the non-downhill components.</p>
</div>
<p class="body">Furthermore, if there is a small flat region with downhill areas preceding and following it, the vanilla gradient-based approach will get stuck in the small flat region (since the gradient there is zero). But averaging with the past allows us to have nonzero updates that take the optimization out of the local small flat region so that it can continue downhill.</p>
<p class="body">A good mental picture in this context is that of a ball rolling downhill. The surface of the hill is rough, and the ball is not going directly downward. Rather, it takes a zigzag path. But as it travels, it gathers downward momentum, and its downward velocity becomes greater.</p>
<p class="body">Following this theory, we take the weighted average of the gradient computed at this iteration with the update used in the previous iteration:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;
\overbrace{
\delta \vec{w}_{t}
}^{\text{current update}}
= \gamma
\overbrace{
\delta \vec{w}_{t-1}
}^{\text{last update}}
+ \eta
\overbrace{
\nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t}  \right)
}^{\text{current gradient}}
\nonumber \\
&amp;
\overbrace{
\delta \vec{b}_{t}
}^{\text{current update}}
= \gamma
\overbrace{
\delta \vec{b}_{t-1}
}^{\text{last update}}
+ \eta
\overbrace{
\nabla_{\vec{b}} \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t}  \right)
}^{\text{current gradient}}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="150" src="../../OEBPS/Images/eq_09-11.png" width="308"/></p>
</div>
<p class="fm-equation-caption">Equation 9.11 <span class="calibre" id="eq-momentum"/></p>
<p class="body">where <i class="timesitalic">Œ≥</i>, <i class="timesitalic">Œ∑</i> are positive constants with values less than <span class="math">1</span>. The weights and bias parameters are updated in the usual fashion using equation <a class="url" href="#eq-optim-updates">9.9</a>.<a id="marker-321"/></p>
<p class="fm-head2" id="unrolling-the-recursion-of-the-momentum-equation">Unrolling the recursion of the momentum equation</p>
<p class="body">Unraveling the recursive equation <a class="url" href="#eq-momentum">9.11</a>, we see</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\delta \vec{w}_{t} &amp;= \eta \nabla_{\vec{w}} \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t}  \right) + \gamma \delta \vec{w}_{t-1}  \\
&amp;=  \eta \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t},
\vec{b}_{t}  \right)
     + \eta \gamma \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t-1},
\vec{b}_{t-1}  \right)
     + \gamma^{2} \delta \vec{w}_{t-2} \\
&amp;= \eta \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t},
\vec{b}_{t}  \right)
     + \eta  \gamma \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t-1},
\vec{b}_{t-1}  \right)
     + \eta \gamma^{2}  \nabla_{\vec{w}} \mathbb{L} \left(
\vec{w}_{t-2}, \vec{b}_{t-2}  \right)
     + \gamma^{3} \delta \vec{w}_{t-3} \\
&amp;\quad \vdots \\
&amp;= \eta \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t},
\vec{b}_{t}  \right)
    +\eta \gamma  \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t-1},
\vec{b}_{t-1}  \right)
    + \cdots
    +\eta  \gamma^{t}  \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{0},
\vec{b}_{0}  \right)
    + \gamma^{t+1} \delta \vec{w}_{-1}  \\\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="197" src="../../OEBPS/Images/eq_09-11-a.png" width="597"/></p>
</div>
<p class="body">Assuming <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><sub class="fm-subscript">‚àí1</sub> = 0</span>, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\delta \vec{w}_{t}  &amp;= \eta \nabla_{\vec{w}} \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t}  \right)
    +\eta \gamma  \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t-1},
\vec{b}_{t-1}  \right)
    + \eta \gamma^{2}  \nabla_{\vec{w}} \mathbb{L} \left( \vec{w}_{t-2},
\vec{b}_{t-2}  \right)  \nonumber \\
    &amp;\quad + \cdots \nonumber \\
    &amp;\quad +\eta  \gamma^{t}  \nabla_{\vec{w}} \mathbb{L} \left(
\vec{w}_{0}, \vec{b}_{0}  \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="107" src="../../OEBPS/Images/eq_09-12.png" width="500"/></p>
</div>
<p class="fm-equation-caption">Equation 9.12 <span class="calibre" id="eq-momentum-unrolled"/></p>
<p class="body">Thus,</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">We are taking a weighted sum of the gradients from past iterations. This is not quite a weighted average, though, as explained later.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Older gradients are weighted by higher powers of <i class="timesitalic">Œ≥</i>. Since <span class="math"><i class="fm-italics">Œ≥</i> &lt; 1</span>, weights decrease with age long-past iterations have less influence).</p>
</li>
<li class="fm-list-bullet">
<p class="list">The sum of the weights of the gradients, going backward from now to the beginning of time (the <span class="math">0</span>th iteration), is</p>
</li>
</ul><!--<p class="Body"><span class="times"><i class="fm-italics">S<sub class="FM-Subscript">t</sub></em> = <i class="fm-italics">Œ∑</em>(1+<i class="fm-italics">Œ≥</em> + <i class="fm-italics">Œ≥</em><sup class="FM-Superscript">2</sup> + <i class="fm-italics">Œ≥</em><sup class="FM-Superscript">3</sup>‚ãØ<i class="fm-italics">Œ≥<sub class="FM-Subscript">t</sub></em>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_09-12-a.png" width="219"/></p>
</div>
<p class="body-ind">Now, using Taylor expansion,</p><!--<p class="Body"><span class="times">$$\lim_{t \to
\infty} \left( 1 + \gamma + \gamma^{2} + \gamma^{3} \cdots \gamma^{t-1}
\right) = \frac{1}{1 - \gamma}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_09-12-b.png" width="288"/></p>
</div>
<p class="body-ind">Thus the sum of the weights of the past gradients in momentum-based gradient descent is <span class="math"><i class="fm-italics">Œ∑</i>/(1‚Äì<i class="fm-italics">Œ≥</i>) ‚â† 1</span>. In other words, this is not quite a weighted average (where the weights should sum up to 1). This is a somewhat undesirable property and is rectified in the Adam algorithm discussed later.</p>
<p class="body">A similar analysis can be done for the biases.</p>
<h3 class="fm-head1" id="geometric-view-constant-loss-contours-gradient-descent-and-momentum">9.2.5 Geometric view: Constant loss contours, gradient descent, and momentum</h3>
<p class="body">Consider a network with a tiny two-element weight vector <!--<span class="times">$\vec{w} = \begin{bmatrix} w_{0} \\ w_{1}\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_09-12-c2.png" width="65"/></span> and no bias. Further, suppose that the loss function is <span class="math"><span class="segoe">ùïÉ</span> = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>||<sup class="fm-superscript">2</sup> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span>. The constant loss contours are concentric circles with the origin as the center. The radius of the circle indicates the loss magnitude.<a id="marker-322"/></p>
<p class="body">If we move along the circle‚Äôs circumference, the loss does not change. The loss changes maximally along the orthogonal direction to that: the radius of the circle. This intuitive observation is confirmed by evaluating the gradient</p><!--<p class="Body"><span class="times">$$\nabla_{\vec{w}}
\mathbb{L} = 2
\begin{bmatrix} w_{0} \\ w_{1}
\end{bmatrix}$$</span> </p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_09-12-d.png" width="100"/></p>
</div>
<p class="body">Thus the gradient is along the radius, and the negative gradient direction is radially inward. So the loss decreases most rapidly if we move radially inward. If we move orthogonal to the radius (that is, along the circumference), the loss remains unchanged; of course, we are moving along the constant loss contour.</p>
<p class="body">Optimization then takes us from outer, larger-radius circles to inner, smaller-radius circles. The minimum is at the origin; ideally, optimization should stop once we reach the origin.</p>
<p class="body">Figure <a class="url" href="#fig-graddesc-momentum-constant-loss-contours">9.9</a> shows optimization for a simple loss function <span class="math"><span class="segoe">ùïÉ</span> = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>||<sup class="fm-superscript">2</sup> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span>. We start at an arbitrary pair of weight values and repeatedly update them via equation <a class="url" href="#eq-optim-updates">9.9</a>. For figure <a class="url" href="#fig-graddesc-momentum-constant-loss-contours">9.9a</a>, we use update without momentum (equation <a class="url" href="#eq-optim-vanilla">9.10</a>). For figure <a class="url" href="#fig-graddesc-momentum-constant-loss-contours">9.9b</a>, we use update with momentum (equation <a class="url" href="#eq-momentum">9.11</a>).</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre18" height="642" id="fig-gradient-descent-without-momentum" src="../../OEBPS/Images/CH09_F09a_Chaudhury.png" width="669"/></p>
<p class="figurecaption">(a) A trajectory through constant loss contours for gradient descent <i class="fm-italics">without</i> momentum</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre18" height="653" id="fig-gradient-descent-with-momentum" src="../../OEBPS/Images/CH09_F09b_Chaudhury.png" width="668"/></p>
<p class="figurecaption">(b) A trajectory through constant loss contours for gradient descent <i class="fm-italics">with</i> momentum</p>
</div>
<p class="fm-table-caption" id="fig-graddesc-momentum-constant-loss-contours">Figure 9.9 Constant loss contours and optimization trajectory for the loss function <span class="math"><span class="segoe">ùïÉ</span> = ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>||<sup class="fm-superscript">2</sup> = <i class="fm-italics">w</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup> + <i class="fm-italics">w</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">2</sup></span>. The loss surface is a cone with its apex on the origin and its base a circle in a plane parallel to the <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span> plane. Optimization takes us down the cone toward smaller and smaller cross-sections as we approach the minimum at the origin. Updates are shown with arrows. Note how the momentum version arrives at a smaller circle in fewer steps.<a id="marker-323"/></p>
<p class="body">When the constant loss contours are concentric circles with the origin as the center, the loss surface is a cone with its apex on the origin. The cross sections are circles on planes parallel to the <span class="math"><i class="fm-italics">w</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">w</i><sub class="fm-subscript">1</sub></span> plane. Optimization takes us down the inner walls of the cone through smaller and smaller circular cross-sections as we approach the minimum. The global minimum is at the origin and corresponds to zero loss. The zero-loss contour is a circle with a zero radius, which is effectively a single point: the origin.</p>
<p class="body">In both cases, progress slows down (step sizes become smaller) as we approach the minimum. This is because the magnitude of the gradient becomes smaller and smaller as we get closer to the minimum (imagine a bowl: it becomes flatter as we get closer to the bottom). However, this effect is countered to a certain extent if we have momentum. So, we can see that we need fewer steps to reach a circle with a smaller radius when we have momentum.</p>
<h3 class="fm-head1" id="nesterov-accelerated-gradients">9.2.6 Nesterov accelerated gradients</h3>
<p class="body">One problem with momentum-based gradient descent is that it may overshoot the minimum. This can be seen in figure <a class="url" href="#fig-nesterov-momentum-constant-loss-contours">9.10a</a> where the loss decreases through a series of updates and then, when we are close to the minimum, an update (shown with a dotted arrow) overshoots the minimum and increases the loss (shown with a dotted circle).</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre18" height="657" id="fig-gradient-descent-with-momentum-overshoots" src="../../OEBPS/Images/CH09_F10a_Chaudhury.png" width="668"/></p>
<p class="figurecaption">(a) Momentum gradient descent overshooting the minimum. Loss decreases for a while and then increases dotted circle and arrow)</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre18" height="657" id="fig-nesterov-gradient-descent" src="../../OEBPS/Images/CH09_F10b_Chaudhury.png" width="668"/></p>
<p class="figurecaption">(b) Nesterov reduces the step size when we are about to overshoot the minimum.</p>
</div>
<p class="fm-table-caption" id="fig-nesterov-momentum-constant-loss-contours">Figure 9.10 Figure (a) shows momentum-based gradient descent overshooting the minimum. Over-shooting the update is shown with a dotted arrow. A circle with a radius larger than the last step is shown with a dotted outline. Nesterov does better by taking smaller steps when overshooting is imminent.<a id="marker-324"/></p>
<p class="body">This is the phenomenon that Nesterov‚Äôs accelerated gradient-based optimization tries to tackle.</p>
<p class="body">In Nesterov‚Äôs technique, we do the following:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Estimate where this update will take us (that is, the destination point) by adding the previous step‚Äôs update to the current point.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Compute the gradient at the estimated destination point. This is where the approach differs from the vanilla momentum-based approach, which takes the gradient at the current point.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Take a weighted average of the gradient (at the estimated destination point) with the previous step‚Äôs update. That is the update for the current step.</p>
</li>
</ol>
<p class="body"><a id="marker-325"/>Mathematically speaking,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;
\overbrace{
\delta \vec{w}_{t}
}^{\text{current update}}
=
\gamma
\overbrace{
\delta \vec{w}_{t-1}
}^{ \text{last update} }
+ \eta
\overbrace{
\nabla_{\vec{w}} \mathbb{L}
\left(
\overbrace{ \vec{w}_{t} - \gamma \delta \vec{w}_{t-1} }^{\text{estimated destination}},
\overbrace{
\vec{b}_{t} - \gamma \delta \vec{b}_{t-1}
}^{\text{estimated destination}}
\right)
}^{\text{gradient at estimated destination}} \nonumber\\% \displaybreak
\\
&amp;
\overbrace{
\delta \vec{b}_{t}
}^{\text{current update}}
=
\gamma
\overbrace{
\delta \vec{b}_{t-1}
}^{ \text{last update} }
+ \eta
\overbrace{
\nabla_{\vec{b}} \mathbb{L} \left(
\overbrace{
\vec{w}_{t} - \gamma \delta \vec{w}_{t-1}
}^{\text{estimated destination}},
\overbrace{
\vec{b}_{t} - \gamma \delta \vec{b}_{t-1}
}^{\text{estimated destination}}
\right)
}^{\text{gradient at estimated destination}}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="264" src="../../OEBPS/Images/eq_09-13.png" width="532"/></p>
</div>
<p class="fm-equation-caption">Equation 9.13 <span class="calibre" id="eq-nag"/></p>
<p class="body">where <i class="timesitalic">Œ≥</i>, <i class="timesitalic">Œ∑</i> are constants with values less than <span class="math">1</span>. The weights and biases are updated in the usual fashion using equation <a class="url" href="#eq-optim-updates">9.9</a>.</p>
<p class="body">Why does this help? Well, consider the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">When we are somewhat far away from the minimum (no possibility of overshooting), the gradient at the estimated destination is more or less the same as the gradient at the current point, so we progress toward the minimum in a fashion similar to momentum-based gradient descent.</p>
</li>
<li class="fm-list-bullet">
<p class="list">But when we are close to the minimum and the current update may potentially take us past it (see the dotted arrow in figure <a class="url" href="#fig-nesterov-momentum-constant-loss-contours">9.10a</a>), the gradient at the estimated destination will lie on the other side of the minimum. As before, imagine this loss surface as a cone with its apex at the origin and base above the apex. We have traveled down one of the cone‚Äôs side walls and just started climbing back up. Thus the gradient at the estimated destination is in the opposite direction from the previous step. When we take their weighted average, they will cancel each other in some dimensions, resulting in a smaller magnitude. The resulting smaller step will mitigate the overshooting phenomenon.</p>
</li>
</ul>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for momentum and Nesterov accelerated gradients, executable via Jupyter notebook, can be found at <a class="url" href="http://mng.bz/p9KR">http://mng.bz/p9KR</a>.</p>
<h3 class="fm-head1" id="adagrad">9.2.7 AdaGrad</h3>
<p class="body">The momentum-based optimization approach (equation <a class="url" href="#eq-momentum">9.11</a>) and the Nesterov approach (equation <a class="url" href="#eq-nag">9.13</a>) both suffer from a serious drawback: they treat all dimensions of the parameter vectors <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> equally. But the loss surface is <i class="fm-italics">not</i> symmetrical in all dimensions. The slope can be high along some dimensions and low along others. We cannot control everything with a single LR for all the dimensions. If we set the LR high, the high-slope dimensions will exhibit too much variance. If we set it low, the low-slope dimensions will barely progress toward the minimum.<a id="marker-326"/></p>
<p class="body">What we need is <i class="fm-italics">per-parameter LR</i>. Then each LR will adapt to the slope of its particular dimension. This is what AdaGrad tries to achieve. Dimensions with historically larger gradients have smaller LRs, while dimensions with historically smaller gradients have larger LRs.</p>
<p class="body">How do we keep track of the historic magnitudes of gradients? To do this, AdaGrad maintains a state vector in which it accumulates the sum of the squared partial derivatives for each dimension of the gradient vector seen so far during training:</p><!--<p class="Body"><span class="times">$$\vec{s}_{t} =
\overbrace{
\nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t} \right)
\circ \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t}
\right)
}^{\text{per-dimension gradient squared}}
+
\vec{s}_{t-1}
=
\begin{bmatrix}
| \frac{\partial \mathbb{L} }{ \partial w_{0} } |^{2} \\
| \frac{\partial \mathbb{L} }{ \partial w_{1} } |^{2} \\
\vdots \\
| \frac{\partial \mathbb{L} }{ \partial b_{0} } |^{2} \\
\vdots
\end{bmatrix}
+
\vec{s}_{t-1}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="183" src="../../OEBPS/Images/eq_09-13-a.png" width="402"/></p>
</div>
<p class="body">where <span class="math">‚àò</span> denotes the Hadamard operator (elementwise multiplication of the two vectors). We can express the previous equation a bit more succinctly as</p><!--<p class="Body"><span class="times"><i class="fm-italics"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_s.png" /></span><sub class="FM-Subscript">t</sub></em> = |‚àá<sub class="FM-Subscript"><span class="inFigure"><img alt="" height="12px" src="imgs/icons/AR_w.png" /></span></sub><span class="segoe">ùïÉ</span>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_w.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>, <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_b.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>)|<sup class="FM-Superscript">2</sup> + <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_s.png" /></span><sub class="FM-Subscript"><i class="fm-italics">t</em> ‚àí 1</sub></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="35" src="../../OEBPS/Images/eq_09-13-b.png" width="200"/></p>
</div>
<p class="body">Unrolling the recursion, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{s}_{t}
&amp;=  | \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t}
\right) |^{2}+ \vec{s}_{t-1} \\
&amp;= | \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t}
\right) |^{2} + | \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t-1},
\vec{b}_{t-1} \right) |^{2}
+ \vec{s}_{t-2} \\
&amp;\quad \vdots \\
&amp;=
| \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t} \right)
|^{2}
+|  \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t-1}, \vec{b}_{t-1}
\right) |^{2}
\cdots
+ | \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{0}, \vec{b}_{0}
\right) |^{2}
+ \vec{s}_{-1}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="156" src="../../OEBPS/Images/eq_09-13-c.png" width="519"/></p>
</div>
<p class="body">assuming <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span><sub class="fm-subscript">‚àí1</sub> = 0</span>, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span><sub class="fm-subscript">t</sub></i> is a vector that holds the cumulative sum over all training iterations of the squared slope for each dimension. For the dimensions with historically high slopes, the corresponding element of <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span><sub class="fm-subscript">t</sub></i> is large, and vice versa. The overall update vector looks like this:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{s}_{t} &amp;= | \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t},
\vec{b}_{t} \right)  |^{2} + \vec{s}_{t-1} \nonumber  \\
\delta \vec{w}_{t} &amp;= \frac{\eta}{ \sqrt{ \vec{s}_{t} + \epsilon } }
\circ \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t}
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="119" src="../../OEBPS/Images/eq_09-14.png" width="223"/></p>
</div>
<p class="fm-equation-caption">Equation 9.14 <span class="calibre" id="eq-adagrad"/></p>
<p class="body">Here <i class="timesitalic">œµ</i> is a very small constant added to prevent division by zero. Then we use equation <a class="url" href="#eq-optim-updates">9.9</a> to update the weights as usual. A parallel set of equations exist for the bias.</p>
<p class="body">Using AdaGrad, in the loss gradient, the dimensions that have seen big slopes in earlier iterations are given less importance during the update. We emphasize the dimensions that have not seen much progress in the past. This is a bit like saying, ‚ÄúI will pay less attention to a person who speaks all the time, and I will pay more attention to someone who speaks infrequently.‚Äù</p>
<h3 class="fm-head1" id="root-mean-squared-propagation">9.2.8 Root-mean-squared propagation</h3>
<p class="body"><a id="marker-327"/>A significant drawback of the AdaGrad algorithm is that the magnitude of the vector <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span><sub class="fm-subscript">t</sub></i> keeps increasing as iteration progresses. This causes the LR for all dimensions to become smaller and smaller. Eventually, when the number of iterations is high, the LR becomes close to zero, and the updates do hardly anything; progress toward the minimum slows to a virtual halt. Thus AdaGrad is an impractical algorithm to use in real life, although the idea of per-component LR is good.</p>
<p class="body">Root mean squared propagation (RMSProp) addresses this drawback without sacrificing the dimension-adaptive nature of AdaGrad. Here again there is a state vector, but its equation is</p><!--<p class="Body"><span class="times"><i class="fm-italics"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_s.png" /></span><sub class="FM-Subscript">t</sub></em> = (1‚àí<i class="fm-italics">Œ≥</em>) |‚àá<sub class="FM-Subscript"><span class="inFigure"><img alt="" height="12px" src="imgs/icons/AR_w.png" /></span></sub><span class="segoe">ùïÉ</span>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_w.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>, <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_b.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>)|<sup class="FM-Superscript">2</sup> + <i class="fm-italics">Œ≥</em> <i class="fm-italics"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_s.png" /></span><sub class="FM-Subscript">t</em> ‚àí 1</sub></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/eq_09-14-a.png" width="270"/></p>
</div>
<p class="body">Compare this with the state vector equation in AdaGrad:</p><!--<p class="Body"><span class="times"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_s.png" /></span><sub class="FM-Subscript"><i class="fm-italics">t</sub></em> = |‚àá<sub class="FM-Subscript"><span class="inFigure"><img alt="" height="12px" src="imgs/icons/AR_w.png" /></span></sub><span class="segoe">ùïÉ</span>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_w.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>, <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_b.png" /></span><i class="fm-italics"><sub class="FM-Subscript">t</sub></em>)|<sup class="FM-Superscript">2</sup> + <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_s.png" /></span><sub class="FM-Subscript"><i class="fm-italics">t</em> ‚àí 1</sub></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_09-14-b.png" width="193"/></p>
</div>
<p class="body">They are almost the same, but the terms are weighted by <span class="math">(1‚àí<i class="fm-italics">Œ≥</i>)</span> and <i class="timesitalic">Œ≥</i>, where <span class="math">0 &lt; <i class="fm-italics">Œ≥</i> &lt; 1</span> is a constant. This particular pair of weights in a recursive equation has a very interesting effect. To see it, we have to unroll the recursion:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{s}_{t} &amp;=
\left( 1- \gamma \right) \; | \nabla_{ \vec{w} } \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t} \right)  |^{2}
+ \gamma \; \vec{s}_{t-1} \\
&amp;=
\left( 1- \gamma \right) \; | \nabla_{ \vec{w} } \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t} \right)  |^{2}
+\left( 1- \gamma \right) \; \gamma \; | \nabla_{ \vec{w} } \mathbb{L}
\left( \vec{w}_{t-1}, \vec{b}_{t-1} \right)  |^{2}
+ \gamma^{2} \; \vec{s}_{t-2} \\
&amp;=
\left( 1- \gamma \right) \; | \nabla_{ \vec{w} } \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t} \right)  |^{2}
+\left( 1- \gamma \right) \; \gamma \; | \nabla_{ \vec{w} } \mathbb{L}
\left( \vec{w}_{t-1}, \vec{b}_{t-1} \right)  |^{2} \\
&amp;\quad +\left( 1- \gamma \right) \; \gamma^{2} \; | \nabla_{ \vec{w}
} \mathbb{L} \left( \vec{w}_{t-2}, \vec{b}_{t-2} \right)  |^{2}
+ \gamma^{3} \; \vec{s}_{t-3} \\
&amp;\vdots \\
&amp;=
\left( 1- \gamma \right) \; | \nabla_{ \vec{w} } \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t} \right)  |^{2}
+\left( 1- \gamma \right) \; \gamma \; | \nabla_{ \vec{w} } \mathbb{L}
\left( \vec{w}_{t-1}, \vec{b}_{t-1} \right)  |^{2} \\
&amp;\quad\cdots
+ \left( 1- \gamma \right) \; \gamma^{t} \; | \nabla_{ \vec{w} }
\mathbb{L} \left( \vec{w}_{0}, \vec{b}_{0}
\right)  |^{2}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="272" src="../../OEBPS/Images/eq_09-14-c.png" width="519"/></p>
</div>
<p class="body">Thus <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span><sub class="fm-subscript">t</sub></i> is a weighted sum of the past term-wise squared gradient magnitude vectors. Going back from now to the beginning of time (the <span class="math">0</span>th iteration), the weights are <span class="math">(1‚àí<i class="fm-italics">Œ≥</i>)</span>, <span class="math">(1‚àí<i class="fm-italics">Œ≥</i>) <i class="fm-italics">Œ≥</i></span>, <span class="math">(1‚àí<i class="fm-italics">Œ≥</i>) <i class="fm-italics">Œ≥</i><sup class="fm-superscript">2</sup></span>, <span class="math">‚ãØ</span>, <span class="math">(1‚àí<i class="fm-italics">Œ≥</i>) <i class="fm-italics">Œ≥<sup class="fm-superscript">t</sup></i></span>. If we add these weights, we get</p><!--<p class="Body"><span class="times">(1‚àí<i class="fm-italics">Œ≥</em>) (1+<i class="fm-italics">Œ≥</em> + <i class="fm-italics">Œ≥</em><sup class="FM-Superscript">2</sup> + <i class="fm-italics">Œ≥</em><sup class="FM-Superscript">3</sup>‚ãØ<i class="fm-italics">Œ≥<sub class="FM-Subscript">t</sub></em>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_09-14-d.png" width="234"/></p>
</div>
<p class="body">As the number of iterations becomes high (<span class="math"><i class="fm-italics">t</i> ‚Üí ‚àû</span>), this becomes</p><!--<p class="Body"><span class="times">$$\left( 1- \gamma \right)  \; \lim_{t \to \infty}
\left( 1 + \gamma + \gamma^{2} + \gamma^{3} \cdots \gamma^{t-1} \right)
= \frac{1}{1 - \gamma} \left( 1- \gamma \right)  = 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_09-14-e.png" width="423"/></p>
</div>
<p class="body">where Taylor expansion has been used to evaluate the term in parentheses.</p>
<p class="body">For a large number of iterations, the sum of the weights approaches <span class="math">1</span>. This implies that after many iterations, the RMSProp state vector effectively takes the <i class="fm-italics">weighted average of the past term-wise squared gradient magnitude vectors</i>. With more iterations, the weights are redistributed and older terms become de-emphasized, but the overall magnitude does not increase. This eliminates the vanishing LR problem from AdaGrad. RMSProp continues de-emphasizing the dimensions with high cumulative partial derivatives with lower LRs, but it does so without making the LR vanishingly small.</p>
<p class="body">The overall RMSProp update equations are</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{s}_{t} &amp;= \left( 1- \gamma \right) \; | \nabla_{ \vec{w} }
\mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t} \right)  |^{2} + \gamma \;
\vec{s}_{t-1} \nonumber \\
\delta \vec{w}_{t} &amp;= \frac{\eta}{\sqrt{\vec{s}_{t} +
\epsilon } } \circ \nabla_{ \vec{w} } \mathbb{L} \left( \vec{w}_{t},
\vec{b}_{t} \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="89" src="../../OEBPS/Images/eq_09-15.png" width="281"/></p>
</div>
<p class="fm-equation-caption">Equation 9.15 <span class="calibre" id="eq-rmsprop"/></p>
<p class="body">There is a parallel set of equations for the bias. The weights and bias parameters are updated in the usual fashion using equation <a class="url" href="#eq-optim-updates">9.9</a>.<a id="marker-328"/></p>
<h3 class="fm-head1" id="adam-optimizer">9.2.9 Adam optimizer</h3>
<p class="body">Momentum-based gradient descent amplifies the downhill component more and more as iterations progress. On the other hand, RMSProp reduces the LR for dimensions that are seeing large gradients and vice versa to balance the progress rate along all dimensions.</p>
<p class="body">Both of these are desirable properties. We want an optimization algorithm that combines them, and that algorithm is Adam. It is increasingly becoming the optimizer of choice for most deep learning researchers.</p>
<p class="body">The Adam optimization algorithm maintains two state vectors:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\vec{v}_{t} &amp;=
\left( 1 - \beta_{1}  \right) \; \nabla_{\vec{w}} \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t}  \right)
+
\beta_{1} \; \delta \vec{w}_{t-1} \\
\vec{s}_{t} &amp;=
\left( 1- \beta_{2} \right) \; | \nabla_{ \vec{w} } \mathbb{L} \left(
\vec{w}_{t}, \vec{b}_{t} \right)  |^{2}
+
\beta_{2} \; \vec{s}_{t-1}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_09-16.png" width="289"/></p>
</div>
<p class="fm-equation-caption">Equation 9.16 <span class="calibre" id="eq-adam-vt"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_09-17.png" width="295"/></p>
</div>
<p class="fm-equation-caption">Equation 9.17 <span class="calibre" id="eq-adam-st"/></p>
<p class="body">where <span class="math">0 &lt; <i class="fm-italics">Œ≤</i><sub class="fm-subscript">1</sub> &lt; 1</span> and <span class="math">0 &lt; <i class="fm-italics">Œ≤</i><sub class="fm-subscript">2</sub> &lt; 1</span> are constants. Note the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Equation <a class="url" href="#eq-adam-vt">9.16</a> is essentially the momentum equation of equation <a class="url" href="#eq-momentum">9.11</a> with one significant difference. We have changed the term weights to <span class="math"><i class="fm-italics">Œ≤</i><sub class="fm-subscript">1</sub></span> and <span class="math">(1‚àí<i class="fm-italics">Œ≤</i><sub class="fm-subscript">1</sub>)</span>. As we‚Äôve seen, with <span class="math"><i class="fm-italics">t</i> ‚Üí ‚àû</span>, this results in the state vector being a weighted average of all the past gradients. This is an improvement over the original momentum scheme.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The second state vector is basically the one from the RMSProp equation <a class="url" href="#eq-rmsprop">9.15</a>.</p>
</li>
</ul>
<p class="body">Using these two state vectors, Adam creates the update vector as follows:</p><!--<p class="Body"><span class="times">$$\delta \vec{w}_{t} = \frac{ \eta
\; \vec{v}_{t}  }{ \sqrt{ \vec{s}_{t} + \epsilon } } \circ \nabla_{
\vec{w} } \mathbb{L} \left( \vec{w}_{t}, \vec{b}_{t} \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_09-18.png" width="216"/></p>
</div>
<p class="fm-equation-caption">Equation 9.18 <span class="calibre" id="eq-adam-update"/></p>
<p class="body">The <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">t</sub></i></span> in the numerator pulls in the benefits of the momentum approach (with the enhancement of averaging). Otherwise, the equation is almost identical to the RMSProp; those benefits are also pulled in.</p>
<p class="body">Finally, the weights and bias parameters are updated in the usual fashion using equation <a class="url" href="#eq-optim-updates">9.9</a>.</p>
<p class="fm-head2" id="bias-correction">Bias correction</p>
<p class="body">The sum of weights of past values in the state vectors <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">t</sub></i></span>, <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span><sub class="fm-subscript">t</sub></i> will approach <span class="math">‚àû</span> only at large values of <i class="timesitalic">t</i>. To improve the approximation at smaller values of <i class="timesitalic">t</i>, Adam introduces a bias correction:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\hat{v}_{t} &amp;= \frac{ \vec{v}_{t} }{ \left( 1 - \beta_{1}^{t}
\right) }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_09-16.png" width="289"/></p>
</div>
<p class="fm-equation-caption">Equation 9.19</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\hat{s}_{t} &amp;= \frac{ \vec{s}_{t} }{ \left( 1 - \beta_{2}^{t}
\right) }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="69" src="../../OEBPS/Images/eq_09-20.png" width="102"/></p>
</div>
<p class="fm-equation-caption">Equation 9.20</p>
<p class="body">Instead of <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_v.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">t</sub></i></span> and <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_s.png" width="14"/></span><sub class="fm-subscript">t</sub></i>, we use the bias-corrected entities <i class="timesitalic">vÃÇ<sub class="fm-subscript">t</sub></i> and <i class="timesitalic">≈ù<sub class="fm-subscript">t</sub></i> in equation <a class="url" href="#eq-adam-update">9.18</a>.<a id="marker-329"/></p>
<p class="fm-code-listing-caption" id="listing-9.12-pytorch-code-for-various-optimizers">Listing 9.12 PyTorch code for various optimizers</p>
<pre class="programlisting">from torch import optim

sgd_optimizer = optim.SGD([params], lr=0.01)                   <span class="fm-combinumeral">‚ë†</span>

sgd_momentum_optimizer = optim.SGD([params], lr=0.01,
                                    momentum=0.9)              <span class="fm-combinumeral">‚ë°</span>

sgd_nesterov_momentum_optimizer = optim.SGD([params], lr=0.01, <span class="fm-combinumeral">‚ë¢</span>
                                    momentum=0.9, nesterov=True)

adagrad_optimizer = optim.Adagrad([params], lr=0.001)

rms_prop_optimizer = RMSprop([params], lr=1e-2,                <span class="fm-combinumeral">‚ë£</span>
                       alpha=0.99)

adam_optimizer = optim.Adam([params], lr=0.001,
                       betas=(0.9, 0.999))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Sets the learning rate to 0.01</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Sets the momentum to 0.9</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Sets the Nesterov flag to True</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Sets the smoothing constant to 0.99 (<i class="timesitalic">Œ≥</i> in <a class="url" href="#eq-rmsprop">9.15</a>)</p>
<h2 class="fm-head" id="sec-regularization">9.3 Regularization</h2>
<p class="body">Suppose we are teaching a baby to recognize cars. We show them red cars, blue cars, black cars, large cars, small cars, medium cars, cars with round tops, cars with rectangular tops, and so on. Soon the baby‚Äôs brain realizes that there are too many varieties to remember them all by rote. So the brain starts forming <i class="fm-italics">abstractions</i>: mysterious common features that occur together are stored in the baby‚Äôs brain with the label <i class="fm-italics">car</i>. The brain has learned to classify an abstract entity called a car. Even though it fails to <i class="fm-italics">remember</i> every car it has seen, it can <i class="fm-italics">recognize</i> cars. We can say it has developed <i class="fm-italics">experience</i>. And so it is with neural networks. We do not want our network to <i class="fm-italics">remember</i> every training instance. Rather, we want the network to form <i class="fm-italics">abstractions</i> that will enable it to <i class="fm-italics">recognize</i> an object during inferencing even though the exact likeness of the object instance encountered during inferencing was never seen during training.<a id="marker-330"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Overfitting and underfitting</p>
<p class="fm-sidebar-text">If the network has too much expressive power (too many perceptrons or, equivalently, too many weights) relative to the number of training instances, the network can and often will rote-remember the training instances. This phenomenon is called <i class="fm-italics">overfitting</i>. Overfitting causes the network to perform very well over the training data but badly during testing or real-life inferencing. Stated another way, the network has adjusted itself to every nook, bend, and kink of the training data and thereby performs great on the training data‚Äîto the detriment of test data performance. This is illustrated in figure <a class="url" href="#fig-overfitting">9.11</a>. <i class="fm-italics">Regularization</i> refers to a bag of tricks that, in general, try to prevent overfitting. This is the topic of this section.</p>
<p class="fm-sidebar-text">There is another phenomenon called <i class="fm-italics">underfitting</i>, where the network simply does not have enough expressive power to model the training data. The symptom of underfitting is that the network performs badly on both training and testing data. If we see this, we should try a more complex network with more perceptrons.</p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre12" height="1022" id="fig-overfitting" src="../../OEBPS/Images/CH09_F11_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 9.11 Overfitting: data points for a binary classifier. Points belonging to different classes are visually demarcated as squares and circles. Filled squares/circles indicate training data, and unfilled squares/circles indicate test data. There are some anomalous training data instances (filled circles in the square zone). The estimated decision boundary (solid line) has become wriggly to accommodate them, which is causing many test points (unfilled squares/circles) to be misclassified. The wiggly solid curve is an example of an overfitted decision boundary. If we had chosen a ‚Äúsimpler‚Äù decision boundary (dashed straight line), the two anomalous training points would have been misclassified, but the machine would have performed much better in tests.<a id="marker-331"/></p>
</div>
<h3 class="fm-head1" id="sec-MDL">9.3.1 Minimum descriptor length: An Occam‚Äôs razor view of optimization</h3>
<p class="body">Is the set of weights and biases that minimizes a particular loss function unique? Let‚Äôs examine a single perceptron (equation <a class="url" href="../Text/07.xhtml#eq-perceptron">7.3</a>) with the output <span class="math"><i class="fm-italics">œï</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i>)</span>. Let‚Äôs say <i class="timesitalic">œï</i> is the Heaviside step function (see section <a class="url" href="../Text/07.xhtml#step-func">7.3.1</a>). Let <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><sub class="fm-subscript">*</sub>, <i class="fm-italics">b</i><sub class="fm-subscript">*</sub></span> be the weights and biases minimizing the loss function. It is easy to see that the perceptron output remains the same if we scale the weights, like <span class="math"><i class="fm-italics">Œ±</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><sub class="fm-subscript">*</sub></span> for all positive real values of <i class="timesitalic">Œ±</i>. Thus the weight vector <span class="math">7<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><sub class="fm-subscript">*</sub></span> will also minimize the loss function.</p>
<p class="body">This is true in general for arbitrary neural networks (composed of many perceptrons): the set of weights and biases minimizing a loss function is non-unique. How does the neural network choose one? Which of them is correct? We can use the principle of Occam‚Äôs razor to answer that.</p>
<p class="body">Occam‚Äôs razor is a philosophical principle. Its literal translation from Latin states, ‚ÄúEntities should not be multiplied beyond necessity.‚Äù This is roughly taken to mean <i class="fm-italics">among adequate explanations, the simplest one is the best</i>. In machine learning, this principle is typically interpreted as follows:</p>
<p class="fm-quote">Among the set of candidate neural network parameter values (weights and biases) that minimize the loss, the ‚Äúsimplest‚Äù one should be chosen.</p>
<p class="body">The general idea is as follows. Suppose we are trying to minimize <span class="math"><span class="segoe">ùïÉ</span>(<i class="fm-italics">Œ∏</i>)</span> (here we have used <i class="timesitalic">Œ∏</i> to denote <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> together). We also want the solution to be as simple as possible. To achieve that, we add a penalty for departing from ‚Äúsimplicity‚Äù to the original loss term. Thus we minimize</p>
<p class="fm-equation"><span class="math"><span class="segoe">ùïÉ</span>(<i class="fm-italics">Œ∏</i>) + <i class="fm-italics">ŒªR</i>(<i class="fm-italics">Œ∏</i>)</span></p>
<p class="body">Here,</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The expression <span class="math"><i class="fm-italics">R</i>(<i class="fm-italics">Œ∏</i>)</span> indicates a measure for un-simplicity. It is sometimes called the <i class="fm-italics">regularization penalty</i>. Adding a regularization penalty to the loss incentivizes the network to try to minimize un-simplicity <span class="math"><i class="fm-italics">R</i>(<i class="fm-italics">Œ∏</i>)</span> (or, equivalently, maximize simplicity) while trying to minimize the original loss term <span class="math"><span class="segoe">ùïÉ</span>(<i class="fm-italics">Œ∏</i>)</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">Œª</i> is a hyperparameter. Its value should be carefully chosen via trial and error. If <i class="timesitalic">Œª</i> is too low, this is akin to no regularization, and the network becomes prone to overfitting. If <i class="timesitalic">Œª</i> is too high, the regularization penalty will dominate, and the network will not adequately minimize the actual loss term.</p>
</li>
</ul>
<p class="body">There are two popular ways of estimating <span class="math"><i class="fm-italics">R</i>(<i class="fm-italics">Œ∏</i>)</span>, outlined in sections <a class="url" href="#sec-l2-reg">9.3.2</a> and <a class="url" href="#sec-l1-reg">9.3.3</a>, respectively. Both try to minimize some norm (length) of the parameter vector (which is basically a network descriptor). This is why regularization can be viewed as minimizing the descriptor length.<a id="marker-332"/></p>
<h3 class="fm-head1" id="sec-l2-reg">9.3.2 L2 regularization</h3>
<p class="body">In L2 regularization, we posit that shorter-length vectors are simpler. In other words, simplicity is inversely proportional to the square of the L2 norm (aka Euclidean norm). Thus,</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">R</i>(<i class="fm-italics">Œ∏</i>) = (||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>||<sup class="fm-superscript">2</sup>+||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>||<sup class="fm-superscript">2</sup>)</span></p>
<p class="body">Overall, we minimize</p><!--<p class="Body"><span class="times">$$\mathbb{L} \left(
\vec{w}, \vec{b} \right) =
\sum_{ i = 0 }^{ n - 1 } \mathbb{L}^{ \left( i \right) } \left(
\vec{y}^{ \left( i \right) }, \bar{y}^{ \left( i \right) }  \right) +
\lambda \; \left( \| \vec{w} \|^{2}  + \| \vec{b}
\|^{2}   \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_09-21.png" width="358"/></p>
</div>
<p class="fm-equation-caption">Equation 9.21 <span class="calibre" id="eq-l2-reg"/></p>
<p class="body">Compare this with equation <a class="url" href="#eq-loss">9.2</a>.</p>
<p class="body">L2 regularization is by far the most popular form of regularization. From this point on, we often use <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span> to mean the L2-regularized version (that is, equation <a class="url" href="#eq-l2-reg">9.21</a>). The hyperparameter <i class="timesitalic">Œª</i> is often called <i class="fm-italics">weight decay</i> in PyTorch. Weight decay is usually set to a small number so that the second term of equation <a class="url" href="#eq-l2-reg">9.21</a> the norm of the weight vector) does not drown the actual loss term. The following code shows how to instantiate an optimizer with regularization enabled.</p>
<p class="fm-code-listing-caption" id="listing-9.13-pytorch-code-to-enable-l2-regularization">Listing 9.13 PyTorch code to enable L2 regularization</p>
<pre class="programlisting">from torch import optim

optimizer = optim.SGD([params], lr=0.2, weight_decay=0.01) <span class="fm-combinumeral">‚ë†</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Sets the weight decay to 0.01</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre23" height="403" src="../../OEBPS/Images/CH09_F12a_Chaudhury.png" width="1023"/><br class="calibre20"/></p>
<p class="figurecaption">(a) L1 regularization</p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre23" height="402" src="../../OEBPS/Images/CH09_F12b_Chaudhury.png" width="1023"/><br class="calibre20"/></p>
<p class="figurecaption">(b) L2 regularization</p>
</div>
<p class="fm-table-caption" id="fig-l1-l2-reg">Figure 9.12 L1 and L2 regularization</p>
<h3 class="fm-head1" id="sec-l1-reg">9.3.3 L1 regularization</h3>
<p class="body">L1 regularization is similar in principle to L2 regularization, but it defines simplicity as the sum of the absolute values of the weights and biases:<a id="marker-333"/></p>
<p class="fm-equation"><span class="math"><i class="fm-italics">R</i>(<i class="fm-italics">Œ∏</i>) = (|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>|+|<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>|)</span></p>
<p class="body">Thus, here we minimize</p><!--<p class="Body"><span class="times">$$\mathbb{L} \left(
\vec{w}, \vec{b} \right) =
\sum_{ i = 0 }^{ n - 1 } \mathbb{L}^{ \left( i \right) } \left(
\vec{y}^{ \left( i \right) }, \bar{y}^{ \left( i \right) }  \right) +
\lambda \; \left( | \vec{w} |  + | \vec{b} |   \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_09-22.png" width="358"/></p>
</div>
<p class="fm-equation-caption">Equation 9.22 <span class="calibre" id="eq-l1-reg"/></p>
<h3 class="fm-head1" id="sparsity-l1-vs.-l2-regularization">9.3.4 Sparsity: L1 vs. L2 regularization</h3>
<p class="body">L1 regularization tends to create sparse models where many of the weights are 0. In comparison, L2 regularization tends to create models with low (but nonzero) weights.</p>
<p class="body">To understand this, consider figure <a class="url" href="#fig-l1-l2-reg">9.12</a>, which plots the loss function and its derivative for both L1 and L2 regularization. Let <i class="timesitalic">w</i> be a single element of the weight vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>. In L1 regularization,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mathbb{L} \left(w\right) = | w | \;\;\;\;\;\;\;\;\;\;\;\;\;\; \\
\frac{\partial \mathbb{L} \left(w\right)}{\partial w} =
\begin{cases}
-1 \;\;\; \text{if $w$ &lt; 0}\\ 0 \;\;\; \text{if $w$ = 0}\\ 1 \;\;\; \text{if $w$ &gt; 0}\\
\end{cases}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="96" src="../../OEBPS/Images/eq_09-22-a.png" width="185"/></p>
</div>
<p class="body">Since the gradient is constant for all values of <i class="timesitalic">w</i>, L1 regularization pushes the weight toward 0 with the same step size at all values of <i class="timesitalic">w</i>. In particular, the step toward 0 does not reduce in magnitude when close to 0. In L2 regularization,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mathbb{L} \left(w\right) = w^{2} \\
\frac{\partial \mathbb{L} \left(w\right)}{\partial w} = 2w\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="78" src="../../OEBPS/Images/eq_09-22-b.png" width="114"/></p>
</div>
<p class="body">Here, the gradient keeps decreasing in magnitude as <i class="timesitalic">w</i> approaches 0. Hence, <i class="timesitalic">w</i> comes closer to 0 but may never reach 0 since the updates take smaller and smaller steps as <i class="timesitalic">w</i> approaches 0. Therefore, L2 regularization produces more dense weight vectors than L1 regularization, which produces sparse weight vectors with many 0s.</p>
<h3 class="fm-head1" id="bayes-theorem-and-the-stochastic-view-of-optimization">9.3.5 Bayes‚Äô theorem and the stochastic view of optimization</h3>
<p class="body"><a id="marker-334"/>In sections <a class="url" href="../Text/06.xhtml#sec-max_likelihood_estimation">6.6.2</a> and <a class="url" href="../Text/06.xhtml#sec-MAP_estimation">6.6.3</a>, we discussed maximum likelihood estimation (MLE) and maximum a posteriori (MAP) in the context of unsupervised learning (you are encouraged to revisit those sections if necessary). Here, we study them in the context of supervised learning.</p>
<p class="body">In the supervised optimization process, we have samples of the input and known output pairs (in the form of training data) <span class="math"><span class="segoe">‚ü®</span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup><span class="segoe">‚ü©</span></span>. Of course, we can do the forward pass and generate the network output at each training data point</p><!--<p class="Body"><span class="times"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup> = <i class="fm-italics">f</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>, <i class="fm-italics">Œ∏</em>)</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_09-22-c.png" width="132"/></p>
</div>
<p class="body">where <i class="timesitalic">Œ∏</i> is the network‚Äôs parameter set (representing weights and biases together).</p>
<p class="body">Suppose we view the sample training data generation as a stochastic process. We can model the probability of a training instance <span class="math"><i class="fm-italics">T</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = <span class="segoe">‚ü®</span><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup><span class="segoe">‚ü©</span></span> given the current network parameters <i class="timesitalic">Œ∏</i> as</p><!--<p class="Body"><span class="times"><i class="fm-italics">p</em>(<i class="fm-italics">T</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>|<i class="fm-italics">Œ∏</em>) <span class="cambria">‚àù</span> <i class="fm-italics">e</em><sup class="FM-Superscript">‚àí||<i class="fm-italics">»≥</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup> ‚àí <i class="fm-italics">f</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>, <i class="fm-italics">Œ∏</em>)||<sup class="FM-Superscript">2</sup></sup> = <i class="fm-italics">e</em><sup class="FM-Superscript">‚àí||<i class="fm-italics">»≥</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup> ‚àí <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>||<sup class="FM-Superscript">2</sup></sup></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_09-22-d.png" width="332"/></p>
</div>
<p class="body">This makes intuitive sense. At an optimal value of <i class="timesitalic">Œ∏</i>, the network output <span class="math"><i class="fm-italics">f</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, <i class="fm-italics">Œ∏</i>)</span> would match the GT <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>. We want our model distribution to have the highest probability density at the location where the network output matches the GT. The probability density should fall off with distance from that location.</p>
<p class="body">A little thought reveals that this Gaussian-like formulation, which leads to a regression-loss-like numerator, is not the only one possible. We can use any loss function in the numerator since all of them have the property of being minimum when the network output matches the GT and gradually increase as the mismatch grows. In general,</p><!--<p class="FM-Equation"><span class="times"><i class="fm-italics">p</em>(<i class="fm-italics">T</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup> | <i class="fm-italics">Œ∏</em>) <span class="cambria">‚àù</span> <i class="fm-italics">e</em><sup class="FM-Superscript">‚àí<span class="segoe">ùïÉ</span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>(<i class="fm-italics">»≥</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>, <i class="fm-italics">f</em>(<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>, <i class="fm-italics">Œ∏</em>))</sup> = <i class="fm-italics">e</em><sup class="FM-Superscript">‚àí<span class="segoe">ùïÉ</span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>(<i class="fm-italics">»≥</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>, <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_y.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>)</sup></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="50" src="../../OEBPS/Images/eq_09-22-e.png" width="359"/></p>
</div>
<p class="body">Assuming the training instances are mutually independent, the probability of the entire training dataset occurring jointly is the product of individual instance probabilities. If we denote the entire training dataset as <i class="timesitalic">T</i>,</p><!--<p class="FM-Equation"><span class="times"><i class="fm-italics">T</em> = {<span class="segoe">‚ü®</span><i class="fm-italics">»≥</em><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup>, <span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_x.png" /></span><sup class="FM-Superscript">(<i class="fm-italics">i</em>)</sup><span class="segoe">‚ü©</span>}</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_09-22-f.png" width="143"/></p>
</div>
<p class="body">Then</p><!--<p class="Body"><span class="times">$$p \left( T \middle\vert \theta
\right) = \prod_{i=0}^{N} p\left( T^{ \left( i \right) } \; \middle\vert
\; \theta \right)
\propto
\prod_{i=0}^{N} e^{- \mathbb{L}^{ \left( i \right) } \left( \bar{y}^{
\left( i \right) }, \vec{y}^{ \left( i \right) }  \right) }
= e^{- \sum_{i=0}^{N}  \mathbb{L}^{ \left( i \right) } \left( \bar{y}^{
\left( i \right) }, \vec{y}^{ \left( i \right) }  \right) }
= e^{ -\mathbb{L} \left( \theta  \right) }$$</span> </p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_09-22-g.png" width="544"/></p>
</div>
<p class="body">At this point, we can take one of two possible approaches described in the next two subsections.</p>
<p class="fm-head2" id="sec-optim-ML">MLE-based optimization</p>
<p class="body"><a id="marker-335"/>In this approach, we choose the optimal value for the parameter set <i class="timesitalic">Œ∏</i> by maximizing the likelihood</p><!--<p class="Body"><span class="times"><i class="fm-italics">p</em>(<i class="fm-italics">T</em>|<i class="fm-italics">Œ∏</em>) <span class="cambria">‚àù</span> <i class="fm-italics">e</em><sup class="FM-Superscript">‚àí<span class="segoe">ùïÉ</span>(<i class="fm-italics">Œ∏</em>)</sup></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="39" src="../../OEBPS/Images/eq_09-22-h.png" width="135"/></p>
</div>
<p class="body">This is equivalent to saying we will choose the optimal parameters <i class="timesitalic">Œ∏</i> such that the probability of occurrence of the training data is maximized. Thus the optimal parameter set <span class="math"><i class="fm-italics">Œ∏</i><sub class="fm-subscript">*</sub></span> is yielded by</p><!--<p class="Body"><span class="times">$$\theta_{*} = \underset{\theta}{argmax}
\;\; p \left( T \middle\vert \theta \right)  = \underset{\theta}{argmax}
\;\; e^{ -\mathbb{L} \left( \theta  \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_09-22-i.png" width="299"/></p>
</div>
<p class="body">Obviously, the optimal <i class="timesitalic">Œ∏</i> that maximizes the likelihood is the one that minimizes <span class="math"><span class="segoe">ùïÉ</span>(<i class="fm-italics">Œ∏</i>)</span>. So, the maximum likelihood formulation is nothing but minimizing the total mismatch between predicted and GT output over the entire training dataset.</p>
<p class="fm-head2" id="sec-optim-MAP">MAP optimization</p>
<p class="body">By Bayes‚Äô theorem (equation <a class="url" href="../Text/06.xhtml#eq-bayes-theorem-2var">6.1</a>),</p><!--<p class="Body"><span class="times">$$p \left( \theta \middle\vert T \right) =
\frac{ p \left( T \middle\vert \theta \right) \; p \left( \theta \right)
}{p \left( T \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_09-22-j.png" width="181"/></p>
</div>
<p class="body">To estimate the optimal <i class="timesitalic">Œ∏</i>, we can also maximize the posterior probability on the left side of this equation. This is equivalent to saying we will choose the optimal parameters <i class="timesitalic">Œ∏</i> such that <i class="timesitalic">Œ∏</i> has the maximal conditional probability given the training dataset. Thus the optimal value for the parameter set <i class="timesitalic">Œ∏</i> is yielded by</p><!--<p class="Body"><span class="times">$$\theta_{*} = \underset{\theta}{argmax} \;\; p \left( \theta \middle\vert T \right) =  \underset{\theta}{argmax} \;\;
\frac{ p \left( T \middle\vert \theta \right) \; p \left( \theta \right)
}{ p \left( T \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_09-22-k.png" width="353"/></p>
</div>
<p class="body">where the last equality is derived via Bayes‚Äô theorem.</p>
<p class="body">Observing the previous equation, we see that the denominator <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">T</i>)</span> in the rightmost term does not involve <i class="timesitalic">Œ∏</i> and hence can be dropped from the optimization. So,</p><!--<p class="Body"><span class="times">$$\theta_{*} = \underset{\theta}{argmax} \;\; p
\left( \theta \middle\vert T \right) =  \underset{\theta}{argmax} \;\; p
\left( T \middle\vert \theta \right) \; p \left( \theta \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_09-22-l.png" width="346"/></p>
</div>
<p class="body">How do we model the <i class="fm-italics">a priori</i> probability <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>)</span>? We can use Occam‚Äôs razor and say that we assign a higher <i class="fm-italics">a priori</i> probability to smaller parameter values. Thus, we can say</p><!--<p class="Body"><span class="times"><i class="fm-italics">p</em>(<i class="fm-italics">Œ∏</em>) <span class="cambria">‚àù</span> <i class="fm-italics">e</em><sup class="FM-Superscript">‚àí<i class="fm-italics">ŒªR</em>(<i class="fm-italics">Œ∏</em>)</sup></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="34" src="../../OEBPS/Images/eq_09-22-m.png" width="117"/></p>
</div>
<p class="body">Then the overall posterior probability maximization becomes</p><!--<p class="Body"><span class="times">$$\theta_{*} = \underset{\theta}{argmax} \;\; p
\left( \theta \middle\vert T \right) =  \underset{\theta}{argmax} \;\; p
\left( T \middle\vert \theta \right) \; p \left( \theta \right)
= e^{ -\left( \mathbb{L} \left( \theta  \right) + \lambda R \left(
\theta  \right) \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_09-22-n.png" width="466"/></p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Maximizing this posterior probability is equivalent to minimizing the regularized loss. Maximizing the likelihood is equivalent to minimizing the un-regularized loss.</p>
<h3 class="fm-head1" id="dropout">9.3.6 Dropout</h3>
<p class="body"><a id="marker-336"/>In the introduction for section <a class="url" href="../Text/09.xhtml#sec-regularization">9.3</a>, we saw that too much expressive power (too many perceptron nodes) sometimes prevents the machine from developing general <i class="fm-italics">abstractions</i> (aka <i class="fm-italics">experience</i>). Instead, the machine may remember the training data instances (see figure <a class="url" href="#fig-overfitting">9.11</a>). This phenomenon is called overfitting. We have already seen that one way to mitigate this problem is to add a regularization penalty‚Äîsuch as adding the L2 norm of the weights‚Äîto the loss to discourage the network from learning large weight values.</p>
<p class="body">Dropout is another method of regularization. Here, somewhat crazily, we turn off random perceptrons in the network (set their value to 0) during training. To be precise, we attach a probability <i class="timesitalic">p<sub class="fm-subscript">i</sub><sup class="fm-superscript">l</sup></i> with the <i class="timesitalic">i</i>th node (perceptron) in layer <i class="timesitalic">l</i>. In any training iteration, the node (perceptron) is off with a probability of <span class="math">(1‚àí<i class="fm-italics">p</i>)</span>. Typically, dropout is only enabled during training and is turned off during inferencing.</p>
<p class="body">What good does it do? Well,</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Dropout prevents the network from relying too much on a small number of nodes. Instead, the network is forced to use all the nodes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Equivalently, dropout encourages the training process to spread the weights to multiple nodes instead of putting much weight on a few nodes. This makes the effect somewhat similar to L2 regularization.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Dropout mitigates <i class="fm-italics">co-adaptation</i>: a behavior whereby a group of nodes in the network behave in a highly correlated fashion, emitting similar outputs most of the time. This means the network could retain only one of them with no significant loss of accuracy.</p>
</li>
</ul>
<p class="fm-head2" id="dropout-simulates-an-ensemble-of-subnetworks">Dropout simulates an ensemble of subnetworks</p>
<p class="body">Consider a small three-node intermediate layer of some neural network with dropout. The <i class="timesitalic">k</i>th input to this layer can turn on with probability <i class="timesitalic">p<sub class="fm-subscript">k</sub></i>. This means the probability of that input node turning off is <span class="math">(1‚àí<i class="fm-italics">p<sub class="fm-subscript">k</sub></i>)</span>. We can express this with a <i class="fm-italics">binary</i> stochastic variable <i class="timesitalic">Œ¥<sub class="fm-subscript">k</sub></i> for <span class="math"><i class="fm-italics">k</i> = 0</span> or <span class="math"><i class="fm-italics">k</i> = 1</span> or <span class="math"><i class="fm-italics">k</i> = 2</span>. This variable <i class="timesitalic">Œ¥<sub class="fm-subscript">k</sub></i> takes one of two possible values: <span class="math">0</span> or <span class="math">1</span>. The probability of it taking the value <span class="math">1</span> is <i class="timesitalic">p<sub class="fm-subscript">k</sub></i>. In other words, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ¥<sub class="fm-subscript">k</sub></i> = 1) = <i class="fm-italics">p<sub class="fm-subscript">k</sub></i></span>, and <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ¥<sub class="fm-subscript">k</sub></i> = 0) = 1 ‚àí <i class="fm-italics">p<sub class="fm-subscript">k</sub></i></span>. The output of this small three-node layer with dropout can be expressed as</p><!--<p class="Body"><span class="times">$$a^{l} = \sum_{k=0}^{2} \delta_{k} w_{k} a_{k}^{l-1}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_09-22-o.png" width="138"/></p>
</div>
<p class="body">We have three variables <span class="math"><i class="fm-italics">Œ¥</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">Œ¥</i><sub class="fm-subscript">1</sub></span>, and <span class="math"><i class="fm-italics">Œ¥</i><sub class="fm-subscript">2</sub></span>, each of which can take two values. Altogether we have <span class="math">2<sup class="fm-superscript">3</sup> = 8</span> possible combinations. Each combination leads to a subnode shown in figure <a class="url" href="#fig-dropout_3layer">9.13</a>. Each of these combinations has a probability of occurrence <i class="timesitalic">P<sub class="fm-subscript">i</sub></i>, also shown in the figure. These observations lead to a very important insight:</p>
<p class="fm-equation">The expected value of the output‚Äîthat is, <span class="math"><span class="segoe">ùîº</span>(<i class="fm-italics">a<sub class="fm-subscript">l</sub></i>)</span>‚Äîis the same as the expected value of the output if we deployed the subnetworks in figure <a class="url" href="#fig-dropout_3layer">9.13</a> randomly, with probabilities <i class="timesitalic">P<sub class="fm-subscript">i</sub></i>.</p>
<p class="body">Why does this matter? Well, given a problem, none of us know the right number of perceptrons for the network to deploy. One thing to do under these circumstances is to deploy networks of various strengths randomly and take an average of their outputs. We have just established that dropout (turning inputs off randomly) achieves the same thing. But deploying a network where inputs get turned on or off randomly is much simpler than deploying a large number of subnetworks. All we have to do is deploy a <i class="fm-italics">dropout</i> layer.</p>
<p class="fm-head2" id="pytorch-code-for-dropout">PyTorch code for dropout</p>
<p class="body">In section <a class="url" href="#sec-pytorch-sgd">9.2.3</a>, we created a simple two-layer neural network classifier to predict whether a Statsville resident is a man, woman, or child based on height and weight data. In this section, we show the same model with dropout layers added. Note that dropout should be enabled only during training, not during inferencing. To do this in PyTorch, you can call <code class="fm-code-in-text">model.eval()</code> before running inferencing. This way, your training and inferencing code remains the same, but PyTorch knows under the hood when to include the dropout layers and when not to.</p>
<p class="fm-code-listing-caption" id="listing-9.14-dropout">Listing 9.14 Dropout</p>
<pre class="programlisting">class ModelWithDropout(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Model, self).__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(input_size, hidden_size),
            torch.nn.Dropout(p=0.2),                 <span class="fm-combinumeral">‚ë†</span>


            torch.nn.Linear(hidden_size, output_size),
            torch.nn.Dropout(p=0.2)
        )

    def forward(self, X):
        return self.net(X)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Instantiates a dropout layer with a probability of dropout <span class="math">=</span> 0.2</p>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="683" id="fig-dropout_3layer_p_000" src="../../OEBPS/Images/CH09_F13a_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(a) Subnetwork 0: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">0</sub> = (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">0</sub>) (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">1</sub>) (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">2</sub>)</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="682" id="fig-dropout_3layer_p_001" src="../../OEBPS/Images/CH09_F13b_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(b) Subnetwork 1: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">1</sub> = (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">0</sub>) (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">1</sub>) <i class="fm-italics">p</i><sub class="fm-subscript">2</sub></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="682" id="fig-adropout_3layer_p_010" src="../../OEBPS/Images/CH09_F13c_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(c) Subnetwork 2: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">2</sub> = (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">0</sub> )<i class="fm-italics">p</i><sub class="fm-subscript">1</sub> (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">2</sub>)</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="682" id="fig-dropout_3layer_p_011" src="../../OEBPS/Images/CH09_F13d_Chaudhury.png" width="534"/></p>
<p class="figurecaption">(d) Subnetwork 3: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">3</sub> = (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">0</sub> )<i class="fm-italics">p</i><sub class="fm-subscript">1</sub> <i class="fm-italics">p</i><sub class="fm-subscript">2</sub></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="682" id="fig-dropout_3layer_p_100" src="../../OEBPS/Images/CH09_F13e_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(e) Subnetwork 4: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">4</sub> = <i class="fm-italics">p</i><sub class="fm-subscript">0</sub> (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">1</sub>) (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">2</sub>)</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="682" id="fig-dropout_3layer_p_101" src="../../OEBPS/Images/CH09_F13f_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(f) Subnetwork 5: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">5</sub> = <i class="fm-italics">p</i><sub class="fm-subscript">0</sub> (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">1</sub>) <i class="fm-italics">p</i><sub class="fm-subscript">2</sub></span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="682" id="fig-adropout_3layer_p_110" src="../../OEBPS/Images/CH09_F13g_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(g) Subnetwork 6: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">6</sub> = <i class="fm-italics">p</i><sub class="fm-subscript">0</sub> <i class="fm-italics">p</i><sub class="fm-subscript">1</sub> (1‚àí<i class="fm-italics">p</i><sub class="fm-subscript">2</sub>)</span></p>
</div>
<div class="figure3">
<p class="figure1"><img alt="" class="calibre16" height="682" id="fig-dropout_3layer_p_111" src="../../OEBPS/Images/CH09_F13h_Chaudhury.png" width="533"/></p>
<p class="figurecaption">(h) Subnetwork 7: probability <span class="math"><i class="fm-italics">P</i><sub class="fm-subscript">7</sub> = <i class="fm-italics">p</i><sub class="fm-subscript">0</sub> <i class="fm-italics">p</i><sub class="fm-subscript">1</sub> <i class="fm-italics">p</i><sub class="fm-subscript">2</sub></span><a id="marker-339"/></p>
</div>
<p class="figurecaption" id="fig-dropout_3layer">Figure 9.13 Dropout simulates subnetworks: illustrated with a three-node intermediate layer of a neural network. The probability of input node <span class="math"><i class="fm-italics">a<sub class="fm-subscript">k</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i> ‚àí 1)</sup></span> being <i class="fm-italics">on</i> is <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ¥<sub class="fm-subscript">k</sub></i> = 1) = <i class="fm-italics">p<sub class="fm-subscript">k</sub></i></span>.</p>
<h2 class="fm-head" id="summary-8">Summary</h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Training is the process by which a neural network identifies the optimal values for its parameters (weights and biases of the individual perceptrons). Training progresses iteratively: in each iteration, we estimate the loss and run an optimization step that updates the parameter values so as to decrease the loss. After doing this many times, we hope to arrive at optimal parameter values.<a id="marker-340"/></p>
</li>
<li class="fm-list-bullet">
<p class="list">In a supervised neural network, loss quantifies the mismatch between the desired output and the network output over sampled training data instances. The desired output (ground truth) is often estimated via manual effort. Training the neural network essentially identifies the weights and biases of the neural network that minimize the loss.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The discrepancy between the ground truth and network output can be expressed in many different ways. Each corresponds to a different loss function. Denoting the ground truth probabilities of all the possible classes given the <i class="timesitalic">i</i>th training input <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> as a vector <span class="math"><i class="fm-italics">»≥</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span> and the network output on the same as <span class="math"><i class="fm-italics">y</i><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup></span>,</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">Regression loss takes the L2 norm of the vector difference between the network output and ground-truth vectors and is mathematically expressed as <!--<span class="times">$\| \vec{y}^{\left ( i \right) } - \bar{y}^{ \left ( i \right) }  \|^{2} =\sum_{j=0}^{N-1} \left( y_{j}^{ \left ( i \right) }  - \bar{y}_{j}^{\left ( i \right) }  \right)^{2}$</span>--><span class="infigure"><img alt="" class="calibre5" height="49" src="../../OEBPS/Images/eq_09-22-p1.png" width="266"/></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If the neural network is a classifier, it typically outputs a vector of class probabilities. This means <!--<span class="times">$\vec{y}^{ \left ( i \right) } = \begin{bmatrix} p_{0}\left( \vec{x}^{ \left ( i \right) } \right) &amp;p_{1}\left(\vec{x}^{ \left ( i \right) } \right) &amp;p_{j}\left( \vec{x}^{ \left ( i \right) } \right) &amp;\cdots &amp;p_{N-1}\left( \vec{x}^{ \left ( i\right) } \right)\end{bmatrix}$</span>--><br class="calibre20"/>
<span class="infigure"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_09-22-p2.png" width="435"/></span><br class="calibre20"/>
          where <span class="math"><i class="fm-italics">p<sub class="fm-subscript">j</sub></i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span> is the network estimated probability of the input belonging to class <i class="timesitalic">j</i>, and <i class="timesitalic">N</i> is the number of possible classes. In general, given an input, a neural network computes per-class scores: the class with the highest score is the predicted class. The scores are unbounded numbers and can be arbitrarily large or small (even negative). The operation converts them into probabilities. If <!--<span class="times">$\vec{s} = \begin{bmatrix} s_{0} &amp;s_{1} &amp;s_{2} &amp;\cdots &amp;s_{N-1}\end{bmatrix}$</span>--><br class="calibre20"/>
<span class="infigure"><img alt="" class="calibre5" height="40" src="../../OEBPS/Images/eq_09-22-p3.png" width="212"/></span><br class="calibre20"/>
          denotes the score vector, the corresponding softmax vector is <!--<span class="times">$\vec{y}^{ \left ( i \right)} =\begin{bmatrix}\frac{e^{s_{0}}}{ S }&amp;\frac{e^{s_{1}}}{ S }&amp;\frac{e^{s_{2}}}{ S }&amp;\cdots&amp;\frac{e^{s_{N-1}}}{ S }\end{bmatrix}$</span>--><span class="infigure"><img alt="" class="calibre5" height="29" src="../../OEBPS/Images/eq_09-22-p4.png" width="250"/></span><br class="calibre20"/>
          where <!--<span class="times">$S =\sum_{k=0}^{N-1} e^{s_{k}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="28" src="../../OEBPS/Images/eq_09-22-p5.png" width="96"/></span>. The softmax output vector consists of probabilities, meaning they are numbers between <span class="math">0</span> and <span class="math">1</span> and they sum to <span class="math">1</span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Given the probability of each class, classifiers can employ the cross-entropy loss, which can be expressed as <!--<span class="times">$-\sum_{j=0}^{N-1} \bar{y}_{j}^{ \left ( i \right) }
\; log \left( y_{j}^{ \left ( i \right) } \right)$</span>--><br class="calibre20"/>
<span class="infigure"><img alt="" class="calibre5" height="33" src="../../OEBPS/Images/eq_09-22-p6.png" width="159"/></span>.<br class="calibre20"/>
          Note that since the ground-truth vector is one-hot, only a single term in this expression survives: the one corresponding to the desired class, denoted <span class="math"><i class="fm-italics">j</i><sup class="fm-superscript">*</sup></span>. The loss becomes the logarithm of the corresponding network output, <span class="math">‚àí<i class="fm-italics">log</i>(<i class="fm-italics">y</i><sub class="fm-subscript"><i class="fm-italics1">j</i><sup class="fm-superscript">*</sup></sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>)</span>, and is zero if <span class="math"><i class="fm-italics">y</i><sub class="fm-subscript"><i class="fm-italics1">j</i><sup class="fm-superscript">*</sup></sub><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup> = 1</span>. This agrees with our expectation: if the network is predicting the GT class with probability <span class="math">1</span>, there is no loss.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Since during training, softmax is almost always followed by cross-entropy loss, PyTorch has a combined operator called softmax cross-entropy loss. It is over doing these operations individually because it has better numerical properties.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Focal loss tries to tackle the data-imbalance problem by putting more weight on the ‚Äúhard‚Äù examples with higher loss.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Hinge loss is another popular loss that becomes zero when the correct class is predicted with the maximum score. Once that criterion is achieved, it no longer tries to improve the relative values of scores.</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list">Total loss can be obtained by adding the losses from all the individual training data instances. However, this requires us to process all the training data points in every iteration, which is prohibitively expensive. Hence, we sample a subset of training data points to create a minibatch, estimate losses for each input data instance in the minibatch, and add them to obtain an estimate for the total loss. This process is known as stochastic gradient descent (SGD).<a id="marker-341"/></p>
</li>
<li class="fm-list-bullet">
<p class="list">Optimization is the process of updating the neural network parameters (weights and biases of perceptrons) so as to reduce the loss. We can plot the loss against weights and bias values and obtain a hypersurface defined over the domain of weights and bias values. Our ultimate goal is to reach the bottom (minimum point) of this loss hypersurface. Optimization is geometrically equivalent to moving down the hypersurface to reduce the loss. The steepest descent (toward the nearest minimum) happens along a negative gradient.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We can combine several other criteria with the gradient to improve the convergence to the minimum loss value. Each of them results in a different optimization technique:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">Due to noisy estimations, the local gradient may not always point toward the minimum, but it will have a strong component toward that minimum along with other noisy components. Instead of blindly following the current gradient, we can follow the direction corresponding to a weighted average of the current gradient estimate and the gradient estimate from the previous iteration. This is a recursive process, which effectively means the direction followed at any iteration is a weighted average of the current and all gradient estimates from the beginning of training. Recent gradients are weighted higher, and older gradients are weighed lower. All these gradients have strong components toward the minimum that reinforce each other, while the noisy components point in random directions and tend to cancel each other. Thus the overall weighted sum is a more reliable move toward the minimum. This is called momentum-based gradient descent.</p>
</li>
<li class="fm-list-bullet">
<p class="list">A drawback of momentum is that it can result in overshooting the minimum. Nesterov accelerated gradients correct this drawback by calculating gradients via one-step lookahead. If the current update takes us to the other side of the minimum, the gradient will have an opposite direction there. We take a weighted average of the update suggested by momentum-based gradient descent and the gradient at the estimated destination point. If we are far from the minimum, it will be roughly equivalent to momentum-based gradient descent. But if we are about to overshoot the minimum, there will be cancelation, and the update will be smaller than that of the pure momentum case. Thus we reduce the chances of overshooting the minimum.</p>
</li>
<li class="fm-list-bullet">
<p class="list">AdaGrad is an optimization technique that imparts additional weight to infrequently changing axes of the loss hypersurface. The Adam optimizer combines many advantages of other optimizers and is often the modern optimizer of choice.</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list">The principle of Occam‚Äôs razor essentially says that among adequate explanations, the simplest one should be preferred. In machine learning, this leads to regularization. There are typically many solutions to the loss-minimization problem. We want to choose the simplest one. Accordingly, we add a penalty for departing from simplicity (regularization loss) to whatever other losses we have. This incentivizes the system to go for the simplest solution. Regularization loss is often the total length of the parameter vector; thus, regularization pushes us toward solutions with smaller absolute values for parameters.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Minimizing the loss function without the regularization term is equivalent to the Bayesian maximum likelihood estimation (MLE) technique. On the other hand, minimizing loss with the regularized term is equivalent to maximum a posteriori (MAP) estimation.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Overfitting is a phenomenon where the neural network has learned all the nuances of the training data. Since anomalous nuances in training data points are often caused by noise, this leads to worse performance during inferencing. Overfitting is symptomized by great accuracy (low loss) on training data but bad accuracy on test data. It often happens because the network has more expressive power than necessary for the problem and is trying to memorize all nooks and bends of the training data. Regularization mitigates the problem. Another trick is dropout, where we deliberately turn off a random subset of neurons during training iterations. This forces all the neurons to learn to do the job with a reduced set of neurons.<a id="marker-342"/></p>
</li>
</ul>
</div></body></html>