<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="chp__transformers"> <span class="chapter-title-numbering"><span class="num-string">3</span></span> <span class="title-text"> Transformers: How inputs become outputs</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Converting tokens into vectors</li> 
    <li class="readable-text" id="p3">Transformers, their types, and their roles</li> 
    <li class="readable-text" id="p4">Converting vectors back into tokens</li> 
    <li class="readable-text" id="p5">Creating the text generation loop</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>In chapter 2, we saw how large language models (LLMs) see text as fundamental units known as tokens. Now it’s time to talk about what LLMs do with the tokens they see. The process that LLMs use to generate their text is markedly different from how humans form coherent sentences. When an LLM operates, it is working on tokens, yet simultaneously cannot <em>manipulate</em> tokens like humans do because the LLM does not understand the structure and relationship of the letters each token represents.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>For example, English speakers know that the words “magic,” “magical,” and “magician” are all related. We can understand that sentences containing these words are all connected to the same subject matter because these words share a common root. However, LLMs that operate on integers representing tokens that make up these words cannot understand the relationships between tokens without additional work to make those connections.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>For this reason, LLMs follow a long history in machine learning and deep learning of performing a kind of cyclical conversion. First, tokens are converted into a numeric form that deep learning algorithms can work on. Then, the LLM converts this numeric representation back into a new token. This cycle repeats iteratively, which is not comparable to how humans work. You would be incredibly concerned if your colleagues had to pull out a calculator to perform several math problems between each word they spoke.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>Yet this process is, indeed, how LLMs produce outputs. In this chapter, we will walk through the process in two stages. First, we will review the entire process at a high level to introduce fundamental concepts and construct a mental model of how LLMs generate text. Next, this model will serve as a scaffolding for a more in-depth discussion of the details and design choices associated with the components that LLMs use to capture the relationships between words and language and, ultimately, generate the output we are familiar with.</p> 
  </div> 
  <div class="readable-text" id="p10"> 
   <h2 class=" readable-text-h2" id="the-transformer-model"><span class="num-string browsable-reference-id">3.1</span> The transformer model</h2> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>Many LLMs you encounter today interpret tokens and produce output using a software architecture known as a transformer. This architecture consists of a collection of algorithms and data structures that store information by representing it as numbers in a neural network. At their core, transformers are sequence prediction algorithms. While it is common to describe them as “reasoning” or “understanding” language, what they actually do is predict tokens. Transformers come with three different approaches to token prediction. While we focus on the famous GPT architecture (more formally known as decoder-only models), it is also worth introducing encoder-only and encoder-decoder models:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p12"><em>Encoder-only models</em>—These models are designed to create knowledge representations that can be used to perform tasks—that is, to encode the input into a numerical representation that is more useful to an algorithm. The best way to think of them is that they take text and process it into a form that is easier for a machine learning algorithm to use. They are widely used in scientific research. Famous examples include BERT and RoBERTa.</li> 
   <li class="readable-text" id="p13"><em>Decoder-only models</em>—These models are designed to generate text. The best way to think of them is that they take a partially written document and then produce a likely continuation of that document by predicting the next token. Famous examples include OpenAI’s GPT and Google’s Gemini.</li> 
   <li class="readable-text" id="p14"><em>Encoder-decoder models</em>—These models are also designed to generate text. Unlike decoder-only models, they take an entire passage of text and create a corresponding passage rather than continue the existing one. They are less popular than decoder-only models because they are more expensive to train, and their use is sometimes more challenging. For tasks with a clearly defined input and output sequence, encoder-decoder models tend to outperform decoder-only models. For example, they’re much better at translation and summarization tasks than decoder-only models. Famous examples include T5 and the algorithm that powers Google Translate.</li> 
  </ul> 
  <div class="readable-text" id="p15"> 
   <p>Regardless of which type of transformer is used, the essential components of the model are built from three basic layers, just arranged in different ways internally. A reasonable analogy to their interchangeability is that of gasoline car engines: they all work similarly and have the same general components. How those components (read: layers) are put together within the engine (read: transformer) elicits various tradeoffs in performance.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p16"> 
    <h5 class=" callout-container-h5 readable-text-h5">What exactly is a neural network layer?</h5> 
   </div> 
   <div class="readable-text" id="p17"> 
    <p> LLMs are one of many hundreds of algorithms that we now call <em>neural networks</em>. However, this is a misnomer in several ways. First, what constitutes a neural net-work approach today is very broad, to such a degree that referencing a “neural network–based approach” does not give the reader too much information about the exact approach described. Second, the <em>neural</em> part of the name has little or nothing to do with neuroscience or how the brain works. Sometimes, there is an intuitive “Hey, the brain kinda does something like this; can we mimic that behavior and get something useful out of it?” style of inspiration, but not for most current methods. Third, a neural network describes more of a standard agreement on assembling data structures rather than a particular algorithm. Think about build-ing a house: you use two-by-fours, sheetrock, and many options for cabinetry, paints, and design choices to assemble everything into a home. Each home looks unique but also familiar: they are all assembled in an expected way. The “layer” of a neural network is the smallest component, but you can use many types of layers in different ways. Transformers are one of many pieces that get assembled into a larger network.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p18"> 
   <h3 class=" readable-text-h3" id="sec__transformer_layers"><span class="num-string browsable-reference-id">3.1.1</span> Layers of the transformer model</h3> 
  </div> 
  <div class="readable-text" id="p19"> 
   <p>Figure <a href="#fig__transformer-basic">3.1</a> describes the essential components of the transformer model: the <em>embedding layer</em>, which generates representations of tokens that can hold more meaning; the <em>transformer layer</em>, which makes predictions based on word relationships; and the <em>output layer</em>, which transforms the numeric representations used within the transformer into words that humans can read.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p20">  
   <img alt="figure" src="../Images/CH03_F01_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__transformer-basic"><span class="num-string">Figure <span class="browsable-reference-id">3.1</span></span> The basic components of the transformer model, consisting of the embedding layer, multiple transformer layers, and the output layer</h5>
  </div> 
  <div class="readable-text" id="p21"> 
   <p>Let’s look at these layers in detail:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p22"><em>Embedding layer</em>—The embedding layer takes raw tokens as input and maps them into representations that capture each token’s meaning. For example, in chapter 2, we discussed how tokens represent concepts, but individual tokens don’t have any relationship with each other. Consider the words “dog” and “wolf.” With our understanding of language, we know these terms are related, but we need some way of capturing this relationship within a neural network. This is precisely what the embedding layer does. It captures information about each token that encodes its meaning and allows us to express its conceptual relationship with other tokens. Consequently, we can capture the idea that the representations of the tokens <code>dog</code> and <code>wolf</code> are more similar to each other than the representations for the tokens <code>red</code> and <code>France</code>. You can think of the embedding layer as the part of the model that processes the words on a page and maps them to abstract conceptual representations in your head.</li> 
   <li class="readable-text" id="p23"><em>Transformer layer</em>—Transformer layers are where most of the computationhappens in a language model: they capture the relationships between words created by the embedding layer and do the bulk of the actual work to obtain the output. While LLMs generally only have one embedding layer and one output layer, they have many transformer layers. More powerful models have more transformer layers.<br/>It is tempting to describe the transformer layer as the “thinking” part of the model. This definition erroneously implies that transformer layers (or the larger model built from them) can think, but thinking as humans do is self-reflecting and variable in duration and effort. You can think about something for a half-second or months, depending on the effort needed for the task. A transformer always repeats the same process with the same effort for every task. There is no introspection and no altering a transformer layer’s mental state. Thus, a better way to imagine a transformer layer is a set of <em>fuzzy rules</em>—<em>fuzzy</em> because they do not require exact matches (because embeddings might return something similar like “dog” to “wolf”) and <em>rules</em> because transformers have no flexibility. Once learning is complete, a transformer layer will do the same thing every time.</li> 
   <li class="readable-text" id="p24"><em>Output layer</em>—After the model has done the computation, additional transformations are performed in the output layer to obtain a useful result. Most commonly, the output layer operates as the inverse of the embedding layer, transforming the result of the computation from the embeddings space, which captures concepts, back into token space, which captures actual subwords to build text output. You can think of this as the part of the model that takes the answer you’ve decided on and then chooses the actual words to express that answer on a page by selecting the words most likely to represent the concepts that make up the answer. Finally, we end with an unembedding process, which converts the embeddings into tokens. Because each token has a one-to-one mapping to a subword, we can use a simple dictionary or map to convert the tokens into human-readable text again. This process is detailed in figure <a href="#fig__full_function">3.2</a>.</li> 
  </ul> 
  <div class="readable-text" id="p25"> 
   <h2 class=" readable-text-h2" id="exploring-the-transformer-architecture-in-detail"><span class="num-string browsable-reference-id">3.2</span> Exploring the transformer architecture in detail</h2> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>To further understand what is happening inside an LLM, it can be helpful to reframe what we described as a sequence of steps. So let us do that in figure <a href="#fig__full_function">3.2</a>, which describes seven steps. We’ll mark each of these with reference to the section where we covered it before or tell you when it is a new detail we are about to explain. This chapter provides a lot of information at once, so we will break it down piece by piece as we go.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p27">  
   <img alt="figure" src="../Images/CH03_F02_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__full_function"><span class="num-string">Figure <span class="browsable-reference-id">3.2</span></span> The process for converting input into output using a large language model</h5>
  </div> 
  <div class="readable-text" id="p28"> 
   <p>The seven steps to an LLM are as follows:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p29">Map text to tokens (chapter 2).</li> 
   <li class="readable-text" id="p30">Map tokens into embedding space (<em>new</em>, subsection <a href="#sec__chp3_token_convert"><i> Representing tokens with vectors</i></a>).</li> 
   <li class="readable-text" id="p31">Add information to each embedding that captures each token’s position in the input text (<em>new</em>, subsection <a href="#sec__chp3_pos_embed"><i> Adding positional information</i></a>).</li> 
   <li class="readable-text" id="p32">Pass the data through a transformer layer (repeat <span><img alt="equation image" src="../Images/eq-chapter-3-32-1.png"/></span> times) (<em>new</em>, subsection <a href="#sec__chp3_trans_layer">3.2.2</a>).</li> 
   <li class="readable-text" id="p33">Apply the unembedding layer to get tokens that could make good responses (<em>new</em>, subsection <a href="#sec__chp3_output">3.2.3</a>).</li> 
   <li class="readable-text" id="p34">Sample from the list of possible tokens to generate a single response (<em>new</em>, subsection <a href="#sec__chp3_sampling"><i> Sampling tokens to produce output</i></a>).</li> 
   <li class="readable-text" id="p35">Decode tokens from the response into actual text (chapter 2).</li> 
  </ol> 
  <div class="readable-text" id="p36"> 
   <h3 class=" readable-text-h3" id="sec__chp3_embeddings"><span class="num-string browsable-reference-id">3.2.1</span> Embedding layers</h3> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>There are a lot of nuances to tokenization, embeddings, and how precisely language gets translated into things that models can understand. The most important nuance is that neural networks still don’t work with tokens directly. On the whole, neural networks need numbers that can be manipulated, and a token has a fixed numeric identity. We cannot change the identity of a token because the identity allows us to convert tokens back to human-readable text. We need a layer that will transform tokens in numeric form into the words or subwords they represent.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h4 class=" readable-text-h4" id="sec__chp3_token_convert"> Representing tokens with vectors</h4> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Our transformer needs numbers to work on. By this, we mean <em>continuous</em> numbers, so any fractional value is available for us to use: 0.3, -5, 3.14, etc. We also need more than one number to represent every token to capture nuances of meaning and relationships between tokens. If you tried to use just one number to represent each word, you would encounter difficulties capturing a word’s multiple meanings, synonyms, antonyms, and the relationships that those create. For example, you may well want to say that the antonym (opposite) of a word should be achievable by multiplying a word by <span><img alt="equation image" src="../Images/eq-chapter-3-39-1.png"/></span>. As figure <a href="#fig__one_number_problem">3.3</a> shows, this quickly leads to silly conclusions about word relationships.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p40">  
   <img alt="figure" src="../Images/CH03_F03_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__one_number_problem"><span class="num-string">Figure <span class="browsable-reference-id">3.3</span></span> If you use just one number to represent a token, you quickly encounter problems where similar/dissimilar words cannot be made to fit each other. Here we see how trying to represent simple synonym/antonym relationships quickly becomes nonsensical even with just a handful of words.</h5>
  </div> 
  <div class="readable-text" id="p41"> 
   <p>For example, say we have a token for <code>stock</code> that we have arbitrarily decided will be converted to some number (e.g., <code>5.2</code>). I want to give related financial words, such as <code>capital</code>, a similar number (e.g., <code>5.3</code>) because they have similar meanings. There are also antonyms of <code>stock</code>’s other meanings, such as <code>rare</code>. Let’s say we use a negative value to capture the idea of an antonym and give it a value of <code>-5.2</code>. But now things get complex because another antonym of <code>capital</code> is <code>debt</code>. But if antonyms are negations, <code>debt</code> and <code>rare</code> have a similar meaning, which is nonsensical. Figure <a href="#fig__one_number_problem">3.3</a> illustrates the problem: when we use a single number to represent a word, we cannot encode their relationships without implying weird relationships with other words, and we have not even gotten past four words yet!</p> 
  </div> 
  <div class="browsable-container figure-container" id="p42">  
   <img alt="figure" src="../Images/CH03_F04_Boozallen.jpg"/> 
   <h5 class=" figure-container-h5" id="fig__vectors"><span class="num-string">Figure <span class="browsable-reference-id">3.4</span></span> Adding another dimension to our token representation allows us to represent a more diverse arrangement of semantic relationships. Here we see how two dimensions can capture relationships for multiple meanings of the same word.</h5>
  </div> 
  <div class="readable-text" id="p43"> 
   <p>The trick is to use multiple numbers to represent each token, allowing you to find better representations that accommodate the different relationships between words. An example that uses two numbers is shown in figure <a href="#fig__vectors">3.4</a>. We can see things like <code>bland</code> being nearly equidistant from <code>rare</code> and <code>well-done</code>, while also having space for <code>bank</code> to be far away from all three just mentioned words and instead be near <code>stock</code>. We were even able to throw in a few extra words. The more numbers you use, called <em>dimensions</em> in the field’s jargon, the more complex relationships you can represent.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p44"> 
    <h5 class=" callout-container-h5 readable-text-h5">The curse of dimensionality</h5> 
   </div> 
   <div class="readable-text" id="p45"> 
    <p> If more dimensions are better at capturing subtle meaning, why not use as many dimensions as possible to represent our data? When dealing with a large number of dimensions, several problems arise. One primary concern is that LLMs deal with many embeddings, and adding more dimensions increases the memory and computation required to store and process embeddings. Furthermore, as we add more dimensions, the size of the semantic space explodes, and the amount of data and time needed to train a machine learning model to learn about all locations in the semantic space similarly grows exponentially. Mathematician Richard E. Bellman coined the term the “curse of dimensionality“ to describe this phenomenon because while we want to create a space capable of capturing nuanced meaning, we are limited by the fundamental properties of the space we create.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>In LLM parlance, the lists of numbers used to represent tokens are referred to as <em>embeddings</em>. You can think of an embedding as an array or list of floating-point values. As a shorthand, we call such arrays <em>vectors</em>. Each position in the vector is called a dimension. As we show in figure <a href="#fig__vectors">3.4</a>, using multiple dimensions allows us to capture subtleties in relationships between words in human language.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Since embeddings exist in multiple dimensions, we often state that they live in a <em>semantic space</em>. In some machine learning applications, this is called a <em>latent space</em>, especially when not dealing with text. Semantic space is wishy-washy jargon that isn’t well defined in the field, but it is most commonly used as a shorthand for saying that the vector embeddings that represent each token are well behaved in that synonyms/antonyms have nearer/farther distances and that we can use those relationships productively. As an example, in figure <a href="#fig__kingQueenWoman">3.5</a>, we show a famous case where a “make female” transformation can be built by subtracting the embedding for <code>male</code> and adding the embedding for <code>female</code>. This transformation can be applied to many different male-gendered words to find female-gendered words of the same concept. The co-location of all the “royal” words in the bottom right of figure <a href="#fig__kingQueenWoman">3.5</a> is also intentional, as many different kinds of relationships can be simultaneously maintained in a high-dimensional space.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p48">  
   <img alt="figure" src="../Images/CH03_F05_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__kingQueenWoman"><span class="num-string">Figure <span class="browsable-reference-id">3.5</span></span> A demonstration of how the relationships between embeddings create a semantic space. Words with similar meanings are near each other, and the same transformation can be applied to multi-ple words to yield a similar result—in this instance, a transformation to find the feminine version of a masculine word.</h5>
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Shockingly, we cannot guarantee that these semantic relationships will form during the training process. It just so happens that they often do, and they were discovered to be very useful. By extension, the relationships in a semantic space are not foolproof, and biases in your data can seep in. For example, models will often determine that <code>doctor</code> is more similar to <code>male</code> and <code>nurse</code> is more similar to <code>female</code> because, in the generally available text used to build most models, it is more common for doctors to be described as male and nurses as female. The relationships are thus not a discovered truth of the world but a reflection of the data that went into the process.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h4 class=" readable-text-h4" id="sec__chp3_pos_embed"> Adding positional information</h4> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>One critical problem is that a standard transformer does not understand sequential information. If you gave the transformer one sentence and rearranged all the tokens, it would view all possible permutations of the tokens as identical! That problem is illustrated in figure <a href="#fig__jumbled_order">3.6</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p52">  
   <img alt="figure" src="../Images/CH03_F06_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__jumbled_order"><span class="num-string">Figure <span class="browsable-reference-id">3.6</span></span> Without positional information, transformers do not understand that their inputs have a specific order, and all possible reorganizations of the tokens look identical to the algorithm. This is problematic because word order can change the word’s context or, if done randomly, become gibberish.</h5>
  </div> 
  <div class="readable-text" id="p53"> 
   <p>For this reason, the embedding layer generates two different kinds of embeddings. First, it creates a <em>word embedding</em> that captures the meaning of the token, and second, it makes a <em>positional embedding</em> that captures the token’s location in a sequence.</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>The idea is surprisingly simple. Just as we mapped every unique token to a unique meaning vector, we will also map every unique token position (first, second, third, and so on) to a position vector. So each token will get embedded twice—once for its identity and again for its position. These two vectors are then added to create one vector representing the word and its location in the sentence. This process is outlined in figure <a href="#fig__embd_token_and_pos">3.7</a>.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p55"> 
   <p> <span class="print-book-callout-head">Note</span> Using multiple dimensions instead of absolute positions makes iteasier for the transformer to learn relative positioning, even if it is exces-sively redundant for us as humans [1]. </p> 
  </div> 
  <div class="browsable-container figure-container" id="p56">  
   <img alt="figure" src="../Images/CH03_F07_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__embd_token_and_pos"><span class="num-string">Figure <span class="browsable-reference-id">3.7</span></span> Word embeddings do not capture the fact that input tokens appear in a specific order. This information is captured by a positional embedding. The position embeddings work the same way as word embeddings and are added together. The resulting combined embeddings have the information the model needs to understand the order of tokens.</h5>
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Those are all the missing details required to understand how tokens are converted into vectors for the transformer layers. This strategy may seem somewhat naive, and that is honestly true. People have tried developing more sophisticated methods to handle this information, but this simple approach of “Let’s make everything a vector and just add them together” works surprisingly well. Importantly, it has also demonstrated success in video and images. Having a straightforward strategy that functions well enough for many different problems is valuable, which is why this naive approach has taken hold.</p> 
  </div> 
  <div class="readable-text" id="p58"> 
   <h3 class=" readable-text-h3" id="sec__chp3_trans_layer"><span class="num-string browsable-reference-id">3.2.2</span> Transformer layers</h3> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>The transformer layer aims to transform the input into a more useful output. Most prior neural network layers, such as an embedding layer, are designed to incorporate very specific beliefs about how the world works into their operation. The idea is that if the encoded belief is accurate to how the world does indeed work, your model will reach a better solution using less data. Transformers go for the opposite strategy. They encode a general-purpose mechanism that can learn many tasks if you get enough data.</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>To do this, transformers operate with three primary components:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p61"><em>Query</em>—Queries are vectors (from an embedding layer) that represent what you are looking for.</li> 
   <li class="readable-text" id="p62"><em>Key</em>—Key vectors represent the possible answers to pair a query against.</li> 
   <li class="readable-text" id="p63"><em>Value</em>—Every key has a corresponding value vector, the actual value to bereturned when a query and key match.</li> 
  </ul> 
  <div class="readable-text" id="p64"> 
   <p>This terminology corresponds to the behavior of a <code>dict</code> or dictionary object in Python. You look up an item in the dictionary by its key so that you can then create some useful output. The difference is that a transformer is fuzzy. It’s not that we are looking up a single key, but we are evaluating <em>all keys</em>, weighted by their degree of similarity to the query. Figure <a href="#fig__qkv">3.8</a> shows how this works with a simple example. While the queries and keys are shown as strings, those strings are stand-ins for the vectors that each string will be mapped to via the embedding layer.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p65">  
   <img alt="figure" src="../Images/CH03_F08_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__qkv"><span class="num-string">Figure <span class="browsable-reference-id">3.8</span></span> An example of how queries, keys, and values work inside a transformer compared to a Python dictionary. When a Python dictionary matches queries to keys, it needs an exact match to find the value, or it will return nothing. A transformer always returns something based on the most similar matches between queries and keys. </h5>
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Having every key contribute to one query could be chaotic, especially if there is one true match between a query and a specific key. This problem is handled by a detail called <em>attention</em> or the <em>attention mechanism</em>.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Attention inside a transformer can be considered similar to your ability to pay attention to what is important. You can tune out irrelevant and distracting information (i.e., bad keys) and focus primarily on what is important (the best matching keys). The analogy extends further in that attention is adaptive; what is important is a function of what other options are available. Your boss giving you directions for the week takes up your attention, but the fire alarm going off changes your attention away from your boss to the alarm (and a potential fire).</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>When generating the next token, a transformer takes the <em>query</em> for the current token and compares it to the <em>key</em> for all previous tokens. Comparing the query and the key generates a series of values that the attention mechanism uses to calculate how much weight it should assign each potential following token when deciding which token to generate next. The <em>value</em> for each token tells the model what each previous token thinks its contribution to the probability should be. The attention function then computes the next token, as shown in figure <a href="#fig__QKV_nextWord">3.9</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p69">  
   <img alt="figure" src="../Images/CH03_F09_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__QKV_nextWord"><span class="num-string">Figure <span class="browsable-reference-id">3.9</span></span> The next token in a sentence is predicted by using the current token as the query and calculating matches with the preceding words as the keys. The individual values themselves do not need to exist in the semantic space; the output of the attention mechanism produces something similar to one of the tokens in the vocabulary.</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p70"> 
    <h5 class=" callout-container-h5 readable-text-h5">What is the math of attention?</h5> 
   </div> 
   <div class="readable-text" id="p71"> 
    <p> We will not go into every detail of the math behind attention because it would take a lot of space to describe it, and it has been covered elsewhere. We did so in a previous book: chapter 11 of <em>Inside Deep Learning</em> [2] explains transformers and attention in much greater technical detail.</p> 
   </div> 
   <div class="readable-text" id="p72"> 
    <p>For the curious, the primary equation is</p> 
   </div> 
   <div class="browsable-container equation-container" id="p73"> 
    <h5 class=" browsable-container-h5">(3.1)</h5> 
    <p><span><img alt="equation image" src="../Images/eq-chapter-3-73-1.png"/></span></p> 
   </div> 
   <div class="browsable-container equation-container" id="p74"> 
    <h5 class=" browsable-container-h5">(3.2)</h5> 
    <p><span><img alt="equation image" src="../Images/eq-chapter-3-74-1.png"/></span></p> 
   </div> 
   <div class="readable-text" id="p75"> 
    <p>The queries, keys, and values are represented by individual matrices <span><img alt="equation image" src="../Images/eq-chapter-3-75-1.png"/></span>, <span><img alt="equation image" src="../Images/eq-chapter-3-75-2.png"/></span>, and <span><img alt="equation image" src="../Images/eq-chapter-3-75-3.png"/></span>, respectively. Matrix multiplication makes attention efficient when implemented on GPUs because they can perform many multiplication operations in parallel. The softmax function implements the main component of the attention analogy byassigning many values nearly equal to zero, which causes the transformer toignore the unimportant items.</p> 
   </div> 
   <div class="readable-text" id="p76"> 
    <p>The final step of <em>norm</em> and <em>Feedforward</em> is the application of <em>layer normalization</em> and a <em>linear layer</em> via a <em>skip connection</em>. If these terms aren’t familiar to you, that is fine; you do not need to know this math to understand the rest of the book. If you want to learn what these terms mean, we refer you to <em>Inside Deep Learning</em> [2] for a technically detailed understanding.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>A transformer model is made up of dozens of transformer layers. The intermediate transformer layers perform the same mechanical task described in figure <a href="#fig__QKV_nextWord">3.9</a> despite not having to predict a token because the last transformer layer is the only one that needs to predict an actual token. The transformer layer is general enough that combining many intermediate layers allows the model to learn complex tasks such as sorting, stacking, and other sophisticated input transformations.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3" id="sec__chp3_output"><span class="num-string browsable-reference-id">3.2.3</span> Unembedding layers</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>The last stage of an LLM is the unembedding layer, which transforms the numeric vector representation that transformers use into a specific output token so that we can ultimately return the text that corresponds to that token. This output generation process is also called <em>decoding</em> because we decode the transformer vector representation to a piece of output text. It is a crucial component for using an LLM to generate text. Not only is decoding the current token essential for producing output, but the next token will depend on each previous token selected for output. This process is shown in figure <a href="#fig__transformer_decode_loop">3.10</a>, where we recursively generate tokens one at a time. In statistical parlance, this is known as an <em>autoregressive</em> process, meaning each element of the output is based on the output that came before it.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p80">  
   <img alt="figure" src="../Images/CH03_F10_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__transformer_decode_loop"><span class="num-string">Figure <span class="browsable-reference-id">3.10</span></span> Producing output from LLMs involves converting from documents to tokens and then using the model to produce output. We loop through this process to both consume text and generate human-readable output.</h5>
  </div> 
  <div class="readable-text" id="p81"> 
   <p>You may be wondering how this process stops. When we build the vocabulary of tokens, we include some special tokens that do not occur in the text. One of these special tokens is an <em>end of sequence</em> (EoS) token. The model trains on texts with natural endpoints that are finished with the EoS marker, and when the model generates a new token, the EoS token is one of the options it can generate. If the EoS is generated, we know it is time to stop the loop and return the full text to the user. It is also a good idea to keep a maximum generation limit if your model gets into a bad state and fails to generate the EoS token.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h4 class=" readable-text-h4" id="sec__chp3_sampling"> Sampling tokens to produce output</h4> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>What is missing from this process is how we convert a vector, an array of floating-point numbers produced by the transformer layers, into a single token. This process is called <em>sampling</em> because it uses a statistical method to choose sample tokens from the vocabulary based on the LLM’s input and its output so far. The LLM’s sampling algorithm evaluates those samples to select which token to produce. There are several techniques for doing this sampling, but all follow the same basic two-step strategy:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p84">For each token in the vocabulary, compute the probability that each token will be the next selected token.</li> 
   <li class="readable-text" id="p85">Randomly pick a token according to the probabilities calculated.</li> 
  </ol> 
  <div class="readable-text" id="p86"> 
   <p>If you have used ChatGPT or other LLMs, you may have noticed that they do not always provide the same output for the same input. The decoding step is why you may get different answers whenever you ask the same question.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>It may seem counterintuitive that tokens are selected randomly. However, it is a critical component to generating good-quality text. Consider the example of text generation in figure <a href="#fig__token_random_next">3.11</a>, where we are trying to finish the sentence “I love to eat.” It would be unrealistic if the model always picked “sushi” as the next token because it had the highest probability. If someone always said “sushi” to you in this context, you would think something was off. We need randomness to handle the fact that there are multiple valid choices, and not all options are likely to occur.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p88">  
   <img alt="figure" src="../Images/CH03_F11_Boozallen.png"/> 
   <h5 class=" figure-container-h5" id="fig__token_random_next"><span class="num-string">Figure <span class="browsable-reference-id">3.11</span></span> We demonstrate text generation by starting with the phrase "I love to eat" and then showing that some possible completions that are foods, such as barbeque and sushi, have high proba-bilities, while a car and the number 42 have low probabilities. Weighted random selection chooses the word <em>tacos</em>. The generation loop is stopped when the EoS token appears.</h5>
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Also note in the example from figure <a href="#fig__token_random_next">3.11</a> that other tokens would be nonsensical, like 42, given tiny probabilities. Again, we need to assign every token a probability to know which tokens are likely or unlikely.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p90"> 
    <h5 class=" callout-container-h5 readable-text-h5">How do you get probabilities for tokens?</h5> 
   </div> 
   <div class="readable-text" id="p91"> 
    <p> Each possible next token has a different probability of being selected. Most of the tokens have nearly zero chance of being selected. A keen reader may wonder: How can we assign a probability to a token before knowing the other tokens? We do so by giving every token a score, indicating how good a match that token’s embedding is compared to the current vector (i.e., the output from the transformer). The score is arbitrary from <span><img alt="equation image" src="../Images/eq-chapter-3-91-1.png"/></span> to <span><img alt="equation image" src="../Images/eq-chapter-3-91-2.png"/></span> and calculated independently for each token. The relative difference in scores is then used to create probabilities. For example, if one token had a score of 65.2 and a second token had a score of -5.0, the probabilities would be near 100% and 0% for the individual token, respectively. If the scores were 65.2 and 65.1, the probabilities would be near 50.5% and 49.5%, respectively. Similarly, scores of 0.2 and 0.1 would give the same probabilities as the scores 65.2 and 65.1 because we are looking at relative differences in scores to assign probabilities, not the individual scores themselves.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>A transformer sometimes gives you unusual or nonsensical generations. It’s not common, but the other tokens have a <em>near-zero</em> probability, and eventually, one weird token will get picked that you would not expect. Once an unexpected token has been chosen, all future generated tokens will be produced in a manner that tries to make sense of the unusual generation.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>For example, if the LLM produced “I love to eat <em>chalk</em>,” you would be pretty surprised. But it is not overly unreasonable because chalk-eating is a symptom of the medical condition called pica. Once the word <em>chalk</em> is selected, the LLM may go into a tangent about pica or some other medical diatribe—that is, of course, if you are so lucky that your unusual generation is in the sphere of “rare but reasonable” and not an utterly errant prediction.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p94"> 
   <p> <span class="print-book-callout-head">Note</span> Many algorithms can compute the final probabilities used to select words for generation. One of these is nucleus sampling, also known as Top-p sampling, which involves determining the tokens with the highest probability as potential outputs and choosing tokens to output from that list. This method can help us avoid unreasonable predictions. If you can, you want to check which sampling algorithm your LLM uses so that you can understand its risks of producing rarer to unreasonable outputs. </p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <h2 class=" readable-text-h2" id="the-tradeoff-between-creativity-and-topical-responses"><span class="num-string browsable-reference-id">3.3</span> The tradeoff between creativity and topical responses</h2> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Depending on how your users plan to interact with an LLM, generating surprising or creative outputs may be desired. Say you are using an LLM to help brainstorm new product ideas, and you are using a chatbot as a digital sounding board to spark ideas. In this case, you probably want unusual outputs generated because the goal is to be creative and think of something new.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Conversely, sometimes creativity is wholly undesired. One potential use for LLMs is offline search, where you could fit an LLM on a (relatively powerful) mobile phone and ask/look up information even when you do not have internet connectivity. In this case, you want the outputs of the LLM to be reliable, on topic, and factual. A creative reinterpretation is not needed.</p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>A feature in LLMs called <em>temperature</em> balances this tradeoff. The temperature variable (which is a number between 0 and 1 and often has a default value of 0.7 or 0.8) is used to exaggerate the probability of low-likelihood tokens (high temperature) or depress the probability of low-likelihood tokens (low temperature).</p> 
  </div> 
  <div class="readable-text" id="p99"> 
   <p>Consider molecules in a glass of water as an analogy. Say we want to know what molecule will be at the top of the glass (don’t ask us why; just go with it). If the glass was lowered to a temperature of absolute zero, all the molecules would be still, and the molecule at the top of the glass would reliably be the same each time (i.e., you will always generate the same token). If you raise the temperature of the glass so much that it starts to boil, the molecules will bounce around, making the molecule at the top of the glass essentially random (i.e., you get a completely random token). As you scale the temperature up and down, you change the balance between picking with greater randomness (and, thus, often creativity) or focusing on just the most likely next token (thus keeping the generation more topical).</p> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>In a practical sense, considering our example of “I like to eat,” a higher temperature would lead to the generation of different types of foods, not just pizza or sushi but possibly less typical or more specific foods like beef wellington or vegetarian chili.</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <h2 class=" readable-text-h2" id="transformers-in-context"><span class="num-string browsable-reference-id">3.4</span> Transformers in context</h2> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>We’ve covered a lot of ground in this chapter. Embedding layers, transformer layers, and unembedding layers are the core building blocks that make LLMs work. The concepts of how LLMs encode meaning and position and then use stacks of transformer layers to uncover the structure in text are all vital to understanding how LLMs capture information and produce the quality of output they are capable of. But we have more details to cover! How do we create these layers to generate embeddings and probabilities by analyzing piles and piles of data in the first place? In chapter 4, we will continue exploring how to feed data into this architecture and incentivize the LLM to “learn” meaningful relationships in text through the training process.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p104">While LLMs use tokens as their basic unit of semantic meaning, they’remathematically represented within the model as <em>embedding vectors</em> rather than as strings. These embedding vectors can capture relationships about nearness, dissimilarity, antonyms, and other linguistic-descriptive properties.</li> 
   <li class="readable-text" id="p105">Position and word order do not come naturally to transformers and are obtained via another vector representing the relative position. The model can represent word order by adding the position and word embedding vectors.</li> 
   <li class="readable-text" id="p106">Transformer layers act as a kind of fuzzy dictionary, returning approximate answers to approximate matches. This fuzzy process is called attention and uses the terms <em>query</em>, <em>key</em>, and <em>value</em> as analogous to the key and value in a Python dictionary.</li> 
   <li class="readable-text" id="p107">ChatGPT is an example of a decoder-only transformer, but encoder-only transformers and encoder-decoder transformers also exist. Decoder-only transformers are best at generating text, but other types of transformers can be better at other tasks.</li> 
   <li class="readable-text" id="p108">LLMs are autoregressive, meaning they work recursively. All previously generated tokens are fed into the model at each step to get the next token. Simply put, autoregressive models predict the next thing using the previous things.</li> 
   <li class="readable-text" id="p109">The output of any transformer isn’t tokens; instead, the output is a probability for how likely every token is. Selecting a specific token is called <em>unembedding</em> or <em>sampling</em> and includes some randomness.</li> 
   <li class="readable-text" id="p110">The strength of randomness can be controlled, resulting in more or less realistic output, more creative or unique output, or more consistent output. Most LLMs have a default threshold for randomness that is reasonable looking, but you may want to change it for different uses.</li> 
  </ul>
 </body></html>