- en: Chapter 3\. Vocabulary and Tokenization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 词汇和标记化
- en: 'In [Chapter 2](ch02.html#ch02), we dug deep into the datasets that are used
    to train the language models of today, including the process of creating them.
    Hopefully this foray has underscored how influential pre-training data is to the
    resulting model. In this chapter, we will discuss another fundamental ingredient
    of a language model: its vocabulary.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#ch02)中，我们深入探讨了用于训练今天语言模型的语料库，包括创建它们的过程。希望这次探索强调了预训练数据对最终模型的影响。在本章中，我们将讨论语言模型的另一个基本组成部分：其词汇。
- en: Vocabulary
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇
- en: 'What do you do first when you start learning a new language? You start acquiring
    its vocabulary, expanding it as you gain more proficiency in the language. Let’s
    define vocabulary here as:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始学习一门新语言时，你首先做什么？你开始获取它的词汇，随着你对语言掌握程度的提高，不断扩大它。让我们在这里定义词汇为：
- en: All the words in a language that are understood by a specific person.
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个语言中所有被特定人理解的所有单词。
- en: The average native English speaker has a vocabulary of [20,000–35,000 words](https://oreil.ly/bkc2C).
    Similarly, every language model has its own vocabulary, with most vocabulary sizes
    ranging anywhere between 5,000 and 500,000 *tokens*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，母语为英语的人的词汇量在[20,000–35,000个单词](https://oreil.ly/bkc2C)之间。同样，每个语言模型都有自己的词汇表，其中大多数词汇表的大小在5,000到500,000个*标记*之间。
- en: As an example, let us explore the vocabulary of the GPT-NeoX-20B model. Open
    the file [*tokenizer.json*](https://oreil.ly/Kages) and Ctrl+F for “vocab,” a
    dictionary containing the vocabulary of the model. You can see that the words
    comprising the language model vocabulary don’t entirely look like English language
    words that appear in a dictionary. These word-like units are called “types,” and
    the instantiation of a type (when it appears in a sequence of text) is called
    a token.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以GPT-NeoX-20B模型为例，让我们来探索其词汇表。打开文件[*tokenizer.json*](https://oreil.ly/Kages)，使用Ctrl+F搜索“vocab”，这是一个包含模型词汇表的字典。你可以看到构成语言模型词汇的单词并不完全像字典中出现的英语单词。这些类似单词的单位被称为“类型”，而类型的实例化（当它在文本序列中出现时）被称为标记。
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Recently, and especially in industry, I seldom hear anyone use the term “type”
    except in older NLP textbooks. The term “token” is broadly used to refer to both
    the vocabulary units and when they appear in a text sequence. We will henceforth
    use the word “token” to describe both concepts, even though I personally am not
    the biggest fan of this usage.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，尤其是在工业界，我很少听到有人除了在较老的NLP教科书中使用“类型”这个词之外，其他情况下都使用“标记”这个词来广泛地指代词汇单元以及它们在文本序列中出现的情况。因此，我们将继续使用“标记”这个词来描述这两个概念，尽管我个人并不是特别喜欢这种用法。
- en: In the vocabulary file, we see that next to each token is a number, which is
    called the *input ID* or the *token index*. The vocabulary size of GPT-NeoX is
    just above 50,000.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在词汇表中，我们可以看到每个标记旁边都有一个数字，这被称为*输入ID*或*标记索引*。GPT-NeoX的词汇量正好超过50,000。
- en: Looking at the vocabulary file in detail, we notice that the first few hundred
    tokens are all single-character tokens, such as special characters, digits, capital
    letters, small letters, and accented characters. Longer words appear later on
    in the vocabulary. A lot of tokens correspond to just a part of a word, called
    a *subword*, like “impl,” “inated,” and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看词汇表文件，我们会发现前几百个标记都是单字符标记，例如特殊字符、数字、大写字母、小写字母和带重音的字符。较长的单词出现在词汇表的后面。许多标记对应于单词的一部分，称为*子词*，例如“impl”、“inated”等等。
- en: 'Let’s Ctrl+F for “office.” We get nine results:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Ctrl+F搜索“office”。我们得到了九个结果：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Ġ character refers to a space before the word. For instance, in the sentence,
    “He stopped going to the office,” the space before the letter “o” in the word
    “office” is considered part of the token. You can see that the tokens are case-sensitive:
    there are separate tokens for “office” and “Office.” Most models these days have
    case-sensitive vocabularies. Back in the day, the BERT model was released with
    both a cased and an uncased version.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Ġ字符表示单词前的空格。例如，在句子“他停止去办公室”中，单词“office”中字母“o”前的空格被认为是标记的一部分。你可以看到标记是区分大小写的：有“office”和“Office”两个不同的标记。如今，大多数模型都有区分大小写的词汇表。BERT模型最初发布时，既有带大小写的版本，也有不带大小写的版本。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Language models learn vector representations called embeddings for each of these
    tokens that reflect their syntactic and semantic meaning. We will go through the
    learning process in [Chapter 4](ch04.html#chapter_transformer-architecture), and
    dive deeper into embeddings in [Chapter 11](ch11.html#chapter_llm_interfaces).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型为每个这些标记学习向量表示，称为嵌入，它们反映了它们的句法和语义意义。我们将在[第4章](ch04.html#chapter_transformer-architecture)中介绍学习过程，并在[第11章](ch11.html#chapter_llm_interfaces)中更深入地探讨嵌入。
- en: Cased vocabularies are almost always better, especially when you are training
    on such a huge body of text such that most tokens are seen by the model enough
    times to learn distinctive representations for them. For instance, there is a
    definite semantic difference between “web” and “Web,” and it is good to have separate
    tokens for them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有大小写的词汇表几乎总是更好的，尤其是当你在一个如此庞大的文本体上进行训练，以至于大多数标记被模型足够多次地看到，以便学习它们的独特表示时。例如，“web”和“Web”之间有明显的语义差异，为它们分别保留单独的标记是很好的。
- en: 'Let’s search for some numbers. Ctrl+F for “93.” There are only three results:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们搜索一些数字。使用Ctrl+F搜索“93。”只有三个结果：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It seems like not all numbers get their own tokens! Where is the token for 934?
    It is impractical to give every number its own token, especially if you want to
    limit your vocabulary size to say, just 50,000\. Later in this chapter, we will
    discuss how vocabulary sizes are determined. Popular names and places get their
    own token. There is a token representing Boston, Toronto, and Amsterdam, but none
    representing Mesa or Chennai. There is a token representing Ahmed and Donald,
    but none for Suhas or Maryam.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来并不是所有的数字都有自己的标记！934的标记在哪里？给每个数字分配一个标记是不切实际的，尤其是如果你想将词汇表的大小限制在，比如说，50,000个单词以内。在本章的后面部分，我们将讨论词汇表大小是如何确定的。流行的人名和地名有自己的标记。有一个标记代表波士顿、多伦多和阿姆斯特丹，但没有代表梅萨或钦奈的标记。有一个标记代表艾哈迈德和唐纳德，但没有代表苏哈斯或玛丽亚姆的标记。
- en: 'You might have noticed that tokens like:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到像这样的标记：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: exist, indicating that GPT-NeoX is also primed to process programming languages.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 存在，这表明GPT-NeoX也准备好处理编程语言。
- en: How are vocabularies determined? Surely, there was no executive committee holding
    emergency meetings burning midnight oil, with members making impassioned pleas
    to include the number 937 in the vocabulary at the expense of 934.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇是如何确定的？当然，肯定没有执行委员会在紧急会议上熬夜，成员们为了在词汇表中包含数字937而向934做出激动的请求。
- en: 'Let us revisit the definition of a vocabulary:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视词汇的定义：
- en: All the words in a language that are understood by a specific person.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个特定的人能理解的语言中的所有单词。
- en: Since we want our language model to be an expert at English, we can just include
    all words in the English dictionary as part of its vocabulary. Problem solved?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望我们的语言模型成为英语的专家，我们可以将其词汇表包括所有英语词典中的单词。问题解决了吗？
- en: Not nearly. What do you do when you communicate with the language model using
    a word that it has never seen? This happens a lot more often than you think. New
    words get invented all the time, words have multiple forms (“understand,” “understanding,”
    “understandable”), multiple words can be combined into a single word, and so on.
    Moreover, there are millions of domain-specific words (biomedical, chemistry,
    and so on).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 远远不够。当你用模型从未见过的词与语言模型进行交流时，你会怎么做？这种情况比你想象的要频繁得多。新词不断被创造出来，有些词有多种形式（“理解”，“理解力”，“可理解的”），多个词可以组合成一个词，等等。此外，还有数百万个特定领域的词汇（生物医学、化学等）。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The account [@NYT_first_said](https://oreil.ly/FzfI9) on the social media platform
    X posts words except proper nouns when they appear in the *New York Times* for
    the first time. Each day, an average of five new words appear in the US paper
    of record for the first time ever. On the day I wrote this section, the words
    were “unflippant,” “dumbeyed,” “dewdrenched,” “faceflat,” “saporous,” and “dronescape.”
    Many of these words might never get added to a dictionary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体平台X上的账户[@NYT_first_said](https://oreil.ly/FzfI9)在《纽约时报》首次出现时发布单词，除了专有名词。每天，平均有五个新词首次出现在美国的记录报纸上。在我写这一部分的那天，这些词是“unflippant”、“dumbeyed”、“dewdrenched”、“faceflat”、“saporous”和“dronescape”。其中许多词可能永远不会被添加到词典中。
- en: 'A token that doesn’t exist in the vocabulary is called an out-of-vocabulary
    (OOV) token. Traditionally, OOV tokens were represented using a special <UNK>
    token. The <UNK> token is a placeholder for all tokens that don’t exist in the
    vocabulary. All OOV tokens share the same embedding (and encode the same meaning),
    which is undesirable. Moreover, the <UNK> token cannot be used in generative models.
    You don’t want your model to output something like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表中不存在的标记被称为OOV标记。传统上，OOV标记使用特殊的<UNK>标记表示。<UNK>标记是所有不在词汇表中标记的占位符。所有OOV标记共享相同的嵌入（并编码相同的意义），这是不理想的。此外，<UNK>标记不能用于生成模型。你不想你的模型输出类似以下内容：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To solve the OOV problem, one possible solution could be to represent tokens
    in terms of characters instead of words. Each character has its own embedding,
    and as long as all valid characters are included in the vocabulary, there will
    never be a chance of encountering an OOV token. However, there are many downsides
    to this. The number of tokens needed to represent the average sentence becomes
    much larger. For example, the sentence, “The number of tokens needed to represent
    the average sentence becomes much larger,” contains 13 tokens when you treat each
    word as a token, but 81 tokens when you treat each character as a token. This
    reduces the amount of content you can represent within a fixed sequence length,
    which makes both model training and inference slower, as we will show further
    in [Chapter 4](ch04.html#chapter_transformer-architecture). Models support a limited
    sequence length, so this also reduces the amount of content you can fit in a single
    prompt. Later in this chapter, we will discuss models like CANINE, ByT5, and Charformer
    that attempt to use character-based tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决OOV（Out-of-Vocabulary）问题，一个可能的解决方案是将标记表示为字符而不是单词。每个字符都有自己的嵌入表示，只要所有有效的字符都包含在词汇表中，就永远不会遇到OOV标记。然而，这种方法有很多缺点。表示平均句子所需的标记数量变得非常大。例如，句子“表示平均句子所需的标记数量变得非常大”当每个单词作为一个标记时包含13个标记，但当每个字符作为一个标记时包含81个标记。这减少了在固定序列长度内可以表示的内容量，这会使模型训练和推理速度变慢，我们将在第4章中进一步展示。模型支持有限的序列长度，这也减少了可以在单个提示中放入的内容量。在本章的后面部分，我们将讨论像CANINE、ByT5和Charformer这样的模型，它们试图使用基于字符的标记。
- en: So, the middle ground and the best of both worlds (or the worst of both worlds—the
    field hasn’t come to a consensus yet) is using subwords. Subwords are the predominant
    mode of representing vocabulary units in the language model space today. The GPT-NeoX
    vocabulary we explored earlier uses subword tokens. [Figure 3-1](#subword-tokens)
    shows the OpenAI tokenizer playground that demonstrates how words are split into
    their constituent subwords by OpenAI models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，折中方案和两者之优（或者两者之劣——该领域尚未达成共识）是使用子词。子词是目前在语言模型空间中表示词汇单位的主要方式。我们之前探索的GPT-NeoX词汇表使用子词标记。[图3-1](#subword-tokens)显示了OpenAI分词器游乐场，展示了OpenAI模型如何将单词分割成它们的构成子词。
- en: '![Subword tokens](assets/dllm_0301.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Subword tokens](assets/dllm_0301.png)'
- en: Figure 3-1\. Subword tokens
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1. 子词标记
- en: Tokenizers
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器
- en: Next, let’s dive into tokenizers, the software that serves as a text-processing
    interface between humans and models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解分词器，它是人类和模型之间作为文本处理接口的软件。
- en: 'A tokenizer has two responsibilities:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器有两个职责：
- en: In the tokenizer pre-training stage, the tokenizer is run over a body of text
    to generate a vocabulary.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分词器预训练阶段，分词器在文本体上运行以生成词汇表。
- en: While processing input during both model training and inference, free-form raw
    text is run through the tokenizer algorithm to break the text into sequences of
    valid tokens. [Figure 3-2](#tokenizer-workflow) depicts the roles played by a
    tokenizer.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型训练和推理过程中处理输入时，自由形式的原始文本会通过分词器算法进行处理，将其分割成有效标记的序列。[图3-2](#tokenizer-workflow)描述了分词器所扮演的角色。
- en: '![Tokenizer Workflows](assets/dllm_0302.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Tokenizer Workflows](assets/dllm_0302.png)'
- en: Figure 3-2\. Tokenizer workflow
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2. 分词器工作流程
- en: When we feed raw text to the tokenizer, it breaks the text into tokens that
    are part of the vocabulary and maps the tokens to their token indices. The sequence
    of token indices (input IDs) are then fed to the language model, where they are
    mapped to their corresponding embeddings. Let us explore this process in detail.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将原始文本输入到分词器时，它会将文本分割成词汇表中的标记，并将标记映射到它们的标记索引。然后，标记索引的序列（输入ID）被输入到语言模型，在那里它们被映射到相应的嵌入表示。让我们详细探讨这个过程。
- en: 'This time, let’s experiment with the FLAN-T5 model. You need a Google Colab
    Pro or equivalent system to be able to run it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，让我们实验FLAN-T5模型。你需要一个Google Colab Pro或等效系统才能运行它：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `encode()` function tokenizes the input text and returns the corresponding
    token indices. The token indices are mapped to the tokens they represent using
    the `convert_ids_to_tokens()` function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode()`函数将输入文本分词，并返回相应的标记索引。使用`convert_ids_to_tokens()`函数将标记索引映射到它们所代表的标记。'
- en: As you can see, the FLAN-T5 tokenizer doesn’t have dedicated tokens for the
    numbers 937 or 934\. Therefore, it splits the numbers into “9” and “37.” The `</s>`
    token is a special token indicating the end of the string. The `_` means that
    the token is preceded by a space.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，FLAN-T5分词器没有为数字937或934分配专门的标记。因此，它将数字拆分为“9”和“37”。`</s>`标记是一个特殊标记，表示字符串的结束。`_`表示标记前面有一个空格。
- en: 'Let’s try another example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一个例子：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I made a deliberate typo with the word “Insufficient.” Note that subword tokenization
    is rather brittle with typos. But at least the OOV problem has been dealt with
    by breaking the words into subwords. The vocabulary also doesn’t seem to have
    an entry for the word “corduroy,” thus confirming its poor sense of fashion. Meanwhile,
    note that there is a distinct token for three contiguous exclamation points, which
    is different from the token that represents a single exclamation point. Semantically,
    they do convey slightly different meanings.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意在单词“Insufficient”上犯了一个拼写错误。请注意，子词分词对拼写错误相当脆弱。但至少通过将单词分解为子词，已经解决了OOV（Out-of-Vocabulary）问题。词汇表似乎也没有“corduroy”这个词的条目，从而证实了它糟糕的时尚感。同时，请注意，有三个连续感叹号有一个独特的标记，这与表示单个感叹号的标记不同。从语义上看，它们确实传达了略微不同的含义。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Very large models trained on a massive body of text are more robust to misspellings.
    A lot of misspellings already occur in the training set. For example, even the
    rare misspelling “Insuffienct” occurs 14 times in the C4 pre-training dataset.
    The more common misspelling “insufficent” occurs over 1,100 times. Larger models
    can also infer the misspelled word from its context. Smaller models like BERT
    are quite sensitive to misspellings.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量文本上训练的非常大的模型对拼写错误更稳健。训练集中已经有很多拼写错误。例如，即使是罕见的拼写错误“Insuffienct”在C4预训练数据集中也出现了14次。更常见的拼写错误“insufficent”出现了超过1,100次。较大的模型还可以从上下文中推断出拼写错误的单词。像BERT这样的较小模型对拼写错误非常敏感。
- en: If you are using models from OpenAI, you can explore their tokenization scheme
    using the [tiktoken library](https://oreil.ly/2QByi) (no relation to the social
    media website).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是OpenAI的模型，你可以使用[tiktoken库](https://oreil.ly/2QByi)（与社交媒体网站无关）来探索它们的分词方案。
- en: 'Using tiktoken, let’s see the different vocabularies available in the OpenAI
    ecosystem:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用tiktoken，让我们看看OpenAI生态系统中可用的不同词汇表：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The numbers like 50K/100K are presumed to be the vocabulary size. OpenAI hasn’t
    revealed much information about these vocabularies. Their documentation does state
    that o200k_base is used by GPT-4o, while cl100k_base is used by GPT-4:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的50K/100K数字被认为是词汇表大小。OpenAI没有透露太多关于这些词汇表的信息。他们的文档确实指出，o200k_base被GPT-4o使用，而cl100k_base被GPT-4使用：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see there is not much difference between the tokenization used by
    GPT-4 and FLAN-T5.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，GPT-4和FLAN-T5使用的分词之间没有太大的区别。
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: For a given task, if you observe strange behavior from LLMs on only a subset
    of your inputs, it is worthwhile to check how they have been tokenized. While
    you cannot definitively diagnose your problem just by analyzing the tokenization,
    it is often helpful in analysis. In my experience, a non-negligible number of
    LLM failures can be attributed to the way the text was tokenized. This is especially
    true if your target domain is different from the pre-training domain.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的任务，如果你只在你输入的一部分上观察到LLM（大型语言模型）的异常行为，检查它们是如何被分词的将是有益的。虽然仅通过分析分词你无法确定地诊断你的问题，但在分析中这通常是有帮助的。根据我的经验，相当数量的LLM失败可以归因于文本的分词方式。这在你目标领域与预训练领域不同的情况下尤其如此。
- en: Tokenization Pipeline
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tokenization Pipeline
- en: '[Figure 3-3](#huggingface-tokenizers-pipeline) depicts the sequence of steps
    performed by a tokenizer.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-3](#huggingface-tokenizers-pipeline)描述了分词器执行的步骤序列。'
- en: '![Hugging Face Tokenizers pipeline](assets/dllm_0303.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![Hugging Face分词器管道](assets/dllm_0303.png)'
- en: Figure 3-3\. Hugging Face tokenizers pipeline
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. Hugging Face分词器管道
- en: 'If you are using the `tokenizers` library from Hugging Face, your input text
    is run through a [multistage tokenization pipeline](https://oreil.ly/CcOKV). This
    pipeline is composed of four components:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Hugging Face的`tokenizers`库，你的输入文本将通过一个[多阶段分词管道](https://oreil.ly/CcOKV)。这个管道由四个组件组成：
- en: Normalization
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化
- en: Pre-tokenization
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预分词
- en: Tokenization
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Postprocessing
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理
- en: Note that different models will execute different steps within these four components.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，不同的模型将在这四个组件中执行不同的步骤。
- en: Normalization
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规范化
- en: 'Different types of normalization applied include:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 应用了不同类型的规范化包括：
- en: Converting text to lowercase (if you are using an uncased model)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为小写（如果你使用的是无大小写模型）
- en: Stripping off accents from characters, like from the word Peña
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从字符中去除重音，例如从单词Peña中去除
- en: Unicode normalization
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unicode规范化
- en: 'Let’s see what kind of normalization is applied on the uncased version of BERT:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看对BERT的无大小写版本应用了什么样的规范化：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, the accents have been removed and the text has been converted
    to lowercase.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，重音符号已被移除，文本已被转换为小写。
- en: There isn’t much normalization done in tokenizers for more recent models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更近期的模型，分词器中进行的规范化并不多。
- en: Pre-Tokenization
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预分词
- en: Before we run the tokenizer on the text, we can optionally perform a pre-tokenization
    step. As mentioned earlier, most tokenizers today employ subword tokenization.
    A common step is to first perform word tokenization and then feed the output of
    it to the subword tokenization algorithm. This step is called pre-tokenization.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对文本运行分词器之前，我们可以选择执行预分词步骤。如前所述，大多数分词器今天都采用子词分词。一个常见的步骤是首先执行词分词，然后将输出传递给子词分词算法。这一步骤被称为预分词。
- en: 'Pre-tokenization is a relatively easy step in English compared to many other
    languages, since you can start with a very strong baseline just by splitting text
    on whitespace. There are outlier decisions to be made, such as how to deal with
    punctuation, multiple spaces, numbers, etc. In Hugging Face the regular expression:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于许多其他语言，预分词在英语中是一个相对简单的步骤，因为你可以通过在空白处分割文本来获得一个非常强大的基线。需要做出一些异常决策，例如如何处理标点符号、多个空格、数字等。在Hugging
    Face中，正则表达式：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: is used to split on whitespace.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在空白处分割。
- en: 'Let’s run the pre-tokenization step of the T5 tokenizer:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行T5分词器的预分词步骤：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Along with the pre-tokens (or word tokens), the character offsets are returned.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预标记（或词标记）之外，还返回字符偏移量。
- en: The T5 pre-tokenizer splits only on whitespace, doesn’t collapse multiple spaces
    into one, and doesn’t split on punctuation or numbers. The behavior can be vastly
    different for other tokenizers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: T5预分词器仅在空白处分割，不会将多个空格合并为一个，也不会在标点符号或数字上分割。其他分词器的行为可能会有很大不同。
- en: Tokenization
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: After the optional pre-tokenization step, the actual tokenization step is performed.
    Some of the important algorithms in this space are byte pair encoding (BPE), byte-level
    BPE, WordPiece, and Unigram LM. The tokenizer comprises a set of rules that is
    learned during a pre-training phase over a pre-training dataset. Now let’s go
    through these algorithms in detail.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在可选的预分词步骤之后，实际的分词步骤被执行。这个领域的一些重要算法包括字节对编码（BPE）、字节级BPE、WordPiece和单语LM。分词器包含一组在预训练阶段通过预训练数据集学习到的规则。现在让我们详细地了解一下这些算法。
- en: Byte Pair Encoding
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字节对编码
- en: This algorithm is the simplest and most widely used tokenization algorithm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法是最简单且最广泛使用的分词算法。
- en: Training stage
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段
- en: We take a training dataset, run it through the normalization and pre-tokenization
    steps discussed earlier, and record the unique tokens in the resulting output
    and their frequencies. We then construct an initial vocabulary consisting of the
    unique characters that make up these tokens. Starting from this initial vocabulary,
    we continue adding new tokens using *merge* rules. The merge rule is simple; we
    create a new token using the most frequent consecutive pairs of tokens. The merges
    continue until we reach the desired vocabulary size.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取一个训练数据集，运行它通过前面讨论的规范化和预分词步骤，并记录结果输出中的唯一标记及其频率。然后我们构建一个由构成这些标记的唯一字符组成的初始词汇表。从这个初始词汇表开始，我们继续使用*合并*规则添加新的标记。合并规则很简单；我们使用最频繁的连续标记对创建一个新的标记。合并会一直持续到达到所需的词汇量。
- en: 'Let’s explore this with an example. Imagine our training dataset is composed
    of six words, each appearing just once:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来探讨这个问题。假设我们的训练数据集由六个单词组成，每个单词只出现一次：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The initial vocabulary is then made up of:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 初始词汇表由以下内容组成：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The frequencies of contiguous token pairs are:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 连续标记对的频率如下：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The most frequent pair is “ap,” so the first merge rule is to merge “a” and
    “p.” The vocabulary now is:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最频繁的配对是“ap”，因此第一个合并规则是将“a”和“p”合并。现在的词汇表是：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The new frequencies are:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 新的频率如下：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, the most frequent pair is “at,” so the next merge rule is to merge “a”
    and “t.” This process continues until we reach the vocabulary size.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最频繁的配对是“at”，因此下一个合并规则是将“a”和“t”合并。这个过程一直持续到达到词汇表大小。
- en: Inference stage
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理阶段
- en: After the tokenizer has been trained, it can be used to divide the text into
    appropriate subword tokens and feed the text into the model. This happens in a
    similar fashion as the training step. After normalization and pre-tokenization
    of the input text, the resulting tokens are broken into individual characters,
    and all the merge rules are applied in order. The tokens standing after all merge
    rules have been applied are the final tokens, which are then fed to the model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词器训练完成后，它可以用来将文本划分为适当的子词标记，并将文本输入到模型中。这个过程与训练步骤类似。在输入文本进行归一化和预分词后，生成的标记被分解为单个字符，并按顺序应用所有合并规则。应用所有合并规则后的标记是最终的标记，然后这些标记被输入到模型中。
- en: You can open the [vocabulary file](https://oreil.ly/7JAyY) for GPT-NeoX again,
    and Ctrl+F “merges” to see the merge rules. As expected, the initial merge rules
    join single characters with each other. At the end of the merge list, you can
    see larger subwords like “out” and “comes” being merged into a single token.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以再次打开 GPT-NeoX 的[词汇文件](https://oreil.ly/7JAyY)，并使用 Ctrl+F 搜索“merges”来查看合并规则。如预期，初始合并规则将单个字符彼此连接。在合并列表的末尾，您可以看到像“out”和“comes”这样的较大子词被合并成一个标记。
- en: Note
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since all unique individual characters in the tokenizer training set will get
    their own token, it is guaranteed that there will be no OOV tokens as long as
    all tokens seen during inference in the future are made up of characters that
    were present in the training set. But Unicode consists of over a million code
    points and around 150,000 valid characters, which would not fit in a vocabulary
    of size 30,000\. This means that if your input text contained a character that
    wasn’t in the training set, that character would be assigned an <UNK> token. To
    resolve this, a variant of BPE called byte-level BPE is used. Byte-level BPE starts
    with 256 tokens, representing all the characters that can be represented by a
    byte. This ensures that every Unicode character can be encoded just by the concatenation
    of the constituent byte tokens. Hence, it also ensures that we will never encounter
    an <UNK> token. The GPT family of models use this tokenizer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分词器训练集中所有唯一的单个字符都将获得自己的标记，因此只要未来推理过程中看到的所有标记都由训练集中存在的字符组成，就可以保证不会有 OOV 标记。但
    Unicode 包含超过一百万个代码点，大约有 150,000 个有效字符，这不会适合 30,000 个词汇量的词汇表。这意味着如果输入文本中包含训练集中没有的字符，该字符将被分配一个
    <UNK> 标记。为了解决这个问题，使用了一种名为字节级 BPE 的 BPE 变体。字节级 BPE 从 256 个标记开始，代表所有可以用字节表示的字符。这确保了每个
    Unicode 字符都可以通过构成字节标记的串联来编码。因此，它还确保我们永远不会遇到 <UNK> 标记。GPT 系列模型使用这种分词器。
- en: WordPiece
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WordPiece
- en: WordPiece is similar to BPE, so we will highlight only the differences.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: WordPiece 与 BPE 类似，所以我们只突出它们之间的差异。
- en: 'Instead of the frequency approach used by BPE, WordPiece uses the maximum likelihood
    approach. The frequency of the token pairs in the dataset is normalized by the
    product of the frequency of the individual tokens. The pairs with the resulting
    highest score are then merged:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: WordPiece 不像 BPE 那样使用频率方法，而是使用最大似然方法。数据集中标记对的频率通过单个标记的频率乘积进行归一化。然后合并具有最高得分的配对：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This means that if a token pair is made up of tokens that individually have
    low frequency, they will be merged first.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果一个标记对由具有低频率的单个标记组成，它们将被首先合并。
- en: '[Figure 3-4](#WordPiece) shows the merge priority and how the normalization
    by individual frequencies affects the order of merging.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-4](#WordPiece) 展示了合并优先级以及如何通过个体频率的归一化影响合并顺序。'
- en: '![WordPiece tokenization](assets/dllm_0304.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![WordPiece 分词](assets/dllm_0304.png)'
- en: Figure 3-4\. WordPiece tokenization
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. WordPiece 分词
- en: During inference, merge rules are not used. Instead, for each pre-tokenized
    token in the input text, the tokenizer finds the longest subword from the vocabulary
    in the token and splits on it. For example, if the token is “understanding” and
    the longest subword in the dictionary within this token is “understand,” then
    it will be split into “understand” and “ing.”
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，不使用合并规则。相反，对于输入文本中的每个预分词标记，分词器会在标记中找到词汇表中的最长子词，并在此基础上进行分割。例如，如果标记是“understanding”，并且在这个标记内字典中的最长子词是“understand”，那么它将被分割成“understand”和“ing”。
- en: Postprocessing
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理
- en: Now that we have looked at a couple of tokenizer algorithms, let’s move on to
    the next stage of the pipeline, the postprocessing stage. This is where model-specific
    special tokens are added. Common tokens include [CLS], the classification token
    used in many language models, and [SEP], a separator token used to separate parts
    of the input.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查看了一些分词器算法，接下来让我们进入管道的下一阶段，即后处理阶段。这是添加特定模型特殊标记的地方。常见的标记包括 [CLS]，这是许多语言模型中使用的分类标记，以及
    [SEP]，这是一个用于分隔输入部分的分隔符标记。
- en: Special Tokens
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特殊标记
- en: 'Depending on the model, a few special tokens are added to the vocabulary to
    facilitate processing. These tokens can include:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型的不同，词汇表中会添加一些特殊标记以方便处理。这些标记可以包括：
- en: <PAD>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <PAD>
- en: To indicate padding, in case the size of the input is less than the maximum
    sequence length.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示填充，以防输入的大小小于最大序列长度。
- en: <EOS>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <EOS>
- en: To indicate the end of the sequence. Generative models stop generating after
    outputting this token.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示序列的结束。生成模型在输出此标记后停止生成。
- en: <UNK>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <UNK>
- en: To indicate an OOV term.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示一个 OOV（Out-of-Vocabulary）术语。
- en: <TOOL_CALL>, </TOOL_CALL>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <TOOL_CALL>, </TOOL_CALL>
- en: Content between these tokens is used as input to an external tool, like an API
    call or a query to a database.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标记之间的内容用作外部工具（如API调用或数据库查询）的输入。
- en: <TOOL_RESULT>, </TOOL_RESULT>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <TOOL_RESULT>, </TOOL_RESULT>
- en: Content between these tokens is used to represent the results from calling the
    aforementioned tools.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标记之间的内容用于表示调用上述工具的结果。
- en: 'As we have seen, if our data is domain-specific like healthcare, scientific
    literature, etc., tokenization from a general-purpose tokenizer will be unsatisfactory.
    GALACTICA by Meta introduced several domain-specific tokens in their model and
    special tokenization rules:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，如果我们的数据是特定领域的，如医疗保健、科学文献等，使用通用分词器进行分词将不会令人满意。Meta 公司的 GALACTICA 模型在其模型中引入了几个特定领域的标记以及特殊的分词规则：
- en: '[START_REF] and [END_REF] for wrapping citations.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[START_REF] 和 [END_REF] 用于封装引用。'
- en: <WORK> to wrap tokens that make up an internal working memory, used for reasoning
    and code generation.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <WORK> 用于封装构成内部工作记忆的标记，该记忆用于推理和代码生成。
- en: Numbers are handled by assigning each digit in the number its own token.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字通过为每个数字分配其自己的标记来处理。
- en: '[START_SMILES], [START_DNA], [START_AMINO], [END_SMILES], [END_DNA], [END_AMINO]
    for protein sequences, DNA sequences, and amino acid sequences, respectively.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[START_SMILES], [START_DNA], [START_AMINO], [END_SMILES], [END_DNA], [END_AMINO]
    分别用于蛋白质序列、DNA序列和氨基酸序列。'
- en: If you are using a model on domain-specific data like healthcare, finance, law,
    biomedical, etc., with a tokenizer that was trained on general-purpose data, the
    compression ratio will be relatively lower because domain-specific words do not
    have their own tokens and will be split into multiple tokens. One way to adapt
    models to specialized domains is for models to learn good vector representations
    for domain-specific terms.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用在通用数据上训练的分词器对特定领域的数据（如医疗保健、金融、法律、生物医学等）进行建模，那么压缩比将相对较低，因为特定领域的单词没有自己的标记，将被分割成多个标记。将模型适应特定领域的一种方法是为特定领域的术语学习良好的向量表示。
- en: To this end, we can add new tokens to existing tokenizers and continue pre-training
    the model on domain-specific data so that those new domain-specific tokens learn
    effective representations. We will learn more about continued pre-training in
    [Chapter 7](ch07.html#ch07).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我们可以向现有的分词器添加新标记，并继续在特定领域数据上预训练模型，以便这些新领域特定标记学习有效的表示。我们将在第7章（[Chapter 7](ch07.html#ch07)）中了解更多关于持续预训练的内容。
- en: For now, let’s see how we can add new tokens to a vocabulary using Hugging Face.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用 Hugging Face 向词汇表中添加新标记。
- en: 'Consider the sentence, “The addition of CAR-T cells and antisense oligonucleotides
    drove down incidence rates.” The FLAN-T5 tokenizer splits this text as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下句子，“CAR-T细胞和反义寡核苷酸的添加降低了发病率。”FLAN-T5分词器将此文本分割如下：
- en: '[''▁The'', ''▁addition'', ''▁of'', ''▁C'', '' AR'', ''-'', '' T'', ''▁cells'',
    ''▁and'', ''▁anti'', '' s'', '' ense'', ''▁'', '' oli'', '' gon'', '' u'', '' cle'',
    '' o'', '' t'', '' ides'', ''▁drove'', ''▁down'', ''▁incidence'', ''▁rates'',
    '' .'', ''</s>'']'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s add the domain-specific terms to the vocabulary:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, tokenizing the string again gives the following tokens, with the domain-specific
    tokens being added:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[''▁The'', ''▁addition'', ''▁of'', '' CAR-T'', ''▁cells'', ''▁and'', '' antisense'',
    '' oligonucleotides'', ''▁drove'', ''▁down'', ''▁incidence'', ''▁rates'', '' .'',
    ''</s>'']'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are only halfway done here. The embedding vectors corresponding to these
    new tokens do not contain any information about these tokens. We will need to
    learn the right representations for these tokens, which we can do using fine-tuning
    or continued pre-training, which we will discuss in [Chapter 7](ch07.html#ch07).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we focused on a key ingredient of language models: their vocabulary.
    We discussed how vocabularies are defined and constructed in the realm of language
    models. We introduced the concept of tokenization and presented tokenization algorithms
    like BPE and WordPiece that are used to construct vocabularies and break down
    raw input text into a sequence of tokens that can be consumed by the language
    model. We also explored the vocabularies of popular language models and noted
    how tokens can differ from human conceptions of a word.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue exploring the remaining ingredients of
    a language model, including its architecture and the learning objectives on which
    models are trained.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
