- en: Chapter 3\. Vocabulary and Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ch02.html#ch02), we dug deep into the datasets that are used
    to train the language models of today, including the process of creating them.
    Hopefully this foray has underscored how influential pre-training data is to the
    resulting model. In this chapter, we will discuss another fundamental ingredient
    of a language model: its vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: Vocabulary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What do you do first when you start learning a new language? You start acquiring
    its vocabulary, expanding it as you gain more proficiency in the language. Let’s
    define vocabulary here as:'
  prefs: []
  type: TYPE_NORMAL
- en: All the words in a language that are understood by a specific person.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The average native English speaker has a vocabulary of [20,000–35,000 words](https://oreil.ly/bkc2C).
    Similarly, every language model has its own vocabulary, with most vocabulary sizes
    ranging anywhere between 5,000 and 500,000 *tokens*.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let us explore the vocabulary of the GPT-NeoX-20B model. Open
    the file [*tokenizer.json*](https://oreil.ly/Kages) and Ctrl+F for “vocab,” a
    dictionary containing the vocabulary of the model. You can see that the words
    comprising the language model vocabulary don’t entirely look like English language
    words that appear in a dictionary. These word-like units are called “types,” and
    the instantiation of a type (when it appears in a sequence of text) is called
    a token.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, and especially in industry, I seldom hear anyone use the term “type”
    except in older NLP textbooks. The term “token” is broadly used to refer to both
    the vocabulary units and when they appear in a text sequence. We will henceforth
    use the word “token” to describe both concepts, even though I personally am not
    the biggest fan of this usage.
  prefs: []
  type: TYPE_NORMAL
- en: In the vocabulary file, we see that next to each token is a number, which is
    called the *input ID* or the *token index*. The vocabulary size of GPT-NeoX is
    just above 50,000.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the vocabulary file in detail, we notice that the first few hundred
    tokens are all single-character tokens, such as special characters, digits, capital
    letters, small letters, and accented characters. Longer words appear later on
    in the vocabulary. A lot of tokens correspond to just a part of a word, called
    a *subword*, like “impl,” “inated,” and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s Ctrl+F for “office.” We get nine results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The Ġ character refers to a space before the word. For instance, in the sentence,
    “He stopped going to the office,” the space before the letter “o” in the word
    “office” is considered part of the token. You can see that the tokens are case-sensitive:
    there are separate tokens for “office” and “Office.” Most models these days have
    case-sensitive vocabularies. Back in the day, the BERT model was released with
    both a cased and an uncased version.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Language models learn vector representations called embeddings for each of these
    tokens that reflect their syntactic and semantic meaning. We will go through the
    learning process in [Chapter 4](ch04.html#chapter_transformer-architecture), and
    dive deeper into embeddings in [Chapter 11](ch11.html#chapter_llm_interfaces).
  prefs: []
  type: TYPE_NORMAL
- en: Cased vocabularies are almost always better, especially when you are training
    on such a huge body of text such that most tokens are seen by the model enough
    times to learn distinctive representations for them. For instance, there is a
    definite semantic difference between “web” and “Web,” and it is good to have separate
    tokens for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s search for some numbers. Ctrl+F for “93.” There are only three results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It seems like not all numbers get their own tokens! Where is the token for 934?
    It is impractical to give every number its own token, especially if you want to
    limit your vocabulary size to say, just 50,000\. Later in this chapter, we will
    discuss how vocabulary sizes are determined. Popular names and places get their
    own token. There is a token representing Boston, Toronto, and Amsterdam, but none
    representing Mesa or Chennai. There is a token representing Ahmed and Donald,
    but none for Suhas or Maryam.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might have noticed that tokens like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: exist, indicating that GPT-NeoX is also primed to process programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: How are vocabularies determined? Surely, there was no executive committee holding
    emergency meetings burning midnight oil, with members making impassioned pleas
    to include the number 937 in the vocabulary at the expense of 934.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us revisit the definition of a vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: All the words in a language that are understood by a specific person.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Since we want our language model to be an expert at English, we can just include
    all words in the English dictionary as part of its vocabulary. Problem solved?
  prefs: []
  type: TYPE_NORMAL
- en: Not nearly. What do you do when you communicate with the language model using
    a word that it has never seen? This happens a lot more often than you think. New
    words get invented all the time, words have multiple forms (“understand,” “understanding,”
    “understandable”), multiple words can be combined into a single word, and so on.
    Moreover, there are millions of domain-specific words (biomedical, chemistry,
    and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The account [@NYT_first_said](https://oreil.ly/FzfI9) on the social media platform
    X posts words except proper nouns when they appear in the *New York Times* for
    the first time. Each day, an average of five new words appear in the US paper
    of record for the first time ever. On the day I wrote this section, the words
    were “unflippant,” “dumbeyed,” “dewdrenched,” “faceflat,” “saporous,” and “dronescape.”
    Many of these words might never get added to a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'A token that doesn’t exist in the vocabulary is called an out-of-vocabulary
    (OOV) token. Traditionally, OOV tokens were represented using a special <UNK>
    token. The <UNK> token is a placeholder for all tokens that don’t exist in the
    vocabulary. All OOV tokens share the same embedding (and encode the same meaning),
    which is undesirable. Moreover, the <UNK> token cannot be used in generative models.
    You don’t want your model to output something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To solve the OOV problem, one possible solution could be to represent tokens
    in terms of characters instead of words. Each character has its own embedding,
    and as long as all valid characters are included in the vocabulary, there will
    never be a chance of encountering an OOV token. However, there are many downsides
    to this. The number of tokens needed to represent the average sentence becomes
    much larger. For example, the sentence, “The number of tokens needed to represent
    the average sentence becomes much larger,” contains 13 tokens when you treat each
    word as a token, but 81 tokens when you treat each character as a token. This
    reduces the amount of content you can represent within a fixed sequence length,
    which makes both model training and inference slower, as we will show further
    in [Chapter 4](ch04.html#chapter_transformer-architecture). Models support a limited
    sequence length, so this also reduces the amount of content you can fit in a single
    prompt. Later in this chapter, we will discuss models like CANINE, ByT5, and Charformer
    that attempt to use character-based tokens.
  prefs: []
  type: TYPE_NORMAL
- en: So, the middle ground and the best of both worlds (or the worst of both worlds—the
    field hasn’t come to a consensus yet) is using subwords. Subwords are the predominant
    mode of representing vocabulary units in the language model space today. The GPT-NeoX
    vocabulary we explored earlier uses subword tokens. [Figure 3-1](#subword-tokens)
    shows the OpenAI tokenizer playground that demonstrates how words are split into
    their constituent subwords by OpenAI models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Subword tokens](assets/dllm_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Subword tokens
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let’s dive into tokenizers, the software that serves as a text-processing
    interface between humans and models.
  prefs: []
  type: TYPE_NORMAL
- en: 'A tokenizer has two responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: In the tokenizer pre-training stage, the tokenizer is run over a body of text
    to generate a vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While processing input during both model training and inference, free-form raw
    text is run through the tokenizer algorithm to break the text into sequences of
    valid tokens. [Figure 3-2](#tokenizer-workflow) depicts the roles played by a
    tokenizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Tokenizer Workflows](assets/dllm_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Tokenizer workflow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When we feed raw text to the tokenizer, it breaks the text into tokens that
    are part of the vocabulary and maps the tokens to their token indices. The sequence
    of token indices (input IDs) are then fed to the language model, where they are
    mapped to their corresponding embeddings. Let us explore this process in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, let’s experiment with the FLAN-T5 model. You need a Google Colab
    Pro or equivalent system to be able to run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `encode()` function tokenizes the input text and returns the corresponding
    token indices. The token indices are mapped to the tokens they represent using
    the `convert_ids_to_tokens()` function.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the FLAN-T5 tokenizer doesn’t have dedicated tokens for the
    numbers 937 or 934\. Therefore, it splits the numbers into “9” and “37.” The `</s>`
    token is a special token indicating the end of the string. The `_` means that
    the token is preceded by a space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: I made a deliberate typo with the word “Insufficient.” Note that subword tokenization
    is rather brittle with typos. But at least the OOV problem has been dealt with
    by breaking the words into subwords. The vocabulary also doesn’t seem to have
    an entry for the word “corduroy,” thus confirming its poor sense of fashion. Meanwhile,
    note that there is a distinct token for three contiguous exclamation points, which
    is different from the token that represents a single exclamation point. Semantically,
    they do convey slightly different meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Very large models trained on a massive body of text are more robust to misspellings.
    A lot of misspellings already occur in the training set. For example, even the
    rare misspelling “Insuffienct” occurs 14 times in the C4 pre-training dataset.
    The more common misspelling “insufficent” occurs over 1,100 times. Larger models
    can also infer the misspelled word from its context. Smaller models like BERT
    are quite sensitive to misspellings.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using models from OpenAI, you can explore their tokenization scheme
    using the [tiktoken library](https://oreil.ly/2QByi) (no relation to the social
    media website).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using tiktoken, let’s see the different vocabularies available in the OpenAI
    ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The numbers like 50K/100K are presumed to be the vocabulary size. OpenAI hasn’t
    revealed much information about these vocabularies. Their documentation does state
    that o200k_base is used by GPT-4o, while cl100k_base is used by GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see there is not much difference between the tokenization used by
    GPT-4 and FLAN-T5.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For a given task, if you observe strange behavior from LLMs on only a subset
    of your inputs, it is worthwhile to check how they have been tokenized. While
    you cannot definitively diagnose your problem just by analyzing the tokenization,
    it is often helpful in analysis. In my experience, a non-negligible number of
    LLM failures can be attributed to the way the text was tokenized. This is especially
    true if your target domain is different from the pre-training domain.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Figure 3-3](#huggingface-tokenizers-pipeline) depicts the sequence of steps
    performed by a tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hugging Face Tokenizers pipeline](assets/dllm_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Hugging Face tokenizers pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are using the `tokenizers` library from Hugging Face, your input text
    is run through a [multistage tokenization pipeline](https://oreil.ly/CcOKV). This
    pipeline is composed of four components:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Postprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that different models will execute different steps within these four components.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Different types of normalization applied include:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting text to lowercase (if you are using an uncased model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stripping off accents from characters, like from the word Peña
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unicode normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see what kind of normalization is applied on the uncased version of BERT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the accents have been removed and the text has been converted
    to lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: There isn’t much normalization done in tokenizers for more recent models.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we run the tokenizer on the text, we can optionally perform a pre-tokenization
    step. As mentioned earlier, most tokenizers today employ subword tokenization.
    A common step is to first perform word tokenization and then feed the output of
    it to the subword tokenization algorithm. This step is called pre-tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-tokenization is a relatively easy step in English compared to many other
    languages, since you can start with a very strong baseline just by splitting text
    on whitespace. There are outlier decisions to be made, such as how to deal with
    punctuation, multiple spaces, numbers, etc. In Hugging Face the regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: is used to split on whitespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the pre-tokenization step of the T5 tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Along with the pre-tokens (or word tokens), the character offsets are returned.
  prefs: []
  type: TYPE_NORMAL
- en: The T5 pre-tokenizer splits only on whitespace, doesn’t collapse multiple spaces
    into one, and doesn’t split on punctuation or numbers. The behavior can be vastly
    different for other tokenizers.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the optional pre-tokenization step, the actual tokenization step is performed.
    Some of the important algorithms in this space are byte pair encoding (BPE), byte-level
    BPE, WordPiece, and Unigram LM. The tokenizer comprises a set of rules that is
    learned during a pre-training phase over a pre-training dataset. Now let’s go
    through these algorithms in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Byte Pair Encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm is the simplest and most widely used tokenization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Training stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We take a training dataset, run it through the normalization and pre-tokenization
    steps discussed earlier, and record the unique tokens in the resulting output
    and their frequencies. We then construct an initial vocabulary consisting of the
    unique characters that make up these tokens. Starting from this initial vocabulary,
    we continue adding new tokens using *merge* rules. The merge rule is simple; we
    create a new token using the most frequent consecutive pairs of tokens. The merges
    continue until we reach the desired vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore this with an example. Imagine our training dataset is composed
    of six words, each appearing just once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial vocabulary is then made up of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The frequencies of contiguous token pairs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The most frequent pair is “ap,” so the first merge rule is to merge “a” and
    “p.” The vocabulary now is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The new frequencies are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, the most frequent pair is “at,” so the next merge rule is to merge “a”
    and “t.” This process continues until we reach the vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: Inference stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the tokenizer has been trained, it can be used to divide the text into
    appropriate subword tokens and feed the text into the model. This happens in a
    similar fashion as the training step. After normalization and pre-tokenization
    of the input text, the resulting tokens are broken into individual characters,
    and all the merge rules are applied in order. The tokens standing after all merge
    rules have been applied are the final tokens, which are then fed to the model.
  prefs: []
  type: TYPE_NORMAL
- en: You can open the [vocabulary file](https://oreil.ly/7JAyY) for GPT-NeoX again,
    and Ctrl+F “merges” to see the merge rules. As expected, the initial merge rules
    join single characters with each other. At the end of the merge list, you can
    see larger subwords like “out” and “comes” being merged into a single token.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since all unique individual characters in the tokenizer training set will get
    their own token, it is guaranteed that there will be no OOV tokens as long as
    all tokens seen during inference in the future are made up of characters that
    were present in the training set. But Unicode consists of over a million code
    points and around 150,000 valid characters, which would not fit in a vocabulary
    of size 30,000\. This means that if your input text contained a character that
    wasn’t in the training set, that character would be assigned an <UNK> token. To
    resolve this, a variant of BPE called byte-level BPE is used. Byte-level BPE starts
    with 256 tokens, representing all the characters that can be represented by a
    byte. This ensures that every Unicode character can be encoded just by the concatenation
    of the constituent byte tokens. Hence, it also ensures that we will never encounter
    an <UNK> token. The GPT family of models use this tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: WordPiece
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WordPiece is similar to BPE, so we will highlight only the differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of the frequency approach used by BPE, WordPiece uses the maximum likelihood
    approach. The frequency of the token pairs in the dataset is normalized by the
    product of the frequency of the individual tokens. The pairs with the resulting
    highest score are then merged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This means that if a token pair is made up of tokens that individually have
    low frequency, they will be merged first.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-4](#WordPiece) shows the merge priority and how the normalization
    by individual frequencies affects the order of merging.'
  prefs: []
  type: TYPE_NORMAL
- en: '![WordPiece tokenization](assets/dllm_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. WordPiece tokenization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: During inference, merge rules are not used. Instead, for each pre-tokenized
    token in the input text, the tokenizer finds the longest subword from the vocabulary
    in the token and splits on it. For example, if the token is “understanding” and
    the longest subword in the dictionary within this token is “understand,” then
    it will be split into “understand” and “ing.”
  prefs: []
  type: TYPE_NORMAL
- en: Postprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have looked at a couple of tokenizer algorithms, let’s move on to
    the next stage of the pipeline, the postprocessing stage. This is where model-specific
    special tokens are added. Common tokens include [CLS], the classification token
    used in many language models, and [SEP], a separator token used to separate parts
    of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Special Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on the model, a few special tokens are added to the vocabulary to
    facilitate processing. These tokens can include:'
  prefs: []
  type: TYPE_NORMAL
- en: <PAD>
  prefs: []
  type: TYPE_NORMAL
- en: To indicate padding, in case the size of the input is less than the maximum
    sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: <EOS>
  prefs: []
  type: TYPE_NORMAL
- en: To indicate the end of the sequence. Generative models stop generating after
    outputting this token.
  prefs: []
  type: TYPE_NORMAL
- en: <UNK>
  prefs: []
  type: TYPE_NORMAL
- en: To indicate an OOV term.
  prefs: []
  type: TYPE_NORMAL
- en: <TOOL_CALL>, </TOOL_CALL>
  prefs: []
  type: TYPE_NORMAL
- en: Content between these tokens is used as input to an external tool, like an API
    call or a query to a database.
  prefs: []
  type: TYPE_NORMAL
- en: <TOOL_RESULT>, </TOOL_RESULT>
  prefs: []
  type: TYPE_NORMAL
- en: Content between these tokens is used to represent the results from calling the
    aforementioned tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, if our data is domain-specific like healthcare, scientific
    literature, etc., tokenization from a general-purpose tokenizer will be unsatisfactory.
    GALACTICA by Meta introduced several domain-specific tokens in their model and
    special tokenization rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[START_REF] and [END_REF] for wrapping citations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <WORK> to wrap tokens that make up an internal working memory, used for reasoning
    and code generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numbers are handled by assigning each digit in the number its own token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[START_SMILES], [START_DNA], [START_AMINO], [END_SMILES], [END_DNA], [END_AMINO]
    for protein sequences, DNA sequences, and amino acid sequences, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using a model on domain-specific data like healthcare, finance, law,
    biomedical, etc., with a tokenizer that was trained on general-purpose data, the
    compression ratio will be relatively lower because domain-specific words do not
    have their own tokens and will be split into multiple tokens. One way to adapt
    models to specialized domains is for models to learn good vector representations
    for domain-specific terms.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we can add new tokens to existing tokenizers and continue pre-training
    the model on domain-specific data so that those new domain-specific tokens learn
    effective representations. We will learn more about continued pre-training in
    [Chapter 7](ch07.html#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s see how we can add new tokens to a vocabulary using Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sentence, “The addition of CAR-T cells and antisense oligonucleotides
    drove down incidence rates.” The FLAN-T5 tokenizer splits this text as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[''▁The'', ''▁addition'', ''▁of'', ''▁C'', '' AR'', ''-'', '' T'', ''▁cells'',
    ''▁and'', ''▁anti'', '' s'', '' ense'', ''▁'', '' oli'', '' gon'', '' u'', '' cle'',
    '' o'', '' t'', '' ides'', ''▁drove'', ''▁down'', ''▁incidence'', ''▁rates'',
    '' .'', ''</s>'']'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s add the domain-specific terms to the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, tokenizing the string again gives the following tokens, with the domain-specific
    tokens being added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[''▁The'', ''▁addition'', ''▁of'', '' CAR-T'', ''▁cells'', ''▁and'', '' antisense'',
    '' oligonucleotides'', ''▁drove'', ''▁down'', ''▁incidence'', ''▁rates'', '' .'',
    ''</s>'']'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are only halfway done here. The embedding vectors corresponding to these
    new tokens do not contain any information about these tokens. We will need to
    learn the right representations for these tokens, which we can do using fine-tuning
    or continued pre-training, which we will discuss in [Chapter 7](ch07.html#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we focused on a key ingredient of language models: their vocabulary.
    We discussed how vocabularies are defined and constructed in the realm of language
    models. We introduced the concept of tokenization and presented tokenization algorithms
    like BPE and WordPiece that are used to construct vocabularies and break down
    raw input text into a sequence of tokens that can be consumed by the language
    model. We also explored the vocabularies of popular language models and noted
    how tokens can differ from human conceptions of a word.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue exploring the remaining ingredients of
    a language model, including its architecture and the learning objectives on which
    models are trained.
  prefs: []
  type: TYPE_NORMAL
