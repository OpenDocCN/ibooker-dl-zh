- en: Chapter 3\. Vocabulary and Tokenization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. 词汇和分词
- en: 'In [Chapter 2](ch02.html#ch02), we dug deep into the datasets that are used
    to train the language models of today, including the process of creating them.
    Hopefully this foray has underscored how influential pre-training data is to the
    resulting model. In this chapter, we will discuss another fundamental ingredient
    of a language model: its vocabulary.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#ch02)中，我们深入探讨了用于训练当今语言模型的语料库，包括创建它们的过程。希望这次探索强调了预训练数据对最终模型的影响。在本章中，我们将讨论语言模型的另一个基本组成部分：其词汇。
- en: Vocabulary
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇
- en: 'What do you do first when you start learning a new language? You start acquiring
    its vocabulary, expanding it as you gain more proficiency in the language. Let’s
    define vocabulary here as:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始学习一门新语言时，你首先做什么？你开始积累它的词汇，随着你在语言上的熟练程度提高，不断扩大词汇量。在这里，我们将词汇定义为：
- en: All the words in a language that are understood by a specific person.
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个特定的人所能理解的语言中的所有单词。
- en: The average native English speaker has a vocabulary of [20,000–35,000 words](https://oreil.ly/bkc2C).
    Similarly, every language model has its own vocabulary, with most vocabulary sizes
    ranging anywhere between 5,000 and 500,000 *tokens*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 平均的母语英语说话者的词汇量在[20,000–35,000]个单词之间（[https://oreil.ly/bkc2C](https://oreil.ly/bkc2C)）。同样，每个语言模型都有自己的词汇表，其中大多数词汇量的大小在5,000到500,000个*分词*之间。
- en: As an example, let us explore the vocabulary of the GPT-NeoX-20B model. Open
    the file [*tokenizer.json*](https://oreil.ly/Kages) and Ctrl+F for “vocab,” a
    dictionary containing the vocabulary of the model. You can see that the words
    comprising the language model vocabulary don’t entirely look like English language
    words that appear in a dictionary. These word-like units are called “types,” and
    the instantiation of a type (when it appears in a sequence of text) is called
    a token.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们探索GPT-NeoX-20B模型的词汇。打开文件[*tokenizer.json*](https://oreil.ly/Kages)并使用Ctrl+F查找“vocab”，这是一个包含模型词汇的字典。你可以看到构成语言模型词汇的单词并不完全像字典中出现的英语单词。这些类似单词的单位被称为“类型”，而类型的实例化（它在文本序列中出现时）被称为分词。
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Recently, and especially in industry, I seldom hear anyone use the term “type”
    except in older NLP textbooks. The term “token” is broadly used to refer to both
    the vocabulary units and when they appear in a text sequence. We will henceforth
    use the word “token” to describe both concepts, even though I personally am not
    the biggest fan of this usage.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，尤其是在工业界，我很少听到有人使用“类型”这个词，除了在较老的自然语言处理教科书中。术语“分词”广泛用于指代词汇单元以及它们在文本序列中的出现。因此，我们将使用“分词”这个词来描述这两个概念，尽管我个人并不是特别喜欢这种用法。
- en: In the vocabulary file, we see that next to each token is a number, which is
    called the *input ID* or the *token index*. The vocabulary size of GPT-NeoX is
    just above 50,000.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在词汇文件中，我们可以看到每个分词旁边都有一个数字，这被称为*输入ID*或*分词索引*。GPT-NeoX的词汇量刚刚超过50,000。
- en: Looking at the vocabulary file in detail, we notice that the first few hundred
    tokens are all single-character tokens, such as special characters, digits, capital
    letters, small letters, and accented characters. Longer words appear later on
    in the vocabulary. A lot of tokens correspond to just a part of a word, called
    a *subword*, like “impl,” “inated,” and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 详细查看词汇文件，我们会注意到前几百个分词都是单字符分词，例如特殊字符、数字、大写字母、小写字母和带重音的字符。较长的单词出现在词汇表的后面。许多分词对应于单词的一部分，称为*子词*，如“impl”、“inated”等等。
- en: 'Let’s Ctrl+F for “office.” We get nine results:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Ctrl+F查找“office”。我们得到了九个结果：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Ġ character refers to a space before the word. For instance, in the sentence,
    “He stopped going to the office,” the space before the letter “o” in the word
    “office” is considered part of the token. You can see that the tokens are case-sensitive:
    there are separate tokens for “office” and “Office.” Most models these days have
    case-sensitive vocabularies. Back in the day, the BERT model was released with
    both a cased and an uncased version.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Ġ字符表示单词前的空格。例如，在句子“他停止去办公室”中，“office”单词中“o”字母前的空格被认为是分词的一部分。你可以看到分词是区分大小写的：有“office”和“Office”两个不同的分词。如今，大多数模型都有区分大小写的词汇表。在早期，BERT模型发布时，既有带大小写的版本，也有不带大小写的版本。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Language models learn vector representations called embeddings for each of these
    tokens that reflect their syntactic and semantic meaning. We will go through the
    learning process in [Chapter 4](ch04.html#chapter_transformer-architecture), and
    dive deeper into embeddings in [Chapter 11](ch11.html#chapter_llm_interfaces).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型为每个这些标记学习向量表示，称为嵌入，这些嵌入反映了它们的句法和语义意义。我们将在[第4章](ch04.html#chapter_transformer-architecture)中介绍学习过程，并在[第11章](ch11.html#chapter_llm_interfaces)中更深入地探讨嵌入。
- en: Cased vocabularies are almost always better, especially when you are training
    on such a huge body of text such that most tokens are seen by the model enough
    times to learn distinctive representations for them. For instance, there is a
    definite semantic difference between “web” and “Web,” and it is good to have separate
    tokens for them.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有标记的词汇几乎总是更好的，尤其是当你在一个如此庞大的文本体上进行训练时，大多数标记都被模型足够多次地看到，以便学习它们的独特表示。例如，“web”和“Web”之间有明显的语义差异，为它们分别保留单独的标记是好的。
- en: 'Let’s search for some numbers. Ctrl+F for “93.” There are only three results:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们搜索一些数字。使用Ctrl+F搜索“93。”只有三个结果：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It seems like not all numbers get their own tokens! Where is the token for 934?
    It is impractical to give every number its own token, especially if you want to
    limit your vocabulary size to say, just 50,000\. Later in this chapter, we will
    discuss how vocabulary sizes are determined. Popular names and places get their
    own token. There is a token representing Boston, Toronto, and Amsterdam, but none
    representing Mesa or Chennai. There is a token representing Ahmed and Donald,
    but none for Suhas or Maryam.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来并非所有数字都有自己的标记！934的标记在哪里？给每个数字分配一个标记是不切实际的，尤其是如果你想将词汇量限制在比如说50,000个单词以内。在本章的后面，我们将讨论词汇量是如何确定的。流行的人名和地名有自己的标记。有一个标记代表波士顿、多伦多和阿姆斯特丹，但没有代表梅萨或钦奈的标记。有一个标记代表艾哈迈德和唐纳德，但没有代表苏哈斯或玛丽亚姆的标记。
- en: 'You might have noticed that tokens like:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到像这样的标记：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: exist, indicating that GPT-NeoX is also primed to process programming languages.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 存在，这表明GPT-NeoX也准备好处理编程语言。
- en: How are vocabularies determined? Surely, there was no executive committee holding
    emergency meetings burning midnight oil, with members making impassioned pleas
    to include the number 937 in the vocabulary at the expense of 934.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇是如何确定的？当然，没有执行委员会在紧急会议上熬夜，成员们为了将数字937纳入词汇而牺牲934而做出激动的请求。
- en: 'Let us revisit the definition of a vocabulary:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视词汇的定义：
- en: All the words in a language that are understood by a specific person.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个特定的人能理解的语言中的所有单词。
- en: Since we want our language model to be an expert at English, we can just include
    all words in the English dictionary as part of its vocabulary. Problem solved?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望我们的语言模型精通英语，我们只需将其英语词典中的所有单词作为其词汇的一部分即可。问题解决了吗？
- en: Not nearly. What do you do when you communicate with the language model using
    a word that it has never seen? This happens a lot more often than you think. New
    words get invented all the time, words have multiple forms (“understand,” “understanding,”
    “understandable”), multiple words can be combined into a single word, and so on.
    Moreover, there are millions of domain-specific words (biomedical, chemistry,
    and so on).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 远远不够。当你使用语言模型从未见过的单词进行交流时，你会怎么做？这种情况比你想象的要频繁得多。新词不断被创造出来，单词有多种形式（“understand”、“understanding”、“understandable”），多个单词可以组合成一个单词，等等。此外，还有数百万个特定领域的单词（生物医学、化学等等）。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The account [@NYT_first_said](https://oreil.ly/FzfI9) on the social media platform
    X posts words except proper nouns when they appear in the *New York Times* for
    the first time. Each day, an average of five new words appear in the US paper
    of record for the first time ever. On the day I wrote this section, the words
    were “unflippant,” “dumbeyed,” “dewdrenched,” “faceflat,” “saporous,” and “dronescape.”
    Many of these words might never get added to a dictionary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体平台X上的账户[@NYT_first_said](https://oreil.ly/FzfI9)在首次出现在《纽约时报》时，除了专有名词外，会发布单词。每天，平均有五个新单词首次出现在美国记录报纸上。在我撰写这一部分的时候，这些单词是“unflippant”、“dumbeyed”、“dewdrenched”、“faceflat”、“saporous”和“dronescape”。其中许多单词可能永远不会被添加到词典中。
- en: 'A token that doesn’t exist in the vocabulary is called an out-of-vocabulary
    (OOV) token. Traditionally, OOV tokens were represented using a special <UNK>
    token. The <UNK> token is a placeholder for all tokens that don’t exist in the
    vocabulary. All OOV tokens share the same embedding (and encode the same meaning),
    which is undesirable. Moreover, the <UNK> token cannot be used in generative models.
    You don’t want your model to output something like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不存在于词汇表中的标记被称为词汇外（OOV）标记。传统上，OOV 标记使用特殊的 <UNK> 标记表示。<UNK> 标记是所有不在词汇表中存在的标记的占位符。所有
    OOV 标记共享相同的嵌入（并编码相同的意义），这是不理想的。此外，<UNK> 标记不能用于生成模型。你不想你的模型输出类似的内容：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To solve the OOV problem, one possible solution could be to represent tokens
    in terms of characters instead of words. Each character has its own embedding,
    and as long as all valid characters are included in the vocabulary, there will
    never be a chance of encountering an OOV token. However, there are many downsides
    to this. The number of tokens needed to represent the average sentence becomes
    much larger. For example, the sentence, “The number of tokens needed to represent
    the average sentence becomes much larger,” contains 13 tokens when you treat each
    word as a token, but 81 tokens when you treat each character as a token. This
    reduces the amount of content you can represent within a fixed sequence length,
    which makes both model training and inference slower, as we will show further
    in [Chapter 4](ch04.html#chapter_transformer-architecture). Models support a limited
    sequence length, so this also reduces the amount of content you can fit in a single
    prompt. Later in this chapter, we will discuss models like CANINE, ByT5, and Charformer
    that attempt to use character-based tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 OOV 问题，一个可能的解决方案可能是用字符而不是单词来表示标记。每个字符都有自己的嵌入，只要所有有效字符都包含在词汇表中，就永远不会遇到 OOV
    标记。然而，这种方法有很多缺点。表示平均句子所需的标记数量变得很大。例如，句子“表示平均句子所需的标记数量变得很大”当每个单词被视为一个标记时包含 13 个标记，但当每个字符被视为一个标记时包含
    81 个标记。这减少了在固定序列长度内可以表示的内容量，这会使模型训练和推理速度变慢，我们将在[第 4 章](ch04.html#chapter_transformer-architecture)中进一步展示。模型支持有限的序列长度，这也减少了可以在单个提示中放入的内容量。在本章的后面部分，我们将讨论像
    CANINE、ByT5 和 Charformer 这样的模型，它们试图使用基于字符的标记。
- en: So, the middle ground and the best of both worlds (or the worst of both worlds—the
    field hasn’t come to a consensus yet) is using subwords. Subwords are the predominant
    mode of representing vocabulary units in the language model space today. The GPT-NeoX
    vocabulary we explored earlier uses subword tokens. [Figure 3-1](#subword-tokens)
    shows the OpenAI tokenizer playground that demonstrates how words are split into
    their constituent subwords by OpenAI models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，折中方案和两者之最佳（或者两者之最差——该领域尚未达成共识）是使用子词。子词是目前在语言模型空间中表示词汇单位的主要方式。我们之前探索的 GPT-NeoX
    词汇表使用子词标记。[图 3-1](#subword-tokens) 展示了 OpenAI 分词器游乐场，该游乐场演示了 OpenAI 模型如何将单词分解为其构成子词。
- en: '![Subword tokens](assets/dllm_0301.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![子词标记](assets/dllm_0301.png)'
- en: Figure 3-1\. Subword tokens
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 子词标记
- en: Tokenizers
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器
- en: Next, let’s dive into tokenizers, the software that serves as a text-processing
    interface between humans and models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解分词器，它是介于人类和模型之间的文本处理接口的软件。
- en: 'A tokenizer has two responsibilities:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器有两个职责：
- en: In the tokenizer pre-training stage, the tokenizer is run over a body of text
    to generate a vocabulary.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分词器预训练阶段，分词器在一段文本上运行以生成词汇表。
- en: While processing input during both model training and inference, free-form raw
    text is run through the tokenizer algorithm to break the text into sequences of
    valid tokens. [Figure 3-2](#tokenizer-workflow) depicts the roles played by a
    tokenizer.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型训练和推理过程中处理输入时，自由形式的原始文本通过分词器算法进行处理，将其分解为有效的标记序列。[图 3-2](#tokenizer-workflow)
    描述了分词器所扮演的角色。
- en: '![Tokenizer Workflows](assets/dllm_0302.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![分词器工作流程](assets/dllm_0302.png)'
- en: Figure 3-2\. Tokenizer workflow
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 分词器工作流程
- en: When we feed raw text to the tokenizer, it breaks the text into tokens that
    are part of the vocabulary and maps the tokens to their token indices. The sequence
    of token indices (input IDs) are then fed to the language model, where they are
    mapped to their corresponding embeddings. Let us explore this process in detail.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将原始文本输入到分词器中时，它会将文本分解为词汇表中的标记，并将标记映射到它们的标记索引。然后，标记索引的序列（输入 ID）被输入到语言模型中，在那里它们被映射到相应的嵌入。让我们详细探讨这个过程。
- en: 'This time, let’s experiment with the FLAN-T5 model. You need a Google Colab
    Pro or equivalent system to be able to run it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，让我们实验一下 FLAN-T5 模型。你需要一个 Google Colab Pro 或等效的系统才能运行它：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `encode()` function tokenizes the input text and returns the corresponding
    token indices. The token indices are mapped to the tokens they represent using
    the `convert_ids_to_tokens()` function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode()` 函数将输入文本进行分词，并返回相应的标记索引。使用 `convert_ids_to_tokens()` 函数将标记索引映射到它们所代表的标记。'
- en: As you can see, the FLAN-T5 tokenizer doesn’t have dedicated tokens for the
    numbers 937 or 934\. Therefore, it splits the numbers into “9” and “37.” The `</s>`
    token is a special token indicating the end of the string. The `_` means that
    the token is preceded by a space.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，FLAN-T5 分词器没有为数字 937 或 934 设置专门的标记。因此，它将数字拆分为“9”和“37”。`</s>` 标记是一个特殊标记，表示字符串的结束。`_`
    表示该标记前面有一个空格。
- en: 'Let’s try another example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一个例子：
- en: '[PRE6] `encoded_text` `=` `tokenizer``.``encode``(``input_text``)` `tokens`
    `=` `tokenizer``.``convert_ids_to_tokens``(``encoded_text``)` `print``(``tokens``)`
    [PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6] `encoded_text` `=` `tokenizer``.``encode``(``input_text``)` `tokens`
    `=` `tokenizer``.``convert_ids_to_tokens``(``encoded_text``)` `print``(``tokens``)`
    [PRE7]'
- en: '[PRE8]` [PRE9] The output is:    [PRE10]py    I made a deliberate typo with
    the word “Insufficient.” Note that subword tokenization is rather brittle with
    typos. But at least the OOV problem has been dealt with by breaking the words
    into subwords. The vocabulary also doesn’t seem to have an entry for the word
    “corduroy,” thus confirming its poor sense of fashion. Meanwhile, note that there
    is a distinct token for three contiguous exclamation points, which is different
    from the token that represents a single exclamation point. Semantically, they
    do convey slightly different meanings.    ###### Note    Very large models trained
    on a massive body of text are more robust to misspellings. A lot of misspellings
    already occur in the training set. For example, even the rare misspelling “Insuffienct”
    occurs 14 times in the C4 pre-training dataset. The more common misspelling “insufficent”
    occurs over 1,100 times. Larger models can also infer the misspelled word from
    its context. Smaller models like BERT are quite sensitive to misspellings.    If
    you are using models from OpenAI, you can explore their tokenization scheme using
    the [tiktoken library](https://oreil.ly/2QByi) (no relation to the social media
    website).    Using tiktoken, let’s see the different vocabularies available in
    the OpenAI ecosystem:    [PRE11]py    The output is:    [PRE12]py    The numbers
    like 50K/100K are presumed to be the vocabulary size. OpenAI hasn’t revealed much
    information about these vocabularies. Their documentation does state that o200k_base
    is used by GPT-4o, while cl100k_base is used by GPT-4:    [PRE13]py `tokens` `=`
    `[``encoding``.``decode_single_token_bytes``(``token``)` `for` `token` `in` `input_ids``]`
    [PRE14]py   [PRE15]`py [PRE16]py`` [PRE17]py # Tokenization Pipeline    [Figure 3-3](#huggingface-tokenizers-pipeline)
    depicts the sequence of steps performed by a tokenizer.  ![Hugging Face Tokenizers
    pipeline](assets/dllm_0303.png)  ###### Figure 3-3\. Hugging Face tokenizers pipeline    If
    you are using the `tokenizers` library from Hugging Face, your input text is run
    through a [multistage tokenization pipeline](https://oreil.ly/CcOKV). This pipeline
    is composed of four components:    *   Normalization           *   Pre-tokenization           *   Tokenization           *   Postprocessing              Note
    that different models will execute different steps within these four components.    ##
    Normalization    Different types of normalization applied include:    *   Converting
    text to lowercase (if you are using an uncased model)           *   Stripping
    off accents from characters, like from the word Peña           *   Unicode normalization              Let’s
    see what kind of normalization is applied on the uncased version of BERT:    [PRE18]py    The
    output is:    [PRE19]py    As we can see, the accents have been removed and the
    text has been converted to lowercase.    There isn’t much normalization done in
    tokenizers for more recent models.    ## Pre-Tokenization    Before we run the
    tokenizer on the text, we can optionally perform a pre-tokenization step. As mentioned
    earlier, most tokenizers today employ subword tokenization. A common step is to
    first perform word tokenization and then feed the output of it to the subword
    tokenization algorithm. This step is called pre-tokenization.    Pre-tokenization
    is a relatively easy step in English compared to many other languages, since you
    can start with a very strong baseline just by splitting text on whitespace. There
    are outlier decisions to be made, such as how to deal with punctuation, multiple
    spaces, numbers, etc. In Hugging Face the regular expression:    [PRE20]py    is
    used to split on whitespace.    Let’s run the pre-tokenization step of the T5
    tokenizer:    [PRE21]py   [PRE22] [PRE23] ''bat'', ''cat'', ''cap'', ''sap'',
    ''map'', ''fan'' [PRE24] ''b'', ''a'', ''t'', ''c'', ''p'', ''s'', ''m'', ''f'',
    ''n'' [PRE25] ''ba'' - 1, ''at'' - 2, ''ca'' - 2, ''ap'' - 3, ''sa'' - 1, ''ma''
    - 1, ''fa'' - 1, ''an'' - 1 [PRE26] ''b'', ''a'', ''t'', ''c'', ''p'', ''s'',
    ''m'', ''f'', ''n'', ''ap'' [PRE27] ''ba'' - 1, ''at'' - 2, ''cap'' - 1, ''sap''
    - 1, ''map'' - 1, ''fa'' - 1, ''an'' - 1 [PRE28] score = freq(a,b)/(freq(a) *
    freq(b)) [PRE29] from transformers import T5Tokenizer, T5ForConditionalGeneration   tokenizer
    = T5Tokenizer.from_pretrained("google/flan-t5-large") model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large",     device_map="auto")  tokenizer.add_tokens(["CAR-T",
    "antisense", "oligonucleotides"]) model.resize_token_embeddings(len(tokenizer))
    [PRE30]` [PRE31][PRE32][PRE33]`````'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
