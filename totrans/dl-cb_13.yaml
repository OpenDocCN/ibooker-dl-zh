- en: Chapter 13\. Generating Images with Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。使用自动编码器生成图像
- en: In [Chapter 5](ch05.html#text_generation) we explored how we can generate text
    in the style of an existing corpus, whether the works of Shakespeare or code from
    the Python standard library, while in [Chapter 12](ch12.html#image_style) we looked
    at generating images by optimizing the activation of channels in a pretrained
    network. In this chapter we combine those techniques and build on them to generate
    images based on examples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#text_generation)中，我们探讨了如何生成文本，以某个现有语料库的风格为基础，无论是莎士比亚的作品还是Python标准库中的代码，而在[第12章](ch12.html#image_style)中，我们研究了通过优化预训练网络中通道的激活来生成图像。在本章中，我们将结合这些技术并在其基础上生成基于示例的图像。
- en: 'Generating images based on examples is an area of active research where new
    ideas and breakthroughs are reported on a monthly basis. The state-of-the-art
    algorithms, however, are beyond the scope of this book in terms of model complexity,
    training time, and data needed. Instead, we’ll be working in a somewhat restricted
    domain: hand-drawn sketches.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于示例生成图像是一个活跃研究领域，新的想法和突破性进展每月都有报道。然而，目前最先进的算法在模型复杂性、训练时间和所需数据方面超出了本书的范围。相反，我们将在一个相对受限的领域进行工作：手绘草图。
- en: 'We’ll start with looking at Google’s Quick Draw data set. This is the result
    of an online drawing game and contains many hand-drawn pictures. The drawings
    are stored in a vector format, so we’ll convert them to bitmaps. We’ll pick sketches
    with one label: cats.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看谷歌的Quick Draw数据集开始。这是一个在线绘图游戏的结果，包含许多手绘图片。这些绘画以矢量格式存储，因此我们将把它们转换为位图。我们将选择带有一个标签的草图：猫。
- en: Based on these cat sketches, we’ll build an autoencoder model that is capable
    of learning *catness*—it can convert a cat drawing into an internal representation
    and then generate something similar-looking from that internal representation.
    We’ll look at visualizing the performance of this network on our cats first.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些猫的草图，我们将构建一个自动编码器模型，能够学习*猫的特征*——它可以将猫的绘图转换为内部表示，然后从该内部表示生成类似的东西。我们将首先查看这个网络在我们的猫上的性能可视化。
- en: We’ll then switch to a dataset of hand-drawn digits and then move on to *variational
    autoencoders*. These networks produce dense spaces that are an abstract representation
    of their inputs from which we can sample. Each sample will result in a realistic
    looking image. We can even interpolate between points and see how the images gradually
    change.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将切换到手绘数字的数据集，然后转向*变分自动编码器*。这些网络生成密集空间，是它们输入的抽象表示，我们可以从中进行采样。每个样本将产生一个看起来逼真的图像。我们甚至可以在点之间进行插值，看看图像是如何逐渐变化的。
- en: Finally, we’ll look at *conditional variational autoencoders*, which take into
    account a label when training and therefore can reproduce images of a certain
    class in a random fashion.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将看看*条件变分自动编码器*，在训练时考虑标签，因此可以以随机方式再现某一类别的图像。
- en: 'Code related to this chapter can be found in the following notebooks:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章相关的代码可以在以下笔记本中找到：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 13.1 Importing Drawings from Google Quick Draw
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.1 从Google Quick Draw导入绘图
- en: Problem
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Where can you get a set of everyday hand drawn images?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你在哪里可以获得一组日常手绘图像？
- en: Solution
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Google Quick Draw’s dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用谷歌Quick Draw的数据集。
- en: '[Google Quick Draw](https://quickdraw.withgoogle.com/) is an online game where
    a user is challenged to draw something and see if an AI can guess what they were
    trying to create. The game is entertaining, and as a side effect a large database
    of labeled drawings is produced. Google has made this dataset accessible for anybody
    wanting to play with machine learning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[Google Quick Draw](https://quickdraw.withgoogle.com/)是一个在线游戏，用户被挑战绘制某物，并查看AI是否能猜出他们试图创建的内容。这个游戏很有趣，作为一个副产品，产生了一个大量带标签的绘画数据库。谷歌已经让任何想玩机器学习的人都可以访问这个数据集。'
- en: 'The data is available in [a number of formats](https://github.com/googlecreativelab/quickdraw-dataset).
    We’ll work with a binary-encoded version of the simplified vector drawings. Let’s
    start by getting all the cats:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可在[多种格式](https://github.com/googlecreativelab/quickdraw-dataset)中获得。我们将使用简化的矢量绘图的二进制编码版本。让我们开始获取所有的猫：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ll collect the images by unpacking them one by one. They are stored in a
    binary vector format that we’ll draw on an empty bitmap. The drawings start with
    a 15-byte header, so we just keep processing until our file no longer has at least
    15 bytes:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个解压这些图像。它们以二进制矢量格式存储，我们将在一个空位图上绘制。这些绘画以15字节的头部开始，因此我们只需继续处理，直到我们的文件不再至少有15字节为止：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A drawing is a list of strokes, each made up of a series of *x* and *y* coordinates.
    The *x* and *y* coordinates are stored separately, so we need to zip them into
    a list to feed into the `ImageDraw` object we just created:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅图是一系列笔画的列表，每个笔画由一系列*x*和*y*坐标组成。*x*和*y*坐标分开存储，因此我们需要将它们压缩成一个列表，以便输入到我们刚刚创建的`ImageDraw`对象中：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Over a hundred thousand drawings of cats are yours.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 超过十万幅猫的绘画属于你。
- en: Discussion
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Harvesting user-generated data using a game is an interesting way to build up
    a dataset for machine learning. It’s not the first time Google has used this technique—a
    few years ago it ran the [Google Image Labeler game](http://bit.ly/wiki-gil),
    where two players that didn’t know each other would label images and get points
    for matching labels. The results of that game were never made available to the
    public, though.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过游戏收集用户生成的数据是建立机器学习数据集的一种有趣方式。这不是谷歌第一次使用这种技术——几年前，它运行了[Google Image Labeler游戏](http://bit.ly/wiki-gil)，两个不相识的玩家会为图像打标签，并根据匹配的标签获得积分。然而，该游戏的结果从未向公众公开。
- en: There are 345 categories in the dataset. In this chapter we’re only using cats,
    but you could take the rest for a spin to build an image classifier. The dataset
    has drawbacks, chief among them the fact that not all drawings are finished; the
    game ends when the AI recognizes the drawing, and for a camel drawing two humps
    might be enough.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有345个类别。在本章中，我们只使用了猫，但您可以尝试其余的类别来构建图像分类器。数据集存在缺点，其中最主要的是并非所有的绘画都是完成的；当AI识别出绘画时，游戏就结束了，对于一幅骆驼画来说，画两个驼峰可能就足够了。
- en: Note
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this recipe we rasterized the images ourselves. Google does make a `numpy`
    array version of the data available where the images have been pre-rasterized
    to 28×28 pixels.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们自己对图像进行了光栅化处理。Google确实提供了一个`numpy`数组版本的数据，其中图像已经被预先光栅化为28×28像素。
- en: 13.2 Creating an Autoencoder for Images
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.2 创建图像的自动编码器
- en: Problem
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Is it possible to automatically represent an image as a fixed-sized vector even
    if it isn’t labeled?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有标签，是否可以自动将图像表示为固定大小的向量？
- en: Solution
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use an autoencoder.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动编码器。
- en: In [Chapter 9](ch09.html#transfer_learning) we saw that we can use a convolutional
    network to classify an image by having consecutive layers go from pixels to local
    features to more structural features and finally to an abstract representation
    of the image that we then can use to predict what the image is about. In [Chapter 10](ch10.html#image_search)
    we interpreted that abstract representation of the image as a vector in a high-dimensional,
    semantic space and used the fact that vectors that are close to each other represent
    similar images as a way to build a reverse image search engine. Finally, in [Chapter 12](ch12.html#image_style)
    we saw that we can visualize what the activations of the various neurons on different
    levels in a convolutional network mean.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html#transfer_learning)中，我们看到我们可以使用卷积网络通过连续的层从像素到局部特征再到更多结构特征最终到图像的抽象表示来对图像进行分类，然后我们可以使用这个抽象表示来预测图像的内容。在[第10章](ch10.html#image_search)中，我们将图像的抽象表示解释为高维语义空间中的向量，并利用接近的向量表示相似的图像作为构建反向图像搜索引擎的一种方式。最后，在[第12章](ch12.html#image_style)中，我们看到我们可以可视化卷积网络中不同层级的各种神经元的激活意味着什么。
- en: To do all this we needed the images to be labeled. Only because the network
    got to see a large number of dogs, cats, and many other things was it able to
    learn an abstract representation of these in this high-dimensional space. What
    if we don’t have labels for our images? Or not enough labels to let the network
    develop an intuition of what is what? In these situations autoencoders can be
    helpful.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要对图像进行标记。只有因为网络看到了大量的狗、猫和许多其他东西，它才能在这个高维空间中学习这些东西的抽象表示。如果我们的图像没有标签怎么办？或者标签不足以让网络形成对事物的直觉？在这些情况下，自动编码器可以提供帮助。
- en: The idea behind an autoencoder is to force the network to represent an image
    as a vector with a certain size and have a loss function based on how accurately
    the network is able to reproduce the input image from that representation. The
    input and the expected output are the same, which means we don’t need labeled
    images. Any set of images will do.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器背后的想法是强制网络将图像表示为具有特定大小的向量，并且基于网络能够从该表示中准确复制输入图像的损失函数。输入和期望输出是相同的，这意味着我们不需要标记的图像。任何一组图像都可以。
- en: 'The structure of the network is very similar to what we’ve seen before; we
    take the original image and use a series of convolutional layers and pooling layers
    to reduce the size and increase the depth until we have a one-dimensional vector
    that is an abstract representation of that image. But instead of calling it a
    day and using that vector to predict what the image is, we follow this up with
    the inverse and go from this abstract representation of the image through a set
    of *upsampling* layers that do the reverse until we are back with an image again.
    As our loss function we then take the difference between the input and the output
    image:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的结构与我们之前看到的非常相似；我们取原始图像并使用一系列卷积层和池化层来减小大小并增加深度，直到我们有一个一维向量，这是该图像的抽象表示。但是，我们不会就此罢手并使用该向量来预测图像是什么，而是跟进并通过一组*上采样*层从图像的抽象表示开始，进行相反操作，直到我们再次得到一个图像。作为我们的损失函数，我们然后取输入和输出图像之间的差异：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can imagine the network architecture as an hourglass. The top and bottom
    layers represent images. The smallest point in the network is in the middle, and
    is often referred to as the *latent representation*. We have a latent space with
    128 entries here, which means that we force the network to represent each 32×32-pixel
    image using 128 floats. The only way the network can minimize the difference between
    the input and the output image is by compressing as much information into the
    latent representation as possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将网络架构想象成一个沙漏。顶部和底部层代表图像。网络中最小的点位于中间，并经常被称为*潜在表示*。我们在这里有一个具有128个条目的潜在空间，这意味着我们强制网络使用128个浮点数来表示每个32×32像素的图像。网络能够最小化输入和输出图像之间的差异的唯一方法是尽可能多地将信息压缩到潜在表示中。
- en: 'We can train the network as before with:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像以前一样训练网络：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This should converge fairly rapidly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该相当快地收敛。
- en: Discussion
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Autoencoders are an interesting type of neural network since they are capable
    of learning a compact, lossy representation of their inputs without any supervision.
    In this recipe we’ve used them on images, but they’ve also successfully been deployed
    to process text or other data in the form of time series.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种有趣的神经网络类型，因为它们能够在没有任何监督的情况下学习其输入的紧凑、有损表示。在这个示例中，我们已经将它们用于图像，但它们也成功地被部署来处理文本或其他形式的时间序列数据。
- en: There are a number of interesting extensions to the autoencoder idea. One of
    them is the *denoising* autoencoder. The idea here is to ask the network to predict
    the target image not from itself, but from a damaged version of itself. For example,
    we could add some random noise to the input images. The loss function would still
    compare the output of the network with the original (non-noised) input, so the
    network would effectively learn how to remove noise from the pictures. In other
    experiments this technique has proven to be useful when it comes to restoring
    colors to black and white pictures.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We used the abstract representation of an image in [Chapter 10](ch10.html#image_search)
    to create a reverse image search engine, but we needed labels for that. With an
    autoencoder we don’t need those labels; we can measure the distance between images
    after the model has trained on nothing but a set of images. It turns out that
    if we use a denoising autoencoder the performance of our image similarity algorithm
    increases. The intuition here is that the noise tells the network what not to
    pay attention to, similarly to how data augmentation works (see [“Preprocessing
    of Images”](ch01.html#preprocessing-of-images)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 13.3 Visualizing Autoencoder Results
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to get an idea of how well your autoencoder worked.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sample a few random cat pictures from the input and have the model predict those;
    then render input and output as two rows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s predict some cats:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And show them in our notebook:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Cats in a row](assets/dlcb_13in01.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: As you can see, the network did pick up on the basic shapes, but doesn’t seem
    to be very sure about itself, which results in vague icon drawings, almost like
    shadows.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe we’ll see if we can do better.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the input and the output of the autoencoder *should* be similar, the best
    way to check the performance of our network is to just pick some random icons
    from our validation set and ask the network to reconstruct them. Using PIL to
    create an image that shows two rows and display it inside of the Jupyter notebook
    is something we’ve seen before.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: One of the issues with the approach here is that the loss function we are using
    causes the network to smudge its output. The input drawings contain thin lines,
    but the output of our model doesn’t. Our model has no incentive to predict sharp
    lines, because it is uncertain of the exact position of the lines, so it would
    rather spread its bets and draw vague lines. This way there is a high chance that
    at least some pixels will be a hit. To improve this, we could try to design a
    loss function that forces the network to limit the number of pixels it draws,
    or puts a premium on sharp lines.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Sampling Images from a Correct Distribution
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you make sure that every point in the vector represents a reasonable
    image?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a *variational* autoencoder.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are quite interesting as a way to represent an image as a vector
    that is much smaller than the image itself. But the space of these vectors is
    not *dense*; that is, every image has a vector in that space, but not every vector
    in that space represents a reasonable image. The decoder part of the autoencoder
    will of course create an image out of any vector, but most of them are just not
    going to be recognizable. Variational autoencoders do have this property.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'In this and the following recipes in the chapter we’ll work with the MNIST
    dataset of handwritten digits, comprised of 60,000 training samples and 10,000
    test samples. The approach described here does work on icons, but it complicates
    the model and for decent performance we’d need more icons than we have. If you
    are interested, there is a working model in the notebook directory. Let’s start
    by loading the data:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The key idea behind a variational autoencoder is to add a term to the loss function
    that represents the difference in statistical distribution between the images
    and the abstract representations. For this we’ll use the Kullback–Leibler divergence.
    We can think of this as a distance metric for the space of probability distributions,
    even though it is technically not a distance metric. The [Wikipedia article](http://bit.ly/k-l-d)
    has the details for those who want to read up on the math.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器背后的关键思想是在损失函数中添加一个项，表示图像和抽象表示之间的统计分布差异。为此，我们将使用Kullback-Leibler散度。我们可以将其视为概率分布空间的距离度量，尽管从技术上讲它不是距离度量。[维基百科文章](http://bit.ly/k-l-d)中有详细信息，供那些想要了解数学知识的人阅读。
- en: 'The basic outline of our model is similar to that of the previous recipe. We
    start out with an input representing our pixels, force this through some hidden
    layers, and sample it down to a very small representation. We then work our way
    up again until we have our pixels back:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的基本轮廓与上一个示例类似。我们从代表我们像素的输入开始，将其通过一些隐藏层，然后将其采样到一个非常小的表示。然后我们再次逐步提升，直到我们恢复我们的像素：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The interesting part here is the `z` tensor and the `Lambda` it gets assigned
    to. This tensor will hold the latent representation of our image, and the `Lambda`
    uses the `sample_z` method to do the sampling:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有趣的部分是`z`张量和它被分配给的`Lambda`。这个张量将保存我们图像的潜在表示，而`Lambda`使用`sample_z`方法进行采样：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is where we randomly sample points with a normal distribution using the
    two variables `z_mean` and `z_log_var`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们使用两个变量`z_mean`和`z_log_var`从正态分布中随机采样点的地方。
- en: 'Now on to our loss function. The first component is the reconstruction loss,
    which measures the difference between the input pixels and the output pixels:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看我们的损失函数。第一个组件是重构损失，它衡量了输入像素和输出像素之间的差异：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The second thing we need is a component in our loss function that uses the
    Kullback–Leibler divergence to steer the distribution in the right direction:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的第二件事是在我们的损失函数中添加一个使用Kullback-Leibler散度来引导分布走向正确方向的组件：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then simply add this up:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们简单地将其相加：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And we can compile our model with:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下方式编译我们的模型：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This will handily also keep track of the individual components of the loss during
    training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这也会方便地跟踪训练过程中损失的各个组件。
- en: 'This model is slightly complicated due to the extra loss function and the out-of-band
    call to `sample_z`; to get a look at the details, it is best viewed in the corresponding
    notebook. We can now train the model as before:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于额外的损失函数和对`sample_z`的带外调用，这个模型稍微复杂；要查看详细信息，最好在相应的笔记本中查看。现在我们可以像以前一样训练模型：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the training is finished, we want to use the results by feeding a random
    point in the latent space and seeing what image rolls out. We can do this by creating
    a second model that has as an input the middle layer of our `auto_encoder` model
    and as output our target image:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们希望通过在潜在空间中提供一个随机点并查看出现的图像来使用结果。我们可以通过创建一个第二个模型，其输入是我们`auto_encoder`模型的中间层，输出是我们的目标图像来实现这一点：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now generate a random input and then convert it to a picture:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成一个随机输入，然后将其转换为一幅图片：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Randomly generated digit](assets/dlcb_13in02.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![随机生成的数字](assets/dlcb_13in02.png)'
- en: Discussion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Variational autoencoders add an important component to autoencoders when it
    comes to generating images rather than just reproducing images; by making sure
    that the abstract representations of the images come from a *dense* space where
    points close to the origin map to likely images, we can now generate images that
    have the same likelihood distribution as our inputs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及生成图像而不仅仅是复制图像时，变分自动编码器为自动编码器添加了一个重要组件；通过确保图像的抽象表示来自一个“密集”空间，其中靠近原点的点映射到可能的图像，我们现在可以生成具有与我们输入相同的可能性分布的图像。
- en: The underlying mathematics are a bit beyond the scope of this book. The intuition
    here is that some images are more “normal” and some are more unexpected. The latent
    space has the same characteristics, so points that are drawn from close to the
    origin correspond to images that are “normal,” while more extreme points map to
    more unlikely images. Sampling from a normal distribution will result in images
    that have the same mixture of expected and unexpected images as the model saw
    during training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基础数学知识超出了本书的范围。这里的直觉是一些图像更“正常”，一些更意外。潜在空间具有相同的特征，因此从原点附近绘制的点对应于“正常”的图像，而更极端的点则对应于更不太可能的图像。从正态分布中采样将导致生成具有与模型训练期间看到的预期和意外图像混合的图像。
- en: Having dense spaces is nice. It allows us to interpolate between points and
    still get valid outcomes. For example, if we know that one point in the latent
    space maps to a 6 and another to an 8, we would expect that the points in between
    would result in images that morph from 6 to 8\. If we find the same images but
    in a different style, we can look for images in between with a mixed style. Or
    we could even go in the other direction and expect to find a more extreme style.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有密集空间很好。它允许我们在点之间进行插值，并仍然获得有效的结果。例如，如果我们知道潜在空间中的一个点映射到6，另一个点映射到8，我们期望在两者之间的点会产生从6到8的图像。如果我们找到相同的图像但是以不同的风格，我们可以寻找混合风格的中间图像。或者我们甚至可以朝着另一个方向前进，并期望找到更极端的风格。
- en: In [Chapter 3](ch03.html#word_embeddings) we looked at word embeddings, where
    each word has a vector that projects it into a semantic space, and the sorts of
    calculations we can do with those. As interesting as that is, since the space
    is not dense we typically don’t expect to find something between two words that
    somehow is a compromise between the two—no *mule* between *donkey* and *horse*.
    Similarly, we can use a pretrained image recognition network to find a vector
    for a picture of a cat, but the vectors around it don’t all represent variations
    of cats.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.html#word_embeddings)中，我们看过了词嵌入，其中每个单词都有一个将其投影到语义空间的向量，以及我们可以对其进行的计算。尽管这很有趣，但由于空间不是密集的，我们通常不会期望在两个单词之间找到某种折中的东西——在
    *驴* 和 *马* 之间没有 *骡子*。同样，我们可以使用预训练的图像识别网络为一张猫的图片找到一个向量，但其周围的向量并不都代表猫的变化。
- en: 13.5 Visualizing a Variational Autoencoder Space
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.5 可视化变分自动编码器空间
- en: Problem
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How can you visualize the diversity of images that you can generate from your
    latent space?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如何可视化您可以从潜在空间生成的图像的多样性？
- en: Solution
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the two dimensions from the latent space to create a grid of generated images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用潜在空间中的两个维度创建一个生成图像的网格。
- en: 'Visualizing two dimensions from our latent space is straightforward. For higher
    dimensions we could first try t-SNE to get back to two dimensions. As luck would
    have it, we were only using two dimensions in the previous recipe, so we can just
    go through a plane and map each (*x*, *y*) position to a point in the latent space.
    Since we are using a normal distribution, we’d expect reasonable images to appear
    in the [–1.5, 1.5] range:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的潜在空间中可视化两个维度是直接的。对于更高的维度，我们可以首先尝试 t-SNE 将其降至两个维度。幸运的是，在前一个示例中我们只使用了两个维度，所以我们可以通过一个平面并将每个
    (*x*, *y*) 位置映射到潜在空间中的一个点。由于我们使用正态分布，我们期望合理的图像出现在 [-1.5, 1.5] 范围内：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This should get us a nice image of the different digits the network learned:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供网络学习的不同数字的漂亮图像：
- en: '![Randomly generated grid](assets/dlcb_13in03.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![随机生成的网格](assets/dlcb_13in03.png)'
- en: Discussion
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: By mapping (*x*, *y*) to our latent space and decoding the results to images
    we get a nice overview of what our space contains. As we can see, the space is
    indeed quite dense. Not all points result in digits per se; some, as expected,
    represent in-between forms. But the model does find a way to distribute the digits
    in a natural way on the grid.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 (*x*, *y*) 映射到我们的潜在空间并将结果解码为图像，我们可以很好地了解我们的空间包含的内容。正如我们所看到的，空间确实相当密集。并非所有点都会产生数字；一些点，如预期的那样，代表中间形式。但模型确实找到了一种在网格上以自然方式分布数字的方法。
- en: The other thing to note here is that our variational autoencoder does a great
    job of compressing images. Every input image is represented in the latent space
    by just 2 floats, while their pixel representations use 28 × 28 = 784 floats.
    That’s a compression ratio of almost 400, outperforming JPEG by quite a margin.
    Of course, the compression is rather lossy—a handwritten 5 will after encoding
    and decoding still look like a handwritten 5 and still be in the same style, but
    at a pixel level there is no real correspondence. Also, this form of compression
    is extremely domain-specific. It only works for handwritten digits, while JPEG
    can be used to compress all sorts of images and photos.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要注意的另一件事是，我们的变分自动编码器在压缩图像方面表现出色。每个输入图像仅由 2 个浮点数在潜在空间中表示，而它们的像素表示使用 28 × 28
    = 784 个浮点数。这是接近 400 的压缩比，远远超过了 JPEG。当然，这种压缩是有损的——一个手写的 5 在编码和解码后仍然看起来像一个手写的 5，仍然是同一风格，但在像素级别上没有真正的对应。此外，这种形式的压缩是极其领域特定的。它只适用于手写数字，而
    JPEG 可用于压缩各种图像和照片。
- en: 13.6 Conditional Variational Autoencoders
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.6 条件变分自动编码器
- en: Problem
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do we generate images of a certain type rather than completely random ones?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如何生成特定类型的图像而不是完全随机的图像？
- en: Solution
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a conditional variational autoencoder.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用条件变分自动编码器。
- en: The autoencoder from the previous two recipes does a great job generating random
    digits and is also capable of taking in a digit and encoding it in a nice, dense,
    latent space. But it doesn’t know a 5 from a 3, and so the only way we can get
    it to generate a random 3 is to first find all the 3s in the latent space and
    then sample from that subspace. Conditional variational autoencoders help here
    by taking in the label as an input and then concatenating the label to the latent
    space vector `z` in the model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个示例中的自动编码器很好地生成了随机数字，还能够接收一个数字并将其编码为一个漂亮、密集的潜在空间。但它无法区分 5 和 3，所以我们唯一能让它生成一个随机的
    3 的方法是首先找到潜在空间中的所有 3，然后从该子空间中进行采样。条件变分自动编码器通过将标签作为输入并将标签连接到模型中的潜在空间向量 `z` 来帮助这里。
- en: 'This does two things. First, it lets the model take the actual label into account
    when learning the encoding. Second, since it adds the label to the latent space,
    our decoder will now take both a point in the latent space and a label, which
    allows us to explicitly ask for a specific digit to be generated. The model now
    looks like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做有两个作用。首先，它让模型在学习编码时考虑实际标签。其次，由于它将标签添加到潜在空间中，我们的解码器现在将同时接收潜在空间中的一个点和一个标签，这使我们能够明确要求生成特定的数字。模型现在看起来像这样：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We train the model by providing it with both the images and the labels:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向模型提供图像和标签来训练模型：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can now generate an explicit number 4:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成一个明确的数字 4：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Digit four](assets/dlcb_13in04.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![数字四](assets/dlcb_13in04.png)'
- en: 'Since we specify which digit to generate in a one-hot encoding, we can also
    ask for something in between two numbers:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们指定了要生成的数字的一位有效编码，我们也可以要求在两个数字之间生成某些东西：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Which produces indeed something in between:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实产生了介于两者之间的东西：
- en: '![Digit eight or three](assets/dlcb_13in05.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![数字八或三](assets/dlcb_13in05.png)'
- en: 'Another interesting thing to try is to put the digits on the *y*-axis and use
    the *x*-axis to pick values for one of our latent dimensions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的尝试是将数字放在 *y* 轴上，使用 *x* 轴来选择我们的潜在维度之一的值：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Style and digits](assets/dlcb_13in06.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![风格和数字](assets/dlcb_13in06.png)'
- en: As you can see, the latent space expresses the style of the digit and the style
    is consistent across digits. In this case it seems that it controls how much the
    digit is slanted.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，潜在空间表达了数字的风格，而这种风格在数字之间是一致的。在这种情况下，它似乎控制了数字倾斜的程度。
- en: Discussion
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The conditional variational autoencoder marks the final stop on our journey
    through the various autoencoders. This type of network enables us to map our digits
    to a dense latent space that is also labeled, allowing us to sample random images
    while specifying what type they should be.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 条件变分自动编码器标志着我们穿越各种自动编码器的旅程的最终站。这种类型的网络使我们能够将我们的数字映射到一个稠密的潜在空间，该空间也带有标签，使我们能够在指定图像类型的同时对随机图像进行采样。
- en: A side effect of providing the labels to the network is that it now no longer
    has to learn the numbers, but can focus on the style of the numbers.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 向网络提供标签的副作用是，它现在不再需要学习数字，而是可以专注于数字的风格。
