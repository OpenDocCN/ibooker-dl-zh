- en: Chapter 6\. Making Sentiment Programmable by Using Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you saw how to take words and encode them into tokens. Then, you saw how to encode
    sentences full of words into sequences full of tokens, padding or truncating them
    as appropriate to end up with a well-shaped set of data that you can use to train
    a neural network. However, in none of that was there any type of modeling of the
    *meaning* of a word. And while it’s true that there’s no absolute numeric encoding
    that could encapsulate meaning, there are relative ones.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn about techniques to encapsulate meaning, and in
    particular, the concept of *embeddings*, in which vectors in high-dimensional
    space are created to represent words. The directions of these vectors can be learned
    over time, based on the use of the words in the corpus. Then, when you’re given
    a sentence, you can investigate the directions of the word vectors, sum them up,
    and from the overall direction of the summation, establish the sentiment of the
    sentence as a product of its words. Also, related to this, as the model scans
    the sentences, the positioning of the words in the sentence can also help train
    an appropriate embedding.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll also explore how that works. Using the News Headlines
    Dataset for Sarcasm Detection dataset from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you’ll build embeddings to help a model detect sarcasm in a sentence. You’ll also
    work with some cool visualization tools that help you understand how words in
    a corpus get mapped to vectors so you can see which words determine the overall
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing Meaning from Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get into the higher-dimensional vectors for embeddings, let’s use
    some simple examples to try to visualize how meaning can be derived from numerics.
    Consider this: using the sarcasm dataset from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    what would happen if you encoded all of the words that make up sarcastic headlines
    with positive numbers and those that make up realistic headlines with negative
    numbers?'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Simple Example: Positives and Negatives'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take, for example, this sarcastic headline from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming that all words in our vocabulary start with a value of 0, we could
    add 1 to the value for each of the words in this sentence, and we would end up
    with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This isn’t the same as the *tokenization* of words that you did in the last
    chapter. You could consider replacing each word (e.g., *christian*) with the token
    representing it that is encoded from the corpus, but I’ll leave the words in for
    now to make the code easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the next step, consider an ordinary headline (not a sarcastic one),
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Because this is a different sentiment, we could instead subtract 1 from the
    current value of each word, so our value set would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the sarcastic `bale` (from `christian bale`) has been offset by the
    nonsarcastic `bale` (from `gareth bale`), so its score ends up as 0\. Repeat this
    process thousands of times and you’ll end up with a huge list of words from your
    corpus that are scored based on their usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, imagine we want to establish the sentiment of this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using our existing value set, we could look at the scores of each word and add
    them up. We would get a score of 2, indicating (because it’s a positive number)
    that this is a sarcastic sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For what it’s worth, the word *bale* is used five times in the Sarcasm dataset,
    twice in a normal headline and three times in a sarcastic one. So, in a model
    like this, the word *bale* would be scored –1 across the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going a Little Deeper: Vectors'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hopefully, the previous example has helped you understand the mental model of
    establishing some form of *relative* meaning for a word, through its association
    with other words in the same “direction.” In our case, while the computer doesn’t
    understand the meanings of individual words, it can move labeled words from a
    known sarcastic headline in one direction (by adding 1) and move labeled words
    from a known normal headline in another direction (by subtracting 1). This gives
    us a basic understanding of the meaning of the words, but it does lose some nuance.
  prefs: []
  type: TYPE_NORMAL
- en: But what if we increased the dimensionality of the direction to try to capture
    some more information? For example, suppose we were to look at characters from
    the Jane Austen novel *Pride and Prejudice*, considering the dimensions of gender
    and nobility. We could plot the former on the *x*-axis and the latter on the *y*-axis,
    with the length of the vector denoting each character’s wealth (see [Figure 6-1](#ch06_clean_figure_1_1748752380714921)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Characters in Pride and Prejudice as vectors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From an inspection of the graph, you can derive a fair amount of information
    about each character. Three of them are male. Mr. Darcy is extremely wealthy,
    but his nobility isn’t clear (he’s called “Mister,” unlike the less wealthy but
    apparently more noble Sir William Lucas). The other “Mister,” Mr. Bennet, is clearly
    not nobility and is struggling financially. Elizabeth Bennet, his daughter, is
    similar to him but female. Lady Catherine, the other female character in our example,
    is noble and incredibly wealthy. The romance between Mr. Darcy and Elizabeth causes
    tension—with *prejudice* coming from the noble side of the vectors toward the
    less-noble.
  prefs: []
  type: TYPE_NORMAL
- en: As this example shows, by considering multiple dimensions, we can begin to see
    real meaning in the words (which are character names here). Again, we’re not talking
    about concrete definitions but more about a *relative* meaning based on the axes
    and the relationship between the vector for one word and the other vectors.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to the concept of an *embedding*, which is simply a vector representation
    of a word that is learned while training a neural network. We’ll explore that
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much like you’ve seen with `Linear` and `Conv2D`, PyTorch implements embeddings
    by using a layer. This creates a lookup table that maps from an integer to an
    embedding table, the contents of which are the coefficients of the vector representing
    the word identified by that integer. So, in the *Pride and Prejudice* example
    from the previous section, the *x* and *y* coordinates would give us the embeddings
    for a particular character from the book. Of course, in a real NLP problem, we’ll
    use far more than two dimensions. Thus, the direction of a vector in the vector
    space could be seen as encoding the “meaning” of a word, and words with similar
    vectors (i.e., pointing in roughly the same direction) could be considered related
    to that word.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer will be initialized randomly—that is, the coordinates of
    the vectors will be completely random to start with and will be learned during
    training by using backpropagation. When training is complete, the embeddings will
    roughly encode similarities between words, allowing us to identify words that
    are somewhat similar based on the direction of the vectors for those words.
  prefs: []
  type: TYPE_NORMAL
- en: This is all quite abstract, so I think the best way to understand how to use
    embeddings is to roll up your sleeves and give them a try. Let’s start with a
    sarcasm detector using the Sarcasm dataset from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759).
  prefs: []
  type: TYPE_NORMAL
- en: Building a Sarcasm Detector by Using Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759),
    you loaded and did some preprocessing on a JSON dataset called the News Headlines
    Dataset for Sarcasm Detection (the sarcasm dataset, for short). By the time you
    were done, you had lists of training and testing data and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For the training data, you created a `build_vocab` helper function to create
    a dictionary of the frequency of each word, sorted in order of the most frequent.
    The size of this dictionary is the `vocab_size`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an embedding layer in PyTorch, you can use the `nn.Embedding` layer
    type, like this, by specifying the desired vocab size and the number of embedding
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will initialize a vector with `embedding_dim` axes for each word. So, for
    example, if `embedding_dim` is `16`, then every word in the vocabulary will be
    assigned a 16-dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, the attributes for each token (encoded as values for the vector in
    each of its dimensions) will be learned through backpropagation as the network
    learns by matching the training data to its labels.
  prefs: []
  type: TYPE_NORMAL
- en: An important next step is feeding the output of the embedding layer into a dense
    layer. The easiest way to do this, similar to how you would when using a convolutional
    neural network, is to use pooling. In this instance, the dimensions of the embeddings
    are averaged out to produce a fixed-length output vector, and `Adaptive​A⁠ve​Pool1d(1)`
    reduces the input along the length of the sequence to a fixed vector size of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider this model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, an embedding layer is defined, and it’s given the vocab size and an embedding
    dimension. Let’s take a look at the number of trainable parameters in the network,
    using `torchinfo.summary`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The vocabulary size is 24,292 words, and as the embedding has 100 dimensions,
    the total number of trainable parameters in the embedding layer will be 2,429,200\.
    The first linear layer has 100 values in with 24 values out, so that’s a total
    of 2,400 weights, but each of the neurons also has a bias, so add 24 to get to
    24, 24\.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the last linear has 24 values in, with just a single neuron out.
    For a total of 24 parameters, plus one for the bias, this equals 25\. The entire
    network has 2,431,649 parameters to learn. Note that the average pooling layer
    has 0 trainable parameters, as it’s just averaging the parameters in the embedding
    layer before it to get a single 16-value vector.
  prefs: []
  type: TYPE_NORMAL
- en: If we train this model, we’ll get a pretty decent training accuracy of 99%+
    after 30 epochs—but our validation accuracy will be below 80% (see [Figure 6-2](#ch06_clean_figure_2_1748752380714952)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Training accuracy versus validation accuracy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That might seem to be a reasonable curve, given that the validation data likely
    contains many words that aren’t present in the training data. However, if you
    examine the loss curves for training versus validation over one hundred epochs,
    you’ll see a problem. Although you would expect to see that the training accuracy
    is higher than the validation accuracy, a clear indicator of overfitting is that
    while the validation accuracy is dropping a little over time (as shown in [Figure 6-2](#ch06_clean_figure_2_1748752380714952)),
    its loss is increasing sharply, as shown in [Figure 6-3](#ch06_clean_figure_3_1748752380714977).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Training loss versus validation loss
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overfitting like this is common with NLP models, due to the somewhat unpredictable
    nature of language. In the next sections, we’ll look at how to reduce this effect
    by using a number of techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing Overfitting in Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting happens when the network becomes overspecialized to the training
    data, and one part of this involves the network becoming very good at matching
    patterns in “noisy” data in the training set that doesn’t exist anywhere else.
    Because this particular noise isn’t present in the validation set, the better
    the network gets at matching it, the worse the loss of the validation set will
    be. This can result in the escalating loss that you saw in [Figure 6-3](#ch06_clean_figure_3_1748752380714977).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll explore several ways to generalize the model and reduce
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the learning rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A hyperparameter of the optimizer is the learning rate (LR). The details of
    this parameter are beyond the scope of this chapter, but consider it to be a value
    that if too high will cause the network to potentially learn too quickly and miss
    nuance. The flipside is also true—if you set it too low, your network may not
    learn effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the biggest factor that can lead to overfitting is whether the LR of
    your optimizer is too high. If it is, then your network learns *too quickly*.
    For this example, the code to define the optimizer was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the defaults for the `Adam` optimizer. One thing to experiment with
    is the `learning rate` parameter (`lr`), and in the following code, you’ll see
    the results of an instance when I reduced by an order of 10 to 0.0001, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `betas` values stay at their defaults, as does `amsgrad`. Also note that
    both `beta` values must be between 0 and 1, and typically, both are close to 1\.
    Amsgrad is an alternative implementation of the Adam optimizer that was introduced
    in the paper [“On the Convergence of Adam and Beyond” by Sashank Reddi, Satyen
    Kale, and Sanjiv Kumar](https://oreil.ly/FhTDi).
  prefs: []
  type: TYPE_NORMAL
- en: This much lower LR has a profound impact on the network. [Figure 6-4](#ch06_clean_figure_4_1748752380714997)
    shows the accuracy of the network over one hundred epochs. The lower LR can be
    seen in the first 10 epochs or so, where it appears that the network isn’t learning,
    before it “breaks out” and starts to learn quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the loss (as illustrated in [Figure 6-5](#ch06_clean_figure_5_1748752380715014)),
    we can see that even while the accuracy wasn’t going up for the first few epochs,
    the loss was going down. You could therefore be confident that the network would
    eventually start to learn, if you were watching it epoch by epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Accuracy with a lower LR
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/aiml_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Loss with a lower LR
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: And while the loss does start to show the same curve of overfitting that you
    saw in [Figure 6-3](#ch06_clean_figure_3_1748752380714977), note that it happens
    much later and at a much lower rate. By epoch 30, the loss is at about 0.49, whereas
    with the higher LR in [Figure 6-3](#ch06_clean_figure_3_1748752380714977), it
    was more than double that amount. And while it takes the network longer to get
    to a good accuracy rate, it does so with less loss, so you can be more confident
    in the results. With these hyperparameters, the loss on the validation set started
    to increase at about epoch 60, at which point, the training set had 90% accuracy
    and the validation set had about 81% accuracy, showing that we have quite an effective
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it’s easy to just tweak the optimizer and then declare victory, but
    there are a number of other methods you can use to improve your model. You’ll
    learn about those in the next few sections, and for them, I’ve reverted back to
    using the default Adam optimizer so the effects of tweaking the LR won’t hide
    the benefits offered by these other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring vocabulary size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sarcasm dataset deals with words, so if you explore the words in the dataset
    and in particular their frequency, you might get a clue that helps fix the overfitting
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve provided a `word_frequency` helper function that lets you explore the
    frequency of words in the vocabulary. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run it with code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll then see results like this: a dictionary containing the frequency of
    each word, starting with the most frequently used one, and moving on from there.
    Here are the first few words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to plot this, you can iterate through each item in the list and
    make the *x* value the ordinal of where you are (1 for the first item, 2 for the
    second item, etc.). The *y* value will then be a `newlist[item]`, which you can
    plot with `matplotlib`. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in [Figure 6-6](#ch06_clean_figure_6_1748752380715030).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Exploring the frequency of words
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This “hockey stick” curve shows us that very few words are used many times,
    whereas most words are used very few times. But every word is effectively weighted
    equally because every word has an “entry” in the embedding. Given that we have
    a relatively large training set in comparison with the validation set, we’re ending
    up in a situation where there are many words present in the training set that
    aren’t present in the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can zoom in on the data by changing the axis of the plot just before calling
    `plt.show`. For example, to look at the volume of words from 300 to 10,000 on
    the *x*-axis with the scale from 0 to 100 on the *y*-axis, you can use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The result is in [Figure 6-7](#ch06_clean_figure_7_1748752380715045).
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequency of words 300–10,000](assets/aiml_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Frequency of words from 300 to 10,000
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are almost 25,000 words in the corpus, and the code is set up to only
    train for all of them! But if we look at the words in positions 2,000 onward,
    which is over 90% of our vocabulary, we’ll see that they’re each used fewer than
    20 times in the entire corpus!
  prefs: []
  type: TYPE_NORMAL
- en: 'This could explain the overfitting, so the logical next step is to see if we
    can reduce the vocabulary we are training for. Within the `build_vocab` helper
    function, we can add a parameter for the maximum vocab size we’re interested in,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, when building our `word_index`, we can specify a maximum vocab size that
    we’re interested in exploring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The embedding layer was already initialized with the vocab size, so the model
    architecture doesn’t need to change. Indeed, with the reduced vocab size, the
    number of learned parameters drops sharply, giving us a simpler network that learns
    faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The model has shrunk from 2.4 million parameters to only 202,549\.
  prefs: []
  type: TYPE_NORMAL
- en: After retraining and exploring the smaller model, we can see that the results
    have changed.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-8](#ch06_clean_figure_8_1748752380715056) shows the accuracy metrics.
    Now, the training set accuracy is about 82% and the validation accuracy is about
    76%. They’re closer to each other and not diverging, which is a good sign that
    we’ve gotten rid of most of the overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Accuracy with a two thousand–word vocabulary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is somewhat reinforced by the loss plot in [Figure 6-9](#ch06_clean_figure_9_1748752380715067).
    The loss on the validation set is rising but much slower than before, so reducing
    the size of the vocabulary to prevent the training set from overfitting on low-frequency
    words that were possibly only present in the training set appears to have worked.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth experimenting with different vocab sizes, but remember that you can
    also have too small of a vocab size and overfit to that. You’ll need to find a
    balance. In this case, my choice of taking words that appear 20 times or more
    was purely arbitrary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Loss with a two thousand–word vocabulary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Exploring embedding dimensions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this example, I arbitrarily chose an embedding dimension of 16\. In this
    instance, words are encoded as vectors in 16-dimensional space, with their directions
    indicating their overall meaning. But is 16 a good number? With only two thousand
    words in our vocabulary, it might be on the high side, leading to a high degree
    of sparseness of direction.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I believe that the best way to think about sparseness is to project into three
    dimensions. Think of it like the earth, with one thousand vectors pointing from
    the core to a place on the surface. The vectors are in three dimensions, *x*,
    *y*, and *z*. There’s a lot of surface area for them to cover, but if many of
    them are missing *x* and *y*, meaning they’re just zero, a lot of them will be
    pointing to (0, 0, *z*) and a whole lot of the earth’s surface will be untouched!
    Thus, there will be a total lack of distinctiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Research has shown that a best practice for embedding size is to have it be
    the fourth root of the vocabulary size. The fourth root of 2,000 is 6.687, so
    let’s explore what happens if we round this up and change the embedding dimension
    to 7.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the result of training for one hundred epochs in [Figure 6-10](#ch06_clean_figure_10_1748752380715076).
    The training set’s accuracy stabilized at about 83%, and the validation set’s
    accuracy stabilized at about 77%. Despite some jitters, the lines are pretty flat,
    showing that the model has converged. This isn’t much different from the results
    in [Figure 6-8](#ch06_clean_figure_8_1748752380715056), but reducing the embedding
    dimensionality allows the model to train significantly faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. Training versus validation accuracy for seven dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-11](#ch06_clean_figure_11_1748752380715085) shows the loss in training
    and validation. While it initially appeared that the loss was climbing at about
    epoch 20, it soon flattened out. Again, a good sign!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. Training versus validation loss for seven dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that the dimensionality has been reduced, we can do a bit more tweaking
    of the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the model architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the optimizations in the previous sections, the model architecture looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: One thing that comes to mind is the dimensionality—the `GlobalAveragePooling1D`
    layer now emits just 7 dimensions, but they’re being fed into a hidden layer of
    24 neurons, which is overkill. Let’s explore what happens when this is reduced
    to 8 neurons and trained for 100 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the training versus validation accuracy in [Figure 6-12](#ch06_clean_figure_12_1748752380715093).
    When compared to [Figure 6-7](#ch06_clean_figure_7_1748752380715045), where 24
    neurons were used, the overall result is quite similar, but the model was somewhat
    faster to train.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. Reduced dense-architecture accuracy results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The loss curves in [Figure 6-13](#ch06_clean_figure_13_1748752380715102) show
    similar results.
  prefs: []
  type: TYPE_NORMAL
- en: By following these exercises, we were able to reduce the model architecture
    significantly, reducing the number of parameters while improving the quality and
    mitigating overfitting. But there are a few more things we can do—starting with
    dropout.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. Reduced dense architecture loss results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A common technique for reducing overfitting is to add dropout to a dense neural
    network. We explored this for convolutional neural networks back in [Chapter 3](ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912),
    so it’s tempting to go straight to it here to see its effects on overfitting.
    But in this case, I want to wait until the vocabulary size, embedding size, and
    architecture complexity have been addressed. Those changes can often have a much
    larger impact than using dropout, and we’ve already seen some nice results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our architecture has been simplified to have only eight neurons in
    the middle dense layer, the effect of dropout may be minimized—but let’s explore
    it anyway. Here’s the updated code for the model architecture to add a dropout
    of 0.25 (which equates to two of our eight neurons):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 6-14](#ch06_clean_figure_14_1748752380715110) shows the accuracy results
    when trained for one hundred epochs. This time, we see that the training accuracy
    and validation accuracy are converging, with the training accuracy now lower than
    before. Similarly, the loss curves in [Figure 6-15](#fig-6-15) show convergence,
    so while dropout is making our network a little *less* accurate, it appears to
    generalize it better.'
  prefs: []
  type: TYPE_NORMAL
- en: But do exercise caution before declaring victory! A close examination of the
    curves shows that the losses have nicely converged but that they *are* higher
    than previously. The training loss is above 0.5 with dropout but was around 0.3
    without. It is also trending downward, so it’s worth experimenting to see whether
    longer training will produce a better result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-14\. Accuracy with added dropout
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/aiml_0615.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-15\. Loss with added dropout
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also see that the model is heading back to its previous pattern of increasing
    validation loss over time. It’s not nearly as bad as before, but it’s heading
    in the wrong direction.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, when there were very few neurons, introducing dropout probably
    wasn’t the right idea. It’s still good to have this tool in your arsenal, though,
    so be sure to keep it in mind for more sophisticated architectures than this one.
  prefs: []
  type: TYPE_NORMAL
- en: Using regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Regularization* is a technique that helps prevent overfitting by reducing
    the polarization of weights. If the weights on some of the neurons are too heavy,
    regularization effectively punishes them. Broadly speaking, there are two types
    of regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs: []
  type: TYPE_NORMAL
- en: This is often called *least absolute shrinkage* and *selection operator* (*lasso*)
    regularization. It effectively helps us ignore the zero or close-to-zero weights
    when calculating a result in a layer.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs: []
  type: TYPE_NORMAL
- en: This is often called *ridge* regression because it pushes values apart by taking
    their squares. This tends to amplify the differences between nonzero values and
    zero or close-to-zero ones, creating a ridge effect.
  prefs: []
  type: TYPE_NORMAL
- en: The two approaches can also be combined into what is sometimes called *elastic*
    regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'For NLP problems like the one we’re considering, L2 is most commonly used.
    It can be added as the `weight_decay` attribute to the `optimizer.` Here’s an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will apply the `weight_decay` of `0.01.` (Usually, you’ll have a value
    between 0.01 and 0.001 here). Alternatively, a neat trick you can do with PyTorch
    is to define different weight decays for different layers by specifying them within
    the `Adam` declaration call, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The impact of adding regularization in a simple model like this isn’t particularly
    large, but it does smooth out our training loss and validation loss somewhat.
    It might be overkill for this scenario, but as with dropout, it’s a good idea
    to understand how to use regularization to prevent your model from getting overspecialized.
  prefs: []
  type: TYPE_NORMAL
- en: Other optimization considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the modifications we’ve made have given us a much-improved model with
    less overfitting, there are other hyperparameters that you can experiment with.
    For example, we chose to make the maximum sentence length one hundred words, but
    that was purely arbitrary and probably not optimal. It’s a good idea to explore
    the corpus and see what a better sentence length might be. Here’s a snippet of
    code that looks at the sentences and plots the lengths of each one, sorted from
    low to high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: See [Figure 6-16](#ch06_clean_figure_15_1748752380715124) for the results of
    this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0616.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-16\. Exploring sentence length
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Less than 200 sentences in the total corpus of 26,000+ have a length of 100
    words or greater, so by choosing this as the maximum length, we’re introducing
    a lot of padding that isn’t necessary and thus affecting the model’s performance.
    Reducing the maximum to 85 words would still keep 26,000 of the sentences (99%+)
    with greatly reduced padding.
  prefs: []
  type: TYPE_NORMAL
- en: Putting It All Together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking all of the preceding optimizations into effect and retraining the model
    for three hundred epochs gives you the results in [Figure 6-17](#ch06_clean_figure_16_1748752380715140)
    for training and validation accuracies. Given that their curves are roughly matched,
    it shows that we’ve taken huge steps toward avoiding overfitting and that we have
    a network that’s learning effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the training and validation loss curves over three hundred epochs
    are showing remarkable similarity, as depicted in [Figure 6-18](#ch06_clean_figure_17_1748752380715155),
    which indicates that the optimizations are a step in the right direction to prevent
    overfitting for this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0617.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-17\. Optimized training and validation accuracy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/aiml_0618.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-18\. Optimized training and validation loss
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the Model to Classify a Sentence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you’ve created the model, trained it, and optimized it to remove a
    lot of the problems that caused the overfitting, the next step is to run the model
    and inspect its results. To do this, you’ll create an array of new sentences.
    Consider, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then encode these by using the same tokenizer that you used when creating
    the vocabulary for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to use this tokenizer because it has the tokens for the words
    that the network was trained on!
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the print statement will be the sequences for the preceding sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: There are a lot of `1` tokens here (“<OOV>”), because words like *granny* and
    *spiders* don’t appear in the dictionary. The sequences are also shorter because
    the stopwords have been removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, before you can pass the sequences to the model, you’ll need to put them
    in the shape that the model expects—that is, the desired length. You can do this
    with `pad_sequences` in the same way you did when training the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the sentences as sequences of length `85`, so the output for
    the first sequence will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: It was a very short sentence, so it’s padded up to 85 characters with a lot
    of zeros!
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve padded and tokenized the sentences to fit the model’s expectations
    for the input dimensions, it’s time to pass them to the model and get predictions
    back.
  prefs: []
  type: TYPE_NORMAL
- en: 'This involves multiple steps. First, convert the padded sequence into an input
    tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, put the model into evaluation mode to get the predictions, and then simply
    pass the `input_ids` to it to get the outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will be passed back as a list and printed, with high values indicating
    likely sarcasm. Here are the results for our sample sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The high score for the first sentence (“granny starting to fear spiders in the
    garden might be real”), despite it having a lot of stopwords and being padded
    with a lot of zeros, indicates that there is a level of sarcasm there. The other
    two sentences scored much lower, indicating a lower likelihood of sarcasm in them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the probabilities, you can call the `squeeze()` method to retrieve the
    tensor values. And if you want to make a comparison to a threshold to get your
    prediction—for example, above 0.5 indicates sarcasm and below 0.5 indicates no
    sarcasm—then you can use code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on your network tuning, you could also establish what you think the appropriate
    threshold should be. Running this with a 0.5 threshold gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: So, with these test sentences, we’re beginning to get a good indication that
    our network is performing as desired. You should test it with other data to see
    if you can break it, and if you break it consistently, then it’ll be time to try
    a different model architecture, use transfer learning from an existing working
    network, or explore using pretrained embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll learn about this in the next section, but before that, I’d like to show
    you how you can visualize the custom embeddings that this network learned.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To visualize embeddings, you can use an online tool called the [Embedding Projector](http://projector.tensorflow.org).
    It comes preloaded with many existing datasets, but in this section, you’ll see
    how to take the data from the model you’ve just trained and visualize it by using
    this tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, you’ll need a function to reverse the word index. It currently has
    the word as the token and the key as the value, but you need to invert it so you’ll
    have word values to plot on the projector. Here’s the code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll also need to extract the weights of the vectors in the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’ve followed the optimizations in this chapter, the output of this will
    be `(2000,7)` because we used a 2,000 word vocabulary and 7 dimensions for the
    embedding. If you want to explore a word and its vector details, you can do so
    with code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: So, the word *new* is represented by a vector with those seven coefficients
    on its axes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Embedding Projector uses two tab-separated values (TSV) files, one for
    the vector dimensions and one for metadata. This code will generate them for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you are using Google Colab, you can download the TSV files
    with the following code or from the Files pane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the files, you can press the Load button on the projector to visualize
    the embeddings (see [Figure 6-19](#ch06_clean_figure_18_1748752380715170)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0619.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-19\. Using the Embeddings Projector
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also use the vectors and meta TSV files where recommended in the resulting
    dialog and then click Sphereize Data on the projector. This will cause the words
    to be clustered in a sphere and will give you a clear visualization of the binary
    nature of this classifier. It’s only been trained on sarcastic and nonsarcastic
    sentences, so words tend to cluster toward one label or another (see [Figure 6-20](#ch06_clean_figure_19_1748752380715184)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0620.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-20\. Visualizing the sarcasm embeddings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Screenshots don’t do all of this justice—you should try it for yourself! You
    can rotate the center sphere and explore the words on each “pole” to see the impact
    they have on the overall classification, and you can also select words and show
    related ones in the righthand pane. Have a play and experiment!
  prefs: []
  type: TYPE_NORMAL
- en: Using Pretrained Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative to training your own embeddings is to use ones that have been
    pretrained by others on your behalf. There are many sources where you can find
    these, including Kaggle and Hugging Face. You can even find pretrained embeddings
    posted alongside research results. One such set of pretrained embeddings is the
    [Stanford GloVe embeddings](https://oreil.ly/s1YWw), and we’ll explore those here.
  prefs: []
  type: TYPE_NORMAL
- en: Note, however, that when using embeddings that have been pretrained, you should
    also consider updating and changing your tokenizer to match any rules used with
    the pretrained embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: For example, with the GloVE pretrained embeddings—which simply comprise a large
    text file of words with their pretrained embedding in a number of dimensions from
    50 to 300—the rules used to tokenize words are a little different from those for
    the handmade tokenizer we’ve been using for raw data. So, for GloVe, you should
    consider rules such as all of the words being lowercase or numbers being normalized
    to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve done this (I’ve provided code for GloVe in the downloads, and I
    discuss it in a little more detail in the next chapter), then it’s simply a matter
    of loading the weights of the pretrained embeddings to your model definition like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t want to learn from these embeddings and you want to just use them,
    then you should set `freeze_embeddings` to `True`. Otherwise, the network will
    fine-tune by using the pre-loaded embedding weights as a starting point.
  prefs: []
  type: TYPE_NORMAL
- en: This model will rapidly reach peak accuracy in training, and it will not overfit
    as much as we saw previously. The accuracy over three hundred epochs shows that
    training and validation are very much in step with each other (see [Figure 6-22](#ch06_clean_figure_21_1748752380715231)).
    The loss values are also in step, which shows that we are fitting very nicely
    over the first couple of hundred epochs. However, they also begin to diverge (see
    [Figure 6-22](#ch06_clean_figure_21_1748752380715231)).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, it is worth noting that the overall accuracy (at about 70%)
    is quite low, considering that a coin flip would have a 50% chance of getting
    it right! So, while using pretrained embeddings can make for much faster training
    with less overfitting, you should also understand what it is that they’re useful
    for and that they may not always be best for your scenario. You may therefore
    need to explore optimization methods or alternatives where appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0621.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-21\. Accuracy metrics using GloVe embeddings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/aiml_0622.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-22\. Loss metrics using GloVe embeddings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you built your first model that can understand sentiment in
    text. It did this by taking the tokenized text from [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)
    and mapping it to vectors. Then, using backpropagation, it learned the appropriate
    “direction” for each vector based on the label for the sentence containing it.
    Finally, it was able to use all of the vectors for a collection of words to build
    up an idea of the sentiment within the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: You also explored ways to optimize your model to avoid overfitting, and you
    saw a neat visualization of the final vectors representing your words. But while
    this was a nice way to classify sentences, it simply treated each sentence as
    a bunch of words. There was no inherent sequence involved, and the order of appearance
    of words is very important in determining the real meaning of a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it’s a good idea to see if we can improve our models by taking sequence
    into account. We’ll explore that in the next chapter with the introduction of
    a new layer type: a *recurrent* layer, which is the foundation of recurrent neural
    networks.'
  prefs: []
  type: TYPE_NORMAL
