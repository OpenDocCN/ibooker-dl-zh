- en: Chapter 10\. Image Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。图像训练
- en: “Everything must be made as simple as possible. But not simpler.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一切都应该尽可能简单。但不要过于简单。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Albert Einstein
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —阿尔伯特·爱因斯坦
- en: I’m going to describe a number to you. I’d like you to figure out the number
    from the description of its features. The number I’m thinking of has a rounded
    top, a line down the right side only, and a loopy thing at the bottom that overlaps.
    Take a moment and mentally map the number I just described. With those three features,
    you can probably figure it out.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我将向您描述一个数字。我希望您根据其特征的描述来猜出这个数字。我想的数字顶部是圆的，右侧只有一条线，底部有一个重叠的环状物。花点时间，心理上映射我刚刚描述的数字。有了这三个特征，你可能可以猜出来。
- en: The features of a visual number can vary, but a smart description means you
    can identify the numbers in your mind. When I said “rounded top,” you can immediately
    throw out some numbers, and the same goes for “a line down the right side only.”
    The features compose what makes the number unique.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉数字的特征可能会有所不同，但聪明的描述意味着您可以在脑海中识别数字。当我说“圆顶”时，您可能会立即排除一些数字，同样的情况也适用于“只有右侧有一条线”。这些特征组成了数字的独特之处。
- en: If you were to describe the numbers as shown in [Figure 10-1](#number_features)
    and put their descriptions in a CSV file, you could probably get 100% accuracy
    with a trained neural network on that data. The whole thing works except that
    it depends on a human to describe the top, middle, and bottom of each number.
    How do you automate the human aspect of this? If you can make a computer identify
    the loops, colors, and curves that are unique characteristics of an image and
    then feed that into a neural network, the machine could learn the needed patterns
    to classify the descriptions into images.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您按照[图10-1](#number_features)中显示的数字描述，并将其描述放入CSV文件中，您可能可以通过训练好的神经网络在这些数据上获得100%的准确性。整个过程都很顺利，只是依赖于人类描述每个数字的顶部、中部和底部。如何自动化这个人类方面呢？如果您可以让计算机识别图像的独特特征，如环、颜色和曲线，然后将其输入神经网络，机器就可以学习将描述分类为图像所需的模式。
- en: '![The number two in three parts.](assets/ltjs_1001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![数字二的三部分。](assets/ltjs_1001.png)'
- en: Figure 10-1\. Congratulations if you figured out it was the number two
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。如果您发现这是数字二，恭喜您。
- en: Thankfully, this problem of feature engineering images has been solved with
    fantastic computer vision tricks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过出色的计算机视觉技巧，解决了图像特征工程的问题。
- en: 'We will:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将：
- en: Learn the concepts of convolutional layers for models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习模型的卷积层的概念
- en: Build your first multiclass model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建您的第一个多类模型
- en: Learn how to read and process images for TensorFlow.js
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何读取和处理图像以供TensorFlow.js使用
- en: Draw on a canvas and classify the drawing accordingly
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在画布上绘制并相应地对绘图进行分类
- en: When you finish this chapter, you’ll be able to create your own image classification
    models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够创建自己的图像分类模型。
- en: Understanding Convolutions
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积
- en: Convolutions come from the mathematical world of expressing shapes and functions.
    You can dig for a long, long time into the concept of what a convolution is in
    mathematics and then reapply that knowledge from the ground up to the idea of
    gathering information on digital images. If you’re a fan of mathematics and computer
    graphics, this is quite an exciting rabbit hole.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积来自于数学世界中表达形状和函数的概念。您可以深入研究卷积在数学中的概念，然后从头开始将这些知识重新应用到数字图像信息的收集概念。如果您是数学和计算机图形的爱好者，这是一个非常令人兴奋的领域。
- en: However, it’s not essential that you spend a week learning the fundamentals
    of convolutional operations when you have a framework like TensorFlow.js. It is
    for that reason that we’ll focus on the high-level benefits and properties of
    convolutional operations and how they are used in neural networks. As always,
    you are encouraged to research the deep history of convolutions beyond this quick-start.^([1](ch10.html#idm45049239647096))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你有像TensorFlow.js这样的框架时，花费一周学习卷积操作的基础并不是必要的。因此，我们将专注于卷积操作的高级优势和特性，以及它们在神经网络中的应用。始终鼓励您在这个快速入门之外深入研究卷积的深层历史。
- en: Let’s look at the most prominent concept you should take from a nonmathematical
    explanation of convolutions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看您应该从非数学解释的卷积中获得的最重要概念。
- en: The two images of the number two in [Figure 10-2](#compare_nums) are the *exact*
    same number shifted from left to right in their bounding boxes. Each of these
    converted into tensors would create significantly different unequal tensors. However,
    in the feature system we described at the beginning of the chapter, these features
    would be the same.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-2](#compare_nums)中的两个数字二的图像是*完全*相同的数字，只是它们在边界框中从左到右移动。将这两个数字转换为张量后，会创建出两个明显不同的不相等张量。然而，在本章开头描述的特征系统中，这些特征将是相同的。'
- en: '![Convolution Features vs no Features](assets/ltjs_1002.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![卷积特征与无特征](assets/ltjs_1002.png)'
- en: Figure 10-2\. Convolutions simplify the essence of an image
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。卷积简化了图像的本质
- en: For visual tensors, an image’s features are more important than the exact locations
    of each pixel.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视觉张量，图像的特征比每个像素的确切位置更重要。
- en: Convolutions Quick Summary
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积快速总结
- en: A convolutional operation is used to extract high-level features such as edges,
    gradients, colors, and more. These are the key features that classify a given
    visual.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作用于提取高级特征，如边缘、梯度、颜色等。这些是分类给定视觉的关键特征。
- en: So what features should it extract? That’s not something we actually decide.
    You can control the number of filters used in finding features, but the actual
    features that best define usable patterns are defined in the training process.
    The filters start with accenting and drawing out features from the image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么应该提取哪些特征呢？这并不是我们实际决定的。您可以控制在查找特征时使用的滤波器数量，但最好定义可用模式的实际特征是在训练过程中定义的。这些滤波器从图像中突出和提取特征。
- en: For instance, take a look at the photo in [Figure 10-3](#pumpkin). The jack-o’-lantern
    is multiple colors and barely contrasts against a blurry but somewhat light and
    dark background. As a human, you have no issue identifying what this is in the
    photo.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看一下[图10-3](#pumpkin)中的照片。南瓜灯是多种颜色，几乎与模糊但略带明暗的背景形成对比。作为人类，您可以轻松识别出照片中的内容。
- en: '![Photograph of jack-o-lantern art](assets/ltjs_1003.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![南瓜灯艺术的照片](assets/ltjs_1003.png)'
- en: Figure 10-3\. Jack-o’-lantern art
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. 南瓜灯艺术
- en: Now here’s the same image with a 3 x 3 edge detection filter convolved over
    the pixels. Notice how the result is significantly simplified and more evident
    in [Figure 10-4](#pumpkin2).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是同一图像，通过3 x 3的边缘检测滤波器卷积在像素上。注意结果在[图10-4](#pumpkin2)中明显简化和更明显。
- en: Different filters accent different aspects of each image to simplify and clarify
    the contents. It doesn’t have to stop there; you can run activations to emphasize
    the detected features, and you can even run convolutions on convolutions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的滤波器突出显示图像的不同方面，以简化和澄清内容。不必止步于此；您可以运行激活以强调检测到的特征，甚至可以在卷积上运行卷积。
- en: What’s the result? You’ve feature engineered the image. Preprocessing images
    through a variety of filters lets your neural network see patterns that would
    have required a much larger, slower, and more complicated model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是什么？您已经对图像进行了特征工程。通过各种滤波器对图像进行预处理，让您的神经网络看到原本需要更大、更慢和更复杂模型才能看到的模式。
- en: '![Convolution result of previous image](assets/ltjs_1004.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![前一图像的卷积结果](assets/ltjs_1004.png)'
- en: Figure 10-4\. Convolution result
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4\. 卷积结果
- en: Adding Convolution Layers
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加卷积层
- en: Thanks to TensorFlow.js, adding a convolutional layer is just as easy as adding
    a dense layer, but it’s called `conv2d` and has its own properties.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢TensorFlow.js，添加卷积层与添加密集层一样简单，但称为`conv2d`，并具有自己的属性。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_image_training_CO1-1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_image_training_CO1-1)'
- en: Identify how many filters to run.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 确定要运行多少个滤波器。
- en: '[![2](assets/2.png)](#co_image_training_CO1-2)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_image_training_CO1-2)'
- en: The `kernelSize` controls the size of the filter. The `3` here represents a
    3 x 3 filter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernelSize`控制滤波器的大小。这里的`3`表示3 x 3的滤波器。'
- en: '[![3](assets/3.png)](#co_image_training_CO1-3)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_image_training_CO1-3)'
- en: The little 3 x 3 filter won’t fit your image, so it will need to slide over
    the image. The stride is how many pixels the filter slides each time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 小小的3 x 3滤波器不适合您的图像，因此需要在图像上滑动。步幅是滤波器每次滑动的像素数。
- en: '[![4](assets/4.png)](#co_image_training_CO1-4)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_image_training_CO1-4)'
- en: The padding lets the convolution decide what to do when your `strides` and `kernelSize`
    don’t evenly divide into your image width and height. When you set padding to
    `same`, zeros are added around the image so that the size of the resulting convolutional
    images is kept the same.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 填充允许卷积在`strides`和`kernelSize`不能均匀地分割成图像宽度和高度时决定如何处理。当将填充设置为`same`时，会在图像周围添加零，以保持生成的卷积图像的大小不变。
- en: '[![5](assets/5.png)](#co_image_training_CO1-5)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_image_training_CO1-5)'
- en: The results are then run through the activation function of your choice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将结果通过您选择的激活函数运行。
- en: '[![6](assets/6.png)](#co_image_training_CO1-6)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_image_training_CO1-6)'
- en: The input is an image tensor, so the input image is the rank-three shape for
    the model. This is not a required restriction for convolutions, as you learned
    in [Chapter 6](ch06.html#the_chapter_6), but it is recommended if you are not
    making a fully convolutional model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个图像张量，因此输入图像是模型的三维形状。这不是卷积的必需限制，正如您在[第6章](ch06.html#the_chapter_6)中学到的那样，但如果您不是在制作完全卷积模型，则建议这样做。
- en: Don’t feel overwhelmed by the list of possible parameters. Imagine having to
    code all those different settings by yourself. You can configure your convolutions
    like existing models or go crazy with numbers to see how it affects your results.
    Tuning these parameters and experimenting is the benefit of a framework like TensorFlow.js.
    Most importantly, build your intuition over time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被可能的参数列表所压倒。想象一下自己必须编写所有这些不同设置。您可以像现有模型那样配置您的卷积，也可以使用数字进行调整以查看其对结果的影响。调整这些参数并进行实验是TensorFlow.js等框架的好处。最重要的是，随着时间的推移建立您的直觉。
- en: It is important to note that this `conv2d` layer is meant for images. Similarly,
    you would use `conv1d` for a linear series and `conv3d` when working on 3D spatial
    objects. Most of the time, 2D is used, but the concept is not limited.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这个`conv2d`层是用于图像的。同样，您将在线性序列上使用`conv1d`，在处理3D空间对象时使用`conv3d`。大多数情况下，使用2D，但概念并不受限制。
- en: Understanding Max Pooling
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解最大池化
- en: Once you’ve simplified the image with filters via a convolutional layer, you’re
    left with a lot of empty space in your filtered graphic. Also, the number of input
    parameters has significantly increased due to all the image filters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过卷积层使用滤波器简化图像后，您在过滤后的图形中留下了大量空白空间。此外，由于所有图像滤波器，输入参数的数量显着增加。
- en: Max pooling is a way to simplify the most active features identified in an image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化是简化图像中识别出的最活跃特征的一种方法。
- en: Max Pooling Quick Summary
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大池化快速总结
- en: To condense the resulting image size, you reduce the output using max pooling.
    Max pooling, put plainly, is keeping the most active pixel in a window as a representation
    of the entire block of pixels in that window. You then slide the window over and
    take the max of that. These results are pooled together to make a much smaller
    image as long as the stride of the window was greater than 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了压缩生成的图像大小，您可以使用最大池化来减少输出。简单地说，最大池化是将窗口中最活跃的像素保留为该窗口中所有像素块的表示。然后您滑动窗口并取其中的最大值。只要窗口的步幅大于1，这些结果就会汇总在一起，以生成一个更小的图像。
- en: The following example quarters the size of an image by taking the largest number
    of each subsquare. Study the illustration in [Figure 10-5](#max_pool).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过取每个子方块中的最大数来将图像的大小分成四分之一。研究[图10-5](#max_pool)中的插图。
- en: '![Max pooling demo](assets/ltjs_1005.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![最大池化演示](assets/ltjs_1005.png)'
- en: Figure 10-5\. Max pool with 2 x 2 kernel and stride of 2
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `kernelSize` in [Figure 10-5](#max_pool) is 2 x 2\. So the four top-left
    squares are evaluated together, and out of the numbers `[12, 5, 11, 7]`, the largest
    is `12`. That max number is passed along to the result. With a stride of two,
    the square of the kernel window moves completely adjacent to the previous square
    and starts again with the numbers `[20, 0, 12, 3]`. This means the most powerful
    activation in each window is passed along.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: You might feel that this process chops up an image and destroys the contents,
    but you’d be surprised to know that the resulting image is quite recognizable.
    Max pooling even emphasizes the detections and makes images more recognizable.
    See [Figure 10-6](#pumpkin3), which is the result of running max pool on the convolution
    of the jack-o’-lantern from earlier.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Max pooling emphasizes detections](assets/ltjs_1006.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. 2 x 2 kernel max pool result of a convolution
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While both [Figure 10-4](#pumpkin2) and [Figure 10-6](#pumpkin3) appear the
    same size for illustration purposes, the latter is somewhat clearer and a quarter
    of the size due to the pooling process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Adding Max Pooling Layers
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the `conv2d`, a max pool is added as a layer, generally immediately
    after a convolution:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_image_training_CO2-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The `poolSize` is the window size, just like `kernelSize` was. The previous
    examples have been 2 (which is short for 2 x 2).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO2-2)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'The `strides` is how far right and down to move the window in each operation.
    This can also be written as `strides: [2, 2]`.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Often, a model that is reading an image will have several layers of convolution,
    then pooling, and then convolution and pooling again and again. This chews up
    the features of an image and breaks them into parts that could potentially identify
    an image.⁠^([2](ch10.html#idm45049239428888))
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Training Image Classification
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After a few layers of convolutions and pooling, you can flatten or serialize
    the resulting filters into a single chain and feed it into a deeply connected
    neural network. This is why people love showing the MNIST training example; it’s
    so simple that you can actually watch the data in a single image.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the entire process to categorize a number using convolutions
    and max pooling. [Figure 10-7](#mnist_layers) should be read from bottom to top.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![layer by layer output of MNIST](assets/ltjs_1007.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. MNIST processing the number five
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you follow the process of this image displayed in [Figure 10-7](#mnist_layers),
    you can see the input drawing down at the bottom and then a convolution of that
    input with six filters directly above it. Next, those six filters are max pooled
    or “downsampled,” and you can see they are smaller because of it. Then one more
    convolution and pooling before they are flattened out to a fully connected dense
    network layer. Above that flattened layer is one dense layer, and the last small
    layer at the top is a softmax layer with 10 possible solutions. The “five” is
    the one that’s lit up.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: From a bird’s-eye view, the convolution and pooling looks like magic, but it
    is digesting features of the image into patterns that neurons can recognize.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'In a layered model, this means the first layers are generally convolution and
    pooling styled layers, and then they are passed into a neural network. [Figure 10-8](#cnns_simple)
    illustrates a high-level view of the process. The following are three stages:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Input image
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature extraction
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deeply connected neural network
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![CNN flowchart](assets/ltjs_1008.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. The three basic stages of CNNs
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Handling Image Data
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the drawbacks of training with images is that the datasets can be quite
    large and unwieldy. Datasets are generally large, but with images, they are often
    immense. This is another reason the same visual datasets are used over and over
    again.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Even when an image dataset is small, it can take up a sizable amount of memory
    when loaded into memory tensor form. You might need to break training into chunks
    of tensors for monolithic image sets. This might explain why models like MobileNet
    were optimized for a size that is considered relatively small by today’s standards.
    Increasing or decreasing an image by a single pixel across all images results
    in exponential size differences. By the very nature of the data, grayscale tensors
    are a third the size of RGB images in memory and a quarter the size of RGBA images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The Sorting Hat
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it’s time for your first convolutional neural network. For this model, you’re
    going to train a CNN to classify grayscale drawings into 10 categories.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: If you’re a fan of the popular book series *Harry Potter*, by J. K. Rowling,
    this will make sense and be entertaining. However, if you’ve never read a single
    *Harry Potter* book or watched any of the movies, this will still be an excellent
    exercise. In the books, there are four houses in the wizard school Hogwarts, and
    each house has animals associated with them. You’re going to ask users to draw
    a picture and use that picture to sort them into houses. I’ve prepared a dataset
    of drawings that somewhat resembles the icons and animals from each group.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The dataset I’ve prepared is made from a subset of drawings from [Google’s Quick,
    Draw! Dataset](https://oreil.ly/kq3bX). The classes have been narrowed down to
    10, and the data has been significantly cleaned up.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code associated with this chapter, which can be found at [*chapter10/node/node-train-houses*](https://oreil.ly/xr3Bu),
    you’ll find a ZIP file with tens of thousands of 28 x 28 drawings of the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Birds
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Owls
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parrots
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Snakes
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Snails
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lions
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tigers
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Raccoons
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Squirrels
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Skulls
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The drawings vary wildly, but the characteristics of each class are discernable.
    Here is a random sampling of doodles, illustrated in [Figure 10-9](#house_classes).
    Once you’ve trained a model to identify each of these 10 classes, you can use
    the model to sort drawings that resemble a particular animal into its associated
    house. Birds go to Ravenclaw, lions and tigers go to Gryffindor, etc.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![The Hogwarts House Drawings in Grid form](assets/ltjs_1009.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. The 10 classes of drawings
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a lot of ways of handling this, but the simplest will be to categorize
    the model using softmax for the final layer. As you remember, softmax will give
    us N numbers that all sum up to one. For example, if a drawing is 0.67 bird, 0.12
    owl, and 0.06 parrot, since all those represent the same house, we can sum them
    together, and the result will always be less than one. While you’re familiar with
    using models that return results like this, it will be your first softmax classification
    model that you’ve created from scratch.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few ways you could use TensorFlow.js to train this model. Getting
    megabytes of images loaded into a browser can be done in several ways:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: You could load each image with subsequent HTTP requests.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could combine the training data into a large sprite sheet and then use your
    tensor skills to extract and stack each image into Xs and Ys.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could load the images into a CSV and from there convert them to tensors.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could Base64-encode the images and load them from a single JSON file.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The one common issue you see repeated here is that you have to do a bit of extra
    work to get your data into the sandbox of the browser. It’s for this reason that
    it’s probably best to use Node.js for image training with a significantly large
    dataset. We’ll cover situations where this is less important later in the book.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The Node.js code associated with this chapter has the training data you’ll need.
    You’ll see a file in the repository that is close to 100 MB (the GitHub limit
    for a single file), which you’ll need to unzip in place (see [Figure 10-10](#zips)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Unzip the files.zip screenshot](assets/ltjs_1010.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. Unzip the images into the files folder
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you have the images and you know how to read images in Node.js, the
    code to train this model would be something like [Example 10-1](#code_training_goal).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-1\. The ideal setup
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_image_training_CO3-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Create a simple function to load the images into the needed X and Y tensors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO3-2)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Create a suitable CNN layers model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO3-3)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Use the `shuffle` property, which shuffles the current batch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO3-4)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Save the resulting trained model locally.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The code in [Example 10-1](#code_training_goal) does not mention setting aside
    any testing data. Because of the nature of this project, the real testing will
    be done when drawing images and identifying how each stroke moves an image closer
    or further from the desired goal. A validation set will still be used in training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Converting Folders of Images
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `folderToTensors` function will need to do the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Identify all the PNG file paths.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect the image tensors and the answers.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomize both sets.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize and stack the tensors.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean up and return the results.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To identify and access all the images, you can use a library like `glob`, which
    takes a given path like *files/**/*.png* and return an array of filenames. The
    /** iterates over all subfolders in that folder and finds all PNGs in each.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install `glob` with NPM like so:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that the node module is available, it can be required or imported:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because globs operate by using a callback, you can wrap the whole function in
    a JavaScript promise to bring it back to async/await. If you’re unfamiliar with
    these concepts, feel free to brush up on them or just study the code provided
    with the chapter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: After you have a collection of file locations, you can load the file, convert
    it to a tensor, and even identify the “answer” or “y” for each image by looking
    at what folder the image came from.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Remember that tensors create a *new tensor* every time you need to modify them.
    So rather than normalizing and stacking the tensors as we go, it will be best
    to keep the tensors in a JavaScript array.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of reading each string into these two arrays can be accomplished
    with this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `encodeDir` function is a simple function I wrote to look at the path of
    each image and return an associated predictive number:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once you have the images in tensor form, you might consider stacking and returning
    them, but it is *crucial* that you shuffle them before. Without mixing the data,
    your model will quickly train in the strangest way. Indulge me in a peculiar metaphor.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if I asked you to point out the shape I’m thinking of in a collection
    of shapes. You quickly learn I’m always thinking of the circle, and you start
    getting 100% accuracy. On our third test, I start saying, “No, that’s not the
    square! You’re very wrong.” So you then switch to pointing at the square and again
    get 100% accuracy. Every third test, I change the shape. While your scores are
    above 99% accurate, you never learned the actual indicator of which one to pick.
    So you go out into the field where the shape changes every time and fail. You
    never learned the indicators because the data was not shuffled.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Unshuffled data will have the same effect: near-perfect training accuracy,
    and abysmal validation and test scores. Even though you’re shuffling each, you’ll
    only be shuffling 256 of the same values most of the time.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: To shuffle the X and the Y in the same permutation, you can use `tf.utils.shuffleCombo`.
    *I hear the guy who added this function to TensorFlow.js is super cool.*
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because this is shuffling JavaScript references, no new tensors are created
    in this shuffle.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you’ll want to convert the answers from an integer to one-hot encoding.
    The one-hot encoding is because your model will be softmax, i.e., 10 values that
    sum up to one with the correct answer being the dominant value exclusively.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow.js has a method called `oneHot` that converts numbers to one-hot
    encoded tensor values. For example, the number `3` out of `5` possible categories
    would be encoded to the tensor `[0,0,1,0,0]`. This is how we want to format our
    answers to match the expected output of the categorical model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now stack the X and Y array values into a large tensor and normalize
    the images to be values `0-1` by dividing by `255`. The stacking and encoding
    would look like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Your computer will likely pause between each log as thousands of images are
    processed. The code prints the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we have our X and Y for training, and their shape is the input and output
    shape of the model we will create.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The CNN Model
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s time to create the convolutional neural network model. The architecture
    for this model will be three pairs of convolution and pooling layers. On each
    new convolutional layer, we will double the number of filters to train. Then we’ll
    flatten the model to a single dense hidden layer of 128 units that have a `tanh`
    activation, and finish with a final layer of 10 possible outputs with the softmax
    activation. If you’re confused about why we’re using softmax, please review the
    structure of classification models we covered in [Chapter 9](ch09.html#the_chapter_9).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able to write the model layers from the description alone, but
    here’s the code to create the described sequential model:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This new final layer for categorical data that is not binary means that you’ll
    need to change your loss function from `binaryCrossentropy` to `categoricalCrossentropy`
    accordingly. So now the `model.compile` code would look like this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s review the `model.summary()` method through the lens of what we’ve learned
    about convolutions and max pooling so we can ensure we’ve built everything correctly.
    You can see the printout of the results in [Example 10-2](#output_conv_model_summary).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-2\. The output of `model.summary()`
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_image_training_CO4-1)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The first convolutional layer takes an input of `[stacksize, 28, 28, 1]` and
    has a convolutional output of `[stacksize, 28, 28, 16]`. The size is the same
    because we’re using `padding: ''same''`, and the 16 is the 16 different filter
    results we got when we specified `filters: 16`. You can think of this as 16 new
    filtered images for each image in the stack. This gives the network 160 new parameters
    to train. The trainable parameters are calculated as (number of images in) * (kernel
    window) * (images out) + (images out), which comes out to `1 * (3x3) * 16 + 16
    = 160`.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO4-2)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The max pooling cuts the filtered image rows and column sizes in half, which
    quarters the pixels. This layer does not have any trainable parameter since the
    algorithm is fixed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO4-3)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The convolution and pooling happens again, and more filters are being employed
    at each level. The size of the image is shrinking, and the number of trainable
    parameters are swiftly growing, i.e., `16 * (3x3) * 32 + 32 = 4,640`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO4-4)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is one final convolution and pooling. Pooling an odd number creates
    a larger-than-50% reduction.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_image_training_CO4-5)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Flattening the 64 3 x 3 images turns into a single layer of 576 units.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_image_training_CO4-6)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Each of the 576 units is densely connected to the 128-unit layer. Using the
    traditional calculation of lines + nodes, this comes out to `(576 * 128) + 128
    = 73,856` trainable parameters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_image_training_CO4-7)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last layer lands with 10 possible values for each class.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we’re evaluating `model.summary()` instead of inspecting
    a graphical representation of what’s happening. Even at lower dimensionality,
    the graphical representation of what’s happening is difficult to illustrate. I’ve
    done my best to create a somewhat exhaustive illustration in [Figure 10-11](#sorting_process).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们评估`model.summary()`而不是检查正在发生的事情的图形表示。即使在较低的维度，图形表示正在发生的事情也很难说明。我已经尽力在[图10-11](#sorting_process)中创建了一个相对详尽的插图。
- en: '![CNN neural network](assets/ltjs_1011.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![CNN神经网络](assets/ltjs_1011.png)'
- en: Figure 10-11\. A visualization of each layer
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-11\. 每一层的可视化
- en: Unlike previous neural network diagrams, the visual explanation of CNNs can
    be somewhat limiting. Stacks upon stacks of filtered images are only so informative.
    The result of the convolutional process is flattened and connected to a deeply
    connected neural network. You’ve reached a point of complexity where the `summary()`
    method of a model is the best way to understand the contents.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的神经网络图不同，CNN的可视化解释可能有些局限性。堆叠在一起的滤波图像只能提供有限的信息。卷积过程的结果被展平并连接到一个深度连接的神经网络。您已经达到了一个复杂性的程度，`summary()`方法是理解内容的最佳方式。
- en: Tip
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’d like a dynamic visual and to watch each trained filter’s result activation
    at each layer, Polo Club of Data Science created a beautiful [CNN Explainer](https://oreil.ly/o24uR)
    in TensorFlow.js. Check out [the interactive visualizer](https://oreil.ly/SYHsp).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要一个动态的可视化，并观看每个训练滤波器在每一层的激活结果，数据科学Polo Club创建了一个美丽的[CNN解释器](https://oreil.ly/o24uR)在TensorFlow.js中。查看[交互式可视化器](https://oreil.ly/SYHsp)。
- en: There you go. Your resulting `[3, 3, 64]` flattens to 576 artificial neurons
    before connecting to the neural network. Not only did you create image features,
    but you’ve simplified an input from a `[28, 28, 1]` image, which was originally
    going to require 784 densely connected inputs. Armed with this more advanced architecture,
    you can load the data from `folderToTensors()` and create the necessary model.
    You’re ready to train.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经到了那里。您的结果`[3, 3, 64]`在连接到神经网络之前展平为576个人工神经元。您不仅创建了图像特征，还简化了一个`[28, 28, 1]`图像的输入，原本需要784个密集连接的输入。有了这种更先进的架构，您可以从`folderToTensors()`加载数据并创建必要的模型。您已经准备好训练了。
- en: Training and Saving
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和保存
- en: Since this is training in Node.js, you will have to setup GPU-acceleration on
    the machine directly. This is often done with NVIDIA CUDA, and CUDA Deep Neural
    Network (cuDNN). You will have to set up CUDA and cuDNN properly to work with
    your GPU if you want to train using `@tensorflow/tfjs-node-gpu` and get significant
    speed boost over normal `tfjs-node`. See [Figure 10-12](#cuda_gpu2).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是在Node.js中进行训练，您将不得不直接在机器上设置GPU加速。这通常是通过NVIDIA CUDA和CUDA深度神经网络（cuDNN）完成的。如果您想使用`@tensorflow/tfjs-node-gpu`进行训练并获得比普通`tfjs-node`更快的速度提升，您将不得不正确设置CUDA和cuDNN以与您的GPU一起工作。请参阅[图10-12](#cuda_gpu2)。
- en: '![CUDA GPU Screenshot](assets/ltjs_1012.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![CUDA GPU截图](assets/ltjs_1012.png)'
- en: Figure 10-12\. 3x–4x speed boost with GPU
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12\. 使用GPU可以提高3-4倍的速度
- en: The resulting model, after 20 epochs, lands around 95% accuracy in training
    and 90% accuracy for the validation set. The file size of the resulting model
    is around 400 KB. You might have noticed the accuracy for the training set continuously
    trending upward, but the validation can sometimes go down. For better or worse,
    the last epoch will be the model that gets saved. If you’d like to ensure the
    highest possible validation accuracy, take a look at the Chapter Challenge at
    the end.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在20个时代之后，生成的模型在训练中的准确率约为95%，在验证集中的准确率约为90%。生成模型的文件大小约为400 KB。您可能已经注意到训练集的准确率不断上升，但验证集有时会下降。不管好坏，最后一个时代将是保存的模型。如果您想确保最高可能的验证准确性，请查看最后的章节挑战。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you run this model for too many epochs, the model will overfit and approach
    100% training accuracy with a diminished validation accuracy.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对这个模型运行了太多时代，模型将过拟合，并接近100%的训练准确率，而验证准确率会降低。
- en: Testing the Model
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: To test this model, you’ll need drawings from the user. You can create a simple
    drawing surface on a web page with a canvas. A canvas can subscribe to events
    on the mouse being pressed down, when the mouse is moving along the canvas, and
    when the mouse is released. Using these events, you can draw from point to point.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试这个模型，您需要用户的绘图。您可以在网页上创建一个简单的绘图表面，使用一个画布。画布可以订阅鼠标按下时、鼠标沿着画布移动时以及鼠标释放时的事件。使用这些事件，您可以从一个点绘制到另一个点。
- en: Building a Sketchpad
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个草图板
- en: 'You can build a simple drawable canvas with those three events. You’ll use
    some new methods to move the canvas path and draw lines, but it’s quite readable.
    The following code sets one up:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这三个事件构建一个简单的可绘制画布。您将使用一些新方法来移动画布路径和绘制线条，但这是相当易读的。以下代码设置了一个：
- en: '[PRE13]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The drawings are made from a bunch of smaller lines that have a stroke width
    of 14 pixels and are automatically rounded on the edges. You can see a test drawing
    in [Figure 10-13](#canvas_test).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图纸是由一堆较小的线条制成的，线条的笔画宽度为14像素，并且在边缘自动圆润。您可以在[图10-13](#canvas_test)中看到一个测试绘图。
- en: '![Example drawing](assets/ltjs_1013.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![示例绘图](assets/ltjs_1013.png)'
- en: Figure 10-13\. Works well enough
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13\. 运行得足够好
- en: When the user clicks the mouse while on the canvas, any movement will be drawn
    from point to new point. Whenever the user stops drawing, the `drawEnd` function
    will be called. You could add a button to classify the canvas or tie directly
    into the `drawEnd` function and classify the image.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户在画布上单击鼠标时，任何移动都将从一个点绘制到新点。每当用户停止绘制时，将调用`drawEnd`函数。您可以添加一个按钮来对画布进行分类，或者直接将其连接到`drawEnd`函数并对图像进行分类。
- en: Reading the Sketchpad
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读草图板
- en: When you call `tf.browser.fromPixels` on the canvas, you’ll get 100% black pixels.
    Why is this? The answer is that the canvas has nothing drawn in some places and
    black pixels in other areas. When the canvas is converted to tensor values, it
    will convert emptiness to black. The canvas might look like it has a white background,
    but it’s actually clear and would show whatever color or pattern is underneath
    (see [Figure 10-14](#under_canvas)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![A canvas is empty](assets/ltjs_1014.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. A canvas is transparent—and so empty pixels are zero
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To fix this, you can add a clear function that writes a large white square
    in the canvas, so the black lines will be on a white background like the training
    images. This is also the function you can use to clear the canvas between drawings.
    To fill the canvas with a white background, you would use the `fillRect` method
    you used to outline the labels in [Chapter 6](ch06.html#the_chapter_6):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the canvas is initialized with a white background, you can make a prediction
    on the canvas drawing:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_image_training_CO5-1)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: When you read the canvas, don’t forget to identify that you’re only interested
    in a single channel; otherwise, you’ll need to turn a 3D tensor into a 1D tensor
    before continuing.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO5-2)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Resize using the nearest neighbor algorithm down to a 28 x 28 image for input
    into the model. Pixelation caused by nearest neighbor is inconsequential here,
    so it’s a smart choice because it is faster than `resizeBilinear`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO5-3)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The model expects a batch, so prepare the data as a batch of one. This creates
    a `[1, 28, 28, 1]` input tensor.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO5-4)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The prediction result has been returned to JavaScript as a single batch of 10
    numbers. Think up a creative way to display the results.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have the results, you can portray the answers in any format you
    please. Personally, I organized the scores by house and used them to set the opacity
    of the labels. This way, you could get feedback as you drew each line. The opacity
    of labels depends on values `0-1`, which works splendidly with the results of
    the softmax predictions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You might wonder if Ravenclaw has a slight mathematical advantage because it
    is composed of more classes, and you’d be right. With all things equal, a completely
    random set of lines is more likely to be classified as Ravenclaw because it has
    a majority of the classes. However, this is statistically insignificant when the
    drawings are non-random. If you want the model to have only nine classes, remove
    `bird` and retrain to create the most balanced classification spectrum.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re interested in identifying which classes are likely to be problematic
    or confused, you can use visual reporting tools like a confusion matrix or the
    [t-SNE](https://oreil.ly/sBio5) algorithm. These tools are especially helpful
    for evaluating the training data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: I highly recommend you load the code for this chapter from [*chapter10/simple/simplest-draw*](https://oreil.ly/emOWR)
    and put your artistic skills to the test! My bird drawing sorted me into Ravenclaw,
    as shown in [Figure 10-15](#ravenclaw_ftw).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![The web page correctly identifying a bird](assets/ltjs_1015.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. A UI and drawing masterpiece
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I was able to poorly draw and properly get sorted into each of the other possible
    houses as well. However, I won’t punish you with any more of my “art.”
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Review
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve trained a model on visual data. While this dataset was limited to grayscale
    drawings, the lessons you’ve learned could work with any image dataset. There
    are plenty of excellent image datasets out there that would work perfectly with
    the model you’ve created. We’ll cover more in the next two chapters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: I’ve created a more elaborate page for the [drawing identification featured](https://oreil.ly/jnlhb)
    in this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter Challenge: Saving the Magic'
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re most interested in getting the highest validation accuracy model,
    it’s slim odds that your best version of the model will be the last one. For example,
    if you take a look at [Figure 10-16](#accuracy_changes), a validations accuracy
    of 90.3% gets lost, and you end up with 89.6% as the final validation of the model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您最感兴趣的是获得最高验证准确性模型，那么您的最佳模型版本很可能不是最后一个版本。例如，如果您查看[图10-16](#accuracy_changes)，90.3%的验证准确性会丢失，最终验证模型为89.6%。
- en: For this Chapter Challenge, rather than saving the final trained version of
    the model, add a callback that saves the model when the validation accuracy has
    reached a new record best. This kind of code is useful because it will allow you
    to run for many epochs. As the model overfits, you’ll be able to keep the best
    generalizable model for production.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章挑战，与其保存模型的最终训练版本，不如添加一个回调函数，当验证准确性达到新的最佳记录时保存模型。这种代码非常有用，因为它允许您运行多个时期。随着模型过拟合，您将能够保留最佳的通用模型用于生产。
- en: '![validation vs. training accuracy](assets/ltjs_1016.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![验证与训练准确性](assets/ltjs_1016.png)'
- en: Figure 10-16\. Evaluate which accuracy is more important
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-16。评估哪个准确性更重要
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[附录B](app02.html#appendix_b)中找到此挑战的答案。
- en: Review Questions
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复习问题
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下你在本章编写的代码中学到的教训。花点时间回答以下问题：
- en: Convolutional layers have many trainable *what* that help extract features of
    an image?
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层有许多可训练的*什么*，可以帮助提取图像的特征？
- en: What is the name of the property that controls the convolutional window size?
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制卷积窗口大小的属性名称是什么？
- en: If you want a convolutional result to be the same size as the original image,
    what should you set as the padding?
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你希望卷积结果与原始图像大小相同，应该将填充设置为什么？
- en: True or false? You must flatten an image before inserting it into a convolutional
    layer.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真或假？在将图像插入卷积层之前，必须将其展平。
- en: What would be the output size of a max pool 3 x 3 with a stride of three on
    an 81 x 81 image?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在81 x 81图像上，步幅为3的最大池3 x 3的输出大小将是多少？
- en: If you were to one-hot encode the number 12, do you have enough information
    to do so?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您要对数字12进行独热编码，您是否有足够的信息来这样做？
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在[附录A](app01.html#book_appendix)中找到。
- en: ^([1](ch10.html#idm45049239647096-marker)) Videos and lectures by [3Blue1Brown](https://oreil.ly/zuGzT)
    on YouTube are excellent starts for anyone looking to go down the convolution
    rabbit hole.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#idm45049239647096-marker)) YouTube上的[3Blue1Brown](https://oreil.ly/zuGzT)的视频和讲座是任何想要深入了解卷积的人的绝佳起点。
- en: ^([2](ch10.html#idm45049239428888-marker)) There are other pooling methods available
    in TensorFlow.js for experimentation.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#idm45049239428888-marker)) TensorFlow.js中还有其他可用于实验的池化方法。
