- en: Chapter 10\. Image Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。图像训练
- en: “Everything must be made as simple as possible. But not simpler.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一切都应该尽可能简单。但不要过于简单。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Albert Einstein
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —阿尔伯特·爱因斯坦
- en: I’m going to describe a number to you. I’d like you to figure out the number
    from the description of its features. The number I’m thinking of has a rounded
    top, a line down the right side only, and a loopy thing at the bottom that overlaps.
    Take a moment and mentally map the number I just described. With those three features,
    you can probably figure it out.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我将向您描述一个数字。我希望您根据其特征的描述来猜出这个数字。我想的数字顶部是圆的，右侧只有一条线，底部有一个重叠的环状物。花点时间，心理上映射我刚刚描述的数字。有了这三个特征，你可能可以猜出来。
- en: The features of a visual number can vary, but a smart description means you
    can identify the numbers in your mind. When I said “rounded top,” you can immediately
    throw out some numbers, and the same goes for “a line down the right side only.”
    The features compose what makes the number unique.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉数字的特征可能会有所不同，但聪明的描述意味着您可以在脑海中识别数字。当我说“圆顶”时，您可能会立即排除一些数字，同样的情况也适用于“只有右侧有一条线”。这些特征组成了数字的独特之处。
- en: If you were to describe the numbers as shown in [Figure 10-1](#number_features)
    and put their descriptions in a CSV file, you could probably get 100% accuracy
    with a trained neural network on that data. The whole thing works except that
    it depends on a human to describe the top, middle, and bottom of each number.
    How do you automate the human aspect of this? If you can make a computer identify
    the loops, colors, and curves that are unique characteristics of an image and
    then feed that into a neural network, the machine could learn the needed patterns
    to classify the descriptions into images.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您按照[图10-1](#number_features)中显示的数字描述，并将其描述放入CSV文件中，您可能可以通过训练好的神经网络在这些数据上获得100%的准确性。整个过程都很顺利，只是依赖于人类描述每个数字的顶部、中部和底部。如何自动化这个人类方面呢？如果您可以让计算机识别图像的独特特征，如环、颜色和曲线，然后将其输入神经网络，机器就可以学习将描述分类为图像所需的模式。
- en: '![The number two in three parts.](assets/ltjs_1001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![数字二的三部分。](assets/ltjs_1001.png)'
- en: Figure 10-1\. Congratulations if you figured out it was the number two
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。如果您发现这是数字二，恭喜您。
- en: Thankfully, this problem of feature engineering images has been solved with
    fantastic computer vision tricks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过出色的计算机视觉技巧，解决了图像特征工程的问题。
- en: 'We will:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将：
- en: Learn the concepts of convolutional layers for models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习模型的卷积层的概念
- en: Build your first multiclass model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建您的第一个多类模型
- en: Learn how to read and process images for TensorFlow.js
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何读取和处理图像以供TensorFlow.js使用
- en: Draw on a canvas and classify the drawing accordingly
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在画布上绘制并相应地对绘图进行分类
- en: When you finish this chapter, you’ll be able to create your own image classification
    models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够创建自己的图像分类模型。
- en: Understanding Convolutions
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积
- en: Convolutions come from the mathematical world of expressing shapes and functions.
    You can dig for a long, long time into the concept of what a convolution is in
    mathematics and then reapply that knowledge from the ground up to the idea of
    gathering information on digital images. If you’re a fan of mathematics and computer
    graphics, this is quite an exciting rabbit hole.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积来自于数学世界中表达形状和函数的概念。您可以深入研究卷积在数学中的概念，然后从头开始将这些知识重新应用到数字图像信息的收集概念。如果您是数学和计算机图形的爱好者，这是一个非常令人兴奋的领域。
- en: However, it’s not essential that you spend a week learning the fundamentals
    of convolutional operations when you have a framework like TensorFlow.js. It is
    for that reason that we’ll focus on the high-level benefits and properties of
    convolutional operations and how they are used in neural networks. As always,
    you are encouraged to research the deep history of convolutions beyond this quick-start.^([1](ch10.html#idm45049239647096))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你有像TensorFlow.js这样的框架时，花费一周学习卷积操作的基础并不是必要的。因此，我们将专注于卷积操作的高级优势和特性，以及它们在神经网络中的应用。始终鼓励您在这个快速入门之外深入研究卷积的深层历史。
- en: Let’s look at the most prominent concept you should take from a nonmathematical
    explanation of convolutions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看您应该从非数学解释的卷积中获得的最重要概念。
- en: The two images of the number two in [Figure 10-2](#compare_nums) are the *exact*
    same number shifted from left to right in their bounding boxes. Each of these
    converted into tensors would create significantly different unequal tensors. However,
    in the feature system we described at the beginning of the chapter, these features
    would be the same.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-2](#compare_nums)中的两个数字二的图像是*完全*相同的数字，只是它们在边界框中从左到右移动。将这两个数字转换为张量后，会创建出两个明显不同的不相等张量。然而，在本章开头描述的特征系统中，这些特征将是相同的。'
- en: '![Convolution Features vs no Features](assets/ltjs_1002.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![卷积特征与无特征](assets/ltjs_1002.png)'
- en: Figure 10-2\. Convolutions simplify the essence of an image
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。卷积简化了图像的本质
- en: For visual tensors, an image’s features are more important than the exact locations
    of each pixel.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视觉张量，图像的特征比每个像素的确切位置更重要。
- en: Convolutions Quick Summary
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积快速总结
- en: A convolutional operation is used to extract high-level features such as edges,
    gradients, colors, and more. These are the key features that classify a given
    visual.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作用于提取高级特征，如边缘、梯度、颜色等。这些是分类给定视觉的关键特征。
- en: So what features should it extract? That’s not something we actually decide.
    You can control the number of filters used in finding features, but the actual
    features that best define usable patterns are defined in the training process.
    The filters start with accenting and drawing out features from the image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么应该提取哪些特征呢？这并不是我们实际决定的。您可以控制在查找特征时使用的滤波器数量，但最好定义可用模式的实际特征是在训练过程中定义的。这些滤波器从图像中突出和提取特征。
- en: For instance, take a look at the photo in [Figure 10-3](#pumpkin). The jack-o’-lantern
    is multiple colors and barely contrasts against a blurry but somewhat light and
    dark background. As a human, you have no issue identifying what this is in the
    photo.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看一下[图10-3](#pumpkin)中的照片。南瓜灯是多种颜色，几乎与模糊但略带明暗的背景形成对比。作为人类，您可以轻松识别出照片中的内容。
- en: '![Photograph of jack-o-lantern art](assets/ltjs_1003.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![南瓜灯艺术的照片](assets/ltjs_1003.png)'
- en: Figure 10-3\. Jack-o’-lantern art
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. 南瓜灯艺术
- en: Now here’s the same image with a 3 x 3 edge detection filter convolved over
    the pixels. Notice how the result is significantly simplified and more evident
    in [Figure 10-4](#pumpkin2).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是同一图像，通过3 x 3的边缘检测滤波器卷积在像素上。注意结果在[图10-4](#pumpkin2)中明显简化和更明显。
- en: Different filters accent different aspects of each image to simplify and clarify
    the contents. It doesn’t have to stop there; you can run activations to emphasize
    the detected features, and you can even run convolutions on convolutions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的滤波器突出显示图像的不同方面，以简化和澄清内容。不必止步于此；您可以运行激活以强调检测到的特征，甚至可以在卷积上运行卷积。
- en: What’s the result? You’ve feature engineered the image. Preprocessing images
    through a variety of filters lets your neural network see patterns that would
    have required a much larger, slower, and more complicated model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是什么？您已经对图像进行了特征工程。通过各种滤波器对图像进行预处理，让您的神经网络看到原本需要更大、更慢和更复杂模型才能看到的模式。
- en: '![Convolution result of previous image](assets/ltjs_1004.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![前一图像的卷积结果](assets/ltjs_1004.png)'
- en: Figure 10-4\. Convolution result
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4\. 卷积结果
- en: Adding Convolution Layers
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加卷积层
- en: Thanks to TensorFlow.js, adding a convolutional layer is just as easy as adding
    a dense layer, but it’s called `conv2d` and has its own properties.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢TensorFlow.js，添加卷积层与添加密集层一样简单，但称为`conv2d`，并具有自己的属性。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_image_training_CO1-1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_image_training_CO1-1)'
- en: Identify how many filters to run.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 确定要运行多少个滤波器。
- en: '[![2](assets/2.png)](#co_image_training_CO1-2)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_image_training_CO1-2)'
- en: The `kernelSize` controls the size of the filter. The `3` here represents a
    3 x 3 filter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernelSize`控制滤波器的大小。这里的`3`表示3 x 3的滤波器。'
- en: '[![3](assets/3.png)](#co_image_training_CO1-3)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_image_training_CO1-3)'
- en: The little 3 x 3 filter won’t fit your image, so it will need to slide over
    the image. The stride is how many pixels the filter slides each time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 小小的3 x 3滤波器不适合您的图像，因此需要在图像上滑动。步幅是滤波器每次滑动的像素数。
- en: '[![4](assets/4.png)](#co_image_training_CO1-4)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_image_training_CO1-4)'
- en: The padding lets the convolution decide what to do when your `strides` and `kernelSize`
    don’t evenly divide into your image width and height. When you set padding to
    `same`, zeros are added around the image so that the size of the resulting convolutional
    images is kept the same.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 填充允许卷积在`strides`和`kernelSize`不能均匀地分割成图像宽度和高度时决定如何处理。当将填充设置为`same`时，会在图像周围添加零，以保持生成的卷积图像的大小不变。
- en: '[![5](assets/5.png)](#co_image_training_CO1-5)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_image_training_CO1-5)'
- en: The results are then run through the activation function of your choice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将结果通过您选择的激活函数运行。
- en: '[![6](assets/6.png)](#co_image_training_CO1-6)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_image_training_CO1-6)'
- en: The input is an image tensor, so the input image is the rank-three shape for
    the model. This is not a required restriction for convolutions, as you learned
    in [Chapter 6](ch06.html#the_chapter_6), but it is recommended if you are not
    making a fully convolutional model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个图像张量，因此输入图像是模型的三维形状。这不是卷积的必需限制，正如您在[第6章](ch06.html#the_chapter_6)中学到的那样，但如果您不是在制作完全卷积模型，则建议这样做。
- en: Don’t feel overwhelmed by the list of possible parameters. Imagine having to
    code all those different settings by yourself. You can configure your convolutions
    like existing models or go crazy with numbers to see how it affects your results.
    Tuning these parameters and experimenting is the benefit of a framework like TensorFlow.js.
    Most importantly, build your intuition over time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被可能的参数列表所压倒。想象一下自己必须编写所有这些不同设置。您可以像现有模型那样配置您的卷积，也可以使用数字进行调整以查看其对结果的影响。调整这些参数并进行实验是TensorFlow.js等框架的好处。最重要的是，随着时间的推移建立您的直觉。
- en: It is important to note that this `conv2d` layer is meant for images. Similarly,
    you would use `conv1d` for a linear series and `conv3d` when working on 3D spatial
    objects. Most of the time, 2D is used, but the concept is not limited.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这个`conv2d`层是用于图像的。同样，您将在线性序列上使用`conv1d`，在处理3D空间对象时使用`conv3d`。大多数情况下，使用2D，但概念并不受限制。
- en: Understanding Max Pooling
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解最大池化
- en: Once you’ve simplified the image with filters via a convolutional layer, you’re
    left with a lot of empty space in your filtered graphic. Also, the number of input
    parameters has significantly increased due to all the image filters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过卷积层使用滤波器简化图像后，您在过滤后的图形中留下了大量空白空间。此外，由于所有图像滤波器，输入参数的数量显着增加。
- en: Max pooling is a way to simplify the most active features identified in an image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化是简化图像中识别出的最活跃特征的一种方法。
- en: Max Pooling Quick Summary
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大池化快速总结
- en: To condense the resulting image size, you reduce the output using max pooling.
    Max pooling, put plainly, is keeping the most active pixel in a window as a representation
    of the entire block of pixels in that window. You then slide the window over and
    take the max of that. These results are pooled together to make a much smaller
    image as long as the stride of the window was greater than 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了压缩生成的图像大小，您可以使用最大池化来减少输出。简单地说，最大池化是将窗口中最活跃的像素保留为该窗口中所有像素块的表示。然后您滑动窗口并取其中的最大值。只要窗口的步幅大于1，这些结果就会汇总在一起，以生成一个更小的图像。
- en: The following example quarters the size of an image by taking the largest number
    of each subsquare. Study the illustration in [Figure 10-5](#max_pool).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过取每个子方块中的最大数来将图像的大小分成四分之一。研究[图10-5](#max_pool)中的插图。
- en: '![Max pooling demo](assets/ltjs_1005.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![最大池化演示](assets/ltjs_1005.png)'
- en: Figure 10-5\. Max pool with 2 x 2 kernel and stride of 2
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5。2 x 2核和步幅为2的最大池
- en: The `kernelSize` in [Figure 10-5](#max_pool) is 2 x 2\. So the four top-left
    squares are evaluated together, and out of the numbers `[12, 5, 11, 7]`, the largest
    is `12`. That max number is passed along to the result. With a stride of two,
    the square of the kernel window moves completely adjacent to the previous square
    and starts again with the numbers `[20, 0, 12, 3]`. This means the most powerful
    activation in each window is passed along.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图10-5](#max_pool)中的`kernelSize`为2 x 2。因此，四个左上角的方块一起进行评估，从数字`[12, 5, 11, 7]`中，最大的是`12`。这个最大数传递给结果。步幅为2，核窗口的方块完全移动到前一个方块的相邻位置，然后重新开始使用数字`[20,
    0, 12, 3]`。这意味着每个窗口中最强的激活被传递下去。
- en: You might feel that this process chops up an image and destroys the contents,
    but you’d be surprised to know that the resulting image is quite recognizable.
    Max pooling even emphasizes the detections and makes images more recognizable.
    See [Figure 10-6](#pumpkin3), which is the result of running max pool on the convolution
    of the jack-o’-lantern from earlier.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会觉得这个过程会切割图像并破坏内容，但你会惊讶地发现，生成的图像是相当容易识别的。最大池化甚至强调了检测，并使图像更容易识别。参见[图10-6](#pumpkin3)，这是在之前的南瓜灯卷积上运行最大池的结果。
- en: '![Max pooling emphasizes detections](assets/ltjs_1006.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: 最大池化强调检测
- en: Figure 10-6\. 2 x 2 kernel max pool result of a convolution
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6。卷积的2 x 2核最大池结果
- en: While both [Figure 10-4](#pumpkin2) and [Figure 10-6](#pumpkin3) appear the
    same size for illustration purposes, the latter is somewhat clearer and a quarter
    of the size due to the pooling process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[图10-4](#pumpkin2)和[图10-6](#pumpkin3)在插图目的上看起来相同大小，但后者由于池化过程而稍微清晰一些，并且是原始大小的四分之一。
- en: Adding Max Pooling Layers
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加最大池化层
- en: 'Similar to the `conv2d`, a max pool is added as a layer, generally immediately
    after a convolution:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`conv2d`，最大池化被添加为一层，通常紧跟在卷积之后：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_image_training_CO2-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_image_training_CO2-1)'
- en: The `poolSize` is the window size, just like `kernelSize` was. The previous
    examples have been 2 (which is short for 2 x 2).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`poolSize`是窗口大小，就像`kernelSize`一样。之前的例子都是2（代表2 x 2）。'
- en: '[![2](assets/2.png)](#co_image_training_CO2-2)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_image_training_CO2-2)'
- en: 'The `strides` is how far right and down to move the window in each operation.
    This can also be written as `strides: [2, 2]`.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`strides`是在每次操作中向右和向下移动窗口的距离。这也可以写成`strides: [2, 2]`。'
- en: Often, a model that is reading an image will have several layers of convolution,
    then pooling, and then convolution and pooling again and again. This chews up
    the features of an image and breaks them into parts that could potentially identify
    an image.⁠^([2](ch10.html#idm45049239428888))
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，阅读图像的模型会有几层卷积，然后池化，然后再次卷积和池化。这会消耗图像的特征，并将它们分解成可能识别图像的部分。⁠^([2](ch10.html#idm45049239428888))
- en: Training Image Classification
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练图像分类
- en: After a few layers of convolutions and pooling, you can flatten or serialize
    the resulting filters into a single chain and feed it into a deeply connected
    neural network. This is why people love showing the MNIST training example; it’s
    so simple that you can actually watch the data in a single image.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几层卷积和池化后，你可以将结果滤波器展平或序列化成一个单一链，并将其馈送到一个深度连接的神经网络中。这就是为什么人们喜欢展示MNIST训练示例；它是如此简单，以至于你实际上可以在一个图像中观察数据。
- en: Take a look at the entire process to categorize a number using convolutions
    and max pooling. [Figure 10-7](#mnist_layers) should be read from bottom to top.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下使用卷积和最大池化对数字进行分类的整个过程。[图10-7](#mnist_layers)应该从底部向顶部阅读。
- en: '![layer by layer output of MNIST](assets/ltjs_1007.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST逐层输出](assets/ltjs_1007.png)'
- en: Figure 10-7\. MNIST processing the number five
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7。MNIST处理数字五
- en: If you follow the process of this image displayed in [Figure 10-7](#mnist_layers),
    you can see the input drawing down at the bottom and then a convolution of that
    input with six filters directly above it. Next, those six filters are max pooled
    or “downsampled,” and you can see they are smaller because of it. Then one more
    convolution and pooling before they are flattened out to a fully connected dense
    network layer. Above that flattened layer is one dense layer, and the last small
    layer at the top is a softmax layer with 10 possible solutions. The “five” is
    the one that’s lit up.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跟随[图10-7](#mnist_layers)中显示的这幅图像的过程，你会看到底部的输入，然后是该输入与六个滤波器的卷积直接在其上方。接下来，这六个滤波器被最大池化或“下采样”，你可以看到它们变小了。然后再进行一次卷积和池化，然后将它们展平到一个完全连接的密集网络层。在展平的层上方是一个密集层，顶部的最后一个小层是一个具有10个可能解的softmax层。被点亮的是“五”。
- en: From a bird’s-eye view, the convolution and pooling looks like magic, but it
    is digesting features of the image into patterns that neurons can recognize.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从鸟瞰视角看，卷积和池化看起来像魔术，但它将图像的特征消化成神经元可以识别的模式。
- en: 'In a layered model, this means the first layers are generally convolution and
    pooling styled layers, and then they are passed into a neural network. [Figure 10-8](#cnns_simple)
    illustrates a high-level view of the process. The following are three stages:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在分层模型中，这意味着第一层通常是卷积和池化风格的层，然后它们被传递到神经网络中。[图10-8](#cnns_simple)展示了这个过程的高层视图。以下是三个阶段：
- en: Input image
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入图像
- en: Feature extraction
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取
- en: Deeply connected neural network
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度连接的神经网络
- en: '![CNN flowchart](assets/ltjs_1008.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![CNN流程图](assets/ltjs_1008.png)'
- en: Figure 10-8\. The three basic stages of CNNs
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-8。CNN的三个基本阶段
- en: Handling Image Data
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理图像数据
- en: One of the drawbacks of training with images is that the datasets can be quite
    large and unwieldy. Datasets are generally large, but with images, they are often
    immense. This is another reason the same visual datasets are used over and over
    again.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图像进行训练的一个缺点是数据集可能非常庞大且难以处理。数据集通常很大，但对于图像来说，它们通常是巨大的。这也是为什么相同的视觉数据集一遍又一遍地被使用的另一个原因。
- en: Even when an image dataset is small, it can take up a sizable amount of memory
    when loaded into memory tensor form. You might need to break training into chunks
    of tensors for monolithic image sets. This might explain why models like MobileNet
    were optimized for a size that is considered relatively small by today’s standards.
    Increasing or decreasing an image by a single pixel across all images results
    in exponential size differences. By the very nature of the data, grayscale tensors
    are a third the size of RGB images in memory and a quarter the size of RGBA images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 即使图像数据集很小，当加载到内存张量形式时，它可能占用大量内存。你可能需要将训练分成张量的块，以处理庞大的图像集。这可能解释了为什么像MobileNet这样的模型被优化为今天标准下被认为相对较小的尺寸。在所有图像上增加或减少一个像素会导致指数级的尺寸差异。由于数据的本质，灰度张量在内存中是RGB图像的三分之一大小，是RGBA图像的四分之一大小。
- en: The Sorting Hat
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分拣帽
- en: Now it’s time for your first convolutional neural network. For this model, you’re
    going to train a CNN to classify grayscale drawings into 10 categories.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进行你的第一个卷积神经网络了。对于这个模型，你将训练一个CNN来将灰度绘画分类为10个类别。
- en: If you’re a fan of the popular book series *Harry Potter*, by J. K. Rowling,
    this will make sense and be entertaining. However, if you’ve never read a single
    *Harry Potter* book or watched any of the movies, this will still be an excellent
    exercise. In the books, there are four houses in the wizard school Hogwarts, and
    each house has animals associated with them. You’re going to ask users to draw
    a picture and use that picture to sort them into houses. I’ve prepared a dataset
    of drawings that somewhat resembles the icons and animals from each group.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是J.K.罗琳的流行书系列《哈利·波特》的粉丝，这将是有意义且有趣的。然而，如果你从未读过一本《哈利·波特》的书或者看过任何一部电影，这仍然是一个很好的练习。在书中，魔法学校霍格沃茨有四个学院，每个学院都有与之相关联的动物。你将要求用户画一幅图片，并使用该图片将它们分类到各个学院。我已经准备了一个数据集，其中的绘画在某种程度上类似于每个组的图标和动物。
- en: The dataset I’ve prepared is made from a subset of drawings from [Google’s Quick,
    Draw! Dataset](https://oreil.ly/kq3bX). The classes have been narrowed down to
    10, and the data has been significantly cleaned up.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我准备的数据集是从[Google的Quick, Draw!数据集](https://oreil.ly/kq3bX)中的一部分绘画中制作的。类别已经缩减到10个，并且数据已经经过了大幅清理。
- en: 'In the code associated with this chapter, which can be found at [*chapter10/node/node-train-houses*](https://oreil.ly/xr3Bu),
    you’ll find a ZIP file with tens of thousands of 28 x 28 drawings of the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章相关的代码可以在[*chapter10/node/node-train-houses*](https://oreil.ly/xr3Bu)找到，你会发现一个包含数万个28
    x 28绘画的ZIP文件，包括以下内容：
- en: Birds
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鸟类
- en: Owls
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 猫头鹰
- en: Parrots
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鹦鹉
- en: Snakes
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蛇
- en: Snails
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蜗牛
- en: Lions
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 狮子
- en: Tigers
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 老虎
- en: Raccoons
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浣熊
- en: Squirrels
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 松鼠
- en: Skulls
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 头巾
- en: The drawings vary wildly, but the characteristics of each class are discernable.
    Here is a random sampling of doodles, illustrated in [Figure 10-9](#house_classes).
    Once you’ve trained a model to identify each of these 10 classes, you can use
    the model to sort drawings that resemble a particular animal into its associated
    house. Birds go to Ravenclaw, lions and tigers go to Gryffindor, etc.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些绘画变化很大，但每个类别的特征是可辨认的。这里是一些涂鸦的随机样本，详见[图10-9](#house_classes)。一旦你训练了一个模型来识别这10个类别中的每一个，你就可以使用该模型将类似特定动物的绘画分类到其相关联的学院。鸟类去拉文克劳，狮子和老虎去格兰芬多，等等。
- en: '![The Hogwarts House Drawings in Grid form](assets/ltjs_1009.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![霍格沃茨学院绘画网格形式](assets/ltjs_1009.png)'
- en: Figure 10-9\. The 10 classes of drawings
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-9\. 绘画的10个类别
- en: There are a lot of ways of handling this, but the simplest will be to categorize
    the model using softmax for the final layer. As you remember, softmax will give
    us N numbers that all sum up to one. For example, if a drawing is 0.67 bird, 0.12
    owl, and 0.06 parrot, since all those represent the same house, we can sum them
    together, and the result will always be less than one. While you’re familiar with
    using models that return results like this, it will be your first softmax classification
    model that you’ve created from scratch.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的方法有很多，但最简单的方法是使用softmax对最终层进行模型分类。正如你记得的那样，softmax会给我们N个数字，它们的总和都为一。例如，如果一幅图是0.67的鸟，0.12的猫头鹰，和0.06的鹦鹉，因为它们都代表同一个学院，我们可以将它们相加，结果总是小于一。虽然你熟悉使用返回这种结果的模型，但这将是你从头开始创建的第一个softmax分类模型。
- en: Getting Started
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: 'There are a few ways you could use TensorFlow.js to train this model. Getting
    megabytes of images loaded into a browser can be done in several ways:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以使用TensorFlow.js来训练这个模型。将几兆字节的图像加载到浏览器中可以通过几种方式完成：
- en: You could load each image with subsequent HTTP requests.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用后续的HTTP请求加载每个图像。
- en: You could combine the training data into a large sprite sheet and then use your
    tensor skills to extract and stack each image into Xs and Ys.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将训练数据合并到一个大的精灵表中，然后使用你的张量技能来提取和堆叠每个图像到X和Y中。
- en: You could load the images into a CSV and from there convert them to tensors.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将图像加载到CSV中，然后将它们转换为张量。
- en: You could Base64-encode the images and load them from a single JSON file.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将图像进行Base64编码，并从单个JSON文件加载它们。
- en: The one common issue you see repeated here is that you have to do a bit of extra
    work to get your data into the sandbox of the browser. It’s for this reason that
    it’s probably best to use Node.js for image training with a significantly large
    dataset. We’ll cover situations where this is less important later in the book.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里看到的一个常见问题是，你必须做一些额外的工作，将数据加载到浏览器的沙盒中。因此，最好使用Node.js进行具有大规模数据集的图像训练。我们将在本书后面讨论这种情况不那么重要的情况。
- en: The Node.js code associated with this chapter has the training data you’ll need.
    You’ll see a file in the repository that is close to 100 MB (the GitHub limit
    for a single file), which you’ll need to unzip in place (see [Figure 10-10](#zips)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章相关的Node.js代码包含了你需要的训练数据。你会在存储库中看到一个接近100MB的文件（GitHub对单个文件的限制），你需要解压到指定位置（见[图10-10](#zips)）。
- en: '![Unzip the files.zip screenshot](assets/ltjs_1010.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![解压文件.zip截图](assets/ltjs_1010.png)'
- en: Figure 10-10\. Unzip the images into the files folder
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-10\. 将图像解压缩到文件夹中
- en: Now that you have the images and you know how to read images in Node.js, the
    code to train this model would be something like [Example 10-1](#code_training_goal).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了图片，也知道如何在Node.js中读取图片，训练这个模型的代码会类似于[示例10-1](#code_training_goal)。
- en: Example 10-1\. The ideal setup
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-1\. 理想的设置
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_image_training_CO3-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_image_training_CO3-1)'
- en: Create a simple function to load the images into the needed X and Y tensors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个简单的函数将图片加载到所需的X和Y张量中。
- en: '[![2](assets/2.png)](#co_image_training_CO3-2)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_image_training_CO3-2)'
- en: Create a suitable CNN layers model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个适合的CNN层模型。
- en: '[![3](assets/3.png)](#co_image_training_CO3-3)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_image_training_CO3-3)'
- en: Use the `shuffle` property, which shuffles the current batch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`shuffle`属性，该属性会对当前批次进行混洗。
- en: '[![4](assets/4.png)](#co_image_training_CO3-4)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_image_training_CO3-4)'
- en: Save the resulting trained model locally.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的训练模型保存在本地。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The code in [Example 10-1](#code_training_goal) does not mention setting aside
    any testing data. Because of the nature of this project, the real testing will
    be done when drawing images and identifying how each stroke moves an image closer
    or further from the desired goal. A validation set will still be used in training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例10-1](#code_training_goal)中的代码没有提及设置任何测试数据。由于这个项目的性质，真正的测试将在绘制图像并确定每个笔画如何使图像更接近或更远离期望目标时进行。在训练中仍将使用验证集。'
- en: Converting Folders of Images
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换图像文件夹
- en: 'The `folderToTensors` function will need to do the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`folderToTensors`函数需要执行以下操作：'
- en: Identify all the PNG file paths.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别所有PNG文件路径。
- en: Collect the image tensors and the answers.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集图像张量和答案。
- en: Randomize both sets.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机化两组数据。
- en: Normalize and stack the tensors.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化和堆叠张量。
- en: Clean up and return the results.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理并返回结果。
- en: To identify and access all the images, you can use a library like `glob`, which
    takes a given path like *files/**/*.png* and return an array of filenames. The
    /** iterates over all subfolders in that folder and finds all PNGs in each.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要识别和访问所有图像，可以使用类似`glob`的库，它接受一个给定的路径，如*files/**/*.png*，并返回一个文件名数组。/**会遍历该文件夹中的所有子文件夹，并找到每个文件夹中的所有PNG文件。
- en: 'You can install `glob` with NPM like so:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式使用NPM安装`glob`：
- en: '[PRE3]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that the node module is available, it can be required or imported:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在节点模块可用，可以被要求或导入：
- en: '[PRE4]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because globs operate by using a callback, you can wrap the whole function in
    a JavaScript promise to bring it back to async/await. If you’re unfamiliar with
    these concepts, feel free to brush up on them or just study the code provided
    with the chapter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于glob是通过回调来操作的，你可以将整个函数包装在JavaScript promise中，以将其转换为异步/等待。如果你对这些概念不熟悉，可以随时查阅相关资料或仅仅学习本章提供的代码。
- en: After you have a collection of file locations, you can load the file, convert
    it to a tensor, and even identify the “answer” or “y” for each image by looking
    at what folder the image came from.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了一组文件位置之后，你可以加载文件，将其转换为张量，甚至通过查看图像来自哪个文件夹来确定每个图像的“答案”或“y”。
- en: Remember that tensors create a *new tensor* every time you need to modify them.
    So rather than normalizing and stacking the tensors as we go, it will be best
    to keep the tensors in a JavaScript array.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，每次需要修改张量时都会创建一个*新张量*。因此，最好将张量保存在JavaScript数组中，而不是在进行归一化和堆叠张量时逐步进行。
- en: 'The process of reading each string into these two arrays can be accomplished
    with this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个字符串读入这两个数组的过程可以通过以下方式完成：
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `encodeDir` function is a simple function I wrote to look at the path of
    each image and return an associated predictive number:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`encodeDir`函数是我编写的一个简单函数，用于查看每个图像的路径并返回一个相关的预测数字：'
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once you have the images in tensor form, you might consider stacking and returning
    them, but it is *crucial* that you shuffle them before. Without mixing the data,
    your model will quickly train in the strangest way. Indulge me in a peculiar metaphor.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将图片转换为张量形式，你可能会考虑堆叠和返回它们，但在此之前*至关重要*的是在混洗之前对它们进行混洗。如果不混合数据，你的模型将会以最奇怪的方式快速训练。请容我用一个奇特的比喻。
- en: Imagine if I asked you to point out the shape I’m thinking of in a collection
    of shapes. You quickly learn I’m always thinking of the circle, and you start
    getting 100% accuracy. On our third test, I start saying, “No, that’s not the
    square! You’re very wrong.” So you then switch to pointing at the square and again
    get 100% accuracy. Every third test, I change the shape. While your scores are
    above 99% accurate, you never learned the actual indicator of which one to pick.
    So you go out into the field where the shape changes every time and fail. You
    never learned the indicators because the data was not shuffled.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果我让你在一堆形状中指出我在想的形状。你很快就会发现我总是在想圆形，然后你开始获得100%的准确率。在我们的第三次测试中，我开始说，“不，那不是正方形！你错得很离谱。”于是你开始指向正方形，再次获得100%的准确率。每三次测试，我都会改变形状。虽然你的分数超过99%的准确率，但你从未学会选择哪个形状的实际指标。因此，当形状每次都在变化时，你就会失败。你从未学会指标，因为数据没有被混洗。
- en: 'Unshuffled data will have the same effect: near-perfect training accuracy,
    and abysmal validation and test scores. Even though you’re shuffling each, you’ll
    only be shuffling 256 of the same values most of the time.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 未经混洗的数据将产生相同的效果：训练准确率接近完美，而验证和测试分数则很差。即使你对每个数据集进行混洗，大部分时间你只会对相同的256个值进行混洗。
- en: To shuffle the X and the Y in the same permutation, you can use `tf.utils.shuffleCombo`.
    *I hear the guy who added this function to TensorFlow.js is super cool.*
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要对X和Y进行相同的排列混洗，可以使用`tf.utils.shuffleCombo`。*我听说向TensorFlow.js添加此功能的人非常酷。*
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because this is shuffling JavaScript references, no new tensors are created
    in this shuffle.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是对JavaScript引用进行混洗，因此在此混洗中不会创建新的张量。
- en: Finally, you’ll want to convert the answers from an integer to one-hot encoding.
    The one-hot encoding is because your model will be softmax, i.e., 10 values that
    sum up to one with the correct answer being the dominant value exclusively.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow.js has a method called `oneHot` that converts numbers to one-hot
    encoded tensor values. For example, the number `3` out of `5` possible categories
    would be encoded to the tensor `[0,0,1,0,0]`. This is how we want to format our
    answers to match the expected output of the categorical model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now stack the X and Y array values into a large tensor and normalize
    the images to be values `0-1` by dividing by `255`. The stacking and encoding
    would look like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Your computer will likely pause between each log as thousands of images are
    processed. The code prints the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we have our X and Y for training, and their shape is the input and output
    shape of the model we will create.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The CNN Model
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s time to create the convolutional neural network model. The architecture
    for this model will be three pairs of convolution and pooling layers. On each
    new convolutional layer, we will double the number of filters to train. Then we’ll
    flatten the model to a single dense hidden layer of 128 units that have a `tanh`
    activation, and finish with a final layer of 10 possible outputs with the softmax
    activation. If you’re confused about why we’re using softmax, please review the
    structure of classification models we covered in [Chapter 9](ch09.html#the_chapter_9).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able to write the model layers from the description alone, but
    here’s the code to create the described sequential model:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This new final layer for categorical data that is not binary means that you’ll
    need to change your loss function from `binaryCrossentropy` to `categoricalCrossentropy`
    accordingly. So now the `model.compile` code would look like this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s review the `model.summary()` method through the lens of what we’ve learned
    about convolutions and max pooling so we can ensure we’ve built everything correctly.
    You can see the printout of the results in [Example 10-2](#output_conv_model_summary).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-2\. The output of `model.summary()`
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_image_training_CO4-1)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The first convolutional layer takes an input of `[stacksize, 28, 28, 1]` and
    has a convolutional output of `[stacksize, 28, 28, 16]`. The size is the same
    because we’re using `padding: ''same''`, and the 16 is the 16 different filter
    results we got when we specified `filters: 16`. You can think of this as 16 new
    filtered images for each image in the stack. This gives the network 160 new parameters
    to train. The trainable parameters are calculated as (number of images in) * (kernel
    window) * (images out) + (images out), which comes out to `1 * (3x3) * 16 + 16
    = 160`.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO4-2)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The max pooling cuts the filtered image rows and column sizes in half, which
    quarters the pixels. This layer does not have any trainable parameter since the
    algorithm is fixed.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO4-3)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The convolution and pooling happens again, and more filters are being employed
    at each level. The size of the image is shrinking, and the number of trainable
    parameters are swiftly growing, i.e., `16 * (3x3) * 32 + 32 = 4,640`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO4-4)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is one final convolution and pooling. Pooling an odd number creates
    a larger-than-50% reduction.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_image_training_CO4-5)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Flattening the 64 3 x 3 images turns into a single layer of 576 units.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_image_training_CO4-6)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Each of the 576 units is densely connected to the 128-unit layer. Using the
    traditional calculation of lines + nodes, this comes out to `(576 * 128) + 128
    = 73,856` trainable parameters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_image_training_CO4-7)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last layer lands with 10 possible values for each class.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we’re evaluating `model.summary()` instead of inspecting
    a graphical representation of what’s happening. Even at lower dimensionality,
    the graphical representation of what’s happening is difficult to illustrate. I’ve
    done my best to create a somewhat exhaustive illustration in [Figure 10-11](#sorting_process).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们评估`model.summary()`而不是检查正在发生的事情的图形表示。即使在较低的维度，图形表示正在发生的事情也很难说明。我已经尽力在[图10-11](#sorting_process)中创建了一个相对详尽的插图。
- en: '![CNN neural network](assets/ltjs_1011.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![CNN神经网络](assets/ltjs_1011.png)'
- en: Figure 10-11\. A visualization of each layer
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-11\. 每一层的可视化
- en: Unlike previous neural network diagrams, the visual explanation of CNNs can
    be somewhat limiting. Stacks upon stacks of filtered images are only so informative.
    The result of the convolutional process is flattened and connected to a deeply
    connected neural network. You’ve reached a point of complexity where the `summary()`
    method of a model is the best way to understand the contents.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的神经网络图不同，CNN的可视化解释可能有些局限性。堆叠在一起的滤波图像只能提供有限的信息。卷积过程的结果被展平并连接到一个深度连接的神经网络。您已经达到了一个复杂性的程度，`summary()`方法是理解内容的最佳方式。
- en: Tip
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’d like a dynamic visual and to watch each trained filter’s result activation
    at each layer, Polo Club of Data Science created a beautiful [CNN Explainer](https://oreil.ly/o24uR)
    in TensorFlow.js. Check out [the interactive visualizer](https://oreil.ly/SYHsp).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要一个动态的可视化，并观看每个训练滤波器在每一层的激活结果，数据科学Polo Club创建了一个美丽的[CNN解释器](https://oreil.ly/o24uR)在TensorFlow.js中。查看[交互式可视化器](https://oreil.ly/SYHsp)。
- en: There you go. Your resulting `[3, 3, 64]` flattens to 576 artificial neurons
    before connecting to the neural network. Not only did you create image features,
    but you’ve simplified an input from a `[28, 28, 1]` image, which was originally
    going to require 784 densely connected inputs. Armed with this more advanced architecture,
    you can load the data from `folderToTensors()` and create the necessary model.
    You’re ready to train.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经到了那里。您的结果`[3, 3, 64]`在连接到神经网络之前展平为576个人工神经元。您不仅创建了图像特征，还简化了一个`[28, 28, 1]`图像的输入，原本需要784个密集连接的输入。有了这种更先进的架构，您可以从`folderToTensors()`加载数据并创建必要的模型。您已经准备好训练了。
- en: Training and Saving
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和保存
- en: Since this is training in Node.js, you will have to setup GPU-acceleration on
    the machine directly. This is often done with NVIDIA CUDA, and CUDA Deep Neural
    Network (cuDNN). You will have to set up CUDA and cuDNN properly to work with
    your GPU if you want to train using `@tensorflow/tfjs-node-gpu` and get significant
    speed boost over normal `tfjs-node`. See [Figure 10-12](#cuda_gpu2).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是在Node.js中进行训练，您将不得不直接在机器上设置GPU加速。这通常是通过NVIDIA CUDA和CUDA深度神经网络（cuDNN）完成的。如果您想使用`@tensorflow/tfjs-node-gpu`进行训练并获得比普通`tfjs-node`更快的速度提升，您将不得不正确设置CUDA和cuDNN以与您的GPU一起工作。请参阅[图10-12](#cuda_gpu2)。
- en: '![CUDA GPU Screenshot](assets/ltjs_1012.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![CUDA GPU截图](assets/ltjs_1012.png)'
- en: Figure 10-12\. 3x–4x speed boost with GPU
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-12\. 使用GPU可以提高3-4倍的速度
- en: The resulting model, after 20 epochs, lands around 95% accuracy in training
    and 90% accuracy for the validation set. The file size of the resulting model
    is around 400 KB. You might have noticed the accuracy for the training set continuously
    trending upward, but the validation can sometimes go down. For better or worse,
    the last epoch will be the model that gets saved. If you’d like to ensure the
    highest possible validation accuracy, take a look at the Chapter Challenge at
    the end.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在20个时代之后，生成的模型在训练中的准确率约为95%，在验证集中的准确率约为90%。生成模型的文件大小约为400 KB。您可能已经注意到训练集的准确率不断上升，但验证集有时会下降。不管好坏，最后一个时代将是保存的模型。如果您想确保最高可能的验证准确性，请查看最后的章节挑战。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you run this model for too many epochs, the model will overfit and approach
    100% training accuracy with a diminished validation accuracy.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对这个模型运行了太多时代，模型将过拟合，并接近100%的训练准确率，而验证准确率会降低。
- en: Testing the Model
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: To test this model, you’ll need drawings from the user. You can create a simple
    drawing surface on a web page with a canvas. A canvas can subscribe to events
    on the mouse being pressed down, when the mouse is moving along the canvas, and
    when the mouse is released. Using these events, you can draw from point to point.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试这个模型，您需要用户的绘图。您可以在网页上创建一个简单的绘图表面，使用一个画布。画布可以订阅鼠标按下时、鼠标沿着画布移动时以及鼠标释放时的事件。使用这些事件，您可以从一个点绘制到另一个点。
- en: Building a Sketchpad
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个草图板
- en: 'You can build a simple drawable canvas with those three events. You’ll use
    some new methods to move the canvas path and draw lines, but it’s quite readable.
    The following code sets one up:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这三个事件构建一个简单的可绘制画布。您将使用一些新方法来移动画布路径和绘制线条，但这是相当易读的。以下代码设置了一个：
- en: '[PRE13]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The drawings are made from a bunch of smaller lines that have a stroke width
    of 14 pixels and are automatically rounded on the edges. You can see a test drawing
    in [Figure 10-13](#canvas_test).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图纸是由一堆较小的线条制成的，线条的笔画宽度为14像素，并且在边缘自动圆润。您可以在[图10-13](#canvas_test)中看到一个测试绘图。
- en: '![Example drawing](assets/ltjs_1013.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![示例绘图](assets/ltjs_1013.png)'
- en: Figure 10-13\. Works well enough
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-13\. 运行得足够好
- en: When the user clicks the mouse while on the canvas, any movement will be drawn
    from point to new point. Whenever the user stops drawing, the `drawEnd` function
    will be called. You could add a button to classify the canvas or tie directly
    into the `drawEnd` function and classify the image.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户在画布上单击鼠标时，任何移动都将从一个点绘制到新点。每当用户停止绘制时，将调用`drawEnd`函数。您可以添加一个按钮来对画布进行分类，或者直接将其连接到`drawEnd`函数并对图像进行分类。
- en: Reading the Sketchpad
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读草图板
- en: When you call `tf.browser.fromPixels` on the canvas, you’ll get 100% black pixels.
    Why is this? The answer is that the canvas has nothing drawn in some places and
    black pixels in other areas. When the canvas is converted to tensor values, it
    will convert emptiness to black. The canvas might look like it has a white background,
    but it’s actually clear and would show whatever color or pattern is underneath
    (see [Figure 10-14](#under_canvas)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在画布上调用 `tf.browser.fromPixels` 时，你会得到 100% 的黑色像素。为什么会这样？答案是画布的某些地方没有绘制任何内容，而其他地方是黑色像素。当画布转换为张量值时，它会将空白转换为黑色。画布可能看起来有白色背景，但实际上是透明的，会显示底部的任何颜色或图案（参见
    [图 10-14](#under_canvas)）。
- en: '![A canvas is empty](assets/ltjs_1014.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![一个空的画布](assets/ltjs_1014.png)'
- en: Figure 10-14\. A canvas is transparent—and so empty pixels are zero
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-14\. 一个画布是透明的，所以空白像素为零
- en: 'To fix this, you can add a clear function that writes a large white square
    in the canvas, so the black lines will be on a white background like the training
    images. This is also the function you can use to clear the canvas between drawings.
    To fill the canvas with a white background, you would use the `fillRect` method
    you used to outline the labels in [Chapter 6](ch06.html#the_chapter_6):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你可以添加一个清除函数，在画布上绘制一个大的白色正方形，这样黑色线条就会在白色背景上，就像训练图像一样。这也是你可以在绘画之间清除画布的函数。要用白色背景填充画布，你可以使用
    `fillRect` 方法，就像你在 [第 6 章](ch06.html#the_chapter_6) 中用来勾画标签的方法一样。
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the canvas is initialized with a white background, you can make a prediction
    on the canvas drawing:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦画布用白色背景初始化，你就可以对画布绘制进行预测了。
- en: '[PRE15]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_image_training_CO5-1)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_image_training_CO5-1)'
- en: When you read the canvas, don’t forget to identify that you’re only interested
    in a single channel; otherwise, you’ll need to turn a 3D tensor into a 1D tensor
    before continuing.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当你读取画布时，不要忘记标识你只对单个通道感兴趣；否则，你需要在继续之前将 3D 张量转换为 1D 张量。
- en: '[![2](assets/2.png)](#co_image_training_CO5-2)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_image_training_CO5-2)'
- en: Resize using the nearest neighbor algorithm down to a 28 x 28 image for input
    into the model. Pixelation caused by nearest neighbor is inconsequential here,
    so it’s a smart choice because it is faster than `resizeBilinear`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最近邻算法将图像调整为 28 x 28 的大小，以输入到模型中。最近邻引起的像素化在这里并不重要，所以这是一个明智的选择，因为它比 `resizeBilinear`
    更快。
- en: '[![3](assets/3.png)](#co_image_training_CO5-3)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_image_training_CO5-3)'
- en: The model expects a batch, so prepare the data as a batch of one. This creates
    a `[1, 28, 28, 1]` input tensor.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 模型期望一个批次数据，所以准备数据作为一个批次的数据。这将创建一个 `[1, 28, 28, 1]` 的输入张量。
- en: '[![4](assets/4.png)](#co_image_training_CO5-4)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_image_training_CO5-4)'
- en: The prediction result has been returned to JavaScript as a single batch of 10
    numbers. Think up a creative way to display the results.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果已经作为一个包含 10 个数字的批次返回到 JavaScript。想出一种创造性的方式来展示结果。
- en: Now that you have the results, you can portray the answers in any format you
    please. Personally, I organized the scores by house and used them to set the opacity
    of the labels. This way, you could get feedback as you drew each line. The opacity
    of labels depends on values `0-1`, which works splendidly with the results of
    the softmax predictions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经得到了结果，你可以以任何你喜欢的格式展示答案。我个人是按照房间组织了分数，并用它们来设置标签的不透明度。这样，你可以在每画一条线时得到反馈。标签的不透明度取决于值
    `0-1`，这与 softmax 预测结果非常契合。
- en: '[PRE16]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You might wonder if Ravenclaw has a slight mathematical advantage because it
    is composed of more classes, and you’d be right. With all things equal, a completely
    random set of lines is more likely to be classified as Ravenclaw because it has
    a majority of the classes. However, this is statistically insignificant when the
    drawings are non-random. If you want the model to have only nine classes, remove
    `bird` and retrain to create the most balanced classification spectrum.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道拉文克劳是否有轻微的数学优势，因为它由更多类别组成，你是对的。在所有条件相同的情况下，一组完全随机的线更有可能被分类为拉文克劳，因为它拥有大多数类别。然而，当图像不是随机的时，这在统计上是不显著的。如果你希望模型只有九个类别，可以移除
    `bird` 并重新训练，以创建最平衡的分类谱。
- en: Tip
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re interested in identifying which classes are likely to be problematic
    or confused, you can use visual reporting tools like a confusion matrix or the
    [t-SNE](https://oreil.ly/sBio5) algorithm. These tools are especially helpful
    for evaluating the training data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣确定哪些类别可能存在问题或混淆，你可以使用视觉报告工具，如混淆矩阵或 [t-SNE](https://oreil.ly/sBio5) 算法。这些工具对于评估训练数据特别有帮助。
- en: I highly recommend you load the code for this chapter from [*chapter10/simple/simplest-draw*](https://oreil.ly/emOWR)
    and put your artistic skills to the test! My bird drawing sorted me into Ravenclaw,
    as shown in [Figure 10-15](#ravenclaw_ftw).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你从[*chapter10/simple/simplest-draw*](https://oreil.ly/emOWR)加载本章的代码，并测试一下你的艺术技能！我的鸟类绘画将我分类到了拉文克劳，如
    [图 10-15](#ravenclaw_ftw) 所示。
- en: '![The web page correctly identifying a bird](assets/ltjs_1015.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![网页正确识别出一只鸟](assets/ltjs_1015.png)'
- en: Figure 10-15\. A UI and drawing masterpiece
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-15\. 一个 UI 和绘画杰作
- en: I was able to poorly draw and properly get sorted into each of the other possible
    houses as well. However, I won’t punish you with any more of my “art.”
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我能够糟糕地画出并被正确分类到其他可能的房间中。然而，我不会再用我的“艺术”来惩罚你。
- en: Chapter Review
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节回顾
- en: You’ve trained a model on visual data. While this dataset was limited to grayscale
    drawings, the lessons you’ve learned could work with any image dataset. There
    are plenty of excellent image datasets out there that would work perfectly with
    the model you’ve created. We’ll cover more in the next two chapters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在视觉数据上训练了一个模型。虽然这个数据集仅限于灰度图，但你所学到的经验可以适用于任何图像数据集。有很多优秀的图像数据集可以与你创建的模型完美配合。我们将在接下来的两章中详细介绍。
- en: I’ve created a more elaborate page for the [drawing identification featured](https://oreil.ly/jnlhb)
    in this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我为本章中的[绘画识别特色](https://oreil.ly/jnlhb)创建了一个更加复杂的页面。
- en: 'Chapter Challenge: Saving the Magic'
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 章节挑战：保存魔法
- en: If you’re most interested in getting the highest validation accuracy model,
    it’s slim odds that your best version of the model will be the last one. For example,
    if you take a look at [Figure 10-16](#accuracy_changes), a validations accuracy
    of 90.3% gets lost, and you end up with 89.6% as the final validation of the model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您最感兴趣的是获得最高验证准确性模型，那么您的最佳模型版本很可能不是最后一个版本。例如，如果您查看[图10-16](#accuracy_changes)，90.3%的验证准确性会丢失，最终验证模型为89.6%。
- en: For this Chapter Challenge, rather than saving the final trained version of
    the model, add a callback that saves the model when the validation accuracy has
    reached a new record best. This kind of code is useful because it will allow you
    to run for many epochs. As the model overfits, you’ll be able to keep the best
    generalizable model for production.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章挑战，与其保存模型的最终训练版本，不如添加一个回调函数，当验证准确性达到新的最佳记录时保存模型。这种代码非常有用，因为它允许您运行多个时期。随着模型过拟合，您将能够保留最佳的通用模型用于生产。
- en: '![validation vs. training accuracy](assets/ltjs_1016.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![验证与训练准确性](assets/ltjs_1016.png)'
- en: Figure 10-16\. Evaluate which accuracy is more important
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-16。评估哪个准确性更重要
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[附录B](app02.html#appendix_b)中找到此挑战的答案。
- en: Review Questions
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复习问题
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下你在本章编写的代码中学到的教训。花点时间回答以下问题：
- en: Convolutional layers have many trainable *what* that help extract features of
    an image?
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层有许多可训练的*什么*，可以帮助提取图像的特征？
- en: What is the name of the property that controls the convolutional window size?
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制卷积窗口大小的属性名称是什么？
- en: If you want a convolutional result to be the same size as the original image,
    what should you set as the padding?
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你希望卷积结果与原始图像大小相同，应该将填充设置为什么？
- en: True or false? You must flatten an image before inserting it into a convolutional
    layer.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真或假？在将图像插入卷积层之前，必须将其展平。
- en: What would be the output size of a max pool 3 x 3 with a stride of three on
    an 81 x 81 image?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在81 x 81图像上，步幅为3的最大池3 x 3的输出大小将是多少？
- en: If you were to one-hot encode the number 12, do you have enough information
    to do so?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您要对数字12进行独热编码，您是否有足够的信息来这样做？
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在[附录A](app01.html#book_appendix)中找到。
- en: ^([1](ch10.html#idm45049239647096-marker)) Videos and lectures by [3Blue1Brown](https://oreil.ly/zuGzT)
    on YouTube are excellent starts for anyone looking to go down the convolution
    rabbit hole.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#idm45049239647096-marker)) YouTube上的[3Blue1Brown](https://oreil.ly/zuGzT)的视频和讲座是任何想要深入了解卷积的人的绝佳起点。
- en: ^([2](ch10.html#idm45049239428888-marker)) There are other pooling methods available
    in TensorFlow.js for experimentation.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#idm45049239428888-marker)) TensorFlow.js中还有其他可用于实验的池化方法。
