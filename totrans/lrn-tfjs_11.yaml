- en: Chapter 10\. Image Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Everything must be made as simple as possible. But not simpler.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Albert Einstein
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I’m going to describe a number to you. I’d like you to figure out the number
    from the description of its features. The number I’m thinking of has a rounded
    top, a line down the right side only, and a loopy thing at the bottom that overlaps.
    Take a moment and mentally map the number I just described. With those three features,
    you can probably figure it out.
  prefs: []
  type: TYPE_NORMAL
- en: The features of a visual number can vary, but a smart description means you
    can identify the numbers in your mind. When I said “rounded top,” you can immediately
    throw out some numbers, and the same goes for “a line down the right side only.”
    The features compose what makes the number unique.
  prefs: []
  type: TYPE_NORMAL
- en: If you were to describe the numbers as shown in [Figure 10-1](#number_features)
    and put their descriptions in a CSV file, you could probably get 100% accuracy
    with a trained neural network on that data. The whole thing works except that
    it depends on a human to describe the top, middle, and bottom of each number.
    How do you automate the human aspect of this? If you can make a computer identify
    the loops, colors, and curves that are unique characteristics of an image and
    then feed that into a neural network, the machine could learn the needed patterns
    to classify the descriptions into images.
  prefs: []
  type: TYPE_NORMAL
- en: '![The number two in three parts.](assets/ltjs_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Congratulations if you figured out it was the number two
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thankfully, this problem of feature engineering images has been solved with
    fantastic computer vision tricks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn the concepts of convolutional layers for models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build your first multiclass model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to read and process images for TensorFlow.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draw on a canvas and classify the drawing accordingly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you finish this chapter, you’ll be able to create your own image classification
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutions come from the mathematical world of expressing shapes and functions.
    You can dig for a long, long time into the concept of what a convolution is in
    mathematics and then reapply that knowledge from the ground up to the idea of
    gathering information on digital images. If you’re a fan of mathematics and computer
    graphics, this is quite an exciting rabbit hole.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s not essential that you spend a week learning the fundamentals
    of convolutional operations when you have a framework like TensorFlow.js. It is
    for that reason that we’ll focus on the high-level benefits and properties of
    convolutional operations and how they are used in neural networks. As always,
    you are encouraged to research the deep history of convolutions beyond this quick-start.^([1](ch10.html#idm45049239647096))
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the most prominent concept you should take from a nonmathematical
    explanation of convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: The two images of the number two in [Figure 10-2](#compare_nums) are the *exact*
    same number shifted from left to right in their bounding boxes. Each of these
    converted into tensors would create significantly different unequal tensors. However,
    in the feature system we described at the beginning of the chapter, these features
    would be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution Features vs no Features](assets/ltjs_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Convolutions simplify the essence of an image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For visual tensors, an image’s features are more important than the exact locations
    of each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions Quick Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A convolutional operation is used to extract high-level features such as edges,
    gradients, colors, and more. These are the key features that classify a given
    visual.
  prefs: []
  type: TYPE_NORMAL
- en: So what features should it extract? That’s not something we actually decide.
    You can control the number of filters used in finding features, but the actual
    features that best define usable patterns are defined in the training process.
    The filters start with accenting and drawing out features from the image.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, take a look at the photo in [Figure 10-3](#pumpkin). The jack-o’-lantern
    is multiple colors and barely contrasts against a blurry but somewhat light and
    dark background. As a human, you have no issue identifying what this is in the
    photo.
  prefs: []
  type: TYPE_NORMAL
- en: '![Photograph of jack-o-lantern art](assets/ltjs_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Jack-o’-lantern art
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now here’s the same image with a 3 x 3 edge detection filter convolved over
    the pixels. Notice how the result is significantly simplified and more evident
    in [Figure 10-4](#pumpkin2).
  prefs: []
  type: TYPE_NORMAL
- en: Different filters accent different aspects of each image to simplify and clarify
    the contents. It doesn’t have to stop there; you can run activations to emphasize
    the detected features, and you can even run convolutions on convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the result? You’ve feature engineered the image. Preprocessing images
    through a variety of filters lets your neural network see patterns that would
    have required a much larger, slower, and more complicated model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution result of previous image](assets/ltjs_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Convolution result
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adding Convolution Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks to TensorFlow.js, adding a convolutional layer is just as easy as adding
    a dense layer, but it’s called `conv2d` and has its own properties.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_training_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Identify how many filters to run.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `kernelSize` controls the size of the filter. The `3` here represents a
    3 x 3 filter.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The little 3 x 3 filter won’t fit your image, so it will need to slide over
    the image. The stride is how many pixels the filter slides each time.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The padding lets the convolution decide what to do when your `strides` and `kernelSize`
    don’t evenly divide into your image width and height. When you set padding to
    `same`, zeros are added around the image so that the size of the resulting convolutional
    images is kept the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_image_training_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The results are then run through the activation function of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_image_training_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The input is an image tensor, so the input image is the rank-three shape for
    the model. This is not a required restriction for convolutions, as you learned
    in [Chapter 6](ch06.html#the_chapter_6), but it is recommended if you are not
    making a fully convolutional model.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t feel overwhelmed by the list of possible parameters. Imagine having to
    code all those different settings by yourself. You can configure your convolutions
    like existing models or go crazy with numbers to see how it affects your results.
    Tuning these parameters and experimenting is the benefit of a framework like TensorFlow.js.
    Most importantly, build your intuition over time.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this `conv2d` layer is meant for images. Similarly,
    you would use `conv1d` for a linear series and `conv3d` when working on 3D spatial
    objects. Most of the time, 2D is used, but the concept is not limited.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Max Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve simplified the image with filters via a convolutional layer, you’re
    left with a lot of empty space in your filtered graphic. Also, the number of input
    parameters has significantly increased due to all the image filters.
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling is a way to simplify the most active features identified in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Max Pooling Quick Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To condense the resulting image size, you reduce the output using max pooling.
    Max pooling, put plainly, is keeping the most active pixel in a window as a representation
    of the entire block of pixels in that window. You then slide the window over and
    take the max of that. These results are pooled together to make a much smaller
    image as long as the stride of the window was greater than 1.
  prefs: []
  type: TYPE_NORMAL
- en: The following example quarters the size of an image by taking the largest number
    of each subsquare. Study the illustration in [Figure 10-5](#max_pool).
  prefs: []
  type: TYPE_NORMAL
- en: '![Max pooling demo](assets/ltjs_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Max pool with 2 x 2 kernel and stride of 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `kernelSize` in [Figure 10-5](#max_pool) is 2 x 2\. So the four top-left
    squares are evaluated together, and out of the numbers `[12, 5, 11, 7]`, the largest
    is `12`. That max number is passed along to the result. With a stride of two,
    the square of the kernel window moves completely adjacent to the previous square
    and starts again with the numbers `[20, 0, 12, 3]`. This means the most powerful
    activation in each window is passed along.
  prefs: []
  type: TYPE_NORMAL
- en: You might feel that this process chops up an image and destroys the contents,
    but you’d be surprised to know that the resulting image is quite recognizable.
    Max pooling even emphasizes the detections and makes images more recognizable.
    See [Figure 10-6](#pumpkin3), which is the result of running max pool on the convolution
    of the jack-o’-lantern from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Max pooling emphasizes detections](assets/ltjs_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. 2 x 2 kernel max pool result of a convolution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While both [Figure 10-4](#pumpkin2) and [Figure 10-6](#pumpkin3) appear the
    same size for illustration purposes, the latter is somewhat clearer and a quarter
    of the size due to the pooling process.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Max Pooling Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the `conv2d`, a max pool is added as a layer, generally immediately
    after a convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_training_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `poolSize` is the window size, just like `kernelSize` was. The previous
    examples have been 2 (which is short for 2 x 2).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `strides` is how far right and down to move the window in each operation.
    This can also be written as `strides: [2, 2]`.'
  prefs: []
  type: TYPE_NORMAL
- en: Often, a model that is reading an image will have several layers of convolution,
    then pooling, and then convolution and pooling again and again. This chews up
    the features of an image and breaks them into parts that could potentially identify
    an image.⁠^([2](ch10.html#idm45049239428888))
  prefs: []
  type: TYPE_NORMAL
- en: Training Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After a few layers of convolutions and pooling, you can flatten or serialize
    the resulting filters into a single chain and feed it into a deeply connected
    neural network. This is why people love showing the MNIST training example; it’s
    so simple that you can actually watch the data in a single image.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the entire process to categorize a number using convolutions
    and max pooling. [Figure 10-7](#mnist_layers) should be read from bottom to top.
  prefs: []
  type: TYPE_NORMAL
- en: '![layer by layer output of MNIST](assets/ltjs_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. MNIST processing the number five
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you follow the process of this image displayed in [Figure 10-7](#mnist_layers),
    you can see the input drawing down at the bottom and then a convolution of that
    input with six filters directly above it. Next, those six filters are max pooled
    or “downsampled,” and you can see they are smaller because of it. Then one more
    convolution and pooling before they are flattened out to a fully connected dense
    network layer. Above that flattened layer is one dense layer, and the last small
    layer at the top is a softmax layer with 10 possible solutions. The “five” is
    the one that’s lit up.
  prefs: []
  type: TYPE_NORMAL
- en: From a bird’s-eye view, the convolution and pooling looks like magic, but it
    is digesting features of the image into patterns that neurons can recognize.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a layered model, this means the first layers are generally convolution and
    pooling styled layers, and then they are passed into a neural network. [Figure 10-8](#cnns_simple)
    illustrates a high-level view of the process. The following are three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Input image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deeply connected neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![CNN flowchart](assets/ltjs_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. The three basic stages of CNNs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Handling Image Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the drawbacks of training with images is that the datasets can be quite
    large and unwieldy. Datasets are generally large, but with images, they are often
    immense. This is another reason the same visual datasets are used over and over
    again.
  prefs: []
  type: TYPE_NORMAL
- en: Even when an image dataset is small, it can take up a sizable amount of memory
    when loaded into memory tensor form. You might need to break training into chunks
    of tensors for monolithic image sets. This might explain why models like MobileNet
    were optimized for a size that is considered relatively small by today’s standards.
    Increasing or decreasing an image by a single pixel across all images results
    in exponential size differences. By the very nature of the data, grayscale tensors
    are a third the size of RGB images in memory and a quarter the size of RGBA images.
  prefs: []
  type: TYPE_NORMAL
- en: The Sorting Hat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it’s time for your first convolutional neural network. For this model, you’re
    going to train a CNN to classify grayscale drawings into 10 categories.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re a fan of the popular book series *Harry Potter*, by J. K. Rowling,
    this will make sense and be entertaining. However, if you’ve never read a single
    *Harry Potter* book or watched any of the movies, this will still be an excellent
    exercise. In the books, there are four houses in the wizard school Hogwarts, and
    each house has animals associated with them. You’re going to ask users to draw
    a picture and use that picture to sort them into houses. I’ve prepared a dataset
    of drawings that somewhat resembles the icons and animals from each group.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset I’ve prepared is made from a subset of drawings from [Google’s Quick,
    Draw! Dataset](https://oreil.ly/kq3bX). The classes have been narrowed down to
    10, and the data has been significantly cleaned up.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code associated with this chapter, which can be found at [*chapter10/node/node-train-houses*](https://oreil.ly/xr3Bu),
    you’ll find a ZIP file with tens of thousands of 28 x 28 drawings of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Birds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Owls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parrots
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Snakes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Snails
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tigers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Raccoons
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Squirrels
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Skulls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The drawings vary wildly, but the characteristics of each class are discernable.
    Here is a random sampling of doodles, illustrated in [Figure 10-9](#house_classes).
    Once you’ve trained a model to identify each of these 10 classes, you can use
    the model to sort drawings that resemble a particular animal into its associated
    house. Birds go to Ravenclaw, lions and tigers go to Gryffindor, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Hogwarts House Drawings in Grid form](assets/ltjs_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. The 10 classes of drawings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a lot of ways of handling this, but the simplest will be to categorize
    the model using softmax for the final layer. As you remember, softmax will give
    us N numbers that all sum up to one. For example, if a drawing is 0.67 bird, 0.12
    owl, and 0.06 parrot, since all those represent the same house, we can sum them
    together, and the result will always be less than one. While you’re familiar with
    using models that return results like this, it will be your first softmax classification
    model that you’ve created from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few ways you could use TensorFlow.js to train this model. Getting
    megabytes of images loaded into a browser can be done in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: You could load each image with subsequent HTTP requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could combine the training data into a large sprite sheet and then use your
    tensor skills to extract and stack each image into Xs and Ys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could load the images into a CSV and from there convert them to tensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could Base64-encode the images and load them from a single JSON file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The one common issue you see repeated here is that you have to do a bit of extra
    work to get your data into the sandbox of the browser. It’s for this reason that
    it’s probably best to use Node.js for image training with a significantly large
    dataset. We’ll cover situations where this is less important later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: The Node.js code associated with this chapter has the training data you’ll need.
    You’ll see a file in the repository that is close to 100 MB (the GitHub limit
    for a single file), which you’ll need to unzip in place (see [Figure 10-10](#zips)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Unzip the files.zip screenshot](assets/ltjs_1010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. Unzip the images into the files folder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you have the images and you know how to read images in Node.js, the
    code to train this model would be something like [Example 10-1](#code_training_goal).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-1\. The ideal setup
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_training_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a simple function to load the images into the needed X and Y tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a suitable CNN layers model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `shuffle` property, which shuffles the current batch.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Save the resulting trained model locally.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The code in [Example 10-1](#code_training_goal) does not mention setting aside
    any testing data. Because of the nature of this project, the real testing will
    be done when drawing images and identifying how each stroke moves an image closer
    or further from the desired goal. A validation set will still be used in training.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Folders of Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `folderToTensors` function will need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify all the PNG file paths.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect the image tensors and the answers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomize both sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize and stack the tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean up and return the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To identify and access all the images, you can use a library like `glob`, which
    takes a given path like *files/**/*.png* and return an array of filenames. The
    /** iterates over all subfolders in that folder and finds all PNGs in each.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install `glob` with NPM like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the node module is available, it can be required or imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Because globs operate by using a callback, you can wrap the whole function in
    a JavaScript promise to bring it back to async/await. If you’re unfamiliar with
    these concepts, feel free to brush up on them or just study the code provided
    with the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After you have a collection of file locations, you can load the file, convert
    it to a tensor, and even identify the “answer” or “y” for each image by looking
    at what folder the image came from.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that tensors create a *new tensor* every time you need to modify them.
    So rather than normalizing and stacking the tensors as we go, it will be best
    to keep the tensors in a JavaScript array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of reading each string into these two arrays can be accomplished
    with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `encodeDir` function is a simple function I wrote to look at the path of
    each image and return an associated predictive number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the images in tensor form, you might consider stacking and returning
    them, but it is *crucial* that you shuffle them before. Without mixing the data,
    your model will quickly train in the strangest way. Indulge me in a peculiar metaphor.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if I asked you to point out the shape I’m thinking of in a collection
    of shapes. You quickly learn I’m always thinking of the circle, and you start
    getting 100% accuracy. On our third test, I start saying, “No, that’s not the
    square! You’re very wrong.” So you then switch to pointing at the square and again
    get 100% accuracy. Every third test, I change the shape. While your scores are
    above 99% accurate, you never learned the actual indicator of which one to pick.
    So you go out into the field where the shape changes every time and fail. You
    never learned the indicators because the data was not shuffled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unshuffled data will have the same effect: near-perfect training accuracy,
    and abysmal validation and test scores. Even though you’re shuffling each, you’ll
    only be shuffling 256 of the same values most of the time.'
  prefs: []
  type: TYPE_NORMAL
- en: To shuffle the X and the Y in the same permutation, you can use `tf.utils.shuffleCombo`.
    *I hear the guy who added this function to TensorFlow.js is super cool.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Because this is shuffling JavaScript references, no new tensors are created
    in this shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you’ll want to convert the answers from an integer to one-hot encoding.
    The one-hot encoding is because your model will be softmax, i.e., 10 values that
    sum up to one with the correct answer being the dominant value exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow.js has a method called `oneHot` that converts numbers to one-hot
    encoded tensor values. For example, the number `3` out of `5` possible categories
    would be encoded to the tensor `[0,0,1,0,0]`. This is how we want to format our
    answers to match the expected output of the categorical model.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now stack the X and Y array values into a large tensor and normalize
    the images to be values `0-1` by dividing by `255`. The stacking and encoding
    would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Your computer will likely pause between each log as thousands of images are
    processed. The code prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we have our X and Y for training, and their shape is the input and output
    shape of the model we will create.
  prefs: []
  type: TYPE_NORMAL
- en: The CNN Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s time to create the convolutional neural network model. The architecture
    for this model will be three pairs of convolution and pooling layers. On each
    new convolutional layer, we will double the number of filters to train. Then we’ll
    flatten the model to a single dense hidden layer of 128 units that have a `tanh`
    activation, and finish with a final layer of 10 possible outputs with the softmax
    activation. If you’re confused about why we’re using softmax, please review the
    structure of classification models we covered in [Chapter 9](ch09.html#the_chapter_9).
  prefs: []
  type: TYPE_NORMAL
- en: 'You should be able to write the model layers from the description alone, but
    here’s the code to create the described sequential model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This new final layer for categorical data that is not binary means that you’ll
    need to change your loss function from `binaryCrossentropy` to `categoricalCrossentropy`
    accordingly. So now the `model.compile` code would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let’s review the `model.summary()` method through the lens of what we’ve learned
    about convolutions and max pooling so we can ensure we’ve built everything correctly.
    You can see the printout of the results in [Example 10-2](#output_conv_model_summary).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-2\. The output of `model.summary()`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_training_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first convolutional layer takes an input of `[stacksize, 28, 28, 1]` and
    has a convolutional output of `[stacksize, 28, 28, 16]`. The size is the same
    because we’re using `padding: ''same''`, and the 16 is the 16 different filter
    results we got when we specified `filters: 16`. You can think of this as 16 new
    filtered images for each image in the stack. This gives the network 160 new parameters
    to train. The trainable parameters are calculated as (number of images in) * (kernel
    window) * (images out) + (images out), which comes out to `1 * (3x3) * 16 + 16
    = 160`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The max pooling cuts the filtered image rows and column sizes in half, which
    quarters the pixels. This layer does not have any trainable parameter since the
    algorithm is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The convolution and pooling happens again, and more filters are being employed
    at each level. The size of the image is shrinking, and the number of trainable
    parameters are swiftly growing, i.e., `16 * (3x3) * 32 + 32 = 4,640`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is one final convolution and pooling. Pooling an odd number creates
    a larger-than-50% reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_image_training_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Flattening the 64 3 x 3 images turns into a single layer of 576 units.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_image_training_CO4-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the 576 units is densely connected to the 128-unit layer. Using the
    traditional calculation of lines + nodes, this comes out to `(576 * 128) + 128
    = 73,856` trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_image_training_CO4-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last layer lands with 10 possible values for each class.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we’re evaluating `model.summary()` instead of inspecting
    a graphical representation of what’s happening. Even at lower dimensionality,
    the graphical representation of what’s happening is difficult to illustrate. I’ve
    done my best to create a somewhat exhaustive illustration in [Figure 10-11](#sorting_process).
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN neural network](assets/ltjs_1011.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-11\. A visualization of each layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike previous neural network diagrams, the visual explanation of CNNs can
    be somewhat limiting. Stacks upon stacks of filtered images are only so informative.
    The result of the convolutional process is flattened and connected to a deeply
    connected neural network. You’ve reached a point of complexity where the `summary()`
    method of a model is the best way to understand the contents.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’d like a dynamic visual and to watch each trained filter’s result activation
    at each layer, Polo Club of Data Science created a beautiful [CNN Explainer](https://oreil.ly/o24uR)
    in TensorFlow.js. Check out [the interactive visualizer](https://oreil.ly/SYHsp).
  prefs: []
  type: TYPE_NORMAL
- en: There you go. Your resulting `[3, 3, 64]` flattens to 576 artificial neurons
    before connecting to the neural network. Not only did you create image features,
    but you’ve simplified an input from a `[28, 28, 1]` image, which was originally
    going to require 784 densely connected inputs. Armed with this more advanced architecture,
    you can load the data from `folderToTensors()` and create the necessary model.
    You’re ready to train.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Saving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this is training in Node.js, you will have to setup GPU-acceleration on
    the machine directly. This is often done with NVIDIA CUDA, and CUDA Deep Neural
    Network (cuDNN). You will have to set up CUDA and cuDNN properly to work with
    your GPU if you want to train using `@tensorflow/tfjs-node-gpu` and get significant
    speed boost over normal `tfjs-node`. See [Figure 10-12](#cuda_gpu2).
  prefs: []
  type: TYPE_NORMAL
- en: '![CUDA GPU Screenshot](assets/ltjs_1012.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-12\. 3x–4x speed boost with GPU
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The resulting model, after 20 epochs, lands around 95% accuracy in training
    and 90% accuracy for the validation set. The file size of the resulting model
    is around 400 KB. You might have noticed the accuracy for the training set continuously
    trending upward, but the validation can sometimes go down. For better or worse,
    the last epoch will be the model that gets saved. If you’d like to ensure the
    highest possible validation accuracy, take a look at the Chapter Challenge at
    the end.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you run this model for too many epochs, the model will overfit and approach
    100% training accuracy with a diminished validation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To test this model, you’ll need drawings from the user. You can create a simple
    drawing surface on a web page with a canvas. A canvas can subscribe to events
    on the mouse being pressed down, when the mouse is moving along the canvas, and
    when the mouse is released. Using these events, you can draw from point to point.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Sketchpad
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can build a simple drawable canvas with those three events. You’ll use
    some new methods to move the canvas path and draw lines, but it’s quite readable.
    The following code sets one up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The drawings are made from a bunch of smaller lines that have a stroke width
    of 14 pixels and are automatically rounded on the edges. You can see a test drawing
    in [Figure 10-13](#canvas_test).
  prefs: []
  type: TYPE_NORMAL
- en: '![Example drawing](assets/ltjs_1013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-13\. Works well enough
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When the user clicks the mouse while on the canvas, any movement will be drawn
    from point to new point. Whenever the user stops drawing, the `drawEnd` function
    will be called. You could add a button to classify the canvas or tie directly
    into the `drawEnd` function and classify the image.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the Sketchpad
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you call `tf.browser.fromPixels` on the canvas, you’ll get 100% black pixels.
    Why is this? The answer is that the canvas has nothing drawn in some places and
    black pixels in other areas. When the canvas is converted to tensor values, it
    will convert emptiness to black. The canvas might look like it has a white background,
    but it’s actually clear and would show whatever color or pattern is underneath
    (see [Figure 10-14](#under_canvas)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A canvas is empty](assets/ltjs_1014.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. A canvas is transparent—and so empty pixels are zero
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To fix this, you can add a clear function that writes a large white square
    in the canvas, so the black lines will be on a white background like the training
    images. This is also the function you can use to clear the canvas between drawings.
    To fill the canvas with a white background, you would use the `fillRect` method
    you used to outline the labels in [Chapter 6](ch06.html#the_chapter_6):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the canvas is initialized with a white background, you can make a prediction
    on the canvas drawing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_training_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: When you read the canvas, don’t forget to identify that you’re only interested
    in a single channel; otherwise, you’ll need to turn a 3D tensor into a 1D tensor
    before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_training_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Resize using the nearest neighbor algorithm down to a 28 x 28 image for input
    into the model. Pixelation caused by nearest neighbor is inconsequential here,
    so it’s a smart choice because it is faster than `resizeBilinear`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_image_training_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The model expects a batch, so prepare the data as a batch of one. This creates
    a `[1, 28, 28, 1]` input tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_image_training_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The prediction result has been returned to JavaScript as a single batch of 10
    numbers. Think up a creative way to display the results.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have the results, you can portray the answers in any format you
    please. Personally, I organized the scores by house and used them to set the opacity
    of the labels. This way, you could get feedback as you drew each line. The opacity
    of labels depends on values `0-1`, which works splendidly with the results of
    the softmax predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You might wonder if Ravenclaw has a slight mathematical advantage because it
    is composed of more classes, and you’d be right. With all things equal, a completely
    random set of lines is more likely to be classified as Ravenclaw because it has
    a majority of the classes. However, this is statistically insignificant when the
    drawings are non-random. If you want the model to have only nine classes, remove
    `bird` and retrain to create the most balanced classification spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re interested in identifying which classes are likely to be problematic
    or confused, you can use visual reporting tools like a confusion matrix or the
    [t-SNE](https://oreil.ly/sBio5) algorithm. These tools are especially helpful
    for evaluating the training data.
  prefs: []
  type: TYPE_NORMAL
- en: I highly recommend you load the code for this chapter from [*chapter10/simple/simplest-draw*](https://oreil.ly/emOWR)
    and put your artistic skills to the test! My bird drawing sorted me into Ravenclaw,
    as shown in [Figure 10-15](#ravenclaw_ftw).
  prefs: []
  type: TYPE_NORMAL
- en: '![The web page correctly identifying a bird](assets/ltjs_1015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. A UI and drawing masterpiece
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I was able to poorly draw and properly get sorted into each of the other possible
    houses as well. However, I won’t punish you with any more of my “art.”
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve trained a model on visual data. While this dataset was limited to grayscale
    drawings, the lessons you’ve learned could work with any image dataset. There
    are plenty of excellent image datasets out there that would work perfectly with
    the model you’ve created. We’ll cover more in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve created a more elaborate page for the [drawing identification featured](https://oreil.ly/jnlhb)
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter Challenge: Saving the Magic'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re most interested in getting the highest validation accuracy model,
    it’s slim odds that your best version of the model will be the last one. For example,
    if you take a look at [Figure 10-16](#accuracy_changes), a validations accuracy
    of 90.3% gets lost, and you end up with 89.6% as the final validation of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For this Chapter Challenge, rather than saving the final trained version of
    the model, add a callback that saves the model when the validation accuracy has
    reached a new record best. This kind of code is useful because it will allow you
    to run for many epochs. As the model overfits, you’ll be able to keep the best
    generalizable model for production.
  prefs: []
  type: TYPE_NORMAL
- en: '![validation vs. training accuracy](assets/ltjs_1016.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-16\. Evaluate which accuracy is more important
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  prefs: []
  type: TYPE_NORMAL
- en: Review Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers have many trainable *what* that help extract features of
    an image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the name of the property that controls the convolutional window size?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want a convolutional result to be the same size as the original image,
    what should you set as the padding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True or false? You must flatten an image before inserting it into a convolutional
    layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What would be the output size of a max pool 3 x 3 with a stride of three on
    an 81 x 81 image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you were to one-hot encode the number 12, do you have enough information
    to do so?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.html#idm45049239647096-marker)) Videos and lectures by [3Blue1Brown](https://oreil.ly/zuGzT)
    on YouTube are excellent starts for anyone looking to go down the convolution
    rabbit hole.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.html#idm45049239428888-marker)) There are other pooling methods available
    in TensorFlow.js for experimentation.
  prefs: []
  type: TYPE_NORMAL
