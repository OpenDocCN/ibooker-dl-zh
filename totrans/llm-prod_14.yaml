- en: appendix C Multimodal latent spaces
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录C 多模态潜在空间
- en: We haven’t had a good opportunity yet to dig into multimodal latent spaces,
    but we wanted to correct that here. An example of a multimodal model includes
    Stable Diffusion, which will turn a text prompt into an image. Diffusion refers
    to the process of comparing embeddings within two different modalities, and that
    comparison must be learned. A useful simplification of this process would be imagining
    all of the text embeddings as a big cloud of points, similar to the embedding
    visualization we made in chapter 2 (section 2.3), but with billions of words represented.
    With that cloud, we can then make another cloud of embeddings in a different but
    related modality—images, for example.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有好的机会深入研究多模态潜在空间，但在这里我们想要纠正这一点。一个多模态模型的例子包括稳定扩散，它可以将文本提示转换为图像。扩散指的是比较两种不同模态中的嵌入过程，而这种比较必须通过学习来实现。这个过程的一个有用简化是想象所有的文本嵌入就像一个由数亿个点组成的大云团，类似于我们在第2章（2.3节）中制作的嵌入可视化，但这里代表的是数亿个单词。有了这个云团，我们可以在不同但相关的模态中（例如图像）创建另一个嵌入云团。
- en: We need to make sure there’s some pragmatic relation between the clouds—in our
    case, having either the text or the image describing the other suffices. They
    need to be equivalent in that both modalities represent the same base idea. Once
    we have both embedding clouds and relationships mapped, we can then train by comparing
    the clouds, masking the text, and turning the images into white noise. Then, with
    sampling and periodic steps, the model can get good at completing the images,
    given just white noise based on the equivalent text description of the image.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保云团之间存在某种实用关系——在我们的案例中，只要文本或图像描述另一个就足够了。它们需要在等效性上相等，即两种模态都代表相同的基本概念。一旦我们有了两个嵌入云团和映射的关系，我们就可以通过比较云团、遮蔽文本并将图像转换为白噪声来训练。然后，通过采样和周期性步骤，模型可以擅长根据基于等效文本描述的白噪声来补全图像。
- en: 'We don’t normally think of these models as language models because the output
    isn’t text; however, can you imagine trying to use one that didn’t understand
    language? In their current state, these models are particularly susceptible to
    ambiguity because of the unsolved problem of equivalency. Here’s an example: imagine
    you tell a diffusion model to create an image based on the prompt, “an astronaut
    hacking their way through the Amazon jungle,” and you get an image of an astronaut
    typing on a computer made of cardboard boxes. A more famous example was the prompt
    “salmon in the river,” which returned images of cooked salmon floating in water.
    (The original source is unknown, but you can find an example at [https://mng.bz/EOrJ](https://mng.bz/EOrJ).)
    Examples like this are why prompt engineering has exploded within the text2X space,
    where that ambiguity is exacerbated, and the worth of being able to lock down
    exactly what tokens to pass to the model to get desired results goes up.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不会将这些模型视为语言模型，因为输出不是文本；然而，你能想象尝试使用一个不理解语言的吗？在当前状态下，这些模型特别容易受到歧义的影响，因为等效性问题尚未解决。这里有一个例子：想象你告诉一个扩散模型根据提示“一个宇航员在亚马逊雨林中艰难前行”创建图像，而你得到了一个宇航员在用纸箱制成的电脑上打字的图像。一个更著名的例子是提示“河里的鲑鱼”，返回的图像是漂浮在水面上的煮熟的鲑鱼。（原始来源未知，但你可以在这里找到示例：[https://mng.bz/EOrJ](https://mng.bz/EOrJ)。）这样的例子是为什么在文本2X空间中，提示工程爆炸式增长，那里的歧义被加剧，能够确切锁定传递给模型以获得所需结果的标记的价值也随之提高。
- en: Going through the entire theory of training these models is out of the scope
    of this book—heck, we barely fit it into the appendix—but here are some things
    to look into if you’re interested. Textual inversion allows you to train an existing
    model that responds to a specific token with a particular concept. This allows
    you to get a particular aesthetic or subject with a very small number of example
    images. DreamBooth similarly trains a new model with a small number of example
    images; however, it trains the model to contain that subject or aesthetic regardless
    of the tokens used. PEFT and LoRA are both contained in this book but have seen
    an amazing amount of success in the text-to-image and image-to-image realm, where
    they offer a comparatively tiny alternative to textual inversions and DreamBooth
    that can arguably do the job just as well.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这些模型的整个理论超出了本书的范围——实际上，我们几乎把它塞进了附录中——但如果你感兴趣，这里有一些值得探讨的事情。文本反转允许你训练一个对特定标记有特定概念的现有模型。这让你可以用非常少的示例图像获得特定的美学或主题。DreamBooth同样使用少量示例图像训练新模型；然而，它训练模型包含该主题或美学，而不管使用的标记是什么。PEFT和LoRA都包含在这本书中，但在文本到图像和图像到图像领域取得了惊人的成功，它们为文本反转和DreamBooth提供了相对较小的替代方案，可以说可以同样有效地完成工作。
- en: In the next listing, we’ll dive into this a bit by showing examples of diffusion
    at work. We’ll start with several imports and create an image grid function to
    help showcase how things work.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我们将通过展示扩散工作的示例来更深入地探讨这个问题。我们将从几个导入开始，创建一个图像网格函数来帮助展示事物是如何运作的。
- en: Listing C.1 Example txt2Img diffusion
  id: totrans-6
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表C.1 示例txt2Img扩散
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we’ll start by showing you the easiest programmatic way to start using
    a Stable Diffusion pipeline from Hugging Face. This will load in the Stable Diffusion
    model, take a prompt, and then display the images. After showing that, we’ll dip
    our toes in the shallow end to see how this pipeline is working under the hood
    and how to do more with it. We realize this pipeline does not work the same as
    latent diffusion, which we will show, but it’s similar enough for our purposes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将向您展示从Hugging Face开始使用Stable Diffusion管道的最简单编程方式。这将加载Stable Diffusion模型，接收一个提示，然后显示图像。展示完这些后，我们将浅尝辄止，看看这个管道在底层是如何工作的，以及如何利用它做更多的事情。我们意识到这个管道的工作方式与潜在扩散不同，我们将展示这一点，但就我们的目的而言，它们足够相似：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After running this pipeline code, you should see a group of images similar to
    figure C.1\. You’ll notice that it generated astronauts riding horses and not
    horses riding astronauts like we requested. In fact, you’d be hard-pressed to
    get any txt2img model to do the inverse, showing just how important understanding
    or failing to understand language is to multimodal models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个管道代码后，你应该会看到一组与图C.1类似的图像。你会注意到它生成了宇航员骑马，而不是我们请求的马骑宇航员。实际上，要得到任何txt2img模型执行逆操作是非常困难的，这显示了理解或未能理解语言对于多模态模型是多么重要。
- en: '![figure](../Images/C-1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/C-1.png)'
- en: Figure C.1 Images generated from Stable Diffusion with the prompt “horse riding
    an astronaut”
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图C.1 使用提示“宇航员骑马”生成的Stable Diffusion图像
- en: 'Now that we see what we are building, we’ll go ahead and start building a latent
    space image pipeline. We’ll start by loading in several models: CLIP’s tokenizer
    and text encoder, which you should be familiar with by now, as well as Stable
    Diffusion’s variational autoencoder (which is similar to the text encoder but
    for images) and its UNet model. We’ll also need a scheduler:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了我们正在构建的内容，我们将继续构建一个潜在空间图像管道。我们将从加载几个模型开始：CLIP的标记器和文本编码器，你现在应该很熟悉了，以及Stable
    Diffusion的变分自动编码器（与文本编码器类似，但用于图像）和它的UNet模型。我们还需要一个调度器：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll define three core pieces of our diffusion pipeline. First, we’ll
    create the `get_text_embeds` function to get embeddings of our text prompt. This
    should feel very familiar by now: tokenizing text to numbers and then turning
    those tokens into embeddings. Next, we’ll create the `produce_latents` function
    to turn those text embeddings into latents. Latents are essentially embeddings
    in the image space. Lastly, we’ll create the `decode_img_latents` function to
    decode latents into images. This works similar to how a tokenizer decodes tokens
    back to text:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们扩散管道的三个核心组件。首先，我们将创建`get_text_embeds`函数来获取文本提示的嵌入。现在这应该已经很熟悉了：将文本分词为数字，然后将这些标记转换为嵌入。接下来，我们将创建`produce_latents`函数，将这些文本嵌入转换为潜在表示。潜在表示本质上是在图像空间中的嵌入。最后，我们将创建`decode_img_latents`函数，将潜在表示解码为图像。这类似于标记器将标记解码回文本的方式：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Tokenizes text and gets embeddings'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将文本分词并获取嵌入'
- en: '#2 Δoes the same for unconditional embeddings'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 对无条件嵌入执行相同的操作'
- en: '#3 Cat for the final embeddings'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 为最终嵌入创建猫（Cat）'
- en: '#4 Expands the latents to avoid doing two forward passes'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 将潜在表示扩展以避免进行两次正向传递'
- en: '#5 Predicts the noise residual'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 预测噪声残差'
- en: '#6 Performs guidance'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 执行引导'
- en: '#7 Computes the previous noisy sample x_t -&gt; x_t-1'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 计算前一个带噪声的样本 x_t -&gt; x_t-1'
- en: 'Now that we have all our pieces created, we can create the pipeline. This will
    take a prompt, turn it into text embeddings, convert those to latents, and then
    decode those latents into images:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了所有组件，我们可以创建管道。这将接受一个提示，将其转换为文本嵌入，将这些嵌入转换为潜在表示，然后将这些潜在表示解码为图像：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Prompts -&gt; text embeddings'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将提示转换为文本嵌入'
- en: '#2 Text embeddings -&gt; img latents'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 将文本嵌入转换为图像潜在表示'
- en: '#3 Img latents → imgs'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将图像潜在表示转换为图像'
- en: At the end, you should see an image grid similar to figure C.2.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该看到一个类似于图C.2的图像网格。
- en: '![figure](../Images/C-2.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/C-2.png)'
- en: Figure C.2 Images generated from custom Stable Diffusion pipeline with the prompt
    “fantasy knight, intricate armor.”
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图C.2 使用提示“幻想骑士，复杂盔甲”从自定义Stable Diffusion管道生成的图像。
- en: We hope you enjoyed this very quick tutorial, and as a final exercise, we challenge
    the reader to figure out how to use the `prompt_to_img` function to perturb existing
    image latents to perform an image-to-image task. We promise it will be a challenge
    to help solidify your understanding. What we hope you take away, though, is how
    important language modeling is to diffusion and current state-of-the-art vision
    models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '我们希望你喜欢这个非常快速的教程，作为最后的练习，我们挑战读者找出如何使用`prompt_to_img`函数来扰动现有的图像潜在表示以执行图像到图像的任务。我们承诺这将是一个挑战，以帮助你巩固理解。尽管如此，我们希望你能带走的是语言模型对扩散和当前最先进的视觉模型的重要性。 '
- en: Because modality is currently the least-explored portion of language modeling,
    there’s enough here to write a whole other book, and who knows? Maybe we will
    later. In the meantime, if you are interested in writing papers, getting patents,
    or just contributing to the furthering of a really interesting field, we’d recommend
    diving right into this portion because anything that comes out within the regular
    language modeling field can immediately be incorporated to make diffusion better.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模态目前是语言模型中最少被探索的部分，这里的内容足以写另一本书，而且谁知道呢？也许我们以后会写。与此同时，如果你对撰写论文、申请专利或只是为推进一个真正有趣的领域做出贡献感兴趣，我们建议你直接深入研究这部分，因为任何在常规语言模型领域产生的东西都可以立即被纳入，以使扩散模型变得更好。
