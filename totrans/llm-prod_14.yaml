- en: appendix C Multimodal latent spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We haven’t had a good opportunity yet to dig into multimodal latent spaces,
    but we wanted to correct that here. An example of a multimodal model includes
    Stable Diffusion, which will turn a text prompt into an image. Diffusion refers
    to the process of comparing embeddings within two different modalities, and that
    comparison must be learned. A useful simplification of this process would be imagining
    all of the text embeddings as a big cloud of points, similar to the embedding
    visualization we made in chapter 2 (section 2.3), but with billions of words represented.
    With that cloud, we can then make another cloud of embeddings in a different but
    related modality—images, for example.
  prefs: []
  type: TYPE_NORMAL
- en: We need to make sure there’s some pragmatic relation between the clouds—in our
    case, having either the text or the image describing the other suffices. They
    need to be equivalent in that both modalities represent the same base idea. Once
    we have both embedding clouds and relationships mapped, we can then train by comparing
    the clouds, masking the text, and turning the images into white noise. Then, with
    sampling and periodic steps, the model can get good at completing the images,
    given just white noise based on the equivalent text description of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t normally think of these models as language models because the output
    isn’t text; however, can you imagine trying to use one that didn’t understand
    language? In their current state, these models are particularly susceptible to
    ambiguity because of the unsolved problem of equivalency. Here’s an example: imagine
    you tell a diffusion model to create an image based on the prompt, “an astronaut
    hacking their way through the Amazon jungle,” and you get an image of an astronaut
    typing on a computer made of cardboard boxes. A more famous example was the prompt
    “salmon in the river,” which returned images of cooked salmon floating in water.
    (The original source is unknown, but you can find an example at [https://mng.bz/EOrJ](https://mng.bz/EOrJ).)
    Examples like this are why prompt engineering has exploded within the text2X space,
    where that ambiguity is exacerbated, and the worth of being able to lock down
    exactly what tokens to pass to the model to get desired results goes up.'
  prefs: []
  type: TYPE_NORMAL
- en: Going through the entire theory of training these models is out of the scope
    of this book—heck, we barely fit it into the appendix—but here are some things
    to look into if you’re interested. Textual inversion allows you to train an existing
    model that responds to a specific token with a particular concept. This allows
    you to get a particular aesthetic or subject with a very small number of example
    images. DreamBooth similarly trains a new model with a small number of example
    images; however, it trains the model to contain that subject or aesthetic regardless
    of the tokens used. PEFT and LoRA are both contained in this book but have seen
    an amazing amount of success in the text-to-image and image-to-image realm, where
    they offer a comparatively tiny alternative to textual inversions and DreamBooth
    that can arguably do the job just as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next listing, we’ll dive into this a bit by showing examples of diffusion
    at work. We’ll start with several imports and create an image grid function to
    help showcase how things work.
  prefs: []
  type: TYPE_NORMAL
- en: Listing C.1 Example txt2Img diffusion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll start by showing you the easiest programmatic way to start using
    a Stable Diffusion pipeline from Hugging Face. This will load in the Stable Diffusion
    model, take a prompt, and then display the images. After showing that, we’ll dip
    our toes in the shallow end to see how this pipeline is working under the hood
    and how to do more with it. We realize this pipeline does not work the same as
    latent diffusion, which we will show, but it’s similar enough for our purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After running this pipeline code, you should see a group of images similar to
    figure C.1\. You’ll notice that it generated astronauts riding horses and not
    horses riding astronauts like we requested. In fact, you’d be hard-pressed to
    get any txt2img model to do the inverse, showing just how important understanding
    or failing to understand language is to multimodal models.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/C-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure C.1 Images generated from Stable Diffusion with the prompt “horse riding
    an astronaut”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now that we see what we are building, we’ll go ahead and start building a latent
    space image pipeline. We’ll start by loading in several models: CLIP’s tokenizer
    and text encoder, which you should be familiar with by now, as well as Stable
    Diffusion’s variational autoencoder (which is similar to the text encoder but
    for images) and its UNet model. We’ll also need a scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll define three core pieces of our diffusion pipeline. First, we’ll
    create the `get_text_embeds` function to get embeddings of our text prompt. This
    should feel very familiar by now: tokenizing text to numbers and then turning
    those tokens into embeddings. Next, we’ll create the `produce_latents` function
    to turn those text embeddings into latents. Latents are essentially embeddings
    in the image space. Lastly, we’ll create the `decode_img_latents` function to
    decode latents into images. This works similar to how a tokenizer decodes tokens
    back to text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Tokenizes text and gets embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Δoes the same for unconditional embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Cat for the final embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Expands the latents to avoid doing two forward passes'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Predicts the noise residual'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Performs guidance'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Computes the previous noisy sample x_t -&gt; x_t-1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have all our pieces created, we can create the pipeline. This will
    take a prompt, turn it into text embeddings, convert those to latents, and then
    decode those latents into images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Prompts -&gt; text embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Text embeddings -&gt; img latents'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Img latents → imgs'
  prefs: []
  type: TYPE_NORMAL
- en: At the end, you should see an image grid similar to figure C.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/C-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure C.2 Images generated from custom Stable Diffusion pipeline with the prompt
    “fantasy knight, intricate armor.”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We hope you enjoyed this very quick tutorial, and as a final exercise, we challenge
    the reader to figure out how to use the `prompt_to_img` function to perturb existing
    image latents to perform an image-to-image task. We promise it will be a challenge
    to help solidify your understanding. What we hope you take away, though, is how
    important language modeling is to diffusion and current state-of-the-art vision
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Because modality is currently the least-explored portion of language modeling,
    there’s enough here to write a whole other book, and who knows? Maybe we will
    later. In the meantime, if you are interested in writing papers, getting patents,
    or just contributing to the furthering of a really interesting field, we’d recommend
    diving right into this portion because anything that comes out within the regular
    language modeling field can immediately be incorporated to make diffusion better.
  prefs: []
  type: TYPE_NORMAL
