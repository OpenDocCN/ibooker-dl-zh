<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__nogpt"> <span class="chapter-title-numbering"><span class="num-string">8</span></span> <span class="title-text"> GPT alternatives</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">Claude, from Anthropic</li> 
    <li id="p3">Command R+, from Cohere</li> 
    <li id="p4">Bard, from Google</li> 
    <li id="p5">Hugging Face’s models</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>Time to meet some of GPT’s “friends”! So far, we have been focusing on GPT and other OpenAI models. But OpenAI is not the only game in town. Quite the contrary: we are currently witnessing a “Cambrian explosion” of language models, with new models popping up every week. Before using language models in production, you want to make sure you’re using the best model for your task. In this chapter, we’ll look at many of the OpenAI alternatives out there and discuss the pros and cons of different models as well as how to use them.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Almost all language models nowadays are based on a similar architecture (the Transformer architecture). However, models from different providers may differ in the way they are trained, the way they represent text, or the way in which they are offered and priced. All those factors can make a difference in terms of processing fees and output quality for your specific task. Models like GPT-4o are powerful and solve almost any task. But this generality comes at a cost: if a small, specialized model is available, trained for just the task you’re interested in, using such a model may very well be the optimal choice.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>You will notice throughout the following sections that many of the models we discuss can be accessed via interfaces that are similar to OpenAI’s interface. That’s good news for you: no need to get into a novel and complex framework each time you want to try a different model! And with that, let’s start our exploration of GPT alternatives.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p9"> 
    <h5 class=" callout-container-h5 readable-text-h5">Why isn’t my favorite model listed?</h5> 
   </div> 
   <div class="readable-text" id="p10"> 
    <p> If your favorite model or model provider is not listed in this chapter, don’t panic! With the growing number of providers and models, it has become impossible to give a full overview of available models. If a model is not included here, it does not mean it can’t be the best alternative for your task. The interfaces of different providers tend to be similar, so you should still be able to use what you have learned so far to employ other models without much trouble. Also, note that we list model providers in alphabetical order in this chapter. Don’t infer any priority from that (we’re not discussing the best providers first).</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class=" readable-text-h2" id="anthropic"><span class="num-string browsable-reference-id">8.1</span> Anthropic</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Many of the stories by Isaac Asimov, one of the most prolific science-fiction authors of all time, center on the “three laws of robotics” and their interpretation:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p13">A robot may not injure a human being or, through inaction, allow a human being to come to harm.</li> 
   <li class="readable-text" id="p14">A robot must obey orders given to it by human beings except where such orders would conflict with the First Law.</li> 
   <li class="readable-text" id="p15">A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.</li> 
  </ol> 
  <div class="readable-text" id="p16"> 
   <p>The goal here is to have a concise set of guidelines that makes sure robots are helpful and harmless. Whether the previously mentioned laws provided any inspiration, this idea connects to the language models produced by Anthropic, yet another provider of large-scale language models.</p> 
  </div> 
  <div class="readable-text" id="p17"> 
   <p>Anthropic, founded in 2021 (by several former OpenAI members), has repeatedly touted the idea of “Constitutional AI” [1] as one of the distinguishing factors, compared to other models. In a nutshell, this means that when training models to provide users with accurate and inoffensive answers, we rely on a small set of rules—a “constitution,” so to speak—to judge the quality of answers. Instead of relying on human testers to label answers generated by the model during training, we employ a second AI, charged with evaluating the answers of the former according to the constitution.</p> 
  </div> 
  <div class="readable-text" id="p18"> 
   <p>At the time of writing, Claude 3.5 (a reference to the amazing Claude Shannon) is the latest model released by Anthropic. In this section, we will try Claude via (you guessed it) a web interface and a Python library.</p> 
  </div> 
  <div class="readable-text" id="p19"> 
   <h3 class=" readable-text-h3" id="chatting-with-claude"><span class="num-string browsable-reference-id">8.1.1</span> Chatting with Claude</h3> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>We will have a quick chat with Claude (currently in version 3.5) to get a sense of its capabilities. First, go to the Anthropic website at <a href="http://www.anthropic.com">www.anthropic.com</a>, and click the Talk to Claude button.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>Unless you have created an Anthropic account before, you will be asked to provide an email address and a phone number. After verifying your data, you should see Claude’s chat interface, shown in figure <a href="#fig__claude">8.1</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p22">  
   <img alt="figure" src="../Images/CH08_F01_Trummer.png" width="1100" height="1105"/> 
   <h5 class=" figure-container-h5" id="fig__claude"><span class="num-string">Figure <span class="browsable-reference-id">8.1</span></span> Web interface for Anthropic’s chatbot Claude</h5>
  </div> 
  <div class="readable-text" id="p23"> 
   <p>The interface is pretty intuitive: simply enter text in the corresponding field, and click the button on the right to start chatting! Begin with a friendly greeting, chat about the weather, or try using Claude to solve some of the tasks from chapter 2 (e.g., classifying reviews by sentiment or translating questions to SQL queries).</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h3 class=" readable-text-h3" id="python-library"><span class="num-string browsable-reference-id">8.1.2</span> Python library</h3> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Assuming you have created an account with Anthropic, you can create keys at the following URL: <a href="https://console.anthropic.com/settings/keys">https://console.anthropic.com/settings/keys</a>. Be sure to copy your access key after creating it (as you will not be able to access it again afterward)!</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>After obtaining your access key, go to the terminal and run the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p27"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install anthropic==0.28</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>This will install the Anthropic Python library. If you are familiar with OpenAI’s Python library (and chances are that, after reading the previous chapters, you are), you should get used to the Anthropic library very quickly.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>For instance, the following listing shows Python code for answering questions using Claude. Of course, this code does not do anything that you cannot do via the web interface. The purpose is just to show how easy it is to use Claude via the Python interface.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p30"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__anthropic"><span class="num-string">Listing <span class="browsable-reference-id">8.1</span></span> Answering questions with Anthropic’s Claude model</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
from anthropic import Anthropic

if __name__ == '__main__':

    parser = argparse.ArgumentParser()                  #1
    parser.add_argument('ai_key', type=str, help='Anthropic access key')
    parser.add_argument('question', type=str, help='A question for Claude')
    args = parser.parse_args()

    anthropic = Anthropic(api_key=args.ai_key)  #2

    completion = anthropic.messages.create(  #3
        model='claude-3-5-sonnet-20241022', 
        max_tokens=100,
        messages=[
            {
                'role':'user', 
                'content':args.question
             }])

    print(completion.content)  #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Defines parameters
     <br/>#2 Configures Anthropic
     <br/>#3 Uses Claude for completion
     <br/>#4 Prints the completion result
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>As input parameters (<strong class="cueball">1</strong>), we use the Anthropic access key and a question we would like answered. Similar to OpenAI’s libraries, we configure access using the access key (<strong class="cueball">2</strong>). After that, we can construct prompts for completion by Claude (<strong class="cueball">3</strong>).</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>Using Anthropic’s <code>anthropic.messages</code> endpoint, we specify the ID of the model to use (<code>claude-3-5-sonnet-20241022</code> is the newest model by Anthropic at the time of writing) and the maximum number of tokens for completion (using the <code>max_tokens</code> parameter). Similar to OpenAI’s chat models, Claude is designed for chats between users and the model. Therefore, the input to Claude is a list of messages (containing only a single element in this specific scenario). We obtain the result of Claude’s prompt completion in the <code>content</code> field (which we print (<strong class="cueball">4</strong>)).</p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>You can find this listing as the Anthropic item on the companion website. To execute it, open your terminal, and switch to the containing folder. Then, execute the following command:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p34"> 
   <div class="code-area-container"> 
    <pre class="code-area">python anthropic_claude.py ... "What is constitutional AI?"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>Replace the three dots with your Anthropic access key. When executing the program, you should see an answer to your question, generated by Anthropic’s model.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <h2 class=" readable-text-h2" id="cohere"><span class="num-string browsable-reference-id">8.2</span> Cohere</h2> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>We briefly discussed hallucinations in chapter 2. Essentially, a hallucination occurs when language models make stuff up because they don’t have access to data that is relevant to the task at hand. The Canadian startup Cohere puts a particular emphasis on avoiding such hallucinations using a method called <em>grounding</em>. Grounding the answer of a language model means linking it to real data, thereby reducing the chances of “creative output” not based in reality.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Cohere supports a wide range of connectors, enabling its models to access external data. For instance, web search is supported, as well access to various databases. Internally, Cohere accesses those data sources and provides the language model with information tailored to the request. But even better, Cohere shows you all the data sources used to generate your answer. If you are the suspicious type (and when it comes to answers from language models, you generally should be), you can follow up on references and validate that they support the generated answer.</p> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Let’s see how all that works in practice. Time to chat with Cohere’s Command R+ model!</p> 
  </div> 
  <div class="readable-text" id="p40"> 
   <h3 class=" readable-text-h3" id="chatting-with-command-r"><span class="num-string browsable-reference-id">8.2.1</span> Chatting with Command R+</h3> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>At the time of writing, Command R+ is one of Cohere’s latest models. As usual, you can try it via web interface and use it via Python when processing large amounts of data. First, we’ll try the web interface. For that, go to <a href="https://cohere.com/">https://cohere.com/</a>, and click Try Now. After signing up for an account, click Playground. You should see the web interface in figure <a href="#fig__cohere_3">8.2</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p42">  
   <img alt="figure" src="../Images/CH08_F02_Trummer.png" width="1100" height="750"/> 
   <h5 class=" figure-container-h5" id="fig__cohere_3"><span class="num-string">Figure <span class="browsable-reference-id">8.2</span></span> Web interface for chatting with Cohere’s language model Command R+</h5>
  </div> 
  <div class="readable-text" id="p43"> 
   <p>Figure <a href="#fig__cohere_3">8.2</a> shows where to enter your prompt and the button to generate an answer. What about the window on the right (containing the Web Connector button)? This is where you specify connectors to use when generating your replies. A connector enables Cohere to access external data sources. We can activate (or deactivate) the web connector by toggling the corresponding button. This connector enables Cohere to query the web, similar to what all of us would do when trying to answer a hard question that involves factual knowledge. Give it a try, and see how the replies to factual questions change if the web connector is activated or deactivated!</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p44"> 
    <h5 class=" callout-container-h5 readable-text-h5">What is RAG?</h5> 
   </div> 
   <div class="readable-text" id="p45"> 
    <p> You may have noticed that Cohere’s website prominently advertises <em>RAG</em>, but what is that? RAG stands for Retrieval Augmented Generation. It means that when generating an answer, we augment the input used by the language model with data we retrieve from an external source.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <h3 class=" readable-text-h3" id="python-library-1"><span class="num-string browsable-reference-id">8.2.2</span> Python library</h3> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Cohere offers a Python library similar to the ones we have seen in previous sections. Enter the following command in the terminal to install the required library:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p48"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install cohere==4.43</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Listing <a href="#code__cohere">8.2</a> (available as the Cohere item on the website) contains code for a simple question-answering interface. Users enter their access key and a question on the command line. Visit <a href="https://dashboard.cohere.com/api-keys">https://dashboard.cohere.com/api-keys</a> to get your access key. In listing <a href="#code__cohere">8.2</a>, after configuring the Cohere library with the access key (<strong class="cueball">1</strong>), we generate an answer using the Cohere library (<strong class="cueball">2</strong>). Note the reference to <code>connectors</code> in the call to the <code>chat</code> function! Here, we specify a list of connectors, enabling Cohere’s model to access data from external sources. Connectors are specified as a list (i.e., we can enable access not to just one but to a multitude of connectors). Here we use the connector with ID <code>web-search</code> (that’s the same web search connector we used over the web interface in the previous section).</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Finally (<strong class="cueball">3</strong>), we print the answer generated by the model and a list of the web sources consulted to generate the answer (along with the queries issued to retrieve those documents). That enables us to verify that the generated answer is indeed implied by the source material.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p51"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__cohere"><span class="num-string">Listing <span class="browsable-reference-id">8.2</span></span> Answering questions using Cohere’s Python library</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import cohere

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument('ai_key', type=str, help='Cohere access key')
    parser.add_argument('question', type=str, help='Answer this question')
    args = parser.parse_args()
    
    client = cohere.Client(args.ai_key)  #1
     #2
    prompt = f'Answer this question: {args.question}'
    result = client.chat(prompt, connectors=[{'id': 'web-search'}])
     #3
    print(f'Answer: result.text')
    print(f'Web searches: result.search_results')
    print(f'Web results: result.documents')</pre> 
    <div class="code-annotations-overlay-container">
     #1 Configures access
     <br/>#2 Generates an answer using Cohere
     <br/>#3 Prints answer and citations
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>Let’s try it! Switch to the folder containing the code, and run the following command in the terminal (replace the three dots with your Cohere access key):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p53"> 
   <div class="code-area-container"> 
    <pre class="code-area">python cohereqa.py ... "Where was Steve Jobs born?"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>You will get an answer similar to the following (which is slightly abbreviated):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p55"> 
   <div class="code-area-container"> 
    <pre class="code-area"> #1
Answer: Steven Paul Jobs was born in San Francisco, California, United States.
His birth name was later changed to Steve Jobs after 
he was adopted by Paul and Clara Jobs. 

Jobs was born to Abdulfattah Jandali and Joanne Schieble on 24th February, 1955. 
After being put up for adoption, Jobs was adopted by Paul and Clara Jobs, 
a lower-middle-class couple.

Web searches: [  #2
    {'search_query': 
        {'text': 'Where was Steve Jobs born', 
        'generation_id': '...'}, 
    'document_ids': [
        'web-search_1:0', 'web-search_3:1', 'web-search_4:0', 
        'web-search_5:0', 'web-search_9:1'], 
    'connector': {'id': 'web-search'}
    }]

Web results: [  #3
    {'id': 'web-search_4:0', 'snippet': 'Short Biography of Steve Jobs 
    The story of Steve Jobs from cradle to grave - and beyond. Steven 
    Paul Jobs was born on February 24, 1955 in San Francisco, California.
    ... ', 'title': 'Short Bio | all about Steve Jobs.com', 
    'url': 'https://allaboutstevejobs.com/bio/short_bio'}, 
    ...]</pre> 
    <div class="code-annotations-overlay-container">
     #1 Generated answer
     <br/>#2 Web search queries
     <br/>#3 Web documents used
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>Let’s have a closer look. The initial part of the output (<strong class="cueball">1</strong>) is the answer generated by the model. The answer seems reasonable, but can we trust it? This is where the remaining parts of the output come into play. The middle part (<strong class="cueball">2</strong>) provides us with information on the web used by Cohere to inform the generated answer. Those web queries are chosen automatically based on the input question. In this case, web queries correspond precisely to the input question (this is not necessarily the case for longer input text). At the end of the output (<strong class="cueball">3</strong>), we find text snippets and the URLs of the documents retrieved via the prior queries. In this case, text snippets taken from the web documents (e.g., “Steven Paul Jobs was born on February 24, 1955 in San Francisco, California”) make a good case supporting the answer from the model.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h2 class=" readable-text-h2" id="google"><span class="num-string browsable-reference-id">8.3</span> Google</h2> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>Google, a company that needs no introduction, has been deeply involved with language models since the very beginning. In fact, the Transformer architecture [2] used by virtually all language models was invented (primarily) by Google researchers. No wonder Google is developing its own models. At the time of writing, Gemini is one of Google’s most recent models, and we’ll try it in this section.</p> 
  </div> 
  <div class="readable-text" id="p59"> 
   <h3 class=" readable-text-h3" id="chatting-with-gemini"><span class="num-string browsable-reference-id">8.3.1</span> Chatting with Gemini</h3> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>To try Gemini, go to <a href="https://gemini.google.com/">https://gemini.google.com/</a>. After signing up for an account, you should see the interface depicted in figure <a href="#fig__google_1">8.3</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/CH08_F03_Trummer.png" width="1100" height="788"/> 
   <h5 class=" figure-container-h5" id="fig__google_1"><span class="num-string">Figure <span class="browsable-reference-id">8.3</span></span> Web interface for chatting with Google’s Gemini model. Click Settings to activate additional functionality.</h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Simply enter your text in the corresponding input field (labeled Enter prompt here in figure <a href="#fig__google_1">8.3</a>), and press Enter to generate an answer. Gemini is not limited to text input. Click the button on the right to upload pictures. In your chats with Gemini, you can reference those pictures and ask questions about them.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>One particularity of Gemini, distinguishing it from all the other models we have discussed so far, is its integration with other Google tools. Click the Settings button (marked in figure <a href="#fig__google_1">8.3</a>) and then the Extensions option. For instance, you can give Gemini access to your emails by clicking the associated button. Ever had the problem of finding information hidden in year-old emails in your inbox? Google’s Gemini has the potential to help with that.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <h3 class=" readable-text-h3" id="the-python-library"><span class="num-string browsable-reference-id">8.3.2</span> The Python library</h3> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Like other providers of language models, Google offers a Python library for model access. You can install the library using the following code in the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p66"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install google-generativeai==0.7</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Go to <a href="https://aistudio.google.com/app/apikey">https://aistudio.google.com/app/apikey</a> to get your access key for the Google API. Follow the instructions, and copy the key after creating it. Listing <a href="#code__gemini">8.3</a> shows how to use Gemini in Python to answer questions. The steps are similar to the previous libraries. The input parameters (<strong class="cueball">1</strong>) include the access key (alternatively, we can store the key in an environment variable) as well as the question to answer. Next, we configure the Google library with the access key (<strong class="cueball">2</strong>). Now we can generate a model and use it to answer questions via the <code>generate_content</code> method (<strong class="cueball">3</strong>). Finally, we print out the generated question (<strong class="cueball">4</strong>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p68"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__gemini"><span class="num-string">Listing <span class="browsable-reference-id">8.3</span></span> Answering questions using Google’s Gemini model</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import google.generativeai as genai

if __name__ == '__main__':
     #1
    parser = argparse.ArgumentParser()
    parser.add_argument('api_key', type=str, help='Google API key')
    parser.add_argument('question', type=str, help='Question to answer')
    args = parser.parse_args()
     #2
    genai.configure(api_key=args.api_key)
     #3
    model = genai.GenerativeModel('gemini-1.5-flash')
    reply = model.generate_content(args.question)
     #4
    print(reply.text)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Defines the input parameters
     <br/>#2 Configures the API with the access key
     <br/>#3 Generates an answer with Gemini
     <br/>#4 Prints the answer
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>You can find the code on the book’s website using the Google link. In the terminal, change to the directory containing the code. For instance, run the following command to test Gemini (replace the three dots with your Google access key):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p70"> 
   <div class="code-area-container"> 
    <pre class="code-area">python google.py ... "What is the meaning of life?"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>A full overview of Google’s library is beyond the scope of this book. However, knowing the libraries of other providers of large language models, you should be able to familiarize yourself quickly with this API as well.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <h2 class=" readable-text-h2" id="hugging-face"><span class="num-string browsable-reference-id">8.4</span> Hugging Face</h2> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>Providers like OpenAI invest millions of dollars to train models like GPT-4o. The result of all that expensive training is values for model parameters that make the model perform best. After investing all that money, you would not necessarily want to share the results of training freely, right? Hence, models like GPT-4o are typically closed, meaning OpenAI does not share the parameter values that result from training (note that OpenAI has shared other models, such as Whisper). Instead, OpenAI processes prompts for you on its own infrastructure while charging you a processing fee (that’s how we ultimately pay for all that expensive model training).</p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>More and more, however, language model providers face competition from an extremely lively open source sector. Universities, startup companies, and enthusiasts all train their own models and often release the models (and their parameter values) freely to the public. That enables you to run those models yourself, locally, on your own dedicated infrastructure. For smaller models, a laptop with a GPU will often suffice. For larger models, you may need to use a GPU cluster (or resort to cloud providers that run those open source models for you). Besides potential financial advantages (running models on your own infrastructure may be cheaper), other considerations can make running models locally the only viable choice. For instance, you may not want to trust external providers with particularly sensitive data. If you don’t want to send your data, running locally is the only option.</p> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>Typically, open source models are significantly smaller than the models offered by cloud providers. That makes sense because after all, who has a few million dollars lying around to train a model? However, due to the sheer number of models available, it is often possible to find an open source model that is specialized in solving just the task you’re interested in. For instance, the Hugging Face Transformers platform features over 1,000,000 Transformer models at the time of writing! Whatever task you are facing, chances are that you may find just the right model. In this section, we will look at the Hugging Face platform and see how to use its models locally.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <h3 class=" readable-text-h3" id="web-platform"><span class="num-string browsable-reference-id">8.4.1</span> Web platform</h3> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Go to <a href="https://huggingface.co/">https://huggingface.co/</a>. Hugging Face Transformers offers various resources around Transformer models. That includes not only the models but also data sets you can use to train your own models as well as cloud offerings that let you run open source models on Hugging Face’s cloud infrastructure.</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>For the moment, we are interested in models. Click the Models button to see the list of models shown in figure <a href="#fig__hf_2">8.4</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p79">  
   <img alt="figure" src="../Images/CH08_F04_Trummer.png" width="1100" height="856"/> 
   <h5 class=" figure-container-h5" id="fig__hf_2"><span class="num-string">Figure <span class="browsable-reference-id">8.4</span></span> Overview of Hugging Face Transformer models. Click the Tasks filters to narrow the selec-tion. Click models in the Models list to see details.</h5>
  </div> 
  <div class="readable-text" id="p80"> 
   <p>We’re seeing a list of over 1 million Transformer models (and, as the number of models is growing daily, you will probably see even more)! Whew. That’s a few too many. Let’s narrow them down. On the left side of the screen are various filter options to get the list down to the models you really care about. For instance, we can filter by the type of task we need the model to do. This includes text classification (e.g., classifying reviews by sentiment), visual question answering (e.g., does the picture show an apple?), and speech-to-text transcription (e.g., transcribing voice queries to text). For almost all the tasks we have discussed in this book, you may be able to find a specialized model. Click any of the standard tasks to see only the models that solve that task.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>When you click one of the remaining models, you will see a detailed model description such as that shown in figure <a href="#fig__hf_3">8.5</a> for the BLIP model from Salesforce, a model that processes images to generate suitable captions. On the left side is a thorough description of the model along with links to relevant papers and code samples showing you how to use the model on different hardware platforms (i.e., locally). On the right side is an interface that allows you to try the model on a few sample pictures.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p82">  
   <img alt="figure" src="../Images/CH08_F05_Trummer.png" width="1100" height="866"/> 
   <h5 class=" figure-container-h5" id="fig__hf_3"><span class="num-string">Figure <span class="browsable-reference-id">8.5</span></span> Detailed description of Salesforce’s BLIP model (<a href="https://huggingface.co/Salesforce/blip-image-captioning-large">https://huggingface.co/Salesforce/blip</a> <a href="https://huggingface.co/Salesforce/blip-image-captioning-large">-image-captioning-large</a>). Read the description on the left, or try the model via the interface on the right.</h5>
  </div> 
  <div class="readable-text" id="p83"> 
   <h3 class=" readable-text-h3" id="python-library-2"><span class="num-string browsable-reference-id">8.4.2</span> Python library</h3> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>We are now about to run Transformer models on our own local infrastructure! The performance you get will, of course, depend on the properties of the hardware you’re using. However, even with moderate computation power, you should be able to work with the models we’re about to try. But first, we must install the Hugging Face Transformers library. Enter the following commands in your terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install transformers==4.36</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>The Transformers library is based on PyTorch, a popular machine-learning framework. If you haven’t installed PyTorch yet, run the following command in your terminal (otherwise, you will receive an error message when trying to run the following code):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install torch==2.1.2</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>That’s it for the setup! We’re ready to use the Hugging Face Transformers library, which we import via <code>import transformers</code>. The Transformers library offers a plethora of features and various ways to use the model in its repository (or to train your own models, for that matter). In this section, we’ll only cover a small subset of them, but it’s enough to get a first impression.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Let’s assume that you have found a model in the Hugging Face model repository that you would like to try. To make things more concrete, let’s say we’re talking about the Roberta model for sentiment classification, offered by Cardiff University (you can find that model at <a href="https://mng.bz/rKoX">https://mng.bz/rKoX</a>). Compared to GPT-4o and most other models discussed in this section, this is a fairly small model. However, it is specialized for analyzing text to determine the underlying sentiment. Although it is much less generic than GPT-4o and similar models, it does one task and does it fairly well. If you are looking to classify reviews, for instance, you may find this model very suitable.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>The easiest way to use a model via the Transformers library is via a <code>pipeline</code>. The following command creates a pipeline for sentiment classification based on the Roberta model:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p91"> 
   <div class="code-area-container"> 
    <pre class="code-area">sentiment_pipeline = transformers.pipeline(
    model='cardiffnlp/twitter-roberta-base-sentiment-latest')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>As you see, we specify the model using the last part of its URL: the name of the account providing the model (<code>cardiffnlp</code>) followed by the ID of the model itself. When using this code for the first time, the Transformers library will automatically download the model from its public model repository. Note that this code works in part because the model we are referring to is associated with a specific task class. For other models, you may have to specify the types of tasks you want them to solve as a separate input parameter.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>We created a pipeline! Now we can use it to classify text—for example, like so (we assume that the variable <code>text_to_classify</code> contains, you guessed it, the text to classify):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">result = sentiment_pipeline(text_to_classify)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>We have all we need to build a simple application that classifies reviews (based on whether the underlying sentiment is positive, meaning a good review, or negative, meaning the review is bad). The next listing shows the corresponding code (you can find it as the Hugging Face item on the book’s website).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p96"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__huggingface"><span class="num-string">Listing <span class="browsable-reference-id">8.4</span></span> Sentiment classification using Hugging Face Transformers</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import argparse
import transformers

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    parser.add_argument('review', type=str, help='Text of a review')
    args = parser.parse_args()

    sentiment_pipeline = transformers.pipeline(          #1
        model='cardiffnlp/twitter-roberta-base-sentiment-latest')

    result = sentiment_pipeline(args.review)      #2
    
    print(result)                        #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Creates a pipeline
     <br/>#2 Applies the pipeline to input
     <br/>#3 Prints out the classification result
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>You may notice a difference from all the code we have seen previously: we don’t need to specify an access key! Because the Hugging Face models are publicly available and we’re running them on our own infrastructure, there is no need to provide any kind of credentials. Instead, the only input is a review text that we want to classify.</p> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>The code composes the snippets discussed earlier. It creates a pipeline (<strong class="cueball">1</strong>), uses it to classify the input text (<strong class="cueball">2</strong>), and finally prints out the result (<strong class="cueball">3</strong>). You can try it by switching to the containing folder in your terminal and entering, for instance, the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p99"> 
   <div class="code-area-container"> 
    <pre class="code-area">python huggingface.py "This movie was really awful!"</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>When running the code for the first time, you may have to wait for a few minutes while the Transformers library downloads the model you are referencing. But no worries: the library caches the downloaded model so you won’t have to wait when you run the code a second time. After processing finishes, you should see output like this:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <div class="code-area-container"> 
    <pre class="code-area">[{'label': 'negative', 'score': 0.9412825107574463}]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>That’s certainly correct for the sample input: the review is concise and 100% negative. Try it with a few different reviews, and compare the output to what you get with models like GPT-4o. In the majority of cases, the classification result should be fairly similar. Of course, GPT-4o is a much more generic model and can be used to solve a variety of other tasks as well. But as long as you’re interested in classifying reviews, this model offers an interesting tradeoff between quality and cost.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p104">In addition to OpenAI, several other providers offer large language models. Most providers offer closed-source models via a cloud API. The models differ based on their generality, output quality, and pricing.</li> 
   <li class="readable-text" id="p105">Most providers offer a Python library to access language models.</li> 
   <li class="readable-text" id="p106">Hugging Face Transformers offers various models for free download.</li> 
  </ul> 
  <div class="readable-text" id="p107"> 
   <h2 class=" readable-text-h2" id="references"><span class="num-string browsable-reference-id">8.6</span> References</h2> 
  </div> 
  <ol> 
   <li class="readable-text" id="p108">Bai, Y., Kadavath, S., Kundu, S., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. <em>CoRR abs/2212.0</em>, 1–32.</li> 
   <li class="readable-text" id="p109">Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is All You Need. In <em>Advances in Neural Information Processing Systems</em>, pp. 5999–6009.</li> 
  </ol>
 </div></div></body></html>