- en: Chapter 12\. A Practical Framework for Responsible AI Security
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。负责任的人工智能安全实用框架
- en: The future is already here—it’s just not evenly distributed.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 未来已经到来——只是分布不均。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: William Gibson, author of *Neuromancer* and inventor of the term “cyberspace”
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 威廉·吉布森，*神经漫游者*的作者和“赛博空间”一词的发明者
- en: 'In 1962, the final installment of a then-obscure comic anthology series unveiled
    what would become one of the world’s most adored superheroes. *Amazing Fantasy*
    issue #15 marked the debut of Spider-Man, a character who, according to a [2022
    CNN story](https://oreil.ly/IDnD3), has ascended to become the world’s most famous
    superhero. But what propelled Spider-Man to this esteemed status? The answer lies
    in the compelling message woven into his origin story.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在1962年，当时一个默默无闻的漫画系列最终篇揭露了一个将成为世界上最受喜爱的超级英雄的故事。*惊奇幻想*第15期标志着蜘蛛侠的首次亮相，这个角色，据2022年CNN的一篇报道[2022
    CNN story](https://oreil.ly/IDnD3)，已经上升为世界上最著名的超级英雄。但是什么推动了蜘蛛侠达到这种崇高的地位？答案在于其起源故事中融入的引人入胜的信息。
- en: In this inaugural tale, Peter Parker is a high school introvert whose life is
    forever changed after being bitten by a radioactive spider. Suddenly equipped
    with remarkable powers—superhuman strength, agility, and the ability to spin webs—Peter
    adopts the alias of Spider-Man and steps into the limelight as a costumed hero.
    However, his early indifference to the broader implications of his actions leads
    to a personal tragedy that costs the life of his beloved Uncle Ben. This pivotal
    moment brings Peter to a critical realization, encapsulated in the now-iconic
    phrase, “With great power comes great responsibility.”
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个开篇故事中，彼得·帕克是一个高中内向者，在被一只放射性蜘蛛咬伤后，他的生活永远改变了。突然拥有了非凡的力量——超人的力量、敏捷性和吐丝的能力——彼得以蜘蛛侠的化名进入公众视野，成为一位穿制服的英雄。然而，他对自己的行为更广泛影响的早期冷漠导致了一场个人悲剧，他最爱的叔叔本尼的生命因此丧生。这个关键时刻让彼得有了现在已成为标志性语句的深刻认识：“能力越大，责任越大。”
- en: 'Just as Peter Parker was thrust into a world of great power and consequent
    responsibility, practitioners in the AI field are navigating an era of unprecedented
    technological acceleration. The rapid evolution of AI and LLMs, while unlocking
    the immense potential for innovation and advancement, also amplifies the responsibility
    of those who wield these technologies. Ensuring their safety and security is a
    technical challenge and a moral imperative. The narrative of Spider-Man serves
    as a poignant reminder that with the great power bestowed by these advanced technologies
    comes a critical responsibility to use them wisely, ethically, and with a keen
    awareness of their impact on society and individual lives. As we stand on the
    brink of AI’s vast potential, we must heed the lesson encapsulated in Peter Parker’s
    journey: to embrace our responsibilities and ensure that our technological advancements
    foster benefits, not detriments.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如彼得·帕克被推入一个充满巨大力量和随之而来的责任的世界一样，人工智能领域的从业者正在经历一个前所未有的技术加速时代。人工智能和LLMs的快速演变，虽然释放了创新和进步的巨大潜力，但也放大了掌握这些技术的人们的责任。确保它们的安全和可靠是一个技术挑战和道德
    imperative。蜘蛛侠的故事作为一段深刻的提醒，表明这些先进技术赋予的巨大力量伴随着一个关键的责任，即明智、道德地使用它们，并敏锐地意识到它们对社会和个体生活的影响。当我们站在人工智能巨大潜力的边缘时，我们必须牢记彼得·帕克旅程中蕴含的教训：承担我们的责任，确保我们的技术进步带来利益，而不是损害。
- en: As we embark on this chapter, our journey mirrors the ever-expanding universe
    of AI and LLM technologies—where the bounds of possibility are constantly redrawn.
    Our purpose here is twofold. Firstly, we aim to examine the trends marking the
    acceleration of these powerful technologies. The velocity at which AI and LLMs
    advance is reshaping our tools and methodologies, as well as redefining our ethical
    and security landscapes. By examining these trends, we seek to understand the
    pace of technological advancement and its broader role in responsible, secure
    AI application development.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始这一章时，我们的旅程与人工智能和LLMs技术的不断扩大的宇宙相呼应——其中可能性的边界不断被重新划定。我们的目的是双重的。首先，我们旨在研究标志着这些强大技术加速的趋势。AI和LLMs的发展速度正在重塑我们的工具和方法，以及重新定义我们的伦理和安全格局。通过研究这些趋势，我们寻求了解技术进步的速度及其在负责任、安全的人工智能应用开发中的更广泛作用。
- en: Secondly, this chapter endeavors to arm the reader with a robust framework for
    the safe, secure, and responsible use of AI and LLM technologies. This framework,
    which I call RAISE, is intended to wrap together all the concepts you’ve learned
    earlier in the book and make them easier to apply. By offering insights into best
    practices, ethical considerations, and security measures, we aim to empower you
    to harness the power of AI and LLMs with a conscientious and informed approach.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，本章旨在为读者提供一个强大框架，用于安全、可靠和负责任地使用人工智能和大型语言模型（LLM）技术。这个框架，我称之为RAISE，旨在将你在本书中早期学到的所有概念整合在一起，使它们更容易应用。通过提供最佳实践、伦理考量和安全措施方面的见解，我们旨在赋予你以负责任和知情的方式利用人工智能和LLM的力量的能力。
- en: Power
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 功率
- en: Let’s start by looking at the trends pushing forward capabilities of LLMs. We
    have recently perceived a spike in the capabilities of AI systems, as evidenced
    by the rush of new applications and investments. But is this a onetime spike that
    is now in the past, or are we still in the early phases of an exponential curve
    that will multiply both the power of and risks associated with these systems?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看推动LLM能力发展的趋势。最近，我们观察到人工智能系统能力的激增，这从新应用和投资的激增中可以看出。但这只是一个一次性激增，现在已经过去了，还是我们仍然处于指数曲线的早期阶段，这些系统的力量和风险都将成倍增加？
- en: I started my first AI software company in the early 1990s. It was called Emergent
    Behavior, which I still think is a super cool name for an AI software company.
    It doesn’t exist anymore, but I think telling you a bit about that experience
    will help illustrate the technology acceleration happening in AI-capable hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我在20世纪90年代初开始了我的第一家AI软件公司。它叫做Emergent Behavior，我认为这是一个为AI软件公司起得非常酷的名字。它现在已经不存在了，但我认为告诉你一些关于那段经历的事情将有助于说明在AI硬件中正在发生的科技加速。
- en: In the 1990s, my team built software with genetic algorithms and neural networks.
    Our software was capable of doing real-world work. We successfully sold it to
    massive investment banks building arbitrage trading strategies and to Fortune
    500 manufacturing companies optimizing their factory floor layouts. However, ultimately,
    the meager computing power and memory to which we had access meant we were severely
    constrained. We just couldn’t accomplish most of the grand tasks we had in mind.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪90年代，我的团队使用遗传算法和神经网络开发了软件。我们的软件能够完成现实世界的工作。我们成功地将它卖给了大型投资银行，用于构建套利交易策略，以及卖给《财富》500强制造公司，用于优化他们的工厂布局。然而，最终，我们可用的微弱的计算能力和内存意味着我们受到了严重的限制。我们根本无法完成我们心中大多数宏伟的任务。
- en: The most powerful computer I had access to back in those days was a Macintosh
    IIfx. It included a Motorola 68030 processor with a clock speed best measured
    in megahertz. My computer had 16 megabytes of RAM. Today’s processors run in gigahertz,
    not megahertz, and the memory is in gigabytes instead of megabytes. That mega
    to giga change alone implies a ~1,000x improvement. But clock speed isn’t the
    only improvement, and Moore’s law implies clever chip designers should have been
    able to provide a doubling of overall computing power every two years. That would
    give us a 64,000-fold increase in speed over that period.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那时候我能用到的最强大的计算机是Macintosh IIfx。它配备了一颗Motorola 68030处理器，其时钟速度以兆赫兹计。我的电脑有16兆字节的RAM。今天的处理器以千兆赫兹运行，而不是兆赫兹，内存容量以千兆字节计算，而不是兆字节。仅从兆到千兆的变化就暗示了大约1000倍的提升。但时钟速度并不是唯一的改进，摩尔定律暗示聪明的芯片设计师应该能够每两年提供整体计算能力翻倍。这将使我们在那个时期内的速度提高64,000倍。
- en: 'An improvement of 64,000 fold sounds impressive—and it is. But even that is
    not nearly enough to account for the explosion in capabilities we’ve seen in that
    period. It simply wouldn’t have given us enough computing power to train and run
    today’s LLMs. There is something else going on here. Two other converging trends
    enabled this: GPUs and Cloud Computing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 64,000倍的提升听起来令人印象深刻——确实如此。但即使这样也几乎无法解释我们在那个时期所看到的能力的爆炸性增长。它根本无法给我们提供足够的计算能力来训练和运行今天的LLM。这里还有其他事情在发生。两个其他汇聚的趋势促成了这一点：GPU和云计算。
- en: GPUs
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU
- en: In the late 1990s, the need for games to render more polygons at faster frame
    rates led to the development of special graphics processing units (GPUs) by companies
    like 3dfx, ATI Technologies, and Nvidia. These companies built GPU architectures
    to handle massive numbers of parallel math operations to compute 3D spatial relationships.
    While this was fantastic for games, it is also just the right recipe for accelerating
    neural networks, which need the exact same kind of support.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪90年代末，为了在更高的帧率下渲染更多的多边形，游戏的需求导致了3dfx、ATI Technologies和Nvidia等公司开发了特殊的图形处理单元（GPU）。这些公司构建了GPU架构来处理大量的并行数学运算，以计算3D空间关系。虽然这对游戏来说很棒，但这也正是加速神经网络（神经网络需要完全相同类型的支持）的正确配方。
- en: In my early 1990s AI startup, my Mac IIfx had a Motorola 68882 math coprocessor
    alongside its regular CPU. This coprocessor speeds up the types of floating-point
    math operations you’d need for gaming or AI, in addition to spreadsheets and other
    more mundane applications. The 68882 was the same coprocessor design used in machines
    from expensive, top-of-the-line workstation vendors like Sun Microsystems and
    was one of the fastest chips available at the time. It was rated at 422,000 floating-point
    operations per second (kFLOPS). That sounds like a lot, but it just wasn’t enough
    to make practical the kinds of AI tasks we wanted to accomplish.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我20世纪90年代初的人工智能初创公司中，我的Mac IIfx除了常规CPU外，还配备了一块Motorola 68882数学协处理器。这个协处理器可以加速游戏或人工智能所需的浮点数学运算，以及电子表格和其他更平凡的应用。68882与Sun
    Microsystems等昂贵的高端工作站供应商使用的相同协处理器设计，是当时最快的芯片之一。它的浮点运算速度为每秒422,000次（kFLOPS）。这听起来很多，但还不足以使我们要完成的AI任务变得实用。
- en: 'How much faster is a modern server than my old workstation? While Moore’s law
    would imply that a new server might be ~64,000 times faster than my old workstation,
    the architecture of GPUs changes the game for the operations you need for AI applications.
    Today, a top-of-the-line GPU is an NVIDIA H100, rated at 60 trillion floating-point
    operations per second (teraflops). Let’s do some math:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现代服务器比我的旧工作站快多少？虽然摩尔定律可能意味着新服务器可能比我的旧工作站快64,000倍，但GPU的架构改变了AI应用所需的操作游戏规则。如今，顶级的GPU是NVIDIA
    H100，每秒浮点运算速度为60万亿次（teraflops）。让我们来做一些数学计算：
- en: $Speed Increase equals StartFraction NVIDIA upper H 100 FLOPS Over Motorola
    68882 FLOPS EndFraction$
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: $速度提升等于 NVIDIA H100 FLOPS 除以 Motorola 68882 FLOPS$
- en: The NVIDIA H100 GPU is approximately 142,180,095 times faster than the Motorola
    68882 math coprocessor! This staggering increase highlights the monumental strides
    made in chip computational capabilities, which underpin the current advancements
    in AI and machine learning technologies. That mind-boggling speed increase shows
    that we are on a massively accelerating hardware curve for AI-capable hardware.
    The curve over that time period is over 2,000 times steeper than even the exponential
    Moore’s law curve would have predicted!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA H100 GPU比Motorola 68882数学协处理器快约142,180,095倍！这个惊人的增长突显了芯片计算能力取得的巨大进步，这些进步支撑了当前人工智能和机器学习技术的进步。这种令人难以置信的速度提升表明，我们在AI硬件的硬件曲线上正经历着巨大的加速。在那个时间段内，曲线的陡峭程度甚至超过了指数级的摩尔定律曲线所能预测的2,000倍以上！
- en: 'One hundred forty-two million times is a shockingly significant improvement:
    what the modern GPU can compute in a single second would have taken 4.5 years
    on my old workstation’s coprocessor! But it’s still not enough computing power
    to account for the explosion we’ve seen. We need cloud computing to complete the
    picture.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一亿四千二百二十万倍是一个惊人的显著提升：现代GPU在一秒内可以完成的计算，在我的旧工作站协处理器上需要4.5年！但这仍然不足以解释我们所看到的爆炸性增长。我们需要云计算来完善这幅画面。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Recently, publications by Taiwan Semiconductor Manufacturing Company (TSMC),
    which fabricates many of the world’s GPUs, say the company expects to see as much
    as another one million times improvement in computational performance/watt of
    electricity over the next 10 to 15 years, with performance tripling every 2 years.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，台湾半导体制造公司（TSMC）的出版物显示，该公司预计在未来10到15年内，其生产的许多世界级GPU的计算性能/每瓦电力将提高多达一百万倍，性能每两年翻三倍。
- en: Cloud
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云计算
- en: The other trend we need to account for is the cloud. Even the massive speed
    improvement on the single-system hardware curve isn’t enough to enable today’s
    sudden AI boom.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要考虑的另一个趋势是云计算。即使单系统硬件曲线上的巨大速度提升也不足以使今天的AI热潮成为可能。
- en: In 2006, most people knew Amazon as an online seller of books, CDs, and DVDs.
    The introduction of Amazon Web Services (AWS) surprised everyone and popularized
    the idea of on-demand, pay-as-you-go cloud computing. Cloud is so pervasive today
    that I don’t need to explain the concept to you, but I will remind you what it
    means to AI.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，大多数人知道亚马逊是一家在线销售书籍、CD和DVD的商家。亚马逊网络服务（AWS）的推出让所有人都感到惊讶，并普及了按需、按使用付费的云计算概念。云服务如此普遍，以至于我无需向你解释这个概念，但我将提醒你这对AI意味着什么。
- en: Today, whether you’re using AWS, Microsoft Azure, or Google Cloud Platform (GCP),
    you can access on-demand clusters of GPU-enabled servers with nearly limitless
    memory attached to ultrafast networks. You can set up massive clusters in minutes
    if you have enough money in your account. The companies that are training today’s
    foundation models see such a high potential return on investment that they are
    willing to pay massive cloud computing bills. It’s been [widely reported](https://oreil.ly/hAsfW)
    that OpenAI spent approximately $100,000,000 on cloud resources to train GPT-4.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，无论你使用AWS、Microsoft Azure还是Google Cloud Platform（GCP），你都可以访问带有几乎无限内存的GPU服务器集群，这些服务器集群连接到超快网络。如果你账户中有足够的资金，你可以在几分钟内设置大规模的集群。今天正在训练基础模型的公司看到了如此高的投资回报潜力，以至于他们愿意支付巨额的云计算账单。据[广泛报道](https://oreil.ly/hAsfW)，OpenAI在云资源上花费了大约1亿美元来训练GPT-4。
- en: I don’t believe we’re yet at the limits. In February 2024, Nvidia CEO Jensen
    Huang and OpenAI CEO Sam Altman were in the news. Huang said the world will quickly
    build a trillion dollars’ worth of new data centers to power AI software, and
    reports say that OpenAI’s Sam Altman is looking to raise seven trillion dollars
    to develop and build new AI chips. We’ve now entered an era where investments
    in AI hardware will be measured in trillions of dollars, ensuring we will see
    continued increases in computing power applied to these models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们还没有达到极限。2024年2月，Nvidia首席执行官黄仁勋和OpenAI首席执行官山姆·奥特曼都成为了新闻焦点。黄仁勋表示，世界将迅速建设价值万亿美元的新数据中心来支持AI软件，而报道说OpenAI的萨姆·奥特曼正在寻求筹集7000亿美元来开发和构建新的AI芯片。我们现在已经进入了一个AI硬件投资将以万亿美元计量的时代，这将确保我们将看到应用于这些模型的计算能力持续增加。
- en: Open Source
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开源
- en: Another accelerant of capabilities and risk is the rise of open source LLM technologies.
    November 30, 2022, is often celebrated for the release of ChatGPT, when OpenAI
    introduced most of the world to LLM technology. However, February 24, 2023, may
    hold even more significance in the annals of LLM technology due to Facebook/Meta’s
    release of the *Large Language Model Meta AI* (LLaMA, now usually written Llama).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个推动能力和风险上升的加速器是开源LLM技术的兴起。2022年11月30日，ChatGPT的发布通常被庆祝，当时OpenAI向全世界介绍了LLM技术。然而，2023年2月24日可能在LLM技术的历史上具有更大的意义，因为Facebook/Meta发布了*大型语言模型Meta
    AI*（LLaMA，现在通常写作Llama）。
- en: Meta’s press release professed a commitment to open science, highlighting the
    release of LLaMA as a step in enabling broader access to state-of-the-art AI technologies.
    LLaMA is provided in multiple sizes to cater to various research needs, from validating
    new approaches to exploring novel use cases. By offering smaller, more efficient
    models, Meta aimed to lower the barrier to entry into the LLM space, allowing
    researchers with limited resources to contribute to and innovate within this rapidly
    evolving field.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Meta的新闻发布会宣称了对开源科学的承诺，强调发布LLaMA作为使更广泛地获取最先进的AI技术成为可能的一步。LLaMA提供多种尺寸，以满足各种研究需求，从验证新方法到探索新颖的使用案例。通过提供更小、更高效的模型，Meta旨在降低进入LLM领域的门槛，让资源有限的研究人员能够参与到这个快速发展的领域并在此进行创新。
- en: While Meta’s initial approach to releasing LLaMA aimed to democratize access
    to cutting-edge AI technology, there was a sense of caution. The company recognized
    the transformative potential of making such powerful models more accessible, but
    was equally aware of the risks associated with their misuse. Meta opted for a
    controlled release under a noncommercial license to navigate this delicate balance,
    making LLaMA accessible only to researchers at academic institutions, government
    agencies, and nongovernmental organizations who met specific criteria. Meta intended
    to foster responsible innovation while mitigating the dangers of widespread access
    to such potent technology.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当Meta最初发布LLaMA时，其目标是为了让尖端AI技术更加民主化，但同时也存在谨慎的态度。公司认识到让如此强大的模型更加易于获取具有变革潜力，但同样也意识到其误用的风险。Meta选择在非商业许可下进行控制性发布，以平衡这种微妙的关系，使得LLaMA仅对符合特定标准的学术机构、政府机构和非政府组织的研究人员开放。Meta旨在培养负责任的创新，同时减轻这种强大技术广泛获取的危险。
- en: Despite these precautions, the situation took an unexpected turn. Just a week
    after LLaMA was released to selected researchers, the model found its way onto
    the internet via a leak on 4chan (the same hacker forum that launched the attack
    on Tay we detailed in [Chapter 1](ch01.html#chatbots_breaking_bad)). The leak
    quickly spiraled out of control, with users redistributing LLaMA across various
    platforms, including GitHub and Hugging Face. Meta’s efforts to contain the spread
    through takedown requests proved futile; the model had already disseminated too
    widely and rapidly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管采取了这些预防措施，但情况出现了意想不到的转折。就在LLaMA向选定研究人员发布后一周，该模型通过4chan（我们详细介绍的[第1章](ch01.html#chatbots_breaking_bad)中攻击Tay的同一黑客论坛）的泄露进入了互联网。泄露事件迅速失控，用户在包括GitHub和Hugging
    Face在内的各种平台上重新分配LLaMA。Meta通过采取删除请求来遏制传播的努力证明是徒劳的；模型已经过于广泛和迅速地传播开来。
- en: Faced with LLaMA’s uncontrollable proliferation, the company decided to reassess
    its stance and ignore its initial trepidation about the risk of widely distributing
    open LLM technology. In a move that marked a significant shift from its original
    restrictive licensing approach, Meta eventually released LLaMA under a more liberal
    license, making it available to anyone.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 面对LLaMA无法控制的扩散，公司决定重新评估其立场，并忽略最初对广泛分发开放LLM技术风险的担忧。在从其原始限制性许可方法发生重大转变的行动中，Meta最终在更自由的许可下发布了LLaMA，使其对任何人开放。
- en: Following this episode, Meta continued to push forward. The company introduced
    LLaMA 2, a more advanced version of the original model, alongside specialized
    variants like Llama Chat and Code Llama. These subsequent releases underscore
    Meta’s commitment to advancing the field of AI, albeit with a nuanced understanding
    of the complexities involved in managing the distribution of powerful technological
    tools in an open and interconnected digital landscape. This evolution in Meta’s
    approach highlights a pivotal moment in the discourse on the democratization of
    AI technology, underscoring the tension between innovation and the imperative
    to ensure the responsible use of AI.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一事件之后，Meta继续推进。公司推出了LLaMA 2，这是原始模型的更高级版本，以及像Llama Chat和Code Llama这样的专用变体。这些后续发布凸显了Meta在推进AI领域方面的承诺，尽管对在开放和互联的数字环境中管理强大技术工具的复杂性有着细微的理解。Meta方法的变化突出了关于AI技术民主化讨论的关键时刻，强调了创新与确保AI负责任使用的必要性之间的紧张关系。
- en: Numerous other high-quality, open source LLMs have emerged in this rapidly evolving
    landscape, including BLOOM, MPT, Falcon, Vicuna, and Mixtral. Among these, Mixtral
    stands out for its innovative approach and technological advancements.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个快速发展的环境中，出现了许多其他高质量的开源LLM，包括BLOOM、MPT、Falcon、Vicuna和Mixtral。在这些中，Mixtral因其创新方法和技术进步而脱颖而出。
- en: Mixtral-8x7B showcases a high-quality sparse mixture of experts (SMoE) model.
    This development represents a significant technological leap forward, offering
    open weights and licensing under the permissive Apache 2.0 license. According
    to the development team, Mixtral has demonstrated superior performance to LLaMA
    2 70B across most benchmarks, achieving up to six times faster inference times,
    and either matches or surpasses the capabilities of OpenAI’s GPT-3.5 on most standard
    benchmarks. It is now considered one of the most robust open-weight models available
    under a permissive license.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral-8x7B 展示了一个高质量的稀疏混合专家（SMoE）模型。这一发展代表了一个重大的技术飞跃，提供了开放权重和 Apache 2.0 许可下的许可。根据开发团队的说法，Mixtral
    在大多数基准测试中表现优于 LLaMA 2 70B，实现了高达六倍更快的推理时间，并且在大多数标准基准测试中与 OpenAI 的 GPT-3.5 相匹配或超过其能力。现在它被认为是许可下最稳健的开源权重模型之一。
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: SMoE is a type of LLM architecture designed to improve efficiency and scalability.
    It allows a model to learn different parts of the input space using specialized
    “expert” subnetworks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SMoE 是一种 LLM 架构，旨在提高效率和可扩展性。它允许模型使用专门的“专家”子网络来学习输入空间的不同部分。
- en: The shift toward open source models marks a significant step in accelerating
    technological progress. With this change, the capabilities once reserved for major
    corporations are now accessible to a wider audience, including scientists, researchers,
    and small companies. This broader access will drive innovation, as demonstrated
    by projects like Mixtral. The sharing of state-of-the-art technology like this
    means the base science of LLM technology will continue to benefit from academic
    and commercial research in the coming years, with no single organization able
    to monopolize it and slow progress.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 向开源模型转变标志着加速技术进步的重要一步。随着这一变化，曾经只为大型企业保留的能力现在对更广泛的受众开放，包括科学家、研究人员和小公司。这种更广泛的访问将推动创新，正如
    Mixtral 项目所证明的那样。这种最先进技术的共享意味着 LLM 技术的基础科学将在未来几年继续受益于学术和商业研究，没有任何单一组织能够垄断它并减缓进步。
- en: However, the open source nature of these technologies also means they are being
    used by malicious actors, including thieves, terrorists, and countries like Russia,
    China, and North Korea. This reality undermines the effectiveness of public pressure
    and regulations aimed at a handful of organizations like OpenAI and Google in
    controlling the proliferation and misuse of LLM and AI technologies. The technology
    has become too widespread to restrict its use to only beneficial purposes. The
    genie is out of the bottle, and there’s no putting it back.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些技术的开源性质也意味着它们被恶意行为者使用，包括盗贼、恐怖分子以及俄罗斯、中国和朝鲜等国家。这一现实削弱了针对 OpenAI 和 Google
    等少数组织的公众压力和法规的有效性，这些法规旨在控制 LLM 和 AI 技术的扩散和滥用。这项技术已经过于普及，无法仅限于有益用途。瓶子里的精灵已经逃逸，无法再将其放回。
- en: Multimodal
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态
- en: Text-to-image models such as DALL-E, Midjourney, and Stable Diffusion have quickly
    revolutionized how many people approach visual creative endeavors. In January
    2021, OpenAI’s DALL-E was the first to make waves by introducing the ability to
    generate complex images from textual descriptions. This model, a variant of the
    GPT-3 LLM, showcased the potential of combining natural language processing with
    image generation, setting a precedent for the kind of creative possibilities that
    AI could unlock.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像模型，如 DALL-E、Midjourney 和 Stable Diffusion，迅速改变了许多人对待视觉创意活动的方式。2021 年 1
    月，OpenAI 的 DALL-E 通过引入从文本描述生成复杂图像的能力而引起轰动。这个模型是 GPT-3 LLM 的一个变体，展示了将自然语言处理与图像生成相结合的潜力，为
    AI 可能解锁的创意可能性树立了先例。
- en: Following DALL-E, the commercial service Midjourney began its open beta in July
    2022, offering a unique approach to image generation. Operated through a Discord
    bot, Midjourney allowed users to create images from text prompts, emphasizing
    an interactive and community-centric creation model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DALL-E 之后，商业服务 Midjourney 于 2022 年 7 月开始公开测试版，提供了一种独特的图像生成方法。通过 Discord 机器人运营，Midjourney
    允许用户从文本提示中创建图像，强调了一个互动和以社区为中心的创作模式。
- en: The field of text-to-image took another turn with the release of the open source
    Stable Diffusion project in August 2022\. As an open source model, Stable Diffusion
    made high-quality image generation accessible to a broader audience, allowing
    anyone with consumer grade hardware to generate detailed visuals from textual
    descriptions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年8月，开源Stable Diffusion项目的发布使文本到图像领域发生了另一场转变。作为一个开源模型，Stable Diffusion使高质量图像生成对更广泛的受众变得可行，任何拥有消费级硬件的人都可以从文本描述中生成详细的视觉图像。
- en: Progress has been astonishingly rapid in this area. In just a few short years,
    we have evolved from the early images, characterized by easily identifiable flaws
    (such as creepy, inaccurately rendered fingers), to the creation of photorealistic
    images that challenge our ability to distinguish them from actual photographs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域，进步的速度令人惊叹。仅仅几年时间，我们就从早期图像发展到具有明显缺陷（如令人毛骨悚然的、渲染不准确的手指）的阶段，到能够创建挑战我们分辨能力的逼真图像。
- en: This era of hyperrealistic AI-generated content has given rise to computer-generated
    Instagram influencers, exemplified by Aitana Lopez, who command substantial online
    followings and earn significant income, often without their fans realizing they
    are not real people. These virtual influencers, created entirely through advanced
    generative models, mark a new phase in digital culture. They highlight not only
    the capabilities of AI to produce content that resonates with human audiences,
    but also raise profound questions about authenticity, identity, and the nature
    of influence in the digital age.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个超现实主义AI生成内容的时代催生了计算机生成的Instagram网红，以Aitana Lopez为代表，他们拥有庞大的在线粉丝群体，并赚取了可观的收入，而他们的粉丝往往没有意识到他们并非真实人物。这些虚拟网红完全通过高级生成模型创建，标志着数字文化的新阶段。它们不仅突显了AI产生与人类观众产生共鸣的内容的能力，而且也提出了关于真实性、身份和数字时代影响力的本质的深刻问题。
- en: When I started writing this book in 2023, accessing text-to-image models was
    challenging. It often required you to set up complex accounts (as with Midjourney)
    or have access to high-end hardware (for open source Stable Diffusion). Today,
    the mainline chatbots from OpenAI and Google are multimodal, treating text and
    images interchangeably. They can read text from uploaded images and generate new
    photorealistic images from a simple prompt—all as part of the same conversation.
    This integration with mainstream chatbots means the bar to access this technology
    has dropped to where almost anyone can use it—for good or bad!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在2023年开始写这本书时，访问文本到图像模型是一项挑战。这通常需要你设置复杂的账户（例如Midjourney）或访问高端硬件（用于开源Stable
    Diffusion）。如今，OpenAI和Google的主线聊天机器人是多模态的，将文本和图像同等对待。它们可以读取上传图像中的文本，并从简单的提示中生成新的逼真图像——所有这些都在同一对话中进行。这种与主流聊天机器人的集成意味着使用这项技术的门槛已经降低到几乎任何人都可以使用——无论好坏！
- en: In February 2024, OpenAI announced Sora, a text-to-video model that creates
    incredibly realistic videos from short prompts. Shortly thereafter, in April 2024,
    [Microsoft announced](https://oreil.ly/I6-pX) a new AI model called VASA that
    can create “lifelike talking faces of virtual characters with appealing visual
    affective skills (VAS), given a single static image and a speech audio clip.”
    With other open source text-to-video models being rapidly developed, we’re about
    to enter an age where the very nature of what’s real will be challenged. Recently,
    a company in Hong Kong lost $25 million when an employee was duped on a Zoom call
    by speaking to a deep fake of the company’s CFO. We’re about to enter a world
    where anyone can instantly and cheaply create a sophisticated deepfake video.
    It’s not hard to imagine that *The Matrix* is not far behind.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年2月，OpenAI宣布推出Sora，这是一种从简短提示中创建极其逼真视频的文本到视频模型。紧接着，2024年4月，[微软宣布](https://oreil.ly/I6-pX)了一种名为VASA的新AI模型，该模型能够根据一张静态图像和一段语音音频剪辑创建“具有吸引人的视觉情感技能（VAS）的逼真虚拟人物对话面部”。随着其他开源文本到视频模型迅速发展，我们即将进入一个挑战现实本质本身的全新时代。最近，一家香港公司因一名员工在Zoom通话中被公司CFO的深度伪造图像欺骗，损失了2500万美元。我们即将进入一个任何人都可以瞬间且低成本地创建复杂深度伪造视频的世界。不难想象，《黑客帝国》离我们并不遥远。
- en: Warning
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If your LLM application is multimodal and can read text from images or video,
    you’re opening up a whole new world of vulnerabilities. Consider that prompt injection
    attacks can now be launched by including malicious text in an image fed into your
    model as a prompt. Or your training data could be poisoned if you include images
    with text that mislead your model. These are just more vectors to watch for!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 LLM 应用程序是跨模态的，并且可以读取图像或视频中的文本，你正在开启一个全新的漏洞世界。考虑一下，现在可以通过在作为提示输入到你的模型中的图像中包含恶意文本来发起提示注入攻击。或者，如果你的训练数据中包含了可能误导你的模型的文本图像，那么你的训练数据可能已经被毒化了。这些都是需要关注的更多向量！
- en: Autonomous Agents
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自主代理
- en: Just a few months after the introduction of ChatGPT, Auto-GPT was launched in
    March 2023 by Toran Bruce Richards of the software development company Significant
    Gravitas. Built on OpenAI’s GPT-4, Auto-GPT introduced the concept of autonomy,
    allowing LLM-powered agents to act toward a goal with minimal human guidance.
    This feature enabled Auto-GPT to generate prompts to achieve a user-defined goal
    autonomously, differentiating it from ChatGPT’s requirement for continuous human
    input. The Auto-GPT framework introduces expanded short-term memory capabilities,
    allowing agents to connect to the internet and call upon third-party services.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 介绍后的几个月，由软件开发公司 Significant Gravitas 的 Toran Bruce Richards 在 2023
    年 3 月推出了 Auto-GPT。建立在 OpenAI 的 GPT-4 之上，Auto-GPT 引入了自主性的概念，允许由 LLM 驱动的代理在最小的人类指导下朝着目标行动。这一特性使得
    Auto-GPT 能够自主生成提示以实现用户定义的目标，与 ChatGPT 需要持续人类输入的要求不同。Auto-GPT 框架引入了扩展的短期记忆能力，允许代理连接到互联网并调用第三方服务。
- en: The introduction of Auto-GPT generated massive buzz at the time, quickly gaining
    traction and generating substantial discussion for its approach to AI autonomy.
    Thousands of users rapidly adopted the tool for various projects, leveraging its
    ability to tackle more complex tasks than ChatGPT could handle alone. This included
    creating and using unsupervised agents for software development, business operations,
    financial transactions, and even health care–related tasks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT 的引入当时引起了巨大的轰动，迅速获得关注并引发了大量关于其 AI 自主方法讨论。成千上万的用户迅速采用这个工具进行各种项目，利用其处理比
    ChatGPT 单独能处理的更复杂任务的能力。这包括创建和使用无监督代理进行软件开发、业务运营、金融交易，甚至与医疗相关的工作。
- en: The adoption of Auto-GPT faced challenges due to its architectural design and
    the operational costs associated with its inefficient use of OpenAI’s expensive
    API resources. The buzz around Auto-GPT soon died out. However, this isn’t the
    end of the story of autonomous agents built on LLMs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-GPT 的采用由于其架构设计和与不高效使用 OpenAI 贵重 API 资源相关的运营成本而面临挑战。围绕 Auto-GPT 的热议很快消散。然而，这并不是基于
    LLM 的自主代理故事的终结。
- en: In the wake of Auto-GPT, dozens of other open source and research projects have
    taken up that mantle, and we’ll surely see fast progress in making these concepts
    more generalizable and less expensive. Beyond that, mainstream players like OpenAI
    have introduced concepts like plug-ins that allow their LLMs to interact directly
    with third-party internet resources. These goal-completion-seeking, autonomous
    agent architectures already show massive potential in many applications. With
    the desire to use AI in this fashion, we’ll undoubtedly see rapid investment and
    progress.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Auto-GPT 之后，数十个其他开源和研究项目接过了这个接力棒，我们肯定会看到在这些概念更通用和更便宜方面的快速进步。除此之外，像 OpenAI
    这样的主流玩家已经引入了像插件这样的概念，允许他们的 LLM 直接与第三方互联网资源交互。这些寻求完成目标、具有自主架构的代理在许多应用中已经显示出巨大的潜力。随着以这种方式使用
    AI 的愿望，我们无疑会看到快速的投资和进步。
- en: 'However, the most critical lesson from Auto-GPT was the incredibly rapid pace
    at which it was deployed in the wild with little to no oversight. We discussed
    excessive agency back in [Chapter 7](ch07.html#trust_no_one): putting unsupervised
    power in the hands of a naive AI, with few guardrails in place, could be incredibly
    dangerous—and few stopped to think about it. The development community’s overall
    lack of caution shown in the rapid adoption of the technology demonstrates with
    some certainty that we must put better security and safety measures in place before
    the next leap in self-directed autonomous systems. We can’t trust the broad human
    population to supervise these capabilities independently. The task is too complex
    to leave to individuals; we must solve it as an industry.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Auto-GPT最重要的教训是它在野外部署的速度之快，几乎没有任何监管。我们之前在[第7章](ch07.html#trust_no_one)讨论了过度的代理：将无监督的权力交到缺乏经验的AI手中，几乎没有障碍，可能会非常危险——而且很少有人停下来思考。开发社区在快速采用这项技术时表现出的总体缺乏谨慎，在一定程度上表明，在自我驱动的自主系统取得下一跃之前，我们必须建立更好的安全和保障措施。我们不能相信广大人类独立监督这些能力。这项任务过于复杂，不能留给个人；我们必须作为一个行业来解决它。
- en: Responsibility
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 责任
- en: We’re on a curve showing a continued, dramatic increase in AI capabilities over
    the coming years. How do you plan for the future and make durable decisions today
    that will pay off and keep you, your customers, your employees, your organization,
    and society at large safe as things accelerate? How do you live up to the *great
    responsibility* of managing this *great power*?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正处于一个曲线图上，显示未来几年人工智能能力的持续、戏剧性增长。你如何规划未来，并在今天做出持久的决策，以确保随着事物的加速，你、你的客户、你的员工、你的组织以及整个社会都能保持安全？你如何履行管理这种“巨大力量”的*重大责任*？
- en: The previous chapters of this book have been grounding to help you understand
    the possible. What risks exist today? What real-world examples have shown the
    impact of these vulnerabilities? We’ve even looked at some far-flung, fictional,
    but plausible examples of how these threats might manifest themselves in the future.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的前几章已经为您奠定了基础，帮助您了解可能存在的风险。今天存在哪些风险？哪些现实世界的例子展示了这些漏洞的影响？我们甚至探讨了某些遥远、虚构但可能的例子，说明了这些威胁如何在未来表现出来。
- en: Throughout the book, I’ve offered you the best practical techniques to address
    these vulnerabilities by using state-of-the-art practice with the input of experts
    across the industry. However, with things moving quickly, your best defense is
    to have a generalized, flexible framework to build your defenses. In this book’s
    last section, I’ll give you a framework you can customize to fit your needs and
    that you can adapt as you grow and the technology moves forward.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我提供了最佳实用技术，通过使用行业专家的最新实践来应对这些漏洞。然而，随着事物的发展迅速，你最好的防御是拥有一个通用的、灵活的框架来构建你的防御。在这本书的最后部分，我将为你提供一个可以根据你的需求定制，并且随着你的成长和技术的发展而适应的框架。
- en: The RAISE Framework
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAISE框架
- en: Let’s walk through a framework I have built to help you plan, organize, and
    achieve your goals for a safe and secure project. As you can see in [Figure 12-1](#fig_1_the_raise_framework),
    I call this six-step process the Responsible Artificial Intelligence Software
    Engineering (RAISE) framework. First, we’ll review each step’s meaning and why
    it matters. Then, we’ll break it down into a manageable checklist your team can
    use to track your work along your journey.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我构建的框架，以帮助您规划、组织和实现安全项目的目标。正如您在[图12-1](#fig_1_the_raise_framework)中可以看到的，我称这个六步过程为负责任的人工智能软件工程（RAISE）框架。首先，我们将回顾每一步的含义以及为什么它很重要。然后，我们将将其分解成一个可管理的清单，您的团队可以使用它来跟踪您在旅程中的工作。
- en: '![](assets/dpls_1201.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dpls_1201.png)'
- en: Figure 12-1\. The RAISE framework
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1. RAISE框架
- en: 'The following list includes the six steps; let’s take look at each of these
    in turn:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表包括六个步骤；让我们依次看看每个步骤：
- en: Limit your domain.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 限制你的领域。
- en: Balance your knowledge base.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平衡你的知识库。
- en: Implement zero trust.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实施零信任。
- en: Manage your supply chain.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管理你的供应链。
- en: Build an AI red team.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个AI红队。
- en: Monitor continuously.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续监控。
- en: Limit your domain
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制你的领域
- en: Constraining your application to focus on a limited functional domain is first
    on the list because it is so fundamental and solves a host of problems. ChatGPT
    is an example of an LLM application with nearly zero domain boundaries. Part of
    its appeal is that it was trained on almost the entire internet, and you can ask
    it almost anything. It doesn’t matter if you want a dessert recipe or a block
    of Python code that calculates pi to a thousand digits. ChatGPT is here to help.
    It has an unconstrained domain.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的应用程序限制在关注有限的功能领域是首要任务，因为它非常基础，并解决了许多问题。ChatGPT是一个几乎没有领域边界的LLM应用的例子。它的一部分吸引力在于它几乎是在整个互联网上训练的，你可以问它几乎所有的问题。无论是你想得到一份甜点食谱还是一段计算到千位数的π的Python代码，ChatGPT都在这里帮助你。它有一个不受限制的领域。
- en: 'The challenge with unconstrained domains is that the development team must
    build broad, general-purpose defenses. Rather than designing a short list of “allowed”
    activities, you must design and maintain a comprehensive and likely ever-growing
    list of “denied” activities. Imagine the job of being on the guardrails team at
    OpenAI. You’re going to be constantly expanding this list that says:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在不受限制的领域中的挑战是，开发团队必须构建广泛的通用防御。而不是设计一个“允许”活动的简短列表，你必须设计和维护一个全面且可能不断增长的“禁止”活动列表。想象一下在OpenAI的护栏团队工作的任务。你将不断地扩展这个列表，它说：
- en: Don’t engage in hate speech.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要参与仇恨言论。
- en: Don’t help hackers steal things.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要帮助黑客窃取东西。
- en: Don’t help people build weapons (even if they miss their grandma—see [Chapter 4](ch04.html#prompt_injection)).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要帮助人们制造武器（即使他们错过了他们的奶奶——见第[4](ch04.html#prompt_injection)章）。
- en: And on and on and on…
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及如此等等……
- en: It’s like playing Whac-A-Mole. This explains why we see reports of new security
    issues with ChatGPT every month. But you’re not building ChatGPT, so how does
    this apply to you? If you’re using a general-purpose foundation model like GPT-4,
    you start with an unconstrained domain. In recent real-world examples, a shipping
    company and a car company both put support chatbots on their websites to help
    improve customer service and reduce costs. Great idea! However, they based these
    on general-purpose foundation models without sufficiently restricting their domain.
    Users quickly jailbroke them via prompt injection (see Chapters [1](ch01.html#chatbots_breaking_bad)
    and [4](ch04.html#prompt_injection)—this isn’t much different than Tay), causing
    them to engage in activities ranging from writing songs about the company’s poor
    customer service to writing Python code that the hacker requested, and all at
    the company’s expense. (See [Chapter 8](ch08.html#don_t_lose_your_wallet) for
    a discussion of DoW attacks.)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像玩“打地鼠”游戏。这也解释了为什么我们每个月都会看到有关ChatGPT的新安全问题的报道。但如果你没有在构建ChatGPT，这对你有什么影响呢？如果你使用的是像GPT-4这样的通用基础模型，你开始时是在一个不受限制的领域。在最近的现实世界例子中，一家航运公司和一家汽车公司都在他们的网站上放置了支持聊天机器人，以帮助提高客户服务和降低成本。好主意！然而，他们基于通用基础模型，而没有充分限制他们的领域。用户很快通过提示注入（见第[1](ch01.html#chatbots_breaking_bad)章和第[4](ch04.html#prompt_injection)章——这并不比Tay差多少）来破解它们，导致它们参与从写关于公司糟糕客户服务的歌曲到编写黑客要求的Python代码等一系列活动，所有这些都是在公司的费用上进行的。（见第[8](ch08.html#don_t_lose_your_wallet)章讨论DoW攻击。）
- en: On the other hand, if your company wants to build an application for use on
    a specific use case, such as giving fashion advice, you can take advantage of
    that limited scope. It will be easier and more effective to drive laser focus
    for your LLM on the latest trends in fashion than enforcing a list of all the
    things not to do.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你的公司想要为特定的用例构建一个应用程序，比如提供时尚建议，你可以利用这个有限的范围。将你的LLM聚焦于时尚的最新趋势，比强制执行一个“不要做的事情”列表更容易也更有效。
- en: 'How do you do this? While this list may evolve as things accelerate, here are
    some tips on driving focus to limit the domain:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你怎么做到这一点？虽然随着事情的发展，这个列表可能会演变，但以下是一些关于如何将重点放在限制领域上的建议：
- en: Where possible, start with a smaller, less-general-purpose foundation model.
    Whether you go the open source route or with an LLM-as-a-service provider, there
    are now thousands of specialized models. These models are usually trained on smaller,
    more focused datasets. If your model wasn’t exposed to hate speech, napalm recipes,
    or Python code while it was trained, it’s almost impossible for someone to trick
    it into straying into such territory. As a bonus, these smaller, special-purpose
    models may be dramatically cheaper to operate at scale.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下，从一个更小、更不通用的基础模型开始。无论你是走开源路线还是使用LLM-as-a-service提供商，现在都有数千个专业模型。这些模型通常在更小、更专注的数据集上训练。如果你的模型在训练过程中没有接触到仇恨言论、凝固汽油弹配方或Python代码，那么几乎不可能有人能诱骗它进入这样的领域。作为额外的好处，这些较小、专门用途的模型在规模上可能显著便宜。
- en: If you start with a more general-purpose model, fine-tune it with a function
    that rewards it for staying on topic. Encoding the “desire” to stay on task and
    in scope can be more powerful and elegant than trying to build restrictive guardrails
    later—although you will probably need to add those, too. Use this to drive alignmentbetween
    the model and your goals.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你从一个更通用的模型开始，请使用一个奖励它保持主题的功能来微调它。编码“保持任务和范围”的“愿望”可能比后来尝试建立限制性护栏更强大和优雅——尽管你可能还需要添加这些护栏。使用这一点来驱动模型与你的目标之间的对齐。
- en: Balance your knowledge base
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平衡你的知识库
- en: You must maintain a dynamic balance regarding how much data you give to your
    LLM at runtime. Striking the right balance is one of the most important tasks
    in your system design and will be a significant factor in its safety and security.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须就运行时提供给LLM的数据量保持动态平衡。找到正确的平衡是系统设计中最重要的任务之一，并将成为其安全性和安全性的重要因素。
- en: If you give your model access to too little information, it may be prone to
    hallucinations. As discussed in [Chapter 6](ch06.html#do_language_models_dream_of_electric_sheep),
    while hallucinations can be cute, they can leave your organization open to reputational,
    legal, and safety risks. Equipping your model with an excellent store of knowledge
    on your intended domain helps ensure answers will be accurate and valuable to
    your intended users.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你给你的模型提供的信息太少，它可能会倾向于产生幻觉。如第6章[第6章](ch06.html#do_language_models_dream_of_electric_sheep)中所述，虽然幻觉可能很可爱，但它们可能会让你的组织面临声誉、法律和安全风险。给你的模型配备关于你打算进入的领域的丰富知识库，有助于确保答案将准确且对你的目标用户有价值。
- en: Tip
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Limiting your domain can help you avoid hallucinations. Hallucinations happen
    when the model lacks enough precise data to make an informed prediction. When
    you carefully scope the domain to a small set of activities and limit its use
    outside of those activities, it becomes easier to ensure that you’ve provided
    adequate training or RAG data to allow the LLM to do its job with minimal risk
    of hallucination.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 限制你的领域可以帮助你避免幻觉。当模型缺乏足够精确的数据来做出明智的预测时，就会发生幻觉。当你仔细地将领域缩小到一小组活动并限制其超出这些活动的使用时，确保你已经提供了足够的训练或RAG数据，以便LLM以最小的幻觉风险完成其工作就变得更容易了。
- en: On the other side of this equation, giving your LLM access to too much data
    has its own drawbacks. The overall security fragility and number of attack vectors
    against an LLM app means that anything the LLM knows is at risk of disclosure.
    If it doesn’t know a fact or have access to related data, it can’t accidentally
    give it to an attacker.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个等式的另一边，给你的大型语言模型（LLM）提供过多的数据有其自身的缺点。针对LLM应用的总体安全脆弱性和攻击向量数量意味着LLM所知道的一切都存在泄露的风险。如果它不知道某个事实或没有访问相关数据，它就不会意外地将其提供给攻击者。
- en: Use techniques we’ve discussed, such as RAG and model fine-tuning, to give your
    LLM the knowledge it needs to be effective. At the same time, draw a clear line
    between data it absolutely needs to have and data it shouldn’t have. Take extreme
    care with PII and confidential data. Remember, any data you give to your LLM is
    in danger of being leaked and exposed via any of the vulnerabilities we’ve discussed
    throughout this book.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们讨论过的技术，例如RAG（检索增强生成）和模型微调，为你的LLM提供它需要的知识以使其有效。同时，在它绝对需要的数据和它不应该拥有的数据之间划清界限。对个人身份信息（PII）和机密数据要格外小心。记住，你给LLM的任何数据都存在通过本书中讨论的任何漏洞泄露和暴露的风险。
- en: Implement zero trust
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施零信任
- en: You can’t trust your users. You can’t trust data on the internet. Of course,
    all users aren’t malicious, and all data on the internet isn’t bad or tainted.
    But if you assume you can trust all potential users and all the data you might
    find on the internet, you are putting yourself at unreasonable risk.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你无法信任你的用户。你无法信任互联网上的数据。当然，并不是所有用户都是恶意的，互联网上的所有数据都不是坏的或受污染的。但如果你假设你可以信任所有潜在的用户和你在互联网上可能找到的所有数据，你是在给自己带来不合理的风险。
- en: By extension, if you assume you can’t trust your users or the data on the internet,
    then you should also assume you can’t trust your LLM. Design your architecture
    assuming that the LLM at the core of your application is an enemy sleeper agent
    or at least a confused deputy. In [Chapter 7](ch07.html#trust_no_one), we discussed
    building a zero trust architecture for your app. This means you inspect everything
    coming in and out of your application.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扩展，如果你假设你无法信任你的用户或互联网上的数据，那么你也应该假设你无法信任你的大型语言模型（LLM）。在设计你的架构时，假设你的应用程序核心的LLM是一个敌人潜伏特工或者至少是一个困惑的副手。在[第7章](ch07.html#trust_no_one)中，我们讨论了为你的应用程序构建零信任架构。这意味着你要检查进入和离开你应用程序的每一件事。
- en: 'This is where guardrails can help. They may not be sufficient alone, but they’re
    a critical backstop for when things go wrong. Consider the following mitigation
    steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是护栏能发挥作用的地方。它们可能单独不足以解决问题，但它们是当事情出错时的关键后盾。考虑以下缓解措施：
- en: Screen the prompts coming into your LLM from users. Use traditional techniques
    such as scrubbing for hidden characters or funky encodings and deny lists of terms
    or phrases. Consider using a commercial or open source guardrails framework as
    discussed in [Chapter 11](ch11.html#trust_the_process).
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对进入你的LLM的用户提示进行筛选。使用传统的技术，如清除隐藏字符或古怪的编码，以及拒绝列表中的术语或短语。考虑使用在第11章（ch11.html#trust_the_process）中讨论的商业或开源的护栏框架。
- en: Also screen the prompts that come into your LLM from outside sources via RAG—especially
    for in-the-wild sources such as results from internet searches—using the same
    techniques you use for user prompts. Data coming into your LLM through RAG is
    even more likely to be dangerous or poisoned than data coming from some classes
    of users.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还要筛选来自外部来源（通过RAG）进入你的LLM的提示，特别是对于野外来源，如互联网搜索结果——使用与用户提示相同的技巧。通过RAG进入你的LLM的数据可能比来自某些用户类别的数据更有危险或被污染。
- en: Screen everything that comes out of your LLM. If you can’t trust what went in—and
    you probably can’t—then you can’t trust what comes out. Watch for cases where
    the LLM may try to generate scripts, code, instructions, or even prompts to feed
    another LLM. These could all be signs that your LLM is being tricked into being
    a confused deputy and using the privileges you’ve given it to access backend sources
    for nefarious purposes.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 筛选从你的LLM输出的每一件事。如果你无法信任进入的内容——你很可能无法信任——那么你也无法信任输出的内容。留意LLM可能尝试生成脚本、代码、指令或甚至提示来喂养另一个LLM的情况。这些都可能是你的LLM被欺骗成为困惑的副手并使用你赋予它的权限以进行恶意目的的迹象。
- en: Consider rate-limiting techniques as we discussed in Chapters [4](ch04.html#prompt_injection)
    and [8](ch08.html#don_t_lose_your_wallet). They can be essential to your defense
    against prompt injections, DoS, DoW, and model cloning attacks.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑我们在第4章（ch04.html#prompt_injection）和第8章（ch08.html#don_t_lose_your_wallet）中讨论的速率限制技术。它们对于防御提示注入、DoS、DoW和模型克隆攻击可能是至关重要的。
- en: Lastly, and perhaps most importantly, make informed decisions about how much
    agency you give your LLM. Earlier in this chapter, we discussed the push to implement
    architectures that allow for more autonomy and goal seeking. If you design your
    application so that the LLM can drive specific actions, you expose yourself to
    the possibility it will take those actions, or related actions to which it has
    incidental permissions, at the time you least expect. You don’t want HAL turning
    off your life-support systems without a human in the loop!
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，也许最重要的是，就你给予LLM的权限做出明智的决定。在本章的早期，我们讨论了实施允许更多自主性和目标寻求的架构的推动。如果你设计你的应用程序，使得LLM可以驱动特定的动作，你将使自己面临它在最不期望的时候采取那些动作或相关动作的可能性，这些动作它可能偶然拥有权限。你不想让HAL在没有人类介入的情况下关闭你的生命维持系统！
- en: Manage your supply chain
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理你的供应链
- en: 'Software supply chain security has been one of the hottest topics in security
    for several years. In [Chapter 9](ch09.html#find_the_weakest_link) we reviewed
    large-scale supply chain failures of both proprietary components (SolarWinds)
    and open source components (Log4Shell). We then went on to look at real examples
    of these risks from sources like Hugging Face. These risks are real, and the consequences
    are severe. Some key considerations include:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 软件供应链安全在过去的几年中一直是安全领域的热门话题。在第 9 章中，我们回顾了专有组件（SolarWinds）和开源组件（Log4Shell）的大规模供应链故障。然后我们继续查看来自
    Hugging Face 等来源的这些风险的实际情况。这些风险是真实的，后果是严重的。一些关键考虑因素包括：
- en: Carefully select your foundation model. Is it from a reputable source?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细选择你的基础模型。它是否来自可靠的来源？
- en: Carefully select any third-party training datasets you may use. If possible,
    use tools to provide additional inspection.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细选择你可能使用的任何第三方训练数据集。如果可能，使用工具提供额外的检查。
- en: Use caution when building your own training datasets from public sources. Apply
    techniques to look for intentional data poisoning or illegal materials.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在从公共来源构建自己的训练数据集时，请谨慎行事。应用技术以寻找故意的数据中毒或非法材料。
- en: Be aware of possible biases in the data you use for training. Biased data could
    lead to behavior considered to be inappropriate by some users and put your organization
    at reputational or even legal risk. For example, back in [Chapter 1](ch01.html#chatbots_breaking_bad),
    we looked at a case where an app for job candidate screening had to be shut down
    because it discriminated against women. It didn’t do this because it was mean;
    it did this because of biases inherent in its training data.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意你用于训练的数据中可能存在的偏差。有偏差的数据可能导致某些用户认为不适当的行为，并使你的组织面临声誉或甚至法律风险。例如，在[第 1 章](ch01.html#chatbots_breaking_bad)中，我们回顾了一个案例，一个用于求职者筛选的应用程序因为歧视女性而被关闭。它之所以这样做，并不是因为它刻薄；而是因为它训练数据中固有的偏差。
- en: Be sure to track your third-party components as part of your ML-BOM. If problems
    or vulnerabilities are discovered down the road, you can determine whether you’re
    affected and quickly remedy the situation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将第三方组件作为你的 ML-BOM 的一部分进行跟踪。如果在将来发现问题或漏洞，你可以确定你是否受到影响，并迅速解决问题。
- en: Build this process into your DevSecOps/MLOps/LLMOps development pipeline, as
    discussed in [Chapter 11](ch11.html#trust_the_process). Rigor around checking
    and scrubbing these things should be automated. Don’t depend on spot-checking
    by hand. Update your ML-BOM and store a new version with every build and deploy
    cycle. That way, you’ll always know what you’re running or be able to rewind and
    know what you were running at a particular time should conditions require that.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 将此过程纳入你的 DevSecOps/MLOps/LLMOps 开发流程中，如第 11 章所述（[Chapter 11](ch11.html#trust_the_process)）。对这些事物进行检查和清理的严格性应该自动化。不要依赖手工抽查。每次构建和部署周期更新你的
    ML-BOM，并存储新版本。这样，你将始终知道你在运行什么，或者在需要的情况下能够回溯并知道在特定时间运行了什么。
- en: Lastly, apply good hygiene to your DevOps build environment itself. Vulnerabilities
    in critical MLOps/LLMOps components such as PyTorch have already been shown to
    be vulnerable points in the chain. Use SCA tools to ensure all the components
    of your DevOps platform are up-to-date and secure.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对你的 DevOps 构建环境本身也要保持良好的卫生。PyTorch 等关键 MLOps/LLMOps 组件中的漏洞已经被证明是链中的脆弱点。使用
    SCA 工具确保你的 DevOps 平台的所有组件都是最新和安全的。
- en: Build an AI red team
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建立一个 AI 红队
- en: The complexity and unpredictability inherent in an LLM-based application make
    security testing tricky. AST tools may help, but you shouldn’t assume they give
    you real safety. Frequent red team testing is a critical component of any responsible
    AI strategy. Use a combination of manual and human-driven red teaming and consider
    using automated red team technology.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大型语言模型的应用程序固有的复杂性和不可预测性使得安全测试变得复杂。AST 工具可能会有所帮助，但你不应假设它们提供了真正的安全性。频繁的红队测试是任何负责任的
    AI 策略的关键组成部分。结合手动和人工驱动的红队，并考虑使用自动红队技术。
- en: Warning
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Red teams are supposed to find security vulnerabilities and safety issues. But
    this won’t always make them popular. This is especially true when red teaming
    is put off until late in the development cycle, impacting committed project schedules.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 红队旨在发现安全漏洞和安全问题。但这并不总是使他们受欢迎。这尤其当红队被推迟到开发周期的后期，影响既定的项目进度时。
- en: Discovering and reporting security and safety issues can sometimes place security
    teams in a challenging position, particularly when such findings clash with tight
    project schedules or imminent deployment deadlines. It’s not uncommon for security
    professionals to face resistance or even hostility when their discoveries could
    lead to delays or increased workloads.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 发现和报告安全和安全问题有时会使安全团队处于困难的位置，尤其是当这些发现与紧张的项目时间表或即将到来的部署截止日期相冲突时。安全专业人士面临阻力或甚至敌意并不罕见，当他们的发现可能导致延误或增加工作量时。
- en: Creating a security-positive culture within an organization goes beyond implementing
    policies or conducting training. It involves a fundamental shift in how security
    is perceived—from a hindrance or afterthought to an integral aspect of the development
    process. Encouraging every team member, from developers to executives, to prioritize
    security and safety can dramatically reduce risks and enhance your project’s resilience
    against threats.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织内部建立一个积极安全的氛围，不仅仅是实施政策或进行培训。它涉及到对安全认知的根本转变——从阻碍或事后考虑转变为发展过程中的一个重要组成部分。鼓励每个团队成员，从开发者到高管，都将安全和安全放在首位，可以显著降低风险并增强项目对威胁的抵抗力。
- en: Security professionals must often persuade and negotiate with various stakeholders
    to ensure security measures are implemented and respected. Developing strong persuasive
    and negotiation skills can facilitate more effective interactions with development
    teams, who may be pressured to meet deadlines or performance targets. Security
    teams can foster a collaborative environment by presenting security testing not
    as a roadblock, but as an essential step toward creating a robust and reliable
    product. Creating win-win scenarios where security and development goals align
    can lead to more successful and secure AI implementations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 安全专业人士必须经常说服和与各种利益相关者谈判，以确保安全措施得到实施和尊重。培养强大的说服和谈判技能可以促进与开发团队的更有效互动，这些团队可能面临满足截止日期或绩效目标的压力。安全团队可以通过将安全测试视为创建稳健可靠产品的必要步骤，而不是障碍，来营造协作环境。在安全和开发目标一致的情况下创造双赢场景，可以导致更成功和安全的AI实施。
- en: Tip
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Mastering the art of win-win persuasion can be crucial. Robert Cialdini’s book
    *Influence: The Psychology of Persuasion* (Harper Business) provides insights
    into the principles of persuasion that can help security professionals effectively
    communicate the importance of robust security practices. Similarly, *Never Split
    the Difference: Negotiating As If Your Life Depended On It* by Chris Voss (Harper
    Business) offers practical negotiation techniques from a former FBI hostage negotiator,
    invaluable for navigating high-stakes discussions with stakeholders. Mastering
    these skills can make a big difference in your project’s success and your career
    over the long haul.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '掌握双赢说服的艺术可能至关重要。罗伯特·西奥迪尼的书籍《影响力：说服心理学》（Harper Business）提供了说服原则的见解，可以帮助安全专业人士有效地传达稳健安全实践的重要性。同样，克里斯·沃斯（Harper
    Business）的《永不分割：谈判如生命攸关》（Never Split the Difference: Negotiating As If Your Life
    Depended On It）提供了前FBI人质谈判专家的实用谈判技巧，这对于与利益相关者进行高风险讨论非常有价值。掌握这些技能可以在你的项目成功和职业生涯的长远发展中产生重大影响。'
- en: Monitor continuously
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持续监控
- en: Trust nothing and record everything. As an extension of our zero trust policy,
    you should carefully monitor all parts of your application. This includes collecting
    logs from traditional components such as web servers and databases. Critically,
    you should also monitor your LLM directly. Log every prompt and every response
    from your LLM and collect data from monitoring APIs provided by your model provider.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 不信任任何东西，记录一切。作为零信任政策的延伸，你应该仔细监控应用程序的所有部分。这包括从Web服务器和数据库等传统组件收集日志。关键的是，你还应该直接监控你的LLM。记录LLM的每个提示和每个响应，并收集模型提供商提供的监控API提供的数据。
- en: Collect these logs and events into a SIEM system and apply anomaly detection
    techniques. Leverage your SIEM’s UEBA functionality as a starting point. Sudden
    changes in application behavior could mean an external change, such as a DoS attack
    (see [Chapter 8](ch08.html#don_t_lose_your_wallet)), or a hacker has gained control
    over some part of your application via an LLM jailbreak or a more traditional
    side channel.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些日志和事件收集到SIEM系统中，并应用异常检测技术。利用SIEM的UEBA功能作为起点。应用程序行为的突然变化可能意味着外部变化，例如DoS攻击（见[第8章](ch08.html#don_t_lose_your_wallet)），或者黑客通过LLM越狱或更传统的侧信道控制了应用程序的某些部分。
- en: Spot-check and review prompt/response pairs regularly to understand your application
    and look for signs of trouble, such as attempted prompt injections or possible
    hallucinations. Use this data to continuously tune your system.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 定期抽查和审查提示/响应对，以了解你的应用并寻找问题迹象，如尝试提示注入或可能的幻觉。使用这些数据来持续调整你的系统。
- en: The RAISE Checklist
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAISE清单
- en: Use this handy checklist to evaluate your project and determine whether additional
    safety techniques, tools, or controls are necessary.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个实用的清单来评估你的项目，并确定是否需要额外的安全技术、工具或控制措施。
- en: Limit your domain
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制你的领域
- en: Be narrow in the design of your application. Clearly define what use cases it
    should support.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用设计中保持狭窄。明确定义它应该支持哪些用例。
- en: Select domain-specific, rather than general-purpose, foundation models to support
    your use case.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择特定领域的，而不是通用目的的基础模型来支持你的用例。
- en: Balance your knowledge base
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡你的知识库
- en: Give your model access to enough data to avoid hallucinations.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给你的模型提供足够的数据以避免幻觉。
- en: Limit additional data sources to only those required to meet your use case.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将额外的数据源限制仅用于满足你的用例。
- en: Implement zero trust
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施零信任
- en: Screen all data being passed to your LLM.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 筛查所有传递给你的LLM的数据。
- en: Screen all output from your LLM.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 筛查你LLM的所有输出。
- en: Implement guardrails.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施安全措施。
- en: Manage your supply chain
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理你的供应链
- en: Evaluate the trustworthiness of model and standard dataset providers.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型和标准数据集提供者的可信度。
- en: Use caution building datasets from public sources.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在从公共来源构建数据集时要谨慎。
- en: Account for possible bias in your training data.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑训练数据中可能的偏差。
- en: Build and maintain your ML-BOM.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立和维护你的ML-BOM。
- en: Secure your DevOps pipeline.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护你的DevOps管道。
- en: Build an AI red team
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个AI红队
- en: Use a human-led team.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以人为核心的团队。
- en: Consider augmenting with automated red teaming tools.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑增加自动化红队工具。
- en: Monitor continuously
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续监控
- en: Log all activity.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录所有活动。
- en: Collect all logs into a SIEM system.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有日志收集到SIEM系统中。
- en: Use data analysis to look for anomalies that could indicate threats.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据分析来寻找可能表明威胁的异常情况。
- en: Conclusion
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The appearance of ChatGPT and the blossoming of the overall LLM ecosystem felt
    sudden. However, it was just part of an accelerating curve of AI capabilities
    that’s been building momentum for years. At the start of this chapter, we examined
    several factors that have contributed to that, but more importantly, those factors
    are still at play and accelerating. As William Gibson said in the quote at the
    start of this chapter, “The future is already here—it’s just not evenly distributed.”
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的出现和整体LLM生态系统的繁荣似乎突然发生。然而，这只是AI能力加速曲线的一部分，这种曲线已经积累了几年的动力。在本章的开头，我们探讨了导致这一现象的几个因素，但更重要的是，这些因素仍在发挥作用并加速发展。正如威廉·吉布森在本章开头引用的那样，“未来已经到来——只是分布不均。”
- en: As the curve extends, we’ll see the power and the risk from these systems grow.
    We will undoubtedly see more capable AI systems. Remember the story of Tay in
    [Chapter 1](ch01.html#chatbots_breaking_bad)? That was 2016, and it’s now eight
    years later. We’re still seeing the same problems that plagued Tay in today’s
    LLM applications, and we’ll see people make the same mistakes in the future. Businesses
    and individuals are tempted to rush forward, provide these systems with access
    to more data, and increase their levels of autonomy and agency. If we’re not careful,
    we’re on a road that will lead to many safety and security disasters.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 随着曲线的延伸，我们将看到这些系统的力量和风险增长。我们无疑将看到更多能力的AI系统。记住[第1章](ch01.html#chatbots_breaking_bad)中Tay的故事？那是在2016年，现在已经过去了八年。我们仍在看到困扰Tay的相同问题，未来人们也会犯同样的错误。企业和个人都倾向于急于求成，为这些系统提供更多数据，并提高它们的自主性和能动性。如果我们不小心，我们可能会走上一条会导致许多安全和安全灾难的道路。
- en: I hope you’ll apply the knowledge you’ve gained throughout the book to help
    keep your LLM-based applications on a responsible path. Use the RAISE framework
    and checklist to help your teams think through the issues and ensure that you’ve
    done your utmost to build a robust and safe system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能将本书中学到的知识应用于帮助保持你的基于LLM的应用在负责任的道路上。使用RAISE框架和清单来帮助你的团队思考问题，并确保你已经尽最大努力构建了一个强大且安全的系统。
- en: The power of LLMs and emerging AI technologies is undoubtedly a game changer.
    Companies and countries that don’t adopt these technologies will fall behind rapidly.
    Be bold, experiment, and build great new applications. But remember, with great
    power comes great responsibility! You can create powerful applications safely,
    securely, and responsibly.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）和新兴的人工智能技术的力量无疑是颠覆性的。那些不采用这些技术的公司和国家将迅速落后。要大胆，要勇于实验，并构建全新的优秀应用。但记住，强大的力量伴随着巨大的责任！你可以安全、可靠、负责任地创建强大的应用。
