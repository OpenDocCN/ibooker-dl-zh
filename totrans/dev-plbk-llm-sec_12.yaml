- en: Chapter 12\. A Practical Framework for Responsible AI Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The future is already here—it’s just not evenly distributed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: William Gibson, author of *Neuromancer* and inventor of the term “cyberspace”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In 1962, the final installment of a then-obscure comic anthology series unveiled
    what would become one of the world’s most adored superheroes. *Amazing Fantasy*
    issue #15 marked the debut of Spider-Man, a character who, according to a [2022
    CNN story](https://oreil.ly/IDnD3), has ascended to become the world’s most famous
    superhero. But what propelled Spider-Man to this esteemed status? The answer lies
    in the compelling message woven into his origin story.'
  prefs: []
  type: TYPE_NORMAL
- en: In this inaugural tale, Peter Parker is a high school introvert whose life is
    forever changed after being bitten by a radioactive spider. Suddenly equipped
    with remarkable powers—superhuman strength, agility, and the ability to spin webs—Peter
    adopts the alias of Spider-Man and steps into the limelight as a costumed hero.
    However, his early indifference to the broader implications of his actions leads
    to a personal tragedy that costs the life of his beloved Uncle Ben. This pivotal
    moment brings Peter to a critical realization, encapsulated in the now-iconic
    phrase, “With great power comes great responsibility.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as Peter Parker was thrust into a world of great power and consequent
    responsibility, practitioners in the AI field are navigating an era of unprecedented
    technological acceleration. The rapid evolution of AI and LLMs, while unlocking
    the immense potential for innovation and advancement, also amplifies the responsibility
    of those who wield these technologies. Ensuring their safety and security is a
    technical challenge and a moral imperative. The narrative of Spider-Man serves
    as a poignant reminder that with the great power bestowed by these advanced technologies
    comes a critical responsibility to use them wisely, ethically, and with a keen
    awareness of their impact on society and individual lives. As we stand on the
    brink of AI’s vast potential, we must heed the lesson encapsulated in Peter Parker’s
    journey: to embrace our responsibilities and ensure that our technological advancements
    foster benefits, not detriments.'
  prefs: []
  type: TYPE_NORMAL
- en: As we embark on this chapter, our journey mirrors the ever-expanding universe
    of AI and LLM technologies—where the bounds of possibility are constantly redrawn.
    Our purpose here is twofold. Firstly, we aim to examine the trends marking the
    acceleration of these powerful technologies. The velocity at which AI and LLMs
    advance is reshaping our tools and methodologies, as well as redefining our ethical
    and security landscapes. By examining these trends, we seek to understand the
    pace of technological advancement and its broader role in responsible, secure
    AI application development.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, this chapter endeavors to arm the reader with a robust framework for
    the safe, secure, and responsible use of AI and LLM technologies. This framework,
    which I call RAISE, is intended to wrap together all the concepts you’ve learned
    earlier in the book and make them easier to apply. By offering insights into best
    practices, ethical considerations, and security measures, we aim to empower you
    to harness the power of AI and LLMs with a conscientious and informed approach.
  prefs: []
  type: TYPE_NORMAL
- en: Power
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start by looking at the trends pushing forward capabilities of LLMs. We
    have recently perceived a spike in the capabilities of AI systems, as evidenced
    by the rush of new applications and investments. But is this a onetime spike that
    is now in the past, or are we still in the early phases of an exponential curve
    that will multiply both the power of and risks associated with these systems?
  prefs: []
  type: TYPE_NORMAL
- en: I started my first AI software company in the early 1990s. It was called Emergent
    Behavior, which I still think is a super cool name for an AI software company.
    It doesn’t exist anymore, but I think telling you a bit about that experience
    will help illustrate the technology acceleration happening in AI-capable hardware.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1990s, my team built software with genetic algorithms and neural networks.
    Our software was capable of doing real-world work. We successfully sold it to
    massive investment banks building arbitrage trading strategies and to Fortune
    500 manufacturing companies optimizing their factory floor layouts. However, ultimately,
    the meager computing power and memory to which we had access meant we were severely
    constrained. We just couldn’t accomplish most of the grand tasks we had in mind.
  prefs: []
  type: TYPE_NORMAL
- en: The most powerful computer I had access to back in those days was a Macintosh
    IIfx. It included a Motorola 68030 processor with a clock speed best measured
    in megahertz. My computer had 16 megabytes of RAM. Today’s processors run in gigahertz,
    not megahertz, and the memory is in gigabytes instead of megabytes. That mega
    to giga change alone implies a ~1,000x improvement. But clock speed isn’t the
    only improvement, and Moore’s law implies clever chip designers should have been
    able to provide a doubling of overall computing power every two years. That would
    give us a 64,000-fold increase in speed over that period.
  prefs: []
  type: TYPE_NORMAL
- en: 'An improvement of 64,000 fold sounds impressive—and it is. But even that is
    not nearly enough to account for the explosion in capabilities we’ve seen in that
    period. It simply wouldn’t have given us enough computing power to train and run
    today’s LLMs. There is something else going on here. Two other converging trends
    enabled this: GPUs and Cloud Computing.'
  prefs: []
  type: TYPE_NORMAL
- en: GPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the late 1990s, the need for games to render more polygons at faster frame
    rates led to the development of special graphics processing units (GPUs) by companies
    like 3dfx, ATI Technologies, and Nvidia. These companies built GPU architectures
    to handle massive numbers of parallel math operations to compute 3D spatial relationships.
    While this was fantastic for games, it is also just the right recipe for accelerating
    neural networks, which need the exact same kind of support.
  prefs: []
  type: TYPE_NORMAL
- en: In my early 1990s AI startup, my Mac IIfx had a Motorola 68882 math coprocessor
    alongside its regular CPU. This coprocessor speeds up the types of floating-point
    math operations you’d need for gaming or AI, in addition to spreadsheets and other
    more mundane applications. The 68882 was the same coprocessor design used in machines
    from expensive, top-of-the-line workstation vendors like Sun Microsystems and
    was one of the fastest chips available at the time. It was rated at 422,000 floating-point
    operations per second (kFLOPS). That sounds like a lot, but it just wasn’t enough
    to make practical the kinds of AI tasks we wanted to accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: 'How much faster is a modern server than my old workstation? While Moore’s law
    would imply that a new server might be ~64,000 times faster than my old workstation,
    the architecture of GPUs changes the game for the operations you need for AI applications.
    Today, a top-of-the-line GPU is an NVIDIA H100, rated at 60 trillion floating-point
    operations per second (teraflops). Let’s do some math:'
  prefs: []
  type: TYPE_NORMAL
- en: $Speed Increase equals StartFraction NVIDIA upper H 100 FLOPS Over Motorola
    68882 FLOPS EndFraction$
  prefs: []
  type: TYPE_NORMAL
- en: The NVIDIA H100 GPU is approximately 142,180,095 times faster than the Motorola
    68882 math coprocessor! This staggering increase highlights the monumental strides
    made in chip computational capabilities, which underpin the current advancements
    in AI and machine learning technologies. That mind-boggling speed increase shows
    that we are on a massively accelerating hardware curve for AI-capable hardware.
    The curve over that time period is over 2,000 times steeper than even the exponential
    Moore’s law curve would have predicted!
  prefs: []
  type: TYPE_NORMAL
- en: 'One hundred forty-two million times is a shockingly significant improvement:
    what the modern GPU can compute in a single second would have taken 4.5 years
    on my old workstation’s coprocessor! But it’s still not enough computing power
    to account for the explosion we’ve seen. We need cloud computing to complete the
    picture.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, publications by Taiwan Semiconductor Manufacturing Company (TSMC),
    which fabricates many of the world’s GPUs, say the company expects to see as much
    as another one million times improvement in computational performance/watt of
    electricity over the next 10 to 15 years, with performance tripling every 2 years.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The other trend we need to account for is the cloud. Even the massive speed
    improvement on the single-system hardware curve isn’t enough to enable today’s
    sudden AI boom.
  prefs: []
  type: TYPE_NORMAL
- en: In 2006, most people knew Amazon as an online seller of books, CDs, and DVDs.
    The introduction of Amazon Web Services (AWS) surprised everyone and popularized
    the idea of on-demand, pay-as-you-go cloud computing. Cloud is so pervasive today
    that I don’t need to explain the concept to you, but I will remind you what it
    means to AI.
  prefs: []
  type: TYPE_NORMAL
- en: Today, whether you’re using AWS, Microsoft Azure, or Google Cloud Platform (GCP),
    you can access on-demand clusters of GPU-enabled servers with nearly limitless
    memory attached to ultrafast networks. You can set up massive clusters in minutes
    if you have enough money in your account. The companies that are training today’s
    foundation models see such a high potential return on investment that they are
    willing to pay massive cloud computing bills. It’s been [widely reported](https://oreil.ly/hAsfW)
    that OpenAI spent approximately $100,000,000 on cloud resources to train GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: I don’t believe we’re yet at the limits. In February 2024, Nvidia CEO Jensen
    Huang and OpenAI CEO Sam Altman were in the news. Huang said the world will quickly
    build a trillion dollars’ worth of new data centers to power AI software, and
    reports say that OpenAI’s Sam Altman is looking to raise seven trillion dollars
    to develop and build new AI chips. We’ve now entered an era where investments
    in AI hardware will be measured in trillions of dollars, ensuring we will see
    continued increases in computing power applied to these models.
  prefs: []
  type: TYPE_NORMAL
- en: Open Source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another accelerant of capabilities and risk is the rise of open source LLM technologies.
    November 30, 2022, is often celebrated for the release of ChatGPT, when OpenAI
    introduced most of the world to LLM technology. However, February 24, 2023, may
    hold even more significance in the annals of LLM technology due to Facebook/Meta’s
    release of the *Large Language Model Meta AI* (LLaMA, now usually written Llama).
  prefs: []
  type: TYPE_NORMAL
- en: Meta’s press release professed a commitment to open science, highlighting the
    release of LLaMA as a step in enabling broader access to state-of-the-art AI technologies.
    LLaMA is provided in multiple sizes to cater to various research needs, from validating
    new approaches to exploring novel use cases. By offering smaller, more efficient
    models, Meta aimed to lower the barrier to entry into the LLM space, allowing
    researchers with limited resources to contribute to and innovate within this rapidly
    evolving field.
  prefs: []
  type: TYPE_NORMAL
- en: While Meta’s initial approach to releasing LLaMA aimed to democratize access
    to cutting-edge AI technology, there was a sense of caution. The company recognized
    the transformative potential of making such powerful models more accessible, but
    was equally aware of the risks associated with their misuse. Meta opted for a
    controlled release under a noncommercial license to navigate this delicate balance,
    making LLaMA accessible only to researchers at academic institutions, government
    agencies, and nongovernmental organizations who met specific criteria. Meta intended
    to foster responsible innovation while mitigating the dangers of widespread access
    to such potent technology.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these precautions, the situation took an unexpected turn. Just a week
    after LLaMA was released to selected researchers, the model found its way onto
    the internet via a leak on 4chan (the same hacker forum that launched the attack
    on Tay we detailed in [Chapter 1](ch01.html#chatbots_breaking_bad)). The leak
    quickly spiraled out of control, with users redistributing LLaMA across various
    platforms, including GitHub and Hugging Face. Meta’s efforts to contain the spread
    through takedown requests proved futile; the model had already disseminated too
    widely and rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Faced with LLaMA’s uncontrollable proliferation, the company decided to reassess
    its stance and ignore its initial trepidation about the risk of widely distributing
    open LLM technology. In a move that marked a significant shift from its original
    restrictive licensing approach, Meta eventually released LLaMA under a more liberal
    license, making it available to anyone.
  prefs: []
  type: TYPE_NORMAL
- en: Following this episode, Meta continued to push forward. The company introduced
    LLaMA 2, a more advanced version of the original model, alongside specialized
    variants like Llama Chat and Code Llama. These subsequent releases underscore
    Meta’s commitment to advancing the field of AI, albeit with a nuanced understanding
    of the complexities involved in managing the distribution of powerful technological
    tools in an open and interconnected digital landscape. This evolution in Meta’s
    approach highlights a pivotal moment in the discourse on the democratization of
    AI technology, underscoring the tension between innovation and the imperative
    to ensure the responsible use of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Numerous other high-quality, open source LLMs have emerged in this rapidly evolving
    landscape, including BLOOM, MPT, Falcon, Vicuna, and Mixtral. Among these, Mixtral
    stands out for its innovative approach and technological advancements.
  prefs: []
  type: TYPE_NORMAL
- en: Mixtral-8x7B showcases a high-quality sparse mixture of experts (SMoE) model.
    This development represents a significant technological leap forward, offering
    open weights and licensing under the permissive Apache 2.0 license. According
    to the development team, Mixtral has demonstrated superior performance to LLaMA
    2 70B across most benchmarks, achieving up to six times faster inference times,
    and either matches or surpasses the capabilities of OpenAI’s GPT-3.5 on most standard
    benchmarks. It is now considered one of the most robust open-weight models available
    under a permissive license.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SMoE is a type of LLM architecture designed to improve efficiency and scalability.
    It allows a model to learn different parts of the input space using specialized
    “expert” subnetworks.
  prefs: []
  type: TYPE_NORMAL
- en: The shift toward open source models marks a significant step in accelerating
    technological progress. With this change, the capabilities once reserved for major
    corporations are now accessible to a wider audience, including scientists, researchers,
    and small companies. This broader access will drive innovation, as demonstrated
    by projects like Mixtral. The sharing of state-of-the-art technology like this
    means the base science of LLM technology will continue to benefit from academic
    and commercial research in the coming years, with no single organization able
    to monopolize it and slow progress.
  prefs: []
  type: TYPE_NORMAL
- en: However, the open source nature of these technologies also means they are being
    used by malicious actors, including thieves, terrorists, and countries like Russia,
    China, and North Korea. This reality undermines the effectiveness of public pressure
    and regulations aimed at a handful of organizations like OpenAI and Google in
    controlling the proliferation and misuse of LLM and AI technologies. The technology
    has become too widespread to restrict its use to only beneficial purposes. The
    genie is out of the bottle, and there’s no putting it back.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-image models such as DALL-E, Midjourney, and Stable Diffusion have quickly
    revolutionized how many people approach visual creative endeavors. In January
    2021, OpenAI’s DALL-E was the first to make waves by introducing the ability to
    generate complex images from textual descriptions. This model, a variant of the
    GPT-3 LLM, showcased the potential of combining natural language processing with
    image generation, setting a precedent for the kind of creative possibilities that
    AI could unlock.
  prefs: []
  type: TYPE_NORMAL
- en: Following DALL-E, the commercial service Midjourney began its open beta in July
    2022, offering a unique approach to image generation. Operated through a Discord
    bot, Midjourney allowed users to create images from text prompts, emphasizing
    an interactive and community-centric creation model.
  prefs: []
  type: TYPE_NORMAL
- en: The field of text-to-image took another turn with the release of the open source
    Stable Diffusion project in August 2022\. As an open source model, Stable Diffusion
    made high-quality image generation accessible to a broader audience, allowing
    anyone with consumer grade hardware to generate detailed visuals from textual
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Progress has been astonishingly rapid in this area. In just a few short years,
    we have evolved from the early images, characterized by easily identifiable flaws
    (such as creepy, inaccurately rendered fingers), to the creation of photorealistic
    images that challenge our ability to distinguish them from actual photographs.
  prefs: []
  type: TYPE_NORMAL
- en: This era of hyperrealistic AI-generated content has given rise to computer-generated
    Instagram influencers, exemplified by Aitana Lopez, who command substantial online
    followings and earn significant income, often without their fans realizing they
    are not real people. These virtual influencers, created entirely through advanced
    generative models, mark a new phase in digital culture. They highlight not only
    the capabilities of AI to produce content that resonates with human audiences,
    but also raise profound questions about authenticity, identity, and the nature
    of influence in the digital age.
  prefs: []
  type: TYPE_NORMAL
- en: When I started writing this book in 2023, accessing text-to-image models was
    challenging. It often required you to set up complex accounts (as with Midjourney)
    or have access to high-end hardware (for open source Stable Diffusion). Today,
    the mainline chatbots from OpenAI and Google are multimodal, treating text and
    images interchangeably. They can read text from uploaded images and generate new
    photorealistic images from a simple prompt—all as part of the same conversation.
    This integration with mainstream chatbots means the bar to access this technology
    has dropped to where almost anyone can use it—for good or bad!
  prefs: []
  type: TYPE_NORMAL
- en: In February 2024, OpenAI announced Sora, a text-to-video model that creates
    incredibly realistic videos from short prompts. Shortly thereafter, in April 2024,
    [Microsoft announced](https://oreil.ly/I6-pX) a new AI model called VASA that
    can create “lifelike talking faces of virtual characters with appealing visual
    affective skills (VAS), given a single static image and a speech audio clip.”
    With other open source text-to-video models being rapidly developed, we’re about
    to enter an age where the very nature of what’s real will be challenged. Recently,
    a company in Hong Kong lost $25 million when an employee was duped on a Zoom call
    by speaking to a deep fake of the company’s CFO. We’re about to enter a world
    where anyone can instantly and cheaply create a sophisticated deepfake video.
    It’s not hard to imagine that *The Matrix* is not far behind.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your LLM application is multimodal and can read text from images or video,
    you’re opening up a whole new world of vulnerabilities. Consider that prompt injection
    attacks can now be launched by including malicious text in an image fed into your
    model as a prompt. Or your training data could be poisoned if you include images
    with text that mislead your model. These are just more vectors to watch for!
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just a few months after the introduction of ChatGPT, Auto-GPT was launched in
    March 2023 by Toran Bruce Richards of the software development company Significant
    Gravitas. Built on OpenAI’s GPT-4, Auto-GPT introduced the concept of autonomy,
    allowing LLM-powered agents to act toward a goal with minimal human guidance.
    This feature enabled Auto-GPT to generate prompts to achieve a user-defined goal
    autonomously, differentiating it from ChatGPT’s requirement for continuous human
    input. The Auto-GPT framework introduces expanded short-term memory capabilities,
    allowing agents to connect to the internet and call upon third-party services.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of Auto-GPT generated massive buzz at the time, quickly gaining
    traction and generating substantial discussion for its approach to AI autonomy.
    Thousands of users rapidly adopted the tool for various projects, leveraging its
    ability to tackle more complex tasks than ChatGPT could handle alone. This included
    creating and using unsupervised agents for software development, business operations,
    financial transactions, and even health care–related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The adoption of Auto-GPT faced challenges due to its architectural design and
    the operational costs associated with its inefficient use of OpenAI’s expensive
    API resources. The buzz around Auto-GPT soon died out. However, this isn’t the
    end of the story of autonomous agents built on LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In the wake of Auto-GPT, dozens of other open source and research projects have
    taken up that mantle, and we’ll surely see fast progress in making these concepts
    more generalizable and less expensive. Beyond that, mainstream players like OpenAI
    have introduced concepts like plug-ins that allow their LLMs to interact directly
    with third-party internet resources. These goal-completion-seeking, autonomous
    agent architectures already show massive potential in many applications. With
    the desire to use AI in this fashion, we’ll undoubtedly see rapid investment and
    progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the most critical lesson from Auto-GPT was the incredibly rapid pace
    at which it was deployed in the wild with little to no oversight. We discussed
    excessive agency back in [Chapter 7](ch07.html#trust_no_one): putting unsupervised
    power in the hands of a naive AI, with few guardrails in place, could be incredibly
    dangerous—and few stopped to think about it. The development community’s overall
    lack of caution shown in the rapid adoption of the technology demonstrates with
    some certainty that we must put better security and safety measures in place before
    the next leap in self-directed autonomous systems. We can’t trust the broad human
    population to supervise these capabilities independently. The task is too complex
    to leave to individuals; we must solve it as an industry.'
  prefs: []
  type: TYPE_NORMAL
- en: Responsibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re on a curve showing a continued, dramatic increase in AI capabilities over
    the coming years. How do you plan for the future and make durable decisions today
    that will pay off and keep you, your customers, your employees, your organization,
    and society at large safe as things accelerate? How do you live up to the *great
    responsibility* of managing this *great power*?
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapters of this book have been grounding to help you understand
    the possible. What risks exist today? What real-world examples have shown the
    impact of these vulnerabilities? We’ve even looked at some far-flung, fictional,
    but plausible examples of how these threats might manifest themselves in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, I’ve offered you the best practical techniques to address
    these vulnerabilities by using state-of-the-art practice with the input of experts
    across the industry. However, with things moving quickly, your best defense is
    to have a generalized, flexible framework to build your defenses. In this book’s
    last section, I’ll give you a framework you can customize to fit your needs and
    that you can adapt as you grow and the technology moves forward.
  prefs: []
  type: TYPE_NORMAL
- en: The RAISE Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s walk through a framework I have built to help you plan, organize, and
    achieve your goals for a safe and secure project. As you can see in [Figure 12-1](#fig_1_the_raise_framework),
    I call this six-step process the Responsible Artificial Intelligence Software
    Engineering (RAISE) framework. First, we’ll review each step’s meaning and why
    it matters. Then, we’ll break it down into a manageable checklist your team can
    use to track your work along your journey.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dpls_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. The RAISE framework
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following list includes the six steps; let’s take look at each of these
    in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: Limit your domain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Balance your knowledge base.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement zero trust.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manage your supply chain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build an AI red team.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitor continuously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limit your domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constraining your application to focus on a limited functional domain is first
    on the list because it is so fundamental and solves a host of problems. ChatGPT
    is an example of an LLM application with nearly zero domain boundaries. Part of
    its appeal is that it was trained on almost the entire internet, and you can ask
    it almost anything. It doesn’t matter if you want a dessert recipe or a block
    of Python code that calculates pi to a thousand digits. ChatGPT is here to help.
    It has an unconstrained domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The challenge with unconstrained domains is that the development team must
    build broad, general-purpose defenses. Rather than designing a short list of “allowed”
    activities, you must design and maintain a comprehensive and likely ever-growing
    list of “denied” activities. Imagine the job of being on the guardrails team at
    OpenAI. You’re going to be constantly expanding this list that says:'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t engage in hate speech.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t help hackers steal things.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t help people build weapons (even if they miss their grandma—see [Chapter 4](ch04.html#prompt_injection)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And on and on and on…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s like playing Whac-A-Mole. This explains why we see reports of new security
    issues with ChatGPT every month. But you’re not building ChatGPT, so how does
    this apply to you? If you’re using a general-purpose foundation model like GPT-4,
    you start with an unconstrained domain. In recent real-world examples, a shipping
    company and a car company both put support chatbots on their websites to help
    improve customer service and reduce costs. Great idea! However, they based these
    on general-purpose foundation models without sufficiently restricting their domain.
    Users quickly jailbroke them via prompt injection (see Chapters [1](ch01.html#chatbots_breaking_bad)
    and [4](ch04.html#prompt_injection)—this isn’t much different than Tay), causing
    them to engage in activities ranging from writing songs about the company’s poor
    customer service to writing Python code that the hacker requested, and all at
    the company’s expense. (See [Chapter 8](ch08.html#don_t_lose_your_wallet) for
    a discussion of DoW attacks.)
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if your company wants to build an application for use on
    a specific use case, such as giving fashion advice, you can take advantage of
    that limited scope. It will be easier and more effective to drive laser focus
    for your LLM on the latest trends in fashion than enforcing a list of all the
    things not to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do you do this? While this list may evolve as things accelerate, here are
    some tips on driving focus to limit the domain:'
  prefs: []
  type: TYPE_NORMAL
- en: Where possible, start with a smaller, less-general-purpose foundation model.
    Whether you go the open source route or with an LLM-as-a-service provider, there
    are now thousands of specialized models. These models are usually trained on smaller,
    more focused datasets. If your model wasn’t exposed to hate speech, napalm recipes,
    or Python code while it was trained, it’s almost impossible for someone to trick
    it into straying into such territory. As a bonus, these smaller, special-purpose
    models may be dramatically cheaper to operate at scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you start with a more general-purpose model, fine-tune it with a function
    that rewards it for staying on topic. Encoding the “desire” to stay on task and
    in scope can be more powerful and elegant than trying to build restrictive guardrails
    later—although you will probably need to add those, too. Use this to drive alignmentbetween
    the model and your goals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balance your knowledge base
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You must maintain a dynamic balance regarding how much data you give to your
    LLM at runtime. Striking the right balance is one of the most important tasks
    in your system design and will be a significant factor in its safety and security.
  prefs: []
  type: TYPE_NORMAL
- en: If you give your model access to too little information, it may be prone to
    hallucinations. As discussed in [Chapter 6](ch06.html#do_language_models_dream_of_electric_sheep),
    while hallucinations can be cute, they can leave your organization open to reputational,
    legal, and safety risks. Equipping your model with an excellent store of knowledge
    on your intended domain helps ensure answers will be accurate and valuable to
    your intended users.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Limiting your domain can help you avoid hallucinations. Hallucinations happen
    when the model lacks enough precise data to make an informed prediction. When
    you carefully scope the domain to a small set of activities and limit its use
    outside of those activities, it becomes easier to ensure that you’ve provided
    adequate training or RAG data to allow the LLM to do its job with minimal risk
    of hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side of this equation, giving your LLM access to too much data
    has its own drawbacks. The overall security fragility and number of attack vectors
    against an LLM app means that anything the LLM knows is at risk of disclosure.
    If it doesn’t know a fact or have access to related data, it can’t accidentally
    give it to an attacker.
  prefs: []
  type: TYPE_NORMAL
- en: Use techniques we’ve discussed, such as RAG and model fine-tuning, to give your
    LLM the knowledge it needs to be effective. At the same time, draw a clear line
    between data it absolutely needs to have and data it shouldn’t have. Take extreme
    care with PII and confidential data. Remember, any data you give to your LLM is
    in danger of being leaked and exposed via any of the vulnerabilities we’ve discussed
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Implement zero trust
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can’t trust your users. You can’t trust data on the internet. Of course,
    all users aren’t malicious, and all data on the internet isn’t bad or tainted.
    But if you assume you can trust all potential users and all the data you might
    find on the internet, you are putting yourself at unreasonable risk.
  prefs: []
  type: TYPE_NORMAL
- en: By extension, if you assume you can’t trust your users or the data on the internet,
    then you should also assume you can’t trust your LLM. Design your architecture
    assuming that the LLM at the core of your application is an enemy sleeper agent
    or at least a confused deputy. In [Chapter 7](ch07.html#trust_no_one), we discussed
    building a zero trust architecture for your app. This means you inspect everything
    coming in and out of your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where guardrails can help. They may not be sufficient alone, but they’re
    a critical backstop for when things go wrong. Consider the following mitigation
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Screen the prompts coming into your LLM from users. Use traditional techniques
    such as scrubbing for hidden characters or funky encodings and deny lists of terms
    or phrases. Consider using a commercial or open source guardrails framework as
    discussed in [Chapter 11](ch11.html#trust_the_process).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also screen the prompts that come into your LLM from outside sources via RAG—especially
    for in-the-wild sources such as results from internet searches—using the same
    techniques you use for user prompts. Data coming into your LLM through RAG is
    even more likely to be dangerous or poisoned than data coming from some classes
    of users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screen everything that comes out of your LLM. If you can’t trust what went in—and
    you probably can’t—then you can’t trust what comes out. Watch for cases where
    the LLM may try to generate scripts, code, instructions, or even prompts to feed
    another LLM. These could all be signs that your LLM is being tricked into being
    a confused deputy and using the privileges you’ve given it to access backend sources
    for nefarious purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider rate-limiting techniques as we discussed in Chapters [4](ch04.html#prompt_injection)
    and [8](ch08.html#don_t_lose_your_wallet). They can be essential to your defense
    against prompt injections, DoS, DoW, and model cloning attacks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, and perhaps most importantly, make informed decisions about how much
    agency you give your LLM. Earlier in this chapter, we discussed the push to implement
    architectures that allow for more autonomy and goal seeking. If you design your
    application so that the LLM can drive specific actions, you expose yourself to
    the possibility it will take those actions, or related actions to which it has
    incidental permissions, at the time you least expect. You don’t want HAL turning
    off your life-support systems without a human in the loop!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage your supply chain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Software supply chain security has been one of the hottest topics in security
    for several years. In [Chapter 9](ch09.html#find_the_weakest_link) we reviewed
    large-scale supply chain failures of both proprietary components (SolarWinds)
    and open source components (Log4Shell). We then went on to look at real examples
    of these risks from sources like Hugging Face. These risks are real, and the consequences
    are severe. Some key considerations include:'
  prefs: []
  type: TYPE_NORMAL
- en: Carefully select your foundation model. Is it from a reputable source?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carefully select any third-party training datasets you may use. If possible,
    use tools to provide additional inspection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use caution when building your own training datasets from public sources. Apply
    techniques to look for intentional data poisoning or illegal materials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware of possible biases in the data you use for training. Biased data could
    lead to behavior considered to be inappropriate by some users and put your organization
    at reputational or even legal risk. For example, back in [Chapter 1](ch01.html#chatbots_breaking_bad),
    we looked at a case where an app for job candidate screening had to be shut down
    because it discriminated against women. It didn’t do this because it was mean;
    it did this because of biases inherent in its training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure to track your third-party components as part of your ML-BOM. If problems
    or vulnerabilities are discovered down the road, you can determine whether you’re
    affected and quickly remedy the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Build this process into your DevSecOps/MLOps/LLMOps development pipeline, as
    discussed in [Chapter 11](ch11.html#trust_the_process). Rigor around checking
    and scrubbing these things should be automated. Don’t depend on spot-checking
    by hand. Update your ML-BOM and store a new version with every build and deploy
    cycle. That way, you’ll always know what you’re running or be able to rewind and
    know what you were running at a particular time should conditions require that.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, apply good hygiene to your DevOps build environment itself. Vulnerabilities
    in critical MLOps/LLMOps components such as PyTorch have already been shown to
    be vulnerable points in the chain. Use SCA tools to ensure all the components
    of your DevOps platform are up-to-date and secure.
  prefs: []
  type: TYPE_NORMAL
- en: Build an AI red team
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The complexity and unpredictability inherent in an LLM-based application make
    security testing tricky. AST tools may help, but you shouldn’t assume they give
    you real safety. Frequent red team testing is a critical component of any responsible
    AI strategy. Use a combination of manual and human-driven red teaming and consider
    using automated red team technology.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Red teams are supposed to find security vulnerabilities and safety issues. But
    this won’t always make them popular. This is especially true when red teaming
    is put off until late in the development cycle, impacting committed project schedules.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering and reporting security and safety issues can sometimes place security
    teams in a challenging position, particularly when such findings clash with tight
    project schedules or imminent deployment deadlines. It’s not uncommon for security
    professionals to face resistance or even hostility when their discoveries could
    lead to delays or increased workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a security-positive culture within an organization goes beyond implementing
    policies or conducting training. It involves a fundamental shift in how security
    is perceived—from a hindrance or afterthought to an integral aspect of the development
    process. Encouraging every team member, from developers to executives, to prioritize
    security and safety can dramatically reduce risks and enhance your project’s resilience
    against threats.
  prefs: []
  type: TYPE_NORMAL
- en: Security professionals must often persuade and negotiate with various stakeholders
    to ensure security measures are implemented and respected. Developing strong persuasive
    and negotiation skills can facilitate more effective interactions with development
    teams, who may be pressured to meet deadlines or performance targets. Security
    teams can foster a collaborative environment by presenting security testing not
    as a roadblock, but as an essential step toward creating a robust and reliable
    product. Creating win-win scenarios where security and development goals align
    can lead to more successful and secure AI implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Mastering the art of win-win persuasion can be crucial. Robert Cialdini’s book
    *Influence: The Psychology of Persuasion* (Harper Business) provides insights
    into the principles of persuasion that can help security professionals effectively
    communicate the importance of robust security practices. Similarly, *Never Split
    the Difference: Negotiating As If Your Life Depended On It* by Chris Voss (Harper
    Business) offers practical negotiation techniques from a former FBI hostage negotiator,
    invaluable for navigating high-stakes discussions with stakeholders. Mastering
    these skills can make a big difference in your project’s success and your career
    over the long haul.'
  prefs: []
  type: TYPE_NORMAL
- en: Monitor continuously
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trust nothing and record everything. As an extension of our zero trust policy,
    you should carefully monitor all parts of your application. This includes collecting
    logs from traditional components such as web servers and databases. Critically,
    you should also monitor your LLM directly. Log every prompt and every response
    from your LLM and collect data from monitoring APIs provided by your model provider.
  prefs: []
  type: TYPE_NORMAL
- en: Collect these logs and events into a SIEM system and apply anomaly detection
    techniques. Leverage your SIEM’s UEBA functionality as a starting point. Sudden
    changes in application behavior could mean an external change, such as a DoS attack
    (see [Chapter 8](ch08.html#don_t_lose_your_wallet)), or a hacker has gained control
    over some part of your application via an LLM jailbreak or a more traditional
    side channel.
  prefs: []
  type: TYPE_NORMAL
- en: Spot-check and review prompt/response pairs regularly to understand your application
    and look for signs of trouble, such as attempted prompt injections or possible
    hallucinations. Use this data to continuously tune your system.
  prefs: []
  type: TYPE_NORMAL
- en: The RAISE Checklist
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use this handy checklist to evaluate your project and determine whether additional
    safety techniques, tools, or controls are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Limit your domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be narrow in the design of your application. Clearly define what use cases it
    should support.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Select domain-specific, rather than general-purpose, foundation models to support
    your use case.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Balance your knowledge base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give your model access to enough data to avoid hallucinations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Limit additional data sources to only those required to meet your use case.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement zero trust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Screen all data being passed to your LLM.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Screen all output from your LLM.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement guardrails.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage your supply chain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the trustworthiness of model and standard dataset providers.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use caution building datasets from public sources.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Account for possible bias in your training data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and maintain your ML-BOM.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure your DevOps pipeline.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an AI red team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a human-led team.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider augmenting with automated red teaming tools.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor continuously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log all activity.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect all logs into a SIEM system.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use data analysis to look for anomalies that could indicate threats.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The appearance of ChatGPT and the blossoming of the overall LLM ecosystem felt
    sudden. However, it was just part of an accelerating curve of AI capabilities
    that’s been building momentum for years. At the start of this chapter, we examined
    several factors that have contributed to that, but more importantly, those factors
    are still at play and accelerating. As William Gibson said in the quote at the
    start of this chapter, “The future is already here—it’s just not evenly distributed.”
  prefs: []
  type: TYPE_NORMAL
- en: As the curve extends, we’ll see the power and the risk from these systems grow.
    We will undoubtedly see more capable AI systems. Remember the story of Tay in
    [Chapter 1](ch01.html#chatbots_breaking_bad)? That was 2016, and it’s now eight
    years later. We’re still seeing the same problems that plagued Tay in today’s
    LLM applications, and we’ll see people make the same mistakes in the future. Businesses
    and individuals are tempted to rush forward, provide these systems with access
    to more data, and increase their levels of autonomy and agency. If we’re not careful,
    we’re on a road that will lead to many safety and security disasters.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you’ll apply the knowledge you’ve gained throughout the book to help
    keep your LLM-based applications on a responsible path. Use the RAISE framework
    and checklist to help your teams think through the issues and ensure that you’ve
    done your utmost to build a robust and safe system.
  prefs: []
  type: TYPE_NORMAL
- en: The power of LLMs and emerging AI technologies is undoubtedly a game changer.
    Companies and countries that don’t adopt these technologies will fall behind rapidly.
    Be bold, experiment, and build great new applications. But remember, with great
    power comes great responsibility! You can create powerful applications safely,
    securely, and responsibly.
  prefs: []
  type: TYPE_NORMAL
