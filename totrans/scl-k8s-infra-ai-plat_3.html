<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Model Deployment and Monitoring"><div class="chapter" id="ch04_model_deployment_and_monitoring_1738498450837987">
      <h1><span class="label">Chapter 4. </span>Model Deployment <span class="keep-together">and Monitoring</span></h1>
      <p>In the previous chapters, you learned about model customization techniques, including fine-tuning and training, and about making training and evaluation repeatable. Once you’ve achieved the results you are looking for with your model, it’s time to deploy your model to production.</p>
      <p>This chapter will prepare you for model deployment and serving by giving you an overview of the major technologies and techniques used with Kubernetes to deploy and monitor machine learning models. While we will focus on specific techniques relevant to large language models (LLMs) and generative AI, much of this chapter will also apply to traditional machine learning models.</p>
      <section data-type="sect1" data-pdf-bookmark="Overview of LLM Serving"><div class="sect1" id="ch04_overview_of_llm_serving_1738498450838137">
        <h1>Overview of LLM Serving</h1>
        <p><em>Model serving</em> is the act of processing inference requests in real time, which requires deploying an already trained model to some location suitable for receiving these requests. At a high level, model serving involves packaging the model, deploying it on hardware accelerators like GPUs or CPUS, exposing APIs for users to query the model, and enabling metrics for monitoring and alerting. The components of a model-serving system include model-serving platforms,<sub> </sub>model-serving runtimes, and metric gathering and monitoring systems. Typically, an API gateway and load balancer to handle bursts of traffic for model queries is also included.</p>
        <p>The model-serving platform component retrieves the model from storage (such as Amazon S3 or a local persistent volume) and then performs various preprocessing tasks such as changing model formats or postprocessing steps such as gathering metrics. It incorporates a model-serving runtime in order to help it serve the model. It also exposes a REST or gRPC API so that the models can be interacted with by users while providing model access security through the gateway and load balancer.</p>
        <p>The model-serving runtime component loads the model into the GPU or CPU memory, deserializes any incoming query or prompt from its over-the-wire representation, converts it into a format suitable for the model, and then executes the inference on the model to retrieve a response. This response is typically serialized into a JSON object or other format and sent back to the calling application.</p>
        <p>The metrics and monitoring components typically aggregate the request metrics such as the request time, error codes, token count, tracing, and more, storing them to a metrics server like <a href="https://prometheus.io">Prometheus</a>. These metrics allow an MLOps practitioner to ensure the health and performance of the models in production and diagnose any issues that may come up via alerting.</p>
        <p>Although the majority of development and compute time is spent on training and tuning a model, <a href="https://oreil.ly/7rEbn">nearly 90% of a model’s lifecycle is spent serving</a>, which is why optimizing serving can be pivotal to delivering business value from the model. In the next sections, you will learn about each of these essential components using Kubernetes-specific tools that will help you scale up your generative AI inference workloads.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Using a Model-Serving Platform"><div class="sect1" id="ch04_using_a_model_serving_platform_1738498450838225">
        <h1>Using a Model-Serving Platform</h1>
        <p>A model-serving platform is the core component of any inference system, managing model deployment and scaling according to the volume of incoming inference requests. There are currently many platforms available for serving models on Kubernetes. Their purpose is to simplify and scale the model deployment and inference serving processes.</p>
       <p class="pagebreak-before">In order to serve an LLM in a scalable fashion on Kubernetes, a serving platform should meet these requirements:</p>
        <ul>
          <li>
            <p>Support for different types of model architecture</p>
          </li>
          <li>
            <p>Extensible by adding new model architectures</p>
          </li>
          <li>
            <p>Support for generating embeddings for different modalities, such as text or image</p>
          </li>
          <li>
            <p>Support for multiple modalities in inference</p>
          </li>
          <li>
            <p>Support for chaining inference across multiple models (<em>model composition</em>)</p>
          </li>
          <li>
            <p>A wide range of hardware accelerator support</p>
          </li>
          <li>
            <p>Integration with standard Kubernetes APIs and tools</p>
          </li>
          <li>
            <p>Robust support for different model artifact formats</p>
          </li>
          <li>
            <p>Support for a wide range of storage technologies</p>
          </li>
          <li>
            <p>Integration with API gateways</p>
          </li>
          <li>
            <p>Ability to deploy models in an A/B or canary rollout fashion</p>
          </li>
          <li>
            <p>Provide flexible integration options with pre- and postprocessing systems</p>
          </li>
          <li>
            <p>Support for automatically scaling inference infrastructure</p>
          </li>
          <li>
            <p>Integration of model monitoring solutions</p>
          </li>
        </ul>
        <p>One of the most popular tools for deploying LLMs with Kubernetes is <a href="https://oreil.ly/volxt">KServe</a>. KServe is a controller for Kubernetes that enables Kubernetes to serve both predictive and generative AI models along with maintaining inference request pre- and postprocessing pipelines. Not only does it meet the previously mentioned requirements, but KServe also provides several benefits that have helped its widespread adoption in the enterprise:</p>
        <ul>
          <li>
            <p>An active and thriving open source community</p>
          </li>
          <li>
            <p>Support for traffic routing and autoscaling, including scaling to zero</p>
          </li>
          <li>
            <p>Support for both batch and real-time inference workloads</p>
          </li>
          <li>
            <p>Support for both predictive and generative AI inference with a standard protocol, the Open Inference Protocol</p>
          </li>
        </ul>
        <p>Like other controllers, KServe is composed of a set of custom resources, which are extensions to the base Kubernetes API. One of these is the ServingRuntime. This is essentially a deployment template that defines the environment from which models will be served. KServe comes with a number of ServingRuntimes available out of the box, but others can be easily added to the system. Each one defines things like the container image to be used for the runtime and the model formats that the ServingRuntime supports, and can be further customized via environment variables set in the container. This allows users to easily add support for new model architectures.</p>
        <p>At the core of KServe is the InferenceService. This is a custom resource definition where you define predictors, storage locations, model format, canaries for gradual deployment, deployment mode, and anything else required to serve your model. Models are typically initialized from cloud storage like Amazon S3 buckets, but it is also possible to use OCI-compliant containers as an alternative to cloud storage with KServe <a href="https://oreil.ly/gDOnq">“Modelcars”</a>.</p>
        <p>To use this, an OCI-compliant container image must be created and then added to a container registry like Quay. When you deploy with KServe, you can then reference the repository holding the container. Because a Kubernetes cluster keeps a cache of downloaded container images, the model doesn’t need to be downloaded multiple times, which can reduce startup time while still reducing overall disk usage.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>KServe offers three deployment modes: <code>RawDeployment</code> mode uses a standard Kubernetes deployment and ingress gateway (an API gateway for routing inbound requests only); serverless mode uses <a href="https://knative.dev">Knative</a> objects to enable serverless deployment; ModelMesh allows for multiple models to be deployed in a pod to scale smaller models with fewer compute resources.</p>
        </div>
        <p>The serving runtime is used within an InferenceService, and the InferenceService is managed by the KServe Controller (<a data-type="xref" href="#ch04_figure_1_1738498450831008">Figure 4-1</a>). This ensures that the deployed application state matches the definition of the InferenceService, creating the deployment for each inference endpoint and enabling features like autoscaling.</p>
        
<p class="pagebreak-before">Each endpoint is composed of three components:</p>
        <dl>
          <dt>Predictor</dt>
          <dd>
            <p>This is the only required component of an endpoint. It consists of a model and model server that makes the model available at the endpoint.</p>
          </dd>
          <dt>Transformer</dt>
          <dd>
            <p>This component allows users to define both pre- and postprocessing steps as needed to manage incoming request data and outgoing inference data.</p>
          </dd>
          <dt>Explainer</dt>
          <dd>
            <p>This enables an alternate workflow that provides both predictions and model explanations. KServe provides APIs so that users can write and configure their own explanation containers.</p>
          </dd>
        </dl>
        <p>When a user calls a KServe endpoint with <code>:predict</code> or <code>:explain</code>, that request is routed to the three components. For either call, the transformer component is the first stop for the request. If <code>:predict</code> was called by the user, the request is then routed to the predictor. If <code>:explain</code> was called by the user, then the request is routed from the transformer to the explainer component, and then the explainer calls <code>:predict</code> on the predictor component (<a data-type="xref" href="#ch04_figure_1_1738498450831008">Figure 4-1</a>).</p>
        <figure><div id="ch04_figure_1_1738498450831008" class="figure">
          <img src="assets/skia_0401.png" width="1341" height="486"/>
          <h6><span class="label">Figure 4-1. </span>The request flow for a user calling a KServe endpoint with the <code>:predict</code> or <code>:explain</code> calls</h6>
        </div></figure>
        <p>While KServe has many features that make it a well-functioning platform for deploying machine learning models out of the box, LLMs often require some additional work. </p>
      </div></section>
      <section class="pagebreak-before" data-type="sect1" data-pdf-bookmark="Diving Into LLM-Serving Runtimes with vLLM"><div class="sect1" id="ch04_diving_into_llm_serving_runtimes_with_vllm_1738498450838311">
        <h1 class="less_space">Diving Into LLM-Serving Runtimes with vLLM</h1>
        <p>Because KServe allows you to define your own ServingRuntime resources, it is possible to use alternative model-serving runtimes with it. One such runtime is <a href="https://docs.vllm.ai">vLLM</a>, a serving system tailored for LLMs that aims to enhance inference efficiency and scalability. It addresses the challenges of deploying LLMs by optimizing memory usage and execution speed, making it suitable for real-time applications that require high throughput and low latency.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>KServe also includes an out-of-the-box LLM runtime, the <a href="https://oreil.ly/K3Bsx">Hugging Face LLM Serving Runtime</a>, which uses vLLM as its default <span class="keep-together">backend</span>.</p>
        </div>
        <p>vLLM provides a server built on FastAPI for online model serving that is compatible with the OpenAI API and also with popular machine learning frameworks like PyTorch, allowing for seamless integration with existing machine learning pipelines and facilitating the deployment of models trained with these <span class="keep-together">frameworks</span>.</p>
        <p>vLLM also supports dynamic batching, which groups multiple inference requests into a single batch to improve processing efficiency. This is particularly beneficial in high-traffic scenarios, where it can significantly increase throughput.</p>
        <p>The keys to vLLM’s serving speed are a few core architectural <span class="keep-together">features</span>:</p>
        <dl>
          <dt>Paged attention</dt>
          <dd>
            <p>This is an algorithm that allows the storage of large continuous key-value pairs in noncontiguous blocks of memory, optimizing memory use.</p>
          </dd>
          <dt>Parallel execution</dt>
          <dd>
            <p>The architecture supports the parallel execution of model components by leveraging model parallelism and tensor slicing. This allows different parts of the model to be processed simultaneously across multiple hardware units, optimizing resource utilization and speeding up inference.</p>
          </dd>
          <dt class="pagebreak-before">Tensor caching</dt>
          <dd>
            <p>vLLM has an in-memory tensor store, which caches frequently accessed tensors to avoid repeated computations. This significantly reduces the time needed for inference by providing fast access to necessary data.</p>
          </dd>
        </dl>
        <p>While many model-serving runtimes can be used with KServe, vLLM is a strong contender for serving LLMs due to these features. Once your model is deployed, it’s imperative to be able to understand the model’s performance over time and to keep track of model families and versions that you have available to your system. In the next section, you will learn about how to monitor LLMs, what metrics to monitor, and how to use registries to keep track of your deployed models.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Monitoring and Keeping Track of Your Models"><div class="sect1" id="ch04_monitoring_and_keeping_track_of_your_models_1738498450838404">
        <h1>Monitoring and Keeping Track of Your Models</h1>
        <p>Whether you have one model or hundreds in production, it is essential to monitor their performance in real time and to keep track of which models you have in production. To monitor performance, you’ll have to know which metrics you should track. This is partially dependent on your use case and infrastructure, but there are also general metrics that you can track. Once you’ve built an understanding of metrics to track for deployed LLMs, we will move on to monitoring these metrics in KServe and then tracking your models in a registry.</p>
        <section data-type="sect2" data-pdf-bookmark="LLM Metrics"><div class="sect2" id="ch04_llm_metrics_1738498450838481">
          <h2>LLM Metrics</h2>
          <p>LLM evaluation is a rapidly advancing field. After all, how do you tell if an LLM is doing what you want? Are you worried about <em>hallucinations</em>, or factually incorrect output? Or is machine creativity important for your use case? Or maybe you don’t care as much about content but want to make sure that your model is serving inference requests at an acceptable rate and that requests are not getting stuck in queues. </p>
          <p>For task-based metrics, such as those measuring summarization or translation, there are countless metrics available to use. You should use caution, however, since these tasks are open-ended, and it is unlikely that there is a single metric that will give you an accurate view of your model’s performance. You will have to evaluate the metrics for your use case and pick a combination that accurately conveys your model’s performance on its specific task. A wide variety of summarization task-specific metrics can be found in the article <a href="https://oreil.ly/qX9dx">“LLM Evaluation for Text Summarization”</a>, published by Neptune.</p>
          <p>Model-serving runtimes typically come with their own metrics that measure how well the server is handling requests at every stage of processing. vLLM, for instance, comes with a <a href="https://oreil.ly/J57gs">large metrics class</a> that holds many different useful metrics for LLMs. Some especially important ones are <code>gauge_gpu_cache_usage</code> and <code>gauge_cpu_cache_usage</code>, which show how much of the key-value cache mentioned earlier in this chapter is being utilized, <code>num_requests_waiting</code>, which shows how many requests are waiting to be processed, and <code>num_requests_running</code>, which shows how many requests are being processed. All of these metrics are exposed by the <code>/metrics</code> endpoint.</p>
          <p>vLLM’s metrics, as well as those exposed by KServe, can be integrated into KServe and visualized in <a href="https://prometheus.io">Prometheus</a>. In KServe, all model-serving runtimes are able to export metrics in a Prometheus-compatible format.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Prediction Logging"><div class="sect2" id="ch04_prediction_logging_1738498450838554">
          <h2>Prediction Logging</h2>
          <p>Prediction logging is important in traditional machine learning, and it remains important with LLMs. Sometimes this is also called <em>generation logging</em> since the result of a generation is logged. When combined with the input prompt, input and output tokens used, and other generation metrics, prediction logs become a powerful tool for auditing model usage and accuracy.</p>
          <p>These can be stored in any existing log storage solution, and all-in-one solutions like <a href="https://oreil.ly/BRA7H">MLflow</a> integrate prediction logging and viewing into their platform.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Production Model Registry"><div class="sect2" id="ch04_production_model_registry_1738498450838626">
          <h2>Production Model Registry</h2>
          <p>In <a data-type="xref" href="ch03.html#ch03_making_training_repeatable_1738498450655759">Chapter 3</a>, we covered the overall importance of keeping a single, centralized model registry for the entire AI lifecycle, as well as why the preceding stages of the lifecycle need a registry. What we haven’t covered yet, though, is what a model registry brings to production.</p>
          <p class="pagebreak-before">When getting ready to deploy a model, it’s important to know which version of the model is ready for production and which version, if any, is currently deployed. This is information that you would store in the model registry, along with each model’s metadata, such as evaluation results and hyperparameters, which can help with the decision to deploy or with configuring the serving environment.</p>
          <p>Once deployed, the model registry can still provide important benefits, mostly around monitoring and observability. A registry can help users easily find model artifacts to track performance metrics for specific deployed model versions, understand traffic patterns to those versions, get training and other details to diagnose issues found in production, and more.</p>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>Like model registries, some groups are beginning to experiment with <em>prompt registries</em>. While these aren’t currently widely available, this will be an area of innovation to watch out for, as prompt registries could provide many of these same benefits to prompts <span class="keep-together">themselves</span>.</p>
          </div>
          <p>Not only does a deployed model need metric monitoring, but it also needs safeguards and compliance built in to prevent abuse. This has become especially urgent with LLMs, due to how well they produce convincing natural language and due to the open interface with users they have compared to traditional APIs, creating a vast attack surface. This poses additional challenges for safely and responsibly serving LLMs.</p>
        </div></section>
      </div></section>
    </div></section></div>
</div>
</body></html>