- en: 1 Improving LLM accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Large language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of large language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shortcomings of continuously finetuning a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-augmented generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining structured and unstructured data to support all types of questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large language models (LLMs) have shown impressive abilities across a variety
    of domains, but they have significant limitations that affect their utility, particularly
    when tasked with generating accurate and up-to-date information. One widely adopted
    approach to addressing these limitations is retrieval-augmented generation (RAG),
    a workflow that combines an LLM with an external knowledge base to deliver accurate
    and current responses. By pulling data from trusted sources at run time, RAG can
    significantly reduce, though not completely eliminate, hallucinations, one of
    the most persistent challenges with LLMs. In addition, RAG allows systems to seamlessly
    bridge general knowledge with niche, domain-specific information that may not
    be well represented in the pretraining data of the model. Despite these advantages,
    RAG implementations have often focused solely on unstructured data, overlooking
    the potential of structured sources like knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs are structured representations of entities, their attributes,
    and their relationships, offering a semantic framework that bridges structured
    and unstructured data. For instance, a customer support transcript is unstructured
    text, while a product catalog or user database is structured. Bridging them means
    enabling a system to connect conversational mentions of “my recent laptop order”
    to the structured record of the exact model, purchase date, and warranty status.
    Knowledge graphs serve as a critical component to RAG by enabling accurate, context-rich,
    and interconnected information retrieval—such as linking a customer query about
    a drug interaction to structured medical guidelines, prior case studies, and the
    patient’s history in real time. Integrating knowledge graphs into RAG pipelines
    can overcome LLM limitations, enhance data retrieval, and facilitate a holistic
    approach to managing and using diverse data types across domains like healthcare,
    finance, and technical support.
  prefs: []
  type: TYPE_NORMAL
- en: This book is for developers, researchers, and data practitioners who want to
    build more robust, explainable, and capable RAG systems. You’ll learn both how
    to augment existing RAG architectures with knowledge graphs and how to build new
    GraphRAG pipelines from scratch. Along the way, you’ll gain practical skills in
    data modeling, graph construction, retrieval workflows, and system evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this book, you’ll have a clear understanding of how LLMs, RAG,
    and knowledge graphs intersect to create robust systems capable of addressing
    complex queries and delivering accurate, reliable, and explainable results.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Introduction to LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By now, you’ve likely encountered or heard about ChatGPT, one of the most prominent
    examples of conversational AI. ChatGPT is a conversational user interface developed
    by OpenAI and powered by LLMs, such as GPT-4 (OpenAI et al., 2024). LLMs are built
    on transformer architecture (Vaswani et al., 2017), which enables them to process
    and generate text efficiently. These models are trained on vast amounts of textual
    data, allowing them to learn patterns, grammar, context, and even some degree
    of reasoning. The training process involves feeding the model large datasets that
    include a diverse range of text with the primary objective of enabling the model
    to accurately predict the next word in a sequence. This extensive exposure enables
    the models to understand and generate human-like text based on the patterns they
    have learned from the data. For example, if you use “Never gonna” as input to
    an LLM, you might get a response similar to that shown in figure 1.1\.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 shows an LLM processing the input “Never gonna” and generating the
    output “give you up.” This highlights how an LLM relies on patterns and associations
    it learned during training, such as those derived from common cultural references,
    including popular music. The quality and relevance of these responses depend significantly
    on the diversity and depth of the training dataset, which determines the LLM’s
    ability to recognize and replicate such patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 LLMs are trained to predict the next word.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While LLMs excel at generating contextually appropriate text, they are far more
    than just autocomplete systems. Their remarkable ability to follow instructions
    and adapt to a wide range of tasks is impressive. For example, as shown in figure
    1.2, you can ask ChatGPT to generate a haiku about a specific topic in a particular
    style. This capability illustrates not just pattern recognition but an understanding
    of task-specific instructions, enabling creative and nuanced outputs well beyond
    simple text prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 Writing a haiku with ChatGPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The ability of LLMs to follow instructions and generate diverse, complex outputs,
    whether crafting a haiku or providing structured responses, goes beyond simply
    predicting the next word in a sequence. This ability to understand and execute
    detailed instructions makes LLMs uniquely suited for a wide variety of tasks.
    In this book, you will use this instruction-following ability to design and refine
    RAG pipelines. By tapping into instruction-following capabilities, you can integrate
    retrieval components more effectively, tailor responses to specific contexts,
    and optimize your systems for accuracy and usability.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT’s breadth of general knowledge is equally remarkable. For example, figure
    1.3 illustrates ChatGPT’s response when prompted about the first manned moon landing.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 Retrieving factual information from ChatGPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you verify this response with external information from NASA or Wikipedia,
    you can observe that the model produces an accurate response with no false information.
    Such a response might give you the impression that an LLM constructs a vast database
    of facts from which it can retrieve when prompted. However, the model doesn’t
    store specific facts, events, or information from its training dataset. Instead,
    it develops complex mathematical representations of the language it is trained
    on. Remember, the LLMs are based on the transformer, which is a deep learning
    architecture based on neural networks to predict the next word, as shown in figure
    1.4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.4 illustrates a neural network predicting the next word in a sequence,
    similar to how LLMs function. The central part shows the network with multiple
    layers of neurons, connected by lines that represent the flow of information.
    Each connection has a weight, such as the example value 0.04, which influences
    the strength of the connection. During training, the model learns the values of
    these weights to improve its predictions. When asked about a specific historical
    event, an LLM doesn’t recall the event from its training data. Instead, it generates
    a response based on the learned weights in its neural network, similar to predicting
    the next word in a sequence. Therefore, while LLMs can provide seemingly knowledgeable
    answers, their responses are based on these learned weights rather than explicit
    memory. To quote Andrej Karpathy: “We kind of understand that they (LLMs) build
    and maintain some kind of a knowledge database, but even this knowledge base is
    very strange and imperfect and weird”([https://www.youtube.com/watch?v=zjkBMFhNj_gat12:40](https://www.youtube.com/watch?v=zjkBMFhNj_gat12:40)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 Neural network trained to predict the next word based on the input
    sequence of words
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.2 Limitations of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs represent a groundbreaking step in the evolution of AI, offering remarkable
    capabilities across a range of applications. Yet, as with any transformative technology,
    they are not without their challenges and constraints. In the following section,
    we will delve into some of these limitations and their implications.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Knowledge cutoff problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most obvious limitation is that LLMs are unaware of events or information
    not included in their training dataset. At this moment, ChatGPT is aware of information
    that occurred up to October 2023\. For example, if you asked ChatGPT about an
    event in 2024, you would get a response similar to that shown in figure 1.5\.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, the *knowledge cutoff date* refers to the most recent
    point at which the model’s training data includes information. The model has access
    to a broad spectrum of text data containing information about events up to this
    date from diverse sources, which it utilizes to generate responses and provide
    information. Anything that has occurred or been published after this cutoff date
    is unknown to the model as it was not included in the training dataset; therefore,
    it cannot provide information about events, developments, or research that occurred
    after the cutoff date.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Example of a knowledge cutoff date disclaimer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.2.2 Outdated information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A less obvious limitation is that LLMs can sometimes provide outdated responses.
    While they can deliver detailed and accurate information up until their knowledge
    cutoff, they may not reflect recent developments. For instance, as of late 2023,
    Mark Cuban sold his majority stake in the Dallas Mavericks franchise to the Adelson
    family and the Dumonts while retaining a minority share. This major update highlights
    how information that was correct in the past can become outdated. For example,
    in a query about the Dallas Mavericks, a response shown in figure 1.6 reflects
    Cuban as the sole owner, which is no longer accurate (Rader, 2023).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 Sometimes ChatGPT responds with outdated information.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This highlights the importance of regularly updating training data for models
    or enabling them to access real-time information. With continuously evolving events
    and facts, even small details like ownership structures can significantly impact
    how we perceive an organization or individual. This limitation underlines the
    importance of ensuring AI systems remain accurate and relevant in dynamic environments.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Pure hallucinations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another well-known limitation of LLMs is their tendency to provide assertive,
    confident answers—even when those answers contain incorrect or fabricated information.
    One might assume that, despite their knowledge cutoff dates, these models provide
    accurate factual data up to that point. However, even information about events
    that occurred before the cutoff can be unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: A striking example of this occurred when lawyers in the United States submitted
    bogus, fictitious legal citations to a court, unaware that they had been generated
    by ChatGPT (Neumeister, 2023). These kinds of confident inaccuracies are commonly
    known as hallucinations, where the model outputs information that sounds plausible
    but is factually incorrect or entirely fabricated. External references such as
    URLs, academic citations, or identifiers like WikiData IDs are especially prone
    to this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations occur because LLMs are not reasoning engines. They are probabilistic
    language models trained to predict what sounds like a good next token, based on
    patterns in their training data. They don’t know facts the way humans do. Rather,
    they generate text by guessing the most likely continuation, regardless of whether
    it’s true. This fundamental difference between statistical pattern matching and
    actual understanding is what separates LLMs from human cognition.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, we can ask ChatGPT to provide the WikiData ID of the Dallas
    Mavericks NBA franchise. As shown in figure 1.7, the model confidently returns
    an identifier—but it’s incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 ChatGPT can produce responses with incorrect information.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The model assertively replied with an ID that follows the WikiData format. However,
    if you verify this information, you can observe that Q152232 is the WikiData ID
    of the movie titled *Womanlight* ([https://www.wikidata.org/wiki/Q152232](https://www.wikidata.org/wiki/Q152232)).
    Therefore, users must recognize that LLMs, while often informative, are not infallible
    and can produce erroneous information. It’s crucial to approach their responses
    critically and verify their accuracy through reliable external sources, especially
    in contexts where precision and factual correctness are central.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 Lack of private information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you were building a company chatbot using an LLM, you’d likely want it to
    answer questions involving internal or proprietary information that isn’t publicly
    available. In such cases, even if the information or events occurred before the
    LLM’s knowledge cutoff date, they wouldn’t have been part of its training data.
    As a result, the model cannot generate accurate responses for such queries, as
    illustrated in figure 1.8\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 ChatGPT didn’t have access to some private or confidential information
    during training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One potential solution would be to make the company’s internal information publicly
    available in the hope that it gets included in the training dataset of an LLM.
    However, this approach is neither practical nor secure. Instead, we will explore
    and demonstrate more effective strategies to overcome these limitations while
    maintaining data privacy and control.
  prefs: []
  type: TYPE_NORMAL
- en: Note on other limitations of LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While this book will focus on the limitations of LLMs in providing factually
    correct and up-to-date information in responses, it’s important to acknowledge
    that LLMs also have other restrictions. Some of these include
  prefs: []
  type: TYPE_NORMAL
- en: '*Bias in responses*—LLMs can sometimes generate biased responses, reflecting
    biases present in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lack of understanding and context*—LLMs, despite their complexity, do not
    truly understand the text. They process language based on patterns learned from
    data, which means they can miss nuances and contextual subtleties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vulnerability to prompt injection*—LLMs are susceptible to prompt injection
    attacks, where malicious users craft inputs to manipulate the model into generating
    inappropriate, biased, or harmful responses. This vulnerability poses significant
    challenges for ensuring the security and integrity of LLM applications in real-world
    scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Inconsistent responses*—LLMs can produce different answers to the same question
    across multiple interactions. This inconsistency arises from their probabilistic
    nature and lack of persistent memory, which can hinder their usefulness in applications
    that require stability and repeatability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book is dedicated to exploring and addressing the specific limitations
    of LLMs concerning the generation of factually accurate and up-to-date responses.
    Although we recognize other limitations of LLMs, our discussion will not cover
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Overcoming the limitations of LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are powerful tools, but they often face limitations when handling domain-specific
    questions or accessing specialized, up-to-date knowledge. Implementing a ChatGPT-like
    application in a business environment requires outputs that are both precise and
    factually accurate. To overcome these challenges, we can inject domain-specific
    knowledge into LLMs using approaches like supervised finetuning and RAG. In this
    section, we’ll explore how these methods work and how they can be applied to inject
    domain-specific knowledge into LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Supervised finetuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At first, many of us thought we would overcome the limitations of LLMs with
    additional training. For example, we could overcome the knowledge cutoff date
    limitation by continuously updating the model. However, to address this limitation
    effectively, we first need to better understand the training of an LLM. The training
    of an LLM like ChatGPT can be split into the following four stages, as described
    by Andrew Karpathy ([https://www.youtube.com/watch?v=bZQun8Y4L2A](https://www.youtube.com/watch?v=bZQun8Y4L2A)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pretraining* —The model reads a vast amount of text, often more than a trillion
    tokens, to learn basic language patterns. It practices predicting what word comes
    next in a sentence. This is the foundational step, like learning vocabulary and
    grammar before you can write. This is the most resource-intensive phase, which
    can require thousands of GPUs and can take months of continuous training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Supervised finetuning* —The model is given specific examples of high-quality
    conversations to improve its ability to respond like a helpful assistant. It continues
    to practice language but now with a focus on generating useful and accurate responses.
    Think of it as moving from basic language learning to practicing conversation
    skills. This requires significantly fewer resources than pretraining and can nowadays
    even run on a single laptop for smaller LLMs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Reward modeling* —The model learns to distinguish between good and bad responses
    by comparing different answers to the same questions. It’s like having a coach
    who shows the model what a good performance looks like so it can aim to replicate
    that quality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Reinforcement learning* —The model interacts with users or simulated environments
    to further refine its responses based on feedback. It’s similar to learning a
    sport: practicing not just by drills but by playing actual games and learning
    from the experience.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the pretraining phase is costly and time consuming and, therefore, not
    feasible for continuous updating, the idea was to use the supervised finetuning
    phase to overcome the limitations of LLMs. During the supervised finetuning phase,
    you supply the language model with specific examples of input prompts along with
    the corresponding desired outputs you aim for the model to produce. One such example
    is shown in figure 1.9\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 Sample record of a supervised finetuning dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 1.9 shows an example of a question–answer pair that could be used to
    finetune an LLM. In this example, the input prompt or the question is about which
    team won the 2023 NBA championship, and the corresponding answer is the Denver
    Nuggets. The theory was that, through this example, the LLM would include this
    fact in its mathematical representation of the language and be able to answer
    questions revolving around the 2023 NBA champions. Some research studies have
    shown that supervised finetuning can improve LLM factuality (Tian et al., 2023).
    However, other studies using different methods also show that LLMs struggle to
    learn new factual information through finetuning (Ovadia et al., 2023).
  prefs: []
  type: TYPE_NORMAL
- en: While supervised finetuning can enhance the overall knowledge of a model, it
    remains a complex and evolving field of research. As such, deploying a reliable,
    finetuned language model in a production environment poses significant challenges
    at the current stage of technological development. Fortunately, a more efficient
    and simpler method to address the knowledge limitations of LLMs exists.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Retrieval-augmented generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second strategy for improving LLM accuracy and overcoming its limitations
    is the RAG workflow, which combines an LLM with an external knowledge base to
    deliver accurate and up-to-date responses. Instead of depending on an LLM’s internal
    knowledge, relevant facts or information are provided directly in the input prompt
    (Lewis et al., 2020). This concept (RAG) uses the LLM’s strengths in understanding
    and generating natural language, while factual information is supplied in the
    prompt, reducing dependence on the LLM’s internal knowledge base and consequently
    hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RAG workflow operates in two main stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmented generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the retrieval stage, relevant information is located from an external knowledge
    base or database. During the augmented generation stage, this retrieved information
    is combined with the user’s input to enhance the context provided to the LLM,
    enabling it to generate a response grounded in reliable, external facts. The RAG
    workflow is illustrated in figure 1.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 Providing relevant information to the LLM as part of the input
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As mentioned, LLMs are great at understanding natural language and following
    instructions in the prompt. In the RAG workflow, the goal shifts to task-oriented
    response generation, where LLMs follow a set of instructions. The process involves
    utilizing a retrieval tool to fetch relevant documents from a specific knowledge
    base. The LLM then generates answers based on the provided documents, ensuring
    responses are accurate, contextually relevant, and aligned with specific guidelines.
    This systematic approach transforms the answer generation process into a targeted
    task of inspecting and using the retrieved information to produce the final answer.
    An example of providing factual information in the input prompt is shown in figure
    1.11.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.11 illustrates an example of how an LLM processes follows the prompt
    instructions of a RAG workflow. The prompt highlights the importance of using
    retrieved context to ensure accurate and relevant responses and can be broken
    down into
  prefs: []
  type: TYPE_NORMAL
- en: '*Provided context* —A factual statement that introduces relevant information—in
    this case, identifying the Denver Nuggets as the 2023 NBA champions with a 4:1
    victory over the Miami Heat. This acts as the knowledge base input for the LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User query* —A specific question, “Who won the 2023 NBA championship?” which
    directs the LLM to extract relevant information from the provided context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generated answer* —The LLM’s response is aligned with the retrieved context:
    “The Denver Nuggets won the 2023 NBA championship.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/1-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 Providing relevant information to the answer as part of the prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You might wonder what the advantage of the RAG process is if the user has to
    provide both the context and the questions. In practice, the retrieval system
    operates independently from the user. The user only needs to provide the question,
    while the retrieval process occurs behind the scenes, as illustrated in figure
    1.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 Populating the relevant data from the user and knowledge base into
    the prompt template and then passing it to an LLM to generate the final answer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the RAG process, the user starts by asking a question. Behind the scenes,
    the system turns that question into a search query and retrieves relevant information
    from sources like company documents, knowledge articles, or databases. Advanced
    retrieval algorithms find the most suitable content, which is then combined with
    the original question to form an enriched prompt. This prompt is sent to an LLM,
    which generates a response based on both the question and the retrieved context.
    The entire retrieval process is automatic, and no extra input is required beyond
    the original question from the user. This makes RAG both seamless and effective,
    improving factual accuracy while reducing the chance of hallucinated answers.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG approach has gained mainstream popularity due to its simplicity and
    efficiency. It is now also part of the ChatGPT interface, where the LLM can use
    Web Search to search for relevant information before generating the final answer.
    Users of the paid version of ChatGPT may be familiar with the RAG process as depicted
    in figure 1.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 ChatGPT uses Web Search to find relevant information to generate
    an up-to-date answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the exact implementation of RAG in ChatGPT is not publicly disclosed,
    we can try to infer what it does under the hood. When the LLM decides, for whatever
    reason, that it needs to pull additional information, it can input a query into
    Web Search. We don’t know precisely how it navigates through search results, parses
    information from web pages, or decides that it has retrieved sufficient information.
    Nevertheless, we know that it used `2023` `NBA` `championship` `winner` keyword
    as input to Web Search and generated the final response based on the information
    available on the official NBA website ([https://www.nba.com/playoffs/2023/the-finals](https://www.nba.com/playoffs/2023/the-finals)).
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Knowledge graphs as the data storage for RAG applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When planning to implement a RAG application, choosing the right storage solution
    is important. While there are many database options, we argue that knowledge graphs
    and graph databases are especially well suited for most RAG applications. A knowledge
    graph is a data structure that uses nodes to represent concepts and entities and
    relationships to connect these nodes. An example knowledge graph is shown in figure
    1.14\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/1-14.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1.14 A knowledge graph can store complex structured and unstructured
    data in a single database system.**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Knowledge graphs are highly versatile, capable of storing both structured information
    (such as employee details, task statuses, and company hierarchies) and unstructured
    information (such as article contents). This dual capability, as illustrated in
    figure 1.14, makes them uniquely suited for complex RAG applications. Structured
    data allows for precise and efficient querying to answer questions such as, “How
    many tasks are assigned to a specific employee?” or “Which employees report to
    a particular manager?” For example, in figure 1.14, structured data such as “Sam
    Altman is the CEO of OpenAI” or “John Doe has been an employee of OpenAI since
    01-01-2023” can be directly queried to answer questions like “Who is the CEO of
    OpenAI?” or “How long has John Doe been with the company?” Similarly, structured
    relationships like “John Doe is assigned to a task with the status Completed”
    enable precise queries such as “Which tasks have been completed by employees?”
    or “Who is assigned to specific tasks at OpenAI?” This capability is critical
    for generating actionable insights from complex, interconnected data.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, unstructured data, such as article text, complements structured
    data by providing rich contextual information that adds depth and nuance. For
    instance, the unstructured article node in figure 1.14 provides details about
    a new LLM model and embeddings, but without a structured framework, it cannot
    answer specific queries like “How is this article related to OpenAI employees?”
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, unstructured data alone cannot answer all types of questions. While
    it can provide insights for open-ended or fuzzy queries, it lacks the structure
    needed for precise operations such as filtering, counting, or aggregating. For
    example, answering “How many tasks are completed within a company?” or “Which
    employees are assigned to tasks related to OpenAI?” requires structured relationships
    and attributes, as depicted in the right-hand side of figure 1.14\. Without structured
    data, these types of queries would require exhaustive text parsing and inference,
    which are computationally expensive and often imprecise. By integrating structured
    and unstructured information in the same framework, knowledge graphs enable the
    seamless blending of both worlds, making them a powerful tool for answering a
    broad range of questions efficiently and accurately in RAG applications. Moreover,
    explicit connections between unstructured and structured data unlock advanced
    retrieval strategies such as linking entities in text to graph nodes or contextualizing
    structured results with source passages that would be difficult or impossible
    to achieve using either type of data alone.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs, such as ChatGPT, are built on transformer architecture, enabling them
    to process and generate text efficiently by learning patterns from extensive textual
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While LLMs exhibit remarkable abilities in natural language understanding and
    generation, they have inherent limitations, such as a knowledge cutoff, the potential
    to generate outdated or hallucinated information, and an inability to access private
    or domain-specific knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous finetuning of LLMs to enhance their internal knowledge base is not
    practical due to resource constraints and the complexity of updating the models
    regularly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG addresses LLM limitations by combining them with external knowledge bases,
    providing accurate, context-rich responses by injecting relevant facts directly
    into the input prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG implementations have traditionally focused on unstructured data sources,
    limiting their scope and effectiveness for tasks requiring structured, precise,
    and interconnected information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge graphs use nodes and relationships to represent and connect entities
    and concepts, integrating structured and unstructured data to provide a holistic
    data representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating knowledge graphs into RAG workflows enhances their capability to
    retrieve and organize contextually relevant data, allowing LLMs to generate accurate,
    reliable, and explainable responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
