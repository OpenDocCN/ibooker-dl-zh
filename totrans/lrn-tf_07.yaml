- en: Chapter 7\. TensorFlow Abstractions and Simplifications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of this chapter is to get you familiarized with important practical
    extensions to TensorFlow. We start by describing what abstractions are and why
    they are useful to us, followed by a brief review of some of the popular TensorFlow
    abstraction libraries. We then go into two of these libraries in more depth, demonstrating
    some of their core functionalities along with some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As most readers probably know, the term *abstraction* in the context of programming
    refers to a layer of code “on top” of existing code that performs purpose-driven
    generalizations of the original code. Abstractions are formed by grouping and
    wrapping pieces of code that are related to some higher-order functionality in
    a way that conveniently reframes them together. The result is simplified code
    that is easier to write, read, and debug, and generally easier and faster to work
    with. In many cases TensorFlow abstractions not only make the code cleaner, but
    can also drastically reduce code length and as a result significantly cut development
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get us going, let’s illustrate this basic notion in the context of TensorFlow,
    and take another look at some code for building a CNN like we did in [Chapter 4](ch04.html#convolutional_neural_networks):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In native TensorFlow, in order to create a convolutional layer, we have to define
    and initialize its weights and biases according to the shapes of the input and
    the desired output, apply the convolution operation with defined strides and padding,
    and finally add the activation function operation. It’s easy to either accidentally
    forget one of these fundamental components or get it wrong. Also, repeating this
    process multiple times can be somewhat laborious and feels as if it could be done
    more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code example we created our own little abstraction by using
    functions that eliminate some of the redundancies in this process. Let’s compare
    the readability of that code with another version of it that does exactly the
    same, but without using any of the functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Even with just three layers, the resulting code looks pretty messy and confusing.
    Clearly, as we progress to larger and more advanced networks, code such as this
    would be hard to manage and pass around.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the more typical medium-sized batching of code, long and complex code
    is often “wrapped up” for us in abstraction libraries. This is particularly effective
    in relatively simple models where very little customization is ever required.
    As a preview to what will follow in the next section, you can already see how
    in `contrib.learn`, one of the abstractions available for TensorFlow, the core
    of defining and training a linear regression model similar to the one at the end
    of [Chapter 3](ch03.html#understanding_tensorflow_basics) could be done in just
    two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: High-Level Survey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'More than a few great TensorFlow open source extensions are available at the
    time of writing this book. Among the popular ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.contrib.learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TFLearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-Slim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While TFLearn needs to be installed, `contrib.learn` and TF-Slim (now `tf.contrib.slim`)
    are merged with TensorFlow and therefore require no installation. In 2017 Keras
    gained official Google support, and it has also been moved into `tf.contrib` as
    of version 1.1 (`tf.contrib.keras`). The name *contrib* refers to the fact that
    code in this library is “contributed” and still requires testing to see if it
    receives broad acceptance. Therefore, it could still change, and is yet to be
    part of the core TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '`contrib.learn` started as an independent simplified interface for TensorFlow
    and was initially called *Scikit Flow*, with the intention of making the creation
    of complex networks with TensorFlow more accessible for those who are transitioning
    from the `scikit-learn` world of “one-liner” machine learning. As is often the
    case, it was later merged to TensorFlow and is now regarded as its *Learn module*,
    with extensive documentation and examples that are available on the official TensorFlow
    website.'
  prefs: []
  type: TYPE_NORMAL
- en: Like other libraries, the main goal of `contrib.learn` is to make it easy to
    configure, train, and evaluate our learning models. For very simple models, you
    can use out-of-the-box implementations to train with just a few lines of code.
    Another great advantage of `contrib.learn`, as we will see shortly, is functionality
    with which data features can be handled very conveniently.
  prefs: []
  type: TYPE_NORMAL
- en: While `contrib.learn` is more transparent and low-level, the other three extensions
    are a bit cleaner and more abstract, and each has its own specialties and little
    advantages that might come in handy depending on the needs of the user.
  prefs: []
  type: TYPE_NORMAL
- en: TFLearn and Keras are full of functionality and have many of the elements needed
    for various types of state-of-the-art modeling. Unlike all the other libraries,
    which were created to communicate solely with TensorFlow, Keras supports both
    TensorFlow and Theano (a popular library for deep learning).
  prefs: []
  type: TYPE_NORMAL
- en: TF-Slim was created mainly for designing complex convolutional nets with ease
    and has a wide variety of pretrained models available, relieving us from the expensive
    process of having to train them ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: These libraries are very dynamic and are constantly changing, with the developers
    adding new models and functionalities, and occasionally modifying their syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Theano* is a Python library that allows you to manipulate symbolic mathematical
    expressions that involve tensor arrays in an efficient way, and as such it can
    serve as a deep learning framework, competing with TensorFlow. Theano has been
    around longer, and therefore is a bit more mature than TensorFlow, which is still
    changing and evolving but is rapidly becoming the leader of the pack (it is widely
    considered by many to already be the leading library, with many advantages over
    other frameworks).'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections we demonstrate how to use these extensions, alongside
    some examples. We begin by focusing on `contrib.learn`, demonstrating how easily
    it lets us train and run simple regression and classification models. Next we
    introduce TFLearn and revisit the more advanced models introduced in the previous
    chapters—CNN and RNN. We then give a short introduction to autoencoders and demonstrate
    how to create one with Keras. Finally, we close this chapter with brief coverage
    of TF-Slim and show how to classify images using a loaded pretrained state-of-the-art
    CNN model.
  prefs: []
  type: TYPE_NORMAL
- en: contrib.learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using `contrib.learn` doesn’t require any installation since it’s been merged
    with TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We start with `contrib.learn`’s out-of-the-box *estimators* (a fancy name for
    models), which we can train in a quick and efficient manner. These predefined
    estimators include simple linear and logistic regression models, a simple linear
    classifier, and a basic deep neural network. [Table 7-1](#tble0701) lists some
    of the popular estimators we can use.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Popular built-in contrib.learn estimators
  prefs: []
  type: TYPE_NORMAL
- en: '| Estimator | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `LinearRegressor()` | Linear regression model to predict label value given
    observation of feature values. |'
  prefs: []
  type: TYPE_TB
- en: '| `LogisticRegressor()` | Logistic regression estimator for binary classification.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `LinearClassifier()` | Linear model to classify instances into one of multiple
    possible classes. When the number of possible classes is 2, this is binary classification.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `DNNRegressor()` | A regressor for TensorFlow deep neural network (DNN) models.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `DNNClassifier()` | A classifier for TensorFlow DNN models. |'
  prefs: []
  type: TYPE_TB
- en: 'Of course, we would also like to use more-advanced and customized models, and
    for that `contrib.learn` lets us conveniently wrap our own homemade estimators,
    a feature that will be covered as we go along. Once we have an estimator ready
    for deployment, whether it was made for us or we made it ourselves, the steps
    are pretty much the same:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We *instantiate* the estimator class to create our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we *fit* it using our training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We *evaluate* the model to see how well it does on some given dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we use our fitted model to *predict* outcomes, usually for new data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These four fundamental stages are also found in other extensions.
  prefs: []
  type: TYPE_NORMAL
- en: '`contrib` offers many other functionalities and features; in particular, `contrib.learn`
    has a very neat way to treat our input data, which will be the focus of the next
    subsection, where we discuss linear models.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start our `contrib.learn` engagement with one of its strongest features:
    linear models. We say that a model is *linear* whenever it is defined by a function
    of a weighted sum of the features, or more formally *f*(*w[1]x[1]* + *w[2]x[2]*
    +...+ *w[n]x[n]*), where *f* could be any sort of function, like the identity
    function (as in linear regression) or a logistic function (as in logistic regression).
    Although limited in their expressive power, linear models have lots of advantages,
    such as clear interpretability, optimization speed, and simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#understanding_tensorflow_basics) we created our own
    linear regression model using native TensorFlow by first creating a graph with
    placeholders for the input and target data, Variables for the set of parameters, a
    loss function, and an optimizer. After the model was defined, we ran the session
    and obtained results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section we first repeat this full process, and then show how
    drastically easier it is to do with `contrib.learn`. For this example we use the
    Boston Housing dataset, available to download using the [`sklearn` library](http://bit.ly/2sXIfrX?). The
    Boston Housing dataset is a relatively small dataset (506 samples), containing
    information concerning housing in the area of Boston, Massachusetts. There are
    13 predictors in this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CRIM: per capita crime rate by town'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ZN: proportion of residential land zoned for lots over 25,000 sq.ft.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'INDUS: proportion of nonretail business acres per town'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'NOX: nitric oxide concentration (parts per 10 million)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RM: average number of rooms per dwelling'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AGE: proportion of owner-occupied units built prior to 1940'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'DIS: weighted distances to five Boston employment centers'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RAD: index of accessibility to radial highways'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'TAX: full-value property tax rate per $10,000'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PTRATIO: pupil–teacher ratio by town'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'B: 1000(Bk – 0.63)^2, where Bk is the proportion of blacks by town'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LSTAT: % lower status of the population'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The target variable is the median value of owner-occupied homes in thousands
    of dollars. In this example we try to predict the target variable by using some
    linear combination of these 13 features.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the same linear regression model as in [Chapter 3](ch03.html#understanding_tensorflow_basics).
    This time we track the “loss” so we can measure the mean squared error (MSE),
    which is the average of the squared differences between the real target value
    and our predicted value. We use this measure as an indicator of how well our model
    performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After 200 iterations, we print out the MSE calculated for the training set.
    Now we perform the exact same process, but using `contrib.learn`’s estimator for
    linear regression. The whole process of defining, fitting, and evaluating the
    model comes down to just a few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear regression model is instantiated using `learn.LinearRegressor()` and
    fed with knowledge about the data representation and the type of optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `regressor` object is trained using `.fit()`. We pass the covariates and
    the target variable, and set the number of steps and batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The MSE loss is returned by `.evaluate()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the code in its entirety:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Some representation of the input data is passed in the regressor instantiation
    as a processed variable called `feature_columns`. We will return to this shortly.
  prefs: []
  type: TYPE_NORMAL
- en: DNN Classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with regression, we can use `contrib.learn` to apply an out-of-the-box classifier.
    In [Chapter 2](ch02.html#go_with_the_flow) we created a simple softmax classifier
    for the MNIST data. The `DNNClassifier` estimator allows us to perform a similar
    task with a considerably reduced amount of code. Also, it lets us add hidden layers (the
    “deep” part of the DNN).
  prefs: []
  type: TYPE_NORMAL
- en: 'As in [Chapter 2](ch02.html#go_with_the_flow), we first import the MNIST data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in this case, due to the requirement of the estimator, we pass the
    target in its class label form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: returning a single integer per sample, corresponding to the correct digit class
    (i.e., values from 0 to [number of classes] – 1), instead of the one-hot form
    where each label is a vector with 1 in the index that corresponds to the correct
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next steps are similar to the ones we took in the previous example, except
    that when we define the model, we add the number of classes (10 digits) and pass
    a list where each element corresponds to a hidden layer with the specified number
    of units. In this example we use one hidden layer with 200 units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Though not as good as our CNN model in [Chapter 4](ch04.html#convolutional_neural_networks)
    (above 99%), the test accuracy here (around 98%) is significantly better than
    it was in the simple softmax example (around 92%) as a result of adding just a
    single layer. In [Figure 7-1](#mnist_classification_test_accuracy) we see how
    the accuracy of the model increases with the number of units in that hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. MNIST classification test accuracy as a function of units added
    in a single hidden layer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using the `<*Estimator*>.predict()` method, we can predict the classes of new
    samples. Here we will use the predictions to demonstrate how we can analyze our
    model’s performance—what classes were best identified and what types of typical
    errors were made. Plotting a *confusion matrix* can help us understand these behaviors.
    We import the code to create the confusion matrix from the `scikit-learn` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The confusion matrix is shown in [Figure 7-2](#predicted_digits_for_each_true).
    Its rows correspond to the true digits, its columns to the predicted digits. We
    see, for example, that the model sometimes misclassified `5` as `3` and `9` as
    `4` and `7`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. A confusion matrix showing the number of predicted digits (columns)
    for each true label (rows).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: FeatureColumn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of `contrib.learn`’s nicest offerings is handling features of different
    types, which can sometimes be a little tricky. To make things easier, `contrib.learn`
    offers us the `FeatureColumn` abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: With a `FeatureColumn` we can maintain a representation of a single feature
    in our data, while performing a range of transformations defined over it. A `FeatureColumn`
    can be either one of the original columns or any new columns that may be added
    depending on our transformations. These may include creating a suitable and effective
    representation for categorical data by encoding it as a sparse vector (often referred
    to as *dummy encoding*), creating feature crosses to look for feature interactions,
    and bucketization (discretization of the data). All this can be done while manipulating
    the feature as a single semantic unit (encompassing, for example, all dummy vectors).
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `FeatureColumn` abstraction to specify the form and structure of
    each feature of our input data. For instance, let’s say that our target variable
    is `height`, and we try to predict it using two features, `weight` and `species`. We
    make our own synthetic data where heights are generated by dividing each weight
    by a factor of 100 and adding a constant that varies according to the species:
    1 is added for Humans, 0.9 for Goblins, and 1.1 for ManBears. We then add normally
    distributed noise to each instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-3](#heights_for_three_species) shows visualizations of the data samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-3\. Left: A histogram of heights for the three types of species: Goblins,
    Humans, and ManBears (distributions centered at 1.6, 1.7, and 1.8, respectively).
    Right: A scatter plot of heights vs. weights.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our target variable is a numeric NumPy array of heights `height`, and our covariates
    are the numeric NumPy array of weights `weight` and a list of strings denoting
    the name of each species `spec`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the Pandas library to have the data represented as a data frame (table),
    so that we can conveniently access each of its columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-4](#fig0704) shows what our data frame looks like.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Ten rows of the Height–Species–Weight data frame. Heights and Weights
    are numeric; Species is categorical with three categories.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pandas is a very popular and useful library in Python for working with relational
    or labeled data like tabular data, multidimensional time series, etc. For more
    information on how to use Pandas, we refer the reader to Wes McKinney’s book [*Python
    for Data Analysis*](http://bit.ly/2uma9Om) (O’Reilly).
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by specifying the nature of each feature. For `Weight` we use the
    following `FeatureColumn` command, indicating that it’s a continuous variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`contrib.layers` is not a part of `contrib.learn`, but another independent
    subsection of the TensorFlow Python API that offers high-level operations and
    tools for building neural network layers.'
  prefs: []
  type: TYPE_NORMAL
- en: The name that was passed to the function (in this case `Weight`) is crucially
    important since it will be used to associate the `FeatureColumn` representation
    with the actual data.
  prefs: []
  type: TYPE_NORMAL
- en: '`Species` is a categorical variable, meaning its values have no natural ordering,
    and therefore cannot be represented as a single variable in the model. Instead,
    it has to be extended and encoded as several variables, depending on the number
    of categories. `FeatureColumn` does this for us, so we just have to use the following
    command to specify that it is a categorical feature and indicate the name of each
    category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate an estimator class and input a list of our `FeatureColumn`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Up to now we’ve defined how the data will be represented in the model; in the
    following stage of fitting the model we need to provide the actual training data.
    In the Boston Housing example, the features were all numeric, and as a result
    we could just input them as `x_data` and target data.
  prefs: []
  type: TYPE_NORMAL
- en: Here, `contrib.learn` requires that we use an additional encapsulating input
    function. The function gets both predictors and target data in their native form
    (Pandas data frame, NumPy array, list, etc.) as input, and returns a dictionary
    of tensors. In these dictionaries, each key is a name of a `FeatureColumn` (the
    names `Weight` and `Species` that were given as input previously), and its value
    needs to be a Tensor that contains the corresponding data. This means that we
    also have to transform the values into a TensorFlow Tensor inside the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our current example, the function receives our data frame, creates a dictionary
    `feature_cols`, and then stores the values of each column in the data frame as
    a Tensor for the corresponding key. It then returns that dictionary and the target
    variable as a Tensor. The keys have to match the names we used to define our `FeatureColumn`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The values of `Species` are required by their `FeatureColumn` specification
    to be encoded in a sparse format. For that we use `tf.SparseTensor()`, where each
    `i` index corresponds to a nonzero value (in this case, all the rows in a one-column
    matrix).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'represents the dense tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass it to the `.fit()` method in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, `input_fn()` is the function we just created, `df` is the data frame containing
    the data, and we also specify the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we pass the function in a form of a `lambda` function rather than
    the function’s outputs, because the `.fit()` method requires a function object.
    Using `lambda` allows us to pass our input arguments and keep it in an object
    form. There are other workarounds we could use to achieve the same outcome, but
    `lambda` does the trick.
  prefs: []
  type: TYPE_NORMAL
- en: The fitting process may take a while. If you don’t want to do it all at once,
    you can split it into segments (see the following note).
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the training process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s possible to perform the fit iteratively since the state of the model is
    preserved in the classifier. For example, instead of performing all 50,000 iterations
    consecutively like we did, we could split it into five segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: and achieve the same outcome. This could be useful if we want to have some tracking
    of the model while training it; however, there are better ways to do that, as
    we will see later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see how well the model does by looking at the estimated weights. We
    can use the the `.get_variable_value()` method to get the variables’ values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We request the values of the weights for both `Weight` and `Species`. `Species`
    is a categorical variable, so its three weights serve as different bias terms.
    We see that the model did quite well in estimating the true weights (`0.01` for
    `Weight` and `0.9`, `1`, `1.1` for `Goblins`, `Humans`, and `ManBears`, respectively,
    for `Species`). We can get the names of the variables by using the `.get_variable_names()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The same process can be used in more complicated scenarios where we want to
    handle many types of features and their interactions. [Table 7-2](#tble0702) lists
    some useful operations you can do with `contrib.learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. Useful feature transformation operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Operation | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `layers.sparse_column_with_keys()` | Handles the conversion of categorical
    values  |'
  prefs: []
  type: TYPE_TB
- en: '| `layers.sparse_column_with_hash_bucket()` | Handles the conversion of categorical
    features for which you don’t know all possible values |'
  prefs: []
  type: TYPE_TB
- en: '| `layers.crossed_column()` | Sets up feature crosses (interactions) |'
  prefs: []
  type: TYPE_TB
- en: '| `layers.bucketized_column()` | Turns a continuous column into a categorical
    column |'
  prefs: []
  type: TYPE_TB
- en: Homemade CNN with contrib.learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We next move on to creating our own estimator by using `contrib.learn`. To do
    so, we first need to construct a model function where our homemade network will
    reside and an object containing our training settings.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example we create a custom CNN estimator that is identical
    to the one used at the beginning of [Chapter 4](ch04.html#convolutional_neural_networks),
    and use it again to classify the MNIST data. We begin by creating a function for
    our estimator with inputs that include our data, the mode of operation (training
    or test), and the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the MNIST data the pixels are concatenated in the form of a vector and therefore
    require that we reshape them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We build the network by using the `contrib.layers` functionality, making the
    process of layer construction simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `layers.convolution2d()` we can set everything in a one-liner command:
    we pass the input (the output of the previous layer), and then indicate the number
    of feature maps (32), the size of the filter (5×5), and the activation function
    (`relu`), and initialize the weights and biases. The dimensionality of the input
    is automatically identified and does not need to be specified. Also, unlike when
    working in lower-level TensorFlow, we don’t need to separately define the shapes
    of the variables and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The padding is set to `'SAME'` by default (unchanged number of pixels), resulting
    in an output of shape 28×28×32.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also add the standard 2×2 pooling layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We then repeat these steps, this time for 64 target feature maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we flatten the 7×7×64 tensor and add a fully connected layer, reducing
    it to 1,024 entries. We use `fully_connected()` similarly to `convolution2d()`,
    except we specify the number of output units instead of the size of the filter
    (there’s just one of those):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add dropout with `keep_prob` as set in the parameters given to the
    function (train/test mode), and the final fully connected layer with 10 output
    entries, corresponding to the 10 classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We complete our model function by defining a training object with the loss and
    the learning rate of the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have one function that encapsulates the entire model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We instantiate the estimator by using `contrib.learn.Estimator()`, and we’re
    good to go. Once defined, we can use it with the same functionalities as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Using `contrib.learn` and `contrib.layers`, the number of lines of code was
    cut down considerably in comparison to lower-level TensorFlow. More important,
    the code is much more organized and easier to follow, debug, and write.
  prefs: []
  type: TYPE_NORMAL
- en: With this example we conclude the `contrib.learn` portion of this chapter. We’ll
    now move on to cover some of the functionalities of the TFLearn library.
  prefs: []
  type: TYPE_NORMAL
- en: TFLearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TFLearn is another library that allows us to create complex custom models in
    a very clean and compressed way, while still having a reasonable amount of flexibility,
    as we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike the previous library, TFLearn first needs to be installed. The installation
    is straightforward using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: If that doesn’t work, it can be downloaded from [GitHub](https://github.com/tflearn/tflearn)
    and installed manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the library has been successfully installed, you should be able to import
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of the functionalities of TFLearn resemble those covered in the previous
    section on `contrib.learn`; however, creating a custom model is a bit simpler
    and cleaner in comparison. In the following code we use the same CNN used earlier
    for the MNIST data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model construction is wrapped and finalized using `regression()`, where we
    set the loss and optimization configuration as we did previously for the training
    object in `contrib.learn` (here we simply specify `''categorical_crossentropy''`
    for the loss, rather than explicitly defining it):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Another layer that’s been added here, and that we briefly mentioned in [Chapter 4](ch04.html#convolutional_neural_networks),
    is the local response normalization layer. See the upcoming note for more details
    about this layer.
  prefs: []
  type: TYPE_NORMAL
- en: The `tflearn.DNN()` function is somewhat equivalent to `contrib.learn.Estimator()`—it’s
    the DNN model wrapper with which we instantiate the model and to which we pass
    our constructed network.
  prefs: []
  type: TYPE_NORMAL
- en: Here we can also set the TensorBoard and checkpoints directories, the level
    of verbosity of TensorBoard’s logs (0–3, from basic loss and accuracy reports
    to other measures like gradients and weights), and other settings.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a model instance ready, we can then perform standard operations
    with it. [Table 7-3](#tble0703) summarizes the model’s functionalities in TFLearn.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-3\. Standard TFLearn operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `evaluate(*X*, *Y*, *batch_size=128*)` | Perform evaluations of the model
    on given samples. |'
  prefs: []
  type: TYPE_TB
- en: '| `fit(*X*, *Y*, *n_epoch=10*)` | Train the model with input features `*X*`
    and target `*Y*` to the network. |'
  prefs: []
  type: TYPE_TB
- en: '| `get_weights(*weight_tensor*)` | Get a variable’s weights. |'
  prefs: []
  type: TYPE_TB
- en: '| `load(*model_file*)` | Restore model weights. |'
  prefs: []
  type: TYPE_TB
- en: '| `predict(*X*)` | Get model predictions for the given input data. |'
  prefs: []
  type: TYPE_TB
- en: '| `save(*model_file*)` | Save model weights. |'
  prefs: []
  type: TYPE_TB
- en: '| `set_weights(*tensor*, *weights*)` | Assign a tensor variable a given value.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Similarly with `contrib.learn`, the fitting operation is performed by using
    the `.fit()` method, to which we feed the data and control training settings:
    the number of epochs, training and validation batch sizes, displayed measures,
    saved summaries frequency, and more. During fitting, TFLearn displays a nice dashboard,
    enabling us to track the training process online.'
  prefs: []
  type: TYPE_NORMAL
- en: Local response normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The local response normalization (LRN) layer performs a kind of *lateral inhibition*
    by normalizing over local input regions. This is done by dividing the input values
    by the weighted, squared sum of all inputs within some depth radius, which we
    can manually choose. The resulting effect is that the activation contrast between
    the excited neurons and their local surroundings increases, producing more salient
    local maxima. This method encourages inhibition since it will diminish activations
    that are large, but uniform. Also, normalization is useful to prevent neurons
    from saturating when inputs may have varying scale (ReLU neurons have unbounded
    activation). There are more modern alternatives for regularization, such as batch
    normalization and dropout, but it is good to know about LRN too.
  prefs: []
  type: TYPE_NORMAL
- en: 'After fitting the model, we evaluate performance on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'and form new predictions (using them here again as a “sanity check” to the
    previous evaluation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Iterations training steps and epochs in TFLearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In TFLearn, each *iteration* is a full pass (forward and backward) over one
    example. The *training step* is the number of full passes to perform, determined
    by the batch size you set (the default is 64), and an *epoch* is a full pass over
    all the training examples (50,000 in the case of MNIST). [Figure 7-5](#fig0705)
    shows an example of the interactive display in TFLearn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Interactive display in TFLearn.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: RNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We wrap up our introduction to TFLearn by constructing a fully functioning text
    classification RNN model that considerably simplifies the code we saw in Chapters
    [5](ch05.html#text_i) and [6](ch06.html#text_ii).
  prefs: []
  type: TYPE_NORMAL
- en: 'The task we perform is a sentiment analysis for movie reviews with binary classification
    (good or bad). We will use a well-known dataset of IMDb reviews, containing 25,000
    training samples and 25,000 test samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We first prepare the data, which has different sequence lengths, by equalizing
    the sequences with zero-padding by using `tflearn.data_utils.pad_sequences()`
    and setting 100 as the maximum sequence length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now we can represent data in one tensor, with samples in its rows and word IDs
    in its columns. As was explained in [Chapter 5](ch05.html#text_i), IDs here are
    integers that are used to encode the actual words arbitrarily. In our case, we
    have 10,000 unique IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we embed each word into a continuous vector space by using `tflearn.embedding()`,
    transforming our two-dimensional tensor `[*samples*, *IDs*]` into a three-dimensional
    tensor, `[*samples*, *IDs*, *embedding-size*]`, where each word ID now corresponds
    to a vector of size of 128\. Before that we use `input_data()` to input/feed data
    to the network (a TensorFlow placeholder is created with the given shape):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add an LSTM layer and a fully connected layer to output the binary
    outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we had just a quick taste of TFLearn. The library has nice
    [documentation](http://tflearn.org) and many examples that are well worth looking
    at.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras is one of the most popular and powerful TensorFlow extension libraries. Among
    the extensions we survey in this chapter, Keras is the only one that supports
    both Theano—upon which it was originally built—and TensorFlow. This is possible
    because of Keras’s complete abstraction of its backend; Keras has its own graph
    data structure for handling computational graphs and communicating with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, because of that it could even be possible to define a Keras model with
    either TensorFlow or Theano and then switch to the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras has two main types of models to work with: sequential and functional.
    The sequential type is designed for simple architectures, where we just want to
    stack layers in a linear fashion. The functional API can support more-general
    models with a diverse layer structure, such as multioutput models.'
  prefs: []
  type: TYPE_NORMAL
- en: We will take a quick look at the syntax used for each type of model.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In TensorFlow 1.1+ Keras can be imported from the `contrib` library; however,
    for older versions it needs to be installed externally. Note that Keras requires
    the `numpy`, `scipy`, and `yaml` dependencies. Similarly to TFLearn, Keras can
    either be installed using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'or downloaded from [GitHub](https://github.com/fchollet/keras) and installed
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Keras will use TensorFlow as its tensor manipulation library. If
    it is set to use Theano, it can be switched by changing the settings in the file
    called `$HOME/.keras/keras.json` (for Linux users—modify the path according to
    your OS), where the attribute `backend` appears in addition to other technical
    settings not important in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to access the backend, we can easily do so by first importing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use it for most tensor operations as we would in TensorFlow (also
    for Theano). For example, this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Sequential model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the sequential type is very straightforward—we define it and can simply
    start adding layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Or equivalently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: A *dense* layer is a fully connected layer. The first argument denotes the number
    of output units, and the input shape is the shape of the input (in this example
    the weight matrix would be of size 784×64). `Dense()` also has an optional argument
    where we can specify and add an activation function, as in the second example.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the model is defined, and just before training it, we set its learning
    configurations by using the `.compile()` method. It has three input arguments—the
    loss function, the optimizer, and another metric function that is used to judge
    the performance of your model (not used as the actual loss when training the model):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We can set the optimizer at a finer resolution (learning rate, method, etc.) using
    `.optimizers`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we feed `.fit()` the data and set the number of epochs and batch size.
    As with the previous libraries, we can now easily evaluate how it does and perform
    predictions with new test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Note that a `callbacks` argument was added to the `fit()` method. Callbacks
    are functions that are applied during the training procedure, and we can use them
    to get a view on statistics and make dynamic training decisions by passing a list
    of them to the `.fit()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example we plug in two callbacks: TensorBoard, specifying its output
    folder, and early stopping.'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Early stopping is used to protect against overfitting by preventing the learner
    from further improving its fit to the training data at the expense of increasing
    the generalization error. In that sense, it can be thought of as a form of regularization.
    In Keras we can specify the minimum change to be monitored (`min_delta`), the
    number of no-improvement epochs to stop after (`patience`), and the direction
    of wanted change (`mode`).
  prefs: []
  type: TYPE_NORMAL
- en: Functional model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main practical difference between the functional model and the sequential
    model is that here we first define our input and output, and only then instantiate
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create an input Tensor according to its shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the layers act as functions, giving the functional model its
    name.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now we instantiate the model, passing both inputs and outputs to `Model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The other steps follow as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We will end this section by introducing the concept of autoencoders and then
    showing how to implement one using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Autoencoders* are neural networks that try to output a reconstruction of the
    input. In most cases the input is reconstructed after having its dimensionality
    reduced. Dimensionality reduction will be our main focus; however, autoencoders
    can also be used to achieve “overcomplete” representations (for more stable decomposition),
    which actually increases dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: In dimensionality reduction we wish to translate each vector of data with size *n* to
    a vector with size *m*, where *m < n*, while trying to keep as much important
    information as possible. One very common way to do that is using principal component
    analysis (PCA), where we can represent each original data column *x[j]* (all data
    points corresponding to an original feature) with some linear combination of the
    new reduced features, called the *principal components*, such that *x[j]* = *Σ[i=1]^mw[i]b[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: PCA, however, is limited to only linear transformation of the data vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are more general compressors, allowing complicated nonlinear transformations
    and finding nontrivial relations between visible and hidden units (in fact, PCA
    is like a one-layer “linear autoencoder”). The weights of the models are learned
    automatically by reducing a given loss function with an optimizer (SGD, for example).
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders that reduce input dimensionality create a bottleneck layer called
    a *hidden layer* that has a smaller number of units than the input layer, forcing
    the data to be represented in a lower dimension ([Figure 7-6](#autoencoder)) before
    it is reconstructed. For the reconstruction (decoding), autoencoders extract representative
    features that capture some hidden abstractions, like the shape of an eye, wheel
    of a car, type of sport, etc., with which we can reconstruct the original input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Illustration of an autoencoder—a typical autoencoder will have
    input and output layers consisting of the same number of units, and bottleneck
    hidden layers, where the dimensionality of the data is reduced (compressed).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like some of the models we’ve seen so far, autoencoder networks can have layers
    stacked on top of each other, and they can include convolutions as in CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are currently not very suitable for real-world data compression
    problems due to their data specificity—they are best used on data that is similar
    to what they were trained on. Their current practical applications are mostly
    for extracting lower-dimensional representations, denoising data, and data visualization
    with reduced dimensionality. Denoising works because the network learns the important
    abstractions of the image, while losing unimportant image-specific signals like
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s build a toy CNN autoencoder with Keras. In this example we will train
    the autoencoder on one category of a noisy version of the CIFAR10 data images,
    and then use it to denoise a test set of the same category. In this example we
    will use the functional model API.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we load the images by using Keras, and then we choose only the images
    that correspond to the label `1` (the automobile class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we do a little pre-processing, by first converting our data to `float32`
    and then normalizing it to a range between [0,1]. This normalization will allow
    us to perform an element-wise comparison at the pixel level, as we will see shortly.
    First, the type conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add some Gaussian noise to create the noisy dataset, and clip values
    that are either smaller than 0 or larger than 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we declare the input layer (every image in the CIFAR10 dataset is 32×32
    pixels with RGB channels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Next, we start adding our usual “LEGO brick" layers.  Our first layer is a 2D
    convolution layer, where the first argument is the number of filters (and thus
    the number of output images), and the second is the size of each filter. Like
    the other libraries, Keras automatically identifies the shape of the input.
  prefs: []
  type: TYPE_NORMAL
- en: We use a 2×2 pooling layer, which reduces the total number of pixels per channel
    by 4, creating the desired bottleneck. After another convolutional layer, we regain
    the same number of units for each channel by applying an up-sampling layer. This
    is done by quadrupling each pixel in a pixel’s near vicinity (repeating the rows
    and columns of the data) to get back the same number of pixels in each image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we add a convolutional output layer where we go back to three channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We declare the functional model format, passing both inputs and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we compile the model, defining the loss function and the optimizer—in
    this case we use the Adagrad optimizer (just to show another example!). For denoising
    of the images, we want our loss to capture the discrepancy between the decoded
    images and the original, pre-noise images. For that we use a binary cross-entropy
    loss, comparing each decoded pixel to its corresponding original one (it’s now
    between [0,1]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'After the model is defined, we fit it with 10 training epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Hopefully the model will capture some internal structure, which it can later
    generalize to other noisy images, and denoise them as a result.
  prefs: []
  type: TYPE_NORMAL
- en: We use our test set as validation data for loss evaluation at the end of each
    epoch (the model will not be trained on this data), and also for visualization
    in TensorBoard. In addition to the TensorBoard callback, we add a model saver
    callback and set it to save our weights every two epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, when we wish to load our weights, we need to reconstruct the network
    and then use the `Model.load_weights()` method, passing our model as the first
    argument and our saved weights file path as the second (more on saving models
    in [Chapter 10](ch10.html#exporting_and_serving_models_with_tensorflow)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: h5py requirement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For model saving, it is required that the `h5py` package is installed. This
    package is primarily used for storing large amounts of data and manipulating it
    from NumPy. You can install it using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-7](#before_and_after_autoencoding) shows the denoised test images
    of our chosen category for different numbers of training epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Noisy CIFAR10 images before autoencoding (upper row) and after
    autoencoding (lower rows). The 4 bottom rows show results after increasing number
    of training epochs.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras also has a bunch of pretrained models to download, like *inception*, *vgg*,
    and *resnet*. In the next and final section of this chapter, we will discuss these
    models and show an example of how to download and use a pretrained VGG model for
    classification using the TF-Slim extension.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained models with TF-Slim
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section of the chapter we will introduce the last abstraction to be
    covered here, TF-Slim. TF-Slim stands out by offering simplified syntax for defining
    convolutional neural networks in TensorFlow—its abstractions make it easy to build
    complex networks in a clean, streamlined manner. Like Keras, it also offers a
    nice variety of [pretrained CNN models](http://bit.ly/2sZt5lE?) to download and
    use.
  prefs: []
  type: TYPE_NORMAL
- en: We start this section by learning about some of the general features and benefits
    of TF-Slim, and why it’s a great tool to use for building CNNs. In the second
    part of this section we will demonstrate how to download and deploy a pretrained
    model (VGG) for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: TF-Slim
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TF-Slim is a relatively new lightweight extension of TensorFlow that, like other
    abstractions, allows us to define and train complex models quickly and intuitively. TF-Slim
    doesn’t require any installation since it’s been merged with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: This extension is all about convolutional neural networks. CNNs are notorious
    for having a lot of messy boilerplate code. TF-Slim was designed with the goal
    of optimizing the creation of very complex CNN models so that they could be elegantly
    written and easy to interpret and debug by using high-level layers, variable abstractions,
    and argument scoping, which we will touch upon shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to enabling us to create and train our own models, TF-Slim has
    available pretrained networks that can be easily downloaded, read, and used: VGG,
    AlexNet, Inception, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: We start this section by briefly describing some of TF-Slim’s abstraction features.
    Then we shift our focus to how to download and use a pretrained model, demonstrating
    it for the VGG image classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating CNN models with TF-Slim
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With TF-Slim we can create a variable easily by defining its initialization,
    regularization, and device with one wrapper. For example, here we define weights
    initialized from a truncated normal distribution using L2 regularization and placed
    on the CPU (we will talk about distributing model parts across devices in [Chapter 9](ch09.html#distributed_tensorflow)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the other abstractions we’ve seen in this chapter, TF-Slim can reduce
    a lot of boilerplate code and redundant duplication. As with Keras or TFLearn,
    we can define a layer operation at an abstract level to include the convolution
    operation, weights initialization, regularization, activation function, and more
    in a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: TF-Slim extends its elegance even beyond that, providing a clean way to replicate
    layers compactly by using the `repeat`, `stack`, and `arg_scope` commands.
  prefs: []
  type: TYPE_NORMAL
- en: '`repeat` saves us the need to copy and paste the same line over and over so
    that, for example, instead of having this redundant duplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'we could just enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'But this is viable only in cases where we have layers of the same size. When
    this does not hold, we can use the `stack` command, allowing us to concatenate
    layers of different shapes. So, instead of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'we can write this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we also have a scoping mechanism referred to as `arg_scope`, allowing
    users to pass a set of shared arguments to each operation defined in the same
    scope. Say, for example, that we have four layers having the same activation function,
    initialization, regularization, and padding. We can then simply use the `slim.arg_scope`
    command, where we specify the shared arguments as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: The individual arguments inside the `arg_scope` command can still be overwritten,
    and we can also nest one `arg_scope` inside another.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these examples we used `conv2d()`: however, TF-Slim has many of the other
    standard methods for building neural networks. [Table 7-4](#tble0704) lists some
    of the available options. For the full list, consult [the documentation](http://bit.ly/2txy6PN?).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-4\. Available layer types in TF-Slim
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer | TF-Slim |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BiasAdd | `slim.bias_add()` |'
  prefs: []
  type: TYPE_TB
- en: '| BatchNorm | `slim.batch_norm()` |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2d | `slim.conv2d()` |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2dInPlane | `slim.conv2d_in_plane()` |'
  prefs: []
  type: TYPE_TB
- en: '| Conv2dTranspose (Deconv) | `slim.conv2d_transpose()` |'
  prefs: []
  type: TYPE_TB
- en: '| FullyConnected | `slim.fully_connected()` |'
  prefs: []
  type: TYPE_TB
- en: '| AvgPool2D | `slim.avg_pool2d()` |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | `slim.dropout()` |'
  prefs: []
  type: TYPE_TB
- en: '| Flatten | `slim.flatten()` |'
  prefs: []
  type: TYPE_TB
- en: '| MaxPool2D | `slim.max_pool2d()` |'
  prefs: []
  type: TYPE_TB
- en: '| OneHotEncoding | `slim.one_hot_encoding()` |'
  prefs: []
  type: TYPE_TB
- en: '| SeparableConv2 | `slim.separable_conv2d()` |'
  prefs: []
  type: TYPE_TB
- en: '| UnitNorm | `slim.unit_norm` |'
  prefs: []
  type: TYPE_TB
- en: 'To illustrate how convenient TF-Slim is for creating complex CNNs, we will
    build the VGG model by Karen Simonyan and Andrew Zisserman that was introduced
    in 2014 (see the upcoming note for more information). VGG serves as a good illustration
    of how a model with many layers can be created compactly using TF-Slim. Here we
    construct the 16-layer version: 13 convolution layers plus 3 fully connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating it, we take advantage of two of the features we’ve just mentioned:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `arg_scope` feature since all of the convolution layers have the
    same activation function and the same regularization and initialization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Many of the layers are exact duplicates of others, and therefore we also take
    advantage of the `repeat` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The result very compelling—the entire model is defined with just 16 lines of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: VGG and the ImageNet Challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [ImageNet project](http://www.image-net.org) is a large database of images
    collected for the purpose of researching visual object recognition. As of 2016
    it contained over 10 million hand-annotated images.
  prefs: []
  type: TYPE_NORMAL
- en: Each year (since 2010) a competition takes place called the ImageNet Large Scale
    Visual Recognition Challenge (ILSVRC), where research teams try to automatically
    classify, detect, and localize objects and scenes in a subset of the ImageNet
    collection. In the 2012 challenge, dramatic progress occurred when a deep convolutional
    neural net called AlexNet, created by Alex Krizhevsky, managed to get a top 5
    (top 5 chosen categories) classification error of only 15.4%, winning the competition
    by a large margin.
  prefs: []
  type: TYPE_NORMAL
- en: Over the next couple of years the error rate kept falling, from ZFNet with 14.8%
    in 2013, to GoogLeNet (introducing the Inception module) with 6.7% in 2014, to
    ResNet with 3.6% in 2015.  The Visual Geometry Group (VGG) was another CNN competitor
    in the 2014 competition that also achieved an impressive low error rate (7.3%).
    A lot of people prefer VGG over GoogLeNet because it has a nicer, simpler architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In VGG the only spatial dimensions used are very small 3×3 filters with a stride
    of 1 and a 2×2 max pooling, again with a stride of 1\. Its superiority is achieved
    by the number of layers it uses, which is between 16 and 19.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and using a pretrained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next we will demonstrate how to download and deploy a pretrained VGG model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we need to clone the repository where the actual models will reside by
    running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have the scripts we need for modeling on our computer, and we can use
    them by setting the path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we will download the pretrained VGG-16 (16 layers) model—it is available
    [on GitHub](http://bit.ly/2vkqMHq), as are other models, such as Inception, ResNet,
    and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The downloaded checkpoint file contains information about both the model and
    the variables. Now we want to load it and use it for classification of new images.
  prefs: []
  type: TYPE_NORMAL
- en: However, before that we first have to prepare our input image, turning it into
    a readable TensorFlow format and performing a little pre-processing to make sure
    that it is resized to match the size of the images the model was trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load the image into TensorFlow either as a URL link or as a desktop
    image. For a URL link, we can load the image as a string with `urllib2` (this
    needs to be imported), and then decode it into a Tensor by using `tf.image_decode_jpeg()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, for PNG:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'To load an image from our computer, we can create a queue of our filenames
    in the target directory, and then read the entire image file by using `tf.WholeFileReader()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Don’t worry about the details for this step; we will discuss queues and reading
    data in much more depth in [Chapter 8](ch08.html#queues_threads_and_reading_data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we want to resize the image so that it matches the size of the images
    VGG was trained on. For that, we first extract the desired size from the VGG script
    (in this case, it is 224):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we feed the raw image and the image size to the VGG pre-processing unit,
    where the image will be resized with a preserved aspect ratio (the width-to-height
    ratio of the image) and then cropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we use `tf.expand_dims()` to insert a dimension of 1 into a tensor’s shape.
    This is done to add a batch dimension to a single element (changing `[*height*,
    *width*, *channels*]` to `[1, *height*, *width*, *channels*]`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Now we create the model from the script we cloned earlier. We pass the model
    function the images and number of classes. The model has shared arguments; therefore,
    we call it using `arg_scope`, as we saw earlier, and use the `vgg_arg_scope()`
    function in the script to define the shared arguments. The function is shown in
    the following code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: '`vgg_16()` returns the logits (numeric values acting as evidence for each class),
    which we can then turn into probabilities by using `tf.nn.softmax()`. We use the
    argument `is_training` to indicate that we are interested in forming predictions
    rather than training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, just before starting the session, we need to load the variables we downloaded
    using `slim.assign_from_checkpoint_fn()`, to which we pass the containing directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the main event—we run the session, load the variables, and feed in
    the images and the desired probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the class names by using the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract the five classes with the highest probabilities for our given image, and
    the probabilities as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: In this example we passed the image shown in [Figure 7-8](#lakeside_in_switzerland)
    as input to the pretrained VGG model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. A lakeside in Switzerland.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the output results for the top-five chosen classes and their probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the classifier does quite well at capturing different elements
    in this image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started this chapter by discussing the importance of abstractions, followed
    by high-level coverage and then focusing in on some of the popular TensorFlow
    extensions: `contrib.learn`, TFLearn, Keras, and TF-Slim. We revisited models
    from previous chapters, using out-of-the-box `contrib.learn` linear regression
    and linear classification models. We then saw how to use the `FeatureColumn` abstraction
    for feature handling and pre-processing, incorporate TensorBoard, and create our
    own custom estimator. We introduced TFLearn and exemplified how easily CNN and
    RNN models can be constructed with it. Using Keras, we demonstrated how to implement
    an autoencoder. Finally, we created complex CNN models with TF-Slim and deployed
    a pretrained model.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters we cover scaling up, with queuing and threading, distributed
    computing, and model serving.
  prefs: []
  type: TYPE_NORMAL
