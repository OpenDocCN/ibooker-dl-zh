- en: Chapter 7\. Advanced Fine-Tuning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we presented the canonical way to fine-tune a typical
    LLM. In the real world, there are a wide variety of motivations for updating an
    LLM, and similarly there are multiple ways to update it. In this chapter, we will
    describe several advanced fine-tuning techniques and highlight the scenarios in
    which each technique would be suitable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why would you want to update the parameters of an LLM? We touched upon this
    in previous chapters but let’s go through it in more detail now:'
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation
  prefs: []
  type: TYPE_NORMAL
- en: The data that we work with belongs to a specialized domain that the LLM might
    not have been familiarized with during pre-training. In this case, we would like
    to update the model by training it on domain-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: Task adaptation
  prefs: []
  type: TYPE_NORMAL
- en: We care about LLM performance on specific downstream tasks. To improve the LLM’s
    performance on these tasks, we can train it on task-specific data. This can be
    supervised or unsupervised.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge updating
  prefs: []
  type: TYPE_NORMAL
- en: We would like to keep the LLM’s knowledge up-to-date by continually training
    it on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Controllability/steerability
  prefs: []
  type: TYPE_NORMAL
- en: We would like to control the behavior of the LLM, including making it more likely
    to follow user requests written in natural language, reject certain types of requests,
    and so on. Techniques to achieve this are collectively called alignment training.
    We will defer discussion of alignment training to [Chapter 8](ch08.html#ch8).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn techniques that can be used to update the LLM
    for the aforementioned reasons. To this end, the chapter is divided into three
    sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Continual pre-training
  prefs: []
  type: TYPE_NORMAL
- en: Primarily used for domain adaptation and keeping the knowledge of the LLM up-to-date
    (the latter is also called lifelong-learning).
  prefs: []
  type: TYPE_NORMAL
- en: Parameter-Efficient Fine-Tuning (PEFT)
  prefs: []
  type: TYPE_NORMAL
- en: A set of fine-tuning techniques that make the fine-tuning process more efficient
    by updating only a small number of model parameters, thus needing less memory
    and compute.
  prefs: []
  type: TYPE_NORMAL
- en: Model merging/model fusion
  prefs: []
  type: TYPE_NORMAL
- en: An exciting new subfield of LLMs that explores combining the parameters of two
    or more models. I call this the “dark arts” of NLP, as it is poorly understood
    but uncannily effective if done the right way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin with my personal favorite: continual pre-training!'
  prefs: []
  type: TYPE_NORMAL
- en: Continual Pre-Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The premise of continual pre-training is simple. Take a pre-trained model checkpoint
    and continue pre-training it with your own data. But why would you want to do
    that? Here are some scenarios where continual pre-training can help.
  prefs: []
  type: TYPE_NORMAL
- en: You work in a specialized domain like law, finance, or biomedical. In each of
    these cases, text belonging to these domains differs linguistically and structurally
    from naturally occurring English text. For example, legal text is characterized
    by long sentences written in a formal tone, containing jargon specific to the
    legal domain. Financial text is interspersed with a lot of numbers. Both legal
    and financial text contain a significant proportion of boilerplate text. Biomedical
    text contains a lot of scientific terms that are not part of the standard English
    vocabulary. In all these cases, you would like to pre-train your LLM on domain-specific
    data so that the LLM is exposed to the nuances and characteristics of domain-specific
    text. This is called *domain-adaptive pre-training (DAPT)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking DAPT one step further, you can also continue pre-training your model
    not just on general text from your domain of interest but also on domain text
    specifically related to your downstream tasks. This is called *task-adaptive pre-training
    (TAPT)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your LLM is a reservoir of knowledge. But this knowledge can become obsolete
    over time. To keep its knowledge up-to-date, you continue pre-training the model
    at regular time periods or when new data is available. This is called *life-long
    learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You might be thinking, “If I want a domain-specific LLM, why don’t I just take
    my domain-specific data and train an LLM from scratch?” Well, you can, but your
    LLM just won’t be as performant, and the exercise will cost a whole lot more than
    continual pre-training. LLMs learn a wide variety of linguistic capabilities that
    might not be able to be learned from domain-specific text alone. Therefore, it
    is better to take an already pre-trained LLM that was trained on general text
    and then continue pre-training it with domain-specific text.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, continual pre-training is a challenging exercise. This is due to
    the phenomenon of catastrophic forgetting, where the LLM *forgets* its previously
    learned capabilities and knowledge when it continues to be trained on new and
    different data. We will soon explore various techniques to combat the catastrophic
    forgetting problem.
  prefs: []
  type: TYPE_NORMAL
- en: How does continual pre-training differ from fine-tuning? The differences are
    mostly cosmetic and terminology-related. Just like pre-training, continual pre-training
    is self-supervised, while we typically use the term fine-tuning when we use supervised
    datasets. Continual pre-training uses the same (but not necessarily) learning
    objective as the one used in the original pre-training setup. Finally, continual
    pre-training datasets are usually orders of magnitude larger than typical fine-tuning
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-1](#continual-pt) depicts the general continual pre-training process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![continual-pt](assets/dllm_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Illustration of the continual pre-training process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This book’s [GitHub repo](https://oreil.ly/llm-playbooks) contains a tutorial
    for continual pre-training. This setup is no different than fine-tuning, except
    that the dataset is not labeled (self-supervised training), and the dataset is
    orders of magnitude larger than typical fine-tuning datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, naive continual pre-training leads to catastrophic forgetting
    of capabilities and knowledge learned previously. Several techniques exist to
    alleviate this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: Replay (memory)
  prefs: []
  type: TYPE_NORMAL
- en: Uses training examples from the original pre-training and mixes them with the
    new training data.
  prefs: []
  type: TYPE_NORMAL
- en: Distillation
  prefs: []
  type: TYPE_NORMAL
- en: Takes an older checkpoint of the model and during training compares the KL-divergence
    between the older and the current representations and penalizes it.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs: []
  type: TYPE_NORMAL
- en: Penalizes large changes to the parameters during continual training.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter expansion
  prefs: []
  type: TYPE_NORMAL
- en: Adds more parameters to the model as continual pre-training is performed. This
    can be done by increasing either the width or the depth of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For a more comprehensive set of continual learning techniques, check out [Jin
    et al.’s paper](https://oreil.ly/yNa-H). In this chapter, we will dive deeper
    into replay and parameter expansion methods.
  prefs: []
  type: TYPE_NORMAL
- en: Replay (Memory)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Replay-based techniques are one of the simplest techniques to alleviate catastrophic
    forgetting. In this approach, we store pre-training examples from the original
    dataset and interleave them with the continual training dataset. Thus, the data
    drift is not so pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following formula has worked very well for me: sample from different subsets
    of the original pre-training datasets and mix them with the continual training
    dataset. At the start of training, let the proportion of new data be around 25%.
    Over training steps, this can be slowly increased up to a maximum proportion,
    like 80%.'
  prefs: []
  type: TYPE_NORMAL
- en: If the original pre-training dataset is a monolith and not made up of several
    smaller datasets, you might need to identify domains yourself so that all domains
    in the original pre-training set are included.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter Expansion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to the replay approach is to use parameter expansion techniques.
    The naive way would be to just add a new layer or two on top of the model and
    train only those parameters during continual pre-training. You can also insert
    and train domain-specific parameter modules (called adapters) within existing
    layers. We will discuss adapter-based approaches in [“Parameter-Efficient Fine-Tuning”](#parameter-efficient-fine-tuning).
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, continual pre-training can also be used to facilitate
    life-long learning, with the model continually being updated with new facts and
    knowledge. However, currently this may not be the most effective paradigm for
    new knowledge learning. You are probably better off using RAG for that. We will
    explore RAG in more detail in [Chapter 12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Task-adaptive pre-training (TAPT)](https://oreil.ly/H38wF) is a useful supplement
    to domain-adaptive pre-training. TAPT involves continual pre-training of the LLM
    on a much smaller but more task-specific unsupervised dataset. To prevent catastrophic
    forgetting, you should perform DAPT first before TAPT, and then subsequently perform
    any supervised fine-tuning on your downstream tasks. Unsupervised data for TAPT
    can be selected using similar methods as that used for DAPT: by constructing embeddings
    of data and selecting data that is clustered with gold-truth sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, continual pre-training can be very effective in cases where you
    have a large body of domain-specific text and the domain is very distinctly characterized
    by a specialized linguistic structure or vocabulary. Continual pre-training can
    also be used to help adapt the LLM to a new language.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Domain-specific text can contain jargon specific to that domain. One strategy
    that has worked well for me is to add extra tokens to represent domain-specific
    jargon.
  prefs: []
  type: TYPE_NORMAL
- en: Continual pre-training can take a lot of computational resources. Fine-tuning
    on smaller datasets takes substantially less resources. However, in the era of
    large language models, it is imperative to do all we can to reduce compute and
    memory requirements. Therefore, let’s next discuss some parameter-efficient fine-tuning
    techniques that make the fine-tuning process more accessible in resource-constrained
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter-Efficient Fine-Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In PEFT, instead of updating all the parameters of the model, we update only
    a small number of parameters. This can vastly bring down compute and storage requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can categorize current PEFT techniques into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding new parameters
  prefs: []
  type: TYPE_NORMAL
- en: This involves adding some extra parameters to the LLM and training only them.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a subset of parameters
  prefs: []
  type: TYPE_NORMAL
- en: This involves choosing to update only a small subset of parameters of the LLM,
    either by selecting the subset apriori or by learning the appropriate subset.
  prefs: []
  type: TYPE_NORMAL
- en: Low-rank methods
  prefs: []
  type: TYPE_NORMAL
- en: This involves using methods that reduce the number of parameters to train by
    finding a smaller matrix containing almost the same information as a larger matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now go through each of these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Adding New Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps your work needs you to fine-tune models for a large number of tasks.
    Or maybe you need to drive personalization by fine-tuning a model for each user.
    It will be cumbersome to maintain and deploy so many full copies of fine-tuned
    models.
  prefs: []
  type: TYPE_NORMAL
- en: One way to avoid updating all the parameters of the model is to add a few extra
    parameters to the model and train only them. Instead of storing and deploying
    full copies of each fine-tuned model, you store only the newly added parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common ways of adding new parameters for fine-tuning include:'
  prefs: []
  type: TYPE_NORMAL
- en: Bottleneck adapters
  prefs: []
  type: TYPE_NORMAL
- en: These are lightweight modules added to the Transformer layers.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix tuning
  prefs: []
  type: TYPE_NORMAL
- en: These are task-specific vectors that are trained and prefixed to the input.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt tuning (soft prompts)
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to prefix tuning but with a simplified training approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss each of these techniques in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Bottleneck adapters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adapters are parameter modules attached to the LLM architecture. Adapters can
    be integrated into the LLM architecture in a variety of ways, but in Transformers,
    the common way is to insert them at each layer of the Transformer. To reduce the
    number of parameters, the width of the adapter module should be much less than
    the width of the underlying Transformer model. This constitutes a *down-projection*,
    also called a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a bottleneck adapter sublayer consists of a down-projection matrix,
    an up-projection matrix at the end to project back to the original dimensions,
    and parameters that can be configured in a variety of ways in the middle. During
    fine-tuning, only the adapter modules are updated. The original pre-trained model
    is not updated. Adapters are initialized with a near-identity initialization to
    ensure smooth training.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-2](#adapters) shows where in the Transformer architecture the bottleneck
    adapters typically are inserted. Note that this is just one possible configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: '![adapters](assets/dllm_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Adapter modules in the Transformer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How does this all work in practice? The [*adapters* library](https://oreil.ly/z05rI)
    comes in handy to facilitate fine-tuning LLMs using these advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how you can start using bottleneck adapters using the adapters library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`DoubleSeqBnConfig` refers to a config natively supported by the library, corresponding
    to the adapter architecture shown in [Figure 7-2](#adapters). But as I mentioned
    before, you can change the size and shape of the adapters as you wish. To do that,
    we need to use `BnConfig`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what these arguments stand for:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mh_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: Refers to the adapter modules added right after the multi-head attention sublayer
    of the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '`output_adapter`'
  prefs: []
  type: TYPE_NORMAL
- en: Refers to the adapter modules added right after the feedforward network sublayer
    of the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '`reduction_factor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refers to the down-projection factor: by how much should the adapter width
    be scaled down compared to the Transformer layer width?'
  prefs: []
  type: TYPE_NORMAL
- en: '`non_linearity`'
  prefs: []
  type: TYPE_NORMAL
- en: Refers to the activation function being used, like RELU or GELU.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the adapters library [documentation](https://oreil.ly/n1Pga) for more
    configuration options. There are so many configuration options available!
  prefs: []
  type: TYPE_NORMAL
- en: While using bottleneck adapters leads to a vast decrease in fine-tuning time
    and complexity, adding parameters across all layers of the Transformer increases
    inference latency by a small amount. Typically, the inference time using commonly
    used adapter configurations is expected to increase by 6%–8%.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is possible to reduce the inference latency by dropping some adapter layers
    during inference. [Rücklé et al. propose AdapterDrop](https://oreil.ly/GM_1X),
    a set of methods for dropping adapter modules during training and inference. They
    propose dropping adapters from the first few layers of the Transformer during
    inference or pruning the adapters from each layer that is the least activated.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One drawback of using adapter-based fine-tuning techniques is that during inference,
    each batch can support only a single adapter instance, i.e., an adapter fine-tuned
    for a particular task. Prefix-tuning, in contrast, enables multiple tasks to be
    run in the same batch.
  prefs: []
  type: TYPE_NORMAL
- en: In prefix-tuning, we add and train task-specific vectors to the prefix of the
    input. This vastly reduces the number of parameters we need to fine-tune. Recall
    that the prompt contains the instruction, the input, and optionally some few-shot
    examples. The text generated by the LLM is conditioned on the output generated
    so far, and the prompt. To this, we add additional context that the LLM can attend
    to, in the form of these prefix vectors. The new tokens prefixed to the input
    are called **virtual tokens** or **soft prompts**.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-3](#prefix-tuning) shows how prefix-tuning occurs in the Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![prefix-tuning](assets/dllm_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Prefix-tuning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As the figure shows, prefix parameters are added at each layer.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix-tuning is much more parameter-efficient than bottleneck adapters, taking
    up only 0.1% or less of a model’s parameters, as compared to adapters where it
    is usually 2% or more. However, prefix-tuning is harder to train effectively than
    adapters. Prefix-tuning also reduces the sequence length of the model in order
    to accommodate the virtual tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to adapters, initialization is very important for prefix-tuning. The
    virtual tokens can be initialized by choosing words that are related to the task
    the model is being fine-tuned for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the adapters library, we can implement prefix-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Prompt tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt tuning is a simplified version of prefix-tuning. Unlike prefix tuning,
    there are no prefix parameters at each layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-4](#prompt-tuning) shows how prompt-tuning occurs in the Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![prompt-tuning](assets/dllm_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Prompt-tuning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The adapters library provides a built-in configuration for prompt tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Some relevant configuration parameters for prompt tuning include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt_length`'
  prefs: []
  type: TYPE_NORMAL
- en: The length of the prompt tokens; 10–30 is a good start.
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt_init`'
  prefs: []
  type: TYPE_NORMAL
- en: The method for initializing these tokens. They can be initialized either through
    the embedding of a string or by a random uniform initialization.
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt_init_text`'
  prefs: []
  type: TYPE_NORMAL
- en: If the soft prompt is initialized by string, the text that is used to initialize
    it. This can be a descriptor of the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '[Lester et al.](https://oreil.ly/BPpRu), who introduced prompt-tuning, also
    leverage it to perform soft prompt ensembling. For soft prompt ensembling, you
    train several soft prompts for each task. Then, for a given input, you use each
    of them as a prefix separately and generate the output. You can then use majority
    voting to select the correct output among the generated ones.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen techniques where new parameters are added to the model
    for fine-tuning. However, we can implement PEFT by fine-tuning only a small subset
    of parameters of the model without having to add new parameters. Let’s explore
    these methods next.
  prefs: []
  type: TYPE_NORMAL
- en: Subset Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A naive way of choosing a subset of parameters to fine-tune on would be to fine-tune
    only the upper layers of the Transformer and keep everything else frozen. The
    lower layers of the Transformer are known to be specialized in more fundamental
    aspects of language like syntax, which we want the LLM to preserve.
  prefs: []
  type: TYPE_NORMAL
- en: Another way is to fine-tune only the bias terms (discussed in [Chapter 2](ch02.html#ch02))
    of the Transformer. This was proposed by [Zaken et al.](https://oreil.ly/SaWoe),
    who show that you can gain almost the same level of performance as that of fully
    fine-tuning a model by just fine-tuning on the bias terms. The authors observed
    that this technique is mostly effective when your training data is limited.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, as we have seen here, there are tradeoffs involved in selecting
    each of these fine-tuning approaches. The ML community is working on developing
    best practices around this area. In the meanwhile, experimentation is key!
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s look at another way to update the parameters of an LLM: by merging
    it with the parameters of another LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining Multiple Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have access to multiple LLMs, each of them overlapping in terms of capabilities
    yet possessing certain unique characteristics, you want to leverage the capabilities
    of all the models in your downstream tasks in some way. This can be done by a
    variety of means, including model ensembling and model fusion or merging. This
    area of LLMs is in its infancy, and more work remains to be done to reap its full
    benefits. I call it the dark arts of NLP because the theoretical underpinnings
    of these techniques remain poorly understood. However, I do believe that even
    with these caveats it merits inclusion in this book, because the practical benefits
    are already visible. Let’s explore a few of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Model Ensembling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different LLMs may possess different but complementary capabilities, a byproduct
    of the difference in their training regimens, training hyperparameters, etc. This
    is especially true when it comes to open source LLMs, where we have a plethora
    of models, most of them being trained on largely overlapping datasets, performing
    very closely to each other in benchmark evaluation metrics. Thus, an ensembling
    approach might bring forth benefits by allowing complementary capabilities from
    multiple models to be leveraged to generate better outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#chapter_utilizing_llms), we discussed how, for generative
    tasks, it is useful to generate multiple outputs for the same input and select
    the best one using heuristics. We can extend this principle to multiple models.
    Each input is passed through *n* models. Optionally, an initial step can choose
    the top k models with the most high-quality or relevant outputs. The outputs from
    these models can be combined and fed through a model (which can be an LLM) to
    generate the final output.
  prefs: []
  type: TYPE_NORMAL
- en: '[Jiang et al.](https://oreil.ly/Sipzu) present a framework called LLM-Blender
    for enabling LLM ensembling. The framework consists of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: PairRanker scores the output from two models, thus choosing a winner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenFuser takes as input the output from k different models to generate the final
    output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 7-5](#LLMBlender) shows the workings of the LLM-Blender framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '![LLMBlender](assets/dllm_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. LLM-Blender
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s dig deeper into each of these modules.
  prefs: []
  type: TYPE_NORMAL
- en: PairRanker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider you have access to *n* different models. For a given input, you feed
    the input to each of these models to generate the outputs. Now, for each pair
    of outputs, you can combine them with the input and feed them to the PairRanker
    module. The PairRanker module is trained to provide scores for each of the outputs.
    If you end up feeding all the pairs of outputs to the PairRanker module, you will
    then find the output (model) with the highest score. This output can then be taken
    as the final output.
  prefs: []
  type: TYPE_NORMAL
- en: However, this just selects the best output and doesn’t necessarily combine the
    capabilities of the different models. For that, the LLM-Blender framework consists
    of a module called GenFuser.
  prefs: []
  type: TYPE_NORMAL
- en: GenFuser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For GenFuser, we take the top k results from the PairRanker scores. We then
    feed them together to the GenFuser, which generates the final output. The GenFuser
    in practice is just a fine-tuned LLM that is tuned to accept several candidate
    inputs and generate an output that combines the characteristics of the different
    candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how this works in practice. We can use the [LLM-Blender library](https://oreil.ly/F2IcX):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Given an input and a list of `candidate_outputs` from *n* different language
    models, we rank the outputs using the PairRanker and then select the top-k ranked
    outputs and fuse them to generate the final output.
  prefs: []
  type: TYPE_NORMAL
- en: While ensembling methods can be effective, there is a lot of recent interest
    in model fusion techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Model Fusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this approach, we combine the parameters of multiple models in some way.
    The idea is that by combining the parameters of multiple models, we might be able
    to benefit from all the complementary capabilities possessed by each of the individual
    models, within a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the common methods used in model fusion are:'
  prefs: []
  type: TYPE_NORMAL
- en: Averaging
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to combine multiple models is to average their parameters.
    Simple averaging has been shown to be quite effective.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted averaging
  prefs: []
  type: TYPE_NORMAL
- en: During averaging, certain models or even certain layers in models can be weighted
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation
  prefs: []
  type: TYPE_NORMAL
- en: 'Each model can be weighted by a factor w1, w2,…wn, with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: where p1, p2, p3…​pn are the parameters of models m1, m2, m3…mn.
  prefs: []
  type: TYPE_NORMAL
- en: One of the benefits in merging multiple models is model reuse. Say you have
    a base LLM at your organization. It is used by people all across the organization,
    who take the model and fine-tune it on their own tasks. They then upload the fine-tuned
    models back. You can then merge the weights of all the models, resulting in a
    stronger pre-trained model. This model can then be used as a new version of the
    base model. This process has been coined Collaborative Descent (ColD) Fusion by
    [Don-Yehiya et al.](https://oreil.ly/LTcdf)
  prefs: []
  type: TYPE_NORMAL
- en: Why would we want to do this? The idea is that if we want to fine-tune an LLM
    on a dataset, it would be nice to have a good starting point such that the training
    is optimal. The hypothesis is that if we already fine-tuned the LLM on another
    task, the fine-tuned LLM is a better starting point than the base LLM. This is
    called intertraining. This too is a fairly new concept, so proceed with caution.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of merging all the parameters of the model, you can merge only a small
    portion of them. In fact, we could just merge the adapter modules.
  prefs: []
  type: TYPE_NORMAL
- en: Adapter Merging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in the chapter, we learned about adapters, which can be used for a variety
    of purposes including domain-adaptive pre-training. While you can train different
    adapters for different domains, the question remains on how you would treat new
    domains seen at inference time. One solution would be to average the adapters
    related to the closest domains and use that for novel domains. This has been shown
    to work well, by [Chronopoulou et al.’s AdapterSoup framework](https://oreil.ly/mKoZ1).
  prefs: []
  type: TYPE_NORMAL
- en: Another way to combine adapter parameters is in the context of an MoE framework,
    introduced in [Chapter 4](ch04.html#chapter_transformer-architecture). Recall
    that in a mixture-of-experts model, the routing function determines which expert(s)
    will handle the input. [Wang et al.’s AdaMix framework](https://oreil.ly/pc7Js)
    extends this to adapter modules. Instead of learning only one adapter module per
    layer, we learn multiple expert modules. During inference, all the adaptation
    layers are merged.
  prefs: []
  type: TYPE_NORMAL
- en: Model merging is a fascinating subarea of LLMs. Even if you are not using it
    in your applications, I highly recommend experimenting with it because it doubles
    as a really neat tool to understand the working of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned a plethora of advanced fine-tuning techniques, including
    continual pre-training strategies like experience replay and parameter expansion;
    parameter-efficient fine-tuning techniques like bottleneck adapters, prefix tuning,
    prompt tuning, and subset selection; and various types of model merging and ensembling.
    We also learned the various motivations for updating model weights and the suitability
    of different methods for each of those situations.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous and current chapter, fine-tuning is not a panacea
    and cannot learn new capabilities or necessarily digest new knowledge. In the
    next chapter, we will discuss limitations of LLMs like poor steerability, hallucinations,
    and reasoning issues, along with techniques for mitigating them.
  prefs: []
  type: TYPE_NORMAL
