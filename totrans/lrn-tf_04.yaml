- en: Chapter 4\. Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。卷积神经网络
- en: In this chapter we introduce convolutional neural networks (CNNs) and the building
    blocks and methods associated with them. We start with a simple model for classification
    of the MNIST dataset, then we introduce the CIFAR10 object-recognition dataset
    and apply several CNN models to it. While small and fast, the CNNs presented in
    this chapter are highly representative of the type of models used in practice
    to obtain state-of-the-art results in object-recognition tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍卷积神经网络（CNNs）以及与之相关的构建块和方法。我们从对MNIST数据集进行分类的简单模型开始，然后介绍CIFAR10对象识别数据集，并将几个CNN模型应用于其中。尽管小巧快速，但本章介绍的CNN在实践中被广泛使用，以获得物体识别任务中的最新结果。
- en: Introduction to CNNs
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN简介
- en: Convolutional neural networks have gained a special status over the last few
    years as an especially promising form of deep learning. Rooted in image processing,
    convolutional layers have found their way into virtually all subfields of deep
    learning, and are very successful for the most part.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，卷积神经网络作为一种特别有前途的深度学习形式获得了特殊地位。根植于图像处理，卷积层已经在几乎所有深度学习的子领域中找到了应用，并且在大多数情况下非常成功。
- en: The fundamental difference between *fully connected* and *convolutional* neural
    networks is the pattern of connections between consecutive layers. In the fully
    connected case, as the name might suggest, each unit is connected to all of the
    units in the previous layer. We saw an example of this in [Chapter 2](ch02.html#go_with_the_flow),
    where the 10 output units were connected to all of the input image pixels.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*全连接*和*卷积*神经网络之间的根本区别在于连续层之间连接的模式。在全连接的情况下，正如名称所示，每个单元都连接到前一层的所有单元。我们在[第2章](ch02.html#go_with_the_flow)中看到了一个例子，其中10个输出单元连接到所有输入图像像素。'
- en: In a convolutional layer of a neural network, on the other hand, each unit is
    connected to a (typically small) number of nearby units in the previous layer.
    Furthermore, all units are connected to the previous layer in the same way, with
    the exact same weights and structure. This leads to an operation known as *convolution*,
    giving the architecture its name (see [Figure 4-1](#fully_connected_and_convolutional_layers)
    for an illustration of this idea). In the next section, we go into the convolution
    operation in some more detail, but in a nutshell all it means for us is applying
    a small “window” of weights (also known as *filters*) across an image, as illustrated
    in [Figure 4-2](#convolutional_filter_applied_across_image) later.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在神经网络的卷积层中，每个单元连接到前一层中附近的（通常很少）几个单元。此外，所有单元以相同的方式连接到前一层，具有相同的权重和结构。这导致了一种称为*卷积*的操作，给这种架构命名（请参见[图4-1](#fully_connected_and_convolutional_layers)以了解这个想法的示例）。在下一节中，我们将更详细地介绍卷积操作，但简而言之，对我们来说，这意味着在图像上应用一小部分“窗口”权重（也称为*滤波器*），如稍后的[图4-2](#convolutional_filter_applied_across_image)所示。
- en: '![](assets/letf_0401.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0401.png)'
- en: Figure 4-1\. In a fully connected layer (left), each unit is connected to all
    units of the previous layers. In a convolutional layer (right), each unit is connected
    to a constant number of units in a local region of the previous layer. Furthermore,
    in a convolutional layer, the units all share the weights for these connections,
    as indicated by the shared linetypes.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。在全连接层（左侧），每个单元都连接到前一层的所有单元。在卷积层（右侧），每个单元连接到前一层的一个局部区域中的固定数量的单元。此外，在卷积层中，所有单元共享这些连接的权重，如共享的线型所示。
- en: There are motivations commonly cited as leading to the CNN approach, coming
    from different schools of thought. The first angle is the so-called neuroscientific
    inspiration behind the model. The second deals with insight into the nature of
    images, and the third relates to learning theory. We will go over each of these
    shortly before diving into the actual mechanics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些常常被引用为导致CNN方法的动机，来自不同的思想流派。第一个角度是所谓的模型背后的神经科学启发。第二个涉及对图像性质的洞察，第三个与学习理论有关。在我们深入了解实际机制之前，我们将简要介绍这些内容。
- en: It has been popular to describe neural networks in general, and specifically
    convolutional neural networks, as biologically inspired models of computation.
    At times, claims go as far as to state that these *mimic the way the brain performs
    computations. *While misleading when taken at face value, the biological analogy
    is of some interest.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常将神经网络总体描述为计算的生物学启发模型，特别是卷积神经网络。有时，有人声称这些模型“模仿大脑执行计算的方式”。尽管直接理解时会产生误导，但生物类比具有一定的兴趣。
- en: The Nobel Prize–winning neurophysiologists Hubel and Wiesel discovered as early
    as the 1960s that the first stages of visual processing in the brain consist of
    application of the same local filter (e.g., edge detectors) to all parts of the
    visual field. The current understanding in the neuroscientific community is that
    as visual processing proceeds, information is integrated from increasingly wider
    parts of the input, and this is done hierarchically.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 诺贝尔奖获得者神经生理学家Hubel和Wiesel早在1960年代就发现，大脑中视觉处理的第一阶段包括将相同的局部滤波器（例如，边缘检测器）应用于视野的所有部分。神经科学界目前的理解是，随着视觉处理的进行，信息从输入的越来越广泛的部分集成，这是按层次进行的。
- en: Convolutional neural networks follow the same pattern. Each convolutional layer
    looks at an increasingly larger part of the image as we go deeper into the network.
    Most commonly, this will be followed by fully connected layers that in the biologically
    inspired analogy act as the higher levels of visual processing dealing with global
    information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络遵循相同的模式。随着我们深入网络，每个卷积层查看图像的越来越大的部分。最常见的情况是，这将被全连接层跟随，这些全连接层在生物启发的类比中充当处理全局信息的更高级别的视觉处理层。
- en: The second angle, more hard fact engineering–oriented, stems from the nature
    of images and their contents. When looking for an object in an image, say the
    face of a cat, we would typically want to be able to detect it regardless of its
    position in the image. This reflects the property of natural images that the same
    content may be found in different locations of an image. This is property is known
    as an *invariance*—invariances of this sort can also be expected with respect
    to (small) rotations, changing lighting conditions, etc.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个角度，更加注重硬性事实工程方面，源于图像及其内容的性质。当在图像中寻找一个对象，比如一只猫的脸时，我们通常希望能够无论其在图像中的位置如何都能检测到它。这反映了自然图像的性质，即相同的内容可能在图像的不同位置找到。这种性质被称为*不变性*——这种类型的不变性也可以预期在（小）旋转、光照变化等方面存在。
- en: Correspondingly, when building an object-recognition system, it should be invariant
    to translation (and, depending on the scenario, probably also rotation and deformations
    of many sorts, but that is another matter). Put simply, it therefore makes sense
    to perform the same exact computation on different parts of the image. In this
    view, a convolutional neural network layer computes the same features of an image,
    across all spatial areas.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在构建一个对象识别系统时，应该对平移具有不变性（并且，根据情况，可能还对旋转和各种变形具有不变性，但这是另一回事）。简而言之，因此在图像的不同部分执行完全相同的计算是有意义的。从这个角度来看，卷积神经网络层在所有空间区域上计算图像的相同特征。
- en: Finally, the convolutional structure can be seen as a regularization mechanism.
    In this view, convolutional layers are like fully connected layers, but instead
    of searching for weights in the full space of matrices (of certain size), we limit
    the search to matrices describing fixed-size convolutions, reducing the number
    of degrees of freedom to the size of the convolution, which is typically very
    small.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，卷积结构可以被看作是一种正则化机制。从这个角度来看，卷积层就像全连接层，但是我们不是在完整的矩阵空间中寻找权重，而是将搜索限制在描述固定大小卷积的矩阵中，将自由度的数量减少到卷积的大小，这通常非常小。
- en: Regularization
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正则化
- en: The term *regularization* is used throughout this book. In machine learning
    and statistics, regularization is mostly used to refer to the restriction of an
    optimization problem by imposing a penalty on the complexity of the solution,
    in the attempt to prevent overfitting to the given examples.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*正则化*在本书中被广泛使用。在机器学习和统计学中，正则化主要用于指的是通过对解的复杂性施加惩罚来限制优化问题，以防止对给定示例的过度拟合。
- en: Overfitting occurs when a rule (for instance, a classifier) is computed in a
    way that explains the training set, but with poor generalization to unseen data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合发生在规则（例如，分类器）以解释训练集的方式计算时，但对未见数据的泛化能力较差。
- en: Regularization is most often applied by adding implicit information regarding
    the desired results (this could take the form of saying we would rather have a
    smoother function, when searching a function space). In the convolutional neural
    network case, we explicitly state that we are looking for weights in a relatively
    low-dimensional subspace corresponding to fixed-size convolutions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化通常通过添加关于期望结果的隐式信息来实现（这可能采取的形式是说在搜索函数空间时我们更希望有一个更平滑的函数）。在卷积神经网络的情况下，我们明确表示我们正在寻找相对低维子空间中的权重，这些权重对应于固定大小的卷积。
- en: In this chapter we cover the types of layers and operations associated with
    convolutional neural networks. We start by revisiting the MNIST dataset, this
    time applying a model with approximately 99% accuracy. Next, we move on to the
    more interesting object recognition CIFAR10 dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了与卷积神经网络相关的层和操作类型。我们首先重新审视MNIST数据集，这次应用一个准确率约为99%的模型。接下来，我们将转向更有趣的对象识别CIFAR10数据集。
- en: 'MNIST: Take II'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST：第二次
- en: In this section we take a second look at the MNIST dataset, this time applying
    a small convolutional neural network as our classifier. Before doing so, there
    are several elements and operations that we must get acquainted with.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们再次查看MNIST数据集，这次将一个小型卷积神经网络应用作为我们的分类器。在这样做之前，有几个元素和操作我们必须熟悉。
- en: Convolution
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积
- en: 'The convolution operation, as you probably expect from the name of the architecture,
     is the fundamental means by which layers are connected in convolutional neural
    networks. We use the built-in TensorFlow `conv2d()`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作，正如你可能从架构的名称中期待的那样，是卷积神经网络中连接层的基本手段。我们使用内置的TensorFlow `conv2d()`：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `x` is the data—the input image, or a downstream feature map obtained
    further along in the network, after applying previous convolution layers. As discussed
    previously, in typical CNN models we stack convolutional layers hierarchically,
    and *feature map* is simply a commonly used term referring to the output of each
    such layer. Another way to view the output of these layers is as *processed images*,
    the result of applying a filter and perhaps some other operations. Here, this
    filter is parameterized by `W`, the learned weights of our network representing
    the convolution filter. This is just the set of weights in the small “sliding
    window” we see in [Figure 4-2](#convolutional_filter_applied_across_image).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`x`是数据——输入图像，或者是在网络中进一步应用之前的卷积层后获得的下游特征图。正如之前讨论的，在典型的CNN模型中，我们按层次堆叠卷积层，并且*特征图*只是一个常用术语，指的是每个这样的层的输出。查看这些层的输出的另一种方式是*处理后的图像*，是应用滤波器和其他操作的结果。在这里，这个滤波器由`W`参数化，表示我们网络中学习的卷积滤波器的权重。这只是我们在[图4-2](#convolutional_filter_applied_across_image)中看到的小“滑动窗口”中的一组权重。
- en: '![](assets/letf_0402.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0402.png)'
- en: Figure 4-2\. The same convolutional filter—a “sliding window”—applied across
    an image.
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2。相同的卷积滤波器——一个“滑动窗口”——应用于图像之上。
- en: 'The output of this operation will depend on the shape of `x` and `W`, and in
    our case is four-dimensional. The image data `x` will be of shape:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作的输出将取决于`x`和`W`的形状，在我们的情况下是四维的。图像数据`x`的形状将是：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'meaning that we have an unknown number of images, each 28×28 pixels and with
    one color channel (since these are grayscale images). The weights `W` we use will
    be of shape:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们有未知数量的图像，每个图像为28×28像素，具有一个颜色通道（因为这些是灰度图像）。我们使用的权重`W`的形状将是：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: where the initial 5×5×1 represents the size of the small “window” in the image
    to be convolved, in our case a 5×5 region. In images that have multiple color
    channels (RGB, as briefly discussed in [Chapter 1](ch01.html#ch01)), we regard
    each image as a three-dimensional tensor of RGB values, but in this one-channel
    data they are just two-dimensional, and convolutional filters are applied to two-dimensional
    regions. Later, when we tackle the CIFAR10 data, we’ll see examples of multiple-channel
    images and how to set the size of weights `W` accordingly.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的5×5×1表示在图像中要进行卷积的小“窗口”的大小，在我们的情况下是一个5×5的区域。在具有多个颜色通道的图像中（RGB，如[第1章](ch01.html#ch01)中简要讨论的），我们将每个图像视为RGB值的三维张量，但在这个单通道数据中它们只是二维的，卷积滤波器应用于二维区域。稍后，当我们处理CIFAR10数据时，我们将看到多通道图像的示例以及如何相应地设置权重`W`的大小。
- en: The final 32 is the number of feature maps**.** In other words, we have multiple
    sets of weights for the convolutional layer—in this case, 32 of them. Recall that
    the idea of a convolutional layer is to compute the same feature along the image;
    we would simply like to compute many such features and thus use multiple sets
    of convolutional filters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的32是特征图的数量。换句话说，我们有卷积层的多组权重——在这种情况下有32组。回想一下，卷积层的概念是沿着图像计算相同的特征；我们希望计算许多这样的特征，因此使用多组卷积滤波器。
- en: The `strides` argument controls the spatial movement of the filter `W` across
    the image (or feature map) `x`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`strides`参数控制滤波器`W`在图像（或特征图）`x`上的空间移动。'
- en: The value `[1, 1, 1, 1]` means that the filter is applied to the input in one-pixel
    intervals in each dimension, corresponding to a “full” convolution. Other settings
    of this argument allow us to introduce skips in the application of the filter—a
    common practice that we apply later—thus making the resulting feature map smaller.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 值`[1, 1, 1, 1]`表示滤波器在每个维度上以一个像素间隔应用于输入，对应于“全”卷积。此参数的其他设置允许我们在应用滤波器时引入跳跃—这是我们稍后会应用的常见做法—从而使得生成的特征图更小。
- en: Finally, setting `padding` to `'SAME'` means that the borders of `x` are padded
    such that the size of the result of the operation is the same as the size of `x`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将`padding`设置为`'SAME'`意味着填充`x`的边界，使得操作的结果大小与`x`的大小相同。
- en: Activation functions
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: Following linear layers, whether convolutional or fully connected, it is common
    practice to apply nonlinear *activation functions* (see [Figure 4-3](#common_activation_functions)
    for some examples). One practical aspect of activation functions is that consecutive
    linear operations can be replaced by a single one, and thus depth doesn’t contribute
    to the expressiveness of the model unless we use nonlinear activations between
    the linear layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性层之后，无论是卷积还是全连接，常见的做法是应用非线性*激活函数*（参见[图4-3](#common_activation_functions)中的一些示例）。激活函数的一个实际方面是，连续的线性操作可以被单个操作替代，因此深度不会为模型的表达能力做出贡献，除非我们在线性层之间使用非线性激活。
- en: '![](assets/letf_0403.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_0403.png)'
- en: 'Figure 4-3\. Common activation functions: logistic (left), hyperbolic tangent
    (center), and rectifying linear unit (right)'
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3。常见的激活函数：逻辑函数（左）、双曲正切函数（中）、修正线性单元（右）
- en: Pooling
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化
- en: It is common to follow convolutional layers with pooling of outputs. Technically,
    *pooling* means reducing the size of the data with some local aggregation function,
    typically within each feature map.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层后跟随输出的池化是常见的。技术上，*池化*意味着使用某种本地聚合函数减少数据的大小，通常在每个特征图内部。
- en: The reasoning behind this is both technical and more theoretical. The technical
    aspect is that pooling reduces the size of the data to be processed downstream.
    This can drastically reduce the number of overall parameters in the model, especially
    if we use fully connected layers after the convolutional ones.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这背后的原因既是技术性的，也是更理论性的。技术方面是，池化会减少下游处理的数据量。这可以极大地减少模型中的总参数数量，特别是在卷积层之后使用全连接层的情况下。
- en: The more theoretical reason for applying pooling is that we would like our computed
    features not to care about small changes in position in an image. For instance,
    a feature looking for eyes in the top-right part of an image should not change
    too much if we move the camera a bit to the right when taking the picture, moving
    the eyes slightly to the center of the image. Aggregating the “eye-detector feature”
    spatially allows the model to overcome such spatial variability between images,
    capturing some form of invariance as discussed at the beginning of this chapter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 应用池化的更理论的原因是，我们希望我们计算的特征不受图像中位置的微小变化的影响。例如，一个在图像右上部寻找眼睛的特征，如果我们稍微向右移动相机拍摄图片，将眼睛略微移动到图像中心，这个特征不应该有太大变化。在空间上聚合“眼睛检测器特征”使模型能够克服图像之间的这种空间变化，捕捉本章开头讨论的某种不变性形式。
- en: 'In our example we apply the max pooling operation on 2×2 blocks of each feature
    map:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们对每个特征图的2×2块应用最大池化操作：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Max pooling outputs the maximum of the input in each region of a predefined
    size (here 2×2). The `ksize` argument controls the size of the pooling (2×2),
    and the `strides` argument controls by how much we “slide” the pooling grids across
    `x`, just as in the case of the convolution layer. Setting this to a 2×2 grid
    means that the output of the pooling will be exactly one-half of the height and
    width of the original, and in total one-quarter of the size.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化输出预定义大小的每个区域中的输入的最大值（这里是2×2）。`ksize`参数控制池化的大小（2×2），`strides`参数控制我们在`x`上“滑动”池化网格的幅度，就像在卷积层的情况下一样。将其设置为2×2网格意味着池化的输出将恰好是原始高度和宽度的一半，总共是原始大小的四分之一。
- en: Dropout
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: The final element we will need for our model is *dropout*. This is a regularization
    trick used in order to force the network to distribute the learned representation
    across all the neurons. Dropout “turns off” a random preset fraction of the units
    in a layer, by setting their values to zero during training. These dropped-out
    neurons are random—different for each computation—forcing the network to learn
    a representation that will work even after the dropout. This process is often
    thought of as training an “ensemble” of multiple networks, thereby increasing
    generalization. When using the network as a classifier at test time (“inference”),
    there is no dropout and the full network is used as is.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中最后需要的元素是*dropout*。这是一种正则化技巧，用于强制网络将学习的表示分布到所有神经元中。在训练期间，dropout会“关闭”一定比例的层中的单位，通过将它们的值设置为零。这些被丢弃的神经元是随机的，每次计算都不同，迫使网络学习一个即使在丢失后仍能正常工作的表示。这个过程通常被认为是训练多个网络的“集成”，从而增加泛化能力。在测试时使用网络作为分类器时（“推断”），不会进行dropout，而是直接使用完整的网络。
- en: 'The only argument in our example other than the layer we would like to apply
    dropout to is `keep_prob,` the fraction of the neurons to keep working at each
    step:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，除了我们希望应用dropout的层之外，唯一的参数是`keep_prob`，即每一步保持工作的神经元的比例：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In order to be able to change this value (which we must do, since for testing
    we would like this to be `1.0`, meaning no dropout at all), we will use a `tf.placeholder`
    and pass one value for train (`.5`) and another for test (`1.0`).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够更改此值（我们必须这样做，因为对于测试，我们希望这个值为`1.0`，表示根本没有丢失），我们将使用`tf.placeholder`并传递一个值用于训练（`.5`）和另一个用于测试（`1.0`）。
- en: The Model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: 'First, we define helper functions that will be used extensively throughout
    this chapter to create our layers. Doing this allows the actual model to be short
    and readable (later in the book we will see that there exist several frameworks
    for greater abstraction of deep learning building blocks, which allow us to concentrate
    on rapidly designing our networks rather than the somewhat tedious work of defining
    all the necessary elements). Our helper functions are:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了一些辅助函数，这些函数将在本章中广泛使用，用于创建我们的层。这样做可以使实际模型简短易读（在本书的后面，我们将看到存在几种框架，用于更抽象地定义深度学习构建块，这样我们可以专注于快速设计我们的网络，而不是定义所有必要的元素的繁琐工作）。我们的辅助函数有：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s take a closer look at these:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些：
- en: '`weight_variable()`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`weight_variable()`'
- en: This specifies the weights for either fully connected or convolutional layers
    of the network. They are initialized randomly using a truncated normal distribution
    with a standard deviation of .1. This sort of initialization with a random normal
    distribution that is truncated at the tails is pretty common and generally produces
    good results (see the upcoming note on random initialization).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这指定了网络的全连接或卷积层的权重。它们使用截断正态分布进行随机初始化，标准差为0.1。这种使用截断在尾部的随机正态分布初始化是相当常见的，通常会产生良好的结果（请参见即将介绍的随机初始化的注释）。
- en: '`bias_variable()`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`bias_variable()`'
- en: This defines the bias elements in either a fully connected or a convolutional
    layer. These are all initialized with the constant value of `.1`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了全连接或卷积层中的偏置元素。它们都使用常数值`.1`进行初始化。
- en: '`conv2d()`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`conv2d()`'
- en: This specifies the convolution we will typically use. A full convolution (no
    skips) with an output the same size as the input.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这指定了我们通常会使用的卷积。一个完整的卷积（没有跳过），输出与输入大小相同。
- en: '`max_pool_2×2`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_pool_2×2`'
- en: This sets the max pool to half the size across the height/width dimensions,
    and in total a quarter the size of the feature map.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这将将最大池设置为高度/宽度维度的一半大小，并且总体上是特征图大小的四分之一。
- en: '`conv_layer()`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`conv_layer()`'
- en: This is the actual layer we will use. Linear convolution as defined in `conv2d`,
    with a bias, followed by the ReLU nonlinearity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将使用的实际层。线性卷积如`conv2d`中定义的，带有偏置，然后是ReLU非线性。
- en: '`full_layer()`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`full_layer()`'
- en: A standard full layer with a bias. Notice that here we didn’t add the ReLU.
    This allows us to use the same layer for the final output, where we don’t need
    the nonlinear part.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 带有偏置的标准全连接层。请注意，这里我们没有添加ReLU。这使我们可以在最终输出时使用相同的层，我们不需要非线性部分。
- en: 'With these layers defined, we are ready to set up our model (see the visualization
    in [Figure 4-4](#fig0404)):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这些层后，我们准备设置我们的模型（请参见[图4-4](#fig0404)中的可视化）：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![CNN](assets/letf_0404.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![CNN](assets/letf_0404.png)'
- en: Figure 4-4\. A visualization of the CNN architecture used.
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4. 所使用的CNN架构的可视化。
- en: Random initialization
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机初始化
- en: 'In the previous chapter we discussed initializers of several types, including
    the random initializer used here for our convolutional layer’s weights:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们讨论了几种类型的初始化器，包括此处用于卷积层权重的随机初始化器：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Much has been said about the importance of initialization in the training of
    deep learning models. Put simply, a bad initialization can make the training process
    “get stuck,” or fail completely due to numerical issues. Using random rather than
    constant initializations helps break the symmetry between learned features, allowing
    the model to learn a diverse and rich representation. Using bound values helps,
    among other things, to control the magnitude of the gradients, allowing the network
    to converge more efficiently.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习模型训练中初始化的重要性已经说了很多。简而言之，糟糕的初始化可能会使训练过程“卡住”，或者由于数值问题完全失败。使用随机初始化而不是常数初始化有助于打破学习特征之间的对称性，使模型能够学习多样化和丰富的表示。使用边界值有助于控制梯度的幅度，使网络更有效地收敛，等等。
- en: We start by defining the placeholders for the images and correct labels, `x`
    and `y_`, respectively. Next, we reshape the image data into the 2D image format
    with size 28×28×1\. Recall we did not need this spatial aspect of the data for
    our previous MNIST model, since all pixels were treated independently, but a major
    source of power in the convolutional neural network framework is the utilization
    of this spatial meaning when considering images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为图像和正确标签定义占位符`x`和`y_`。接下来，我们将图像数据重新整形为尺寸为28×28×1的2D图像格式。回想一下，我们在之前的MNIST模型中不需要数据的空间方面，因为所有像素都是独立处理的，但在卷积神经网络框架中，考虑图像时利用这种空间含义是一个重要的优势。
- en: Next we have two consecutive layers of convolution and pooling, each with 5×5
    convolutions and 32 feature maps, followed by a single fully connected layer with
    1,024 units. Before applying the fully connected layer we flatten the image back
    to a single vector form, since the fully connected layer no longer needs the spatial
    aspect.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们有两个连续的卷积和池化层，每个层都有5×5的卷积和32个特征图，然后是一个具有1,024个单元的单个全连接层。在应用全连接层之前，我们将图像展平为单个向量形式，因为全连接层不再需要空间方面。
- en: Notice that the size of the image following the two convolution and pooling
    layers is 7×7×64\. The original 28×28 pixel image is reduced first to 14×14, and
    then to 7×7 in the two pooling operations. The 64 is the number of feature maps
    we created in the second convolutional layer. When considering the total number
    of learned parameters in the model, a large proportion will be in the fully connected
    layer (going from 7×7×64 to 1,024 gives us 3.2 million parameters). This number
    would have been 16 times as large (i.e., 28×28×64×1,024, which is roughly 51 million)
    if we hadn’t used max-pooling.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在两个卷积和池化层之后，图像的尺寸为7×7×64。原始的28×28像素图像首先缩小到14×14，然后在两个池化操作中缩小到7×7。64是我们在第二个卷积层中创建的特征图的数量。在考虑模型中学习参数的总数时，大部分将在全连接层中（从7×7×64到1,024的转换给我们提供了3.2百万个参数）。如果我们没有使用最大池化，这个数字将是原来的16倍（即28×28×64×1,024，大约为51百万）。
- en: Finally, the output is a fully connected layer with 10 units, corresponding
    to the number of labels in the dataset (recall that MNIST is a handwritten digit
    dataset, so the number of possible labels is 10).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出是一个具有10个单元的全连接层，对应数据集中的标签数量（回想一下MNIST是一个手写数字数据集，因此可能的标签数量是10）。
- en: 'The rest is the same as in the first MNIST model in [Chapter 2](ch02.html#go_with_the_flow),
    with a few minor changes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其余部分与第二章中第一个MNIST模型中的内容相同，只有一些细微的变化：
- en: '`train_accuracy`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_accuracy`'
- en: We print the accuracy of the model on the batch used for training every 100
    steps. This is done *before* the training step, and therefore is a good estimate
    of the current performance of the model on the training set.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每100步打印模型在用于训练的批次上的准确率。这是在训练步骤之前完成的，因此是对模型在训练集上当前性能的良好估计。
- en: '`test_accuracy`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_accuracy`'
- en: We split the test procedure into 10 blocks of 1,000 images each. Doing this
    is important mostly for much larger datasets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试过程分为10个包含1,000张图像的块。对于更大的数据集，这样做非常重要。
- en: 'Here’s the complete code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完整的代码：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The performance of this model is already relatively good, with just over 99%
    correct after as little as 5 epochs,^([1](ch04.html#idm139707898624224)) which
    are 5,000 steps with mini-batches of size 50.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的性能已经相当不错，仅经过5个周期，准确率就超过了99%，^([1](ch04.html#idm139707898624224))这相当于5,000步，每个步骤的迷你批次大小为50。
- en: For a list of models that have been used over the years with this dataset, and
    some ideas on how to further improve this result, take a look at [*http://yann.lecun.com/exdb/mnist/*](http://yann.lecun.com/exdb/mnist/).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有关多年来使用该数据集的模型列表以及如何进一步改进结果的一些想法，请查看[*http://yann.lecun.com/exdb/mnist/*](http://yann.lecun.com/exdb/mnist/)。
- en: CIFAR10
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CIFAR10
- en: '*CIFAR10* is another dataset with a long history in computer vision and machine
    learning. Like MNIST, it is a common benchmark that various methods are tested
    against. [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) is a set of 60,000
    color images of size 32×32 pixels, each belonging to one of ten categories: airplane,
    automobile, bird, cat, deer, dog, frog, horse, ship, and truck.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*CIFAR10*是另一个在计算机视觉和机器学习领域有着悠久历史的数据集。与MNIST类似，它是一个常见的基准，各种方法都会被测试。[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)是一个包含60,000张尺寸为32×32像素的彩色图像的数据集，每张图像属于以下十个类别之一：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。'
- en: State-of-the-art deep learning methods for this dataset are as good as humans
    at classifying these images. In this section we start off with much simpler methods
    that will run relatively quickly. Then, we discuss briefly what the gap is between
    these and the state of the art.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这个数据集的最先进的深度学习方法在分类这些图像方面与人类一样出色。在本节中，我们首先使用相对较简单的方法，这些方法将运行相对较快。然后，我们简要讨论这些方法与最先进方法之间的差距。
- en: Loading the CIFAR10 Dataset
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载CIFAR10数据集
- en: In this section we build a data manager for CIFAR10, similarly to the built-in
    `input_data.read_data_sets()` we used for MNIST.^([2](ch04.html#idm139707898293216))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们构建了一个类似于用于MNIST的内置`input_data.read_data_sets()`的CIFAR10数据管理器。^([2](ch04.html#idm139707898293216))
- en: 'First, [download the Python version of the dataset](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)
    and extract the files into a local directory. You should now have the following
    files:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，[下载数据集的Python版本](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)并将文件提取到本地目录中。现在应该有以下文件：
- en: '*data_batch_1*, *data_batch_2*, *data_batch_3*, *data_batch_4*, *data_batch_5*'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*data_batch_1*, *data_batch_2*, *data_batch_3*, *data_batch_4*, *data_batch_5*'
- en: '*test_batch*'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*test_batch*'
- en: '*batches_meta*'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*batches_meta*'
- en: '*readme.html*'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*readme.html*'
- en: The *data_batch_X* files are serialized data files containing the training data,
    and *test_batch* is a similar serialized file containing the test data. The *batches_meta*
    file contains the mapping from numeric to semantic labels. The *.html* file is
    a copy of the CIFAR-10 dataset’s web page.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*data_batch_X*文件是包含训练数据的序列化数据文件，*test_batch*是一个类似的包含测试数据的序列化文件。*batches_meta*文件包含从数字到语义标签的映射。*.html*文件是CIFAR-10数据集网页的副本。'
- en: 'Since this is a relatively small dataset, we load it all into memory:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个相对较小的数据集，我们将所有数据加载到内存中：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'where we use the following utility functions:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们使用以下实用函数：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `unpickle()` function returns a `dict` with fields `data` and `labels`,
    containing the image data and the labels, respectively. `one_hot()` recodes the
    labels from integers (in the range 0 to 9) to vectors of length 10, containing
    all 0s except for a 1 at the position of the label.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`unpickle()`函数返回一个带有`data`和`labels`字段的`dict`，分别包含图像数据和标签。`one_hot()`将标签从整数（范围为0到9）重新编码为长度为10的向量，其中除了标签位置上的1之外，所有位置都是0。'
- en: 'Finally, we create a data manager that includes both the training and test
    data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个包含训练和测试数据的数据管理器：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using Matplotlib, we can now use the data manager in order to display some
    of the CIFAR10 images and get a better idea of what is in this dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Matplotlib，我们现在可以使用数据管理器来显示一些CIFAR10图像，并更好地了解这个数据集中的内容：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Matplotlib
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Matplotlib
- en: '*Matplotlib* is a useful Python library for plotting, designed to look and
    behave like MATLAB plots. It is often the easiest way to quickly plot and visualize
    a dataset.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*Matplotlib*是一个用于绘图的有用的Python库，设计得看起来和行为类似于MATLAB绘图。这通常是快速绘制和可视化数据集的最简单方法。'
- en: The `display_cifar()`function takes as arguments `images` (an iterable containing
    images), and `size` (the number of images we would like to display), and constructs
    and displays a `size×size` grid of images. This is done by concatenating the actual
    images vertically and horizontally to form a large image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`display_cifar()`函数的参数是`images`（包含图像的可迭代对象）和`size`（我们想要显示的图像数量），并构建并显示一个`size×size`的图像网格。这是通过垂直和水平连接实际图像来形成一个大图像。'
- en: 'Before displaying the image grid, we start by printing the sizes of the train/test
    sets. CIFAR10 contains 50K training images and 10K test images:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在显示图像网格之前，我们首先打印训练/测试集的大小。CIFAR10包含50K个训练图像和10K个测试图像：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The image produced and shown in [Figure 4-5](#one_hundred_cifar10_images) is
    meant to give some idea of what CIFAR10 images actually look like. Notably, these
    small, 32×32 pixel images each contain a full single object that is both centered
    and more or less recognizable even at this resolution.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图4-5](#one_hundred_cifar10_images)中生成并显示的图像旨在让人了解CIFAR10图像实际上是什么样子的。值得注意的是，这些小的32×32像素图像每个都包含一个完整的单个对象，该对象位于中心位置，即使在这种分辨率下也基本上是可识别的。
- en: '![CIFAR10](assets/letf_0405.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![CIFAR10](assets/letf_0405.png)'
- en: Figure 4-5\. 100 random CIFAR10 images.
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。100个随机的CIFAR10图像。
- en: Simple CIFAR10 Models
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的CIFAR10模型
- en: 'We will start by using the model that we have previously used successfully
    for the MNIST dataset. Recall that the MNIST dataset is composed of 28×28-pixel
    grayscale images, while the CIFAR10 images are color images with 32×32 pixels.
    This will necessitate minor adaptations to the setup of the computation graph:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从先前成功用于MNIST数据集的模型开始。回想一下，MNIST数据集由28×28像素的灰度图像组成，而CIFAR10图像是带有32×32像素的彩色图像。这将需要对计算图的设置进行轻微调整：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This first attempt will achieve approximately 70% accuracy within a few minutes
    (using a batch size of 100, and depending naturally on hardware and configurations).
    Is this good? As of now, state-of-the-art deep learning methods achieve over 95%
    accuracy on this dataset,^([3](ch04.html#idm139707897326192)) but using much larger
    models and usually many, many hours of training.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一次尝试将在几分钟内达到大约70%的准确度（使用批量大小为100，自然取决于硬件和配置）。这好吗？截至目前，最先进的深度学习方法在这个数据集上实现了超过95%的准确度，但是使用更大的模型并且通常需要许多小时的训练。
- en: 'There are a few differences between this and the similar MNIST model presented
    earlier.  First, the input consists of images of size 32×32×3, the third dimension
    being the three color channels:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前介绍的类似MNIST模型之间存在一些差异。首先，输入由大小为32×32×3的图像组成，第三维是三个颜色通道：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Similarly, after the two pooling operations, we are left this time with 64
    feature maps of size 8×8:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在两次池化操作之后，我们这次剩下的是大小为8×8的64个特征图：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Finally, as a matter of convenience, we group the test procedure into a separate
    function called `test()`, and we do not print training accuracy values (which
    can be added back in using the same code as in the MNIST model).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了方便起见，我们将测试过程分组到一个名为`test()`的单独函数中，并且我们不打印训练准确度值（可以使用与MNIST模型中相同代码添加回来）。
- en: Once we have a model with some acceptable baseline accuracy (whether derived
    from a simple MNIST model or from a state-of-the-art model for some other dataset),
    a common practice is to try to improve it by means of a sequence of adaptations
    and changes, until reaching what is necessary for our purposes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一些可接受的基线准确度的模型（无论是从简单的MNIST模型还是从其他数据集的最先进模型中派生的），一个常见的做法是通过一系列的适应和更改来尝试改进它，直到达到我们的目的所需的内容。
- en: 'In this case, leaving all the rest the same, we will add a third convolution
    layer with 128 feature maps and dropout. We will also reduce the number of units
    in the fully connected layer from 1,024 to 512:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，保持其他所有内容不变，我们将添加一个具有128个特征图和dropout的第三个卷积层。我们还将把完全连接层中的单元数从1,024减少到512：
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This model will take slightly longer to run (but still way under an hour, even
    without sophisticated hardware) and achieve an accuracy of approximately 75%.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型将需要稍长一点的时间来运行（但即使没有复杂的硬件，也不会超过一个小时），并且可以达到大约75%的准确度。
- en: 'There is still a rather large gap between this and the best known methods.
    There are several  independently applicable elements that can help close this
    gap:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然与最佳已知方法之间存在相当大的差距。有几个独立适用的元素可以帮助缩小这个差距：
- en: Model size
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小
- en: Most successful methods for this and similar datasets use much deeper networks
    with many more adjustable parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种数据集和类似数据集，大多数成功的方法使用更深的网络和更多可调参数。
- en: Additional types of layers and methods
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的层和方法
- en: Additional types of popular layers are often used together with the layers presented
    here, such as local response normalization.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通常与这里介绍的层一起使用的是其他类型的流行层，比如局部响应归一化。
- en: Optimization tricks
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 优化技巧
- en: More about this later!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于这个的内容以后再说！
- en: Domain knowledge
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 领域知识
- en: Pre-processing utilizing domain knowledge often goes a long way. In this case
    that would be good old-fashioned image processing.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 利用领域知识进行预处理通常是很有帮助的。在这种情况下，这将是传统的图像处理。
- en: Data augmentation
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强
- en: Adding training data based on the existing set can help. For instance, if an
    image of a dog is flipped horizontally, then it is clearly still an image of a
    dog (but what about a vertical flip?). Small shifts and rotations are also commonly
    used.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 基于现有数据集添加训练数据可能会有所帮助。例如，如果一张狗的图片水平翻转，那么显然仍然是一张狗的图片（但垂直翻转呢？）。小的位移和旋转也经常被使用。
- en: Reusing successful methods and architectures
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 重用成功的方法和架构
- en: As in most engineering fields, starting from a time-proven method and adapting
    it to your needs is often the way to go. In the field of deep learning this is
    often done by fine-tuning pretrained models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 和大多数工程领域一样，从一个经过时间验证的方法开始，并根据自己的需求进行调整通常是正确的方式。在深度学习领域，这经常通过微调预训练模型来实现。
- en: 'The final model we will present in this chapter is a smaller version of the
    type of model that actually produces great results for this dataset. This model
    is still compact and fast, and achieves approximately 83% accuracy after ~150
    epochs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中介绍的最终模型是实际为这个数据集产生出色结果的模型类型的缩小版本。这个模型仍然紧凑快速，在大约150个epochs后达到约83%的准确率：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This model consists of three blocks of convolutional layers, followed by the
    fully connected and output layers we have already seen a few times before.  Each
    block of convolutional layers contains three consecutive convolutional layers,
    followed by a single pooling and dropout.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型由三个卷积层块组成，接着是我们之前已经见过几次的全连接和输出层。每个卷积层块包含三个连续的卷积层，然后是一个池化层和dropout。
- en: The constants `C1`, `C2`, and `C3` control the number of feature maps in each
    layer of each of the convolutional blocks, and the constant `F1` controls the
    number of units in the fully connected layer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 常数`C1`、`C2`和`C3`控制每个卷积块中每个层的特征图数量，常数`F1`控制全连接层中的单元数量。
- en: 'After the third block of convolutional layers, we use an 8×8 max pool layer:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个卷积层之后，我们使用了一个8×8的最大池层：
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Since at this point the feature maps are of size 8×8 (following the first two
    poolings that each reduced the 32×32 pictures by half on each axis), this globally
    pools each of the feature maps and keeps only the maximal value. The number of
    feature maps at the third block was set to 80, so at this point (following the
    max pooling) the representation is reduced to only 80 numbers. This keeps the
    overall size of the model small, as the number of parameters in the transition
    to the fully connected layer is kept down to 80×500.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在这一点上特征图的大小为8×8（在前两个池化层之后，每个轴上都将32×32的图片减半），这样全局池化每个特征图并保留最大值。第三个块的特征图数量设置为80，所以在这一点上（在最大池化之后），表示被减少到只有80个数字。这使得模型的整体大小保持较小，因为在过渡到全连接层时参数的数量保持在80×500。
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we introduced convolutional neural networks and the various
    building blocks they are typically made of. Once you are able to get small models
    working properly, try running larger and deeper ones, following the same principles.
    While you can always have a peek in the latest literature and see what works,
    a lot can be learned from trial and error and figuring some of it out for yourself.
    In the next chapters, we will see how to work with text and sequence data and
    how to use TensorFlow abstractions to build CNN models with ease.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了卷积神经网络及其通常由各种构建模块组成。一旦你能够正确运行小型模型，请尝试运行更大更深的模型，遵循相同的原则。虽然你可以随时查看最新的文献并了解哪些方法有效，但通过试错和自己摸索也能学到很多。在接下来的章节中，我们将看到如何处理文本和序列数据，以及如何使用TensorFlow抽象来轻松构建CNN模型。
- en: ^([1](ch04.html#idm139707898624224-marker)) In machine learning and especially
    in deep learning, an *epoch* refers to a single pass over all the training data;
    i.e., when the learning model has seen each training example exactly one time.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm139707898624224-marker)) 在机器学习和特别是深度学习中，*epoch*指的是对所有训练数据的一次完整遍历；即，当学习模型已经看到每个训练示例一次时。
- en: ^([2](ch04.html#idm139707898293216-marker)) This is done mostly for the purpose
    of illustration. There are existing open source libraries containing this sort
    of data wrapper built in, for many popular datasets. See, for example, the datasets
    module in Keras (`keras.datasets`), and specifically `keras.datasets.cifar10`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#idm139707898293216-marker)) 这主要是为了说明的目的。已经存在包含这种数据包装器的开源库，适用于许多流行的数据集。例如，查看Keras中的数据集模块（`keras.datasets`），特别是`keras.datasets.cifar10`。
- en: ^([3](ch04.html#idm139707897326192-marker)) See [Who Is the Best in CIFAR-10?](http://bit.ly/2srV5OO)
    for a list of methods and associated papers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#idm139707897326192-marker)) 参见[谁在CIFAR-10中表现最好？](http://bit.ly/2srV5OO)以获取方法列表和相关论文。
