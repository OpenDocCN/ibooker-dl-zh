- en: Appendix C. Exercise Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complete code examples for the exercises answers can be found in the supplementary
    GitHub repository at [https://github.com/rasbt/LLMs-from-scratch](rasbt.html).
  prefs: []
  type: TYPE_NORMAL
- en: C.1 Chapter 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 2.1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can obtain the individual token IDs by prompting the encoder with one string
    at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then use the following code to assemble the original string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 2.2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The code for the data loader with `max_length=2 and stride=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces batches of the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The code of the second data loader with `max_length=8 and stride=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'An example batch looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: C.2 Chapter 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 3.1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The correct weight assignment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 3.2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To achieve an output dimension of 2, similar to what we had in single-head attention,
    we need to change the projection dimension `d_out` to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 3.3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The initialization for the smallest GPT-2 model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: C.3 Chapter 4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 4.1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can calculate the number of parameters in the feed forward and attention
    modules as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the feed forward module contains approximately twice as many
    parameters as the attention module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 4.2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To instantiate the other GPT model sizes, we can modify the configuration dictionary
    as follows (here shown for GPT-2 XL):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, reusing the code from Section 4.6 to calculate the number of parameters
    and RAM requirements, we find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: C.4 Chapter 5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exercise 5.1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can print the number of times the token (or word) "pizza" is sampled using
    the `print_sampled_tokens` function we defined in this section. Let's start with
    the code we defined in section 5.3.1.
  prefs: []
  type: TYPE_NORMAL
- en: The "pizza" token is sampled 0x if the temperature is 0 or 0.1, and it is sampled
    32× if the temperature is scaled up to 5\. The estimated probability is 32/1000
    × 100% = 3.2%.
  prefs: []
  type: TYPE_NORMAL
- en: The actual probability is 4.3% and contained in the rescaled softmax probability
    tensor (`scaled_probas[2][6]`).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Top-k sampling and temperature scaling are settings that have to be adjusted
    based on the LLM and the desired degree of diversity and randomness in the output.
  prefs: []
  type: TYPE_NORMAL
- en: When using relatively small top-k values (e.g., smaller than 10) and the temperature
    is set below 1, the model's output becomes less random and more deterministic.
    This setting is useful when we need the generated text to be more predictable,
    coherent, and closer to the most likely outcomes based on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Applications for such low k and temperature settings include generating formal
    documents or reports where clarity and accuracy are most important. Other examples
    of applications include technical analysis or code generation tasks, where precision
    is crucial. Also, question answering and educational content require accurate
    answers where a temperature below 1 is helpful.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, larger top-k values (e.g., values in the range of 20 to 40)
    and temperature values above 1 are useful when using LLMs for brainstorming or
    generating creative content, such as fiction.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are multiple ways to force deterministic behavior with the `generate`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting to `top_k=None` and applying no temperature scaling;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting `top_k=1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise 5.4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In essence, we have to load the model and optimizer that we saved in the main
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Then, call the `train_simple_function` with `num_epochs=1` to train the model
    for another epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.5
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can use the following code to calculate the training and validation set
    losses of the GPT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting losses for the 124M parameter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The main observation is that the training and validation set performances are
    in the same ballpark. This can have multiple explanations.
  prefs: []
  type: TYPE_NORMAL
- en: The Verdict was not part of the pretraining dataset when OpenAI trained GPT-2\.
    Hence, the model is not explicitly overfitting to the training set and performs
    similarly well on The Verdict's training and validation set portions. (The validation
    set loss is slightly lower than the training set loss, which is unusual in deep
    learning. However, it's likely due to random noise since the dataset is relatively
    small. In practice, if there is no overfitting, the training and validation set
    performances are expected to be roughly identical).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Verdict was part of GPT -2's training dataset. In this case, we can't tell
    whether the model is overfitting the training data because the validation set
    would have been used for training as well. To evaluate the degree of overfitting,
    we'd need a new dataset generated after OpenAI finished training GPT-2 to make
    sure that it couldn't have been part of the pretraining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exercise 5.6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the main chapter, we experimented with the smallest GPT-2 model, which has
    only 124M parameters. The reason was to keep the resource requirements as low
    as possible. However, you can easily experiment with larger models with minimal
    code changes. For example, instead of loading the 1558M instead of 124M model
    in chapter 5, the only 2 lines of code that we have to change are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The updated code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
