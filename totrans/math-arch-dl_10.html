<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-deep-learning-object-recognition-detection">11 Neural networks for image classification and object detection</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Using deeper neural networks for image classification and object detection</li>
<li class="co-summary-bullet">Understanding convolutional neural networks and other deep neural network architectures</li>
<li class="co-summary-bullet">Correcting imbalances in neural networks</li>
</ul>
<p class="body"><a id="marker-385"/>If a human is shown the image in figure <a class="url" href="#fig-bird-plane-superman">11.1</a>, they can instantly recognize the objects in it, categorizing them as a bird, a plane, and Superman. In image classification, we want to impart this capability to computers—the ability to recognize objects in an image and classify them into one or more known and predetermined categories. Apart from identifying the object categories, we can also identify the location of the objects in the image. An object’s location can be described by a <i class="fm-italics">bounding box</i>: a rectangle whose sides are parallel to coordinate axes. A bounding box is typically specified by four parameters: <span class="math">[(<i class="fm-italics">xtl</i>, <i class="fm-italics">ytl</i>),(<i class="fm-italics">xbr</i>, <i class="fm-italics">ybr</i>)]</span>, where <span class="math">(<i class="fm-italics">xtl</i>, <i class="fm-italics">y</i><i class="fm-italics">tl</i>)</span> are the xy coordinates of the top-left corner and <span class="math">(<i class="fm-italics">xbr</i>, <i class="fm-italics">ybr</i>)</span> are the <i class="fm-italics">xy</i> coordinates of the bottom-right corner of the bounding</p>
<p class="body">box. The problem of identifying and categorizing the objects present in the image is called <i class="fm-italics">image classification</i>. If we also want to identify their location in the image, it is referred to as <i class="fm-italics">object detection</i>. Image classification and object detection are some of the most fundamental problems in computer vision. While the human brain can both classify and localize objects in images almost intuitively, how do we train a machine to do this? Before deep learning, computer vision techniques involved hand-crafting image features (to encode color, edges, and shapes) and designing rules on top of these features to classify/localize objects. However, this is not a scalable approach because images are extremely complex and varied. Think of a simple object like an automobile. It can come in various sizes, shapes, and colors. It can be seen from afar or close (scales), from various viewpoints (perspectives), and on a cloudy day or a sunny day (lighting conditions). The car can be on a busy street or a mountain road backgrounds). It is nearly impossible to engineer features and rules that can handle all such variations.<a id="marker-386"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="547" id="fig-bird-plane-superman" src="../../OEBPS/Images/CH11_F01_Chaudhury.jpg" width="750"/></p>
<p class="figurecaption">Figure 11.1 Is it a bird? Is it a plane? Is it Superman?</p>
</div>
<p class="body">Over the last 10 years, a new class of algorithms has emerged: convolutional neural networks (CNNs). They do not rely on hand-engineered features but instead <i class="fm-italics">learn</i> the relevant features from <i class="fm-italics">data</i>. These models have shown tremendous success in several computer vision tasks, achieving (and sometimes even surpassing) human-level accuracy. They are increasingly used in the industry for applications ranging from medical diagnostics to e-commerce to manufacturing. In this chapter, we detail some of the most popular deep neural network architectures used for image classification and object detection. We look at some of their salient features, take a deep dive into the architectural details to understand how and why they work, and apply them to real-world problems.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this chapter, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/vojq">http://mng.bz/vojq</a></p>
<h2 class="fm-head" id="cnns-for-image-classification-lenet">11.1 CNNs for image classification: LeNet</h2>
<p class="body">In chapter <a class="url" href="../Text/10.xhtml#ch-conv2d3dtranspose">10</a>, we discussed the convolution operation in 1D, 2D, and 3D scenarios. We also saw how to implement a single convolutional layer as part of a larger neural network. This section shows how a neural network with multiple convolutional layers can be used for image classification. (If needed, ou are encouraged to revisit chapter <a class="url" href="../Text/10.xhtml#ch-conv2d3dtranspose">10</a>.) For this purpose, let’s consider the MNIST data set, a large collection of handwritten digits (0 through 9). It contains a training set of 60,000 images and a test set of 10,000 images. Each image is 28 <span class="math">×</span> 28 in size and contains a center crop of a single digit. Figure <a class="url" href="#fig-mnist-sample">11.2</a> shows sample images from the MNIST data set.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="563" id="fig-mnist-sample" src="../../OEBPS/Images/CH11_F02_Chaudhury.jpg" width="594"/></p>
<p class="figurecaption">Figure 11.2 Sample images from the MNIST data set. (Source: “Gradient-based learning applied to document recognition”; <a class="url" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">http://mng.bz/Wz0a.</a>)</p>
</div>
<p class="body"><a id="marker-387"/>We’d like to build a classifier that takes in a 28 <span class="math">×</span> 28 image as input and emits a label from 0 to 9 based on the digit contained in the image. One of the most popular neural network architectures for this task is the LeNet, which was proposed by LeCun et al. in their 1998 paper, “Gradient-based learning applied to document recognition” (<a class="url" href="http://mng.bz/Wz0a">http://mng.bz/Wz0a</a>). The LeNet architecture is illustrated in figure <a class="url" href="#fig-lenet-architecture">11.3</a> (LeNet expects input images of size 32 <span class="math">×</span> 32, so the 28 <span class="math">×</span> 28 MNIST images are resized to 32 <span class="math">×</span> 32 before being fed into the network):</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="320" id="fig-lenet-architecture" src="../../OEBPS/Images/CH11_F03_Chaudhury.png" width="1022"/></p>
<p class="figurecaption">Figure 11.3 LeNet. (Source: “Gradient-based learning applied to document recognition”; <a class="url" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">http://mng.bz/Wz0a.</a>)</p>
</div>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">It consists of three convolutional layers with <span class="math">5 × 5</span> kernels convolved with a stride of 1. The first convolution layer produces 6 feature maps of size 28 <span class="math">×</span> 28, the</p>
<p class="body">second convolution layer produces 16 feature maps of size 10 <span class="math">×</span> 10, and the third convolution layer produces 120 feature maps of size <span class="math">1 × 1</span> which are flattened into a 120-dimensional vector)</p>
</li>
<li class="fm-list-bullet">
<p class="list">The first two convolutional layers are followed by subsampling (aka pooling) layers, which perform a local averaging and subsampling of the feature map, thus reducing the resolution of the feature map and the sensitivity of the output to shifts and distortions in the input. A pooling kernel of size <span class="math">2 × 2</span> is applied, reducing the feature map size to half its original size. Refer to section <a class="url" href="../Text/10.xhtml#sec-pooling">10.7</a> for more about pooling.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Every feature map is followed by a tanh activation layer. This introduces nonlinearity into the network, increasing its expressive power because it can now model the output as a nonlinear combination of the inputs. If we did not have a nonlinear activation function, no matter how many layers we had, the neural network would still behave as a single-linear-layer network because the combination of multiple linear layers is just another linear layer. While the original LeNet paper used tanh as the activation function, several activation functions such as ReLU and sigmoid can also be used. ReLU is discussed in detail in section <a class="url" href="#subsec-relu">11.2.1.1</a>. Detailed discussions of sigmoid and tanh can be found in sections <a class="url" href="../Text/08.xhtml#sec-sigmoid-etc">8.1</a> and <a class="url" href="../Text/08.xhtml#sec-tanh">8.1.2</a>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The output feature map is passed through two fully connected (FC, aka linear) layers, which finally produce a 10-dimensional <i class="fm-italics">logits</i> vector that represents the score for every class. The logits scores are converted into probabilities using the softmax layer.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><code class="fm-code-in-text">CrossEntropyLoss</code>, discussed in section <a class="url" href="../Text/06.xhtml#sec-cross-entropy">6.3</a>, is used to compute the difference between the predicted probabilities and the ground truth.</p>
</li>
</ul>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> A <i class="fm-italics">feature map</i> is a <span class="math">2</span>D array of points (that is, a grid) with a fixed-size vector associated with every point. An image is an example of a feature map, with each point being a pixel and the associated vector representing the pixel’s color. A convolution layer transforms an input feature map into an output feature map. The output feature map usually has smaller width and height but a longer per-point vector.<a id="marker-388"/></p>
<p class="body">The LeNet performs very well on the MNIST data set, achieving test accuracies greater than 99%. A PyTorch implementation of LeNet is presented next.</p>
<h3 class="fm-head1" id="pytorch-implementing-lenet-for-image-classification-on-mnist">11.1.1 PyTorch- Implementing LeNet for image classification on MNIST</h3>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for training the LeNet, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/q2gz">http://mng.bz/q2gz</a>.</p>
<p class="fm-code-listing-caption" id="listing-11.1-pytorch-code-for-the-lenet">Listing 11.1 PyTorch code for the LeNet</p>
<pre class="programlisting">import torch

class LeNet(torch.nn.Module):
    def __init__(self, num_classes):
        super(LeNet, self).__init__()
        self.conv1 = torch.nn.Sequential(
                        torch.nn.Conv2d(
                            in_channels=1, out_channels=6,     <span class="fm-combinumeral">①</span>
                            kernel_size=5, stride=1),
                        torch.nn.Tanh(),                       <span class="fm-combinumeral">②</span>
                        torch.nn.AvgPool2d(kernel_size=2))     <span class="fm-combinumeral">③</span>
        self.conv2 = torch.nn.Sequential(
                        torch.nn.Conv2d(
                            in_channels=6, out_channels=16,
                            kernel_size=5, stride=1),
                        torch.nn.Tanh(),
                        torch.nn.AvgPool2d(kernel_size=2))
        self.conv3 = torch.nn.Sequential(
                        torch.nn.Conv2d(
                            in_channels=16, out_channels=120,
                            kernel_size=5, stride=1),
                        torch.nn.Tanh())
        self.fc1 = torch.nn.Sequential(
                        torch.nn.Linear(
                            in_features=120, out_features=84), <span class="fm-combinumeral">④</span>
                        torch.nn.Tanh())
        self.fc2 = torch.nn.Linear(
            in_features=84, out_features=num_classes)          <span class="fm-combinumeral">⑤</span>

    def forward(self, X):                                      <span class="fm-combinumeral">⑥</span>
        conv_out = self.conv3(self.conv2(self.conv1(X)))
        batch_size = conv_out.shape[0]
        conv_out = conv_out.reshape(batch_size, -1)            <span class="fm-combinumeral">⑦</span>
        logits = self.fc2(self.fc1(conv_out))                  <span class="fm-combinumeral">⑧</span>
        return logits

    def predict(self, X):
        logits = self.forward(X)
        probs = torch.softmax(logits, dim=1)                   <span class="fm-combinumeral">⑨</span>
        return torch.argmax(probs, 1)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="math">5 × 5</span> conv</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Tanh activation</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math">2 × 2</span> average pooling</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> First FC layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Second FC layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> X.shape: N <span class="math">×</span> 3 <span class="math">×</span> 32 <span class="math">×</span> 32. N is the batch size.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> conv_out.shape: N <span class="math">×</span> 120 <span class="math">×</span> 1 <span class="math">×</span> 1</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> logits.shape: N <span class="math">×</span> 10</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Computes the probabilities using softmax</p>
<h2 class="fm-head" id="sec-towards-deeper-neural-nets">11.2 Toward deeper neural networks</h2>
<p class="body"><a id="marker-389"/>The LeNet model is not a very deep network since it has only three convolutional layers. While this is sufficient to achieve accurate results on a simple data set like MNIST, it doesn’t work well on real-world image classification problems since it does not have enough expressive power to model complex images. So, we typically go for much deeper neural networks with multiple convolutional layers. Adding more layers does the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Brings extra expressive power due to extra nonlinearity</i>—Since every layer brings with it a new set of learnable parameters and extra nonlinearity, a deeper network can model more complex relationships between input data elements. Lower layers typically learn simpler features of the object, like lines and edges, whereas higher layers learn more abstract features of the object, like shapes or sets of lines.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Achieves the same reach with fewer parameters</i>—Let’s examine this via an example. Consider two output feature maps, one produced by a single <span class="math">5 × 5</span> convolution on the input and another produced by two <span class="math">3 × 3</span> convolutions applied one after another in sequence on the input. Assume a stride of 1 and the same (zero) padding. Figure <a class="url" href="#fig-receptive-field">11.4</a> illustrates this scenario. Consider a single grid point in the output feature map. In both cases, the output value of the grid point is derived from a <span class="math">5 × 5</span> patch in the input. We say the indicated <span class="math">5 × 5</span> input patch is the <i class="fm-italics">receptive field</i> of the output grid points. Thus, in both cases, the output grid point is a digest of the same input: that is, it expresses the same information. However, in the deeper network, there are fewer parameters. The number of parameters in a single <span class="math">5 × 5</span> filter is 25, whereas that in two <span class="math">3 × 3</span> filters is 2 <span class="math">×</span> 9 = 18 (assuming a single channel input image). This is a 38% difference. Similarly, if we compare one <span class="math">7 × 7</span> filter with three <span class="math">3 × 3</span> filters, they have the same receptive field, but the <span class="math">7 × 7</span> filter has 81% more parameters than the <span class="math">3 × 3</span> filter.<a id="marker-390"/></p>
</li>
</ul>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="593" id="fig-receptive-field" src="../../OEBPS/Images/CH11_F04_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 11.4 A single 5 <span class="math">×</span> 5 convolution layer vs. two 3 <span class="math">×</span> 3 convolution layers</p>
</div>
<p class="body">Now, let’s look at some of the most popular deep convolutional networks used for image classification. The first deep network that reignited the deep learning revolution was <i class="fm-italics">AlexNet</i>, which was published by Krizhevsky et al. in 2012. It significantly outperformed all previous state-of-the-art algorithms on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a complex data set with 1.3 million images across 1,000 classes. Since AlexNet, several deep networks have improved on the previous state of the art, such as GoogleNet, VGG, and ResNet. In this chapter, we discuss the key concepts that make each of these networks work. For a detailed review of their architectures, training methodologies, and final results, you are encouraged to read the original papers linked in each section.</p>
<h3 class="fm-head1" id="vgg-visual-geometry-group-net">11.2.1 VGG (Visual Geometry Group) Net</h3>
<p class="body">The VGG family of networks was created by the Visual Geometry Group from the University of Oxford (<a class="url" href="https://arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a>). Their main contribution was a thorough evaluation of networks of increasing depth using an architecture with very small (<span class="math">3 × 3</span>) convolution filters. They demonstrated that by using <span class="math">3 × 3</span> convolutions and networks with 16–19 weight layers, they could outperform previous state-of-the-art results on the ILSVRC-2014 challenge. The VGG network had two main differences compared to prior works:<a id="marker-391"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Use of smaller (<span class="math">3 × 3</span>) convolution filters</i>—Prior networks often relied on larger kernels of size <span class="math">7 × 7</span> or <span class="math">11 × 11</span> in the first convolution layers. VGG instead only used <span class="math">3 × 3</span> kernels throughout the network. As discussed in section <a class="url" href="#sec-towards-deeper-neural-nets">11.2</a>, three <span class="math">3 × 3</span> filters have the same receptive field as a single <span class="math">7 × 7</span> filter. So what does replacing the <span class="math">7 × 7</span> filter with three smaller filters buy?</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">More nonlinearity and hence more expressive power because we have a ReLU activation function applied at the end of every convolution layer</p>
</li>
<li class="fm-list-bullet">
<p class="list">Fewer parameters (<span class="math">49<i class="fm-italics">C</i><sup class="fm-superscript">2</sup></span> vs. <span class="math">27<i class="fm-italics">C</i><sup class="fm-superscript">2</sup></span>), which means faster learning and more robustness to overfitting</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Removal of the local response normalization (LRN) layers</i>—LRN was first introduced in the AlexNet architecture. Its purpose was twofold: to bound the output of the ReLU layer, which is an unbounded function and can produce outputs as large as the training permits; and to encourage <i class="fm-italics">lateral inhibition</i> wherein a neuron can suppress the activity of its neighbors (this in effect acts as a regularization). The VGG paper demonstrated that adding LRN layers did not improve accuracy, so VGG chose to remove them from its architecture.</p>
</li>
</ul>
<p class="body">The VGG family of networks comes in five different configurations, which mainly differ in the number of layers (VGG-11, VGG-13, VGG-16, and VGG-19). Regardless of the exact configuration, the VGG family of networks follows a common structure. Here, we discuss these commonalities (a detailed description of the differences can be found in the original paper):</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">All architectures work on <span class="math">224 × 224</span> input images.</p>
</li>
<li class="fm-list-bullet">
<p class="list">All architectures have five convolutional blocks (conv blocks):</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">Each block can have multiple convolution layers followed by a max pool layer at the end.</p>
</li>
<li class="fm-list-bullet">
<p class="list">All individual convolution layers use <span class="math">3 × 3</span> kernels with a stride of 1 and same padding. Therefore, they don’t change the spatial resolution of the output feature map.</p>
</li>
<li class="fm-list-bullet">
<p class="list">All convolution layers within a single conv block have the same-sized output feature maps.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Each convolution layer is followed by a ReLU layer that adds nonlinearity.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The max pool layer at the end of every conv block reduces the spatial resolution to half.</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list">Since each conv block downsamples by 2, the input feature map is reduced <span class="math">2<sup class="fm-superscript">5</sup></span> (32) times, resulting in an output feature map of size <span class="math">7 × 7</span>. Additionally, at each conv block, the number of feature maps is doubled.</p>
</li>
<li class="fm-list-bullet">
<p class="list">All architectures end with three FC layers:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">The first takes a 51,277-sized input and converts it into a 4,096-dimensional output.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The second takes the resulting 4,096-dimensional output and converts it into another 4,096-dimensional output.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The final takes the resulting 4,096-dimensional output and converts it into a <i class="timesitalic">C</i>-dimensional output, where <i class="timesitalic">C</i> stands for the number of classes. In the case of ImageNet classification, <i class="timesitalic">C</i> is 1,000.</p>
</li>
</ul>
</li>
</ul>
<p class="body"><a id="marker-392"/>The architecture diagram for VGG-11 is shown in figure <a class="url" href="#fig-vgg-architecture">11.5</a>. The column on the left represents the shape of the input tensor to each layer. The column on the right represents the shape of the output tensor from each layer.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="1331" id="fig-vgg-architecture" src="../../OEBPS/Images/CH11_F05_Chaudhury.png" width="908"/></p>
<p class="figurecaption">Figure 11.5 VGG-11 architecture diagram. All shapes are of the form N <span class="math">×</span> C <span class="math">×</span> H <span class="math">×</span> W, where N is the batch size, C is the number of channels, H is the height, and W is the width.<a id="marker-393"/></p>
</div>
<p class="fm-head2" id="subsec-relu">ReLU nonlinearity</p>
<p class="body">As we’ve discussed previously, nonlinear layers give the deep neural network more expressive power to model complex mathematical functions. In chapter <a class="url" href="../Text/08.xhtml#ch-training-neural-networks">8</a>, we looked at two nonlinear functions: sigmoid and tanh. However, the VGG network (like AlexNet) consists of a different nonlinear layer called rectified linear unit (ReLU). To understand the rationale for this choice, let’s revisit the sigmoid function and look at some of its drawbacks.</p>
<p class="body">Figure <a class="url" href="#fig-sigmoid1d-with-derivative-2">11.6</a> plots the sigmoid function along with its derivative. As the plot shows, the gradient (derivative) is maximum when the input is 0, and it quickly tapers down to 0 as the input increases/decreases. This is true for the tanh activation function as well. It means when the output of a neuron before the sigmoid layer) is either high or low, the gradient becomes small. While this may not be an issue in shallow networks, it becomes a problem in larger networks because the gradients can become too small for training to work effectively. Gradients of neural networks are calculated using backpropagation. By the chain rule, the derivatives of each layer are multiplied down the network, starting from the final layer and moving toward the initial layers. If the gradients at each layer are small, a small number multiplied by another small number is an even smaller number. Thus the gradients at the initial layers are very close to 0, making the training ineffective. This is known as the <i class="fm-italics">vanishing gradient</i> problem.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="692" id="fig-sigmoid1d-with-derivative-2" src="../../OEBPS/Images/CH11_F06_Chaudhury.png" width="891"/></p>
<p class="figurecaption">Figure 11.6 Graph of a <span class="math">1</span>D sigmoid function (dotted curve) and its derivative (solid curve)</p>
</div>
<p class="body">The ReLU function addresses this problem. Figure <a class="url" href="#fig-relu">11.7</a> shows a graph of the ReLU function. Its equation is given by</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">ReLU</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">max</i>(0, <i class="fm-italics">x</i>)</span></p>
<p class="fm-equation-caption">Equation 11.1</p>
<p class="body"><a id="marker-394"/>The derivative of ReLU is 1 (constant) when x is greater than 0, and 0 everywhere else. Therefore, it doesn’t suffer from the vanishing gradient problem. Most deep networks today use ReLU as their activation function. The AlexNet paper demonstrated that using ReLU nonlinearity significantly speeds up training because it helps with faster convergence.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="436" id="fig-relu" src="../../OEBPS/Images/CH11_F07_Chaudhury.png" width="891"/></p>
<p class="figurecaption">Figure 11.7 Graph of the ReLU function</p>
</div>
<p class="fm-head2" id="pytorch-vgg">PyTorch- VGG</p>
<p class="body">Now let’s see how to implement the VGG network in PyTorch. First, let’s implement a single conv block, which is the core component of the VGG net. This conv block will later be repeated multiple times to form the entire VGG network.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the VGG network, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/7WE4">http://mng.bz/7WE4</a>.</p>
<p class="fm-code-listing-caption" id="code-conv-block">Listing 11.2 PyTorch code for a convolutional block<a id="marker-395"/></p>
<pre class="programlisting">class ConvBlock(nn.Module):
    def __init__(self, in_channels, num_conv_layers, num_features):
        super(ConvBlock, self).__init__()

        modules = []

        for i in range(num_conv_layers):
            modules.extend([
                nn.Conv2d(
                    in_channels, num_features,      <span class="fm-combinumeral">①</span>
                    kernel_size=3, padding=1),      <span class="fm-combinumeral">②</span>
                nn.ReLU(inplace=True)
            ])

            in_channels = num_features
        modules.append(nn.MaxPool2d(kernel_size=2)) <span class="fm-combinumeral">③</span>
        self.conv_block = nn.Sequential(*modules)

    def forward(self, x):
        return self.conv_block(x)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="math">3 × 3</span> conv</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> ReLU nonlinearity</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math">2 × 2</span> max pooling</p>
<p class="body">Next, let’s implement the convolutional backbone (conv backbone) builder, which allows us to create different VGG architectures via simple configuration changes.</p>
<p class="fm-code-listing-caption" id="code-vgg-backbone">Listing 11.3 PyTorch code for the conv backbone</p>
<pre class="programlisting">class ConvBackbone(nn.Module):
    def __init__(self, cfg):                        <span class="fm-combinumeral">①</span>


        super(ConvBackbone, self).__init__()

        self.cfg = cfg

        self.validate_config(cfg)

        modules = []
        for block_cfg in cfg:                       <span class="fm-combinumeral">②</span>
            in_channels, num_conv_layers, num_features = block_cfg 
            modules.append(ConvBlock(               <span class="fm-combinumeral">③</span>
            in_channels, num_conv_layers, num_features))
        self.features = nn.Sequential(*modules)

    def validate_config(self, cfg):
        assert len(cfg) == 5 # 5  conv blocks
        for i, block_cfg in enumerate(cfg):
            assert type(block_cfg) == tuple and len(block_cfg) == 3
            if i == 0:
                assert block_cfg[0] == 3            <span class="fm-combinumeral">④</span>
            else:
                assert block_cfg[0] == cfg[i-1][-1] <span class="fm-combinumeral">⑤</span>

    def forward(self, x):
        return self.features(x)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Cfg: [(in_channels, num_conv_layers, num_features),] The different VGG networks can be created without duplicating code by passing in the right cfg.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Iterates over conv block configurations</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates the conv block defined in listing <a class="url" href="#code-conv-block">11.2</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> There must be three input channels.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> out_Features of the previous block should be equal to in_features of the current block.</p>
<p class="body">The conv backbone is instantiated with a config that contains the list of configurations for each of the conv blocks. The config for VGG-11 contains fewer layers, whereas that for VGG-19 contains more layers. The output of the conv backbone is fed into the classifier, which consists of three FC layers. Together, the conv backbone and the classifier form the VGG module.<a id="marker-396"/></p>
<p class="fm-code-listing-caption" id="listing-11.4-pytorch-code-for-the-vgg-network">Listing 11.4 PyTorch code for the VGG network</p>
<pre class="programlisting">class VGG(nn.Module):
    def __init__(self, conv_backbone, num_classes):
        super(VGG, self).__init__()
        self.conv_backbone = conv_backbone   <span class="fm-combinumeral">①</span>
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 4096),           <span class="fm-combinumeral">②</span>
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, num_classes)
        )

    def forward(self, x):
        conv_features = self.conv_backbone(x)

        logits = self.classifier(
            conv_features.view(
                conv_features.shape[0], -1)) <span class="fm-combinumeral">③</span>
        return logits</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Backbone network defined in listing <a class="url" href="#code-vgg-backbone">11.3</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The classifier is made up of three linear Layers. The first two are followed by ReLU nonlinearity.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Flattens the conv features before passing it to the classifier</p>
<p class="body">A VGG-11 network can be instantiated as follows.</p>
<p class="fm-code-listing-caption" id="listing-11.5-pytorch-code-instantiating-a-vgg-network-from-a-specific-config">Listing 11.5 PyTorch code instantiating a VGG network from a specific config</p>
<pre class="programlisting">vgg11_cfg = [                             <span class="fm-combinumeral">①</span>
    (3, 1, 64),
    (64, 1, 128),
    (128, 2, 256),
    (256, 2, 512),
    (512, 2, 512)
]

vgg11_backbone = ConvBackbone(vgg11_cfg)  <span class="fm-combinumeral">①</span>
num_classes = 1000
vgg11 = VGG(vgg11_backbone, num_classes)  <span class="fm-combinumeral">①</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates the cfg for VGG-11</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the conv backbone</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates the VGG network</p>
<p class="body">While we have discussed how to implement VGG in PyTorch, we don’t do this in practice because the <code class="fm-code-in-text">torchvision</code> package already implements the VGG network, along with several other popular deep networks. It is recommended that you use the <code class="fm-code-in-text">torchvision</code> implementation, as shown here:</p>
<pre class="programlisting">import torchvision
vgg11 = torchvision.models.vgg11()</pre>
<h3 class="fm-head1" id="inception-network-in-network-paradigm">11.2.2 Inception: Network-in-network paradigm</h3>
<p class="body"><a id="marker-397"/>Previously, we saw how increasing the depth of a neural network—that is, the number of layers—can improve accuracy because it increases the expressive power of the network. Alternatively, we could increase the width of the network—the number of units at each level—to improve accuracy. However, both these methods suffer from two main drawbacks. First, blindly increasing the size of the network can lead to overfitting, wherein the network memorizes certain patterns in the training data that don’t extend well to test data. And second, increased computation resources are required during both training and inference times. The Inception architecture, introduced by Szegedy et al. in their paper "Going deeper with convolutions" (<a class="url" href="https://arxiv.org/pdf/1409.4842v1.pdf">https://arxiv.org/pdf/1409.4842v1.pdf</a>), aims to address both these drawbacks. The Inception architecture increases the network’s depth and width while keeping the computational budget constant. In this section, we examine the main idea behind the Inception architecture. While there have been several improvements to it Inception_v2, Inception_v3, Inception_ResNet, and so on), we discuss the original: Inception_v1.</p>
<p class="body">Prior deep learning architectures typically stacked convolutional filters sequentially: each layer applied a set of convolutional filters of the same size and passed it to the subsequent layer. The kernel size of the filter at each layer depended on the architecture. But with such an architecture, how do we know we have chosen the right kernel size for each layer? If we are detecting a car, say, the fraction of the image area (that is, the number of pixels) occupied by the car is different in an image taken close up than in one taken from far away. We say the <i class="fm-italics">scale</i> of the car object is different in the two images. Consequently, the number of pixels that must be digested to recognize the car will differ at different scales. A larger kernel is preferred for information at a larger scale, and vice versa. An architecture that is forced to choose one kernel size may not be optimal. The Inception module tackles this problem by having multiple kernels of different sizes at each level and taking weighted combinations of the outputs. The network can learn to weigh the appropriate kernel more than others. The naive implementation of the Inception module performs convolutions on the input using three kernel sizes: <span class="math">1 × 1</span>, <span class="math">3 × 3</span>, and <span class="math">5 × 5</span>. Max pooling is also performed, using a <span class="math">3 × 3</span> kernel with stride 1 and padding 1 (for output and input to be the same size). The outputs are concatenated and sent into the next Inception module. See figure <a class="url" href="#fig-inception-architecture">11.8</a> for details.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre23" height="414" id="fig-inception-architecture" src="../../OEBPS/Images/CH11_F08_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 11.8 Inception_v1 architecture</p>
</div>
<p class="body"><a id="marker-398"/>This naive Inception block has a major flaw. Using even a small number of <span class="math">5 × 5</span> filters can prohibitively increase the number of parameters. This becomes even more expensive when we add the pooling layer, where the number of output filters equals the number of filters in the previous stage. Thus, concatenating the output of the pooling layer with the outputs of convolutional layers would lead to an inevitable increase in the number of output features. To fix this, the Inception module uses <span class="math">1 × 1</span> convolution layers before the <span class="math">3 × 3</span> and <span class="math">5 × 5</span> filters to reduce the number of input channels. This drastically reduces the number of parameters of the <span class="math">3 × 3</span> and <span class="math">5 × 5</span> convs. While it may seem counterintuitive, <span class="math">1 × 1</span> convs are much cheaper than <span class="math">3 × 3</span> and <span class="math">5 × 5</span> convs. Additionally, <span class="math">1 × 1</span> convolution is applied after pooling see figure <a class="url" href="#fig-inception-architecture">11.8</a>).</p>
<p class="body">A neural network architecture was built using the dimension-reduced Inception module and was popularly known as GoogLeNet. GoogLeNet has nine such Inception modules stacked linearly. It is 22 layers deep (27, including the pooling layers). It uses global average pooling at the end of the last Inception module. With such a deep network, there is always the vanishing gradient problem; to prevent the middle part of the network from “dying out,” the paper introduced two auxiliary classifiers. This is done by applying softmax to the output of two of the intermediate Inception modules and computing an auxiliary loss over the ground truth. The total loss function is a weighted sum of the auxiliary loss and the real loss. You are encouraged to read the original paper to understand the details.</p>
<p class="fm-head2" id="pytorch-inception-block">PyTorch- Inception block</p>
<p class="body">Let’s see how to implement an Inception block in PyTorch. We typically don’t do this in practice because end-to-end deep network architectures containing Inception blocks are already implemented in the <code class="fm-code-in-text">torchvision</code> package. However, we implement the Inception block from scratch to understand the details.<a id="marker-399"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the Inception block, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/mxn0">http://mng.bz/mxn0</a>.</p>
<p class="fm-code-listing-caption" id="listing-11.6-pytorch-code-for-a-naive-inception-block">Listing 11.6 PyTorch code for a naive Inception block</p>
<pre class="programlisting">class NaiveInceptionModule(nn.Module):
    def __init__(self, in_channels, num_features=64):
        super(NaiveInceptionModule, self).__init__()

        self.branch1x1 = torch.nn.Sequential(                   <span class="fm-combinumeral">①</span>
                        nn.Conv2d(
                            in_channels, num_features,
                            kernel_size=1, bias=False),
                        nn.BatchNorm2d(num_features, eps=0.001),
                        nn.ReLU(inplace=True))

        self.branch3x3 = torch.nn.Sequential( 
                        nn.Conv2d(                              <span class="fm-combinumeral">②</span>
                            in_channels, num_features,
                            kernel_size=3, padding=1, bias=False),
                        nn.BatchNorm2d(num_features, eps=0.001),
                        nn.ReLU(inplace=True))

        self.branch5x5 = torch.nn.Sequential(                   <span class="fm-combinumeral">③</span>
                        nn.Conv2d(
                            in_channels, num_features,
                            kernel_size=5, padding=2, bias=False),
                        nn.BatchNorm2d(num_features, eps=0.001),
                        nn.ReLU(inplace=True))

        self.pool = torch.nn.MaxPool2d(                         <span class="fm-combinumeral">④</span>
            kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        conv1x1 = self.branch1x1(x)
        conv3x3 = self.branch3x3(x)
        conv5x5 = self.branch5x5(x)
        pool_out = self.pool(x)
        out = torch.cat(                                        <span class="fm-combinumeral">⑤</span>
            [conv1x1, conv3x3, conv5x5, pool_out], 1)
        return out</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="math">1 × 1</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">3 × 3</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math">5 × 5</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> <span class="math">3 × 3</span> pooling</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Concatenates the outputs of the parallel branches</p>
<p class="fm-code-listing-caption" id="listing-11.7-pytorch-code-for-a-dimensionality-reduced-inception-block">Listing 11.7 PyTorch code for a dimensionality reduced Inception block<a id="marker-400"/></p>
<pre class="programlisting">class Inceptionv1Module(nn.Module):
    def __init__(self, in_channels, num_1x1=64,
                 reduce_3x3=96, num_3x3=128,
                 reduce_5x5=16, num_5x5=32,
                 pool_proj=32):
        super(Inceptionv1Module, self).__init__()

        self.branch1x1 = torch.nn.Sequential(
                        nn.Conv2d(               <span class="fm-combinumeral">①</span>
                            in_channels, num_1x1,
                            kernel_size=1, bias=False),
                        nn.BatchNorm2d(num_1x1, eps=0.001),
                        nn.ReLU(inplace=True))

        self.branch3x3_1 = torch.nn.Sequential( <span class="fm-combinumeral">②</span>
                        nn.Conv2d(
                            in_channels, reduce_3x3,
                            kernel_size=1, bias=False),
                        nn.BatchNorm2d(reduce_3x3, eps=0.001),
                        nn.ReLU(inplace=True))

        self.branch3x3_2 = torch.nn.Sequential(  <span class="fm-combinumeral">③</span>
                        nn.Conv2d(
                            reduce_3x3, num_3x3,
                            kernel_size=3, padding=1, bias=False),
                        nn.BatchNorm2d(num_3x3, eps=0.001),
                        nn.ReLU(inplace=True))

        self.branch5x5_1 = torch.nn.Sequential(  <span class="fm-combinumeral">④</span>
                        nn.Conv2d(
                            in_channels, reduce_5x5,
                            kernel_size=5, padding=2, bias=False),
                        nn.BatchNorm2d(reduce_5x5, eps=0.001),
                        nn.ReLU(inplace=True))
        self.branch5x5_2 = torch.nn.Sequential(  <span class="fm-combinumeral">⑤</span>
                        nn.Conv2d(
                            reduce_5x5, num_5x5,
                            kernel_size=5, padding=2, bias=False),
                        nn.BatchNorm2d(num_5x5, eps=0.001),
                        nn.ReLU(inplace=True))

        self.pool = torch.nn.Sequential(         <span class="fm-combinumeral">⑥</span>
                        torch.nn.MaxPool2d(
                            kernel_size=3, stride=1, padding=1),
                        nn.Conv2d(
                            in_channels, pool_proj,
                            kernel_size=1, bias=False),
                        nn.BatchNorm2d(pool_proj, eps=0.001),
                        nn.ReLU(inplace=True))


    def forward(self, x):
        conv1x1 = self.branch1x1(x)
        conv3x3 = self.branch3x3_2(self.branch3x3_1((x)))
        conv5x5 = self.branch5x5_2(self.branch5x5_1((x)))
        pool_out = self.pool(x)
        out = torch.cat(                         <span class="fm-combinumeral">⑦</span>
            [conv1x1, conv3x3, conv5x5, pool_out], 1)
        return out</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> <span class="math">1 × 1</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">1 × 1</span> conv in the <span class="math">3 × 3</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> <span class="math">3 × 3</span> conv in the <span class="math">3 × 3</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> <span class="math">1 × 1</span> conv in the <span class="math">5 × 5</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> <span class="math">5 × 5</span> conv in the <span class="math">5 × 5</span> branch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Max pooling followed by a <span class="math">1 × 1</span> conv</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Concatenates the outputs of the parallel branches</p>
<h3 class="fm-head1" id="resnet-why-stacking-layers-to-add-depth-does-not-scale">11.2.3 ResNet: Why stacking layers to add depth does not scale</h3>
<p class="body"><a id="marker-401"/>We start with a fundamental question: is learning better networks as easy as stacking multiple layers? Consider the graphs in figure <a class="url" href="#fig-varying-depth-deep-net-error-rates">11.9</a>.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="688" id="fig-varying-depth-deep-net-error-rates" src="../../OEBPS/Images/CH11_F09_Chaudhury.jpg" width="1650"/></p>
<p class="figurecaption">Figure 11.9 Training error (left) and test error (right) on the CIFAR-10 data set with 20-layer and 56-layer networks. (Source: “Deep residual learning for image recognition”; <a class="url" href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a>.)</p>
</div>
<p class="body">This image from the ResNet paper “Deep residual learning for image recognition” (<a class="url" href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a>) shows the training and test error rates for two networks: a shallower network with 20 layers and a deeper network with 56 layers, on the CIFAR-10 data set. Surprisingly, the training and test errors are <i class="fm-italics">higher</i> for the deeper (56-layer) network. This result is extremely counterintuitive because we expect deeper networks to have more expressive power and hence higher accuracies/lower error rates than their shallower counterparts. This phenomenon is referred to as the <i class="fm-italics">degradation</i> problem: with the network depth increasing, the accuracy becomes saturated and degrades rapidly. We might attribute this to overfitting, but that is not the case because even the training errors are higher for the deeper network. Another cause could be vanishing/exploding gradients. However, the authors of the ResNet paper investigated the gradients at each layer and established that they are healthy (not vanishing/exploding). So, what causes the degradation problem, and how do we solve it?</p>
<p class="body">Let’s consider a shallower architecture with <i class="timesitalic">n</i> layers and a deeper counterpart that adds more layers to it (<span class="math"><i class="fm-italics">n</i> + <i class="fm-italics">m</i></span> layers). The deeper architecture should be able to achieve no higher loss than the shallow architecture. Intuitively, a trivial solution is to learn the exact <i class="timesitalic">n</i> layers of the shallow architecture and the identity function for the additional <i class="timesitalic">m</i> layers. The fact that this doesn’t happen in practice indicates that the neural network layers have a hard time learning the identity function. Thus the paper proposes "shortcut/skip connections" that enable the layers to potentially learn the identity function easily. This “identity shortcut connection” is the core idea of ResNet. Let’s look at a mathematical analogy. Let <span class="math"><i class="fm-italics">h</i>(<i class="fm-italics">x</i>)</span> be the function we are trying to model (learn) via a stack of layers (not necessarily the entire network). It is reasonable to expect that the function <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">h</i>(<i class="fm-italics">x</i>) − <i class="fm-italics">x</i></span> is simpler than <span class="math"><i class="fm-italics">h</i>(<i class="fm-italics">x</i>)</span> and hence easier to learn. But we already have <i class="timesitalic">x</i> at the input. So if we learn <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span> and add <i class="timesitalic">x</i> to it to obtain <span class="math"><i class="fm-italics">h</i>(<i class="fm-italics">x</i>)</span>, we have effectively modeled <span class="math"><i class="fm-italics">h</i>(<i class="fm-italics">x</i>)</span> by learning the simpler <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span> function. The name <i class="fm-italics">residual</i> comes from <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">h</i>(<i class="fm-italics">x</i>) − <i class="fm-italics">x</i></span>. Figure <a class="url" href="#fig-residual-block">11.10</a> shows this in detail.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="535" id="fig-residual-block" src="../../OEBPS/Images/CH11_F10_Chaudhury.png" width="791"/></p>
<p class="figurecaption">Figure 11.10 column shows a residual block with skip connections.</p>
</div>
<p class="body"><a id="marker-402"/>Now let’s revisit the earlier problem of degradation. We posited that normal neural network layers generally have difficulty learning the identity function. In the case of residual learning, to learn the identity function, <span class="math"><i class="fm-italics">h</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">x</i></span>, the layers need to learn <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>)</span>=0. This can easily be done by driving all the layers’ weights to 0. Here is another way to think about it: if we initialize a regular neural network’s weights and biases to be 0 at the start, then every layer starts with the “zero” function: <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = 0</span>. Thus, the output of every stack of layers with a shortcut connection, <span class="math"><i class="fm-italics">h</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">g</i>(<i class="fm-italics">x</i>) + <i class="fm-italics">x</i></span>, is already the identity function: <span class="math"><i class="fm-italics">h</i>(<i class="fm-italics">x</i>) = <i class="fm-italics">x</i></span> when <span class="math"><i class="fm-italics">g</i>(<i class="fm-italics">x</i>) = 0</span>.</p>
<p class="body">In real cases, it is important to note that identity mappings are unlikely to be optimal: the network layers will want to learn actual features. In such cases, this reformulation isn’t preventing the network lawyers from doing so; the layers can still learn other functions like a regular stack of layers. We can think of this reformulation as preconditioning, which makes learning the identity function easier if needed. Additionally, by adding skip connections, we allow a direct path for the gradient to flow from layer to layer: the deeper layer has a direct path to <i class="timesitalic">x</i>. This allows for better learning as information from the lower layers passes directly into the higher layers.</p>
<p class="fm-head2" id="resnet-architecture">ResNet architecture</p>
<p class="body"><a id="marker-403"/>Now that we have seen the basic building block—a stack of convolutional (conv) layers with a skip connection—let’s delve deeper into the architecture of ResNet. ResNet architectures are constructed by stacking multiple building blocks on top of each other. They follow the same idea as VGG:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The convolutional layers mostly have <span class="math">3 × 3</span> filters.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The layers have the same number of filters for a given output feature-map size.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If the feature-map size is halved, the number of filters is doubled to preserve the time complexity per layer.</p>
</li>
</ul>
<p class="body">ResNet uses conv layers with a stride of 2 to downsample, unlike VGG, which had multiple max pooling layers. The core architecture consists of the following components:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Five convolutional layer blocks</i>—The first convolutional block consists of a <span class="math">7 × 7</span> kernel with <code class="fm-code-in-text">stride=2</code>, <code class="fm-code-in-text">padding=3</code>, and <code class="fm-code-in-text">num_features=64</code>, followed by a max pooling layer with a <span class="math">3 × 3</span> kernel, <code class="fm-code-in-text">stride=2</code>, and <code class="fm-code-in-text">padding=1</code>. The feature map size is reduced from <span class="math">(224, 224)</span> to <span class="math">(56, 56)</span>. The remaining convolutional blocks <code class="fm-code-in-text">ResidualConvBlock</code>) are built by stacking multiple basic shortcut blocks together. Each basic block uses <span class="math">3 × 3</span> filters, as described.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Classifier</i>—An average pooling block that runs on top of the conv block output, followed by a FC layer, which is used for classification.</p>
</li>
</ul>
<p class="body">You are encouraged to examine the diagrams in the original paper to understand the details. Now, let’s see how to implement a ResNet in PyTorch.</p>
<p class="fm-head2" id="pytorch-resnet">PyTorch- ResNet</p>
<p class="body">In this section, we discuss how to implement a ResNet-34 from scratch. Note that this is seldom done in practice. The <code class="fm-code-in-text">torchvision</code> package provides ready-made implementations for all ResNet architectures. However, by building the network from scratch, we gain a deeper understanding of the architecture. First, let’s implement a basic skip connection block (<code class="fm-code-in-text">BasicBlock</code>) to see how the shortcut connection works.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for ResNet, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/5K9q">http://mng.bz/5K9q</a>.</p>
<p class="fm-code-listing-caption" id="listing-11.8-pytorch-code-for-basicblock">Listing 11.8 PyTorch code for <code class="fm-code-in-text">BasicBlock</code></p>
<pre class="programlisting">class BasicBlock(nn.Module):
    def __init__(self, in_channels, num_features, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Sequential(                        <span class="fm-combinumeral">①</span>

                        nn.Conv2d(
                            in_channels, num_features,
                            kernel_size=3, stride=stride, padding=1, bias=False),
                        nn.BatchNorm2d(num_features, eps=0.001),
                        nn.ReLU(inplace=True))
        self.conv2 = nn.Sequential(
                        nn.Conv2d(
                            num_features, num_features,
                            kernel_size=3, stride=1, padding=1, bias=False),
                        nn.BatchNorm2d(num_features, eps=0.001))



        self.downsample = downsample                       <span class="fm-combinumeral">②</span>

        self.relu = nn.ReLU(inplace=True)


    def forward(self, x):
        conv_out = self.conv2(self.conv1(x))
        identity = x
        if self.downsample is not None:
            identity = self.downsample(x)
        assert identity.shape == conv_out.shape,
            f"Identity {identity.shape} and conv out {conv_out.shape} have different shapes"

        out = self.relu(conv_out + identity)               <span class="fm-combinumeral">③</span>
        return out</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates two conv layers of filter size <span class="math">3 × 3</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> When input and output feature maps are not the same size, the input feature map is downsampled using a <span class="math">1 × 1</span> convolution layer.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a skip connection</p>
<p class="body"><a id="marker-404"/>Notice how the output of the residual block is a function of both the input and the output of the convolutional layer: <code class="fm-code-in-text">ReLU(conv_out+x)</code>. This assumes that <i class="timesitalic">x</i> and <span class="math"><i class="fm-italics">conv</i>_<i class="fm-italics">out</i></span> have the same shape. (Shortly, we discuss what to do when this isn’t the case.) Also note that adding the skip connections does not increase the number of parameters. The shortcut connections are parameter-free. This makes the solution cheap from a computational point of view and is one of the charms of shortcut connections.</p>
<p class="body">Next, let’s implement a residual conv block consisting of a number of basic blocks stacked on top of each other. We have to handle two cases when it comes to basic blocks:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Case 1</i>—Output feature map spatial resolution = Input feature map spatial resolution AND Number of output features = Number of input features. This is the most common case. Since there is no change in the number of features or the spatial resolution of the feature map, we can easily add the input and output via shortcut connections.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Case 2</i>—Output feature map spatial resolution = 1/2 * Input feature map spatial resolution AND Number of output features = 2 * Number of input features. Remember that ResNet uses conv layers with a stride of 2 to downsample. The number of features is also doubled. This is done by the first basic block of every conv block except the second conv block). In this case, the input and output are not the same size. So how do we add them together as part of the skip connection? <span class="math">1 × 1</span> convs are the answer. The spatial resolution of the input feature map is halved, and the number of input features is doubled by using a <span class="math">1 × 1</span> conv with <code class="fm-code-in-text">stride=2</code> and <code class="fm-code-in-text">num_features=2 * num_input_features</code>.</p>
</li>
</ul>
<p class="fm-code-listing-caption" id="listing-11.9-pytorch-code-for-residualconvblock">Listing 11.9 PyTorch code for <code class="fm-code-in-text">ResidualConvBlock</code><a id="marker-405"/></p>
<pre class="programlisting">class ResidualConvBlock(nn.Module):
    def __init__(self, in_channels, num_blocks, reduce_fm_size=True):
        super(ResidualConvBlock, self).__init__()

        num_features = in_channels * 2 if reduce_fm_size else in_channels
        modules = []

        for i in range(num_blocks):                          <span class="fm-combinumeral">①</span>
            if i == 0 and reduce_fm_size:
                stride = 2
                downsample = nn.Sequential( 
                    nn.Conv2d(                               <span class="fm-combinumeral">②</span>
                        in_channels, num_features,
                        kernel_size=1, stride=stride, bias=False),
                    nn.BatchNorm2d(num_features, eps=0.001),
                )
                basic_block = BasicBlock(
                    in_channels=in_channels, num_features=num_features,
                    stride=stride, downsample=downsample)
            else:
                basic_block = BasicBlock(
                    in_channels=num_features, num_features=num_features, stride=1)
            modules.append(basic_block)

        self.conv_block = nn.Sequential(*modules)


    def forward(self, x):
        return self.conv_block(x)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The residual block is a stack of basic blocks.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> <span class="math">1 × 1</span> convs to downsample the input feature map</p>
<p class="body">With this, we are ready to implement ResNet-34.</p>
<p class="fm-code-listing-caption" id="listing-11.10-pytorch-code-for-resnet-34">Listing 11.10 PyTorch code for ResNet-34</p>
<pre class="programlisting">class ResNet34(nn.Module):
    def __init__(self, num_basic_blocks, num_classes):
        super(ResNet, self).__init__()
        conv1 = nn.Sequential(                       <span class="fm-combinumeral">①</span>
            nn.Conv2d(3, 64, kernel_size=7,
                stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64, eps=0.001),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(
                kernel_size=3, stride=2, padding=1)
        )



        assert len(num_basic_blocks) == 4            <span class="fm-combinumeral">②</span>

        conv2 = ResidualConvBlock(                   <span class="fm-combinumeral">③</span>
            in_channels=64, num_blocks=num_basic_blocks[0], reduce_fm_size=False)
        conv3 = ResidualConvBlock(
            in_channels=64, num_blocks=num_basic_blocks[1], reduce_fm_size=True)
        conv4 = ResidualConvBlock(
            in_channels=128, num_blocks=num_basic_blocks[2], reduce_fm_size=True)
        conv5 = ResidualConvBlock(
            in_channels=256, num_blocks=num_basic_blocks[3], reduce_fm_size=True)

        self.conv_backbone = nn.Sequential(*[conv1, conv2, conv3, conv4, conv5])

        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(512, num_classes)


    def forward(self, x):
        conv_out = self.conv_backbone(x)
        conv_out = self.avg_pool(conv_out)
        logits = self.classifier(                    <span class="fm-combinumeral">④</span>
            conv_out.view(conv_out.shape[0], -1))
        return logits</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the first conv layer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> List of size 4, specifying the number of basic blocks per ResidualConvBlock</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates four residual blocks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Flattens the conv feature before passing it to the classifier</p>
<p class="body"><a id="marker-406"/>As discussed earlier, we typically don’t implement our own ResNet. Instead, we use the ready-made implementation from the <i class="fm-italics">torchvision</i> package like this:</p>
<pre class="programlisting">import torchvision
resnet34 = torchvision.models.resnet34()  <span class="fm-combinumeral">①</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates resnet34 from the torchvision package</p>
<p class="body">While we looked at the ResNet-34, there are deeper ResNet architectures like ResNet-50, ResNet-101, and ResNet-151 that use a different version of <code class="fm-code-in-text">BasicBlock</code> called <code class="fm-code-in-text">BottleneckLayer</code>. Similarly, there are several other variants inspired by ResNet, like ResNext, Wide ResNet, and so on. We don’t discuss these individual variants in this book because the core idea behind them remains the same. You are encouraged to read the original papers for a deeper understanding of the subject.</p>
<h3 class="fm-head1" id="pytorch-lightning">11.2.4 PyTorch Lightning</h3>
<p class="body">Let’s revisit the problem of digit classification that we looked at earlier. We primarily discussed the LeNet architecture and implemented it in PyTorch. Now, let’s implement the end-to-end code for training the LeNet model. Instead of doing it in vanilla PyTorch, we use the Lightning framework because it significantly simplifies the model development and training process.</p>
<p class="body">Although PyTorch has all we need to train models, there’s much more to deep learning than attaching layers. When it comes to the actual training, we need to write a lot of boilerplate code, as we have seen in previous examples. This includes transferring data from CPU to GPU, implementing the training driver, and so on. Additionally, if we need to scale training/inferencing on multiple devices/machines, another set of integrations often needs to be done.</p>
<p class="body">PyTorch Lightning is a solution that provides the APIs required to build models, data sets, and so on. It provides clean interfaces with hooks to be implemented. The underlying Lightning framework calls these hooks at appropriate points in the training process. The idea is that Lightning leaves the research logic to us while automating the rest of the boilerplate code. Additionally, Lightning brings in features like multi-GPU training, floating-point 16, and training on TPU inherently without requiring any code changes. More details about PyTorch Lightning can be found at <a class="url" href="https://www.pytorchlightning.ai/tutorials">https://www.pytorchlightning.ai/tutorials</a>.</p>
<p class="body">Training a model using PyTorch Lightning involves three main components: <code class="fm-code-in-text">DataModule</code>, <code class="fm-code-in-text">LightningModule</code>, and <code class="fm-code-in-text">Trainer</code>. Let’s see what each of these does.</p>
<p class="fm-head2" id="datamodule">DataModule</p>
<p class="body"><code class="fm-code-in-text">DataModule</code> is a shareable, reusable class that encapsulates all the steps needed to process data. All data modules must inherit from <code class="fm-code-in-text">LightningDataModule</code>, which provides methods to be overridden. In this specific case, we will implement MNIST as a data module. This data module can now be used across multiple experiments spanning various models and architectures.<a id="marker-407"/></p>
<p class="fm-code-listing-caption" id="listing-11.11-pytorch-code-for-an-mnist-data-module">Listing 11.11 PyTorch code for an MNIST data module</p>
<pre class="programlisting">class MNISTDataModule(LightningDataModule):
    DATASET_DIR = "datasets"

    def __init__(self, transform=None, batch_size=100):
        super(MNISTDataModule, self).__init__()
        if transform is None:
            transform = transforms.Compose(
                [transforms.Resize((32, 32)),
                transforms.ToTensor()])
        self.transform = transform
        self.batch_size = batch_size

    def prepare_data(self):                                  <span class="fm-combinumeral">①</span>
        datasets.MNIST(root = MNISTDataModule.DATASET_DIR,
            train=True, download=True)
        datasets.MNIST(root=MNISTDataModule.DATASET_DIR,
            train=False, download=True)

    def setup(self, stage=None):
        train_dataset = datasets.MNIST(
            root = MNISTDataModule.DATASET_DIR, train=True,
            download=False, transform=self.transform)
        self.train_dataset, self.val_dataset = random_split( <span class="fm-combinumeral">②</span>
            train_dataset, [55000, 5000])

        self.test_dataset = datasets.MNIST(
            root = MNISTDataModule.DATASET_DIR, train = False,
            download = False, transform=self.transform)

    def train_dataloader(self):                              <span class="fm-combinumeral">③</span>
        return DataLoader(
            self.train_dataset, batch_size=self.batch_size,
            shuffle=True, num_workers=0)

    def val_dataloader(self):                                <span class="fm-combinumeral">④</span>
        return DataLoader(
            self.val_dataset, batch_size=self.batch_size,
            shuffle=False, num_workers=0)

    def test_dataloader(self):                               <span class="fm-combinumeral">⑤</span>
        return DataLoader(
            self.test_dataset, batch_size=self.batch_size,
            shuffle=False, num_workers=0)

    @property
    def num_classes(self):                                   <span class="fm-combinumeral">⑥</span>
        return 10</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Download, tokenizes, and prepares the raw data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Splits the training data set into training and validation sets</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates the train data loader, which provides a clean interface for iterating over the data set. It handles batching, shuffling, and fetching data via multiprocessing, all under the hood.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates the val data loader</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates the test data loader</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Number of object categories in the data set</p>
<p class="fm-head2" id="lightningmodule">LightningModule</p>
<p class="body"><code class="fm-code-in-text">LightningModule</code> essentially groups all the research code into a single module, making it self-contained. Notice the clean separation between <code class="fm-code-in-text">DataModule</code> and <code class="fm-code-in-text">LightningModule</code>—this makes it easy to train/evaluate the same model on different data sets. Similarly, different models can be easily trained/evaluated on the same data set.<a id="marker-408"/></p>
<p class="body">A Lightning module consists of the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A model or system of models defined in the <code class="fm-code-in-text">init</code> method</p>
</li>
<li class="fm-list-bullet">
<p class="list">A training loop defined in <code class="fm-code-in-text">training_step</code></p>
</li>
<li class="fm-list-bullet">
<p class="list">A validation loop defined in <code class="fm-code-in-text">validation_step</code></p>
</li>
<li class="fm-list-bullet">
<p class="list">A testing loop defined in <code class="fm-code-in-text">testing_step</code></p>
</li>
<li class="fm-list-bullet">
<p class="list">Optimizers and schedulers defined in <code class="fm-code-in-text">configure_optimizers</code></p>
</li>
</ul>
<p class="body">Let’s see how we can define the LeNet classifier as a Lightning module.</p>
<p class="fm-code-listing-caption" id="listing-11.12-pytorch-code-for-lenet-as-a-lightning-module">Listing 11.12 PyTorch code for LeNet as a Lightning module</p>
<pre class="programlisting">class LeNetClassifier(LightningModule):

    def __init__(self, num_classes):                 <span class="fm-combinumeral">①</span>

        super(LeNetClassifier, self).__init__()
        self.save_hyperparameters()

        self.conv1 = torch.nn.Sequential(
                        torch.nn.Conv2d(
                            in_channels=1, out_channels=6,
                            kernel_size=5, stride=1),
                        torch.nn.Tanh(),

                        torch.nn.AvgPool2d(kernel_size=2))
        self.conv2 = torch.nn.Sequential(
                        torch.nn.Conv2d(
                            in_channels=6, out_channels=16,
                            kernel_size=5, stride=1),
                        torch.nn.Tanh(),
                        torch.nn.AvgPool2d(kernel_size=2))
        self.conv3 = torch.nn.Sequential(
                        torch.nn.Conv2d(
                            in_channels=16, out_channels=120,
                            kernel_size=5, stride=1),
                        torch.nn.Tanh())
        self.fc1 = torch.nn.Sequential(
                        torch.nn.Linear(in_features=120, out_features=84),
                        torch.nn.Tanh())
        self.fc2 = torch.nn.Linear(in_features=84,
            out_features=num_classes)

        self.criterion = torch.nn.CrossEntropyLoss() <span class="fm-combinumeral">②</span>

        self.accuracy = torchmetrics.Accuracy()

    def forward(self, X):                            <span class="fm-combinumeral">③</span>
        conv_out = self.conv3(
            self.conv2(self.conv1(X)))
        batch_size = conv_out.shape[0]
        conv_out = conv_out.reshape(
            batch_size, -1)
        logits = self.fc2(self.fc1(conv_out))
        return logits                                <span class="fm-combinumeral">④</span>

    def predict(self, X):                            <span class="fm-combinumeral">⑤</span>
        logits = self.forward(X)
        probs = torch.softmax(logits, dim=1)
        return torch.argmax(probs, 1)

    def core_step(self, batch):                      <span class="fm-combinumeral">⑥</span>
        X, y_true = batch
        y_pred_logits = self.forward(X)
        loss = self.criterion(y_pred_logits, y_true)
        accuracy = self.accuracy(y_pred_logits, y_true)
        return loss, accuracy

    def training_step(self, batch, batch_idx):       <span class="fm-combinumeral">⑦</span>

        loss, accuracy = self.core_step(batch)
        if self.global_step \% 100 == 0:
            self.log("train_loss", loss, on_step=True, on_epoch=True)
            self.log("train_accuracy", accuracy, on_step=True, on_epoch=True)
        return loss

    def validation_step(self, batch,
        batch_idx, dataset_idx=None):                <span class="fm-combinumeral">⑧</span>
        return self.core_step(batch)

    def validation_epoch_end(self, outputs):         <span class="fm-combinumeral">⑨</span>
        avg_loss = torch.tensor([x[0] for x in outputs]).mean()
        avg_accuracy = torch.tensor([x[1] for x in outputs]).mean()
        self.log("val_loss", avg_loss)
        self.log("val_accuracy", avg_accuracy)
        print(f"Epoch {self.current_epoch},
            Val loss: {avg_loss:0.2f}, Accuracy: {avg_accuracy:0.2f}")
        return avg_loss

    def configure_optimizers(self):                  <span class="fm-combinumeral">⑩</span>
        return torch.optim.SGD(model.parameters(), lr=0.01,
                      momentum=0.9)


    def checkpoint_callback(self):                   <span class="fm-combinumeral">⑪</span>
        return ModelCheckpoint(monitor="val_accuracy", mode="max", save_top_k=1)<a id="marker-40"/></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> In the init method, we typically define the model, the criterion, and any other setup steps required for training the model.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates cross-entropy loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Implements the model’s forward pass. In this case, the input is a batch of images, and the output is the logits. X.shape: [batch_size, C, H, W].</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Logits.shape: [batch_size, num_classes]</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Runs the forward pass, performs softmax to convert the resulting logits into probabilities, and returns the class with the highest probability<a id="marker-409"/></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Abstracts out common functionality between the training and test loops, including the running forward pass, computing loss, and accuracy</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Implements the basic training step: run forward pass, compute loss, accuracy. Logs any necessary values and returns the total loss.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Called at the end of all test steps for each epoch. The output of every test step is available via outputs. Here we compute the average test loss and accuracy by averaging across all test batches.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Implements the basic validation step: run forward pass, compute loss and accuracy, return them.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Configures the SGD optimizer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑪</span> Implements logic to save the model. We save the model with the best val accuracy.</p>
<p class="body">The model is independent of the data. This allows us to potentially run the <code class="fm-code-in-text">LeNetClassifier</code> model on other data modules without any code changes. Note that we are not doing the following steps:<a id="marker-410"/></p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Moving the data to a device</p>
</li>
<li class="fm-list-bullet">
<p class="list">Calling <code class="fm-code-in-text">loss.backward</code></p>
</li>
<li class="fm-list-bullet">
<p class="list">Calling <code class="fm-code-in-text">optimizer.backward</code></p>
</li>
<li class="fm-list-bullet">
<p class="list">Setting <code class="fm-code-in-text">model.train()</code> or <code class="fm-code-in-text">eval()</code></p>
</li>
<li class="fm-list-bullet">
<p class="list">Resetting the gradients</p>
</li>
<li class="fm-list-bullet">
<p class="list">Implementing the trainer loop</p>
</li>
</ol>
<p class="body">All of these are taken care of by PyTorch Lightning, thus eliminating a lot of boilerplate code.</p>
<p class="fm-head2" id="trainer">Trainer</p>
<p class="body">We are ready to train our model, which can be done using the <code class="fm-code-in-text">Trainer</code> class. This abstraction achieves the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">We maintain control over all aspects via PyTorch code without an added abstraction.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The trainer uses best practices embedded by contributors and users from top AI labs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The trainer allows us to override any key part that we don’t want automated.</p>
</li>
</ul>
<p class="fm-code-listing-caption" id="listing-11.13-pytorch-code-for-trainer">Listing 11.13 PyTorch code for <code class="fm-code-in-text">Trainer</code></p>
<pre class="programlisting">dm = MNISTDataModule()                               <span class="fm-combinumeral">①</span>
model = LeNetClassifier(num_classes=dm.num_classes)  <span class="fm-combinumeral">②</span>
exp_dir = "/tmp/mnist"
trainer = Trainer(                                   <span class="fm-combinumeral">③</span>
        default_root_dir=exp_dir,
        callbacks=[model.checkpoint_callback()],
        gpus=torch.cuda.device_count(), # Number of GPUs to run on
        max_epochs=10,
        num_sanity_val_steps=0
    )
trainer.fit(model, dm)                               <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the data set</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates the trainer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Trains the model</p>
<p class="body">Note that we do not write the trainer loop: we just call <code class="fm-code-in-text">trainer.fit</code> to train the model. Additionally, the logging automatically enables us to look at the loss and accuracy curves via TensorBoard.</p>
<p class="fm-code-listing-caption" id="listing-11.14-pytorch-code-for-inferencing-a-model">Listing 11.14 PyTorch code for inferencing a model</p>
<pre class="programlisting">X, y_true = (iter(dm.test_dataloader())).next()
with torch.no_grad():
    y_pred = model.predict(X) <span class="fm-combinumeral">①</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Runs model.predict()</p>
<p class="body">To run inferencing using the trained model, we run <code class="fm-code-in-text">model.predict</code> on the input.</p>
<h2 class="fm-head" id="object-detection-a-brief-history">11.3 Object detection: A brief history</h2>
<p class="body"><a id="marker-411"/>Until now, we have discussed the classification problem wherein we categorize an image as 1 of <i class="timesitalic">N</i> object categories. But in many cases, this is not sufficient to truly describe an image. Consider figure <a class="url" href="#fig-multi-object-image">11.11</a>—a very realistic image with four animals standing one on top of another, posing for the camera. It would be useful to know the object categories of each of the animals and their location (bounding-box coordinates) in the image. This is referred to as the object detection/localization problem. So, how do we localize objects in images?</p>
<p class="body">Let’s say we could extract regions in the image so that each region contained only one object. We could then run an image classifier deep neural network (which we looked at earlier) to classify each region and select the regions with the highest confidence. This was the approach adopted by one of the first deep learning-based object detectors, a region-based CNN (R-CNN; <a class="url" href="https://arxiv.org/pdf/1311.2524.pdf">https://arxiv.org/pdf/1311.2524.pdf</a>). Let’s look at this in more detail.</p>
<h3 class="fm-head1" id="r-cnn">11.3.1 R-CNN</h3>
<p class="body">The R-CNN approach to object detection consists of three main stages:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Selective search to identify regions of interest</i>—This step uses a computer vision-based algorithm capable of extracting candidate regions. We do not go into the details of the selective search; you are encouraged to go through the original paper to understand the details. Selective search generates around 2,000 region proposals per image.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Feature extraction</i>—A deep convolution neural network extracts features from each region of interest. Since deep neural networks typically take in fixed-sized inputs, the regions (which could be arbitrarily sized) are warped into a fixed size before being fed into the deep neural network.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Classification/Localization</i>—A class-specific support vector machine (SVM) is trained on the extracted features to classify the region. Additionally, bounding-box regressors are added to fine-tune the object’s location within the region. During training, each region is assigned a ground-truth (GT) class based on its overlap with GT boxes. It is assigned a positive label if there is a high overlap and a negative label otherwise.</p>
</li>
</ul>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="533" id="fig-multi-object-image" src="../../OEBPS/Images/CH11_F11_Chaudhury.png" width="494"/></p>
<p class="figurecaption">Figure 11.11 An image with multiple objects of different shapes and sizes</p>
</div>
<h3 class="fm-head1" id="subsec-fast-rcnn">11.3.2 Fast R-CNN</h3>
<p class="body"><a id="marker-412"/>One of the biggest disadvantages of the R-CNN-based approach is that we have to extract features for every region proposal independently. So, if we generate 2,000 proposals for a single image, we have to run 2,000 forward passes to extract the region features. This is prohibitively expensive and extremely slow (during both training and inference). Additionally, training is a multistage pipeline—selective search, the deep network, the SVMs on top of the features, and the bounding-box regressors—that is cumbersome to train and inference. To solve these problems, the authors of the R-CNN introduced a new technique called a Fast R-CNN <a class="url" href="https://arxiv.org/pdf/1504.08083.pdf">https://arxiv.org/pdf/1504.08083.pdf</a>). It significantly improved speeds: it is <span class="math">9×</span> faster than the R-CNN during training and <span class="math">213×</span> faster at test time. Additionally, it improves the quality of object detection.</p>
<p class="body">Fast R-CNN makes two major contributions:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Region of interest (RoI) pooling</i>—As mentioned, one of the fundamental issues with R-CNN is the need for multiple forward passes to extract the features for the region proposals of a single image. Instead, can we extract the features in one go? This problem is solved using RoI pooling. The Fast R-CNN uses the entire image as the input to the CNN instead of a single region proposal. Then, the RoIs (region proposal bounding boxes) are used on top of the CNN output to extract the region features in one pass. We will go into the details of RoI pooling as part of our Faster R-CNN discussion.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Multitask loss</i>—The Fast R-CNN eliminates the need to use SVMs. Instead, the deep neural network does both classification and bounding-box regression. Unlike R-CNN, which only uses deep networks for feature extraction, the Fast R-CNN is more end-to-end. It is a single architecture for region proposal feature extraction, classification, and regression.</p>
</li>
</ul>
<p class="body">The high-level algorithm is as follows:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Use selective search to generate 2,000 region proposals/RoIs per image.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In a single pass of the Fast R-CNN, extract all the RoI features in a single pass using RoI pooling and then classify and localize objects using the classification and regression heads.</p>
</li>
</ol>
<p class="body">Since the feature extraction for all the region proposals happens in one pass, this approach is significantly faster than the R-CNN, where every proposal needs a separate forward pass. Additionally, since the neural network is trained end to end—that is, asked to do classification and regression—the accuracy of object detection is also improved.</p>
<h3 class="fm-head1" id="faster-r-cnn">11.3.3 Faster R-CNN</h3>
<p class="body"><a id="marker-413"/>Why settle for fast when we can be faster? The Fast R-CNN was significantly faster than the R-CNN. However, it still needed selective search to be run to obtain region proposals. The selective-search algorithm can only be run on CPUs. Additionally, the algorithm is slow and time-consuming. Thus it became a bottleneck. Is there a way to get rid of selective search?</p>
<p class="body">The obvious idea to consider is using deep networks to generate region proposals. This is the core idea of Faster R-CNN (FRCNN; <a class="url" href="https://arxiv.org/pdf/1506.01497.pdf">https://arxiv.org/pdf/1506.01497.pdf</a>): it eliminates the need for selective search and lets a deep network learn the region proposals. It was one of the first near-real-time object detectors. Since we are using a deep network to learn the region proposals, the region proposals are also better. Thus the resulting accuracy of the overall architecture is also much better.</p>
<p class="body">We can view the FRCNN as consisting of two core modules:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Region proposal network (RPN)</i>—This is the module responsible for generating the region proposals. RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">R-CNN module</i>—This is the same as the Fast R-CNN. It receives a bunch of region proposals and performs RoI pooling followed by classification and regression.</p>
</li>
</ul>
<p class="body">Another important thing to note is that the RPN and the R-CNN module share the same convolutional layers: the weights are shared rather than learning two separate networks. In the next section, we discuss the Faster R-CNN in detail.</p>
<h2 class="fm-head" id="faster-r-cnn-a-deep-dive">11.4 Faster R-CNN: A deep dive</h2>
<p class="body"><a id="marker-414"/>Figure <a class="url" href="#fig-frcnn-architecture">11.12</a> shows the high-level architecture of the FRCNN. The convolutional layers (which we also call the convolutional backbone) extract feature maps from the input image. The RPN operates on these feature maps and emits candidate RoIs. The RoI pooling layer generates a fixed-sized feature vector for each region of interest and passes it on to a set of FC layers that emit softmax probability estimates over <i class="timesitalic">K</i> object classes (plus a catch-all “background” class) and four numbers representing the bounding-box coordinates for each of the <i class="timesitalic">K</i> classes. Let’s look at each of the components in more detail.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="772" id="fig-frcnn-architecture" src="../../OEBPS/Images/CH11_F12_Chaudhury.png" width="791"/></p>
<p class="figurecaption">Figure 11.12 architecture. (Source: “Faster R-CNN: Toward real-time proposal networks”; <a class="url" href="https://arxiv.org/abs/1506.01497">https://arxiv.org/abs/1506.01497</a>.)</p>
</div>
<h3 class="fm-head1" id="convolutional-backbone">11.4.1 Convolutional backbone</h3>
<p class="body">In the original implementation, the FRCNN used the convolution layers of VGG-16 as the convolutional backbone for both the RPN and the R-CNN modules. There has been one minor modification: the last pooling layer after the fifth convolution layer (conv5) is removed. As we’ve discussed regarding VGG architectures earlier, VGG reduces the spatial size of the feature map by 2 in every conv block via max pooling. Since the last pooling layer is removed, the spatial size is reduced by a factor of <span class="math">2<sup class="fm-superscript">4</sup> = 16</span>. So a <span class="math">224 × 224</span> image is reduced to a <span class="math">14 × 14</span> feature map at the output. Similarly, an <span class="math">800 × 800</span> image would be reduced to a <span class="math">50 × 50</span> feature map.</p>
<h3 class="fm-head1" id="region-proposal-network">11.4.2 Region proposal network</h3>
<p class="body"><a id="marker-415"/>The RPN takes in an image (of any arbitrary size) as input and emits a set of rectangular proposals that could potentially contain objects as output. The RPN operates on top of the convolutional feature map output by the last shared convolution layer. With the VGG backbone, an input image of size (h, w) is scaled down to (h/16, w/16). So each <span class="math">16 × 16</span> spatial region in the input image is reduced to a single point on the convolutional feature map. Thus each point in the output convolutional feature map represents a <span class="math">16 × 16</span> patch in the input image. The RPN operates on top of this feature map. Another subtle point to remember is that while each point in the convolutional feature map is chosen to correspond to a <span class="math">16 × 16</span> patch, it has a significantly larger receptive field (the region in the input feature map that a particular output feature is affected by). The embedding at each point in the feature map is thus, in effect, the digest of a large receptive field.</p>
<p class="fm-head2" id="anchors">Anchors</p>
<p class="body">A key aspect of the object-detection problem is the variety of object sizes and shapes. Objects can range from very small (cats) to very large elephants). Additionally, objects can have different aspect ratios. Some objects may be wide, some may be tall, and so on. A naive solution is to have a single neural network detector head capable of identifying and recognizing all these objects of varying sizes and shapes. As you can imagine, this would make the job of the neural network detector extremely complex. A simpler solution is to have a wide variety of neural network detector heads, each responsible for solving a much simpler problem. For example, one head will only focus on large, tall objects and will only fire when such objects are present in the image. The other heads will focus on other sizes and aspect ratios. We can think of each head as being responsible for doing a single simple job. This type of setup greatly aids and benefits learning.</p>
<p class="body">This was the intuition behind the introduction of <i class="fm-italics">anchors</i>. Anchors are like reference boxes of varying shapes and sizes. All proposals are made relative to anchors. Each anchor is uniquely characterized by its size and aspect ratio and is tasked with detecting similarly shaped objects in the image. At each sliding-window location, we have multiple anchors spanning different sizes and aspect ratios. The original FRCNN architecture supported nine anchor configurations spanning three sizes and three aspect ratios, thus supporting a wide variety of shapes. These correspond to anchor boxes of scales (8, 16, 32) and aspect ratios (0.5, 1.0, and 2.0), respectively (see figure <a class="url" href="#fig-anchors">11.13</a>). Anchors are now ubiquitous across object detectors.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="469" id="fig-anchors" src="../../OEBPS/Images/CH11_F13_Chaudhury.png" width="892"/></p>
<p class="figurecaption">Figure 11.13 The left column shows the various grid-point locations on the output convolution feature original implementation) anchors across multiple sizes and aspect ratios. The right column shows the various anchors at a particular grid point.</p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for generating anchors, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.<a id="marker-416"/></p>
<p class="fm-code-listing-caption" id="code-generate-anchors-at-grid-point">Listing 11.15 PyTorch code to generate anchors at a particular grid point</p>
<pre class="programlisting">def generate_anchors_at_grid_point(
    ctr_x, ctr_y, subsample, scales, aspect_ratios):
    anchors = torch.zeros(
        (len(aspect_ratios) * len(scales), 4), dtype=torch.float)

    for i, scale in enumerate(scales):
        for j, aspect_ratio in enumerate(aspect_ratios): <span class="fm-combinumeral">①</span>


            w = subsample * scale * torch.sqrt(aspect_ratio)
            h = subsample * scale * torch.sqrt(1 / aspect_ratio)


            xtl = ctr_x - w / 2                          <span class="fm-combinumeral">②</span>
            ytl = ctr_y - h / 2
            xbr = ctr_x + w / 2
            ybr = ctr_y + h / 2

            index = i * len(aspect_ratios) + j
            anchors[index] = torch.tensor([xtl, ytl, xbr, ybr])
    return anchors</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates the height and width for different scales and aspect ratios</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates a bounding box centered around ctr_x, ctr_y) with width w, and height h</p>
<p class="fm-code-listing-caption" id="listing-11.16-pytorch-code-to-generate-all-anchors-for-a-given-image">Listing 11.16 PyTorch code to generate all anchors for a given image</p>
<pre class="programlisting"> def generate_all_anchors(                       <span class="fm-combinumeral">①</span>


     input_img_size, subsample, scales, aspect_ratios):

    _, h, w = input_img_size

    conv_feature_map_size = (h//subsample, w//subsample)

    all_anchors = []                             <span class="fm-combinumeral">②</span>


    ctr_x = torch.arange(
        subsample/2, conv_feature_map_size[1]*subsample+1, subsample)
    ctr_y = torch.arange(
        subsample/2, conv_feature_map_size[0]*subsample+1, subsample)

    for y in ctr_y:
        for x in ctr_x:
            all_anchors.append(
                generate_anchors_at_grid_point(  <span class="fm-combinumeral">③</span>
                    x, y, subsample, scales, aspect_ratios))

    all_anchors = torch.cat(all_anchors)
    return all_anchors


input_img_size = (3, 800, 800)                   <span class="fm-combinumeral">④</span>
c, height, width = input_img_size
scales = torch.tensor([8, 16, 32], dtype=torch.float)
aspect_ratios = torch.tensor([0.5, 1, 2])
subsample = 16
anchors = generate_all_anchors(input_img_size, subsample, scales, aspect_ratios)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> This isn’t the most efficient way to generate anchors. We’ve written simple code to ease understanding.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates anchor boxes centered at every point in the conv feature map, which corresponds to a <span class="math">16 × 16</span> (subsample, subsample) region in the input</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses a function defined in listing <a class="url" href="#code-generate-anchors-at-grid-point">11.15</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines config parameters and generates anchors</p>
<p class="body"><a id="marker-417"/>The RPN slides a small network over the output convolution feature map. The small network operates on an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> spatial window of the convolution feature map. At each sliding-window location, it generates a lower-dimensional feature vector (512 dimensions for VGG) that is fed into a box-regression layer (reg) and a box-classification layer (cls). For each of the anchor boxes centered at that sliding window location, the classifier predicts <i class="fm-italics">objectness</i>: a value from 0 to 1, where 1 indicates the presence of the object and the regressor predicts the region proposal relative to the anchor box. This architecture is naturally implemented with an <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> convolutional layer followed by two sibling <span class="math">1 × 1</span> convolutional layers (for reg and cls), respectively. The original implementation in the FRCNN paper uses <span class="math"><i class="fm-italics">n</i> = 3</span>, which results in an effective receptive field of 228 pixels when using the VGG backbone. Figure <a class="url" href="#fig-rpn-architecture">11.14</a> illustrates this in detail. Note that this network consists of only convolutional layers. Such an architecture is called a <i class="fm-italics">fully convolutional network</i> FCN). FCNs do not have an input size restriction. Because they</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="465" id="fig-rpn-architecture" src="../../OEBPS/Images/CH11_F14_Chaudhury.png" width="373"/></p>
<p class="figurecaption">Figure 11.14 RPN architecture. From each sliding window, a 512-dimensional feature vector is generated using <span class="math">3 × 3</span> convs. A <span class="math">1 × 1</span> conv layer (classifier) takes the 512-dimensional feature Similarly, another <span class="math">1 × 1</span> conv layer (regressor) generates <span class="math">4<i class="fm-italics">k</i></span> bounding-box coordinates from the 512-dimensional feature vector.</p>
</div>
<p class="body">consist of only convolution layers, they can work with arbitrary-sized inputs. In the FCN, the combination of the <span class="math"><i class="fm-italics">n</i> × <i class="fm-italics">n</i></span> and <span class="math">1 × 1</span> layers is equivalent to applying an FC layer over every embedding at each point in the convolutional feature map. Also, because we are convolving a convolutional network on top of the feature map to generate the regression and classification scores, the convolutional weights are common/shared across different positions on the feature map. This makes the approach translation invariant. A cat at the top of the image and a cat at the bottom of the image are picked up by the same anchor configuration (scale, aspect ratio) if they are similarly sized.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the fully convolutional network of the RPN, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.<a id="marker-418"/></p>
<p class="fm-code-listing-caption" id="listing-11.17-pytorch-code-for-the-fcn-of-the-rpn">Listing 11.17 PyTorch code for the FCN of the RPN</p>
<pre class="programlisting">class RPN_FCN(nn.Module):

    def __init__(self, k, in_channels=512):  <span class="fm-combinumeral">①</span>
        super(RPN_FCN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(
                in_channels, 512, kernel_size=3,
                stride=1, padding=1),
            nn.ReLU(True))
        self.cls = nn.Conv2d(512, 2*k, kernel_size=1)
        self.reg = nn.Conv2d(512, 4*k, kernel_size=1)


    def forward(self, x):
        out = self.conv(x)                   <span class="fm-combinumeral">②</span>

        rpn_cls_scores = self.cls(out).view( <span class="fm-combinumeral">③</span>
            x.shape[0], -1, 2)
        rpn_loc = self.reg(out).view(        <span class="fm-combinumeral">④</span>
            x.shape[0], -1, 4)

        <span class="fm-combinumeral">⑤</span>
       return rpn_cls_scores       ,rpn_loc  <span class="fm-combinumeral">⑥</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the small network that is convolved over the output conv feature map. It consists of a <span class="math">3 × 3</span> conv layer followed by a <span class="math">1 × 1</span> conv layer for classification and another <span class="math">1 × 1</span> conv layer for regression.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Output of the backbone: a convolutional feature map of size (batch_size, in_channels, h, w)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts (batch_size, h, w, 2k) to batch_size, h*w*k, 2)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Converts (batch_size, h, w, 4k) to batch_size, h*w*k, 4)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> (batch_size, num_anchors, 2) tensor representing the classification score for each anchor box</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> (batch_size, num_anchors, 4) tensor representing the box coordinates relative to the anchor box</p>
<p class="fm-head2" id="generating-gt-for-an-rpn">Generating GT for an RPN</p>
<p class="body"><a id="marker-419"/>So far, we have generated many anchor bounding boxes and a neural network capable of generating the classification and regression offsets for every anchor. While training the RPN, we need to provide a target GT) that both the classifier and regressor should predict for each anchor box. To do so, we need to look at the objects in the image and assign them to relevant anchors that contain the object. The idea is as follows: out of the thousands of anchors, <i class="fm-italics">the anchors that contain most of the object should try predicting and localizing the object.</i> We saw earlier that the intuition behind creating anchors was to ensure that each anchor is responsible for one particular type of object shape, aspect ratio). Thus it makes sense that only anchors that contain the object are responsible for classifying it.</p>
<p class="body">To measure whether the object lies within the anchor, we rely on intersection over union (IoU) scores. The IoU between two bounding boxes is defined as <span class="math">(area of overlap)/(area of union)</span>. So, if the two bounding boxes are very similar, their overlap is high, and their union is close to the overlap, resulting in a high IoU. If the two bounding boxes are varied, then their area of overlap is minimal, resulting in a low IoU (see figure <a class="url" href="#fig-iou">11.15</a>).</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="368" id="fig-iou" src="../../OEBPS/Images/CH11_F15_Chaudhury.png" width="739"/></p>
<p class="figurecaption">Figure 11.15 section of the two areas divided by the union of the two areas.</p>
</div>
<p class="body">FRCNN provides some guidelines for assigning labels to the anchor boxes:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">We assign a positive label 1 (which represents an object being present in the anchor box) to two kinds of anchors:</p>
<ul class="calibre7">
<li class="fm-list-bullet">
<p class="list">The anchor(s) with the greatest IoU overlap with a GT box</p>
</li>
<li class="fm-list-bullet">
<p class="list">An anchor that has an IoU overlap greater than 0.7 with the GT box</p>
</li>
</ul>
</li>
<li class="fm-list-bullet">
<p class="list">We assign a negative label 0 (which represents no object being present in the anchor box, implying that it contains only background) to a non-positive anchor if its IoU ratio is less than 0.3 for all GT boxes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Anchors that are neither positive nor negative do not contribute to the training objective.</p>
</li>
</ul>
<p class="body">Note that a single GT object may assign positive labels to multiple anchors. These outputs must be suppressed later to prevent duplicate detections (we discuss this in the subsequent sections). Also, any anchor box that lies partially outside the image is ignored.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for assigning GT labels to anchor boxes, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.<a id="marker-420"/></p>
<p class="fm-code-listing-caption" id="listing-11.18-pytorch-code-to-assign-gt-labels-for-each-anchor-box">Listing 11.18 PyTorch code to assign GT labels for each anchor box</p>
<pre class="programlisting">valid_indices = torch.where(
    (anchors[:, 0] &gt;=0) &amp;
    (anchors[:, 1] &gt;=0) &amp;
    (anchors[:, 2] &lt;=width) &amp;                         <span class="fm-combinumeral">①</span>
    (anchors[:, 3] &lt;=height))[0]

rpn_valid_labels = -1 * torch.ones_like(              <span class="fm-combinumeral">②</span>
    valid_indices, dtype=torch.int)

valid_anchor_bboxes = anchors[valid_indices]          <span class="fm-combinumeral">③</span>

ious = torchvision.ops.box_iou(                       <span class="fm-combinumeral">④</span>
    gt_bboxes, valid_anchor_bboxes)

assert ious.shape == torch.Size(
    [gt_bboxes.shape[0], valid_anchor_bboxes.shape[0]])

gt_ious_max = torch.max(ious, dim=1)[0]               <span class="fm-combinumeral">⑤</span>

# Find all the indices where the IOU = highest GT IOU 
gt_ious_argmax = torch.where(                         <span class="fm-combinumeral">⑥</span>
    gt_ious_max.unsqueeze(1).repeat(1, gt_ious_max.shape[1]) == ious)[1]

anchor_ious_argmax = torch.argmax(ious, dim=0)        <span class="fm-combinumeral">⑦</span>
anchor_ious = ious[anchor_ious_argmax, torch.arange(len(anchor_ious_argmax))]

pos_iou_threshold  = 0.7
neg_iou_threshold = 0.3

rpn_valid_labels[anchor_ious &lt; neg_iou_threshold] = 0 <span class="fm-combinumeral">⑧</span>

rpn_valid_labels[anchor_ious &gt; pos_iou_threshold] = 1 <span class="fm-combinumeral">⑨</span>

rpn_valid_labels[gt_ious_argmax] = 1                  <span class="fm-combinumeral">⑩</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Finds valid anchors that lie completely inside the image</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Assigns s label of -1 (not any class) for each valid anchor</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Obtains the valid anchor boxes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Tensor of shape (num_gt_bboxes, num_valid_anchor_bboxes), representing the IoU between the GT and anchors</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Finds the highest IoU for every GT bounding box</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Finds all the indices where the IOU = highest GT IOU</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Finds the highest IoU for every anchor box</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Assigns 0 (background) for negative anchors where IoU &lt; 0.3</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Assigns 1 (objectness) for positive anchor where IoU &gt; 0.7</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> For every GT bounding box, assigns the anchor with the highest IoU as a positive anchor</p>
<p class="fm-head2" id="dealing-with-imbalance">Dealing with imbalance</p>
<p class="body"><a id="marker-421"/>Given our strategy of assigning labels to anchors, notice that the number of negative anchors is significantly greater than the number of positive anchors. For example, for the example image, we obtained only 24 positive anchors as opposed to 7,439 negative anchors. If we train directly on such an imbalanced data set, neural networks can typically learn a local minimum by classifying every anchor as a negative anchor. In our example, if we predicted every anchor to be a negative anchor, our resulting accuracy would be <span class="math">7439/(7439+22)</span>: 99.7%. However, the resulting neural network is practically useless because it has not learned anything. In other words, the imbalance will lead to bias toward the dominant class. To deal with this imbalance, there are typically three strategies:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Undersampling</i>—Sample less of the dominant class.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Oversampling</i>—Sample more of the less-dominant class.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Weighted loss</i>—Set the cost for misclassifying less-dominant classes much higher than the dominant class.</p>
</li>
</ul>
<p class="body">FRCNN utilizes the idea of undersampling. For a single image, there are multiple positive and negative anchors. From these thousands of anchors, we randomly sample 256 anchors in an image to compute the loss function, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the minibatch with negative ones.</p>
<p class="fm-head2" id="assigning-targets-to-anchor-boxes">Assigning targets to anchor boxes</p>
<p class="body">We have seen how to sample and assign labels to anchors. The next question is how to come up with the regression targets:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Case 1: label <span class="math">= −1</span></i>—Unsampled/invalid anchor. These do not contribute to the training objective, so regression targets do not matter.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Case 2: label <span class="math">= 0</span></i>—Background anchor. These anchors do not contain any objects, so they also should not contribute to regression.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Case 3: label <span class="math">= 1</span></i>—Positive anchor. These anchors contain objects. We need to generate regression targets for these anchors.</p>
</li>
</ul>
<p class="body">Let’s consider only the case of positive anchors. The key intuition here is that <i class="fm-italics">the anchors already contain a majority of the object</i>. Otherwise, they wouldn’t have become positive anchors. So there is already significant overlap between the anchor and the object in question. Therefore it makes sense to learn the offset from the anchor bounding box to the object bounding box. The regressor is tasked with learning this offset: that is, what delta we must make to the anchor bounding box for it to become the object bounding box. the FRCNN adopts the following parameterization:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">t<sub class="fm-subscript">x</sub></i> = (<i class="fm-italics">x</i> - <i class="fm-italics">x<sub class="fm-subscript">a</sub></i>)/<i class="fm-italics">w<sub class="fm-subscript">a</sub></i></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">t<sub class="fm-subscript">y</sub></i> = (<i class="fm-italics">y</i> - <i class="fm-italics">y<sub class="fm-subscript">a</sub></i>)/<i class="fm-italics">h<sub class="fm-subscript">a</sub></i></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">t<sub class="fm-subscript">w</sub></i> = <i class="fm-italics">log</i>(<i class="fm-italics">w</i>/<i class="fm-italics">w<sub class="fm-subscript">a</sub></i>)</span><br class="calibre20"/>
<span class="math"><i class="fm-italics">t<sub class="fm-subscript">h</sub></i> = <i class="fm-italics">log</i>(<i class="fm-italics">h</i>/<i class="fm-italics">h<sub class="fm-subscript">a</sub></i>)</span></p>
<p class="fm-equation-caption">Equation 11.2 <span class="calibre" id="eq-frcnn-target-coords"/></p>
<p class="body">where <i class="timesitalic">x</i>, <i class="timesitalic">y</i>, <i class="timesitalic">w</i>, and <i class="timesitalic">h</i> denote the GT bounding box’s center coordinates and its width and height, and <i class="timesitalic">x<sub class="fm-subscript">a</sub></i>, <span class="math"><i class="fm-italics">y</i><i class="fm-italics"><sub class="fm-subscript">a</sub></i></span>, <i class="timesitalic">w<sub class="fm-subscript">a</sub></i>, and <i class="timesitalic">h<sub class="fm-subscript">a</sub></i> denote the anchor bounding box’s center coordinates and its width and height. <i class="timesitalic">t<sub class="fm-subscript">x</sub></i>, <i class="timesitalic">t<sub class="fm-subscript">y</sub></i>, <i class="timesitalic">t<sub class="fm-subscript">w</sub></i>, and <i class="timesitalic">t<sub class="fm-subscript">h</sub></i> are the regression targets. The regressor is, in effect, learning to predict the delta between the anchor bounding box and the GT bounding box.<a id="marker-422"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for assigning regression targets to anchor boxes, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.</p>
<p class="fm-code-listing-caption" id="listing-11.19-pytorch-code-to-assign-regression-targets-for-each-anchor-box">Listing 11.19 PyTorch code to assign regression targets for each anchor box</p>
<pre class="programlisting">def transform_bboxes(bboxes):                      <span class="fm-combinumeral">①</span>
    height = bboxes[:, 3] - bboxes[:, 1]
    width = bboxes[:, 2] - bboxes[:, 0]
    x_ctr = bboxes[:, 0] + width / 2
    y_ctr = bboxes[:, 1] +  height /2 
    return torch.stack(                            <span class="fm-combinumeral">②</span>
        [x_ctr, y_ctr, width, height], dim=1)




def get_regression_targets(roi_bboxes, gt_bboxes): <span class="fm-combinumeral">③</span>

    assert roi_bboxes.shape == gt_bboxes.shape
    roi_bboxes_t = transform_bboxes(roi_bboxes)
    gt_bboxes_t = transform_bboxes(gt_bboxes)
    tx = (gt_bboxes_t[:, 0] - roi_bboxes_t[:, 0]) / roi_bboxes_t[:, 2]
    ty = (gt_bboxes_t[:, 1] - roi_bboxes_t[:, 1]) / roi_bboxes_t[:, 3]
    tw = torch.log(gt_bboxes_t[:, 2] / roi_bboxes_t[:, 2])
    th = torch.log(gt_bboxes_t[:, 3] / roi_bboxes_t[:, 3])

    return  torch.stack([tx, ty, tw, th], dim=1)   <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> (n, 4) tensor in (xtl, ytl, xbr, br) format</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> (n, 4 tensor) in (x, y, w, h) format</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> (n, 4) tensors representing the bounding boxes for the region of interest and GT, respectively</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> (n, 4) tensor containing the regression targets</p>
<p class="fm-head2" id="rpn-loss-function">RPN loss function</p>
<p class="body">We have defined the RPN fully convolutional network and how we can generate labels and regression targets for the outputs of the RPN FCN. Now we need to discuss the loss function that enables us to train the RPN. As you would expect, there are two loss terms:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Classification loss</i>—Applies to both the positive and negative anchors. We use the standard cross-entropy loss used in any standard classifier.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Regression loss</i>—Applies <i class="fm-italics">only</i> to the positive anchors. Here we use smooth L1 loss, which is defined as</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned} L_{1;smooth} = \begin{cases}
        0.5 (x_{n} - y_{n})^2 / beta, &amp; \text{if } |x_{n} - y_{n}|
&lt; beta \\
        |x_{n} - y_{n}| - 0.5 * beta, &amp; \text{otherwise }
        \end{cases}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_11-03.png" width="398"/></p>
</div>
<p class="fm-equation-caption">Equation 11.3 <span class="calibre" id="eq-smooth-l1-loss"/></p>
<p class="body"><a id="marker-423"/>We can think of smooth L1 loss as a combination of L1 and L2 loss. If the value is &lt; beta, it behaves like an L2 loss (mean squared error [MSE]). Otherwise, it behaves like an L1 loss. In the case of the FRCNN, beta is set to 1. The intuition behind this is simple. If we use pure L2 loss (MSE), then higher loss terms contribute to exponential loss because of the quadratic nature of the loss. This can lead to a bias where loss can be reduced by focusing on high-value items. Instead, if we use pure L1 loss, the higher loss terms still contribute more loss, but the effect is linear instead of quadratic. This still has a slightly worse bias toward higher loss terms. We get the best of both worlds by using L2 loss when the loss values are small and L1 loss when the loss values are large. When the loss value is small, because we are using L2 loss, its contribution is exponential/quadratic. And when the loss value is high, it still contributes linearly via L1 loss. Thus the network is incentivized to pay attention to low- and high-loss items.</p>
<p class="body">Overall loss for an image can be defined as follows:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;L_{cls} = \frac{\sum_{i}CrossEntropy(p_i, p_i^{*})} { N_{cls}}
\nonumber \\
&amp;L_{reg} = \frac{\sum_{i}p_i^{*} L_{1;smooth}(t_i , t_i^{*})}{N_{pos}} \nonumber \\
&amp;L_{RPN} =  L_{cls} + \lambda L_{reg}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="145" src="../../OEBPS/Images/eq_11-04.png" width="249"/></p>
</div>
<p class="fm-equation-caption">Equation 11.4 <span class="calibre" id="eq-rpn-overall-loss"/></p>
<p class="body">where, <i class="timesitalic">p<sub class="fm-subscript">i</sub></i> is the predicted objectness probability for the anchor <i class="timesitalic">i</i>. <span class="math"><i class="fm-italics">p<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">*</sup></span> is the true objectness label for anchor <i class="timesitalic">i</i>. It is 1 if the anchor is positive and 0 if the anchor is negative. <i class="timesitalic">t<sub class="fm-subscript">i</sub></i> = <span class="math">(<i class="fm-italics">t<sub class="fm-subscript">x</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">y</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">w</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">h</sub></i>)</span> are the regression predictions for anchor <i class="timesitalic">i</i>, <span class="math"><i class="fm-italics">t<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">*</sup></span> = <span class="math">(<i class="fm-italics">t<sub class="fm-subscript">x</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">y</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">w</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">h</sub></i><sup class="fm-superscript">*</sup>)</span> are the regression targets for anchor <i class="timesitalic">i</i>, <i class="timesitalic">N<sub class="fm-subscript">cls</sub></i> is the number of anchors, and <i class="timesitalic">N<sub class="fm-subscript">pos</sub></i> is the number of positive anchors.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the RPN loss function, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.</p>
<p class="fm-code-listing-caption" id="listing-11.20-pytorch-code-for-the-rpn-loss-function">Listing 11.20 PyTorch code for the RPN loss function</p>
<pre class="programlisting">def rpn_loss(
    rpn_cls_scores, rpn_loc, rpn_labels,
    rpn_loc_targets, lambda_ = 10):                  <span class="fm-combinumeral">①</span>


    classification_criterion = nn.CrossEntropyLoss( 
        ignore_index=-1)                             <span class="fm-combinumeral">②</span>
    reg_criterion = nn.SmoothL1Loss(reduction="sum")

    cls_loss = classification_criterion(rpn_cls_scores, rpn_labels)

    positive_indices = torch.where(rpn_labels==1)[0] <span class="fm-combinumeral">③</span>
    pred_positive_anchor_offsets = rpn_loc[positive_indices]
    gt_positive_loc_targets = rpn_loc_targets[positive_indices]
    reg_loss = reg_criterion(
        pred_positive_anchor_offsets,
        gt_positive_loc_targets) / len(positive_indices)
    return {
        "rpn_cls_loss": cls_loss,
        "rpn_reg_loss": reg_loss,
        "rpn_total_loss": cls_loss + lambda_* reg_loss

    }</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> rpn_cls_scores: (num_anchors, 2) tensor representing RPN classifier scores for each anchor. rpn_loc: (num_anchors, 4) tensor representing RPN regressor predictions for each anchor. rpn_labels: (num_anchors) representing the class for each anchor (-1, 0, 1). rpn_loc_targets: (num_anchors, 4) tensor representing RPN regressor targets for each anchor.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Ignores -1 as they are not sampled</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Finds the positive anchors</p>
<p class="fm-head2" id="generating-region-proposals">Generating region proposals</p>
<p class="body"><a id="marker-424"/>We have so far discussed how the RPN works. The RPN predicts objectness and the regression offsets for every anchor. The next task is to generate good region RoIs and use them for training the R-CNN module. Since we are emitting objectness and regression offsets for every anchor, we have thousands of predictions. We cannot use all of them as RoIs. We need to generate the best RoIs from these scores and offsets to train our R-CNN. An obvious way to do this is to rely on the objectness scores: the higher the objectness score, the greater the likelihood that it contains an object and thus is a good RoI. Before we get there, we must do some basic processing steps:</p>
<p class="body-dialog">  1.  Convert the predicted offsets to bounding boxes. This is done by reversing the sequence of transformations</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">x</i><sup class="fm-superscript">∗</sup> = <i class="fm-italics">t<sub class="fm-subscript">x</sub></i><sup class="fm-superscript">∗</sup> ∗ <i class="fm-italics">w<sub class="fm-subscript">a</sub></i> + <i class="fm-italics">x<sub class="fm-subscript">a</sub></i></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">y</i><sup class="fm-superscript">∗</sup> = <i class="fm-italics">t<sub class="fm-subscript">y</sub></i><sup class="fm-superscript">∗</sup> ∗ <i class="fm-italics">h<sub class="fm-subscript">a</sub></i> + <i class="fm-italics">y<sub class="fm-subscript">a</sub></i></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">w</i><sup class="fm-superscript">∗</sup> = <i class="fm-italics">e</i><sup class="fm-superscript"><i class="fm-italics1">t<sub class="fm-subscript">w</sub></i><sup class="fm-superscript">∗</sup></sup> ∗ <i class="fm-italics">w<sub class="fm-subscript">a</sub></i></span><br class="calibre20"/>
<span class="math"><i class="fm-italics">h</i><sup class="fm-superscript">∗</sup> = <i class="fm-italics">e</i><sup class="fm-superscript"><i class="fm-italics1">t<sub class="fm-subscript">h</sub></i><sup class="fm-superscript">∗</sup></sup> ∗ <i class="fm-italics">h<sub class="fm-subscript">a</sub></i></span></p>
<p class="fm-equation-caption">Equation 11.5 <span class="calibre" id="eq-rpn-bbox-inv-transform"/></p>
<p class="body-ind">where <span class="math"><i class="fm-italics">x</i><sup class="fm-superscript">*</sup></span>, <span class="math"><i class="fm-italics">y</i><sup class="fm-superscript">*</sup></span>, <span class="math"><i class="fm-italics">w</i><sup class="fm-superscript">*</sup></span>, and <span class="math"><i class="fm-italics">h</i><sup class="fm-superscript">*</sup></span> denote the predicted bounding box’s center coordinates and its width and height, and <span class="math"><i class="fm-italics">t<sub class="fm-subscript">x</sub></i><sup class="fm-superscript">*</sup></span>, <span class="math"><i class="fm-italics">t<sub class="fm-subscript">y</sub></i><sup class="fm-superscript">*</sup></span>, <span class="math"><i class="fm-italics">t<sub class="fm-subscript">w</sub></i><sup class="fm-superscript">*</sup></span>, and <span class="math"><i class="fm-italics">t<sub class="fm-subscript">h</sub></i><sup class="fm-superscript">*</sup></span> are the RPN loc predictions. The bounding boxes are then converted back into <code class="fm-code-in-text">xtl</code>, <code class="fm-code-in-text">ytl</code>, <code class="fm-code-in-text">xbr</code>, <code class="fm-code-in-text">ybr</code> format.</p>
<p class="body-dialog">  2.  The predicted bounding boxes can lie partially outside the image. We clip all the predicted bounding boxes to within the image.</p>
<p class="body-dialog">  3.  Remove any predicted bounding boxes with height or width less than <code class="fm-code-in-text">min_roi_threshold</code>.</p>
<p class="body">Once these processing steps are done, we sort the predicted bounding boxes by objectness score and select <i class="timesitalic">N</i> candidates. <span class="math"><i class="fm-italics">N</i> = 12000</span> during training and <span class="math"><i class="fm-italics">N</i> = 6000</span> while testing.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional generating region proposals from the RPN output, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.<a id="marker-425"/></p>
<p class="fm-code-listing-caption" id="listing-11.21-pytorch-code-to-generate-region-proposals-from-rpn-output">Listing 11.21 PyTorch code to generate region proposals from RPN output</p>
<pre class="programlisting">rois = generate_bboxes_from_offset(rpn_loc, anchors)

rois = rois.clamp(min=0, max=width)       <span class="fm-combinumeral">①</span>

roi_heights = rois[:, 3] - rois[:, 1]     <span class="fm-combinumeral">②</span>
roi_widths = rois[:, 2] - rois[:, 0]
min_roi_threshold = 16

valid_idxes = torch.where((roi_heights &gt; min_roi_threshold) &amp;
    (roi_widths &gt; min_roi_threshold))[0]
rois = rois[valid_idxes]
valid_cls_scores = rpn_loc[valid_idxes]


objectness_scores = valid_cls_scores[:, 1]

sorted_idx = torch.argsort(               <span class="fm-combinumeral">③</span>
    objectness_scores, descending=True)
n_train_pre_nms = 12000
n_val_pre_nms = 300

rois = rois[sorted_idx][:n_train_pre_nms] <span class="fm-combinumeral">④</span>

objectness_scores = objectness_scores[ 
    sorted_idx][:n_train_pre_nms]         <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Clips the ROIs</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Threshold based on min_roi_threshold</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sorts based on objectness</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Selects the top regions of interest. Shape: (n_train_pre_nms, 4).</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Selects the top objectness scores. Shape: (n_train_pre_nms,).</p>
<p class="fm-head2" id="non-maximal-suppression-nms">Non-maximal suppression (NMS)</p>
<p class="body">Many of the proposals will overlap. We are effectively selecting anchors at a stride of 16 pixels. Therefore even a reasonably sized object is picked up by multiple anchors, each of which will try to predict the object independently. We can see this overlapping nature when we look at the positive anchors in figure <a class="url" href="#fig-pos-neg-anchors">11.16</a>. We want to choose the most effective set of RoIs. But it is evident that choosing all the similar proposals does not make a good set of RoIs because they carry redundant information.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="526" id="fig-pos-neg-anchors" src="../../OEBPS/Images/CH11_F16_Chaudhury.png" width="1013"/></p>
<p class="figurecaption">Figure 11.16 minimum where every anchor is classified as negative. To prevent this, the FRCNN under-samples the negative anchors before training.</p>
</div>
<p class="body">To address this problem, we use a technique called <i class="fm-italics">non-maximal suppression</i> (NMS). NMS is an algorithm that suppresses highly overlapping bounding boxes. The algorithm takes in bounding boxes and scores and works as follows.</p>
<div class="calibre3">
<p class="fm-algorithm-caption">Algorithm 11.6 Non-maximal suppression<a id="marker-426"/></p>
<p class="algorithm-body">Input: A list of bounding boxes B, corresponding scores S, and overlap threshold N</p>
<p class="algorithm-body">Output: A list of filtered bounding boxes D</p>
<p class="algorithm-body"><b class="fm-bold">while</b> likelihood is increasing <b class="fm-bold">do</b></p>
<p class="algorithm-body">    Select bounding box with highest confidence score</p>
<p class="algorithm-body">    Remove it from B and add it to the final list D</p>
<p class="algorithm-body">    Compare selected bounding box with remaining boxes in B using IoU</p>
<p class="algorithm-body">    Remove all bounding boxes from B with IoU &gt; threshold</p>
<p class="algorithm-body"><b class="fm-bold">end</b> <b class="fm-bold">while</b></p>
<p class="algorithm-body">return D</p>
</div>
<p class="body">We use NMS with a 0.7 threshold to suppress the highly overlapping RoIs and choose the top <i class="timesitalic">N</i> RoIs post-NMS to train the R-CNN. <span class="math"><i class="fm-italics">N</i> = 2000</span> during training, and <span class="math"><i class="fm-italics">N</i> = 300</span> while testing. Figure <a class="url" href="#fig-pre-post-nms">11.17</a> shows bounding boxes on a sample image before and after NMS.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="529" id="fig-pre-post-nms" src="../../OEBPS/Images/CH11_F17_Chaudhury.png" width="1013"/></p>
<p class="figurecaption">Figure 11.17 The left column shows the bounding boxes (24) before NMS. The right column shows the bounding boxes (4) that remain after NMS.</p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for NMS, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.</p>
<p class="fm-code-listing-caption" id="listing-11.22-pytorch-code-for-nms-of-rois">Listing 11.22 PyTorch code for NMS of RoIs</p>
<pre class="programlisting">n_train_post_nms = 2000
n_val_post_nms = 300
nms_threshold = 0.7

post_nms_indices = torchvision.ops.nms(     <span class="fm-combinumeral">①</span>
    rois, objectness_scores, nms_threshold)


post_nms_rois = rois[post_nms_indices[:n_train_post_nms]]</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Calls NMS implemented by torchvision</p>
<h3 class="fm-head1" id="fast-r-cnn">11.4.3 Fast R-CNN</h3>
<p class="body"><a id="marker-427"/>In the previous section, we saw how the RPN network takes an input image and emits a set of regions of interest that are likely to contain objects. Now let’s discuss the second leg of the FRCNN architecture, which takes in the RoIs and generates class probabilities and bounding-box coordinates for each object in the image. We briefly discussed this earlier. Here we revisit it in greater detail.</p>
<p class="body">We are given a set of RoIs (some of which contain the object). Our task is to train an object detector capable of localizing the objects. To do this, we need to extract the features corresponding to each RoI and pass them through a neural network (classifier and regressor) that learns to predict the class and the regression targets. The R-CNN solved this in a naive way: it extracted each RoI one at a time, warped it to make it a fixed size, and passed it through a deep CNN to extract the features corresponding to the RoI. Each RoI required a separate forward pass, making the approach very slow. The question, as always, is: can we do better?</p>
<p class="fm-head2" id="roi-pooling">RoI pooling</p>
<p class="body">Let’s consider the convolutional backbone. It processes the whole image with several conv and max pooling layers to produce a conv feature map. We have also seen a subsampling factor of 16: that is, <span class="math">16 × 16</span> pixels in the input image are reduced to a single point in the feature map. Also remember that the embedding at every grid point on the feature map is the representation/digest of a region in the input image.</p>
<p class="body"><i class="fm-italics">Key idea 1</i> is that the features corresponding to each RoI are already present in the conv feature map, and we can extract them via the feature map. For example, say our RoI is (0, 0, 256, 256). We know that the (0, 0, 256, 256) region in the input image is represented by 0, 0, 256/16, 256/16): that is, the (0, 0, 16, 16) region in the conv feature map. Since the embedding for a point in the conv feature map is a digest of the receptive field, we can use these features directly as the features of the RoI. So to obtain the features for an RoI of (0, 0, 256, 256), we take all the embeddings corresponding to the region (0, 0, 16, 16) in the conv feature map. Since we are performing this feature extraction directly on the convolutional feature map, which is obtained for the entire image, we can obtain the RoI features for all the RoIs in a single forward pass. This eliminates the need for multiple forward passes.</p>
<p class="body"><i class="fm-italics">Key idea 2</i> is as follows. We discussed a clever way of extracting the features corresponding to each RoI, and we want to use these features to train our classifier and regressor. However, there is a problem. As we know, RoIs are different sizes. And different-sized RoIs will lead to different feature embedding sizes. For example, if our RoI is (0, 0, 256, 256), our RoI feature embeddings are (16, 16, 512): that is, all the embeddings (of size 512) in the (0, 0, 16, 16) region of the conv feature map. If our RoI is (0, 0, 128, 128), then our RoI feature embeddings are (8, 8, 512): all the embeddings in the (0, 0, 8, 8) region of the conv feature map. And we know that neural networks typically need same-sized input. So how do we deal with input embeddings of different sizes? The answer is <i class="fm-italics">RoI pooling</i>.<a id="marker-428"/></p>
<p class="body">Let’s fix the size of the input ROI feature map that goes into the neural network. Our task is to reduce variable-sized RoI feature maps to a fixed size. If the fixed feature map size is set to be <span class="math"><i class="fm-italics">H</i>, <i class="fm-italics">W</i></span>, and our RoI corresponds to <span class="math">(<i class="fm-italics">r</i>, <i class="fm-italics">c</i>, <i class="fm-italics">h</i>, <i class="fm-italics">w</i>)</span> in the conv feature map, we divide <i class="timesitalic">h</i> and <i class="timesitalic">w</i> into equal-sized blocks of size <span class="math"><i class="fm-italics">h</i>/<i class="fm-italics">H</i></span> and <span class="math"><i class="fm-italics">w</i>/<i class="fm-italics">W</i></span>, respectively, and apply max pooling on these blocks to obtain a <span class="math"><i class="fm-italics">H</i>, <i class="fm-italics">W</i></span> feature map. Going back to our example, let’s fix <span class="math"><i class="fm-italics">H</i> = <i class="fm-italics">W</i> = 4</span>. Our expected fixed feature map size is <span class="math">(4,4,512)</span>. So when our RoI is (0, 0, 256, 256), our RoI feature embeddings are (16, 16, 512): <span class="math"><i class="fm-italics">h</i> = <i class="fm-italics">w</i> = 16</span>. We divide the <span class="math">16 × 16</span> region into four <span class="math">16/4</span>, <span class="math">16/4</span>) regions and perform max pooling on each region to obtain a fixed-size <span class="math">(4,4,512)</span> feature. Similarly, when our RoI is (0, 0, 128, 128), <span class="math"><i class="fm-italics">h</i> = <i class="fm-italics">w</i> = 8</span>. We divide the <span class="math">8 × 8</span> region into four (<span class="math">8/4</span>, <span class="math">8/4</span>) regions and perform max pooling to obtain the fixed-size <span class="math">(4,4,512)</span> feature.</p>
<p class="body">Astute readers will notice that we have carefully chosen our RoIs so that they are multiples of <i class="timesitalic">H</i> and <i class="timesitalic">W</i>, resulting in integer values for <span class="math"><i class="fm-italics">h</i>/<i class="fm-italics">H</i></span> and <span class="math"><i class="fm-italics">w</i>/<i class="fm-italics">W</i></span>, respectively. But in reality, this rarely happens. <span class="math"><i class="fm-italics">h</i>/<i class="fm-italics">H</i></span> and <span class="math"><i class="fm-italics">w</i>/<i class="fm-italics">W</i></span> are often floating-point numbers. What do we do in this case? The answer is <i class="fm-italics">quantization</i>: that is, we choose the integer closest to <span class="math"><i class="fm-italics">h</i>/<i class="fm-italics">H</i></span> and <span class="math"><i class="fm-italics">w</i>/<i class="fm-italics">W</i></span>, respectively (floor operation, in the original implementation). This has been improved on by RoIAlign, which uses bilinear interpolation instead of quantization. We do not get into the details of RoIAlign here.</p>
<p class="body">In effect, if we have a large RoI, we divide the feature map into a fixed number of large regions and perform max pooling. And when we have a small RoI, we divide the feature map into a fixed number of small regions and perform max pooling. The size of the region used for pooling can change, but the output size remains fixed. The dimension of the RoI pooling output doesn’t depend on the size of the input feature map or the size of the RoIs: it’s determined solely by the number of sections we divide the RoI into—<i class="timesitalic">H</i> and <i class="timesitalic">W</i> (see figure <a class="url" href="#fig-roi-pooling">11.18</a>).<a id="marker-429"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="714" id="fig-roi-pooling" src="../../OEBPS/Images/CH11_F18_Chaudhury.png" width="752"/></p>
<p class="figurecaption">Figure 11.18 A conv feature map with two regions of interest of different sizes. RoI pooling extracts a fixed-sized output feature map (<span class="math">2 × 2</span> in this image) from each of the RoIs in a single pass via max pooling. This enables us to extract fixed-sized representative feature vectors for each RoI, which are then fed into further classifier and regressor layers for classification and localization.</p>
</div>
<p class="body">Thus, the purpose of RoI pooling is to perform max pooling on inputs of non-uniform sizes to obtain fixed-size feature maps. The Fast R-CNN and Faster R-CNN use <span class="math">7 × 7</span> as the fixed feature map size.</p>
<p class="fm-head2" id="fast-r-cnn-architecture">Fast R-CNN architecture</p>
<p class="body">Given the conv feature map and a set of RoIs, we have seen how the RoI pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of FC layers that finally branch into two sibling output layers: a classifier that produces softmax probability estimates over <i class="timesitalic">K</i> object classes plus a catch-all “background” class, and a regressor that produces four real-valued numbers for each of the <i class="timesitalic">K</i> object classes.</p>
<p class="fm-head2" id="generating-gt-for-the-fast-r-cnn">Generating GT for the Fast R-CNN</p>
<p class="body">For every image, we have a list of RoIs generated by the RPN and a list of GT bounding boxes. How do we generate the GT and regression targets for each RoI? The idea remains the same as for our RPN: we use IoU scores. The algorithm is as follows:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">Compute the IoU between all RoIs and GT boxes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">For each RoI, determine the GT bounding box with the highest IoU.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If the highest IoU is greater than a threshold (0.5), assign the corresponding GT label as the label for the RoI.</p>
</li>
<li class="fm-list-bullet">
<p class="list">If the IoU is between [0.1, 0.5], assign the background label. Using a lower bound of 0.1 ensures that certain RoIs with small intersections with the GT are selected as background. This is helpful as it chooses hard examples for background; it is a form of hard negative mining.</p>
</li>
</ol>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the Fast R-CNN RoI head, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.<a id="marker-430"/></p>
<p class="fm-code-listing-caption" id="listing-11.23-pytorch-code-for-the-fast-r-cnn-roi-head">Listing 11.23 PyTorch code for the Fast R-CNN RoI head</p>
<pre class="programlisting">class Fast_RCNN_ROI_Head(nn.Module):
    def __init__(self, num_classes, H, W, subsample=16, embedding_size=512):

        super(Fast_RCNN_ROI_Head, self).__init__()

        self.num_classes = num_classes
        self.H = H
        self.W = W
        self.embedding_size = embedding_size
        self.subsample = 16

        self.roi_head_classifier = nn.Sequential(
            nn.Linear(H*W*embedding_size, 4096),
            nn.ReLU(True),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
        )
        self.cls = torch.nn.Linear(4096, num_classes+1) <span class="fm-combinumeral">①</span>
        self.reg = torch.nn.Linear(4096, (num_classes+1)*4)

    def forward(self, x, rois):                         <span class="fm-combinumeral">②</span>
        assert x.shape[0] == 1 # This code only supports batch size of 1
        roi_pooled_features = torchvision.ops.roi_pool(
            x, [rois], output_size=(self.H, self.W), spatial_scale=1/subsample)
        roi_pooled_features = roi_pooled_features.view(
            -1, self.H*self.W*self.embedding_size)
        fc_out = self.roi_head_classifier(roi_pooled_features)
        roi_cls_scores = self.cls(fc_out)
        roi_loc = self.reg(fc_out)

        return roi_cls_scores, roi_loc                  <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> num_classes + background</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> x : (1, c, h, w) tensor representing the conv feature map. rois: (n, 4) tensor representing bounding boxes of RoIs</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> roi_cls_scores: (n, num_classes+1) tensor representing classification scores for each RoI. roi_loc: (n, (num_classes + 1) * 4) tensor representing the regression scores for each RoI</p>
<p class="fm-head2" id="training-the-fast-r-cnn">Training the Fast R-CNN</p>
<p class="body"><a id="marker-431"/>The RPN generates about 2,000 RoIs per image. Due to computational constraints, we cannot use all <i class="timesitalic">N</i> RoIs. Instead, we sample a subset of them. The training minibatches are sampled hierarchically, first by sampling <i class="timesitalic">K</i> images and then by sampling <span class="math"><i class="fm-italics">R</i>/<i class="fm-italics">K</i></span> RoIs from each image. <i class="timesitalic">R</i> is set to 128 in the FRCNN. For this discussion, we assume that <span class="math"><i class="fm-italics">K</i> = 1</span>: that is, we have a single image per minibatch. So, given the RoIs for a single image, how do we sample 128 RoIs from it?</p>
<p class="body">A simple solution is to randomly sample 128 RoIs. However, this runs into the same data-imbalance issue that we discussed earlier: we end up sampling backgrounds a lot more frequently than the classes. To solve this problem, we adopt a sampling strategy similar to before. In particular, for a single image, we sample 128 RoIs such that the ratio of background to object is 0.75:0.25. If fewer than 32 RoIs contain the objects, we pad the minibatch with more background RoIs.</p>
<p class="fm-head2" id="assigning-targets-to-roi-boxes">Assigning targets to RoI boxes</p>
<p class="body">Just as in the case of the RPN, we generate regression targets as offsets of the GT box from the region of interest for all RoIs that contain objects. For all background RoIs, the regression targets are not applicable.</p>
<p class="fm-head2" id="fast-r-cnn-loss-function">Fast R-CNN loss function</p>
<p class="body">We have defined the Fast R-CNN network and how we can generate labels and regression targets for its outputs. We need to discuss the loss function that enables us to train the Fast R-CNN. As you would expect, there are two loss terms:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Classification loss</i>— We use the standard cross-entropy loss used in any standard classifier.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Regression loss</i>—The regression loss applies <i class="fm-italics">only</i> to the object RoIs: background RoIs do not contribute to regression. Here we use the smooth L1 loss as we did in the RPN.</p>
</li>
</ul>
<p class="body">Thus the overall loss for a single RoI can be defined as follows:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;L_{cls} = CrossEntropy(p, u) \nonumber \\
&amp;L_{reg} = L_{1;smooth}(t^{u} , v) \nonumber \\
&amp;L_{RCNN} =  L_{cls} + \lambda[u&gt;0]L_{reg}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="87" src="../../OEBPS/Images/eq_11-06.png" width="220"/></p>
</div>
<p class="fm-equation-caption">Equation 11.6 <span class="calibre" id="eq-fast-rcnn-loss-per-roi"/></p>
<p class="body">where <i class="timesitalic">p</i> is the predicted label for the RoI, <i class="timesitalic">u</i> is the true label for the RoI, <i class="timesitalic">t<sub class="fm-subscript">u</sub></i> = <span class="math">(<i class="fm-italics">t<sub class="fm-subscript">x</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">y</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">w</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">h</sub></i>)</span> are the regression predictions for class <i class="timesitalic">u</i>, and <i class="timesitalic">v</i> = <span class="math">(<i class="fm-italics">v<sub class="fm-subscript">x</sub></i>, <i class="fm-italics">v<sub class="fm-subscript">y</sub></i>, <i class="fm-italics">v<sub class="fm-subscript">w</sub></i>, <i class="fm-italics">v<sub class="fm-subscript">h</sub></i>)</span> are the regression targets. The overall loss can therefore be defined as</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;L_{cls} = \frac{\sum_{i}CrossEntropy(p_i, p_i^{*})} { N_{roi}}  \nonumber \\
&amp;L_{reg} = \frac{\sum_{\{\forall i | p_i^{*}!=0\}} L_{1;smooth}(t_i
, t_i^{*})}{N_{pos}} \nonumber \\
&amp;L_{RCNN} =  L_{cls} + \lambda L_{reg}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="134" src="../../OEBPS/Images/eq_11-07.png" width="263"/></p>
</div>
<p class="fm-equation-caption">Equation 11.7 <span class="calibre" id="eq-fast-rcnn-overall-loss"/></p>
<p class="body">where <i class="timesitalic">p<sub class="fm-subscript">i</sub></i> are the prediction probabilities for the RoI <i class="timesitalic">i</i>, <span class="math"><i class="fm-italics">p<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">*</sup></span> is the true label for RoI <i class="timesitalic">i</i>; <i class="timesitalic">t<sub class="fm-subscript">i</sub></i> = <span class="math">(<i class="fm-italics">t<sub class="fm-subscript">x</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">y</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">w</sub></i>, <i class="fm-italics">t<sub class="fm-subscript">h</sub></i>)</span> are the regression predictions for RoI <i class="timesitalic">i</i> corresponding to class <span class="math"><i class="fm-italics">p<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">*</sup></span>, <span class="math"><i class="fm-italics">t<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">*</sup></span> = <span class="math">(<i class="fm-italics">t<sub class="fm-subscript">x</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">y</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">w</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">h</sub></i><sup class="fm-superscript">*</sup>)</span> are the regression targets for RoI <i class="timesitalic">i</i>, <span class="math"><i class="fm-italics">t<sub class="fm-subscript">i</sub></i><sup class="fm-superscript">*</sup></span> = <span class="math">(<i class="fm-italics">t<sub class="fm-subscript">x</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">y</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">w</sub></i><sup class="fm-superscript">*</sup>, <i class="fm-italics">t<sub class="fm-subscript">h</sub></i><sup class="fm-superscript">*</sup>)</span> are the regression targets for RoI <i class="timesitalic">i</i>, and <i class="timesitalic">N<sub class="fm-subscript">pos</sub></i> is the number of object RoIs (non-background RoIs).</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the Fast R-CNN loss function, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.<a id="marker-432"/></p>
<p class="fm-code-listing-caption" id="listing-11.24-pytorch-code-for-the-fast-r-cnn-loss-function">Listing 11.24 PyTorch code for the Fast R-CNN loss function</p>
<pre class="programlisting">def rcnn_loss(
    roi_cls_scores,                              <span class="fm-combinumeral">①</span>
    roi_loc,                                     <span class="fm-combinumeral">②</span>
    roi_labels,                                  <span class="fm-combinumeral">③</span>
    rcnn_loc_targets,                            <span class="fm-combinumeral">④</span>
    lambda_ = 1):

    classification_criterion = nn.CrossEntropyLoss()
    reg_criterion = nn.SmoothL1Loss(reduction="sum")

    cls_loss = classification_criterion(roi_cls_scores, roi_labels)

    pos_roi_idxes = torch.where(roi_labels&gt;0)[0] <span class="fm-combinumeral">⑤</span>
    pred_all_offsets = roi_loc[pos_roi_idxes]

    num_pos_rois = len(pos_roi_idxes)
    pred_all_offsets = pred_all_offsets.view(    <span class="fm-combinumeral">⑥</span>
        num_pos_rois, -1, 4)

    pred_cls_offsets = pred_all_offsets[
        torch.arange(num_pos_rois) , roi_labels[pos_roi_idxes]]

    gt_offsets = rcnn_loc_targets[pos_roi_idxes]

    reg_loss = reg_criterion(pred_cls_offsets, gt_offsets) / num_pos_rois
    return {
        "rcnn_cls_loss": cls_loss,
        "rcnn_reg_loss": reg_loss,
        "rcnn_total_loss": cls_loss + lambda_* reg_loss

    }</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> (128, num_classes) tensor: RCNN classifier scores for each RoI</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> (128, num_classes*4) tensor: RCNN regressor predictions for each class, RoI</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> (128,) tensor: true class for each RoI</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> (128, 4) tensor: RoI regressor targets for each RoI</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Finds the positive RoIs</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> (n, num_classes*4) to n, num_classes, 4)</p>
<p class="fm-head2" id="fast-r-cnn-inference">Fast R-CNN inference</p>
<p class="body">We have looked at how to train the Fast R-CNN module. Once the model is trained, the next question is how to use the model to generate output classes and bounding boxes.</p>
<p class="body">The Fast R-CNN model outputs a classification score and regression offsets for every RoI. We can safely ignore the background RoIs. For the rest of the RoIs, the class with the highest probability is chosen as the output label, and the offsets corresponding to that class are chosen. We apply post-processing steps similar to that of the RPN:</p>
<ol class="calibre10">
<li class="fm-list-bullet">
<p class="list">We translate the offsets back to (<code class="fm-code-in-text">xtl</code>, <code class="fm-code-in-text">ytl</code>, <code class="fm-code-in-text">xbr</code>, <code class="fm-code-in-text">ybr</code>) format using the RoI.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We clip the output bounding box to within the image boundaries</p>
</li>
</ol>
<p class="body">We face a problem similar to before: the output probably has multiple bounding boxes corresponding to the same object. We deal with it in the same way as earlier: using NMS. There is one difference, however. In the case of the RPN, we applied a global NMS across all bounding boxes predicted by the RPN. Here, NMS is applied only across the bounding boxes belonging to the same class. This is done for all classes, which should intuitively make sense: there is no point in suppressing highly overlapping bounding boxes if the bounding boxes represent different classes.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for the Fast R-CNN inference, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/nY48">http://mng.bz/nY48</a>.<a id="marker-433"/></p>
<p class="fm-code-listing-caption" id="listing-11.25-pytorch-code-for-the-fast-r-cnn-inference">Listing 11.25 PyTorch code for the Fast R-CNN inference</p>
<pre class="programlisting">def fast_rcnn_inference(
    frcnn_roi_head,                                      <span class="fm-combinumeral">①</span>
    rois,                                                <span class="fm-combinumeral">②</span>
    conv_feature_map,                                    <span class="fm-combinumeral">③</span>
    nms_threshold=0.7):

    frcnn_roi_head.eval()                                <span class="fm-combinumeral">④</span>
    roi_cls_scores, roi_loc = frcnn_roi_head(conv_feature_map, rois)

    output_labels = torch.argmax(                        <span class="fm-combinumeral">⑤</span>
        roi_cls_scores, dim=1)

    output_probs = nn.functional.softmax(
        roi_cls_scores, dim=1)[torch.arange(             <span class="fm-combinumeral">⑥</span>
            rois.shape[0]), output_labels]

    output_offsets = roi_loc.view(                       <span class="fm-combinumeral">⑦</span>
        rois.shape[0], -1, 4)


    output_offsets = output_offsets[                     <span class="fm-combinumeral">⑧</span>
        torch.arange(rois.shape[0]), output_labels]


    assert output_offsets.shape == torch.Size(           <span class="fm-combinumeral">⑨</span>
        [rois.shape[0], 4])

    output_bboxes = generate_bboxes_from_offset(         <span class="fm-combinumeral">⑩</span>
        output_offsets, rois)

    rois = output_bboxes.clamp(min=0, max=width)         <span class="fm-combinumeral">⑪</span>

    post_nms_labels, post_nms_probs, post_nms_boxes = [], [], []

    for cls in range(1, frcnn_roi_head.num_classes+1):   <span class="fm-combinumeral">⑫</span>

        cls_idxes = torch.where(output_labels == cls)[0] <span class="fm-combinumeral">⑬</span>
        cls_labels = output_labels[cls_idxes]
        cls_bboxes = output_bboxes[cls_idxes]
        cls_probs = output_probs[cls_idxes]
        keep_indices = torchvision.ops.nms(
            cls_bboxes, cls_probs, nms_threshold)

        post_nms_labels.append(cls_labels[keep_indices])
        post_nms_probs.append(cls_probs[keep_indices])
        post_nms_boxes.append(cls_bboxes[keep_indices])

    return {
        "labels": torch.cat(post_nms_labels),
        "probs": torch.cat(post_nms_probs),
        "bboxes": torch.cat(post_nms_boxes)
    }</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Trained instance of Fast_RCNN_ROI_Head</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> RoIs to inference</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> (n, c, h, w) convolutional feature map</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Sets eval mode</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The predicted class is the class with the highest score.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> The predicted probabilities are obtained via softmax. The highest probability is chosen as the probability score for this prediction.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Converts locs from (n, num_classes*4) to (n, num_classes, 4)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Selects offsets corresponding to the predicted label</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Asserts that we have outputs for each RoI</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Converts offsets to xtl, ytl, xbr, ybr)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑪</span> Clips bounding boxes to within images</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑫</span> 0 is background, thus ignored</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑬</span> Performs NMS for each class</p>
<h3 class="fm-head1" id="training-the-faster-r-cnn">11.4.4 Training the Faster R-CNN</h3>
<p class="body"><a id="marker-434"/>As we have seen, the FRCNN consists of two subnetworks:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">An RPN responsible for generating good region proposals that contain objects</p>
</li>
<li class="fm-list-bullet">
<p class="list">A Fast R-CNN responsible for object classification and detection from a list of RoIs</p>
</li>
</ul>
<p class="body">Thus the FRCNN is a two-stage object detector. We have one stage that generates good region proposals and another that takes the region proposals and detects objects in the image. So how do we train the FRCNN?</p>
<p class="body">A simple idea would be to train two independent networks (RPN and Fast R-CNN). However, we do not want to do this because it is expensive. Additionally, if we do so, each network will modify the convolutional layers in its own way. As discussed earlier, we want to share the convolutional layers across the RPN and the Fast R-CNN modules. This ensures efficiency (only one conv backbone as opposed to two independent backbones). Additionally, both the RPN and FRCNN are performing similar tasks, so it intuitively makes sense to share the same set of convolutional features. Therefore, we need to develop a technique that allows for sharing convolutional layers between the two networks rather than learning two separate networks. The original FRCNN paper proposed two techniques to train the model:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Alternate optimization (AltOpt)</i>—We first train RPN and use the proposals to train the Fast R-CNN. The network tuned by the Fast R-CNN is then used to initialize the RPN, and this process is iterated. This involves multiple rounds of training alternating between training the RPN and Fast R-CNN.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Approximate joint training</i>—The RPN and Fast R-CNN networks are merged into one network during training. In each SGD iteration, the forward pass generates region proposals that are treated just like fixed, precomputed proposals when training a Fast R-CNN detector. We combine the RPN and Fast R-CNN losses and perform backpropagation as usual. This training is significantly faster as we are training both networks together end to end. However, the optimization is approximate because we treat the RPN-generated proposals as fixed, whereas in reality, they are a function of the RPN. So, we’re ignoring one derivative.</p>
</li>
</ul>
<p class="body">Both techniques give similar accuracy. So joint training, which is significantly faster, is preferred.</p>
<h3 class="fm-head1" id="other-object-detection-paradigms">11.4.5 Other object-detection paradigms</h3>
<p class="body"><a id="marker-435"/>So far, we have looked at the FRCNN in detail and discussed several key ideas that contribute to its success. Several other object-detection paradigms have also been developed. Some are inspired by the FRCNN, borrowing and/or improving on the ideas established by the FRCNN. In this section, we briefly look at a few of them.</p>
<p class="fm-head2" id="you-only-look-once-yolo">You Only Look Once (YOLO)</p>
<p class="body">The FRCNN is a two-stage detector: an RPN followed by the Fast R-CNN, which runs on the region proposals generated by the RPN. YOLO (<a class="url" href="https://arxiv.org/pdf/1506.02640.pdf">https://arxiv.org/pdf/1506.02640</a><a class="url" href="https://arxiv.org/pdf/1506.02640.pdf">.pdf</a>), as the name implies, is a single-stage object detector. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes directly from full images in one go. Some of the salient features of YOLO are as follows:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">YOLO is significantly faster ( 10<span class="math">×</span> faster than the FRCNN) due to its much simpler architecture. YOLO can even be used for real-time object detection.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Unlike the FRCNN, where the R-CNN module looks only at the region proposals, YOLO looks directly at the full image during training and testing.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The speed of YOLO comes at the cost of accuracy. While YOLO is significantly faster, it is less accurate than the FRCNN.</p>
</li>
</ul>
<p class="body">Several other improvements have been made on top of YOLO to improve accuracy while trying to maintain the simple, fast architecture. These include YOLO v2, YOLO v3, and so on.</p>
<p class="fm-head2" id="multibox-single-shot-detector-ssd">MultiBox Single-Shot Detector SSD)</p>
<p class="body">SSD (<a class="url" href="https://arxiv.org/pdf/1512.02325.pdf">https://arxiv.org/pdf/1512.02325.pdf</a>) tries to achieve a good balance between speed and accuracy. It is a single-stage network like YOLO: that is, it eliminates the proposal-generation (RPN) and subsequent feature-resampling stages. It also borrows the ideas of anchors from the FRCNN: applying a conv net on top of feature maps to make predictions relative to a fixed set of bounding boxes.</p>
<p class="body">Thus, a single deep network predicts class scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. To achieve high detection accuracy, feature maps at different scales are used to make predictions at different scales.</p>
<p class="body">SSD is much more accurate than YOLO; however, it is still not as accurate as the FRCNN (especially for small objects).</p>
<p class="fm-head2" id="feature-pyramid-network-fpn">Feature pyramid network (FPN)</p>
<p class="body"><a id="marker-436"/>The feature maps generated by conv nets are pyramidal: as we go deeper, the spatial resolution of the feature map keeps decreasing, and we expect the semantic information represented by the feature map to be more meaningful. High-resolution maps have low-level features, whereas low-resolution maps have more semantic features.</p>
<p class="body">In the case of the FRCNN, we applied object detection on only the last convolution map. SSD shows that there is useful information by using other feature maps for prediction. But SSD builds this pyramid high up the network (past the fourth convolution layer [conv4] of VGG). It specifically avoids the use of lower-layer features. Thus it misses the opportunity to reuse the higher-resolution maps of the feature hierarchy. FPN shows that these features are important, especially for detecting small objects.</p>
<p class="body">The FPN (<a class="url" href="https://arxiv.org/pdf/1612.03144.pdf">https://arxiv.org/pdf/1612.03144.pdf</a>) relies on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections. The bottom-up pathway is the forward pass of the convolutional layers. In the top-down path, there is a back connection from lower resolution to higher resolution via simple upsampling (it is merged with the bottom-up feature map using <span class="math">1 × 1</span> convolutions). This merged feature map is used at every level to learn and make predictions.</p>
<p class="body">The FPN was originally implemented on top of the FRCNN. It is much more accurate, but it is much slower than YOLO/SSD-style approaches.</p>
<p class="body">We have only briefly mentioned the other prominent detection paradigms. The fundamental principles behind them remain the same. You are encouraged to read the papers to get a deeper and better understanding.</p>
<h2 class="fm-head" id="summary-10">Summary</h2>
<p class="body">In this chapter, we took an in-depth look at various deep-learning techniques for object classification and localization:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">LeNet is a simple neural network that can classify handwritten digits from the MNIST data set.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Simple networks like LeNet don’t extend well to more real-world image classification problems. Hence deeper neural networks that have more expressive power are needed.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The VGG network is one of the most popular deep convolutional neural networks. It improves on prior state-of-the-art deep neural networks by using more convolution layers with smaller <span class="math">3 × 3</span>) filters. Such an architecture has two advantages: (1) more expressive power because of the added nonlinearity that comes from stacking more layers, and (2) a reduced number of parameters. Three <span class="math">3 × 3</span> filters have <span class="math">27<i class="fm-italics">C</i><sup class="fm-superscript">2</sup></span> parameters, whereas a single <span class="math">7 × 7</span> filter (which cover the same receptive field) has <span class="math">49<i class="fm-italics">C</i><sup class="fm-superscript">2</sup></span> parameters (81% more).</p>
</li>
<li class="fm-list-bullet">
<p class="list">VGG (and AlexNet) use ReLU nonlinear layers instead of sigmoid layers because they do not suffer from the vanishing gradient problem. Using ReLUs speeds up training, resulting in faster convergence.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Inception blocks provide an efficient way to increase the depth and width of a neural network while keeping the computational budget constant. Multiscale filters are used at each convolution layer to learn patterns of different sizes, and <span class="math">1 × 1</span> convolutions are used for dimensionality reduction (which reduces the number of parameters needed, thereby improving computational efficiency).</p>
</li>
<li class="fm-list-bullet">
<p class="list">ResNet is another popular convolutional neural network. The ResNet architecture was motivated by the fact that simply stacking layers beyond a certain point does not help and causes degradation even in training accuracies. This is a counterintuitive result because we expect that deeper networks can, at the very least, learn as much as their shallower counterparts. The authors of the ResNet papers showed that this may happen because the identity function is hard for neural networks to learn. To tackle this, they proposed shortcut/skip connections to simplify the neural network’s learning objective. This is the key idea behind ResNet and enables training of much deeper neural networks.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Faster R-CNN is one of the most popular object detectors. It is a two-stage network consisting of (1) a region proposal network RPN), which is responsible for predicting regions of interest that could potentially contain objects; and (2) an R-CNN module, which takes the region proposals as input and emits class scores and bounding boxes efficiently.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The RPN module uses multiple anchor boxes (at each point on the conv feature map) to handle objects of different sizes and aspect ratios. It convolves a small network over the conv feature map to make predictions about objectness and bounding boxes at each sliding window location. Remember that the small network is a fully convolutional network (FCN) comprising <span class="math">3 × 3</span> convs followed by <span class="math">1 × 1</span> convs, enabling this approach to work with arbitrary-sized inputs and making it translation invariant.</p>
</li>
<li class="fm-list-bullet">
<p class="list">RoI pooling provides an efficient way to extract a fixed-sized feature vector from region proposals of varying sizes, all in one pass. These feature vectors are fed to a classifier and regressor for classification and localization, respectively.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Non-maxima suppression (NMS) is a technique to de-duplicate overlapping bounding boxes.</p>
</li>
<li class="fm-list-bullet">
<p class="list">FRCNN can be trained using two methods: alternative optimization (AltOpt) and approximate joint training. Both approaches lead to similar accuracy numbers, but approximate joint training is significantly faster.</p>
</li>
<li class="fm-list-bullet">
<p class="list">You Only Look Once (YOLO), MultiBox Single-Shot Detector SSD), and feature pyramid networks (FPN) are some other popular object detectors.<a id="marker-437"/></p>
</li>
</ul>
</div></body></html>