- en: Chapter 11\. Methods in Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of interpretability is broad and can be uniquely applied to a variety
    of tasks. Simply put, interpretability defines a model’s ability to “explain”
    its decision making to a third party. There are many modern architectures that
    do not have this capability just by construction. A neural network, for example,
    is a prime example of one of these modern architectures. The term “opaque” is
    often used to describe neural networks, both in media and in literature. This
    is because, without post hoc techniques to explain the final classification or
    regression result of a neural network, the data transformations occurring within
    the trained model are unclear and difficult for the end user to interpret. All
    we know is that we fed in an example and out popped a result. Although we can
    examine the learned weights of a neural network, the composition of all of these
    weights is an extremely complex function. This makes it difficult to tell what
    part of the input ended up contributing the most to the final result.
  prefs: []
  type: TYPE_NORMAL
- en: A variety of post hoc methodologies have been designed to explain the output
    of a neural network—*saliency mapping* is a prime example. Saliency mapping measures
    the gradient of the output of a trained model with respect to the input. By the
    definition of the gradient, the input positions with the highest magnitude gradients
    would affect the output value, or class in the case of classification, the most
    when their values are changed slightly. Saliency mapping thus interprets the set
    of positions (and their respective values) with the highest magnitude gradients
    as the part of the input that contributes the most to the final result.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not the be-all and end-all of interpretability. One issue with
    saliency mapping is that it can be a bit noisy, especially when we consider the
    gradient at the individual pixel level for tasks like image classification. Additionally,
    if the input is categorical in nature rather than continuous, e.g., one-hot encodings
    for sentences, the gradient with respect to the input isn’t interpretable in itself
    since the input space is discontinuous.
  prefs: []
  type: TYPE_NORMAL
- en: Further, as mentioned earlier, the task at hand is often key for determining
    what makes sense as a valid method of interpretability. We will expound on this
    more in the sections to come.
  prefs: []
  type: TYPE_NORMAL
- en: Oftentimes, interpretability comes at the expense of performance. Building interpretability
    into a model often adds some bias (the bias in bias-variance trade-off) by making
    simplifying model assumptions, e.g., in vanilla linear regression, where we assume
    a linear relationship between the features and the target variable. These simplifying
    assumptions, however, are what make the relationship between the input features
    and the target variable much clearer in a vanilla linear regression as opposed
    to a complex, neural architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'This all begs the question: why do we care about interpretability in the first
    place? In a world that is becoming increasingly dominated by technology, complex
    algorithms, and machine learning, the ability to explain decision making is imperative.
    Especially in fields such as medicine, where patient’s lives are on the line,
    or in finance, where peoples’ financial livelihoods are at stake, the ability
    to explain a model’s decision making is a key step toward widespread adoption.
    In the next section, we will cover some classical models that have strong notions
    of interpretability built into their design.'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees and Tree-Based Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most classical data science and machine learning methodologies have some built-in
    form of interpretability. Tree-based algorithms are a clear example of this. Decision
    trees are designed to classify an input based on a series of conditional statements,
    where each node in the tree is associated with a conditional statement. To understand
    how a trained tree-based model is making a decision, all we must do for any given
    input is follow the correct branch at each node in the tree ([Figure 11-1](#fig1101)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. A decision tree trained to classify bird species. Given a set
    of bird features, follow the right “Yes” or “No” branch at each node to reach
    a final classification.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: More complex tree-based algorithms, such as the random forest algorithm, which
    is composed of an ensemble of large decision trees, are also interpretable. For
    example, in the case of classification, random forest algorithms function by running
    a given input through each decision tree and then taking the majority output class
    amongst the decision trees as the final output (or an average in the case of regression).
    By the algorithm’s construction, we know exactly how random forest came to a final
    conclusion regarding the input.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to interpretability at the individual example level, decision trees
    and their more complex ensembles have built-in metrics for feature importance
    at the global level. For example, when a decision tree is being trained, it must
    determine which feature to split on and the threshold(s) of that feature at which
    to split. In the classification regime, one methodology to do this is to calculate
    the information gain by splitting on a proposed feature at a proposed threshold.
    To frame our thinking, let’s think of the possible training labels as the domain
    of a discrete probability distribution, where the probability of each label is
    the frequency with which that label appears in the training dataset ([Figure 11-2](#fig1102)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Label probabilities.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thinking back to [Chapter 2](ch02.xhtml#fundamentals-of-proba), a metric that
    summarizes the uncertainty within a probability distribution is the entropy of
    the distribution. When given a proposed feature and associated threshold(s) to
    split on, we can split the training data population into at least two separate
    groups based on which branch we would follow for each input example. Each subgroup
    now has its own distribution over the possible labels, and we take the difference
    between the training dataset’s entropy and the weighted sum of each subgroup’s
    entropy to calculate the information gain, where the weight is proportional to
    the number of elements in each subgroup. The feature and associated threshold(s)
    with the highest information gain at each branching point are the optimal split.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this work? Although we won’t do a rigorous proof here, consider the
    problem where we have a molecular dataset with a binary label, for example, indicating
    whether each compound is toxic or not, and we’d like to build a classifier to
    predict compound toxicity. Also assume that one of the features associated with
    each compound is a binary feature of whether the molecule contains a phenol functional
    group or not. The phenol functional group is both quite toxic and is a common
    cause of toxicity in compounds, so splitting on this feature would lead to two
    well-separated subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: The positive subgroup, which contains compounds with phenol functional groups,
    is likely to have few false positives due to the phenol’s level of toxicity. The
    negative subgroup, which contains compounds without phenol functional groups,
    is likely to have few false negatives due to phenol being a common cause of toxicity.
    Thus, each subgroup’s associated entropy is quite low since the true label distribution
    over compounds in each subgroup is quite concentrated over a single label. The
    sum of their weighted entropies removed from the entire dataset’s associated entropy
    demonstrates a significant information gain ([Figure 11-3](#fig1103)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. The original dataset can be broken down into 30% toxic and 70%
    nontoxic, where a true label of 1 indicates toxicity and 0 otherwise. Breaking
    up the n examples into two subgroups based on containing phenol greatly concentrates
    the true probability over a single label in each subgroup.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This checks out well with our a priori knowledge of the phenol group—due to
    both its widespread nature in toxic compounds and its level of toxicity, we would
    have expected it to be a great feature for toxicity classification. The way we
    select features and their splits in decision trees is actually the same way we
    approach *greedy algorithms* in the more general algorithmic framework. Greedy
    algorithms select the most optimal local action at each decision point, and depending
    on the properties of the problem, the composition of these locally optimal actions
    leads to the global optimum. Decision trees similarly select the feature and split
    that locally lead to the largest gain in some metric at each decision point. For
    example, we just used information gain for toxicity classification, and although
    we showed the result of just one split, assuming splitting on the phenol trait
    leads to the highest information gain, we perform this same greedy procedure at
    each junction of every level in the tree. However, it turns out that the problem
    of finding the globally optimal decision tree for a dataset is NP-complete, which
    for the purposes of this text means that it is computationally very difficult.
    The best we can do to approach the problem in a tractable manner is the greedy
    approach, although it does not provably lead to the global optimum.
  prefs: []
  type: TYPE_NORMAL
- en: For each feature we split on in a tree, there exists an associated information
    gain with that feature. The order of importance of each feature is simply a list
    of the features sorted by their information gain. If we have a random forest rather
    than a single decision tree, we average the information gain for each feature
    across all of the trees in the forest and sort using the mean. Note that there
    is no extra work required in calculating the information gain, since we use information
    gain to train the individual decision trees in the first place. Thus, we have
    both example-level interpretability and a global understanding of feature importance
    for free in tree-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A quick background on linear regression: given a set of features and a target
    variable, our goal is to find the “best” linear combination of features that approximates
    the target variable. Implicit in this model is the assumption that the input features
    are linearly related to the target variable. We define “best” as the set of coefficients
    that results in the linear combination with the lowest root mean squared error
    when compared against the ground truth:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals beta dot x plus epsilon comma epsilon tilde upper N
    left-parenthesis 0 comma sigma squared right-parenthesis"><mrow><mi>y</mi> <mo>=</mo>
    <mi>β</mi> <mo>·</mo> <mi>x</mi> <mo>+</mo> <mi>ϵ</mi> <mo>,</mo> <mi>ϵ</mi> <mo>∼</mo>
    <mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Where  <math alttext="beta"><mi>β</mi></math> represents the vector of coefficients.
    Our built-in, global notion of feature importance follows directly from this.
    The features that correspond with the coefficients with the highest magnitude
    are, globally, the most important features in the regression.
  prefs: []
  type: TYPE_NORMAL
- en: How about an example-level notion of feature importance? Recall that to get
    a prediction for a given example, we take the dot product between the example
    and the learned coefficients. Logically, the feature associated with the feature-coefficient
    product that contributes the most, in magnitude, to the final result is the feature
    that is most important for prediction. Without much effort, we have both an example-level
    and global-level notion of interpretability more or less built into linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: However, linear regression has some unaddressed issues when considering feature
    importance. For example, when there exist significant correlations between the
    features in a multivariate regression, it is often difficult for the model to
    disentangle the effects of these correlated features on the output. In [“SHAP”](#shap-sect),
    we will describe Shapley values, which were designed to measure the marginal,
    unbiased impact of a given feature on the output in such cases.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for Evaluating Feature Importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For models where feature importance isn’t built in, researchers have developed
    a variety of methods over the years to evaluate it. In this section, we will discuss
    a few that are used in the industry, in addition to their benefits and shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: Permutation Feature Importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea behind permutation feature importance is quite simple: assume we have
    a trained neural model *f* and a set of features *U* that *f* has been trained
    on. We’d like to understand the impact that an individual feature *s* has on the
    predictions of *f*. One way to do this is to randomly rearrange the values that
    *s* takes on in the dataset amongst all of the examples and measure the resulting
    decrease in predictive accuracy. If the feature *s* did not add much predictive
    accuracy in the first place, we should see that the predictive accuracy of *f*
    decreases minimally when using the permuted samples. Inversely, if feature *s*
    was predictive of the output in the first place, we should see a large drop in
    predictive accuracy upon permuting the values of *s* in the dataset. In essence,
    if the feature *s* was originally strongly correlated with the true labels, randomizing
    the values of *s* would break this strong correlation and nullify its effectiveness
    at predicting the true label.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, as with all interpretability methods, this one is not perfect.
    Imagine the scenario in which our target is ice cream sales in a given region
    and two features in *U* are the readings of two temperature sensors within a one-mile
    radius of each other. We’d expect that each of these features is independently
    quite predictive of ice cream sales due to the seasonality of our target. However,
    if we were to perform the permutation methodology presented previously on this
    dataset, we’d counterintuitively get a low feature importance for both of these
    features. Why is this the case? Although each of these features is strongly predictive
    of the target, they are also strongly correlated due to the close proximity of
    the two temperature sensors. Additionally, permuting only one of these features
    at a time to compute its importance means that the other is kept intact, preserving
    most of the predictive information contained within the two features. Thus, we’d
    see little change in the predictive performance of *f* for both features, leading
    us to believe that the weather is not predictive of ice cream sales.
  prefs: []
  type: TYPE_NORMAL
- en: The moral of the story here is that we must always keep in mind correlations
    between features in our dataset. It is good data science and machine learning
    practice to understand the relationships between the features themselves before
    running these features through any sort of predictive modeling algorithm, simple
    or complex. One way to do this is to plot each of the z-scored features against
    each other to get a visual idea of feature correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Partial Dependence Plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Partial dependence plots, or PDPs for short, measure the marginal impact that
    a subset of features included in the model has on the output. As previously discussed,
    measuring this marginal impact in an unbiased manner is difficult for complex
    neural models. In the case of regression, we can represent the trained neural
    network (or any other complex, uninterpretable model) as a function *f* that takes
    as input a set of features *U* and outputs a value in the reals. Imagine that,
    as a user of this model, you are looking for an interpretability method that can
    measure the marginal impact of any subset of features *S* on the output of *f*.
    That is, if we are given an arbitrary setting of feature set *S*, we would like
    to calculate the expected output of the function *f* conditioned on this setting.
    The expectation of *f* is taken over *U \ S*, the rest of the features in *U*
    (conditioned on the known setting of *S*). Intuitively, we have marginalized out
    the feature subset *U \ S* and have the output of a new function *f’* that takes
    as input only the feature set *S*. If we carry out this process for enough settings
    of *S*, we can learn the patterns of how *f’* changes as the feature set *S* changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, assume the output is the number of vehicles on the road in a given
    region and our feature set *S* consists of a single feature: precipitation levels
    in that region. The features that make up *U \ S* could be variables like time
    of day, geographical location, population density, etc. By running the above process
    for a range of precipitation levels, we can estimate the number of vehicles we’d
    expect to see on the road at each level and observe the trend as precipitation
    levels get higher or lower. Plotting this trend is what gives us a PDP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of important notes: the first is that we do not plan on actually learning
    *f’*, but rather estimating it using our trained model *f*. Learning *f’* itself
    would require retraining for every potential subset *S* to be explained, which
    is exponential in the number of features and thus intractable. The second is that
    it is currently unclear how we would go about computing the expectation of *f*
    taken over *U \ S*. As we will soon see, the PDP methodology addresses this second
    point. Before diving into the weeds, here is a simple yet concrete mathematical
    formulation of the process we have just described:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"f prime left-parenthesis upper S right-parenthesis equals double-struck\
    \ upper E Subscript upper U minus upper S vertical-bar upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\"\
    ><mrow><mi>f</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mrow><mo>(</mo> <mi>S</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C</mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi><mo>|</mo><mi>S</mi></mrow></msub>\
    \ <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi>\
    \ <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated, the conditional expectation is a bit tricky to estimate. So far
    in the text, we have approximated expectations in an unbiased manner via empirical
    averages. To estimate a conditional expectation, however, we are further constrained
    by the fact that we must take only the average over samples that contain the exact
    setting of *S* in question. Unfortunately, the only samples we have from the underlying
    distribution over *U* are contained within the dataset we are provided with. And
    in the common case that the features of *U* are continuous, the likelihood that
    we see the exact setting of *S* in question even once in the dataset is exceedingly
    low. To get around this, PDP makes an independence assumption that allows us to
    use the entire dataset directly to estimate this expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"f prime left-parenthesis upper S right-parenthesis equals double-struck\
    \ upper E Subscript upper U minus upper S vertical-bar upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\
    \ equals double-struck upper E Subscript upper U minus upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\
    \ almost-equals StartFraction 1 Over n EndFraction sigma-summation Underscript\
    \ i equals 1 Overscript n Endscripts f left-parenthesis left-parenthesis upper\
    \ U minus upper S right-parenthesis Superscript i Baseline comma upper S right-parenthesis\"\
    ><mrow><mi>f</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mrow><mo>(</mo> <mi>S</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C</mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi><mo>|</mo><mi>S</mi></mrow></msub>\
    \ <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi>\
    \ <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C\
    </mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi></mrow></msub> <mrow><mo>[</mo> <mi>f</mi>\
    \ <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi> <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow>\
    \ <mo>]</mo></mrow> <mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo>\
    \ <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mi>f</mi>\
    \ <mrow><mo>(</mo> <msup><mrow><mo>(</mo><mi>U</mi><mo>∖</mo><mi>S</mi><mo>)</mo></mrow>\
    \ <mi>i</mi></msup> <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: Where *n* is the number of samples in the dataset. PDP assumes that the features
    in *S* are independent from the features in *U \ S*. This assumption allows us
    to use all of the training samples indiscriminately for computing the expectation
    since, under this assumption, the sampling of *U \ S* is independent from the
    setting of *S* anyway. We now have a concrete method for estimating the marginal
    impact of any arbitrary subset of features *S* on the output of *f*.
  prefs: []
  type: TYPE_NORMAL
- en: If there are significant correlations between the features in *S* and those
    in *U \ S*, then our generated PDP is likely not reflective of the true marginal
    effect of *S* on the output due to bias in our sampling assumption. Essentially,
    we would be taking the average over many samples that are very unlikely to occur,
    which means that we (1) can’t expect *f* to generate meaningful outputs on these
    samples, and (2) are taking the average over the outputs for samples that do not
    accurately reflect the relationships in the underlying distribution over *U*.
  prefs: []
  type: TYPE_NORMAL
- en: The second concern is likely pretty clear, but to illustrate the first, imagine
    that you have trained a neural network to completion on the MNIST dataset. Now,
    I find an image of a dog online and run this image through the network. By chance,
    it turns out that the network returns a 9 with high confidence—should we trust
    these results? Since the input image is completely out of the distribution of
    images that the model expects to see, we can’t trust the model to generalize to
    this extent. Although our situation with PDP is a bit less extreme, it is analogous—we
    have essentially created these unlikely, out-of-distribution “franken-samples”
    and are expecting *f* to produce meaningful outputs on these samples. The independence
    assumption that PDP makes is an inherent limitation of the method, again since
    the only samples we have are those from the dataset. Additionally, PDPs are often
    used to analyze the impact of small subsets of features ( <math alttext="2"><mrow><mo>≤</mo>
    <mn>2</mn></mrow></math> ), since humans can interpret only up to three dimensions
    visually. Regardless, PDPs can be an effective method for visualizing trends between
    subsets of input features and the output of a complex model.
  prefs: []
  type: TYPE_NORMAL
- en: Extractive Rationalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Extractive rationalization, or the selection of concise portions of the input
    that retain most, or all, of the relevant information necessary to predict a property,
    is a built-in form of interpretability at the example level. In this section,
    we will review the methodology of the paper “Rationalizing Neural Predictions,”^([1](ch11.xhtml#idm45934166216704))
    which attempts to do this in the natural language space. In this paper, the task
    at hand is property prediction: given a textual review, predict some properties
    regarding the text. The paper specifically worked with a beer review dataset,
    where each review consisted of some text along with an appearance score, a smell
    score, and a palate score.'
  prefs: []
  type: TYPE_NORMAL
- en: The high-performing but uninterpretable method would be to train a classic property
    predictor using a recurrent architecture, followed by a vanilla regression neural
    network that takes as input the final embedding produced by the recurrent architecture,
    as shown in [Figure 11-4](#fig1104).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. Depicted is the classical property predictor, where x is an encoding
    of the original sentence, h(x) is the hidden state produced by the recurrent architecture
    after reaching the end of x, and y is the result of a standard feed-forward neural
    architecture.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The goal of this paper is to additionally generate rationales, or selected,
    concise portions of the input text that are most relevant to the property being
    predicted, while limiting the hit to performance. This is why this method of rationalization
    is referred to as “extractive”—it works by extracting relevant portions of the
    input. You might be wondering why there is an emphasis on conciseness. If there
    were no limit or penalty on the conciseness of the rationale produced by the model,
    there would be no reason for the model to just return the entire input, which
    is a trivial solution. Of course, all of the information necessary for predicting
    the output is within the rationale if the rationale is the entire input.
  prefs: []
  type: TYPE_NORMAL
- en: How do we modify the structure of the proposed property predictor to also produce
    rationales as a built-in mechanism? This paper proposed a two-network approach,
    where the first network is termed the generator and the second network is termed
    the encoder. The generator is an RNN responsible for selecting the rationale,
    while the encoder is an RNN responsible for predicting the property given solely
    the rationale, not the entire input. The logic behind this is that, given the
    right objective function, the generator will have to learn to select meaningful
    portions of the input text to be able to accurately predict the ground truth rating.
    The generator parameterizes a distribution over all possible binary masks that
    can be applied to the input, where a 1 indicates that the word should be included
    in the rationale, and a 0 indicates otherwise. [Figure 11-5](#fig1105) shows the
    proposed two-step architecture, where the encoder is just the single-step property
    predictor diagrammed earlier, and z represents a binary mask sampled from the
    generator.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-5\. The generator parametrizes a distribution over masks z given input
    x, which we sample from to get the input to the encoder. The encoder follows the
    same structure as that of the classical property predictor depicted earlier.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'More formally, we represent the input text *x* as a vector, where *x[i]* represents
    the token at position *i*. The generator parameterizes the distribution *p(z|x)*,
    where *z* is a vector consisting of individual Bernoulli random variables *z[i]*,
    which each take on the value 1 if *x[i]* is to be included in the rationale, and
    0 otherwise. Note that *z* is the same length as *x*, which changes depending
    on *x*. How exactly do we represent this distribution? A first step is to make
    a reasonable conditional independence assumption, which is that all *z[i]* are
    mutually independent of each other conditioned on *x*: <math alttext="p left-parenthesis
    z vertical-bar x right-parenthesis equals product Underscript i equals 1 Overscript
    n Endscripts p left-parenthesis z Subscript i Baseline vertical-bar z 1 comma
    ellipsis comma z Subscript i minus 1 Baseline comma x right-parenthesis equals
    product Underscript i equals 1 Overscript n Endscripts p left-parenthesis z Subscript
    i Baseline vertical-bar x right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>z</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>z</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> . This is a very reasonable
    assumption because all of the information regarding whether *x[i]* should be in
    the rationale or not should be contained within *x* itself (the token *x[i]* and
    its surrounding context). Converting this to neural net speak, we can implement
    this by applying a fully connected layer followed by a sigmoid activation to each
    final hidden state *h[i]* of the generator independently to get the probability
    of *z[i]* taking on value 1, as we will see soon.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going into the specifics of the objective function, we’ll describe the
    architectures of the generator and the encoder in more detail. The generator and
    encoder are both recurrent architectures, where the recurrent unit could be an
    LSTM or a GRU. As stated in the previous paragraph, the generator produces a hidden
    unit *h[i]* for each token *x[i]*. The final embedding for a token consists of
    two intermediate embeddings: the first intermediate embedding is the result of
    a forward pass through the tokens, while the second intermediate embedding is
    the result of a backward pass through the tokens. More formally, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  ModifyingAbove h With right-arrow Subscript
    i Baseline equals ModifyingAbove f With right-arrow left-parenthesis ModifyingAbove
    h With right-arrow Subscript i minus 1 Baseline comma x Subscript i Baseline right-parenthesis
    2nd Row  ModifyingAbove h With left-arrow Subscript i Baseline equals ModifyingAbove
    f With left-arrow left-parenthesis ModifyingAbove h With left-arrow Subscript
    i plus 1 Baseline comma x Subscript i Baseline right-parenthesis 3rd Row  h Subscript
    i Baseline equals concat left-parenthesis ModifyingAbove h With right-arrow Subscript
    i Baseline comma ModifyingAbove h With left-arrow Subscript i Baseline right-parenthesis
    EndLayout"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mover
    accent="true"><mi>h</mi> <mo>→</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mover
    accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover>
    <mi>i</mi></msub> <mo>=</mo> <mover accent="true"><mi>f</mi> <mo>←</mo></mover>
    <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msub><mi>h</mi> <mi>i</mi></msub> <mo>=</mo>
    <mtext>concat</mtext> <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi> <mo>→</mo></mover>
    <mi>i</mi></msub> <mo>,</mo> <msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Where <math alttext="ModifyingAbove f With right-arrow"><mover accent="true"><mi>f</mi>
    <mo>→</mo></mover></math> and <math alttext="ModifyingAbove f With left-arrow"><mover
    accent="true"><mi>f</mi> <mo>←</mo></mover></math> correspond to two independent
    recurrent units, the former trained on the forward pass and the latter trained
    on the backward pass. From this formulation, we can see that the final embedding
    is bidirectional, incorporating information from the entire context of a token
    rather than information in solely one direction. The paper then applies a single,
    fully connected layer and sigmoid to each embedding to generate an independent
    Bernoulli random variable for each token:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p left-parenthesis z Subscript i Baseline vertical-bar x right-parenthesis
    equals sigma left-parenthesis w Subscript z Baseline dot h Subscript i Baseline
    plus b Subscript z Baseline right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>z</mi></msub> <mo>·</mo> <msub><mi>h</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>z</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The encoder is also a recurrent architecture, but is designed to be a regressive
    architecture due to its purpose of predicting the rating associated with the text.
    For this reason, the encoder can be designed the same way we design the vanilla
    property predictor alluded to earlier in the section.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is the right objective function for training the two networks in tandem?
    In addition to any constraints we may want to have on the rationales the generator
    produces, we must also ensure that the predictor is accurate. If the predictor
    were not accurate, there would be no reason for the generator to produce meaningful
    rationales. Putting this all together into a mathematical formulation, we have
    the following objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  theta Superscript asterisk Baseline comma
    phi Superscript asterisk Baseline equals argmin Subscript theta comma phi Baseline
    upper L left-parenthesis theta comma phi right-parenthesis 2nd Row  upper L left-parenthesis
    theta comma phi right-parenthesis equals sigma-summation Underscript left-parenthesis
    x comma y right-parenthesis element-of upper D Endscripts double-struck upper
    E Subscript z tilde g e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis
    Baseline left-bracket cost left-parenthesis x comma y comma z right-parenthesis
    right-bracket 3rd Row  cost left-parenthesis x comma y comma z right-parenthesis
    equals lamda 1 asterisk StartAbsoluteValue z EndAbsoluteValue plus lamda 2 asterisk
    sigma-summation Underscript t Endscripts StartAbsoluteValue z Subscript t Baseline
    minus z Subscript t minus 1 Baseline EndAbsoluteValue plus StartAbsoluteValue
    EndAbsoluteValue e n c Subscript phi Baseline left-parenthesis x comma z right-parenthesis
    minus y StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>,</mo> <msup><mi>φ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext>
    <mrow><mi>θ</mi><mo>,</mo><mi>φ</mi></mrow></msub> <mi>L</mi> <mrow><mo>(</mo>
    <mi>θ</mi> <mo>,</mo> <mi>φ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>,</mo> <mi>φ</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>∈</mo><mi>D</mi></mrow></munder>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <mrow><mo>*</mo> <mo>|</mo> <mi>z</mi>
    <mo>|</mo></mrow> <mo>+</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <mo>*</mo> <munder><mo>∑</mo>
    <mi>t</mi></munder> <mrow><mo>|</mo></mrow> <msub><mi>z</mi> <mi>t</mi></msub>
    <mo>-</mo> <msub><mi>z</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>|</mo> <mo>+</mo> <mo>|</mo> <mo>|</mo> <mi>e</mi> <mi>n</mi></mrow>
    <msub><mi>c</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>-</mo> <mi>y</mi> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Where  <math alttext="lamda 1"><msub><mi>λ</mi> <mn>1</mn></msub></math> and 
    <math alttext="lamda 2"><msub><mi>λ</mi> <mn>2</mn></msub></math> are hyperparameters
    we can tune during validation. The cost function used in the paper additionally
    contains a continuity penalty, which is higher when the rationale is interspersed
    throughout the text rather than one contiguous block. We want to minimize the
    sum of the expected cost for each training example, where the rationales are drawn
    according to the generator distribution. Calculating the expected cost exactly
    is computationally prohibitive due to the number of configurations of *z* growing
    exponentially with the length of *x*, so we’d instead like to be able to approximate
    the gradient of the expected cost via some empirical, sampled estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is feasible for the gradient of the cost function with respect to the
    parameters of the encoder, but when we try to do this for the generator, we run
    into a similar issue as we did when we first tried optimizing the VAE encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript theta Baseline double-struck upper E Subscript
    z tilde g e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis
    Baseline left-bracket cost left-parenthesis x comma y comma z right-parenthesis
    right-bracket equals sigma-summation Underscript z Endscripts cost left-parenthesis
    x comma y comma z right-parenthesis asterisk normal nabla Subscript theta Baseline
    p Subscript theta Baseline left-parenthesis z vertical-bar x right-parenthesis"><mrow><msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>z</mi></msub>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the cost function is only indirectly a function of  <math alttext="theta"><mi>θ</mi></math>
     via sampling from the generator, and thus can be treated as a constant. We can’t
    re-express this as an expectation since the gradient is with respect to the distribution
    from which we are sampling from. This paper uses the log trick, which we also
    introduced in the section on VAEs, to resolve this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  sigma-summation Underscript z Endscripts
    cost left-parenthesis x comma y comma z right-parenthesis asterisk normal nabla
    Subscript theta Baseline p Subscript theta Baseline left-parenthesis z vertical-bar
    x right-parenthesis 2nd Row  equals sigma-summation Underscript z Endscripts cost
    left-parenthesis x comma y comma z right-parenthesis asterisk p Subscript theta
    Baseline left-parenthesis z vertical-bar x right-parenthesis asterisk normal nabla
    Subscript theta Baseline log p Subscript theta Baseline left-parenthesis z vertical-bar
    x right-parenthesis 3rd Row  equals double-struck upper E Subscript z tilde g
    e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis Baseline
    left-bracket cost left-parenthesis x comma y comma z right-parenthesis asterisk
    normal nabla Subscript theta Baseline log p Subscript theta Baseline left-parenthesis
    z vertical-bar x right-parenthesis right-bracket EndLayout"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><munder><mo>∑</mo> <mi>z</mi></munder> <mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <munder><mo>∑</mo> <mi>z</mi></munder>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi>
    <mi>θ</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <mo
    form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient of the cost function with respect to the parameters of the encoder
    is just:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  normal nabla Subscript phi Baseline double-struck
    upper E Subscript z tilde g e n Sub Subscript theta Subscript left-parenthesis
    x right-parenthesis Baseline left-bracket cost left-parenthesis x comma y comma
    z right-parenthesis right-bracket 2nd Row  equals sigma-summation Underscript
    z Endscripts p Subscript theta Baseline left-parenthesis z vertical-bar x right-parenthesis
    asterisk normal nabla Subscript phi Baseline cost left-parenthesis x comma y comma
    z right-parenthesis 3rd Row  equals double-struck upper E Subscript z tilde g
    e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis Baseline
    left-bracket normal nabla Subscript phi Baseline cost left-parenthesis x comma
    y comma z right-parenthesis right-bracket EndLayout"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <munder><mo>∑</mo> <mi>z</mi></munder> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi>
    <mi>φ</mi></msub> <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <msub><mi>∇</mi> <mi>φ</mi></msub> <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Which resembles the standard empirical estimate of the expected gradient when
    performing SGD or minibatch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about training these two networks in tandem? It might be easier
    to consider a single training example for starters. We first select a training
    example at random from the dataset, where a training example consists of a text
    review and an associated rating, and feed the text review to the generator. The
    generator, which now represents a probability distribution over all possible binary
    masks given the input text review, can be sampled from by sampling each *z[i]*
    independently due to our conditional independence claim from earlier. Each sampled
    binary mask represents a possible rationale, which we then feed to the encoder
    for prediction. After obtaining the result of the encoder for each rationale,
    we have all the information we need to calculate the cost function for each rationale.
    This will suffice for updating the weights of the encoder, but to update the weights
    of the generator we will also need to keep track of the log likelihood of the
    rationale, or  <math alttext="log p Subscript theta Baseline left-parenthesis
    z Superscript k Baseline vertical-bar x right-parenthesis"><mrow><mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mi>k</mi></msup>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> for each sampled *z^k*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a mechanism for training, how do we translate this to validating
    and testing our model? During the validation and testing phases, instead of sampling
    binary masks from the generator, we select the most likely binary mask according
    to the generator probability distribution. To select the most likely binary mask,
    all we need to do is select the most likely *z[i]* for each *x[i]* in our input
    test review *x*, again due to our conditional independence assumption from earlier.
    This is a very reasonable approach to testing, since this is how we would determine
    the intended rationale when using this model in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed some parallels to the concept of attention. After all,
    the generated binary mask can be thought of as a vector of weights we use to multiply
    the feature vectors that make up the input text review, where these weights are
    either 0 or 1, rather than some continuous weighting scheme implemented in standard
    attention. Indeed, the authors of this paper mention that their approach can be
    viewed as a form of “hard” attention, where we completely mask out or input tokens
    of the input according to a probability distribution rather than computing a weighted
    average of the feature vectors in the input. You might be wondering why hard attention
    makes more sense in this case rather than the “soft” attention schemes presented
    in the previous section. In this case, hard attention schemes make more sense
    because fractional weights on words in a sentence are hard to interpret as a measure
    of importance, while selecting a strict subset of words in the text as the explanation
    for a rating is much more interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: LIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LIME, or Local Interpretable Model-agnostic Explanations,^([2](ch11.xhtml#idm45934166151520))
    is an interpretability technique that is applied to a trained model rather than
    a built-in feature of the model itself. LIME is a per-example interpretability
    method, meaning that it generates a simple, local explanation of the underlying
    model’s potentially complex behavior. It is also model agnostic, meaning that
    the structure of the underlying model itself does not matter when applying LIME.
  prefs: []
  type: TYPE_NORMAL
- en: Before describing the methodology of LIME, the authors take some time to delineate
    a few characteristics they believe to be necessary components of any explainer.
    The first is that it should be interpretable, meaning that the explainer should
    provide a “qualitative relationship between input variables and response” that
    is easy for the user to understand. Even if the features used in the original
    model are uninterpretable, the explainer must use features that a human can interpret.
    For example, in an application of natural language processing, even if the underlying
    model utilizes a complex word embedding for any given word, the explainer must
    use features that a human can understand, such as the original words themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The second characteristic is local fidelity, which means that the explainer
    must behave similarly to the underlying model within some vicinity of the chosen
    example. We might ask, why local and not global fidelity? Global fidelity, however,
    as the paper notes, is quite difficult to achieve and would require drastic advances
    in the field—much of the field of interpretability would be solved if global fidelity
    could be achieved. Thus, we settle for local fidelity.
  prefs: []
  type: TYPE_NORMAL
- en: The third is for the explainer to be model agnostic, which, as we explained
    earlier, means that the structure of the underlying model itself should not matter.
    The underlying model can range from a linear regression model to a complex convolutional
    neural architecture, and the explainer should still be able to satisfy the other
    three characteristics. Being model agnostic allows for flexibility in the structure
    of the underlying model, which is desirable as this doesn’t necessitate changes
    in the structure of the explainer.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth and final characteristic is global perspective, which is to select
    explanations for a subset of examples that is representative of the model’s behavior.
    This helps build user trust in the model.
  prefs: []
  type: TYPE_NORMAL
- en: "Now we will take some time to develop the methodology of LIME. As stated, the\
    \ features of the original model may not be interpretable to a human (and usually\
    \ aren’t for most complex models), so the features used by the explainer will\
    \ be different from those used by the underlying model. The features used by the\
    \ explainer could be individual words in an NLP task, or functional groups in\
    \ a chemical property prediction task—units, or interpretable components, that\
    \ the end user can understand easily. Thus, any example when converted to the\
    \ feature space of the explainer becomes a binary vector, where each index is\
    \ associated with a distinct interpretable component (such as a functional group).\
    \ A one at any index *i* indicates the presence of the associated interpretable\
    \ component in the original example, and a zero indicates a lack of that component\
    \ in the original example. Following the notation used in the referenced paper,\
    \ we denote  <math alttext=\"x element-of double-struck upper R Superscript d\"\
    ><mrow><mi>x</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mi>d</mi></msup></mrow></math>\
    \ to be the original feature representation of the example to be explained and \
    \ <math alttext=\"x prime element-of StartSet 0 comma 1 EndSet Superscript d prime\"\
    ><mrow><mi>x</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∈</mo> <msup><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow>\
    \ <mrow><mi>d</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi></mrow></msup></mrow></math>\
    \ to be the representation acted upon by the explainer, where *d’* is the number\
    \ of interpretable components."
  prefs: []
  type: TYPE_NORMAL
- en: Further, the paper defines *G* to be a class of potentially interpretable models,
    such as linear regression or random forest, and an explainer to be an instance
    <math alttext="g element-of upper G"><mrow><mi>g</mi> <mo>∈</mo> <mi>G</mi></mrow></math>
    . *g* acts on an instance *x’* and returns a value in the range of the underlying
    model. We denote the underlying model to be *f*, which acts on an instance *x*
    and is a function from  <math alttext="double-struck upper R Superscript d Baseline
    right-arrow double-struck upper R"><mrow><msup><mi>ℝ</mi> <mi>d</mi></msup> <mo>→</mo>
    <mi>ℝ</mi></mrow></math> in the case of regression or a function from <math alttext="double-struck
    upper R Superscript d"><msup><mi>ℝ</mi> <mi>d</mi></msup></math>  to the range
    [0,1] in the case of classification, where *f* returns a probability distribution.
    Additionally, the paper defines a proximity measure, or kernel,  <math alttext="pi
    Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    around the instance *x*. This function can be defined in a multitude of ways—most
    implementations of LIME use an exponential kernel that attains a maximum value
    at *x* and decreases exponentially as one gets farther and farther from *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, LIME attempts to find the explanation  <math alttext="g Superscript
    asterisk"><msup><mi>g</mi> <mo>*</mo></msup></math> that minimizes a loss function
    that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="g Superscript asterisk Baseline equals argmin Subscript g element-of
    upper G Baseline upper L left-parenthesis f comma g comma x right-parenthesis
    plus omega left-parenthesis g right-parenthesis"><mrow><msup><mi>g</mi> <mo>*</mo></msup>
    <mo>=</mo> <msub><mtext>argmin</mtext> <mrow><mi>g</mi><mo>∈</mo><mi>G</mi></mrow></msub>
    <mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>g</mi> <mo>,</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>+</mo> <mi>ω</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Where *L(f,g,x)* is a measure of the unfaithfulness of *g* in modeling *f* around
    the instance in question *x*, and  <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> is a measure of the complexity
    of *g*. Thus, minimizing their sum results in an optimal explainer  <math alttext="g
    Superscript asterisk"><msup><mi>g</mi> <mo>*</mo></msup></math> that has the desired
    characteristics of local fidelity and interpretability described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we measure the unfaithfulness of a potential explainer? The paper’s
    methodology is to sample an instance *z’* from the vicinity of *x’*, convert *z’*
    back to an example *z* in the original feature space, and compute the difference
    between *f(z)* and *g(z’)*. The difference represents the loss for that sample—if
    *g(z’)* is far from *f(z)*, then it is not faithful to the model’s predictions
    at that point. We can then weight this loss using the kernel <math alttext="pi
    Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , which increasingly discounts the loss as the sample *z* gets further and further
    from the original example *x*. Putting this together, the loss function looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"upper L left-parenthesis f comma g comma x right-parenthesis\
    \ equals sigma-summation Underscript z comma z Superscript prime Baseline Endscripts\
    \ pi Subscript x Baseline left-parenthesis z right-parenthesis asterisk left-parenthesis\
    \ f left-parenthesis z right-parenthesis minus g left-parenthesis z prime right-parenthesis\
    \ right-parenthesis squared\"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo>\
    \ <mi>g</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo>\
    \ <mrow><mi>z</mi><mo>,</mo><mi>z</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi></mrow></msub>\
    \ <msub><mi>π</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>\
    \ <mo>*</mo> <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><mi>z</mi><mi>â</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: How do we achieve the samples *z’* used in this loss function? The paper samples
    from the vicinity of *x’* by selecting a subset of the *x’* nonzero components,
    where each subset is chosen uniformly at random, and setting all other indices
    of the sample to zero ([Figure 11-6](#fig1106)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-6\. x can be thought of as some high-dimensional input such as an
    image, while each index of x’ is associated with some interpretable feature, where
    a 1 denotes the existence of that feature in x. The sampling procedure selects
    some subset of nonzero indices in x’ to keep nonzero in each of w’ and z', which
    are then mapped back to the original input space.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LIME then maps these samples *z’* back to samples *z* from the original feature
    space so we can measure the fidelity of the explainer via *f(z) – g(z’)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'LIME also takes into account the complexity of the explainer via <math alttext="omega
    left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    , which enforces the interpretability aspect of viable explainers. In the specific
    case where *G* represents the class of linear models, the paper uses a version
    of <math alttext="omega"><mi>ω</mi></math>  that places a hard limit on the number
    of nonzero weights in *g*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="omega left-parenthesis g right-parenthesis equals normal infinity
    asterisk 1 left-bracket StartAbsoluteValue EndAbsoluteValue w Subscript g Baseline
    StartAbsoluteValue EndAbsoluteValue Subscript 0 Baseline greater-than upper K
    right-bracket"><mrow><mi>ω</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>∞</mi> <mo>*</mo> <mn>1</mn> <mo>[</mo> <mo>|</mo> <mo>|</mo> <msub><mi>w</mi>
    <mi>g</mi></msub> <mo>|</mo> <msub><mo>|</mo> <mn>0</mn></msub> <mo>></mo> <mi>K</mi>
    <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Where <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> represents *g’*s weight vector,
    the L0 norm counts the number of nonzero elements in <math alttext="omega left-parenthesis
    g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    , and *1[*]* is the indicator function, which evaluates to 1 if the condition
    within the function is satisfied, and 0 otherwise. The result is that  <math alttext="omega
    left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    attains a value of infinity when <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> has more than *K* nonzero elements,
    and is 0 otherwise. This ensures that the chosen <math alttext="omega left-parenthesis
    g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    will have at most *K* nonzero elements, since one can always do better than any
    proposed <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> that has more than *K* nonzero
    elements by simply zeroing out weights until there are at most *K* nonzero weights.
    This regularization approach is likely different from regularization approaches
    you have encountered in the past, such as the L1 or L2 norm on the weight vector.
    In fact, to optimize the objective function defined in the paper, the authors
    utilize an algorithm they term K-LASSO, which involves first selecting *K* features
    via LASSO and then performing standard least squares optimization.
  prefs: []
  type: TYPE_NORMAL
- en: After performing LIME, we are left with an optimal explainer *g*, which is a
    linear model with at most *K* nonzero weights in this case. Now we must check
    if *g* satisfies the goals that the authors set out from the beginning of the
    paper. First, *g* must be interpretable. Since we chose a relatively simple class
    of explainer models *G*, which were linear models in this example, all we need
    to explain the behavior of the model around the chosen example *x* are the values
    of the (at most) *K* nonzero weights of *g*. The interpretable components associated
    with the nonzero weights are considered to be most important for prediction in
    that locality. In terms of local fidelity, our optimization procedure helps to
    ensure local fidelity by minimizing the least squares loss between the explainer’s
    predictions and the model’s predictions. However, there do exist limitations;
    for example, the paper notes that if the underlying model is highly nonlinear
    even within a short vicinity of the example we are explaining, our linear explainer
    won’t be able to do the model’s local behavior justice. With regards to being
    model agnostic, note that methodology of LIME does not concern itself with the
    underlying model’s structure. All LIME needs to function are predictions *f(z)*
    from the underlying model. And finally, to achieve a global perspective, we can
    select examples that are representative of the model’s behavior and display their
    explanations to the user.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SHAP, or Shapley Additive Explanations,^([3](ch11.xhtml#idm45934166079056))
    are similarly a per-prediction interpretability method for complex models. The
    paper that introduces the methodology of SHAP first provides a framework that
    the authors feel unifies a variety of interpretability methods in the field. This
    framework is termed *additive feature attribution*, where all instances of this
    framework utilize a linear explanation model that acts on binary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="g left-parenthesis x prime right-parenthesis equals phi 0 plus
    sigma-summation Underscript i equals 1 Overscript upper M Endscripts phi Subscript
    i Baseline x prime Subscript i"><mrow><mi>g</mi> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mo>'</mo></msup> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>φ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup>
    <msub><mi>φ</mi> <mi>i</mi></msub> <msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Where *M* is the number of binary variables. For example, LIME, when using
    a class of linear explainer models, follows this framework exactly, since each
    example to be explained is first converted to a binary vector over interpretable
    components. It turns out that, in the additive feature attribution framework,
    there exists a unique solution in this class that has three desirable properties:
    local accuracy, missingness, and consistency. Before discussing the unique solution,
    we will describe the three properties in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first is *local accuracy*, which states that the explainer model must match
    the underlying model exactly at the example being interpreted. This is understandable
    as a desirable property, since it is reasonable that at least the example being
    interpreted should be explained perfectly. It’s important to note that not all
    interpretability frameworks necessarily follow this property. For example, the
    explainer that is generated by LIME, as presented in its original paper and described
    in the previous section, need not be locally accurate in the way that the authors
    of SHAP define local accuracy. This will be discussed further near the end of
    this section. Mathematically, local accuracy in SHAP is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"f left-parenthesis x right-parenthesis equals g left-parenthesis\
    \ x prime right-parenthesis equals phi 0 plus sigma-summation Underscript i equals\
    \ 1 Overscript upper M Endscripts phi Subscript i Baseline x Subscript i Superscript\
    \ prime\"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>\
    \ <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>φ</mi> <mn>0</mn></msub> <mo>+</mo> <msubsup><mo>∑</mo>\
    \ <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup> <msub><mi>φ</mi>\
    \ <mi>i</mi></msub> <msubsup><mi>x</mi> <mi>i</mi> <msup><mo>'</mo></msup></msubsup></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: Note that *x’* is a simplified feature vector, where each feature in *x’* is
    a binary variable that represents the presence or absence of a complex feature
    in the original input space. The second desirable property is *missingness*, which
    states that if *x’* contains features equal to zero, the weights associated with
    those features in the explainer model should also be zero. This is also understandable
    as a desirable property, since there would be no influence of a feature with value
    of zero on the output under a linear explainer *g*, and thus no need to assign
    a nonzero weight to that feature in the explainer.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, the third desirable property is *consistency*. This property states
    that if the underlying model changes such that a feature in the explainer space
    either increases or keeps its contribution constant, regardless of the values
    of the other features in the explainer space when compared to the original model,
    the explainer weight associated with that feature should be larger for the changed
    underlying model as compared to the original. That was a mouthful, so we represent
    it more precisely in mathematical notation:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"If f prime left-parenthesis h Subscript x Baseline left-parenthesis\
    \ z Superscript prime Baseline right-parenthesis right-parenthesis minus f prime\
    \ left-parenthesis h Subscript x Baseline left-parenthesis z prime minus StartSet\
    \ i EndSet right-parenthesis right-parenthesis greater-than-or-equal-to f left-parenthesis\
    \ h Subscript x Baseline left-parenthesis z Superscript prime Baseline right-parenthesis\
    \ right-parenthesis minus f left-parenthesis h Subscript x Baseline left-parenthesis\
    \ z prime minus StartSet i EndSet right-parenthesis right-parenthesis comma for-all\
    \ z Superscript prime Baseline comma then phi Subscript i Baseline left-parenthesis\
    \ f prime comma x right-parenthesis greater-than-or-equal-to phi Subscript i Baseline\
    \ left-parenthesis f comma x right-parenthesis\"><mrow><mtext>If</mtext> <msup><mi>f</mi>\
    \ <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo>\
    \ <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∖</mo>\
    \ <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>\
    \ <mo>≥</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo>\
    \ <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∖</mo> <mrow><mo>{</mo>\
    \ <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo>\
    \ <mo>∀</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>,</mo> <mtext>then</mtext>\
    \ <msub><mi>φ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msup><mi>f</mi> <mo>'</mo></msup>\
    \ <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>≥</mo> <msub><mi>φ</mi> <mi>i</mi></msub>\
    \ <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: Where *h* is the function that maps inputs from the interpretable space back
    to the original input space. Why is consistency a desirable property? For the
    new model, the delta between the existence of the corresponding, more complex
    feature in the input space and its absence is greater than or equal to the delta
    for the old model, regardless of all other feature settings. Thus, it makes sense
    that we should attribute at least as large a weight to it in the explainer for
    the new model as compared to the old model, since its existence is clearly more
    important for the new model.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, for each underlying model *f* there exists a unique *g* within
    the additive attribution framework that also satisfies all three properties listed.
    Although we won’t show this here, this result is one that follows from earlier
    results in cooperative game theory, where the learned weights are called Shapley
    values. Shapley values were originally defined to quantify per-example feature
    importance in multivariate linear regression models where individual features
    have significant correlations. This is an important problem, especially in the
    setting of significant correlations due to ambiguity between which features are
    the most predictive. It could be the case that a feature A is correlated with
    the target y, but when factoring in feature B it turns out that feature A provides
    only negligible additional value (i.e., predictions don’t change significantly,
    test statistics remain relatively constant, etc.). On the other hand, feature
    B may provide significant predictive power both in the individual case and in
    the case where feature A is included.
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining the relative importance of feature A and B in a vanilla multivariate
    regression that includes both features is difficult due to their non-negligible
    correlation. Shapley values tease out these relationships and compute the true
    marginal impact of a given feature, as we will soon see. Here is the formula for
    Shapley values, where *i* represents the feature in question:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="phi Subscript i Baseline equals sigma-summation Underscript upper
    S element-of upper F minus StartSet i EndSet Endscripts StartFraction StartAbsoluteValue
    upper S EndAbsoluteValue factorial asterisk left-parenthesis StartAbsoluteValue
    upper F EndAbsoluteValue minus StartAbsoluteValue upper S EndAbsoluteValue minus
    1 right-parenthesis factorial Over StartAbsoluteValue upper F EndAbsoluteValue
    factorial EndFraction asterisk left-bracket f Subscript upper S union StartSet
    i EndSet Baseline left-parenthesis x Subscript upper S union StartSet i EndSet
    Baseline right-parenthesis minus f Subscript upper S Baseline left-parenthesis
    x Subscript upper S Baseline right-parenthesis right-bracket"><mrow><msub><mi>φ</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>S</mi><mo>∈</mo><mi>F</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mfrac><mrow><mo>|</mo><mi>S</mi><mo>|</mo><mo>!</mo><mo>*</mo><mo>(</mo><mo>|</mo><mi>F</mi><mo>|</mo><mo>-</mo><mo>|</mo><mi>S</mi><mo>|</mo><mo>-</mo><mn>1</mn><mo>)</mo><mo>!</mo></mrow>
    <mrow><mo>|</mo><mi>F</mi><mo>|</mo><mo>!</mo></mrow></mfrac> <mo>*</mo> <mrow><mo>[</mo>
    <msub><mi>f</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>S</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We will now break down this formula. Intuitively, the Shapley value for an individual
    feature is computed by first taking the difference between the predictions for
    a model trained over a subset of features *S* plus the feature in question *i*
    included, and predictions for a model trained over the same subset of features
    *S* but with feature *i* withheld. The final Shapley value is a weighted sum of
    these differences over all possible subsets of features *S*.
  prefs: []
  type: TYPE_NORMAL
- en: To find these differences, we can first train a multivariate linear regression
    model  <math alttext="f Subscript upper S"><msub><mi>f</mi> <mi>S</mi></msub></math>
    that only uses some subset of features *S*, and then train a second multivariate
    linear regression model  <math alttext="f Subscript upper S union StartSet i EndSet"><msub><mi>f</mi>
    <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub></math>
    that uses the subset of features <math alttext="upper S union StartSet i EndSet"><mrow><mi>S</mi>
    <mo>∪</mo> <mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></math> . Let the example we
    are explaining be denoted as *x*, and *x[A]* denote the portion of *x* that corresponds
    to some feature subset A. The difference  <math alttext="f Subscript upper S union
    StartSet i EndSet Baseline left-parenthesis x Subscript upper S union StartSet
    i EndSet Baseline right-parenthesis"><mrow><msub><mi>f</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></mrow></math> – <math alttext="f Subscript upper S Baseline
    left-parenthesis x Subscript upper S Baseline right-parenthesis"><mrow><msub><mi>f</mi>
    <mi>S</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>S</mi></msub> <mo>)</mo></mrow></mrow></math>
    represents how much the prediction changes when we include the feature *i*. Additionally,
    note that the formula is a sum over all possible feature subsets, which means
    that if the computed Shapley value for feature *i* is high, the difference between
    including feature *i* and not including feature *i* was likely substantial for
    a majority of possible feature subsets. This result signifies that feature *i*
    generally adds significant predictive power regardless of the features in *S*,
    which is captured by the high Shapley value. In the example provided earlier,
    we’d find that feature B has a higher Shapley value than feature A.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the intuition behind the weighting scheme is that it achieves
    a more unbiased result for feature importance, since subsets of a given size occur
    either more or less frequently than subsets of a different size in the set of
    all subsets. The number of subsets of a given size is computed using the choose
    function, a concept from counting and probability. When this is inverted and used
    as a weighting scheme, the result of a subset whose size occurs more frequently
    in the set of all possible subsets is weighted less than, for example, a feature
    subset consisting of only a single feature other than the feature in question.
    As stated earlier, we won’t prove why this is unbiased in full, but we hope that
    this makes the intuition clearer.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may notice that computing exact Shapley regression values for any reasonable
    number of features is intractable. This would involve training a regression model
    on all possible subsets of features, where the number of subsets of features (and
    thus models to train) grows exponentially with the number of features. We instead
    resort to approximation via sampling to help. Given an example *x* to explain
    and a regression model  <math alttext="f Subscript upper S"><msub><mi>f</mi> <mi>S</mi></msub></math>
    trained on some subset of features *S*, we can compute  <math alttext="f Subscript
    upper S minus StartSet i EndSet"><msub><mi>f</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub></math>
    by taking an expectation of  <math alttext="f Subscript upper S"><msub><mi>f</mi>
    <mi>S</mi></msub></math> with respect to the distribution of feature *i* conditioned
    on *x*’s setting of features <math alttext="upper S minus StartSet i EndSet"><mrow><mi>S</mi>
    <mo>∖</mo> <mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f Subscript upper S minus StartSet i EndSet Baseline equals double-struck
    upper E Subscript p left-parenthesis x Sub Subscript i Subscript vertical-bar
    x Sub Subscript upper S minus StartSet i EndSet Subscript right-parenthesis Baseline
    left-bracket f Subscript upper S Baseline left-parenthesis x Subscript upper S
    minus StartSet i EndSet Baseline comma x Subscript i Baseline right-parenthesis
    right-bracket"><mrow><msub><mi>f</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>|</mo><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Where the bold represents that <math alttext="x Subscript upper S minus StartSet
    i EndSet"><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub></math>
    is being treated as a known entity taken from *x*, the example being explained,
    while *x[i]* is being treated as an unknown, i.e., is a random variable rather
    than taking on the value provided by *x*. As has been a common theme throughout
    this book, we can approximate expectations like the preceding one in an unbiased
    manner by sampling and averaging:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row  double-struck upper E Subscript p left-parenthesis
    x Sub Subscript i Subscript vertical-bar x Sub Subscript upper S minus StartSet
    i EndSet Subscript right-parenthesis Baseline left-bracket f Subscript upper S
    Baseline left-parenthesis x Subscript upper S minus StartSet i EndSet Baseline
    comma x Subscript i Baseline right-parenthesis right-bracket 2nd Row  equals double-struck
    upper E Subscript p left-parenthesis x Sub Subscript i Subscript right-parenthesis
    Baseline left-bracket f Subscript upper S Baseline left-parenthesis x Subscript
    upper S minus StartSet i EndSet Baseline comma x Subscript i Baseline right-parenthesis
    right-bracket 3rd Row  almost-equals StartFraction 1 Over n EndFraction sigma-summation
    Underscript j equals 1 Overscript n Endscripts f Subscript upper S Baseline left-parenthesis
    x Subscript upper S minus StartSet i EndSet Baseline comma x Subscript i Superscript
    left-parenthesis j right-parenthesis Baseline right-parenthesis EndLayout"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>|</mo><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msubsup><mi>x</mi> <mi>i</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msubsup>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed the parallels between the procedure we just described and
    the estimation procedure described in [“Partial Dependence Plots”](#pdp-sect).
    In fact, these are doing the exact same thing—notice that we again assume independence
    between feature subset <math alttext="upper S minus StartSet i EndSet"><mrow><mi>S</mi>
    <mo>∖</mo> <mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></math> and feature *i*, which
    allows us to use all samples of feature *i* from the dataset indiscriminately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors propose SHAP values in the general case, where SHAP values are
    given by the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="phi Subscript i Baseline left-parenthesis f comma x right-parenthesis
    equals sigma-summation Underscript z prime subset-of-or-equal-to x Superscript
    prime Baseline Endscripts StartFraction StartAbsoluteValue z Superscript prime
    Baseline EndAbsoluteValue factorial asterisk left-parenthesis upper M minus StartAbsoluteValue
    z Superscript prime Baseline EndAbsoluteValue minus 1 right-parenthesis factorial
    Over upper M factorial EndFraction left-bracket f left-parenthesis h Subscript
    x Baseline left-parenthesis z prime right-parenthesis right-parenthesis minus
    f left-parenthesis h Subscript x Baseline left-parenthesis z prime minus StartSet
    i EndSet right-parenthesis right-parenthesis right-bracket"><mrow><msub><mi>φ</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mrow><msup><mi>z</mi> <mo>'</mo></msup> <mo>⊆</mo><msup><mi>x</mi>
    <mo>'</mo></msup></mrow></msub> <mfrac><mrow><mrow><mo>|</mo></mrow><msup><mi>z</mi>
    <mo>'</mo></msup> <mrow><mo>|</mo><mo>!</mo><mo>*</mo><mo>(</mo><mi>M</mi><mo>-</mo><mo>|</mo></mrow><msup><mi>z</mi>
    <mo>'</mo></msup> <mrow><mo>|</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>!</mo></mrow>
    <mrow><mi>M</mi><mo>!</mo></mrow></mfrac> <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>∖</mo>
    <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: "Where *z’* is a subset of the nonzero components of *x’*. Additionally,  <math\
    \ alttext=\"z prime minus StartSet i EndSet\"><mrow><msup><mi>z</mi> <mo>'</mo></msup>\
    \ <mrow><mo>∖</mo> <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></mrow></mrow></math>\
    \  represents setting feature *i* in the interpretable space equal to zero. Note\
    \ that if feature *i* is already zero in the input *x’*, then the formula outputs\
    \ zero as well since  <math alttext=\"f left-parenthesis h Subscript x Baseline\
    \ left-parenthesis z Superscript prime Baseline right-parenthesis right-parenthesis\
    \ equals f left-parenthesis h Subscript x Baseline left-parenthesis z prime minus\
    \ StartSet i EndSet right-parenthesis right-parenthesis comma for-all z Superscript\
    \ prime Baseline subset-of-or-equal-to x prime\"><mrow><mi>f</mi> <mrow><mo>(</mo>\
    \ <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup>\
    \ <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi>\
    \ <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99\
    </mi> <mo>∖</mo> <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow>\
    \ <mo>)</mo></mrow> <mo>,</mo> <mo>∀</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi>\
    \ <mi>\x99</mi> <mo>⊆</mo> <msup><mi>x</mi> <mo>'</mo></msup></mrow></math> .\
    \ This quick check shows that the formula indeed satisfies the property of missingness.\
    \ The vector consisting of SHAP values for each feature in *x’* completely defines\
    \ the optimal explainer model *g* in the additive attribution framework, where\
    \ optimal signifies that *g* satisfies all three properties defined earlier: local\
    \ accuracy, consistency, and missingness. Right off the bat, we can see the parallels\
    \ between the proposed SHAP values and the Shapley values from multivariate regression.\
    \ Additionally, we can use the same sampling procedure to estimate SHAP values."
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, LIME is in the additive attribution framework. In the original
    LIME paper, the optimal explainer model *g* was selected via a specialized optimization
    procedure that first selected *k* features to have a nonzero contribution and
    then performed standard least squares optimization to achieve the final weights
    of *g*. Due to these heuristics, including the choice of kernel <math alttext="pi
    Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , there is no guarantee that the explainer selected using the procedure presented
    in the original LIME paper will satisfy the SHAP criteria of local accuracy, missingness,
    and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: However, the optimization procedure presented in the LIME paper does achieve
    an explainer that satisfies the criteria for explainer models proposed in LIME;
    recall the concepts of being interpretable, having local fidelity, being model
    agnostic, and achieving global perspective from the previous section. We point
    this out specifically to show that different groups of knowledgeable individuals
    don’t necessarily have the exact same idea of what it means for an explainer to
    be interpretable, and that interpretability as a concept has evolved over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that in LIME, there exists an exact form to the proximity measure
    <math alttext="pi Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , <math alttext="omega"><mi>ω</mi></math> , and loss function *L* such that when
    minimized, results in an optimal explainer *g* that satisfies all three SHAP criteria
    for interpretability:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"StartLayout 1st Row  omega left-parenthesis g right-parenthesis\
    \ equals 0 2nd Row  pi Subscript x prime Baseline left-parenthesis z prime right-parenthesis\
    \ equals StartFraction upper M minus 1 Over left-parenthesis upper M choose StartAbsoluteValue\
    \ z Superscript prime Baseline EndAbsoluteValue right-parenthesis asterisk StartAbsoluteValue\
    \ z prime EndAbsoluteValue asterisk left-parenthesis upper M minus StartAbsoluteValue\
    \ z prime EndAbsoluteValue right-parenthesis EndFraction 3rd Row  upper L left-parenthesis\
    \ f comma g comma pi right-parenthesis equals sigma-summation Underscript z prime\
    \ element-of upper Z Endscripts left-parenthesis f left-parenthesis h Subscript\
    \ x Baseline left-parenthesis z prime right-parenthesis right-parenthesis minus\
    \ g left-parenthesis z prime right-parenthesis right-parenthesis squared asterisk\
    \ pi Subscript x prime Baseline left-parenthesis z prime right-parenthesis EndLayout\"\
    ><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>ω</mi>\
    \ <mo>(</mo> <mi>g</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd\
    \ columnalign=\"right\"><mrow><msub><mi>π</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub>\
    \ <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>=</mo>\
    \ <mfrac><mrow><mi>M</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mrow><mo>(</mo><mi>M</mi><mtext>choose</mtext><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>)</mo><mo>*</mo><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>*</mo><mo>(</mo><mi>M</mi><mo>-</mo><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>)</mo></mrow></mrow></mfrac></mrow></mtd></mtr>\
    \ <mtr><mtd columnalign=\"right\"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>f</mi>\
    \ <mo>,</mo> <mi>g</mi> <mo>,</mo> <mi>π</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo>\
    \ <mrow><mi>z</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi><mo>∈</mo><mi>Z</mi></mrow></munder>\
    \ <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>h</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo><msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup> <mo>*</mo>\
    \ <msub><mi>π</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>"
  prefs: []
  type: TYPE_NORMAL
- en: We can optimize this loss function using a weighted least squares optimization
    to obtain the unique optimal *g*. Note that the kernel here is distinct in interpretation
    from the kernel choices presented in the original LIME paper. Instead of the kernel
    decreasing in value as the samples get farther from the example being explained,
    the SHAP kernel is symmetric. This can be verified by examining the output of
    the kernel when *|z'| = k* and when *|z'| = M – k*. In fact, from just looking
    at the formula, we can see that the value of the kernel isn’t even dependent on
    *x’*.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, SHAP values unify several existing interpretability methods by
    first defining the additive attribution framework, which is shared amongst these
    methods, and second by proving the existence of a unique optimal explainer within
    this framework that satisfies three desirable properties.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although interpretability often comes in a variety of forms, they are all designed
    with the end goal of being able to explain model behavior. We learned that not
    every model is interpretable by construction, and even those that are might only
    be superficially so. For example, although vanilla linear regression seems to
    be quite interpretable by design, correlations between features can muddle this
    initially clear picture. Additionally, we learned about interpretability methods
    that are built into the model itself, such as extractive rationalization, and
    post hoc interpretability methods such as LIME and SHAP. The right form of interpretability
    will often depend on the domain—for example, using gradient-based methods for
    image classification may make sense, but not so much in language problems. The
    soft attention scheme discussed in previous chapters may not be as desirable for
    sentiment analysis as, say, the hard selection methodology presented in our section
    on extractive rationalization. And finally, we learned about how interpretability
    does not carry the exact same meaning across the board, even in research—note
    our discussion on the differences between the optimal explainers generated by
    LIME and SHAP. We hope that this chapter served as a fruitful foray into the vast
    landscape of interpretability research.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11.xhtml#idm45934166216704-marker)) Lei et al. “Rationalizing Neural
    Predictions.” *arXiv Preprint arXiv*:1606.04155\. 2016.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch11.xhtml#idm45934166151520-marker)) Ribeiro et al. “Why Should I Trust
    You? Explaining the Predictions of Any Classifier.” *arXiv Preprint arXiv*:1602.04938\.
    2016.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch11.xhtml#idm45934166079056-marker)) Lundberg et al. “A Unified Approach
    to Interpreting Model Predictions.” *arXiv Preprint arXiv*:1705.07874\. 2017.
  prefs: []
  type: TYPE_NORMAL
