- en: Chapter 11\. Methods in Interpretability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。可解释性方法
- en: Overview
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: The field of interpretability is broad and can be uniquely applied to a variety
    of tasks. Simply put, interpretability defines a model’s ability to “explain”
    its decision making to a third party. There are many modern architectures that
    do not have this capability just by construction. A neural network, for example,
    is a prime example of one of these modern architectures. The term “opaque” is
    often used to describe neural networks, both in media and in literature. This
    is because, without post hoc techniques to explain the final classification or
    regression result of a neural network, the data transformations occurring within
    the trained model are unclear and difficult for the end user to interpret. All
    we know is that we fed in an example and out popped a result. Although we can
    examine the learned weights of a neural network, the composition of all of these
    weights is an extremely complex function. This makes it difficult to tell what
    part of the input ended up contributing the most to the final result.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性领域广泛，可以独特地应用于各种任务。简而言之，可解释性定义了模型向第三方“解释”其决策的能力。许多现代架构并没有通过构建具有这种能力。例如，神经网络就是这类现代架构的典型例子。术语“不透明”经常用来描述神经网络，无论是在媒体还是文献中。这是因为，没有事后技术来解释神经网络的最终分类或回归结果，训练模型内部发生的数据转换对最终用户来说是不清楚且难以解释的。我们只知道我们输入了一个示例，然后输出了一个结果。虽然我们可以检查神经网络学习到的权重，但所有这些权重的组合是一个极其复杂的函数。这使得很难确定输入的哪个部分最终对最终结果产生了最大的贡献。
- en: A variety of post hoc methodologies have been designed to explain the output
    of a neural network—*saliency mapping* is a prime example. Saliency mapping measures
    the gradient of the output of a trained model with respect to the input. By the
    definition of the gradient, the input positions with the highest magnitude gradients
    would affect the output value, or class in the case of classification, the most
    when their values are changed slightly. Saliency mapping thus interprets the set
    of positions (and their respective values) with the highest magnitude gradients
    as the part of the input that contributes the most to the final result.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 已经设计了各种事后方法来解释神经网络的输出——*显著性映射*就是一个典型例子。显著性映射测量了经过训练模型的输出相对于输入的梯度。根据梯度的定义，具有最大幅度梯度的输入位置在其值稍微改变时会对输出值或分类情况产生最大影响。因此，显著性映射将解释具有最大幅度梯度的位置（及其相应的值）作为对最终结果贡献最大的输入部分。
- en: However, this is not the be-all and end-all of interpretability. One issue with
    saliency mapping is that it can be a bit noisy, especially when we consider the
    gradient at the individual pixel level for tasks like image classification. Additionally,
    if the input is categorical in nature rather than continuous, e.g., one-hot encodings
    for sentences, the gradient with respect to the input isn’t interpretable in itself
    since the input space is discontinuous.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是可解释性的全部和终结。显著性映射的一个问题是，它可能有点嘈杂，特别是当我们考虑像图像分类这样的任务时在单个像素级别的梯度。此外，如果输入的性质是分类的而不是连续的，例如句子的独热编码，那么相对于输入的梯度本身是不可解释的，因为输入空间是不连续的。
- en: Further, as mentioned earlier, the task at hand is often key for determining
    what makes sense as a valid method of interpretability. We will expound on this
    more in the sections to come.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如前面提到的，手头的任务通常是决定什么是有效的可解释性方法的关键。我们将在接下来的部分中更详细地阐述这一点。
- en: Oftentimes, interpretability comes at the expense of performance. Building interpretability
    into a model often adds some bias (the bias in bias-variance trade-off) by making
    simplifying model assumptions, e.g., in vanilla linear regression, where we assume
    a linear relationship between the features and the target variable. These simplifying
    assumptions, however, are what make the relationship between the input features
    and the target variable much clearer in a vanilla linear regression as opposed
    to a complex, neural architecture.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，可解释性是以性能为代价的。将可解释性构建到模型中通常会通过做出简化模型假设（偏差-方差权衡中的偏差）而增加一些偏差，例如在普通线性回归中，我们假设特征和目标变量之间存在线性关系。然而，这些简化假设正是使得普通线性回归中输入特征和目标变量之间的关系比复杂的神经架构更清晰的原因。
- en: 'This all begs the question: why do we care about interpretability in the first
    place? In a world that is becoming increasingly dominated by technology, complex
    algorithms, and machine learning, the ability to explain decision making is imperative.
    Especially in fields such as medicine, where patient’s lives are on the line,
    or in finance, where peoples’ financial livelihoods are at stake, the ability
    to explain a model’s decision making is a key step toward widespread adoption.
    In the next section, we will cover some classical models that have strong notions
    of interpretability built into their design.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都引出了一个问题：为什么我们首先关心可解释性？在一个越来越被技术、复杂算法和机器学习主导的世界中，解释决策的能力是至关重要的。特别是在医学等领域，患者的生命受到威胁，或者在金融领域，人们的财务生计岌岌可危时，解释模型的决策是向广泛采用迈出的关键一步。在接下来的部分中，我们将介绍一些经典模型，这些模型在设计中具有强烈的可解释性概念。
- en: Decision Trees and Tree-Based Algorithms
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和基于树的算法
- en: Most classical data science and machine learning methodologies have some built-in
    form of interpretability. Tree-based algorithms are a clear example of this. Decision
    trees are designed to classify an input based on a series of conditional statements,
    where each node in the tree is associated with a conditional statement. To understand
    how a trained tree-based model is making a decision, all we must do for any given
    input is follow the correct branch at each node in the tree ([Figure 11-1](#fig1101)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1101.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. A decision tree trained to classify bird species. Given a set
    of bird features, follow the right “Yes” or “No” branch at each node to reach
    a final classification.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: More complex tree-based algorithms, such as the random forest algorithm, which
    is composed of an ensemble of large decision trees, are also interpretable. For
    example, in the case of classification, random forest algorithms function by running
    a given input through each decision tree and then taking the majority output class
    amongst the decision trees as the final output (or an average in the case of regression).
    By the algorithm’s construction, we know exactly how random forest came to a final
    conclusion regarding the input.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: In addition to interpretability at the individual example level, decision trees
    and their more complex ensembles have built-in metrics for feature importance
    at the global level. For example, when a decision tree is being trained, it must
    determine which feature to split on and the threshold(s) of that feature at which
    to split. In the classification regime, one methodology to do this is to calculate
    the information gain by splitting on a proposed feature at a proposed threshold.
    To frame our thinking, let’s think of the possible training labels as the domain
    of a discrete probability distribution, where the probability of each label is
    the frequency with which that label appears in the training dataset ([Figure 11-2](#fig1102)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1102.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Label probabilities.
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thinking back to [Chapter 2](ch02.xhtml#fundamentals-of-proba), a metric that
    summarizes the uncertainty within a probability distribution is the entropy of
    the distribution. When given a proposed feature and associated threshold(s) to
    split on, we can split the training data population into at least two separate
    groups based on which branch we would follow for each input example. Each subgroup
    now has its own distribution over the possible labels, and we take the difference
    between the training dataset’s entropy and the weighted sum of each subgroup’s
    entropy to calculate the information gain, where the weight is proportional to
    the number of elements in each subgroup. The feature and associated threshold(s)
    with the highest information gain at each branching point are the optimal split.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Why does this work? Although we won’t do a rigorous proof here, consider the
    problem where we have a molecular dataset with a binary label, for example, indicating
    whether each compound is toxic or not, and we’d like to build a classifier to
    predict compound toxicity. Also assume that one of the features associated with
    each compound is a binary feature of whether the molecule contains a phenol functional
    group or not. The phenol functional group is both quite toxic and is a common
    cause of toxicity in compounds, so splitting on this feature would lead to two
    well-separated subgroups.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The positive subgroup, which contains compounds with phenol functional groups,
    is likely to have few false positives due to the phenol’s level of toxicity. The
    negative subgroup, which contains compounds without phenol functional groups,
    is likely to have few false negatives due to phenol being a common cause of toxicity.
    Thus, each subgroup’s associated entropy is quite low since the true label distribution
    over compounds in each subgroup is quite concentrated over a single label. The
    sum of their weighted entropies removed from the entire dataset’s associated entropy
    demonstrates a significant information gain ([Figure 11-3](#fig1103)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 包含酚官能团的正子组可能由于酚的毒性水平而几乎没有假阳性。不包含酚官能团的负子组可能由于酚是毒性的常见原因而几乎没有假阴性。因此，每个子组的相关熵非常低，因为每个子组中化合物的真实标签分布在一个标签上非常集中。从整个数据集的相关熵中减去它们的加权熵之和展示了显著的信息增益（[图11-3](#fig1103)）。
- en: '![](Images/fdl2_1103.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1103.png)'
- en: Figure 11-3\. The original dataset can be broken down into 30% toxic and 70%
    nontoxic, where a true label of 1 indicates toxicity and 0 otherwise. Breaking
    up the n examples into two subgroups based on containing phenol greatly concentrates
    the true probability over a single label in each subgroup.
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-3. 原始数据集可以分为30%有毒和70%无毒，其中真实标签为1表示有毒，否则为0。根据是否含有酚将n个示例分成两个子组，大大集中了每个子组中的真实概率在一个标签上。
- en: This checks out well with our a priori knowledge of the phenol group—due to
    both its widespread nature in toxic compounds and its level of toxicity, we would
    have expected it to be a great feature for toxicity classification. The way we
    select features and their splits in decision trees is actually the same way we
    approach *greedy algorithms* in the more general algorithmic framework. Greedy
    algorithms select the most optimal local action at each decision point, and depending
    on the properties of the problem, the composition of these locally optimal actions
    leads to the global optimum. Decision trees similarly select the feature and split
    that locally lead to the largest gain in some metric at each decision point. For
    example, we just used information gain for toxicity classification, and although
    we showed the result of just one split, assuming splitting on the phenol trait
    leads to the highest information gain, we perform this same greedy procedure at
    each junction of every level in the tree. However, it turns out that the problem
    of finding the globally optimal decision tree for a dataset is NP-complete, which
    for the purposes of this text means that it is computationally very difficult.
    The best we can do to approach the problem in a tractable manner is the greedy
    approach, although it does not provably lead to the global optimum.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们对酚基的先验知识非常吻合——由于其在有毒化合物中的广泛存在和毒性水平，我们本来就期望它是毒性分类的一个重要特征。我们在决策树中选择特征和它们的分裂方式实际上与我们在更一般的算法框架中处理“贪婪算法”的方式相同。贪婪算法在每个决策点选择最优的局部行动，并根据问题的属性，这些局部最优行动的组合导致全局最优解。决策树类似地在每个决策点选择在局部导致某些指标上最大增益的特征和分裂。例如，我们刚刚在毒性分类中使用了信息增益，尽管我们只展示了一个分裂的结果，假设在酚特征上分裂导致最高的信息增益，我们在树的每个级别的每个交叉点执行相同的贪婪过程。然而，事实证明，为数据集找到全局最优决策树的问题是NP完全的，这意味着在这个文本中，这是计算上非常困难的。我们能够以可行的方式接近这个问题的最佳方法是贪婪方法，尽管不能证明它会导致全局最优解。
- en: For each feature we split on in a tree, there exists an associated information
    gain with that feature. The order of importance of each feature is simply a list
    of the features sorted by their information gain. If we have a random forest rather
    than a single decision tree, we average the information gain for each feature
    across all of the trees in the forest and sort using the mean. Note that there
    is no extra work required in calculating the information gain, since we use information
    gain to train the individual decision trees in the first place. Thus, we have
    both example-level interpretability and a global understanding of feature importance
    for free in tree-based algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树中我们分裂的每个特征，都存在与该特征相关的信息增益。每个特征的重要性顺序只是根据它们的信息增益排序的特征列表。如果我们有一个随机森林而不是单个决策树，我们会在整个森林中对每个特征的信息增益进行平均，并使用平均值进行排序。请注意，在计算信息增益时不需要额外的工作，因为我们首先使用信息增益来训练单个决策树。因此，在基于树的算法中，我们既有每个示例级别的可解释性，又有全局特征重要性的理解。
- en: Linear Regression
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'A quick background on linear regression: given a set of features and a target
    variable, our goal is to find the “best” linear combination of features that approximates
    the target variable. Implicit in this model is the assumption that the input features
    are linearly related to the target variable. We define “best” as the set of coefficients
    that results in the linear combination with the lowest root mean squared error
    when compared against the ground truth:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的简要背景：给定一组特征和一个目标变量，我们的目标是找到最佳的特征线性组合，以逼近目标变量。这个模型的隐含假设是输入特征与目标变量呈线性关系。我们将“最佳”定义为导致线性组合与基本事实之间具有最低均方根误差的系数集合：
- en: <math alttext="y equals beta dot x plus epsilon comma epsilon tilde upper N
    left-parenthesis 0 comma sigma squared right-parenthesis"><mrow><mi>y</mi> <mo>=</mo>
    <mi>β</mi> <mo>·</mo> <mi>x</mi> <mo>+</mo> <mi>ϵ</mi> <mo>,</mo> <mi>ϵ</mi> <mo>∼</mo>
    <mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></math>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals beta dot x plus epsilon comma epsilon tilde upper N
    left-parenthesis 0 comma sigma squared right-parenthesis"><mrow><mi>y</mi> <mo>=</mo>
    <mi>β</mi> <mo>·</mo> <mi>x</mi> <mo>+</mo> <mi>ϵ</mi> <mo>,</mo> <mi>ϵ</mi> <mo>∼</mo>
    <mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></math>
- en: Where  <math alttext="beta"><mi>β</mi></math> represents the vector of coefficients.
    Our built-in, global notion of feature importance follows directly from this.
    The features that correspond with the coefficients with the highest magnitude
    are, globally, the most important features in the regression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="beta"><mi>β</mi></math> 代表系数向量。我们内置的全局特征重要性概念直接源自此。与具有最大幅度系数对应的特征在回归中是全局最重要的特征。
- en: How about an example-level notion of feature importance? Recall that to get
    a prediction for a given example, we take the dot product between the example
    and the learned coefficients. Logically, the feature associated with the feature-coefficient
    product that contributes the most, in magnitude, to the final result is the feature
    that is most important for prediction. Without much effort, we have both an example-level
    and global-level notion of interpretability more or less built into linear regression.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，特征重要性的一个例子级别的概念呢？回想一下，为了对给定示例进行预测，我们取示例和学习系数之间的点积。逻辑上，对最终结果产生最大贡献的特征与特征系数乘积相关的特征是最重要的预测特征。毫不费力地，我们在线性回归中几乎内置了一个例子级别和全局级别的可解释性概念。
- en: However, linear regression has some unaddressed issues when considering feature
    importance. For example, when there exist significant correlations between the
    features in a multivariate regression, it is often difficult for the model to
    disentangle the effects of these correlated features on the output. In [“SHAP”](#shap-sect),
    we will describe Shapley values, which were designed to measure the marginal,
    unbiased impact of a given feature on the output in such cases.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当考虑特征重要性时，线性回归存在一些未解决的问题。例如，在多元回归中特征之间存在显著相关性时，模型往往很难分离这些相关特征对输出的影响。在[“SHAP”](#shap-sect)中，我们将描述Shapley值，这些值旨在衡量在这种情况下给定特征对输出的边际、无偏影响。
- en: Methods for Evaluating Feature Importance
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估特征重要性的方法
- en: For models where feature importance isn’t built in, researchers have developed
    a variety of methods over the years to evaluate it. In this section, we will discuss
    a few that are used in the industry, in addition to their benefits and shortcomings.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有内置特征重要性的模型，研究人员多年来已经开发了各种方法来评估它。在本节中，我们将讨论一些在行业中使用的方法，以及它们的优点和缺点。
- en: Permutation Feature Importance
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排列特征重要性
- en: 'The idea behind permutation feature importance is quite simple: assume we have
    a trained neural model *f* and a set of features *U* that *f* has been trained
    on. We’d like to understand the impact that an individual feature *s* has on the
    predictions of *f*. One way to do this is to randomly rearrange the values that
    *s* takes on in the dataset amongst all of the examples and measure the resulting
    decrease in predictive accuracy. If the feature *s* did not add much predictive
    accuracy in the first place, we should see that the predictive accuracy of *f*
    decreases minimally when using the permuted samples. Inversely, if feature *s*
    was predictive of the output in the first place, we should see a large drop in
    predictive accuracy upon permuting the values of *s* in the dataset. In essence,
    if the feature *s* was originally strongly correlated with the true labels, randomizing
    the values of *s* would break this strong correlation and nullify its effectiveness
    at predicting the true label.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 排列特征重要性背后的想法非常简单：假设我们有一个经过训练的神经模型*f*和一组特征*U*，*f*已经在这些特征上进行了训练。我们想要了解单个特征*s*对*f*的预测有什么影响。一种方法是随机重新排列数据集中*s*的取值，并测量预测准确性的降低。如果特征*s*本来就没有增加太多的预测准确性，那么在使用排列样本时，我们应该看到*f*的预测准确性降低得很少。相反，如果特征*s*本来就对输出有预测能力，那么在对数据集中的*s*的值进行排列时，我们应该看到预测准确性大幅下降。实质上，如果特征*s*最初与真实标签强相关，那么随机化*s*的值将破坏这种强相关性，并使其在预测真实标签方面失效。
- en: Unfortunately, as with all interpretability methods, this one is not perfect.
    Imagine the scenario in which our target is ice cream sales in a given region
    and two features in *U* are the readings of two temperature sensors within a one-mile
    radius of each other. We’d expect that each of these features is independently
    quite predictive of ice cream sales due to the seasonality of our target. However,
    if we were to perform the permutation methodology presented previously on this
    dataset, we’d counterintuitively get a low feature importance for both of these
    features. Why is this the case? Although each of these features is strongly predictive
    of the target, they are also strongly correlated due to the close proximity of
    the two temperature sensors. Additionally, permuting only one of these features
    at a time to compute its importance means that the other is kept intact, preserving
    most of the predictive information contained within the two features. Thus, we’d
    see little change in the predictive performance of *f* for both features, leading
    us to believe that the weather is not predictive of ice cream sales.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，与所有可解释性方法一样，这种方法并不完美。想象一下，我们的目标是某个地区的冰淇淋销售额，*U*中的两个特征是彼此之间一英里半径内的两个温度传感器的读数。我们预期这些特征中的每一个独立地对冰淇淋销售额具有相当大的预测能力，因为我们的目标具有季节性。然而，如果我们在这个数据集上执行先前介绍的排列方法，我们会出乎意料地得到这两个特征的低特征重要性。为什么会这样？尽管这些特征中的每一个都对目标有很强的预测能力，但由于两个温度传感器的紧密接近，它们也具有很强的相关性。此外，每次只对其中一个特征进行排列以计算其重要性意味着另一个特征保持不变，保留了这两个特征中包含的大部分预测信息。因此，我们会看到*f*对这两个特征的预测性能几乎没有变化，导致我们认为天气对冰淇淋销售没有预测能力。
- en: The moral of the story here is that we must always keep in mind correlations
    between features in our dataset. It is good data science and machine learning
    practice to understand the relationships between the features themselves before
    running these features through any sort of predictive modeling algorithm, simple
    or complex. One way to do this is to plot each of the z-scored features against
    each other to get a visual idea of feature correlation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的故事教训是，我们必须始终牢记数据集中特征之间的相关性。在将这些特征通过任何形式的预测建模算法（简单或复杂）之前，了解特征之间的关系是良好的数据科学和机器学习实践。一种方法是将每个经过z-score处理的特征相互绘制，以获得特征相关性的视觉概念。
- en: Partial Dependence Plots
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分依赖图
- en: Partial dependence plots, or PDPs for short, measure the marginal impact that
    a subset of features included in the model has on the output. As previously discussed,
    measuring this marginal impact in an unbiased manner is difficult for complex
    neural models. In the case of regression, we can represent the trained neural
    network (or any other complex, uninterpretable model) as a function *f* that takes
    as input a set of features *U* and outputs a value in the reals. Imagine that,
    as a user of this model, you are looking for an interpretability method that can
    measure the marginal impact of any subset of features *S* on the output of *f*.
    That is, if we are given an arbitrary setting of feature set *S*, we would like
    to calculate the expected output of the function *f* conditioned on this setting.
    The expectation of *f* is taken over *U \ S*, the rest of the features in *U*
    (conditioned on the known setting of *S*). Intuitively, we have marginalized out
    the feature subset *U \ S* and have the output of a new function *f’* that takes
    as input only the feature set *S*. If we carry out this process for enough settings
    of *S*, we can learn the patterns of how *f’* changes as the feature set *S* changes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 部分依赖图，简称PDP，衡量了模型中包含的特征子集对输出的边际影响。正如之前讨论的，以无偏的方式测量这种边际影响对于复杂的神经模型来说是困难的。在回归的情况下，我们可以将训练好的神经网络（或任何其他复杂的、不可解释的模型）表示为一个函数*f*，它以特征集*U*作为输入，并在实数中输出一个值。想象一下，作为这个模型的用户，你正在寻找一种可以测量任意特征子集*S*对*f*输出的边际影响的可解释性方法。也就是说，如果我们给定一个任意的特征集*S*的设置，我们希望计算在这个设置条件下函数*f*的期望输出。对*f*的期望是在*U
    \ S*上取的，即*U*中其余特征（在已知*S*的设置条件下）。直观地说，我们已经将特征子集*U \ S*边缘化，并得到了一个新函数*f’*的输出，它只以特征集*S*作为输入。如果我们对足够多的*S*设置进行这个过程，我们就可以学习到*f’*随着特征集*S*变化而变化的模式。
- en: 'For example, assume the output is the number of vehicles on the road in a given
    region and our feature set *S* consists of a single feature: precipitation levels
    in that region. The features that make up *U \ S* could be variables like time
    of day, geographical location, population density, etc. By running the above process
    for a range of precipitation levels, we can estimate the number of vehicles we’d
    expect to see on the road at each level and observe the trend as precipitation
    levels get higher or lower. Plotting this trend is what gives us a PDP.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设输出是某个地区道路上的车辆数量，我们的特征集*S*包括一个单一特征：该地区的降水量。构成*U \ S*的特征可能是诸如时间、地理位置、人口密度等变量。通过对一系列降水量运行上述过程，我们可以估计在每个水平上预期看到的道路上的车辆数量，并观察随着降水量的升高或降低而出现的趋势。绘制这种趋势就是PDP的作用。
- en: 'A couple of important notes: the first is that we do not plan on actually learning
    *f’*, but rather estimating it using our trained model *f*. Learning *f’* itself
    would require retraining for every potential subset *S* to be explained, which
    is exponential in the number of features and thus intractable. The second is that
    it is currently unclear how we would go about computing the expectation of *f*
    taken over *U \ S*. As we will soon see, the PDP methodology addresses this second
    point. Before diving into the weeds, here is a simple yet concrete mathematical
    formulation of the process we have just described:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一些重要的注意事项：首先，我们并不打算实际学习*f’*，而是使用我们训练好的模型*f*来估计它。学习*f’*本身需要为每个要解释的潜在子集*S*重新训练，这在特征数量方面是指数级的，因此难以处理。其次，目前尚不清楚我们将如何计算在*U
    \ S*上取的*f*的期望。正如我们将很快看到的，PDP方法解决了这第二点。在深入讨论之前，这里是我们刚刚描述的过程的一个简单而具体的数学公式：
- en: "<math alttext=\"f prime left-parenthesis upper S right-parenthesis equals double-struck\
    \ upper E Subscript upper U minus upper S vertical-bar upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\"\
    ><mrow><mi>f</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mrow><mo>(</mo> <mi>S</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C</mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi><mo>|</mo><mi>S</mi></mrow></msub>\
    \ <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi>\
    \ <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>"
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"f prime left-parenthesis upper S right-parenthesis equals double-struck\
    \ upper E Subscript upper U minus upper S vertical-bar upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\"\
    ><mrow><mi>f</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mrow><mo>(</mo> <mi>S</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C</mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi><mo>|</mo><mi>S</mi></mrow></msub>\
    \ <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi>\
    \ <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>"
- en: 'As stated, the conditional expectation is a bit tricky to estimate. So far
    in the text, we have approximated expectations in an unbiased manner via empirical
    averages. To estimate a conditional expectation, however, we are further constrained
    by the fact that we must take only the average over samples that contain the exact
    setting of *S* in question. Unfortunately, the only samples we have from the underlying
    distribution over *U* are contained within the dataset we are provided with. And
    in the common case that the features of *U* are continuous, the likelihood that
    we see the exact setting of *S* in question even once in the dataset is exceedingly
    low. To get around this, PDP makes an independence assumption that allows us to
    use the entire dataset directly to estimate this expectation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，条件期望有点难以估计。到目前为止，在文本中，我们通过经验平均值以无偏的方式近似期望。然而，为了估计条件期望，我们受到了一个进一步的限制，即我们必须仅对包含所讨论的*S*的确切设置的样本取平均值。不幸的是，我们从*U*的基础分布中获得的唯一样本都包含在我们提供的数据集中。在*U*的特征通常是连续的情况下，我们在数据集中看到所讨论的*S*的确切设置的可能性甚至一次都很低。为了解决这个问题，PDP做出了一个独立性假设，允许我们直接使用整个数据集来估计这个期望。
- en: "<math alttext=\"f prime left-parenthesis upper S right-parenthesis equals double-struck\
    \ upper E Subscript upper U minus upper S vertical-bar upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\
    \ equals double-struck upper E Subscript upper U minus upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\
    \ almost-equals StartFraction 1 Over n EndFraction sigma-summation Underscript\
    \ i equals 1 Overscript n Endscripts f left-parenthesis left-parenthesis upper\
    \ U minus upper S right-parenthesis Superscript i Baseline comma upper S right-parenthesis\"\
    ><mrow><mi>f</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mrow><mo>(</mo> <mi>S</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C</mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi><mo>|</mo><mi>S</mi></mrow></msub>\
    \ <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi>\
    \ <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C\
    </mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi></mrow></msub> <mrow><mo>[</mo> <mi>f</mi>\
    \ <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi> <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow>\
    \ <mo>]</mo></mrow> <mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo>\
    \ <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mi>f</mi>\
    \ <mrow><mo>(</mo> <msup><mrow><mo>(</mo><mi>U</mi><mo>∖</mo><mi>S</mi><mo>)</mo></mrow>\
    \ <mi>i</mi></msup> <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>"
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"f prime left-parenthesis upper S right-parenthesis equals double-struck\
    \ upper E Subscript upper U minus upper S vertical-bar upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\
    \ equals double-struck upper E Subscript upper U minus upper S Baseline left-bracket\
    \ f left-parenthesis upper U minus upper S comma upper S right-parenthesis right-bracket\
    \ almost-equals StartFraction 1 Over n EndFraction sigma-summation Underscript\
    \ i equals 1 Overscript n Endscripts f left-parenthesis left-parenthesis upper\
    \ U minus upper S right-parenthesis Superscript i Baseline comma upper S right-parenthesis\"\
    ><mrow><mi>f</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mrow><mo>(</mo> <mi>S</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C</mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi><mo>|</mo><mi>S</mi></mrow></msub>\
    \ <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi>\
    \ <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>=</mo> <msub><mi>\U0001D53C\
    </mi> <mrow><mi>U</mi><mo>∖</mo><mi>S</mi></mrow></msub> <mrow><mo>[</mo> <mi>f</mi>\
    \ <mrow><mo>(</mo> <mi>U</mi> <mo>∖</mo> <mi>S</mi> <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow>\
    \ <mo>]</mo></mrow> <mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo>\
    \ <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mi>f</mi>\
    \ <mrow><mo>(</mo> <msup><mrow><mo>(</mo><mi>U</mi><mo>∖</mo><mi>S</mi><mo>)</mo></mrow>\
    \ <mi>i</mi></msup> <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>"
- en: Where *n* is the number of samples in the dataset. PDP assumes that the features
    in *S* are independent from the features in *U \ S*. This assumption allows us
    to use all of the training samples indiscriminately for computing the expectation
    since, under this assumption, the sampling of *U \ S* is independent from the
    setting of *S* anyway. We now have a concrete method for estimating the marginal
    impact of any arbitrary subset of features *S* on the output of *f*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中有*n*个样本。PDP假设*S*中的特征与*U \ S*中的特征是独立的。这个假设使我们能够无差别地使用所有的训练样本来计算期望，因为在这个假设下，*U
    \ S*的抽样与*S*的设置是独立的。我们现在有了一个具体的方法来估计任意特征子集*S*对*f*输出的边际影响。
- en: If there are significant correlations between the features in *S* and those
    in *U \ S*, then our generated PDP is likely not reflective of the true marginal
    effect of *S* on the output due to bias in our sampling assumption. Essentially,
    we would be taking the average over many samples that are very unlikely to occur,
    which means that we (1) can’t expect *f* to generate meaningful outputs on these
    samples, and (2) are taking the average over the outputs for samples that do not
    accurately reflect the relationships in the underlying distribution over *U*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*S*中的特征与*U \ S*中的特征之间存在显著相关性，那么我们生成的PDP可能不反映*S*对输出的真实边际效应，因为我们的抽样假设存在偏差。基本上，我们会对许多极不可能发生的样本进行平均，这意味着我们（1）不能期望*f*在这些样本上生成有意义的输出，以及（2）对不准确反映*U*上的基础分布关系的样本输出进行平均。
- en: The second concern is likely pretty clear, but to illustrate the first, imagine
    that you have trained a neural network to completion on the MNIST dataset. Now,
    I find an image of a dog online and run this image through the network. By chance,
    it turns out that the network returns a 9 with high confidence—should we trust
    these results? Since the input image is completely out of the distribution of
    images that the model expects to see, we can’t trust the model to generalize to
    this extent. Although our situation with PDP is a bit less extreme, it is analogous—we
    have essentially created these unlikely, out-of-distribution “franken-samples”
    and are expecting *f* to produce meaningful outputs on these samples. The independence
    assumption that PDP makes is an inherent limitation of the method, again since
    the only samples we have are those from the dataset. Additionally, PDPs are often
    used to analyze the impact of small subsets of features ( <math alttext="2"><mrow><mo>≤</mo>
    <mn>2</mn></mrow></math> ), since humans can interpret only up to three dimensions
    visually. Regardless, PDPs can be an effective method for visualizing trends between
    subsets of input features and the output of a complex model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题可能很明显，但为了说明第一个问题，想象一下你已经在MNIST数据集上完全训练了一个神经网络。现在，我在网上找到一张狗的图片并将这张图片通过网络运行。碰巧，网络以高置信度返回一个9——我们应该相信这些结果吗？由于输入图像完全超出了模型期望看到的图像分布范围，我们不能相信模型能够推广到这种程度。尽管我们与PDP的情况稍微不同，但是类似——我们基本上创建了这些不太可能的、超出分布范围的“弗兰肯样本”，并期望*f*在这些样本上产生有意义的输出。PDP做出的独立假设是该方法的固有限制，因为我们唯一拥有的样本是数据集中的样本。此外，PDP经常用于分析小特征子集（<math
    alttext="2"><mrow><mo>≤</mo> <mn>2</mn></mrow></math>），因为人类只能直观地解释最多三个维度。不过，PDP可以是一种有效的方法，用于可视化输入特征子集与复杂模型输出之间的趋势。
- en: Extractive Rationalization
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽取性理性化
- en: 'Extractive rationalization, or the selection of concise portions of the input
    that retain most, or all, of the relevant information necessary to predict a property,
    is a built-in form of interpretability at the example level. In this section,
    we will review the methodology of the paper “Rationalizing Neural Predictions,”^([1](ch11.xhtml#idm45934166216704))
    which attempts to do this in the natural language space. In this paper, the task
    at hand is property prediction: given a textual review, predict some properties
    regarding the text. The paper specifically worked with a beer review dataset,
    where each review consisted of some text along with an appearance score, a smell
    score, and a palate score.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 抽取性理性化，或者选择保留大部分或全部相关信息以预测属性所需的输入的简洁部分，是一种内置的可解释性形式在示例级别。在本节中，我们将回顾论文“理性化神经预测”的方法，试图在自然语言空间中实现这一目标。在这篇论文中，任务是属性预测：给定一个文本评论，预测文本的一些属性。该论文专门使用了一个啤酒评论数据集，其中每个评论包括一些文本以及外观评分、气味评分和口感评分。
- en: The high-performing but uninterpretable method would be to train a classic property
    predictor using a recurrent architecture, followed by a vanilla regression neural
    network that takes as input the final embedding produced by the recurrent architecture,
    as shown in [Figure 11-4](#fig1104).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 高性能但不可解释的方法是使用循环架构训练经典属性预测器，然后使用香草回归神经网络，该神经网络以循环架构生成的最终嵌入作为输入，如[图11-4](#fig1104)所示。
- en: '![](Images/fdl2_1104.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1104.png)'
- en: Figure 11-4\. Depicted is the classical property predictor, where x is an encoding
    of the original sentence, h(x) is the hidden state produced by the recurrent architecture
    after reaching the end of x, and y is the result of a standard feed-forward neural
    architecture.
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4。描绘了经典属性预测器，其中x是原始句子的编码，h(x)是在到达x末尾后由循环架构产生的隐藏状态，y是标准前馈神经架构的结果。
- en: The goal of this paper is to additionally generate rationales, or selected,
    concise portions of the input text that are most relevant to the property being
    predicted, while limiting the hit to performance. This is why this method of rationalization
    is referred to as “extractive”—it works by extracting relevant portions of the
    input. You might be wondering why there is an emphasis on conciseness. If there
    were no limit or penalty on the conciseness of the rationale produced by the model,
    there would be no reason for the model to just return the entire input, which
    is a trivial solution. Of course, all of the information necessary for predicting
    the output is within the rationale if the rationale is the entire input.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是额外生成理由，或者选择性的，输入文本中与被预测属性最相关的简洁部分，同时限制对性能的影响。这就是为什么这种理性化方法被称为“抽取性”——它通过提取输入的相关部分来工作。你可能会想为什么要强调简洁性。如果模型生成的理由没有简洁性的限制或惩罚，那么模型就没有理由只返回整个输入，这是一个微不足道的解决方案。当然，如果理由是整个输入，那么预测输出所需的所有信息都在理由中。
- en: How do we modify the structure of the proposed property predictor to also produce
    rationales as a built-in mechanism? This paper proposed a two-network approach,
    where the first network is termed the generator and the second network is termed
    the encoder. The generator is an RNN responsible for selecting the rationale,
    while the encoder is an RNN responsible for predicting the property given solely
    the rationale, not the entire input. The logic behind this is that, given the
    right objective function, the generator will have to learn to select meaningful
    portions of the input text to be able to accurately predict the ground truth rating.
    The generator parameterizes a distribution over all possible binary masks that
    can be applied to the input, where a 1 indicates that the word should be included
    in the rationale, and a 0 indicates otherwise. [Figure 11-5](#fig1105) shows the
    proposed two-step architecture, where the encoder is just the single-step property
    predictor diagrammed earlier, and z represents a binary mask sampled from the
    generator.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如何修改所提出的属性预测器的结构，以便也能产生原因作为内置机制？本文提出了一种双网络方法，第一个网络被称为生成器，第二个网络被称为编码器。生成器是一个负责选择原因的RNN，而编码器是一个负责仅根据原因而不是整个输入来预测属性的RNN。这背后的逻辑是，给定正确的目标函数，生成器将必须学会选择输入文本的有意义部分，以便准确预测真实评分。生成器参数化了一个分布，覆盖了可以应用于输入的所有可能的二进制掩码，其中1表示该词应包含在原因中，0表示否。[图11-5](#fig1105)展示了所提出的两步架构，其中编码器只是之前图中显示的单步属性预测器，z代表从生成器中采样的二进制掩码。
- en: '![](Images/fdl2_1105.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1105.png)'
- en: Figure 11-5\. The generator parametrizes a distribution over masks z given input
    x, which we sample from to get the input to the encoder. The encoder follows the
    same structure as that of the classical property predictor depicted earlier.
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-5. 生成器参数化了给定输入x的掩码z的分布，我们从中采样以获得输入到编码器的输入。编码器遵循之前所示的经典属性预测器的相同结构。
- en: 'More formally, we represent the input text *x* as a vector, where *x[i]* represents
    the token at position *i*. The generator parameterizes the distribution *p(z|x)*,
    where *z* is a vector consisting of individual Bernoulli random variables *z[i]*,
    which each take on the value 1 if *x[i]* is to be included in the rationale, and
    0 otherwise. Note that *z* is the same length as *x*, which changes depending
    on *x*. How exactly do we represent this distribution? A first step is to make
    a reasonable conditional independence assumption, which is that all *z[i]* are
    mutually independent of each other conditioned on *x*: <math alttext="p left-parenthesis
    z vertical-bar x right-parenthesis equals product Underscript i equals 1 Overscript
    n Endscripts p left-parenthesis z Subscript i Baseline vertical-bar z 1 comma
    ellipsis comma z Subscript i minus 1 Baseline comma x right-parenthesis equals
    product Underscript i equals 1 Overscript n Endscripts p left-parenthesis z Subscript
    i Baseline vertical-bar x right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>z</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>z</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> . This is a very reasonable
    assumption because all of the information regarding whether *x[i]* should be in
    the rationale or not should be contained within *x* itself (the token *x[i]* and
    its surrounding context). Converting this to neural net speak, we can implement
    this by applying a fully connected layer followed by a sigmoid activation to each
    final hidden state *h[i]* of the generator independently to get the probability
    of *z[i]* taking on value 1, as we will see soon.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地，我们将输入文本*x*表示为一个向量，其中*x[i]*表示位置*i*处的标记。生成器参数化了分布*p(z|x)*，其中*z*是由各个伯努利随机变量*z[i]*组成的向量，如果*x[i]*应包含在原因中，则每个*z[i]*取值为1，否则为0。注意*z*的长度与*x*相同，这取决于*x*。我们如何表示这个分布呢？第一步是做一个合理的条件独立假设，即所有*z[i]*在给定*x*的条件下互相独立：这是一个非常合理的假设，因为关于*x[i]*是否应该包含在原因中的所有信息应该包含在*x*本身中（标记*x[i]*及其周围的上下文）。将这转换为神经网络术语，我们可以通过将完全连接的层应用于生成器的每个最终隐藏状态*h[i]*，然后跟随一个sigmoid激活，独立地得到*z[i]*取值为1的概率，我们很快将看到。
- en: 'Before going into the specifics of the objective function, we’ll describe the
    architectures of the generator and the encoder in more detail. The generator and
    encoder are both recurrent architectures, where the recurrent unit could be an
    LSTM or a GRU. As stated in the previous paragraph, the generator produces a hidden
    unit *h[i]* for each token *x[i]*. The final embedding for a token consists of
    two intermediate embeddings: the first intermediate embedding is the result of
    a forward pass through the tokens, while the second intermediate embedding is
    the result of a backward pass through the tokens. More formally, we have:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入目标函数的具体细节之前，我们将更详细地描述生成器和编码器的架构。生成器和编码器都是循环架构，其中循环单元可以是LSTM或GRU。正如前一段所述，生成器为每个标记*x[i]*生成一个隐藏单元*h[i]*。标记的最终嵌入由两个中间嵌入组成：第一个中间嵌入是通过标记的前向传递的结果，而第二个中间嵌入是通过标记的后向传递的结果。更正式地说，我们有：
- en: <math alttext="StartLayout 1st Row  ModifyingAbove h With right-arrow Subscript
    i Baseline equals ModifyingAbove f With right-arrow left-parenthesis ModifyingAbove
    h With right-arrow Subscript i minus 1 Baseline comma x Subscript i Baseline right-parenthesis
    2nd Row  ModifyingAbove h With left-arrow Subscript i Baseline equals ModifyingAbove
    f With left-arrow left-parenthesis ModifyingAbove h With left-arrow Subscript
    i plus 1 Baseline comma x Subscript i Baseline right-parenthesis 3rd Row  h Subscript
    i Baseline equals concat left-parenthesis ModifyingAbove h With right-arrow Subscript
    i Baseline comma ModifyingAbove h With left-arrow Subscript i Baseline right-parenthesis
    EndLayout"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mover
    accent="true"><mi>h</mi> <mo>→</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mover
    accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover>
    <mi>i</mi></msub> <mo>=</mo> <mover accent="true"><mi>f</mi> <mo>←</mo></mover>
    <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msub><mi>h</mi> <mi>i</mi></msub> <mo>=</mo>
    <mtext>concat</mtext> <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi> <mo>→</mo></mover>
    <mi>i</mi></msub> <mo>,</mo> <msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  ModifyingAbove h With right-arrow Subscript
    i Baseline equals ModifyingAbove f With right-arrow left-parenthesis ModifyingAbove
    h With right-arrow Subscript i minus 1 Baseline comma x Subscript i Baseline right-parenthesis
    2nd Row  ModifyingAbove h With left-arrow Subscript i Baseline equals ModifyingAbove
    f With left-arrow left-parenthesis ModifyingAbove h With left-arrow Subscript
    i plus 1 Baseline comma x Subscript i Baseline right-parenthesis 3rd Row  h Subscript
    i Baseline equals concat left-parenthesis ModifyingAbove h With right-arrow Subscript
    i Baseline comma ModifyingAbove h With left-arrow Subscript i Baseline right-parenthesis
    EndLayout"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mover
    accent="true"><mi>h</mi> <mo>→</mo></mover> <mi>i</mi></msub> <mo>=</mo> <mover
    accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover>
    <mi>i</mi></msub> <mo>=</mo> <mover accent="true"><mi>f</mi> <mo>←</mo></mover>
    <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msub><mi>h</mi> <mi>i</mi></msub> <mo>=</mo>
    <mtext>concat</mtext> <mrow><mo>(</mo> <msub><mover accent="true"><mi>h</mi> <mo>→</mo></mover>
    <mi>i</mi></msub> <mo>,</mo> <msub><mover accent="true"><mi>h</mi> <mo>←</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: 'Where <math alttext="ModifyingAbove f With right-arrow"><mover accent="true"><mi>f</mi>
    <mo>→</mo></mover></math> and <math alttext="ModifyingAbove f With left-arrow"><mover
    accent="true"><mi>f</mi> <mo>←</mo></mover></math> correspond to two independent
    recurrent units, the former trained on the forward pass and the latter trained
    on the backward pass. From this formulation, we can see that the final embedding
    is bidirectional, incorporating information from the entire context of a token
    rather than information in solely one direction. The paper then applies a single,
    fully connected layer and sigmoid to each embedding to generate an independent
    Bernoulli random variable for each token:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="ModifyingAbove f With right-arrow"><mover accent="true"><mi>f</mi>
    <mo>→</mo></mover></math>和<math alttext="ModifyingAbove f With left-arrow"><mover
    accent="true"><mi>f</mi> <mo>←</mo></mover></math>对应于两个独立的循环单元，前者在前向传递上训练，后者在后向传递上训练。从这个公式中，我们可以看到最终嵌入是双向的，包含来自标记整个上下文的信息，而不仅仅是单向的信息。然后，本文对每个嵌入应用单个全连接层和Sigmoid函数，以生成每个标记的独立伯努利随机变量：
- en: <math alttext="p left-parenthesis z Subscript i Baseline vertical-bar x right-parenthesis
    equals sigma left-parenthesis w Subscript z Baseline dot h Subscript i Baseline
    plus b Subscript z Baseline right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>z</mi></msub> <mo>·</mo> <msub><mi>h</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>z</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p left-parenthesis z Subscript i Baseline vertical-bar x right-parenthesis
    equals sigma left-parenthesis w Subscript z Baseline dot h Subscript i Baseline
    plus b Subscript z Baseline right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>z</mi></msub> <mo>·</mo> <msub><mi>h</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>z</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: The encoder is also a recurrent architecture, but is designed to be a regressive
    architecture due to its purpose of predicting the rating associated with the text.
    For this reason, the encoder can be designed the same way we design the vanilla
    property predictor alluded to earlier in the section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器也是一个循环架构，但由于其预测与文本相关联的评分的目的，它被设计为一个回归架构。因此，编码器可以像我们在前面部分提到的普通属性预测器一样设计。
- en: 'So, what is the right objective function for training the two networks in tandem?
    In addition to any constraints we may want to have on the rationales the generator
    produces, we must also ensure that the predictor is accurate. If the predictor
    were not accurate, there would be no reason for the generator to produce meaningful
    rationales. Putting this all together into a mathematical formulation, we have
    the following objective function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于同时训练这两个网络的正确目标函数是什么？除了我们可能希望生成器产生的理由的任何约束条件外，我们还必须确保预测器准确。如果预测器不准确，生成器就没有理由产生有意义的理由。将所有这些放在一起形成数学公式，我们有以下目标函数：
- en: <math alttext="StartLayout 1st Row  theta Superscript asterisk Baseline comma
    phi Superscript asterisk Baseline equals argmin Subscript theta comma phi Baseline
    upper L left-parenthesis theta comma phi right-parenthesis 2nd Row  upper L left-parenthesis
    theta comma phi right-parenthesis equals sigma-summation Underscript left-parenthesis
    x comma y right-parenthesis element-of upper D Endscripts double-struck upper
    E Subscript z tilde g e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis
    Baseline left-bracket cost left-parenthesis x comma y comma z right-parenthesis
    right-bracket 3rd Row  cost left-parenthesis x comma y comma z right-parenthesis
    equals lamda 1 asterisk StartAbsoluteValue z EndAbsoluteValue plus lamda 2 asterisk
    sigma-summation Underscript t Endscripts StartAbsoluteValue z Subscript t Baseline
    minus z Subscript t minus 1 Baseline EndAbsoluteValue plus StartAbsoluteValue
    EndAbsoluteValue e n c Subscript phi Baseline left-parenthesis x comma z right-parenthesis
    minus y StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>,</mo> <msup><mi>φ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext>
    <mrow><mi>θ</mi><mo>,</mo><mi>φ</mi></mrow></msub> <mi>L</mi> <mrow><mo>(</mo>
    <mi>θ</mi> <mo>,</mo> <mi>φ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>,</mo> <mi>φ</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>∈</mo><mi>D</mi></mrow></munder>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <mrow><mo>*</mo> <mo>|</mo> <mi>z</mi>
    <mo>|</mo></mrow> <mo>+</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <mo>*</mo> <munder><mo>∑</mo>
    <mi>t</mi></munder> <mrow><mo>|</mo></mrow> <msub><mi>z</mi> <mi>t</mi></msub>
    <mo>-</mo> <msub><mi>z</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>|</mo> <mo>+</mo> <mo>|</mo> <mo>|</mo> <mi>e</mi> <mi>n</mi></mrow>
    <msub><mi>c</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>-</mo> <mi>y</mi> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></mtd></mtr></mtable></math>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  theta Superscript asterisk Baseline comma
    phi Superscript asterisk Baseline equals argmin Subscript theta comma phi Baseline
    upper L left-parenthesis theta comma phi right-parenthesis 2nd Row  upper L left-parenthesis
    theta comma phi right-parenthesis equals sigma-summation Underscript left-parenthesis
    x comma y right-parenthesis element-of upper D Endscripts double-struck upper
    E Subscript z tilde g e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis
    Baseline left-bracket cost left-parenthesis x comma y comma z right-parenthesis
    right-bracket 3rd Row  cost left-parenthesis x comma y comma z right-parenthesis
    equals lamda 1 asterisk StartAbsoluteValue z EndAbsoluteValue plus lamda 2 asterisk
    sigma-summation Underscript t Endscripts StartAbsoluteValue z Subscript t Baseline
    minus z Subscript t minus 1 Baseline EndAbsoluteValue plus StartAbsoluteValue
    EndAbsoluteValue e n c Subscript phi Baseline left-parenthesis x comma z right-parenthesis
    minus y StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>,</mo> <msup><mi>φ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext>
    <mrow><mi>θ</mi><mo>,</mo><mi>φ</mi></mrow></msub> <mi>L</mi> <mrow><mo>(</mo>
    <mi>θ</mi> <mo>,</mo> <mi>φ</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>,</mo> <mi>φ</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>∈</mo><mi>D</mi></mrow></munder>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <mrow><mo>*</mo> <mo>|</mo> <mi>z</mi>
    <mo>|</mo></mrow> <mo>+</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <mo>*</mo> <munder><mo>∑</mo>
    <mi>t</mi></munder> <mrow><mo>|</mo></mrow> <msub><mi>z</mi> <mi>t</mi></msub>
    <mo>-</mo> <msub><mi>z</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>|</mo> <mo>+</mo> <mo>|</mo> <mo>|</mo> <mi>e</mi> <mi>n</mi></mrow>
    <msub><mi>c</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>-</mo> <mi>y</mi> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></mtd></mtr></mtable></math>
- en: Where  <math alttext="lamda 1"><msub><mi>λ</mi> <mn>1</mn></msub></math> and 
    <math alttext="lamda 2"><msub><mi>λ</mi> <mn>2</mn></msub></math> are hyperparameters
    we can tune during validation. The cost function used in the paper additionally
    contains a continuity penalty, which is higher when the rationale is interspersed
    throughout the text rather than one contiguous block. We want to minimize the
    sum of the expected cost for each training example, where the rationales are drawn
    according to the generator distribution. Calculating the expected cost exactly
    is computationally prohibitive due to the number of configurations of *z* growing
    exponentially with the length of *x*, so we’d instead like to be able to approximate
    the gradient of the expected cost via some empirical, sampled estimate.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中λ1和λ2是我们可以在验证过程中调整的超参数。本文中使用的成本函数还包含一个连续性惩罚，当理由在文本中间隔而不是一个连续的块时，惩罚更高。我们希望最小化每个训练示例的预期成本之和，其中理由根据生成器分布进行抽样。由于*z*的配置数量随着*x*的长度呈指数增长，精确计算预期成本是计算上的障碍，因此我们希望能够通过一些经验抽样估计来近似预期成本的梯度。
- en: 'This is feasible for the gradient of the cost function with respect to the
    parameters of the encoder, but when we try to do this for the generator, we run
    into a similar issue as we did when we first tried optimizing the VAE encoder:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于成本函数相对于编码器参数的梯度是可行的，但是当我们尝试为生成器做同样的事情时，我们遇到了一个类似的问题，就像我们第一次尝试优化VAE编码器时一样：
- en: <math alttext="normal nabla Subscript theta Baseline double-struck upper E Subscript
    z tilde g e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis
    Baseline left-bracket cost left-parenthesis x comma y comma z right-parenthesis
    right-bracket equals sigma-summation Underscript z Endscripts cost left-parenthesis
    x comma y comma z right-parenthesis asterisk normal nabla Subscript theta Baseline
    p Subscript theta Baseline left-parenthesis z vertical-bar x right-parenthesis"><mrow><msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>z</mi></msub>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal nabla Subscript theta Baseline double-struck upper E Subscript
    z tilde g e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis
    Baseline left-bracket cost left-parenthesis x comma y comma z right-parenthesis
    right-bracket equals sigma-summation Underscript z Endscripts cost left-parenthesis
    x comma y comma z right-parenthesis asterisk normal nabla Subscript theta Baseline
    p Subscript theta Baseline left-parenthesis z vertical-bar x right-parenthesis"><mrow><msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>z</mi></msub>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
- en: 'Note that the cost function is only indirectly a function of  <math alttext="theta"><mi>θ</mi></math>
     via sampling from the generator, and thus can be treated as a constant. We can’t
    re-express this as an expectation since the gradient is with respect to the distribution
    from which we are sampling from. This paper uses the log trick, which we also
    introduced in the section on VAEs, to resolve this issue:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，成本函数间接地是通过从生成器中抽样而不是通过θ的函数，因此可以视为常数。我们无法将其重新表达为期望，因为梯度是相对于我们从中抽样的分布。本文使用log技巧来解决这个问题，我们在VAE部分也介绍过这个技巧：
- en: <math alttext="StartLayout 1st Row  sigma-summation Underscript z Endscripts
    cost left-parenthesis x comma y comma z right-parenthesis asterisk normal nabla
    Subscript theta Baseline p Subscript theta Baseline left-parenthesis z vertical-bar
    x right-parenthesis 2nd Row  equals sigma-summation Underscript z Endscripts cost
    left-parenthesis x comma y comma z right-parenthesis asterisk p Subscript theta
    Baseline left-parenthesis z vertical-bar x right-parenthesis asterisk normal nabla
    Subscript theta Baseline log p Subscript theta Baseline left-parenthesis z vertical-bar
    x right-parenthesis 3rd Row  equals double-struck upper E Subscript z tilde g
    e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis Baseline
    left-bracket cost left-parenthesis x comma y comma z right-parenthesis asterisk
    normal nabla Subscript theta Baseline log p Subscript theta Baseline left-parenthesis
    z vertical-bar x right-parenthesis right-bracket EndLayout"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><munder><mo>∑</mo> <mi>z</mi></munder> <mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <munder><mo>∑</mo> <mi>z</mi></munder>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi>
    <mi>θ</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <mo
    form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  sigma-summation Underscript z Endscripts
    cost left-parenthesis x comma y comma z right-parenthesis asterisk normal nabla
    Subscript theta Baseline p Subscript theta Baseline left-parenthesis z vertical-bar
    x right-parenthesis 2nd Row  equals sigma-summation Underscript z Endscripts cost
    left-parenthesis x comma y comma z right-parenthesis asterisk p Subscript theta
    Baseline left-parenthesis z vertical-bar x right-parenthesis asterisk normal nabla
    Subscript theta Baseline log p Subscript theta Baseline left-parenthesis z vertical-bar
    x right-parenthesis 3rd Row  equals double-struck upper E Subscript z tilde g
    e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis Baseline
    left-bracket cost left-parenthesis x comma y comma z right-parenthesis asterisk
    normal nabla Subscript theta Baseline log p Subscript theta Baseline left-parenthesis
    z vertical-bar x right-parenthesis right-bracket EndLayout"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><munder><mo>∑</mo> <mi>z</mi></munder> <mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <munder><mo>∑</mo> <mi>z</mi></munder>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi>
    <mi>θ</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <mo
    form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: 'The gradient of the cost function with respect to the parameters of the encoder
    is just:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数相对于编码器参数的梯度就是：
- en: <math alttext="StartLayout 1st Row  normal nabla Subscript phi Baseline double-struck
    upper E Subscript z tilde g e n Sub Subscript theta Subscript left-parenthesis
    x right-parenthesis Baseline left-bracket cost left-parenthesis x comma y comma
    z right-parenthesis right-bracket 2nd Row  equals sigma-summation Underscript
    z Endscripts p Subscript theta Baseline left-parenthesis z vertical-bar x right-parenthesis
    asterisk normal nabla Subscript phi Baseline cost left-parenthesis x comma y comma
    z right-parenthesis 3rd Row  equals double-struck upper E Subscript z tilde g
    e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis Baseline
    left-bracket normal nabla Subscript phi Baseline cost left-parenthesis x comma
    y comma z right-parenthesis right-bracket EndLayout"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <munder><mo>∑</mo> <mi>z</mi></munder> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi>
    <mi>φ</mi></msub> <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <msub><mi>∇</mi> <mi>φ</mi></msub> <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  normal nabla Subscript phi Baseline double-struck
    upper E Subscript z tilde g e n Sub Subscript theta Subscript left-parenthesis
    x right-parenthesis Baseline left-bracket cost left-parenthesis x comma y comma
    z right-parenthesis right-bracket 2nd Row  equals sigma-summation Underscript
    z Endscripts p Subscript theta Baseline left-parenthesis z vertical-bar x right-parenthesis
    asterisk normal nabla Subscript phi Baseline cost left-parenthesis x comma y comma
    z right-parenthesis 3rd Row  equals double-struck upper E Subscript z tilde g
    e n Sub Subscript theta Subscript left-parenthesis x right-parenthesis Baseline
    left-bracket normal nabla Subscript phi Baseline cost left-parenthesis x comma
    y comma z right-parenthesis right-bracket EndLayout"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>𝔼</mi>
    <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>cost</mtext>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <munder><mo>∑</mo> <mi>z</mi></munder> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <msub><mi>∇</mi>
    <mi>φ</mi></msub> <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>g</mi><mi>e</mi><msub><mi>n</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <msub><mi>∇</mi> <mi>φ</mi></msub> <mtext>cost</mtext> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: Which resembles the standard empirical estimate of the expected gradient when
    performing SGD or minibatch gradient descent.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于在执行SGD或小批量梯度下降时的期望梯度的标准经验估计。
- en: How do we go about training these two networks in tandem? It might be easier
    to consider a single training example for starters. We first select a training
    example at random from the dataset, where a training example consists of a text
    review and an associated rating, and feed the text review to the generator. The
    generator, which now represents a probability distribution over all possible binary
    masks given the input text review, can be sampled from by sampling each *z[i]*
    independently due to our conditional independence claim from earlier. Each sampled
    binary mask represents a possible rationale, which we then feed to the encoder
    for prediction. After obtaining the result of the encoder for each rationale,
    we have all the information we need to calculate the cost function for each rationale.
    This will suffice for updating the weights of the encoder, but to update the weights
    of the generator we will also need to keep track of the log likelihood of the
    rationale, or  <math alttext="log p Subscript theta Baseline left-parenthesis
    z Superscript k Baseline vertical-bar x right-parenthesis"><mrow><mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mi>k</mi></msup>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> for each sampled *z^k*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何同时训练这两个网络？也许从一个单一的训练示例开始会更容易。我们首先从数据集中随机选择一个训练示例，其中一个训练示例包括一个文本评论和一个相关的评分，并将文本评论输入生成器。生成器现在代表了给定输入文本评论的所有可能二进制掩码的概率分布，可以通过独立抽样每个*z[i]*来从中抽样，这是由于我们之前的条件独立声明。每个抽样的二进制掩码代表一个可能的理由，然后我们将其馈送给编码器进行预测。在获得每个理由的编码器结果后，我们就有了计算每个理由的成本函数所需的所有信息。这足以更新编码器的权重，但要更新生成器的权重，我们还需要跟踪理由的对数似然，或者对于每个抽样的*z^k*，即*log
    p Subscript theta Baseline left-parenthesis z Superscript k Baseline vertical-bar
    x right-parenthesis*。
- en: Now that we have a mechanism for training, how do we translate this to validating
    and testing our model? During the validation and testing phases, instead of sampling
    binary masks from the generator, we select the most likely binary mask according
    to the generator probability distribution. To select the most likely binary mask,
    all we need to do is select the most likely *z[i]* for each *x[i]* in our input
    test review *x*, again due to our conditional independence assumption from earlier.
    This is a very reasonable approach to testing, since this is how we would determine
    the intended rationale when using this model in the real world.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个训练的机制，那么如何将其转化为验证和测试我们的模型呢？在验证和测试阶段，我们不再从生成器中对二进制掩码进行抽样，而是根据生成器的概率分布选择最可能的二进制掩码。为了选择最可能的二进制掩码，我们只需要为输入测试评论*x*中的每个*x[i]*选择最可能的*z[i]*，这是由于我们之前的条件独立假设。这是一个非常合理的测试方法，因为这是我们在现实世界中使用该模型时确定预期理由的方式。
- en: You may have noticed some parallels to the concept of attention. After all,
    the generated binary mask can be thought of as a vector of weights we use to multiply
    the feature vectors that make up the input text review, where these weights are
    either 0 or 1, rather than some continuous weighting scheme implemented in standard
    attention. Indeed, the authors of this paper mention that their approach can be
    viewed as a form of “hard” attention, where we completely mask out or input tokens
    of the input according to a probability distribution rather than computing a weighted
    average of the feature vectors in the input. You might be wondering why hard attention
    makes more sense in this case rather than the “soft” attention schemes presented
    in the previous section. In this case, hard attention schemes make more sense
    because fractional weights on words in a sentence are hard to interpret as a measure
    of importance, while selecting a strict subset of words in the text as the explanation
    for a rating is much more interpretable.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到了与注意力概念的一些相似之处。毕竟，生成的二进制掩码可以被视为一组权重向量，我们用这些权重乘以构成输入文本评论的特征向量，其中这些权重要么是0要么是1，而不是标准注意力中实现的一些连续加权方案。事实上，本文的作者提到他们的方法可以被视为一种“硬”注意力形式，其中我们根据概率分布完全屏蔽或输入输入令牌，而不是计算输入中特征向量的加权平均值。您可能会想知道为什么在这种情况下硬注意力比前一节中提出的“软”注意力方案更有意义。在这种情况下，硬注意力方案更有意义，因为句子中的单词上的分数权重很难解释为重要性的度量，而选择文本中的严格子集作为评分解释则更容易解释。
- en: LIME
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 石灰
- en: LIME, or Local Interpretable Model-agnostic Explanations,^([2](ch11.xhtml#idm45934166151520))
    is an interpretability technique that is applied to a trained model rather than
    a built-in feature of the model itself. LIME is a per-example interpretability
    method, meaning that it generates a simple, local explanation of the underlying
    model’s potentially complex behavior. It is also model agnostic, meaning that
    the structure of the underlying model itself does not matter when applying LIME.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 石灰，或称为局部可解释的模型无关解释，是一种可解释性技术，应用于经过训练的模型而不是模型本身的内置特性。石灰是一种逐例解释方法，意味着它生成了对潜在复杂行为的简单、局部解释。它还是模型无关的，这意味着在应用石灰时，基础模型的结构本身并不重要。
- en: Before describing the methodology of LIME, the authors take some time to delineate
    a few characteristics they believe to be necessary components of any explainer.
    The first is that it should be interpretable, meaning that the explainer should
    provide a “qualitative relationship between input variables and response” that
    is easy for the user to understand. Even if the features used in the original
    model are uninterpretable, the explainer must use features that a human can interpret.
    For example, in an application of natural language processing, even if the underlying
    model utilizes a complex word embedding for any given word, the explainer must
    use features that a human can understand, such as the original words themselves.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述LIME的方法论之前，作者花了一些时间来勾勒出他们认为是任何解释器必要组成部分的几个特征。第一个是可解释性，意味着解释器应该提供一个“输入变量和响应之间的定性关系”，这对用户来说应该是容易理解的。即使原始模型中使用的特征是不可解释的，解释器也必须使用人类可以解释的特征。例如，在自然语言处理的应用中，即使基础模型利用复杂的词嵌入来表示任何给定的单词，解释器也必须使用人类可以理解的特征，比如原始单词本身。
- en: The second characteristic is local fidelity, which means that the explainer
    must behave similarly to the underlying model within some vicinity of the chosen
    example. We might ask, why local and not global fidelity? Global fidelity, however,
    as the paper notes, is quite difficult to achieve and would require drastic advances
    in the field—much of the field of interpretability would be solved if global fidelity
    could be achieved. Thus, we settle for local fidelity.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个特征是局部忠实度，这意味着解释器必须在所选示例的某个邻域内与基础模型表现出类似的行为。我们可能会问，为什么是局部而不是全局的忠实度？然而，正如论文所指出的，全局忠实度相当难以实现，需要领域内的重大进展——如果能够实现全局忠实度，那么解释性领域的大部分问题将得到解决。因此，我们选择局部忠实度。
- en: The third is for the explainer to be model agnostic, which, as we explained
    earlier, means that the structure of the underlying model itself should not matter.
    The underlying model can range from a linear regression model to a complex convolutional
    neural architecture, and the explainer should still be able to satisfy the other
    three characteristics. Being model agnostic allows for flexibility in the structure
    of the underlying model, which is desirable as this doesn’t necessitate changes
    in the structure of the explainer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个是解释器对模型不可知，这意味着，正如我们之前解释的那样，基础模型的结构本身不应该重要。基础模型可以是线性回归模型，也可以是复杂的卷积神经网络结构，解释器仍然应该能够满足其他三个特征。解释器对模型不可知允许基础模型结构的灵活性，这是可取的，因为这不需要解释器结构的改变。
- en: The fourth and final characteristic is global perspective, which is to select
    explanations for a subset of examples that is representative of the model’s behavior.
    This helps build user trust in the model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个也是最后一个特征是全局视角，即选择代表模型行为的一部分示例的解释。这有助于建立用户对模型的信任。
- en: "Now we will take some time to develop the methodology of LIME. As stated, the\
    \ features of the original model may not be interpretable to a human (and usually\
    \ aren’t for most complex models), so the features used by the explainer will\
    \ be different from those used by the underlying model. The features used by the\
    \ explainer could be individual words in an NLP task, or functional groups in\
    \ a chemical property prediction task—units, or interpretable components, that\
    \ the end user can understand easily. Thus, any example when converted to the\
    \ feature space of the explainer becomes a binary vector, where each index is\
    \ associated with a distinct interpretable component (such as a functional group).\
    \ A one at any index *i* indicates the presence of the associated interpretable\
    \ component in the original example, and a zero indicates a lack of that component\
    \ in the original example. Following the notation used in the referenced paper,\
    \ we denote  <math alttext=\"x element-of double-struck upper R Superscript d\"\
    ><mrow><mi>x</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mi>d</mi></msup></mrow></math>\
    \ to be the original feature representation of the example to be explained and \
    \ <math alttext=\"x prime element-of StartSet 0 comma 1 EndSet Superscript d prime\"\
    ><mrow><mi>x</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∈</mo> <msup><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow>\
    \ <mrow><mi>d</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi></mrow></msup></mrow></math>\
    \ to be the representation acted upon by the explainer, where *d’* is the number\
    \ of interpretable components."
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: "现在我们将花一些时间来发展LIME的方法论。正如所述，原始模型的特征可能对人类不可解释（对于大多数复杂模型来说通常如此），因此解释器使用的特征将不同于基础模型使用的特征。解释器使用的特征可以是NLP任务中的单词，也可以是化学性质预测任务中的功能组——即，最终用户可以轻松理解的单位或可解释的组件。因此，任何转换为解释器特征空间的示例都将成为一个二进制向量，其中每个索引与一个不同的可解释组件（如功能组）相关联。在任何索引*i*处的1表示原始示例中存在相关的可解释组件，而0表示原始示例中缺少该组件。根据参考文献中使用的符号，我们将表示为要解释的示例的原始特征表示为\
    \ <math alttext=\"x element-of double-struck upper R Superscript d\"><mrow><mi>x</mi>\
    \ <mo>∈</mo> <msup><mi>ℝ</mi> <mi>d</mi></msup></mrow></math>，表示为由解释器处理的表示为 <math\
    \ alttext=\"x prime element-of StartSet 0 comma 1 EndSet Superscript d prime\"\
    ><mrow><mi>x</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∈</mo> <msup><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>}</mo></mrow>\
    \ <mrow><mi>d</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi></mrow></msup></mrow></math>，其中*d’*是可解释组件的数量。"
- en: Further, the paper defines *G* to be a class of potentially interpretable models,
    such as linear regression or random forest, and an explainer to be an instance
    <math alttext="g element-of upper G"><mrow><mi>g</mi> <mo>∈</mo> <mi>G</mi></mrow></math>
    . *g* acts on an instance *x’* and returns a value in the range of the underlying
    model. We denote the underlying model to be *f*, which acts on an instance *x*
    and is a function from  <math alttext="double-struck upper R Superscript d Baseline
    right-arrow double-struck upper R"><mrow><msup><mi>ℝ</mi> <mi>d</mi></msup> <mo>→</mo>
    <mi>ℝ</mi></mrow></math> in the case of regression or a function from <math alttext="double-struck
    upper R Superscript d"><msup><mi>ℝ</mi> <mi>d</mi></msup></math>  to the range
    [0,1] in the case of classification, where *f* returns a probability distribution.
    Additionally, the paper defines a proximity measure, or kernel,  <math alttext="pi
    Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    around the instance *x*. This function can be defined in a multitude of ways—most
    implementations of LIME use an exponential kernel that attains a maximum value
    at *x* and decreases exponentially as one gets farther and farther from *x*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，论文定义*G*为一类潜在可解释模型，如线性回归或随机森林，解释器是一个实例<math alttext="g element-of upper G"><mrow><mi>g</mi>
    <mo>∈</mo> <mi>G</mi></mrow></math>。*g*作用于实例*x'*并返回基础模型范围内的值。我们将基础模型表示为*f*，它作用于实例*x*，是从<math
    alttext="double-struck upper R Superscript d Baseline right-arrow double-struck
    upper R"><mrow><msup><mi>ℝ</mi> <mi>d</mi></msup> <mo>→</mo> <mi>ℝ</mi></mrow></math>到[0,1]范围的函数，其中*f*返回一个概率分布。此外，论文定义了一个接近度度量或核<math
    alttext="pi Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>，围绕实例*x*定义。这个函数可以以多种方式定义——大多数LIME的实现使用一个指数核，在*x*处达到最大值，并随着离*x*越来越远而指数级下降。
- en: 'At a high level, LIME attempts to find the explanation  <math alttext="g Superscript
    asterisk"><msup><mi>g</mi> <mo>*</mo></msup></math> that minimizes a loss function
    that looks like:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，LIME试图找到最小化类似于损失函数的解释<math alttext="g Superscript asterisk"><msup><mi>g</mi>
    <mo>*</mo></msup></math>：
- en: <math alttext="g Superscript asterisk Baseline equals argmin Subscript g element-of
    upper G Baseline upper L left-parenthesis f comma g comma x right-parenthesis
    plus omega left-parenthesis g right-parenthesis"><mrow><msup><mi>g</mi> <mo>*</mo></msup>
    <mo>=</mo> <msub><mtext>argmin</mtext> <mrow><mi>g</mi><mo>∈</mo><mi>G</mi></mrow></msub>
    <mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>g</mi> <mo>,</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>+</mo> <mi>ω</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="g Superscript asterisk Baseline equals argmin Subscript g element-of
    upper G Baseline upper L left-parenthesis f comma g comma x right-parenthesis
    plus omega left-parenthesis g right-parenthesis"><mrow><msup><mi>g</mi> <mo>*</mo></msup>
    <mo>=</mo> <msub><mtext>argmin</mtext> <mrow><mi>g</mi><mo>∈</mo><mi>G</mi></mrow></msub>
    <mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>g</mi> <mo>,</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>+</mo> <mi>ω</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></mrow></math>
- en: Where *L(f,g,x)* is a measure of the unfaithfulness of *g* in modeling *f* around
    the instance in question *x*, and  <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> is a measure of the complexity
    of *g*. Thus, minimizing their sum results in an optimal explainer  <math alttext="g
    Superscript asterisk"><msup><mi>g</mi> <mo>*</mo></msup></math> that has the desired
    characteristics of local fidelity and interpretability described earlier.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*L(f,g,x)*是*g*在问题实例*x*周围建模*f*不忠实的度量，<math alttext="omega left-parenthesis g
    right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>是*g*的复杂度的度量。因此，最小化它们的和会得到一个具有局部忠实度和可解释性特征的最佳解释器<math
    alttext="g Superscript asterisk"><msup><mi>g</mi> <mo>*</mo></msup></math>。'
- en: 'How do we measure the unfaithfulness of a potential explainer? The paper’s
    methodology is to sample an instance *z’* from the vicinity of *x’*, convert *z’*
    back to an example *z* in the original feature space, and compute the difference
    between *f(z)* and *g(z’)*. The difference represents the loss for that sample—if
    *g(z’)* is far from *f(z)*, then it is not faithful to the model’s predictions
    at that point. We can then weight this loss using the kernel <math alttext="pi
    Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , which increasingly discounts the loss as the sample *z* gets further and further
    from the original example *x*. Putting this together, the loss function looks
    like:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何衡量潜在解释器的不忠实度？论文的方法是从*x'*的附近对*z'*进行采样，将*z'*转换回原始特征空间中的示例*z*，并计算*f(z)*和*g(z')*之间的差异。差异代表该样本的损失——如果*g(z')*远离*f(z)*，那么它就不忠实于该点的模型预测。然后，我们可以使用核<math
    alttext="pi Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>对这种损失进行加权，随着样本*z*越来越远离原始示例*x*，这种损失逐渐减少。将这些放在一起，损失函数看起来像：
- en: "<math alttext=\"upper L left-parenthesis f comma g comma x right-parenthesis\
    \ equals sigma-summation Underscript z comma z Superscript prime Baseline Endscripts\
    \ pi Subscript x Baseline left-parenthesis z right-parenthesis asterisk left-parenthesis\
    \ f left-parenthesis z right-parenthesis minus g left-parenthesis z prime right-parenthesis\
    \ right-parenthesis squared\"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo>\
    \ <mi>g</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo>\
    \ <mrow><mi>z</mi><mo>,</mo><mi>z</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi></mrow></msub>\
    \ <msub><mi>π</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>\
    \ <mo>*</mo> <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><mi>z</mi><mi>â</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>"
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"upper L left-parenthesis f comma g comma x right-parenthesis\
    \ equals sigma-summation Underscript z comma z Superscript prime Baseline Endscripts\
    \ pi Subscript x Baseline left-parenthesis z right-parenthesis asterisk left-parenthesis\
    \ f left-parenthesis z right-parenthesis minus g left-parenthesis z prime right-parenthesis\
    \ right-parenthesis squared\"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo>\
    \ <mi>g</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo>\
    \ <mrow><mi>z</mi><mo>,</mo><mi>z</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi></mrow></msub>\
    \ <msub><mi>π</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>\
    \ <mo>*</mo> <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><mi>z</mi><mi>â</mi><mi>\x80\
    </mi><mi>\x99</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>"
- en: How do we achieve the samples *z’* used in this loss function? The paper samples
    from the vicinity of *x’* by selecting a subset of the *x’* nonzero components,
    where each subset is chosen uniformly at random, and setting all other indices
    of the sample to zero ([Figure 11-6](#fig1106)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何获得用于这个损失函数的样本*z'*？论文从*x'*的附近对*z'*进行采样，通过随机选择*x'*的非零分量的子集，并将样本的所有其他索引设置为零来实现这一目的（[图11-6](#fig1106)）。
- en: '![](Images/fdl2_1106.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1106.png)'
- en: Figure 11-6\. x can be thought of as some high-dimensional input such as an
    image, while each index of x’ is associated with some interpretable feature, where
    a 1 denotes the existence of that feature in x. The sampling procedure selects
    some subset of nonzero indices in x’ to keep nonzero in each of w’ and z', which
    are then mapped back to the original input space.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-6。x可以被视为一些高维输入，比如图像，而x'的每个索引与一些可解释特征相关联，其中1表示该特征在x中存在。采样过程选择x'中的一些非零索引的子集，以保持w'和z'中的每个非零索引，然后将它们映射回原始输入空间。
- en: LIME then maps these samples *z’* back to samples *z* from the original feature
    space so we can measure the fidelity of the explainer via *f(z) – g(z’)*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，LIME将这些样本*z'*映射回原始特征空间中的样本*z*，以便我们可以通过*f(z) - g(z')*来衡量解释器的忠实度。
- en: 'LIME also takes into account the complexity of the explainer via <math alttext="omega
    left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    , which enforces the interpretability aspect of viable explainers. In the specific
    case where *G* represents the class of linear models, the paper uses a version
    of <math alttext="omega"><mi>ω</mi></math>  that places a hard limit on the number
    of nonzero weights in *g*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: LIME还考虑了解释器的复杂性，通过<math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>，这强调了可行解释器的可解释性方面。在*G*代表线性模型类的特定情况下，论文使用了一个版本的<math
    alttext="omega"><mi>ω</mi></math>，它对*g*中的非零权重数量设置了一个硬限制：
- en: <math alttext="omega left-parenthesis g right-parenthesis equals normal infinity
    asterisk 1 left-bracket StartAbsoluteValue EndAbsoluteValue w Subscript g Baseline
    StartAbsoluteValue EndAbsoluteValue Subscript 0 Baseline greater-than upper K
    right-bracket"><mrow><mi>ω</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>∞</mi> <mo>*</mo> <mn>1</mn> <mo>[</mo> <mo>|</mo> <mo>|</mo> <msub><mi>w</mi>
    <mi>g</mi></msub> <mo>|</mo> <msub><mo>|</mo> <mn>0</mn></msub> <mo>></mo> <mi>K</mi>
    <mo>]</mo></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="omega left-parenthesis g right-parenthesis equals normal infinity
    asterisk 1 left-bracket StartAbsoluteValue EndAbsoluteValue w Subscript g Baseline
    StartAbsoluteValue EndAbsoluteValue Subscript 0 Baseline greater-than upper K
    right-bracket"><mrow><mi>ω</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>∞</mi> <mo>*</mo> <mn>1</mn> <mo>[</mo> <mo>|</mo> <mo>|</mo> <msub><mi>w</mi>
    <mi>g</mi></msub> <mo>|</mo> <msub><mo>|</mo> <mn>0</mn></msub> <mo>></mo> <mi>K</mi>
    <mo>]</mo></mrow></math>
- en: Where <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> represents *g’*s weight vector,
    the L0 norm counts the number of nonzero elements in <math alttext="omega left-parenthesis
    g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    , and *1[*]* is the indicator function, which evaluates to 1 if the condition
    within the function is satisfied, and 0 otherwise. The result is that  <math alttext="omega
    left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    attains a value of infinity when <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> has more than *K* nonzero elements,
    and is 0 otherwise. This ensures that the chosen <math alttext="omega left-parenthesis
    g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>
    will have at most *K* nonzero elements, since one can always do better than any
    proposed <math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math> that has more than *K* nonzero
    elements by simply zeroing out weights until there are at most *K* nonzero weights.
    This regularization approach is likely different from regularization approaches
    you have encountered in the past, such as the L1 or L2 norm on the weight vector.
    In fact, to optimize the objective function defined in the paper, the authors
    utilize an algorithm they term K-LASSO, which involves first selecting *K* features
    via LASSO and then performing standard least squares optimization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>代表*g*的权重向量，L0范数计算<math alttext="omega
    left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>中的非零元素数量，*1[*]*是指示函数，如果函数内的条件满足则结果为1，否则为0。结果是<math
    alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo>
    <mi>g</mi> <mo>)</mo></mrow></math>在<math alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi>
    <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>中有超过*K*个非零元素时取得无穷大的值，否则为0。这确保了所选的<math
    alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo>
    <mi>g</mi> <mo>)</mo></mrow></math>最多有*K*个非零元素，因为通过将权重归零直到最多有*K*个非零权重，总是可以比任何具有超过*K*个非零元素的提议<math
    alttext="omega left-parenthesis g right-parenthesis"><mrow><mi>ω</mi> <mo>(</mo>
    <mi>g</mi> <mo>)</mo></mrow></math>更好。这种正则化方法可能与您过去遇到的正则化方法不同，比如对权重向量的L1或L2范数。事实上，为了优化论文中定义的目标函数，作者们利用了一个他们称之为K-LASSO的算法，该算法首先通过LASSO选择*K*个特征，然后执行标准的最小二乘优化。
- en: After performing LIME, we are left with an optimal explainer *g*, which is a
    linear model with at most *K* nonzero weights in this case. Now we must check
    if *g* satisfies the goals that the authors set out from the beginning of the
    paper. First, *g* must be interpretable. Since we chose a relatively simple class
    of explainer models *G*, which were linear models in this example, all we need
    to explain the behavior of the model around the chosen example *x* are the values
    of the (at most) *K* nonzero weights of *g*. The interpretable components associated
    with the nonzero weights are considered to be most important for prediction in
    that locality. In terms of local fidelity, our optimization procedure helps to
    ensure local fidelity by minimizing the least squares loss between the explainer’s
    predictions and the model’s predictions. However, there do exist limitations;
    for example, the paper notes that if the underlying model is highly nonlinear
    even within a short vicinity of the example we are explaining, our linear explainer
    won’t be able to do the model’s local behavior justice. With regards to being
    model agnostic, note that methodology of LIME does not concern itself with the
    underlying model’s structure. All LIME needs to function are predictions *f(z)*
    from the underlying model. And finally, to achieve a global perspective, we can
    select examples that are representative of the model’s behavior and display their
    explanations to the user.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 经过LIME的处理，我们得到了一个最优的解释器*g*，在这种情况下，它是一个具有最多*K*个非零权重的线性模型。现在我们必须检查*g*是否满足作者在论文开头设定的目标。首先，*g*必须是可解释的。由于我们选择了一个相对简单的解释器模型*G*，在这个例子中是线性模型，我们只需要解释模型在所选示例*x*周围的行为的（最多）*K*个非零权重的值。与非零权重相关联的可解释组件被认为在该区域的预测中最为重要。在局部忠实度方面，我们的优化过程有助于通过最小化解释器的预测与模型预测之间的最小二乘损失来确保局部忠实度。然而，存在一些限制；例如，论文指出，如果底层模型即使在我们解释的示例的短范围内也是高度非线性的，我们的线性解释器将无法充分展现模型的局部行为。关于模型不可知性，需要注意的是，LIME的方法并不关心底层模型的结构。LIME运行所需的仅仅是来自底层模型的预测*f(z)*。最后，为了获得全局视角，我们可以选择代表模型行为的示例，并向用户展示它们的解释。
- en: SHAP
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SHAP
- en: 'SHAP, or Shapley Additive Explanations,^([3](ch11.xhtml#idm45934166079056))
    are similarly a per-prediction interpretability method for complex models. The
    paper that introduces the methodology of SHAP first provides a framework that
    the authors feel unifies a variety of interpretability methods in the field. This
    framework is termed *additive feature attribution*, where all instances of this
    framework utilize a linear explanation model that acts on binary variables:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP，或Shapley Additive Explanations，是一种针对复杂模型的每个预测的可解释性方法。介绍SHAP方法的论文首先提供了一个框架，作者认为这个框架统一了领域中各种可解释性方法。这个框架被称为*加性特征归因*，其中所有这个框架的实例都使用作用于二进制变量的线性解释模型：
- en: <math alttext="g left-parenthesis x prime right-parenthesis equals phi 0 plus
    sigma-summation Underscript i equals 1 Overscript upper M Endscripts phi Subscript
    i Baseline x prime Subscript i"><mrow><mi>g</mi> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mo>'</mo></msup> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>φ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup>
    <msub><mi>φ</mi> <mi>i</mi></msub> <msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup></mrow></math>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="g left-parenthesis x prime right-parenthesis equals phi 0 plus
    sigma-summation Underscript i equals 1 Overscript upper M Endscripts phi Subscript
    i Baseline x prime Subscript i"><mrow><mi>g</mi> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mo>'</mo></msup> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>φ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup>
    <msub><mi>φ</mi> <mi>i</mi></msub> <msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup></mrow></math>
- en: 'Where *M* is the number of binary variables. For example, LIME, when using
    a class of linear explainer models, follows this framework exactly, since each
    example to be explained is first converted to a binary vector over interpretable
    components. It turns out that, in the additive feature attribution framework,
    there exists a unique solution in this class that has three desirable properties:
    local accuracy, missingness, and consistency. Before discussing the unique solution,
    we will describe the three properties in more detail.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*M*是二进制变量的数量。例如，当使用一类线性解释器模型时，LIME完全遵循这个框架，因为要解释的每个示例首先被转换为一个关于可解释组件的二进制向量。事实证明，在加性特征归因框架中，存在一个具有三个理想属性的唯一解决方案：局部准确性、缺失性和一致性。在讨论这个唯一解决方案之前，我们将更详细地描述这三个属性。
- en: 'The first is *local accuracy*, which states that the explainer model must match
    the underlying model exactly at the example being interpreted. This is understandable
    as a desirable property, since it is reasonable that at least the example being
    interpreted should be explained perfectly. It’s important to note that not all
    interpretability frameworks necessarily follow this property. For example, the
    explainer that is generated by LIME, as presented in its original paper and described
    in the previous section, need not be locally accurate in the way that the authors
    of SHAP define local accuracy. This will be discussed further near the end of
    this section. Mathematically, local accuracy in SHAP is defined as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是*局部准确性*，它指出解释器模型必须在被解释的示例上与基础模型完全匹配。这是一个理想的属性，因为至少应该完美解释被解释的示例。重要的是要注意，并非所有的可解释性框架都必须遵循这个属性。例如，LIME生成的解释器，正如其原始论文中所述并在前一节中描述的那样，不一定在SHAP作者定义的局部准确性方面是准确的。这将在本节末尾进一步讨论。在SHAP中，局部准确性在数学上定义为：
- en: "<math alttext=\"f left-parenthesis x right-parenthesis equals g left-parenthesis\
    \ x prime right-parenthesis equals phi 0 plus sigma-summation Underscript i equals\
    \ 1 Overscript upper M Endscripts phi Subscript i Baseline x Subscript i Superscript\
    \ prime\"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>\
    \ <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>φ</mi> <mn>0</mn></msub> <mo>+</mo> <msubsup><mo>∑</mo>\
    \ <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup> <msub><mi>φ</mi>\
    \ <mi>i</mi></msub> <msubsup><mi>x</mi> <mi>i</mi> <msup><mo>'</mo></msup></msubsup></mrow></math>"
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"f left-parenthesis x right-parenthesis equals g left-parenthesis\
    \ x prime right-parenthesis equals phi 0 plus sigma-summation Underscript i equals\
    \ 1 Overscript upper M Endscripts phi Subscript i Baseline x Subscript i Superscript\
    \ prime\"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>\
    \ <mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi>\
    \ <mo>)</mo></mrow> <mo>=</mo> <msub><mi>φ</mi> <mn>0</mn></msub> <mo>+</mo> <msubsup><mo>∑</mo>\
    \ <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup> <msub><mi>φ</mi>\
    \ <mi>i</mi></msub> <msubsup><mi>x</mi> <mi>i</mi> <msup><mo>'</mo></msup></msubsup></mrow></math>"
- en: Note that *x’* is a simplified feature vector, where each feature in *x’* is
    a binary variable that represents the presence or absence of a complex feature
    in the original input space. The second desirable property is *missingness*, which
    states that if *x’* contains features equal to zero, the weights associated with
    those features in the explainer model should also be zero. This is also understandable
    as a desirable property, since there would be no influence of a feature with value
    of zero on the output under a linear explainer *g*, and thus no need to assign
    a nonzero weight to that feature in the explainer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意*x'*是一个简化的特征向量，其中*x'*中的每个特征都是一个二进制变量，表示原始输入空间中复杂特征的存在或缺失。第二个理想属性是*缺失性*，它指出如果*x'*包含值为零的特征，则解释器模型中与这些特征相关的权重也应为零。这也是一个理想的属性，因为在线性解释器*g*下，值为零的特征对输出没有影响，因此在解释器中不需要为该特征分配非零权重。
- en: 'And finally, the third desirable property is *consistency*. This property states
    that if the underlying model changes such that a feature in the explainer space
    either increases or keeps its contribution constant, regardless of the values
    of the other features in the explainer space when compared to the original model,
    the explainer weight associated with that feature should be larger for the changed
    underlying model as compared to the original. That was a mouthful, so we represent
    it more precisely in mathematical notation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三个理想属性是*一致性*。该属性指出，如果基础模型发生变化，使得解释器空间中的一个特征要么增加要么保持其贡献恒定，无论与原始模型相比解释器空间中其他特征的值如何，那么与该特征相关的解释器权重应该对于更改后的基础模型而言比原始模型更大。这是一个复杂的概念，因此我们更精确地用数学符号表示：
- en: "<math alttext=\"If f prime left-parenthesis h Subscript x Baseline left-parenthesis\
    \ z Superscript prime Baseline right-parenthesis right-parenthesis minus f prime\
    \ left-parenthesis h Subscript x Baseline left-parenthesis z prime minus StartSet\
    \ i EndSet right-parenthesis right-parenthesis greater-than-or-equal-to f left-parenthesis\
    \ h Subscript x Baseline left-parenthesis z Superscript prime Baseline right-parenthesis\
    \ right-parenthesis minus f left-parenthesis h Subscript x Baseline left-parenthesis\
    \ z prime minus StartSet i EndSet right-parenthesis right-parenthesis comma for-all\
    \ z Superscript prime Baseline comma then phi Subscript i Baseline left-parenthesis\
    \ f prime comma x right-parenthesis greater-than-or-equal-to phi Subscript i Baseline\
    \ left-parenthesis f comma x right-parenthesis\"><mrow><mtext>If</mtext> <msup><mi>f</mi>\
    \ <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo>\
    \ <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∖</mo>\
    \ <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>\
    \ <mo>≥</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo>\
    \ <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∖</mo> <mrow><mo>{</mo>\
    \ <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo>\
    \ <mo>∀</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>,</mo> <mtext>then</mtext>\
    \ <msub><mi>φ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msup><mi>f</mi> <mo>'</mo></msup>\
    \ <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>≥</mo> <msub><mi>φ</mi> <mi>i</mi></msub>\
    \ <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>"
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"If f prime left-parenthesis h Subscript x Baseline left-parenthesis\
    \ z Superscript prime Baseline right-parenthesis right-parenthesis minus f prime\
    \ left-parenthesis h Subscript x Baseline left-parenthesis z prime minus StartSet\
    \ i EndSet right-parenthesis right-parenthesis greater-than-or-equal-to f left-parenthesis\
    \ h Subscript x Baseline left-parenthesis z Superscript prime Baseline right-parenthesis\
    \ right-parenthesis minus f left-parenthesis h Subscript x Baseline left-parenthesis\
    \ z prime minus StartSet i EndSet right-parenthesis right-parenthesis comma for-all\
    \ z Superscript prime Baseline comma then phi Subscript i Baseline left-parenthesis\
    \ f prime comma x right-parenthesis greater-than-or-equal-to phi Subscript i Baseline\
    \ left-parenthesis f comma x right-parenthesis\"><mrow><mtext>If</mtext> <msup><mi>f</mi>\
    \ <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo>\
    \ <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∖</mo>\
    \ <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>\
    \ <mo>≥</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo>\
    \ <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∖</mo> <mrow><mo>{</mo>\
    \ <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo>\
    \ <mo>∀</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>,</mo> <mtext>then</mtext>\
    \ <msub><mi>φ</mi> <mi>i</mi></msub> <mrow><mo>(</mo> <msup><mi>f</mi> <mo>'</mo></msup>\
    \ <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>≥</mo> <msub><mi>φ</mi> <mi>i</mi></msub>\
    \ <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>"
- en: Where *h* is the function that maps inputs from the interpretable space back
    to the original input space. Why is consistency a desirable property? For the
    new model, the delta between the existence of the corresponding, more complex
    feature in the input space and its absence is greater than or equal to the delta
    for the old model, regardless of all other feature settings. Thus, it makes sense
    that we should attribute at least as large a weight to it in the explainer for
    the new model as compared to the old model, since its existence is clearly more
    important for the new model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*是将可解释空间的输入映射回原始输入空间的函数。为什么一致性是一个理想的属性？对于新模型，输入空间中对应更复杂特征的存在与其缺失之间的差值大于或等于旧模型的差值，无论其他特征设置如何。因此，我们应该至少在新模型的解释器中给予它与旧模型相同的权重，因为它在新模型中的重要性显然更大。'
- en: As mentioned, for each underlying model *f* there exists a unique *g* within
    the additive attribution framework that also satisfies all three properties listed.
    Although we won’t show this here, this result is one that follows from earlier
    results in cooperative game theory, where the learned weights are called Shapley
    values. Shapley values were originally defined to quantify per-example feature
    importance in multivariate linear regression models where individual features
    have significant correlations. This is an important problem, especially in the
    setting of significant correlations due to ambiguity between which features are
    the most predictive. It could be the case that a feature A is correlated with
    the target y, but when factoring in feature B it turns out that feature A provides
    only negligible additional value (i.e., predictions don’t change significantly,
    test statistics remain relatively constant, etc.). On the other hand, feature
    B may provide significant predictive power both in the individual case and in
    the case where feature A is included.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，对于每个基础模型f，都存在一个在加性归因框架内满足所有三个属性的唯一g。虽然我们不会在这里展示这一点，但这个结果是从合作博弈理论的早期结果中得出的，学到的权重被称为Shapley值。Shapley值最初是为了量化多元线性回归模型中每个示例特征的重要性，其中个体特征具有显著的相关性。这是一个重要的问题，特别是在由于模糊性导致哪些特征最具预测性的情况下。可能情况是特征A与目标y相关，但考虑到特征B时，结果是特征A提供的附加值微不足道（即，预测没有显著变化，测试统计量保持相对恒定等）。另一方面，特征B可能在个体情况下和包括特征A的情况下都提供显著的预测能力。
- en: 'Determining the relative importance of feature A and B in a vanilla multivariate
    regression that includes both features is difficult due to their non-negligible
    correlation. Shapley values tease out these relationships and compute the true
    marginal impact of a given feature, as we will soon see. Here is the formula for
    Shapley values, where *i* represents the feature in question:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 确定包括两个特征的普通多元回归中特征A和B的相对重要性是困难的，因为它们之间的相关性不可忽略。Shapley值揭示了这些关系，并计算了给定特征的真实边际影响，我们很快就会看到。以下是Shapley值的公式，其中i代表感兴趣的特征：
- en: <math alttext="phi Subscript i Baseline equals sigma-summation Underscript upper
    S element-of upper F minus StartSet i EndSet Endscripts StartFraction StartAbsoluteValue
    upper S EndAbsoluteValue factorial asterisk left-parenthesis StartAbsoluteValue
    upper F EndAbsoluteValue minus StartAbsoluteValue upper S EndAbsoluteValue minus
    1 right-parenthesis factorial Over StartAbsoluteValue upper F EndAbsoluteValue
    factorial EndFraction asterisk left-bracket f Subscript upper S union StartSet
    i EndSet Baseline left-parenthesis x Subscript upper S union StartSet i EndSet
    Baseline right-parenthesis minus f Subscript upper S Baseline left-parenthesis
    x Subscript upper S Baseline right-parenthesis right-bracket"><mrow><msub><mi>φ</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>S</mi><mo>∈</mo><mi>F</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mfrac><mrow><mo>|</mo><mi>S</mi><mo>|</mo><mo>!</mo><mo>*</mo><mo>(</mo><mo>|</mo><mi>F</mi><mo>|</mo><mo>-</mo><mo>|</mo><mi>S</mi><mo>|</mo><mo>-</mo><mn>1</mn><mo>)</mo><mo>!</mo></mrow>
    <mrow><mo>|</mo><mi>F</mi><mo>|</mo><mo>!</mo></mrow></mfrac> <mo>*</mo> <mrow><mo>[</mo>
    <msub><mi>f</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>S</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="phi Subscript i Baseline equals sigma-summation Underscript upper
    S element-of upper F minus StartSet i EndSet Endscripts StartFraction StartAbsoluteValue
    upper S EndAbsoluteValue factorial asterisk left-parenthesis StartAbsoluteValue
    upper F EndAbsoluteValue minus StartAbsoluteValue upper S EndAbsoluteValue minus
    1 right-parenthesis factorial Over StartAbsoluteValue upper F EndAbsoluteValue
    factorial EndFraction asterisk left-bracket f Subscript upper S union StartSet
    i EndSet Baseline left-parenthesis x Subscript upper S union StartSet i EndSet
    Baseline right-parenthesis minus f Subscript upper S Baseline left-parenthesis
    x Subscript upper S Baseline right-parenthesis right-bracket"><mrow><msub><mi>φ</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>S</mi><mo>∈</mo><mi>F</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mfrac><mrow><mo>|</mo><mi>S</mi><mo>|</mo><mo>!</mo><mo>*</mo><mo>(</mo><mo>|</mo><mi>F</mi><mo>|</mo><mo>-</mo><mo>|</mo><mi>S</mi><mo>|</mo><mo>-</mo><mn>1</mn><mo>)</mo><mo>!</mo></mrow>
    <mrow><mo>|</mo><mi>F</mi><mo>|</mo><mo>!</mo></mrow></mfrac> <mo>*</mo> <mrow><mo>[</mo>
    <msub><mi>f</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>S</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: We will now break down this formula. Intuitively, the Shapley value for an individual
    feature is computed by first taking the difference between the predictions for
    a model trained over a subset of features *S* plus the feature in question *i*
    included, and predictions for a model trained over the same subset of features
    *S* but with feature *i* withheld. The final Shapley value is a weighted sum of
    these differences over all possible subsets of features *S*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将分解这个公式。直觉上，对于一个个体特征的Shapley值是通过首先计算模型在一个包含特征i的特征子集S上训练的预测与在相同特征子集S上但不包含特征i的模型训练的预测之间的差异来计算的。最终的Shapley值是所有可能特征子集S的这些差异的加权和。
- en: To find these differences, we can first train a multivariate linear regression
    model  <math alttext="f Subscript upper S"><msub><mi>f</mi> <mi>S</mi></msub></math>
    that only uses some subset of features *S*, and then train a second multivariate
    linear regression model  <math alttext="f Subscript upper S union StartSet i EndSet"><msub><mi>f</mi>
    <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub></math>
    that uses the subset of features <math alttext="upper S union StartSet i EndSet"><mrow><mi>S</mi>
    <mo>∪</mo> <mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></math> . Let the example we
    are explaining be denoted as *x*, and *x[A]* denote the portion of *x* that corresponds
    to some feature subset A. The difference  <math alttext="f Subscript upper S union
    StartSet i EndSet Baseline left-parenthesis x Subscript upper S union StartSet
    i EndSet Baseline right-parenthesis"><mrow><msub><mi>f</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∪</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></mrow></math> – <math alttext="f Subscript upper S Baseline
    left-parenthesis x Subscript upper S Baseline right-parenthesis"><mrow><msub><mi>f</mi>
    <mi>S</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>S</mi></msub> <mo>)</mo></mrow></mrow></math>
    represents how much the prediction changes when we include the feature *i*. Additionally,
    note that the formula is a sum over all possible feature subsets, which means
    that if the computed Shapley value for feature *i* is high, the difference between
    including feature *i* and not including feature *i* was likely substantial for
    a majority of possible feature subsets. This result signifies that feature *i*
    generally adds significant predictive power regardless of the features in *S*,
    which is captured by the high Shapley value. In the example provided earlier,
    we’d find that feature B has a higher Shapley value than feature A.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到这些差异，我们可以首先训练一个只使用一些特征子集S的多元线性回归模型f_S，然后训练第二个多元线性回归模型f_{S∪{i}}，它使用特征子集S∪{i}。让我们解释的示例表示为x，x[A]表示与某个特征子集A对应的x的部分。差异f_{S∪{i}}(x_{S∪{i}})
    - f_S(x_S)表示当我们包含特征i时预测变化多少。另外，注意这个公式是对所有可能的特征子集求和，这意味着如果计算出的特征i的Shapley值很高，那么包含特征i和不包含特征i的差异在大多数可能的特征子集中可能是显著的。这个结果表明，无论S中的特征如何，特征i通常都会增加显著的预测能力，这由高Shapley值捕捉到。在前面提供的示例中，我们会发现特征B的Shapley值比特征A高。
- en: Additionally, the intuition behind the weighting scheme is that it achieves
    a more unbiased result for feature importance, since subsets of a given size occur
    either more or less frequently than subsets of a different size in the set of
    all subsets. The number of subsets of a given size is computed using the choose
    function, a concept from counting and probability. When this is inverted and used
    as a weighting scheme, the result of a subset whose size occurs more frequently
    in the set of all possible subsets is weighted less than, for example, a feature
    subset consisting of only a single feature other than the feature in question.
    As stated earlier, we won’t prove why this is unbiased in full, but we hope that
    this makes the intuition clearer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，加权方案背后的直觉是，它实现了对特征重要性的更加无偏的结果，因为给定大小的子集在所有子集的集合中出现的频率要么更高，要么更低。给定大小的子集的数量是使用选择函数计算的，这是来自计数和概率的概念。当这被倒置并用作加权方案时，一个子集的结果，其大小在所有可能子集的集合中出现得更频繁，比如，一个仅包含除了问题中的特征之外的单个特征的特征子集，将被赋予比较少的权重。正如前面所述，我们不会完全证明为什么这是无偏的，但我们希望这能让直觉更加清晰。
- en: 'You may notice that computing exact Shapley regression values for any reasonable
    number of features is intractable. This would involve training a regression model
    on all possible subsets of features, where the number of subsets of features (and
    thus models to train) grows exponentially with the number of features. We instead
    resort to approximation via sampling to help. Given an example *x* to explain
    and a regression model  <math alttext="f Subscript upper S"><msub><mi>f</mi> <mi>S</mi></msub></math>
    trained on some subset of features *S*, we can compute  <math alttext="f Subscript
    upper S minus StartSet i EndSet"><msub><mi>f</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub></math>
    by taking an expectation of  <math alttext="f Subscript upper S"><msub><mi>f</mi>
    <mi>S</mi></msub></math> with respect to the distribution of feature *i* conditioned
    on *x*’s setting of features <math alttext="upper S minus StartSet i EndSet"><mrow><mi>S</mi>
    <mo>∖</mo> <mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></math> :'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，对任何合理数量的特征计算精确的Shapley回归值是不可行的。这将涉及在所有可能的特征子集上训练回归模型，其中特征子集的数量（因此需要训练的模型）随着特征数量的增加呈指数增长。我们转而通过抽样来进行近似。给定一个要解释的示例*x*和在一些特征子集*S*上训练的回归模型*f*，我们可以通过对特征*i*的分布关于*x*的特征设置<mi>S</mi>∖{<mi>i</mi>}的期望来计算*f*：
- en: <math alttext="f Subscript upper S minus StartSet i EndSet Baseline equals double-struck
    upper E Subscript p left-parenthesis x Sub Subscript i Subscript vertical-bar
    x Sub Subscript upper S minus StartSet i EndSet Subscript right-parenthesis Baseline
    left-bracket f Subscript upper S Baseline left-parenthesis x Subscript upper S
    minus StartSet i EndSet Baseline comma x Subscript i Baseline right-parenthesis
    right-bracket"><mrow><msub><mi>f</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>|</mo><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f Subscript upper S minus StartSet i EndSet Baseline equals double-struck
    upper E Subscript p left-parenthesis x Sub Subscript i Subscript vertical-bar
    x Sub Subscript upper S minus StartSet i EndSet Subscript right-parenthesis Baseline
    left-bracket f Subscript upper S Baseline left-parenthesis x Subscript upper S
    minus StartSet i EndSet Baseline comma x Subscript i Baseline right-parenthesis
    right-bracket"><mrow><msub><mi>f</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>|</mo><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: 'Where the bold represents that <math alttext="x Subscript upper S minus StartSet
    i EndSet"><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub></math>
    is being treated as a known entity taken from *x*, the example being explained,
    while *x[i]* is being treated as an unknown, i.e., is a random variable rather
    than taking on the value provided by *x*. As has been a common theme throughout
    this book, we can approximate expectations like the preceding one in an unbiased
    manner by sampling and averaging:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中粗体表示<mi>x</mi><mi>S</mi>∖{<mi>i</mi>}被视为从*x*中获取的已知实体，即被解释的示例，而*x[i]*被视为未知的，即是一个随机变量而不是取自*x*提供的值。正如本书中一贯的主题，我们可以通过抽样和平均来以无偏的方式近似前述期望。
- en: <math alttext="StartLayout 1st Row  double-struck upper E Subscript p left-parenthesis
    x Sub Subscript i Subscript vertical-bar x Sub Subscript upper S minus StartSet
    i EndSet Subscript right-parenthesis Baseline left-bracket f Subscript upper S
    Baseline left-parenthesis x Subscript upper S minus StartSet i EndSet Baseline
    comma x Subscript i Baseline right-parenthesis right-bracket 2nd Row  equals double-struck
    upper E Subscript p left-parenthesis x Sub Subscript i Subscript right-parenthesis
    Baseline left-bracket f Subscript upper S Baseline left-parenthesis x Subscript
    upper S minus StartSet i EndSet Baseline comma x Subscript i Baseline right-parenthesis
    right-bracket 3rd Row  almost-equals StartFraction 1 Over n EndFraction sigma-summation
    Underscript j equals 1 Overscript n Endscripts f Subscript upper S Baseline left-parenthesis
    x Subscript upper S minus StartSet i EndSet Baseline comma x Subscript i Superscript
    left-parenthesis j right-parenthesis Baseline right-parenthesis EndLayout"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>|</mo><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msubsup><mi>x</mi> <mi>i</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msubsup>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  double-struck upper E Subscript p left-parenthesis
    x Sub Subscript i Subscript vertical-bar x Sub Subscript upper S minus StartSet
    i EndSet Subscript right-parenthesis Baseline left-bracket f Subscript upper S
    Baseline left-parenthesis x Subscript upper S minus StartSet i EndSet Baseline
    comma x Subscript i Baseline right-parenthesis right-bracket 2nd Row  equals double-struck
    upper E Subscript p left-parenthesis x Sub Subscript i Subscript right-parenthesis
    Baseline left-bracket f Subscript upper S Baseline left-parenthesis x Subscript
    upper S minus StartSet i EndSet Baseline comma x Subscript i Baseline right-parenthesis
    right-bracket 3rd Row  almost-equals StartFraction 1 Over n EndFraction sigma-summation
    Underscript j equals 1 Overscript n Endscripts f Subscript upper S Baseline left-parenthesis
    x Subscript upper S minus StartSet i EndSet Baseline comma x Subscript i Superscript
    left-parenthesis j right-parenthesis Baseline right-parenthesis EndLayout"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>|</mo><msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>f</mi> <mi>S</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <msub><mi>f</mi> <mi>S</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>S</mi><mo>∖</mo><mo>{</mo><mi>i</mi><mo>}</mo></mrow></msub>
    <mo>,</mo> <msubsup><mi>x</mi> <mi>i</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msubsup>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: You may have noticed the parallels between the procedure we just described and
    the estimation procedure described in [“Partial Dependence Plots”](#pdp-sect).
    In fact, these are doing the exact same thing—notice that we again assume independence
    between feature subset <math alttext="upper S minus StartSet i EndSet"><mrow><mi>S</mi>
    <mo>∖</mo> <mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></math> and feature *i*, which
    allows us to use all samples of feature *i* from the dataset indiscriminately.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们刚刚描述的过程与[“部分依赖图”](#pdp-sect)中描述的估计过程之间的相似之处。事实上，这些都在做同样的事情——请注意，我们再次假设特征子集<mrow><mi>S</mi>
    <mo>∖</mo> <mo>{</mo> <mi>i</mi> <mo>}</mo></mrow>和特征*i*之间是独立的，这使我们可以不加区分地使用数据集中特征*i*的所有样本。
- en: 'The authors propose SHAP values in the general case, where SHAP values are
    given by the formula:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们在一般情况下提出了SHAP值，其中SHAP值由以下公式给出：
- en: <math alttext="phi Subscript i Baseline left-parenthesis f comma x right-parenthesis
    equals sigma-summation Underscript z prime subset-of-or-equal-to x Superscript
    prime Baseline Endscripts StartFraction StartAbsoluteValue z Superscript prime
    Baseline EndAbsoluteValue factorial asterisk left-parenthesis upper M minus StartAbsoluteValue
    z Superscript prime Baseline EndAbsoluteValue minus 1 right-parenthesis factorial
    Over upper M factorial EndFraction left-bracket f left-parenthesis h Subscript
    x Baseline left-parenthesis z prime right-parenthesis right-parenthesis minus
    f left-parenthesis h Subscript x Baseline left-parenthesis z prime minus StartSet
    i EndSet right-parenthesis right-parenthesis right-bracket"><mrow><msub><mi>φ</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mrow><msup><mi>z</mi> <mo>'</mo></msup> <mo>⊆</mo><msup><mi>x</mi>
    <mo>'</mo></msup></mrow></msub> <mfrac><mrow><mrow><mo>|</mo></mrow><msup><mi>z</mi>
    <mo>'</mo></msup> <mrow><mo>|</mo><mo>!</mo><mo>*</mo><mo>(</mo><mi>M</mi><mo>-</mo><mo>|</mo></mrow><msup><mi>z</mi>
    <mo>'</mo></msup> <mrow><mo>|</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>!</mo></mrow>
    <mrow><mi>M</mi><mo>!</mo></mrow></mfrac> <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>∖</mo>
    <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="phi Subscript i Baseline left-parenthesis f comma x right-parenthesis
    equals sigma-summation Underscript z prime subset-of-or-equal-to x Superscript
    prime Baseline Endscripts StartFraction StartAbsoluteValue z Superscript prime
    Baseline EndAbsoluteValue factorial asterisk left-parenthesis upper M minus StartAbsoluteValue
    z Superscript prime Baseline EndAbsoluteValue minus 1 right-parenthesis factorial
    Over upper M factorial EndFraction left-bracket f left-parenthesis h Subscript
    x Baseline left-parenthesis z prime right-parenthesis right-parenthesis minus
    f left-parenthesis h Subscript x Baseline left-parenthesis z prime minus StartSet
    i EndSet right-parenthesis right-parenthesis right-bracket"><mrow><msub><mi>φ</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mrow><msup><mi>z</mi> <mo>'</mo></msup> <mo>⊆</mo><msup><mi>x</mi>
    <mo>'</mo></msup></mrow></msub> <mfrac><mrow><mrow><mo>|</mo></mrow><msup><mi>z</mi>
    <mo>'</mo></msup> <mrow><mo>|</mo><mo>!</mo><mo>*</mo><mo>(</mo><mi>M</mi><mo>-</mo><mo>|</mo></mrow><msup><mi>z</mi>
    <mo>'</mo></msup> <mrow><mo>|</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>!</mo></mrow>
    <mrow><mi>M</mi><mo>!</mo></mrow></mfrac> <mrow><mo>[</mo> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>∖</mo>
    <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
- en: "Where *z’* is a subset of the nonzero components of *x’*. Additionally,  <math\
    \ alttext=\"z prime minus StartSet i EndSet\"><mrow><msup><mi>z</mi> <mo>'</mo></msup>\
    \ <mrow><mo>∖</mo> <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></mrow></mrow></math>\
    \  represents setting feature *i* in the interpretable space equal to zero. Note\
    \ that if feature *i* is already zero in the input *x’*, then the formula outputs\
    \ zero as well since  <math alttext=\"f left-parenthesis h Subscript x Baseline\
    \ left-parenthesis z Superscript prime Baseline right-parenthesis right-parenthesis\
    \ equals f left-parenthesis h Subscript x Baseline left-parenthesis z prime minus\
    \ StartSet i EndSet right-parenthesis right-parenthesis comma for-all z Superscript\
    \ prime Baseline subset-of-or-equal-to x prime\"><mrow><mi>f</mi> <mrow><mo>(</mo>\
    \ <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup>\
    \ <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi>\
    \ <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99\
    </mi> <mo>∖</mo> <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow>\
    \ <mo>)</mo></mrow> <mo>,</mo> <mo>∀</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi>\
    \ <mi>\x99</mi> <mo>⊆</mo> <msup><mi>x</mi> <mo>'</mo></msup></mrow></math> .\
    \ This quick check shows that the formula indeed satisfies the property of missingness.\
    \ The vector consisting of SHAP values for each feature in *x’* completely defines\
    \ the optimal explainer model *g* in the additive attribution framework, where\
    \ optimal signifies that *g* satisfies all three properties defined earlier: local\
    \ accuracy, consistency, and missingness. Right off the bat, we can see the parallels\
    \ between the proposed SHAP values and the Shapley values from multivariate regression.\
    \ Additionally, we can use the same sampling procedure to estimate SHAP values."
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: "其中*z'*是*x'*的非零分量的子集。此外，<math alttext=\"z prime minus StartSet i EndSet\"><mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>∖</mo> <mrow><mo>{</mo> <mi>i</mi> <mo>}</mo></mrow></mrow></mrow></math>表示将可解释空间中的特征*i*设置为零。请注意，如果特征*i*在输入*x'*中已经为零，则该公式也会输出零，因为<math\
    \ alttext=\"f left-parenthesis h Subscript x Baseline left-parenthesis z Superscript\
    \ prime Baseline right-parenthesis right-parenthesis equals f left-parenthesis\
    \ h Subscript x Baseline left-parenthesis z prime minus StartSet i EndSet right-parenthesis\
    \ right-parenthesis comma for-all z Superscript prime Baseline subset-of-or-equal-to\
    \ x prime\"><mrow><mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow>\
    \ <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>∖</mo> <mrow><mo>{</mo>\
    \ <mi>i</mi> <mo>}</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo>\
    \ <mo>∀</mo> <mi>z</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>⊆</mo> <msup><mi>x</mi>\
    \ <mo>'</mo></msup></mrow></math>。这个快速检查表明该公式确实满足缺失性的属性。由SHAP值组成的向量完全定义了加法归因框架中每个特征的最佳解释模型*g*，其中最佳表示*g*满足前面定义的三个属性：局部准确性、一致性和缺失性。我们可以立即看到提出的SHAP值与多元回归中的Shapley值之间的相似之处。此外，我们可以使用相同的抽样过程来估计SHAP值。"
- en: As discussed, LIME is in the additive attribution framework. In the original
    LIME paper, the optimal explainer model *g* was selected via a specialized optimization
    procedure that first selected *k* features to have a nonzero contribution and
    then performed standard least squares optimization to achieve the final weights
    of *g*. Due to these heuristics, including the choice of kernel <math alttext="pi
    Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , there is no guarantee that the explainer selected using the procedure presented
    in the original LIME paper will satisfy the SHAP criteria of local accuracy, missingness,
    and consistency.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论的那样，LIME处于加法归因框架中。在原始的LIME论文中，通过专门的优化过程选择了最佳的解释模型*g*，该过程首先选择了*k*个特征具有非零贡献，然后执行标准的最小二乘优化以获得*g*的最终权重。由于这些启发式方法，包括核选择<math
    alttext="pi Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>，无法保证使用原始LIME论文中提出的程序选择的解释符合SHAP的局部准确性、缺失性和一致性标准。
- en: However, the optimization procedure presented in the LIME paper does achieve
    an explainer that satisfies the criteria for explainer models proposed in LIME;
    recall the concepts of being interpretable, having local fidelity, being model
    agnostic, and achieving global perspective from the previous section. We point
    this out specifically to show that different groups of knowledgeable individuals
    don’t necessarily have the exact same idea of what it means for an explainer to
    be interpretable, and that interpretability as a concept has evolved over time.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LIME论文中提出的优化过程确实可以获得一个满足LIME中提出的解释模型标准的解释器；回顾前一节中关于可解释性、局部忠实度、模型无关性和全局视角的概念。我们特别指出这一点是为了表明不同的知识渊博的个体并不一定对于解释器何为可解释性有完全相同的理解，而且可解释性作为一个概念随着时间的推移而发展。
- en: 'It turns out that in LIME, there exists an exact form to the proximity measure
    <math alttext="pi Subscript x Baseline left-parenthesis z right-parenthesis"><mrow><msub><mi>π</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , <math alttext="omega"><mi>ω</mi></math> , and loss function *L* such that when
    minimized, results in an optimal explainer *g* that satisfies all three SHAP criteria
    for interpretability:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，在LIME中存在一种确切的形式来衡量接近度<math alttext="pi Subscript x Baseline left-parenthesis
    z right-parenthesis"><mrow><msub><mi>π</mi> <mi>x</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow></mrow></math>，<math alttext="omega"><mi>ω</mi></math>，和损失函数*L*，当最小化时，会得到一个满足解释性的最佳解释器*g*，满足解释性的三个SHAP标准：
- en: "<math alttext=\"StartLayout 1st Row  omega left-parenthesis g right-parenthesis\
    \ equals 0 2nd Row  pi Subscript x prime Baseline left-parenthesis z prime right-parenthesis\
    \ equals StartFraction upper M minus 1 Over left-parenthesis upper M choose StartAbsoluteValue\
    \ z Superscript prime Baseline EndAbsoluteValue right-parenthesis asterisk StartAbsoluteValue\
    \ z prime EndAbsoluteValue asterisk left-parenthesis upper M minus StartAbsoluteValue\
    \ z prime EndAbsoluteValue right-parenthesis EndFraction 3rd Row  upper L left-parenthesis\
    \ f comma g comma pi right-parenthesis equals sigma-summation Underscript z prime\
    \ element-of upper Z Endscripts left-parenthesis f left-parenthesis h Subscript\
    \ x Baseline left-parenthesis z prime right-parenthesis right-parenthesis minus\
    \ g left-parenthesis z prime right-parenthesis right-parenthesis squared asterisk\
    \ pi Subscript x prime Baseline left-parenthesis z prime right-parenthesis EndLayout\"\
    ><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>ω</mi>\
    \ <mo>(</mo> <mi>g</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd\
    \ columnalign=\"right\"><mrow><msub><mi>π</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub>\
    \ <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>=</mo>\
    \ <mfrac><mrow><mi>M</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mrow><mo>(</mo><mi>M</mi><mtext>choose</mtext><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>)</mo><mo>*</mo><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>*</mo><mo>(</mo><mi>M</mi><mo>-</mo><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>)</mo></mrow></mrow></mfrac></mrow></mtd></mtr>\
    \ <mtr><mtd columnalign=\"right\"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>f</mi>\
    \ <mo>,</mo> <mi>g</mi> <mo>,</mo> <mi>π</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo>\
    \ <mrow><mi>z</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi><mo>∈</mo><mi>Z</mi></mrow></munder>\
    \ <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>h</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo><msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup> <mo>*</mo>\
    \ <msub><mi>π</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>"
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"StartLayout 1st Row  omega left-parenthesis g right-parenthesis\
    \ equals 0 2nd Row  pi Subscript x prime Baseline left-parenthesis z prime right-parenthesis\
    \ equals StartFraction upper M minus 1 Over left-parenthesis upper M choose StartAbsoluteValue\
    \ z Superscript prime Baseline EndAbsoluteValue right-parenthesis asterisk StartAbsoluteValue\
    \ z prime EndAbsoluteValue asterisk left-parenthesis upper M minus StartAbsoluteValue\
    \ z prime EndAbsoluteValue right-parenthesis EndFraction 3rd Row  upper L left-parenthesis\
    \ f comma g comma pi right-parenthesis equals sigma-summation Underscript z prime\
    \ element-of upper Z Endscripts left-parenthesis f left-parenthesis h Subscript\
    \ x Baseline left-parenthesis z prime right-parenthesis right-parenthesis minus\
    \ g left-parenthesis z prime right-parenthesis right-parenthesis squared asterisk\
    \ pi Subscript x prime Baseline left-parenthesis z prime right-parenthesis EndLayout\"\
    ><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>ω</mi>\
    \ <mo>(</mo> <mi>g</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd\
    \ columnalign=\"right\"><mrow><msub><mi>π</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub>\
    \ <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>=</mo>\
    \ <mfrac><mrow><mi>M</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mrow><mo>(</mo><mi>M</mi><mtext>choose</mtext><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>)</mo><mo>*</mo><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>*</mo><mo>(</mo><mi>M</mi><mo>-</mo><mo>|</mo></mrow><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mrow><mo>|</mo><mo>)</mo></mrow></mrow></mfrac></mrow></mtd></mtr>\
    \ <mtr><mtd columnalign=\"right\"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>f</mi>\
    \ <mo>,</mo> <mi>g</mi> <mo>,</mo> <mi>π</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo>\
    \ <mrow><mi>z</mi><mi>â</mi><mi>\x80</mi><mi>\x99</mi><mo>∈</mo><mi>Z</mi></mrow></munder>\
    \ <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>h</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo><msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup> <mo>*</mo>\
    \ <msub><mi>π</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mrow><mo>(</mo>\
    \ <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>"
- en: We can optimize this loss function using a weighted least squares optimization
    to obtain the unique optimal *g*. Note that the kernel here is distinct in interpretation
    from the kernel choices presented in the original LIME paper. Instead of the kernel
    decreasing in value as the samples get farther from the example being explained,
    the SHAP kernel is symmetric. This can be verified by examining the output of
    the kernel when *|z'| = k* and when *|z'| = M – k*. In fact, from just looking
    at the formula, we can see that the value of the kernel isn’t even dependent on
    *x’*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用加权最小二乘优化来优化这个损失函数，以获得唯一的最优*g*。请注意，这里的核函数在解释上与原始LIME论文中提出的核函数选择是不同的。SHAP核函数是对称的，而不是像LIME中的核函数随着样本远离被解释的示例而减小。这可以通过检查核函数在*|z'|
    = k*和*|z'| = M – k*时的输出来验证。实际上，仅仅通过观察公式，我们就可以看到核函数的值甚至不依赖于*x'*。
- en: In conclusion, SHAP values unify several existing interpretability methods by
    first defining the additive attribution framework, which is shared amongst these
    methods, and second by proving the existence of a unique optimal explainer within
    this framework that satisfies three desirable properties.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，SHAP值通过首先定义加法归因框架，将几种现有的可解释性方法统一起来，其次通过证明在这个框架内存在一个满足三个理想属性的唯一最优解释器的存在。
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Although interpretability often comes in a variety of forms, they are all designed
    with the end goal of being able to explain model behavior. We learned that not
    every model is interpretable by construction, and even those that are might only
    be superficially so. For example, although vanilla linear regression seems to
    be quite interpretable by design, correlations between features can muddle this
    initially clear picture. Additionally, we learned about interpretability methods
    that are built into the model itself, such as extractive rationalization, and
    post hoc interpretability methods such as LIME and SHAP. The right form of interpretability
    will often depend on the domain—for example, using gradient-based methods for
    image classification may make sense, but not so much in language problems. The
    soft attention scheme discussed in previous chapters may not be as desirable for
    sentiment analysis as, say, the hard selection methodology presented in our section
    on extractive rationalization. And finally, we learned about how interpretability
    does not carry the exact same meaning across the board, even in research—note
    our discussion on the differences between the optimal explainers generated by
    LIME and SHAP. We hope that this chapter served as a fruitful foray into the vast
    landscape of interpretability research.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可解释性通常以各种形式出现，但它们都旨在能够解释模型行为。我们了解到，并非每个模型都是通过构建可解释的，即使是那些可能表面上看起来是可解释的。例如，尽管普通的线性回归似乎在设计上是相当可解释的，但特征之间的相关性可能会混淆这个最初清晰的图像。此外，我们了解到模型本身内置的可解释性方法，如抽取式合理化，以及事后可解释性方法，如LIME和SHAP。正确的可解释性形式通常取决于领域——例如，在图像分类中使用基于梯度的方法可能是有意义的，但在语言问题中可能不那么合适。在先前章节讨论的软注意力方案对于情感分析可能不如我们在抽取式合理化部分提出的硬选择方法那么理想。最后，我们了解到可解释性在研究中甚至在整个领域中并不具有完全相同的含义——请注意我们对LIME和SHAP生成的最优解释器之间差异的讨论。我们希望这一章对广阔的可解释性研究领域起到了富有成效的探索作用。
- en: ^([1](ch11.xhtml#idm45934166216704-marker)) Lei et al. “Rationalizing Neural
    Predictions.” *arXiv Preprint arXiv*:1606.04155\. 2016.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.xhtml#idm45934166216704-marker)) 雷等人。“理性化神经预测。”*arXiv Preprint arXiv*:1606.04155\.
    2016年。
- en: ^([2](ch11.xhtml#idm45934166151520-marker)) Ribeiro et al. “Why Should I Trust
    You? Explaining the Predictions of Any Classifier.” *arXiv Preprint arXiv*:1602.04938\.
    2016.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.xhtml#idm45934166151520-marker)) 里贝罗等人。“我为什么应该相信你？解释任何分类器的预测。”*arXiv
    Preprint arXiv*:1602.04938\. 2016年。
- en: ^([3](ch11.xhtml#idm45934166079056-marker)) Lundberg et al. “A Unified Approach
    to Interpreting Model Predictions.” *arXiv Preprint arXiv*:1705.07874\. 2017.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11.xhtml#idm45934166079056-marker)) 兰德伯格等人。“解释模型预测的统一方法。”*arXiv Preprint
    arXiv*:1705.07874\. 2017年。
