<html><head></head><body><section data-pdf-bookmark="Chapter 6. Setting Goals and Measuring Success" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch06_setting_goals_and_measuring_success_1736793247821683">
<h1><span class="label">Chapter 6. </span>Setting Goals and Measuring Success</h1>

<p> </p>

<p>Unpacking<a contenteditable="false" data-primary="metrics" data-type="indexterm" id="icd601"/> success in AI products can be surprisingly challenging. No single metric can fully capture an AI product’s impact. Instead, an understanding of success emerges from a balanced combination of diverse metrics working together to provide a comprehensive view of product health. When determining whether a product or feature is ready for launch, it’s crucial to consider these metrics strategically. Remember, there’s no one-size-fits-all recipe; the right metrics depend on the nature of your product, its users, and the problem it aims to solve.</p>

<p>To truly understand the performance of an AI feature, relying on a single metric will almost always fall short. The real insights come from examining multiple metrics in tandem. I like to think of this as an AI <em>product metric blend</em> (see <a data-type="xref" href="#ch06_figure_1_1736793247813164">Figure 6-1</a>). Each<a contenteditable="false" data-primary="metrics" data-secondary="product metric blend" data-type="indexterm" id="id1127"/> metric highlights a specific aspect of the product’s characteristics. By blending these metrics and examining the results, you can gain a holistic understanding of an AI feature’s performance and overall impact.</p>

<p>A successful AI product has three core components: product health metrics, system health metrics, and AI proxy metrics. This chapter explores each in detail to build a solid foundation for evaluating your AI product’s success, then presents a framework to help you craft the right OKRs.</p>

<figure><div class="figure" id="ch06_figure_1_1736793247813164"><img src="assets/bapp_0601.png"/>
<h6><span class="label">Figure 6-1. </span>The product metric blend (source: Dr. Marily Nika)</h6>
</div></figure>

<section data-pdf-bookmark="Product Health Metrics" data-type="sect1"><div class="sect1" id="ch06_product_health_metrics_1736793247821817">
<h1>Product Health Metrics</h1>

<p>Product<a contenteditable="false" data-primary="metrics" data-secondary="product health metrics" data-type="indexterm" id="icd602"/><a contenteditable="false" data-primary="product health metrics" data-type="indexterm" id="icd608"/> health metrics, such as engagement, retention, and satisfaction, fall squarely within your domain of responsibility as an AI PM. These metrics will become the cornerstone of how you monitor and optimize your product.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The lists in this and the following two sections are not comprehensive. Rather, they highlight the most commonly used product health metrics a PM may come across.</p>
</div>

<p class="fix_tracking">Let’s break down the key metrics in this category by exploring an example that mirrors what you might encounter in your day-to-day work. Suppose you’re the product manager for a (fictional) AI-driven fitness app that we’ll call FitAI, which aims to become the go-to solution for fitness enthusiasts. Your mission? Continuously monitor and optimize FitAI using a variety of product health <span class="keep-together">metrics</span>:</p>

<dl>
	<dt>Engagement</dt>
	<dd>
	<p>Engagement <a contenteditable="false" data-primary="FitAI (fictional app)" data-secondary="engagement metric" data-type="indexterm" id="id1128"/>measures how actively users interact with your AI product. Frequency of use, session duration, and the number of interactions per session are strong indicators of engagement. For AI products, engagement often focuses on how frequently users rely on AI-driven features, such as recommendations or insights.</p>
	</dd>
	<dd>
	<p>Imagine your users opening the FitAI app every morning to check out their personalized, AI-generated workout plans, track their progress, and share their achievements. High engagement here means the product is providing clear value. You can track this by measuring usage frequency, consistency, and time spent within the app. For example, you might experiment by introducing a new feature that allows users to adjust their workout intensity based on daily performance. An increase in engagement following the introduction of this feature could indicate it resonates with users, whereas a decrease might signal areas for improvement.</p>
	</dd>
	<dt>User satisfaction</dt>
	<dd>
	<p>While <a contenteditable="false" data-primary="FitAI (fictional app)" data-secondary="user satisfaction metric" data-type="indexterm" id="id1129"/>engagement is essential, it’s usually closely linked with user satisfaction. This qualitative metric reflects how happy users are with your AI product. High satisfaction often translates into loyalty and advocacy.<a contenteditable="false" data-primary="surveys" data-type="indexterm" id="id1130"/> Surveys, feedback forms,<a contenteditable="false" data-primary="feedback forms" data-type="indexterm" id="id1131"/> and <a href="https://oreil.ly/E7dFZ">Net Promoter Scores (NPSs)</a> <a contenteditable="false" data-primary="Net Promoter Scores (NPSs)" data-type="indexterm" id="id1132"/><a contenteditable="false" data-primary="NPSs (Net Promoter Scores)" data-type="indexterm" id="id1133"/>are great ways to measure this. For AI products, satisfaction also hinges on whether the AI meets user expectations, provides relevant outputs, and enhances the overall experience.</p>
	</dd>
	<dd>
	<p>In the context of FitAI, you may run surveys to discover that users are thrilled with the app’s personalized workout plans and progress tracking but find the interface rigid and unfriendly. This feedback can guide you in making design decisions that make the app more intuitive, directly impacting user satisfaction scores.</p>
	</dd>
	<dt>Adoption</dt>
	<dd>
	<p>Adoption tracks the rate at which new users start using your AI product. High adoption suggests that the product is gaining traction in the market. Monitoring sign-up rates and identifying trends can provide insights into what drives user adoption, like successful marketing campaigns or word <span class="keep-together">of mouth.</span></p>
	</dd>
	<dd>
	<p>Suppose FitAI <a contenteditable="false" data-primary="FitAI (fictional app)" data-secondary="adoption metric" data-type="indexterm" id="id1134"/>experiences a surge in new users after a partnership with a famous sports team, like the Boston Celtics. This spike in adoption confirms that the product is on the right track, encouraging you to explore additional collaborations to sustain growth.</p>
	</dd>
	<dt>Conversion</dt>
	<dd>
	<p>Conversion measures to what extent you’ve achieved your end goal. For example, sales bots and agents should be measured on their ability to close deals. A donation solicitation bot should be measured on the number of donations received after engagement.</p>
	</dd>
	<dt>Retention</dt>
	<dd>
	<p>Retention rates measure how well your AI product keeps users coming back over time. This is the metric that tests your product’s lasting value. <a contenteditable="false" data-primary="FitAI (fictional app)" data-secondary="retention metric" data-type="indexterm" id="id1135"/>For FitAI, retention would be counted when users not only download the app but also consistently use it to meet their fitness goals.</p>
	</dd>
	<dd>
	<p>Suppose you notice that users who complete their first AI-recommended workout are 50% more likely to remain active. This insight suggests that new features should focus on guiding users through their initial interactions, possibly with gamified rewards, to boost retention. It is also important to measure <a contenteditable="false" data-primary="churn" data-type="indexterm" id="id1136"/>churn. <em>Churn</em> is the flip side of retention. It’s the rate at which users stop using your product. Analyzing churn patterns can provide valuable insights into potential issues. If you notice a wave of users quitting FitAI after one month because they find the AI-generated workouts repetitive, it signals an opportunity to introduce customizable workout options. Understanding why users leave is just as crucial as knowing why they stay.</p>
	</dd>
	<dt>Financial metrics</dt>
	<dd>
	<p>Financial metrics such as revenue and ROI gauge the economic impact of your product. <a contenteditable="false" data-primary="FitAI (fictional app)" data-secondary="financial metrics" data-type="indexterm" id="id1137"/>For FitAI, assessing the balance between costs and revenue from subscriptions and in-app purchases will help you make informed decisions about future investments.</p>
	</dd>
</dl>

<p>Your job as an AI PM is to monitor these product health metrics continuously, analyze them, and implement strategies for optimization. This often means collaborating with your development team to address technical issues and adjusting your marketing strategies to boost adoption and retention. Now let’s pivot to system health metrics to understand how the product performs under the <a contenteditable="false" data-primary="metrics" data-secondary="product health metrics" data-startref="icd602" data-type="indexterm" id="id1138"/><a contenteditable="false" data-primary="product health metrics" data-startref="icd608" data-type="indexterm" id="id1139"/>hood.</p>
</div></section>

<section data-pdf-bookmark="System Health Metrics" data-type="sect1"><div class="sect1" id="ch06_system_health_metrics_1736793247821883">
<h1>System Health Metrics</h1>

<p>While <a contenteditable="false" data-primary="metrics" data-secondary="system health metrics" data-type="indexterm" id="id1140"/><a contenteditable="false" data-primary="system health metrics" data-type="indexterm" id="id1141"/>your main focus as an AI PM might be on product health, it’s essential to have a grasp of system health metrics. These metrics reveal how the product performs at a technical level, providing insights into scalability, reliability, and overall performance. They include factors such as the following:</p>

<dl>
	<dt>Uptime and latency</dt>
	<dd>
	<p>Uptime<a contenteditable="false" data-primary="uptime metric" data-type="indexterm" id="id1142"/> tracks how often the system is available to users, while <a contenteditable="false" data-primary="latency metric" data-type="indexterm" id="id1143"/>latency measures the system’s response time. High uptime and low latency are vital for maintaining user trust. For <a contenteditable="false" data-primary="FitAI (fictional app)" data-secondary="uptime and latency metrics" data-type="indexterm" id="id1144"/>FitAI, maximizing uptime and minimizing latency ensures a smooth experience as users interact with their personalized workout plans.</p>
	</dd>
	<dt>Scalability</dt>
	<dd>
	<p>Scalability <a contenteditable="false" data-primary="scalability " data-secondary="system health metric" data-type="indexterm" id="id1145"/>becomes critical as your user base grows. By conducting load testing and monitoring resource usage during peak traffic, you can ensure that your AI system can handle increasing loads effectively.</p>
	</dd>
	<dt>Error rate</dt>
	<dd>
	<p>Error rates track the frequency of system errors. High error rates can lead to user dissatisfaction and disengagement. <a contenteditable="false" data-primary="FitAI (fictional app)" data-secondary="error rate" data-type="indexterm" id="id1146"/>If FitAI experiences a spike in errors after a new feature release, it’s a prompt to investigate and resolve bugs swiftly.</p>
	</dd>
</dl>

<p>To maintain system health, routine monitoring and audits are essential. Implement automated alerts for key metrics and conduct stress tests regularly. Developing an incident response plan will help you act swiftly if issues arise.</p>

<p>With system health under control, let’s now focus on the AI component—the proxy metrics that directly impact model performance.</p>
</div></section>

<section data-pdf-bookmark="AI Proxy Metrics" data-type="sect1"><div class="sect1" id="ch06_ai_proxy_metrics_1736793247821948">
<h1>AI Proxy Metrics</h1>

<p>Similarly<a contenteditable="false" data-primary="metrics" data-secondary="AI proxy metrics" data-type="indexterm" id="icd603"/><a contenteditable="false" data-primary="AI proxy metrics" data-type="indexterm" id="icd609"/> to system health metrics, you may not have direct control over AI proxy metrics as an AI PM. Still, you must recognize when these metrics indicate a change in how the user base interacts with a product. AI proxy metrics play a pivotal role in assessing trade-offs and making strategic decisions, and they serve as a yardstick for evaluating the effectiveness of the underlying model. Proxy metrics focus on the integrity of the underlying models used in a product.</p>

<p>In ML, proxy metrics play a significant role in gauging model accuracy. The name <em>proxy</em> reflects that they measure the model’s performance but are different from the ultimate goal of the product or feature. Let me give you a few examples.</p>

<section data-pdf-bookmark="Model Quality Metrics" data-type="sect2"><div class="sect2" id="ch06_model_quality_metrics_1736793247822011">
<h2>Model Quality Metrics</h2>

<p><em>Model quality</em><a contenteditable="false" data-primary="metrics" data-secondary="AI proxy metrics" data-tertiary="model quality metrics" data-type="indexterm" id="icd604"/><a contenteditable="false" data-primary="model quality metrics" data-type="indexterm" id="icd614"/><em> </em>refers to the effectiveness of a trained model in making predictions or decisions based on new, unseen data. It is typically assessed through various performance metrics that indicate how well the model’s predictions align with actual outcomes.</p>

<p>Understanding model quality is crucial for AI PMs because it directly impacts the reliability, efficiency, and impact of an AI product. For instance, in a healthcare AI application used to diagnose diseases, high precision ensures that the diagnoses provided by the model are correct. In contrast, high recall ensures that the model identifies as many true cases of the disease as possible. Poor model quality could lead to incorrect diagnoses, potentially harming patients and damaging the credibility of the healthcare provider.</p>

<p>As an AI PM, you must critically evaluate model performance using these metrics to manage AI products effectively. This involves understanding what each metric tells you about the model’s behavior and how to improve these metrics through various optimization strategies.</p>

<p>The AIPDL, which we’ve explored throughout this book, is inherently iterative. After deployment, you’ll frequently revisit earlier phases, particularly model training and validation, as new user data becomes available. This iterative cycle includes a series of experiments where you run evaluations (evals) to compare the performance of the live model against offline versions. These evaluations help you determine if a new model offers a significant improvement, guiding your decision on whether it’s time to launch the updated version.</p>

<p>Let’s cover a few of the most frequently used model quality metrics, using the example of a system that classifies incoming emails by whether or not they are spam:</p>

<dl>
	<dt class="pagebreak-before less_space">Accuracy</dt>
	<dd>
	<p>We measure the accuracy of this email classification feature by the percentage of correct classifications made by the system. Think of it as a test; out of all the emails classified, how many did the algorithm accurately identify as spam or nonspam?</p>
	</dd>
	<dt>Precision</dt>
	<dd>
	<p>Precision<a contenteditable="false" data-primary="precision metric" data-type="indexterm" id="id1147"/> is the ratio of true positive results to all positive results predicted by the model. It determines the accuracy of positive predictions. In our spam example, out of all the emails the algorithm marked as spam, we want to know the number of emails that are truly spam. High precision indicates that the model has few false positives; in other words, fewer type I errors.</p>
	</dd>
	<dt>Sensitivity</dt>
	<dd>
	<p>Sensitivity <a contenteditable="false" data-primary="sensitivity metric" data-type="indexterm" id="id1148"/>measures how good the model is at finding all the positive cases. Let’s use the spam example to provide some context. The sensitivity metric evaluates the algorithm’s success in identifying every spam email in our inbox. High sensitivity means you have few false negatives—in other words, fewer type II errors.</p>
	</dd>
	<dt>Recall</dt>
	<dd>
	<p>Recall <a contenteditable="false" data-primary="recall metric" data-type="indexterm" id="id1149"/>measures the model’s ability to correctly identify all relevant positive cases. In the spam example, it tells us how many of the actual spam emails were successfully flagged by the algorithm. High recall indicates that the model misses very few spam emails, meaning it is effective at capturing positive cases with minimal false negatives. Suppose there are 100 actual spam emails in the inbox, and the algorithm correctly identifies 80 of them as spam; however, it misses 20 spam emails by classifying them as nonspam. If the algorithm successfully identified 80% of all spam emails, you would say that it had 80% recall.</p>
	</dd>
	<dt>Receiver operating characteristic (ROC) curve</dt>
	<dd>
	<p>The <a contenteditable="false" data-primary="receiver operating characteristic (ROC) curve" data-type="indexterm" id="id1150"/><a contenteditable="false" data-primary="ROC (receiver operating characteristic) curve" data-type="indexterm" id="id1151"/>ROC curve is a graphical representation that illustrates the trade-off between the true positive rate and the false positive rate at various threshold settings. In the context of our spam example, the ROC curve helps evaluate how well the model can distinguish between spam and nonspam emails across different decision thresholds. A model with a curve closer to the top-left corner has better discriminatory <a contenteditable="false" data-primary="metrics" data-secondary="AI proxy metrics" data-startref="icd604" data-tertiary="model quality metrics" data-type="indexterm" id="id1152"/><a contenteditable="false" data-primary="model quality metrics" data-startref="icd614" data-type="indexterm" id="id1153"/>power.</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Objective Functions" data-type="sect2"><div class="sect2" id="ch06_objective_functions_1736793247822069">
<h2>Objective Functions</h2>

<p><em>Objective functions</em><a contenteditable="false" data-primary="metrics" data-secondary="AI proxy metrics" data-tertiary="objective functions" data-type="indexterm" id="id1154"/><a contenteditable="false" data-primary="objective functions" data-type="indexterm" id="id1155"/> are proxy metrics that evaluate an ML model’s performance during training. They measure how well the model’s predictions match the actual outcomes, guiding the learning process.</p>

<p>The most commonly used objective functions are <em>loss functions</em>, <a contenteditable="false" data-primary="loss functions" data-type="indexterm" id="id1156"/>which calculate the difference between the predicted and actual values for each prediction. The goal is to minimize this loss to improve the model’s accuracy<a contenteditable="false" data-primary="mean square error (MSE)" data-type="indexterm" id="id1157"/><a contenteditable="false" data-primary="MSE (mean square error)" data-type="indexterm" id="id1158"/>. <em>Mean square error</em> (MSE), for example, is a loss function used for tasks that use regression models, called<em> regression tasks</em>. <a contenteditable="false" data-primary="regression tasks" data-type="indexterm" id="id1159"/>An example of a regression task is predicting the demand a store may experience for a given product. Depending on a set of features such as price or utility, a regression model can help predict the demand for the product.</p>

<p>MSE calculates the average of the squared differences between predicted and actual values. Imagine you are tasked with predicting the weekly sales of a new AI product in a popular retail store. Accurate sales predictions help you manage inventory effectively, cutting the losses from overstocking and stockouts. Suppose you predicted sales for Week 1 to be 100 units, but it was 50. The error for Week 1 is 50 units. Each week represents a “loss.” The bigger the error, the higher the loss.</p>

<p>Now, imagine this happening over several weeks. The error differs each week, and you want to penalize weeks with a more significant error. To do this, you square the error. To get a meaningful metric across time, you then calculate the average of the squared errors and the MSE. Loss functions show how accurate a model’s predictions are. Workshopping an algorithm to minimize loss functions will enable systems to work as intended.</p>

<p>When you’re launching a feature or product, you aim to address specific user pain points. Just relying on an ML model is not sufficient. As a PM, you must ensure that your ML model is seamlessly integrated into a product experience so that users can benefit from the technology. Consider, for instance, a <a contenteditable="false" data-primary="content recommendation systems" data-secondary="objective functions" data-type="indexterm" id="id1160"/>recommendation widget on Spotify <a contenteditable="false" data-primary="Spotify" data-type="indexterm" id="id1161"/><a contenteditable="false" data-primary="music streaming services" data-secondary="objective functions" data-type="indexterm" id="id1162"/>that suggests new songs to enrich playlists, or a smart-matching feature on Tinder that connects individuals based on shared hobbies. Fusing the model with the user experience is the key to unlocking the true potential of this AI technology. By considering all of the metrics, you will get a complete picture of your AI feature’s success.</p>
</div></section>

<section class="pagebreak-before" data-pdf-bookmark="Confusion Matrices" data-type="sect2"><div class="sect2" id="ch06_confusion_matrices_1736793247822126">
<h2 class="less_space">Confusion Matrices</h2>

<p>Confusion<a contenteditable="false" data-primary="metrics" data-secondary="AI proxy metrics" data-tertiary="confusion matrices" data-type="indexterm" id="icd605"/><a contenteditable="false" data-primary="confusion matrices" data-type="indexterm" id="icd615"/> matrices are handy evaluation tools highlighting many critical model performance metrics. A <em>confusion matrix </em>is a table of actual and predicted binary features used to evaluate a classification algorithm’s performance. It provides a summary of prediction results on a classification problem. The matrix in <a data-type="xref" href="#ch06_table_1_1736793247815966">Table 6-1</a> compares the actual values with the values predicted by the model.</p>

<table id="ch06_table_1_1736793247815966">
	<caption><span class="label">Table 6-1. </span>Describing the four elements of the confusion matrix for the email classification example</caption>
	<thead>
		<tr>
			<th>Type</th>
			<th>Definition</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>True positive</p>
			</td>
			<td>
			<p>When a prediction is correctly classified as spam</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>True negative</p>
			</td>
			<td>
			<p>When a prediction is correctly classified as nonspam</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>False positive</p>
			</td>
			<td>
			<p>When a prediction is incorrectly classified as spam</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>False negative</p>
			</td>
			<td>
			<p>When a prediction is incorrectly classified as nonspam</p>
			</td>
		</tr>
	</tbody>
</table>

<p>This can be applied to a feature that distinguishes between spam and nonspam emails in your inbox, as shown in <a data-type="xref" href="#ch06_figure_2_1736793247813200">Figure 6-2</a>.</p>

<figure><div class="figure" id="ch06_figure_2_1736793247813200"><img src="assets/bapp_0602.png"/>
<h6><span class="label">Figure 6-2. </span>Visual representation of four possible outcomes when AI classifies emails as spam <span class="keep-together">or nonspam</span></h6>
</div></figure>

<p class="pagebreak-before"><a data-type="xref" href="#ch06_figure_3_1736793247813223">Figure 6-3</a> provides a visual representation of how the actual values and the values predicted by the model translate to the <a contenteditable="false" data-primary="metrics" data-secondary="AI proxy metrics" data-startref="icd605" data-tertiary="confusion matrices" data-type="indexterm" id="id1163"/><a contenteditable="false" data-primary="confusion matrices" data-startref="icd615" data-type="indexterm" id="id1164"/>user <a contenteditable="false" data-primary="metrics" data-secondary="AI proxy metrics" data-startref="icd603" data-type="indexterm" id="id1165"/><a contenteditable="false" data-primary="AI proxy metrics" data-startref="icd609" data-type="indexterm" id="id1166"/>experience.</p>

<figure><div class="figure" id="ch06_figure_3_1736793247813223"><img src="assets/bapp_0603.png"/>
<h6><span class="label">Figure 6-3. </span>AI proxy metrics inbox (source: Dr. Marily Nika)</h6>
</div></figure>
</div></section>
</div></section>

<section data-pdf-bookmark="OKRs for AI Products" data-type="sect1"><div class="sect1" id="ch06_okrs_for_ai_products_1736793247822190">
<h1>OKRs for AI Products</h1>

<p>Now<a contenteditable="false" data-primary="metrics" data-secondary="OKRs" data-type="indexterm" id="icd606"/><a contenteditable="false" data-primary="OKRs (Objectives and Key Results)" data-type="indexterm" id="icd612"/> that we’ve explored various metrics, it’s time to discuss how these metrics can transform into actionable OKRs. The key is to balance all three categories: product health, system health, and AI proxy metrics. A well-rounded OKR framework should include a North Star metric that captures the core value your product delivers, supported by other metrics that track specific aspects <span class="keep-together">of performance.</span></p>

<p class="pagebreak-before">In the rest of this chapter, we will discuss how to craft effective OKRs for your AI product, ensuring that your team focuses on what truly matters to drive success. We’ll also explore examples of setting ambitious, user-focused objectives and defining measurable key results to monitor progress.</p>

<section data-pdf-bookmark="Tying Metrics to Goals" data-type="sect2"><div class="sect2" id="ch06_tying_metrics_to_goals_1736793247822259">
<h2>Tying Metrics to Goals</h2>

<p>Having<a contenteditable="false" data-primary="metrics" data-secondary="OKRs" data-tertiary="tying metrics to goals" data-type="indexterm" id="id1167"/><a contenteditable="false" data-primary="OKRs (Objectives and Key Results)" data-secondary="tying metrics to goals" data-type="indexterm" id="id1168"/> a well-defined set of goals is crucial for the success of any AI product. By understanding and leveraging the AI product metric blend, you can craft a holistic view of your product’s performance and align it with user needs, technical excellence, and business goals. With this comprehensive framework, you’ll be well prepared to measure success accurately and drive impactful product development. Let’s now discuss building actionable OKRs that will help your team stay focused and aligned in this dynamic, AI-driven landscape.</p>

<p>OKRs represent your primary goal: the overarching objective you aim to achieve. It should be ambitious, inspirational, and aligned with the strategic vision of your AI product. You can have several OKRs, each addressing aspects of your AI product’s development and performance. Each OKR consists of multiple<a contenteditable="false" data-primary="KPIs (key performance indicators)" data-secondary="tying to OKRs" data-type="indexterm" id="id1169"/> KPIs or quantifiable metrics that communicate various dimensions of how well the product is performing relative to your OKRs. Your KPIs should be<a contenteditable="false" data-primary="SMART goals" data-type="indexterm" id="id1170"/> specific, measurable, achievable, relevant, and time bound (SMART). These indicators help track progress and determine whether you are on the right path to achieving your goal. Each objective should be comprehensive and aligned with the company’s broader goals.</p>

<p>In <a data-type="xref" data-xrefstyle="select:nopage" href="#ch06_a_framework_for_crafting_ai_product_okrs_1736793247822325">“A Framework for Crafting AI Product OKRs”</a>, I’ll share the framework I use for crafting OKRs for AI products. I like this framework because it provides a structured approach to setting and measuring goals. It emphasizes clarity, focus, and alignment with the product’s core values.</p>

<p>The North Star metric <a contenteditable="false" data-primary=" North Star metric" data-type="indexterm" id="icd612a"/>represents the important KPI. It captures and represents the overall value your AI product is delivering. Each OKR can have multiple key metrics, depending on the complexity and scope of the objective. While the North Star metric represents the primary goal, other supporting KPIs can help measure progress in specific dimensions. Despite having multiple KPIs, each OKR should have only one North Star metric as a precise, focused measure of success that encapsulates the core value the product creates. It is the primary indicator of success for any OKR, reflecting the ultimate impact and progress.</p>
</div></section>

<section data-pdf-bookmark="A Framework for Crafting AI Product OKRs" data-type="sect2"><div class="sect2" id="ch06_a_framework_for_crafting_ai_product_okrs_1736793247822325">
<h2>A Framework for Crafting AI Product OKRs</h2>

<p>This <a contenteditable="false" data-primary="metrics" data-secondary="OKRs" data-tertiary="framework for crafting" data-type="indexterm" id="icd607"/><a contenteditable="false" data-primary="OKRs (Objectives and Key Results)" data-secondary="framework for crafting" data-type="indexterm" id="icd613"/>framework incorporates a mix of metrics covered earlier in this chapter to provide a balanced and holistic view of progress and success. When crafting OKRs for AI products, you should include at least one metric from each metric bucket in the AI product metric cocktail: product health metrics, system health metrics, and AI proxy metrics. This framework helps maintain a structured and strategic approach to goal setting and performance measurement in AI product management. We always want to prioritize delivering impact and measurable value.</p>

<p>Having a reliable OKR framework can act as a foundation. For each AI product or feature, you’d fill in the framework with specifics relevant to your product. The examples provided are just illustrative. Adjust the framework as necessary based on the nuances of the AI product and your organization’s strategic priorities. By integrating diverse metrics, you can craft well-rounded OKRs that drive your AI product’s development and ensure its alignment with user needs, technical excellence, and business goals:</p>

<dl>
	<dt>OKRs</dt>
	<dd>
	<p>Here<em> </em>you state the main goal for the next quarter. This should be user focused and explicitly stated (who is it for?) and needs to state the desired outcome (i.e., enhance the user experience by providing more personalized music recommendations).</p>
	</dd>
	<dt>Specific features</dt>
	<dd>
	<p>What features or changes will you introduce to achieve the objective?</p>
	</dd>
	<dt>North Star (KPI)</dt>
	<dd>
	<p>What <a contenteditable="false" data-primary="North Star metric" data-startref="icd612a" data-type="indexterm" id="id1171"/>is <a contenteditable="false" data-primary="KPIs (key performance indicators)" data-secondary="crafting OKRs" data-type="indexterm" id="id1172"/>the primary metric showcasing product success? While this is typically a singular, focused metric, teams may use supporting metrics to provide additional context.</p>
	</dd>
	<dt>Product health metrics (KPIs)</dt>
	<dd>
	<p>Which<a contenteditable="false" data-primary="product health metrics" data-type="indexterm" id="id1173"/> metrics will measure user satisfaction or the product’s health? Consider multiple relevant metrics such as retention rate, user satisfaction surveys, or feature adoption rates.</p>
	</dd>
	<dt>Guardrail metrics (KPIs)</dt>
	<dd>
	<p>What <a contenteditable="false" data-primary="guardrail metrics" data-type="indexterm" id="id1174"/>potential adverse side effects or risks do you want to monitor and minimize? Use a combination of metrics to track these risks effectively, such as error rates, response times, or user complaints.</p>
	</dd>
	<dt>System health metrics (KPIs)</dt>
	<dd>
	<p>How<a contenteditable="false" data-primary="system health metrics" data-type="indexterm" id="id1175"/> will you ensure that the tool or feature remains reliable and performant? Include multiple metrics to measure aspects such as system uptime, latency, or resource utilization.</p>
	</dd>
	<dt>AI proxy metrics (KPIs)</dt>
	<dd>
	<p>Which AI-specific metrics will you track to assess algorithm performance? Consider metrics such as model accuracy, precision, recall, or user engagement with AI-driven features.</p>
	</dd>
</dl>

<p><a data-type="xref" href="#ch06_table_2_1736793247816002">Table 6-2</a> uses this framework to provide a detailed hypothetical OKR example for a streaming <a contenteditable="false" data-primary="music streaming services" data-secondary="OKRs" data-type="indexterm" id="id1176"/><a contenteditable="false" data-primary="content recommendation systems" data-secondary="OKRs" data-type="indexterm" id="id1177"/>music service’s <a contenteditable="false" data-primary="metrics" data-secondary="OKRs" data-startref="icd607" data-tertiary="framework for crafting" data-type="indexterm" id="id1178"/><a contenteditable="false" data-primary="OKRs (Objectives and Key Results)" data-secondary="framework for crafting" data-startref="icd613" data-type="indexterm" id="id1179"/>​recommendation <a contenteditable="false" data-primary="metrics" data-secondary="OKRs" data-startref="icd606" data-type="indexterm" id="id1180"/><a contenteditable="false" data-primary="OKRs (Objectives and Key Results)" data-startref="icd612" data-type="indexterm" id="id1181"/>system.</p>

<table id="ch06_table_2_1736793247816002">
	<caption><span class="label">Table 6-2. </span>Example OKRs for a streaming music service’s recommendation system</caption>
	<thead>
		<tr>
			<th>Component</th>
			<th>Example</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Objective</p>
			</td>
			<td>
			<p>Enhance the user experience by providing more personalized music recommendations.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Specific feature</p>
			</td>
			<td>
			<p>Introduce three new personalization algorithms based on user behavior, mood, and music trends.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>North Star metric (KPI)</p>
			</td>
			<td>
			<p class="fix_tracking">Increase user engagement with recommended playlists by 25%.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Product health metric (KPI)</p>
			</td>
			<td>
			<p>Reduce the number of users skipping songs within AI-generated playlists by 20%.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Guardrail metric (KPI)</p>
			</td>
			<td>
			<p>Ensure that the overall time spent listening to the music does not decrease by more than 5%.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>System health metric (KPI)</p>
			</td>
			<td>
			<p>Maintain 99% system uptime, and reduce playlist loading times to under one second.</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>AI proxy metric (KPI)</p>
			</td>
			<td>
			<p>Increase the precision of the recommendation algorithm <span class="keep-together">by 15%.</span></p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="ch06_conclusion_1736793247822383">
<h1>Conclusion</h1>

<p>Having a well-defined set of goals is essential for the success of any AI product. I hope the frameworks in this chapter for evaluating product success provide a structured approach to achieving your and your team’s goals. There are always nuanced metrics that apply to specific situations, so it may take time to find the ones that work best for you and your team. That said, the framework for setting actionable goals and defining success will be similar across different projects.</p>

<p>Identifying the relevant metrics to add to your AI product metrics blend will help your teams capture a holistic view of the product’s success. Defining specific objectives and OKRs can help AI teams focus on what truly matters and track their progress with precision. Identifying the right metrics and concentrating on a small set of crucial objectives will help teams maintain clarity and drive toward the most critical outcomes. Setting well-defined goals and measuring success accurately through a robust framework will drive progress and help align AI product development with broader business objectives. Using these frameworks will allow teams to understand and address the multifaceted nature of AI product performance.</p>

<p>In <a data-type="xref" href="ch07.html#ch07_ai_tools_for_product_managers_1736793248032450">Chapter 7</a>, I will cover the most popular product management tools I’ve come across as an AI product <a contenteditable="false" data-primary="metrics" data-startref="icd601" data-type="indexterm" id="id1182"/>manager.</p>
</div></section>
</div></section></body></html>