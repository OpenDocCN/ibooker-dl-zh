- en: 4 Understanding what your users really want
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recognizing indicators of weak understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring chatbot understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing your chatbot’s current state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting and preparing log data to measure chatbot understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting initial log data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good chatbot experience is generally associated with the chatbot identifying
    (understanding) what the user wants. This is one of the key metrics you will use
    to measure performance. Sometimes a chatbot is deployed and has great initial
    understanding (or at least “good enough” for a pilot program). Over time, though,
    you may notice that it is returning wrong answers. Maybe your users are complaining
    more, either directly to the chatbot (“That doesn’t answer my question!”) or in
    the form of survey responses. Engagement could be trending downward while abandonment
    trends upward. You may start hearing from the call center about escalations that
    should have been handled in the virtual assistant. These are all indications that
    your conversational solution might be suffering from weak understanding.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, chatbots should get better over time, but it is not uncommon to see
    a decline in understanding. We want to help you recognize when and why this could
    be happening in your solution. We will explain how to avoid some of the pitfalls
    and plan for common eventualities in the lifecycle of your solution. In this chapter,
    we will explore what it means for your conversational AI to have “good performance”
    in terms of its ability to correctly identify or classify a user’s goal (i.e.,
    to understand the user). We will also offer techniques for preparing data for
    use in measuring a classifier’s performance or assessing generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Fundamentals of understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being understood is a fundamental aspect of human communication. In a conversational
    AI, we use natural language processing techniques to try to understand what our
    user wants or needs. Because the scope of things a user could want is nearly infinite,
    and the way they might combine words to express those wants or needs is also infinite,
    this is a very difficult problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 The impact of weak understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not being understood by a chatbot is probably the biggest source of frustration
    for a user. They came to your chatbot to get answers, and they may get an answer,
    but it may have nothing to do with their question. Perhaps the chatbot instructed
    them to rephrase their question, so they come up with different words to express
    the same goal. Sometimes this works, and other times they get a response asking
    them to rephrase (again!). Oftentimes, as in figure 4.1, your users will end up
    asking for an agent after one or two failures.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F01_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 Accuracy or coverage problems frustrate the user because it takes
    more time—and sometimes multiple contacts—to achieve their goal. It also causes
    the user to lose confidence in the virtual agent.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If this is happening to your users, your chatbot most likely has a problem with
    *accuracy* (the chatbot’s ability to match what it heard against what it knows),
    *coverage* (the range of topics that your solution is expected to know about),
    or both. From the outside, it is impossible to tell which is the underlying root
    cause. For that, you are going to need to collect data. Without that information,
    it is difficult to know what to fix—and fixing the wrong thing can obfuscate or
    compound existing problems. Before you know it, your conversational solution becomes
    costly and difficult to maintain. Worse still, it is not delivering the value
    it promised (by failing to reduce, or perhaps even increasing, the need for human
    intervention).
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest success factors for a chat solution is how an organization
    approaches the ongoing maintenance of the solution. Ideally, the project sponsor
    and support team will have set the expectation that the solution needs iterative
    improvements—especially in the beginning—as it is exposed to more data from real-world
    users. Despite advances in autolearning, large language models, and generative
    AI, chatbots don’t tend to magically get better over time.
  prefs: []
  type: TYPE_NORMAL
- en: Expect to commit support resources throughout the bot’s lifecycle
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Does the organization feel that a chatbot should be a “set it and forget it”
    solution? Is there a lack of commitment to the ongoing care and feeding of the
    virtual assistant? These are the red flags of neglect, and they pretty much guarantee
    eventual failure.
  prefs: []
  type: TYPE_NORMAL
- en: A chatbot is essentially a digital employee. Much like a human resource, it
    requires initial training plus occasional retraining, reinforcement, and the opportunity
    to acquire new skills.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 What causes weak understanding?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are the most common reasons a chatbot will exhibit a decline in understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Manufactured training data (trained examples that do not reflect a representative
    user’s vocabulary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insufficient scope or gaps in topic coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New information in the world that is not passed on to the virtual assistant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of a vetting or gatekeeping process when adding new intents, updating existing
    intents, or changing model inference parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That last point—lack of a gatekeeping process—results in the types of weak understanding
    problems that are the most difficult to resolve. Without oversight by a knowledgeable
    owner or a dedicated model-training team, unvetted changes can quickly compound
    the problem of weak understanding. In traditional classifiers, model updates made
    by someone who is not familiar with the entire training set often introduce duplications,
    intent training conflicts, and unjustified disparities in the volume of training
    examples across intents. Untested model parameter or prompt changes will cause
    unexpected behavior in a generative model.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we saw this happen with a client who had been making changes to their
    classifier training set, growing the total intents from 21 to 53 over the course
    of nine production deployments. The business did not see an effect right away;
    rather, over time the result of these untested changes manifested as poor survey
    results, incomplete journeys, unnecessary escalations, and lots of negative feedback.
    Subject matter experts reported that the bot was giving wrong answers for questions
    that it used to get right. These are classic symptoms of weak understanding, but
    they could not pinpoint exactly when it all started. A series of retroactive experiments
    against their prior versions told the story, which is shown in figure 4.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F02_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 A retroactive assessment of classifier performance shows a hard-won
    lesson on the effect of untested changes over time. Had each version been tested
    as part of a predeployment process, the team would have postponed any version
    updates until the classifier problems were resolved. It took several weeks to
    get the classifier back into good working order.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.1.3 How do we achieve understanding with traditional conversational AI?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditional (non-generative) conversational AI systems are taught by ingesting
    examples of user requests grouped by *intents*, sometimes referred to as *classifications*
    or *clusters*. Intents contain a variety of paraphrases that all express the same
    goal. Some systems also incorporate *entities*, which are like keywords that further
    refine the meaning or specifications of a request.
  prefs: []
  type: TYPE_NORMAL
- en: The conversational logic is configured to identify an intent (or a combination
    of intent + entity) and take an action based on that identification. This action
    could be as simple as answering a question, or it could initiate a complex transactional
    exchange. Table 4.1 shows examples of intents, entities, and potential next steps
    in a conversational exchange.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Example utterances may be handled differently based on the presence
    or absence of entities.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Intent | Entity | Possible next step |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| “How many bags can I check?”  | `Bag_Allowance`  |  | Display bag check policy  |'
  prefs: []
  type: TYPE_TB
- en: '| “I want to book a flight”  | `Book_Flight`  |  | Collect destination  |'
  prefs: []
  type: TYPE_TB
- en: '| “I need a one-way ticket to Costa Rica”  | `Book_Flight`  | Costa Rica  |
    Collect departure details  |'
  prefs: []
  type: TYPE_TB
- en: '| “I’d like to upgrade my seat to first class”  | `Flight_Upgrade`  | first
    class  | Initiate upgrade process  |'
  prefs: []
  type: TYPE_TB
- en: The types of bots that use traditional classification technology tend to be
    topic routing agents, question/answer (FAQ) bots, and, to some extent, process-oriented
    (self-service) assistants. Keep in mind that classification-based bots rely on
    a predefined set of question topics (intents). You need to know in advance what
    questions you expect your bot to encounter.
  prefs: []
  type: TYPE_NORMAL
- en: As a matter of practicality, the range of topics or intents that you teach your
    system will be specific to your domain and your solution’s use case or purpose.
    As solution owners, one of our primary and ongoing tasks is to tune our system
    to correctly understand the greatest volume of user demands. Finding the ideal
    balance between topic breadth and topic depth can be difficult and often involves
    tradeoffs. For example, it is not cost effective to train a classifier to understand
    every possible topic. Furthermore, attempting to do so can weaken its understanding
    of topics that are salient to your users.
  prefs: []
  type: TYPE_NORMAL
- en: When an organization tries to train a classifier to detect every possible topic,
    the classifier’s ability to see clear distinctions across all intents can be diminished.
    If the intents trained in your system aren’t representative of user demand (meaning
    you have a large number of low-volume topics), they tend to cause problems with
    accuracy and confidence. Figure 4.3 illustrates a “long tail” chart; the greatest
    business value for a classifier-based chatbot is typically realized by focusing
    on the high-to-moderate volume requests. Low-volume requests are typically handled
    by some sort of fallback mechanism, such as escalation, search, or generative
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F03_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 As request volume tapers off to the right, the chart has the appearance
    of a long tail. Each use case must define the optimum tradeoff of depth and breadth
    as it relates to topic coverage. The cutoff point for business value is usually
    somewhere in the moderate-volume range. This is not to say that all low-volume
    requests should be excluded, but there may be diminishing returns associated with
    extending your classifier’s coverage for these topics.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prior to initial launch, you need to make some predictions about which topics
    will be most important for your bot to understand. These predictions are often
    based on logs from human interactions, call center metrics, focus groups, surveys,
    or other research or information-gathering activities. Your focus should be on
    training your model to be good at recognizing these requests, as well as any other
    ancillary conversational maintenance intents (such as greetings, chitchat, repeat,
    and escalate). Once your solution is in production, you’ll need to validate those
    predictions by collecting and analyzing data about your conversational interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 How do we achieve understanding with generative AI?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How does a generative AI model achieve understanding? This is a trick question,
    because generative AI does not so much understand the meaning of an utterance
    as it creates new data that looks like the data it was trained on, using the utterance
    as a reference point. This is a nuanced distinction, but with generative AI, we
    try to simulate understanding by instructing a model to assess the input from
    a certain viewpoint and then generate a specific type of output.
  prefs: []
  type: TYPE_NORMAL
- en: Particularly for conversational AI, our goal is to generate output that reflects
    or addresses the user’s request with specificity and/or personalization (not just
    a high-level categorization, such as topic classification or entity extraction).
    Figure 4.4 demonstrates the fundamental difference between classification model
    outputs and generative model outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F04_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 Traditional classification models use supervised learning to predict
    one of several predefined classifications. They look for the intent, or meaning,
    of a user input. Generative models use decoding transformers to create a text
    completion. They predict the next sequence of tokens (loosely, words or characters)
    that are most likely to occur after the user input.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A quick note on LLM foundation architectures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Encoder-only* architectures are best for non-generative use cases, such as
    training predictive models based on text embeddings. They focus on extracting
    meaningful context from inputs and require labeled data for fine tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Decoder-only* architectures are designed explicitly for generative AI use
    cases. They are “trained” in an unsupervised fashion by ingesting large amounts
    of data. They focus on predicting the next token in a sequence and can be instructed
    to perform specific tasks, including classification, question answering, and summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: Some LLM model architectures are *encoder–decoder*, which means they can support
    both generative and non-generative uses cases. These are typically used in scenarios
    where the input is large, but the output is relatively small, such as translation
    or summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike traditional classifier-based AI, there is no predefined list of intents
    that are “in-scope” for the generative model. But like traditional AI, you still
    need to have good command of the domain and of the range of problems your users
    are likely to bring to your bot. This will inform the strategies you employ that
    nudge your LLM to produce responses demonstrating that the user’s input was understood.
    There are several effective tools at your disposal to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Selecting the right model for the job*—Some models are more optimized for
    conversational output (as opposed to generating code or writing an essay or news
    article).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prompt engineering*—This technique supplies a model with inputs in order to
    produce optimal outputs. These inputs might include instructions, context, input
    data, and output indicators. Prompt engineering can often achieve a good simulation
    of understanding, and it can instruct the model to produce output in a conversational
    tone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*One-shot or few-shot prompting*—You can enhance your prompt with one or more
    examples of the output and format you want the model to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Parameter tuning*—Parameters such as temperature, top-*p*, and top-*k* influence
    the randomness and diversity of the generated text. Increasing these values tends
    to increase the “creativity” in a generated response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval-augmented generation (RAG)*—RAG can enhance the perception that
    the bot understands while keeping the generated answer grounded in your domain.
    Many businesses employ RAG in their conversational solutions to ensure that the
    generated responses are based on external, verifiable facts and the latest information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, enterprise conversational solutions most often employ
    generative AI as question-answering (Q&A) bots. Most business-oriented chatbots
    that use this technology are not fully generative—they often employ a hybrid approach
    of classification (with predefined response pairs), task-oriented flows, and generated
    responses. Generated responses may be incorporated into the dialogue design, invoked
    as a fallback option (e.g., when classification fails to predict an intent with
    sufficient confidence), or both.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI can also be used to enhance classification response outputs by
    inserting a personalized greeting or problem summary before delivering a “canned”
    (preconfigured) dialogue response or launching a task flow. Done well, this can
    engage a chatbot user on a deeper level, exhibiting “understanding” with empathy
    by acknowledging the user’s specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Reflect on the solution you are currently building or supporting. Ask yourself
    these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is my solution exhibiting any symptoms of weak understanding, such as
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Giving wrong answers, especially answers that are not relevant or are completely
    unrelated to the input topic
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking the fallback/anything else/escalation routes more often than you would
    expect
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disambiguating, or clarifying the topic, more often than you would expect on
    a seemingly straightforward request (for solutions that employ a disambiguation
    feature)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Producing outdated or incorrect information
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Receiving negative feedback or poor NPS scores
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How was my solution originally trained and tested? If it was deployed, was a
    baseline measurement taken?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the solution been updated to recognize new topics and produce answers that
    are accurate and current?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Who is allowed to make changes to the solution? Are these changes documented?
    Is the solution monitored after a change to ensure the change produces the intended
    effect?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4.2 How is understanding measured?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding, for a chat solution, is typically measured in terms of accuracy.
    For a classifier, that means an ability to accurately predict the intent. For
    generative models, it is the ability to create correct and useful output. There
    are multiple methodologies and tools for measuring how well a solution understands
    user inputs. The approach you take will depend on which technologies your solution
    uses (traditional, generative, or both) and what phase you are currently in (predeployment
    or post).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Measuring understanding for traditional (classification-based) AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classifier performance is measured in terms of accuracy, precision, and recall.
    *Accuracy* is the percentage of correct predictions that were made. *Recall* refers
    to the classifier’s ability to identify the correct intent, while *precision*
    is the classifier’s ability to refrain from giving a wrong intent. Higher accuracy
    usually correlates to a perception of “good understanding.” A chatbot can’t deliver
    a predefined response or invoke the correct process-oriented flow if it does not
    understand the user’s intent.
  prefs: []
  type: TYPE_NORMAL
- en: You can assess your classifier’s performance using some data science techniques,
    such as *k*-fold cross validation or blind testing. *Blind testing* refers to
    the fact that a given test utterance does not already exist in the training set;
    i.e., the classifier has not “seen” the utterance before. Your test set may be
    manufactured, such as with AI-generated data, or representative (constructed from
    actual user utterances pulled from logs). *K*-fold and blind tests can provide
    information about your model’s overall accuracy, as well as report on its recall
    and precision. The metrics produced by such tests help identify where the model
    is performing well and where it might be confused. Chapter 5 contains detailed
    instructions for improving classifier understanding, so we will just give an overview
    of the testing approaches here.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring understanding with k-fold cross validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If your chatbot has not yet been deployed, a *k*-fold cross validation test
    is the easiest and most accessible method for measuring accuracy because it does
    not require additional annotated data. It uses only your existing training set.
    This method essentially measures the internal consistency of your data labeling—a
    high accuracy score mainly indicates that your training examples were grouped
    with other similar examples. The process involves pulling a percentage of data
    out of training, creating a temporary blind test set. The remaining data is used
    to create a temporary classifier. Next, each blind example is run against the
    classifier, and the predictions are scored. Finally, the temporary blind set is
    folded back into the training set. This process is iterated *k* times so that
    every example is used as a training example and as a test utterance, but never
    both at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: A *k*-fold test will give you a prediction of the accuracy of your classifier,
    assuming the data you used to train your model is representative of the inputs
    your model will encounter when it is deployed to production. However, this can
    lead to a false sense of security, especially if your training data is highly
    manufactured or does not quite resemble actual user utterances. Another caveat
    is that small datasets can produce unreliable measurements if there isn’t enough
    data to withhold examples for testing while still training each intent with minimally
    sufficient examples. For these reasons, *k*-fold testing is not the preferred
    testing method once your solution is in production.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring understanding with AI-generated blind test data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Obtaining test data through a generative process is done through the same means
    as obtaining generated training data: you prompt a model to generate variations
    of examples and use them as a “blind” test set. This method is best suited for
    predeployment but may also be appropriate in the early go-live phase to supplement
    gaps in your production logs.'
  prefs: []
  type: TYPE_NORMAL
- en: Like *k*-fold testing, the validity of your accuracy measurements is wholly
    dependent on whether the test data closely mirrors the inputs your model receives
    at production runtime. This approach can be vulnerable to bias and over-fitting.
    As such, we advise caution and suggest you validate your generated data against
    production logs as soon as they are available.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring understanding with representative blind test data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If your chatbot has already been deployed, the production logs are one of your
    key tools for assessing your chatbot’s accuracy. These logs contain truly representative
    data about what your users ask for and how they phrase these requests. By “representative,”
    we mean both a realistic volume distribution of the intents triggered in your
    system as well as utterances that capture the user’s goal—in whatever combination
    of words comes naturally to them.
  prefs: []
  type: TYPE_NORMAL
- en: Using production logs will produce the least biased testing data, but it also
    requires a degree of upfront, manual effort. That effort does pay off, however,
    as you will have created a reusable asset for taking measurements of future changes.
    You’ll need to obtain a sample of these logs and review the customer inputs (utterances)
    against the intents returned by your system. This data will need to be annotated
    by a human who can identify the definitive correct (aka “golden”) intent that
    the utterance belongs to. Your initial annotations will give you a baseline accuracy.
    This data will then be used to build your *representative blind test set*, which
    is essentially a list of test questions and the answer key all in one file.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best method for your situation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The cost and effort tradeoffs for each method are entirely dependent on the
    size and current phase of your solution (predeployment or post):'
  prefs: []
  type: TYPE_NORMAL
- en: '*K*-fold cross validation may be seen as “cheap and easy” because it does not
    require human annotation beyond the task of the initial annotation done for training
    purposes. However, there may be an API cost to running your experiment *k* times.
    This cost is usually negligible for smaller systems but could result in thousands
    or tens of thousands of API calls per experiment for larger systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generated test datasets incur the cost of generating the data in addition to
    the API cost of running an experiment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A representative blind test set may have a lower API cost for running an experiment
    (compared to *k*-fold, assuming your test set is smaller than your training set),
    but the cost of human annotation can be significant. This also requires that the
    solution is in production, interacting with real users. The benefit is that the
    experiment results are going to be more meaningful than *k*-fold and generated
    test set results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, there are three primary methods to measure your classifier’s ability
    to understand users. The method you choose should align with your current stage
    of development or deployment, as outlined in figure 4.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F05_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 *K*-fold cross validation and generated test data are suitable for
    situations where representative data is not available. Once a solution is deployed
    to production, representative blind test data will produce the most reliable measurement
    of your classifier’s ability to understand.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.2.2 Measuring understanding for generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Measuring whether a generated answer has demonstrated “good understanding”
    is an onerous task, and automated test approaches are still emerging. Our challenge
    is the nature of generative AI: every generated response is possible or likely
    to be unique to each user input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you deploy a solution with generative AI, you should define what it
    looks like for your bot to demonstrate good understanding. For generative conversational
    AI, we suggest you define “good understanding” by the following dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: The generated answer matches any specified output format or style, including
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positioning of the bot (the purpose and viewpoint of the bot’s persona)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The designated tone and personality of the bot’s persona
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated answer is appropriate for the user’s input in terms of content
    length and structure (for example, does the nature of the user input require a
    response that is a short answer, step-by-step instructions, or a detailed essay?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated answer is free from false information (hallucinations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated answer is free from hate, abuse, profanity, bias, and discrimination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated answer is free from damaging information—even if true—such that
    a company would be legally liable or incur damage to their reputation (for example,
    negative commentary about a competitor or leaking sensitive data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated answer is resilient to prompt-injection attempts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated answer is correct and complete and either successfully terminates
    a flow or progresses the flow to a next step or the next best action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your solution has already been deployed, obtain a representative sample of
    your logs. Perform a manual review to assess your bot’s level of understanding.
    Each generated answer will be judged as correct, sufficient, or appropriate against
    the dimensions you have defined for the solution.
  prefs: []
  type: TYPE_NORMAL
- en: This is, of course, time-consuming, but the effort will pay off. Your annotated
    set can be used as a golden test set for future improvements. This test set will
    give you a baseline for tracking the effect of changes to your model parameters
    (such as temperature, top *p*, top *k*) and other LLM configuration settings.
    These samples can also inform any few-shot examples (sample inputs paired with
    desired outputs) you include in your prompt engineering or fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Measuring understanding with direct user feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One way to measure good understanding at scale is to incorporate an answer feedback
    mechanism directly in the user experience, such as a thumbs up/down reply option.
    This method can be used for both traditional and generative solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be mindful of how often you solicit feedback, and know what purpose your feedback
    serves. Which aspect of the experience is the rating meant to reflect: satisfaction
    or dissatisfaction with a particular answer (for a question/answer use case),
    the self-serve process and its outcome (for a process-oriented bot), or the conversational
    experience as a whole?'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Explore and document your solution (or review and update it as needed), with
    emphasis on the components most responsible for demonstrating understanding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For classifiers, this means auditing the training data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For solutions that include search and retrieval, audit the source documents
    or URLs, any supplemental document enrichments, and the ingestion schedule to
    ensure that your knowledge base contains the most relevant and up-to-date information.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For generative AI solutions, audit the dialogue flows that invoke generated
    responses, and map the prompts, parameters, and LLM settings to their intended
    outcomes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflect on your current test methodologies, if any. Do you have any historical
    test metrics that can be correlated to current symptoms of weak understanding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think about the test methodologies presented in this section. Which approach
    is optimal for the current phase of your solution lifecycle?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4.3 Assessing where you are today
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start making plans for improvements, you will want to perform an
    assessment of where the solution stands in terms of its ability to accurately
    identify the users’ goals and needs. The nature of your assessment will depend
    on which technology your solution uses. Classification and generative models perform
    very different functions and therefore have different aspects to be assessed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Assessing your traditional (classification-based) AI solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For traditional AI, start by reviewing the training set to orient yourself
    to the domain and current scope:'
  prefs: []
  type: TYPE_NORMAL
- en: How many classifiers are used in your solution?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many different intents does the system (or each classifier) handle?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How unique is each intent?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do the training examples in any intent seem to overlap with other intents?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the range of topics (intents) align with your impression of the chatbot’s
    purpose?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the solution handle input it does not understand?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the complexity of the dialogue? Are there complex flows, backend integrations,
    or search integrations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be helpful to visualize your classifier training data volume in chart
    form. Figure 4.6 shows an example training set. There isn’t a lot of information
    to be gleaned just yet, but this will give us a basis for comparison once we assemble
    our test data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F06_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 This classifier has 13 intents. The training example counts range
    from 7 to 30.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In general, we expect our intents with higher counts of training examples to
    be more popular. We want our most popular topics to be understood a majority of
    the time. Higher-volume intents may also represent topics that handle a greater
    variety of phrases. For the most part, we don’t like to see a huge disparity in
    volumes across the training set. For instance, a training set that has some intents
    trained with hundreds of examples while others have just a handful might exhibit
    performance problems such as over-selection (frequently selecting a wrong intent
    due to the bias of training volume).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Assessing your generative AI solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With generative AI, as in traditional AI, you need to understand the domain
    and scope your bot operates within. However, instead of concerning yourself with
    classifications of input, you need to appraise the data sources that your model
    will draw its answers from when it produces an output. Is generative AI used to
    produce answers or responses in your solution? If so, familiarize yourself with
    the circumstances:'
  prefs: []
  type: TYPE_NORMAL
- en: Are answers generated for every user input?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you call for generated answers as a fallback option for your classifier?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you call for generated text to supplement a classification-based “canned”
    answer in the dialogue?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does your solution make use of more than one LLM, such as different models for
    different types of responses, multiple language support, etc.?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does your solution make use of prompt engineering, prompt tuning, fine tuning,
    or other customized settings? Is this documented anywhere, along with the outcome
    goals for which each setting was originally implemented?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does your solution make use of RAG? If so, what is that data source? How often
    is it updated? Does it contain additional data enrichments?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Assess your solution using the criteria we described in this section (according
    to the type of AI you use).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have performed your initial solution assessment, be sure to document
    its current state—this will be your baseline system configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you follow along with the improvement recommendations and examples given
    throughout this book, be prepared to record your changes in a way that will help
    you correlate any updates you make to the subsequent performance measurements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4.4 Obtaining and preparing test data from logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the rest of this chapter, we’ll assume that you do have a production system
    and access to the logs. We will show you how to obtain and prepare that data to
    create an asset you can use to measure the current state (and to validate future
    changes).
  prefs: []
  type: TYPE_NORMAL
- en: There’s a bit of initial work involved to build a test set from production logs.
    Figure 4.7 shows the major tasks involved in preparing data for testing (or training).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F07_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 Once you obtain some data, each utterance should first be sorted
    into buckets to identify potential candidates; this will separate the good, usable
    test data from the bad or irrelevant user inputs. The data may also need to be
    scrubbed to fix problems like personal identifiable information (PII). After that,
    the data will need to be annotated (for classifiers, it will need to be labeled
    with the correct intent; for generative AI, it will need to be associated with
    an ideal output response). Finally, the data will need to be converted into one
    or more sets that can be consumed by your testing tool.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.4.1 Obtaining production logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ideally, you will have access to production logs that span a full year or more.
    This will help ensure that your test set will have a true representative sample
    of the range of topics your bot encounters for the various seasons and events
    that influence your industry. Collect log samples from various weeks or months
    throughout the year. If your solution is newer, expect to refresh your test sets
    more frequently during your solution’s first 12 to 18 months.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have obtained some production logs, you may find it easiest to convert
    this data into a CSV or Excel file (if it hasn’t already been updated). We find
    it most useful to transform the data into one row per conversational exchange
    (a user input and a bot output), grouped by conversation ID. Depending on the
    timeframe you select, the volume of users, and the complexity and purpose of your
    solution, your file may have just a few hundred rows of data, or it could have
    100,000 rows or more of conversational exchanges.
  prefs: []
  type: TYPE_NORMAL
- en: One simple shortcut for reducing the volume to a manageable set is to select
    the first user utterance in each conversation. This may not work in all cases,
    but figure 4.8 shows that it is often a reliable way to harvest useful data from
    your logs. In a natural language-driven exchange, users tend to express their
    most important need in the initial input. If your average conversation lasts ten
    turns, a conversation log with 100,000 rows of raw data could be reduced to about
    10,000 rows of data to review. Deduplication can often further reduce this by
    a few thousand. This is a very workable volume and will usually contain rich and
    diverse examples that you can use for testing your solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F08_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 Raw chat logs show that a user’s primary goal is often captured in
    the first turn of a conversation, but sometimes it occurs as an additional request
    later in the conversation or after exchanging a pleasantry. It might even follow
    an opt-out request. Selecting the first row will usually yield enough usable data
    while reducing the amount of time your annotators spend sorting through the utterances
    that aren’t useful to the classifier, such as button clicks, common responses,
    and PII or other user-specific information. (The structure of your logs may vary
    by tool.)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.4.2 Guidelines for identifying candidate test utterances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Whatever you do to obtain and preprocess your logs, your next task is to identify
    potential blind test candidates. We treat this as a “first pass” exercise: just
    determine if an utterance is *potentially* usable. Additionally, we counsel the
    reviewers not to over-analyze what they see; if you cannot make a determination
    about any given utterance within a minute or so, discard the utterance and move
    on. (If you’re feeling really conflicted or sense a pattern, mark it for later
    review and move on.) We use the following criteria to identify potential test
    candidates from production logs, along with any special handling instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the utterance unintelligible?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the utterance completely unrelated to the domain?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the utterance ambiguous?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the utterance contain multiple intents?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the utterance related to the domain but out of scope?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the utterance express a goal that is in domain and in scope?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Is the utterance unintelligible?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Maybe a cat walked across the keyboard, or the user just mashed the keys in
    a fit of frustration. Perhaps the speech-to-text technology mistranscribed the
    caller’s question into an unintelligible mess. Speech solutions can also pick
    up background noise and conversations, especially if they are not properly tuned
    for the environment. Your file may contain a number of user inputs that just don’t
    make any sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are examples of unintelligible or unrelated utterances:'
  prefs: []
  type: TYPE_NORMAL
- en: “does it school” (Incoherent—if this came from a voice solution, it was potentially
    a speech mistranscription.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “she didn’t she said there are four and only gave us one yes you can do that
    I’m about to catch my flight and I’ll check on it when I get to the office” (Potentially
    a speech transcription of a background conversation on the caller side.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “klewtkhaccalifornia liense” (Likely typos, severe enough to render the utterance
    unintelligible.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lines can be excluded from your blind test set. Any recognizable patterns,
    such as possible speech transcription problems, should be set aside for further
    evaluation or forwarded to the appropriate team.
  prefs: []
  type: TYPE_NORMAL
- en: Is the utterance completely unrelated to the domain?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You may occasionally come across questions that are intelligible but entirely
    off-topic for the domain or the bot’s intended purpose. For example, if your solution
    is designed to help electric utility customers manage their account and services,
    you can exclude questions about pop culture trivia if they happen to appear in
    the logs.
  prefs: []
  type: TYPE_NORMAL
- en: Though you could configure a solution to send unrecognized topics to an LLM,
    these utterances do not belong in your classifier test set because a golden intent
    cannot be assigned. Such utterances could be used in negative testing, which will
    help you understand if your solution is appropriately identifying when it should
    *not* attempt to answer.
  prefs: []
  type: TYPE_NORMAL
- en: Is the utterance ambiguous?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perhaps you’ll find a single word or a short phrase that is related to the domain
    but doesn’t express a clear goal. For example, if a user of a banking chatbot
    simply says, “account,” what do they want? Do they want to open an account? Close
    an account? Check an account balance? Who knows?
  prefs: []
  type: TYPE_NORMAL
- en: A subset of ambiguous utterances may include responses generated by a button
    click or as part of an information-gathering flow. (If you selected the first
    natural language utterance of every conversation, you might not see these.) These
    are generally not useful for the classifier’s performance testing unless they
    align with an intent that is used within a flow. Include such utterances only
    when appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are examples of ambiguous utterances:'
  prefs: []
  type: TYPE_NORMAL
- en: “driver license” (Perhaps relevant to the domain, but no clear goal is expressed.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “that one” (An anaphor referring to contextual information that appears to have
    been provided in an earlier statement but may have lost its meaning as an individual
    utterance.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “2” (Could refer to a button choice or phone channel selection, or to an amount
    or quantity provided as a response to the previous question.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most cases, these utterances should not be included in your classifier accuracy
    test because they likely will not align with any single intent, but rather multiple
    intents. They are not meaningless, however. Set these aside to understand how
    often your users communicate in this way. Determine whether your other chatbot
    features, such as disambiguation or clarifying questions, are handling them appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Does the utterance contain multiple intents?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most classifier-based chatbots perform best when they are given one goal at
    a time. Utterances that express multiple valid, distinct goals should be excluded
    from your classifier accuracy test set because you cannot definitively assign
    a “correct” intent.
  prefs: []
  type: TYPE_NORMAL
- en: The exception to this rule would be if your solution has a disambiguation mechanism.
    Disambiguation is a way to clarify the user’s primary goal by presenting the top
    *n* intents identified by a classifier. For these solutions, you may want to run
    your multi-intent utterances against your classifier to verify that all intents
    listed would be presented with the appropriate disambiguation choices.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some examples of utterances with multiple intents:'
  prefs: []
  type: TYPE_NORMAL
- en: '“Do you have the COVID booster? How can I make an appointment?” (Two goals
    expressed: 1) availability of vaccine booster, 2) make an appointment.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“I want to update the address on my driver’s license and find out what is required
    to get a commercial driver’s license.” (Two goals expressed: 1) update address,
    2) get information for obtaining a CDL.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“I currently have 95,000 loyalty points. Do they expire? How many more points
    do I need to reach Platinum status? Can I purchase points for this?” (Three goals
    expressed: 1) find out if reward points expire, 2) find out the delta between
    current point balance and next level reward status, 3) get information about purchasing
    points to reach a higher status.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“I want to talk to an agent about reporting a stolen vehicle.” This is very
    common. A user will often pair a request for an agent along with their true goal.
    If both intents exist in your classifier training set, you can handle such utterances
    in one of two ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exclude these as candidates if it is impossible to label a single “correct”
    intent.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include these candidates, but label them according to the “preferred” intent.
    (A preferred intent might be the self-service option if containment is a priority
    and the competing intent would escalate.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As with ambiguous utterances, these should be set aside and evaluated separately
    to better understand your users. You may want to devise additional strategies
    to handle these situations if they are occurring often. If users tend to ask related
    questions, or they pair common requests in a single utterance, your output responses
    in these intents could be updated to anticipate or meet all of the needs. For
    the first example we gave—“Do you have the COVID booster? How can I make an appointment?”—your
    answer regarding booster availability may include a link to make an appointment.
  prefs: []
  type: TYPE_NORMAL
- en: A word about handling multiple intents with classification models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We have seen extensive and heroic attempts to handle multiple intents programmatically
    in conversational AI solutions. This usually involves logic to collect the top
    *n* intents and store them in context, and then more logic to present the additional
    topics after the first one is answered. In most cases, the result is an over-engineered
    solution that is brittle, difficult to scale, or simply wasted effort. This approach
    also has a major flaw: such logic cannot reliably distinguish between an utterance
    that truly contains multiple goals and an utterance that contains a single goal
    that may have triggered multiple intents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many modern chatbot frameworks provide automated topic disambiguation (for
    example, “Did you mean: [Intent 1] [Intent 2] [Intent 3]”). Our general recommendation
    is to allow the disambiguation feature to do its job. Sometimes, this means that
    the user must ask their questions or state their goals one at a time. The frequency
    and importance of such scenarios is usually not worth the effort required to build
    and maintain custom logic for handling multiple intents in a classification-based
    chatbot.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI is typically much better at handling multiple intents than classification-based
    solutions, so you can include these utterances as candidates in a test set if
    your solution has this capability.
  prefs: []
  type: TYPE_NORMAL
- en: Is the utterance related to the domain but out of scope?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You are likely to come across utterances that express a single, clear goal that
    is relevant to the domain, but the current solution is not equipped to handle
    them. For example, a banking chatbot may allow users to check an account balance
    but may not be trained to recognize requests about interest rates. An airline
    chatbot may be versed on airline policy but not be grounded in facts about airport
    security.
  prefs: []
  type: TYPE_NORMAL
- en: Such questions may be very reasonable from the user’s perspective, and gaps
    in topic coverage often lead to frustration for your users. This is especially
    true if you don’t have a generative AI or search fallback. If your bot responds,
    “I’m sorry, I don’t understand. Please rephrase your question,” no amount of rephrasing
    will get the user to a satisfactory answer. How should these be handled?
  prefs: []
  type: TYPE_NORMAL
- en: If your classifier does not have any trained intents to handle such requests,
    these should set aside. On further review, they may be grouped into topics or
    categories, but they will be excluded from your test set for now because a golden
    intent cannot be assigned. Monitor these topics for volume and add them to your
    improvement backlog as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if your generative solution is not prepared to answer such questions
    (for example, the document repository in a RAG solution does not have content
    to address the topic), set these aside for the time being, but monitor the volume.
  prefs: []
  type: TYPE_NORMAL
- en: Does the utterance express a goal that is in domain and in scope?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Score! Questions or requests that are in scope for your solution and domain
    belong in your golden test set.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Preparing and scrubbing data for use in iterative improvements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you’ve never seen production logs for a chatbot, you will be surprised at
    how messy they are. You are going to see a lot of bad or informal grammar, misspelled
    words or typos (on a text-based channel), speech mistranscriptions (on a voice
    channel), and potentially various forms of personal identifiable information (PII).
    Here’s how we recommend handling these.
  prefs: []
  type: TYPE_NORMAL
- en: Bad or informal grammar
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the most part, leave it be! There is a lot of diversity in how humans express
    themselves. The user may not know exactly how to communicate what they need—especially
    to a machine. If a goal can be identified, it is a representative example and
    should be generally preserved as is.
  prefs: []
  type: TYPE_NORMAL
- en: Typos and misspelled words
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unless a typo or misspelled word significantly changes the meaning of the overall
    phrase, leave it as is. Commonly misspelled words are representative of how your
    users communicate. Your classifier should be able to give a good answer whether
    the user asks, “What’s the difference between loan balance and principal?” or
    “whats teh diffrence between loan balance and principle?”
  prefs: []
  type: TYPE_NORMAL
- en: Proper case and punctuation are generally ignored by a classifier, but you may
    need to verify this with your technology platform.
  prefs: []
  type: TYPE_NORMAL
- en: Speech mistranscriptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If your solution uses speech-to-text (aka automated speech recognition), you
    won’t encounter typos, but you probably will see unexpected words that are most
    likely the result of a speech mistranscription. The first line of attack is to
    train your speech models, if possible. The underlying technology of a chatbot
    classifier is text-driven, so it is best to have the most faithful representation
    of the user’s utterance before it hits the text classifier.
  prefs: []
  type: TYPE_NORMAL
- en: If you find that the speech models are still consistently mistranscribing words
    that are significant within your domain, include these in your test set (and ultimately,
    you will probably end up supplementing your training data). For example, for an
    electric utility company, we consistently saw an important domain term, “residential,”
    mistranscribed as “presidential.” As speech model updates can take longer to implement,
    and this was causing loss of call containment, an immediate fix was to add “presidential”
    as a synonym to our chat solution. Another example was the mistranscription of
    “VIN” as “BIN” for a use case that needed to understand “vehicle identification
    number.” For this, we made sure that the training data contained both variations.
    We also preserved the mistranscriptions for our testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Personal identifiable information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You may also find various forms of PII, such as names, phone numbers, physical
    or email addresses, social security numbers, account numbers, etc. These do not
    belong in your training or test data. Ideally, this information would be masked
    in your logs, but even this technology is not perfect. If your solution has a
    PII masking function, you should replace any real data with the same type of masking
    characters (e.g., ###-###-#### for a ten-digit phone number). If not, either remove
    the PII entirely, or replace it with an obviously fictionalized representation,
    such as “username@email.com.”'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 The annotation process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After you have narrowed down your data to utterances that express a clear goal
    that belongs in your domain (and have scrubbed them where appropriate), they need
    to be properly annotated for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Annotating a golden test set for traditional (classifier-based) AI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Annotating a test set for a classifier involves labeling each utterance with
    the appropriate intent. This task is a little easier said than done, and it’s
    where you will spend the most time building your test set.
  prefs: []
  type: TYPE_NORMAL
- en: It’s fairly easy to identify and discard an unintelligible or ambiguous user
    input. However, once you know an utterance belongs in your domain, it takes a
    bit more time to label it with the correct intent. The person or team tasked with
    annotating (labeling each utterance with the correct intent) will need to be familiar
    with the current training data. This process will definitely expose problems with
    overlap in your intents, as your human annotators will be stuck with the question
    of how to label an utterance.
  prefs: []
  type: TYPE_NORMAL
- en: A team might take several approaches to complete the work of labeling data for
    testing or training. Sometimes a single person is tasked with the job. Sometimes
    a whole team will try to take this on. When that happens, they often think that
    a “divide and conquer” approach is most efficient. In our experience, this can
    lead to problems that take longer to resolve.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, everyone would sit in the same room and judge each utterance
    together. This approach facilitates discussion regarding the purpose of each intent.
    All annotators need to understand the criteria used to differentiate intents that
    share a lot of the same key words but have different goals. Another equally valid
    approach is to have multiple annotators judge the same data separately (or at
    least a percentage of overlapping data) and compare any differences to reach a
    resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one shortcut we wouldn’t hesitate to take if your logs include the
    intent that was predicted at runtime for each utterance: make a first pass and
    judge whether the predicted intent was correct. Then you need only judge and label
    the remaining incorrect utterances with the correct intent.'
  prefs: []
  type: TYPE_NORMAL
- en: This exercise may take anywhere from a few hours to several days, and it can
    be taxing on both your vision and your cognitive load. As a first run, instruct
    your annotators to make their best judgment and move on. If it takes more than
    sixty seconds to judge an utterance, skip it and come back later. It is also important
    to take breaks every hour or two. It helps to walk away and come back after a
    period of refresh.
  prefs: []
  type: TYPE_NORMAL
- en: Could I just use an LLM to do all that work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you are building your first classifier, you could certainly run utterances
    against an LLM as a first pass at labeling or classifying your data. However,
    if you already have production logs, there will be no added value to running the
    utterances against a separate classification LLM because you still need a human
    judge to review the classifications produced by this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have annotated the test set, you will have a golden set of human-judged,
    labeled data. Depending on your use case, this could include a few hundred to
    a few thousand utterances. This asset will give you some immediate information
    about your classifier’s current accuracy. It will also be used to help tune your
    system.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing you need to do is convert your data into a file that can be consumed
    by your testing tool. This will produce an asset that can be used to measure the
    effect of future updates. The format may vary by tool, but it will typically be
    a text or CSV file that contains a row for each test utterance in one column and
    the golden intent in the other column. Table 4.2 shows how a test set might look.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2 Sample test set with one utterance/intent pair per row
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Golden intent |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| I want to speak with a real person  | `Request_Agent`  |'
  prefs: []
  type: TYPE_TB
- en: '| Can I talk to a manager  | `Request_Agent`  |'
  prefs: []
  type: TYPE_TB
- en: '| Get me customer service  | `Request_Agent`  |'
  prefs: []
  type: TYPE_TB
- en: '| Are you open on Sundays  | `Office_Hours`  |'
  prefs: []
  type: TYPE_TB
- en: '| What time do you open  | `Office_Hours`  |'
  prefs: []
  type: TYPE_TB
- en: '| When does your office close  | `Office_Hours`  |'
  prefs: []
  type: TYPE_TB
- en: '| What are your weekend hours  | `Office_Hours`  |'
  prefs: []
  type: TYPE_TB
- en: Annotating a golden test set for generative AI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Creating a test set to measure generative AI involves judging the quality of
    the answer produced by your solution (if you are working with production logs)
    and updating or replacing it with the ideal answer, according to the dimensions
    you previously defined for your solution. Subject matter experts will need to
    review each example output to ensure that it is factual and complete, represents
    the brand, and reflects the purpose and positioning of the virtual agent persona.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have reviewed the output, you will have a set of utterances paired
    with a golden answer or response. This asset will give you some immediate information
    about the quality of your generated responses. It will also be used to tune your
    prompts and LLM configurations.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing you need to do is convert your data into a file that can be consumed
    by your testing tool. The format may vary by tool, but it will typically be a
    text or CSV file that contains a row for each test utterance in one column and
    the golden response in the other column. Table 4.3 shows a sample test set.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.3 Sample test set with one utterance/answer pair per row
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Utterance | Golden response |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Can I bring a snowboard on my flight as checked baggage?  | You can bring
    one set of snowboard equipment as a checked bag. The set must be in one bag and
    can include up to two snowboards and one snow boot bag. If the set weighs more
    than 50 pounds (23 kg), you’ll have to pay overweight bag fees.  |'
  prefs: []
  type: TYPE_TB
- en: '| How long do I have to wait to get my refund?  | Credit card refunds will
    be processed within five business days of the request. All other refunds will
    be processed within 20 business days of the request.  |'
  prefs: []
  type: TYPE_TB
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Obtain data from your own logs, and identify candidate test utterances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scrub the data as needed to remove PII.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assess the classification predictions or generated answer content. Record these
    outcomes as baseline performance measurements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign a golden intent or ideal response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the file in a format that can be consumed by your testing tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4.5 What does the data tell us?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your logs included the original intent prediction or generated answer, you
    now have what is needed to calculate a baseline measurement of your solution’s
    current accuracy rate for understanding. (Divide the number of correct predictions
    or answers by the total candidates judged.) Your annotated utterances will show
    you the range and frequency of topics your users present to the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Interpreting annotated logs for traditional (classification-based) AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For classifier-based systems, you might be interested in looking at the volume
    distribution across your intents. How does this compare to your training example
    volumes for each intent? Figure 4.9 shows an idealized, fairly balanced distribution
    of training examples compared to occurrences seen in the logs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F09_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 The dark bars represent the number of training examples in a system.
    The light bars represent the number of annotated utterances for each intent. If
    your chart follows a similar pattern, your training priorities are probably in
    good alignment with the demands of your solution.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A stark disparity between trained examples and actual occurrences in the logs
    is not indicative of problems in and of itself, but it can inform your priorities
    if your accuracy is low. Figure 4.10 shows an example of annotated utterances
    that are wildly out of alignment with how the system was trained.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH04_F10_Freed2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 The training example counts (dark bars) show a large disparity across
    many intents, as compared to the annotated log data (light bars). Without accuracy
    numbers for each intent, we cannot immediately tell if this disparity is having
    a negative effect. However, we can make some observations, such as 1) the first
    five intents are not nearly as important to our users as we thought they might
    be, and 2) the intents with the highest volume in our logs (the light bars for
    intents 6, 10, 11, and 12) may be a lot more important to our users than we predicted.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You should also review the volume of utterances that were judged to be in domain
    but out of the current scope. (These would have been identified and set aside
    as part of the preparation tasks described in section 4.1.4.) Does there appear
    to be a demand for topics that the classifier is not currently trained on? A misalignment
    between what your users expect to be able to ask and what your classifier is trained
    to recognize contributes to a perception of weak understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Your overall accuracy provided a big-picture view of the solution’s ability
    to understand. The next step is to drill down into the specific intents. You might
    start by looking at the poorest performers that are also high-volume/high-value
    in your solution. In chapter 5, we will explore in depth the process for improving
    classifier understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Interpreting annotated logs for generative AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your annotated logs for a generative AI solution will give you a picture of
    the range of questions and requests that users are providing. Throughout the annotation
    process, you may have discovered gaps in coverage about the domain. You may also
    have gained a better grasp of how prompt engineering or fine-tuning improvements
    could make your generated answers better. If your solution employs RAG, you might
    start correlating the quality of your answers to the documents in your repository.
  prefs: []
  type: TYPE_NORMAL
- en: Your overall accuracy provided a big-picture view of the solution’s ability
    to understand. In chapter 6, we will explore in depth the process for improving
    your generative AI so that it conveys good understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3 The case for iterative improvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you should be armed with the data you need to begin planning
    improvement cycles. Your performance findings will serve as a roadmap for improvements.
    Keep in mind that this is an iterative process. You will make changes. Then you
    will take measurements to determine whether your change had a positive, neutral,
    or negative effect on understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also important to note that your blind or golden test set will need to
    be refreshed throughout the lifecycle of your solution. Recall that one of the
    reasons a chatbot can become inaccurate is due to new information in the world.
    These are some examples we have seen:'
  prefs: []
  type: TYPE_NORMAL
- en: The global COVID-19 pandemic, which changed the way nearly everyone worked,
    navigated public spaces, and supported their families.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New legislation passed, resulting in government organizations getting related
    questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New products on the market or product recalls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A company experienced a data breach, and once the news broke, the chatbot was
    bombarded with questions like, “Is my data safe?” and “I want to know more about
    the hack.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plan to review your logs on a regular basis. Depending on the volume of your
    solution, that might start out daily right after launch, then weekly, monthly,
    and quarterly. Don’t forget to update your test sets according to the changes
    you make:'
  prefs: []
  type: TYPE_NORMAL
- en: If new intents are added to your system, new utterances need to be added to
    your test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If intents were merged or split as part of your improvement efforts, the affected
    intents will need to be updated in your test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If new areas of coverage are added to the knowledge base your generative solution
    references, your test set should include validations for this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your solution adds new LLM scenarios or prompt customizations, these should
    be reflected in the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Review your annotated data and reflect on the findings. Are there areas that
    show poor understanding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If so, what would you hypothesize is the root cause?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there more than one root cause?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you prioritize the improvements needed to achieve better understanding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chatbots demonstrate good understanding when they identify what a user wants
    and they provide a satisfactory answer or progress the user toward their goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For traditional AI, understanding relies on at least two mechanisms: correct
    classification of an intent and an ability to deliver an output based on that
    classification. (Additional mechanisms, such as entity detection or context, may
    modify or personalize outputs.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For generative AI, understanding relies on the utterance and any accompanying
    prompt to create a response meant to address a user’s question or goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weak understanding is detrimental to business value and is often exhibited by
    a chatbot returning wrong answers or no answers at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can’t assess the performance of your chatbot without first collecting some
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbot understanding is usually measured in terms of accuracy or the rate at
    which the solution delivers a correct answer or takes the correct action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple tools and methods for measuring understanding. Some are dependent
    on the type of AI and/or the current phase, whether predeployment or post.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A representative golden test set, curated from real user utterances (production
    logs), can be used to measure the bot’s baseline performance and can be converted
    into a reusable asset to measure the effect of future changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should plan to monitor and retrain your solution throughout the life of
    the bot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates to training may require corresponding updates to the blind test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
