- en: Chapter 10\. The Future of LLMs and LLMOps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章\. LLMs和LLMOps的未来
- en: In the next decade, the future of LLMOps, LLMs, NLP, and knowledge graphs will
    converge in ways we can barely imagine today. Imagine AI systems no longer as
    distant tools but as deeply integrated into every facet of our lives. Even the
    most popular LLMs today are [somewhat clunky iterations](https://oreil.ly/O3IFm),
    but in the near future, I believe they will be [refined](https://oreil.ly/yW4T3)
    to a point where their understanding of language will [rival human intuition](https://oreil.ly/k8EyH).
    This is because of emergent traits in LLMs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的十年里，LLMOps、LLMs、NLP和知识图谱的未来将以我们今天几乎无法想象的方式汇聚。想象一下，AI系统不再是遥远的工具，而是深深地融入我们生活的每一个方面。即使是今天最受欢迎的LLMs也有些笨拙的迭代（[链接](https://oreil.ly/O3IFm)），但在我看来，在不久的将来，它们将被[精炼](https://oreil.ly/yW4T3)到一定程度，其语言理解能力将[与人类直觉相媲美](https://oreil.ly/k8EyH)。这是因为LLMs中出现的特性。
- en: Currently, the main way users interact with LLMs is through text-based chats,
    but in coming years, LLMs won’t just be answering questions; they’ll be engaging
    in complex problem-solving, offering insights, and pushing the boundaries of creativity
    itself. For example, in September 2024, OpenAI released Advanced Voice Mode for
    its ChatGPT application, which can detect voice tone—including sarcasm. Much of
    this work is related to impending innovations across the infrastructure stack.
    Meta [recently wrote](https://oreil.ly/16e8R) about issues beyond algorithms and
    architecture that arise in training these models at scale (see [Figure 10-1](#ch10_figure_1_1748896837889780)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，用户与LLMs的主要交互方式是通过基于文本的聊天，但未来几年，LLMs不仅会回答问题；它们将参与复杂的解决问题，提供见解，并推动创造力本身的边界。例如，2024年9月，OpenAI为其ChatGPT应用程序发布了高级语音模式，可以检测语音语调——包括讽刺。这项工作的大部分与基础设施堆栈即将到来的创新有关。Meta
    [最近撰文](https://oreil.ly/16e8R)讨论了在规模训练这些模型时出现的算法和架构之外的问题（见[图10-1](#ch10_figure_1_1748896837889780)）。
- en: LLMOps will be the backbone supporting these systems as they mature into a seamless,
    self-sustaining infrastructure. Instead of manual intervention, pipelines for
    training, fine-tuning, and deploying these models will be fully automated, speeding
    up advances in this area. LLMOps engineers will spend less time debugging code
    and more time refining high-level system strategies, including platform and infrastructure
    designs that automatically balance model training with compute costs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些系统成熟为一个无缝、自我维持的基础设施，LLMOps将成为支撑它们的骨干。不再需要人工干预，训练、微调和部署这些模型的管道将完全自动化，从而加快该领域的进步。LLMOps工程师将花费更少的时间调试代码，更多的时间完善高级系统策略，包括自动平衡模型训练与计算成本的平台和基础设施设计。
- en: '![](assets/llmo_1001.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_1001.png)'
- en: 'Figure 10-1\. Reliability is a key goal for LLMOps, but even Meta struggles
    with it, as more innovation is needed at the infrastructure level (source: [Engineering
    at Meta](https://oreil.ly/xlokW); used with permission)'
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 可靠性是LLMOps的关键目标，但即使是Meta也面临着挑战，因为需要在基础设施层面进行更多创新（来源：[Meta的工程](https://oreil.ly/xlokW)；经许可使用）
- en: These models will adapt on the fly, learning from real-world feedback at a rate
    that feels almost magical. This has been the primary goal of AutoML, which is
    an active area of research in ML. Most importantly, as LLMs start generating higher-quality
    translated content, they can more easily expand their capabilities to additional
    languages, even less common ones. An additional source of excitement ([Figure 10-2](#ch10_figure_2_1748896837889817))
    is that LLMs are getting better at using multimodal inputs. Currently, it takes
    an LLM a few seconds to process text, voice, and images, and a lot of [progress
    is being made](https://oreil.ly/IWfM4) in simultaneous speech-to-text translation,
    a critical step toward speech-to-speech translation. Once simultaneous speech-to-text
    quality is high enough, existing text-to-speech models such as OpenAI Whisper
    can be used to complete the speech-to-speech translation pipeline.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型将即时适应，以几乎神奇的速度从现实世界的反馈中学习。这是AutoML的主要目标，它是机器学习中的一个活跃研究领域。最重要的是，随着LLMs开始生成更高质量的翻译内容，它们可以更容易地将能力扩展到其他语言，甚至是不太常见的语言。另一个令人兴奋的来源（[图10-2](#ch10_figure_2_1748896837889817)）是LLMs在利用多模态输入方面变得越来越好。目前，LLMs处理文本、语音和图像需要几秒钟，在同时语音转文本翻译方面取得了大量[进展](https://oreil.ly/IWfM4)，这是语音到语音翻译的关键步骤。一旦同时语音转文本的质量足够高，现有的文本到语音模型，如OpenAI
    Whisper，就可以用于完成语音到语音翻译的管道。
- en: '![](assets/llmo_1002.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_1002.png)'
- en: Figure 10-2\. [X post](https://oreil.ly/fbw7V) by Barret Zoph on September 24,
    2024, during his time as vice president of research at OpenAI
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 巴雷特·索普于2024年9月24日作为OpenAI研究副总裁期间发布的[X post](https://oreil.ly/fbw7V)
- en: One of the limitations of LLMs is that, at their core, they generate the most
    likely next word based on the data they were trained on and the prompt submitted,
    but they don’t seem to understand even simple concepts. In a popular example (pictured
    in [Figure 10-3](#ch10_figure_3_1748896837889849)), Meta’s LLM could easily tell
    that Tom Cruise’s mom is Mary Lee Pfeiffer but had trouble with the question “Who
    is Mary Lee Pfeiffer’s famous actor son?” It frequently answered with the names
    of other famous actors, such as Matt Damon, Tom Hanks, and Michelle Pfeiffer—the
    “famous actor” part of the prompt took precedence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的一个局限性在于，它们的核心是依据训练数据和提交的提示生成最可能的下一个单词，但它们似乎甚至不理解简单概念。在一个流行的例子（如图10-3所示）中，Meta的LLM可以轻易地告诉汤姆·克鲁斯的母亲是玛丽·李·费弗，但在回答“玛丽·李·费弗的著名演员儿子是谁？”这个问题时遇到了困难。它经常用其他著名演员的名字回答，如马特·达蒙、汤姆·汉克斯和米歇尔·菲佛——提示中的“著名演员”部分优先级更高。
- en: '![](assets/llmo_1003.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_1003.png)'
- en: Figure 10-3\. Meta’s AI answers to questions about actor Tom Cruise and his
    mother, Mary Lee Pfeiffer, that show a lack of conceptual understanding
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. Meta的AI对关于演员汤姆·克鲁斯及其母亲玛丽·李·费弗的问题的回答，显示了缺乏概念理解
- en: 'One solution that is under research is to use knowledge graphs that contain
    relationships between concepts. *Knowledge graphs* contain representations of
    interconnected concepts and their relationships. For example, Wikipedia lists
    Mary Lee Pfeiffer as Tom Cruise’s mother, but the fact that he is her son is not
    written down: it’s implicit. Knowledge graphs [make relationships explicit](https://oreil.ly/aGwVQ),
    creating systems that understand context in a way that’s [almost human](https://oreil.ly/ILvsK).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正在研究的一个解决方案是使用包含概念之间关系的知识图谱。*知识图谱*包含相互关联的概念及其关系的表示。例如，维基百科列出了玛丽·李·费弗作为汤姆·克鲁斯的母亲，但他作为她的儿子的事实并没有写下来：这是隐含的。知识图谱[使关系明确](https://oreil.ly/aGwVQ)，创建出能够以几乎人类的方式理解情境的系统。[几乎人类](https://oreil.ly/ILvsK)。
- en: Conversations with LLMs will become indistinguishable from human conversations.
    No more fumbling with chatbots or dealing with “robotic” responses. Advances in
    personalization can allow future LLM applications to combine several facts they
    learn about users in the course of their interactions. Simple versions of this
    already exist today. For example, ChatGPT already learns what programming language
    an individual user frequently asks questions about and provides answers in that
    language by default. [Recent research](https://oreil.ly/n6lZt) provides several
    other examples of adding personalization for education, healthcare, and finance.
    Although these use cases currently appear mostly in research papers, when deployed
    successfully in production across every ecommerce application out there, LLMs
    will anticipate users’ needs, contextualize interactions, and even predict trends—all
    while learning continuously from new data streams.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与大型语言模型（LLM）的对话将变得与人类对话难以区分。不再需要与聊天机器人笨拙地交流或处理“机器人”式的回应。个性化技术的进步可以使未来的LLM应用在交互过程中结合他们从用户那里学到的多个事实。这种简单版本今天已经存在。例如，ChatGPT已经学会了一个用户经常询问的编程语言，并默认用该语言提供答案。[最近的研究](https://oreil.ly/n6lZt)提供了为教育、医疗和金融添加个性化的其他几个例子。尽管这些用例目前主要出现在研究论文中，但当它们在所有电子商务应用中成功部署时，LLM将预测用户需求，情境化交互，甚至预测趋势——所有这些同时，它们会从新的数据流中持续学习。
- en: One of the key concerns people voice about LLMs has to do with the alignment
    risks associated with these models; namely, if a model is capable of showing emergence
    and can predict and emulate human behavior, then how do we know it will continue
    to be helpful in the long run? Should development pause while researchers closely
    monitor these models’ performance? How can we ensure that the model is not behaving
    in a sociopathic way, providing harmless answers only when it realizes it’s being
    tested? [Researchers are exploring answers](https://oreil.ly/BAilf) to these questions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人们关于LLM提出的一个关键担忧与这些模型相关的对齐风险有关；也就是说，如果一个模型能够表现出涌现性并能预测和模仿人类行为，那么我们如何知道它将在长期内继续有用？在研究人员密切监控这些模型的表现时，发展应该暂停吗？我们如何确保模型不是以病态的方式行事，只有在意识到自己正在被测试时才提供无害的答案？[研究人员正在探索这些问题的答案](https://oreil.ly/BAilf)。
- en: As someone who believes in holding a [Stoic outlook](https://oreil.ly/yfbPd)
    toward this future, my opinion is that we should embrace any technology that makes
    life simpler, richer, and more meaningful. As with all progress, the true winners
    will be those who understand that it’s not about the machine but about how we,
    as humans, use it wisely.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一位相信对这一未来持有 [斯多葛主义观点](https://oreil.ly/yfbPd) 的人，我的观点是，我们应该拥抱任何使生活更简单、更丰富、更有意义的科技。与所有进步一样，真正的赢家将是那些理解这不仅仅关乎机器，而关乎我们作为人类如何明智地使用它的人。
- en: LLM architecture has advanced exponentially from the mid-2010s to the mid-2020s,
    but the coming years promise even more profound shifts. These changes will redefine
    not only how LLMs are structured but also how they interact with data, humans,
    and each other. From increased scalability to more efficient computation, from
    hybrid architectures combining multiple paradigms to emergent self-learning systems,
    the future of LLMs has breathtaking potential. This chapter explores several aspects
    of that potential.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2010 年代中期到 2020 年代中期，LLM 架构的进步呈指数级增长，但未来几年将带来更深刻的转变。这些变化不仅将重新定义 LLM 的结构，还将重新定义它们与数据、人类以及彼此的交互方式。从提高可扩展性到更高效的计算，从结合多个范例的混合架构到涌现的自学习系统，LLM
    的未来具有令人叹为观止的潜力。本章探讨了该潜力的几个方面。
- en: Scaling Beyond Current Boundaries
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越当前边界
- en: Today’s LLMs, like GPT-4, have reached impressive scales, but they are far from
    their limits. Going into the 2030s, we should expect to see architectures designed
    with scalability in mind from the ground up. This is not simply about increasing
    the number of parameters; it’s about efficient, targeted scaling. Future architectures
    will incorporate hierarchical layers of models (also known as *hierarchical attention
    networks*), where each layer is optimized for a specific domain of understanding
    such as reasoning, emotion, or even creativity (see [Figure 10-4](#ch10_figure_4_1748896837889880)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的 LLM，如 GPT-4，已经达到了令人印象深刻的规模，但它们离极限还远。进入 2030 年代，我们应该期待看到从底层设计时就考虑可扩展性的架构。这不仅仅是增加参数数量的问题；这是关于高效、有针对性的扩展。未来的架构将结合层次化的模型层（也称为
    *层次化注意力网络*），其中每一层都针对推理、情感甚至创造力等特定理解领域进行了优化（参见 [图 10-4](#ch10_figure_4_1748896837889880)）。
- en: '![](assets/llmo_1004.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/llmo_1004.png)'
- en: 'Figure 10-4\. In the scaling law, N represents the number of data points, and
    D is the number of parameters; larger models with more data points tend to perform
    better than larger models with less data (source: “First Principles on AI Scaling”
    by [Dynomight](https://oreil.ly/uCTJD))'
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 在缩放定律中，N 代表数据点的数量，D 代表参数的数量；具有更多数据点的更大模型往往比具有较少数据点的更大模型表现更好（来源：“人工智能缩放原理”由
    [Dynomight](https://oreil.ly/uCTJD) 撰写）
- en: 'Rather than creating ever-larger monolithic models, we’ll see more modular
    LLMs that can delegate tasks to specialized submodels. Imagine an architecture
    where the base LLM understands language but calls on additional “expert” modules
    trained for niche tasks like legal reasoning, medical diagnostics, or creative
    writing. Instead of having one large base model trying to be an expert in everything,
    we would have models that are experts in different things: a healthcare model,
    a legal model, a creative writing model, etc.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是创建越来越大的单体模型，我们将看到更多模块化的 LLM，它们可以将任务委托给专门子模型。想象一个架构，其中基础 LLM 理解语言，但调用额外的“专家”模块，这些模块针对法律推理、医学诊断或创意写作等利基任务进行了训练。而不是有一个试图成为所有事物专家的大型基础模型，我们将拥有在不同事物上成为专家的模型：一个医疗保健模型、一个法律模型、一个创意写作模型等。
- en: A specialist model can outperform a generalist model and use fewer resources.
    This is already happening for math problems and web searches in ChatGPT. [Researchers
    have found](https://oreil.ly/c1svH) that GPT performs below average in graduate-level
    math, and all GPT models have [knowledge cutoffs](https://oreil.ly/pv6sA), meaning
    that they are unaware of events after their cutoff date. Currently, when GPT detects
    that an user is asking a question about an event that occurred after its training
    cutoff date, it outsources the question to a different model called [SearchGPT](https://oreil.ly/kJtXj),
    then uses the results to provide an answer inside the existing chat. Combining
    generalist models with specialist models allows LLMs to operate efficiently and
    with greater depth in specialized areas, reducing the computational overhead while
    improving output precision.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 专门模型可以超越通用模型并使用更少的资源。这在ChatGPT中的数学问题和网络搜索中已经发生。研究人员发现[研究人员发现](https://oreil.ly/c1svH)，GPT在研究生水平的数学测试中表现低于平均水平，所有GPT模型都有[知识截止点](https://oreil.ly/pv6sA)，这意味着它们对其截止日期之后的事件一无所知。目前，当GPT检测到用户询问其训练截止日期之后发生的事件时，它会将问题外包给另一个名为[SearchGPT](https://oreil.ly/kJtXj)的不同模型，然后使用这些结果在现有的聊天中提供答案。将通用模型与专门模型相结合，可以使LLMs在专业领域内更高效、更深入地运行，同时减少计算开销并提高输出精度。
- en: To support such adaptable massive systems, innovations in distributed computing
    and parallelization will be key. As Evan Morikawa, who led OpenAI engineering
    when ChatGPT was becoming ever more popular, explains in an [interview](https://oreil.ly/bMlsK),
    models need to be hosted not on singular clusters but across decentralized networks
    of nodes. This shift will optimize training times, data latency, and real-time
    inference, making LLMs vastly more efficient in handling large-scale, real-world
    applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这种适应性强的庞大系统，分布式计算和并行化的创新将是关键。正如Evan Morikawa在[一次采访](https://oreil.ly/bMlsK)中解释的那样，模型需要托管在单个集群上，而不是分散的节点网络中。这种转变将优化训练时间、数据延迟和实时推理，使LLMs在处理大规模、实际应用方面效率大大提高。
- en: 'Hybrid Architectures: Merging Neural Networks with Symbolic AI'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合架构：将神经网络与符号AI结合
- en: One of the current limitations of LLMs lies in their reliance on deep learning
    alone, which excels at pattern recognition but struggles with symbolic reasoning
    and logic. In many cases, scientists have already discovered patterns and codified
    them into symbolic formulas (e.g., Newton’s theory of gravity), but the way neural
    networks are trained doesn’t allow them to use existing formulas—they have to
    rediscover patterns by themselves.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs当前的一个局限性在于它们仅依赖于深度学习，深度学习在模式识别方面表现出色，但在符号推理和逻辑方面存在困难。在许多情况下，科学家们已经发现了模式并将它们编码成符号公式（例如，牛顿的万有引力理论），但神经网络训练的方式不允许它们使用现有的公式——它们必须自己重新发现模式。
- en: The future of LLMs will involve hybrid architectures that merge the strengths
    of neural networks with symbolic AI approaches. These architectures will allow
    models not only to predict the next word in a sentence but also to use known rules
    and formulas, as humans do.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的未来将涉及混合架构，这些架构将神经网络的优势与符号AI方法相结合。这些架构将使模型不仅能够预测句子中的下一个单词，还能够使用已知规则和公式，就像人类一样。
- en: '*Neurosymbolic*, or “hybrid,” AI architectures unite the intuitive, pattern-matching
    power of neural networks with the precise, rule-based reasoning of symbolic systems.
    LLMs excel at processing text and generating natural-sounding responses by learning
    statistical regularities from massive datasets, while symbolic AI can represent
    explicit facts, logical constraints, and rules, making it far easier to trace
    its reasoning process and enforce consistency. By merging these two approaches,
    we will develop systems that can understand human language, perform rigorous logical
    operations, and provide explanations for their conclusions.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经符号学*，或称为“混合”的AI架构，将神经网络的直观、模式匹配能力与符号系统的精确、基于规则的推理能力相结合。大型语言模型（LLMs）通过从大量数据集中学习统计规律，在处理文本和生成自然流畅的响应方面表现出色，而符号AI可以表示明确的事实、逻辑约束和规则，这使得追踪其推理过程和确保一致性变得容易得多。通过合并这两种方法，我们将开发能够理解人类语言、执行严格的逻辑运算并提供结论解释的系统。'
- en: In practice, this can manifest in multiple ways. For instance, one method is
    to have an LLM convert user queries into structured representations—such as logical
    formulas—and then rely on a symbolic reasoner to apply domain-specific rules or
    constraints. This hybrid approach can also aid in explainability—one of the key
    weaknesses in today’s LLMs. Users will be able to query why the model arrived
    at a particular conclusion, and the model can refer to the symbolic pathways used
    in the answer, providing a more transparent window into its decision-making process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这可以以多种方式体现。例如，一种方法是将用户的查询转换为结构化表示——例如逻辑公式——然后依靠符号推理器应用特定领域的规则或约束。这种混合方法还可以帮助提高可解释性——这是当今LLMs的关键弱点之一。用户将能够查询模型为何得出特定结论，模型可以引用在答案中使用的符号路径，为决策过程提供更透明的窗口。
- en: Sparse and Mixture-of-Experts Models
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏和专家混合模型
- en: One of the biggest bottlenecks in scaling LLMs today is their sheer computational
    cost. Current models process every input with all their parameters, at inference
    time, regardless of the complexity or simplicity of the task. Future architectures
    will move toward sparse models and mixture-of-experts systems, where only a subset
    of the model’s parameters is activated for a given task.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当今在扩展LLMs时最大的瓶颈之一是它们的计算成本。当前的模型在推理时使用所有参数处理每个输入，无论任务的复杂程度或简单程度如何。未来的架构将朝着稀疏模型和专家混合系统发展，其中只有模型参数的子集在特定任务中被激活。
- en: In *sparse models*, only the most relevant parameters or neurons are activated
    for a particular query, allowing for massive reductions in resource consumption
    while maintaining high-quality results. We believe that sparse modeling, combined
    with the modular approach, will lead to the development of powerful, efficient
    LLMs capable of running on consumer-grade hardware while delivering enterprise-level
    performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在*稀疏模型*中，只有与特定查询最相关的参数或神经元被激活，这允许在保持高质量结果的同时，大幅减少资源消耗。我们相信，稀疏模型与模块化方法的结合，将引领开发出强大、高效的LLMs（大型语言模型），它们能够在消费级硬件上运行，同时提供企业级性能。
- en: '*Mixture-of-experts (MoE) models*, by contrast, allow LLMs to dynamically activate
    specialized “experts” based on the input’s requirements. A user asking for medical
    advice would engage a different subset of the model’s neurons than a user requesting
    help with poetry. This approach drastically reduces the number of computations
    per query while increasing the depth of understanding in each domain. It’s a “divide
    and conquer” strategy, where LLMs focus computational resources only where they
    are needed most.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，*专家混合（MoE）模型*允许LLMs根据输入的要求动态激活专门的“专家”。请求医疗建议的用户将激活模型中与请求帮助写诗的用户不同的神经元子集。这种方法在查询时大幅减少了计算量，同时增加了每个领域的理解深度。这是一种“分而治之”的策略，其中LLMs将计算资源集中在最需要的地方。
- en: 'Memory-Augmented Models: Toward Persistent, Context-Rich AI'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存增强模型：迈向持久、上下文丰富的AI
- en: Current LLMs operate with limited memory. While capable of handling context
    within a few thousand tokens, they struggle to maintain long-term memory across
    sessions. The next generation of LLMs will address this with *memory-augmented
    architectures* capable of storing and retrieving vast amounts of data over long
    periods. These models will have persistent memory layers, allowing them to recall
    user interactions from years ago or build a comprehensive knowledge base that
    evolves with time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的LLMs在内存方面运作有限。虽然能够处理几千个标记的上下文，但它们在会话之间保持长期记忆方面存在困难。下一代LLMs将通过*内存增强架构*来解决这个问题，这种架构能够在长时间内存储和检索大量数据。这些模型将具有持久性内存层，允许它们回忆起数年前的用户交互或构建随时间演变的综合知识库。
- en: This kind of persistent memory will also revolutionize [how LLMs handle personalized
    tasks](https://oreil.ly/tsGoZ). Rather than starting from scratch with each interaction,
    future models will remember the user’s preferences, needs, and history, enabling
    richer, more nuanced conversations and solutions. However, that comes with its
    own challenges in production, including unexpected behavior when dealing with
    inconsistencies in data. For example, imagine a single parent who is a senior
    government official but who also asks questions about how to raise a female child
    without specifically telling the model that the questions are about another person.
    For example, they ask, “What are the signs that my first period is arriving?”
    instead of “What are the signs that my daughter’s first period is arriving?” The
    personalization algorithm may incorrectly conclude that the user is a teenage
    girl who is also a senior government official.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种持久性记忆也将彻底改变[LLMs处理个性化任务的方式](https://oreil.ly/tsGoZ)。未来的模型将记住用户的偏好、需求和历史，从而实现更丰富、更细腻的对话和解决方案。然而，这也带来了生产中的挑战，包括处理数据不一致时的意外行为。例如，想象一个既是高级政府官员又是单身父亲的个人，但他没有明确告诉模型这些问题是关于另一个人的。例如，他们问：“我第一次月经到来的迹象是什么？”而不是“我女儿第一次月经到来的迹象是什么？”个性化算法可能会错误地得出结论，认为用户是一个既是高级政府官员又是少女的青少年。
- en: Persistent memory will be key in enterprise applications, where models will
    continuously learn from organizational data, building an ever-growing repository
    of insights and expertise.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业应用中，持久性记忆将是关键，模型将不断从组织数据中学习，构建一个不断增长的见解和专业知识库。
- en: Interpretable and Self-Optimizing Models
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释和自优化的模型
- en: As LLMs become more pervasive, the need for interpretability will grow. Users
    and businesses alike will demand models that can explain their reasoning, mitigate
    biases, and adapt in real time. Future LLM architectures will include built-in
    interpretability features using causal learning, where each decision or prediction
    can be traced back through a chain of reasoning or probabilistic mapping.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs的普及，对可解释性的需求将增加。用户和企业都将要求模型能够解释其推理、减轻偏见并实时适应。未来的LLM架构将包括内置的可解释性功能，使用因果学习，其中每个决策或预测都可以通过推理链或概率映射追溯到其根源。
- en: These models will also be self-optimizing. Using reinforcement learning, LLMs
    will learn from user feedback, fine-tuning their own parameters to better align
    with desired outcomes. Over time, these models will become more personalized,
    adjusting not just to individuals but also to the specific needs of organizations
    or industries. Imagine a legal AI model that, after interacting with a team of
    lawyers for months, begins to understand the specific nuances of that firm’s legal
    style, approach to risk, and preferred legal precedents. These models will learn
    through iterative feedback, continuously improving without needing massive retraining
    efforts. They are currently being explored ([Huang et al. 2022](https://oreil.ly/F30ZX);
    [Jin et al. 2025](https://oreil.ly/Xaf-t)) as agents to aid operational productivity,
    but many applications remain unexplored.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型也将是自优化的。通过强化学习，大型语言模型（LLMs）将根据用户反馈进行学习，微调自己的参数以更好地与期望的结果对齐。随着时间的推移，这些模型将变得更加个性化，不仅适应个人，也适应组织或行业的特定需求。想象一下，一个法律AI模型在与一个律师团队互动了几个月后，开始理解该律师事务所特定的法律风格、风险处理方法和首选的法律先例。这些模型将通过迭代反馈进行学习，不断改进，而无需进行大规模的再训练。它们目前正在被探索作为提高运营效率的代理，但许多应用仍待开发。
- en: Cross-Model Collaboration, Meta-Learning, and Multi-Modal Fine-Tuning
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型间协作、元学习和多模态微调
- en: In the future, no single LLM will operate in isolation. We’ll see architectures
    where multiple models collaborate, exchanging data, insights, and strategies in
    real time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来，没有任何单个LLM将独立运行。我们将看到多个模型协作的架构，实时交换数据、见解和策略。
- en: '*Meta-learning* will also become more prominent, so instead of having to be
    trained from scratch, LLMs will learn how to learn. This means they will be capable
    of adjusting their architectures dynamically based on the tasks they encounter
    and will optimize themselves without human intervention, using different distillation
    techniques. This shift will push LLMs toward becoming self-evolving entities,
    reducing the need for constant retraining and manual updates. Overall, we will
    move beyond the brute-force scaling of today’s models to more refined, hybridized
    architectures capable of reasoning, learning, and adapting in ways that feel almost
    human.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*元学习*也将变得更加突出，因此，LLMs将不再需要从头开始训练，而是学会如何学习。这意味着它们将能够根据遇到的任务动态调整其架构，并在无需人工干预的情况下自我优化，使用不同的蒸馏技术。这种转变将推动LLMs向自我进化的实体发展，减少对持续重新训练和手动更新的需求。总体而言，我们将超越今天模型的粗放式扩展，转向更精细、混合化的架构，这些架构能够以几乎人类的方式推理、学习和适应。'
- en: In addition, as LLMs increasingly interact with multimodal data (text, images,
    audio, etc.), multimodal fine-tuning techniques will become essential. These methods
    will enable LLMs to integrate and process information from various modalities,
    enhancing their ability to perform complex tasks that require understanding of
    diverse data types.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着LLMs越来越多地与多模态数据（文本、图像、音频等）交互，多模态微调技术将变得至关重要。这些方法将使LLMs能够整合和处理来自各种模态的信息，增强它们执行需要理解多种数据类型复杂任务的能力。
- en: RAG
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG
- en: RAG models will continue to evolve, integrating retrieval-based components with
    generative models to enhance accuracy and relevance. These hybrid models will
    retrieve relevant information from large databases or knowledge sources and use
    it to generate more informed and contextually appropriate responses. Advances
    in real-time retrieval mechanisms will allow them to access and utilize up-to-date
    information dynamically so they can provide current and contextually relevant
    responses. This promises to improve RAG models’ effectiveness in applications
    such as customer support, knowledge management, and content creation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RAG模型将继续发展，将基于检索的组件与生成模型集成，以提高准确性和相关性。这些混合模型将从大型数据库或知识源检索相关信息，并利用它来生成更明智和情境适当的响应。实时检索机制的进步将使它们能够动态地访问和利用最新信息，以便提供当前和情境相关的响应。这有望提高RAG模型在客户支持、知识管理和内容创作等应用中的有效性。
- en: Future RAG systems will better integrate with knowledge graphs and external
    databases, enabling LLMs to leverage structured knowledge for more accurate, detailed,
    factually correct, and comprehensive responses—even to complex queries.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的RAG系统将更好地与知识图谱和外部数据库集成，使LLMs能够利用结构化知识以提供更准确、详细、事实正确和全面的响应——即使是针对复杂查询。
- en: Innovations in retrieval mechanisms will focus on improving efficiency and scalability
    ([Gao et al. 2024](https://oreil.ly/EO8IV)). Techniques like approximate nearest
    neighbor search and indexing will be optimized to handle large-scale data and
    reduce latency in retrieval processes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索机制方面的创新将侧重于提高效率和可扩展性([高etal. 2024](https://oreil.ly/EO8IV))。近似最近邻搜索和索引等技术将被优化以处理大规模数据并减少检索过程中的延迟。
- en: To conclude, sparse models and modular frameworks will make LLMs far more efficient,
    while memory-augmented models will bring persistence and depth to their understanding.
    The future of LLMs is not just one of bigger models but smarter ones—architectures
    that grow, learn, and reason, forming a new foundation for AI-driven innovation
    across every aspect ​of society.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，稀疏模型和模块化框架将使LLMs的效率大大提高，而记忆增强模型将为它们的理解带来持久性和深度。LLMs的未来不仅仅是更大型的模型，而是更智能的模型——能够成长、学习和推理的架构，为AI驱动的社会各个方面的创新奠定新的基础。
- en: The Future of LLMOps
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOps的未来
- en: I expect the coming decade to bring a lot of innovation to LLMOps, including
    the infrastructure layer that will be guided by that framework. One of the biggest
    contributions of any Ops framework is that it helps practitioners understand what
    tools they can build to automate and streamline best practices across the industry.
    For example, the biggest contribution of DevOps was the boom in cloud services
    infrastructures. For MLOps, it was data- and model-versioning tools. For LLMOps,
    I personally believe that the biggest booms will be in tools for resource optimization,
    evaluation, and multimodal data management.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我预计未来十年将为 LLMOps 带来许多创新，包括由该框架指导的基础设施层。任何 Ops 框架的最大贡献之一是它帮助从业者了解他们可以构建哪些工具来自动化和简化整个行业的最佳实践。例如，DevOps
    的最大贡献是云服务基础设施的繁荣。对于 MLOps，是数据版本和控制工具。对于 LLMOps，我个人认为最大的繁荣将是资源优化、评估和多模态数据管理工具。
- en: Advances in GPU Technology
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 技术的进步
- en: GPUs play a key role in LLMOps, and their evolution will continue to drive advancements
    in model performance and efficiency. Over the next decade, several emerging trends
    in GPU technology will significantly impact LLMOps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 在 LLMOps 中扮演着关键角色，其发展将继续推动模型性能和效率的提升。在未来十年内，GPU 技术的几个新兴趋势将对 LLMOps 产生重大影响。
- en: The future of GPUs will see the rise of highly specialized AI hardware designed
    specifically for the unique demands of LLM training and inference, as [Figure 10-5](#ch10_figure_5_1748896837889900)
    illustrates. Companies like NVIDIA, AMD, and Intel are developing next-generation
    GPUs with enhanced architectures tailored for AI workloads. These include more
    GPU cores, increased memory bandwidth, and optimized tensor operations to accelerate
    model training and reduce latency during inference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 10-5](#ch10_figure_5_1748896837889900) 所示，未来 GPU 的未来将看到专为 LLM 训练和推理的独特需求而设计的、高度专业化的
    AI 硬件兴起。像 NVIDIA、AMD 和 Intel 这样的公司正在开发下一代 GPU，其架构经过增强，专门针对 AI 工作负载。这包括更多的 GPU
    核心、增加的内存带宽以及优化的张量操作，以加速模型训练并减少推理过程中的延迟。
- en: '![](assets/llmo_1005.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_1005.png)'
- en: 'Figure 10-5\. Predicting GPU performance over the next 30 years (source: [Epoch
    AI](https://oreil.ly/TzddI), used with permission)'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 预测未来 30 年的 GPU 性能（来源：[Epoch AI](https://oreil.ly/TzddI)，经许可使用）
- en: As LLMs grow in size, the need for efficient multi-GPU and distributed training
    strategies will become more pronounced. Advances in distributed computing frameworks,
    such as NVIDIA’s NVLink and AMD’s Infinity Fabric, will enable more seamless scaling
    across multiple GPUs and nodes. This will improve training efficiency and reduce
    the time required to develop and deploy large-scale models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 体积的增长，对高效的多 GPU 和分布式训练策略的需求将变得更加明显。分布式计算框架的进步，如 NVIDIA 的 NVLink 和 AMD
    的 Infinity Fabric，将使跨多个 GPU 和节点的扩展更加无缝。这将提高训练效率，并减少开发和部署大规模模型所需的时间。
- en: With the increasing computational demands of LLMs, energy consumption is a critical
    concern. Future GPUs will focus on enhancing energy efficiency. Incorporating
    techniques like dynamic voltage and frequency scaling (DVFS) and using advanced
    cooling solutions will mitigate their environmental impact and operational costs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 计算需求的增加，能耗成为一个关键问题。未来的 GPU 将专注于提高能效。采用动态电压和频率缩放（DVFS）等技术以及使用先进的冷却解决方案将减轻其环境影响和运营成本。
- en: Finally, although still in its infancy, quantum computing presents potential
    opportunities for accelerating LLM operations. Quantum processors could complement
    traditional GPUs, offering exponential speedups for certain types of calculations.
    Researchers are exploring hybrid approaches that combine quantum and classical
    computing to tackle complex LLM tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管量子计算仍处于起步阶段，但它为加速 LLM 操作提供了潜在的机会。量子处理器可以补充传统的 GPU，为某些类型的计算提供指数级的加速。研究人员正在探索结合量子计算和经典计算的综合方法，以应对复杂的
    LLM 任务。
- en: Data Management and Efficiency
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管理和效率
- en: As you learned in [Chapter 4](ch04.html#ch04_data_engineering_for_llms_1748895507364914),
    effective data management is critical for training and deploying LLMs. Since LLMs
    require vast amounts of high-quality data, there will be an increasing emphasis
    on data curation and quality control. We expect techniques for automated data
    cleaning, augmentation, and validation to become more sophisticated, ensuring
    that training datasets are diverse, accurate, and representative.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[第4章](ch04.html#ch04_data_engineering_for_llms_1748895507364914)中学到的，有效的数据管理对于训练和部署LLMs至关重要。由于LLMs需要大量高质量的数据，对数据整理和质量控制的重视程度将不断提高。我们预计自动化数据清理、增强和验证的技术将变得更加复杂，以确保训练数据集的多样性、准确性和代表性。
- en: Innovations in data storage and retrieval will be important for managing these
    massive datasets. Distributed file systems, object storage solutions, and high-performance
    databases will be incorporated into existing vector databases to handle the scale
    and complexity of data efficiently.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储和检索方面的创新对于管理这些大规模数据集至关重要。分布式文件系统、对象存储解决方案和高性能数据库将被纳入现有的向量数据库中，以有效地处理数据的规模和复杂性。
- en: With growing awareness of data privacy, organizations will adopt advanced methods
    for protecting user data. Techniques like federated learning and differential
    privacy will be integrated into data management, allowing LLMs to learn from decentralized
    data sources without compromising individual privacy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对数据隐私意识的提高，组织将采用先进的方法来保护用户数据。联邦学习和差分隐私等技术将被整合到数据管理中，允许LLMs从去中心化的数据源中学习，而不会损害个人隐私。
- en: '[Synthetically generated data](https://oreil.ly/gJ7J4) will become a key supplement
    to real-world data, improving training speed, reducing privacy concerns (as synthetic
    data is machine generated), and reducing reliance on expensive or scarce data.
    The [Microsoft phi-4 small language model](https://oreil.ly/_DXcC) released in
    late 2024 has, by using some synthetic data, achieved good benchmarks at low cost
    and a small number of parameters.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[合成生成数据](https://oreil.ly/gJ7J4)将成为现实世界数据的补充，提高训练速度，减少隐私担忧（因为合成数据是机器生成的），并减少对昂贵或稀缺数据的依赖。2024年底发布的[微软phi-4小型语言模型](https://oreil.ly/_DXcC)通过使用一些合成数据，以低成本和少量参数实现了良好的基准。'
- en: Privacy and Security
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私和安全
- en: Privacy and security will be paramount as LLMs become more integrated into sensitive
    and high-stakes applications. The deployment of LLMs will involve advanced security
    measures to protect against attacks and ensure data integrity. Techniques such
    as model watermarking, adversarial training, and secure multi-party computation
    will be employed to safeguard models from tampering and misuse.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs更多地集成到敏感和高风险应用中，隐私和安全将变得至关重要。LLMs的部署将涉及高级安全措施，以防止攻击并确保数据完整性。将采用诸如模型水印、对抗性训练和安全的多方计算等技术来保护模型免受篡改和滥用。
- en: As LLMs become more pervasive, ethical considerations will drive the development
    of guidelines and best practices for their use. New laws like the ones in the
    [United States](https://oreil.ly/KjaEW), the state of [California](https://oreil.ly/48e1Z),
    and the [European Union](https://oreil.ly/xV4NL) will incentivize organizations
    to focus on transparency, fairness, and accountability, implementing measures
    to ensure that LLMs are used responsibly and ethically. Also, given the massive
    training and maintenance costs, it’s economical for large AI providers to develop
    models that follow a large market’s most restrictive set of rules and deploy them
    broadly, rather than train and maintain different models for each set of legal
    requirements. For example, if the EU requires that models only use anonymized
    data, it’s economical to train and maintain one model worldwide that uses anonymized
    data rather than two, one that does and one that does not.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的日益普及，伦理考量将推动制定其使用的指南和最佳实践。像美国[https://oreil.ly/KjaEW]、加州[https://oreil.ly/48e1Z]和欧盟[https://oreil.ly/xV4NL]这样的新法律将激励组织关注透明度、公平性和问责制，实施措施以确保LLMs的使用是负责任和道德的。此外，鉴于大规模训练和维护成本，大型AI提供商开发遵循大型市场最严格规则集的模型并广泛部署，比针对每套法律要求训练和维护不同的模型更为经济。例如，如果欧盟要求模型只能使用匿名数据，那么在全球范围内训练和维护一个使用匿名数据的模型，而不是两个（一个使用，一个不使用），在经济上更为合理。
- en: Comprehensive Evaluation Frameworks
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 综合评估框架
- en: I expect that new evaluation frameworks will be developed to assess LLMs across
    a range of dimensions beyond standard metrics like recall and precision, including
    factual accuracy, logical accuracy, and coherence. These frameworks will incorporate
    both qualitative and quantitative measures to provide a holistic view of model
    performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我预计将开发新的评估框架，以评估LLM在召回率和精确度等标准指标之外的多个维度，包括事实准确性、逻辑准确性和连贯性。这些框架将结合定性和定量措施，以提供模型性能的整体视图。
- en: Establishing industry benchmarks and standards, ideally from organizations such
    as the United Nations International Telecommunication Union (ITU), the Institute
    of Electrical and Electronics Engineers Standards Association (IEEE SA), and the
    International Organization for Standardization (ISO), will be essential for comparing
    LLM performance across different models and platforms. Standardized benchmarks
    will facilitate fair evaluation and enable organizations to make informed decisions
    when selecting or developing LLMs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 建立行业基准和标准，理想情况下来自联合国国际电信联盟（ITU）、电气和电子工程师协会标准协会（IEEE SA）和国际标准化组织（ISO）等组织，对于比较不同模型和平台上的LLM性能至关重要。标准化的基准将促进公平评估，并使组织在选择或开发LLM时能够做出明智的决定。
- en: Ongoing monitoring and evaluation will become standard practice to ensure that
    LLMs maintain high performance over time. Techniques for continuous evaluation
    and performance tracking will help identify and address issues as they arise,
    ensuring that models remain effective and relevant.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 持续的监控和评估将成为确保LLM在长时间内保持高性能的标准做法。持续评估和性能跟踪的技术将有助于在问题出现时识别和解决它们，确保模型保持有效和相关性。
- en: How to Succeed as an LLMOps Engineer
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何成为一名成功的LLMOps工程师
- en: To succeed as an LLMOps engineer, you need to take a system administrator approach
    to productionizing LLMs. As directly responsible individuals (DRIs), engineers
    must be able to understand, evaluate, and manage risk.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为一名成功的LLMOps工程师，你需要采取系统管理员的方法来生产化LLM。作为直接责任个人（DRIs），工程师必须能够理解、评估和管理风险。
- en: 'Our LLMOps maturity model ([Chapter 2](ch02.html#ch02_introduction_to_llmops_1748895480208948))
    can come handy when budgeting for different kinds of errors, from software fault
    tolerance to model evaluation errors. You cannot monitor everything. Set reasonable
    expectations and automate labeling and debugging for different kinds of errors.
    These could mean keeping an error trail, creating groups of related errors, and
    using LLM agents to explain and even debug them. An LLMOps engineer often acts
    as the on-call engineer, which requires dealing with all sorts of problems: hardware,
    data quality, privacy, and user errors. These issues need to be dealt with quickly,
    often as emergencies. LLMs can help LLMOps engineers sift through the problems
    and provide suggested solutions, increasing productivity.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LLMOps成熟度模型([第2章](ch02.html#ch02_introduction_to_llmops_1748895480208948))在为不同类型的错误（从软件容错到模型评估错误）进行预算时可能会很有用。你不能监控一切。设定合理的期望，并为不同类型的错误自动化标记和调试。这可能意味着保持错误跟踪，创建相关错误的组，并使用LLM代理来解释甚至调试它们。LLMOps工程师通常充当值班工程师，这需要处理各种问题：硬件、数据质量、隐私和用户错误。这些问题需要迅速处理，通常作为紧急情况。LLM可以帮助LLMOps工程师筛选问题并提供建议解决方案，提高生产力。
- en: 'As you learned in [Chapter 2](ch02.html#ch02_introduction_to_llmops_1748895480208948),
    LLMOps has four goals: security, scalability, robustness, and reliability. It
    can be a tough balancing act to prioritize among monitoring inferences for security
    testing, optimizing the model inference pipeline, A/B testing the model releases,
    managing the compute nodes, and optimizing the run pipelines.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在[第2章](ch02.html#ch02_introduction_to_llmops_1748895480208948)中学到的，LLMOps有四个目标：安全性、可扩展性、鲁棒性和可靠性。在监控推理以进行安全测试、优化模型推理管道、A/B测试模型发布、管理计算节点和优化运行管道之间进行优先级排序可能是一项艰巨的任务。
- en: Depending on the number of active users the LLM-based application has, the features
    under development, and the size of the team, LLMOps engineers’ workloads can vary
    massively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据LLM应用中活跃用户数量、开发中的功能以及团队规模，LLMOps工程师的工作负载可能会有很大差异。
- en: Conclusion
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The world of LLMOps is not just about cutting-edge technology; it’s also about
    processes and measurements. Together, technology, measurements, and processes
    are the pillars supporting the future of AI.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps的世界不仅关乎尖端技术，还关乎流程和度量。技术、度量、流程共同构成了支持人工智能未来的支柱。
- en: In this book, you’ve learned about how to apply, deploy, and maintain LLMs efficiently.
    We discussed the importance of data quality and data management, and we explored
    the art and science of improving LLMs through fine-tuning and prompt engineering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你学习了如何高效地应用、部署和维护LLMs。我们讨论了数据质量和数据管理的重要性，并探讨了通过微调和提示工程改进LLMs的艺术和科学。
- en: We’ve examined the revolutionary potential of RAG to bridge the gap between
    the general knowledge possessed by LLMs and the recent and/or specialized data
    some applications need. We’ve also discussed privacy and security, recognizing
    that safeguarding our digital interactions is as important as advancing our technological
    frontiers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了RAG的革命性潜力，以弥合LLMs所拥有的通用知识与某些应用所需的新近和/或专业数据之间的差距。我们还讨论了隐私和安全问题，认识到保护我们的数字互动与推进我们的技术前沿一样重要。
- en: Yet, amid these advancements, it’s important to remember the essence of our
    pursuit. LLMOps is more than a technical discipline; it’s about ensuring that
    our creations serve humanity in ways that are ethical, transparent, and equitable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这些进步中，记住我们追求的本质是很重要的。LLMOps不仅仅是一门技术学科；它关乎确保我们的创造以符合道德、透明和公平的方式服务于人类。
- en: Economists recognize AI as one of a very few [general-purpose technologies](https://oreil.ly/H2GYl),
    in the same class as the internet, electricity, or the printing press. These general-purpose
    technologies tend to be incorporated into almost every human activity, changing
    the trajectory of progress. LLMOps can help us make the most of AI, preventing
    and correcting problems and accelerating advances.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 经济学家将AI视为极少数[通用技术](https://oreil.ly/H2GYl)之一，与互联网、电力或印刷术处于同一类别。这些通用技术往往被纳入几乎所有的人类活动中，改变进步的轨迹。LLMOps可以帮助我们充分利用AI，防止和纠正问题，并加速进步。
- en: The future is ours to shape. Let’s make it a future that reflects our highest
    aspirations and our deepest values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 未来在我们手中塑造。让我们创造一个反映我们最高愿望和最深价值观的未来。
- en: References
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdin, Marah, et al. [“Phi-4 Technical Report”](https://oreil.ly/_DXcC), arXiv,
    December 12, 2024.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Abdin, Marah，等人。[“Phi-4技术报告”](https://oreil.ly/_DXcC)，arXiv，2024年12月12日。
- en: 'Amodei, Dario. [“Machines of Loving Grace: How AI Could Transform the World
    for the Better”](https://oreil.ly/yW4T3), October 11, 2024.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Amodei, Dario. [“机器之爱的恩典：人工智能如何让世界变得更好”](https://oreil.ly/yW4T3), 2024年10月11日。
- en: 'Bolaños Guerra, Bernardo and Jorge Luis Morton Gutierrez. [“On Singularity
    and the Stoics: Why Stoicism Offers a Valuable Approach to Navigating the Risks
    of AI (Artificial Intelligence)”](https://oreil.ly/yfbPd), *AI and Ethics*, August
    2024.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Bolaños Guerra, Bernardo和Jorge Luis Morton Gutierrez。[“关于奇点和斯多葛主义：为什么斯多葛主义为导航AI（人工智能）风险提供了一种有价值的途径”](https://oreil.ly/yfbPd)，*AI和伦理*，2024年8月。
- en: '[California AI Transparency Act](https://oreil.ly/48e1Z), Sb-942 (2023–2024)
    (enacted).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[加利福尼亚人工智能透明度法案](https://oreil.ly/48e1Z)，Sb-942（2023-2024年）（实施）。'
- en: 'Chen, Zhuo, et al. [“Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive
    Survey”](https://oreil.ly/aGwVQ), arXiv, February 2024.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Chen, Zhuo，等人。[“知识图谱与多模态学习：全面综述”](https://oreil.ly/aGwVQ)，arXiv，2024年2月。
- en: Dynomight. [“First Principles on AI Scaling”](https://oreil.ly/uCTJD), July
    2023.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Dynomight。[“人工智能扩展的原理”](https://oreil.ly/uCTJD)，2023年7月。
- en: 'Eloundou, Tyna, et al. [“GPTs Are GPTs: An Early Look at the Labor Market Impact
    Potential of Large Language Models”](https://oreil.ly/H2GYl), arXiv, August 2023.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Eloundou, Tyna，等人。[“GPTs都是GPTs：对大型语言模型对劳动力市场影响潜力的早期观察”](https://oreil.ly/H2GYl)，arXiv，2023年8月。
- en: EU Artificial Intelligence Act. [“Article 50—Transparency Obligations for Providers
    and Deployers of Certain AI Systems”](https://oreil.ly/xV4NL), (enacted).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟人工智能法案。[“第50条——某些人工智能系统提供者和部署者的透明度义务”](https://oreil.ly/xV4NL)，（实施）。
- en: Federal A.I. Governance and Transparency Act of 2024, H.R.7532, 118th Congress
    (2023–2024) (introduced). [*https://oreil.ly/KjaEW*](https://oreil.ly/KjaEW).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2024年联邦人工智能治理和透明度法案，H.R.7532，第118届国会（2023-2024年）（提出）。[*https://oreil.ly/KjaEW*](https://oreil.ly/KjaEW)。
- en: Fountas, Zafeirios, et al. [“Human-like Episodic Memory for Infinite Context
    LLMs”](https://oreil.ly/tsGoZ), arXiv, October 2024.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Fountas, Zafeirios，等人。[“无限上下文LLMs的人类式情景记忆”](https://oreil.ly/tsGoZ)，arXiv，2024年10月。
- en: Frieder, Simon, et al. [“Mathematical Capabilities of ChatGPT”](https://oreil.ly/c1svH)
    arXiv, July 2023.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Frieder, Simon，等人。[“ChatGPT的数学能力”](https://oreil.ly/c1svH) arXiv，2023年7月。
- en: 'Gao, Yunfan, et al. [“Retrieval-Augmented Generation for Large Language Models:
    A Survey”](https://oreil.ly/EO8IV), arXiv, March 2024.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Gao, Yunfan，等人。[“大型语言模型的检索增强生成：综述”](https://oreil.ly/EO8IV)，arXiv，2024年3月。
- en: 'Hagendorff, Thilo, et al. [“Human-like Intuitive Behavior and Reasoning Biases
    Emerged in Large Language Models but Disappeared in ChatGPT”](https://oreil.ly/k8EyH),
    *Nature Computational Science* 3 (10): 833–38 (2023).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hagendorff, Thilo, 等人. [“大型语言模型中出现了类似人类的直觉行为和推理偏差，但在ChatGPT中消失了”](https://oreil.ly/k8EyH),
    *自然计算科学* 3 (10): 833–38 (2023).'
- en: Hobbhahn, Marius and Tamay Besiroglu. [“Predicting GPU Performance”](https://oreil.ly/TzddI),
    Epoch.ai, December 1, 2022.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Hobbhahn, Marius 和 Tamay Besiroglu. [“预测GPU性能”](https://oreil.ly/TzddI), Epoch.ai,
    2022年12月1日.
- en: Huang, Jiaxin, et al. [“Large Language Models Can Self-Improve”](https://oreil.ly/F30ZX),
    arXiv, October 2022.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Huang, Jiaxin, 等人. [“大型语言模型可以自我改进”](https://oreil.ly/F30ZX), arXiv, 2022年10月.
- en: 'Ji, Jiaming, et al. [“AI Alignment: A Comprehensive Survey”](https://oreil.ly/BAilf),
    arXiv, April 2025.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Ji, Jiaming, 等人. [“AI 对齐：全面调查”](https://oreil.ly/BAilf), arXiv, 2025年4月.
- en: 'Jin, Haolin, et al. [“From LLMs to LLM-Based Agents for Software Engineering:
    A Survey of Current, Challenges and Future”](https://oreil.ly/Xaf-t), arXiv, April
    2025.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Jin, Haolin, 等人. [“从LLM到基于LLM的软件工程代理：当前、挑战和未来的调查”](https://oreil.ly/Xaf-t),
    arXiv, 2025年4月.
- en: Lee, Jenya, et al. [“How Meta Trains Large Language Models at Scale”](https://oreil.ly/16e8R),
    *Engineering at Meta* (blog), June 12, 2024
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Lee, Jenya, 等人. [“Meta如何大规模训练大型语言模型”](https://oreil.ly/16e8R), *Meta工程博客*，2024年6月12日.
- en: Liu, Ruibo, et al. [“Best Practices and Lessons Learned on Synthetic Data for
    Language Models”](https://oreil.ly/gJ7J4), arXiv, August 2024.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Liu, Ruibo, 等人. [“关于语言模型合成数据的最佳实践和经验教训”](https://oreil.ly/gJ7J4), arXiv, 2024年8月.
- en: OpenAI. [SearchGPT Prototype](https://oreil.ly/kJtXj), July 25, 2024.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI. [SearchGPT 原型](https://oreil.ly/kJtXj), 2024年7月25日.
- en: OpenAI Platform. n.d. [Models](https://oreil.ly/pv6sA), accessed May 21, 2025.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 平台. n.d. [模型](https://oreil.ly/pv6sA), 访问日期：2025年5月21日.
- en: 'Orosz, Gergely, [“Scaling ChatGPT: Five Real-World Engineering Challenges”](https://oreil.ly/bMlsK),
    *The Pragmatic Engineer*, February 20, 2024.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Orosz, Gergely. [“扩展ChatGPT：五个现实世界的工程挑战”](https://oreil.ly/bMlsK), *实用工程师*，2024年2月20日.
- en: 'Pan, Shirui et al. [“Unifying Large Language Models and Knowledge Graphs: A
    Roadmap”](https://oreil.ly/ILvsK) *IEEE Transactions on Knowledge and Data Engineering*
    36 (7): 3580–99 (2024).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pan, Shirui 等人. [“统一大型语言模型和知识图谱：路线图”](https://oreil.ly/ILvsK) *IEEE知识数据工程杂志*
    36 (7): 3580–99 (2024).'
- en: Papi, Sara, et al. [“How ‘Real’ Is Your Real-Time Simultaneous Speech-to-Text
    Translation System?”](https://oreil.ly/IWfM4), arXiv, December 2024.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Papi, Sara, 等人. [“你的实时同声传译系统‘真实’吗？”](https://oreil.ly/IWfM4), arXiv, 2024年12月.
- en: 'Tu, Shangqing, et al. [“ChatLog: Carefully Evaluating the Evolution of ChatGPT
    Across Time”](https://oreil.ly/O3IFm), arXiv, June 2024.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Tu, Shangqing, 等人. [“ChatLog：仔细评估ChatGPT随时间演化的过程”](https://oreil.ly/O3IFm),
    arXiv, 2024年6月.
- en: 'Zhang, Zhehao, et al. [“Personalization of Large Language Models: A Survey”](https://oreil.ly/n6lZt),
    arXiv, May 2025.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang, Zhehao, 等人. [“大型语言模型的个性化：调查”](https://oreil.ly/n6lZt), arXiv, 2025年5月.
- en: Further Reading
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Hagendorff, Thilo, et al. [“Thinking Fast and Slow in Large Language Models”](https://oreil.ly/AQZbR),
    *Nature Computational Science* 3 (10): 833–38 (2023).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hagendorff, Thilo, 等人. [“大型语言模型中的快速思考和慢思考”](https://oreil.ly/AQZbR), *自然计算科学*
    3 (10): 833–38 (2023).'
