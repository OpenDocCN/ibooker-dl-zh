- en: Chapter 2\. Pre-Training Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#chapter_llm-introduction), we introduced language models,
    noted their strengths and limitations, explored current and potential use cases,
    and presented the scaling laws that seemingly govern progress in this field. To
    set the stage for the rest of this book, in the next three chapters we will discuss
    in detail the recipe for pre-training LLMs and the ingredients that go into them.
    But wait, this book is about utilizing pre-trained LLMs to design and build user
    applications. Why do we need to discuss the nuances of pre-training these gargantuan
    models from scratch, something most machine learning practitioners are never going
    to do in their lives?
  prefs: []
  type: TYPE_NORMAL
- en: Actually, this information is very important because many of the decisions made
    during the pre-training process heavily impact downstream performance. As we will
    notice in subsequent chapters, failure modes are more easily understandable when
    you comprehend the training process. Just like we appreciate having ingredients
    listed on packages at our grocery stores, we would like to know the ingredients
    that go into making a language model before we use it in serious applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Not much information is available in the public realm about some of the proprietary
    LLMs that are accessible only through an API. This book will provide as much information
    as has been made public. While the lack of information doesn’t mean that we should
    avoid using these models, model transparency is something that you might need
    to consider while making a final decision regarding what model to use.
  prefs: []
  type: TYPE_NORMAL
- en: Ingredients of an LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with the ingredients that go into making an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly speaking, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-training data: What’s it trained on?'
  prefs: []
  type: TYPE_NORMAL
- en: The old computer science adage “garbage in, garbage out” is still accurate when
    it comes to language modeling. In this chapter we will explore popular pre-training
    datasets and dig into the various preprocessing steps taken to ensure *high-quality*
    data is fed to the model. We will also showcase some tools that allow us to probe
    these datasets and understand how pre-training data composition impacts downstream
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vocabulary and tokenizer: What’s it trained over?'
  prefs: []
  type: TYPE_NORMAL
- en: To build a model over a language, we first have to determine the vocabulary
    of the language we are modeling and rules to break down a stream of text into
    the right vocabulary units, referred to as tokenization. (We will dedicate [Chapter 3](ch03.html#chapter-LLM-tokenization)
    to discussing these concepts.) Linguistically, humans process language in terms
    of meaning-bearing words and sentences. Language models process language in terms
    of tokens. We will explore the downstream impact when there is a mismatch between
    the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning objective: What is it being trained to do?'
  prefs: []
  type: TYPE_NORMAL
- en: By pre-training a language model, we aim to imbue the language model with general
    skills in syntax, semantics, reasoning, and so on, that hopefully will enable
    it to reliably solve any task you throw at it, even if it was not specifically
    trained on the task. Therefore the training objectives should be sufficiently
    general to capture all these skills. In [Chapter 4](ch04.html#chapter_transformer-architecture),
    we will discuss the various tasks (learning objectives) that pre-trained models
    are trained on. You might wonder if LLMs are better suited to solving downstream
    tasks that are similar to the tasks the pre-trained model has been trained to
    solve. We will test this assumption and discuss the impact various learning objectives
    have on task performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture: What’s its internal structure?'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a model refers to the components of a model, how they connect
    and interact with each other, and how they process input. Each architecture has
    its own inductive bias, a set of assumptions made about the data and tasks it
    will be used for, biasing the model toward certain types of solutions. In [Chapter 4](ch04.html#chapter_transformer-architecture),
    we will conduct a deep dive into the Transformer architecture, which, as discussed
    in [Chapter 1](ch01.html#chapter_llm-introduction), is the predominantly used
    architecture currently.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how these ingredients fit together in [Figure 2-1](#ingredients-of-llm).
  prefs: []
  type: TYPE_NORMAL
- en: '![LLM Ingredients](assets/dllm_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. How all the ingredients come together to make an LLM
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The language models trained using the process described in this chapter and
    the next are called *base models*. Lately, model providers have been augmenting
    the base model by fine-tuning it on much smaller datasets to steer them toward
    being more aligned with human needs and preferences. Some popular tuning modes
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised instruction fine-tuning (SFT), so that the model is better at following
    human instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning by human feedback (RLHF), so that the model is better
    aligned with human preferences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-adaptive or task-adaptive continued pre-training, so that the model is
    better attuned to specific domains and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the specific augmentation carried out, the resulting models are called
    *instruct models*, *chat models*, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover instruct and chat models in [Chapter 6](ch06.html#llm-fine-tuning),
    and domain-adaptive and task-adaptive pre-training in [Chapter 7](ch07.html#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: '![Derivative Models](assets/dllm_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. The relationship between base models and their derivatives
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pre-Training Data Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although it has been shown that higher-capacity models are relatively more [sample
    efficient](https://oreil.ly/PbN6F), in general today’s language models are very
    sample inefficient, meaning they need tons of examples to learn a task. It is
    infeasible to create such a large supervised dataset with human annotations, hence
    the predominant means to pre-train language models is using *self-supervised*
    learning, where the target labels exist within your training inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Using this setup, virtually any type of text is fair game to be included in
    a pre-training dataset, and theoretically any nontextual signal with some structure
    can be encoded in text and included as part of a pre-training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: From our scaling laws discussion in [Chapter 1](ch01.html#chapter_llm-introduction),
    we know that model performance increases by just training them longer and on more
    data. Also, as discussed in [Chapter 1](ch01.html#chapter_llm-introduction), the
    *consolidation effect* at play in the field raises expectations on what a single
    language model is expected to do end-to-end. Today a single model is expected
    to answer factual questions about the world, employ arithmetic and logical reasoning,
    write code, and come up with creative ideas.
  prefs: []
  type: TYPE_NORMAL
- en: All this means that the data needs for language model pre-training are enormous.
    Now, the key question is whether textual data available in the world actually
    contains sufficient and relevant signals needed to learn all the skills we want
    LLMs to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Note that language models that are trained solely on text only have access to
    the linguistic form, i.e., the sequence of characters making up a sentence like,
    “Walter White tossed the pizza onto the roof.” To understand its meaning, the
    linguistic form has to be mapped to the communicative intent of the writer/speaker.
    While a [section](https://oreil.ly/3iYA2) of the research community argues that
    one cannot learn meaning from form alone, recent language models are increasingly
    proving otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have access to the full picture, the linguistic form needs to be grounded
    to the real world. In the cognitive sciences, grounding is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: The process of establishing what mutual information is required for successful
    communication between two interlocutors
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chandu et al., [“Grounding ‘grounding’ in NLP”](https://oreil.ly/kPyXu)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Human text is generally very underspecified, with a lot of communicative intent
    existing outside the textual context, depending on the reader/listener to use
    their common sense, world knowledge, and ability to detect and understand emotional
    subtext to interpret it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is estimated that only around [12% of information](https://oreil.ly/jg4tW)
    we understand from text is explicitly mentioned in text. There are several theories
    explaining why we communicate thus, including [Zipf’s principle of least effort](https://oreil.ly/UX7Nd),
    which states it is “human nature to want the greatest outcome at the least amount
    of work.”
  prefs: []
  type: TYPE_NORMAL
- en: The field of NLP has seen [a lot of work](https://oreil.ly/PbIhT) in grounding
    language models to the real world. [Multimodal models](https://oreil.ly/ysAeM)
    that combine different modalities like image, video, speech, and text are a promising
    avenue of research, and they are likely to see more widespread usage in the coming
    years. Imagine a model seeing “pizza” in the training text, but also getting signals
    on how it looks, how it sounds, and how it tastes!
  prefs: []
  type: TYPE_NORMAL
- en: But do multimodal models really help with the grounding problem? Can we instead
    achieve the effect of grounding by just feeding the model with massive amounts
    of diverse text? These are unsolved questions, and there are good arguments in
    both directions as shown by this [debate](https://oreil.ly/oacht).
  prefs: []
  type: TYPE_NORMAL
- en: Whether training on massive amounts of text alone can enable language models
    to learn skills like logical reasoning is another open question. Note that text
    on the internet contains a lot of text describing reasoning steps, like theorem
    proofs, explanations of jokes, step-by-step answers to puzzles, and so on. However,
    there is simply not enough of derivational text going around, which leads us to
    cover the shortfall by using prompting methods like CoT (described further in
    [Chapter 5](ch05.html#chapter_utilizing_llms)). There is [recent evidence](https://oreil.ly/Qlntp)
    that process supervision, where feedback is provided for each step of the problem-solving
    process, as opposed to outcome supervision, where feedback is provided only on
    the final solution, helps improve arithmetic reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: A crucial skill that language models have to learn is dealing with the inherently
    ambiguous nature of language. Following up on the aforementioned Zipf’s principle
    of least effort, ambiguity enables speakers to manage the efficiency-clarity tradeoff
    in communication. We can leave a lot unsaid because we have established sufficient
    common ground with the people we are communicating with and trust that they are
    able to fill in the gaps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier language models struggled a lot with modeling ambiguity. I long used
    this sentence as a canonical example in my NLP talks to highlight ambiguity in
    language: “WWE’s John Cena surprises Make-A-Wish 7-year-old with cancer.”'
  prefs: []
  type: TYPE_NORMAL
- en: While state-of-the-art models are able to correctly interpret this particular
    sentence and not mistakenly identify John Cena as an evil disease-spreading wizard,
    [recent work](https://oreil.ly/BrSwb) shows that even the best models of today
    still struggle to deal with ambiguity in general. Whether just scaling up models
    and data is enough for LLMs to model ambiguity is an open question.
  prefs: []
  type: TYPE_NORMAL
- en: If our only option to resolve all these shortcomings is to scale up dataset
    sizes, the next question is if we actually have enough data available in the world
    that is sufficient for LLMs to learn these skills. Are we at risk of running out
    of training data anytime soon? There is a misconception in certain quarters of
    our field that we already have. However, lack of raw data is not yet a bottleneck
    in training models. For instance, there are billions of publicly available documents
    accessible by scraping or via a free API that haven’t yet made it into most pre-training
    data sets, such as parliamentary proceedings, court judgments, and most SEC filings.
    [“How much LLM training data is there, in the limit?”](https://oreil.ly/XnmHL)
    by Educating Silicon estimates the amount of text present in the world. On the
    other hand, it is true that at a sufficiently large scale, there is simply not
    enough naturally occurring data to feed our models.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, there are efforts to use text generated by language models, termed *synthetic
    data*, to train models, albeit with the [risk](https://oreil.ly/RdzX0) that training
    on LLM-generated data can potentially be detrimental, as the model deviates from
    the true distribution of the data. Later in this chapter, we will learn the process
    behind creating synthetic data for pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, not all data is created equal. We can achieve more sample efficiency
    with high-quality data, thus needing smaller dataset sizes. We can preprocess
    data in order to filter out low-quality data or make them higher quality. What
    exactly makes data high quality is a nuanced question, which we will explore later
    in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Popular Pre-Training Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of text is not freely available in public. This includes data exposed
    behind paywalled APIs and login screens, and paywalled books and documents, many
    of which may not even be digitized. Larger companies like Google and OpenAI can
    afford to purchase this data; for example, OpenAI has [struck deals](https://oreil.ly/ygIO2)
    worth hundreds of millions of dollars with the *Wall Street Journal*, *Financial
    Times*, and other news organizations for access to their data. Domain-specific
    text is often proprietary and available only to large incumbents (for example,
    Bloomberg trained [BloombergGPT](https://oreil.ly/87r4j) partly on its proprietary
    financial data). However, even for models trained by the largest companies, a
    significant proportion of training data comes from publicly available data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will cover some of the most popular general-purpose pre-training datasets
    that are being used to train LLMs. While this is not a comprehensive list, most
    LLMs, including closed-source ones, have at least a large subset of their training
    data drawn from these sources. We will defer discussion of domain-specific (catered
    to a particular field like social media, finance, biomedical, etc.) datasets to
    [Chapter 7](ch07.html#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Most general-purpose LLMs are trained to be a jack-of-all-trades—to be able
    to solve tasks from a variety of domains. If the data domain for your use case
    is included in the pre-training dataset, models trained on those datasets may
    show relative performance improvements on downstream tasks compared to models
    that aren’t, even if the pre-training data is unlabeled. This means that if you
    intend to use LLMs for specific, well-defined use cases in a particular domain,
    domain-specific models could prove promising. You can also perform *continued
    domain-adaptive* or *task-adaptive pre-training* on your domain data to leverage
    this phenomenon. This will be discussed in detail in [Chapter 7](ch07.html#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of commonly used data sources for general-purpose language
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: Common Crawl/C4
  prefs: []
  type: TYPE_NORMAL
- en: The web is the largest source of openly available textual data, and hence forms
    a significant proportion of pre-training datasets. [Common Crawl](https://oreil.ly/dhBvu)
    is a nonprofit that creates and publishes snapshots of all web crawl data, updated
    every month. However, as one could imagine, this is an extremely coarse dataset
    and needs to be significantly cleaned before it is ready to use. Google prepared
    C4 (Colossal Clean Crawled Corpus), a 750GB English-language dataset, after applying
    a set of preprocessing and filtering steps to a Common Crawl snapshot from 2019
    and released the code for it. [Dodge et al.](https://oreil.ly/bxmVR) used this
    script to reproduce C4 and have made it publicly available. C4 has been used for
    training several well-known LLMs including all models from the T5 family.
  prefs: []
  type: TYPE_NORMAL
- en: The Pile
  prefs: []
  type: TYPE_NORMAL
- en: '[The Pile](https://oreil.ly/7UAcY) is a 825GB dataset from Eleuther AI, which
    focused on publishing a dataset drawn from more diverse sources. Diversity of
    data is important since in-domain unlabeled data in pre-training is helpful for
    downstream performance on that domain, and diverse data sets also enable generalization
    to previously unseen tasks and domains. To this end, the data from The Pile comes
    not only from Common Crawl but also PubMed Central, arXiv, GitHub, the FreeLaw
    Project, Stack Exchange, the US Patent and Trademark Office, Ubuntu IRC, HackerNews,
    YouTube, PhilPapers, NIH ExPorter, Project Gutenberg, and Wikipedia, among others.
    The Pile and its subsets have been preferred as a data source for training several
    LLMs, including [Llama](https://oreil.ly/_8eOD).'
  prefs: []
  type: TYPE_NORMAL
- en: WebText/OpenWebText/OpenWebText2
  prefs: []
  type: TYPE_NORMAL
- en: These refer to a subset of web text and are limited to web pages representing
    outbound links on Reddit that have at least three *karma*, the absolute difference
    between user upvotes and downvotes. The assumption is that the wisdom of the crowd
    will enable only high-quality links to surface, which contain information people
    actually find interesting. Models that have been trained on this data include
    GPT-2 and GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia assumes a major role in the training of just about every general-purpose
    LLM. A full dump of Wikipedia contains valuable encyclopedic text that provides
    factual knowledge to the model. Wikipedia’s editorial system ensures that the
    text follows a highly structured format. However, it is not diverse stylistically,
    as the text is written in a formal manner. Therefore, Wikipedia alone is not sufficient
    to train a rudimentary language model and needs to be combined with data sources
    comprising diverse writing styles.
  prefs: []
  type: TYPE_NORMAL
- en: BooksCorpus/BooksCorpus2
  prefs: []
  type: TYPE_NORMAL
- en: Probably the most historically influential of all pre-training datasets, this
    dataset was part of the training corpus for well-known models like BERT, RoBERTa,
    GPT-2/3, etc. The BooksCorpus contains over 7,000 free, mostly fiction books written
    by unpublished authors. Twenty-six percent of books in the original dataset belonged
    to the Romance genre. A replication of the BooksCorpus is present in The Pile
    as BooksCorpus2.
  prefs: []
  type: TYPE_NORMAL
- en: FineWeb
  prefs: []
  type: TYPE_NORMAL
- en: As of the book’s writing, [FineWeb](https://oreil.ly/1GyZd) is the world’s largest
    publicly available pre-training dataset. Published by Hugging Face, FineWeb has
    15 trillion tokens and is drawn from 96 snapshots of Common Crawl, after a rigorous
    cleaning and filtering process. Hugging Face also released [FineWeb-Edu](https://oreil.ly/8XHH-),
    a subset of FineWeb composed of educational data, which is crucial in enabling
    LLMs to pass standardized tests and popular benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-1](#popular-pretraining-datasets) provides a list of some of the most
    commonly used datasets, their size, year of release, and the means to access them.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Popular pretraining datasets
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Data source(s) | Size | Year released | Public? | Models using this
    dataset |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | Common Crawl | 750GB | 2019 | Yes (reproduced version) | T5, FLAN-T5,
    UL2, Llama, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| The Pile | Common Crawl, PubMed Central, Wikipedia, arXiv, Project Gutenberg,
    Stack Exchange, USPTO, GitHub, etc. | 825GB | 2020 | Yes | GPT-NeoX, GPT-J, Cerebras-GPT,
    StableLM, Pythia, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| RedPajama | Common Crawl, GitHub, Wikipedia, arXiv, Stack Exchange, etc.
    | 1.2T tokens | 2023 | Yes | Red Pajama-INCITE, MPT |'
  prefs: []
  type: TYPE_TB
- en: '| BooksCorpus | Sampled from smashwords.com | 74M sentences | 2015 | Original
    not available anymore | Most models including BERT, GPT, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| OpenWebText2 | Outbound Reddit links | 65GB | 2020 | Yes | GPT-2, GPT-3 |'
  prefs: []
  type: TYPE_TB
- en: '| ROOTS | Big Science Catalogue, Common Crawl, GitHub | 1.6T tokens | 2022
    | No (but available on request) | BLOOM |'
  prefs: []
  type: TYPE_TB
- en: '| RefinedWeb | Common Crawl | 5T tokens | 2023 | Yes (600B subset only) | Falcon
    |'
  prefs: []
  type: TYPE_TB
- en: '| SlimPajama | Cleaned from RedPajama | 627B tokens | 2023 | Yes | N/A |'
  prefs: []
  type: TYPE_TB
- en: The table highlights the fact that most models are trained on similar data sources.
    In this chapter, we are limiting our coverage to pre-training datasets for base
    models. We will cover datasets used to augment base models like instruction tuning
    datasets, RLHF datasets, etc. in [Chapter 6](ch06.html#llm-fine-tuning).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the content of these pre-training datasets. Using a Google Colab
    notebook or a code editor of your choice, load the `realnewslike` subset of the
    C4 dataset, which consumes around 15 GB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using this code, we can observe all the instances in which Iceland appears in
    this C4 subset.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic Pre-Training Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An emerging trend is the use of LLMs to generate synthetic data that can be
    used for pre-training LLMs. One of the first success stories in training LLMs
    on datasets with a significant proportion of synthetic data is Microsoft’s [phi
    series of models](https://oreil.ly/eFphR). For the phi-1.5 model, Microsoft created
    20 billion tokens of synthetic data, using 20,000 seed topics and samples from
    real-world web datasets in their prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face released [Cosmopedia](https://oreil.ly/Pdwnw), an open source synthetic
    dataset used to train the SmolLM series of models. Its seed data included curated
    sources like Stanford courses, Khan Academy, and WikiHow, as well as general web
    data.
  prefs: []
  type: TYPE_NORMAL
- en: For curated sources, synthetic data was generated by extracting outlines of
    courses from Khan Academy and other sources and prompting the Mistral LLM to generate
    lengthy, detailed textbooks for individual sections. To generate diverse data
    at scale, Hugging Face issues several variants of the same prompt for each topic,
    like “create a textbook on this topic for young children” and “create a textbook
    on this topic for professionals.”
  prefs: []
  type: TYPE_NORMAL
- en: For general web data, Hugging Face clustered a subset of the RefinedWeb dataset
    into over a hundred topics. The LLM was then prompted with web page snippets and
    asked to generate an extensive blog post within the context of the topic the web
    page fell under. The cluster visualization can be explored in [Nomic Atlas](https://oreil.ly/t8R-6).
  prefs: []
  type: TYPE_NORMAL
- en: Training Data Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have collected or procured data, we need to filter and clean the data
    by running it through a preprocessing pipeline. Data preprocessing is the most
    unglamorous and underappreciated part of the LLM training pipeline, yet perhaps
    the most important. Based on my experience, spending more effort and resources
    during this phase can lead to significant downstream performance gains. As we
    walk through the data processing pipeline, I hope you come to appreciate the complexity
    of language text and the difficulty in processing it. Note that since these datasets
    are enormous, any preprocessing step should also be very efficient (ideally linear
    time).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-3](#data-collection) shows the typical preprocessing steps used to
    generate a pre-training dataset. The ordering of steps is not fixed, but there
    are dependencies between some of the steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data preprocessing pipeline](assets/dllm_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Data collection and preprocessing pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s go through these steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data Filtering and Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A majority of text extracted from HTML files is gibberish, like menu text from
    websites, boilerplate text, and random web page artifacts. There is a significant
    amount of pornography and toxic/hateful language on the web as well. For example,
    here is a text sample from an uncleaned version of the C4 dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Skip to Main Content Skip to Footer Skip to Email Signup Skip to Feedback Form
    MY REWARDS SIGN OUT SIGN IN & EARN REWARDS 0 Keyboard Controls Welcome to the
    main navigation. This menu has three levels of product categories. Use and keys
    to navigate between each category in the current level. Use the key to navigate
    down a level. Use the key to navigate up a level. Hit the key to be taken to the
    selected category page. Men What’s Hot New Arrivals Brand That Unites Performance
    Shop Online Exclusives Express Essentials Vacation Getaway Wedding Tuxedos Military
    Trend 9 Pieces / 33 Looks The Edit x Express NBA Collection Express + NBA Fashion
    NBA Game Changers Suiting & Blazers Find
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How useful do you think this text is for language and task learning?
  prefs: []
  type: TYPE_NORMAL
- en: Data from Common Crawl is made available via both raw HTML and web-extracted
    text (WET) format. While many dataset creators directly use the WET files, the
    open source organization Eleuther AI [noticed](https://oreil.ly/hciZS) that the
    quality of the WET files left much to be desired, with HTML boilerplate still
    prominent as seen above. To create The Pile, Eleuther AI thus used the [jusText
    library](https://oreil.ly/YRFzZ) to more reliably remove boilerplate text from
    HTML documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the effect of using jusText with an example. In your Google Colab
    or Jupyter notebook, try this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays all the boilerplate that is filtered out from a standard
    Wikipedia article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: jusText just so happens to be more aggressive in removing content, but this
    is generally OK for cleaning pre-trained datasets since there is an abundance
    of text available. Some alternative libraries used for this task include [Dragnet](https://oreil.ly/URvsq),
    [html2text](https://oreil.ly/xk7Hc), [inscriptis](https://oreil.ly/6-2z1), [Newspaper](https://oreil.ly/LPXe1),
    and [Trafilatura](https://oreil.ly/zdZxj). According to the creators of [The Pile](https://oreil.ly/DZG7w),
    dividing the extraction pipeline across multiple libraries can reduce the risk
    of the resulting dataset being affected by any bias introduced by one of these
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Boilerplate removal in web pages is a challenging task. Web pages may also contain
    code blocks, tables, and math formulas, which need careful processing. [Meta](https://oreil.ly/bXELJ)
    noted that it built a custom HTML parser for preparing the dataset to train Llama
    3\. It also mentioned that Meta retains the *alt* attribute in images, which it
    found contains useful information like math content.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs can also be utilized for accurate content extraction from web pages. However,
    as of this book’s writing, it is prohibitively expensive to do so, given the scale
    of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once text is extracted, the documents are passed through a series of data filtering
    steps. First, rudimentary filtering steps based on heuristics are applied. While
    the details differ across datasets, here are some of the steps typically performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Boilerplate removal
  prefs: []
  type: TYPE_NORMAL
- en: Only lines that end with punctuation, like the period, exclamation point, and
    question mark are retained. This ensures that menu text from websites is removed.
    Only lines with greater than a particular threshold of words and documents with
    greater than a particular threshold of sentences are retained. The latter helps
    in modeling long sequences, which is an important capability for language models
    to have. Documents containing “lorem ipsum…” and other boilerplate text are filtered
    out.
  prefs: []
  type: TYPE_NORMAL
- en: Non-English text removal
  prefs: []
  type: TYPE_NORMAL
- en: Libraries like langdetect, langid, fasttext, and pycld2 are used to detect the
    language of the text. For example, C4 retains text that has > 0.99 probability
    of English as judged by langdetect. Note that these libraries can also be used
    to remove boilerplate and web page artifacts since they give a lower probability
    of English to those texts.
  prefs: []
  type: TYPE_NORMAL
- en: Search engine optimization (SEO) text/spam removal
  prefs: []
  type: TYPE_NORMAL
- en: Documents with a lot of repeated character sequences are removed. Documents
    with a low proportion of closed class words are removed. Closed class words in
    English are function words like “of,” “at,” “the,” and “is.” If a page is engaged
    in keyword stuffing and other SEO tricks, then they would have a lower closed
    class words ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Pornographic/abusive text removal
  prefs: []
  type: TYPE_NORMAL
- en: Documents containing any words from keyword lists like the [“List of Dirty,
    Naughty, Obscene or Otherwise Bad Words”](https://oreil.ly/w3u_r) are removed.
  prefs: []
  type: TYPE_NORMAL
- en: Tools like langdetect and langid are helpful for speedy determination of the
    language in which the text is written at scale, but how do they deal with code-switched
    text (text in multiple languages, where English is often interspersed with a local
    language)?
  prefs: []
  type: TYPE_NORMAL
- en: 'You can try it! Here is an example for Taglish (Tagalog + English, which is
    a common mode of communication in the Philippines). In your notebook, run the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The second paragraph would get included in the C4 dataset, as per its filtering
    criteria (probability of English should be greater than .99). Therefore, even
    datasets that claim to be English-only routinely contain text in other languages,
    leading to surprising multilingual behavior during inference. Ever wondered why
    some monolingual models seem to perform well at machine translation? This is a
    major reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way langdetect is implemented makes it poor at identifying language when
    short sequences are provided. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: returns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: sk refers to Slovak here.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Quality Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all data is created equal. Text from a high school physics textbook is considered
    higher quality compared to promotional text about a footwear brand. There are
    several ways we can operationalize the notion of quality and separate high-quality
    from low-quality data. In this section we will highlight a few such ways.
  prefs: []
  type: TYPE_NORMAL
- en: Token-distribution K-L divergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this method, documents with a token distribution that deviates too much from
    a reference token distribution are removed. In effect, this removes documents
    that have a lot of outlier tokens. This is calculated by using the [Kullback-Liebler
    (K-L) divergence](https://oreil.ly/gd5GH).
  prefs: []
  type: TYPE_NORMAL
- en: Classifier-based approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also build a classifier for identifying high-quality data. A simple way
    to build a quality-based classifier is to have examples for the positive class
    come from high-quality data sources like Wikipedia, and examples for the negative
    class to be drawn from random documents in the Common Crawl data.
  prefs: []
  type: TYPE_NORMAL
- en: Meta employed a variety of classifier models for high-quality data extraction
    for its [Llama 3 model](https://oreil.ly/O-CKF). One of them was a [fasttext classification
    model](https://oreil.ly/EWic6) trained to identify if a text is likely to be referenced
    by Wikipedia. Meta also trained a classifier whose training data was generated
    by Llama 2 by providing it with cleaned web documents and quality requirements
    and asking it to determine if the quality requirements are met. To extract code
    and text containing reasoning steps, Meta built classifiers that can identify
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-4](#classifier-filtering) shows how a classifier can be built to
    discriminate between high-quality and low-quality data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![classifier-filtering](assets/dllm_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Classifier-based quality filtering
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Perplexity for quality selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Perplexity](https://oreil.ly/OfycZ), an intrinsic evaluation measure for language
    models, has been used for document filtering in the context of preparing pre-training
    datasets, notably by the creators of [CCNet](https://oreil.ly/VF98y). Perplexity
    measures how well a model can predict a given text; the lower the perplexity,
    the better the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Just like the classifier approach, we select documents from data sources that
    we deem high quality (like Wikipedia) as the positive class. We then train a 5-gram
    language model using [KenLM](https://oreil.ly/EU5r3) (a library facilitating training
    of n-gram language models) over it. Next, we take the dataset we want to filter
    and calculate the perplexity of each paragraph in it over the trained language
    model. The lower the perplexity, the more similar it is to the positive class.
    We can then discard documents with high perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: Low perplexity may not always be a good thing, however. Short, repetitive text
    can have low perplexity. Note that writing style gets factored into perplexity.
    If the reference language model is trained over Wikipedia, then documents written
    in an informal style may receive higher perplexity scores. Therefore, it would
    be beneficial to have a more involved filtering strategy.
  prefs: []
  type: TYPE_NORMAL
- en: To resolve this, the creators of [BERTIN](https://oreil.ly/uI9eV) introduced
    the concept of perplexity sampling. In perplexity sampling, instead of just filtering
    out low-perplexity text, it uses a sampling strategy that oversamples from the
    middle part of the perplexity probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-5](#perplexity-sampling) shows how perplexity sampling is achieved
    in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: '![perplexity-sampling](assets/dllm_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. Perplexity sampling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s explore the perplexity scores assigned by a model trained on Wikipedia
    text. Download this [file](https://oreil.ly/xwYjY). After placing the file in
    your home directory, run this code in a new file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`` `###### Note    According to an [analysis of C4](https://oreil.ly/Nzla7),
    the internet domain that contributed the largest proportion of text in the dataset
    was patents.google.com. Over 10% of the text from this domain is in fact machine
    translated, with patents from countries like Japan being translated from Japanese
    to English. So a significant amount of pre-training data is already not generated
    by humans!    Propelled by LLMs, the internet is slated to see widespread prevalence
    of AI-generated text. Recognizing whether text was written by a human or an LLM
    is a nontrivial task and certainly not feasible at scale. How this will affect
    future LLM performance is an open research question.    Despite all the data cleaning
    steps, the resulting dataset is still not going to be perfect at this level of
    scale. For example, Eleuther AI [reported](https://oreil.ly/WEBne) that the boilerplate
    sentence “select the forum that you want to visit from the selection below” occurs
    180K times in The Pile.` ``  [PRE10]` [PRE11] ## Deduplication    So far we have
    discussed data extraction and cleaning, language identification, and quality filtering.
    Let’s now explore the most contentious step in the pipeline: deduplication.    We
    know that web-crawled text is ridden with a lot of duplicates. Duplicates form
    a nontrivial portion of the training dataset, so any decision made about them
    will have a noticeable impact on the ensuing model.    How do we define a duplicate?
    We will make a distinction between three kinds:    Exact matches      Two sequences
    with the same text are exact-match duplicates. They are the easiest to handle.      Approximate
    matches      In many cases, there are near-duplicates, where sequences of text
    are identical except for a few characters. Sometimes these sequences are slightly
    different only due to HTML text extraction artifacts and other filtering processes.      Semantic
    duplicates      Duplicates that semantically convey the same content but using
    different wordings. This is usually treated as out of scope.      Duplicates can
    also be categorized based on the granularity at which they occur:    Document-level
    duplicates      Duplicate documents are removed during the preparation of most
    pre-training datasets. However, in some datasets like The Pile, certain subsets
    (like Wikipedia) are deliberately duplicated, so that they are seen more often
    by the model.      Sequence-level duplicates      These are lines or sentences
    in documents that are repeated across multiple documents. In some cases they can
    be massively duplicated, like terms of service text, copyright notices, website
    prefaces, etc.      ###### Note    Dededuplication is a very complex process,
    typically performed using the MinHash algorithm. This writeup by [Cheng Hao](https://oreil.ly/2RO9f)
    details the deduplication process followed in the Big Science and Big Code open
    source LLM projects.    Deduplicating data has several benefits:    *   A small
    subset of the pre-training dataset is usually set aside for validation/test. Deduplication
    can ensure the removal/reduction of overlap between the train and test sets, which
    is essential for an unbiased evaluation. Without sequence-level deduplication,
    there is a high likelihood of overlap of common text sequences in the train and
    test sets.           *   Removing duplicate sequences reduces the overall size
    of the training dataset. However, [Lee et al.](https://oreil.ly/k5OwJ) show that
    the perplexity of a model trained on the smaller dataset isn’t affected. Thus,
    the model can be trained for a shorter period yet with the same benefit.           *   Deduplication
    can also reduce the tendency of the model to memorize its training data. Memorization
    is closely linked to model overfitting and thwarts the model’s ability to generalize.
    While there are many ways to quantify memorization, we will focus on *memorization
    by generation*, where a model is said to have memorized a sequence if it is capable
    of generating it verbatim. [Lee et al.](https://oreil.ly/xpoz7) have shown that
    models trained on datasets that have been deduplicated at the sequence level generate
    ten times less verbatim training data.              ###### Tip    One advantage
    of using models trained on publicly available datasets is that you can search
    through the datasets to see if the text generated by the model exists verbatim
    in the dataset.    [Figure 2-6](#privacy-attacks-against-llms) demonstrates the
    flow of a rudimentary training-data extraction attack.  ![Privacy attacks](assets/dllm_0206.png)  ######
    Figure 2-6\. Privacy attacks against LLMs    ## Removing Personally Identifiable
    Information    While deduplication can reduce the likelihood of the model memorizing
    training data, it is by no means a panacea for the memorization problem. Even
    information that appears only once in the training set could potentially be memorized
    (and leaked). While a lot of content in the training data is innocuous (terms
    of service text) and perhaps even desirable to memorize (factual information,
    like the capital of Canada), memorization of personally identifiable information
    (PII) is a major concern.    Let us see what PII entails. The formal definition
    from [Cornell Law](https://oreil.ly/kN3J8) is as follows:    > Information that
    can be used to distinguish or trace an individual’s identity, either alone or
    when combined with other personal or identifying information that is linked or
    linkable to a specific individual.    Based on this definition, non-PII can become
    PII when another piece of information becomes public, which when combined with
    the non-PII can be used to uniquely identify an individual.    The legal definition
    of PII varies by jurisdiction. For example, the [General Data Protection Regulation
    (GDPR)](https://oreil.ly/F2dGL) in Europe says:    > Protection should be extended
    to anything used to directly or indirectly identify a person (or data subject).
    This may be extended to include characteristics that describe “physical, physiological,
    genetic, mental, commercial, cultural, or social identity of a person.”    Most
    open source models are trained on publicly available datasets. These datasets
    might contain PII, but one might be tempted to say, “Well it is already out in
    the open, so there is no need for privacy protection.” This argument overlooks
    the importance of consent and discoverability controls. For instance, I might
    have shared my PII on my blog, which resides in an obscure corner of the internet
    and is not easily discoverable through search engines, but if it ends up being
    added to a pre-training dataset, it suddenly brings this data into the spotlight,
    without my consent. This concept is called *contextual integrity*: data should
    only be shared in the original context in which it was shared.    So ideally,
    we would like to *detect* PII in the dataset, and then *remediate* it in some
    fashion, so that the PII is no longer present in the training data or at least
    not memorizable. The presence of *public-figure PII* adds a layer of complexity
    to this problem. We would like our model to be able to accurately answer factual
    questions about public figures, such as providing their birth date. The privacy
    expectations for public figures are lower, showcasing how the values of transparency
    and openness clash with privacy. Determining who is a public figure and what level
    of privacy they are entitled to is a complex socio-technical challenge.    Data
    considered private includes names, addresses, credit card data, government IDs,
    medical history and diagnosis data, email IDs, phone numbers, identity and affinity
    groups the person belongs to (religion, race, union membership), geolocation data,
    and so on.    Attacks can be either targeted or untargeted. In an untargeted attack,
    the attacker just generates a large body of text using the model and then runs
    a membership inference attack to determine text within it that is most likely
    to be memorized. In a targeted attack, the attacker attempts to recover personal
    information about a particular individual or a group of individuals. Targeted
    attacks are more difficult to execute, because while language models are good
    at memorization, they are bad at *association*, for instance, identifying that
    an email ID belongs to a specific person.    Most pre-training datasets have undergone
    little to no PII remediation. The Privacy working group (of which I was the co-lead)
    of the Big Science project that trained the BLOOM model developed a pipeline for
    PII detection and remediation, which we will discuss next.    ###### Note    Language
    models are also susceptible to training data poisoning attacks. Since a large
    portion of training data is sourced from web-crawled text, bad actors have an
    opportunity to influence the content of the training set. [Tramer er al.](https://oreil.ly/g_A-d)
    have shown that one can poison less than 0.1% of the training set with data whose
    effect is to make it easier for other data in the training set to leak more easily.    As
    LLMs increasingly get used as search engines, the demand for LLM SEO is cropping
    up. For example, a company could write content on their web sites in a manner
    that makes it more likely to be chosen in a pre-training dataset creation process
    that uses perplexity filtering.    [Figure 2-7](#PII-processing-pipeline) shows
    a typical PII processing pipeline.  ![PII Processing Pipeline](assets/dllm_0207.png)  ######
    Figure 2-7\. PII processing pipeline    ### PII detection    The task of PII detection
    is similar to the NLP task of NER, introduced in [Chapter 1](ch01.html#chapter_llm-introduction).
    However, not all named entities constitute PII. For our task we determined the
    PII tags to be PERSON, AGE, NORP (nationality, race, religion, political party
    affiliation, socio-economic class, and union membership), STREET_ADDRESS, CREDIT_CARD,
    GOVT_ID, EMAIL_ADDRESS, USER_ID, and PUBLIC_FIGURE.    We used the PUBLIC_FIGURE
    tag to identify information about public figures, since we didn’t want to filter
    them out. We also assigned fictional characters this tag.    Some of the structured
    tags in this list like emails and government IDs can be identified using regular
    expressions. For other tags, we annotated datasets that could then be used to
    train Transformer-based NER-like models. Interestingly, we observed a very high
    degree of inter-annotator disagreement (same example being annotated differently
    by different people) that underscored the cultural nuances of the definition of
    privacy and what constitutes personal information.    Here is the [regular expression](https://oreil.ly/8YwG9)
    to detect SSN (US Social Security numbers):    [PRE12]py   [PRE13] [PRE14] from
    faker import Faker fake = Faker(''en_IN'')   # Indian locale Faker.seed(0) for
    i in range(5):    print(fake.aadhaar_id) [PRE15] for i in range(5):    print(fake.address)
    [PRE16]` [PRE17][PRE18][PRE19] `` `# Effect of Pre-Training Data on Downstream
    Tasks    Given a pre-training dataset for an LLM, what assumptions can we make
    from it about downstream performance? It turns out that there is a correlation
    between the model’s performance on a given task or input and the pre-training
    dataset frequency of the task or the salient words in the input, respectively.
    First observed by [Razeghi et al.](https://oreil.ly/cPYej), this phenomenon has
    been studied in detail in McCoy et al.’s [“Embers of Autoregression” paper](https://oreil.ly/_O2NK).    McCoy
    et al. show that language models perform better at tasks that are more frequently
    represented in the training dataset than ones that are less frequently represented.
    For example, language models are better at base 10 addition than base 9 addition.
    They are also better at sorting by alphabetical order than they are at sorting
    by reverse alphabetical order.    Similarly, McCoy et al. also show that for a
    given task, models perform relatively better when the output is text with high
    frequency in the pre-training dataset as opposed to when the text is lower frequency.
    This phenomenon is also observed for inputs; models do relatively better with
    higher-frequency inputs compared to lower-frequency inputs.    As an example,
    consider the sentence: “record a be that miles, yes, hour, per fifty clocked he.”
    We ask the LLM to reverse the words in the sentence, which would lead to “He clocked
    fifty per hour, yes, miles, that be a record,” a rather low-probability sequence,
    due to its odd linguistic construction.    As of the book’s writing, GPT-4o returns
    the wrong answer: “He clocked fifty miles per hour that be a record,” but you
    can notice that it performs relatively better when the output sequence is higher
    probability.    # Bias and Fairness Issues in Pre-Training Datasets    A multitude
    of ethical questions arise during the productization of large language models.
    The existence of significant bias and fairness issues in these models often leads
    to a no-ship condition for a large number of use cases. In this section we will
    go through some bias and fairness issues specifically related to the collection
    and filtering of pre-training data.    The scale of data that LLMs are fed with
    means that they are not just constructing models of language but also of the world
    we inhabit. This gives rise to the question of whether we want to model the world
    the way it is or the way we would like it to be. The internet is filled with hate,
    violence, and abusive language and is often used as an outlet for humanity’s worst
    impulses. The text in it implicitly encodes long-existing biases against groups
    of people. For example, in The Pile, an [analysis](https://oreil.ly/hu3-b) of
    word co-occurrence statistics shows the word “radical” co-occurs with the word
    “Muslim” substantially more than it does for other religions.    The phenomenon
    of *bias amplification* makes these problems all the more critical. It has been
    shown that large language models [amplify the biases](https://oreil.ly/x-ba9)
    that are encoded in their pre-training data: they make biased predictions against
    groups of people at higher rates than what the training data statistics would
    suggest.    So, can we “fix” our training data such that we can model a world
    that encodes our values and principles that downstream applications will inherit?
    There is substantial debate in the research community about this. Opponents argue
    it is hard to identify and fix all societal biases encoded in the data since there
    are so many dimensions of bias that intersect in complex ways. Values are not
    universal, and model providers would like to be value-neutral to cater to all
    sections of society.    However, as Anna Rogers describes in her [paper](https://oreil.ly/hxU_-),
    this question is already moot. Data curation is already happening, whether we
    like it or not, and the values and interests of model providers are already being
    encoded into the models. For example, only a small proportion of available data
    is selected to be part of the pre-training set. This selection process is not
    value-neutral, even if one might not explicitly think in terms of it.    Wikipedia
    is one of the more popular datasets used in training LLMs. While it might be a
    no-brainer to include Wikipedia in a pre-training dataset, let’s explore the implications.
    Wikipedia is edited by volunteers, a very large proportion of them being men.
    Since the determination of whether a topic is reputable enough to deserve a Wikipedia
    page rests with the editors who are largely made up of men, we see disparities
    like obscure male football players from lower-level leagues getting their own
    pages while a disproportionate number of biography articles about women are slated
    for deletion.    Similarly, the highly influential WebText dataset is sourced
    from Reddit outbound links. Reddit is a predominantly male site, with [74% of
    users](https://oreil.ly/i2RkB) being men. Naturally, links posted on Reddit are
    more likely to be catered to male interests.    Bias can also be introduced during
    the data filtering stages. Earlier, we noted that keyword lists are often used
    to filter out pornographic material and abusive text. However, using a naive keyword
    list is a lazy approach that not only has problems with effectiveness (false negatives)
    but also inadvertently [results in](https://oreil.ly/XWBjV) filtering out positive
    text written by or about minority communities, as well as text written in dialects
    like African American English and Hispanic-aligned English. The fact that words
    in English have multiple senses has resulted in certain documents about breastfeeding
    being filtered out of the C4 dataset.    Overall, whether a word is hateful, abusive,
    or toxic depends on the social context, the intentions of the reader, and the
    intended audience. Keyword-based methods simply do not capture this nuance. The
    question of whether it is more effective to handle these issues at the pre-training
    stage or further downstream is an open area of research. We will explore techniques
    that can be employed downstream in [Chapter 10](ch10.html#ch10).    ###### Note    The
    authors of the [Pythia model](https://oreil.ly/r4oAT) experimented by replacing
    masculine pronouns with feminine ones for the last 7% of training tokens and noticed
    a de-biasing impact on downstream tasks.    # Summary    In this chapter, we outlined
    the key ingredients of a language model: the pre-training data, the vocabulary
    and tokenizer, the language objective, and the model architecture. We walked through
    the steps involved in creating a pre-training dataset in detail, including language
    identification, text extraction and cleaning, quality filtering, deduplication,
    PII removal, and test set decontamination. We also provided a list of commonly
    used pre-training datasets and the steps taken for preprocessing each of them.
    In the next chapter, we will explore the vocabulary and tokenizer of the language
    model: the language we intend the model to learn.    ^([1](ch02.html#id625-marker))
    From Dodge et al., [“A Case Study on the Colossal Clean Crawled Corpus”](https://oreil.ly/PwtVp),
    EMNLP 2021.` `` ```'
  prefs: []
  type: TYPE_NORMAL
