- en: Capitolo 12\. Distribuzione dei servizi di intelligenza artificiale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  prefs: []
  type: TYPE_NORMAL
- en: In questo capitolo finale, è il momento di completare la tua soluzione GenAI
    distribuendola. Go imparerà diverse strategie di distribuzione e, come parte della
    distribuzione, conterrà i suoi servizi con Docker seguendo le sue migliori pratiche.
  prefs: []
  type: TYPE_NORMAL
- en: Opzioni di distribuzione
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ora hai un servizio GenAI funzionante che vuoi rendere accessibile ai tuoi
    utenti. Quali sono le opzioni di distribuzione? Ci sono alcune strategie di distribuzione
    comuni che puoi adattare per rendere le tue app accessibili agli utenti:'
  prefs: []
  type: TYPE_NORMAL
- en: Macchine virtuali (VM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Funzioni serverless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piattaforme applicative gestite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerizzazione
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analizziamo ciascuno di essi in modo più dettagliato.
  prefs: []
  type: TYPE_NORMAL
- en: Distribuzione su macchine virtuali
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Se intendi utilizzare i tuoi server on-premises o preferisci distribuire i tuoi
    servizi sullo stesso hardware che ospita le altre applicazioni per ottenere un
    elevato isolamento e sicurezza, puoi distribuire il tuo servizio GenAI in una
    VM.
  prefs: []
  type: TYPE_NORMAL
- en: Una macchina virtuale è un'emulazione software di un computer fisico che esegue
    un sistema operativo (OS) e delle applicazioni. Non è diversa da un computer fisico
    come un laptop, uno smartphone o un server.
  prefs: []
  type: TYPE_NORMAL
- en: Il sistema *host* della VM fornisce risorse come CPU, memoria e storage, mentre
    un livello software chiamato *hypervisor* gestisce la VM e alloca le risorse dall'host
    alla VM. Le risorse che l'hypervisor alloca alla VM sono l'*hardware virtuale*
    su cui girano il sistema operativo e le applicazioni.
  prefs: []
  type: TYPE_NORMAL
- en: La macchina virtuale può essere eseguita direttamente sull'hardware dell'host
    (bare metal) o su un sistema operativo convenzionale (cioè essere ospitata). Di
    conseguenza, il sistema operativo installato all'interno della macchina virtuale
    viene chiamato *sistema operativo guest*.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 12-1](#deployment_vm_architecture) mostra l'architettura del sistema
    della tecnologia di virtualizzazione.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1201](assets/bgai_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 12-1\. Architettura del sistema di virtualizzazione
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I provider Cloud o il tuo data center possono essere costituiti da diversi server
    fisici, ognuno dei quali ospita più macchine virtuali con il proprio sistema operativo
    guest e le proprie applicazioni ospitate. Per una condivisione delle risorse efficace
    dal punto di vista dei costi, queste macchine virtuali possono condividere la
    stessa unità di archiviazione fisica montata, anche se sono contenute in ambienti
    completamente isolati, come puoi vedere nella [Figura 12-2](#deployment_vm_data_center).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1202](assets/bgai_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 12-2\. Macchine virtuali ospitate in un data center
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Il vantaggio di usare una macchina virtuale è che hai accesso diretto al sistema
    operativo guest, alle risorse hardware virtuali e ai driver della GPU. Se ci sono
    problemi con l'implementazione, puoi connetterti alla macchina virtuale tramite
    il protocollo *Secure Shell Transfer* (SHH) per ispezionare i log dell'applicazione,
    impostare l'ambiente dell'applicazione e fare il debug dei problemi di produzione.
  prefs: []
  type: TYPE_NORMAL
- en: Per distribuire i tuoi servizi sulle macchine virtuali è sufficiente clonare
    il repository del codice sulla macchina virtuale e installare le dipendenze, i
    pacchetti e i driver necessari per avviare con successo l'applicazione. Tuttavia,
    il metodo consigliato è quello di utilizzare una piattaforma di containerizzazione
    come Docker in esecuzione sulla macchina virtuale per consentire distribuzioni
    continue e altri vantaggi. Dovresti anche assicurarti di dimensionare le risorse
    della macchina virtuale in modo appropriato, in modo che i tuoi servizi non siano
    affamati di core CPU/GPU, memoria o spazio su disco.
  prefs: []
  type: TYPE_NORMAL
- en: Con le macchine virtuali on-premises puoi risparmiare sui costi di hosting on-cloud
    o di noleggio dei server e puoi proteggere completamente i tuoi ambienti applicativi
    per una manciata di utenti, isolati dalla rete internet pubblica. Questi vantaggi
    sono ottenibili anche con le macchine virtuali in cloud, ma richiedonouna configurazioneaggiuntiva
    delle reti e delle risorse da configurare. Inoltre, puoi avere accesso all'hardware
    delle GPU e configurare i driver per i requisiti delle tue applicazioni.
  prefs: []
  type: TYPE_NORMAL
- en: Tieni presente che l'utilizzo del modello di distribuzione VM potrebbe non essere
    facilmente scalabile e richiede un notevole sforzo di manutenzione. Inoltre, i
    server VM funzionano normalmente 24 ore su 24, 7 giorni su 7, comportando costi
    di gestione costanti, a meno che tu non ne automatizzi l'avvio e lo spegnimento
    in base alle tue esigenze. Sarai responsabile dell'applicazione delle patch di
    sicurezza, degli aggiornamenti del sistema operativo e degli aggiornamenti dei
    pacchetti, oltre che delle configurazioni di rete. Con l'accesso diretto alle
    risorse hardware, dovrai anche prendere un maggior numero di decisioni che possono
    rallentare la tua attività, portandoti alla stanchezza decisionale.
  prefs: []
  type: TYPE_NORMAL
- en: Il mio consiglio è quello di implementare una macchina virtuale se non hai intenzione
    di scalare i tuoi servizi a breve o se hai bisogno di mantenere bassi i costi
    dei server e un ambiente applicativo isolato e sicuro per una manciata di utenti.
    Inoltre, assicurati di aver pianificato un tempo sufficiente per l'implementazione,
    il collegamento in rete e la configurazione delle macchine virtuali.
  prefs: []
  type: TYPE_NORMAL
- en: Distribuire le funzioni Serverless
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Oltre alle macchine virtuali, puoi anche distribuire i tuoi servizi su funzioni
    cloud che i provider cloud forniscono come sistemi *serverless*. Nel serverless
    computing, il tuo codice viene eseguito in risposta a eventi come le modifiche
    al database, gli aggiornamenti dei blob in uno storage, le richieste HTTP o i
    messaggi aggiunti a una coda. Questo significa che paghi solo per le richieste
    o le risorse di calcolo che i tuoi servizi utilizzano, invece che per un intero
    server come nel caso di una macchina virtuale in esecuzione continua.
  prefs: []
  type: TYPE_NORMAL
- en: 'Le implementazioni serverless sono spesso utili quando:'
  prefs: []
  type: TYPE_NORMAL
- en: Vuoi avere sistemi guidati dagli eventi invece di una macchina virtuale in esecuzione,
    che potrebbe essere attiva 24 ore su 24, 7 giorni su 7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vuoi distribuire i tuoi servizi API utilizzando un'architettura serverless che
    sia altamente efficiente dal punto di vista dei costi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I tuoi servizi devono eseguire lavori di elaborazione batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hai bisogno dell'automazione del flusso di lavoro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Il termine *serverless* non significa che le funzioni del cloud non richiedono
    risorse hardware per essere eseguite, ma piuttosto che la gestione di queste risorse
    è gestita dal provider del cloud. Questo ti permette di concentrarti sulla scrittura
    del codice dell'applicazione senza preoccuparti dei dettagli a livello di server
    e di sistema operativo.
  prefs: []
  type: TYPE_NORMAL
- en: I Cloud provider istanziano le risorse di calcolo per soddisfare la domanda
    dei loro clienti. Spesso si verifica un'impennata della domanda, che richiede
    la creazione di risorse aggiuntive in anticipo per gestire il picco di richieste.
    Tuttavia, una volta che la domanda diminuisce, rimangono risorse di calcolo non
    allocate in eccesso che devono essere chiuse o condivise tra altri clienti.
  prefs: []
  type: TYPE_NORMAL
- en: La rimozione e la creazione di risorse è un'operazione di calcolo intensiva
    da eseguire. Su scala, queste operazioni comportano costi significativi per i
    cloud provider. Per questo motivo, i cloud provider preferiscono mantenere queste
    risorse attive il più possibile e distribuirle tra i clienti esistenti per massimizzare
    la fatturazione.
  prefs: []
  type: TYPE_NORMAL
- en: Per incoraggiare i clienti a utilizzare questi calcoli in eccesso, hanno creato
    dei servizi di funzioni cloud che puoi sfruttare per eseguire i tuoi servizi backend
    su calcoli in eccesso (cioè serverless). Fortunatamente, esistono pacchetti come
    Magnum che ti permettono di pacchettizzare i serviziFastAPI su funzioni cloud
    AWS. Vedrai presto che i servizi FastAPI possono essere distribuiti anche come
    funzioni cloud Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Devi tenere presente che a queste funzioni viene assegnata solo una piccola
    quantità di risorse e hanno un timeout breve. Tuttavia, puoi richiedere timeout
    più lunghi e l'assegnazione di risorse di calcolo, ma potrebbe essere necessario
    più tempo per ricevere queste allocazioni, con conseguenti latenze più elevate
    per i tuoi utenti.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Se la tua logica aziendale consuma molte risorse o richiede più di una manciata
    di minuti per essere eseguita, le funzioni cloud potrebbero non essere un'opzione
    di distribuzione adatta a te.
  prefs: []
  type: TYPE_NORMAL
- en: Tuttavia, puoi dividere i tuoi servizi FastAPI in più funzioni, con ogni funzione
    che gestisce un singolo endpoint esposto. In questo modo, puoi distribuire parti
    del tuo servizio come funzioni Cloud, riducendo la parte del servizio FastAPI
    che deve essere distribuita con altri metodi.
  prefs: []
  type: TYPE_NORMAL
- en: 'Il vantaggio principale dell''utilizzo di funzioni serverless per l''implementazione
    dei tuoi servizi è la loro scalabilità: puoi scalare le tue applicazioni in base
    alle esigenze e pagare solo una frazione del costo rispetto alla prenotazione
    di risorse VM dedicate. I fornitori di Cloud solitamente applicano tariffe basate
    sul numero di esecuzioni delle funzioni e sul runtime, spesso con generose quote
    mensili. Questo significa che se le tue funzioni vengono eseguite rapidamente
    e hai un numero moderato di utenti contemporanei, potresti essere in grado di
    ospitare tutti i tuoi servizi gratuitamente.'
  prefs: []
  type: TYPE_NORMAL
- en: Inoltre, i fornitori di cloud forniscono anche runtime di funzioni che puoi
    installare localmente per i test e lo sviluppo locale, in modo da accorciare notevolmente
    le iterazioni di sviluppo.
  prefs: []
  type: TYPE_NORMAL
- en: Ogni cloud provider ha un proprio approccio alla distribuzione delle funzioni
    serverless. Spesso è necessario uno script di ingresso, come *main.py*, che può
    importare le dipendenze da altri moduli secondo le necessità. Oltre allo script
    di ingresso, dovrai caricare un file di configurazione JSON dell'host della funzione
    insieme al *file requirements.txt* per le dipendenze necessarie da installare
    al momento della distribuzione su un runtime Python.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi quindi distribuire le funzioni caricando tutti i file necessari in una
    cartella zippata o utilizzando pipeline CI/CD che si autenticano con il provider
    ed eseguono i comandi di distribuzione all'interno del tuo progetto cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'A titolo di esempio, proviamo a distribuire un''applicazione FastAPI semplice
    e semplice che restituisca risposte LLM. La struttura del progetto sarà la seguente:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Puoi quindi pacchettizzare un'applicazione FastAPI come [funzione Azure serverless](https://oreil.ly/ZaOuF)
    seguendo i prossimi esempi di codice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dovrai installare il pacchetto `azure-functions` per eseguire il runtime delle
    funzioni serverless di Azure per lo sviluppo e i test locali:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2][PRE3] `"extensions"``:` `{` [PRE4] `"http"``:` `{` [PRE5]`py [PRE6]
    [PRE7]`` [PRE8]` Successivamente, implementa il tuo servizio GenAI con il servizio
    FastAPI come di consueto seguendo l''[Esempio 12-2](#deployment_function_azure_app).    #####
    Esempio 12-2\. Semplice applicazione FastAPI che serve le risposte LLM    [PRE9]    Infine,
    avvolgi il tuo FastAPI `app` all''interno di `func.AsgiFunctionApp` in modo che
    il runtime delle funzioni serverless di Azure possa agganciarsi ad esso, come
    mostrato nell''[Esempio 12-3](#deployment_function_azure_function).    ##### Esempio
    12-3\. Distribuzione di un servizio FastAPI con Azure Functions    [PRE10]    Puoi
    quindi avviare l''applicazione della funzione eseguendo il comando `func start`,
    che dovrebbe essere disponibile come comando CLI una volta installato ilpacchetto
    `azure-functions`:    [PRE11] >> Functions: `` >>        http_app_func: `[`GET,POST,DELETE,HEAD,PATCH,PUT,OPTIONS`]`
    `\`                           http://localhost:7071//`{`*route`}` `>> Job host
    started` `` [PRE12]   [PRE13] [PRE14]`py [PRE15]py`` [PRE16]py[PRE17][PRE18][PRE19][PRE20][PRE21]
    [PRE22][PRE23][PRE24][PRE25][PRE26] [PRE27]`py[PRE28]py[PRE29]`py[PRE30]py[PRE31]py[PRE32]py[PRE33][PRE34][PRE35][PRE36][PRE37]py[PRE38]py[PRE39]py[PRE40][PRE41]
    ### Gestione dei permessi del filesystem    Una grande fonte di frustrazione e
    un problema di sicurezza per molti sviluppatori alle prime armi con Docker è la
    gestione dei permessi delle directory quando si utilizzano i mount bind del filesystem
    tra il sistema operativo host e il container.    Per impostazione predefinita,
    Docker esegue i container come utente `root` e di conseguenza i container hanno
    pieno accesso in lettura/scrittura alle directory montate sul sistema operativo
    host. Se l''utente `root` all''interno del container crea directory o file, questi
    saranno di proprietà di `root` anche sull''host. Puoi quindi incorrere in problemi
    di permessi se hai un account utente non root sull''host quando cerchi di accedere
    o modificare queste directory o file.    ###### Avvertenze    L''esecuzione dei
    container come utente predefinito di `root` è anche un grande rischio per la sicurezza
    se un malintenzionato riesce ad accedere al container, poiché avrà accesso al
    sistema host come `root`. Inoltre, se esegui un''immagine compromessa, potresti
    rischiare di eseguire codice maligno sul sistema host con i privilegi di `root`.    Per
    attenuare i problemi di permessi durante l''esecuzione di container con mount
    bind, puoi utilizzare il flag `--user` per eseguire il container come utente non
    root:    [PRE42]py   [PRE43] In alternativa, puoi creare e passare a un utente
    non root nei livelli finali della creazione dell''immagine all''interno del file
    Docker, come mostrato nell''[Esempio 12-6](#docker_permissions).    ##### Esempio
    12-6\. Creazione e passaggio all''utente non root durante la creazione delle immagini
    del container (solo container Ubuntu/Debian)    [PRE44]    [![1](assets/1.png)](#co_deployment_of_ai_services_CO3-1)      Usa
    gli argomenti di compilazione per specificare le variabili durante la creazione
    dell''immagine.      [![2](assets/2.png)](#co_deployment_of_ai_services_CO3-2)      Crea
    un gruppo di utenti con il nome `USER_GID`.      [![3](assets/3.png)](#co_deployment_of_ai_services_CO3-3)      Disabilita
    completamente il login degli utenti, compreso quello basato sulla password.      [![4](assets/4.png)](#co_deployment_of_ai_services_CO3-4)      Evita
    di creare una home directory per l''utente.      [![5](assets/5.png)](#co_deployment_of_ai_services_CO3-5)      Crea
    un account utente non root con il nome `$USER_UID` e assegnalo al gruppo `USER_GID`
    appena creato. Imposta il nome dell''account utente su `fastapi`.      [![6](assets/6.png)](#co_deployment_of_ai_services_CO3-6)      Passa
    all''utente non root `fastapi`.      ###### Suggerimento    Spesso dovrai installare
    dei pacchetti o aggiungere delle configurazioni che richiedono un accesso privilegiato
    al disco o dei permessi. Dovresti passare a un utente non root solo alla fine
    della creazione di un''immagine, una volta completate queste installazioni e configurazioni.
    Evita di passare da un utente root a uno non root per evitare inutili complessità
    e livelli di immagine in eccesso.    Se hai problemi con la creazione di nuovi
    gruppi o utenti nell''[Esempio 12-6](#docker_permissions), prova a cambiare gli
    ID `USER_UID` e `USER_GID` perché potrebbero essere già utilizzati da un altro
    utente non root nell''immagine.    Supponiamo che durante la creazione dell''immagine,
    l''utente `root` nel container abbia creato la cartella `myscripts`. Puoi controllare
    i permessi del filesystem utilizzando il comando `ls -l`, che restituisce il seguente
    output:    [PRE45]   [PRE46][PRE47]``py[PRE48]`` In alternativa, se hai bisogno
    di eseguire gli script solo nella directory `myscripts`, usa il comando `chmod`
    per modificare i permessi dei file o delle directory:    [PRE49]   [PRE50]`` [PRE51]`
    ###### Suggerimento    Il flag `-R` imposta ricorsivamente la proprietà o i permessi
    di una directory annidata.    Questo comando permette ai membri del gruppo `root`
    e ad altri utenti di eseguire i file presenti nella directory `myscripts`. Altri
    utenti possono eseguire i file solo se utilizzano il comando `bash`. Tuttavia,
    solo il proprietario può modificarli.    Se ispezioni nuovamente i permessi del
    filesystem utilizzando `ls -l`, vedrai il seguente output:    [PRE52]   [PRE53]
    [PRE54]`py [PRE55]py`` [PRE56]py[PRE57][PRE58][PRE59][PRE60]py[PRE61]py`  [PRE62]`py[PRE63]py`
    [PRE64]`py[PRE65]py [PRE66]`py[PRE67]py [PRE68]py`` Il *bridge di rete* in Docker
    è un dispositivo software di livello link che gira all''interno del kernel della
    macchina host e che permette ai container collegati di comunicare isolando i container
    non collegati. Il driver del bridge installa automaticamente delle regole nella
    macchina host in modo che i container su reti bridge diverse non possano comunicare
    direttamente.    ###### Suggerimento    Le reti bridge si applicano solo ai container
    in esecuzione sullo stesso motore Docker/ host demone. Per collegare i container
    in esecuzione su altri host demone, puoi gestire il routing a livello del sistema
    operativo host o utilizzare un driver *overlay*.    Oltre alle reti bridge predefinite,
    puoi creare le tue reti personalizzate, che possono offrire un isolamento e un''esperienza
    di routing dei pacchetti superiori.    #### Configurare reti ponte definite dall''utente    Se
    hai bisogno di ambienti di rete più avanzati o isolati per i tuoi container, puoi
    creare una rete separata definita dall''utente.    Le reti definite dall''utente
    sono superiori alle reti di bridge predefinite in quanto garantiscono un migliore
    isolamento. Inoltre, i container possono risolversi l''un l''altro tramite nome
    o alias sulle reti di bridge definite dall''utente, a differenza della rete predefinita
    in cui possono comunicare solo tramite indirizzi IP.    ###### Avvertenze    Se
    esegui i container senza specificare `--network`, verranno collegati alla rete
    bridge predefinita, il che può rappresentare un problema di sicurezza in quanto
    i servizi non correlati possono comunicare e accedere l''uno all''altro.    Per
    creare una rete, puoi utilizzare il comando `docker network create`, che utilizzerà
    il flag `--driver bridge` per impostazione predefinita:    [PRE69]py   [PRE70]py
    [PRE71]`py [PRE72]py [PRE73]`py [PRE74]py[PRE75][PRE76]`  [PRE77]` [PRE78] [PRE79]`py
    [PRE80]py[PRE81]py[PRE82]``py[PRE83]py [PRE84]`py[PRE85]py` [PRE86]py`` `# ...`
    [PRE87]py `watch``:` [PRE88]`py [PRE89]py`` [PRE90]py[PRE91]py [PRE92]py`` [PRE93]py[PRE94][PRE95][PRE96][PRE97]py[PRE98]py[PRE99]``py[PRE100]py[PRE101]py[PRE102]`py[PRE103]py[PRE104]`
    [PRE105] FROM python:3.12-slim as base `COPY` requirements.txt requirements.txt
    [PRE106] [PRE107] `` `Ora qualsiasi modifica ai file sorgente non influirà sulla
    lunga fase di installazione delle dipendenze, velocizzando drasticamente il processo
    di compilazione.` `` [PRE108]` [PRE109][PRE110][PRE111][PRE112][PRE113] RUN apt-get
    update && apt-get install -y [PRE114]`` #### Mantenere un contesto di costruzione
    piccolo    Il *contesto di compilazione* è l''insieme dei file e delle directory
    che verranno inviati al costruttore per eseguire l''istruzione Dockerfile. Un
    contesto di compilazione più piccolo riduce la quantità di dati inviati al costruttore
    e diminuisce la possibilità di invalidare la cache, rendendo le compilazioni più
    veloci.    Quando usi il comando `COPY . .` in un file Docker per copiare la tua
    directory di lavoro in un''immagine, potresti anche aggiungere cache di strumenti,
    dipendenze di sviluppo, ambienti virtuali e file inutilizzati nel contesto di
    compilazione. Non solo le dimensioni dell''immagine aumenteranno, ma anche il
    costruttore Docker metterà in cache questi file non necessari. Qualsiasi modifica
    a questi file invaliderà la compilazione, riavviando l''intero processo di compilazione.    Per
    evitare di invalidare inutilmente la cache, puoi aggiungere un file *.dockerignore*
    accanto al tuo file Docker, elencando tutti i file e le directory di cui i tuoi
    servizi non avranno bisogno in produzione. A titolo di esempio, ecco gli elementi
    che puoi includere in un file *.dockerignore*:    [PRE115]    Docker builder ignorerà
    questi file anche quando eseguirai il comando `COPY` su tutta la tua directory
    di lavoro.    #### Usa la cache e i montaggi bind    Puoi utilizzare i *montaggi
    bind* per evitare di aggiungere livelli non necessari all''immagine e i *montaggi
    cache* per velocizzare le build successive.    I montaggi Bind includono temporaneamente
    i file nel contesto di compilazione per una singola istruzione `RUN` e non persisteranno
    come livelli di immagine in seguito. I montaggi Cache specificano una posizione
    persistente della cache in cui puoi leggere e scrivere dati in più build.    Ecco
    un esempio in cui puoi scaricare un modello pre-addestrato da Hugging Face in
    una cache montata per ottimizzare la cache dei livelli:    [PRE116]   ``Questa
    istruzione `RUN` crea una cache del modello preaddestrato scaricato all''indirizzo`/root/.cache/huggingface`,
    che può essere condivisa in più build. Questo aiuta a evitare download ridondanti
    e ottimizza il processo di creazione riutilizzando i livelli in cache.    Puoi
    anche utilizzare il flag `--no-cache-dir` quando utilizzi il gestore di pacchetti
    `pip` per evitare del tutto la cache e ridurre al minimo le dimensioni dell''immagine.
    Tuttavia, il processo di compilazione sarà significativamente più lento perché
    le compilazioni successive dovranno essere riscaricate ogni volta.``  [PRE117]
    docker buildx build --cache-from type=registry,ref=user/app:buildcache . [PRE118]`
    [PRE119][PRE120][PRE121][PRE122][PRE123][PRE124]``py[PRE125]py` ### Costruzioni
    in più fasi    Utilizzando le *build multi-stadio*, puoi ridurre le dimensioni
    dell''immagine finale suddividendo le istruzioni del file Docker in fasi distinte.
    Le fasi comuni possono essere riutilizzate per includere componenti condivisi
    e fungere da punto di partenza per le fasi successive.    Puoi anche copiare selettivamente
    gli artefatti da una fase all''altra, lasciando indietro tutto ciò che non vuoi
    nell''immagine finale. Questo assicura che solo gli output necessari siano inclusi
    nell''immagine finale dalle fasi precedenti, evitando gli artefatti non essenziali.
    Inoltre, puoi anche eseguire più fasi di creazione in parallelo per accelerare
    il processo di creazione delle tue immagini.    Un modello di compilazione multi-stadio
    comune è quello che prevede un''immagine di test/sviluppo e una di produzione
    più snella, che partono entrambe da un''immagine condivisa del primo stadio. L''immagine
    di sviluppo o di test può includere ulteriori livelli di strumenti (ad esempio,
    compilatori, sistemi di compilazione e strumenti di debug) per supportare i flussi
    di lavoro richiesti.    Immagina di dover servire un modello di trasformatore
    bert da Hugging Face in un servizio FastAPI. Puoi scrivere le istruzioni del tuo
    Dockerfile in modo da utilizzare tre fasi sequenziali distinte.    La prima fase
    scarica il modello del trasformatore su `/root/.cache/huggingface` e crea un ambiente
    virtuale Python su `/opt/venv`:    [PRE126]py`` `RUN` python -m venv /opt/venv
    [PRE127]` [PRE128][PRE129][PRE130]``py[PRE131]`py` La seconda fase copia gli artefatti
    del modello e l''ambiente virtuale Python `/opt/ven` dalla fase `base` prima di
    copiare i file sorgente e creare una versione di produzione del servizio FastAPI:    [PRE132]py[PRE133]py
    [PRE134]`py`` [PRE135]`py [PRE136]`py` [PRE137]`py`` [PRE138]`py[PRE139][PRE140][PRE141]
    [PRE142][PRE143][PRE144][PRE145][PRE146]``  [PRE147] [PRE148]`py [PRE149]`py[PRE150]py[PRE151]py[PRE152]py[PRE153]py[PRE154]py[PRE155]py`
    [PRE156]`py[PRE157]py[PRE158]py[PRE159][PRE160][PRE161][PRE162][PRE163]``py[PRE164]py[PRE165]py[PRE166]py[PRE167][PRE168][PRE169][PRE170][PRE171]
    [PRE172][PRE173][PRE174][PRE175][PRE176]` [PRE177] [PRE178][PRE179][PRE180][PRE181][PRE182]``
    [PRE183][PRE184][PRE185][PRE186][PRE187] [PRE188]`py[PRE189]py[PRE190]`'
  prefs: []
  type: TYPE_NORMAL
