- en: Chapter 12\. Deployment of AI Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, it is time to complete your GenAI solution by deploying
    it. You’re going to learn several deployment strategies and, as part of deployment,
    containerize your services with Docker following its best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment Options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You now have a working GenAI service that you want to make accessible to your
    users. What are your deployment options? There are a few common deployment strategies
    you can adapt to make your apps accessible to users:'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines (VMs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managed application platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore each in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Virtual Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you plan to use your own on-premises servers or prefer to deploy your services
    on the same hardware hosting your other applications for high isolation and security,
    you can deploy your GenAI service to a VM.
  prefs: []
  type: TYPE_NORMAL
- en: A VM is a software emulation of a physical computer running an operating system
    (OS) and applications. It’s no different from a physical computer like a laptop,
    smartphone, or server.
  prefs: []
  type: TYPE_NORMAL
- en: The VM’s *host* system provides resources such as CPU, memory, and storage,
    while a software layer called the *hypervisor* manages the VM and allocates resources
    from the host to the VM. The resources that the hypervisor allocates to the VM
    is the *virtual hardware* that its OS and applications run on.
  prefs: []
  type: TYPE_NORMAL
- en: The VM could run directly on host’s hardware (bare metal) or on a conventional
    operating system (i.e., be hosted). As a result, the OS installed within the VM
    is then referred to as the *guest OS*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-1](#deployment_vm_architecture) shows the virtualization technology
    system architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1201](assets/bgai_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Virtualization system architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cloud providers or your own data center can consist of several physical servers,
    each hosting multiple VMs with their own guest OS and hosted applications. For
    cost-effective resource sharing, these VMs may share the same mounted physical
    storage drive even though they’re contained within fully isolated environments,
    as you can see in [Figure 12-2](#deployment_vm_data_center).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1202](assets/bgai_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. Hosted VMs in a data center
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The benefit of using a VM is that you have direct access to the guest OS, virtual
    hardware resources, and GPU drivers. If there are any issues with deployment,
    you can connect to the VM via the *Secure Shell Transfer* (SHH) protocol to inspect
    application logs, set up application environment, and debug production issues.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your services to VMs will be as straightforward as cloning your code
    repository to the VM and then installing the required dependencies, packages,
    and drivers to successfully start up your application. However, the recommended
    way to do this is to use a containerization platform such as Docker running on
    the VM to enable continuous deployments and other benefits. You should also ensure
    you size your VM resources appropriately so that your services aren’t starved
    for CPU/GPU cores, memory, or disk storage.
  prefs: []
  type: TYPE_NORMAL
- en: With on-premises VMs, you can save on-cloud hosting or server rental costs and
    can fully secure your application environments to a handful of users, isolated
    from the public internet. These benefits are also achievable with cloud VMs but
    require additional networking and resource configuration to set up. In addition,
    you can have access to GPU hardware and configure drivers for your application
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that using the VM deployment pattern may not be easily scalable
    and requires significant effort to maintain. Additionally, VM servers normally
    run 24/7 incurring constant running costs, unless you automate their startup and
    shutdown based on your needs. You’ll be responsible for applying security patches,
    OS updates, and package upgrades alongside any networking configurations. With
    direct access to hardware resources, you’ll also have more decisions to make that
    can slow you down, leading to decision fatigue.
  prefs: []
  type: TYPE_NORMAL
- en: My advice is to deploy to a VM if you don’t plan to scale your services anytime
    soon or need to maintain low server costs and a secure isolated application environment
    for a handful of users. In addition, make sure you’ve planned sufficient time
    for deployment, networking, and configuration of your VMs.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Serverless Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aside from VMs, you can also deploy your services on cloud functions that cloud
    providers supply as *serverless* systems. In serverless computing, your code is
    executed in response to events such as database changes, updates to blobs in a
    storage, HTTP requests, or messages added to a queue. This means you pay only
    for the requests or compute resources your services use, rather than for an entire
    server as with a continuously running VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Serverless deployments are often useful when:'
  prefs: []
  type: TYPE_NORMAL
- en: You want to have event-driven systems instead of a running VM, which might be
    on 24/7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to deploy your API services using a serverless architecture that’s
    highly cost-efficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your services are to perform batch processing jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need workflow automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term *serverless* doesn’t mean that cloud functions don’t require hardware
    resources to execute but rather that the management of these resources is handled
    by the cloud provider. This allows you to focus on writing application code without
    worrying about server and OS-level details.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud providers instantiate compute resources to meet the demand of their customers.
    Often, there is a surge in demand, requiring them to create additional resources
    in advance to handle the demand spike. However, once the demand drops, excess
    unallocated compute resources remain that must be either shut down or shared among
    other customers.
  prefs: []
  type: TYPE_NORMAL
- en: Removing and creating resources is an intensive compute operation to perform.
    At scale, these operations carry significant costs for cloud providers. Therefore,
    cloud providers prefer to keep these resources running as much as possible and
    distribute them among existing customers to maximize billing.
  prefs: []
  type: TYPE_NORMAL
- en: To encourage customers to use these excess compute, they’ve built cloud function
    services that you can leverage to run your backend services on excess (i.e., serverless)
    compute. Luckily, there are packages such as Magnum that allow you to package
    FastAPI services on AWS cloud functions. You will soon see that FastAPI services
    can also be deployed as Azure cloud functions.
  prefs: []
  type: TYPE_NORMAL
- en: What you need to bear in mind is that these functions are allocated only a small
    amount of resources and have a short timeout. However, you can request longer
    timeouts and compute resources to be allocated, but it may take longer to receive
    these allocations, leading to higher latencies for your users.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your business logic consumes a lot of resources or requires longer than a
    handful of minutes to execute, cloud functions may not be a suitable deployment
    option for you.
  prefs: []
  type: TYPE_NORMAL
- en: However, you can split your FastAPI services across multiple functions, with
    each function handling a single exposed endpoint. This way, you can deploy parts
    of your service as cloud functions, reducing the portion of the FastAPI service
    that needs to be deployed using other methods.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of using serverless functions for deploying your services
    is their scalability. You can scale your applications as needed and pay only a
    fraction of the cost compared to reserving dedicated VM resources. Cloud providers
    typically charge based on the number of function executions and runtime, often
    with generous monthly quotas. This means that if your functions run quickly and
    you have a moderate number of concurrent users, you might be able to host all
    your services for free.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, cloud providers also supply function runtimes that you can install
    locally for local testing and development so that you can significantly shorten
    development iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Each cloud provider has their own approach to deploying serverless functions.
    Often, you require an entry script such as *main.py* that can import dependencies
    from other modules as needed. Alongside the entry point script, you’ll need to
    upload a function host JSON configuration file alongside *requirements.txt* for
    required dependencies to be installed on deployment on a Python runtime.
  prefs: []
  type: TYPE_NORMAL
- en: You can then deploy functions by uploading all the required files as a zipped
    directory or using CI/CD pipelines that authenticate with the provider and execute
    the deployment commands within your cloud project.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s try to deploy a bare-bones FastAPI app that returns LLM
    responses. The structure of the project will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can then package a FastAPI app as an [Azure serverless function](https://oreil.ly/ZaOuF)
    by following the upcoming code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to install the `azure-functions` package to run Azure’s serverless
    function runtime for local development and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, create *host.json* by following [Example 12-1](#deployment_function_azure_host).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-1\. Azure Functions host configurations (host.json)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Afterward, implement your GenAI service with the FastAPI service as usual by
    following [Example 12-2](#deployment_function_azure_app).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-2\. Simple FastAPI application serving LLM responses
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, wrap your FastAPI `app` within `func.AsgiFunctionApp` for the Azure
    serverless function runtime to hook into it, as shown in [Example 12-3](#deployment_function_azure_function).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-3\. Deploying a FastAPI service with Azure Functions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then start the function app by running the `func start` command, which
    should be available as a CLI command once you install the `azure-functions` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then try URLs corresponding to the handlers in the app by sending HTTP
    requests to both simple and the parameterized paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once ready, you can then deploy your FastAPI wrapped serverless function to
    the Azure cloud and then run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `publish` command will then publish the project files from the project directory
    to `<FunctionAppName>` as a ZIP deployment package.
  prefs: []
  type: TYPE_NORMAL
- en: 'After deployment, you can then test different paths on the deployed URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Your chosen cloud provider may not support serving a FastAPI server within its
    function runtime. If that’s the case, you may want to seek alternative deployment
    options. Otherwise, you’ll need to migrate the logic of your endpoints to the
    supported web framework of the function runtime and create separate functions
    for each endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: As you see, deploying your FastAPI service as cloud functions is straightforward
    and allows you to delegate the management and scaling of your services to cloud
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that if you decide to serve a GenAI model in your service, cloud
    functions wouldn’t be suitable deployment targets due to their short timeout periods
    (10 minutes). Instead, you’d want to use a model provider API in your services
    so that you have reliable and scalable access to the model without being constrained
    by execution time limits.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Managed App Platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to cloud functions or VMs, you can upload your codebase as ZIP files
    to app platforms managed by cloud providers. Managed app platforms let you delegate
    several tasks related to maintenance and management of your services to the cloud
    provider. In exchange, you pay only for the compute resources managed by the cloud
    provider that serve your application. The cloud provider systems allocate and
    optimize resources based on your application’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of such services include Azure App Services, AWS Elastic Beanstalk,
    Google App Engine, or Digital Ocean app platform.
  prefs: []
  type: TYPE_NORMAL
- en: Third-party platforms such as Heroku, Hugging Face Spaces, railway.app, render.com,
    or fly.io also exist for deploying your services directly from code in repositories,
    which abstract away certain decisions from you so that you can deploy faster and
    easier. Under the hood, third-party managed app platforms may be using the infrastructure
    of main cloud providers like Azure, Google, or AWS.
  prefs: []
  type: TYPE_NORMAL
- en: The main benefit of deploying to managed app platforms is the ease and speed
    of deployment, networking, scaling, and maintaining your services. Such platforms
    provide you with tools you need to secure, monitor, scale, and manage your services
    without having to worry about the underlying resource allocations, security, or
    software updates. They can let you configure load balancers, SSL certificates,
    domain mappings, monitoring, and staging environments so that you can focus more
    on application development than deployment workload of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Because these platforms follow the platform-as-a-service (PaaS) payment model,
    you’ll be billed a higher rate compared to relying on your own infrastructure
    or using lower-level resources such as bare-bone VMs or serverless compute options.
    Alternative services may use the infrastructure-as-a-service (IaaS) payment model
    that often is more cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I find managed app platforms a convenient way to deploy my applications
    without much hassle. If I’m working on a prototype and need to get my services
    available to users as fast as possible, managed app platforms is my first go-to
    option. Although, bear in mind that if you need access to GPU hardware for running
    inference services, you’ll have to rely on dedicated VMs, on-premises servers,
    or specialized AI platform services to serve your models. The app platforms can
    only provide CPU, memory, and disk storage for serving backend services or frontend
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A handful of managed cloud provider AI platforms include Azure Machine Learning
    Studio or Azure AI, Google Cloud Vertex AI Platform, AWS Bedrock and SageMaker,
    or IBM Watson Studio.
  prefs: []
  type: TYPE_NORMAL
- en: There are also third-party platforms for hosting your models including Hugging
    Face Inference Endpoints, Weights & Biases Platform, or Replicate.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying from code repositories will often require you to add certain configuration
    files to the root of your project depending on which app platform you will be
    deploying to. The process also depends on whether the app platform supports the
    application runtime, libraries, and framework versions you’re using, so a successful
    deployment isn’t always guaranteed. It’s also often challenging to migrate to
    supported runtimes or versions.
  prefs: []
  type: TYPE_NORMAL
- en: Due to these unforeseen issues, many engineers are switching to containerization
    technologies such as Docker or Podman to package up and deploy their services.
    These containerized applications can then be deployed directly to any app platform
    supporting containers with guarantees that the application will run no matter
    what the underlying resources, runtime, or dependency versions are.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying services with containers is now one of the most reliable strategies
    for shipping your applications to production for users to access.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *container* is a loosely isolated environment designed for building and running
    applications. Containers can run your services quickly and reliably in any computing
    environment by packaging your code with all the required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, containers rely on an OS-virtualization method that enables
    them to run on physical hardware, in the cloud, on VMs, or across multiple operating
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar to managed app platforms and serverless functions, you can configure
    containers to automatically restart and self-heal, if your application exits for
    any reason.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike VMs whose underlying technologies rely on virtualization, containers
    rely on containerization.
  prefs: []
  type: TYPE_NORMAL
- en: Containerization packages applications and their dependencies into lightweight,
    isolated units that share the host OS kernel. On the other hand, virtualization
    enables running multiple operating systems on a single physical machine using
    hypervisors. Therefore, unlike virtual machines, containers don’t virtualize hardware
    resources. Instead, they run on top of a container runtime platform that abstracts
    the resources, making them lightweight (i.e., as low as a few megabytes to store)
    and faster than VMs since they don’t require a separate OS per container.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In essence, virtualization is about abstracting hardware resources on the host
    machine while containerization is about abstracting the operating system kernel
    and running all application components inside an isolated unit called a *container*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-3](#deployment_container_architecture) compares the virtualization
    and containerization system architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1203](assets/bgai_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. Comparison of containerization and virtualization system architectures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The main benefit from using containers is their *portability*, *boot-up speed*,
    *compactness*, and *reliability* across various computing environments, as they
    don’t require a guest OS and a hypervisor software layer.
  prefs: []
  type: TYPE_NORMAL
- en: This makes them perfect for deploying your services with minimal resources,
    deployment effort and overheads. They boot up faster than a VM, and scaling them
    is also more straightforward. You can add more containers to *horizontally scale*
    your services.
  prefs: []
  type: TYPE_NORMAL
- en: To help with containerizing your applications, you can rely on platforms such
    as Docker that have been battle-tested across the MLOps and DevOps communities.
  prefs: []
  type: TYPE_NORMAL
- en: Containerization with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker is a containerization platform used to build, ship, and run containers.
    At the time of writing, Docker has around [22% market share](https://oreil.ly/A5x63)
    in the virtualization platforms market with more than 9 million developers and
    [11 billion monthly image downloads](https://oreil.ly/8-wx4), making it the most
    popular containerization platform. Many server environments and cloud providers
    support Docker within many variants of Linux and Windows server.
  prefs: []
  type: TYPE_NORMAL
- en: Chances are if you need to deploy your GenAI services, the easiest and most
    straightforward option will be to use Docker to containerize your application.
    However, to get comfortable with Docker, you need to understand its architecture
    and the underlying subsystems such as storage and networking.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Docker system is composed of an engine, a client, and a server:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker engine
  prefs: []
  type: TYPE_NORMAL
- en: The engine consists of several components including a client and a server running
    on the same host OS.
  prefs: []
  type: TYPE_NORMAL
- en: Docker client
  prefs: []
  type: TYPE_NORMAL
- en: Docker comes with both a *CLI tool* named `docker` and a graphical user interface
    (GUI) application called *Docker Desktop*. Using the client-server implementation,
    the Docker client can communicate with the local or a remote server instance using
    a REST API to manage containers by running commands such as running, stopping,
    and terminating containers. You can also use the client to pull images from an
    image registry.
  prefs: []
  type: TYPE_NORMAL
- en: Docker server
  prefs: []
  type: TYPE_NORMAL
- en: The server is a *daemon* named `dockerd`. The Docker daemon responds to the
    client HTTP requests via the REST API and can interact with other daemons. It’s
    also responsible for tracking the lifecycle of containers.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker platform also allows you to create and configure objects such as
    *networks*, *storage volumes*, *plug-ins*, and service objects to support your
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Most important, to containerize your applications with Docker, you’ll need to
    build Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: A *Docker image* is a portable package containing software and acts as a recipe
    for creating and running your application containers. In essence, a container
    is an in-memory instance of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A container image is *immutable*, so once you’ve built one, you can’t change
    it. You can only add to an image and not subtract. You’ll have to re-create a
    new one if you want to apply changes.
  prefs: []
  type: TYPE_NORMAL
- en: Docker images are the first step toward containerizing your services as you’ll
    learn in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building Docker Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s imagine you have a small GenAI service using FastAPI, as shown in [Example 12-4](#docker_app),
    that you want to containerize.
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-4\. A simple GenAI FastAPI service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deployment_of_ai_services_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Assume that the `generate_text` function is calling a model provider API or
    an external model server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build this application into a container image, you’ll need to write instructions
    in a text file called a *Dockerfile*. Inside this Dockerfile, you can specify
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: The *base* image to create a new image from, supplying the OS and environment
    upon which additional application layers are built
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commands to update the guest OS and install additional software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build artifacts to include such as your application code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services to expose like storage and networking configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command to run when the container starts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Example 12-5](#containers_dockerfile) illustrates how to build an application
    image in a Dockerfile.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-5\. Dockerfile to containerize a FastAPI application
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deployment_of_ai_services_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the official Python 3.12 slim image as the `base` image.^([1](ch12.html#id1291))
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deployment_of_ai_services_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Set the working directory inside the container to `/code`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deployment_of_ai_services_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Copy the `requirements.txt` file from the host to the current directory in the
    container.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deployment_of_ai_services_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Install the Python dependencies listed in `requirements.txt` without using the
    cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deployment_of_ai_services_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Copy all files from the host’s current directory to the current directory in
    the container.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_deployment_of_ai_services_CO2-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Inform Docker daemon that the application inside the container is listening
    on `8000` at runtime. The `EXPOSE` command doesn’t automatically map or allow
    access on ports.^([2](ch12.html#id1292))
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_deployment_of_ai_services_CO2-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Run the `uvicorn` server with the application module and host/port configuration,
    when container is launched.
  prefs: []
  type: TYPE_NORMAL
- en: We won’t be covering the full [Dockerfile specification](https://oreil.ly/8fJ6l)
    in this chapter. However, notice how each command changes the image structure
    that enables you to run your full GenAI services within a container.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `docker build` command to build the image in [Example 12-5](#containers_dockerfile):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice the steps listed in the output. When each step executes, a new layer
    gets added to the image you’re building.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a container image, you can then use container registries to store,
    share, and download images.
  prefs: []
  type: TYPE_NORMAL
- en: Container Registries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To store and distribute images in a version-controlled environment, you can
    use *container registries*, which include both the public or private flavors.
  prefs: []
  type: TYPE_NORMAL
- en: '*Docker Hub* is a managed software-as-a-service (SaaS) container registry for
    storing and distributing images you create.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Hub is public by default. However, you can also use self-hosted or cloud
    provider private registries such as Azure Container Registry (ACR), AWS Elastic
    Container Registry (ECR), or Google Cloud Artifact Registry.
  prefs: []
  type: TYPE_NORMAL
- en: You can view the full Docker platform system architecture in [Figure 12-4](#docker_architecture).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1204](assets/bgai_1204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. Docker platform system architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in [Figure 12-4](#docker_architecture), the Docker daemon manages
    containers and images. It creates containers from images and communicates with
    the Docker client, handling commands to build and run images. The Docker daemon
    can also pull images from or push them to a registry (e.g., Docker Hub) that contains
    images like Ubuntu, Redis, or PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Docker Hub registry, you can access other contributed images alongside
    distributing and version controlling your own. Registries like Docker Hub play
    a crucial role in scaling your services as container orchestration platforms like
    Kubernetes need access to registries to pull and run multiple container instances
    from images.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pull public images from Docker Hub using the `docker pull` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When you push and pull images, you’ll need to specify a *tag* using the `<name>:<tag>`
    syntax. If you don’t provide a tag, Docker engine will use the `latest` tag by
    default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from pulling, you can also store your own images in container registries.
    First, you need to build and tag your image with both a version label and the
    image repository URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once your image is built and tagged, you can then push it to the Docker Hub
    container registry using the `docker push` command. You may need to log in first
    to authenticate with the hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be careful that during a push, you don’t overwrite the tag for an image in many
    repositories. For instance, an image built and tagged `genai:latest` in a repository
    can be overwritten by another image tagged `genai:latest`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that your image is stored in the registry, you can pull it down on another
    machine^([3](ch12.html#id1297)) or at a later time to run the image without the
    need to rebuild it.
  prefs: []
  type: TYPE_NORMAL
- en: Container Filesystem and Docker Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building the image, Docker uses a special filesystem called the `Unionfs`
    (stackable unification filesystem) to merge the contents of several directories
    (i.e., *branches* or in Docker terminology *layers*), while keeping their physical
    content separate.
  prefs: []
  type: TYPE_NORMAL
- en: Using `Unionfs`, directories of distinct filesystems can be combined and overlaid
    to form a single coherent virtual filesystem, as shown in [Figure 12-5](#docker_unionfs).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1205](assets/bgai_1205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. Unified virtual filesystem from multiple filesystems
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the `Unionfs`, Docker can add or remove branches as you build out your
    container filesystem from an image.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the mechanism of layered architecture in containers, let’s review
    the image from [Example 12-5](#containers_dockerfile).
  prefs: []
  type: TYPE_NORMAL
- en: When building the image using [Example 12-5](#containers_dockerfile), you’re
    layering a Python 3.12 base image running on a Linux distribution on top of a
    root filesystem. Next, you’re adding *requirements.txt* on top of the Python base
    image and then installing dependencies on top of *requirements.txt* layer. You
    then add a new layer by coping the content of your project directory into the
    container, layering it on top of everything else. Finally, when you start the
    container with the `uvicorn` command, you add a final writable layer as part of
    the container filesystem. As a result, the ordering of layers becomes important
    when building Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-6](#docker_branches) shows the layered filesystem architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1206](assets/bgai_1206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. Layered Unionfs filesystem architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Example 12-5](#containers_dockerfile), each of the command steps is creating
    a cached image as the build process finalizes the container image. To run commands,
    intermediate containers are created and then automatically deleted after. The
    underlying cached image is kept on the build host and isn’t removed. These temporary
    images are layered over the previous image and combined into a single image once
    all steps are completed. This optimization allows future builds to reuse these
    images to speed up build times.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, the container will comprise one or more image layers and a final
    ephemeral container layer (i.e., that won’t be persisted) when the container is
    destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn about various Docker storage mechanisms. During
    the development of your services as containers, you can use these tools to manage
    data persistence, sharing data between containers and maintaining state between
    container restarts.
  prefs: []
  type: TYPE_NORMAL
- en: When working with containers, your application may need to write data to the
    disk, which will persist in an *ephemeral* storage. Ephemeral storage is a short-lived,
    temporary storage deleted once the container is stopped, restarted, or removed.
    If you restart your container, you’ll notice that previously persisted data is
    no longer available. Under the hood, Docker writes the runtime data to an ephemeral
    writable container layer in the container’s virtual filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll lose all your application generated data and log files you’ve written
    to disk during a container’s runtime if you rely on the container’s default storage
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid loss of application runtime data and logs, you have several storage
    options available that enable you to persist data during a container’s lifetime.
    During development, you can use *volumes* or *bind mounts* to persist data to
    the host OS filesystem or rely on local databases for persisting data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 12-1](#docker_storage_options) shows the Docker storage mount options.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 12-1\. Docker storage mounts
  prefs: []
  type: TYPE_NORMAL
- en: '| Storage | Description | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Volumes | I/O optimized and preferred storage solution. Managed by Docker
    and stored in a specific location on the host but decoupled from the host filesystem
    structure. | If you need to store and share data across multiple containers.If
    you don’t need to modify files or directories from the host. |'
  prefs: []
  type: TYPE_TB
- en: '| Bind mounts | Mount files or directories on host into the container but have
    limited functionality compared to volumes. | If you want both containers and host
    processes to access and modify host’s files and directories. For instance, during
    local development and testing. |'
  prefs: []
  type: TYPE_TB
- en: '| Temporary (tmpfs) mounts | Stores data in the host’s memory (RAM) and never
    written to the container or host’s filesystem. | If you need high-performance
    temporary storage for sensitive or nonstateful data that won’t persist after the
    container stops. |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 12-7](#docker_storage_mounts) shows the different types of mounts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1207](assets/bgai_1207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-7\. Docker storage mounts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ll now study each storage option in detail so you can simulate your production
    environment locally with Docker containers using the appropriate storage. When
    deploying containers to production within a cloud environment, you can use a database
    or cloud storage offering for persisting data instead of Docker volumes or bind
    mounts to centralize storage across multiple containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker volumes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Docker allows you to create isolated *volumes* for persisting application data
    between container runtimes. To create a volume, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Once created, you can use volumes to persist data between container runs. Volumes
    also allow you to persist data when you use database and memory store containers.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Restarting a database container with new environment variables may not be enough
    to reset them with new settings.
  prefs: []
  type: TYPE_NORMAL
- en: Some database systems may require you to re-create the container volume if you
    need to update settings like administrator user credentials.
  prefs: []
  type: TYPE_NORMAL
- en: By default, any volumes you create will be stored on the host machine filesystem
    until you explicitly remove them via the `docker volume remove` command.
  prefs: []
  type: TYPE_NORMAL
- en: Bind mounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to volumes, you can also use filesystem mappings via volume *bind
    mounts* that map directories residing on the host filesystem to the container
    filesystem, as shown in [Figure 12-8](#docker_bind_mounts).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1208](assets/bgai_1208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-8\. Bind mounts between host filesystem and a container
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The mounts happen as you start your container. With the mounted directories,
    you can then directly access them from within the container. You can read and
    persist data to the mounted directories as you run and stop your containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run a container with a volume bind mount, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `-v` flag allows you to map the host directory to a container directory
    using the `<host_dir>:<container_dir>` syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The functionality of the `COPY` command you use in a Dockerfile is different
    from directory mounting.
  prefs: []
  type: TYPE_NORMAL
- en: The former makes a separate copy of a host directory into the container during
    the image build process while the latter allows you to access and update the mapped
    host directory from within the container.
  prefs: []
  type: TYPE_NORMAL
- en: This means that if you’re not careful, you can unintentionally modify or delete
    all your original files on the host machine permanently, from within the container.
  prefs: []
  type: TYPE_NORMAL
- en: Bind mount volumes can still be useful in a local development environment. As
    you change the source code of your services, you’ll be able to observe the real-time
    impact of modifications on the running application containers.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary mounts (tmpfs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have some nonpersistent data such as model caches or sensitive files
    that you don’t need to store permanently, you should consider using temporary
    *tmpfs mounts*.
  prefs: []
  type: TYPE_NORMAL
- en: This temporary mount will only persist the data to the host memory (RAM) during
    the container’s runtime and increases the container’s performance by avoiding
    writes into the container’s writeable layer.
  prefs: []
  type: TYPE_NORMAL
- en: When containerizing GenAI applications, you can use temporary mounts to store
    cached results, intermediate model computations, temporary files, and session-specific
    logs that you won’t need once the container stops.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The container’s writeable layer is tightly coupled with the host machine through
    a storage driver to implement the union filesystem. Therefore, writing to the
    container’s writable layer reduces performance due to this additional layer of
    abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you can use data volumes for persistent storage that writes directly
    to the host filesystem or tmpfs mounts for temporary in-memory storage.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike bind mounts and volumes, you can’t share the tmpfs mount between containers,
    and the functionality is available only on Linux systems. In addition, if you
    adjust directory permissions on tmpfs mounts, they can reset when the container
    restarts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few other use cases of tmpfs mounts:'
  prefs: []
  type: TYPE_NORMAL
- en: Temporarily storing data caches, API responses, logs, test data, configuration
    files, and AI model artifacts in-memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding I/O writes to disks while working with library APIs that require file-like
    objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating high-speed I/O with rapid file access and writes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing excessive or unnecessary disk writes if you need temporary directories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To set a tmpfs mount, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, you are setting a tmpfs mount on the `/cache` directory for model caches,
    which will cease to exist once the container stops.
  prefs: []
  type: TYPE_NORMAL
- en: Handling filesystem permissions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A big source of frustration and a security consideration for many developers
    new to Docker is managing directory permissions when using filesystem bind mounts
    between the host OS and the container.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Docker runs containers as the `root` user leading to containers
    having full read/write access to mounted directories on the host OS. If the `root`
    user inside the container creates directories or files, they will be owned by
    `root` on the host as well. You can then face permission issues if you have a
    nonroot user account on the host when you try to access or modify these directories
    or files.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Running containers as the default `root` user is also a great security risk
    if a malicious actor gets access to the container since they’ll have access to
    the host system as `root`. Additionally, if you run a compromised image, you might
    risk executing malicious code on your host system with `root` privileges.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate permission issues when running containers with bind mounts, you
    can use the `--user` flag to run the container as a nonroot user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can create and switch to a nonroot user within the final
    layers of the image build inside the Dockerfile, as shown in [Example 12-6](#docker_permissions).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-6\. Creating and switching to nonroot user when building container
    images (Ubuntu/Debian containers only)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deployment_of_ai_services_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Use build arguments to specify variables during the image build.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deployment_of_ai_services_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a user group with the given `USER_GID`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deployment_of_ai_services_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Disable user login completely including password-based login.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deployment_of_ai_services_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid creating a home directory for the user.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deployment_of_ai_services_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a nonroot user account with the given `$USER_UID` and assign it to the
    newly created `USER_GID` group. Set the name of the user account to `fastapi`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_deployment_of_ai_services_CO3-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Switch to the nonroot `fastapi` user.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Often, you’ll need to install packages or add configurations that require privileged
    disk access or permissions. You should only switch to a nonroot user at the end
    of an image build once you’ve completed such installations and configurations.
    Avoid switching back and forth between root and nonroot users to prevent any unnecessary
    complexity and excess image layers.
  prefs: []
  type: TYPE_NORMAL
- en: If you hit issues with creating new groups or users in [Example 12-6](#docker_permissions),
    try changing the `USER_UID` and `USER_GID` as those IDs may already be in use
    by another nonroot user in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that during the image creation, the `root` user in the container
    has created the `myscripts` folder. You can inspect filesystem permissions using
    the `ls -l` command, which returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You can read permissions `drwxr-xr-x` for the `myscripts` directory using the
    following breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '`d`: Specifies that `myscripts` is a directory; otherwise would show a `-`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rwx`: Owner `root` user can (r)read, (w)rite, and e(x)ecute files in this
    directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r--`: Group `root` members can perform (r)ead-only operations but can’t write
    or execute any files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r--`: Everyone else can read the file but cannot write to or execute it.^([4](ch12.html#id1317))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to set ownership or permissions on the `myscripts` directory, you
    can use the `chmod` or `chown` commands in Linux systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `chown` command to change the directory owner on host so that you can
    edit the files in your code editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you only need to execute the scripts in the `myscripts` directory,
    use the `chmod` command to change the file or directory permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `-R` flag will recursively set the ownership or permissions on a nested
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: This command will allow `root` group members and other users to execute files
    in the `myscripts` directory. Others can execute the files only if they use the
    `bash` command. However, only the owner can modify them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you inspect the filesystem permissions again using `ls -l`, you’ll see the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`rwx`: Owner `root` user can still (r)read, (w)rite, and e(x)ecute files in
    this directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r-x`: Group `root` members can perform (r)ead and e(x)ecute operations but
    can’t modify any files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r-x`: Anyone else can’t modify files in `myscripts` directory but can read
    and execute them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use [Example 12-7](#docker_permissions_execute) to set permissions when
    creating directories inside an image.
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-7\. Creating scripts folder and allowing files to be executed (Ubuntu/Debian
    containers only)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The instructions in [Example 12-7](#docker_permissions_execute) will allow you
    to configure permissions to execute files in the `scripts` directory from within
    the container.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using container volumes, be careful with mount bindings as they replace
    the permissions inside the container with those from the host filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: The most frustrating issues when working with containers will be related to
    filesystem permissions. Therefore, knowing how to set and correct file permissions
    will save you hours of development when working with containers that produce or
    modify artifacts on the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker networking is one of the hardest concepts to grasp in multicontainer
    projects. This section covers how Docker networking works and how to set up local
    containers to communicate, simulating production environments during development.
  prefs: []
  type: TYPE_NORMAL
- en: Often, when you’re deploying to production environments in the cloud, you configure
    networking using the cloud provider’s solutions. However, if you need to connect
    containers in a development environment for local testing or deploying on on-premises
    resources, then you’ll benefit from understanding how Docker networking works.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re developing GenAI services that interact with external systems like
    databases, chances are you’ll be using multiple containers; one for your application
    and one for running each of your databases or external systems.
  prefs: []
  type: TYPE_NORMAL
- en: Docker ships with a networking subsystem that allows containers to connect with
    each other on the same or different hosts. You can even connect containers via
    internet-facing hosts.
  prefs: []
  type: TYPE_NORMAL
- en: When you create containers using the `docker run` command, they’ll have networking
    enabled by default on a *bridge network* so that they can make outgoing connections.
    However, they won’t expose or publish their ports to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the default settings, Docker interacts with the OS kernels to configure
    *firewall rules* (e.g., `iptables` and `ip6tables` rules on Linux) to implement
    network isolation, port publishing, and filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Since Docker can override these firewall rules, if you have a port on host like
    `8000` closed, Docker can force it open and expose it outside the host machine
    when you run a container with the `-p 8000:8000` flag. To prevent such an exposure,
    a solution is to run containers using `-p 127.0.0.1:8000:8000`.
  prefs: []
  type: TYPE_NORMAL
- en: For the networking subsystem to function, Docker uses *networking drivers*,
    as shown in [Table 12-3](#docker_networking_drivers).
  prefs: []
  type: TYPE_NORMAL
- en: Table 12-3\. Docker networking drivers
  prefs: []
  type: TYPE_NORMAL
- en: '| Driver | Description | Use case |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Bridge (default) | Connects containers running on the same Docker daemon
    host. User-defined networks can leverage an embedded DNS server. | Control container
    communication in isolated Docker networks with a simple setup. |'
  prefs: []
  type: TYPE_TB
- en: '| Host | Removes the isolation layer between containers and the host system,
    so any TCP/UDP connections are accessible directly via host network such as the
    localhost without the need to publish ports. | Simplify access to container from
    the host network (e.g., localhost) or when a container needs to handle a large
    range of ports. |'
  prefs: []
  type: TYPE_TB
- en: '| None | Disables all networking services and isolates running containers within
    the Docker environment. | Isolate containers from any Docker and non-Docker process
    for security reasons. Network debugging or simulating outages. Resource isolation
    and transient containers for short-lived processes. |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay | Connects containers across multiple hosts/engines or in a *Docker
    Swarm* cluster.**Note:** Docker engine has *swarm* mode that enables container
    orchestration via *clusters* of Docker daemons/engines. | Remove the need for
    OS-level routing when connecting containers across Docker hosts. |'
  prefs: []
  type: TYPE_TB
- en: '| Macvlan | Assigns mac addresses to containers as if they’re physical devices.Misconfiguration
    may lead to unintentional degradation of your network due to IP address exhaustion,
    leading to VLAN spread (large number of mac addresses) or promiscuous mode (overlapping
    addresses). | Used in legacy systems or applications that monitor network traffic
    that expect to be directly connected to a physical network. |'
  prefs: []
  type: TYPE_TB
- en: '| IPVlan | Gives you total control over container IPv4 and IPv6 addressing,
    providing easy access to external services with no need for port mappings. | Advanced
    networking setup that bypasses the traditional Linux bridge for isolation, enhanced
    performance and simplified networking topology. |'
  prefs: []
  type: TYPE_TB
- en: To ensure your containers can communicate together, you may need to specify
    networking settings and drivers. You can select a networking driver that matches
    your use case based on [Table 12-3](#docker_networking_drivers).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some of these drivers may not be available depending on the platform/host OS
    you’re running Docker on (Windows, Linux, or macOS host).
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used network drivers are bridge, host, and none. You likely
    won’t need to use other drivers (e.g., overlay, Macvlan, IPVlan) unless you need
    more advanced networking configurations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-9](#docker_networking_drivers_viz) visualizes the functionality
    of the bridge, host, none, overlay, Macvlan, and IPVlan drivers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1209](assets/bgai_1209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-9\. Docker networking drivers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s explore these networking drivers in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Bridge network driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The bridge network driver connects containers by creating a default bridge network
    `docker0` and associating containers with it and the host’s main network interface,
    unless otherwise specified. This will allow your containers to access the host
    network (and the internet) plus allow you to access the containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view the networks using the `docker network ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The *network bridge* in Docker is a link layer software device running within
    the host machine’s kernel, allowing linked containers to communicate while isolating
    non-connected containers. The bridge driver automatically installs rules in the
    host machine so that containers on different bridge networks can’t communicate
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bridge networks only apply to containers running on the same Docker engine/daemon
    host. To connect containers running on other daemon hosts, you can manage routing
    at the host OS layer or use an *overlay* driver.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to default bridge networks, you can create your own custom networks,
    which can provide superior isolation and packet routing experience.
  prefs: []
  type: TYPE_NORMAL
- en: Configure user-defined bridge networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you need more advanced or isolated networking environments for your containers,
    you can create a separate user-defined network.
  prefs: []
  type: TYPE_NORMAL
- en: User-defined networks are superior to the default bridge networks as they provide
    better isolation. In addition, containers can resolve each other by name or alias
    on user-defined bridge networks unlike the default network where they can only
    communicate via IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you run containers without specifying `--network`, they’ll be attached to
    the default bridge network. This can be a security issue as unrelated services
    are then able to communicate and access each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a network, you can use the `docker network create` command, which
    will use `--driver bridge` flag by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you create user-defined networks, Docker uses the host OS tools to manage
    the underlying network infrastructure, such as adding or removing bridge devices
    and configuring `iptables` rules on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the network is created, you can list the networks using the `docker network
    ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The network topology will now look like [Figure 12-10](#docker_networking_isolated).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1210](assets/bgai_1210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-10\. Isolated bridge networks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When you run containers, you can now attach them to the created network using
    the `--network genai-net` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On Linux, there is a limit of 1,000 containers that can connect to a single
    bridge network due to the Linux kernel restrictions. Linking more containers to
    a single bridge network can make it unstable and break inter-container communication.
  prefs: []
  type: TYPE_NORMAL
- en: Both your containers can now access each other on your better isolated `genai-net`
    user-defined network with automatic *DNS resolution* between containers.
  prefs: []
  type: TYPE_NORMAL
- en: Embedded DNS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Docker leverages an embedded DNS server with user-defined networks, as shown
    in [Figure 12-11](#docker_networking_bridge_dns), to map internal IP addresses
    so that containers can reach one by name.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1211](assets/bgai_1211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-11\. Embedded DNS
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For instance, if you name your application container as `genai-service` and
    your database container as `db`, then your `genai-service` container can communicate
    with the database by calling the `db` hostname.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can’t access the `db` container from outside of the Docker bridge network
    by its name, as the embedded DNS server is not visible to the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you can expose a container port `5432` and access the `db` container
    using host’s network (e.g., via `localhost:5432`).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss how you can publish container ports to the outside environment
    such as the host machine next.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing ports
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you run containers in a network, they automatically expose ports to each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to access containers from the host machine or non-Docker processes
    on different networks, you’ll need to expose the container ports by publishing
    them using the `--publish` or `-p` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This command allows you to create a container with exposed port `8000` mapped
    to `8000` port on the host machine (e.g., localhost) using the `<host_port>:​<con⁠tainer_port>`
    syntax.
  prefs: []
  type: TYPE_NORMAL
- en: When you don’t specify a container port, Docker will publish and map port `80`
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Always double-check ports you want to expose and avoid publishing container
    ports that are already in use on your host machine. Otherwise, there’ll be *port
    conflicts* leading to requests being routed to conflicting services, which will
    also be time-consuming to troubleshoot.
  prefs: []
  type: TYPE_NORMAL
- en: If using bridge networks and port mappings are causing you a lot of trouble,
    you can also use the *host* networking driver for connecting your containers,
    albeit without the same isolation and security benefits of bridge networks.
  prefs: []
  type: TYPE_NORMAL
- en: Host network driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A *host* network driver is useful for cases where you want to improve performance,
    when you want to avoid the container port mapping, or when one of your containers
    needs to handle a large number of ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running a container with the host driver is as simple as using the `--net=host`
    flag with the `docker run` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In host networking, containers share the host machine’s network namespace, meaning
    that containers won’t be isolated from the Docker host. Therefore, containers
    won’t be allocated their own IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As soon as you enable the host network driver, previously published ports will
    be discarded, as containers won’t have their own IP address.
  prefs: []
  type: TYPE_NORMAL
- en: The host network driver is more performant because it doesn’t need a *network
    address translation* (NAT) for mapping IP addresses from one namespace (containers)
    to another (host machine) and avoids creating a *user-land proxy* (i.e., port
    forwarding) for each port. However, host networking is only supported with Linux—and
    not Windows—containers. In addition, containers won’t have access to the network
    interfaces of the host so can’t bind to host’s IP addresses, leading to added
    complexity in the network configuration you need.
  prefs: []
  type: TYPE_NORMAL
- en: None network driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to completely isolate the networking stack of a container, you
    can use the `--network none` flag when starting the container. Within the container,
    only the loopback device is created, a virtual network interface that the container
    uses to communicate with itself. You can specify the none network driver using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'These are a few cases where isolating containers are useful:'
  prefs: []
  type: TYPE_NORMAL
- en: Applications handling highly sensitive data or running critical processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where there’s a higher risk of network-based attacks or malware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing network debugging and simulating network outages by eliminating external
    interference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running stand-alone containers without external dependencies can run independently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating transient containers for short-lived processes to minimize network
    exposure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, use the none network driver if you need to isolate containers from
    any Docker and non-Docker processes for security reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling GPU Driver
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have an NVIDIA graphics card with the CUDA toolkit and necessary drivers
    installed, then you can use the `--gpus=all` flag to enable GPU support for your
    containers in Docker.^([5](ch12.html#id1336))
  prefs: []
  type: TYPE_NORMAL
- en: 'To test that your system has the necessary drivers and supports GPU in Docker,
    run the following command to benchmark your GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also use the NVIDIA system management interface `nvidia-smi` tool to
    help manage and monitor NVIDIA GPU devices.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning frameworks such as `tensorflow` or `pytorch` can automatically
    detect and use the GPU device when running your applications in a GPU-enabled
    container. This includes Hugging Face libraries such as `transformers` that lets
    you self-host language models.
  prefs: []
  type: TYPE_NORMAL
- en: 'If using the `transformers` package, make sure to also install the `accelerate`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You can now move the model to GPU before it’s loaded in CPU by using `device_map='cuda'`,
    as shown in [Example 12-8](#docker_gpu).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-8\. Transferring Hugging Face models to the GPU
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You should be able to run the predictions on the GPU by passing the `--gpus=all`
    flag to `docker run`.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In multicontainer environments, you can use the *Docker Compose* tool for defining
    and running application containers for a streamlined development and deployment
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker Compose can help you simplify managing several containers, networks,
    volumes, variables, and secrets with a single *YAML configuration file*. This
    simplifies the complex task of orchestrating and coordinating various containers,
    making it easier to manage and replicate your services across different application
    environments using environment variables. You can also share the YAML file with
    others so that they can replicate your container environment. Additionally, it
    caches configurations to prevent re-creating containers when you restart services.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 12-9](#docker_compose) shows an example YAML configuration file.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-9\. Docker Compose YAML configuration file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deployment_of_ai_services_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create the containers alongside the associated volumes, networks, and secrets.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deployment_of_ai_services_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the Dockerfile located at the same directory as the Compose file to build
    the `server` image.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deployment_of_ai_services_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Use Docker secrets to mask sensitive data like API keys within the container
    shell environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deployment_of_ai_services_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a bridge `genai-net` network and attach both `server` and `db` containers
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you have Docker objects like volumes and networks that you’re managing yourself,
    you can tag them with `external: true` in the compose file so that Docker Compose
    doesn’t manage them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a `compose.yaml` file, you can then use simple compose commands
    to manage your containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You can use these commands to start/stop/restart services and view their logs
    or container statuses. Additionally, you can edit the Compose file shown in [Example 12-9](#docker_compose)
    to use `watch` so that your services are automatically updated as you edit and
    save your code.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 12-10](#docker_compose_watch) shows how to use the `watch` instruction
    on a given directory.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-10\. Enabling Docker Compose `watch` on a given directory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Whenever a file changes in the `./src` folder on your host machine, Compose
    will sync its content to `/code` and update the running application (server service)
    without restarting them.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then run the `watch` process using `docker compose watch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Docker Compose `watch` allows for greater granularity than is practical with
    bind mounts, as shown in [Example 12-9](#docker_compose). For instance, it lets
    you ignore specific files or entire directories within the watched tree to avoid
    I/O performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: Besides using Docker Compose `watch`, you can merge and override multiple Compose
    files to create a composite configuration tailored for specific build environments.
    Typically, the `compose.yml` file contains the base configurations, which can
    be overridden by an optional `compose.override.yml` file. For instance, as shown
    in [Example 12-11](#compose_override), you can inject local environment settings,
    mount local volumes, and create new a database service.
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-11\. Merging and overriding Compose files for environment-specific
    build configurations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deployment_of_ai_services_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The base Compose file contains instructions for running the production version
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deployment_of_ai_services_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Override base instructions by replacing the container start command, inject
    local variables, and add volume and networking configurations with a local database
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use these files, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Docker Compose will automatically merge configurations from both Compose files,
    applying the environment-specific settings from the override Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling GPU Access in Docker Compose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To access GPU devices with services managed by Docker Compose, you’ll need to
    add the instructions to the composed file (see [Example 12-12](#DockerComposeapp)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-12\. Adding GPU configurations to the Docker Compose app service
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deployment_of_ai_services_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Limit the number of GPU devices accessible by the app service.
  prefs: []
  type: TYPE_NORMAL
- en: These instructions will give you more granular control over how your services
    should use your GPU resources.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Docker Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your Docker images grow in size, they’ll also be slower to run, build, and
    test in production. You’ll also be spending a lot of development time iterating
    over the development of the image.
  prefs: []
  type: TYPE_NORMAL
- en: In that case, it’s important to understand image optimization strategies, including
    how to use Docker’s layering mechanism to keep images lightweight and efficient
    to run, in particular with GenAI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are a few ways to reduce image size and speed up the build process:'
  prefs: []
  type: TYPE_NORMAL
- en: Using minimal base images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding GPU inference runtimes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Externalizing application data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layering ordering and caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multi-stage builds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing these optimizations as shown in [Table 12-4](#build_optimization_impact)
    may reduce typical image sizes from several gigabytes to less than 1 GB. Similarly,
    build times can reduce from several minutes on average to less than a minute.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12-4\. Impact of build optimization on a typical image^([a](ch12.html#id1343))
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimization step | Build time (seconds) | Image size (GB) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Initial | 352.9 | 1.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Using minimal base images | 38.5 | 1.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Use caching | 24.4 | 1.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Layer ordering | 17.9 | 1.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-stage builds | 10.3 | 0.034 (34 MB) |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch12.html#id1343-marker)) Source: [warpbuild.com](https://www.warpbuild.com)
    |'
  prefs: []
  type: TYPE_TB
- en: Let’s review each in more detail with code examples for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: Use minimal base image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Base images allow you to start from a preconfigured image so you don’t have
    to install everything from scratch, including the Python interpreter. However,
    some base images available on the Docker Hub may not be suitable for production
    deployments. Instead, you’ll want to select the right base image with a minimal
    OS footprint to work from for faster builds and smaller image sizes, possibly
    with pre-installed Python dependencies and support for installing its various
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: Alpine base images use a lightweight Alpine Linux distribution designed to be
    small and secure, containing only the *base minimum* essential tools to run your
    application, but this won’t support installing many Python packages. On the other
    hand, slim base images may use other Linux distributions like Debian or CentOS,
    containing the *necessary* essential tools for running applications that make
    them larger than Alpine base images.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Use slim base images if you care about build time and Alpine base images if
    you care about image size.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `slim` base images such as `python:3.12-slim` or even Alpine
    base images like `python:3.12-alpine` that can be as small as 71.4 MB. A bare-bones
    Alpine image can even go down to 12.1 MB. The following command shows a list of
    base images pulled from the Docker repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Standard-sized images typically contain a full Linux distribution like Ubuntu
    or Debian containing a variety of pre-installed packages and dependencies, making
    them suitable for local development but perhaps not production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid GPU inference runtimes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In AI workloads where you’re serving ML/GenAI models, you may need to install
    deep learning frameworks, dependencies, and GPU libraries that can suddenly explode
    the footprint of your images. For instance, to make inferences on a GPU using
    the `transformers` library, you’ll need to install 3 GB of NVIDIA packages for
    GPU inference, 1.6 GB for the `torch` to perform the inference.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, you can’t reduce the image size if you need to use a GPU to perform
    an inference. However, if you can avoid GPU inference and just rely on CPUs, you
    may be able to reduce the image size by up to 10 times using the Open Neural Network
    Exchange (ONNX) runtime with model quantization.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 10](ch10.html#ch10), you can use the INT8 quantization
    with an ONNX model to benefit from model compression without much loss in output
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'To switch from the GPU inference runtime to the ONNX runtime for Hugging Face
    transformer models, you can use the `transformers[onnx]` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then export any Hugging Face transformer model checkpoint with default
    configurations to the ONNX format with `transformers.onnx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This command exports the `distilbert/distilbert-base-uncased` model checkpoint
    as an ONNX graph stored in `onnx/model.onnx`, which can be run with any Hugging
    Face model accelerator that supports the ONNX standard, as shown in [Example 12-13](#docker_onnx).
  prefs: []
  type: TYPE_NORMAL
- en: Example 12-13\. Model inference using the ONNX runtime with quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deployment_of_ai_services_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: ONNX runtime expects `numpy` arrays as input.
  prefs: []
  type: TYPE_NORMAL
- en: Using a technique such as shown in [Example 12-13](#docker_onnx), you can downsize
    from image sizes between 5 and 10 GB to around 0.5 GB, which is a massive footprint
    reduction, significantly more cost-effective and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Externalize application data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A core contributor to image size is copying models and application data into
    the image during build time. This approach increases both the build time and image
    size.
  prefs: []
  type: TYPE_NORMAL
- en: A better approach is to use volumes during local development and external storage
    solutions for downloading and loading models at application startup in production.
    In Kubernetes container orchestration environments, you can also use persistent
    volumes for model storage.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your application container takes a long time to download data and model artifacts
    from an external source, your health checks may fail, and the hosting platform
    can kill your containers prematurely. In such cases, configure health check probes
    to wait longer or as a last resort, bake the model into the image.
  prefs: []
  type: TYPE_NORMAL
- en: Layer ordering and caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker uses a layered filesystem to create layers in an image for each instruction
    in the Dockerfile. These layers are like a stack, with each layer adding more
    content on top of the previous layers. Whenever a layer changes, that layer (and
    further layers) will need to be rebuilt for those changes to appear in the image
    (i.e., build cache must be invalidated).
  prefs: []
  type: TYPE_NORMAL
- en: A layer (i.e., a filesystem snapshot) is created if the instruction is writing
    or deleting files into the container’s union filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dockerfile instructions that modify the filesystem like `ENV`, `COPY`, `ADD`,
    and `RUN` will contribute new layers to the build process, effectively increasing
    the image size. On the other hand, instructions such as `WORKDIR`, `ENTRYPOINT`,
    `LABEL`, and `CMD` that only update the image metadata don’t create any layers
    and any build cache.
  prefs: []
  type: TYPE_NORMAL
- en: After creation, each layer is then cached for reusability across image rebuilds
    if the instruction and files it depends on haven’t changed since the last build.
    Therefore, ideally, you want to write a Dockerfile that allows you to stop, destroy,
    rebuild, and replace containers with minimal setup and configuration.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few techniques you can use to minimize and optimize these layers
    as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Layer ordering to avoid frequent cache invalidation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since changes to the earlier layers can invalidate the build cache leading to
    repeating steps, you should order your Dockerfile from the most stable (e.g.,
    installations) to the most frequently changing or volatile (e.g., application
    code, configuration files).
  prefs: []
  type: TYPE_NORMAL
- en: Following this ordering, place the most stable yet expensive instructions (e.g.,
    model downloads or heavy dependency installations) at the start of the Dockerfile,
    and volatile, fast operations (e.g., copying application code) at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine your Dockerfile file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Here you’re creating a layer by copying your working directory containing the
    application code into the image before downloading and installing dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: If any one of source files changes, Docker builder will invalidate the cache
    causing the dependency installation to be repeated, which is expensive and can
    take several minutes to complete, if not cached by `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid repeating expensive steps, you can logically order your Dockerfile
    instructions to optimize the layer caching by reordering instructions like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now any changes to the source files won’t affect the long dependency installation
    step, drastically speeding up the build process.
  prefs: []
  type: TYPE_NORMAL
- en: Minimize layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To keep image sizes small, you’ll want to minimize image layers as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple technique to achieve this is to combine multiple `RUN` instructions
    into one. For instance, instead of writing multiple `RUN apt-get` installations,
    you can combine them into a single `RUN` command with `&&`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This will avoid adding unnecessary layers and prevents caching issues with `apt-get
    update` using the *cache busting* technique.
  prefs: []
  type: TYPE_NORMAL
- en: Since the builder may potentially skip updating the package index, causing installations
    to fail or use outdated packages, using the `&&` ensures that the latest packages
    are installed if the package index is updated.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also use the `--no-cache` flag when using `docker build` to avoid cache
    hits and ensure fresh downloads of base images and dependencies on every build.
  prefs: []
  type: TYPE_NORMAL
- en: Keep build context small
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *build context* is the set of files and directories that’ll be sent to the
    builder to carry out the Dockerfile instruction. A smaller build context reduces
    the amount of data sent to the builder and lowers the chance of cache invalidation,
    resulting in faster builds.
  prefs: []
  type: TYPE_NORMAL
- en: When you use the `COPY . .` command in a Dockerfile to copy your working directory
    into an image, you may also add tool caches, development dependencies, virtual
    environments, and unused files into the build context. Not only the image size
    will be increased, but also the Docker builder will cache these unnecessary files.
    Any changes to these files will then invalidate the build, restarting the whole
    build process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent the unnecessary cache invalidation, you can add a *.dockerignore*
    file next to your Dockerfile, listing all files and directories that your services
    won’t need in production. As an example, here are items you can include in a *.dockerignore*
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Docker builder will then ignore these files even when you run the `COPY` command
    across your entire working directory.
  prefs: []
  type: TYPE_NORMAL
- en: Use cache and bind mounts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can use *bind mounts* to avoid adding unnecessary layers to the image and
    *cache mounts* to speed up subsequent builds.
  prefs: []
  type: TYPE_NORMAL
- en: Bind mounts temporarily include files in the build context for a single `RUN`
    instruction and won’t persist as image layers after. Cache mounts specify a persistent
    cache location that you can read and write data to across multiple builds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example where you can download a pretrained model from Hugging Face
    into a mounted cache to optimize layer caching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This `RUN` instruction creates a cache of the downloaded pretrained model at
    `/root/.cache/huggingface`, which can be shared across multiple builds. This helps
    avoid redundant downloads and optimizes the build process by reusing cached layers.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the `--no-cache-dir` flag when using the `pip` package manager
    to avoid caching altogether for minimizing image size. However, you’ll have a
    significantly slower build process as follow-on builds will need to redownload
    each time.
  prefs: []
  type: TYPE_NORMAL
- en: Use external cache
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you’re building and deploying containers using a CI/CD pipeline, you can
    benefit from an external cache hosted on a remote location. An external cache
    can drastically speed up the build process in CI/CD pipelines where builders are
    often ephemeral and build minutes are precious.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use an external cache, you can specify the `--cache-to` and `--cache-from`
    options with the `docker buildx build` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Besides layer ordering and cache optimization, you can use multi-stage builds
    to significantly shrink your image sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-stage builds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using *multi-stage builds*, you can reduce the size of your final image by splitting
    out the Dockerfile instructions into distinct stages. Common stages can be reused
    to include shared components and serve as a starting point for further stages.
  prefs: []
  type: TYPE_NORMAL
- en: You can also selectively copy artifacts from one stage to another, leaving behind
    everything you don’t want in the final image. This ensures that only the required
    outputs are included in the final image from previous stages, avoiding any non-essential
    artifacts. Furthermore, you can also execute multiple build stages in parallel
    to speed up the build process of your images.
  prefs: []
  type: TYPE_NORMAL
- en: A common multi-stage build pattern is when you need a testing/development image
    and a slimmer production one with both starting from a shared first stage image.
    The development or testing image can include additional layers of tooling (i.e.,
    compilers, build systems, and debugging tools) to support the required workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you need to serve a bert transformer model from Hugging Face in a FastAPI
    service. You can write your Dockerfile instructions to use three distinct sequential
    stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first stage downloads the transformer model into `/root/.cache/huggingface`
    and creates a Python virtual environment at `/opt/venv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The second stage then copies the model artifacts and virtual Python environment
    `/opt/ven` from the `base` stage before copying source files over and creating
    a production version of the FastAPI service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The last stage copies the production stage virtual Python environment with
    installed packages and adds several development tools on top. It then starts the
    server with hot reload functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Using a single Dockerfile, we were able to create three distinct stages and
    use them as we see fit via the `--target development` command when needed.
  prefs: []
  type: TYPE_NORMAL
- en: docker init
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You now have an in-depth understanding of the containerization process with
    the Docker platform and the relevant best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ever need to add Docker to an existing project, you can use the `docker
    init` command, which will guide you through a wizard to create all the necessary
    Docker deployment files in your current working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This will provide a great starting point that you can work from to include additional
    configuration steps, dependencies, or services as required.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I recommend using `docker init` when starting out as every generated file will
    adhere to best practices including leveraging `dockerignore`, optimizing image
    layers, using bind and cache mounts for package installation, and switching to
    nonroot users.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have an optimized image and a set of working containers, you can choose
    any cloud provider or self-hosting solution for pushing images to registries and
    deploying your new GenAI services.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed various strategies for deploying your GenAI services—for
    instance, on virtual machines, as cloud functions, with managed app service platforms,
    or via containers. As part of this, I covered how virtualization differs from
    containerization and why you may want to deploy your services as containers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you learned about the Docker containerization platform and how you can
    use it to build self-contained images of your applications that can run as containers.
  prefs: []
  type: TYPE_NORMAL
- en: We covered the Docker storage and networking mechanisms that allow you to persist
    data using the union filesystem in containers and how to connect containers with
    different networking drivers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you were introduced to various optimization techniques for reducing
    the build time and size of your images to deploy your GenAI services as efficiently
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: With services containerized, you can push them to container registries to share,
    distribute, and run them on any cloud or hosting environment of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch12.html#id1291-marker)) Slim base Python images balance the size and
    compatibility of the Linux distribution with a wider range of Python packages
    out of the box compared to Alpine base Python images that minimize size but require
    extra configurations.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch12.html#id1292-marker)) You can use the `-p` or `--publish` flag when
    running the container to map and enable container access via a port.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch12.html#id1297-marker)) Images built on one machine can only run on
    other machines with the same processor architecture.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch12.html#id1317-marker)) You can still run executable files with the
    `r` permission alone by using the `bash script.sh` command instead of `./script.sh`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch12.html#id1336-marker)) Refer to the NVIDIA documentation on how to
    install the latest CUDA toolkit and graphics drivers for your system.
  prefs: []
  type: TYPE_NORMAL
