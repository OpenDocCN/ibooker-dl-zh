- en: Chapter 5\. Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover convolutional neural networks (CNNs). CNNs are
    the standard neural network architecture used for prediction when the input observations
    are images, which is the case in a wide range of neural network applications.
    So far in the book, we’ve focused exclusively on fully connected neural networks,
    which we implemented as a series of `Dense` layers. Thus, we’ll start this chapter
    by reviewing some key elements of these networks and use this to motivate why
    we might want to use a different architecture for images. We’ll then cover CNNs
    in a manner similar to that in which we introduced other concepts in this book:
    we’ll first discuss how they work at a high level, then move to discussing them
    at a lower level, and finally show in detail how they work by coding up the convolution
    operation from scratch.^([1](ch05.html#idm45732618773272)) By the end of this
    chapter, you’ll have a thorough enough understanding of how CNNs work to be able
    to use them both to solve problems and to learn about advanced CNN variants, such
    as ResNets, DenseNets, and Octave Convolutions on your own.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks and Representation Learning
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks initially receive data on observations, with each observation
    represented by some number *n* features. So far we’ve seen two examples of this
    in two very different domains: the first was the house prices dataset, where each
    observation was made up of 13 features, each of which represented a numeric characteristic
    about that house. The second was the MNIST dataset of handwritten digits; since
    the images were represented with 784 pixels (28 pixels wide by 28 pixels high),
    each observation was represented by 784 values indicating the lightness or darkness
    of each pixel.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In each case, after appropriately scaling the data, we were able to build a
    model that predicted the appropriate outcome for that dataset with high accuracy.
    Also in each case, a simple neural network model with one hidden layer performed
    better than a model without that hidden layer. Why is that? One reason, as I showed
    in the case of the house prices data, is that the neural network could learn *nonlinear*
    relationships between input and output. However, a more general reason is that
    in machine learning, we often need *linear combinations* of our original features
    in order to effectively predict our target. Let’s say that the pixel values for
    an MNIST digit are *x*[1] through *x*[784]. It could be the case, for example,
    that a combination of *x*[1] being higher than average, *x*[139] being lower than
    average, *and* *x*[237] also being lower than average strongly predicts that an
    image will be of digit 9\. There may be many other such combinations, all of which
    contribute positively or negatively to the probability that an image is of a particular
    digit. Neural networks can automatically *discover* combinations of the original
    features that are important through their training process. That process starts
    by creating initially random combinations of the original features via multiplication
    by a random weight matrix; through training, the neural network learns to refine
    combinations that are helpful and discard those that aren’t. This process of learning
    which combinations of features are important is known as *representation learning*,
    and it’s the main reason why neural networks are successful across different domains.
    This is summarized in [Figure 5-1](#fig_05_01).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0501.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. The neural networks we have seen so far start with <math><mi>n</mi></math>
    features and then learn somewhere between <math><msqrt><mi>n</mi></msqrt></math>
    and <math><mi>n</mi></math> “combinations” of these features to make predictions
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Is there any reason to modify this process for image data? The fundamental
    insight that suggests the answer is “yes” is that *in images, the interesting
    “combinations of features” (pixels) tend to come from pixels that are close together
    in the image*. In an image, it is simply much less likely that an interesting
    feature will result from a combination of 9 randomly selected pixels throughout
    the image than from a 3 × 3 patch of adjacent pixels. We want to exploit this
    fundamental fact about image data: that the order of the features matters since
    it tells us which pixels are near each other spatially, whereas in the house prices
    data the order of the features doesn’t matter. But how do we do it?'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有理由修改这个过程以适应图像数据？提示答案是“是”的基本见解是，在图像中，有趣的“特征组合”（像素）往往来自图像中彼此靠近的像素。在图像中，有趣的特征不太可能来自整个图像中随机选择的9个像素的组合，而更可能来自相邻像素的3×3补丁。我们想要利用关于图像数据的这一基本事实：特征的顺序很重要，因为它告诉我们哪些像素在空间上彼此靠近，而在房价数据中，特征的顺序并不重要。但是我们该如何做呢？
- en: A Different Architecture for Image Data
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像数据的不同架构
- en: The solution, at a high level, will be to create combinations of features, as
    before, but an order of magnitude more of them, and have each one be only a combination
    of the pixels from a small rectangular patch in the input image. [Figure 5-2](#fig_05_02)
    describes this.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，解决方案将是创建特征的组合，就像以前一样，但是数量级更多，并且每个特征只是输入图像中一个小矩形补丁的像素的组合。图5-2描述了这一点。
- en: '![Neural net diagram](assets/dlfs_0502.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图](assets/dlfs_0502.png)'
- en: Figure 5-2\. With image data, we can define each learned feature to be a function
    of a small patch of data, and thus define somewhere between <math><mi>n</mi></math>
    and <math><msup><mi>n</mi> <mn>2</mn></msup></math> output neurons
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2。对于图像数据，我们可以将每个学习到的特征定义为数据的一个小补丁的函数，因此可以定义介于n和n²之间的输出神经元
- en: 'Having our neural network learn combinations of *all* of the input features—that
    is, combinations of *all* of the pixels in the input image—turns out to be very
    inefficient, since it ignores the insight described in the prior section: that
    most of the interesting combinations of features in images occur in these small
    patches. Nevertheless, previously it was at least extremely easy to compute new
    features that were combinations of all the input features: if we had *f* input
    features and wanted to compute *n* new features, we could simply multiply the
    `ndarray` containing our input features by an `f` × `n` matrix. What operation
    can we use to compute many combinations of the pixels from local patches of the
    input image? The answer is the convolution operation.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们的神经网络学习*所有*输入特征的组合——也就是说，输入图像中*所有*像素的组合——事实证明是非常低效的，因为它忽略了前一节描述的见解：图像中有趣的特征组合大多出现在这些小补丁中。尽管如此，以前至少非常容易计算新特征，这些特征是所有输入特征的组合：如果我们有*f*个输入特征，并且想要计算*n*个新特征，我们只需将包含我们输入特征的`ndarray`乘以一个`f`×`n`矩阵。我们可以使用什么操作来计算输入图像的局部补丁中像素的许多组合？答案是卷积操作。
- en: The Convolution Operation
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'Before we describe the convolution operation, let’s make clear what we mean
    by “a feature that is a combination of pixels from a local patch of an image.”
    Let’s say we have a 5 × 5 input image *I*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述卷积操作之前，让我们明确一下我们所说的“来自图像局部补丁的像素组合的特征”是什么意思。假设我们有一个5×5的输入图像*I*：
- en: <math display="block"><mrow><mi>I</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>i</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>14</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>15</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>23</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>24</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>25</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>i</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>33</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>34</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>35</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi>
    <mn>41</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>42</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>43</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>44</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>45</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi> <mn>51</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>52</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>53</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>54</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>55</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>I</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>i</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>14</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>15</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>23</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>24</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>25</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>i</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>33</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>34</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>35</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi>
    <mn>41</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>42</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>43</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>44</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>45</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi> <mn>51</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>52</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>53</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>54</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>55</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'And let’s say we want to calculate a new feature that is a function of the
    3 × 3 patch of pixels in the middle. Well, just as we’ve defined new features
    as linear combinations of old features in the neural networks we’ve seen so far,
    we’ll define a new feature that is a function of this 3 × 3 patch, which we’ll
    do by defining a 3 × 3 set of weights, *W*:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想计算一个新特征，它是中间3×3像素补丁的函数。就像我们迄今所见的神经网络中将新特征定义为旧特征的线性组合一样，我们将定义一个新特征，它是这个3×3补丁的函数，我们将通过定义一个3×3的权重*W*来实现：
- en: <math display="block"><mrow><mi>w</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>w</mi>
    <mn>13</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>w</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>23</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>w</mi> <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>w</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>w</mi>
    <mn>13</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>w</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>23</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>w</mi> <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Then we’ll simply take the dot product of *W* with the relevant patch from
    *I* to get the value of the feature in the output, which, since the section of
    the input image involved was centered at (3,3), we’ll denote as *o*[33] (the *o*
    stands for “output”):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将简单地将*W*与*I*中相关补丁的点积，以获取输出中特征的值，由于涉及的输入图像部分位于(3,3)处，我们将表示为*o*[33]（*o*代表“输出”）：
- en: <math display="block"><mrow><msub><mi>o</mi> <mn>33</mn></msub> <mo>=</mo> <msub><mi>w</mi>
    <mn>11</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>22</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>12</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>23</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>13</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>24</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>21</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>32</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>22</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>33</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>23</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>34</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>31</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>42</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>32</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>43</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>33</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>44</mn></msub></mrow></math>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>o</mi> <mn>33</mn></msub> <mo>=</mo> <msub><mi>w</mi>
    <mn>11</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>22</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>12</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>23</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>13</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>24</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>21</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>32</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>22</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>33</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>23</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>34</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>31</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>42</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>32</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>43</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>33</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>44</mn></msub></mrow></math>
- en: 'This value will then be treated like the other computed features we’ve seen
    in neural networks: it may have a bias added to it and then will probably be fed
    through an activation function, and then it will represent a “neuron” or “learned
    feature” that will get passed along to subsequent layers of the network. Thus
    we *can* define features that are functions of small patches of an input image.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个值将被视为我们在神经网络中看到的其他计算特征：可能会添加一个偏差，然后可能会通过激活函数，然后它将代表一个“神经元”或“学习到的特征”，将被传递到网络的后续层。因此，我们可以定义特征，这些特征是输入图像的小补丁的函数。
- en: 'How should we interpret such features? It turns out that features computed
    in this way have a special interpretation: they represent whether a *visual pattern
    defined by the weights* is present at that location of the image. The fact that
    3 × 3 or 5 × 5 arrays of numbers can represent “pattern detectors” when their
    dot product is taken with the pixel values at each location of an image has been
    well known in the field of computer vision for a long time. For example, taking
    the dot product of the following 3 × 3 array of numbers:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何解释这些特征？事实证明，以这种方式计算的特征有一个特殊的解释：它们表示权重定义的*视觉模式*是否存在于图像的该位置。当将3×3或5×5的数字数组与图像的每个位置的像素值进行点积时，它们可以表示“模式检测器”，这在计算机视觉领域已经很久了。例如，将以下3×3数字数组进行点积：
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mrow><mo>-</mo>
    <mn>4</mn></mrow></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></math>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mrow><mo>-</mo>
    <mn>4</mn></mrow></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></math>
- en: with a given section of an input image detects whether there is an edge at that
    location of the image. There are similar matrices known to be able to detect whether
    corners exist, whether vertical or horizontal lines exist, and so on.^([2](ch05.html#idm45732618587560))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入图像的给定部分，检测该图像位置是否存在边缘。已知有类似的矩阵可以检测角是否存在，垂直或水平线是否存在，等等。
- en: Now suppose that we used the *same set of weights* *W* to detect whether the
    visual pattern defined by *W* existed at each location in the input image. We
    could imagine “sliding *W* over the input image,” taking the dot product of *W*
    with the pixels at each location of the image, and ending up with a new image
    *O* of almost identical size to the original image (it may be slightly different,
    depending on how we handle the edges). This image *O* would be a kind of “feature
    map” showing the locations in the input image where the pattern defined by *W*
    was present. This operation is in fact what happens in convolutional neural networks;
    it is called a *convolution*, and its output is indeed called a *feature map*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们使用*相同的权重集* *W* 来检测输入图像的每个位置是否存在由*W*定义的视觉模式。我们可以想象“在输入图像上滑动*W*”，将*W*与图像每个位置的像素进行点积，最终得到一个几乎与原始图像大小相同的新图像*O*（可能略有不同，取决于我们如何处理边缘）。这个图像*O*将是一种“特征图”，显示了输入图像中*W*定义的模式存在的位置。这实际上就是卷积神经网络中发生的操作；它被称为*卷积*，其输出确实被称为*特征图*。
- en: This operation is at the core of how CNNs work. Before we can incorporate it
    into a full-fledged `Operation`, of the kind we’ve seen in the prior chapters,
    we have to add another dimension to it—literally.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作是CNN如何工作的核心。在我们可以将其整合到我们在前几章中看到的那种完整的`Operation`之前，我们必须为其添加另一个维度-字面上。
- en: The Multichannel Convolution Operation
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多通道卷积操作
- en: 'To review: convolutional neural networks differ from regular neural networks
    in that they create an order of magnitude more features, and in that each feature
    is a function of just a small patch from the input image. Now we can get more
    specific: starting with *n* input pixels, the convolution operation just described
    will create *n* output features, one for each location in the input image. What
    actually happens in a convolutional `Layer` in a neural network goes one step
    further: there, we’ll create *f* *sets* of *n* features, *each* with a corresponding
    (initially random) set of weights defining a visual pattern whose detection at
    each location in the input image will be captured in the feature map. These *f*
    feature maps will be created via *f* convolution operations. This is captured
    in [Figure 5-3](#fig_05_03).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下：卷积神经网络与常规神经网络的不同之处在于它们创建了数量级更多的特征，并且每个特征仅是来自输入图像的一个小块的函数。现在我们可以更具体：从*n*个输入像素开始，刚刚描述的卷积操作将为输入图像中的每个位置创建*n*个输出特征。在神经网络的卷积`Layer`中实际发生的事情更进一步：在那里，我们将创建*f*组*n*个特征，*每个*都有一个对应的（最初是随机的）权重集，定义了在输入图像的每个位置检测到的视觉模式，这将在特征图中捕获。这*f*个特征图将通过*f*个卷积操作创建。这在[图5-3](#fig_05_03)中有所体现。
- en: '![Neural net diagram](assets/dlfs_0503.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图示](assets/dlfs_0503.png)'
- en: Figure 5-3\. More specifically than before, for an input image with n pixels,
    we define an output with f feature maps, each of which has about the same size
    as the original image, for a total of n × f total output neurons for the image,
    each of which is a function of only a small patch of the original image
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3。比以前更具体，对于具有n个像素的输入图像，我们定义一个输出，其中包含f个特征图，每个特征图的大小与原始图像大致相同，总共有n×f个输出神经元用于图像，每个神经元仅是原始图像的一个小块的函数
- en: Now that we’ve introduced a bunch of concepts, let’s define them for clarity.
    While each “set of features” detected by a particular set of weights is called
    a feature map, in the context of a convolutional `Layer`, the number of feature
    maps is referred to as the number of *channels* of the `Layer`—this is why the
    operation involved with the `Layer` is called the multichannel convolution. In
    addition, the *f* sets of weights *W*[*i*] are called the convolutional *filters*.^([3](ch05.html#idm45732618558136))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一堆概念，让我们为了清晰起见对它们进行定义。在卷积`Layer`的上下文中，每个由特定权重集检测到的“特征集”称为特征图，特征图的数量在卷积`Layer`中被称为`Layer`的*通道数*，这就是为什么与`Layer`相关的操作被称为多通道卷积。此外，*f*组权重*W*[*i*]被称为卷积*滤波器*。
- en: Convolutional Layers
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'Now that we understand the multichannel convolution operation, we can think
    about how to incorporate this operation into a neural network layer. Previously,
    our neural network layers were relatively straightforward: they received two-dimensional
    `ndarray`s as input and produced two-dimensional `ndarray`s as output. Based on
    the description in the prior section, however, convolutional layers will have
    a 3D `ndarray` as output *for a single image*, with dimensions *number of channels*
    (same as “feature maps”) × *image height* × *image width*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了多通道卷积操作，我们可以考虑如何将这个操作整合到神经网络层中。以前，我们的神经网络层相对简单：它们接收二维的`ndarray`作为输入，并产生二维的`ndarray`作为输出。然而，根据前一节的描述，卷积层将会为*单个图像*产生一个三维的`ndarray`作为输出，维度为*通道数*（与“特征图”相同）×*图像高度*×*图像宽度*。
- en: 'This raises a question: how can we feed this `ndarray` forward into another
    convolutional layer to create a “deep convolutional” neural network? We’ve seen
    how to perform the convolution operation on an image with a single channel and
    our filters; how can we perform the multichannel convolution on an *input* with
    *multiple* channels, as we’ll have to do when two convolutional layers are strung
    together? Understanding this is the key to understanding *deep* convolutional
    neural networks.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个问题：我们如何将这个`ndarray`前向传递到另一个卷积层中，以创建一个“深度卷积”神经网络？我们已经看到如何在具有单个通道和我们的滤波器的图像上执行卷积操作；当两个卷积层串联时，我们如何在*具有多个*通道的*输入*上执行多通道卷积？理解这一点是理解*深度*卷积神经网络的关键。
- en: 'Consider what happens in a neural network with fully connected layers: in the
    first hidden layer, we have, let’s say, *h*[1] features that are combinations
    of all of the original features from the input layer. In the layer that follows,
    the features are combinations of all of the features from the prior layer, so
    that we might have *h*[2] “features of features” of the original features. To
    create this next layer of *h*[2] features, we use <math><mrow><msub><mi>h</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>h</mi> <mn>2</mn></msub></mrow></math>
    weights to represent that each of the *h*[2] features is a function of each of
    the *h*[1] features in the prior layer.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在具有全连接层的神经网络中会发生什么：在第一个隐藏层中，我们有，假设，*h*[1]个特征，这些特征是来自输入层的所有原始特征的组合。在接下来的层中，特征是来自前一层的所有特征的组合，因此我们可能有*h*[2]个原始特征的“特征的特征”。为了创建这一层的*h*[2]个特征，我们使用<math><mrow><msub><mi>h</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>h</mi> <mn>2</mn></msub></mrow></math>个权重来表示*h*[2]个特征中的每一个都是前一层中的*h*[1]个特征的函数。
- en: 'As described in the prior section, an analogous process happens in the first
    layer of a convolutional neural network: we first transform the input image into
    *m*[1] *feature maps*, using *m*[1] convolutional *filters*. We should think of
    the output of this layer as representing whether each of the *m*[1] different
    visual patterns represented by the weights of the *m*[1] filters is present at
    each location in the input image. Just as different layers of a fully connected
    neural network can contain different numbers of neurons, the next layer of the
    convolutional neural network could contain *m*[2] filters. In order for the network
    to learn complex patterns, the interpretation of each of these should be whether
    each of the *“patterns of patterns”* or higher-order visual features represented
    by *combinations of the* *m*[1] *visual patterns from the prior layer* was present
    at that location of the image. This implies that if the output of the convolutional
    layer is a 3D `ndarray` of shape *m*[2] channels × image height × image width,
    then a given location in the image on one of the *m*[2] feature maps is *a linear
    combination of convolving* *m*[1] *different filters over that same location in
    each of the corresponding* *m*[1] *feature maps from the prior layer*. This will
    allow each location in each of the *m*[2] filter maps to represent a *combination*
    of the *m*[1] visual features already learned in the prior convolutional layer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，在卷积神经网络的第一层中会发生类似的过程：我们首先使用*m*[1]个卷积*滤波器*将输入图像转换为*m*[1]个*特征图*。我们应该将这一层的输出看作是表示权重的*m*[1]个滤波器在输入图像的每个位置是否存在的不同视觉模式。就像全连接神经网络的不同层可以包含不同数量的神经元一样，卷积神经网络的下一层可能包含*m*[2]个滤波器。为了使网络学习复杂的模式，每个滤波器的解释应该是在图像的每个位置是否存在前一层中*m*[1]个视觉模式的*组合*或更高阶视觉特征。这意味着如果卷积层的输出是一个形状为*m*[2]个通道×图像高度×图像宽度的3D
    `ndarray`，那么图像中一个给定位置上的*m*[2]个特征图中的一个是在前一层对应的*m*[1]个特征图的每个相同位置上卷积*m*[1]个不同的滤波器的线性组合。这将使得*m*[2]个滤波器图中的每个位置都表示前一卷积层中已学习的*m*[1]个视觉特征的*组合*。
- en: Implementation Implications
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现影响
- en: 'This understanding of how two multichannel convolutional layers are connected
    tells us how to implement the operation: just as we need <math><mrow><msub><mi>h</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>h</mi> <mn>2</mn></msub></mrow></math>
    weights to connect a fully connected layer with *h*[1] neurons to one with <math><msub><mi>h</mi>
    <mn>2</mn></msub></math> , we need <math><mrow><msub><mi>m</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>m</mi> <mn>2</mn></msub></mrow></math> *convolutional filters*
    to connect a convolutional layer with *m*[1] channels to one with *m*[2]. With
    this last detail in place, we can now specify the dimensions of the `ndarray`s
    that will make up the input, output, and parameters of the full, multichannel
    convolution operation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 了解两个多通道卷积层如何连接告诉我们如何实现这个操作：正如我们需要<math><mrow><msub><mi>h</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>h</mi> <mn>2</mn></msub></mrow></math>个权重来连接一个具有*h*[1]个神经元的全连接层和一个具有<math><msub><mi>h</mi>
    <mn>2</mn></msub></math>个神经元的全连接层，我们需要<math><mrow><msub><mi>m</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>m</mi> <mn>2</mn></msub></mrow></math>个*卷积滤波器*来连接一个具有*m*[1]个通道的卷积层和一个具有*m*[2]个通道的卷积层。有了这个最后的细节，我们现在可以指定构成完整的多通道卷积操作的`ndarray`的维度，包括输入、输出和参数：
- en: 'The input will have shape:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入的形状将是：
- en: Batch size
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小
- en: Input channels
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入通道
- en: Image height
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像高度
- en: Image width
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像宽度
- en: 'The output will have shape:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出的形状将是：
- en: Batch size
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小
- en: Output channels
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出通道
- en: Image height
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像高度
- en: Image width
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像宽度
- en: 'The convolutional filters themselves will have shape:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积滤波器本身的形状将是：
- en: Input channels
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入通道
- en: Output channels
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出通道
- en: Filter height
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器高度
- en: Filter width
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器宽度
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The order of the dimensions may vary from library to library, but these four
    dimensions will always be present.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 维度的顺序可能因库而异，但这四个维度始终存在。
- en: We’ll keep all of this in mind when we implement this convolution operation
    later in the chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面实现这个卷积操作时牢记所有这些。
- en: The Differences Between Convolutional and Fully Connected Layers
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层和全连接层之间的差异
- en: At the beginning of the chapter, we discussed the differences between convolutional
    and fully connected layers at a high level; [Figure 5-4](#fig_05_04) revisits
    that comparison, now that we’ve described convolutional layers in more detail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们讨论了卷积层和全连接层在高层次上的区别；[图5-4](#fig_05_04)重新审视了这种比较，现在我们已经更详细地描述了卷积层。
- en: '![Neural net diagram](assets/dlfs_0504.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图示](assets/dlfs_0504.png)'
- en: Figure 5-4\. Comparison between convolutional and fully connected layers
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。卷积层和全连接层之间的比较
- en: 'In addition, one last difference between the two kinds of layers is the way
    in which the individual neurons themselves are interpreted:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这两种层之间的最后一个区别是个体神经元本身的解释方式：
- en: The interpretation of each neuron of a fully connected layer is that it detects
    whether or not *a particular combination of the features learned by the prior
    layer* is present in the current observation.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层中每个神经元的解释是，它检测先前层学习的*特定特征组合*是否存在于当前观察中。
- en: The interpretation of a neuron of a convolutional layer is that it detects whether
    or not *a particular combination of visual patterns* learned by the prior layer
    is present *at the given location* of the input image.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层中的神经元的解释是，它检测先前层学习的*特定视觉模式组合*是否存在于输入图像的*给定位置*。
- en: 'There’s one more problem we need to solve before we can incorporate such a
    layer into a neural network: how to use the dimensional `ndarray`s we obtain as
    output to make predictions.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将这样的层合并到神经网络之前，我们需要解决另一个问题：如何使用输出的多维数组来进行预测。
- en: 'Making Predictions with Convolutional Layers: The Flatten Layer'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用卷积层进行预测：Flatten层
- en: We’ve covered how convolutional layers learn features that represent whether
    visual patterns exist in images and store those features in layers of feature
    maps; how do we use these layers of feature maps to make predictions? When using
    fully connected neural networks to predict which of 10 classes an image belonged
    to in the prior chapter, we just had to ensure that the last layer had dimension
    10; we could then feed these 10 numbers into the softmax cross entropy loss function
    to ensure they were interpreted as probabilities. Now we need to figure out what
    we can do in the case of our convolutional layer, where we have a three-dimensional
    `ndarray` per observation of shape *m* channels × image height × image width.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了卷积层如何学习代表图像中是否存在视觉模式的特征，并将这些特征存储在特征图层中；我们如何使用这些特征图层来进行预测呢？在上一章中使用全连接神经网络预测图像属于10个类别中的哪一个时，我们只需要确保最后一层的维度为10；然后我们可以将这10个数字输入到softmax交叉熵损失函数中，以确保它们被解释为概率。现在我们需要弄清楚在卷积层的情况下我们可以做什么，其中每个观察都有一个三维的形状为*m*通道数
    × 图像高度 × 图像宽度的`ndarray`。
- en: 'To see the answer, recall that each neuron simply represents whether a particular
    combination of visual features (which, if this is a deep convolutional neural
    network, could be a feature of features or a feature of features of features)
    is present at a given location in the image. This is no different from the features
    that would be learned if we applied a fully connected neural network to this image:
    the first fully connected layer would represent features of the individual pixels,
    the second would represent features of these features, and so on. And in a fully
    connected architecture, we would simply treat each “feature of features” that
    the network had learned as a single neuron that would be used as input to a prediction
    of which class the image belonged to.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到答案，回想一下，每个神经元只是表示图像中是否存在特定视觉特征组合（如果这是一个深度卷积神经网络，则可能是特征的特征或特征的特征的特征）在图像的给定位置。这与如果我们将全连接神经网络应用于此图像时学习的特征没有区别：第一个全连接层将表示单个像素的特征，第二个将表示这些特征的特征，依此类推。在全连接架构中，我们只需将网络学习的每个“特征的特征”视为单个神经元，用作预测图像属于哪个类别的输入。
- en: It turns out that we can do the same thing with convolutional neural networks—we
    treat the *m* feature maps as <math><mrow><mi>m</mi> <mo>×</mo> <mi>i</mi> <mi>m</mi>
    <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mo>×</mo> <mi>i</mi> <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow></msub></mrow></math>
    neurons and use a `Flatten` operation to squash these three dimensions (the number
    of channels, the image height, and the image width) down into a one-dimensional
    vector, after which we can use a simple matrix multiplication to make our final
    predictions. The intuition for why this works is that each individual neuron *fundamentally
    represents the same “kind of thing”* as the neurons in a fully connected layer—specifically,
    whether a given visual feature (or combination of features) is present at a given
    location in an image)—and thus we can treat them the same way in the final layer
    of the neural network.^([4](ch05.html#idm45732618457096))
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们可以用卷积神经网络做同样的事情——我们将*m*个特征图视为<math><mrow><mi>m</mi> <mo>×</mo> <mi>i</mi>
    <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mo>×</mo> <mi>i</mi> <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow></msub></mrow></math>个神经元，并使用`Flatten`操作将这三个维度（通道数、图像高度和图像宽度）压缩成一个一维向量，然后我们可以使用简单的矩阵乘法进行最终预测。这样做的直觉是，每个单独的神经元*基本上代表与全连接层中的神经元相同的“类型”*——具体来说，表示在图像的给定位置是否存在给定的视觉特征（或特征组合）——因此我们可以在神经网络的最后一层中以相同的方式处理它们。^([4](ch05.html#idm45732618457096))
- en: We’ll see how to implement the `Flatten` layer later in the chapter. But before
    we dive into the implementation, let’s discuss another kind of layer that is important
    in many CNN architectures, though we won’t cover it in great detail in this book.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到如何实现`Flatten`层。但在我们深入实现之前，让我们讨论另一种在许多CNN架构中很重要的层，尽管本书不会详细介绍它。
- en: Pooling Layers
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: '*Pooling* layers are another kind of layer commonly used in convolutional neural
    networks. They simply *downsample* each of the feature maps created by a convolution
    operation; for the most typically used pooling size of 2, this involves mapping
    each 2 × 2 section of each feature map either to the maximum value of that section,
    in the case of *max-pooling*, or to the average value of that section, in the
    case of *average-pooling*. For an *n* × *n* image, then, this would map the entire
    image to one of size <math><mfrac><mi>n</mi> <mn>2</mn></mfrac></math> × <math><mfrac><mi>n</mi>
    <mn>2</mn></mfrac></math> . [Figure 5-5](#fig_05_05) illustrates this.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*池化*层是卷积神经网络中常用的另一种类型的层。它们简单地对由卷积操作创建的每个特征图进行*下采样*；对于最常用的池化大小为2，这涉及将每个特征图的每个2×2部分映射到该部分的最大值（最大池化）或该部分的平均值（平均池化）。因此，对于一个n×n的图像，整个图像将被映射到一个<math><mfrac><mi>n</mi>
    <mn>2</mn></mfrac></math> × <math><mfrac><mi>n</mi> <mn>2</mn></mfrac></math>的大小。图5-5说明了这一点。'
- en: '![Neural net diagram](assets/dlfs_0505.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络图示](assets/dlfs_0505.png)'
- en: Figure 5-5\. An illustration of max- and average-pooling with a 4 × 4 input;
    each 2 × 2 patch is mapped to either the average or the max values for that patch
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5。一个4×4输入的最大池化和平均池化示例；每个2×2的块被映射到该块的平均值或最大值
- en: 'The main advantage of pooling is computational: by downsampling the image to
    contain one-fourth as many pixels as the prior layer, pooling decreases both the
    number of weights and the number of computations needed to train the network by
    a factor of 4; this can be further compounded if multiple pooling layers are used
    in the network, as they were in many architectures in the early days of CNNs.
    The downside of pooling, of course, is that only one-fourth as much information
    can be extracted from the downsampled image. However, the fact that architectures
    showed very strong performance on benchmarks in image recognition despite the
    use of pooling suggested that, even though pooling was causing the networks to
    “lose information” about the images by decreasing the images’ resolution, the
    trade-offs in terms of increased computational speed were worth it. Nevertheless,
    pooling was considered by many to be a trick that just happened to work but should
    probably be done away with; as Geoffrey Hinton wrote on [a Reddit AMA](https://oreil.ly/2YZU0Kc)
    in 2014, “The pooling operation used in convolutional neural networks is a big
    mistake and the fact that it works so well is a disaster.” And indeed, most recent
    CNN architectures (such as Residual Networks, or “ResNets”^([5](ch05.html#idm45732618426472)))
    use pooling minimally or not at all. Thus, in this book, we’re not going to implement
    pooling layers, but given their importance for “putting CNNs on the map” via their
    use in famous architectures such as AlexNet, we mention them here for completeness.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 池化的主要优势在于计算：通过将图像下采样为前一层的四分之一像素数，池化将网络训练所需的权重数量和计算数量减少了四分之一；如果网络中使用了多个池化层，这种减少可以进一步叠加，就像在CNN的早期架构中使用的许多架构中一样。当然，池化的缺点是，从下采样的图像中只能提取四分之一的信息。然而，尽管池化通过降低图像的分辨率导致网络“丢失”了关于图像的信息，但尽管如此，这种权衡在增加计算速度方面是值得的，因为架构在图像识别基准测试中表现非常出色。然而，许多人认为池化只是一个偶然起作用的技巧，应该被淘汰；正如Geoffrey
    Hinton在2014年的Reddit AMA中写道：“卷积神经网络中使用的池化操作是一个大错误，它能够如此成功地运行是一场灾难。”事实上，大多数最近的CNN架构（如残差网络或“ResNets”）最小化或根本不使用池化。因此，在本书中，我们不会实现池化层，但考虑到它们在著名架构（如AlexNet）中的使用对“推动CNN发展”至关重要，我们在这里提及它们以保持完整性。
- en: Applying CNNs beyond images
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将CNN应用于图像之外
- en: 'Everything we have described so far is extremely standard for dealing with
    images using neural networks: the images are typically represented as a set of
    *m*[1] channels of pixels, where *m*[1] = 1 for black-and-white images, and *m*[1]
    = 3 for color images—and then some number *m*[2] of convolution operations are
    applied to each channel (using the <math><mrow><msub><mi>m</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>m</mi> <mn>2</mn></msub></mrow></math> filter maps as explained
    previously), with this pattern continuing on for several layers. This has all
    been covered in other treatments of convolutional neural networks; what is less
    commonly covered is that the idea of organizing data into “channels” and then
    processing that data using a CNN goes beyond just images. For example, this data
    representation was a key to DeepMind’s series of AlphaGo programs showing that
    neural networks could learn to play Go. To quote the paper:^([6](ch05.html#idm45732618415512))'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所描述的一切在使用神经网络处理图像方面都是非常标准的：图像通常被表示为一组*m*[1]通道的像素，其中*m*[1]=1表示黑白图像，*m*[1]=3表示彩色图像—然后对每个通道应用一定数量的卷积操作（使用之前解释过的<math><mrow><msub><mi>m</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>m</mi> <mn>2</mn></msub></mrow></math>滤波器映射），这种模式会持续几层。这些内容在其他卷积神经网络的处理中已经涵盖过；不太常见的是，将数据组织成“通道”，然后使用CNN处理数据的想法不仅仅适用于图像。例如，这种数据表示是DeepMind的AlphaGo系列程序的关键，展示了神经网络可以学会下围棋。引用论文中的话：
- en: The input to the neural network is a 19 × 19 × 17 image stack comprising 17
    binary feature planes. 8 feature planes *X*[t] consist of binary values indicating
    the presence of the current player’s stones ( <math><mrow><msub><mi>X</mi> <msub><mi>t</mi>
    <mi>i</mi></msub></msub> <mo>=</mo> <mn>1</mn></mrow></math> if intersection *i*
    contains a stone of the player’s color at time-step t; 0 if the intersection is
    empty, contains an opponent stone, or if t < 0). A further 8 feature planes, *Y*[t],
    represent the corresponding features for the opponent’s stones. The final feature
    plane, C, represents the color to play, and has a constant value of either 1 if
    black is to play or 0 if white is to play. These planes are concatenated together
    to give input features *s*[t] = *X*[t], *Y*[t], *X*[t – 1], *Y*[t – 1], …, *X*[t
    – 7], *Y*[t – 7], *C*. History features *X*[t], *Y*[t] are necessary because Go
    is not fully observable solely from the current stones, as repetitions are forbidden;
    similarly, the color feature C is necessary because the *komi* is not observable.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, they essentially represented the board as a 19 × 19 pixel “image”
    with 17 channels! They used 16 of these channels to encode what had happened on
    the 8 prior moves that each player had taken; this was necessary so that they
    could encode rules that prevented repetition of earlier moves. The 17th channel
    was actually a 19 × 19 grid of either all 1s or all 0s, depending on whose turn
    it was to move.^([7](ch05.html#idm45732618397624)) CNNs and their multichannel
    convolution operations are mostly commonly applied to images, but the even more
    general idea of representing data that is arranged along some spatial dimension
    with multiple “channels” is applicable even beyond images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: In keeping with the theme of this book, however, to truly understand the multichannel
    convolution operation, you have to implement it from scratch; the next several
    sections will describe this process in detail.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Multichannel Convolution Operation
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It turns out that implementing this daunting operation—which involves a four-dimensional
    input `ndarray` and a four-dimensional parameter `ndarray`—is much clearer if
    we first examine the *one*-dimensional case. Building up to the full operation
    from that starting point will turn out mostly to be a matter of adding a bunch
    of `for` loops. Throughout, we’ll take the same approach we took in [Chapter 1](ch01.html#foundations),
    alternating between diagrams, math, and working Python code.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The Forward Pass
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The convolution in one dimension is conceptually identical to the convolution
    in two dimensions: we take in a one-dimensional input and a one-dimensional convolutional
    filter as inputs and then create the output by sliding the filter along the input.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s suppose our input is of length 5:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutions](assets/dlfs_05in01.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'And let’s say the size of the “patterns” we want to detect is length 3:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Filter length three](assets/dlfs_05in02.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Diagrams and math
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first element of the output would be created by convolving the first element
    of the input with the filter:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![Output element 1](assets/dlfs_05in03.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: 'The second element of the output would be created by sliding the filter one
    unit to the right and convolving it with the next set values of the series:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of length one](assets/dlfs_05in04.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'Fair enough. However, when we compute the next output value, we realize that
    we have run out of room:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of length one](assets/dlfs_05in05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: We have hit the end of our input, and the resulting output has just three elements,
    when we started with five! How can we address this?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid the output shrinking as a result of the convolution operation, we’ll
    introduce a trick used throughout convolutional neural networks: we “pad” the
    input with zeros around the edges, enough so that the output remains the same
    size as the input. Otherwise, every time we convolve a filter over the input,
    we’ll end up with an output that is slightly smaller than the input, as seen previously.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免由于卷积操作导致输出缩小，我们将引入一种在卷积神经网络中广泛使用的技巧：我们在边缘周围“填充”输入与零，以使输出保持与输入大小相同。否则，每次我们在输入上卷积一个滤波器时，我们最终得到的输出会略小于输入，就像之前看到的那样。
- en: 'As you can reason from the preceding convolution example: for a filter of size
    3, there should be one unit of padding around the edges to keep the output the
    same size as the input. More generally, since we almost always use odd-numbered
    filter sizes, we add padding equal to the filter size divided by 2 and rounded
    down to the nearest integer.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以从前面的卷积示例推理出的：对于大小为3的滤波器，应该在边缘周围添加一个单位的填充，以保持输出与输入大小相同。更一般地，由于我们几乎总是使用奇数大小的滤波器，我们添加填充等于滤波器大小除以2并向下舍入到最接近的整数。
- en: 'Let’s say we add this padding, so that instead of the input ranging from *i*[1]
    to *i*[5], it ranges from *i*[0] to *i*[6], where both *i*[0] and *i*[6] are 0\.
    Then we can compute the output of the convolution as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们添加了这种填充，这样，输入不再是从*i*[1]到*i*[5]，而是从*i*[0]到*i*[6]，其中*i*[0]和*i*[6]都是0。然后我们可以计算卷积的输出为：
- en: <math display="block"><mrow><msub><mi>o</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>i</mi>
    <mn>0</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>2</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub></mrow></math>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>o</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>i</mi>
    <mn>0</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>2</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub></mrow></math>
- en: 'And so on, up until:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 依此类推，直到：
- en: <math display="block"><mrow><msub><mi>o</mi> <mn>5</mn></msub> <mo>=</mo> <msub><mi>i</mi>
    <mn>4</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>5</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>6</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub></mrow></math>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>o</mi> <mn>5</mn></msub> <mo>=</mo> <msub><mi>i</mi>
    <mn>4</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>5</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>6</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub></mrow></math>
- en: And now the output is the same size as the input. How might we code this up?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在输出与输入大小相同了。我们如何编写代码呢？
- en: Code
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码
- en: 'Coding up this part turns out to be pretty straightforward. Before we do, let’s
    summarize the steps we just discussed:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 编写这部分代码实际上非常简单。在我们开始之前，让我们总结一下我们刚刚讨论的步骤：
- en: We ultimately want to produce an output that is the same size as the input.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终希望生成一个与输入大小相同的输出。
- en: To do this without “shrinking” the output, we’ll first need to pad the input.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在不“缩小”输出的情况下执行此操作，我们首先需要填充输入。
- en: Then we’ll have to write some sort of loop that goes through the input and convolves
    each position of it with the filter.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将不得不编写一些循环，通过输入并将其每个位置与滤波器进行卷积。
- en: 'We’ll start with our input and our filter:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从我们的输入和滤波器开始：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s a helper function that can pad our one-dimensional input on each end:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个辅助函数，可以在一维输入的两端填充：
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: What about the convolution itself? Observe that for each element in the output
    that we want to produce, we have a corresponding element in the *padded* input
    where we “start” the convolution operation; once we figure out where to start,
    we simply loop through all the elements in the filter, doing a multiplication
    at each element and adding the result to the total.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积本身呢？观察到，对于我们想要生成的每个输出元素，我们在“填充”输入中有一个对应的元素，我们在那里“开始”卷积操作；一旦我们弄清楚从哪里开始，我们只需循环遍历滤波器中的所有元素，在每个元素上进行乘法并将结果添加到总和中。
- en: 'How do we find this “corresponding element”? Note that, simply, the value at
    the first element in the output gets its value starting at the first element of
    the padded input! This makes the `for` loop quite easy to write:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何找到这个“对应的元素”？注意，简单地说，输出中第一个元素的值从填充输入的第一个元素开始！这使得`for`循环非常容易编写：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'That’s simple enough. Before we move on to the backward pass of this operation—the
    tricky part—let’s briefly discuss a hyperparameter of convolutions that we’re
    glossing over: stride.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经足够简单了。在我们继续进行此操作的反向传递之前——棘手的部分——让我们简要讨论一下我们正在忽略的卷积的一个超参数：步幅。
- en: A note on stride
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于步幅的说明
- en: 'We noted earlier that pooling operations were one way to downsample images
    from feature maps. In many early convolutional architectures, these did indeed
    significantly reduce the amount of computation needed without any significant
    hit to accuracy; nevertheless, they’ve fallen out of favor because of their downside:
    they effectively downsample the image so that an image with just half the resolution
    is passed forward into the next layer.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前注意到，池化操作是从特征图中对图像进行下采样的一种方法。在许多早期的卷积架构中，这确实显著减少了所需的计算量，而且没有对准确性造成重大影响；然而，它们已经不再受欢迎，因为它们的缺点：它们有效地对图像进行下采样，使得分辨率减半的图像传递到下一层。
- en: 'A much more widely accepted way to do this is to modify the *stride* of the
    convolution operation. The stride is the amount that the filter is incrementally
    slid over the image—in the previous case, we are using a stride of 1, and as a
    result each filter is convolved with every element of the input, which is why
    the output ends up being the same size as the input. With a stride of 2, the filter
    would be convolved with *every other* element of the input image, so that the
    output would be half the size of the input; with a stride of 3, the filter would
    be convolved with *every third* element of the input image, and so on. This means
    that, for example, using a stride of 2 would result in the same output size and
    thus much the same reduction in computation we would get from pooling with size
    2, but without as much *loss of information*: with pooling of size 2, only one-fourth
    of the elements in the input have *any* effect on the output, whereas with a stride
    of 2, *every* element of the input has *some* effect on the output. The use of
    a stride of greater than 1 is thus significantly more prevalent than pooling for
    downsampling even in the most advanced CNN architectures of today.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更广泛接受的方法是修改卷积操作的*步幅*。步幅是滤波器在图像上逐步滑动的量——在先前的情况下，我们使用步幅为1，因此每个滤波器与输入的每个元素进行卷积，这就是为什么输出的大小与输入的大小相同。使用步幅为2，滤波器将与输入图像的*每隔一个*元素进行卷积，因此输出的大小将是输入的一半；使用步幅为3，滤波器将与输入图像的*每隔两个*元素进行卷积，依此类推。这意味着，例如，使用步幅为2将导致相同的输出大小，因此与使用大小为2的池化相比，计算减少了很多，但没有太多的*信息损失*：使用大小为2的池化，只有输入中四分之一的元素对输出产生*任何*影响，而使用步幅为2，*每个*输入元素对输出都有*一些*影响。因此，即使在今天最先进的CNN架构中，使用大于1的步幅进行下采样的情况比池化更为普遍。
- en: Nevertheless, in this book I’ll just show examples with a stride of 1—modifying
    these operations to allow a stride of greater than 1 is left as an exercise for
    the reader. Using a stride equal to 1 also makes writing the backward pass easier.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这本书中，我只会展示步幅为1的示例，将这些操作修改为允许大于1的步幅是留给读者的练习。使用步幅等于1也使得编写反向传播更容易。
- en: 'Convolutions: The Backward Pass'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积：反向传播
- en: 'The backward pass is where convolutions get a bit trickier. Let’s recall what
    we’re trying to do: before, we produced the output of a convolution operation
    using the input and the parameters. We now want to compute:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是卷积变得有点棘手的地方。让我们回顾一下我们要做的事情：之前，我们使用输入和参数生成了卷积操作的输出。现在我们想要计算：
- en: The partial derivative of the loss with respect to each element of the *input*
    to the convolution operation—`inp` previously
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失相对于卷积操作的*输入*的每个元素的偏导数——之前是`inp`
- en: The partial derivative of the loss with respect to each element of the *filter*—`param_1d`
    previously
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失相对于卷积操作的*滤波器*的每个元素的偏导数——之前是`param_1d`
- en: 'Think of how the `ParamOperation`s we saw in [Chapter 4](ch04.html#extensions)
    work: in the `backward` method, they receive an output gradient representing how
    much each element of the output ultimately affects the loss and then use this
    output gradient to compute the gradients for the input and the parameters. So
    we need to write a function that takes in an `output_grad` with the same shape
    as the input and produces an `input_grad` and a `param_grad`.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 想想我们在[第4章](ch04.html#extensions)中看到的`ParamOperation`是如何工作的：在`backward`方法中，它们接收一个表示每个输出元素最终影响损失程度的输出梯度，然后使用这个输出梯度来计算输入和参数的梯度。因此，我们需要编写一个函数，该函数接受与输入形状相同的`output_grad`，并产生一个`input_grad`和一个`param_grad`。
- en: 'How can we test whether the computed gradients are correct? We’ll bring back
    an idea from the first chapter: we know that the partial derivative of a sum with
    respect to any one of its inputs is 1 (if the sum *s* = *a* + *b* + *c*, then
    <math><mrow><mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>a</mi></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>c</mi></mrow></mfrac>
    <mo>=</mo> <mn>1</mn></mrow></math> ). So we can compute the `input_grad` and
    `param_grad` quantities using our `_input_grad` and `_param_grad` functions (which
    we’ll reason through and write shortly) and an `output_grad` equal to all 1s.
    Then we’ll check whether these gradients are correct by changing elements of the
    input by some quantity *α* and seeing whether the resulting sum changes by the
    gradient times *α*.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何测试计算出的梯度是否正确？我们将从第一章中带回一个想法：我们知道对于任何一个输入，对于和的偏导数是1（如果和*s* = *a* + *b* +
    *c*，那么<math><mrow><mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>a</mi></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>c</mi></mrow></mfrac>
    <mo>=</mo> <mn>1</mn></mrow></math>）。因此，我们可以使用我们的`_input_grad`和`_param_grad`函数（我们将很快推理和编写）以及一个全部为1的`output_grad`来计算`input_grad`和`param_grad`量。然后，我们将通过改变输入的元素一些数量*α*，并查看结果的总和是否通过梯度乘以*α*而改变来检查这些梯度是否正确。
- en: What “should” the gradient be?
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度“应该”是多少？
- en: 'Using the logic just described, let’s calculate what an element of the gradient
    vector for the input *should* be:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用刚才描述的逻辑，让我们计算输入向量的一个元素的梯度*应该*是多少：
- en: '[PRE5]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So, the gradient of the fifth element of the input *should* be 41 – 39 = 2.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输入的第五个元素的梯度*应该*是41 - 39 = 2。
- en: Now let’s try to reason through how we should compute such a gradient without
    simply computing the difference between these two sums. Here is where things get
    interesting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试推理如何计算这样的梯度，而不仅仅是计算这两个总和之间的差异。这就是事情变得有趣的地方。
- en: Computing the gradient of a 1D convolution
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算1D卷积的梯度
- en: 'We see that increasing this element of the input increased the output by 2\.
    Taking a close look at the output shows exactly how it does this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到增加输入的这个元素使输出增加了2。仔细观察输出，可以清楚地看到它是如何做到这一点的：
- en: '![Full output](assets/dlfs_05in06.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![完整输出](assets/dlfs_05in06.png)'
- en: 'This particular element of the input is denoted *t*[5]. It appears in the output
    in two places:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的特定元素被表示为*t*[5]。它在输出中出现在两个地方：
- en: As part of *o*[4], it is multiplied by *w*[3].
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为*o*[4]的一部分，它与*w*[3]相乘。
- en: As part of *o*[5], it is multiplied by *w*[2].
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为*o*[5]的一部分，它与*w*[2]相乘。
- en: To help see the general pattern of how inputs map to the sum of outputs, note
    that if there was an *o*[6] present, *t*[5] would also contribute to the output
    through being multiplied by *w*[1].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助看到输入如何映射到输出总和的一般模式，请注意，如果存在*o*[6]，*t*[5]也将通过与*w*[1]相乘而对输出产生影响。
- en: 'Therefore, the amount that <math><msub><mi>t</mi> <mn>5</mn></msub></math>
    ultimately affects the loss, which we can denote as <math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>t</mi> <mn>5</mn></msub></mrow></mfrac></math> , will
    be:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，<math><msub><mi>t</mi> <mn>5</mn></msub></math>最终影响损失的数量，我们可以表示为<math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>t</mi> <mn>5</mn></msub></mrow></mfrac></math>，将是：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>o</mi> <mn>4</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi>
    <mn>3</mn></msub> <mo>+</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>o</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub>
    <mo>+</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>o</mi>
    <mn>6</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>o</mi> <mn>4</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi>
    <mn>3</mn></msub> <mo>+</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>o</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub>
    <mo>+</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>o</mi>
    <mn>6</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
- en: 'Of course, in this simple example, when the loss is just the sum, <math><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>o</mi> <mi>i</mi></msub></mrow></mfrac> <mo>=</mo> <mn>1</mn></mrow></math>
    for all elements, in the output (except for the “padding” elements for which this
    quantity is 0). This sum is very easy to compute: it is simply *w*[2] + *w*[3],
    which is indeed 2 since *w*[2] = *w*[3] = 1.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在这个简单的例子中，当损失只是总和时，对于所有输出元素，<math><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>o</mi> <mi>i</mi></msub></mrow></mfrac> <mo>=</mo> <mn>1</mn></mrow></math>（对于“填充”元素除外，该数量为0）。这个总和非常容易计算：它只是*w*[2]
    + *w*[3]，确实是2，因为*w*[2] = *w*[3] = 1。
- en: What’s the general pattern?
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一般模式是什么？
- en: 'Now let’s look for the general pattern for a generic input element. This turns
    out to be an exercise in keeping track of indices. Since we’re translating math
    into code here, let’s use <math><msubsup><mi>o</mi> <mi>i</mi> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></math>
    to denote the *i*th element of the output gradient (since we’ll ultimately be
    accessing it via `output_grad[i]`). Then:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们寻找通用输入元素的一般模式。这实际上是一个跟踪索引的练习。由于我们在这里将数学转换为代码，让我们使用<math><msubsup><mi>o</mi>
    <mi>i</mi> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></math>来表示输出梯度的第*i*个元素（因为我们最终将通过`output_grad[i]`访问它）。然后：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>5</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>6</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>5</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>6</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
- en: 'Looking closely at this output, we can reason similarly that:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察这个输出，我们可以类似地推理：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>3</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>2</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>3</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>3</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>2</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>3</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
- en: 'and:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 和：
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>3</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>4</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>5</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>3</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>4</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>5</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
- en: 'There’s clearly a pattern here, and translating it into code is a bit tricky,
    especially since the indices on the output increase at the same time the indices
    on the weights decrease. Nevertheless, the way to express this turns out to be
    via the following double `for` loop:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显然有一个模式，将其转换为代码有点棘手，特别是因为输出上的索引增加的同时权重上的索引减少。然而，表达这一点的方式是通过以下双重`for`循环：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This does the appropriate incrementing of the indices of the weights, while
    decreasing the weights on the output at the same time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做适当地增加了权重的索引，同时减少了输出上的权重。
- en: Though it may not be obvious now, reasoning through this and getting it is out
    to be the trickiest part of calculating the gradients for convolution operations.
    Adding more complexity to this, such as batch sizes, convolutions with two-dimensional
    inputs, or inputs with multiple channels, is simply a matter of adding more `for`
    loops to the preceding lines, as we’ll see in the next few sections.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现在可能不明显，但通过推理并得到它是计算卷积操作的梯度中最棘手的部分。增加更多复杂性，例如批量大小、具有二维输入的卷积或具有多个通道的输入，只是在前面的几行中添加更多的`for`循环，我们将在接下来的几节中看到。
- en: Computing the parameter gradient
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算参数梯度
- en: 'We can reason similarly about how increasing an element of the filter should
    increase the output. First, let’s increase (arbitrarily) the first element of
    the filter by one unit and observe the resulting impact on the sum:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地推理，关于如何增加滤波器的一个元素应该增加输出。首先，让我们增加（任意地）滤波器的第一个元素一个单位，并观察对总和的影响：
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So we should find that <math><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>w</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mn>10</mn></mrow></math>
    .
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们应该发现<math><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>w</mi>
    <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mn>10</mn></mrow></math>。
- en: 'Just as we did for the input, by closely examining the output and seeing which
    elements of the filter affect it, as well as padding the input to more clearly
    see the pattern, we see that:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们为输入所做的那样，通过仔细检查输出并看到哪些滤波器元素影响它，以及填充输入以更清楚地看到模式，我们看到：
- en: <math display="block"><mrow><msubsup><mi>w</mi> <mn>1</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>=</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>1</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>+</mo> <msub><mi>t</mi>
    <mn>1</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>2</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>+</mo> <msub><mi>t</mi> <mn>2</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>3</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>+</mo> <msub><mi>t</mi>
    <mn>3</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>+</mo> <msub><mi>t</mi> <mn>4</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>5</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></mrow></math>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msubsup><mi>w</mi> <mn>1</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>=</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>1</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>+</mo> <msub><mi>t</mi>
    <mn>1</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>2</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>+</mo> <msub><mi>t</mi> <mn>2</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>3</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>+</mo> <msub><mi>t</mi>
    <mn>3</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>+</mo> <msub><mi>t</mi> <mn>4</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>5</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></mrow></math>
- en: 'And since, for the sum, all of the <math><msubsup><mi>o</mi> <mi>i</mi> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></math>
    elements are just 1, and *t*[0] is 0, we have:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对于总和，所有的<math><msubsup><mi>o</mi> <mi>i</mi> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></math>元素都是1，而*t*[0]是0，我们有：
- en: <math display="block"><mrow><msubsup><mi>w</mi> <mn>1</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>=</mo> <msub><mi>t</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>t</mi> <mn>2</mn></msub>
    <mo>+</mo> <msub><mi>t</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>t</mi> <mn>4</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>+</mo> <mn>2</mn> <mo>+</mo> <mn>3</mn> <mo>+</mo> <mn>4</mn>
    <mo>=</mo> <mn>10</mn></mrow></math>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msubsup><mi>w</mi> <mn>1</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>=</mo> <msub><mi>t</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>t</mi> <mn>2</mn></msub>
    <mo>+</mo> <msub><mi>t</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>t</mi> <mn>4</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>+</mo> <mn>2</mn> <mo>+</mo> <mn>3</mn> <mo>+</mo> <mn>4</mn>
    <mo>=</mo> <mn>10</mn></mrow></math>
- en: This confirms the calculation from earlier.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了之前的计算。
- en: Coding this up
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码这个
- en: 'Coding this up turns out to be easier than writing the code for the input gradient,
    since this time “the indices are moving in the same direction.” Within the same
    nested `for` loop, the code is:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 编码这个比编写输入梯度的代码更容易，因为这次“索引是朝着同一个方向移动的。”在同一个嵌套的`for`循环中，代码是：
- en: '[PRE11]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can combine these two computations and write a function to compute
    both the input gradient and the filter gradient with the following steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以结合这两个计算，并编写一个函数来计算输入梯度和滤波器梯度，具体步骤如下：
- en: Take the input and filter as arguments.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入和滤波器作为参数。
- en: Compute the output.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出。
- en: Pad the input and the output gradient (to get, say, `input_pad` and `output_pad`).
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充输入和输出梯度（例如，得到`input_pad`和`output_pad`）。
- en: As shown earlier, use the padded output gradient and the filter to compute the
    gradient.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所示，使用填充的输出梯度和滤波器来计算梯度。
- en: Similarly, use the output gradient (not padded) and the padded input to compute
    the filter gradient.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，使用输出梯度（未填充）和填充输入来计算滤波器梯度。
- en: I show the full function that wraps around the preceding code blocks in the
    book’s [GitHub repo](https://oreil.ly/2H99xkJ).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our explanation of how to implement convolutions in 1D! As we’ll
    see in the next several sections, extending this reasoning to work on two-dimensional
    inputs, batches of two-dimensional inputs, or even multichannel batches of two-dimensional
    inputs is (perhaps surprisingly) straightforward.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Batches, 2D Convolutions, and Multiple Channels
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s add the capability for these convolution functions to work with
    *batches* of inputs—2D inputs whose first dimension represents the batch size
    of the input and whose second dimension represents the length of the 1D sequence:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can follow the same general steps defined before: we’ll first pad the input,
    use this to compute the output, and then pad the output gradient to compute both
    the input and filter gradients.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '1D convolutions with batches: forward pass'
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The only difference in implementing the forward pass when the input has a second
    dimension representing the batch size is that we have to pad and compute the output
    for each observation individually (as we did previously) and then `stack` the
    results to get a batch of outputs. For example, `conv_1d` becomes:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '1D convolutions with batches: backward pass'
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The backward pass is similar: computing the input gradient now simply takes
    the `for` loop for computing the input gradient from the prior section, computes
    it for each observation, and `stack`s the results:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The gradient for the filter when dealing with a batch of observations is a
    bit different. This is because the filter is convolved with every observation
    in the input and is thus connected to every observation in the output. So, to
    compute the parameter gradient, we have to loop through all of the observations
    and increment the appropriate values of the parameter gradient as we do so. Still,
    this just involves adding an outer `for` loop to the code to compute the parameter
    gradient that we saw earlier:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Adding this dimension on top of the original 1D convolution was indeed simple;
    extending this from one- to two-dimensional inputs is similarly straightforward.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 2D Convolutions
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The 2D convolution is a straightforward extension of the 1D case because, fundamentally,
    the way the input is connected to the output via the filters in each dimension
    of the 2D case is identical to the 1D case. As a result, the high-level steps
    on both the forward and backward passes remain the same:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'On the forward pass, we:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appropriately pad the input.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the padded input and the parameters to compute the output.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the backward pass, to compute the input gradient we:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appropriately pad the output gradient.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use this padded output gradient, along with the input and the parameters, to
    compute both the input gradient and the parameter gradient.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also on the backward pass, to compute the parameter gradient we:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appropriately pad the input.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Loop through the elements of the padded input and increment the parameter gradient
    appropriately as we go along.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '2D convolutions: coding the forward pass'
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To make this concrete, recall that for 1D convolutions the code for computing
    the output given the input and the parameters on the forward pass looked as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For 2D convolutions, we simply modify this to be:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can see that we’ve simply “blown each `for` loop out” into two `for` loops.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The extension to two dimensions when we have a batch of images is also similar
    to the 1D case: just as we did there, we simply add a `for` loop to the outside
    of the loops shown here.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '2D convolutions: coding the backward pass'
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sure enough, just as in the forward pass, we can use the same indexing for
    the backward pass as in the 1D case. Recall that in the 1D case, the code was:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the 2D case, the code is simply:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that the indexing on the output is the same as in the 1D case but is simply
    taking place in two dimensions; in the 1D case, we had:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'and in the 2D case, we have:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The other facts from the 1D case also apply:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: For a batch of input images, we simply perform the preceding operation for each
    observation and then `stack` the results.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the parameter gradient, we have to loop through all the images in the batch
    and add components from each one to the appropriate places in the parameter gradient:^([8](ch05.html#idm45732616694312))
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: At this point, we’ve almost written the code for the complete multichannel convolution
    operation; currently, our code convolves filters over a two-dimensional input
    and produces a two-dimensional output. Of course, as we described earlier, each
    convolutional layer not only has neurons arranged along these two dimensions but
    also has some number of “channels” equal to the number of feature maps that the
    layer creates. Addressing this last challenge is what we’ll cover next.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The Last Element: Adding “Channels”'
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How can we modify what we’ve written thus far to account for cases where both
    the input and the output are multichannel? The answer, as it was when we added
    batches earlier, is simple: we add two outer `for` loops to the code we’ve already
    seen—one loop for the input channels and another for the output channels. By looping
    through all combinations of the input channel and the output channel, we make
    each output feature map a combination of all of the input feature maps, as desired.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: For this to work, we will have to *always* represent our images as three-dimensional
    `ndarray`s, as opposed to the two-dimensional arrays we’ve been using; we’ll represent
    black-and-white images with one channel and color images with three channels (one
    for the red values at each location in the image, one for the blue values, and
    one for the green values). Then, regardless of the number of channels, the operation
    proceeds as described earlier, with a number of feature maps being created from
    the image, each of which is a combination of the convolutions resulting from all
    of the channels in the image (or from the channels in the prior layer, if dealing
    with layers further on in the network).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Forward pass
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given all this, the full code to compute the output for a convolutional layer,
    given four-dimensional `ndarray`s for the input and the parameters, is:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that `_pad_2d_channel` is a function that pads the input along the channel
    dimension.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Again, the actual code that does the computation is similar to the code in the
    simpler 2D case (without channels) shown before, except now we have, for example,
    `fil[c_out][c_in][p_w][p_h]` instead of just `fil[p_w][p_h]`, since there are
    two more dimensions and `c_out × c_in` more elements in the filter array.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Backward pass
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The backward pass is similar and follows the same conceptual principles as
    the backward pass in the simple 2D case:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: For the input gradients, we compute the gradients of each observation individually—padding
    the output gradient to do so—and then `stack` the gradients.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also use the padded output gradient for the parameter gradient, but we loop
    through the observations as well and use the appropriate values from each one
    to update the parameter gradient.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s the code for computing the output gradient:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And here’s the parameter gradient:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: These three functions—`_output`, `_input_grad`, and `_param_grad`—are just what
    we need to create a `Conv2DOperation`, which will ultimately form the core of
    the `Conv2DLayer`s we’ll use in our CNNs! There are just a few more details to
    work out before we can use this `Operation` in a working convolutional neural
    network.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Using This Operation to Train a CNN
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to implement a few more pieces before we can have a working CNN model:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We have to implement the `Flatten` operation discussed earlier in the chapter;
    this is necessary to enable the model to make predictions.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have to incorporate this `Operation` as well as the `Conv2DOpOperation` into
    a `Conv2D` `Layer`.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, for any of this to be usable, we have to write a faster version of
    the `Conv2D` `Operation`. We’ll outline this here and share the details in [“Matrix
    Chain Rule”](app01.html#matrix-chain-rule).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Flatten Operation
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There’s one other `Operation` we’ll need to complete our convolutional layer:
    the `Flatten` operation. The output of a convolution operation is a 3D `ndarray`
    for each observation, of dimension `(channels, img_height, img_width)`. However,
    unless we are passing this data into another convolutional layer, we’ll first
    need to transform it into a *vector* for each observation. Luckily, as described
    previously, since each of the individual neurons involved encodes whether a particular
    visual feature is present at that location in the image, we can simply “flatten”
    this 3D `ndarray` into a 1D vector and pass it forward without any problem. The
    `Flatten` operation shown here does this, accounting for the fact that in convolutional
    layers, as with any other layer, the first dimension of our `ndarray` is always
    the batch size:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That’s the last `Operation` we’ll need; let’s wrap these `Operation`s up in
    a `Layer`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The Full Conv2D Layer
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The full convolutional layer, then, would look something like this:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `Flatten` operation is optionally added on at the end, depending on whether
    we want the output of this layer to be passed forward into another convolutional
    layer or passed into another fully connected layer for predictions.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: A note on speed, and an alternative implementation
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As those of you who are familiar with computational complexity will realize,
    this code is catastrophically slow: to calculate the parameter gradient, we needed
    to write *seven* nested `for` loops! There’s nothing wrong with doing this, since
    the purpose of writing the convolution operation from scratch was to solidify
    our understanding of how CNNs work. Still, it is possible to write convolutions
    in a completely different way; instead of breaking down that process like we have
    in this chapter, we can break it down into the following steps:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: From the input, extract `image_height × image_width × num_channels` patches
    of size `filter_height × filter_width` from the test set.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of these patches, perform a dot product of the patch with the appropriate
    filter connecting the input channels to the output channels.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stack and reshape the results of all of these dot products to form the output.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With a bit of cleverness, we can express almost all of the operations described
    previously in terms of a batch matrix multiplication, implemented using NumPy’s
    `np.matmul` function. The details of how to do this are described in [Appendix A](app01.html#appendix)
    and are implemented on [the book’s website](https://oreil.ly/2H99xkJ), but suffice
    it to say that this allows us to write relatively small convolutional neural networks
    that can train in a reasonable amount of time. This lets us actually run experiments
    to see how well convolutional neural networks work!
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even using the convolution operation defined by reshaping and the `matmul`
    functions, it takes about 10 minutes to train this model for one epoch with just
    one convolutional layer, so we restrict ourselves to demonstrating a model with
    just one convolutional layer, with 32 channels (a number chosen somewhat arbitrarily):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that this model has just 32 × 5 × 5 = 800 parameters in the first layer,
    but these parameters are used to create 32 × 28 × 28 = 25,088 neurons, or “learned
    features.” By contrast, a fully connected layer with hidden size 32 would have
    784 × 32 = 25,088 *parameters*, and just 32 neurons.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Some simple trial and error—training this model for just a few hundred batches
    with different learning rates and observing the resulting validation losses—shows
    that a learning rate of 0.01 works better than a learning rate of 0.1 now that
    we have a convolutional layer as our first layer, rather than a fully connected
    layer. Training this network for one epoch with optimizer `SGDMomentum(lr = 0.01,
    momentum=0.9)` gives:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This shows that we can indeed train a convolutional neural network from scratch
    that ends up getting above 90% accuracy on MNIST with just one pass through the
    training set!^([9](ch05.html#idm45732615365528))
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned about convolutional neural networks. You started
    out learning at a high level what they are and about their similarities and differences
    from fully connected neural networks, and then you went all the way down to seeing
    how they work at the lowest level, implementing the core multichannel convolution
    operation from scratch in Python.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Starting at a high level, convolutional layers create roughly an order of magnitude
    more neurons than the fully connected layers we’ve seen so far, with each neuron
    being a combination of just a few features from the prior layer, rather than each
    neuron being a combination of *all* of the features from the prior layer as they
    are in fully connected layers. A level below that, we saw that these neurons are
    in fact grouped into “feature maps,” each of which represents whether a particular
    visual feature—or a particular combination of visual features, in the case of
    deep convolutional neural networks—is present at a given location in an image.
    Collectively we refer to these feature maps as the convolutional `Layer`’s “channels.”
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite all these differences from the `Operation`s we saw involved with the
    `Dense` layer, the convolution operation fits into the same template as other
    `ParamOperation`s we’ve seen:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: It has an `_output` method that computes the output given its input and the
    parameter.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has `_input_grad` and `_param_grad` methods that, given an `output_grad`
    of the same shape as the `Operation`’s `output`, compute gradients of the same
    shape as the input and parameters, respectively.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference is just that the `_input`, `output`, and `param`s are now four-dimensional
    `ndarray`s, whereas they were two-dimensional in the case of fully connected layers.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'This knowledge should serve as an extremely solid foundation for any future
    learning about or application of the convolutional neural networks you undertake.
    Next, we’ll cover another common kind of advanced neural network architecture:
    recurrent neural networks, designed for dealing with data that appears in sequences,
    rather than simply the nonsequential batches we’ve dealt with in the cases of
    houses and images. Onward!'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#idm45732618773272-marker)) The code we’ll write, while clearly
    expressing how convolutions work, will be extremely inefficient. In [“Gradient
    of the Loss with Respect to the Bias Terms”](app01.html#gradient-loss-bias-terms),
    I provide a more efficient implementation of the batch, multichannel convolution
    operation we’ll describe in this chapter using NumPy.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.html#idm45732618587560-marker)) See the Wikipedia page for [“Kernel
    (image processing)”](https://oreil.ly/2KOwfzs) for more examples.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#idm45732618558136-marker)) These are also referred to as *kernels*.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.html#idm45732618457096-marker)) This is why it is important to understand
    the output of convolution operations both as creating a number of filter maps—say,
    <math><mi>m</mi></math> —as well as creating <math><mrow><mi>m</mi> <mo>×</mo>
    <mi>i</mi> <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mo>×</mo> <mi>i</mi> <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow></msub></mrow></math>
    individual neurons. As is the case throughout neural networks, holding multiple
    levels of interpretation in one’s mind all at one time, and seeing the connection
    between them, is key.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.html#idm45732618426472-marker)) See the [original ResNet paper](http://tiny.cc/dlfs_resnet_paper),
    “Deep Residual Learning for Image Recognition,” by Kaiming He et al.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.html#idm45732618415512-marker)) DeepMind (David Silver et al.), [*Mastering
    the Game of Go Without Human Knowledge*](https://oreil.ly/wUpMW), 2017.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.html#idm45732618397624-marker)) A year later, DeepMind published
    results using a similar representation with chess—only this time, to encode the
    more complex ruleset of chess, the input had 119 channels! See DeepMind (David
    Silver et al.), [“A General Reinforcement Learning Algorithm That Masters Chess,
    Shogi, and Go Through Self-Play”](https://oreil.ly/E6ydw).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.html#idm45732616694312-marker)) See the full implementations of these
    on [the book’s website](https://oreil.ly/2H99xkJ).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.html#idm45732615365528-marker)) The full code can be found in the
    section for this chapter on [the book’s GitHub repo](https://oreil.ly/2H99xkJ).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
