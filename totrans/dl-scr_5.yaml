- en: Chapter 5\. Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover convolutional neural networks (CNNs). CNNs are
    the standard neural network architecture used for prediction when the input observations
    are images, which is the case in a wide range of neural network applications.
    So far in the book, we’ve focused exclusively on fully connected neural networks,
    which we implemented as a series of `Dense` layers. Thus, we’ll start this chapter
    by reviewing some key elements of these networks and use this to motivate why
    we might want to use a different architecture for images. We’ll then cover CNNs
    in a manner similar to that in which we introduced other concepts in this book:
    we’ll first discuss how they work at a high level, then move to discussing them
    at a lower level, and finally show in detail how they work by coding up the convolution
    operation from scratch.^([1](ch05.html#idm45732618773272)) By the end of this
    chapter, you’ll have a thorough enough understanding of how CNNs work to be able
    to use them both to solve problems and to learn about advanced CNN variants, such
    as ResNets, DenseNets, and Octave Convolutions on your own.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks and Representation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks initially receive data on observations, with each observation
    represented by some number *n* features. So far we’ve seen two examples of this
    in two very different domains: the first was the house prices dataset, where each
    observation was made up of 13 features, each of which represented a numeric characteristic
    about that house. The second was the MNIST dataset of handwritten digits; since
    the images were represented with 784 pixels (28 pixels wide by 28 pixels high),
    each observation was represented by 784 values indicating the lightness or darkness
    of each pixel.'
  prefs: []
  type: TYPE_NORMAL
- en: In each case, after appropriately scaling the data, we were able to build a
    model that predicted the appropriate outcome for that dataset with high accuracy.
    Also in each case, a simple neural network model with one hidden layer performed
    better than a model without that hidden layer. Why is that? One reason, as I showed
    in the case of the house prices data, is that the neural network could learn *nonlinear*
    relationships between input and output. However, a more general reason is that
    in machine learning, we often need *linear combinations* of our original features
    in order to effectively predict our target. Let’s say that the pixel values for
    an MNIST digit are *x*[1] through *x*[784]. It could be the case, for example,
    that a combination of *x*[1] being higher than average, *x*[139] being lower than
    average, *and* *x*[237] also being lower than average strongly predicts that an
    image will be of digit 9\. There may be many other such combinations, all of which
    contribute positively or negatively to the probability that an image is of a particular
    digit. Neural networks can automatically *discover* combinations of the original
    features that are important through their training process. That process starts
    by creating initially random combinations of the original features via multiplication
    by a random weight matrix; through training, the neural network learns to refine
    combinations that are helpful and discard those that aren’t. This process of learning
    which combinations of features are important is known as *representation learning*,
    and it’s the main reason why neural networks are successful across different domains.
    This is summarized in [Figure 5-1](#fig_05_01).
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. The neural networks we have seen so far start with <math><mi>n</mi></math>
    features and then learn somewhere between <math><msqrt><mi>n</mi></msqrt></math>
    and <math><mi>n</mi></math> “combinations” of these features to make predictions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Is there any reason to modify this process for image data? The fundamental
    insight that suggests the answer is “yes” is that *in images, the interesting
    “combinations of features” (pixels) tend to come from pixels that are close together
    in the image*. In an image, it is simply much less likely that an interesting
    feature will result from a combination of 9 randomly selected pixels throughout
    the image than from a 3 × 3 patch of adjacent pixels. We want to exploit this
    fundamental fact about image data: that the order of the features matters since
    it tells us which pixels are near each other spatially, whereas in the house prices
    data the order of the features doesn’t matter. But how do we do it?'
  prefs: []
  type: TYPE_NORMAL
- en: A Different Architecture for Image Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The solution, at a high level, will be to create combinations of features, as
    before, but an order of magnitude more of them, and have each one be only a combination
    of the pixels from a small rectangular patch in the input image. [Figure 5-2](#fig_05_02)
    describes this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. With image data, we can define each learned feature to be a function
    of a small patch of data, and thus define somewhere between <math><mi>n</mi></math>
    and <math><msup><mi>n</mi> <mn>2</mn></msup></math> output neurons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Having our neural network learn combinations of *all* of the input features—that
    is, combinations of *all* of the pixels in the input image—turns out to be very
    inefficient, since it ignores the insight described in the prior section: that
    most of the interesting combinations of features in images occur in these small
    patches. Nevertheless, previously it was at least extremely easy to compute new
    features that were combinations of all the input features: if we had *f* input
    features and wanted to compute *n* new features, we could simply multiply the
    `ndarray` containing our input features by an `f` × `n` matrix. What operation
    can we use to compute many combinations of the pixels from local patches of the
    input image? The answer is the convolution operation.'
  prefs: []
  type: TYPE_NORMAL
- en: The Convolution Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we describe the convolution operation, let’s make clear what we mean
    by “a feature that is a combination of pixels from a local patch of an image.”
    Let’s say we have a 5 × 5 input image *I*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>I</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>i</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>14</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>15</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>23</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>24</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>25</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>i</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>33</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>34</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>35</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi>
    <mn>41</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>42</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>43</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>44</mn></msub></mtd> <mtd><msub><mi>i</mi>
    <mn>45</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>i</mi> <mn>51</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>52</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>53</mn></msub></mtd>
    <mtd><msub><mi>i</mi> <mn>54</mn></msub></mtd> <mtd><msub><mi>i</mi> <mn>55</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'And let’s say we want to calculate a new feature that is a function of the
    3 × 3 patch of pixels in the middle. Well, just as we’ve defined new features
    as linear combinations of old features in the neural networks we’ve seen so far,
    we’ll define a new feature that is a function of this 3 × 3 patch, which we’ll
    do by defining a 3 × 3 set of weights, *W*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>w</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>w</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>w</mi>
    <mn>13</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>w</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>w</mi> <mn>22</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>23</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>w</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>w</mi> <mn>32</mn></msub></mtd>
    <mtd><msub><mi>w</mi> <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we’ll simply take the dot product of *W* with the relevant patch from
    *I* to get the value of the feature in the output, which, since the section of
    the input image involved was centered at (3,3), we’ll denote as *o*[33] (the *o*
    stands for “output”):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>o</mi> <mn>33</mn></msub> <mo>=</mo> <msub><mi>w</mi>
    <mn>11</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>22</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>12</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>23</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>13</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>24</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>21</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>32</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>22</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>33</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>23</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>34</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>31</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>42</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>32</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>43</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>33</mn></msub> <mo>×</mo> <msub><mi>i</mi> <mn>44</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This value will then be treated like the other computed features we’ve seen
    in neural networks: it may have a bias added to it and then will probably be fed
    through an activation function, and then it will represent a “neuron” or “learned
    feature” that will get passed along to subsequent layers of the network. Thus
    we *can* define features that are functions of small patches of an input image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How should we interpret such features? It turns out that features computed
    in this way have a special interpretation: they represent whether a *visual pattern
    defined by the weights* is present at that location of the image. The fact that
    3 × 3 or 5 × 5 arrays of numbers can represent “pattern detectors” when their
    dot product is taken with the pixel values at each location of an image has been
    well known in the field of computer vision for a long time. For example, taking
    the dot product of the following 3 × 3 array of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mrow><mo>-</mo>
    <mn>4</mn></mrow></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: with a given section of an input image detects whether there is an edge at that
    location of the image. There are similar matrices known to be able to detect whether
    corners exist, whether vertical or horizontal lines exist, and so on.^([2](ch05.html#idm45732618587560))
  prefs: []
  type: TYPE_NORMAL
- en: Now suppose that we used the *same set of weights* *W* to detect whether the
    visual pattern defined by *W* existed at each location in the input image. We
    could imagine “sliding *W* over the input image,” taking the dot product of *W*
    with the pixels at each location of the image, and ending up with a new image
    *O* of almost identical size to the original image (it may be slightly different,
    depending on how we handle the edges). This image *O* would be a kind of “feature
    map” showing the locations in the input image where the pattern defined by *W*
    was present. This operation is in fact what happens in convolutional neural networks;
    it is called a *convolution*, and its output is indeed called a *feature map*.
  prefs: []
  type: TYPE_NORMAL
- en: This operation is at the core of how CNNs work. Before we can incorporate it
    into a full-fledged `Operation`, of the kind we’ve seen in the prior chapters,
    we have to add another dimension to it—literally.
  prefs: []
  type: TYPE_NORMAL
- en: The Multichannel Convolution Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To review: convolutional neural networks differ from regular neural networks
    in that they create an order of magnitude more features, and in that each feature
    is a function of just a small patch from the input image. Now we can get more
    specific: starting with *n* input pixels, the convolution operation just described
    will create *n* output features, one for each location in the input image. What
    actually happens in a convolutional `Layer` in a neural network goes one step
    further: there, we’ll create *f* *sets* of *n* features, *each* with a corresponding
    (initially random) set of weights defining a visual pattern whose detection at
    each location in the input image will be captured in the feature map. These *f*
    feature maps will be created via *f* convolution operations. This is captured
    in [Figure 5-3](#fig_05_03).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. More specifically than before, for an input image with n pixels,
    we define an output with f feature maps, each of which has about the same size
    as the original image, for a total of n × f total output neurons for the image,
    each of which is a function of only a small patch of the original image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we’ve introduced a bunch of concepts, let’s define them for clarity.
    While each “set of features” detected by a particular set of weights is called
    a feature map, in the context of a convolutional `Layer`, the number of feature
    maps is referred to as the number of *channels* of the `Layer`—this is why the
    operation involved with the `Layer` is called the multichannel convolution. In
    addition, the *f* sets of weights *W*[*i*] are called the convolutional *filters*.^([3](ch05.html#idm45732618558136))
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understand the multichannel convolution operation, we can think
    about how to incorporate this operation into a neural network layer. Previously,
    our neural network layers were relatively straightforward: they received two-dimensional
    `ndarray`s as input and produced two-dimensional `ndarray`s as output. Based on
    the description in the prior section, however, convolutional layers will have
    a 3D `ndarray` as output *for a single image*, with dimensions *number of channels*
    (same as “feature maps”) × *image height* × *image width*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This raises a question: how can we feed this `ndarray` forward into another
    convolutional layer to create a “deep convolutional” neural network? We’ve seen
    how to perform the convolution operation on an image with a single channel and
    our filters; how can we perform the multichannel convolution on an *input* with
    *multiple* channels, as we’ll have to do when two convolutional layers are strung
    together? Understanding this is the key to understanding *deep* convolutional
    neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider what happens in a neural network with fully connected layers: in the
    first hidden layer, we have, let’s say, *h*[1] features that are combinations
    of all of the original features from the input layer. In the layer that follows,
    the features are combinations of all of the features from the prior layer, so
    that we might have *h*[2] “features of features” of the original features. To
    create this next layer of *h*[2] features, we use <math><mrow><msub><mi>h</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>h</mi> <mn>2</mn></msub></mrow></math>
    weights to represent that each of the *h*[2] features is a function of each of
    the *h*[1] features in the prior layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As described in the prior section, an analogous process happens in the first
    layer of a convolutional neural network: we first transform the input image into
    *m*[1] *feature maps*, using *m*[1] convolutional *filters*. We should think of
    the output of this layer as representing whether each of the *m*[1] different
    visual patterns represented by the weights of the *m*[1] filters is present at
    each location in the input image. Just as different layers of a fully connected
    neural network can contain different numbers of neurons, the next layer of the
    convolutional neural network could contain *m*[2] filters. In order for the network
    to learn complex patterns, the interpretation of each of these should be whether
    each of the *“patterns of patterns”* or higher-order visual features represented
    by *combinations of the* *m*[1] *visual patterns from the prior layer* was present
    at that location of the image. This implies that if the output of the convolutional
    layer is a 3D `ndarray` of shape *m*[2] channels × image height × image width,
    then a given location in the image on one of the *m*[2] feature maps is *a linear
    combination of convolving* *m*[1] *different filters over that same location in
    each of the corresponding* *m*[1] *feature maps from the prior layer*. This will
    allow each location in each of the *m*[2] filter maps to represent a *combination*
    of the *m*[1] visual features already learned in the prior convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Implications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This understanding of how two multichannel convolutional layers are connected
    tells us how to implement the operation: just as we need <math><mrow><msub><mi>h</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>h</mi> <mn>2</mn></msub></mrow></math>
    weights to connect a fully connected layer with *h*[1] neurons to one with <math><msub><mi>h</mi>
    <mn>2</mn></msub></math> , we need <math><mrow><msub><mi>m</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>m</mi> <mn>2</mn></msub></mrow></math> *convolutional filters*
    to connect a convolutional layer with *m*[1] channels to one with *m*[2]. With
    this last detail in place, we can now specify the dimensions of the `ndarray`s
    that will make up the input, output, and parameters of the full, multichannel
    convolution operation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input will have shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Input channels
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image height
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image width
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output will have shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Output channels
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image height
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image width
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The convolutional filters themselves will have shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input channels
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Output channels
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter height
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter width
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The order of the dimensions may vary from library to library, but these four
    dimensions will always be present.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll keep all of this in mind when we implement this convolution operation
    later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The Differences Between Convolutional and Fully Connected Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the beginning of the chapter, we discussed the differences between convolutional
    and fully connected layers at a high level; [Figure 5-4](#fig_05_04) revisits
    that comparison, now that we’ve described convolutional layers in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Comparison between convolutional and fully connected layers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In addition, one last difference between the two kinds of layers is the way
    in which the individual neurons themselves are interpreted:'
  prefs: []
  type: TYPE_NORMAL
- en: The interpretation of each neuron of a fully connected layer is that it detects
    whether or not *a particular combination of the features learned by the prior
    layer* is present in the current observation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interpretation of a neuron of a convolutional layer is that it detects whether
    or not *a particular combination of visual patterns* learned by the prior layer
    is present *at the given location* of the input image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There’s one more problem we need to solve before we can incorporate such a
    layer into a neural network: how to use the dimensional `ndarray`s we obtain as
    output to make predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Making Predictions with Convolutional Layers: The Flatten Layer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve covered how convolutional layers learn features that represent whether
    visual patterns exist in images and store those features in layers of feature
    maps; how do we use these layers of feature maps to make predictions? When using
    fully connected neural networks to predict which of 10 classes an image belonged
    to in the prior chapter, we just had to ensure that the last layer had dimension
    10; we could then feed these 10 numbers into the softmax cross entropy loss function
    to ensure they were interpreted as probabilities. Now we need to figure out what
    we can do in the case of our convolutional layer, where we have a three-dimensional
    `ndarray` per observation of shape *m* channels × image height × image width.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the answer, recall that each neuron simply represents whether a particular
    combination of visual features (which, if this is a deep convolutional neural
    network, could be a feature of features or a feature of features of features)
    is present at a given location in the image. This is no different from the features
    that would be learned if we applied a fully connected neural network to this image:
    the first fully connected layer would represent features of the individual pixels,
    the second would represent features of these features, and so on. And in a fully
    connected architecture, we would simply treat each “feature of features” that
    the network had learned as a single neuron that would be used as input to a prediction
    of which class the image belonged to.'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that we can do the same thing with convolutional neural networks—we
    treat the *m* feature maps as <math><mrow><mi>m</mi> <mo>×</mo> <mi>i</mi> <mi>m</mi>
    <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mo>×</mo> <mi>i</mi> <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow></msub></mrow></math>
    neurons and use a `Flatten` operation to squash these three dimensions (the number
    of channels, the image height, and the image width) down into a one-dimensional
    vector, after which we can use a simple matrix multiplication to make our final
    predictions. The intuition for why this works is that each individual neuron *fundamentally
    represents the same “kind of thing”* as the neurons in a fully connected layer—specifically,
    whether a given visual feature (or combination of features) is present at a given
    location in an image)—and thus we can treat them the same way in the final layer
    of the neural network.^([4](ch05.html#idm45732618457096))
  prefs: []
  type: TYPE_NORMAL
- en: We’ll see how to implement the `Flatten` layer later in the chapter. But before
    we dive into the implementation, let’s discuss another kind of layer that is important
    in many CNN architectures, though we won’t cover it in great detail in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Pooling* layers are another kind of layer commonly used in convolutional neural
    networks. They simply *downsample* each of the feature maps created by a convolution
    operation; for the most typically used pooling size of 2, this involves mapping
    each 2 × 2 section of each feature map either to the maximum value of that section,
    in the case of *max-pooling*, or to the average value of that section, in the
    case of *average-pooling*. For an *n* × *n* image, then, this would map the entire
    image to one of size <math><mfrac><mi>n</mi> <mn>2</mn></mfrac></math> × <math><mfrac><mi>n</mi>
    <mn>2</mn></mfrac></math> . [Figure 5-5](#fig_05_05) illustrates this.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural net diagram](assets/dlfs_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. An illustration of max- and average-pooling with a 4 × 4 input;
    each 2 × 2 patch is mapped to either the average or the max values for that patch
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The main advantage of pooling is computational: by downsampling the image to
    contain one-fourth as many pixels as the prior layer, pooling decreases both the
    number of weights and the number of computations needed to train the network by
    a factor of 4; this can be further compounded if multiple pooling layers are used
    in the network, as they were in many architectures in the early days of CNNs.
    The downside of pooling, of course, is that only one-fourth as much information
    can be extracted from the downsampled image. However, the fact that architectures
    showed very strong performance on benchmarks in image recognition despite the
    use of pooling suggested that, even though pooling was causing the networks to
    “lose information” about the images by decreasing the images’ resolution, the
    trade-offs in terms of increased computational speed were worth it. Nevertheless,
    pooling was considered by many to be a trick that just happened to work but should
    probably be done away with; as Geoffrey Hinton wrote on [a Reddit AMA](https://oreil.ly/2YZU0Kc)
    in 2014, “The pooling operation used in convolutional neural networks is a big
    mistake and the fact that it works so well is a disaster.” And indeed, most recent
    CNN architectures (such as Residual Networks, or “ResNets”^([5](ch05.html#idm45732618426472)))
    use pooling minimally or not at all. Thus, in this book, we’re not going to implement
    pooling layers, but given their importance for “putting CNNs on the map” via their
    use in famous architectures such as AlexNet, we mention them here for completeness.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying CNNs beyond images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Everything we have described so far is extremely standard for dealing with
    images using neural networks: the images are typically represented as a set of
    *m*[1] channels of pixels, where *m*[1] = 1 for black-and-white images, and *m*[1]
    = 3 for color images—and then some number *m*[2] of convolution operations are
    applied to each channel (using the <math><mrow><msub><mi>m</mi> <mn>1</mn></msub>
    <mo>×</mo> <msub><mi>m</mi> <mn>2</mn></msub></mrow></math> filter maps as explained
    previously), with this pattern continuing on for several layers. This has all
    been covered in other treatments of convolutional neural networks; what is less
    commonly covered is that the idea of organizing data into “channels” and then
    processing that data using a CNN goes beyond just images. For example, this data
    representation was a key to DeepMind’s series of AlphaGo programs showing that
    neural networks could learn to play Go. To quote the paper:^([6](ch05.html#idm45732618415512))'
  prefs: []
  type: TYPE_NORMAL
- en: The input to the neural network is a 19 × 19 × 17 image stack comprising 17
    binary feature planes. 8 feature planes *X*[t] consist of binary values indicating
    the presence of the current player’s stones ( <math><mrow><msub><mi>X</mi> <msub><mi>t</mi>
    <mi>i</mi></msub></msub> <mo>=</mo> <mn>1</mn></mrow></math> if intersection *i*
    contains a stone of the player’s color at time-step t; 0 if the intersection is
    empty, contains an opponent stone, or if t < 0). A further 8 feature planes, *Y*[t],
    represent the corresponding features for the opponent’s stones. The final feature
    plane, C, represents the color to play, and has a constant value of either 1 if
    black is to play or 0 if white is to play. These planes are concatenated together
    to give input features *s*[t] = *X*[t], *Y*[t], *X*[t – 1], *Y*[t – 1], …, *X*[t
    – 7], *Y*[t – 7], *C*. History features *X*[t], *Y*[t] are necessary because Go
    is not fully observable solely from the current stones, as repetitions are forbidden;
    similarly, the color feature C is necessary because the *komi* is not observable.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, they essentially represented the board as a 19 × 19 pixel “image”
    with 17 channels! They used 16 of these channels to encode what had happened on
    the 8 prior moves that each player had taken; this was necessary so that they
    could encode rules that prevented repetition of earlier moves. The 17th channel
    was actually a 19 × 19 grid of either all 1s or all 0s, depending on whose turn
    it was to move.^([7](ch05.html#idm45732618397624)) CNNs and their multichannel
    convolution operations are mostly commonly applied to images, but the even more
    general idea of representing data that is arranged along some spatial dimension
    with multiple “channels” is applicable even beyond images.
  prefs: []
  type: TYPE_NORMAL
- en: In keeping with the theme of this book, however, to truly understand the multichannel
    convolution operation, you have to implement it from scratch; the next several
    sections will describe this process in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Multichannel Convolution Operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It turns out that implementing this daunting operation—which involves a four-dimensional
    input `ndarray` and a four-dimensional parameter `ndarray`—is much clearer if
    we first examine the *one*-dimensional case. Building up to the full operation
    from that starting point will turn out mostly to be a matter of adding a bunch
    of `for` loops. Throughout, we’ll take the same approach we took in [Chapter 1](ch01.html#foundations),
    alternating between diagrams, math, and working Python code.
  prefs: []
  type: TYPE_NORMAL
- en: The Forward Pass
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The convolution in one dimension is conceptually identical to the convolution
    in two dimensions: we take in a one-dimensional input and a one-dimensional convolutional
    filter as inputs and then create the output by sliding the filter along the input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s suppose our input is of length 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutions](assets/dlfs_05in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And let’s say the size of the “patterns” we want to detect is length 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filter length three](assets/dlfs_05in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagrams and math
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first element of the output would be created by convolving the first element
    of the input with the filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output element 1](assets/dlfs_05in03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second element of the output would be created by sliding the filter one
    unit to the right and convolving it with the next set values of the series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of length one](assets/dlfs_05in04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fair enough. However, when we compute the next output value, we realize that
    we have run out of room:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of length one](assets/dlfs_05in05.png)'
  prefs: []
  type: TYPE_IMG
- en: We have hit the end of our input, and the resulting output has just three elements,
    when we started with five! How can we address this?
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid the output shrinking as a result of the convolution operation, we’ll
    introduce a trick used throughout convolutional neural networks: we “pad” the
    input with zeros around the edges, enough so that the output remains the same
    size as the input. Otherwise, every time we convolve a filter over the input,
    we’ll end up with an output that is slightly smaller than the input, as seen previously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can reason from the preceding convolution example: for a filter of size
    3, there should be one unit of padding around the edges to keep the output the
    same size as the input. More generally, since we almost always use odd-numbered
    filter sizes, we add padding equal to the filter size divided by 2 and rounded
    down to the nearest integer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we add this padding, so that instead of the input ranging from *i*[1]
    to *i*[5], it ranges from *i*[0] to *i*[6], where both *i*[0] and *i*[6] are 0\.
    Then we can compute the output of the convolution as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>o</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>i</mi>
    <mn>0</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>2</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'And so on, up until:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>o</mi> <mn>5</mn></msub> <mo>=</mo> <msub><mi>i</mi>
    <mn>4</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>5</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>i</mi>
    <mn>6</mn></msub> <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: And now the output is the same size as the input. How might we code this up?
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coding up this part turns out to be pretty straightforward. Before we do, let’s
    summarize the steps we just discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: We ultimately want to produce an output that is the same size as the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To do this without “shrinking” the output, we’ll first need to pad the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we’ll have to write some sort of loop that goes through the input and convolves
    each position of it with the filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We’ll start with our input and our filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a helper function that can pad our one-dimensional input on each end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: What about the convolution itself? Observe that for each element in the output
    that we want to produce, we have a corresponding element in the *padded* input
    where we “start” the convolution operation; once we figure out where to start,
    we simply loop through all the elements in the filter, doing a multiplication
    at each element and adding the result to the total.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we find this “corresponding element”? Note that, simply, the value at
    the first element in the output gets its value starting at the first element of
    the padded input! This makes the `for` loop quite easy to write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s simple enough. Before we move on to the backward pass of this operation—the
    tricky part—let’s briefly discuss a hyperparameter of convolutions that we’re
    glossing over: stride.'
  prefs: []
  type: TYPE_NORMAL
- en: A note on stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We noted earlier that pooling operations were one way to downsample images
    from feature maps. In many early convolutional architectures, these did indeed
    significantly reduce the amount of computation needed without any significant
    hit to accuracy; nevertheless, they’ve fallen out of favor because of their downside:
    they effectively downsample the image so that an image with just half the resolution
    is passed forward into the next layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A much more widely accepted way to do this is to modify the *stride* of the
    convolution operation. The stride is the amount that the filter is incrementally
    slid over the image—in the previous case, we are using a stride of 1, and as a
    result each filter is convolved with every element of the input, which is why
    the output ends up being the same size as the input. With a stride of 2, the filter
    would be convolved with *every other* element of the input image, so that the
    output would be half the size of the input; with a stride of 3, the filter would
    be convolved with *every third* element of the input image, and so on. This means
    that, for example, using a stride of 2 would result in the same output size and
    thus much the same reduction in computation we would get from pooling with size
    2, but without as much *loss of information*: with pooling of size 2, only one-fourth
    of the elements in the input have *any* effect on the output, whereas with a stride
    of 2, *every* element of the input has *some* effect on the output. The use of
    a stride of greater than 1 is thus significantly more prevalent than pooling for
    downsampling even in the most advanced CNN architectures of today.'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, in this book I’ll just show examples with a stride of 1—modifying
    these operations to allow a stride of greater than 1 is left as an exercise for
    the reader. Using a stride equal to 1 also makes writing the backward pass easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutions: The Backward Pass'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The backward pass is where convolutions get a bit trickier. Let’s recall what
    we’re trying to do: before, we produced the output of a convolution operation
    using the input and the parameters. We now want to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: The partial derivative of the loss with respect to each element of the *input*
    to the convolution operation—`inp` previously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partial derivative of the loss with respect to each element of the *filter*—`param_1d`
    previously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of how the `ParamOperation`s we saw in [Chapter 4](ch04.html#extensions)
    work: in the `backward` method, they receive an output gradient representing how
    much each element of the output ultimately affects the loss and then use this
    output gradient to compute the gradients for the input and the parameters. So
    we need to write a function that takes in an `output_grad` with the same shape
    as the input and produces an `input_grad` and a `param_grad`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we test whether the computed gradients are correct? We’ll bring back
    an idea from the first chapter: we know that the partial derivative of a sum with
    respect to any one of its inputs is 1 (if the sum *s* = *a* + *b* + *c*, then
    <math><mrow><mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>a</mi></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>b</mi></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>s</mi></mrow> <mrow><mi>∂</mi><mi>c</mi></mrow></mfrac>
    <mo>=</mo> <mn>1</mn></mrow></math> ). So we can compute the `input_grad` and
    `param_grad` quantities using our `_input_grad` and `_param_grad` functions (which
    we’ll reason through and write shortly) and an `output_grad` equal to all 1s.
    Then we’ll check whether these gradients are correct by changing elements of the
    input by some quantity *α* and seeing whether the resulting sum changes by the
    gradient times *α*.'
  prefs: []
  type: TYPE_NORMAL
- en: What “should” the gradient be?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the logic just described, let’s calculate what an element of the gradient
    vector for the input *should* be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: So, the gradient of the fifth element of the input *should* be 41 – 39 = 2.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try to reason through how we should compute such a gradient without
    simply computing the difference between these two sums. Here is where things get
    interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the gradient of a 1D convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We see that increasing this element of the input increased the output by 2\.
    Taking a close look at the output shows exactly how it does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Full output](assets/dlfs_05in06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This particular element of the input is denoted *t*[5]. It appears in the output
    in two places:'
  prefs: []
  type: TYPE_NORMAL
- en: As part of *o*[4], it is multiplied by *w*[3].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As part of *o*[5], it is multiplied by *w*[2].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help see the general pattern of how inputs map to the sum of outputs, note
    that if there was an *o*[6] present, *t*[5] would also contribute to the output
    through being multiplied by *w*[1].
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the amount that <math><msub><mi>t</mi> <mn>5</mn></msub></math>
    ultimately affects the loss, which we can denote as <math><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>t</mi> <mn>5</mn></msub></mrow></mfrac></math> , will
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>o</mi> <mn>4</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi>
    <mn>3</mn></msub> <mo>+</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>o</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi> <mn>2</mn></msub>
    <mo>+</mo> <mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>o</mi>
    <mn>6</mn></msub></mrow></mfrac> <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in this simple example, when the loss is just the sum, <math><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>o</mi> <mi>i</mi></msub></mrow></mfrac> <mo>=</mo> <mn>1</mn></mrow></math>
    for all elements, in the output (except for the “padding” elements for which this
    quantity is 0). This sum is very easy to compute: it is simply *w*[2] + *w*[3],
    which is indeed 2 since *w*[2] = *w*[3] = 1.'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the general pattern?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s look for the general pattern for a generic input element. This turns
    out to be an exercise in keeping track of indices. Since we’re translating math
    into code here, let’s use <math><msubsup><mi>o</mi> <mi>i</mi> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></math>
    to denote the *i*th element of the output gradient (since we’ll ultimately be
    accessing it via `output_grad[i]`). Then:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>5</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>5</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>6</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking closely at this output, we can reason similarly that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>3</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>2</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>3</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msub><mi>t</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo>=</mo> <msubsup><mi>o</mi> <mn>3</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>4</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>×</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>o</mi> <mn>5</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>×</mo> <msub><mi>w</mi> <mn>1</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s clearly a pattern here, and translating it into code is a bit tricky,
    especially since the indices on the output increase at the same time the indices
    on the weights decrease. Nevertheless, the way to express this turns out to be
    via the following double `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This does the appropriate incrementing of the indices of the weights, while
    decreasing the weights on the output at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Though it may not be obvious now, reasoning through this and getting it is out
    to be the trickiest part of calculating the gradients for convolution operations.
    Adding more complexity to this, such as batch sizes, convolutions with two-dimensional
    inputs, or inputs with multiple channels, is simply a matter of adding more `for`
    loops to the preceding lines, as we’ll see in the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the parameter gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can reason similarly about how increasing an element of the filter should
    increase the output. First, let’s increase (arbitrarily) the first element of
    the filter by one unit and observe the resulting impact on the sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: So we should find that <math><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msub><mi>w</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mn>10</mn></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we did for the input, by closely examining the output and seeing which
    elements of the filter affect it, as well as padding the input to more clearly
    see the pattern, we see that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msubsup><mi>w</mi> <mn>1</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>=</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>1</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>+</mo> <msub><mi>t</mi>
    <mn>1</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>2</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>+</mo> <msub><mi>t</mi> <mn>2</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>3</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup> <mo>+</mo> <msub><mi>t</mi>
    <mn>3</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>4</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>+</mo> <msub><mi>t</mi> <mn>4</mn></msub> <mo>×</mo> <msubsup><mi>o</mi> <mn>5</mn>
    <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'And since, for the sum, all of the <math><msubsup><mi>o</mi> <mi>i</mi> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup></math>
    elements are just 1, and *t*[0] is 0, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msubsup><mi>w</mi> <mn>1</mn> <mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi></mrow></msubsup>
    <mo>=</mo> <msub><mi>t</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>t</mi> <mn>2</mn></msub>
    <mo>+</mo> <msub><mi>t</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>t</mi> <mn>4</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>+</mo> <mn>2</mn> <mo>+</mo> <mn>3</mn> <mo>+</mo> <mn>4</mn>
    <mo>=</mo> <mn>10</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This confirms the calculation from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Coding this up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coding this up turns out to be easier than writing the code for the input gradient,
    since this time “the indices are moving in the same direction.” Within the same
    nested `for` loop, the code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can combine these two computations and write a function to compute
    both the input gradient and the filter gradient with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Take the input and filter as arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pad the input and the output gradient (to get, say, `input_pad` and `output_pad`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As shown earlier, use the padded output gradient and the filter to compute the
    gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, use the output gradient (not padded) and the padded input to compute
    the filter gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I show the full function that wraps around the preceding code blocks in the
    book’s [GitHub repo](https://oreil.ly/2H99xkJ).
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our explanation of how to implement convolutions in 1D! As we’ll
    see in the next several sections, extending this reasoning to work on two-dimensional
    inputs, batches of two-dimensional inputs, or even multichannel batches of two-dimensional
    inputs is (perhaps surprisingly) straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Batches, 2D Convolutions, and Multiple Channels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s add the capability for these convolution functions to work with
    *batches* of inputs—2D inputs whose first dimension represents the batch size
    of the input and whose second dimension represents the length of the 1D sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can follow the same general steps defined before: we’ll first pad the input,
    use this to compute the output, and then pad the output gradient to compute both
    the input and filter gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: '1D convolutions with batches: forward pass'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The only difference in implementing the forward pass when the input has a second
    dimension representing the batch size is that we have to pad and compute the output
    for each observation individually (as we did previously) and then `stack` the
    results to get a batch of outputs. For example, `conv_1d` becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '1D convolutions with batches: backward pass'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The backward pass is similar: computing the input gradient now simply takes
    the `for` loop for computing the input gradient from the prior section, computes
    it for each observation, and `stack`s the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The gradient for the filter when dealing with a batch of observations is a
    bit different. This is because the filter is convolved with every observation
    in the input and is thus connected to every observation in the output. So, to
    compute the parameter gradient, we have to loop through all of the observations
    and increment the appropriate values of the parameter gradient as we do so. Still,
    this just involves adding an outer `for` loop to the code to compute the parameter
    gradient that we saw earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Adding this dimension on top of the original 1D convolution was indeed simple;
    extending this from one- to two-dimensional inputs is similarly straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 2D Convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The 2D convolution is a straightforward extension of the 1D case because, fundamentally,
    the way the input is connected to the output via the filters in each dimension
    of the 2D case is identical to the 1D case. As a result, the high-level steps
    on both the forward and backward passes remain the same:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the forward pass, we:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appropriately pad the input.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the padded input and the parameters to compute the output.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the backward pass, to compute the input gradient we:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appropriately pad the output gradient.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use this padded output gradient, along with the input and the parameters, to
    compute both the input gradient and the parameter gradient.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also on the backward pass, to compute the parameter gradient we:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Appropriately pad the input.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Loop through the elements of the padded input and increment the parameter gradient
    appropriately as we go along.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '2D convolutions: coding the forward pass'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To make this concrete, recall that for 1D convolutions the code for computing
    the output given the input and the parameters on the forward pass looked as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For 2D convolutions, we simply modify this to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we’ve simply “blown each `for` loop out” into two `for` loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'The extension to two dimensions when we have a batch of images is also similar
    to the 1D case: just as we did there, we simply add a `for` loop to the outside
    of the loops shown here.'
  prefs: []
  type: TYPE_NORMAL
- en: '2D convolutions: coding the backward pass'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sure enough, just as in the forward pass, we can use the same indexing for
    the backward pass as in the 1D case. Recall that in the 1D case, the code was:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the 2D case, the code is simply:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the indexing on the output is the same as in the 1D case but is simply
    taking place in two dimensions; in the 1D case, we had:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'and in the 2D case, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The other facts from the 1D case also apply:'
  prefs: []
  type: TYPE_NORMAL
- en: For a batch of input images, we simply perform the preceding operation for each
    observation and then `stack` the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the parameter gradient, we have to loop through all the images in the batch
    and add components from each one to the appropriate places in the parameter gradient:^([8](ch05.html#idm45732616694312))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we’ve almost written the code for the complete multichannel convolution
    operation; currently, our code convolves filters over a two-dimensional input
    and produces a two-dimensional output. Of course, as we described earlier, each
    convolutional layer not only has neurons arranged along these two dimensions but
    also has some number of “channels” equal to the number of feature maps that the
    layer creates. Addressing this last challenge is what we’ll cover next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Last Element: Adding “Channels”'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How can we modify what we’ve written thus far to account for cases where both
    the input and the output are multichannel? The answer, as it was when we added
    batches earlier, is simple: we add two outer `for` loops to the code we’ve already
    seen—one loop for the input channels and another for the output channels. By looping
    through all combinations of the input channel and the output channel, we make
    each output feature map a combination of all of the input feature maps, as desired.'
  prefs: []
  type: TYPE_NORMAL
- en: For this to work, we will have to *always* represent our images as three-dimensional
    `ndarray`s, as opposed to the two-dimensional arrays we’ve been using; we’ll represent
    black-and-white images with one channel and color images with three channels (one
    for the red values at each location in the image, one for the blue values, and
    one for the green values). Then, regardless of the number of channels, the operation
    proceeds as described earlier, with a number of feature maps being created from
    the image, each of which is a combination of the convolutions resulting from all
    of the channels in the image (or from the channels in the prior layer, if dealing
    with layers further on in the network).
  prefs: []
  type: TYPE_NORMAL
- en: Forward pass
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given all this, the full code to compute the output for a convolutional layer,
    given four-dimensional `ndarray`s for the input and the parameters, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that `_pad_2d_channel` is a function that pads the input along the channel
    dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the actual code that does the computation is similar to the code in the
    simpler 2D case (without channels) shown before, except now we have, for example,
    `fil[c_out][c_in][p_w][p_h]` instead of just `fil[p_w][p_h]`, since there are
    two more dimensions and `c_out × c_in` more elements in the filter array.
  prefs: []
  type: TYPE_NORMAL
- en: Backward pass
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The backward pass is similar and follows the same conceptual principles as
    the backward pass in the simple 2D case:'
  prefs: []
  type: TYPE_NORMAL
- en: For the input gradients, we compute the gradients of each observation individually—padding
    the output gradient to do so—and then `stack` the gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also use the padded output gradient for the parameter gradient, but we loop
    through the observations as well and use the appropriate values from each one
    to update the parameter gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s the code for computing the output gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s the parameter gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: These three functions—`_output`, `_input_grad`, and `_param_grad`—are just what
    we need to create a `Conv2DOperation`, which will ultimately form the core of
    the `Conv2DLayer`s we’ll use in our CNNs! There are just a few more details to
    work out before we can use this `Operation` in a working convolutional neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Using This Operation to Train a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to implement a few more pieces before we can have a working CNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: We have to implement the `Flatten` operation discussed earlier in the chapter;
    this is necessary to enable the model to make predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have to incorporate this `Operation` as well as the `Conv2DOpOperation` into
    a `Conv2D` `Layer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, for any of this to be usable, we have to write a faster version of
    the `Conv2D` `Operation`. We’ll outline this here and share the details in [“Matrix
    Chain Rule”](app01.html#matrix-chain-rule).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Flatten Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There’s one other `Operation` we’ll need to complete our convolutional layer:
    the `Flatten` operation. The output of a convolution operation is a 3D `ndarray`
    for each observation, of dimension `(channels, img_height, img_width)`. However,
    unless we are passing this data into another convolutional layer, we’ll first
    need to transform it into a *vector* for each observation. Luckily, as described
    previously, since each of the individual neurons involved encodes whether a particular
    visual feature is present at that location in the image, we can simply “flatten”
    this 3D `ndarray` into a 1D vector and pass it forward without any problem. The
    `Flatten` operation shown here does this, accounting for the fact that in convolutional
    layers, as with any other layer, the first dimension of our `ndarray` is always
    the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: That’s the last `Operation` we’ll need; let’s wrap these `Operation`s up in
    a `Layer`.
  prefs: []
  type: TYPE_NORMAL
- en: The Full Conv2D Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The full convolutional layer, then, would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `Flatten` operation is optionally added on at the end, depending on whether
    we want the output of this layer to be passed forward into another convolutional
    layer or passed into another fully connected layer for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: A note on speed, and an alternative implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As those of you who are familiar with computational complexity will realize,
    this code is catastrophically slow: to calculate the parameter gradient, we needed
    to write *seven* nested `for` loops! There’s nothing wrong with doing this, since
    the purpose of writing the convolution operation from scratch was to solidify
    our understanding of how CNNs work. Still, it is possible to write convolutions
    in a completely different way; instead of breaking down that process like we have
    in this chapter, we can break it down into the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: From the input, extract `image_height × image_width × num_channels` patches
    of size `filter_height × filter_width` from the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of these patches, perform a dot product of the patch with the appropriate
    filter connecting the input channels to the output channels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stack and reshape the results of all of these dot products to form the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With a bit of cleverness, we can express almost all of the operations described
    previously in terms of a batch matrix multiplication, implemented using NumPy’s
    `np.matmul` function. The details of how to do this are described in [Appendix A](app01.html#appendix)
    and are implemented on [the book’s website](https://oreil.ly/2H99xkJ), but suffice
    it to say that this allows us to write relatively small convolutional neural networks
    that can train in a reasonable amount of time. This lets us actually run experiments
    to see how well convolutional neural networks work!
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even using the convolution operation defined by reshaping and the `matmul`
    functions, it takes about 10 minutes to train this model for one epoch with just
    one convolutional layer, so we restrict ourselves to demonstrating a model with
    just one convolutional layer, with 32 channels (a number chosen somewhat arbitrarily):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that this model has just 32 × 5 × 5 = 800 parameters in the first layer,
    but these parameters are used to create 32 × 28 × 28 = 25,088 neurons, or “learned
    features.” By contrast, a fully connected layer with hidden size 32 would have
    784 × 32 = 25,088 *parameters*, and just 32 neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some simple trial and error—training this model for just a few hundred batches
    with different learning rates and observing the resulting validation losses—shows
    that a learning rate of 0.01 works better than a learning rate of 0.1 now that
    we have a convolutional layer as our first layer, rather than a fully connected
    layer. Training this network for one epoch with optimizer `SGDMomentum(lr = 0.01,
    momentum=0.9)` gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This shows that we can indeed train a convolutional neural network from scratch
    that ends up getting above 90% accuracy on MNIST with just one pass through the
    training set!^([9](ch05.html#idm45732615365528))
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned about convolutional neural networks. You started
    out learning at a high level what they are and about their similarities and differences
    from fully connected neural networks, and then you went all the way down to seeing
    how they work at the lowest level, implementing the core multichannel convolution
    operation from scratch in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Starting at a high level, convolutional layers create roughly an order of magnitude
    more neurons than the fully connected layers we’ve seen so far, with each neuron
    being a combination of just a few features from the prior layer, rather than each
    neuron being a combination of *all* of the features from the prior layer as they
    are in fully connected layers. A level below that, we saw that these neurons are
    in fact grouped into “feature maps,” each of which represents whether a particular
    visual feature—or a particular combination of visual features, in the case of
    deep convolutional neural networks—is present at a given location in an image.
    Collectively we refer to these feature maps as the convolutional `Layer`’s “channels.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite all these differences from the `Operation`s we saw involved with the
    `Dense` layer, the convolution operation fits into the same template as other
    `ParamOperation`s we’ve seen:'
  prefs: []
  type: TYPE_NORMAL
- en: It has an `_output` method that computes the output given its input and the
    parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has `_input_grad` and `_param_grad` methods that, given an `output_grad`
    of the same shape as the `Operation`’s `output`, compute gradients of the same
    shape as the input and parameters, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference is just that the `_input`, `output`, and `param`s are now four-dimensional
    `ndarray`s, whereas they were two-dimensional in the case of fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This knowledge should serve as an extremely solid foundation for any future
    learning about or application of the convolutional neural networks you undertake.
    Next, we’ll cover another common kind of advanced neural network architecture:
    recurrent neural networks, designed for dealing with data that appears in sequences,
    rather than simply the nonsequential batches we’ve dealt with in the cases of
    houses and images. Onward!'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#idm45732618773272-marker)) The code we’ll write, while clearly
    expressing how convolutions work, will be extremely inefficient. In [“Gradient
    of the Loss with Respect to the Bias Terms”](app01.html#gradient-loss-bias-terms),
    I provide a more efficient implementation of the batch, multichannel convolution
    operation we’ll describe in this chapter using NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.html#idm45732618587560-marker)) See the Wikipedia page for [“Kernel
    (image processing)”](https://oreil.ly/2KOwfzs) for more examples.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#idm45732618558136-marker)) These are also referred to as *kernels*.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.html#idm45732618457096-marker)) This is why it is important to understand
    the output of convolution operations both as creating a number of filter maps—say,
    <math><mi>m</mi></math> —as well as creating <math><mrow><mi>m</mi> <mo>×</mo>
    <mi>i</mi> <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mo>×</mo> <mi>i</mi> <mi>m</mi> <mi>a</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow></msub></mrow></math>
    individual neurons. As is the case throughout neural networks, holding multiple
    levels of interpretation in one’s mind all at one time, and seeing the connection
    between them, is key.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.html#idm45732618426472-marker)) See the [original ResNet paper](http://tiny.cc/dlfs_resnet_paper),
    “Deep Residual Learning for Image Recognition,” by Kaiming He et al.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.html#idm45732618415512-marker)) DeepMind (David Silver et al.), [*Mastering
    the Game of Go Without Human Knowledge*](https://oreil.ly/wUpMW), 2017.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.html#idm45732618397624-marker)) A year later, DeepMind published
    results using a similar representation with chess—only this time, to encode the
    more complex ruleset of chess, the input had 119 channels! See DeepMind (David
    Silver et al.), [“A General Reinforcement Learning Algorithm That Masters Chess,
    Shogi, and Go Through Self-Play”](https://oreil.ly/E6ydw).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.html#idm45732616694312-marker)) See the full implementations of these
    on [the book’s website](https://oreil.ly/2H99xkJ).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.html#idm45732615365528-marker)) The full code can be found in the
    section for this chapter on [the book’s GitHub repo](https://oreil.ly/2H99xkJ).
  prefs: []
  type: TYPE_NORMAL
