- en: Chapter 10\. Introduction to Artificial Neural Networks with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Birds inspired us to fly, burdock plants inspired Velcro, and nature has inspired
    countless more inventions. It seems only logical, then, to look at the brain’s
    architecture for inspiration on how to build an intelligent machine. This is the
    logic that sparked *artificial neural networks* (ANNs), machine learning models
    inspired by the networks of biological neurons found in our brains. However, although
    planes were inspired by birds, they don’t have to flap their wings to fly. Similarly,
    ANNs have gradually become quite different from their biological cousins. Some
    researchers even argue that we should drop the biological analogy altogether (e.g.,
    by saying “units” rather than “neurons”), lest we restrict our creativity to biologically
    plausible systems.⁠^([1](ch10.html#idm45720205300576))
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are at the very core of deep learning. They are versatile, powerful, and
    scalable, making them ideal to tackle large and highly complex machine learning
    tasks such as classifying billions of images (e.g., Google Images), powering speech
    recognition services (e.g., Apple’s Siri), recommending the best videos to watch
    to hundreds of millions of users every day (e.g., YouTube), or learning to beat
    the world champion at the game of Go (DeepMind’s AlphaGo).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of this chapter introduces artificial neural networks, starting
    with a quick tour of the very first ANN architectures and leading up to multilayer
    perceptrons, which are heavily used today (other architectures will be explored
    in the next chapters). In the second part, we will look at how to implement neural
    networks using TensorFlow’s Keras API. This is a beautifully designed and simple
    high-level API for building, training, evaluating, and running neural networks.
    But don’t be fooled by its simplicity: it is expressive and flexible enough to
    let you build a wide variety of neural network architectures. In fact, it will
    probably be sufficient for most of your use cases. And should you ever need extra
    flexibility, you can always write custom Keras components using its lower-level
    API, or even use TensorFlow directly, as you will see in [Chapter 12](ch12.html#tensorflow_chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s go back in time to see how artificial neural networks came
    to be!
  prefs: []
  type: TYPE_NORMAL
- en: From Biological to Artificial Neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Surprisingly, ANNs have been around for quite a while: they were first introduced
    back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter
    Pitts. In their [landmark paper](https://homl.info/43)⁠^([2](ch10.html#idm45720205291952))
    “A Logical Calculus of Ideas Immanent in Nervous Activity”, McCulloch and Pitts
    presented a simplified computational model of how biological neurons might work
    together in animal brains to perform complex computations using *propositional
    logic*. This was the first artificial neural network architecture. Since then
    many other architectures have been invented, as you will see.'
  prefs: []
  type: TYPE_NORMAL
- en: The early successes of ANNs led to the widespread belief that we would soon
    be conversing with truly intelligent machines. When it became clear in the 1960s
    that this promise would go unfulfilled (at least for quite a while), funding flew
    elsewhere, and ANNs entered a long winter. In the early 1980s, new architectures
    were invented and better training techniques were developed, sparking a revival
    of interest in *connectionism*, the study of neural networks. But progress was
    slow, and by the 1990s other powerful machine learning techniques had been invented,
    such as support vector machines (see [Chapter 5](ch05.html#svm_chapter)). These
    techniques seemed to offer better results and stronger theoretical foundations
    than ANNs, so once again the study of neural networks was put on hold.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now witnessing yet another wave of interest in ANNs. Will this wave
    die out like the previous ones did? Well, here are a few good reasons to believe
    that this time is different and that the renewed interest in ANNs will have a
    much more profound impact on our lives:'
  prefs: []
  type: TYPE_NORMAL
- en: There is now a huge quantity of data available to train neural networks, and
    ANNs frequently outperform other ML techniques on very large and complex problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tremendous increase in computing power since the 1990s now makes it possible
    to train large neural networks in a reasonable amount of time. This is in part
    due to Moore’s law (the number of components in integrated circuits has doubled
    about every 2 years over the last 50 years), but also thanks to the gaming industry,
    which has stimulated the production of powerful GPU cards by the millions. Moreover,
    cloud platforms have made this power accessible to everyone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training algorithms have been improved. To be fair they are only slightly
    different from the ones used in the 1990s, but these relatively small tweaks have
    had a huge positive impact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some theoretical limitations of ANNs have turned out to be benign in practice.
    For example, many people thought that ANN training algorithms were doomed because
    they were likely to get stuck in local optima, but it turns out that this is not
    a big problem in practice, especially for larger neural networks: the local optima
    often perform almost as well as the global optimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ANNs seem to have entered a virtuous circle of funding and progress. Amazing
    products based on ANNs regularly make the headline news, which pulls more and
    more attention and funding toward them, resulting in more and more progress and
    even more amazing products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biological Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we discuss artificial neurons, let’s take a quick look at a biological
    neuron (represented in [Figure 10-1](#biological_neuron_wikipedia)). It is an
    unusual-looking cell mostly found in animal brains. It’s composed of a *cell body*
    containing the nucleus and most of the cell’s complex components, many branching
    extensions called *dendrites*, plus one very long extension called the *axon*.
    The axon’s length may be just a few times longer than the cell body, or up to
    tens of thousands of times longer. Near its extremity the axon splits off into
    many branches called *telodendria*, and at the tip of these branches are minuscule
    structures called *synaptic terminals* (or simply *synapses*), which are connected
    to the dendrites or cell bodies of other neurons.⁠^([3](ch10.html#idm45720205271104))
    Biological neurons produce short electrical impulses called *action potentials*
    (APs, or just *signals*), which travel along the axons and make the synapses release
    chemical signals called *neurotransmitters*. When a neuron receives a sufficient
    amount of these neurotransmitters within a few milliseconds, it fires its own
    electrical impulses (actually, it depends on the neurotransmitters, as some of
    them inhibit the neuron from firing).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1001](assets/mls3_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. A biological neuron⁠^([4](ch10.html#idm45720205264656))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thus, individual biological neurons seem to behave in a simple way, but they’re
    organized in a vast network of billions, with each neuron typically connected
    to thousands of other neurons. Highly complex computations can be performed by
    a network of fairly simple neurons, much like a complex anthill can emerge from
    the combined efforts of simple ants. The architecture of biological neural networks
    (BNNs)⁠^([5](ch10.html#idm45720205261008)) is the subject of active research,
    but some parts of the brain have been mapped. These efforts show that neurons
    are often organized in consecutive layers, especially in the cerebral cortex (the
    outer layer of the brain), as shown in [Figure 10-2](#biological_neural_network_wikipedia).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1002](assets/mls3_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Multiple layers in a biological neural network (human cortex)⁠^([6](ch10.html#idm45720205254384))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Logical Computations with Neurons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'McCulloch and Pitts proposed a very simple model of the biological neuron,
    which later became known as an *artificial neuron*: it has one or more binary
    (on/off) inputs and one binary output. The artificial neuron activates its output
    when more than a certain number of its inputs are active. In their paper, McCulloch
    and Pitts showed that even with such a simplified model it is possible to build
    a network of artificial neurons that can compute any logical proposition you want.
    To see how such a network works, let’s build a few ANNs that perform various logical
    computations (see [Figure 10-3](#nn_propositional_logic_diagram)), assuming that
    a neuron is activated when at least two of its input connections are active.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1003](assets/mls3_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. ANNs performing simple logical computations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s see what these networks do:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first network on the left is the identity function: if neuron A is activated,
    then neuron C gets activated as well (since it receives two input signals from
    neuron A); but if neuron A is off, then neuron C is off as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second network performs a logical AND: neuron C is activated only when
    both neurons A and B are activated (a single input signal is not enough to activate
    neuron C).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third network performs a logical OR: neuron C gets activated if either
    neuron A or neuron B is activated (or both).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, if we suppose that an input connection can inhibit the neuron’s activity
    (which is the case with biological neurons), then the fourth network computes
    a slightly more complex logical proposition: neuron C is activated only if neuron
    A is active and neuron B is off. If neuron A is active all the time, then you
    get a logical NOT: neuron C is active when neuron B is off, and vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can imagine how these networks can be combined to compute complex logical
    expressions (see the exercises at the end of the chapter for an example).
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *perceptron* is one of the simplest ANN architectures, invented in 1957
    by Frank Rosenblatt. It is based on a slightly different artificial neuron (see
    [Figure 10-4](#artificial_neuron_diagram)) called a *threshold logic unit* (TLU),
    or sometimes a *linear threshold unit* (LTU). The inputs and output are numbers
    (instead of binary on/off values), and each input connection is associated with
    a weight. The TLU first computes a linear function of its inputs: *z* = *w*[1]
    *x*[1] + *w*[2] *x*[2] + ⋯ + *w*[*n*] *x*[*n*] + *b* = **w**^⊺ **x** + *b*. Then
    it applies a *step function* to the result: *h*[**w**](**x**) = step(*z*). So
    it’s almost like logistic regression, except it uses a step function instead of
    the logistic function ([Chapter 4](ch04.html#linear_models_chapter)). Just like
    in logistic regression, the model parameters are the input weights **w** and the
    bias term *b*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1004](assets/mls3_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-4\. TLU: an artificial neuron that computes a weighted sum of its
    inputs **w**^⊺ **x**, plus a bias term *b*, then applies a step function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The most common step function used in perceptrons is the *Heaviside step function*
    (see [Equation 10-1](#step_functions_equation)). Sometimes the sign function is
    used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 10-1\. Common step functions used in perceptrons (assuming threshold
    = 0)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo
    form="prefix">heaviside</mo> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mn>1</mn></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo form="prefix">sgn</mo> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <mi>z</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>+</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: A single TLU can be used for simple linear binary classification. It computes
    a linear function of its inputs, and if the result exceeds a threshold, it outputs
    the positive class. Otherwise, it outputs the negative class. This may remind
    you of logistic regression ([Chapter 4](ch04.html#linear_models_chapter)) or linear
    SVM classification ([Chapter 5](ch05.html#svm_chapter)). You could, for example,
    use a single TLU to classify iris flowers based on petal length and width. Training
    such a TLU would require finding the right values for *w*[1], *w*[2], and *b*
    (the training algorithm is discussed shortly).
  prefs: []
  type: TYPE_NORMAL
- en: A perceptron is composed of one or more TLUs organized in a single layer, where
    every TLU is connected to every input. Such a layer is called a *fully connected
    layer*, or a *dense layer*. The inputs constitute the *input layer*. And since
    the layer of TLUs produces the final outputs, it is called the *output layer*.
    For example, a perceptron with two inputs and three outputs is represented in
    [Figure 10-5](#perceptron_diagram).
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1005](assets/mls3_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Architecture of a perceptron with two inputs and three output
    neurons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This perceptron can classify instances simultaneously into three different binary
    classes, which makes it a multilabel classifier. It may also be used for multiclass
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the magic of linear algebra, [Equation 10-2](#neural_network_layer_equation)
    can be used to efficiently compute the outputs of a layer of artificial neurons
    for several instances at once.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 10-2\. Computing the outputs of a fully connected layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>h</mi> <mrow><mi mathvariant="bold">W</mi><mo>,</mo><mi
    mathvariant="bold">b</mi></mrow></msub> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mi mathvariant="normal">ϕ</mi> <mrow><mo>(</mo>
    <mi mathvariant="bold">X</mi> <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: As always, **X** represents the matrix of input features. It has one row per
    instance and one column per feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weight matrix **W** contains all the connection weights. It has one row
    per input and one column per neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bias vector **b** contains all the bias terms: one per neuron.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function ϕ is called the *activation function*: when the artificial neurons
    are TLUs, it is a step function (we will discuss other activation functions shortly).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In mathematics, the sum of a matrix and a vector is undefined. However, in
    data science, we allow “broadcasting”: adding a vector to a matrix means adding
    it to every row in the matrix. So, **XW** + **b** first multiplies **X** by **W**—which
    results in a matrix with one row per instance and one column per output—then adds
    the vector **b** to every row of that matrix, which adds each bias term to the
    corresponding output, for every instance. Moreover, ϕ is then applied itemwise
    to each item in the resulting matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how is a perceptron trained? The perceptron training algorithm proposed
    by Rosenblatt was largely inspired by *Hebb’s rule*. In his 1949 book *The Organization
    of Behavior* (Wiley), Donald Hebb suggested that when a biological neuron triggers
    another neuron often, the connection between these two neurons grows stronger.
    Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire
    together, wire together”; that is, the connection weight between two neurons tends
    to increase when they fire simultaneously. This rule later became known as Hebb’s
    rule (or *Hebbian learning*). Perceptrons are trained using a variant of this
    rule that takes into account the error made by the network when it makes a prediction;
    the perceptron learning rule reinforces connections that help reduce the error.
    More specifically, the perceptron is fed one training instance at a time, and
    for each instance it makes its predictions. For every output neuron that produced
    a wrong prediction, it reinforces the connection weights from the inputs that
    would have contributed to the correct prediction. The rule is shown in [Equation
    10-3](#perceptron_update_rule).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 10-3\. Perceptron learning rule (weight update)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mrow><msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow>
    <mrow><mo>(</mo><mtext>next</mtext><mtext>step</mtext><mo>)</mo></mrow></msup>
    <mo>=</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>+</mo> <mi>η</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>j</mi></msub> <mo>-</mo>
    <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi></msub> <mo>)</mo></mrow>
    <msub><mi>x</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w*[*i*,] [*j*] is the connection weight between the *i*^(th) input and the
    *j*^(th) neuron.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[*i*] is the *i*^(th) input value of the current training instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math><msub><mover><mi>y</mi><mo>^</mo></mover><mi>j</mi></msub></math> is the
    output of the *j*^(th) output neuron for the current training instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[*j*] is the target output of the *j*^(th) output neuron for the current
    training instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*η* is the learning rate (see [Chapter 4](ch04.html#linear_models_chapter)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision boundary of each output neuron is linear, so perceptrons are incapable
    of learning complex patterns (just like logistic regression classifiers). However,
    if the training instances are linearly separable, Rosenblatt demonstrated that
    this algorithm would converge to a solution.⁠^([7](ch10.html#idm45720205089040))
    This is called the *perceptron convergence theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides a `Perceptron` class that can be used pretty much as
    you would expect—for example, on the iris dataset (introduced in [Chapter 4](ch04.html#linear_models_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have noticed that the perceptron learning algorithm strongly resembles
    stochastic gradient descent (introduced in [Chapter 4](ch04.html#linear_models_chapter)).
    In fact, Scikit-Learn’s `Perceptron` class is equivalent to using an `SGDClassifier`
    with the following hyperparameters: `loss="perceptron"`, `learning_rate="constant"`,
    `eta0=1` (the learning rate), and `penalty=None` (no regularization).'
  prefs: []
  type: TYPE_NORMAL
- en: In their 1969 monograph *Perceptrons*, Marvin Minsky and Seymour Papert highlighted
    a number of serious weaknesses of perceptrons—in particular, the fact that they
    are incapable of solving some trivial problems (e.g., the *exclusive OR* (XOR)
    classification problem; see the left side of [Figure 10-6](#xor_diagram)). This
    is true of any other linear classification model (such as logistic regression
    classifiers), but researchers had expected much more from perceptrons, and some
    were so disappointed that they dropped neural networks altogether in favor of
    higher-level problems such as logic, problem solving, and search. The lack of
    practical applications also didn’t help.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that some of the limitations of perceptrons can be eliminated
    by stacking multiple perceptrons. The resulting ANN is called a *multilayer perceptron*
    (MLP). An MLP can solve the XOR problem, as you can verify by computing the output
    of the MLP represented on the right side of [Figure 10-6](#xor_diagram): with
    inputs (0, 0) or (1, 1), the network outputs 0, and with inputs (0, 1) or (1,
    0) it outputs 1\. Try verifying that this network indeed solves the XOR problem!^([8](ch10.html#idm45720204942848))'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1006](assets/mls3_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. XOR classification problem and an MLP that solves it
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Contrary to logistic regression classifiers, perceptrons do not output a class
    probability. This is one reason to prefer logistic regression over perceptrons.
    Moreover, perceptrons do not use any regularization by default, and training stops
    as soon as there are no more prediction errors on the training set, so the model
    typically does not generalize as well as logistic regression or a linear SVM classifier.
    However, perceptrons may train a bit faster.
  prefs: []
  type: TYPE_NORMAL
- en: The Multilayer Perceptron and Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An MLP is composed of one input layer, one or more layers of TLUs called *hidden
    layers*, and one final layer of TLUs called the *output layer* (see [Figure 10-7](#mlp_diagram)).
    The layers close to the input layer are usually called the *lower layers*, and
    the ones close to the outputs are usually called the *upper layers*.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1007](assets/mls3_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Architecture of a multilayer perceptron with two inputs, one hidden
    layer of four neurons, and three output neurons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The signal flows only in one direction (from the inputs to the outputs), so
    this architecture is an example of a *feedforward neural network* (FNN).
  prefs: []
  type: TYPE_NORMAL
- en: When an ANN contains a deep stack of hidden layers,⁠^([9](ch10.html#idm45720204921696))
    it is called a *deep neural network* (DNN). The field of deep learning studies
    DNNs, and more generally it is interested in models containing deep stacks of
    computations. Even so, many people talk about deep learning whenever neural networks
    are involved (even shallow ones).
  prefs: []
  type: TYPE_NORMAL
- en: For many years researchers struggled to find a way to train MLPs, without success.
    In the early 1960s several researchers discussed the possibility of using gradient
    descent to train neural networks, but as we saw in [Chapter 4](ch04.html#linear_models_chapter),
    this requires computing the gradients of the model’s error with regard to the
    model parameters; it wasn’t clear at the time how to do this efficiently with
    such a complex model containing so many parameters, especially with the computers
    they had back then.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in 1970, a researcher named Seppo Linnainmaa introduced in his master’s
    thesis a technique to compute all the gradients automatically and efficiently.
    This algorithm is now called *reverse-mode automatic differentiation* (or *reverse-mode
    autodiff* for short). In just two passes through the network (one forward, one
    backward), it is able to compute the gradients of the neural network’s error with
    regard to every single model parameter. In other words, it can find out how each
    connection weight and each bias should be tweaked in order to reduce the neural
    network’s error. These gradients can then be used to perform a gradient descent
    step. If you repeat this process of computing the gradients automatically and
    taking a gradient descent step, the neural network’s error will gradually drop
    until it eventually reaches a minimum. This combination of reverse-mode autodiff
    and gradient descent is now called *backpropagation* (or *backprop* for short).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are various autodiff techniques, with different pros and cons. *Reverse-mode
    autodiff* is well suited when the function to differentiate has many variables
    (e.g., connection weights and biases) and few outputs (e.g., one loss). If you
    want to learn more about autodiff, check out [Appendix B](app02.html#autodiff_appendix).
  prefs: []
  type: TYPE_NORMAL
- en: 'Backpropagation can actually be applied to all sorts of computational graphs,
    not just neural networks: indeed, Linnainmaa’s master’s thesis was not about neural
    nets, it was more general. It was several more years before backprop started to
    be used to train neural networks, but it still wasn’t mainstream. Then, in 1985,
    David Rumelhart, Geoffrey Hinton, and Ronald Williams published a [groundbreaking
    paper](https://homl.info/44)⁠^([10](ch10.html#idm45720204912752)) analyzing how
    backpropagation allowed neural networks to learn useful internal representations.
    Their results were so impressive that backpropagation was quickly popularized
    in the field. Today, it is by far the most popular training technique for neural
    networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run through how backpropagation works again in a bit more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: It handles one mini-batch at a time (for example, containing 32 instances each),
    and it goes through the full training set multiple times. Each pass is called
    an *epoch*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each mini-batch enters the network through the input layer. The algorithm then
    computes the output of all the neurons in the first hidden layer, for every instance
    in the mini-batch. The result is passed on to the next layer, its output is computed
    and passed to the next layer, and so on until we get the output of the last layer,
    the output layer. This is the *forward pass*: it is exactly like making predictions,
    except all intermediate results are preserved since they are needed for the backward
    pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, the algorithm measures the network’s output error (i.e., it uses a loss
    function that compares the desired output and the actual output of the network,
    and returns some measure of the error).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then it computes how much each output bias and each connection to the output
    layer contributed to the error. This is done analytically by applying the *chain
    rule* (perhaps the most fundamental rule in calculus), which makes this step fast
    and precise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm then measures how much of these error contributions came from
    each connection in the layer below, again using the chain rule, working backward
    until it reaches the input layer. As explained earlier, this reverse pass efficiently
    measures the error gradient across all the connection weights and biases in the
    network by propagating the error gradient backward through the network (hence
    the name of the algorithm).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the algorithm performs a gradient descent step to tweak all the connection
    weights in the network, using the error gradients it just computed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is important to initialize all the hidden layers’ connection weights randomly,
    or else training will fail. For example, if you initialize all weights and biases
    to zero, then all neurons in a given layer will be perfectly identical, and thus
    backpropagation will affect them in exactly the same way, so they will remain
    identical. In other words, despite having hundreds of neurons per layer, your
    model will act as if it had only one neuron per layer: it won’t be too smart.
    If instead you randomly initialize the weights, you *break the symmetry* and allow
    backpropagation to train a diverse team of neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, backpropagation makes predictions for a mini-batch (forward pass),
    measures the error, then goes through each layer in reverse to measure the error
    contribution from each parameter (reverse pass), and finally tweaks the connection
    weights and biases to reduce the error (gradient descent step).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for backprop to work properly, Rumelhart and his colleagues made a
    key change to the MLP’s architecture: they replaced the step function with the
    logistic function, *σ*(*z*) = 1 / (1 + exp(–*z*)), also called the *sigmoid* function.
    This was essential because the step function contains only flat segments, so there
    is no gradient to work with (gradient descent cannot move on a flat surface),
    while the sigmoid function has a well-defined nonzero derivative everywhere, allowing
    gradient descent to make some progress at every step. In fact, the backpropagation
    algorithm works well with many other activation functions, not just the sigmoid
    function. Here are two other popular choices:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *hyperbolic tangent* function: tanh(*z*) = 2*σ*(2*z*) – 1'
  prefs: []
  type: TYPE_NORMAL
- en: Just like the sigmoid function, this activation function is *S*-shaped, continuous,
    and differentiable, but its output value ranges from –1 to 1 (instead of 0 to
    1 in the case of the sigmoid function). That range tends to make each layer’s
    output more or less centered around 0 at the beginning of training, which often
    helps speed up convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rectified linear unit function: ReLU(*z*) = max(0, *z*)'
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU function is continuous but unfortunately not differentiable at *z*
    = 0 (the slope changes abruptly, which can make gradient descent bounce around),
    and its derivative is 0 for *z* < 0\. In practice, however, it works very well
    and has the advantage of being fast to compute, so it has become the default.⁠^([11](ch10.html#idm45720204883120))
    Importantly, the fact that it does not have a maximum output value helps reduce
    some issues during gradient descent (we will come back to this in [Chapter 11](ch11.html#deep_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: 'These popular activation functions and their derivatives are represented in
    [Figure 10-8](#activation_functions_plot). But wait! Why do we need activation
    functions in the first place? Well, if you chain several linear transformations,
    all you get is a linear transformation. For example, if f(*x*) = 2*x* + 3 and
    g(*x*) = 5*x* – 1, then chaining these two linear functions gives you another
    linear function: f(g(*x*)) = 2(5*x* – 1) + 3 = 10*x* + 1\. So if you don’t have
    some nonlinearity between layers, then even a deep stack of layers is equivalent
    to a single layer, and you can’t solve very complex problems with that. Conversely,
    a large enough DNN with nonlinear activations can theoretically approximate any
    continuous function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1008](assets/mls3_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. Activation functions (left) and their derivatives (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: OK! You know where neural nets came from, what their architecture is, and how
    to compute their outputs. You’ve also learned about the backpropagation algorithm.
    But what exactly can you do with neural nets?
  prefs: []
  type: TYPE_NORMAL
- en: Regression MLPs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, MLPs can be used for regression tasks. If you want to predict a single
    value (e.g., the price of a house, given many of its features), then you just
    need a single output neuron: its output is the predicted value. For multivariate
    regression (i.e., to predict multiple values at once), you need one output neuron
    per output dimension. For example, to locate the center of an object in an image,
    you need to predict 2D coordinates, so you need two output neurons. If you also
    want to place a bounding box around the object, then you need two more numbers:
    the width and the height of the object. So, you end up with four output neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn includes an `MLPRegressor` class, so let’s use it to build an
    MLP with three hidden layers composed of 50 neurons each, and train it on the
    California housing dataset. For simplicity, we will use Scikit-Learn’s `fetch_california_housing()`
    function to load the data. This dataset is simpler than the one we used in [Chapter 2](ch02.html#project_chapter),
    since it contains only numerical features (there is no `ocean_proximity` feature),
    and there are no missing values. The following code starts by fetching and splitting
    the dataset, then it creates a pipeline to standardize the input features before
    sending them to the `MLPRegressor`. This is very important for neural networks
    because they are trained using gradient descent, and as we saw in [Chapter 4](ch04.html#linear_models_chapter),
    gradient descent does not converge very well when the features have very different
    scales. Finally, the code trains the model and evaluates its validation error.
    The model uses the ReLU activation function in the hidden layers, and it uses
    a variant of gradient descent called *Adam* (see [Chapter 11](ch11.html#deep_chapter))
    to minimize the mean squared error, with a little bit of ℓ[2] regularization (which
    you can control via the `alpha` hyperparameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We get a validation RMSE of about 0.505, which is comparable to what you would
    get with a random forest classifier. Not too bad for a first try!
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this MLP does not use any activation function for the output layer,
    so it’s free to output any value it wants. This is generally fine, but if you
    want to guarantee that the output will always be positive, then you should use
    the ReLU activation function in the output layer, or the *softplus* activation
    function, which is a smooth variant of ReLU: softplus(*z*) = log(1 + exp(*z*)).
    Softplus is close to 0 when *z* is negative, and close to *z* when *z* is positive.
    Finally, if you want to guarantee that the predictions will always fall within
    a given range of values, then you should use the sigmoid function or the hyperbolic
    tangent, and scale the targets to the appropriate range: 0 to 1 for sigmoid and
    –1 to 1 for tanh. Sadly, the `MLPRegressor` class does not support activation
    functions in the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Building and training a standard MLP with Scikit-Learn in just a few lines of
    code is very convenient, but the neural net features are limited. This is why
    we will switch to Keras in the second part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The `MLPRegressor` class uses the mean squared error, which is usually what
    you want for regression, but if you have a lot of outliers in the training set,
    you may prefer to use the mean absolute error instead. Alternatively, you may
    want to use the *Huber loss*, which is a combination of both. It is quadratic
    when the error is smaller than a threshold *δ* (typically 1) but linear when the
    error is larger than *δ*. The linear part makes it less sensitive to outliers
    than the mean squared error, and the quadratic part allows it to converge faster
    and be more precise than the mean absolute error. However, `MLPRegressor` only
    supports the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-1](#regression_mlp_architecture) summarizes the typical architecture
    of a regression MLP.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. Typical regression MLP architecture
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameter | Typical value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| # hidden layers | Depends on the problem, but typically 1 to 5 |'
  prefs: []
  type: TYPE_TB
- en: '| # neurons per hidden layer | Depends on the problem, but typically 10 to
    100 |'
  prefs: []
  type: TYPE_TB
- en: '| # output neurons | 1 per prediction dimension |'
  prefs: []
  type: TYPE_TB
- en: '| Hidden activation | ReLU |'
  prefs: []
  type: TYPE_TB
- en: '| Output activation | None, or ReLU/softplus (if positive outputs) or sigmoid/tanh
    (if bounded outputs) |'
  prefs: []
  type: TYPE_TB
- en: '| Loss function | MSE, or Huber if outliers |'
  prefs: []
  type: TYPE_TB
- en: Classification MLPs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MLPs can also be used for classification tasks. For a binary classification
    problem, you just need a single output neuron using the sigmoid activation function:
    the output will be a number between 0 and 1, which you can interpret as the estimated
    probability of the positive class. The estimated probability of the negative class
    is equal to one minus that number.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MLPs can also easily handle multilabel binary classification tasks (see [Chapter 3](ch03.html#classification_chapter)).
    For example, you could have an email classification system that predicts whether
    each incoming email is ham or spam, and simultaneously predicts whether it is
    an urgent or nonurgent email. In this case, you would need two output neurons,
    both using the sigmoid activation function: the first would output the probability
    that the email is spam, and the second would output the probability that it is
    urgent. More generally, you would dedicate one output neuron for each positive
    class. Note that the output probabilities do not necessarily add up to 1\. This
    lets the model output any combination of labels: you can have nonurgent ham, urgent
    ham, nonurgent spam, and perhaps even urgent spam (although that would probably
    be an error).'
  prefs: []
  type: TYPE_NORMAL
- en: If each instance can belong only to a single class, out of three or more possible
    classes (e.g., classes 0 through 9 for digit image classification), then you need
    to have one output neuron per class, and you should use the softmax activation
    function for the whole output layer (see [Figure 10-9](#fnn_for_classification_diagram)).
    The softmax function (introduced in [Chapter 4](ch04.html#linear_models_chapter))
    will ensure that all the estimated probabilities are between 0 and 1 and that
    they add up to 1, since the classes are exclusive. As you saw in [Chapter 3](ch03.html#classification_chapter),
    this is called multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the loss function, since we are predicting probability distributions,
    the cross-entropy loss (or *x-entropy* or log loss for short, see [Chapter 4](ch04.html#linear_models_chapter))
    is generally a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1009](assets/mls3_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. A modern MLP (including ReLU and softmax) for classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scikit-Learn has an `MLPClassifier` class in the `sklearn.neural_network` package.
    It is almost identical to the `MLPRegressor` class, except that it minimizes the
    cross entropy rather than the MSE. Give it a try now, for example on the iris
    dataset. It’s almost a linear task, so a single layer with 5 to 10 neurons should
    suffice (make sure to scale the features).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-2](#classification_mlp_architecture) summarizes the typical architecture
    of a classification MLP.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-2\. Typical classification MLP architecture
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameter | Binary classification | Multilabel binary classification
    | Multiclass classification |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| # hidden layers | Typically 1 to 5 layers, depending on the task |'
  prefs: []
  type: TYPE_TB
- en: '| # output neurons | 1 | 1 per binary label | 1 per class |'
  prefs: []
  type: TYPE_TB
- en: '| Output layer activation | Sigmoid | Sigmoid | Softmax |'
  prefs: []
  type: TYPE_TB
- en: '| Loss function | X-entropy | X-entropy | X-entropy |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before we go on, I recommend you go through exercise 1 at the end of this chapter.
    You will play with various neural network architectures and visualize their outputs
    using the *TensorFlow playground*. This will be very useful to better understand
    MLPs, including the effects of all the hyperparameters (number of layers and neurons,
    activation functions, and more).
  prefs: []
  type: TYPE_NORMAL
- en: Now you have all the concepts you need to start implementing MLPs with Keras!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing MLPs with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras is TensorFlow’s high-level deep learning API: it allows you to build,
    train, evaluate, and execute all sorts of neural networks. The original Keras
    library was developed by François Chollet as part of a research project⁠^([12](ch10.html#idm45720204597504))
    and was released as a standalone open source project in March 2015\. It quickly
    gained popularity, owing to its ease of use, flexibility, and beautiful design.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras used to support multiple backends, including TensorFlow, PlaidML, Theano,
    and Microsoft Cognitive Toolkit (CNTK) (the last two are sadly deprecated), but
    since version 2.4, Keras is TensorFlow-only. Similarly, TensorFlow used to include
    multiple high-level APIs, but Keras was officially chosen as its preferred high-level
    API when TensorFlow 2 came out. Installing TensorFlow will automatically install
    Keras as well, and Keras will not work without TensorFlow installed. In short,
    Keras and TensorFlow fell in love and got married. Other popular deep learning
    libraries include [PyTorch by Facebook](https://pytorch.org) and [JAX by Google](https://github.com/google/jax).^([13](ch10.html#idm45720204591712))
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s use Keras! We will start by building an MLP for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Colab runtimes come with recent versions of TensorFlow and Keras preinstalled.
    However, if you want to install them on your own machine, please see the installation
    instructions at [*https://homl.info/install*](https://homl.info/install).
  prefs: []
  type: TYPE_NORMAL
- en: Building an Image Classifier Using the Sequential API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to load a dataset. We will use Fashion MNIST, which is a drop-in
    replacement of MNIST (introduced in [Chapter 3](ch03.html#classification_chapter)).
    It has the exact same format as MNIST (70,000 grayscale images of 28 × 28 pixels
    each, with 10 classes), but the images represent fashion items rather than handwritten
    digits, so each class is more diverse, and the problem turns out to be significantly
    more challenging than MNIST. For example, a simple linear model reaches about
    92% accuracy on MNIST, but only about 83% on Fashion MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: Using Keras to load the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Keras provides some utility functions to fetch and load common datasets, including
    MNIST, Fashion MNIST, and a few more. Let’s load Fashion MNIST. It’s already shuffled
    and split into a training set (60,000 images) and a test set (10,000 images),
    but we’ll hold out the last 5,000 images from the training set for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: TensorFlow is usually imported as `tf`, and the Keras API is available via `tf.keras`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one
    important difference is that every image is represented as a 28 × 28 array rather
    than a 1D array of size 784\. Moreover, the pixel intensities are represented
    as integers (from 0 to 255) rather than floats (from 0.0 to 255.0). Let’s take
    a look at the shape and data type of the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity, we’ll scale the pixel intensities down to the 0–1 range by
    dividing them by 255.0 (this also converts them to floats):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With MNIST, when the label is equal to 5, it means that the image represents
    the handwritten digit 5\. Easy. For Fashion MNIST, however, we need the list of
    class names to know what we are dealing with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the first image in the training set represents an ankle boot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 10-10](#fashion_mnist_plot) shows some samples from the Fashion MNIST
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1010](assets/mls3_1010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. Samples from Fashion MNIST
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Creating the model using the sequential API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s build the neural network! Here is a classification MLP with two hidden
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through this code line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set TensorFlow’s random seed to make the results reproducible: the random
    weights of the hidden layers and the output layer will be the same every time
    you run the notebook. You could also choose to use the `tf.keras.utils.set_random_seed()`
    function, which conveniently sets the random seeds for TensorFlow, Python (`random.seed()`),
    and NumPy (`np.random.seed()`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next line creates a `Sequential` model. This is the simplest kind of Keras
    model for neural networks that are just composed of a single stack of layers connected
    sequentially. This is called the sequential API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we build the first layer (an `Input` layer) and add it to the model. We
    specify the input `shape`, which doesn’t include the batch size, only the shape
    of the instances. Keras needs to know the shape of the inputs so it can determine
    the shape of the connection weight matrix of the first hidden layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we add a `Flatten` layer. Its role is to convert each input image into
    a 1D array: for example, if it receives a batch of shape [32, 28, 28], it will
    reshape it to [32, 784]. In other words, if it receives input data `X`, it computes
    `X.reshape(-1, 784)`. This layer doesn’t have any parameters; it’s just there
    to do some simple preprocessing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next we add a `Dense` hidden layer with 300 neurons. It will use the ReLU activation
    function. Each `Dense` layer manages its own weight matrix, containing all the
    connection weights between the neurons and their inputs. It also manages a vector
    of bias terms (one per neuron). When it receives some input data, it computes
    [Equation 10-2](#neural_network_layer_equation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we add a second `Dense` hidden layer with 100 neurons, also using the ReLU
    activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we add a `Dense` output layer with 10 neurons (one per class), using
    the softmax activation function because the classes are exclusive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Specifying `activation="relu"` is equivalent to specifying `activation=tf.keras.activations.relu`.
    Other activation functions are available in the `tf.keras.activations` package.
    We will use many of them in this book; see [*https://keras.io/api/layers/activations*](https://keras.io/api/layers/activations)
    for the full list. We will also define our own custom activation functions in
    [Chapter 12](ch12.html#tensorflow_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of adding the layers one by one as we just did, it’s often more convenient
    to pass a list of layers when creating the `Sequential` model. You can also drop
    the `Input` layer and instead specify the `input_shape` in the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The model’s `summary()` method displays all the model’s layers,⁠^([14](ch10.html#idm45720204105760))
    including each layer’s name (which is automatically generated unless you set it
    when creating the layer), its output shape (`None` means the batch size can be
    anything), and its number of parameters. The summary ends with the total number
    of parameters, including trainable and non-trainable parameters. Here we only
    have trainable parameters (you will see some non-trainable parameters later in
    this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that `Dense` layers often have a *lot* of parameters. For example, the
    first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which
    adds up to 235,500 parameters! This gives the model quite a lot of flexibility
    to fit the training data, but it also means that the model runs the risk of overfitting,
    especially when you do not have a lot of training data. We will come back to this
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer in a model must have a unique name (e.g., `"dense_2"`). You can set
    the layer names explicitly using the constructor’s `name` argument, but generally
    it’s simpler to let Keras name the layers automatically, as we just did. Keras
    takes the layer’s class name and converts it to snake case (e.g., a layer from
    the `MyCoolLayer` class is named `"my_cool_layer"` by default). Keras also ensures
    that the name is globally unique, even across models, by appending an index if
    needed, as in `"dense_2"`. But why does it bother making the names unique across
    models? Well, this makes it possible to merge models easily without getting name
    conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All global state managed by Keras is stored in a *Keras session*, which you
    can clear using `tf.keras.backend.clear_session()`. In particular, this resets
    the name counters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily get a model’s list of layers using the `layers` attribute, or
    use the `get_layer()` method to access a layer by name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'All the parameters of a layer can be accessed using its `get_weights()` and
    `set_weights()` methods. For a `Dense` layer, this includes both the connection
    weights and the bias terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `Dense` layer initialized the connection weights randomly (which
    is needed to break symmetry, as discussed earlier), and the biases were initialized
    to zeros, which is fine. If you want to use a different initialization method,
    you can set `kernel_initializer` (*kernel* is another name for the matrix of connection
    weights) or `bias_initializer` when creating the layer. We’ll discuss initializers
    further in [Chapter 11](ch11.html#deep_chapter), and the full list is at [*https://keras.io/api/layers/initializers*](https://keras.io/api/layers/initializers).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The shape of the weight matrix depends on the number of inputs, which is why
    we specified the `input_shape` when creating the model. If you do not specify
    the input shape, it’s OK: Keras will simply wait until it knows the input shape
    before it actually builds the model parameters. This will happen either when you
    feed it some data (e.g., during training), or when you call its `build()` method.
    Until the model parameters are built, you will not be able to do certain things,
    such as display the model summary or save the model. So, if you know the input
    shape when creating the model, it is best to specify it.'
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After a model is created, you must call its `compile()` method to specify the
    loss function and the optimizer to use. Optionally, you can specify a list of
    extra metrics to compute during training and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using `loss="sparse_categorical_crossentropy"` is the equivalent of using `loss=tf.keras.losses.sparse_categorical_​cross⁠entropy`.
    Similarly, using `optimizer="sgd"` is the equivalent of using `optimizer=tf.keras.optimizers.SGD()`,
    and using `metrics=["accuracy"]` is the equivalent of using `metrics=​[tf.keras.metrics.sparse_categorical_accuracy`]
    (when using this loss). We will use many other losses, optimizers, and metrics
    in this book; for the full lists, see [*https://keras.io/api/losses*](https://keras.io/api/losses),
    [*https://keras.io/api/optimizers*](https://keras.io/api/optimizers), and [*https://keras.io/api/metrics*](https://keras.io/api/metrics).
  prefs: []
  type: TYPE_NORMAL
- en: This code requires explanation. We use the `"sparse_categorical_crossentropy"`
    loss because we have sparse labels (i.e., for each instance, there is just a target
    class index, from 0 to 9 in this case), and the classes are exclusive. If instead
    we had one target probability per class for each instance (such as one-hot vectors,
    e.g., `[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]` to represent class 3), then we
    would need to use the `"categorical_crossentropy"` loss instead. If we were doing
    binary classification or multilabel binary classification, then we would use the
    `"sigmoid"` activation function in the output layer instead of the `"softmax"`
    activation function, and we would use the `"binary_crossentropy"` loss.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you want to convert sparse labels (i.e., class indices) to one-hot vector
    labels, use the `tf.keras.utils.to_categorical()` function. To go the other way
    round, use the `np.argmax()` function with `axis=1`.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the optimizer, `"sgd"` means that we will train the model using stochastic
    gradient descent. In other words, Keras will perform the backpropagation algorithm
    described earlier (i.e., reverse-mode autodiff plus gradient descent). We will
    discuss more efficient optimizers in [Chapter 11](ch11.html#deep_chapter). They
    improve gradient descent, not autodiff.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When using the `SGD` optimizer, it is important to tune the learning rate. So,
    you will generally want to use `optimizer=tf.keras. ​opti⁠mizers.SGD(learning_rate=__???__)`
    to set the learning rate, rather than `optimizer="sgd"`, which defaults to a learning
    rate of 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, since this is a classifier, it’s useful to measure its accuracy during
    training and evaluation, which is why we set `metrics=["accuracy"]`.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now the model is ready to be trained. For this we simply need to call its `fit()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We pass it the input features (`X_train`) and the target classes (`y_train`),
    as well as the number of epochs to train (or else it would default to just 1,
    which would definitely not be enough to converge to a good solution). We also
    pass a validation set (this is optional). Keras will measure the loss and the
    extra metrics on this set at the end of each epoch, which is very useful to see
    how well the model really performs. If the performance on the training set is
    much better than on the validation set, your model is probably overfitting the
    training set, or there is a bug, such as a data mismatch between the training
    set and the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Shape errors are quite common, especially when getting started, so you should
    familiarize yourself with the error messages: try fitting a model with inputs
    and/or labels of the wrong shape, and see the errors you get. Similarly, try compiling
    the model with `loss="categorical_crossentropy"` instead of `loss="sparse_​cat⁠egorical_crossentropy"`.
    Or you can remove the `Flatten` layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And that’s it! The neural network is trained. At each epoch during training,
    Keras displays the number of mini-batches processed so far on the left side of
    the progress bar. The batch size is 32 by default, and since the training set
    has 55,000 images, the model goes through 1,719 batches per epoch: 1,718 of size
    32, and 1 of size 24\. After the progress bar, you can see the mean training time
    per sample, and the loss and accuracy (or any other extra metrics you asked for)
    on both the training set and the validation set. Notice that the training loss
    went down, which is a good sign, and the validation accuracy reached 88.94% after
    30 epochs. That’s slightly below the training accuracy, so there is a little bit
    of overfitting going on, but not a huge amount.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of passing a validation set using the `validation_data` argument, you
    could set `validation_split` to the ratio of the training set that you want Keras
    to use for validation. For example, `validation_split=0.1` tells Keras to use
    the last 10% of the data (before shuffling) for validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the training set was very skewed, with some classes being overrepresented
    and others underrepresented, it would be useful to set the `class_weight` argument
    when calling the `fit()` method, to give a larger weight to underrepresented classes
    and a lower weight to overrepresented classes. These weights would be used by
    Keras when computing the loss. If you need per-instance weights, set the `sample_weight`
    argument. If both `class_weight` and `sample_weight` are provided, then Keras
    multiplies them. Per-instance weights could be useful, for example, if some instances
    were labeled by experts while others were labeled using a crowdsourcing platform:
    you might want to give more weight to the former. You can also provide sample
    weights (but not class weights) for the validation set by adding them as a third
    item in the `validation_data` tuple.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fit()` method returns a `History` object containing the training parameters
    (`history.params`), the list of epochs it went through (`history.epoch`), and
    most importantly a dictionary (`history.history`) containing the loss and extra
    metrics it measured at the end of each epoch on the training set and on the validation
    set (if any). If you use this dictionary to create a Pandas DataFrame and call
    its `plot()` method, you get the learning curves shown in [Figure 10-11](#keras_learning_curves_plot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 1011](assets/mls3_1011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-11\. Learning curves: the mean training loss and accuracy measured
    over each epoch, and the mean validation loss and accuracy measured at the end
    of each epoch'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that both the training accuracy and the validation accuracy steadily
    increase during training, while the training loss and the validation loss decrease.
    This is good. The validation curves are relatively close to each other at first,
    but they get further apart over time, which shows that there’s a little bit of
    overfitting. In this particular case, the model looks like it performed better
    on the validation set than on the training set at the beginning of training, but
    that’s not actually the case. The validation error is computed at the *end* of
    each epoch, while the training error is computed using a running mean *during*
    each epoch, so the training curve should be shifted by half an epoch to the left.
    If you do that, you will see that the training and validation curves overlap almost
    perfectly at the beginning of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training set performance ends up beating the validation performance, as
    is generally the case when you train for long enough. You can tell that the model
    has not quite converged yet, as the validation loss is still going down, so you
    should probably continue training. This is as simple as calling the `fit()` method
    again, since Keras just continues training where it left off: you should be able
    to reach about 89.8% validation accuracy, while the training accuracy will continue
    to rise up to 100% (this is not always the case).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not satisfied with the performance of your model, you should go
    back and tune the hyperparameters. The first one to check is the learning rate.
    If that doesn’t help, try another optimizer (and always retune the learning rate
    after changing any hyperparameter). If the performance is still not great, then
    try tuning model hyperparameters such as the number of layers, the number of neurons
    per layer, and the types of activation functions to use for each hidden layer.
    You can also try tuning other hyperparameters, such as the batch size (it can
    be set in the `fit()` method using the `batch_size` argument, which defaults to
    32). We will get back to hyperparameter tuning at the end of this chapter. Once
    you are satisfied with your model’s validation accuracy, you should evaluate it
    on the test set to estimate the generalization error before you deploy the model
    to production. You can easily do this using the `evaluate()` method (it also supports
    several other arguments, such as `batch_size` and `sample_weight`; please check
    the documentation for more details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you saw in [Chapter 2](ch02.html#project_chapter), it is common to get slightly
    lower performance on the test set than on the validation set, because the hyperparameters
    are tuned on the validation set, not the test set (however, in this example, we
    did not do any hyperparameter tuning, so the lower accuracy is just bad luck).
    Remember to resist the temptation to tweak the hyperparameters on the test set,
    or else your estimate of the generalization error will be too optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: Using the model to make predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s use the model’s `predict()` method to make predictions on new instances.
    Since we don’t have actual new instances, we’ll just use the first three instances
    of the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For each instance the model estimates one probability per class, from class
    0 to class 9\. This is similar to the output of the `predict_proba()` method in
    Scikit-Learn classifiers. For example, for the first image it estimates that the
    probability of class 9 (ankle boot) is 96%, the probability of class 7 (sneaker)
    is 2%, the probability of class 5 (sandal) is 1%, and the probabilities of the
    other classes are negligible. In other words, it is highly confident that the
    first image is footwear, most likely ankle boots but possibly sneakers or sandals.
    If you only care about the class with the highest estimated probability (even
    if that probability is quite low), then you can use the `argmax()` method to get
    the highest probability class index for each instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the classifier actually classified all three images correctly (these
    images are shown in [Figure 10-12](#fashion_mnist_images_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 1012](assets/mls3_1012.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-12\. Correctly classified Fashion MNIST images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now you know how to use the sequential API to build, train, and evaluate a classification
    MLP. But what about regression?
  prefs: []
  type: TYPE_NORMAL
- en: Building a Regression MLP Using the Sequential API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s switch back to the California housing problem and tackle it using the
    same MLP as earlier, with 3 hidden layers composed of 50 neurons each, but this
    time building it with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the sequential API to build, train, evaluate, and use a regression MLP
    is quite similar to what we did for classification. The main differences in the
    following code example are the fact that the output layer has a single neuron
    (since we only want to predict a single value) and it uses no activation function,
    the loss function is the mean squared error, the metric is the RMSE, and we’re
    using an Adam optimizer like Scikit-Learn’s `MLPRegressor` did. Moreover, in this
    example we don’t need a `Flatten` layer, and instead we’re using a `Normalization`
    layer as the first layer: it does the same thing as Scikit-Learn’s `StandardScaler`,
    but it must be fitted to the training data using its `adapt()` method *before*
    you call the model’s `fit()` method. (Keras has other preprocessing layers, which
    will be covered in [Chapter 13](ch13.html#data_chapter)). Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `Normalization` layer learns the feature means and standard deviations in
    the training data when you call the `adapt()` method. Yet when you display the
    model’s summary, these statistics are listed as non-trainable. This is because
    these parameters are not affected by gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the sequential API is quite clean and straightforward. However,
    although `Sequential` models are extremely common, it is sometimes useful to build
    neural networks with more complex topologies, or with multiple inputs or outputs.
    For this purpose, Keras offers the functional API.
  prefs: []
  type: TYPE_NORMAL
- en: Building Complex Models Using the Functional API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One example of a nonsequential neural network is a *Wide & Deep* neural network.
    This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng
    et al.⁠^([15](ch10.html#idm45720203290048)) It connects all or part of the inputs
    directly to the output layer, as shown in [Figure 10-13](#wide_deep_diagram).
    This architecture makes it possible for the neural network to learn both deep
    patterns (using the deep path) and simple rules (through the short path).⁠^([16](ch10.html#idm45720203287488))
    In contrast, a regular MLP forces all the data to flow through the full stack
    of layers; thus, simple patterns in the data may end up being distorted by this
    sequence of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1013](assets/mls3_1013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-13\. Wide & Deep neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s build such a neural network to tackle the California housing problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'At a high level, the first five lines create all the layers we need to build
    the model, the next six lines use these layers just like functions to go from
    the input to the output, and the last line creates a Keras `Model` object by pointing
    to the input and the output. Let’s go through this code in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create five layers: a `Normalization` layer to standardize the inputs,
    two `Dense` layers with 30 neurons each, using the ReLU activation function, a
    `Concatenate` layer, and one more `Dense` layer with a single neuron for the output
    layer, without any activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we create an `Input` object (the variable name `input_` is used to avoid
    overshadowing Python’s built-in `input()` function). This is a specification of
    the kind of input the model will get, including its `shape` and optionally its
    `dtype`, which defaults to 32-bit floats. A model may actually have multiple inputs,
    as you will see shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we use the `Normalization` layer just like a function, passing it the
    `Input` object. This is why this is called the functional API. Note that we are
    just telling Keras how it should connect the layers together; no actual data is
    being processed yet, as the `Input` object is just a data specification. In other
    words, it’s a symbolic input. The output of this call is also symbolic: `normalized`
    doesn’t store any actual data, it’s just used to construct the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the same way, we then pass `normalized` to `hidden_layer1`, which outputs
    `hidden1`, and we pass `hidden1` to `hidden_layer2`, which outputs `hidden2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far we’ve connected the layers sequentially, but then we use the `concat_layer`
    to concatenate the input and the second hidden layer’s output. Again, no actual
    data is concatenated yet: it’s all symbolic, to build the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we pass `concat` to the `output_layer`, which gives us the final `output`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we create a Keras `Model`, specifying which inputs and outputs to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have built this Keras model, everything is exactly like earlier, so
    there’s no need to repeat it here: you compile the model, adapt the `Normalization`
    layer, fit the model, evaluate it, and use it to make predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if you want to send a subset of the features through the wide path
    and a different subset (possibly overlapping) through the deep path, as illustrated
    in [Figure 10-14](#multiple_inputs_diagram)? In this case, one solution is to
    use multiple inputs. For example, suppose we want to send five features through
    the wide path (features 0 to 4), and six features through the deep path (features
    2 to 7). We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![mls3 1014](assets/mls3_1014.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. Handling multiple inputs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are a few things to note in this example, compared to the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: Each `Dense` layer is created and called on the same line. This is a common
    practice, as it makes the code more concise without losing clarity. However, we
    can’t do this with the `Normalization` layer since we need a reference to the
    layer to be able to call its `adapt()` method before fitting the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used `tf.keras.layers.concatenate()`, which creates a `Concatenate` layer
    and calls it with the given inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We specified `inputs=[input_wide, input_deep]` when creating the model, since
    there are two inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we can compile the model as usual, but when we call the `fit()` method,
    instead of passing a single input matrix `X_train`, we must pass a pair of matrices
    `(X_train_wide, X_train_deep)`, one per input. The same is true for `X_valid`,
    and also for `X_test` and `X_new` when you call `evaluate()` or `predict()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instead of passing a tuple `(X_train_wide,` `X_train_deep)`, you can pass a
    dictionary `{"input_wide":` `X_train_wide, "input_deep": X_train_deep}`, if you
    set `name="input_wide"` and `name="input_deep"` when creating the inputs. This
    is highly recommended when there are many inputs, to clarify the code and avoid
    getting the order wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also many use cases in which you may want to have multiple outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The task may demand it. For instance, you may want to locate and classify the
    main object in a picture. This is both a regression tasks and a classification
    task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, you may have multiple independent tasks based on the same data. Sure,
    you could train one neural network per task, but in many cases you will get better
    results on all tasks by training a single neural network with one output per task.
    This is because the neural network can learn features in the data that are useful
    across tasks. For example, you could perform *multitask classification* on pictures
    of faces, using one output to classify the person’s facial expression (smiling,
    surprised, etc.) and another output to identify whether they are wearing glasses
    or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another use case is as a regularization technique (i.e., a training constraint
    whose objective is to reduce overfitting and thus improve the model’s ability
    to generalize). For example, you may want to add an auxiliary output in a neural
    network architecture (see [Figure 10-15](#multiple_outputs_diagram)) to ensure
    that the underlying part of the network learns something useful on its own, without
    relying on the rest of the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![mls3 1015](assets/mls3_1015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. Handling multiple outputs, in this example to add an auxiliary
    output for regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Adding an extra output is quite easy: we just connect it to the appropriate
    layer and add it to the model’s list of outputs. For example, the following code
    builds the network represented in [Figure 10-15](#multiple_outputs_diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Each output will need its own loss function. Therefore, when we compile the
    model, we should pass a list of losses. If we pass a single loss, Keras will assume
    that the same loss must be used for all outputs. By default, Keras will compute
    all the losses and simply add them up to get the final loss used for training.
    Since we care much more about the main output than about the auxiliary output
    (as it is just used for regularization), we want to give the main output’s loss
    a much greater weight. Luckily, it is possible to set all the loss weights when
    compiling the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instead of passing a tuple `loss=("mse", "mse")`, you can pass a dictionary
    `loss={"output": "mse", "aux_output": "mse"}`, assuming you created the output
    layers with `name="output"` and `name="aux_output"`. Just like for the inputs,
    this clarifies the code and avoids errors when there are several outputs. You
    can also pass a dictionary for `loss_weights`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now when we train the model, we need to provide labels for each output. In
    this example, the main output and the auxiliary output should try to predict the
    same thing, so they should use the same labels. So instead of passing `y_train`,
    we need to pass `(y_train, y_train)`, or a dictionary `{"output": y_train, "aux_output":
    y_train}` if the outputs were named `"output"` and `"aux_output"`. The same goes
    for `y_valid` and `y_test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When we evaluate the model, Keras returns the weighted sum of the losses, as
    well as all the individual losses and metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you set `return_dict=True`, then `evaluate()` will return a dictionary instead
    of a big tuple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the `predict()` method will return predictions for each output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict()` method returns a tuple, and it does not have a `return_dict`
    argument to get a dictionary instead. However, you can create one using `model.output_names`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, you can build all sorts of architectures with the functional
    API. Next, we’ll look at one last way you can build Keras models.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Subclassing API to Build Dynamic Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both the sequential API and the functional API are declarative: you start by
    declaring which layers you want to use and how they should be connected, and only
    then can you start feeding the model some data for training or inference. This
    has many advantages: the model can easily be saved, cloned, and shared; its structure
    can be displayed and analyzed; the framework can infer shapes and check types,
    so errors can be caught early (i.e., before any data ever goes through the model).
    It’s also fairly straightforward to debug, since the whole model is a static graph
    of layers. But the flip side is just that: it’s static. Some models involve loops,
    varying shapes, conditional branching, and other dynamic behaviors. For such cases,
    or simply if you prefer a more imperative programming style, the subclassing API
    is for you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this approach, you subclass the `Model` class, create the layers you need
    in the constructor, and use them to perform the computations you want in the `call()`
    method. For example, creating an instance of the following `WideAndDeepModel`
    class gives us an equivalent model to the one we just built with the functional
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This example looks like the previous one, except we separate the creation of
    the layers⁠^([17](ch10.html#idm45720201814912)) in the constructor from their
    usage in the `call()` method. And we don’t need to create the `Input` objects:
    we can use the `input` argument to the `call()` method.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a model instance, we can compile it, adapt its normalization
    layers (e.g., using `model.norm_layer_wide.adapt(...)` and `model.norm_​layer_deep.adapt(...)`),
    fit it, evaluate it, and use it to make predictions, exactly like we did with
    the functional API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The big difference with this API is that you can include pretty much anything
    you want in the `call()` method: `for` loops, `if` statements, low-level TensorFlow
    operations—your imagination is the limit (see [Chapter 12](ch12.html#tensorflow_chapter))!
    This makes it a great API when experimenting with new ideas, especially for researchers.
    However, this extra flexibility does come at a cost: your model’s architecture
    is hidden within the `call()` method, so Keras cannot easily inspect it; the model
    cannot be cloned using `tf.keras.models.clone_model()`; and when you call the
    `summary()` method, you only get a list of layers, without any information on
    how they are connected to each other. Moreover, Keras cannot check types and shapes
    ahead of time, and it is easier to make mistakes. So unless you really need that
    extra flexibility, you should probably stick to the sequential API or the functional
    API.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keras models can be used just like regular layers, so you can easily combine
    them to build complex architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to build and train neural nets using Keras, you will want
    to save them!
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Restoring a Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Saving a trained Keras model is as simple as it gets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'When you set `save_format="tf"`,^([18](ch10.html#idm45720201799248)) Keras
    saves the model using TensorFlow’s *SavedModel* format: this is a directory (with
    the given name) containing several files and subdirectories. In particular, the
    *saved_model.pb* file contains the model’s architecture and logic in the form
    of a serialized computation graph, so you don’t need to deploy the model’s source
    code in order to use it in production; the SavedModel is sufficient (you will
    see how this works in [Chapter 12](ch12.html#tensorflow_chapter)). The *keras_metadata.pb*
    file contains extra information needed by Keras. The *variables* subdirectory
    contains all the parameter values (including the connection weights, the biases,
    the normalization statistics, and the optimizer’s parameters), possibly split
    across multiple files if the model is very large. Lastly, the *assets* directory
    may contain extra files, such as data samples, feature names, class names, and
    so on. By default, the *assets* directory is empty. Since the optimizer is also
    saved, including its hyperparameters and any state it may have, after loading
    the model you can continue training if you want.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you set `save_format="h5"` or use a filename that ends with *.h5*, *.hdf5*,
    or *.keras*, then Keras will save the model to a single file using a Keras-specific
    format based on the HDF5 format. However, most TensorFlow deployment tools require
    the SavedModel format instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will typically have a script that trains a model and saves it, and one
    or more scripts (or web services) that load the model and use it to evaluate it
    or to make predictions. Loading the model is just as easy as saving it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You can also use `save_weights()` and `load_weights()` to save and load only
    the parameter values. This includes the connection weights, biases, preprocessing
    stats, optimizer state, etc. The parameter values are saved in one or more files
    such as *my_weights.data-00004-of-00052*, plus an index file like *my_weights.index*.
  prefs: []
  type: TYPE_NORMAL
- en: Saving just the weights is faster and uses less disk space than saving the whole
    model, so it’s perfect to save quick checkpoints during training. If you’re training
    a big model, and it takes hours or days, then you must save checkpoints regularly
    in case the computer crashes. But how can you tell the `fit()` method to save
    checkpoints? Use callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Using Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `fit()` method accepts a `callbacks` argument that lets you specify a list
    of objects that Keras will call before and after training, before and after each
    epoch, and even before and after processing each batch. For example, the `ModelCheckpoint`
    callback saves checkpoints of your model at regular intervals during training,
    by default at the end of each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, if you use a validation set during training, you can set `save_​best_only=True`
    when creating the `ModelCheckpoint`. In this case, it will only save your model
    when its performance on the validation set is the best so far. This way, you do
    not need to worry about training for too long and overfitting the training set:
    simply restore the last saved model after training, and this will be the best
    model on the validation set. This is one way to implement early stopping (introduced
    in [Chapter 4](ch04.html#linear_models_chapter)), but it won’t actually stop training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way is to use the `EarlyStopping` callback. It will interrupt training
    when it measures no progress on the validation set for a number of epochs (defined
    by the `patience` argument), and if you set `restore_best_weights=True` it will
    roll back to the best model at the end of training. You can combine both callbacks
    to save checkpoints of your model in case your computer crashes, and interrupt
    training early when there is no more progress, to avoid wasting time and resources
    and to reduce overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The number of epochs can be set to a large value since training will stop automatically
    when there is no more progress (just make sure the learning rate is not too small,
    or else it might keep making slow progress until the end). The `EarlyStopping`
    callback will store the weights of the best model in RAM, and it will restore
    them for you at the end of training.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many other callbacks are available in the [`tf.keras.callbacks` package](https://keras.io/api/callbacks).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need extra control, you can easily write your own custom callbacks.
    For example, the following custom callback will display the ratio between the
    validation loss and the training loss during training (e.g., to detect overfitting):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you might expect, you can implement `on_train_begin()`, `on_train_end()`,
    `on_epoch_begin()`, `on_epoch_end()`, `on_batch_begin()`, and `on_batch_end()`.
    Callbacks can also be used during evaluation and predictions, should you ever
    need them (e.g., for debugging). For evaluation, you should implement `on_test_begin()`,
    `on_test_end()`, `on_test_batch_begin()`, or `on_test_batch_end()`, which are
    called by `evaluate()`. For prediction, you should implement `on_predict_begin()`,
    `on_predict_end()`, `on_predict_batch_begin()`, or `on_predict_batch_end()`, which
    are called by `predict()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s take a look at one more tool you should definitely have in your toolbox
    when using Keras: TensorBoard.'
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard for Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard is a great interactive visualization tool that you can use to view
    the learning curves during training, compare curves and metrics between multiple
    runs, visualize the computation graph, analyze training statistics, view images
    generated by your model, visualize complex multidimensional data projected down
    to 3D and automatically clustered for you, *profile* your network (i.e., measure
    its speed to identify bottlenecks), and more!
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorBoard is installed automatically when you install TensorFlow. However,
    you will need a TensorBoard plug-in to visualize profiling data. If you followed
    the installation instructions at [*https://homl.info/install*](https://homl.info/install)
    to run everything locally, then you already have the plug-in installed, but if
    you are using Colab, then you must run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To use TensorBoard, you must modify your program so that it outputs the data
    you want to visualize to special binary logfiles called *event files*. Each binary
    data record is called a *summary*. The TensorBoard server will monitor the log
    directory, and it will automatically pick up the changes and update the visualizations:
    this allows you to visualize live data (with a short delay), such as the learning
    curves during training. In general, you want to point the TensorBoard server to
    a root log directory and configure your program so that it writes to a different
    subdirectory every time it runs. This way, the same TensorBoard server instance
    will allow you to visualize and compare data from multiple runs of your program,
    without getting everything mixed up.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s name the root log directory *my_logs*, and let’s define a little function
    that generates the path of the log subdirectory based on the current date and
    time, so that it’s different at every run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The good news is that Keras provides a convenient `TensorBoard()` callback
    that will take care of creating the log directory for you (along with its parent
    directories if needed), and it will create event files and write summaries to
    them during training. It will measure your model’s training and validation loss
    and metrics (in this case, the MSE and RMSE), and it will also profile your neural
    network. It is straightforward to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: That’s all there is to it! In this example, it will profile the network between
    batches 100 and 200 during the first epoch. Why 100 and 200? Well, it often takes
    a few batches for the neural network to “warm up”, so you don’t want to profile
    too early, and profiling uses resources, so it’s best not to do it for every batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, try changing the learning rate from 0.001 to 0.002, and run the code
    again, with a new log subdirectory. You will end up with a directory structure
    similar to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: There’s one directory per run, each containing one subdirectory for training
    logs and one for validation logs. Both contain event files, and the training logs
    also include profiling traces.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have the event files ready, it’s time to start the TensorBoard
    server. This can be done directly within Jupyter or Colab using the Jupyter extension
    for TensorBoard, which gets installed along with the TensorBoard library. This
    extension is preinstalled in Colab. The following code loads the Jupyter extension
    for TensorBoard, and the second line starts a TensorBoard server for the *my_logs*
    directory, connects to this server and displays the user interface directly inside
    of Jupyter. The server, listens on the first available TCP port greater than or
    equal to 6006 (or you can set the port you want using the `--port` option).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re running everything on your own machine, it’s possible to start TensorBoard
    by executing `tensorboard --logdir=./my_logs` in a terminal. You must first activate
    the Conda environment in which you installed TensorBoard, and go to the *handson-ml3*
    directory. Once the server is started, visit [*http://localhost:6006*](http://localhost:6006).
  prefs: []
  type: TYPE_NORMAL
- en: Now you should see TensorBoard’s user interface. Click the SCALARS tab to view
    the learning curves (see [Figure 10-16](#tensorboard_diagram)). At the bottom
    left, select the logs you want to visualize (e.g., the training logs from the
    first and second run), and click the `epoch_loss` scalar. Notice that the training
    loss went down nicely during both runs, but in the second run it went down a bit
    faster thanks to the higher learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1016](assets/mls3_1016.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-16\. Visualizing learning curves with TensorBoard
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also visualize the whole computation graph in the GRAPHS tab, the learned
    weights projected to 3D in the PROJECTOR tab, and the profiling traces in the
    PROFILE tab. The `TensorBoard()` callback has options to log extra data too (see
    the documentation for more details). You can click the refresh button (⟳) at the
    top right to make TensorBoard refresh data, and you can click the settings button
    (⚙) to activate auto-refresh and specify the refresh interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, TensorFlow offers a lower-level API in the `tf.summary` package.
    The following code creates a `SummaryWriter` using the `create_file_writer()`
    function, and it uses this writer as a Python context to log scalars, histograms,
    images, audio, and text, all of which can then be visualized using TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code and click the refresh button in TensorBoard, you will
    see several tabs appear: IMAGES, AUDIO, DISTRIBUTIONS, HISTOGRAMS, and TEXT. Try
    clicking the IMAGES tab, and use the slider above each image to view the images
    at different time steps. Similarly, go to the AUDIO tab and try listening to the
    audio at different time steps. As you can see, TensorBoard is a useful tool even
    beyond TensorFlow or deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can share your results online by publishing them to [*https://tensorboard.dev*](https://tensorboard.dev).
    For this, just run `!tensorboard dev upload` `--logdir` `./my_logs`. The first
    time, it will ask you to accept the terms and conditions and authenticate. Then
    your logs will be uploaded, and you will get a permanent link to view your results
    in a TensorBoard interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize what you’ve learned so far in this chapter: you now know where
    neural nets came from, what an MLP is and how you can use it for classification
    and regression, how to use Keras’s sequential API to build MLPs, and how to use
    the functional API or the subclassing API to build more complex model architectures
    (including Wide & Deep models, as well as models with multiple inputs and outputs).
    You also learned how to save and restore a model and how to use callbacks for
    checkpointing, early stopping, and more. Finally, you learned how to use TensorBoard
    for visualization. You can already go ahead and use neural networks to tackle
    many problems! However, you may wonder how to choose the number of hidden layers,
    the number of neurons in the network, and all the other hyperparameters. Let’s
    look at this now.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Neural Network Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The flexibility of neural networks is also one of their main drawbacks: there
    are many hyperparameters to tweak. Not only can you use any imaginable network
    architecture, but even in a basic MLP you can change the number of layers, the
    number of neurons and the type of activation function to use in each layer, the
    weight initialization logic, the type of optimizer to use, its learning rate,
    the batch size, and more. How do you know what combination of hyperparameters
    is the best for your task?'
  prefs: []
  type: TYPE_NORMAL
- en: 'One option is to convert your Keras model to a Scikit-Learn estimator, and
    then use `GridSearchCV` or `RandomizedSearchCV` to fine-tune the hyperparameters,
    as you did in [Chapter 2](ch02.html#project_chapter). For this, you can use the
    `KerasRegressor` and `KerasClassifier` wrapper classes from the SciKeras library
    (see [*https://github.com/adriangb/scikeras*](https://github.com/adriangb/scikeras)
    for more details). However, there’s a better way: you can use the *Keras Tuner*
    library, which is a hyperparameter tuning library for Keras models. It offers
    several tuning strategies, it’s highly customizable, and it has excellent integration
    with TensorBoard. Let’s see how to use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you followed the installation instructions at [*https://homl.info/install*](https://homl.info/install)
    to run everything locally, then you already have Keras Tuner installed, but if
    you are using Colab, you’ll need to run `%pip install -q -U keras-tuner`. Next,
    import `keras_tuner`, usually as `kt`, then write a function that builds, compiles,
    and returns a Keras model. The function must take a `kt.HyperParameters` object
    as an argument, which it can use to define hyperparameters (integers, floats,
    strings, etc.) along with their range of possible values, and these hyperparameters
    may be used to build and compile the model. For example, the following function
    builds and compiles an MLP to classify Fashion MNIST images, using hyperparameters
    such as the number of hidden layers (`n_hidden`), the number of neurons per layer
    (`n_neurons`), the learning rate (`learning_rate`), and the type of optimizer
    to use (`optimizer`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of the function defines the hyperparameters. For example, `hp.Int("n_hidden",
    min_value=0, max_value=8, default=2)` checks whether a hyperparameter named `"n_hidden"`
    is already present in the `HyperParameters` object `hp`, and if so it returns
    its value. If not, then it registers a new integer hyperparameter named `"n_hidden"`,
    whose possible values range from 0 to 8 (inclusive), and it returns the default
    value, which is 2 in this case (when `default` is not set, then `min_value` is
    returned). The `"n_neurons"` hyperparameter is registered in a similar way. The
    `"learning_rate"` hyperparameter is registered as a float ranging from 10^(–4)
    to 10^(–2), and since `sampling="log"`, learning rates of all scales will be sampled
    equally. Lastly, the `optimizer` hyperparameter is registered with two possible
    values: `"sgd"` or `"adam"` (the default value is the first one, which is `"sgd"`
    in this case). Depending on the value of `optimizer`, we create an `SGD` optimizer
    or an `Adam` optimizer with the given learning rate.'
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the function just builds the model using the hyperparameter
    values. It creates a `Sequential` model starting with a `Flatten` layer, followed
    by the requested number of hidden layers (as determined by the `n_hidden` hyperparameter)
    using the ReLU activation function, and an output layer with 10 neurons (one per
    class) using the softmax activation function. Lastly, the function compiles the
    model and returns it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you want to do a basic random search, you can create a `kt.RandomSearch`
    tuner, passing the `build_model` function to the constructor, and call the tuner’s
    `search()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `RandomSearch` tuner first calls `build_model()` once with an empty `Hyperparameters`
    object, just to gather all the hyperparameter specifications. Then, in this example,
    it runs 5 trials; for each trial it builds a model using hyperparameters sampled
    randomly within their respective ranges, then it trains that model for 10 epochs
    and saves it to a subdirectory of the *my_fashion_mnist/my_rnd_search* directory.
    Since `overwrite=True`, the *my_rnd_search* directory is deleted before training
    starts. If you run this code a second time but with `overwrite=False` and `max_​tri⁠als=10`,
    the tuner will continue tuning where it left off, running 5 more trials: this
    means you don’t have to run all the trials in one shot. Lastly, since `objective`
    is set to `"val_accuracy"`, the tuner prefers models with a higher validation
    accuracy, so once the tuner has finished searching, you can get the best models
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also call `get_best_hyperparameters()` to get the `kt.HyperParameters`
    of the best models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Each tuner is guided by a so-called *oracle*: before each trial, the tuner
    asks the oracle to tell it what the next trial should be. The `RandomSearch` tuner
    uses a `RandomSearchOracle`, which is pretty basic: it just picks the next trial
    randomly, as we saw earlier. Since the oracle keeps track of all the trials, you
    can ask it to give you the best one, and you can display a summary of that trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows the best hyperparameters (like earlier), as well as the validation
    accuracy. You can also access all the metrics directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are happy with the best model’s performance, you may continue training
    it for a few epochs on the full training set (`X_train_full` and `y_train_full`),
    then evaluate it on the test set, and deploy it to production (see [Chapter 19](ch19.html#deployment_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, you may want to fine-tune data preprocessing hyperparameters,
    or `model.fit()` arguments, such as the batch size. For this, you must use a slightly
    different technique: instead of writing a `build_model()` function, you must subclass
    the `kt.HyperModel` class and define two methods, `build()` and `fit()`. The `build()`
    method does the exact same thing as the `build_model()` function. The `fit()`
    method takes a `HyperParameters` object and a compiled model as an argument, as
    well as all the `model.fit()` arguments, and fits the model and returns the `History`
    object. Crucially, the `fit()` method may use hyperparameters to decide how to
    preprocess the data, tweak the batch size, and more. For example, the following
    class builds the same model as before, with the same hyperparameters, but it also
    uses a Boolean `"normalize"` hyperparameter to control whether or not to standardize
    the training data before fitting the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then pass an instance of this class to the tuner of your choice, instead
    of passing the `build_model` function. For example, let’s build a `kt.Hyperband`
    tuner based on a `MyClassificationHyperModel` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This tuner is similar to the `HalvingRandomSearchCV` class we discussed in
    [Chapter 2](ch02.html#project_chapter): it starts by training many different models
    for few epochs, then it eliminates the worst models and keeps only the top `1
    / factor` models (i.e., the top third in this case), repeating this selection
    process until a single model is left.^([19](ch10.html#idm45720200186752)) The
    `max_epochs` argument controls the max number of epochs that the best model will
    be trained for. The whole process is repeated twice in this case (`hyperband_iterations=2`).
    The total number of training epochs across all models for each hyperband iteration
    is about `max_epochs * (log(max_epochs) / log(factor)) ** 2`, so it’s about 44
    epochs in this example. The other arguments are the same as for `kt.RandomSearch`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the Hyperband tuner now. We’ll use the `TensorBoard` callback, this
    time pointing to the root log directory (the tuner will take care of using a different
    subdirectory for each trial), as well as an `EarlyStopping` callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if you open TensorBoard, pointing `--logdir` to the *my_fashion_mnist/hyperband/tensorboard*
    directory, you will see all the trial results as they unfold. Make sure to visit
    the HPARAMS tab: it contains a summary of all the hyperparameter combinations
    that were tried, along with the corresponding metrics. Notice that there are three
    tabs inside the HPARAMS tab: a table view, a parallel coordinates view, and a
    scatterplot matrix view. In the lower part of the left panel, uncheck all metrics
    except for `validation.epoch_accuracy`: this will make the graphs clearer. In
    the parallel coordinates view, try selecting a range of high values in the `validation.epoch_accuracy`
    column: this will filter only the hyperparameter combinations that reached a good
    performance. Click one of the hyperparameter combinations, and the corresponding
    learning curves will appear at the bottom of the page. Take some time to go through
    each tab; this will help you understand the effect of each hyperparameter on performance,
    as well as the interactions between the hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperband is smarter than pure random search in the way it allocates resources,
    but at its core it still explores the hyperparameter space randomly; it’s fast,
    but coarse. However, Keras Tuner also includes a `kt.BayesianOptimization` tuner:
    this algorithm gradually learns which regions of the hyperparameter space are
    most promising by fitting a probabilistic model called a *Gaussian process*. This
    allows it to gradually zoom in on the best hyperparameters. The downside is that
    the algorithm has its own hyperparameters: `alpha` represents the level of noise
    you expect in the performance measures across trials (it defaults to 10^(–4)),
    and `beta` specifies how much you want the algorithm to explore, instead of simply
    exploiting the known good regions of hyperparameter space (it defaults to 2.6).
    Other than that, this tuner can be used just like the previous ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Hyperparameter tuning is still an active area of research, and many other approaches
    are being explored. For example, check out DeepMind’s excellent [2017 paper](https://homl.info/pbt),⁠^([20](ch10.html#idm45720199979472))
    where the authors used an evolutionary algorithm to jointly optimize a population
    of models and their hyperparameters. Google has also used an evolutionary approach,
    not just to search for hyperparameters but also to explore all sorts of model
    architectures: it powers their AutoML service on Google Vertex AI (see [Chapter 19](ch19.html#deployment_chapter)).
    The term *AutoML* refers to any system that takes care of a large part of the
    ML workflow. Evolutionary algorithms have even been used successfully to train
    individual neural networks, replacing the ubiquitous gradient descent! For an
    example, see the [2017 post](https://homl.info/neuroevol) by Uber where the authors
    introduce their *Deep Neuroevolution* technique.'
  prefs: []
  type: TYPE_NORMAL
- en: But despite all this exciting progress and all these tools and services, it
    still helps to have an idea of what values are reasonable for each hyperparameter
    so that you can build a quick prototype and restrict the search space. The following
    sections provide guidelines for choosing the number of hidden layers and neurons
    in an MLP and for selecting good values for some of the main hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Number of Hidden Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For many problems, you can begin with a single hidden layer and get reasonable
    results. An MLP with just one hidden layer can theoretically model even the most
    complex functions, provided it has enough neurons. But for complex problems, deep
    networks have a much higher *parameter efficiency* than shallow ones: they can
    model complex functions using exponentially fewer neurons than shallow nets, allowing
    them to reach much better performance with the same amount of training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand why, suppose you are asked to draw a forest using some drawing
    software, but you are forbidden to copy and paste anything. It would take an enormous
    amount of time: you would have to draw each tree individually, branch by branch,
    leaf by leaf. If you could instead draw one leaf, copy and paste it to draw a
    branch, then copy and paste that branch to create a tree, and finally copy and
    paste this tree to make a forest, you would be finished in no time. Real-world
    data is often structured in such a hierarchical way, and deep neural networks
    automatically take advantage of this fact: lower hidden layers model low-level
    structures (e.g., line segments of various shapes and orientations), intermediate
    hidden layers combine these low-level structures to model intermediate-level structures
    (e.g., squares, circles), and the highest hidden layers and the output layer combine
    these intermediate structures to model high-level structures (e.g., faces).'
  prefs: []
  type: TYPE_NORMAL
- en: Not only does this hierarchical architecture help DNNs converge faster to a
    good solution, but it also improves their ability to generalize to new datasets.
    For example, if you have already trained a model to recognize faces in pictures
    and you now want to train a new neural network to recognize hairstyles, you can
    kickstart the training by reusing the lower layers of the first network. Instead
    of randomly initializing the weights and biases of the first few layers of the
    new neural network, you can initialize them to the values of the weights and biases
    of the lower layers of the first network. This way the network will not have to
    learn from scratch all the low-level structures that occur in most pictures; it
    will only have to learn the higher-level structures (e.g., hairstyles). This is
    called *transfer learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, for many problems you can start with just one or two hidden layers
    and the neural network will work just fine. For instance, you can easily reach
    above 97% accuracy on the MNIST dataset using just one hidden layer with a few
    hundred neurons, and above 98% accuracy using two hidden layers with the same
    total number of neurons, in roughly the same amount of training time. For more
    complex problems, you can ramp up the number of hidden layers until you start
    overfitting the training set. Very complex tasks, such as large image classification
    or speech recognition, typically require networks with dozens of layers (or even
    hundreds, but not fully connected ones, as you will see in [Chapter 14](ch14.html#cnn_chapter)),
    and they need a huge amount of training data. You will rarely have to train such
    networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art
    network that performs a similar task. Training will then be a lot faster and require
    much less data (we will discuss this in [Chapter 11](ch11.html#deep_chapter)).'
  prefs: []
  type: TYPE_NORMAL
- en: Number of Neurons per Hidden Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The number of neurons in the input and output layers is determined by the type
    of input and output your task requires. For example, the MNIST task requires 28
    × 28 = 784 inputs and 10 output neurons.
  prefs: []
  type: TYPE_NORMAL
- en: As for the hidden layers, it used to be common to size them to form a pyramid,
    with fewer and fewer neurons at each layer—the rationale being that many low-level
    features can coalesce into far fewer high-level features. A typical neural network
    for MNIST might have 3 hidden layers, the first with 300 neurons, the second with
    200, and the third with 100\. However, this practice has been largely abandoned
    because it seems that using the same number of neurons in all hidden layers performs
    just as well in most cases, or even better; plus, there is only one hyperparameter
    to tune, instead of one per layer. That said, depending on the dataset, it can
    sometimes help to make the first hidden layer bigger than the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the number of layers, you can try increasing the number of neurons
    gradually until the network starts overfitting. Alternatively, you can try building
    a model with slightly more layers and neurons than you actually need, then use
    early stopping and other regularization techniques to prevent it from overfitting
    too much. Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch
    pants” approach: instead of wasting time looking for pants that perfectly match
    your size, just use large stretch pants that will shrink down to the right size.
    With this approach, you avoid bottleneck layers that could ruin your model. Indeed,
    if a layer has too few neurons, it will not have enough representational power
    to preserve all the useful information from the inputs (e.g., a layer with two
    neurons can only output 2D data, so if it gets 3D data as input, some information
    will be lost). No matter how big and powerful the rest of the network is, that
    information will never be recovered.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In general you will get more bang for your buck by increasing the number of
    layers instead of the number of neurons per layer.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate, Batch Size, and Other Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The number of hidden layers and neurons are not the only hyperparameters you
    can tweak in an MLP. Here are some of the most important ones, as well as tips
    on how to set them:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning rate is arguably the most important hyperparameter. In general,
    the optimal learning rate is about half of the maximum learning rate (i.e., the
    learning rate above which the training algorithm diverges, as we saw in [Chapter 4](ch04.html#linear_models_chapter)).
    One way to find a good learning rate is to train the model for a few hundred iterations,
    starting with a very low learning rate (e.g., 10^(–5)) and gradually increasing
    it up to a very large value (e.g., 10). This is done by multiplying the learning
    rate by a constant factor at each iteration (e.g., by (10 / 10^(-5))^(1 / 500)
    to go from 10^(–5) to 10 in 500 iterations). If you plot the loss as a function
    of the learning rate (using a log scale for the learning rate), you should see
    it dropping at first. But after a while, the learning rate will be too large,
    so the loss will shoot back up: the optimal learning rate will be a bit lower
    than the point at which the loss starts to climb (typically about 10 times lower
    than the turning point). You can then reinitialize your model and train it normally
    using this good learning rate. We will look at more learning rate optimization
    techniques in [Chapter 11](ch11.html#deep_chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a better optimizer than plain old mini-batch gradient descent (and
    tuning its hyperparameters) is also quite important. We will examine several advanced
    optimizers in [Chapter 11](ch11.html#deep_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs: []
  type: TYPE_NORMAL
- en: 'The batch size can have a significant impact on your model’s performance and
    training time. The main benefit of using large batch sizes is that hardware accelerators
    like GPUs can process them efficiently (see [Chapter 19](ch19.html#deployment_chapter)),
    so the training algorithm will see more instances per second. Therefore, many
    researchers and practitioners recommend using the largest batch size that can
    fit in GPU RAM. There’s a catch, though: in practice, large batch sizes often
    lead to training instabilities, especially at the beginning of training, and the
    resulting model may not generalize as well as a model trained with a small batch
    size. In April 2018, Yann LeCun even tweeted “Friends don’t let friends use mini-batches
    larger than 32”, citing a [2018 paper](https://homl.info/smallbatch)⁠^([21](ch10.html#idm45720199898400))
    by Dominic Masters and Carlo Luschi which concluded that using small batches (from
    2 to 32) was preferable because small batches led to better models in less training
    time. Other research points in the opposite direction, however. For example, in
    2017, papers by [Elad Hoffer et al.](https://homl.info/largebatch)⁠^([22](ch10.html#idm45720199896896))
    and [Priya Goyal et al.](https://homl.info/largebatch2)⁠^([23](ch10.html#idm45720199894992))
    showed that it was possible to use very large batch sizes (up to 8,192) along
    with various techniques such as warming up the learning rate (i.e., starting training
    with a small learning rate, then ramping it up, as discussed in [Chapter 11](ch11.html#deep_chapter))
    and to obtain very short training times, without any generalization gap. So, one
    strategy is to try to using a large batch size, with learning rate warmup, and
    if training is unstable or the final performance is disappointing, then try using
    a small batch size instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed how to choose the activation function earlier in this chapter:
    in general, the ReLU activation function will be a good default for all hidden
    layers, but for the output layer it really depends on your task.'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, the number of training iterations does not actually need to
    be tweaked: just use early stopping instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The optimal learning rate depends on the other hyperparameters—especially the
    batch size—so if you modify any hyperparameter, make sure to update the learning
    rate as well.
  prefs: []
  type: TYPE_NORMAL
- en: For more best practices regarding tuning neural network hyperparameters, check
    out the excellent [2018 paper](https://homl.info/1cycle)⁠^([24](ch10.html#idm45720199885792))
    by Leslie Smith.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our introduction to artificial neural networks and their implementation
    with Keras. In the next few chapters, we will discuss techniques to train very
    deep nets. We will also explore how to customize models using TensorFlow’s lower-level
    API and how to load and preprocess data efficiently using the tf.data API. And
    we will dive into other popular neural network architectures: convolutional neural
    networks for image processing, recurrent neural networks and transformers for
    sequential data and text, autoencoders for representation learning, and generative
    adversarial networks to model and generate data.⁠^([25](ch10.html#idm45720199882240))'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The [TensorFlow playground](https://playground.tensorflow.org) is a handy neural
    network simulator built by the TensorFlow team. In this exercise, you will train
    several binary classifiers in just a few clicks, and tweak the model’s architecture
    and its hyperparameters to gain some intuition on how neural networks work and
    what their hyperparameters do. Take some time to explore the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The patterns learned by a neural net. Try training the default neural network
    by clicking the Run button (top left). Notice how it quickly finds a good solution
    for the classification task. The neurons in the first hidden layer have learned
    simple patterns, while the neurons in the second hidden layer have learned to
    combine the simple patterns of the first hidden layer into more complex patterns.
    In general, the more layers there are, the more complex the patterns can be.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Activation functions. Try replacing the tanh activation function with a ReLU
    activation function, and train the network again. Notice that it finds a solution
    even faster, but this time the boundaries are linear. This is due to the shape
    of the ReLU function.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The risk of local minima. Modify the network architecture to have just one hidden
    layer with three neurons. Train it multiple times (to reset the network weights,
    click the Reset button next to the Play button). Notice that the training time
    varies a lot, and sometimes it even gets stuck in a local minimum.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens when neural nets are too small. Remove one neuron to keep just
    two. Notice that the neural network is now incapable of finding a good solution,
    even if you try multiple times. The model has too few parameters and systematically
    underfits the training set.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What happens when neural nets are large enough. Set the number of neurons to
    eight, and train the network several times. Notice that it is now consistently
    fast and never gets stuck. This highlights an important finding in neural network
    theory: large neural networks rarely get stuck in local minima, and even when
    they do these local optima are often almost as good as the global optimum. However,
    they can still get stuck on long plateaus for a long time.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The risk of vanishing gradients in deep networks. Select the spiral dataset
    (the bottom-right dataset under “DATA”), and change the network architecture to
    have four hidden layers with eight neurons each. Notice that training takes much
    longer and often gets stuck on plateaus for long periods of time. Also notice
    that the neurons in the highest layers (on the right) tend to evolve faster than
    the neurons in the lowest layers (on the left). This problem, called the *vanishing
    gradients* problem, can be alleviated with better weight initialization and other
    techniques, better optimizers (such as AdaGrad or Adam), or batch normalization
    (discussed in [Chapter 11](ch11.html#deep_chapter)).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Go further. Take an hour or so to play around with other parameters and get
    a feel for what they do, to build an intuitive understanding about neural networks.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Draw an ANN using the original artificial neurons (like the ones in [Figure 10-3](#nn_propositional_logic_diagram))
    that computes *A* ⊕ *B* (where ⊕ represents the XOR operation). Hint: *A* ⊕ *B*
    = (*A* ∧ ¬ *B*) ∨ (¬ *A* ∧ *B*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it generally preferable to use a logistic regression classifier rather
    than a classic perceptron (i.e., a single layer of threshold logic units trained
    using the perceptron training algorithm)? How can you tweak a perceptron to make
    it equivalent to a logistic regression classifier?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why was the sigmoid activation function a key ingredient in training the first
    MLPs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name three popular activation functions. Can you draw them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you have an MLP composed of one input layer with 10 passthrough neurons,
    followed by one hidden layer with 50 artificial neurons, and finally one output
    layer with 3 artificial neurons. All artificial neurons use the ReLU activation
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the shape of the input matrix **X**?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the shapes of the hidden layer’s weight matrix **W**[*h*] and bias
    vector **b**[*h*]?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the shapes of the output layer’s weight matrix **W**[*o*] and bias
    vector **b**[*o*]?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the shape of the network’s output matrix **Y**?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the equation that computes the network’s output matrix **Y** as a function
    of **X**, **W**[*h*], **b**[*h*], **W**[*o*], and **b**[*o*].
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How many neurons do you need in the output layer if you want to classify email
    into spam or ham? What activation function should you use in the output layer?
    If instead you want to tackle MNIST, how many neurons do you need in the output
    layer, and which activation function should you use? What about for getting your
    network to predict housing prices, as in [Chapter 2](ch02.html#project_chapter)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is backpropagation and how does it work? What is the difference between
    backpropagation and reverse-mode autodiff?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP
    overfits the training data, how could you tweak these hyperparameters to try to
    solve the problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a deep MLP on the MNIST dataset (you can load it using `tf.keras.​data⁠sets.mnist.load_data()`).
    See if you can get over 98% accuracy by manually tuning the hyperparameters. Try
    searching for the optimal learning rate by using the approach presented in this
    chapter (i.e., by growing the learning rate exponentially, plotting the loss,
    and finding the point where the loss shoots up). Next, try tuning the hyperparameters
    using Keras Tuner with all the bells and whistles—save checkpoints, use early
    stopping, and plot learning curves using TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.html#idm45720205300576-marker)) You can get the best of both worlds
    by being open to biological inspirations without being afraid to create biologically
    unrealistic models, as long as they work well.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch10.html#idm45720205291952-marker)) Warren S. McCulloch and Walter Pitts,
    “A Logical Calculus of the Ideas Immanent in Nervous Activity”, *The Bulletin
    of Mathematical Biology* 5, no. 4 (1943): 115–113.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch10.html#idm45720205271104-marker)) They are not actually attached, just
    so close that they can very quickly exchange chemical signals.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch10.html#idm45720205264656-marker)) Image by Bruce Blaus ([Creative Commons
    3.0](https://creativecommons.org/licenses/by/3.0)). Reproduced from [*https://en.wikipedia.org/wiki/Neuron*](https://en.wikipedia.org/wiki/Neuron).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch10.html#idm45720205261008-marker)) In the context of machine learning,
    the phrase “neural networks” generally refers to ANNs, not BNNs.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch10.html#idm45720205254384-marker)) Drawing of a cortical lamination
    by S. Ramon y Cajal (public domain). Reproduced from [*https://en.wikipedia.org/wiki/Cerebral_cortex*](https://en.wikipedia.org/wiki/Cerebral_cortex).
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch10.html#idm45720205089040-marker)) Note that this solution is not unique:
    when data points are linearly separable, there is an infinity of hyperplanes that
    can separate them.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch10.html#idm45720204942848-marker)) For example, when the inputs are
    (0, 1) the lower-left neuron computes 0 × 1 + 1 × 1 – 3 / 2 = –1 / 2, which is
    negative, so it outputs 0\. The lower-right neuron computes 0 × 1 + 1 × 1 – 1
    / 2 = 1 / 2, which is positive, so it outputs 1\. The output neuron receives the
    outputs of the first two neurons as its inputs, so it computes 0 × (–1) + 1 ×
    1 - 1 / 2 = 1 / 2. This is positive, so it outputs 1.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch10.html#idm45720204921696-marker)) In the 1990s, an ANN with more than
    two hidden layers was considered deep. Nowadays, it is common to see ANNs with
    dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch10.html#idm45720204912752-marker)) David Rumelhart et al., “Learning
    Internal Representations by Error Propagation” (Defense Technical Information
    Center technical report, September 1985).
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch10.html#idm45720204883120-marker)) Biological neurons seem to implement
    a roughly sigmoid (*S*-shaped) activation function, so researchers stuck to sigmoid
    functions for a very long time. But it turns out that ReLU generally works better
    in ANNs. This is one of the cases where the biological analogy was perhaps misleading.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch10.html#idm45720204597504-marker)) Project ONEIROS (Open-ended Neuro-Electronic
    Intelligent Robot Operating System). Chollet joined Google in 2015, where he continues
    to lead the Keras project.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch10.html#idm45720204591712-marker)) PyTorch’s API is quite similar to
    Keras’s, so once you know Keras, it is not difficult to switch to PyTorch, if
    you ever want to. PyTorch’s popularity grew exponentially in 2018, largely thanks
    to its simplicity and excellent documentation, which were not TensorFlow 1.x’s
    main strengths back then. However, TensorFlow 2 is just as simple as PyTorch,
    in part because it has adopted Keras as its official high-level API, and also
    because the developers have greatly simplified and cleaned up the rest of the
    API. The documentation has also been completely reorganized, and it is much easier
    to find what you need now. Similarly, PyTorch’s main weaknesses (e.g., limited
    portability and no computation graph analysis) have been largely addressed in
    PyTorch 1.0\. Healthy competition seems beneficial to everyone.
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch10.html#idm45720204105760-marker)) You can also use `tf.keras.utils.plot_model()`
    to generate an image of your model.
  prefs: []
  type: TYPE_NORMAL
- en: '^([15](ch10.html#idm45720203290048-marker)) Heng-Tze Cheng et al., [“Wide &
    Deep Learning for Recommender Systems”](https://homl.info/widedeep), *Proceedings
    of the First Workshop on Deep Learning for Recommender Systems* (2016): 7–10.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch10.html#idm45720203287488-marker)) The short path can also be used
    to provide manually engineered features to the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch10.html#idm45720201814912-marker)) Keras models have an `output` attribute,
    so we cannot use that name for the main output layer, which is why we renamed
    it to `main_output`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch10.html#idm45720201799248-marker)) This is currently the default, but
    the Keras team is working on a new format that may become the default in upcoming
    versions, so I prefer to set the format explicitly to be future-proof.
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch10.html#idm45720200186752-marker)) Hyperband is actually a bit more
    sophisticated than successive halving; see [the paper](https://homl.info/hyperband)
    by Lisha Li et al., “Hyperband: A Novel Bandit-Based Approach to Hyperparameter
    Optimization”, *Journal of Machine Learning Research* 18 (April 2018): 1–52.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch10.html#idm45720199979472-marker)) Max Jaderberg et al., “Population
    Based Training of Neural Networks”, arXiv preprint arXiv:1711.09846 (2017).
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch10.html#idm45720199898400-marker)) Dominic Masters and Carlo Luschi,
    “Revisiting Small Batch Training for Deep Neural Networks”, arXiv preprint arXiv:1804.07612
    (2018).
  prefs: []
  type: TYPE_NORMAL
- en: '^([22](ch10.html#idm45720199896896-marker)) Elad Hoffer et al., “Train Longer,
    Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural
    Networks”, *Proceedings of the 31st International Conference on Neural Information
    Processing Systems* (2017): 1729–1739.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([23](ch10.html#idm45720199894992-marker)) Priya Goyal et al., “Accurate,
    Large Minibatch SGD: Training ImageNet in 1 Hour”, arXiv preprint arXiv:1706.02677
    (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([24](ch10.html#idm45720199885792-marker)) Leslie N. Smith, “A Disciplined
    Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch Size,
    Momentum, and Weight Decay”, arXiv preprint arXiv:1803.09820 (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([25](ch10.html#idm45720199882240-marker)) A few extra ANN architectures are
    presented in the online notebook at [*https://homl.info/extra-anns*](https://homl.info/extra-anns).
  prefs: []
  type: TYPE_NORMAL
