- en: '4 Generation pipeline: Generating contextual LLM responses'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrievers and retrieval methodologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmentation using prompt engineering techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation using LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic implementation of the RAG pipeline in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 3, we discussed the creation of the knowledge base, or the non-parametric
    memory of retrieval augmented generation (RAG)-based applications, via the indexing
    pipeline. To use this knowledge base for accurate and contextual responses, we
    need to create a generation pipeline that includes the steps of retrieval, augmentation,
    and generation.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter elaborates on the three components of the generation pipeline.
    We begin by discussing the retrieval process, which primarily involves searching
    through the embeddings stored in vector databases of the knowledge base and returning
    a list of documents that closely match the input query of the user. You will also
    learn about the concept of retrievers and a few retrieval algorithms. Next, we
    move to the augmentation step. At this point, it is also beneficial to understand
    different prompt engineering frameworks used with RAG. Finally, as part of the
    generation step, we discuss a few stages of the LLM life cycle, such as using
    foundation models versus supervised fine-tuning, models of different sizes, and
    open source versus proprietary models in the RAG context. In each of these steps,
    we also highlight the benefits and drawbacks of different methods.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be equipped with an understanding of the
    two foundational pipelines of a RAG system. You should also be ready to build
    a basic RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should
  prefs: []
  type: TYPE_NORMAL
- en: Know several retrievers used in RAG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get an understanding of augmentation using prompt engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn some details about how LLMs are used in the context of RAG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an end-to-end knowledge of setting up a basic RAG system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started with an overview of the generation pipeline before diving
    into each component.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Generation pipeline overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall the generation pipeline introduced in chapter 2\. When a user provides
    an input, the generation pipeline is responsible for providing the contextual
    response. The retriever searches for the most appropriate information from the
    knowledge base. The user question is augmented with this information and passed
    as input to the LLM for generating the final response. This process is illustrated
    in figure 4.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generation pipeline involves three processes: retrieval, augmentation,
    and generation. The retrieval process is responsible for fetching the information
    relevant to the user query from the knowledge base. Augmentation is the process
    of combining the fetched information with the user query. Generation is the last
    step, in which the LLM generates a response based on the augmented prompt. This
    chapter discusses these three processes in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval refers to the process of finding and extracting relevant pieces of
    information from a large corpus or knowledge base. As you saw in chapter 3, the
    information from various sources is parsed, chunked, and stored as embeddings
    in vector databases. These stored embeddings are also sometimes referred to as
    documents, and the knowledge base consists of several volumes of documents. Retrieval,
    essentially, is a search problem to find the documents that best match the input
    query.
  prefs: []
  type: TYPE_NORMAL
- en: Searching through the knowledge base and retrieving the right documents is done
    by a component called the *retriever*. In simple terms, retrievers accept a query
    as input and return a list of matching documents as output. This process is illustrated
    in figure 4.2\. You can imagine that retrieval is a crucial step since the quality
    of the retrieved information directly affects the quality of the output that will
    be generated.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F01_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1  Generation pipeline overview with the three components (i.e., retrieval,
    augmentation, and generation)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![A diagram of a user query'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F02_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2  A retriever searches through the knowledge base and returns the
    most relevant documents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We have already discussed embeddings in chapter 3 while building the indexing
    pipeline. Using embeddings, we can find documents that match the user query. Embeddings
    is one method in which retrieval can happen. There are other methods, too, and
    it is worth spending some time understanding different types of retrieval methods
    and the way they calculate the results.
  prefs: []
  type: TYPE_NORMAL
- en: This section on retrievers first discusses different retrieval algorithms and
    their significance in the context of RAG. In RAG systems, one or more retrieval
    methods can be used to build the retriever component. Next, we look at a few examples
    of prebuilt retrievers that can be used directly through a framework (e.g., LangChain).
    These retrievers are integrated with services such as databases, cloud providers,
    or third-party information sources. Finally, we will close this section by building
    a very simple retriever in LangChain using Python. We will continue to demonstrate
    with this example the augmentation and generation steps, too, so that we have
    a full implementation of the generation pipeline by the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note Chapter 3 discussed indexing and how to convert and store data in a numerical
    form that can be used to retrieve information later. You may recall we discussed
    embeddings at length in section 3.3\. It should be intuitive that since we stored
    the data in the form of embeddings, to fetch this data, we will also have to work
    on the search using embeddings. Therefore, the retrieval process is tightly coupled
    with the indexing process. Whatever we use to index, we will have to use to retrieve.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Progression of retrieval methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Information retrieval, or IR, is the science of searching. Whether you are
    searching for information in a document or for documents themselves, it falls
    under the gamut of information retrieval. IR has a rich history in computing,
    starting from Joseph Marie Jacquard’s invention of the Jacquard Loom, the first
    device that could read punched cards, back in the early 19th century. Since then,
    IR has evolved leaps and bounds from simple to highly sophisticated search and
    retrieval. *Boolean retrieval* is a simple keyword-based search (like the one
    you encounter when you press CTRL/CMD + F on your browser or word processor) where
    Boolean logic is used to match documents with queries based on the absence or
    presence of the words. Documents are retrieved if they contain the exact terms
    in the query, often combined with AND, NOT, and OR operators. *Bag of Words* *(BoW)*
    was used quite often in the early days of NLP. It creates a vocabulary of all
    the words in the documents as a vector indicating the presence or absence of each
    word. Consider two sentences: “The cat sat on the mat” and “The cat in the hat.”
    The vocabulary is `[``"``the``"``,` `"``cat``"``,` `"``in``"``,` `"``hat``"``,`
    `"``on``"``,` `"``mat``"``]` and the first sentence is represented as a vector
    `[2,` `1,` `1,` `1,` `0,` `0]`, while the one is `[2,` `1,` `0,` `0,` `1, 1]`.
    While simple, it ignores the context, meaning, and the order of words.'
  prefs: []
  type: TYPE_NORMAL
- en: Some of these, although popular in ML and IR space, don’t make sense in the
    context of RAG for a variety of reasons. For our purpose, we focus on a few of
    the popular retrieval techniques that have been used in RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Term Frequency-Inverse Document Frequency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Term Frequency–Inverse Document Frequency (TF-IDF) is a statistical measure
    used to evaluate the importance of a word in a document relative to a collection
    of documents (corpus). It assigns higher weights to words that appear frequently
    in a document but infrequently across the corpus. Figure 4.3 illustrates how TF-IDF
    is calculated for a unigram search term.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F03_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3  Calculating TF-IDF to rank documents based on search terms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'LangChain also provides an abstract implementation of TF-IDF using retrievers
    from `langchain_community`, which, in turn, uses `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: TF-IDF not only can be used for unigrams, but also for phrases (n-grams). However,
    even TF-IDF improves on simpler search methods by emphasizing unique words, it
    still lacks context and word-order consideration, making it less suitable for
    complex tasks like RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Best Match 25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Best Match 25 (BM25) is an advanced probabilistic model used to rank documents
    based on the query terms appearing in each document. It is part of the family
    of probabilistic information retrieval models and is considered an advancement
    over the classic TF-IDF model. The improvement that BM25 brings is that it adjusts
    for the length of the documents so that longer documents do not unfairly get higher
    scores. Figure 4.4 illustrates the BM25 calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F04_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4  BM25 also considers the length of the documents.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Like TF-IDF, LangChain also has an abstract implementation of BM25 (Okapi BM25,
    specifically) using the `rank_bm25` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For long queries instead of single keywords, the BM25 value is calculated for
    each word in the query, and the final BM25 value for the query is a summation
    of the values for all the words. BM25 is a powerful tool in traditional IR, but
    it still doesn’t capture the full semantic meaning of queries and documents required
    for RAG applications. BM25 is generally used in RAG for quick initial retrieval,
    and then a more powerful retriever is used to re-rank the results. We will learn
    about re-ranking later in chapter 6, when we discuss advanced strategies for RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Static word embeddings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Static embeddings such as Word2Vec and GloVe represent words as dense vectors
    in a continuous vector space, capturing semantic relationships based on context.
    For instance, “king” − “man” + “woman” approximates “queen.” These embeddings
    can capture nuances such as similarity and analogy, which BoW, TF-IDF, and BM25
    miss. However, while they provide a richer representation, they still lack full
    contextual understanding and are limited in handling polysemy (words with multiple
    meanings). The term *static* here highlights that the vector representation of
    words does not change with the context of the word in the input query.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual embeddings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generated by models such as BERT or OpenAI’s text embeddings, contextual embeddings
    produce high-dimensional, context-aware representations for queries and documents.
    These models, based on transformers, capture deep semantic meanings and relationships.
    For example, a query about “apple” will retrieve documents discussing apple the
    fruit, or Apple the technology company, depending on the input query. Figure 4.5
    illustrates the difference between static and contextual embeddings. Contextual
    embeddings represent a significant advancement in IR, providing the context and
    understanding necessary for RAG tasks. Despite being computationally intensive,
    contextual embeddings are the most widely used retrievers in RAG. Examples of
    embedding models discussed in section 3.3.2 are contextual embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Methods such as TF-IDF and BM25 use frequency-based calculations to rank documents.
    In embeddings (both static and contextual), ranking is done based on a similarity
    score. Similarity is popularly calculated using the cosine of the angle between
    document vectors. We discussed cosine similarity calculation in section 3.3.3\.
    Figure 4.6 illustrates the process of retrieval using embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Other retrieval methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While the discussed methods are most popular in the discourse, other methods
    are also available. These methods represent more recent developments and specialized
    approaches and are good to refer to if you want to dive deeper into the world
    of information retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learned sparse retrieval*—Generates sparse, interpretable representations
    using neural networks (examples: SPLADE, DeepCT, and DocT5Quer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dense retrieval*—Encodes queries and documents as dense vectors for semantic
    matching (examples: dense passage retriever [DPR], ANCE, RepBERT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A diagram of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F05_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5  Static vs. contextual embeddings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Hybrid retrieva**l*—Combines sparse and dense methods for balanced efficiency
    and effectiveness (examples: ColBERT, COIL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cross-encoder retrieva**l*—Directly compares query-document pairs using transformer
    models (example: BERT-based re-rankers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph-based retrieva**l*—Uses graph structures to model relationships between
    documents (examples: TextGraphs, graph neural networks for IR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quantum-inspired retrieva**l*—Applies quantum computing principles to information
    retrieval (example: quantum language models [QLM])'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural IR model**s*—Encompass various neural network-based approaches to information
    retrieval (examples: NPRF [neural PRF], KNRM [Kernel-based Neural Ranking Model])'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A diagram of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F06_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6  Similarity calculation and results ranking in embeddings-based retrieval
    technique
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Table 4.1 notes the weaknesses and strengths of different retrievers. While
    contextual embeddings are the only ones you need to know to get started with RAG,
    it is useful to get familiar with other retrievers for further exploration and
    for cases where you want to improve retriever performance. As we discussed, the
    implementation of TF-IDF using the `scikit-learn` retriever and BM25 using `rank_bm25`
    retriever in LangChain, there are many others available that use one of the mentioned
    methodologies. We will look at some of the popular ones in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 Comparison of different retrieval techniques for RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Technique | Key feature | Strengths | Weaknesses | Suitability for RAG |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Boolean retrieval | Exact matching with logical operators | Simple, fast,
    and precise | Limited relevance ranking; no partial matching | Low: Too rigid
    |'
  prefs: []
  type: TYPE_TB
- en: '| BoW | Unordered word frequency counts | Simple and intuitive | Ignores word
    order and context | Low: Lacks semantic understanding |'
  prefs: []
  type: TYPE_TB
- en: '| TF-IDF | Term weighting based on document and corpus frequency | Improved
    relevance ranking over BoW | Still ignores semantics and word relationships |
    Low–medium: Better than BoW but limited; used in hybrid retrieval |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | Advanced ranking function with length normalization | Robust performance;
    industry standard | Limited semantic understanding | Medium: Good baseline for
    simple RAG; used in hybrid retrieval. |'
  prefs: []
  type: TYPE_TB
- en: '| Static embeddings | Fixed dense vector representations | Captures some semantic
    relationships | Context-independent; limited in polysemy handling | Medium: Introduces
    basic semantics |'
  prefs: []
  type: TYPE_TB
- en: '| Contextual embeddings | Context-aware dense representations | Rich semantic
    understanding; handles polysemy | Computationally intensive | High: Excellent
    semantic capture |'
  prefs: []
  type: TYPE_TB
- en: '| Learned sparse retrievers | Neural-network-generated sparse representations
    | Efficient, interpretable, and has some semantic understanding | May miss some
    semantic relationships | High: Balances efficiency and semantics |'
  prefs: []
  type: TYPE_TB
- en: '| Dense retrievers | Dense vector matching for queries and documents | Strong
    semantic matching | Computationally intensive; less interpretable | High: Excellent
    for semantic search in RAG |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid retrievers | Combination of sparse and dense methods | Balances efficiency
    and effectiveness | Complex to implement and tune | High: Versatile for various
    RAG needs |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-encoder retrievers | Direct query-document comparison | Very accurate
    relevance assessment | Extremely computationally expensive | Medium–high: Great
    for reranking in RAG |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-based retrievers | Graph structure for document relationships | Captures
    complex relationships in data | Can be complex to construct and query | Medium–high:
    Good for structured data in RAG |'
  prefs: []
  type: TYPE_TB
- en: '| Quantum-inspired retrievers | Quantum computing concepts in IR | Potential
    for handling complex queries | Emerging field; practical benefits not fully proven
    | Low–medium: Potentially promising but not mature |'
  prefs: []
  type: TYPE_TB
- en: '| Neural IR models | Various neural network approaches to IR | Flexible; can
    capture complex patterns | Often require large training data; can be black-box
    | High: Adaptable to various RAG scenarios |'
  prefs: []
  type: TYPE_TB
- en: 4.2.2 Popular retrievers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Developers can build their retrievers based on one or a combination of multiple
    retrieval methodologies. Retrievers are used not just in RAG but in a variety
    of search-related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For RAG, LangChain provides many integrations where the algorithms such as TF-IDF,
    embeddings and similarity search, and BM25 have been abstracted as retrievers
    for developers to use. We have already seen the ones for TF-IDF and BM25\. Some
    of the other popular retrievers are described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Vector stores and databases as retrievers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vector stores can act as the retrievers, taking away the responsibility from
    the developer to convert the query vector into embeddings by calculating similarity
    and ranking the results. FAISS is typically used in tandem with a contextual embedding
    model for retrieval. Other vector DBs such as PineCone, Milvus, and Weaviate provide
    hybrid search functionality by combining dense retrieval methods such as embeddings
    and sparse methods such as BM25 and SPLADE.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud providers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cloud providers Azure, AWS, and Google also offer their retrievers. Integration
    with Amazon Kendra, Azure AI Search, AWS Bedrock, Google Drive, and Google Vertex
    AI Search provides developers with infrastructure, APIs, and tools for information
    retrieval of vector, keyword, and hybrid queries at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Web information resources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Connections to information resources such as Wikipedia, Arxiv, and AskNews provide
    optimized search and retrieval from these sources. You can check these retrievers
    and more in the official LangChain documentation ([https://mng.bz/gm4R](https://mng.bz/gm4R))
  prefs: []
  type: TYPE_NORMAL
- en: This was a brief introduction to the world of retrievers. If you found the information
    slightly complex, you can always revisit it. At this stage, the understanding
    of contextual embeddings will suffice. Contextual embeddings are the most popular
    technique for basic RAG pipelines, and we will now create a simple retriever using
    OpenAI embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 A simple retriever implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we move to the next step of the generation pipeline, let’s look at a
    simple example of a retriever. In chapter 3, we were working on indexing the Wikipedia
    page for the 2023 Cricket World Cup. If you recall, we used embeddings from OpenAI
    to encode the text and used FAISS as the vector index to store the embeddings.
    We also stored the FAISS index in a local directory. Let’s reuse this index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This `similarity_search ()` function returns a list of matching documents ordered
    by a score. This score is a quantification of the similarity between the query
    and the document and is hence called the similarity score. In this example, the
    vector index’s inbuilt similarity search feature was used for retrieval. As one
    of the retrievers we discussed in section 4.2.2, the vector store itself acted
    as the retriever. `K=2` tells the function to retrieve the top two documents.
    This is the most basic implementation of a retriever in the generation pipeline
    of a RAG system, and the retrieval method is enabled by embeddings. We used the
    text-embedding-3-small from OpenAI. FAISS calculated the similarity score based
    on these embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Retrievers are the backbone of RAG systems. The quality of the retriever has
    a great bearing on the quality of the generated output. In this section, you learned
    about vanilla retrieval methods. Multiple strategies are used when designing production-grade
    systems. We will read about these advanced strategies in chapter 6\. Now that
    we have gained an understanding of the retrievers, we will move on to the next
    important step—augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A retriever fetches the information (or documents) that are most relevant to
    the user query. But, what next? How do we use this information? The answer is
    quite intuitive. If you recall the discussion in chapter 1, the input to an LLM
    is a natural language prompt. This information fetched by the retriever should
    also be sent to the LLM in the form of a natural language prompt. This process
    of combining the user query and the retrieved information is called *augmentation*.
  prefs: []
  type: TYPE_NORMAL
- en: The augmentation step in RAG largely falls under the discipline of prompt engineering.
    Prompt engineering can be defined as the technique of giving instructions to an
    LLM to attain a desired outcome. The goal of prompt engineering is to construct
    the prompts to achieve accuracy and relevance in the LLM responses to the desired
    outcome(s). At the first glance, augmentation is quite simple—just add the retrieved
    information to the query. However, some nuanced augmentation techniques help improve
    the quality of the generated results. See figure 4.7 for an example of simple
    augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F07_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7  Simple augmentation combines the user query with retrieved documents
    to send to the LLM.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4.3.1 RAG prompt engineering techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt engineering as a discipline has, sometimes, been dismissed as being too
    simple to be called engineering. You may have heard the phrase, “English is the
    new programming language.” Interaction with LLMs is indeed in natural language.
    However, what is also true is that the principles of programming are not the language
    in which code is written but the logic in which the machine is instructed. With
    that in mind, let’s examine different logical approaches that can be taken to
    augment the user query with the retrieved information.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand a simple augmentation technique, let’s revisit chapter 1\. Recall
    our example of “Who won the 2023 Cricket World Cup?” We copied an excerpt from
    the Wikipedia article. This excerpt is the retrieved information. We then added
    this information to the prompt and provided an extra instruction—“Answer only
    based on the context provided below.” Figure 4.8 illustrates this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'By adding this instruction, we have set up our generation to focus only on
    the provided information and not on LLM’s internal knowledge (or parametric knowledge).
    This is a simple augmentation technique that is also referred to as *contextual
    prompting*. Please note that the instruction can be given in any linguistic construct.
    For example, we could have added the instruction at the beginning of the prompt
    as, “Given the context below, answer the question, Who won the 2023 Cricket World
    Cup. Information: <Wikipedia excerpt>.” We can also reiterate the instruction
    at the end of the prompt—“Remember to answer only based on the context provided
    and not from any other source.'
  prefs: []
  type: TYPE_NORMAL
- en: Controlled generation prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, the information might not be present in the retrieved document. This
    happens when the documents in the knowledge base do not have any informationrelevant
    to the user query. The retriever might still fetch some documents that are the
    closest to the user query. In these cases, the chances of hallucination increase
    because the LLM will still try to follow the instructions for answering the question.
    To avoid this scenario, an additional instruction is added, which tells the LLM
    not to answer if the retrieved document does not have proper information to answer
    the user question (something like, “If the question cannot be answered based on
    the provided context, say I don’t know.”). In the context of RAG, this technique
    is particularly valuable because it ensures that the model’s responses are grounded
    in the retrieved information. If the relevant information hasn’t been retrieved
    or isn’t present in the knowledge base, the model is instructed to acknowledge
    this lack of information rather than attempting to generate a potentially incorrect
    answer.”
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a chat'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F08_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8  Information is augmented to the original question with an added
    instruction.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Few-shot prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It has been observed that while generating responses, LLMs adhere quite well
    to the examples provided in the prompt. If you want the generation to be in a
    certain format or style, it is recommended to provide a few examples. In RAG,
    while providing the retrieved information in the prompt, we can also specify certain
    examples to help guide the generation in the way we need the retrieved information
    to be used. This technique is called *few-shot prompting*. Here “shot” refers
    to the examples given in the prompt. Figure 4.9 illustrates a prompt that includes
    two examples with the question.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a text box'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F09_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9  Example of few-shot prompting in the context of RAG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You might come across terms such as *one-shot prompting* or two-shot prompting,
    which replaces the word “few” with the number of examples given. Conversely, when
    no example is given, and the LLM is expected to answer correctly, the technique
    is also called *zero-shot prompting*.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of thought prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It has been observed that the introduction of intermediate reasoning steps improves
    the performance of LLMs in tasks requiring complex reasoning, such as arithmetic,
    common sense, and symbolic reasoning. The same can be applied in the context of
    RAG. This is called *chain-of-thought*, or CoT, *prompting*. In figure 4.10, I
    asked ChatGPT to analyze the performance of two teams based on the retrieved information.
  prefs: []
  type: TYPE_NORMAL
- en: '![A paper with text and numbers'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F10_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10  Chain-of-thought (CoT) prompting for reasoning tasks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The CoT prompting approach can also be combined with the few-shot prompting
    technique, where a few examples of reasoning are provided before the final question.
    Creating these examples is a manually intensive task. In auto-CoT, the examples
    are also created using an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Other advanced prompting techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Prompt engineering is becoming an increasingly intricate discipline. Ongoing
    research constantly presents new improvements in prompting techniques. To dive
    deeper into prompt engineering, let’s check out some of the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Self-consistency*—While CoT uses a single reasoning chain in CoT prompting,
    self-consistency aims to sample multiple diverse reasoning paths and use their
    respective generations to arrive at the most consistent answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generated knowledge promptin**g*—This technique explores the idea of prompt-based
    knowledge generation by dynamically constructing relevant knowledge chains, using
    models’ latent knowledge to strengthen reasoning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tree-of-thoughts promptin**g*—This technique maintains an explorable tree
    structure of coherent intermediate thought steps aimed at solving problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automatic reasoning and tool use* (ART)—The ART framework automatically interleaves
    model generations with tool use for complex reasoning tasks. ART employs demonstrations
    to decompose problems and integrate tools without task-specific scripting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automatic prompt engineer* (APE)—The APE framework automatically generates
    and selects optimal instructions to guide models. It uses an LLM to synthesize
    candidate prompt solutions for a task based on output demonstrations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Active promp**t*—Active-prompt improves CoT methods by dynamically adapting
    language models to task-specific prompts through a process involving query, uncertainty
    analysis, human annotation, and enhanced inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ReAct prompting*—ReAct integrates LLMs for concurrent reasoning traces and
    task-specific actions, improving performance by interacting with external tools
    for information retrieval. When combined with CoT, it optimally utilizes internal
    knowledge and external information, enhancing the interpretability and trustworthiness
    of LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recursive promptin**g***—**Recursive prompting breaks down complex problems
    into subproblems, solving them by sequentially using prompts. This method aids
    compositional generalization in tasks such as math problems or question answering,
    with the model building on solutions from previous steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table 4.2 summarizes different prompting techniques. Prompt engineering for
    augmentation is an evolving discipline. It is important to note that there is
    a lot of scope for creativity in writing prompts for RAG applications. Efficient
    prompting has a significant effect on the generated output. The kind of prompts
    you use will depend a lot on your use case and the nature of the information in
    the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2  Comparison of prompting techniques for augmentation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Technique | Description | Key advantage | Best use case | Complexity |'
  prefs: []
  type: TYPE_TB
- en: '| Contextual prompting | Adds retrieved information to the prompt with instructions
    to focus on the provided context | Ensures focus on relevant information | General
    RAG queries | Low |'
  prefs: []
  type: TYPE_TB
- en: '| Controlled generation prompting | Instructs the model to say “I don’t know”
    when information is not available | Reduces hallucination risk | When accuracy
    is critical | Low |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot prompting | Provides examples in the prompt to guide response format
    and style | Improves output consistency and format adherence | When a specific
    output format is required | Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Chain-of-thought (CoT) prompting | Introduces intermediate reasoning steps
    | Improves performance on complex reasoning tasks | Complex queries requiring
    step-by-step analysis | Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Self-consistency | Samples multiple diverse reasoning paths | Improves answer
    consistency and accuracy | Tasks with multiple possible reasoning approaches |
    High |'
  prefs: []
  type: TYPE_TB
- en: '| Generated knowledge prompting | Dynamically constructs relevant knowledge
    chains | Uses the model’s latent knowledge | Tasks requiring broad knowledge application
    | High |'
  prefs: []
  type: TYPE_TB
- en: '| Tree-of-thoughts prompting | Maintains an explorable tree structure of thought
    steps | Allows for more comprehensive problem-solving | Complex, multistep problem
    solving | High |'
  prefs: []
  type: TYPE_TB
- en: '| Automatic reasoning and tool use (ART) | Interleaves model generations with
    tool use | Enhances problem decomposition and tool integration | Tasks requiring
    external tool use | Very High |'
  prefs: []
  type: TYPE_TB
- en: '| Automatic prompt engineer (APE) | Automatically generates and selects optimal
    instructions | Optimizes prompts for specific tasks | Prompt optimization for
    complex tasks | Very High |'
  prefs: []
  type: TYPE_TB
- en: '| Active prompt | Dynamically adapts LMs to task-specific prompts | Improves
    task-specific performance | Tasks requiring adaptive prompting | High |'
  prefs: []
  type: TYPE_TB
- en: '| ReAct prompting | Integrates reasoning traces with task-specific actions
    | Improves performance and interpretability | Tasks requiring both reasoning and
    action | High |'
  prefs: []
  type: TYPE_TB
- en: '| Recursive prompting | Breaks down complex problems into subproblems | Aids
    in compositional generalization | Complex, multistep problems | High |'
  prefs: []
  type: TYPE_TB
- en: We have already built a simple retriever in the previous section. We will now
    execute augmentation with a simple contextual prompt with controlled generation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 A simple augmentation prompt creation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In section 4.2.3, we were able to implement a FAISS-based retriever using OpenAI
    embeddings. We will now make use of this retriever and create the augmentation
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the augmentation step complete, we are now ready to send the prompt to
    the LLM for the generation of the desired outcome. You will now learn how LLMs
    generate text and the nuances of generation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generation is the final step of this pipeline. While LLMs may be used in any
    of the previous steps, the generation step relies completely on the LLM. The most
    popular LLMs are the ones being developed by OpenAI, Anthropic, Meta, Google,
    Microsoft, and Mistral, among other developers. While text generation is the core
    capability of LLMs, we are now seeing multimodal models that can handle images
    and audio along with text. Simultaneously, researchers are developing faster and
    smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the factors that can help choose a language
    model for your RAG system. We will then continue with our example of the retriever
    and augmented prompt we have built so far and complete it by adding the generation
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Categorization of LLMs and suitability for RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of June 2024, there are over a hundred LLMs available to use, and new ones
    are coming out every week. How do we decide then which LLM to choose for our RAG
    system? To show you the decision-making process, let’s discuss three themes under
    which we can broadly categorize LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: How they have been trained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How they can be accessed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss the LLMs under these themes and understand the factors that
    may influence the LLM choice for RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Original vs. fine-tuned models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training an LLM takes massive amounts of data and computational resources. LLMs
    training is done through an unsupervised learning process. All modern LLMs are
    autoregressive models and are trained to generate the next token in a sequence.
    These massive pre-trained LLMs are also called *foundation models.*
  prefs: []
  type: TYPE_NORMAL
- en: The question that you may ask is, if LLMs just predict the next tokens in a
    sequence, how are we able to ask questions and chat with these models? The answer
    is in what we call *supervised fine-tuning*, or *SFT.*
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning is a process used to adapt a pre-trained language model
    for specific tasks or behaviors such as question-answering or chat. It involves
    further training a pre-trained foundation model on a labeled dataset, where the
    model learns to map inputs to specific desired outputs. You start with a pre-trained
    model, prepare a labelled dataset for the target task, and train the model on
    this dataset, which adjusts the model parameters to perform better on the target
    task. Figure 4.11 gives an overview of the SFT process.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a training program'
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F11_Kimothi.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11  Supervised fine-tuning is a classification mode-training process.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While foundation models generalize well for a wide array of tasks, there are
    several use cases where the need for a fine-tuned model arises. Domain adaptation
    for specialized fields such as law and healthcare, task specific optimization
    such as classification and NER (named entity recognition), and conversational
    AI, personalization are some use cases where you may observe a fine-tuned model
    performing better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in the context of RAG, some criteria should be considered, while
    choosing between a foundation model and a fine-tuning one:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Domain specificit**y*—Foundation models have broader knowledge and can handle
    a wider range of topics and queries for general-purpose RAG systems. If your RAG
    application is specialized (say, dealing with patient records or instruction manuals
    for heavy machinery), you may find that fine-tuning the model for specific domains
    may improve performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval integratio**n*—If you observe that a foundation model you are using
    is not integrating the retrieved information well, a fine-tuned model trained
    to better utilize information can lead to better quality of generations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment spee**d*—A foundation model can be quickly deployed since there
    is no additional training required. To fine-tune a model, you will need to spend
    time in gathering training data and the actual training of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Customization of response**s*—For generating results in a specific format
    or custom-style elements such as tone or vocabulary, a fine-tuned model may result
    in better adherence to the requirements compared to foundation models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resource efficienc**y*—Fine-tuning a model requires more storage and computational
    resources. Depending on the scale of deployment, the costs may be higher for a
    fine-tuned model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ethical alignmen**t*—A fine-tuned model allows for better control over the
    responses in adherence to ethical guidelines and even certain privacy aspects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of the criteria is presented in table 4.3.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.3 Criteria for choosing between foundation and fine-tuned models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Criteria | Better suitability | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| Domain specificity | Fine-tuned models | Better performance for specialized
    applications (e.g., patient records and instruction manuals) |'
  prefs: []
  type: TYPE_TB
- en: '| Retrieval integration | Fine-tuned models | Can be trained to better utilize
    retrieved information |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment speed | Foundation models | Quicker deployment with no additional
    training required |'
  prefs: []
  type: TYPE_TB
- en: '| Customization of responses | Fine-tuned models | Better adherence to specific
    format, style, tone, or vocabulary requirements |'
  prefs: []
  type: TYPE_TB
- en: '| Resource efficiency | Foundation models | Requires less storage and computational
    resources |'
  prefs: []
  type: TYPE_TB
- en: '| Ethical alignment | Fine-tuned models | Allows better control over responses
    to ethical guidelines and privacy |'
  prefs: []
  type: TYPE_TB
- en: Fine-tuned models give better control over your RAG systems, but they are costly.
    There’s also a risk of overreliance on retrieval and a potential tradeoff between
    RAG performance and inherent LLM language abilities. Therefore, whether to use
    a foundation model or fine-tuning one depends on the improvements you are targeting,
    availability of data, cost, and other tradeoffs. The general recommendation is
    to start experimenting with a foundation model and then progress to supervised
    fine-tuning for performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Open source vs. proprietary models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Software development and distribution are represented by two fundamentally
    different approaches: open versus proprietary software. The world of LLMs is no
    different. Some LLM developers such as Meta and Mistral have made the model weights
    public to foster collaboration and community-driven innovation. In contrast, pioneers
    such as OpenAI, Anthropic, and Google have kept the models closed, offering support,
    managed services, and better user experience.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For RAG systems, open source models provide the flexibility of customization,
    deployment method, and transparency, but warrant the need for the necessary infrastructure
    to maintain the models. Proprietary model providers might be costlier for high
    volumes but provide regular updates, ease of use, scalability, and faster development,
    among other things. Some proprietary model providers such as OpenAI have prebuilt
    RAG capabilities. Your choice of the type of model you choose may depend on some
    of the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Customizatio**n*—Open source LLMs are generally considered better for customizations
    such as deep integration with custom retrieval mechanisms. A better control over
    fine-tuning is also something that open source LLMs allow for. Customization of
    proprietary models is limited to API capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ease of us**e*—Proprietary models, however, are much easier to use. Some of
    the models such as OpenAI, Cohere, and similar offer optimized, prebuilt RAG solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment flexibilit**y*—Open source models can be deployed according to
    your preference (private cloud, on-premises), while proprietary models are managed
    by the providers. This also has a bearing on data security and privacy. Most proprietary
    model providers are now offering multiple deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: options.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Cos**t*—Open source LLMs may come with upfront infrastructure costs, while
    proprietary models are priced based on usage. Long-term costs and query volumes
    are considerations to choose between open source and proprietary models. Large-scale
    deployments may favor the use of open source models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between open source and proprietary models for RAG depends on factors
    such as the scale of deployment, specific domain requirements, integration needs,
    and the importance of customization in the retrieval and generation process. Apart
    from these, the need for knowledge updates, transparency, scalability, the structure
    of data, compliance, and the like will determine the choice of the model. A summary
    of the discussion is presented in table 4.4
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.4 Criteria for choosing between open source and proprietary models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Criteria | Better suitability | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| Customization | Open source | Allows deeper integration with custom retrieval
    mechanisms and better control over fine-tuning |'
  prefs: []
  type: TYPE_TB
- en: '| Ease of use | Proprietary | Offers optimized, prebuilt RAG solutions and
    are generally easier to use |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment flexibility | Open source | Can be deployed on private cloud or
    on-premises, offering more options |'
  prefs: []
  type: TYPE_TB
- en: '| Cost for large-scale deployment | Open source | May be more cost-effective
    for large-scale deployments despite upfront infrastructure costs |'
  prefs: []
  type: TYPE_TB
- en: '| Data security and privacy | Open source | Offers more control over data,
    though some private models now offer various deployment options |'
  prefs: []
  type: TYPE_TB
- en: '| Regular updates and support | Proprietary | Typically provides regular updates
    and better support |'
  prefs: []
  type: TYPE_TB
- en: A hybrid approach is also not ruled out. At a PoC stage, a proprietary model
    may make sense for quick experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of popular proprietary models:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT series by OpenAI ([https://platform.openai.com/docs/models](https://platform.openai.com/docs/models))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claude series by Anthropic ([https://www.anthropic.com/claude](https://www.anthropic.com/claude))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemini series by Google ([https://mng.bz/eBnJ](https://mng.bz/eBnJ))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Command R series by Cohere ([https://cohere.com/command](https://cohere.com/command))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of open source models are
  prefs: []
  type: TYPE_NORMAL
- en: Llama series by Meta ([https://llama.meta.com/](https://llama.meta.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mistral ([https://docs.mistral.ai/getting-started/models/](https://docs.mistral.ai/getting-started/models/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model sizes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLMs come in various sizes, typically measured by the number of parameters they
    contain. The size of the model greatly affects the capabilities along with the
    resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Larger models have several billion, even trillions, of parameters. These models
    exhibit superior performance in reasoning abilities, and language understanding,
    and have broader knowledge. They can generate more coherent text, and their responses
    are contextually more accurate. However, these larger models have significantly
    high computation, storage, and energy requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Smaller models with parameter sizes in millions or a few billion offer benefits
    such as faster inference times, lower resource usage, and easier deployment on
    edge devices or resource constrained environments. Researchers and developers
    continue to explore methods to achieve large-model performance with smaller and
    more efficient architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a RAG system, the following should be assessed:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Resource constraints*—Small models have a much lower resource usage. Lightweight
    RAG applications with faster inference can be built with smaller'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Reasoning capability*—On the other spectrum of resource constraints is the
    language-processing ability of the model. Large models are better suited for complex
    reasoning tasks and can deal with ambiguity in the retrieved information. Smaller
    models, therefore, will rely heavily on the quality of retrieved information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment options*—The size of large models makes it difficult to deploy
    on-edge devices. This is a flexibility that smaller models provide, bringing RAG
    applications to a wide range of devices and environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Context handling*—Large models may be better at integrating multiple pieces
    of retrieved information in RAG systems since they have longer context windows.
    Large models are also better at handling diverse queries, while small models struggle
    with out-of-domain queries. Large models might perform better in RAG systems with
    diverse or unpredictable query types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, most RAG applications are built on large models. However, smaller
    models make more sense in the long-term adoption and application of the technology.
    The various factors are summarized in table 4.5
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.5 Criteria for choosing between small and large models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Criteria | Better suitability | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| Resource constraints | Small models | Lower resource usage; suitable for
    lightweight RAG applications |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning capability | Large models | Better for complex reasoning tasks
    and handling ambiguity in retrieved information |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment options | Small models | More flexible; can be deployed on edge
    devices and resource-constrained environments |'
  prefs: []
  type: TYPE_TB
- en: '| Context handling | Large models | Better at integrating multiple pieces of
    retrieved information; longer context windows |'
  prefs: []
  type: TYPE_TB
- en: '| Query diversity | Large models | Handle diverse and unpredictable query types
    better |'
  prefs: []
  type: TYPE_TB
- en: '| Inference speed | Small models | Faster inference times; suitable for applications
    requiring quick responses |'
  prefs: []
  type: TYPE_TB
- en: 'Examples of popular small language models are:'
  prefs: []
  type: TYPE_NORMAL
- en: Phi-3 by Microsoft (h[ttps://azure.microsoft.com/en-us/products/phi-3](https://azure.microsoft.com/en-us/products/phi-3))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemma by Google ([https://ai.google.dev/gemma](https://ai.google.dev/gemma))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of the LLM is a core consideration in your RAG system that requires
    close attention and iterations. The performance of your system may require experimenting
    and adapting your choice of the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: The list of LLMs has become almost endless. What this means for developers and
    businesses is that the technology has truly been democratized. While all LLMs
    have their unique propositions and architecture, for practical applications, there
    are a wide array of choices available. While simple RAG applications may rely
    on a single LLM provider, for more complex applications, a multi-LLM strategy
    may be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented a simple retriever and created an augmented prompt. In the
    last section of this chapter, we round up the pipeline by creating the generation
    step.
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.2 Completing the RAG pipeline: Generation using LLMs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have built a simple retriever using FAISS and OpenAI embeddings, and we
    created a simple augmented prompt. Now we will use OpenAI’s latest model, GPT-4o,
    to generate the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And there it is. We have built a generation pipeline, albeit a very simple one.
    It can now fetch information from the knowledge base and generate an answer pertinent
    to the question asked and rooted in the knowledge base. Try asking a different
    question to see how well the pipeline generalizes.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered all three steps—retrieval, augmentation, and generation—of
    the generation pipeline. With the knowledge of the indexing pipeline (covered
    in chapter 3) and the generation pipeline, you are now all set to create a basic
    RAG system. What we have discussed so far can be termed a *naïve RAG implementation*.
    Naïve RAG can be marred by inaccuracies. It can be inefficient in retrieving and
    ranking information correctly. The LLM can ignore the retrieved information and
    still hallucinate. To discuss and address these challenges, in chapter 6, we examine
    advanced strategies that allow for more complex and better-performing RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: But before that, the question of evaluating the system arises. Is it generating
    the responses on the expected lines? Is the LLM still hallucinating? Before trying
    to improve the performance of the system, we need to be able to measure and benchmark
    it. That is what we will do in chapter 5\. We will look at the evaluation metrics
    and the popular RAG benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval is the process of finding relevant information from the knowledge
    base based on a user query. It is a search problem to match documents with input
    queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The popular retrieval methods for RAG include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TF-IDF (Term Frequency-Inverse Document Frequency**)*—Statistical measure
    of word importance in a document relative to a corpus. It can be implemented using
    LangChain’s TFIDFRetriever.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BM25 (Best Match 25**)*—Advanced probabilistic model, an improvement over
    TF-IDF. It adjusts for document length and can be implemented using Lang­Chain’s
    BM25Retriever.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Static word embedding**s*—Represent words as dense vectors (e.g., Word2Vec,
    GloVe) and capture semantic relationships but lack full contextual under­standing.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contextual embedding**s*—Produced by models like BERT or OpenAI’s text embeddings.
    They provide context-aware representations and are most widely used in RAG, despite
    being computationally intensive.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advanced retrieval method**s*—They include learned sparse retrieval, dense
    retrieval, hybrid retrieval, cross-encoder retrieval, graph-based retrieval, quantum-inspired
    retrieval, and neural IR models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Most advanced implementations will include a hybrid approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector stores and databases (e.g., FAISS, PineCone, Milvus, Weaviate), cloud
    provider solutions (e.g., Amazon Kendra, Azure AI Search, Google Vertex AI Search),
    and web information resources (e.g., Wikipedia, Arxiv, AskNews) are some of the
    popular retriever integrations provided by LangChain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of retriever depends on factors such as accuracy, speed, and compatibility
    with the indexing method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Augmentation combines the user query with retrieved information to create a
    prompt for the LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering is crucial for effective augmentation, aiming for accuracy
    and relevance in LLM responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key prompt engineering techniques for RAG include
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contextual promptin**g*—Adding retrieved information with instructions to
    focus on the provided context.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Controlled generation promptin**g*—Instructing the LLM to admit lack of knowledge
    when information is insufficient.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Few-shot promptin**g*—Providing examples to guide the LLM’s response format
    or style.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chain-of-thought (CoT) promptin**g*—Introducing intermediate reasoning steps
    for complex tasks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced techniques—These include self-consistency, generated knowledge prompting,
    and tree of thought.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of augmentation technique depends on the task complexity, desired
    output format, and LLM capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generation is the final step in which the LLM produces the response based on
    the augmented prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be categorized based on how they’ve been trained, how they can be accessed,
    and the number of parameters they have.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised fine-tuning, or SFT, improves context use and domain optimization,
    enhances coherence, and enables source attribution; however, it comes with challenges
    such as cost, risk of overreliance on retrieval, and potential tradeoffs with
    inherent LLM abilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between open source and proprietary LLMs depends on customization
    needs, long-term costs, and data sensitivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger models come with superior reasoning, language understanding, and broader
    knowledge, and generate more coherent and contextually accurate responses but
    come with high computational and resource requirements. Smaller models allow faster
    inference, lower resource usage, and are easier to deploy on edge devices or resource-constrained
    environments but do not have the same language understanding abilities as large
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular LLMs include offerings from OpenAI, Anthropic, Google, and similar,
    and open source models are available through platforms such as Hugging Face.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of LLM depends on factors such as performance requirements, resource
    constraints, deployment environment, and data sensitivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of LLM for RAG systems requires careful consideration, experimentation,
    and potential adaptation based on performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
