- en: '4 Generation pipeline: Generating contextual LLM responses'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 生成管道：生成上下文LLM响应
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Retrievers and retrieval methodologies
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索器和检索方法
- en: Augmentation using prompt engineering techniques
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提示工程技术进行增强
- en: Generation using LLMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM进行生成
- en: Basic implementation of the RAG pipeline in Python
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中RAG管道的基本实现
- en: In chapter 3, we discussed the creation of the knowledge base, or the non-parametric
    memory of retrieval augmented generation (RAG)-based applications, via the indexing
    pipeline. To use this knowledge base for accurate and contextual responses, we
    need to create a generation pipeline that includes the steps of retrieval, augmentation,
    and generation.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们讨论了通过索引管道创建知识库，即检索增强生成（RAG）应用的非参数记忆。为了使用这个知识库进行准确和上下文相关的响应，我们需要创建一个包含检索、增强和生成步骤的生成管道。
- en: This chapter elaborates on the three components of the generation pipeline.
    We begin by discussing the retrieval process, which primarily involves searching
    through the embeddings stored in vector databases of the knowledge base and returning
    a list of documents that closely match the input query of the user. You will also
    learn about the concept of retrievers and a few retrieval algorithms. Next, we
    move to the augmentation step. At this point, it is also beneficial to understand
    different prompt engineering frameworks used with RAG. Finally, as part of the
    generation step, we discuss a few stages of the LLM life cycle, such as using
    foundation models versus supervised fine-tuning, models of different sizes, and
    open source versus proprietary models in the RAG context. In each of these steps,
    we also highlight the benefits and drawbacks of different methods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细阐述了生成管道的三个组成部分。我们首先讨论检索过程，这主要涉及在知识库的向量数据库中搜索存储的嵌入，并返回与用户输入查询密切匹配的文档列表。你还将了解检索器及其检索算法的概念。接下来，我们转向增强步骤。在此阶段，了解与RAG一起使用的不同提示工程框架也是有益的。最后，作为生成步骤的一部分，我们讨论了LLM生命周期的一些阶段，例如使用基础模型与监督微调、不同大小的模型，以及在RAG环境中开源与专有模型。在这些步骤中，我们还强调了不同方法的优缺点。
- en: By the end of this chapter, you will be equipped with an understanding of the
    two foundational pipelines of a RAG system. You should also be ready to build
    a basic RAG system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将具备对RAG系统两个基础管道的理解。你也应该准备好构建一个基本的RAG系统。
- en: By the end of this chapter, you should
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该
- en: Know several retrievers used in RAG.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解RAG中使用的几个检索器。
- en: Get an understanding of augmentation using prompt engineering.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解使用提示工程进行增强。
- en: Learn some details about how LLMs are used in the context of RAG.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解LLM在RAG环境中的使用细节。
- en: Have an end-to-end knowledge of setting up a basic RAG system.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对设置基本RAG系统有端到端的知识。
- en: Let’s get started with an overview of the generation pipeline before diving
    into each component.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨每个组件之前，让我们先对生成管道进行一个概述。
- en: 4.1 Generation pipeline overview
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 生成管道概述
- en: Recall the generation pipeline introduced in chapter 2\. When a user provides
    an input, the generation pipeline is responsible for providing the contextual
    response. The retriever searches for the most appropriate information from the
    knowledge base. The user question is augmented with this information and passed
    as input to the LLM for generating the final response. This process is illustrated
    in figure 4.1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下第2章中介绍的生成管道。当用户提供输入时，生成管道负责提供上下文响应。检索器从知识库中搜索最合适的信息。用户问题通过这些信息进行增强，并作为输入传递给LLM以生成最终响应。这个过程如图4.1所示。
- en: 'The generation pipeline involves three processes: retrieval, augmentation,
    and generation. The retrieval process is responsible for fetching the information
    relevant to the user query from the knowledge base. Augmentation is the process
    of combining the fetched information with the user query. Generation is the last
    step, in which the LLM generates a response based on the augmented prompt. This
    chapter discusses these three processes in detail.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生成管道涉及三个过程：检索、增强和生成。检索过程负责从知识库中检索与用户查询相关的信息。增强是将检索到的信息与用户查询相结合的过程。生成是最后一步，LLM根据增强的提示生成响应。本章详细讨论了这三个过程。
- en: 4.2 Retrieval
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 检索
- en: Retrieval refers to the process of finding and extracting relevant pieces of
    information from a large corpus or knowledge base. As you saw in chapter 3, the
    information from various sources is parsed, chunked, and stored as embeddings
    in vector databases. These stored embeddings are also sometimes referred to as
    documents, and the knowledge base consists of several volumes of documents. Retrieval,
    essentially, is a search problem to find the documents that best match the input
    query.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 检索是指从大量语料库或知识库中查找和提取相关信息的过程。正如你在第3章中看到的，来自各种来源的信息被解析、分块并以嵌入的形式存储在向量数据库中。这些存储的嵌入有时也被称为文档，知识库由几卷文档组成。本质上，检索是一个搜索问题，旨在找到与输入查询最匹配的文档。
- en: Searching through the knowledge base and retrieving the right documents is done
    by a component called the *retriever*. In simple terms, retrievers accept a query
    as input and return a list of matching documents as output. This process is illustrated
    in figure 4.2\. You can imagine that retrieval is a crucial step since the quality
    of the retrieved information directly affects the quality of the output that will
    be generated.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个称为检索器的组件来搜索知识库并检索正确的文档。简单来说，检索器接受查询作为输入，并返回一个匹配文档的列表作为输出。这个过程如图4.2所示。你可以想象检索是一个关键步骤，因为检索到的信息质量直接影响最终生成的输出质量。
- en: '![A diagram of a process'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个流程图'
- en: AI-generated content may be incorrect.](../Images/CH04_F01_Kimothi.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH04_F01_Kimothi.png)
- en: Figure 4.1  Generation pipeline overview with the three components (i.e., retrieval,
    augmentation, and generation)
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1  包含三个组件（即检索、增强和生成）的生成管道概述
- en: '![A diagram of a user query'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个用户查询的流程图'
- en: AI-generated content may be incorrect.](../Images/CH04_F02_Kimothi.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH04_F02_Kimothi.png)
- en: Figure 4.2  A retriever searches through the knowledge base and returns the
    most relevant documents.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2  检索器在知识库中搜索并返回最相关的文档。
- en: We have already discussed embeddings in chapter 3 while building the indexing
    pipeline. Using embeddings, we can find documents that match the user query. Embeddings
    is one method in which retrieval can happen. There are other methods, too, and
    it is worth spending some time understanding different types of retrieval methods
    and the way they calculate the results.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第3章构建索引管道时已经讨论了嵌入。使用嵌入，我们可以找到与用户查询匹配的文档。嵌入是检索可以发生的一种方法。还有其他方法，花些时间了解不同类型的检索方法和它们计算结果的方式是值得的。
- en: This section on retrievers first discusses different retrieval algorithms and
    their significance in the context of RAG. In RAG systems, one or more retrieval
    methods can be used to build the retriever component. Next, we look at a few examples
    of prebuilt retrievers that can be used directly through a framework (e.g., LangChain).
    These retrievers are integrated with services such as databases, cloud providers,
    or third-party information sources. Finally, we will close this section by building
    a very simple retriever in LangChain using Python. We will continue to demonstrate
    with this example the augmentation and generation steps, too, so that we have
    a full implementation of the generation pipeline by the end of this chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先讨论了检索算法及其在RAG（检索增强生成）环境中的重要性。在RAG系统中，可以使用一种或多种检索方法来构建检索组件。接下来，我们将探讨一些可以直接通过框架（例如LangChain）使用的预构建检索器的例子。这些检索器与数据库、云服务提供商或第三方信息源等服务集成。最后，我们将通过在LangChain中使用Python构建一个非常简单的检索器来结束本节。我们还将继续使用这个例子来展示增强和生成步骤，以便在本章结束时，我们有一个完整的生成管道实现。
- en: Note Chapter 3 discussed indexing and how to convert and store data in a numerical
    form that can be used to retrieve information later. You may recall we discussed
    embeddings at length in section 3.3\. It should be intuitive that since we stored
    the data in the form of embeddings, to fetch this data, we will also have to work
    on the search using embeddings. Therefore, the retrieval process is tightly coupled
    with the indexing process. Whatever we use to index, we will have to use to retrieve.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第3章讨论了索引以及如何将数据转换为数值形式以便以后检索信息。你可能还记得我们在3.3节中详细讨论了嵌入。由于我们以嵌入的形式存储了数据，为了检索这些数据，我们也必须在嵌入的搜索上工作。因此，检索过程与索引过程紧密耦合。我们用于索引的任何东西，我们也将用于检索。
- en: 4.2.1 Progression of retrieval methods
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 检索方法的进展
- en: 'Information retrieval, or IR, is the science of searching. Whether you are
    searching for information in a document or for documents themselves, it falls
    under the gamut of information retrieval. IR has a rich history in computing,
    starting from Joseph Marie Jacquard’s invention of the Jacquard Loom, the first
    device that could read punched cards, back in the early 19th century. Since then,
    IR has evolved leaps and bounds from simple to highly sophisticated search and
    retrieval. *Boolean retrieval* is a simple keyword-based search (like the one
    you encounter when you press CTRL/CMD + F on your browser or word processor) where
    Boolean logic is used to match documents with queries based on the absence or
    presence of the words. Documents are retrieved if they contain the exact terms
    in the query, often combined with AND, NOT, and OR operators. *Bag of Words* *(BoW)*
    was used quite often in the early days of NLP. It creates a vocabulary of all
    the words in the documents as a vector indicating the presence or absence of each
    word. Consider two sentences: “The cat sat on the mat” and “The cat in the hat.”
    The vocabulary is `[``"``the``"``,` `"``cat``"``,` `"``in``"``,` `"``hat``"``,`
    `"``on``"``,` `"``mat``"``]` and the first sentence is represented as a vector
    `[2,` `1,` `1,` `1,` `0,` `0]`, while the one is `[2,` `1,` `0,` `0,` `1, 1]`.
    While simple, it ignores the context, meaning, and the order of words.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索，或称 IR，是搜索的科学。无论你是在文档中搜索信息还是搜索文档本身，它都属于信息检索的范畴。信息检索在计算机科学中有着丰富的历史，始于19世纪初约瑟夫·玛丽·雅卡尔发明的雅卡尔织机，这是第一个能够读取穿孔卡的设备。从那时起，信息检索从简单的搜索和检索发展到高度复杂的搜索和检索。*布尔检索*是一种基于关键词的简单搜索（就像你在浏览器或文字处理器的搜索栏中按下
    CTRL/CMD + F 时遇到的搜索），其中使用布尔逻辑根据单词的存在或不存在来匹配文档与查询。如果文档包含查询中的确切术语，通常结合 AND、NOT 和
    OR 运算符，则会检索文档。"词袋模型" *(BoW)* 在 NLP 的早期阶段被广泛使用。它创建了一个包含文档中所有单词的词汇表，作为一个向量，表示每个单词的存在或不存在。考虑两个句子："The
    cat sat on the mat" 和 "The cat in the hat。" 词汇表是 `[``"``the``"``,` `"``cat``"``,`
    `"``in``"``,` `"``hat``"``,` `"``on``"``,` `"``mat``"``]`，而第一个句子表示为向量 `[2,` `1,`
    `1,` `1,` `0,` `0]`，而第二个句子是 `[2,` `1,` `0,` `0,` `1, 1]`。虽然简单，但它忽略了上下文、意义和单词的顺序。
- en: Some of these, although popular in ML and IR space, don’t make sense in the
    context of RAG for a variety of reasons. For our purpose, we focus on a few of
    the popular retrieval techniques that have been used in RAG.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些，尽管在机器学习和信息检索领域很受欢迎，但由于各种原因，在 RAG 的上下文中并不合理。为了我们的目的，我们关注在 RAG 中使用的一些流行的检索技术。
- en: Term Frequency-Inverse Document Frequency
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 术语频率-逆文档频率
- en: Term Frequency–Inverse Document Frequency (TF-IDF) is a statistical measure
    used to evaluate the importance of a word in a document relative to a collection
    of documents (corpus). It assigns higher weights to words that appear frequently
    in a document but infrequently across the corpus. Figure 4.3 illustrates how TF-IDF
    is calculated for a unigram search term.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 术语频率-逆文档频率 (TF-IDF) 是一种用于评估单词在文档中相对于文档集合（语料库）的重要性的一种统计度量。它将更高的权重分配给在文档中频繁出现但在语料库中不频繁出现的单词。图
    4.3 阐述了如何计算单语元搜索词的 TF-IDF。
- en: '![A screenshot of a computer'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机屏幕截图'
- en: AI-generated content may be incorrect.](../Images/CH04_F03_Kimothi.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成的内容可能是不正确的。](../Images/CH04_F03_Kimothi.png)
- en: Figure 4.3  Calculating TF-IDF to rank documents based on search terms
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3  根据搜索词计算 TF-IDF 以对文档进行排名
- en: 'LangChain also provides an abstract implementation of TF-IDF using retrievers
    from `langchain_community`, which, in turn, uses `scikit-learn`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 还提供了一个 TF-IDF 的抽象实现，使用来自 `langchain_community` 的检索器，它反过来又使用 `scikit-learn`：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: TF-IDF not only can be used for unigrams, but also for phrases (n-grams). However,
    even TF-IDF improves on simpler search methods by emphasizing unique words, it
    still lacks context and word-order consideration, making it less suitable for
    complex tasks like RAG.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 不仅可用于单语元，也可用于短语（n-gram）。然而，即使 TF-IDF 通过强调独特单词来改进简单的搜索方法，它仍然缺乏对上下文和单词顺序的考虑，这使得它对于像
    RAG 这样的复杂任务不太适用。
- en: Best Match 25
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最佳匹配 25
- en: Best Match 25 (BM25) is an advanced probabilistic model used to rank documents
    based on the query terms appearing in each document. It is part of the family
    of probabilistic information retrieval models and is considered an advancement
    over the classic TF-IDF model. The improvement that BM25 brings is that it adjusts
    for the length of the documents so that longer documents do not unfairly get higher
    scores. Figure 4.4 illustrates the BM25 calculation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳匹配25（BM25）是一种用于根据每个文档中出现的查询术语对文档进行排名的高级概率模型。它是概率信息检索模型家族的一部分，被认为是经典TF-IDF模型的进步。BM25带来的改进是它调整了文档的长度，这样较长的文档不会得到不公平的高分。图4.4展示了BM25的计算过程。
- en: '![A screenshot of a computer'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机屏幕截图](../Images/CH04_F04_Kimothi.png)'
- en: AI-generated content may be incorrect.](../Images/CH04_F04_Kimothi.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH04_F04_Kimothi.png)
- en: Figure 4.4  BM25 also considers the length of the documents.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4 BM25也考虑了文档的长度。
- en: 'Like TF-IDF, LangChain also has an abstract implementation of BM25 (Okapi BM25,
    specifically) using the `rank_bm25` package:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与TF-IDF类似，LangChain也使用`rank_bm25`包提供了一个BM25（具体来说是Okapi BM25）的抽象实现：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For long queries instead of single keywords, the BM25 value is calculated for
    each word in the query, and the final BM25 value for the query is a summation
    of the values for all the words. BM25 is a powerful tool in traditional IR, but
    it still doesn’t capture the full semantic meaning of queries and documents required
    for RAG applications. BM25 is generally used in RAG for quick initial retrieval,
    and then a more powerful retriever is used to re-rank the results. We will learn
    about re-ranking later in chapter 6, when we discuss advanced strategies for RAG.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长查询而不是单个关键词，计算查询中每个词的BM25值，查询的最终BM25值是所有词值的总和。BM25是传统信息检索（IR）中的一个强大工具，但它仍然无法捕捉到RAG应用所需的查询和文档的完整语义意义。BM25通常在RAG中用于快速初始检索，然后使用更强大的检索器重新排序结果。我们将在第6章学习重新排序，届时我们将讨论RAG的高级策略。
- en: Static word embeddings
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 静态词嵌入
- en: Static embeddings such as Word2Vec and GloVe represent words as dense vectors
    in a continuous vector space, capturing semantic relationships based on context.
    For instance, “king” − “man” + “woman” approximates “queen.” These embeddings
    can capture nuances such as similarity and analogy, which BoW, TF-IDF, and BM25
    miss. However, while they provide a richer representation, they still lack full
    contextual understanding and are limited in handling polysemy (words with multiple
    meanings). The term *static* here highlights that the vector representation of
    words does not change with the context of the word in the input query.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如Word2Vec和GloVe之类的静态嵌入将词表示为连续向量空间中的密集向量，基于上下文捕捉语义关系。例如，“king” − “man” + “woman”近似于“queen”。这些嵌入可以捕捉到BoW、TF-IDF和BM25所遗漏的细微差别，如相似性和类比。然而，尽管它们提供了更丰富的表示，但它们仍然缺乏完整的上下文理解，并且在处理多义词（具有多个意义的词）方面有限。这里的“静态”一词强调了词的向量表示不会随着输入查询中词的上下文而改变。
- en: Contextual embeddings
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文嵌入
- en: Generated by models such as BERT or OpenAI’s text embeddings, contextual embeddings
    produce high-dimensional, context-aware representations for queries and documents.
    These models, based on transformers, capture deep semantic meanings and relationships.
    For example, a query about “apple” will retrieve documents discussing apple the
    fruit, or Apple the technology company, depending on the input query. Figure 4.5
    illustrates the difference between static and contextual embeddings. Contextual
    embeddings represent a significant advancement in IR, providing the context and
    understanding necessary for RAG tasks. Despite being computationally intensive,
    contextual embeddings are the most widely used retrievers in RAG. Examples of
    embedding models discussed in section 3.3.2 are contextual embeddings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由BERT或OpenAI的文本嵌入模型生成，上下文嵌入为查询和文档生成高维、上下文感知的表示。这些基于transformers的模型捕捉深层的语义意义和关系。例如，关于“apple”的查询将检索讨论苹果这种水果或苹果科技公司的文档，具体取决于输入查询。图4.5展示了静态嵌入和上下文嵌入之间的差异。上下文嵌入在信息检索（IR）中代表了一个重大进步，为RAG任务提供了必要的上下文和理解。尽管计算密集，上下文嵌入在RAG中是最广泛使用的检索器。第3.3.2节中讨论的嵌入模型示例是上下文嵌入。
- en: Methods such as TF-IDF and BM25 use frequency-based calculations to rank documents.
    In embeddings (both static and contextual), ranking is done based on a similarity
    score. Similarity is popularly calculated using the cosine of the angle between
    document vectors. We discussed cosine similarity calculation in section 3.3.3\.
    Figure 4.6 illustrates the process of retrieval using embeddings.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF和BM25等方法使用基于频率的计算来对文档进行排序。在嵌入（静态和上下文）中，排序是基于相似度分数。相似度通常使用文档向量之间角度的余弦值来计算。我们在第3.3.3节中讨论了余弦相似度计算。图4.6说明了使用嵌入进行检索的过程。
- en: Other retrieval methods
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他检索方法
- en: 'While the discussed methods are most popular in the discourse, other methods
    are also available. These methods represent more recent developments and specialized
    approaches and are good to refer to if you want to dive deeper into the world
    of information retrieval:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所讨论的方法在讨论中最为流行，但还有其他方法可供选择。这些方法代表了更近期的进展和专门的方法，如果你想要深入了解信息检索的世界，这些方法是非常好的参考：
- en: '*Learned sparse retrieval*—Generates sparse, interpretable representations
    using neural networks (examples: SPLADE, DeepCT, and DocT5Quer)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习稀疏检索*—使用神经网络生成稀疏、可解释的表示（例如：SPLADE，DeepCT，和DocT5Quer）'
- en: '*Dense retrieval*—Encodes queries and documents as dense vectors for semantic
    matching (examples: dense passage retriever [DPR], ANCE, RepBERT)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*密集检索*—将查询和文档编码为密集向量以进行语义匹配（例如：密集段落检索器 [DPR]，ANCE，RepBERT）'
- en: '![A diagram of a diagram'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的图表'
- en: AI-generated content may be incorrect.](../Images/CH04_F05_Kimothi.png)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的可能不正确。](../Images/CH04_F05_Kimothi.png)
- en: Figure 4.5  Static vs. contextual embeddings
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5  静态嵌入与上下文嵌入的比较
- en: '*Hybrid retrieva**l*—Combines sparse and dense methods for balanced efficiency
    and effectiveness (examples: ColBERT, COIL)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合检索*—结合稀疏和密集方法以实现平衡的效率和效果（例如：ColBERT，COIL）'
- en: '*Cross-encoder retrieva**l*—Directly compares query-document pairs using transformer
    models (example: BERT-based re-rankers)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*交叉编码检索*—直接使用转换器模型比较查询-文档对（例如：基于BERT的重排序器）'
- en: '*Graph-based retrieva**l*—Uses graph structures to model relationships between
    documents (examples: TextGraphs, graph neural networks for IR)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于图的检索*—使用图结构来建模文档之间的关系（例如：TextGraphs，用于信息检索的图神经网络）'
- en: '*Quantum-inspired retrieva**l*—Applies quantum computing principles to information
    retrieval (example: quantum language models [QLM])'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*量子启发检索*—将量子计算原理应用于信息检索（例如：量子语言模型 [QLM]）'
- en: '*Neural IR model**s*—Encompass various neural network-based approaches to information
    retrieval (examples: NPRF [neural PRF], KNRM [Kernel-based Neural Ranking Model])'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经信息检索模型*—包含各种基于神经网络的信息检索方法（例如：NPRF [神经PRF]，KNRM [基于核的神经网络排序模型]）'
- en: '![A diagram of a diagram'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的图表'
- en: AI-generated content may be incorrect.](../Images/CH04_F06_Kimothi.png)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的可能不正确。](../Images/CH04_F06_Kimothi.png)
- en: Figure 4.6  Similarity calculation and results ranking in embeddings-based retrieval
    technique
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6  基于嵌入的检索技术中的相似度计算和结果排序
- en: Table 4.1 notes the weaknesses and strengths of different retrievers. While
    contextual embeddings are the only ones you need to know to get started with RAG,
    it is useful to get familiar with other retrievers for further exploration and
    for cases where you want to improve retriever performance. As we discussed, the
    implementation of TF-IDF using the `scikit-learn` retriever and BM25 using `rank_bm25`
    retriever in LangChain, there are many others available that use one of the mentioned
    methodologies. We will look at some of the popular ones in the next section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1列出了不同检索器的优缺点。虽然上下文嵌入是开始使用RAG所需了解的唯一内容，但熟悉其他检索器对于进一步探索以及当你想要提高检索器性能的情况是有用的。正如我们讨论的，在LangChain中使用`scikit-learn`检索器实现TF-IDF和`rank_bm25`检索器实现BM25，还有许多其他使用上述提到的方法之一的方法。我们将在下一节中查看一些流行的例子。
- en: Table 4.1 Comparison of different retrieval techniques for RAG
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.1 RAG不同检索技术的比较
- en: '| Technique | Key feature | Strengths | Weaknesses | Suitability for RAG |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 关键特性 | 优点 | 缺点 | 适用于RAG |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Boolean retrieval | Exact matching with logical operators | Simple, fast,
    and precise | Limited relevance ranking; no partial matching | Low: Too rigid
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 布尔检索 | 使用逻辑运算符的精确匹配 | 简单、快速、精确 | 有限的相关性排名；无部分匹配 | 低：过于僵化 |'
- en: '| BoW | Unordered word frequency counts | Simple and intuitive | Ignores word
    order and context | Low: Lacks semantic understanding |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| TF-IDF | Term weighting based on document and corpus frequency | Improved
    relevance ranking over BoW | Still ignores semantics and word relationships |
    Low–medium: Better than BoW but limited; used in hybrid retrieval |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| BM25 | Advanced ranking function with length normalization | Robust performance;
    industry standard | Limited semantic understanding | Medium: Good baseline for
    simple RAG; used in hybrid retrieval. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| Static embeddings | Fixed dense vector representations | Captures some semantic
    relationships | Context-independent; limited in polysemy handling | Medium: Introduces
    basic semantics |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| Contextual embeddings | Context-aware dense representations | Rich semantic
    understanding; handles polysemy | Computationally intensive | High: Excellent
    semantic capture |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| Learned sparse retrievers | Neural-network-generated sparse representations
    | Efficient, interpretable, and has some semantic understanding | May miss some
    semantic relationships | High: Balances efficiency and semantics |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| Dense retrievers | Dense vector matching for queries and documents | Strong
    semantic matching | Computationally intensive; less interpretable | High: Excellent
    for semantic search in RAG |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| Hybrid retrievers | Combination of sparse and dense methods | Balances efficiency
    and effectiveness | Complex to implement and tune | High: Versatile for various
    RAG needs |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| Cross-encoder retrievers | Direct query-document comparison | Very accurate
    relevance assessment | Extremely computationally expensive | Medium–high: Great
    for reranking in RAG |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| Graph-based retrievers | Graph structure for document relationships | Captures
    complex relationships in data | Can be complex to construct and query | Medium–high:
    Good for structured data in RAG |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| Quantum-inspired retrievers | Quantum computing concepts in IR | Potential
    for handling complex queries | Emerging field; practical benefits not fully proven
    | Low–medium: Potentially promising but not mature |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| Neural IR models | Various neural network approaches to IR | Flexible; can
    capture complex patterns | Often require large training data; can be black-box
    | High: Adaptable to various RAG scenarios |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: 4.2.2 Popular retrievers
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Developers can build their retrievers based on one or a combination of multiple
    retrieval methodologies. Retrievers are used not just in RAG but in a variety
    of search-related tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: For RAG, LangChain provides many integrations where the algorithms such as TF-IDF,
    embeddings and similarity search, and BM25 have been abstracted as retrievers
    for developers to use. We have already seen the ones for TF-IDF and BM25\. Some
    of the other popular retrievers are described in the following sections.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Vector stores and databases as retrievers
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vector stores can act as the retrievers, taking away the responsibility from
    the developer to convert the query vector into embeddings by calculating similarity
    and ranking the results. FAISS is typically used in tandem with a contextual embedding
    model for retrieval. Other vector DBs such as PineCone, Milvus, and Weaviate provide
    hybrid search functionality by combining dense retrieval methods such as embeddings
    and sparse methods such as BM25 and SPLADE.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 向量存储可以作为检索器使用，从而免除开发者计算查询向量嵌入并按相似度和排名结果的责任。FAISS通常与上下文嵌入模型一起用于检索。其他向量数据库如PineCone、Milvus和Weaviate通过结合嵌入等密集检索方法和BM25和SPLADE等稀疏方法提供混合搜索功能。
- en: Cloud providers
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 云服务提供商
- en: Cloud providers Azure, AWS, and Google also offer their retrievers. Integration
    with Amazon Kendra, Azure AI Search, AWS Bedrock, Google Drive, and Google Vertex
    AI Search provides developers with infrastructure, APIs, and tools for information
    retrieval of vector, keyword, and hybrid queries at scale.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商Azure、AWS和Google也提供了他们的检索器。与Amazon Kendra、Azure AI Search、AWS Bedrock、Google
    Drive和Google Vertex AI Search的集成为开发者提供了在规模上执行向量、关键词和混合查询信息检索的基础设施、API和工具。
- en: Web information resources
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络信息资源
- en: Connections to information resources such as Wikipedia, Arxiv, and AskNews provide
    optimized search and retrieval from these sources. You can check these retrievers
    and more in the official LangChain documentation ([https://mng.bz/gm4R](https://mng.bz/gm4R))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与信息资源如维基百科、arXiv和AskNews的连接提供了从这些来源的优化搜索和检索。你可以在官方LangChain文档中查看这些检索器和其他更多内容（[https://mng.bz/gm4R](https://mng.bz/gm4R)）
- en: This was a brief introduction to the world of retrievers. If you found the information
    slightly complex, you can always revisit it. At this stage, the understanding
    of contextual embeddings will suffice. Contextual embeddings are the most popular
    technique for basic RAG pipelines, and we will now create a simple retriever using
    OpenAI embeddings.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对检索器世界的简要介绍。如果你觉得信息有点复杂，你总是可以随时回过头来再看。在这个阶段，对上下文嵌入的理解就足够了。上下文嵌入是基本RAG管道中最受欢迎的技术，我们现在将使用OpenAI嵌入创建一个简单的检索器。
- en: 4.2.3 A simple retriever implementation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 简单检索器实现
- en: 'Before we move to the next step of the generation pipeline, let’s look at a
    simple example of a retriever. In chapter 3, we were working on indexing the Wikipedia
    page for the 2023 Cricket World Cup. If you recall, we used embeddings from OpenAI
    to encode the text and used FAISS as the vector index to store the embeddings.
    We also stored the FAISS index in a local directory. Let’s reuse this index:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入生成管道的下一步之前，让我们看看一个简单的检索器示例。在第3章中，我们正在为2023年板球世界杯的维基百科页面进行索引。如果你还记得，我们使用了OpenAI的嵌入来编码文本，并使用FAISS作为向量索引来存储嵌入。我们还把FAISS索引存储在本地目录中。现在让我们重用这个索引：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This `similarity_search ()` function returns a list of matching documents ordered
    by a score. This score is a quantification of the similarity between the query
    and the document and is hence called the similarity score. In this example, the
    vector index’s inbuilt similarity search feature was used for retrieval. As one
    of the retrievers we discussed in section 4.2.2, the vector store itself acted
    as the retriever. `K=2` tells the function to retrieve the top two documents.
    This is the most basic implementation of a retriever in the generation pipeline
    of a RAG system, and the retrieval method is enabled by embeddings. We used the
    text-embedding-3-small from OpenAI. FAISS calculated the similarity score based
    on these embeddings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`similarity_search()`函数返回一个按分数排序的匹配文档列表。这个分数是查询与文档之间相似度的量化，因此被称为相似度分数。在这个例子中，使用了向量索引的内置相似度搜索功能进行检索。作为我们在第4.2.2节讨论的检索器之一，向量存储本身充当了检索器。`K=2`告诉函数检索前两个文档。这是RAG系统生成管道中检索器最基本实现，检索方法是通过嵌入实现的。我们使用了OpenAI的text-embedding-3-small。FAISS根据这些嵌入计算相似度分数。
- en: Retrievers are the backbone of RAG systems. The quality of the retriever has
    a great bearing on the quality of the generated output. In this section, you learned
    about vanilla retrieval methods. Multiple strategies are used when designing production-grade
    systems. We will read about these advanced strategies in chapter 6\. Now that
    we have gained an understanding of the retrievers, we will move on to the next
    important step—augmentation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器是RAG系统的骨架。检索器的质量对生成输出的质量有很大影响。在本节中，你学习了关于vanilla检索方法的内容。在设计生产级系统时，会使用多种策略。我们将在第6章中了解这些高级策略。现在，我们已经了解了检索器，接下来我们将进入下一个重要步骤——增强。
- en: 4.3 Augmentation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 增强技术
- en: A retriever fetches the information (or documents) that are most relevant to
    the user query. But, what next? How do we use this information? The answer is
    quite intuitive. If you recall the discussion in chapter 1, the input to an LLM
    is a natural language prompt. This information fetched by the retriever should
    also be sent to the LLM in the form of a natural language prompt. This process
    of combining the user query and the retrieved information is called *augmentation*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器检索与用户查询最相关的信息（或文档）。但是，接下来怎么办？我们如何使用这些信息？答案是相当直观的。如果你还记得第1章的讨论，LLM的输入是自然语言提示。检索器检索到的信息也应该以自然语言提示的形式发送给LLM。将用户查询和检索到的信息结合起来的这个过程被称为*增强*。
- en: The augmentation step in RAG largely falls under the discipline of prompt engineering.
    Prompt engineering can be defined as the technique of giving instructions to an
    LLM to attain a desired outcome. The goal of prompt engineering is to construct
    the prompts to achieve accuracy and relevance in the LLM responses to the desired
    outcome(s). At the first glance, augmentation is quite simple—just add the retrieved
    information to the query. However, some nuanced augmentation techniques help improve
    the quality of the generated results. See figure 4.7 for an example of simple
    augmentation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: RAG中的增强步骤主要属于提示工程领域。提示工程可以定义为向LLM提供指令以实现预期结果的技术。提示工程的目标是通过构建提示，在LLM对预期结果（s）的响应中实现准确性和相关性。乍一看，增强似乎很简单——只需将检索到的信息添加到查询中。然而，一些细微的增强技术有助于提高生成结果的品质。见图4.7简单增强的示例。
- en: '![A diagram of a diagram'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的图表'
- en: AI-generated content may be incorrect.](../Images/CH04_F07_Kimothi.png)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH04_F07_Kimothi.png)
- en: Figure 4.7  Simple augmentation combines the user query with retrieved documents
    to send to the LLM.
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.7 简单增强是将用户查询与检索到的文档结合起来发送给LLM。
- en: 4.3.1 RAG prompt engineering techniques
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 RAG提示工程技术
- en: Prompt engineering as a discipline has, sometimes, been dismissed as being too
    simple to be called engineering. You may have heard the phrase, “English is the
    new programming language.” Interaction with LLMs is indeed in natural language.
    However, what is also true is that the principles of programming are not the language
    in which code is written but the logic in which the machine is instructed. With
    that in mind, let’s examine different logical approaches that can be taken to
    augment the user query with the retrieved information.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程作为一个学科，有时被认为过于简单，不足以被称为工程。你可能听说过这样的话，“英语是新的编程语言。”与LLM的交互确实是在自然语言中进行的。然而，同样真实的是，编程的原则不是代码所写的语言，而是机器被指令的逻辑。考虑到这一点，让我们考察不同的逻辑方法，这些方法可以用来增强用户查询与检索到的信息。
- en: Contextual prompting
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文提示
- en: To understand a simple augmentation technique, let’s revisit chapter 1\. Recall
    our example of “Who won the 2023 Cricket World Cup?” We copied an excerpt from
    the Wikipedia article. This excerpt is the retrieved information. We then added
    this information to the prompt and provided an extra instruction—“Answer only
    based on the context provided below.” Figure 4.8 illustrates this example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解简单的增强技术，让我们回顾第1章。回忆我们的例子“谁赢得了2023年板球世界杯？”我们复制了维基百科文章的摘录。这个摘录是检索到的信息。然后我们将这些信息添加到提示中，并提供了额外的指令——“仅根据以下提供的上下文回答。”图4.8展示了这个例子。
- en: 'By adding this instruction, we have set up our generation to focus only on
    the provided information and not on LLM’s internal knowledge (or parametric knowledge).
    This is a simple augmentation technique that is also referred to as *contextual
    prompting*. Please note that the instruction can be given in any linguistic construct.
    For example, we could have added the instruction at the beginning of the prompt
    as, “Given the context below, answer the question, Who won the 2023 Cricket World
    Cup. Information: <Wikipedia excerpt>.” We can also reiterate the instruction
    at the end of the prompt—“Remember to answer only based on the context provided
    and not from any other source.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加这个指令，我们已经设置了我们的生成过程只关注提供的信息，而不是LLM的内部知识（或参数化知识）。这是一种简单的增强技术，也被称为*上下文提示*。请注意，指令可以以任何语言结构给出。例如，我们可以在提示的开头添加指令，例如，“在以下背景下回答问题，谁赢得了2023年板球世界杯。信息：<维基百科摘录>。”我们也可以在提示的末尾重申指令——“请记住，只根据提供的背景回答问题，不要从任何其他来源获取信息。”
- en: Controlled generation prompting
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 控制生成提示
- en: Sometimes, the information might not be present in the retrieved document. This
    happens when the documents in the knowledge base do not have any informationrelevant
    to the user query. The retriever might still fetch some documents that are the
    closest to the user query. In these cases, the chances of hallucination increase
    because the LLM will still try to follow the instructions for answering the question.
    To avoid this scenario, an additional instruction is added, which tells the LLM
    not to answer if the retrieved document does not have proper information to answer
    the user question (something like, “If the question cannot be answered based on
    the provided context, say I don’t know.”). In the context of RAG, this technique
    is particularly valuable because it ensures that the model’s responses are grounded
    in the retrieved information. If the relevant information hasn’t been retrieved
    or isn’t present in the knowledge base, the model is instructed to acknowledge
    this lack of information rather than attempting to generate a potentially incorrect
    answer.”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，信息可能不在检索到的文档中。这种情况发生在知识库中的文档没有与用户查询相关的任何信息时。检索器可能仍然检索到一些与用户查询最接近的文档。在这些情况下，幻觉的可能性增加，因为LLM仍然会尝试遵循回答问题的指令。为了避免这种情况，添加了一个额外的指令，告诉LLM如果检索到的文档没有适当的信息来回答用户问题（类似于“如果问题不能根据提供的情况回答，就说我不知道。”），则不要回答。在RAG的上下文中，这种技术特别有价值，因为它确保了模型的响应基于检索到的信息。如果相关信息尚未检索到或不在知识库中，模型被指示承认这种信息缺失，而不是尝试生成一个可能是不正确的答案。”
- en: '![A screenshot of a chat'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![聊天截图](../Images/CH04_F09_Kimothi.png)'
- en: AI-generated content may be incorrect.](../Images/CH04_F08_Kimothi.png)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH04_F09_Kimothi.png)
- en: Figure 4.8  Information is augmented to the original question with an added
    instruction.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.8  信息通过添加指令增强到原始问题中。
- en: Few-shot prompting
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 少样本提示
- en: It has been observed that while generating responses, LLMs adhere quite well
    to the examples provided in the prompt. If you want the generation to be in a
    certain format or style, it is recommended to provide a few examples. In RAG,
    while providing the retrieved information in the prompt, we can also specify certain
    examples to help guide the generation in the way we need the retrieved information
    to be used. This technique is called *few-shot prompting*. Here “shot” refers
    to the examples given in the prompt. Figure 4.9 illustrates a prompt that includes
    two examples with the question.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到，当生成响应时，LLM相当好地遵循提示中提供的示例。如果你想生成的内容以特定的格式或风格呈现，建议提供一些示例。在RAG中，当在提示中提供检索到的信息时，我们也可以指定某些示例，以帮助引导生成过程，使其以我们需要的方式使用检索到的信息。这种技术被称为*少样本提示*。在这里，“shot”指的是提示中给出的示例。图4.9展示了包含两个示例的提示。
- en: '![A screenshot of a text box'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![文本框截图](../Images/CH04_F09_Kimothi.png)'
- en: AI-generated content may be incorrect.](../Images/CH04_F09_Kimothi.png)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH04_F09_Kimothi.png)
- en: Figure 4.9  Example of few-shot prompting in the context of RAG
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9  RAG上下文中少样本提示的示例
- en: You might come across terms such as *one-shot prompting* or two-shot prompting,
    which replaces the word “few” with the number of examples given. Conversely, when
    no example is given, and the LLM is expected to answer correctly, the technique
    is also called *zero-shot prompting*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到诸如 *单次提示* 或 *两次提示* 等术语，它们用给出的示例数量替换了“少量”一词。相反，当没有给出示例，并且期望 LLM 正确回答时，该技术也称为
    *零次提示*。
- en: Chain of thought prompting
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 思维链提示
- en: It has been observed that the introduction of intermediate reasoning steps improves
    the performance of LLMs in tasks requiring complex reasoning, such as arithmetic,
    common sense, and symbolic reasoning. The same can be applied in the context of
    RAG. This is called *chain-of-thought*, or CoT, *prompting*. In figure 4.10, I
    asked ChatGPT to analyze the performance of two teams based on the retrieved information.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到引入中间推理步骤可以提高 LLM 在需要复杂推理的任务（如算术、常识和符号推理）中的性能。在 RAG 的上下文中也可以应用这一点。这被称为 *思维链*，或
    CoT，*提示*。在图 4.10 中，我要求 ChatGPT 根据检索到的信息分析两个团队的表现。
- en: '![A paper with text and numbers'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![包含文本和数字的论文'
- en: AI-generated content may be incorrect.](../Images/CH04_F10_Kimothi.png)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成的内容可能是不正确的。](../Images/CH04_F10_Kimothi.png)
- en: Figure 4.10  Chain-of-thought (CoT) prompting for reasoning tasks
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.10  推理任务的思维链（CoT）提示
- en: The CoT prompting approach can also be combined with the few-shot prompting
    technique, where a few examples of reasoning are provided before the final question.
    Creating these examples is a manually intensive task. In auto-CoT, the examples
    are also created using an LLM.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 提示方法还可以与少量提示技术相结合，在最终问题之前提供少量推理示例。创建这些示例是一项劳动密集型任务。在 auto-CoT 中，示例也是使用 LLM
    创建的。
- en: Other advanced prompting techniques
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他高级提示技术
- en: 'Prompt engineering is becoming an increasingly intricate discipline. Ongoing
    research constantly presents new improvements in prompting techniques. To dive
    deeper into prompt engineering, let’s check out some of the following techniques:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程正变得越来越复杂。持续的研究不断提出提示技术的改进。要深入了解提示工程，让我们来看看以下一些技术：
- en: '*Self-consistency*—While CoT uses a single reasoning chain in CoT prompting,
    self-consistency aims to sample multiple diverse reasoning paths and use their
    respective generations to arrive at the most consistent answer.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自洽性*——虽然 CoT 在 CoT 提示中使用单个推理链，但自洽性旨在采样多个不同的推理路径，并使用它们各自的生成结果来得出最一致的答案。'
- en: '*Generated knowledge promptin**g*—This technique explores the idea of prompt-based
    knowledge generation by dynamically constructing relevant knowledge chains, using
    models’ latent knowledge to strengthen reasoning.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生成式知识提示*——这种技术通过动态构建相关的知识链，使用模型的潜在知识来加强推理，探索基于提示的知识生成理念。'
- en: '*Tree-of-thoughts promptin**g*—This technique maintains an explorable tree
    structure of coherent intermediate thought steps aimed at solving problems.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*思维树提示*——这种技术维护一个可探索的思维树结构，其中包含旨在解决问题的连贯的中间思维步骤。'
- en: '*Automatic reasoning and tool use* (ART)—The ART framework automatically interleaves
    model generations with tool use for complex reasoning tasks. ART employs demonstrations
    to decompose problems and integrate tools without task-specific scripting.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动推理和工具使用*（ART）——ART 框架自动将模型生成与工具使用交织在一起，用于复杂的推理任务。ART 使用演示来分解问题并集成工具，而无需特定于任务的脚本。'
- en: '*Automatic prompt engineer* (APE)—The APE framework automatically generates
    and selects optimal instructions to guide models. It uses an LLM to synthesize
    candidate prompt solutions for a task based on output demonstrations.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动提示工程师*（APE）——APE 框架自动生成和选择最佳指令以引导模型。它使用大型语言模型（LLM）根据输出演示合成任务候选提示解决方案。'
- en: '*Active promp**t*—Active-prompt improves CoT methods by dynamically adapting
    language models to task-specific prompts through a process involving query, uncertainty
    analysis, human annotation, and enhanced inference.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主动提示*——主动提示通过涉及查询、不确定性分析、人工标注和增强推理的过程，动态调整语言模型以适应特定任务的提示。'
- en: '*ReAct prompting*—ReAct integrates LLMs for concurrent reasoning traces and
    task-specific actions, improving performance by interacting with external tools
    for information retrieval. When combined with CoT, it optimally utilizes internal
    knowledge and external information, enhancing the interpretability and trustworthiness
    of LLMs.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recursive promptin**g***—**Recursive prompting breaks down complex problems
    into subproblems, solving them by sequentially using prompts. This method aids
    compositional generalization in tasks such as math problems or question answering,
    with the model building on solutions from previous steps.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table 4.2 summarizes different prompting techniques. Prompt engineering for
    augmentation is an evolving discipline. It is important to note that there is
    a lot of scope for creativity in writing prompts for RAG applications. Efficient
    prompting has a significant effect on the generated output. The kind of prompts
    you use will depend a lot on your use case and the nature of the information in
    the knowledge base.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2  Comparison of prompting techniques for augmentation
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Technique | Description | Key advantage | Best use case | Complexity |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| Contextual prompting | Adds retrieved information to the prompt with instructions
    to focus on the provided context | Ensures focus on relevant information | General
    RAG queries | Low |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| Controlled generation prompting | Instructs the model to say “I don’t know”
    when information is not available | Reduces hallucination risk | When accuracy
    is critical | Low |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| Few-shot prompting | Provides examples in the prompt to guide response format
    and style | Improves output consistency and format adherence | When a specific
    output format is required | Medium |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Chain-of-thought (CoT) prompting | Introduces intermediate reasoning steps
    | Improves performance on complex reasoning tasks | Complex queries requiring
    step-by-step analysis | Medium |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Self-consistency | Samples multiple diverse reasoning paths | Improves answer
    consistency and accuracy | Tasks with multiple possible reasoning approaches |
    High |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Generated knowledge prompting | Dynamically constructs relevant knowledge
    chains | Uses the model’s latent knowledge | Tasks requiring broad knowledge application
    | High |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| Tree-of-thoughts prompting | Maintains an explorable tree structure of thought
    steps | Allows for more comprehensive problem-solving | Complex, multistep problem
    solving | High |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Automatic reasoning and tool use (ART) | Interleaves model generations with
    tool use | Enhances problem decomposition and tool integration | Tasks requiring
    external tool use | Very High |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| Automatic prompt engineer (APE) | Automatically generates and selects optimal
    instructions | Optimizes prompts for specific tasks | Prompt optimization for
    complex tasks | Very High |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| Active prompt | Dynamically adapts LMs to task-specific prompts | Improves
    task-specific performance | Tasks requiring adaptive prompting | High |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| ReAct prompting | Integrates reasoning traces with task-specific actions
    | Improves performance and interpretability | Tasks requiring both reasoning and
    action | High |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Recursive prompting | Breaks down complex problems into subproblems | Aids
    in compositional generalization | Complex, multistep problems | High |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: We have already built a simple retriever in the previous section. We will now
    execute augmentation with a simple contextual prompt with controlled generation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 A simple augmentation prompt creation
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In section 4.2.3, we were able to implement a FAISS-based retriever using OpenAI
    embeddings. We will now make use of this retriever and create the augmentation
    prompt:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With the augmentation step complete, we are now ready to send the prompt to
    the LLM for the generation of the desired outcome. You will now learn how LLMs
    generate text and the nuances of generation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Generation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generation is the final step of this pipeline. While LLMs may be used in any
    of the previous steps, the generation step relies completely on the LLM. The most
    popular LLMs are the ones being developed by OpenAI, Anthropic, Meta, Google,
    Microsoft, and Mistral, among other developers. While text generation is the core
    capability of LLMs, we are now seeing multimodal models that can handle images
    and audio along with text. Simultaneously, researchers are developing faster and
    smaller models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the factors that can help choose a language
    model for your RAG system. We will then continue with our example of the retriever
    and augmented prompt we have built so far and complete it by adding the generation
    step.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Categorization of LLMs and suitability for RAG
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of June 2024, there are over a hundred LLMs available to use, and new ones
    are coming out every week. How do we decide then which LLM to choose for our RAG
    system? To show you the decision-making process, let’s discuss three themes under
    which we can broadly categorize LLMs:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: How they have been trained
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How they can be accessed
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their size
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss the LLMs under these themes and understand the factors that
    may influence the LLM choice for RAG.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Original vs. fine-tuned models
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training an LLM takes massive amounts of data and computational resources. LLMs
    training is done through an unsupervised learning process. All modern LLMs are
    autoregressive models and are trained to generate the next token in a sequence.
    These massive pre-trained LLMs are also called *foundation models.*
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The question that you may ask is, if LLMs just predict the next tokens in a
    sequence, how are we able to ask questions and chat with these models? The answer
    is in what we call *supervised fine-tuning*, or *SFT.*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Supervised fine-tuning is a process used to adapt a pre-trained language model
    for specific tasks or behaviors such as question-answering or chat. It involves
    further training a pre-trained foundation model on a labeled dataset, where the
    model learns to map inputs to specific desired outputs. You start with a pre-trained
    model, prepare a labelled dataset for the target task, and train the model on
    this dataset, which adjusts the model parameters to perform better on the target
    task. Figure 4.11 gives an overview of the SFT process.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a training program'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated content may be incorrect.](../Images/CH04_F11_Kimothi.png)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11  Supervised fine-tuning is a classification mode-training process.
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While foundation models generalize well for a wide array of tasks, there are
    several use cases where the need for a fine-tuned model arises. Domain adaptation
    for specialized fields such as law and healthcare, task specific optimization
    such as classification and NER (named entity recognition), and conversational
    AI, personalization are some use cases where you may observe a fine-tuned model
    performing better.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in the context of RAG, some criteria should be considered, while
    choosing between a foundation model and a fine-tuning one:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '*Domain specificit**y*—Foundation models have broader knowledge and can handle
    a wider range of topics and queries for general-purpose RAG systems. If your RAG
    application is specialized (say, dealing with patient records or instruction manuals
    for heavy machinery), you may find that fine-tuning the model for specific domains
    may improve performance.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval integratio**n*—If you observe that a foundation model you are using
    is not integrating the retrieved information well, a fine-tuned model trained
    to better utilize information can lead to better quality of generations.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment spee**d*—A foundation model can be quickly deployed since there
    is no additional training required. To fine-tune a model, you will need to spend
    time in gathering training data and the actual training of the model.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Customization of response**s*—For generating results in a specific format
    or custom-style elements such as tone or vocabulary, a fine-tuned model may result
    in better adherence to the requirements compared to foundation models.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Resource efficienc**y*—Fine-tuning a model requires more storage and computational
    resources. Depending on the scale of deployment, the costs may be higher for a
    fine-tuned model.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ethical alignmen**t*—A fine-tuned model allows for better control over the
    responses in adherence to ethical guidelines and even certain privacy aspects.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of the criteria is presented in table 4.3.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.3 Criteria for choosing between foundation and fine-tuned models
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Criteria | Better suitability | Explanation |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| Domain specificity | Fine-tuned models | Better performance for specialized
    applications (e.g., patient records and instruction manuals) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| Retrieval integration | Fine-tuned models | Can be trained to better utilize
    retrieved information |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| Deployment speed | Foundation models | Quicker deployment with no additional
    training required |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| Customization of responses | Fine-tuned models | Better adherence to specific
    format, style, tone, or vocabulary requirements |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| Resource efficiency | Foundation models | Requires less storage and computational
    resources |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| Ethical alignment | Fine-tuned models | Allows better control over responses
    to ethical guidelines and privacy |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: Fine-tuned models give better control over your RAG systems, but they are costly.
    There’s also a risk of overreliance on retrieval and a potential tradeoff between
    RAG performance and inherent LLM language abilities. Therefore, whether to use
    a foundation model or fine-tuning one depends on the improvements you are targeting,
    availability of data, cost, and other tradeoffs. The general recommendation is
    to start experimenting with a foundation model and then progress to supervised
    fine-tuning for performance improvement.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Open source vs. proprietary models
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Software development and distribution are represented by two fundamentally
    different approaches: open versus proprietary software. The world of LLMs is no
    different. Some LLM developers such as Meta and Mistral have made the model weights
    public to foster collaboration and community-driven innovation. In contrast, pioneers
    such as OpenAI, Anthropic, and Google have kept the models closed, offering support,
    managed services, and better user experience.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'For RAG systems, open source models provide the flexibility of customization,
    deployment method, and transparency, but warrant the need for the necessary infrastructure
    to maintain the models. Proprietary model providers might be costlier for high
    volumes but provide regular updates, ease of use, scalability, and faster development,
    among other things. Some proprietary model providers such as OpenAI have prebuilt
    RAG capabilities. Your choice of the type of model you choose may depend on some
    of the following criteria:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '*Customizatio**n*—Open source LLMs are generally considered better for customizations
    such as deep integration with custom retrieval mechanisms. A better control over
    fine-tuning is also something that open source LLMs allow for. Customization of
    proprietary models is limited to API capabilities.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ease of us**e*—Proprietary models, however, are much easier to use. Some of
    the models such as OpenAI, Cohere, and similar offer optimized, prebuilt RAG solutions.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment flexibilit**y*—Open source models can be deployed according to
    your preference (private cloud, on-premises), while proprietary models are managed
    by the providers. This also has a bearing on data security and privacy. Most proprietary
    model providers are now offering multiple deployment'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: options.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Cos**t*—Open source LLMs may come with upfront infrastructure costs, while
    proprietary models are priced based on usage. Long-term costs and query volumes
    are considerations to choose between open source and proprietary models. Large-scale
    deployments may favor the use of open source models.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between open source and proprietary models for RAG depends on factors
    such as the scale of deployment, specific domain requirements, integration needs,
    and the importance of customization in the retrieval and generation process. Apart
    from these, the need for knowledge updates, transparency, scalability, the structure
    of data, compliance, and the like will determine the choice of the model. A summary
    of the discussion is presented in table 4.4
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.4 Criteria for choosing between open source and proprietary models
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Criteria | Better suitability | Explanation |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Customization | Open source | Allows deeper integration with custom retrieval
    mechanisms and better control over fine-tuning |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| Ease of use | Proprietary | Offers optimized, prebuilt RAG solutions and
    are generally easier to use |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| Deployment flexibility | Open source | Can be deployed on private cloud or
    on-premises, offering more options |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| Cost for large-scale deployment | Open source | May be more cost-effective
    for large-scale deployments despite upfront infrastructure costs |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| Data security and privacy | Open source | Offers more control over data,
    though some private models now offer various deployment options |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| Regular updates and support | Proprietary | Typically provides regular updates
    and better support |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: A hybrid approach is also not ruled out. At a PoC stage, a proprietary model
    may make sense for quick experimentation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of popular proprietary models:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: GPT series by OpenAI ([https://platform.openai.com/docs/models](https://platform.openai.com/docs/models))
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claude series by Anthropic ([https://www.anthropic.com/claude](https://www.anthropic.com/claude))
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemini series by Google ([https://mng.bz/eBnJ](https://mng.bz/eBnJ))
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Command R series by Cohere ([https://cohere.com/command](https://cohere.com/command))
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of open source models are
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Llama series by Meta ([https://llama.meta.com/](https://llama.meta.com/))
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mistral ([https://docs.mistral.ai/getting-started/models/](https://docs.mistral.ai/getting-started/models/))
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model sizes
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLMs come in various sizes, typically measured by the number of parameters they
    contain. The size of the model greatly affects the capabilities along with the
    resource requirements.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Larger models have several billion, even trillions, of parameters. These models
    exhibit superior performance in reasoning abilities, and language understanding,
    and have broader knowledge. They can generate more coherent text, and their responses
    are contextually more accurate. However, these larger models have significantly
    high computation, storage, and energy requirements.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Smaller models with parameter sizes in millions or a few billion offer benefits
    such as faster inference times, lower resource usage, and easier deployment on
    edge devices or resource constrained environments. Researchers and developers
    continue to explore methods to achieve large-model performance with smaller and
    more efficient architectures.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'For a RAG system, the following should be assessed:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '*Resource constraints*—Small models have a much lower resource usage. Lightweight
    RAG applications with faster inference can be built with smaller'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: models.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Reasoning capability*—On the other spectrum of resource constraints is the
    language-processing ability of the model. Large models are better suited for complex
    reasoning tasks and can deal with ambiguity in the retrieved information. Smaller
    models, therefore, will rely heavily on the quality of retrieved information.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment options*—The size of large models makes it difficult to deploy
    on-edge devices. This is a flexibility that smaller models provide, bringing RAG
    applications to a wide range of devices and environments.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Context handling*—Large models may be better at integrating multiple pieces
    of retrieved information in RAG systems since they have longer context windows.
    Large models are also better at handling diverse queries, while small models struggle
    with out-of-domain queries. Large models might perform better in RAG systems with
    diverse or unpredictable query types.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, most RAG applications are built on large models. However, smaller
    models make more sense in the long-term adoption and application of the technology.
    The various factors are summarized in table 4.5
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.5 Criteria for choosing between small and large models
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Criteria | Better suitability | Explanation |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| Resource constraints | Small models | Lower resource usage; suitable for
    lightweight RAG applications |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| Reasoning capability | Large models | Better for complex reasoning tasks
    and handling ambiguity in retrieved information |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| Deployment options | Small models | More flexible; can be deployed on edge
    devices and resource-constrained environments |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| Context handling | Large models | Better at integrating multiple pieces of
    retrieved information; longer context windows |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| Query diversity | Large models | Handle diverse and unpredictable query types
    better |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| Inference speed | Small models | Faster inference times; suitable for applications
    requiring quick responses |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: 'Examples of popular small language models are:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Phi-3 by Microsoft (h[ttps://azure.microsoft.com/en-us/products/phi-3](https://azure.microsoft.com/en-us/products/phi-3))
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gemma by Google ([https://ai.google.dev/gemma](https://ai.google.dev/gemma))
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of the LLM is a core consideration in your RAG system that requires
    close attention and iterations. The performance of your system may require experimenting
    and adapting your choice of the LLM.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The list of LLMs has become almost endless. What this means for developers and
    businesses is that the technology has truly been democratized. While all LLMs
    have their unique propositions and architecture, for practical applications, there
    are a wide array of choices available. While simple RAG applications may rely
    on a single LLM provider, for more complex applications, a multi-LLM strategy
    may be beneficial.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented a simple retriever and created an augmented prompt. In the
    last section of this chapter, we round up the pipeline by creating the generation
    step.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '4.4.2 Completing the RAG pipeline: Generation using LLMs'
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have built a simple retriever using FAISS and OpenAI embeddings, and we
    created a simple augmented prompt. Now we will use OpenAI’s latest model, GPT-4o,
    to generate the response:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And there it is. We have built a generation pipeline, albeit a very simple one.
    It can now fetch information from the knowledge base and generate an answer pertinent
    to the question asked and rooted in the knowledge base. Try asking a different
    question to see how well the pipeline generalizes.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered all three steps—retrieval, augmentation, and generation—of
    the generation pipeline. With the knowledge of the indexing pipeline (covered
    in chapter 3) and the generation pipeline, you are now all set to create a basic
    RAG system. What we have discussed so far can be termed a *naïve RAG implementation*.
    Naïve RAG can be marred by inaccuracies. It can be inefficient in retrieving and
    ranking information correctly. The LLM can ignore the retrieved information and
    still hallucinate. To discuss and address these challenges, in chapter 6, we examine
    advanced strategies that allow for more complex and better-performing RAG systems.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: But before that, the question of evaluating the system arises. Is it generating
    the responses on the expected lines? Is the LLM still hallucinating? Before trying
    to improve the performance of the system, we need to be able to measure and benchmark
    it. That is what we will do in chapter 5\. We will look at the evaluation metrics
    and the popular RAG benchmarks.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval is the process of finding relevant information from the knowledge
    base based on a user query. It is a search problem to match documents with input
    queries.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The popular retrieval methods for RAG include
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TF-IDF (Term Frequency-Inverse Document Frequency**)*—Statistical measure
    of word importance in a document relative to a corpus. It can be implemented using
    LangChain’s TFIDFRetriever.'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BM25 (Best Match 25**)*—Advanced probabilistic model, an improvement over
    TF-IDF. It adjusts for document length and can be implemented using Lang­Chain’s
    BM25Retriever.'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Static word embedding**s*—Represent words as dense vectors (e.g., Word2Vec,
    GloVe) and capture semantic relationships but lack full contextual under­standing.'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contextual embedding**s*—Produced by models like BERT or OpenAI’s text embeddings.
    They provide context-aware representations and are most widely used in RAG, despite
    being computationally intensive.'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advanced retrieval method**s*—They include learned sparse retrieval, dense
    retrieval, hybrid retrieval, cross-encoder retrieval, graph-based retrieval, quantum-inspired
    retrieval, and neural IR models.'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Most advanced implementations will include a hybrid approach.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector stores and databases (e.g., FAISS, PineCone, Milvus, Weaviate), cloud
    provider solutions (e.g., Amazon Kendra, Azure AI Search, Google Vertex AI Search),
    and web information resources (e.g., Wikipedia, Arxiv, AskNews) are some of the
    popular retriever integrations provided by LangChain.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of retriever depends on factors such as accuracy, speed, and compatibility
    with the indexing method.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmentation
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Augmentation combines the user query with retrieved information to create a
    prompt for the LLM.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering is crucial for effective augmentation, aiming for accuracy
    and relevance in LLM responses.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key prompt engineering techniques for RAG include
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contextual promptin**g*—Adding retrieved information with instructions to
    focus on the provided context.'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Controlled generation promptin**g*—Instructing the LLM to admit lack of knowledge
    when information is insufficient.'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Few-shot promptin**g*—Providing examples to guide the LLM’s response format
    or style.'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chain-of-thought (CoT) promptin**g*—Introducing intermediate reasoning steps
    for complex tasks.'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced techniques—These include self-consistency, generated knowledge prompting,
    and tree of thought.
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of augmentation technique depends on the task complexity, desired
    output format, and LLM capabilities.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generation is the final step in which the LLM produces the response based on
    the augmented prompt.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be categorized based on how they’ve been trained, how they can be accessed,
    and the number of parameters they have.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised fine-tuning, or SFT, improves context use and domain optimization,
    enhances coherence, and enables source attribution; however, it comes with challenges
    such as cost, risk of overreliance on retrieval, and potential tradeoffs with
    inherent LLM abilities.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between open source and proprietary LLMs depends on customization
    needs, long-term costs, and data sensitivity.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger models come with superior reasoning, language understanding, and broader
    knowledge, and generate more coherent and contextually accurate responses but
    come with high computational and resource requirements. Smaller models allow faster
    inference, lower resource usage, and are easier to deploy on edge devices or resource-constrained
    environments but do not have the same language understanding abilities as large
    models.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular LLMs include offerings from OpenAI, Anthropic, Google, and similar,
    and open source models are available through platforms such as Hugging Face.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of LLM depends on factors such as performance requirements, resource
    constraints, deployment environment, and data sensitivity.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of LLM for RAG systems requires careful consideration, experimentation,
    and potential adaptation based on performance.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
