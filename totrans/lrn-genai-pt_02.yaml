- en: 2 Deep learning with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch tensors and basic operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for deep learning in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and training deep neural networks with PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting binary and multicategory classifications with deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a validation set to decide training stop points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we’ll use deep neural networks to generate a wide range of content,
    including text, images, shapes, music, and more. I assume you already have a foundational
    understanding of machine learning (ML) and, in particular, artificial neural networks.
    In this chapter, I’ll refresh your memory on essential concepts such as loss functions,
    activation functions, optimizers, and learning rates, which are crucial for developing
    and training deep neural networks. If you find any gaps in your understanding
    of these topics, I strongly encourage you to address them before proceeding with
    the projects in this book. Appendix B provides a summary of the basic skills and
    concepts needed, including the architecture and training of artificial neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE There are plenty of great ML books out there for you to choose from. Examples
    include *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow* (2019,
    O’Reilly) and *Machine Learning, Animated* (2023, CRC Press). Both books use TensorFlow
    to create neural networks. If you prefer a book that uses PyTorch, I recommend
    *Deep Learning with PyTorch* (2020, Manning Publications).
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI models are frequently confronted with the task of either binary
    or multicategory classification. For instance, in generative adversarial networks
    (GANs), the discriminator undertakes the essential role of a binary classifier,
    its purpose being to distinguish between the fake samples created by the generator
    from real samples from the training set. Similarly, in the context of text generation
    models, whether in recurrent neural networks or Transformers, the overarching
    objective is to predict the subsequent character or word from an extensive array
    of possibilities (essentially a multicategory classification task).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to use PyTorch to create deep neural networks
    to perform binary and multicategory classifications so that you become well-versed
    in deep learning and classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, you’ll engage in an end-to-end deep learning project in PyTorch,
    on a quest to classify grayscale images of clothing items into different categories
    such as coats, bags, sneakers, shirts, and so on. The intention is to prepare
    you for the creation of deep neural networks, capable of performing both binary
    and multicategory classification tasks in PyTorch. This, in turn, will get you
    ready for the upcoming chapters, where you use deep neural networks in PyTorch
    to create various generative models.
  prefs: []
  type: TYPE_NORMAL
- en: To train generative AI models, we harness a diverse range of data formats such
    as raw text, audio files, image pixels, and arrays of numbers. Deep neural networks
    created in PyTorch cannot take these forms of data directly as inputs. Instead,
    we must first convert them into a format that the neural networks understand and
    accept. Specifically, you’ll convert various forms of raw data into PyTorch tensors
    (fundamental data structures used to represent and manipulate data) before feeding
    them to generative AI models. Therefore, in this chapter, you’ll also learn the
    basics of data types, how to create various forms of PyTorch tensors, and how
    to use them in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing how to perform classification tasks has many practical applications
    in our society. Classifications are widely used in healthcare for diagnostic purposes,
    such as identifying whether a patient has a particular disease (e.g., positive
    or negative for a specific cancer based on medical imaging or test results). They
    play a vital role in many business tasks (stock recommendations, credit card fraud
    detection, and so on). Classification tasks are also integral to many systems
    and services that we use daily such as spam detection and facial recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Data types in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use datasets from a wide range of sources and formats in this book, and
    the first step in deep learning is to transform the inputs into arrays of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll learn how PyTorch converts different formats of data
    into algebraic structures known as *tensors*. Tensors can be represented as multidimensional
    arrays of numbers, similar to NumPy arrays but with several key differences, chief
    among them the ability of GPU accelerated training. There are different types
    of tensors depending on their end use, and you’ll learn how to create different
    types of tensors and when to use each type. We’ll discuss the data structure in
    PyTorch in this section by using the heights of the 46 U.S. presidents as our
    running example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the instructions in appendix A to create a virtual environment and
    install PyTorch and Jupyter Notebook on your computer. Open the Jupyter Notebook
    app within the virtual environment and run the following line of code in a new
    cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This command will install the Matplotlib library on your computer, enabling
    you to plot images in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Creating PyTorch tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training deep neural networks, we feed the models with arrays of numbers
    as inputs. Depending on what a generative model is trying to create, these numbers
    have different types. For example, when generating images, the inputs are raw
    pixels in the form of integers between 0 and 255, but we’ll convert them to floating-point
    numbers between –1 and 1; when generating text, there is a “vocabulary” akin to
    a dictionary, and the input is a sequence of integers telling you which entry
    in the dictionary the word corresponds to.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE The code for this chapter, as well as other chapters in this book, is
    available at the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you want to use PyTorch to calculate the average height of the 46 U.S.
    presidents. We can first collect the heights of the 46 U.S. presidents in centimeters
    and store them in a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The numbers are in chronological order: the first value in the list, 189, indicates
    that the first U.S. president, George Washington, was 189 centimeters tall. The
    last value shows that Joe Biden’s height is 183 centimeters. We can convert a
    Python list into a PyTorch tensor by using the `tensor()` method in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① Converts a Python list to a PyTorch tensor
  prefs: []
  type: TYPE_NORMAL
- en: ② Specifies the data type in the PyTorch tensor
  prefs: []
  type: TYPE_NORMAL
- en: We specify the data type using the `dtype` argument in the `tensor()` method.
    The default data type in PyTorch tensors is `float32`, a 32-bit floating-point
    number. In the preceding code cell, we converted the data type to `float64`, double-precision
    floating-point numbers. `float64` provides more precise results than `float32`,
    but it takes longer to compute. There is a tradeoff between precision and computational
    costs. Which data type to use depends on the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 lists different data types and the corresponding PyTorch tensor types.
    These include integers and floating-point numbers with different precisions. Integers
    can also be either signed or unsigned.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 Data and tensor types in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch tensor type | dtype argument in `tensor()` | Data type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `FloatTensor` | `torch.float32 or torch.float` | 32-bit floating point |'
  prefs: []
  type: TYPE_TB
- en: '| `HalfTensor` | `torch.float16 or torch.half` | 16-bit floating point |'
  prefs: []
  type: TYPE_TB
- en: '| `DoubleTensor` | `torch.float64 or torch.double` | 64-bit floating point
    |'
  prefs: []
  type: TYPE_TB
- en: '| `CharTensor` | `torch.int8` | 8-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: '| `ByteTensor` | `torch.uint8` | 8-bit integer (unsigned) |'
  prefs: []
  type: TYPE_TB
- en: '| `ShortTensor` | `torch.int16 or torch.short` | 16-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: '| `IntTensor` | `torch.int32 or torch.int` | 32-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: '| `LongTensor` | `torch.int64 or torch.long` | 64-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: You can create a tensor with a certain data type in one of the two ways. The
    first way is to use the PyTorch class as specified in the first column of table
    2.1\. The second way is to use the `torch.tensor()` method and specify the data
    type using the `dtype` argument (the value of the argument is listed in the second
    column of table 2.1). For example, to convert the Python list `[1, 2, 3]` into
    a PyTorch tensor with 32-bit integers in it, you can use two methods in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Two ways of specifying tensor types
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Uses torch.IntTensor() to specify the tensor type
  prefs: []
  type: TYPE_NORMAL
- en: ② Uses dtype=torch.int to specify the tensor type
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 2.1
  prefs: []
  type: TYPE_NORMAL
- en: Use two different methods to convert the Python list `[5, 8, 10]` into a PyTorch
    tensor with 64-bit floating-point numbers in it. Consult the third row in table
    2.1 for this question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many times, you need to create a PyTorch tensor with values 0 everywhere. For
    example, in GANs, we create a tensor of zeros as the labels for fake samples,
    as you’ll see in chapter 3\. The `zeros()` method in PyTorch generates a tensor
    of zeros with a certain shape. In PyTorch, a tensor is an n-dimensional array,
    and its shape is a tuple representing the size along each of its dimensions. The
    following lines of code generate a tensor of zeros with two rows and three columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The tensor has a shape of (2, 3), which means the tensor is a 2D array; there
    are two elements in the first dimension and three elements in the second dimension.
    Here, we didn’t specify the data type, and the output has the default data type
    of `float32`*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'From time to time, you need to create a PyTorch tensor with values 1 everywhere.
    For example, in GANs, we create a tensor of ones as the labels for real samples.
    Here we use the `ones()` method to create a 3D tensor with values 1 everywhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We have generated a 3D PyTorch tensor. The shape of the tensor is (1, 4, 5).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.2
  prefs: []
  type: TYPE_NORMAL
- en: Create a 3D PyTorch tensor with values 0 in it. Make the shape of the tensor
    (2, 3, 4).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a NumPy array instead of a Python list in the tensor constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 2.1.2 Index and slice PyTorch tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use square brackets (`[ ]`) to index and slice PyTorch tensors, as we do
    with Python lists. Indexing and slicing allow us to operate on one or more elements
    in a tensor, instead of on all elements. To continue our example of the heights
    of the 46 U.S. presidents, if we want to assess the height of the third president,
    Thomas Jefferson, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This leads to an output of
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the height of Thomas Jefferson was 189 centimeters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use negative indexing to count from the back of the tensor. For example,
    to find the height of Donald Trump, who is the second to last president in the
    list, we use index –2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that Trump’s height is 191 centimeters.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to know the heights of five recent presidents in the tensor
    `heights_tensor`? We can obtain a slice of the tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The colon (`:`) is used to separate the starting and end index. If no starting
    index is provided, the default is 0; if no end index is provided, you include
    the very last element in the tensor (as we did in the preceding code cell). Negative
    indexing means you count from the back. The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The results show that the five recent presidents in the tensor (Clinton, Bush,
    Obama, Trump, and Biden) are 188, 182, 185, 191, and 183 centimeters tall, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.3
  prefs: []
  type: TYPE_NORMAL
- en: Use slicing to obtain the heights of the first five U.S. presidents in the tensor
    `heights_tensor`.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 PyTorch tensor shapes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PyTorch tensors have an attribute *shape*, which tells us the dimensions of
    a tensor. It’s important to know the shapes of PyTorch tensors because mismatched
    shapes will lead to errors when we operate on them. For example, if we want to
    find out the shape of the tensor `heights_tensor`, we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that `heights_tensor` is a 1D tensor with 46 values in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also change the shape of a PyTorch tensor. To learn how, let’s first
    convert the heights from centimeters to feet. Since a foot is about 30.48 centimeters,
    we can accomplish this by dividing the tensor by 30.48:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following output (I omitted some values to save space; the
    complete output is in the book’s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The new tensor, `heights_in_feet`, stores the heights in feet. For example,
    the last value in the tensor shows that Joe Biden is 6.0039 feet tall.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `cat()` method in PyTorch to concatenate the two tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dim` argument is used in various tensor operations to specify the dimension
    along which the operation is to be performed. In the preceding code cell, `dim=0`
    means we concatenate the two tensors along the first dimension. This leads to
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting tensor is 1D with 92 values, with some values in centimeters
    and others in feet. Therefore, we need to reshape it into two rows and 46 columns
    so that the first row represents heights in centimeters and the second in feet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The new tensor, `heights_reshaped`, is 2D with a shape of (2, 46). We can index
    and slice multidimensional tensors using square brackets as well. For example,
    to print out the height of Trump in feet, we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This leads to a result of
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The command `heights_reshaped[1,-2]` tells Python to look for the value in the
    second row and the second to last column, which returns Trump’s height in feet,
    6.2664.
  prefs: []
  type: TYPE_NORMAL
- en: tip The number of indexes needed to refer to scalar values within the tensor
    is the same as the dimensionality of the tensor. That’s why we used only one index
    to locate values in the 1D tensor `heights_tensor` but we used two indexes to
    locate values in the 2D tensor `heights_reshaped`.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.4
  prefs: []
  type: TYPE_NORMAL
- en: Use indexing to obtain the height of Joe Biden in the tensor `heights_reshaped`
    in centimeters.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 Mathematical operations on PyTorch tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can conduct mathematical operations on PyTorch tensors by using different
    methods such as `mean()`, `median()`, `sum()`, `max()`, and so on. For example,
    to find the median height of the 46 presidents in centimeters, we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The code snippet `heights_reshaped[0,:]` returns the first row and all columns
    in the tensor `heights_reshaped`. The preceding line of code returns the median
    value in the first row, and this leads to an output of
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This means the median height of U.S. presidents is 182 centimeters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the average height in both rows, we can use the `dim=1` argument in
    the `mean()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `dim=1` argument indicates that the averages are calculated by collapsing
    columns (the dimension indexed 1), effectively obtaining averages along the dimension
    indexed 0 (rows). The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The results show that the average values in the two rows are 180.0652 centimeters
    and 5.9077 feet.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out the tallest president, we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `torch.max()` method returns two tensors: a tensor `values` with the tallest
    president’s height (in centimeters and in feet), and a tensor `indices` with the
    indexes of the president with the maximum height. The results show that the 16th
    president (Lincoln) is the tallest, at 193 centimeters, or 6.332 feet.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2.5
  prefs: []
  type: TYPE_NORMAL
- en: Use the `torch.min()` method to find out the index and height of the shortest
    U.S. president.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 An end-to-end deep learning project with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next few sections, you’ll work through an example deep learning project
    with PyTorch, learning to classify grayscale images of clothing items into 1 of
    the 10 types. In this section, we’ll first provide a high-level overview of the
    steps involved. We then discuss how to obtain training data for this project and
    how to preprocess the data.
  prefs: []
  type: TYPE_NORMAL
- en: '2.2.1 Deep learning in PyTorch: A high-level overview'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our job in this project is to create and train a deep neural network in PyTorch
    to classify grayscale images of clothing items. Figure 2.1 provides a diagram
    of the steps involved.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F01_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 The steps involved in training a deep learning model
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll obtain a dataset of grayscale clothing images, as shown on the
    left of figure 2.1\. The images are in raw pixels, and we’ll convert them to PyTorch
    tensors in the form of float numbers (step 1). Each image comes with a label.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then create a deep neural network in PyTorch, as shown in the center of
    figure 2.1\. Some neural networks in this book involve convolutional neural networks
    (CNNs). For this simple classification problem, we’ll use dense layers only for
    the moment.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll select a loss function for multicategory classification, and cross-entropy
    loss is commonly used for this task. Cross-entropy loss measures the difference
    between the predicted probability distribution and the true distribution of the
    labels. We’ll use the Adam optimizer (a variant of the gradient descent algorithm)
    to update the network’s weights during training. We set the learning rate to 0.001\.
    The learning rate controls how much the model’s weights are adjusted with respect
    to the loss gradient during training.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers in ML
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers in ML are algorithms that update model parameters based on gradient
    information to minimize the loss function. Stochastic Gradient Descent (SGD) is
    the most fundamental optimizer, utilizing straightforward updates based on the
    loss gradient. Adam, the most popular optimizer, is known for its efficiency and
    out-of-the-box performance, as it combines the strengths of the Adaptive Gradient
    Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Despite their
    differences, all optimizers aim to iteratively adjust parameters to minimize the
    loss function, each creating a unique optimization path to reach this goal.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll divide the training data into a train set and a validation set. In ML,
    we usually use the validation set to provide an unbiased evaluation of the model
    and to select the best hyperparameters such as the learning rate, number of epochs
    of training, and so on. The validation set can also be used to avoid overfitting
    the model in which the model works well in the training set but poorly on unseen
    data. An epoch is when all the training data is used to train the model once and
    only once.
  prefs: []
  type: TYPE_NORMAL
- en: During training, you’ll iterate through the training data. During forward passes,
    you feed images through the network to obtain predictions (step 2) and compute
    the loss by comparing the predicted labels with the actual labels (step 3; see
    the right side of figure 2.1). You’ll then backpropagate the gradient through
    the network to update the weights. This is where the learning happens (step 4),
    as shown at the bottom of figure 2.1.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll use the validation set to determine when we should stop training. We
    calculate the loss in the validation set. If the model stops improving after a
    fixed number of epochs, we consider the model trained. We then evaluate the trained
    model on the test set to assess its performance in classifying images into different
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a high-level overview of how deep learning in PyTorch works,
    let’s dive into the end-to-end project!
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Preprocessing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll be using the Fashion Modified National Institute of Standards and Technology
    (MNIST) dataset in this project. Along the way, you’ll learn how to use the `datasets`
    and `transforms` packages in the Torchvision library, as well as the `Dataloader`
    packages in PyTorch that will help you for the rest of the book. You’ll use these
    tools to preprocess data throughout the book. The Torchvision library provides
    tools for image processing, including popular datasets, model architectures, and
    common image transformations for deep learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: We first import needed libraries and instantiate a `Compose()` class in the
    `transforms` package to transform raw images to PyTorch tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Transforming raw image data to PyTorch tensors
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: ① Composes several transforms together
  prefs: []
  type: TYPE_NORMAL
- en: ② Converts image pixels to PyTorch tensors
  prefs: []
  type: TYPE_NORMAL
- en: ③ Normalizes the values to the range [–1, 1]
  prefs: []
  type: TYPE_NORMAL
- en: We use the `manual_seed()` method in PyTorch to fix the random state so that
    results are reproducible. The *transforms* package in Torchvision can help create
    a series of transformations to preprocess images. The `ToTensor()` class converts
    image data (in either Python Imaging Library (PIL) image formats or NumPy arrays)
    into PyTorch tensors. In particular, the image data are integers ranging from
    0 to 255, and the `ToTensor()` class converts them to float tensors with values
    in the range of 0.0 and 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: The `Normalize()` class normalizes tensor images with mean and standard deviation
    for *n* channels. The Fashion MNIST data are grayscale images of clothing items
    so there is only one color channel. Later in this book, we’ll deal with images
    of three different color channels (red, green, and blue). In the preceding code
    cell, `Normalize([0.5],[0.5])` means that we subtract 0.5 from the data and divide
    the difference by 0.5\. The resulting image data range from –1 to 1\. Normalizing
    the input data to the range [–1, 1] allows gradient descent to operate more efficiently
    by maintaining more uniform step sizes across dimensions. This helps in faster
    convergence during training, and you’ll do this often in this book.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The code in listing 2.2 only defines the data transformation process. It
    doesn’t perform the actual transformation, which happens in the next code cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use the *datasets* package in Torchvision to download the dataset
    to a folder on your computer and perform the transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: ① Which dataset to download
  prefs: []
  type: TYPE_NORMAL
- en: ② Where to savethe data
  prefs: []
  type: TYPE_NORMAL
- en: ③ The training or test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ④ Whether or not to download the data to your computer
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Performs data transformation
  prefs: []
  type: TYPE_NORMAL
- en: 'You can print out the first sample in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The first sample consists of a tensor with 784 values and a label 9\. The 784
    numbers represent a 28 by 28 grayscale image (28 × 28 = 784), and the label 9
    means it’s an ankle boot. You may be wondering: How do you know the label 9 indicates
    an ankle boot? There are 10 different types of clothing items. The labels in the
    dataset are numbered from 0 to 9\. You can search online and find the text labels
    for the 10 categories (for example, I got the text labels here [https://github.com/pranay414/Fashion-MNIST-Pytorch](https://github.com/pranay414/Fashion-MNIST-Pytorch)).
    The list `text_labels` contains the 10 text labels corresponding to the numerical
    labels 0 to 9\. For example, if an item has a numerical label of 0 in the dataset,
    the corresponding text label is “t-shirt.” The list `text_labels` is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We can plot the data to visualize the clothing items in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Visualizing the clothing items
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: ① Where to place the image
  prefs: []
  type: TYPE_NORMAL
- en: ② Obtains the i-th image from the training data
  prefs: []
  type: TYPE_NORMAL
- en: ③ Converts the values from [–1,1] to [0,1]
  prefs: []
  type: TYPE_NORMAL
- en: ④ Reshapes the image to 28 by 28
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Adds text label to each image
  prefs: []
  type: TYPE_NORMAL
- en: The plot in figure 2.2 shows 24 clothing items such as coats, pullovers, sandals,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F02_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 Grayscale images of clothing items in the Fashion MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll learn how to create deep neural networks with PyTorch to perform binary
    and multicategory classification problems in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Binary classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll first create batches of data for training. We then build
    a deep neural network in PyTorch for this purpose and train the model using the
    data. Finally, we’ll use the trained model to make predictions and test how accurate
    the predictions are. The steps involved with binary and multicategory classifications
    are similar, with a few notable exceptions that I’ll highlight later.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Creating batches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll create a training set and a test set that contain only two types of clothing
    items: t-shirts and ankle boots. (Later in this chapter when we discuss multicategory
    classification, you’ll also learn to create a validation set to determine when
    to stop training.) The following code cell accomplishes that goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We only keep samples with numerical labels 0 and 9 to create a binary classification
    problem with a balanced training set. Next, we create batches for training the
    deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Creating batches for training and testing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates batches for the binary training set
  prefs: []
  type: TYPE_NORMAL
- en: ② Number of samples in each batch
  prefs: []
  type: TYPE_NORMAL
- en: ③ Shuffles the observations when batching
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates batches for the binary test set
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataLoader` class in the PyTorch *utils* package helps create data iterators
    in batches. We set the batch size to 64\. We created two data loaders in listing
    2.4: a training set and a test set for binary classification. We shuffle the observations
    when creating batches to avoid correlations among the original dataset: the training
    is more stable if different labels are evenly distributed in the data loader.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Building and training a binary classification model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll first create a binary classification model. We then train the model by
    using the images of t-shirts and ankle boots. Once it’s trained, we’ll see if
    the model can tell t-shirts from ankle boots. We use PyTorch to create the following
    neural network by using the Pytorch `nn.Sequential` class (in later chapters,
    you’ll also learn to use the `nn.Module` class to create PyTorch neural networks).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Creating a binary classification model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: ① PyTorch automatically detects if a CUDA-enabled GPU is available.
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a sequential neural network in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: ③ Numbers of input and output neurons in a linear layer
  prefs: []
  type: TYPE_NORMAL
- en: ④ Applies ReLU activation to outputs of the layer
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Applies sigmoid activation and moves the model to a GPU if available
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Linear()` class in PyTorch creates a linear transformation of the incoming
    data. This effectively creates a dense layer in the neural network. The input
    shape is 784 because we’ll later flatten the 2D image to a 1D vector with 28 ×
    28 = 784 values in it. We flatten the 2D image into a 1D tensor because dense
    layers only take 1D inputs. In later chapters, you’ll see that you don’t need
    to flatten images when you use convolutional layers. There are three hidden layers
    in the network, with 256, 128, and 32 neurons in them, respectively. The numbers
    256, 128, and 32 are chosen somewhat arbitrarily: changing them to, say, 300,
    200, and 50 won’t affect the training process. We apply the rectified linear unit
    (ReLU) activation function on the three hidden layers. The ReLU activation function
    decides whether a neuron should be turned on based on the weighted sum. These
    functions introduce nonlinearity to the output of a neuron so that the network
    can learn nonlinear relations between inputs and outputs. ReLU is your go-to activation
    function with very few exceptions, and you’ll encounter a few other activation
    functions in later chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: The output of the last layer of the model contains a single value, and we use
    the sigmoid activation function to squeeze the number to the range [0, 1] so that
    it can be interpreted as the probability that the object is an ankle boot. With
    the complementary probability, the object is a t-shirt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we set the learning rate and define the optimizer and the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We set the learning rate to 0.001\. What learning rate to set is an empirical
    question, and the answer comes with experience. It can also be determined by using
    hyperparameter tuning using a validation set. Most optimizers in PyTorch use a
    default learning rate of 0.001\. The Adam optimizer is a variant of the gradient
    descent algorithm, which is used to determine how much to adjust the model parameters
    in each training step. The Adam optimizer was first introduced in 2014 by Diederik
    Kingma and Jimmy Ba.^([1](#footnote-000)) In the traditional gradient descent
    algorithm, only gradients in the current iteration are considered. The Adam optimizer,
    in contrast, takes into consideration gradients in previous iterations as well.
  prefs: []
  type: TYPE_NORMAL
- en: We use `nn.BCELoss()`, which is the binary cross-entropy loss function. Loss
    functions measure how well an ML model performs. The training of a model involves
    adjusting parameters to minimize the loss function. The binary cross-entropy loss
    function is widely used in ML, particularly in binary classification problems.
    It measures the performance of a classification model whose output is a probability
    value between 0 and 1\. The cross-entropy loss increases as the predicted probability
    diverges from the actual label.
  prefs: []
  type: TYPE_NORMAL
- en: We train the neural network we just created as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Training a binary classification model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: ① Trains for 50 epochs
  prefs: []
  type: TYPE_NORMAL
- en: ② Iterates through all batches
  prefs: []
  type: TYPE_NORMAL
- en: ③ Flattens the image before moving the tensor to GPU
  prefs: []
  type: TYPE_NORMAL
- en: ④ Converts labels to 0 and 1
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculates the loss
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Backpropagation
  prefs: []
  type: TYPE_NORMAL
- en: In training deep learning models in PyTorch, `loss.backward()` computes the
    gradient of the loss with respect to each model parameter, enabling backpropagation,
    while `optimizer.step()` updates the model parameters based on these computed
    gradients to minimize the loss. We train the model for 50 epochs for simplicity
    (an epoch is when the training data is used to train the model once). In the next
    section, you’ll use a validation set and an early stopping class to determine
    how many epochs to train. In binary classifications, we label the targets as 0s
    and 1s. Since we have kept only t-shirts and ankle boots with labels 0 and 9,
    respectively, we converted them to 0 and 1 in listing 2.6\. As a result, the labels
    for the two categories of clothing items are 0 and 1, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: This training takes a few minutes if you use GPU training. It takes longer if
    you use CPU training, but the training time should be less than an hour.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Testing the binary classification model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The prediction from the trained binary classification model is a number between
    0 and 1\. We’ll use the `torch.where()` method to convert the predictions into
    0s and 1s: if the predicted probability is less than 0.5, we label the prediction
    as 0; otherwise, we label the prediction as 1\. We then compare these predictions
    with the actual labels to calculate the accuracy of the predictions. In the following
    listing, we use the trained model to make predictions on the test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 Calculating the accuracy of the predictions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all batches in the test set
  prefs: []
  type: TYPE_NORMAL
- en: ② Makes predictions using the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Compares predictions with labels
  prefs: []
  type: TYPE_NORMAL
- en: ④ Calculates accuracy in the batch
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculates accuracy in the test set
  prefs: []
  type: TYPE_NORMAL
- en: We iterate through all batches of data in the test set. The trained model produces
    a probability that the image is an ankle boot. We then convert the probability
    into 0 or 1 based on the cutoff value of 0.5, by using the `torch.where()` method.
    The predictions are either 0 (i.e., a t-shirt) or 1 (an ankle boot) after the
    conversion. We compare the predictions with the actual labels and see how many
    times the model gets it right. Results show that the accuracy of the predictions
    is 87.84% in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Multicategory classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll build a deep neural network in PyTorch to classify the
    clothing items into one of the 10 categories. We’ll then train the model with
    the Fashion MNIST dataset. Finally, we’ll use the trained model to make predictions
    and see how accurate they are. We first create a validation set and define an
    early stopping class so that we can determine when to stop training.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Validation set and early stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we build and train a deep neural network, there are many hyperparameters
    that we can choose (such as the learning rate and the number of epochs to train).
    These hyperparameters affect the performance of the model. To find the best hyperparameters,
    we can create a validation set to test the performance of the model with different
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: To give you an example, we’ll create a validation set in the multicategory classification
    to determine the optimal number of epochs to train. The reason we do this in the
    validation set instead of the training set is to avoid overfitting, when a model
    performs well in the training set but poorly in out-of-the-sample tests (i.e.,
    on unseen data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we divide 60,000 observations of the training dataset into a train set
    and a validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The original train set now becomes two sets: the new train set with 50,000
    observations and a validation set with the remaining 10,000 observations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `DataLoader` class in the PyTorch *utils* package to convert the
    train, validation, and test sets into three data iterators in batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define an `EarlyStop()` class and create an instance of the class.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 The `EarlyStop()` class to determine when to stop training
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the default value of patience to 10
  prefs: []
  type: TYPE_NORMAL
- en: ② Defines the stop() method
  prefs: []
  type: TYPE_NORMAL
- en: ③ If a new minimum loss is reached, updates the value of min_loss
  prefs: []
  type: TYPE_NORMAL
- en: ④ Counts how many epochs since the last minimum loss
  prefs: []
  type: TYPE_NORMAL
- en: The `EarlyStop()` class determines if the loss in the validation set has stopped
    improving in the last `patience=10` epochs. We set the default value of `patience`
    argument to 10, but you can choose a different value when you instantiate the
    class. The value of `patience` measures how many epochs you want to train since
    the last time the model reached the minimum loss. The `stop()` method keeps a
    record of the minimum loss and the number of epochs since the minimum loss and
    compares the number to the value of `patience`. The method returns a value of
    `True` if the number of epochs since the minimum loss is greater than the value
    of `patience`.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Building and training a multicategory classification model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Fashion MNIST dataset contains 10 different categories of clothing items.
    Therefore, we create a multicategory classification model to classify them. Next,
    you’ll learn how to create such a model and train it. You’ll also learn how to
    make predictions using the trained model and assess the accuracy of the predictions.
    We use PyTorch to create the neural network for multicategory classification in
    the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 Creating a multicategory classification model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: ① There are 10 neurons in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: ② Does not apply softmax activation on the output
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to the binary classification model we created in the last section,
    we have made a few changes here. First, the output now has 10 values in it, representing
    the 10 different types of clothing items in the dataset. Second, we have changed
    the number of neurons in the last hidden layer from 32 to 64\. A rule of thumb
    in creating deep neural networks is to gradually increase or decrease the number
    of neurons from one layer to the next. Since the number of output neurons has
    increased from 1 (in binary classification) to 10 (in multicategory classification),
    we change the number of neurons from 32 to 64 in the second to last layer to match
    the increase. However, there is nothing special about the number 64: if you use,
    say, 100 neurons in the second to last layer, you’ll get similar results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the PyTorch `nn.CrossEntropyLoss()` class as our loss function, which
    combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class. See the documentation
    here for details: [https://mng.bz/pxd2](https://mng.bz/pxd2). In particular, the
    documentation states, “This criterion computes the cross entropy loss between
    input logits and target.” This explains why we didn’t apply the softmax activation
    in the proceeding listing. In the book’s GitHub repository, I have demonstrated
    that if we use `nn.LogSoftmax()` in the model and use `nn.NLLLoss()` as the loss
    function, we obtain identical results.'
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the `nn.CrossEntropyLoss()` class will apply the softmax activation
    function on the output to squeeze the 10 numbers into the range [0, 1] before
    the logarithm operation. The preferred activation function on the output is sigmoid
    in binary classifications and softmax in multicategory classifications. Further,
    the 10 numbers after softmax activation add up to 1, which can be interpreted
    as the probabilities corresponding to the 10 types of clothing items. We’ll use
    the same learning rate and optimizer as those in the binary classification in
    the last section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `train_epoch()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The function trains the model for one epoch. The code is similar to what we
    have seen in the binary classification, except that the labels are from 0 to 9,
    instead of two numbers (0 and 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also define a `val_epoch()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The function uses the model to make predictions on images in the validation
    set and calculate the average loss per batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now train the multicategory classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We train a maximum of 100 epochs. In each epoch, we first train the model using
    the training set. We then calculate the average loss per batch in the validation
    set. We use the `EarlyStop()` class to determine if the training should stop by
    looking at the loss in the validation set. The training stops if the loss hasn’t
    improved in the last 10 epochs. After 19 epochs, the training stops.
  prefs: []
  type: TYPE_NORMAL
- en: The training takes about 5 minutes if you use GPU training, which is longer
    than the training process in binary classification since we have more observations
    in the training set now (10 clothing items instead of just 2).
  prefs: []
  type: TYPE_NORMAL
- en: The output from the model is a vector of 10 numbers. We use `torch.argmax()`
    to assign each observation a label based on the highest probability. We then compare
    the predicted label with the actual label. To illustrate how the prediction works,
    let’s look at the predictions on the first five images in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.10 Testing the trained model on five images
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: ① Plots the first five images in the test set with their labels
  prefs: []
  type: TYPE_NORMAL
- en: ② Obtains the i-th image and label in the test set
  prefs: []
  type: TYPE_NORMAL
- en: ③ Predicts using the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ④ Uses the torch.argmax() method to obtain the predicted label
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints out the actual label and the predicted label
  prefs: []
  type: TYPE_NORMAL
- en: We plot the first five clothing items in the test set in a 1 × 5 grid. We then
    use the trained model to make a prediction on each clothing item. The prediction
    is a tensor with 10 values. The `torch.argmax()` method returns the position of
    the highest probability in the tensor, and we use it as the predicted label. Finally,
    we print out both the actual label and the predicted label to compare and see
    if the predictions are correct. After running the previous code listing, you should
    see the image in figure 2.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F03_Liu.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 The first five clothing items in the test dataset and their respective
    labels. Each clothing item has a text label and a numerical label between 0 and
    9.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 shows that the first five clothing items in the test set are ankle
    boot, pullover, trouser, trouser, and shirt, respectively, with numerical labels
    9, 2, 1, 1, and 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output after running the code in listing 2.10 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows that the model has made correct predictions on all
    five clothing items.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing the random state in PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: The `torch.manual_seed()` method fixes the random state so the results are the
    same when you rerun your programs. However, you may get different results from
    those reported in this chapter even if you use the same random seed. This happens
    because different hardware and different versions of PyTorch handle floating point
    operations slightly differently. See, for example, the explanations at [https://mng.bz/RNva](https://mng.bz/RNva).
    The difference is generally minor, though, so no need to be alarmed.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we calculate the accuracy of the predictions on the whole test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.11 Testing the trained multicategory classification model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through all batches in the test set
  prefs: []
  type: TYPE_NORMAL
- en: ② Predicts using the trained model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Converts probabilities to a predicted label
  prefs: []
  type: TYPE_NORMAL
- en: ④ Compares the predicted label with the actual label
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Calculates accuracy in the test set
  prefs: []
  type: TYPE_NORMAL
- en: The output is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We iterate through all clothing items in the test set and use the trained model
    to make predictions. We then compare the predictions with the actual labels. The
    accuracy is about 88% in the out-of-sample test. Given that a random guess has
    an accuracy of about 10%, 88% accuracy is fairly high. This indicates that we
    have built and trained two successful deep learning models in PyTorch! You’ll
    use these skills quite often later in this book. For example, in chapter 3, the
    discriminator network you’ll construct is essentially a binary classification
    model, similar to what you have created in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In PyTorch, we use tensors to hold various forms of input data so we can feed
    them to deep learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can index and slice PyTorch tensors, reshape them, and conduct mathematical
    operations on them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning is a type of ML method that uses deep artificial neural networks
    to learn the relation between input and output data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ReLU activation function decides whether a neuron should be turned on based
    on the weighted sum. It introduces nonlinearity to the output of a neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss functions measure how well an ML model performs. The training of a model
    involves adjusting parameters to minimize the loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary classification is an ML model to classify observations into one of two
    categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicategory classification is an ML model to classify observations into one
    of multiple categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](#footnote-000-backlink))  Diederik Kingma and Jimmy Ba, 2014, “Adam:
    A Method for Stochastic Optimization.” [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).'
  prefs: []
  type: TYPE_NORMAL
