<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="appendix" epub:type="appendix" data-pdf-bookmark="Appendix A. Autodiff"><div class="appendix" id="autodiff_appendix">
<h1><span class="label">Appendix A. </span>Autodiff</h1>


<p>This <a data-type="indexterm" data-primary="autodiff (automatic differentiation)" id="xi_autodiffautomaticdifferentiation2166_1"/>appendix explains how PyTorch’s automatic differentiation (autodiff) feature works, and how it compares to other solutions.</p>

<p>Suppose you define a function <em>f</em>(<em>x</em>, <em>y</em>) = <em>x</em><sup>2</sup><em>y</em> + <em>y</em> + 2, and you need its partial derivatives ∂<em>f</em>/∂<em>x</em> and ∂<em>f</em>/∂<em>y</em>, typically to perform gradient descent (or some other optimization algorithm). Your main options are manual differentiation, finite difference approximation, forward-mode autodiff, and reverse-mode autodiff. PyTorch implements reverse-mode autodiff, but to fully understand it, it’s useful to look at the other options first. So let’s go through each of them, starting with manual differentiation.</p>






<section data-type="sect1" data-pdf-bookmark="Manual Differentiation"><div class="sect1" id="id409">
<h1>Manual Differentiation</h1>

<p>The <a data-type="indexterm" data-primary="autodiff (automatic differentiation)" data-secondary="manual differentiation" id="id4335"/>first approach to compute derivatives is to pick up a pencil and a piece of paper and use your calculus knowledge to derive the appropriate equation. For the function <em>f</em>(<em>x</em>, <em>y</em>) just defined, it is not too hard; you just need to use five rules:</p>

<ul>
<li>
<p>The derivative of a constant is 0.</p>
</li>
<li>
<p>The derivative of <em>λx</em> is <em>λ</em> (where <em>λ</em> is a constant).</p>
</li>
<li>
<p>The derivative of <em>x</em><sup>λ</sup> is <em>λx</em><sup><em>λ</em></sup> <sup>–</sup> <sup>1</sup>, so the derivative of <em>x</em><sup>2</sup> is 2<em>x</em>.</p>
</li>
<li>
<p>The derivative of a sum of functions is the sum of these functions’ derivatives.</p>
</li>
<li>
<p>The derivative of <em>λ</em> times a function is <em>λ</em> times its derivative.</p>
</li>
</ul>

<p>From these rules, you can derive <a data-type="xref" href="#partial_derivatives_equations">Equation A-1</a>.</p>
<div data-type="equation" id="partial_derivatives_equations">
<h5><span class="label">Equation A-1. </span>Partial derivatives of <em>f</em>(<em>x</em>, <em>y</em>)</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mstyle scriptlevel="0" displaystyle="true">
          <mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac>
        </mstyle>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi> <mn>2</mn> </msup><mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac>
          </mstyle>
          <mo>+</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>∂</mi><mi>y</mi></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac>
          </mstyle>
          <mo>+</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac>
          </mstyle>
          <mo>=</mo>
          <mi>y</mi>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi> <mn>2</mn> </msup><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac>
          </mstyle>
          <mo>+</mo>
          <mn>0</mn>
          <mo>+</mo>
          <mn>0</mn>
          <mo>=</mo>
          <mn>2</mn>
          <mi>x</mi>
          <mi>y</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mstyle scriptlevel="0" displaystyle="true">
          <mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac>
        </mstyle>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi> <mn>2</mn> </msup><mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac>
          </mstyle>
          <mo>+</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>∂</mi><mi>y</mi></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac>
          </mstyle>
          <mo>+</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac>
          </mstyle>
          <mo>=</mo>
          <msup><mi>x</mi> <mn>2</mn> </msup>
          <mo>+</mo>
          <mn>1</mn>
          <mo>+</mo>
          <mn>0</mn>
          <mo>=</mo>
          <msup><mi>x</mi> <mn>2</mn> </msup>
          <mo>+</mo>
          <mn>1</mn>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>This approach can become very tedious for more complex functions, and you run the risk of making mistakes. Fortunately, there are other options. Let’s look at finite difference approximation now.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Finite Difference Approximation"><div class="sect1" id="id373">
<h1>Finite Difference Approximation</h1>

<p>Recall<a data-type="indexterm" data-primary="autodiff (automatic differentiation)" data-secondary="finite difference approximation" id="xi_autodiffautomaticdifferentiationfinitedifferenceapproximation211037_1"/> that the derivative <em>h</em>′(<em>x</em><sub>0</sub>) of a function <em>h</em>(<em>x</em>) at a point <em>x</em><sub>0</sub> is the slope of the function at that point. More precisely, the derivative is defined as the limit of the slope of a straight line going through this point <em>x</em><sub>0</sub> and another point <em>x</em> on the function, as <em>x</em> gets infinitely close to <em>x</em><sub>0</sub> (see <a data-type="xref" href="#derivative_definition">Equation A-2</a>).</p>
<div data-type="equation" id="derivative_definition">
<h5><span class="label">Equation A-2. </span>Definition of the derivative of a function <em>h</em>(<em>x</em>) at point <em>x</em><sub>0</sub></h5>
<math display="block">
        <mrow>
          <msup><mi>h</mi> <mo>'</mo> </msup>
          <mrow>
            <mo>(</mo>
            <msub><mi>x</mi> <mn>0</mn> </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mrow>
          <mo>=</mo>
          <munder><mo movablelimits="true" form="prefix">lim</mo> <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>→</mo><msub><mi>x</mi> <mn>0</mn> </msub></mrow></mstyle></munder>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>h</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>0</mn> </msub><mo>)</mo></mrow></mrow> <mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi> <mn>0</mn> </msub></mrow></mfrac>
          </mstyle>
        </mrow>
        <mrow>
          <mo>=</mo>
          <munder><mo movablelimits="true" form="prefix">lim</mo> <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>ε</mi><mo>→</mo><mn>0</mn></mrow></mstyle></munder>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>0</mn> </msub><mo>+</mo><mi>ε</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>0</mn> </msub><mo>)</mo></mrow></mrow> <mi>ε</mi></mfrac>
          </mstyle>
        </mrow>
</math>
</div>

<p>So if we wanted to calculate the partial derivative of <em>f</em>(<em>x</em>, <em>y</em>) with regard to <em>x</em> at <em>x</em> = 3 and <em>y</em> = 4, we could compute <em>f</em>(3 + <em>ε</em>, 4) – <em>f</em>(3, 4) and divide the result by <em>ε</em>, using a very small value for <em>ε</em>. This type of numerical approximation of the derivative is called a <em>finite difference approximation</em>, and this specific equation is called <em>Newton’s difference quotient</em>. That’s exactly what the following code does:</p>

<pre translate="no" data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">f</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">x</code><code class="o">**</code><code class="mi">2</code><code class="o">*</code><code class="n">y</code> <code class="o">+</code> <code class="n">y</code> <code class="o">+</code> <code class="mi">2</code>

<code class="k">def</code> <code class="nf">derivative</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">x_eps</code><code class="p">,</code> <code class="n">y_eps</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">(</code><code class="n">f</code><code class="p">(</code><code class="n">x</code> <code class="o">+</code> <code class="n">x_eps</code><code class="p">,</code> <code class="n">y</code> <code class="o">+</code> <code class="n">y_eps</code><code class="p">)</code> <code class="o">-</code> <code class="n">f</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">))</code> <code class="o">/</code> <code class="p">(</code><code class="n">x_eps</code> <code class="o">+</code> <code class="n">y_eps</code><code class="p">)</code>

<code class="n">df_dx</code> <code class="o">=</code> <code class="n">derivative</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mf">0.00001</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
<code class="n">df_dy</code> <code class="o">=</code> <code class="n">derivative</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mf">0.00001</code><code class="p">)</code></pre>

<p>Unfortunately, the result is imprecise (and it gets worse for more complicated functions). The correct results are respectively 24 and 10, but instead we get:</p>

<pre translate="no" data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">df_dx</code>
<code class="go">24.000039999805264</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">df_dy</code>
<code class="go">10.000000000331966</code></pre>

<p>Notice that to compute both partial derivatives, we have to call <code translate="no">f()</code> at least three times (we called it four times in the preceding code, but it could be optimized). If there were 1,000 parameters, we would need to call <code translate="no">f()</code> at least 1,001 times. When you are dealing with large neural networks, this makes finite difference approximation way too inefficient.</p>

<p>However, this method is so simple to implement that it is a great tool to check that the other methods are implemented correctly. For example, if it disagrees with your manually derived function, then your function probably contains a mistake.</p>

<p>So far, we have considered two ways to compute gradients: using manual differentiation and using finite difference approximation. Unfortunately, both are fatally flawed for  training a large-scale neural network. So let’s turn to autodiff, starting with forward mode.<a data-type="indexterm" data-startref="xi_autodiffautomaticdifferentiationfinitedifferenceapproximation211037_1" id="id4336"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Forward-Mode Autodiff"><div class="sect1" id="id374">
<h1>Forward-Mode Autodiff</h1>

<p><a data-type="xref" href="#symbolic_differentiation_diagram">Figure A-1</a> <a data-type="indexterm" data-primary="autodiff (automatic differentiation)" data-secondary="forward-mode" id="xi_autodiffautomaticdifferentiationforwardmode2118038_1"/>shows how forward-mode autodiff works on an even simpler function, <em>g</em>(<em>x</em>, <em>y</em>) = 5 + <em>xy</em>. The graph for that function is represented on the left. After forward-mode autodiff, we get the graph on the right, which represents the partial derivative ∂<em>g</em>/∂<em>x</em> = 0 + (0 × <em>x</em> + <em>y</em> × 1) = <em>y</em> (we could similarly obtain the partial derivative with regard to <em>y</em>).</p>

<p>The algorithm will go through the computation graph from the inputs to the outputs (hence the name “forward mode”). It starts by getting the partial derivatives of the leaf nodes. The constant node (5) returns the constant 0, since the derivative of a constant is always 0. The variable <em>x</em> returns the constant 1 since ∂<em>x</em>/∂<em>x</em> = 1, and the variable <em>y</em> returns the constant 0 since ∂<em>y</em>/∂<em>x</em> = 0 (if we were looking for the partial derivative with regard to <em>y</em>, it would be the reverse).</p>

<p>Now we have all we need to move up the graph to the multiplication node in function <em>g</em>. Calculus tells us that the derivative of the product of two functions <em>u</em> and <em>v</em> is <span class="keep-together">∂(<em>u</em> × <em>v</em>)/∂<em>x</em></span> = ∂<em>v</em>/∂<em>x</em> × <em>u</em> + <em>v</em> × ∂<em>u</em>/∂<em>x</em>. We can therefore construct a large part of the graph on the right, representing 0 × <em>x</em> + <em>y</em> × 1.</p>

<p>Finally, we can go up to the addition node in function <em>g</em>. As mentioned, the derivative of a sum of functions is the sum of these functions’ derivatives, so we just need to create an addition node and connect it to the parts of the graph we have already computed. We get the correct partial derivative: ∂<em>g</em>/∂<em>x</em> = 0 + (0 × <em>x</em> + <em>y</em> × 1).</p>

<figure><div id="symbolic_differentiation_diagram" class="figure">
<img src="assets/hmls_aa01.png" alt="Forward-mode autodiff diagram illustrating the derivative calculation of the function g(x, y) = 5 + xy, highlighting the step-by-step computation and simplification to ∂g/∂x = y." width="1202" height="1068"/>
<h6><span class="label">Figure A-1. </span>Forward-mode autodiff</h6>
</div></figure>

<p>However, this equation can be simplified (a lot). By applying a few pruning steps to the computation graph to get rid of all the unnecessary operations, we get a much smaller graph with just one node: ∂<em>g</em>/∂<em>x</em> = <em>y</em>. In this case simplification is fairly easy, but for a more complex function, forward-mode autodiff can produce a huge graph that may be tough to simplify and lead to suboptimal performance.</p>

<p>Note that we started with a computation graph, and forward-mode autodiff produced another computation graph. This is called <em>symbolic differentiation</em>,<a data-type="indexterm" data-primary="symbolic differentiation" id="id4337"/> and it has two nice features. First, once the computation graph of the derivative has been produced, we can use it as many times as we want to compute the derivatives of the given function for any value of <em>x</em> and <em>y</em>. Second, we can run forward-mode autodiff again on the resulting graph to get second-order derivatives if we ever need to (i.e., derivatives of derivatives). We could even compute third-order derivatives, and so on.</p>

<p>But it is also possible to run forward-mode autodiff without constructing a graph (i.e., numerically, not symbolically) just by computing intermediate results on the fly. One way to do this is to use <em>dual numbers</em>,<a data-type="indexterm" data-primary="dual numbers" id="id4338"/> which are weird but fascinating numbers of the form <em>a</em> + <em>bε</em>, where <em>a</em> and <em>b</em> are real numbers, and <em>ε</em> is an infinitesimal number such that <em>ε</em><sup>2</sup> = 0 (but <em>ε</em> ≠ 0). You can think of the dual number 42 + 24<em>ε</em> as something akin to 42.0000⋯000024 with an infinite number of 0s (but of course this is simplified just to give you some idea of what dual numbers are). A dual number is represented in memory as a pair of floats. For example, 42 + 24<em>ε</em> is represented by the pair (42.0, 24.0).</p>

<p>Dual numbers can be added, multiplied, and so on, as shown in <a data-type="xref" href="#dual_numbers_operations">Equation A-3</a>.</p>
<div data-type="equation" id="dual_numbers_operations">
<h5><span class="label">Equation A-3. </span>A few operations with dual numbers</h5>
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mi>λ</mi>
          <mo>(</mo>
          <mi>a</mi>
          <mo>+</mo>
          <mi>b</mi>
          <mi>ε</mi>
          <mo>)</mo>
          <mo>=</mo>
          <mi>λ</mi>
          <mi>a</mi>
          <mo>+</mo>
          <mi>λ</mi>
          <mi>b</mi>
          <mi>ε</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mo>(</mo>
          <mi>a</mi>
          <mo>+</mo>
          <mi>b</mi>
          <mi>ε</mi>
          <mo>)</mo>
          <mo>+</mo>
          <mo>(</mo>
          <mi>c</mi>
          <mo>+</mo>
          <mi>d</mi>
          <mi>ε</mi>
          <mo>)</mo>
          <mo>=</mo>
          <mo>(</mo>
          <mi>a</mi>
          <mo>+</mo>
          <mi>c</mi>
          <mo>)</mo>
          <mo>+</mo>
          <mo>(</mo>
          <mi>b</mi>
          <mo>+</mo>
          <mi>d</mi>
          <mo>)</mo>
          <mi>ε</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd columnalign="left">
        <mrow>
          <mrow>
            <mo>(</mo>
            <mi>a</mi>
            <mo>+</mo>
            <mi>b</mi>
            <mi>ε</mi>
            <mo>)</mo>
          </mrow>
          <mo>×</mo>
          <mrow>
            <mo>(</mo>
            <mi>c</mi>
            <mo>+</mo>
            <mi>d</mi>
            <mi>ε</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mi>a</mi>
          <mi>c</mi>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mi>a</mi>
            <mi>d</mi>
            <mo>+</mo>
            <mi>b</mi>
            <mi>c</mi>
            <mo>)</mo>
          </mrow>
          <mi>ε</mi>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mi>b</mi>
            <mi>d</mi>
            <mo>)</mo>
          </mrow>
          <msup><mi>ε</mi> <mn>2</mn> </msup>
          <mo>=</mo>
          <mi>a</mi>
          <mi>c</mi>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mi>a</mi>
            <mi>d</mi>
            <mo>+</mo>
            <mi>b</mi>
            <mi>c</mi>
            <mo>)</mo>
          </mrow>
          <mi>ε</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>Most importantly, it can be shown that <em>h</em>(<em>a</em> + <em>bε</em>) = <em>h</em>(<em>a</em>) + <em>b</em> × <em>h</em>′(<em>a</em>)<em>ε</em>, so computing <em>h</em>(<em>a</em> + <em>ε</em>) gives you both <em>h</em>(<em>a</em>) and the derivative <em>h</em>′(<em>a</em>) in just one shot. <a data-type="xref" href="#autodiff_forward_diagram">Figure A-2</a> shows that the partial derivative of <em>f</em>(<em>x</em>, <em>y</em>) with regard to <em>x</em> at <em>x</em> = 3 and <em>y</em> = 4 (which I will write as ∂<em>f</em>/∂<em>x</em> (3, 4)) can be computed using dual numbers. All we need to do is compute <em>f</em>(3 + <em>ε</em>, 4); this will output a dual number whose first component is equal to <em>f</em>(3, 4) and whose second component is equal to ∂<em>f</em>/∂<em>x</em> (3, 4).</p>

<figure class="width-65"><div id="autodiff_forward_diagram" class="figure">
<img src="assets/hmls_aa02.png" alt="Diagram illustrating forward-mode autodiff using dual numbers to compute the partial derivative of a function \( f(x, y) = x^2y + y + 2 \) with respect to \( x \) at \( x = 3 \) and \( y = 4 \), resulting in the derivative value of 24." width="974" height="1168"/>
<h6><span class="label">Figure A-2. </span>Forward-mode autodiff using dual numbers</h6>
</div></figure>

<p>To compute ∂<em>f</em>/∂<em>y</em> (3, 4) we would have to go through the graph again, but this time with <em>x</em> = 3 and <em>y</em> = 4 + <em>ε</em>.</p>

<p>So, forward-mode autodiff is much more accurate than finite difference approximation, but it suffers from the same major flaw, at least when there are many inputs and few outputs (as is the case when dealing with neural networks): if there were 1,000 parameters, it would require 1,000 passes through the graph to compute all the partial derivatives. This is where reverse-mode autodiff shines: it can compute all of them in just two passes through the graph. Let’s see how.<a data-type="indexterm" data-startref="xi_autodiffautomaticdifferentiationforwardmode2118038_1" id="id4339"/></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Reverse-Mode Autodiff"><div class="sect1" id="id375">
<h1>Reverse-Mode Autodiff</h1>

<p>Reverse-mode<a data-type="indexterm" data-primary="autodiff (automatic differentiation)" data-secondary="reverse-mode" id="xi_autodiffautomaticdifferentiationreversemode2133613_1"/> autodiff is the solution implemented by PyTorch. It first goes through the graph in the forward direction (i.e., from the inputs to the output) to compute the value of each node. Then it does a second pass, this time in the reverse direction (i.e., from the output to the inputs) to compute all the partial derivatives. The name “reverse mode” comes from this second pass through the graph, where gradients flow in the reverse direction. <a data-type="xref" href="#autodiff_reverse_diagram">Figure A-3</a> represents the second pass. During the first pass, all the node values were computed, starting from <em>x</em> = 3 and <em>y</em> = 4. You can see those values at the bottom right of each node (e.g., <em>x</em> × <em>x</em> = 9). The nodes are labeled <em>n</em><sub>1</sub> to <em>n</em><sub>7</sub> for clarity. The output node is <em>n</em><sub>7</sub>: <em>f</em>(3, 4) = <em>n</em><sub>7</sub> = 42.</p>

<figure><div id="autodiff_reverse_diagram" class="figure">
<img src="assets/hmls_aa03.png" alt="Diagram illustrating reverse-mode autodiff showing the computation of partial derivatives in the reverse pass from output node \( n_7 \) with labeled values and derivatives for each node." width="1445" height="981"/>
<h6><span class="label">Figure A-3. </span>Reverse-mode autodiff</h6>
</div></figure>

<p>The idea is to gradually go down the graph, computing the partial derivative of 
<span class="keep-together"><em>f</em>(<em>x</em>, <em>y</em>)</span> with regard to each consecutive node, until we reach the variable nodes. For this, reverse-mode autodiff relies heavily on the <em>chain rule</em>, shown in <a data-type="xref" href="#chain_rule">Equation A-4</a>.</p>
<div data-type="equation" id="chain_rule">
<h5><span class="label">Equation A-4. </span>Chain rule</h5>
<math display="block">
  <mrow>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac>
    </mstyle>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><msub><mi>n</mi> <mi>i</mi> </msub></mrow></mfrac>
    </mstyle>
    <mo>×</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mi>∂</mi><msub><mi>n</mi> <mi>i</mi> </msub></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac>
    </mstyle>
  </mrow>
</math>
</div>

<p>Since <em>n</em><sub>7</sub> is the output node, <em>f</em> = <em>n</em><sub>7</sub> so ∂<em>f</em> / ∂<em>n</em><sub>7</sub> = 1.</p>

<p>Let’s continue down the graph to <em>n</em><sub>5</sub>: how much does <em>f</em> vary when <em>n</em><sub>5</sub> varies? The answer is ∂<em>f</em> / ∂<em>n</em><sub>5</sub> = ∂<em>f</em> / ∂<em>n</em><sub>7</sub> × ∂<em>n</em><sub>7</sub> / ∂<em>n</em><sub>5</sub>. We already know that ∂<em>f</em> / ∂<em>n</em><sub>7</sub> = 1, so all we need is ∂<em>n</em><sub>7</sub> / ∂<em>n</em><sub>5</sub>. Since <em>n</em><sub>7</sub> simply performs the sum <em>n</em><sub>5</sub> + <em>n</em><sub>6</sub>, we find that ∂<em>n</em><sub>7</sub> / ∂<em>n</em><sub>5</sub> = 1, so ∂<em>f</em> / ∂<em>n</em><sub>5</sub> = 1 × 1 = 1.</p>

<p>Now we can proceed to node <em>n</em><sub>4</sub>: how much does <em>f</em> vary when <em>n</em><sub>4</sub> varies? The answer is ∂<em>f</em> / ∂<em>n</em><sub>4</sub> = ∂<em>f</em> / ∂<em>n</em><sub>5</sub> × ∂<em>n</em><sub>5</sub> / ∂<em>n</em><sub>4</sub>. Since <em>n</em><sub>5</sub> = <em>n</em><sub>4</sub> × <em>n</em><sub>2</sub>, we find that ∂<em>n</em><sub>5</sub> / ∂<em>n</em><sub>4</sub> = <em>n</em><sub>2</sub>, so ∂<em>f</em> / ∂<em>n</em><sub>4</sub> = 1 × <em>n</em><sub>2</sub> = 4.</p>

<p>The process continues until we reach the bottom of the graph. At that point we will have calculated all the partial derivatives of <em>f</em>(<em>x</em>, <em>y</em>) at the point <em>x</em> = 3 and <em>y</em> = 4. In this example, we find ∂<em>f</em> / ∂<em>x</em> = 24 and ∂<em>f</em> / ∂<em>y</em> = 10. Sounds about right!</p>

<p>Reverse-mode autodiff is a very powerful and accurate technique, especially when there are many inputs and few outputs, since it requires only one forward pass plus one reverse pass per output to compute all the partial derivatives for all outputs with regard to all the inputs. When training neural networks, we generally want to minimize the loss, so there is a single output (the loss), and hence only two passes through the graph are needed to compute the gradients.</p>

<p>PyTorch builds a new graph on the fly during each forward pass. Whenever you run an operation on a tensor with <code translate="no">requires_grad=True</code>, PyTorch computes the resulting tensor and sets its <code translate="no">grad_fn</code> attribute to an operation-specific object that allows PyTorch to propagate the gradients backwards through this operation. Since the graph is built on the fly, your code can be highly dynamic, containing loops and conditionals, and everything will still work fine.</p>

<p>Reverse-mode autodiff can also handle functions that are not entirely differentiable, as long as you ask it to compute the partial derivatives at points that <em>are</em> differentiable.<a data-type="indexterm" data-startref="xi_autodiffautomaticdifferentiation2166_1" id="id4340"/><a data-type="indexterm" data-startref="xi_autodiffautomaticdifferentiationreversemode2133613_1" id="id4341"/></p>
<div data-type="tip"><h6>Tip</h6>
<p>Creating a tiny autodiff framework is a great exercise to truly master autodiff. Try creating one from scratch for a small set of operations. If you get stuck, check out this project’s <code translate="no">extra_autodiff.ipynb</code> notebook, which you can run on Colab at <a href="https://homl.info/colab-p" class="bare"><em class="hyperlink">https://homl.info/colab-p</em></a>. You can also watch Andrej Karpathy’s excellent YouTube video where he builds the <a href="https://homl.info/micrograd">micrograd library from scratch</a>.</p>
</div>
</div></section>
</div></section></div></div></body></html>