- en: Chapter 4\. Training Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we have treated machine learning models and their training algorithms
    mostly like black boxes. If you went through some of the exercises in the previous
    chapters, you may have been surprised by how much you can get done without knowing
    anything about what’s under the hood: you optimized a regression system, you improved
    a digit image classifier, and you even built a spam classifier from scratch, all
    without knowing how they actually work. Indeed, in many situations you don’t really
    need to know the implementation details.'
  prefs: []
  type: TYPE_NORMAL
- en: However, having a good understanding of how things work can help you quickly
    home in on the appropriate model, the right training algorithm to use, and a good
    set of hyperparameters for your task. Understanding what’s under the hood will
    also help you debug issues and perform error analysis more efficiently. Lastly,
    most of the topics discussed in this chapter will be essential in understanding,
    building, and training neural networks (discussed in [Part II](part02.html#neural_nets_part)
    of this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will start by looking at the linear regression model, one
    of the simplest models there is. We will discuss two very different ways to train
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a “closed-form” equation⁠^([1](ch04.html#id1433)) that directly computes
    the model parameters that best fit the model to the training set (i.e., the model
    parameters that minimize the cost function over the training set).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using an iterative optimization approach called gradient descent (GD) that
    gradually tweaks the model parameters to minimize the cost function over the training
    set, eventually converging to the same set of parameters as the first method.
    We will look at a few variants of gradient descent that we will use again and
    again when we study neural networks in [Part II](part02.html#neural_nets_part):
    batch GD, mini-batch GD, and stochastic GD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next we will look at polynomial regression, a more complex model that can fit
    nonlinear datasets. Since this model has more parameters than linear regression,
    it is more prone to overfitting the training data. We will explore how to detect
    whether this is the case using learning curves, and then we will look at several
    regularization techniques that can reduce the risk of overfitting the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will examine two more models that are commonly used for classification
    tasks: logistic regression and softmax regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There will be quite a few math equations in this chapter using basic concepts
    of linear algebra and calculus. To understand these equations, you need to be
    familiar with vectors and matrices—how to transpose them, multiply them, and invert
    them—as well as partial derivatives. If these concepts are unfamiliar, please
    review the introductory Jupyter notebooks on linear algebra and calculus provided
    in the [online supplemental material](https://github.com/ageron/handson-mlp).
    If you are truly allergic to math, you can just skip the equations; the text should
    still help you grasp most of the concepts. That said, learning the mathematical
    formalism is extremely useful, as it will allow you to read ML papers. Although
    it may seem daunting at first, it’s actually not that hard, and this chapter includes
    code that should help you make sense of the equations.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#landscape_chapter) we looked at a simple linear model
    of life satisfaction ([Equation 4-1](#life_satisfaction_equation)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-1\. A simple linear model of life satisfaction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $life normal bar satisfaction equals theta 0 plus theta 1 times GDP normal bar
    per normal bar capita$
  prefs: []
  type: TYPE_NORMAL
- en: This model is just a linear function of the input feature `GDP_per_capita`.
    *θ*[0] and *θ*[1] are the model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, a linear model makes a prediction by simply computing a weighted
    sum of the input features, plus a constant called the *bias term* (also called
    the *intercept term*), as shown in [Equation 4-2](#linear_regression_equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-2\. Linear regression model prediction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $ModifyingAbove y With caret equals theta 0 plus theta 1 x 1 plus theta 2 x
    2 plus midline-horizontal-ellipsis plus theta Subscript n Baseline x Subscript
    n$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ŷ* is the predicted value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n* is the number of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[*i*] is the *i*^(th) feature value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*θ*[*j*] is the *j*^(th) model parameter, including the bias term *θ*[0] and
    the feature weights *θ*[1], *θ*[2], ⋯, *θ*[*n*].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be written much more concisely using a vectorized form, as shown in
    [Equation 4-3](#linear_regression_prediction_vectorized_equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-3\. Linear regression model prediction (vectorized form)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>h</mi><mi
    mathvariant="bold">θ</mi></msub><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi
    mathvariant="bold">θ</mi><mo>·</mo><mi mathvariant="bold">x</mi>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[**θ**] is the hypothesis function, using the model parameters **θ**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**θ** is the model’s *parameter vector*, containing the bias term *θ*[0] and
    the feature weights *θ*[1] to *θ*[*n*].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**x** is the instance’s *feature vector*, containing *x*[0] to *x*[*n*], with
    *x*[0] always equal to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**θ** · **x** is the dot product of the vectors **θ** and **x**, which is equal
    to *θ*[0]*x*[0] + *θ*[1]*x*[1] + *θ*[2]*x*[2] + ... + *θ*[*n*]*x*[*n*].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In machine learning, vectors are often represented as *column vectors*, which
    are 2D arrays with a single column. If **θ** and **x** are column vectors, then
    the prediction is <mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi
    mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi mathvariant="bold">x</mi>, where
    <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup> is the *transpose* of **θ**
    (a row vector instead of a column vector) and <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup><mi
    mathvariant="bold">x</mi> is the matrix multiplication of <msup><mi mathvariant="bold">θ</mi><mo>⊺</mo></msup>
    and **x**. It is of course the same prediction, except that it is now represented
    as a single-cell matrix rather than a scalar value. In this book I will use this
    notation to avoid switching between dot products and matrix multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: OK, that’s the linear regression model—but how do we train it? Well, recall
    that training a model means setting its parameters so that the model best fits
    the training set. For this purpose, we first need a measure of how well (or poorly)
    the model fits the training data. In [Chapter 2](ch02.html#project_chapter) we
    saw that the most common performance measure of a regression model is the root
    mean squared error ([Equation 2-1](ch02.html#rmse_equation)). Therefore, to train
    a linear regression model, we need to find the value of **θ** that minimizes the
    RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than
    the RMSE, and it leads to the same result (because the value that minimizes a
    positive function also minimizes its square root).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Learning algorithms will often optimize a different loss function during training
    than the performance measure used to evaluate the final model. This is generally
    because the function is easier to optimize and/or because it has extra terms needed
    during training only (e.g., for regularization). A good performance metric is
    as close as possible to the final business objective. A good training loss is
    easy to optimize and strongly correlated with the metric. For example, classifiers
    are often trained using a cost function such as the log loss (as you will see
    later in this chapter) but evaluated using precision/recall. The log loss is easy
    to minimize, and doing so will usually improve precision/recall.
  prefs: []
  type: TYPE_NORMAL
- en: The MSE of a linear regression hypothesis *h*[**θ**] on a training set **X**
    is calculated using [Equation 4-4](#mse_cost_function).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-4\. MSE cost function for a linear regression model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mrow><mtext>MSE</mtext> <mrow><mo>(</mo> <mi mathvariant="bold">X</mi> <mo
    lspace="0%" rspace="0%">,</mo> <mi mathvariant="bold">y</mi> <mo lspace="0%" rspace="0%">,</mo>
    <msub><mi>h</mi> <mi mathvariant="bold">θ</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn> <mi>m</mi></mfrac></mstyle>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover>
    <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">θ</mi> <mo>⊺</mo></msup> <msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><msup><mi>y</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow>
  prefs: []
  type: TYPE_NORMAL
- en: Most of these notations were presented in [Chapter 2](ch02.html#project_chapter)
    (see [“Notations”](ch02.html#notations)). The only difference is that we write
    *h*[**θ**] instead of just *h* to make it clear that the model is parametrized
    by the vector **θ**. To simplify notations, we will just write MSE(**θ**) instead
    of MSE(**X**, *h*[**θ**]).
  prefs: []
  type: TYPE_NORMAL
- en: The Normal Equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To find the value of **θ** that minimizes the MSE, there exists a *closed-form
    solution*—in other words, a mathematical equation that gives the result directly.
    This is called the *normal equation* ([Equation 4-5](#equation_four_four)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-5\. Normal equation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mrow><mover accent="true"><mi mathvariant="bold">θ</mi> <mo>^</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup>
    <mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">y</mi></mrow>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover> is the
    value of **θ** that minimizes the cost function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**y** is the vector of target values containing *y*^((1)) to *y*^((*m*)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s generate some linear-looking data to test this equation on ([Figure 4-1](#generated_data_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Scatter plot showing a linear dataset with increasing trend, generated to
    illustrate the application of the normal equation in linear regression.](assets/hmls_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. A randomly generated linear dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let’s compute <mover accent="true"><mi mathvariant="bold">θ</mi><mo>^</mo></mover>
    using the normal equation. We will use the `inv()` function from NumPy’s linear
    algebra module (`np.linalg`) to compute the inverse of a matrix, and the `@` operator
    for matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `@` operator performs matrix multiplication. If `A` and `B` are NumPy arrays,
    then `A @ B` is equivalent to `np.matmul(A, B)`. Many other libraries, like TensorFlow,
    PyTorch, and JAX, support the `@` operator as well. However, you cannot use `@`
    on pure Python arrays (i.e., lists of lists).
  prefs: []
  type: TYPE_NORMAL
- en: 'The function that we used to generate the data is *y* = 4 + 3*x*[1] + Gaussian
    noise. Let’s see what the equation found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3][PRE4] [PRE5][PRE6]`` [PRE7][PRE8]``py[PRE9] import matplotlib.pyplot
    as plt  plt.plot(X_new, y_predict, "r-", label="Predictions") plt.plot(X, y, "b.")
    [...]  # beautify the figure: add labels, axis, grid, and legend plt.show() [PRE10]
    >>> from sklearn.linear_model import LinearRegression `>>>` `lin_reg` `=` `LinearRegression``()`
    [PRE11]` `>>>` `lin_reg``.``intercept_``,` `lin_reg``.``coef_` [PRE12] [PRE13]``
    [PRE14][PRE15][PRE16][PRE17] >>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b,
    y, rcond=1e-6) `>>>` `theta_best_svd` `` `array([[3.69084138],`  `[3.32960458]])`
    `` [PRE18]`` [PRE19] >>> np.linalg.pinv(X_b) @ y `array([[3.69084138],`  `[3.32960458]])`
    [PRE20]` [PRE21][PRE22][PRE23][PRE24][PRE25] [PRE26]`py` [PRE27]  [PRE28] ``##
    Computational Complexity    The normal equation computes the inverse of **X**^⊺
    **X**, which is an (*n* + 1) × (*n* + 1) matrix (where *n* is the number of features).
    The *computational complexity* of inverting such a matrix is typically about *O*(*n*^(2.4))
    to *O*(*n*³), depending on the implementation. In other words, if you double the
    number of features, you multiply the computation time by roughly 2^(2.4) = 5.3
    to 2³ = 8.    The SVD approach used by Scikit-Learn’s `LinearRegression` class
    is about *O*(*n*²). If you double the number of features, you multiply the computation
    time by roughly 4.    ###### Warning    Both the normal equation and the SVD approach
    get very slow when the number of features grows large (e.g., 100,000). On the
    positive side, both are linear with regard to the number of instances in the training
    set (they are *O*(*m*)), so they handle large training sets efficiently, provided
    they can fit in memory.    Also, once you have trained your linear regression
    model (using the normal equation or any other algorithm), predictions are very
    fast: the computational complexity is linear with regard to both the number of
    instances you want to make predictions on and the number of features. In other
    words, making predictions on twice as many instances (or twice as many features)
    will take roughly twice as much time.    Now we will look at a very different
    way to train a linear regression model, which is better suited for cases where
    there are a large number of features or too many training instances to fit in
    memory.`` [PRE29]`  [PRE30][PRE31][PRE32]` [PRE33][PRE34][PRE35] [PRE36]`py[PRE37]`py[PRE38]py[PRE39][PRE40][PRE41]py
    rng = np.random.default_rng(seed=42) m = 200  # number of instances X = 6 * rng.random((m,
    1)) - 3 y = 0.5 * X ** 2 + X + 2 + rng.standard_normal((m, 1)) [PRE42]py >>> from
    sklearn.preprocessing import PolynomialFeatures `>>>` `poly_features` `=` `PolynomialFeatures``(``degree``=``2``,`
    `include_bias``=``False``)` [PRE43] `X_poly` now contains the original feature
    of `X` plus the square of this feature. Now we can fit a `LinearRegression` model
    to this extended training data ([Figure 4-13](#quadratic_predictions_plot)):    [PRE44]
    `>>>` `lin_reg``.``intercept_``,` `lin_reg``.``coef_` `` `(array([2.00540719]),
    array([[1.11022126, 0.50526985]]))` `` [PRE45]  [PRE46] ``![Scatter plot showing
    polynomial regression model predictions with a red curve fitting the data points,
    illustrating the relationship between x1 and y.](assets/hmls_0413.png)  ######
    Figure 4-13\. Polynomial regression model predictions    Not bad: the model estimates
    <mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mo>=</mo> <mn>0.56</mn>
    <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo>
    <mn>0.93</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mn>1.78</mn></mrow>
    when in fact the original function was <mrow><mi>y</mi> <mo>=</mo> <mn>0.5</mn>
    <msup><mrow><msub><mi>x</mi> <mn>1</mn></msub></mrow> <mn>2</mn></msup> <mo>+</mo>
    <mn>1.0</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mn>2.0</mn> <mo>+</mo>
    <mtext>Gaussian noise</mtext></mrow> .    Note that when there are multiple features,
    polynomial regression is capable of finding relationships between features, which
    is something a plain linear regression model cannot do. This is made possible
    by the fact that `PolynomialFeatures` also adds all combinations of features up
    to the given degree. For example, if there were two features *a* and *b*, `PolynomialFeatures`
    with `degree=3` would not only add the features *a*², *a*³, *b*², and *b*³, but
    also the combinations *ab*, *a*²*b*, and *ab*².    ###### Warning    `PolynomialFeatures(degree=*d*)`
    transforms an array containing *n* features into an array containing (*n* + *d*)!
    / *d*!*n*! features, where *n*! is the *factorial* of *n*, equal to 1 × 2 × 3
    × ⋯ × *n*. Beware of the combinatorial explosion of the number of features!``
    [PRE47]` [PRE48][PRE49][PRE50][PRE51][PRE52]``py[PRE53]py # Learning Curves    If
    you perform high-degree polynomial regression, you will likely fit the training
    data much better than with plain linear regression. For example, [Figure 4-14](#high_degree_polynomials_plot)
    applies a 300-degree polynomial model to the preceding training data, and compares
    the result with a pure linear model and a quadratic model (second-degree polynomial).
    Notice how the 300-degree polynomial model wiggles around to get as close as possible
    to the training instances.  ![Graph comparing linear (1-degree), quadratic (2-degree),
    and 300-degree polynomial regression models, illustrating overfitting with the
    high-degree polynomial as it closely follows the data points.](assets/hmls_0414.png)  ######
    Figure 4-14\. High-degree polynomial regression    This high-degree polynomial
    regression model is severely overfitting the training data, while the linear model
    is underfitting it. The model that will generalize best in this case is the quadratic
    model, which makes sense because the data was generated using a quadratic model.
    But in general you won’t know what function generated the data, so how can you
    decide how complex your model should be? How can you tell that your model is overfitting
    or underfitting the data?    In [Chapter 2](ch02.html#project_chapter) you used
    cross-validation to get an estimate of a model’s generalization performance. If
    a model performs well on the training data but generalizes poorly according to
    the cross-validation metrics, then your model is overfitting. If it performs poorly
    on both, then it is underfitting. This is one way to tell when a model is too
    simple or too complex.    Another way to tell is to look at the *learning curves*,
    which are plots of the model’s training error and validation error as a function
    of the training iteration: just evaluate the model at regular intervals during
    training on both the training set and the validation set, and plot the results.
    If the model cannot be trained incrementally (i.e., if it does not support `partial_fit()`
    or `warm_start`), then you must train it several times on gradually larger subsets
    of the training set.    Scikit-Learn has a useful `learning_curve()` function
    to help with this: it trains and evaluates the model using cross-validation. By
    default it retrains the model on growing subsets of the training set, but if the
    model supports incremental learning you can set `exploit_incremental_learning=True`
    when calling `learning_curve()` and it will train the model incrementally instead.
    The function returns the training set sizes at which it evaluated the model, and
    the training and validation scores it measured for each size and for each cross-validation
    fold. Let’s use this function to look at the learning curves of the plain linear
    regression model (see [Figure 4-15](#underfitting_learning_curves_plot)):    [PRE54]py  ![Line
    graph of learning curves shows root mean square error (RMSE) decreasing and plateauing
    for both training and validation sets as training set size increases, indicating
    underfitting.](assets/hmls_0415.png)  ###### Figure 4-15\. Learning curves    This
    model is underfitting, it’s too simple for the data. How can we tell? Well, let’s
    look at the training error. When there are just one or two instances in the training
    set, the model can fit them perfectly, which is why the curve starts at zero.
    But as new instances are added to the training set, it becomes impossible for
    the model to fit the training data perfectly, both because the data is noisy and
    because it is not linear at all. So the error on the training data goes up until
    it reaches a plateau, at which point adding new instances to the training set
    doesn’t make the average error much better or worse. Now let’s look at the validation
    error. When the model is trained on very few training instances, it is incapable
    of generalizing properly, which is why the validation error is initially quite
    large. Then, as the model is shown more training examples, it learns, and thus
    the validation error slowly goes down. However, once again a straight line cannot
    do a good job of modeling the data, so the error ends up at a plateau, very close
    to the other curve.    These learning curves are typical of a model that’s underfitting.
    Both curves have reached a plateau; they are close and fairly high.    ######
    Tip    If your model is underfitting the training data, adding more training examples
    will not help. You need to use a better model or come up with better features.    Now
    let’s look at the learning curves of a 10th-degree polynomial model on the same
    data ([Figure 4-16](#learning_curves_plot)):    [PRE55]py  ![Learning curves for
    a 10th-degree polynomial model showing root mean square error (RMSE) decreasing
    with larger training set sizes, with validation error stabilizing.](assets/hmls_0416.png)  ######
    Figure 4-16\. Learning curves for the 10th-degree polynomial model    These learning
    curves look a bit like the previous ones, but there are two very important differences:    *   The
    error on the training data is much lower than before.           *   There is a
    gap between the curves. This means that the model performs better on the training
    data than on the validation data, which is the hallmark of an overfitting model.
    If you used a much larger training set, however, the two curves would continue
    to get closer.              ###### Tip    One way to improve an overfitting model
    is to feed it more training data until the validation error gets close enough
    to the training error.    # Regularized Linear Models    As you saw in Chapters
    [1](ch01.html#landscape_chapter) and [2](ch02.html#project_chapter), a good way
    to reduce overfitting is to regularize the model (i.e., to constrain it): the
    fewer degrees of freedom it has, the harder it will be for it to overfit the data.
    A simple way to regularize a polynomial model is to reduce the number of polynomial
    degrees.    What about linear models? Can we regularize them too? You may wonder
    why we may want to do that: aren’t linear models constrained enough already? Well,
    linear regression makes a few assumptions, including the fact that the true relationship
    between the inputs and the outputs is linear, the noise has zero mean, constant
    variance, and is independent of the inputs, plus the input matrix has full rank,
    meaning that the inputs are not colinear⁠^([7](ch04.html#id1539)) and there at
    least as many samples as parameters. In practice, some assumptions don’t hold
    perfectly. For example, some inputs may be close to colinear, which makes linear
    regression numerically unstable, meaning that very small differences in the training
    set can have a big impact on the trained model. Regularization can stabilize linear
    models and make them more accurate.    So how can we regularize a linear model?
    This is usually done by constraining its weights. In this section, we will discuss
    ridge regression, lasso regression, and elastic net regression, which implement
    three different ways to do that.    ## Ridge Regression    *Ridge regression*
    (also called *Tikhonov regularization*) is a regularized version of linear regression:
    a *regularization term* equal to <mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>
    is added to the MSE. This forces the learning algorithm to not only fit the data
    but also keep the model weights as small as possible. This constraint makes the
    model less flexible, preventing it from stretching itself too much to fit every
    data point: this reduces the risk of overfitting. Note that the regularization
    term should only be added to the cost function during training. Once the model
    is trained, you want to use the unregularized MSE (or the RMSE) to evaluate the
    model’s performance.    The hyperparameter *α* controls how much you want to regularize
    the model. If *α* = 0, then ridge regression is just linear regression. If *α*
    is very large, then all weights end up very close to zero and the result is a
    flat line going through the data’s mean. [Equation 4-9](#ridge_cost_function)
    presents the ridge regression cost function.⁠^([8](ch04.html#id1544))    #####
    Equation 4-9\. Ridge regression cost function  <mrow><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>=</mo><mrow><mtext>MSE</mtext><mo>(</mo><mi
    mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>α</mi><mi>m</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><msub><mi>θ</mi><mi>i</mi></msub><mn>2</mn></msup>  Note
    that the bias term *θ*[0] is not regularized (the sum starts at *i* = 1, not 0).
    If we define **w** as the vector of feature weights (*θ*[1] to *θ*[*n*]), then
    the regularization term is equal to *α*(∥**w**∥[2])² / *m*, where ∥**w**∥[2] represents
    the ℓ[2] norm of the weight vector.⁠^([9](ch04.html#id1545)) For batch gradient
    descent, just add 2*α***w** / *m* to the part of the MSE gradient vector that
    corresponds to the feature weights, without adding anything to the gradient of
    the bias term (see [Equation 4-7](#mse_gradient_vector)).    ###### Warning    It
    is important to scale the data (e.g., using a `StandardScaler`) before performing
    ridge regression, as it is sensitive to the scale of the input features. This
    is true of most regularized models.    [Figure 4-18](#ridge_regression_plot) shows
    several ridge models that were trained on some very noisy linear data using different
    *α* values. On the left, plain ridge models are used, leading to linear predictions.
    On the right, the data is first expanded using `PolynomialFeatures(degree=10)`,
    then it is scaled using a `StandardScaler`, and finally the ridge models are applied
    to the resulting features: this is polynomial regression with ridge regularization.
    Note how increasing *α* leads to flatter (i.e., less extreme, more reasonable)
    predictions, thus reducing the model’s variance but increasing its bias.  ![Graphs
    showing linear and polynomial ridge regression models applied to noisy linear
    data, with increasing alpha values resulting in flatter predictions.](assets/hmls_0418.png)  ######
    Figure 4-18\. Linear (left) and polynomial (right) models, both with various levels
    of ridge regularization    As with linear regression, we can perform ridge regression
    either by computing a closed-form equation or by performing gradient descent.
    The pros and cons are the same. [Equation 4-10](#ridge_regression_solution) shows
    the closed-form solution, where **A** is the (*n* + 1) × (*n* + 1) *identity matrix*,⁠^([10](ch04.html#id1552))
    except with a 0 in the top-left cell, corresponding to the bias term.    #####
    Equation 4-10\. Ridge regression closed-form solution  <mrow><mover accent="true"><mi
    mathvariant="bold">θ</mi> <mo>^</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi
    mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">X</mi><mo>+</mo><mi>α</mi><mi
    mathvariant="bold">A</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi mathvariant="bold">X</mi> <mo>⊺</mo></msup> <mi mathvariant="bold">y</mi></mrow>  Here
    is how to perform ridge regression with Scikit-Learn using a closed-form solution
    (a variant of [Equation 4-10](#ridge_regression_solution) that uses a matrix factorization
    technique by André-Louis Cholesky):    [PRE56]py` `>>>` `ridge_reg``.``fit``(``X``,`
    `y``)` [PRE57]py [PRE58]``py`` [PRE59]py And using stochastic gradient descent:⁠^([11](ch04.html#id1553))    [PRE60]py``
    `...` [PRE61]` [PRE62]` [PRE63] [PRE64][PRE65][PRE66][PRE67][PRE68][PRE69]py[PRE70]py`
    [PRE71]  [PRE72][PRE73]py[PRE74]`py `>>>` `list``(``iris``)` [PRE75]`py [PRE76]py[PRE77]``
    [PRE78] from sklearn.linear_model import LogisticRegression from sklearn.model_selection
    import train_test_split  X = iris.data[["petal width (cm)"]].values y = iris.target_names[iris.target]
    == ''virginica'' X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)  log_reg
    = LogisticRegression(random_state=42) log_reg.fit(X_train, y_train) [PRE79] X_new
    = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector y_proba
    = log_reg.predict_proba(X_new) decision_boundary = X_new[y_proba[:, 1] >= 0.5][0,
    0]  plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2,          label="Not Iris
    virginica proba") plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris
    virginica proba") plt.plot([decision_boundary, decision_boundary], [0, 1], "k:",
    linewidth=2,          label="Decision boundary") [...] # beautify the figure:
    add grid, labels, axis, legend, arrows, and samples plt.show() [PRE80] >>> decision_boundary
    `np.float64(1.6516516516516517)` `>>>` `log_reg``.``predict``([[``1.7``],` `[``1.5``]])`
    `` `array([ True, False])` `` [PRE81] ``[Figure 4-25](#logistic_regression_contour_plot)
    shows the same dataset, but this time displaying two features: petal width and
    length. Once trained, the logistic regression classifier can, based on these two
    features, estimate the probability that a new flower is an *Iris virginica*. The
    dashed line represents the points where the model estimates a 50% probability:
    this is the model’s decision boundary. Note that it is a linear boundary.⁠^([16](ch04.html#id1613))
    Each parallel line represents the points where the model outputs a specific probability,
    from 15% (bottom left) to 90% (top right). All the flowers beyond the top-right
    line have over a 90% chance of being *Iris virginica*, according to the model.  ![Contour
    plot of logistic regression showing decision boundary and probability lines for
    classifying Iris virginica based on petal width and length.](assets/hmls_0425.png)  ######
    Figure 4-25\. Linear decision boundary    ###### Note    The hyperparameter controlling
    the regularization strength of a Scikit-Learn `LogisticRegression` model is not
    `alpha` (as in other linear models), but its inverse: `C`. The higher the value
    of `C`, the *less* the model is regularized.    Just like the other linear models,
    logistic regression models can be regularized using ℓ[1] or ℓ[2] penalties. Scikit-Learn
    actually adds an ℓ[2] penalty by default.`` [PRE82]` [PRE83][PRE84][PRE85]`` [PRE86]
    X = iris.data[["petal length (cm)", "petal width (cm)"]].values y = iris["target"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)  softmax_reg
    = LogisticRegression(C=30, random_state=42) softmax_reg.fit(X_train, y_train)
    [PRE87] >>> softmax_reg.predict([[5, 2]]) `array([2])` `>>>` `softmax_reg``.``predict_proba``([[``5``,`
    `2``]])``.``round``(``2``)` `` `array([[0\.  , 0.04, 0.96]])` `` [PRE88]` [PRE89][PRE90]`
    [PRE91] [PRE92][PRE93][PRE94][PRE95] [PRE96]`py [PRE97]`py` [PRE98]`py`` [PRE99]`py[PRE100][PRE101][PRE102]
    [PRE103][PRE104][PRE105][PRE106][PRE107]`` [PRE108][PRE109][PRE110] [PRE111]`py[PRE112]`'
  prefs: []
  type: TYPE_NORMAL
