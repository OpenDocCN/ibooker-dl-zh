["```py\nimport torch\n\ntorch.manual_seed(0)                                ①\n\nobservations = 2048\n\ntrain_data = torch.zeros((observations, 2))         ②\n\ntrain_data[:,0]=50*torch.rand(observations)         ③\n\ntrain_data[:,1]=1.08**train_data[:,0]               ④\n```", "```py\nimport matplotlib.pyplot as plt\n\nfig=plt.figure(dpi=100,figsize=(8,6))\nplt.plot(train_data[:,0],train_data[:,1],\".\",c=\"r\")    ①\nplt.xlabel(\"values of x\",fontsize=15)\nplt.ylabel(\"values of $y=1.08^x$\",fontsize=15)         ②\nplt.title(\"An exponential growth shape\",fontsize=20)   ③\nplt.show()\n```", "```py\nfrom torch.utils.data import DataLoader\n\nbatch_size=128\ntrain_loader=DataLoader(\n    train_data,\n    batch_size=batch_size,\n    shuffle=True)\n```", "```py\nbatch0=next(iter(train_loader))\nprint(batch0)\n```", "```py\nimport torch.nn as nn\n\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"    ①\n\nD=nn.Sequential(\n    nn.Linear(2,256),                                    ②\n    nn.ReLU(),\n    nn.Dropout(0.3),                                     ③\n    nn.Linear(256,128),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(128,64),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(64,1),                                     ④\n    nn.Sigmoid()).to(device)\n```", "```py\nG=nn.Sequential(\n    nn.Linear(2,16),               ①\n    nn.ReLU(),\n    nn.Linear(16,32),\n    nn.ReLU(),\n    nn.Linear(32,2)).to(device)    ②\n```", "```py\nloss_fn=nn.BCELoss()\nlr=0.0005\noptimD=torch.optim.Adam(D.parameters(),lr=lr)\noptimG=torch.optim.Adam(G.parameters(),lr=lr)\n```", "```py\nmse=nn.MSELoss()                              ①\n\ndef performance(fake_samples):\n    real=1.08**fake_samples[:,0]              ②\n    mseloss=mse(fake_samples[:,1],real)       ③\n    return mseloss\n```", "```py\nclass EarlyStop:\n    def __init__(self, patience=1000):       ①\n        self.patience = patience\n        self.steps = 0\n        self.min_gdif = float('inf')\n    def stop(self, gdif):                    ②\n        if gdif < self.min_gdif:             ③\n            self.min_gdif = gdif\n            self.steps = 0\n        elif gdif >= self.min_gdif:\n            self.steps += 1\n        if self.steps >= self.patience:      ④\n            return True\n        else:\n            return False\n\nstopper=EarlyStop()\n```", "```py\nreal_labels=torch.ones((batch_size,1))\nreal_labels=real_labels.to(device)\n\nfake_labels=torch.zeros((batch_size,1))\nfake_labels=fake_labels.to(device)\n```", "```py\ndef train_D_on_real(real_samples):\n    real_samples=real_samples.to(device)\n    optimD.zero_grad()\n    out_D=D(real_samples)                    ①\n    loss_D=loss_fn(out_D,real_labels)        ②\n    loss_D.backward()\n    optimD.step()                            ③\n    return loss_D\n```", "```py\ndef train_D_on_fake():\n    noise=torch.randn((batch_size,2))\n    noise=noise.to(device)\n    fake_samples=G(noise)                    ①\n    optimD.zero_grad()\n    out_D=D(fake_samples)                    ②\n    loss_D=loss_fn(out_D,fake_labels)        ③\n    loss_D.backward()\n    optimD.step()                            ④\n    return loss_D\n```", "```py\ndef train_G(): \n    noise=torch.randn((batch_size,2))\n    noise=noise.to(device)\n    optimG.zero_grad()\n    fake_samples=G(noise)                     ①\n    out_G=D(fake_samples)                     ②\n    loss_G=loss_fn(out_G,real_labels)         ③\n    loss_G.backward()\n    optimG.step()                             ④\n    return loss_G, fake_samples \n```", "```py\nimport os\nos.makedirs(\"files\", exist_ok=True)                           ①\n\ndef test_epoch(epoch,gloss,dloss,n,fake_samples):\n    if epoch==0 or (epoch+1)%25==0:\n        g=gloss.item()/n\n        d=dloss.item()/n\n        print(f\"at epoch {epoch+1}, G loss: {g}, D loss {d}\") ②\n        fake=fake_samples.detach().cpu().numpy()\n        plt.figure(dpi=200)\n        plt.plot(fake[:,0],fake[:,1],\"*\",c=\"g\",\n            label=\"generated samples\")                        ③\n        plt.plot(train_data[:,0],train_data[:,1],\".\",c=\"r\",\n            alpha=0.1,label=\"real samples\")                   ④\n        plt.title(f\"epoch {epoch+1}\")\n        plt.xlim(0,50)\n        plt.ylim(0,50)\n        plt.legend()\n        plt.savefig(f\"files/p{epoch+1}.png\")\n        plt.show()\n```", "```py\nfor epoch in range(10000):                                  ①\n    gloss=0\n    dloss=0\n    for n, real_samples in enumerate(train_loader):         ②\n        loss_D=train_D_on_real(real_samples)\n        dloss+=loss_D\n        loss_D=train_D_on_fake()\n        dloss+=loss_D\n        loss_G,fake_samples=train_G()\n        gloss+=loss_G\n    test_epoch(epoch,gloss,dloss,n,fake_samples)            ③\n    gdif=performance(fake_samples).item()\n    if stopper.stop(gdif)==True:                            ④\n        break\n```", "```py\nimport os\nos.makedirs(\"files\", exist_ok=True)\nscripted = torch.jit.script(G) \nscripted.save('files/exponential.pt') \n```", "```py\nnew_G=torch.jit.load('files/exponential.pt',\n                     map_location=device)\nnew_G.eval()\n```", "```py\nnoise=torch.randn((batch_size,2)).to(device)\nnew_data=new_G(noise) \n```", "```py\nfig=plt.figure(dpi=100)\nplt.plot(new_data.detach().cpu().numpy()[:,0],\n  new_data.detach().cpu().numpy()[:,1],\"*\",c=\"g\",\n        label=\"generated samples\")                    ①\nplt.plot(train_data[:,0],train_data[:,1],\".\",c=\"r\",\n         alpha=0.1,label=\"real samples\")              ②\nplt.title(\"Inverted-U Shape Generated by GANs\")\nplt.xlim(0,50)\nplt.ylim(0,50)\nplt.legend()\nplt.show()\n```", "```py\nimport torch\ndef onehot_encoder(position,depth):\n    onehot=torch.zeros((depth,))\n    onehot[position]=1\n    return onehot\n```", "```py\nprint(onehot_encoder(1,5))\n```", "```py\ntensor([0., 1., 0., 0., 0.])\n```", "```py\ndef int_to_onehot(number):\n    onehot=onehot_encoder(number,100)\n    return onehot\n```", "```py\nonehot75=int_to_onehot(75)\nprint(onehot75)\n```", "```py\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0.])\n```", "```py\ndef onehot_to_int(onehot):\n    num=torch.argmax(onehot)\n    return num.item()\n```", "```py\nprint(onehot_to_int(onehot75))\n```", "```py\n75\n```", "```py\ndef gen_sequence():\n    indices = torch.randint(0, 20, (10,))\n    values = indices*5\n    return values   \n```", "```py\nsequence=gen_sequence()\nprint(sequence)\n```", "```py\ntensor([60, 95, 50, 55, 25, 40, 70,  5,  0, 55])\n```", "```py\nimport numpy as np\n\ndef gen_batch():\n    sequence=gen_sequence()                            ①\n    batch=[int_to_onehot(i).numpy() for i in sequence] ②\n    batch=np.array(batch)\n    return torch.tensor(batch)\nbatch=gen_batch()\n```", "```py\ndef data_to_num(data):\n    num=torch.argmax(data,dim=-1)                      ①\n    return num\nnumbers=data_to_num(batch)                             ②\n```", "```py\nfrom torch import nn\nD=nn.Sequential(\n    nn.Linear(100,1),\n    nn.Sigmoid()).to(device)\n```", "```py\nG=nn.Sequential(\n    nn.Linear(100,100),\n    nn.ReLU()).to(device)\n```", "```py\nloss_fn=nn.BCELoss()\nlr=0.0005\noptimD=torch.optim.Adam(D.parameters(),lr=lr)\noptimG=torch.optim.Adam(G.parameters(),lr=lr)\n```", "```py\nstopper=EarlyStop(800)                                  ①\n\nmse=nn.MSELoss()\nreal_labels=torch.ones((10,1)).to(device)\nfake_labels=torch.zeros((10,1)).to(device)\ndef distance(generated_data):                           ②\n    nums=data_to_num(generated_data)\n    remainders=nums%5\n    ten_zeros=torch.zeros((10,1)).to(device)\n    mseloss=mse(remainders,ten_zeros)\n    return mseloss\n\nfor i in range(10000):\n    gloss=0\n    dloss=0\n    generated_data=train_D_G(D,G,loss_fn,optimD,optimG) ③\n    dis=distance(generated_data)\n    if stopper.stop(dis)==True:\n        break   \n    if i % 50 == 0:\n        print(data_to_num(generated_data))              ④\n```", "```py\ntensor([14, 34, 19, 89, 44,  5, 58,  6, 41, 87], device='cuda:0')\n… \ntensor([ 0, 80, 65,  0,  0, 10, 80, 75, 75, 75], device='cuda:0')\ntensor([25, 30,  0,  0, 65, 20, 80, 20, 80, 20], device='cuda:0')\ntensor([65, 95, 10, 65, 75, 20, 20, 20, 65, 75], device='cuda:0')\n```", "```py\nimport os\nos.makedirs(\"files\", exist_ok=True)\nscripted = torch.jit.script(G) \nscripted.save('files/num_gen.pt') \n```", "```py\nnew_G=torch.jit.load('files/num_gen.pt',\n                     map_location=device)             ①\nnew_G.eval()\nnoise=torch.randn((10,100)).to(device)                ②\nnew_data=new_G(noise)                                 ③\nprint(data_to_num(new_data))\n```", "```py\ntensor([40, 25, 65, 25, 20, 25, 95, 10, 10, 65], device='cuda:0')\n```"]