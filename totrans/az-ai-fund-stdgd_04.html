<html><head></head><body><section data-pdf-bookmark="Chapter 4. Fundamental Principles of Machine Learning" data-type="chapter" epub:type="chapter"><div class="chapter" id="i04_chapter4_fundamental_principles_of_machine_learning_1742068262070986">
<h1><span class="label">Chapter 4. </span>Fundamental Principles <span class="keep-together">of Machine Learning</span></h1>

<p>In the AI-900 exam, about 20%–25% of the material covers the core principles of ML on Microsoft Azure. This includes foundational techniques like regression analysis, classification, and clustering. Each of these techniques offers a unique approach to problem solving. They allow you to select the right method based on the data type and the predictions you need to make. In this chapter, we’re going to dig into these must-know concepts and services. Understanding these ideas will help you tackle questions on the AI-900 exam, so you’ll know not only what these services are but also how they work and why they matter.</p>

<section data-pdf-bookmark="What Is Machine Learning?" data-type="sect1"><div class="sect1" id="i04_chapter4_what_is_machine_learning_1742068262071369">
<h1>What Is Machine Learning?</h1>

<p>ML, <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="overview of" data-type="indexterm" id="id492"/>which is a branch of AI, allows systems to perform tasks like data analysis without needing explicit instructions. Instead, it processes large amounts of historical data, identifies patterns, and makes predictions based on those patterns. For instance, you can use ML to classify images, numbers, or documents and make predictions from them.</p>

<p>Let’s say you work for a financial services organization looking to differentiate between fraudulent and genuine transactions. With ML, the system would learn to identify patterns from known examples and then apply that knowledge to predict whether a new transaction is genuine.</p>

<p>ML is essential for modern businesses because it helps automate data collection, classification, and analysis. This speeds up decision making and drives growth. It improves processes like customer experience and resource management, enabling businesses to solve problems faster.</p>

<p>Although the terms <em>artificial intelligence</em> (<em>AI</em>) and <em>machine learning</em> (<em>ML</em>) are often used interchangeably, they aren’t the same. AI is a broader concept that includes anything from voice assistants to self-driving cars. ML, however, focuses on specific tasks, such as predicting equipment maintenance schedules or classifying documents.</p>

<p>Take manufacturing, for example. ML can help with the following:</p>

<dl>
	<dt>Predictive maintenance</dt>
	<dd>
	<p>Identify<a contenteditable="false" data-primary="predictive maintenance" data-type="indexterm" id="id493"/> potential equipment failures before they occur, reducing downtime and repair costs</p>
	</dd>
	<dt>Quality control</dt>
	<dd>
	<p>Monitor<a contenteditable="false" data-primary="quality control" data-secondary="in manufacturing" data-secondary-sortas="manufacturing" data-type="indexterm" id="id494"/> production lines to detect defects and ensure consistent product quality</p>
	</dd>
	<dt>Product design optimization</dt>
	<dd>
	<p>Refine<a contenteditable="false" data-primary="product design optimization" data-type="indexterm" id="id495"/> designs, such as improving the abrasiveness of sandpaper by analyzing small changes in size and shape</p>
	</dd>
	<dt>Supply chain management</dt>
	<dd>
	<p>Predict<a contenteditable="false" data-primary="supply chain management" data-type="indexterm" id="id496"/> demand, identify bottlenecks, and streamline logistics to improve <span class="keep-together">efficiency</span></p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="The ML Model Workflow" data-type="sect1"><div class="sect1" id="i04_chapter4_the_ml_model_workflow_1742068262071447">
<h1>The ML Model Workflow</h1>

<p>At <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="workflow" data-type="indexterm" id="icd402"/>its core, an <em>ML model</em> is simply a software application that takes one or more input values and calculates an output value. <a contenteditable="false" data-primary="training ML models" data-secondary="ML model workflow" data-type="indexterm" id="icd406"/>The process of figuring out how to make that calculation is called <em>training</em>. Once the model is trained, you can use it to make predictions. This<a contenteditable="false" data-primary="inferencing" data-type="indexterm" id="id497"/> is a process known as <em>inferencing</em>.</p>

<p>Let’s take a closer look at the main steps of an ML system, which you can see in <a data-type="xref" href="#i04_chapter4_figure_1_1742068262037129">Figure 4-1</a>. This is a very basic example. Keep in mind that the algorithms can be quite complex. But for our purposes, we just want to get a sense of the workflow.</p>

<figure><div class="figure" id="i04_chapter4_figure_1_1742068262037129"><img src="assets/aaif_0401.png"/>
<h6><span class="label">Figure 4-1. </span>The basic workflow of an ML model</h6>
</div></figure>

<p>Let’s take a look at each of these steps in more detail.</p>

<section data-pdf-bookmark="Step 1: Train the Model" data-type="sect2"><div class="sect2" id="i04_chapter4_step_1_train_the_model_1742068262071526">
<h2>Step 1: Train the Model</h2>

<p>The process of training an ML model begins with past data, which is referred to as the <em>training data</em> or <em>dataset</em>. Each data point in the training dataset is made up of two essential components:</p>

<dl>
	<dt>Features</dt>
	<dd>
	<p>The <a contenteditable="false" data-primary="features, in training datasets" data-secondary="defined" data-type="indexterm" id="id498"/>characteristics or attributes you observe in each data point</p>
	</dd>
	<dt>Labels</dt>
	<dd>
	<p>The <a contenteditable="false" data-primary="labels, in training datasets" data-type="indexterm" id="id499"/>outcome or result you want the model to predict</p>
	</dd>
</dl>

<p>A dataset in table form is organized into rows and columns, as you would see in a spreadsheet or database. Each row represents an individual record or entry, and each column contains specific attributes or features of the data. This structure allows for easy comparison and analysis of data across multiple entries by syncing similar information in a consistent format. <a data-type="xref" href="#i04_chapter4_table_1_1742068262048473">Table 4-1</a> shows an example of a table for a dataset for customer feedback and return requests.</p>

<table class="border" id="i04_chapter4_table_1_1742068262048473">
	<caption><span class="label">Table 4-1. </span>Sample dataset showing customer feedback and return requests</caption>
	<thead>
		<tr>
			<th>Customer_ID</th>
			<th>Purchase_Date</th>
			<th>Product_Category</th>
			<th>Rating</th>
			<th>Feedback_Comment</th>
			<th>Return_Requested</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>001</p>
			</td>
			<td>
			<p>2024-10-01</p>
			</td>
			<td>
			<p>Electronics</p>
			</td>
			<td>
			<p>5</p>
			</td>
			<td>
			<p>“Great product, fast delivery!”</p>
			</td>
			<td>
			<p>No</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>002</p>
			</td>
			<td>
			<p>2024-10-05</p>
			</td>
			<td>
			<p>Apparel</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>“Size did not match description.”</p>
			</td>
			<td>
			<p>Yes</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>003</p>
			</td>
			<td>
			<p>2024-10-07</p>
			</td>
			<td>
			<p>Home Goods</p>
			</td>
			<td>
			<p>4</p>
			</td>
			<td>
			<p>“Quality is good, but color is off.”</p>
			</td>
			<td>
			<p>No</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>004</p>
			</td>
			<td>
			<p>2024-10-10</p>
			</td>
			<td>
			<p>Electronics</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>“Item arrived damaged.”</p>
			</td>
			<td>
			<p>Yes</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>005</p>
			</td>
			<td>
			<p>2024-10-12</p>
			</td>
			<td>
			<p>Beauty</p>
			</td>
			<td>
			<p>3</p>
			</td>
			<td>
			<p>“Average product, packaging was poor.”</p>
			</td>
			<td>
			<p>No</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Data is often messy and incomplete. These imperfections can distort results, lead to inaccuracies, and complicate decision making.</p>

<p>For example, a common issue is missing data. While removing incomplete information is a straightforward approach, it can lead to unintended distortions, particularly when the missing data is not randomly distributed. Instead, consider alternative methods to handle <a contenteditable="false" data-primary="missing data" data-type="indexterm" id="id500"/>missing data effectively:</p>

<dl>
	<dt>Mean/median imputation</dt>
	<dd>
	<p>Replace<a contenteditable="false" data-primary="mean/median imputation" data-type="indexterm" id="id501"/> missing values with the average (mean) or the middle value (median) of the dataset to maintain consistency</p>
	</dd>
	<dt>Predictive imputation</dt>
	<dd>
	<p>Leverage<a contenteditable="false" data-primary="predictive imputation" data-type="indexterm" id="id502"/> statistical models to predict and fill in missing values based on patterns in the available data, providing a more accurate estimate</p>
	</dd>
</dl>

<p>Another common challenge in data analysis is dealing with <a contenteditable="false" data-primary="outliers" data-type="indexterm" id="id503"/>outliers. <em>Outliers</em> are extreme data points that differ significantly from other observations. They can distort results or obscure important patterns. Properly managing outliers is essential for improving the accuracy and reliability of your findings. Consider the following approaches:</p>

<dl>
	<dt>Removal</dt>
	<dd>
	<p>Exclude <a contenteditable="false" data-primary="removal of outliers" data-type="indexterm" id="id504"/>outliers from the dataset if they are identified as errors or are irrelevant to the analysis</p>
	</dd>
	<dt>Transformation</dt>
	<dd>
	<p>Use mathematical techniques, such as logarithmic or square root transformations, to minimize the influence of outliers without removing them entirely</p>
	</dd>
	<dt>Investigation</dt>
	<dd>
	<p>Examine<a contenteditable="false" data-primary="investigation of outliers" data-type="indexterm" id="id505"/> outliers closely to determine whether they provide valuable insights or are the result of data-entry mistakes</p>
	</dd>
</dl>

<p>Data may also have<a contenteditable="false" data-primary="duplicate data" data-type="indexterm" id="id506"/> duplications. For this, you can consider these options:</p>

<dl>
	<dt>Deduplication algorithms</dt>
	<dd>
	<p>Use<a contenteditable="false" data-primary="deduplication algorithms" data-type="indexterm" id="id507"/> tools or scripts to identify and merge duplicate data</p>
	</dd>
	<dt>Unique identifiers</dt>
	<dd>
	<p>Ensure<a contenteditable="false" data-primary="unique identifiers" data-type="indexterm" id="id508"/> that each record has a unique key to prevent duplication at the entry point</p>
	</dd>
	<dt>Quality control</dt>
	<dd>
	<p>Regularly<a contenteditable="false" data-primary="quality control" data-secondary="outlier management" data-type="indexterm" id="id509"/> audit data to detect and resolve duplicates</p>
	</dd>
</dl>

<p>Then there is the problem with inconsistent data formats. You can use the following techniques to address this:</p>

<dl>
	<dt>Data normalization</dt>
	<dd>
	<p>Convert<a contenteditable="false" data-primary="data normalization" data-type="indexterm" id="id510"/> data to a standard format (e.g., ISO 8601 for dates)</p>
	</dd>
	<dt>Validation rules</dt>
	<dd>
	<p>Implement<a contenteditable="false" data-primary="validation rules" data-type="indexterm" id="id511"/> automated checks during data entry to enforce consistent formatting</p>
	</dd>
	<dt>Preprocessing pipelines</dt>
	<dd>
	<p>Develop<a contenteditable="false" data-primary="extract, transform, and load (ETL) processes" data-type="indexterm" id="id512"/><a contenteditable="false" data-primary="ETL (extract, transform, and load) processes" data-type="indexterm" id="id513"/> extract, transform, and load (ETL) processes<a contenteditable="false" data-primary="preprocessing pipelines" data-type="indexterm" id="id514"/> to clean and standardize data before analysis</p>
	</dd>
</dl>

<p>Once you have improved the quality of the dataset—a process known<a contenteditable="false" data-primary="data preparation" data-type="indexterm" id="id515"/> as <em>data preparation</em>—you can then look at training. One way to think of this is in terms of basic algebra. The features are represented by <em>x</em>, and there are often several of them, forming a vector (an array of values). The label is represented by <em>y</em>, which is the prediction or result the model tries to output.</p>

<p>For example, let’s say you’re building a model that predicts the price of a house based on its characteristics. The features (<em>x</em>) might include the number of bedrooms, square footage, and location. The label (<em>y</em>) would be the house’s sale price. The model is trained to learn how these features relate to the price and can then predict the price of a new house based on similar characteristics.</p>

<p>Another example could be a model designed to predict whether an email is spam. The features (<em>x</em>) could be things like the presence of certain keywords, the sender’s address, and the time the email was sent. The label (<em>y</em>) would be a binary outcome: 1 if the email is spam, 0 if it’s not. Over time, the model learns to recognize patterns in the features that indicate whether an email is spam or <a contenteditable="false" data-primary="training ML models" data-secondary="ML model workflow" data-startref="icd406" data-type="indexterm" id="id516"/>legitimate.</p>
</div></section>

<section data-pdf-bookmark="Step 2: Apply the Algorithm" data-type="sect2"><div class="sect2" id="i04_chapter4_step_2_apply_the_algorithm_1742068262071601">
<h2>Step 2: Apply the Algorithm</h2>

<p>When <a contenteditable="false" data-primary="algorithms" data-secondary="applying" data-type="indexterm" id="id517"/>you’re working with an ML model, you need an algorithm to figure out the relationship between the features and the label. The goal is to find a way to take the features (<em>x</em>) and use them to predict the label (<em>y</em>). The basic idea is to fit a function to your data. In other words, the algorithm looks for a pattern that can calculate <em>y</em> based on <em>x</em>. For our example, this is what it looks like:</p>

<div data-type="equation">
<p><em>y</em> = f(<em>x</em>)</p>
</div>

<p>Once the algorithm has done its job, it gives you a model. This model is essentially a function that performs the calculation. In this chapter, we’ll see various examples of this, like linear regression.</p>
</div></section>

<section data-pdf-bookmark="Step 3: Use Inferencing" data-type="sect2"><div class="sect2" id="i04_chapter4_step_3_use_inferencing_1742068262071659">
<h2>Step 3: Use Inferencing</h2>

<p>Once<a contenteditable="false" data-primary="inferencing" data-type="indexterm" id="id518"/> the training phase is done, you can use the trained model to make predictions, a process called <em>inferencing</em>. The model now works like a software program that contains the function learned during training. You provide it with a set of feature values, and it gives you a prediction for the label.</p>

<p>Since this output is a prediction generated by the function rather than an actual observed value, it’s typically written as <em>ŷ</em>, pronounced “y-hat.” This is a useful way to show that the result is an estimate based on what the model has learned, not a guaranteed <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="workflow" data-startref="icd402" data-type="indexterm" id="id519"/>outcome.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Types of ML" data-type="sect1"><div class="sect1" id="i04_chapter4_types_of_ml_1742068262071715">
<h1>Types of ML</h1>

<p>There<a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-type="indexterm" id="icd407"/> are several types of ML, each designed for different kinds of problems. At the highest level, ML is divided into two main types: <em>supervised learning</em>, <a contenteditable="false" data-primary="supervised learning" data-type="indexterm" id="id520"/>which has labeled data, and <em>unsupervised learning</em>, <a contenteditable="false" data-primary="unsupervised learning" data-type="indexterm" id="id521"/>where the data does not have labels. These two categories help determine how the model learns from the data and what kind of tasks it can perform. Under the umbrellas of supervised and unsupervised learning, there are other types of ML. <a data-type="xref" href="#i04_chapter4_figure_2_1742068262037163">Figure 4-2</a> shows the hierarchy.</p>

<figure><div class="figure" id="i04_chapter4_figure_2_1742068262037163"><img src="assets/aaif_0402.png"/>
<h6><span class="label">Figure 4-2. </span>The categories of ML</h6>
</div></figure>

<p>In the next few sections, we’ll look at the approaches for supervised learning and then follow this up by looking at unsupervised learning.</p>
</div></section>

<section data-pdf-bookmark="Regression Analysis" data-type="sect1"><div class="sect1" id="i04_chapter4_regression_analysis_1742068262071791">
<h1>Regression Analysis</h1>

<p><em>Regression analysis</em> is<a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-tertiary="regression analysis" data-type="indexterm" id="icd408"/><a contenteditable="false" data-primary="regression analysis" data-type="indexterm" id="icd409"/><a contenteditable="false" data-primary="supervised learning" data-secondary="regression analysis" data-type="indexterm" id="icd410"/> a statistical method used to predict a numerical outcome based on one or more known factors, or variables. It helps you understand the relationship between these variables and the result you’re trying to predict.</p>

<p>For example, let’s say you want to predict how much of a new product will sell based on factors like advertising spend, the season, and market trends. Regression analysis allows you to analyze these factors and estimate how they affect sales so that you can forecast future performance based on current data.</p>

<p>Here’s how you would approach it:</p>

<dl>
	<dt>Split the data</dt>
	<dd>
	<p>Begin<a contenteditable="false" data-primary="splitting data" data-type="indexterm" id="id522"/> by dividing your data into three subsets: a training set, a validation set, and a testing set. The <a contenteditable="false" data-primary="training dataset" data-type="indexterm" id="id523"/>training set is used to develop the model, the validation set<a contenteditable="false" data-primary="validation dataset" data-type="indexterm" id="icd1017"/> helps fine-tune the model’s parameters, and the<a contenteditable="false" data-primary="testing dataset" data-type="indexterm" id="icd1014"/> testing set is reserved for assessing the model’s performance on new, unseen data.</p>
	</dd>
	<dt>Train the model</dt>
	<dd>
	<p>Using<a contenteditable="false" data-primary="training ML models" data-secondary="regression analysis" data-type="indexterm" id="id524"/> the training data, apply an algorithm to identify relationships between key variables, such as the advertising budget, seasonality, and market trends, and their impact on sales. The model searches for patterns, such as whether increased ad spending leads to higher sales or if certain times of the year naturally see better sales performance.</p>
	</dd>
	<dt>Fine-tune the model</dt>
	<dd>
	<p>With the <a contenteditable="false" data-primary="validation dataset" data-startref="icd1017" data-type="indexterm" id="id525"/><a contenteditable="false" data-primary="fine-tuning models" data-type="indexterm" id="id526"/>validation set, optimize the model’s parameters or hyperparameters to enhance its predictive accuracy. This step helps ensure that the model generalizes well and avoids overfitting to the training data.</p>
	</dd>
	<dt>Test the model</dt>
	<dd>
	<p>Evaluate <a contenteditable="false" data-primary="testing dataset" data-startref="icd1014" data-type="indexterm" id="id527"/>the performance of the fine-tuned model using the testing set. In this stage, the model generates sales predictions based on input variables (e.g., advertising budget, season, market trends) and compares these predictions to actual sales data to measure accuracy.</p>
	</dd>
	<dt>Evaluate the model</dt>
	<dd>
	<p>Assess the accuracy of the model’s predictions using relevant metrics, such as mean absolute error (MAE),<a contenteditable="false" data-primary="MAE (mean absolute error)" data-type="indexterm" id="id528"/> root mean square error (RMSE)<a contenteditable="false" data-primary="RMSE (root mean squared error)" data-type="indexterm" id="id529"/>, or <em>R</em>², <a contenteditable="false" data-primary="R² (coefficient of determination)" data-type="indexterm" id="id530"/><a contenteditable="false" data-primary="coefficient of determination (R²)" data-type="indexterm" id="id531"/>which we’ll learn about later in this chapter. If the model’s performance falls short of expectations, refine the process by revisiting earlier steps, such as trying a different algorithm, adjusting parameters, or including new features.</p>
	</dd>
	<dt>Iterate and improve</dt>
	<dd>
	<p>Using the insights gained from the evaluation, iterate on the model to improve its performance. This iterative process might include collecting additional data or exploring alternative modeling techniques to boost accuracy.</p>
	</dd>
</dl>

<section data-pdf-bookmark="Example: Ticket Sales" data-type="sect2"><div class="sect2" id="i04_chapter4_example_ticket_sales_1742068262071946">
<h2>Example: Ticket Sales</h2>

<p>Let’s<a contenteditable="false" data-primary="regression analysis" data-secondary="ticket sales example" data-type="indexterm" id="icd411"/> take a look at a more detailed example to better understand how regression analysis works. For this example, we won’t cover every step outlined in the previous section. When it comes to the exam, such in-depth detail about regression analysis isn’t necessary. Instead, we’ll focus on the main phases.</p>

<p>In our example, we’ll predict a numeric label (<em>y</em>) based on a single feature (<em>x</em>). While most real-world applications involve multiple features, starting with just one keeps things simple and allows us to focus on the core idea.</p>

<p>Let’s consider the example of predicting concert ticket prices based on the popularity of the performing artist. Here, the popularity score is derived from survey data collected from fans, representing a value on a scale from 1 to 100. This score serves as our feature while the ticket price for the artist’s concert is the label we aim to predict. To build this prediction model, we’ll use historical data that pairs popularity scores (<em>x</em>) with their corresponding ticket prices (<em>y</em>) from past concerts, as shown in <a data-type="xref" href="#i04_chapter4_table_2_1742068262048550">Table 4-2</a>.</p>

<table class="border" id="i04_chapter4_table_2_1742068262048550">
	<caption><span class="label">Table 4-2. </span>Comparing an artist’s popularity with the concert ticket price</caption>
	<thead>
		<tr>
			<th>Artist popularity (<em>x</em>)</th>
			<th>Ticket price (<em>y</em>)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>35</p>
			</td>
			<td>
			<p>$45</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>40</p>
			</td>
			<td>
			<p>$60</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>45</p>
			</td>
			<td>
			<p>$55</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>50</p>
			</td>
			<td>
			<p>$75</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>55</p>
			</td>
			<td>
			<p>$65</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>60</p>
			</td>
			<td>
			<p>$85</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>65</p>
			</td>
			<td>
			<p>$100</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>70</p>
			</td>
			<td>
			<p>$105</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>75</p>
			</td>
			<td>
			<p>$115</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>80</p>
			</td>
			<td>
			<p>$135</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>85</p>
			</td>
			<td>
			<p>$140</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>90</p>
			</td>
			<td>
			<p>$170</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Next, we’ll <a contenteditable="false" data-primary="splitting data" data-type="indexterm" id="id532"/>split the data and use a portion of it to train the model. In this case, the data was split randomly to ensure a balanced representation of the dataset. <a data-type="xref" href="#i04_chapter4_table_3_1742068262048585">Table 4-3</a> shows the subset of data selected for training.</p>

<table class="border" id="i04_chapter4_table_3_1742068262048585">
	<caption><span class="label">Table 4-3. </span>The subset of data for the training of the model</caption>
	<thead>
		<tr>
			<th>Artist popularity (<em>x</em>)</th>
			<th>Ticket price (<em>y</em>)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>40</p>
			</td>
			<td>
			<p>$60</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>50</p>
			</td>
			<td>
			<p>$75</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>60</p>
			</td>
			<td>
			<p>$85</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>70</p>
			</td>
			<td>
			<p>$105</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>75</p>
			</td>
			<td>
			<p>$115</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>80</p>
			</td>
			<td>
			<p>$135</p>
			</td>
		</tr>
	</tbody>
</table>

<p>While random splitting is a common approach, there are other methods that can be used based on the scenario:</p>

<dl>
	<dt>Stratified splitting</dt>
	<dd>
	<p>Ensures <a contenteditable="false" data-primary="stratified splitting" data-type="indexterm" id="id533"/>that specific proportions of key features (like popularity ranges) are maintained in both training and testing sets</p>
	</dd>
	<dt class="less_space pagebreak-before">Time-based splitting</dt>
	<dd>
	<p>Used<a contenteditable="false" data-primary="time-based splitting" data-type="indexterm" id="id534"/> when the data has a chronological order, such as time-series data, where older data is used for training and newer data for testing</p>
	</dd>
	<dt>K-fold cross-validation</dt>
	<dd>
	<p>Splits <a contenteditable="false" data-primary="k-fold cross-validation" data-type="indexterm" id="id535"/>the data into multiple folds, where each subset takes a turn as the test set while the others are used for training</p>
	</dd>
</dl>

<p>To get a better understanding of how the popularity scores (<em>x</em>) and ticket prices (<em>y</em>) relate to each other, we can plot these values as points on a graph. You can see this in <a data-type="xref" href="#i04_chapter4_figure_3_1742068262037187">Figure 4-3</a>. By plotting these points, you can start to see the relationship between the two—typically, as the popularity increases, so does the ticket price.</p>

<figure><div class="figure" id="i04_chapter4_figure_3_1742068262037187"><img src="assets/aaif_0403.png"/>
<h6><span class="label">Figure 4-3. </span>This scatter plot illustrates the relationship between artist popularity and ticket prices</h6>
</div></figure>

<p>With the training dataset, we’re ready to apply an algorithm that can model the relationship between artist popularity and ticket price. We’ll use the linear regression formula, which essentially finds the best-fit line through the points that minimizes the distance between the line and the actual data points. This line represents a function where the slope tells you how much the ticket price will increase with each increase in the artist’s popularity. This is shown in <a data-type="xref" href="#i04_chapter4_figure_4_1742068262037209">Figure 4-4</a>.</p>

<figure><div class="figure" id="i04_chapter4_figure_4_1742068262037209"><img src="assets/aaif_0404.png"/>
<h6><span class="label">Figure 4-4. </span>The scatter plot with the linear regression line added for our training dataset</h6>
</div></figure>

<p>Let’s say an artist has a popularity score of 77. By applying the equation derived from the linear regression line, we can estimate the corresponding ticket price. In this case, the price would be something like $120 based on the trend we’ve established.</p>

<p>The next step is to evaluate the accuracy of this regression model. Using the testing dataset, we can predict the ticket prices for each artist’s popularity score, shown in <a data-type="xref" href="#i04_chapter4_table_4_1742068262048610">Table 4-4</a>.</p>

<table class="border" id="i04_chapter4_table_4_1742068262048610">
	<caption><span class="label">Table 4-4. </span>The predicted ticket price</caption>
	<thead>
		<tr>
			<th>Artist popularity (<em>x</em>)</th>
			<th>Actual ticket price (<em>y</em>)</th>
			<th>Predicted ticket price (ŷ)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>35</p>
			</td>
			<td>
			<p>$45</p>
			</td>
			<td>
			<p>$50</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>45</p>
			</td>
			<td>
			<p>$55</p>
			</td>
			<td>
			<p>$60</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>55</p>
			</td>
			<td>
			<p>$65</p>
			</td>
			<td>
			<p>$70</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>65</p>
			</td>
			<td>
			<p>$100</p>
			</td>
			<td>
			<p>$85</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>80</p>
			</td>
			<td>
			<p>$135</p>
			</td>
			<td>
			<p>$120</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>90</p>
			</td>
			<td>
			<p>$170</p>
			</td>
			<td>
			<p>$140</p>
			</td>
		</tr>
	</tbody>
</table>

<p>We can then plot these values on a chart, which you can see in <a data-type="xref" href="#i04_chapter4_figure_5_1742068262037231">Figure 4-5</a>.</p>

<figure><div class="figure" id="i04_chapter4_figure_5_1742068262037231"><img src="assets/aaif_0405.png"/>
<h6><span class="label">Figure 4-5. </span>The original dataset shown with actual and predicted ticket prices for the test data</h6>
</div></figure>

<p>The original data points capture the historical trend while the test data highlights how closely the model’s predictions align with actual ticket prices. The chart makes it easy to visualize where the model performed well and where deviations occurred, particularly by comparing the actual and predicted ticket prices across different levels of artist popularity.</p>

<p>In this case, the model performed fairly well. For midrange popularity scores, the predicted values are close to the actual values, indicating minimal error. However, at the lower (35) and higher (90) ends of the popularity scale, the model’s predictions underestimate the actual ticket prices. While the performance is promising, the noticeable deviations at these extremes suggest the model could benefit from additional fine-tuning to improve accuracy, especially for edge <a contenteditable="false" data-primary="regression analysis" data-secondary="ticket sales example" data-startref="icd411" data-type="indexterm" id="id536"/>cases.</p>
</div></section>

<section data-pdf-bookmark="Evaluation Metrics for Regression Models" data-type="sect2"><div class="sect2" id="i04_chapter4_evaluation_metrics_for_regression_models_1742068262072035">
<h2>Evaluation Metrics for Regression Models</h2>

<p>When<a contenteditable="false" data-primary="regression analysis" data-secondary="evaluation metrics for" data-type="indexterm" id="icd412"/> it comes to measuring how well your regression model performs, there are a few handy metrics based on the differences between your predicted values and the actual ones.</p>

<section data-pdf-bookmark="Mean absolute error" data-type="sect3"><div class="sect3" id="i04_chapter4_mean_absolute_error_1742068262072098">
<h3>Mean absolute error</h3>

<p>Imagine<a contenteditable="false" data-primary="MAE (mean absolute error)" data-type="indexterm" id="id537"/> you’re predicting how many pizzas a group of friends will eat at a party. <em>Mean absolute error (MAE)</em> helps you figure out, on average, how far off your predictions were—whether you guessed too high or too low. For instance, if you predicted four pizzas but your friends ate seven, you missed by three. MAE ignores whether the difference is positive or negative, so it treats both -3 and +3 as a difference of 3. If your absolute errors for a set of predictions were 1, 2, 3, and 4 pizzas, the MAE would simply be the average of those numbers: 2.5 pizzas.</p>
</div></section>

<section data-pdf-bookmark="Mean squared error" data-type="sect3"><div class="sect3" id="i04_chapter4_mean_squared_error_1742068262072153">
<h3>Mean squared error</h3>

<p>Sometimes, <a contenteditable="false" data-primary="mean squared error (MSE)" data-type="indexterm" id="id538"/><a contenteditable="false" data-primary="MSE (mean squared error)" data-type="indexterm" id="id539"/>you want to give more weight to bigger errors. After all, consistently being off by one pizza isn’t as bad as being wildly off by five. That’s where <em>mean squared error (MSE) </em>comes in. Instead of just taking the differences as they are, you square each error (making bigger mistakes stand out), and then average those squared values. In our pizza example, if your errors were 1, 2, 3, and 4, squaring them gives you 1, 4, 9, and 16. The MSE would then be the average of these squared <span class="keep-together">errors: 7.5</span>.</p>
</div></section>

<section data-pdf-bookmark="Root mean squared error" data-type="sect3"><div class="sect3" id="i04_chapter4_root_mean_squared_error_1742068262072209">
<h3>Root mean squared error</h3>

<p>While <a contenteditable="false" data-primary="RMSE (root mean squared error)" data-type="indexterm" id="id540"/>MSE is useful, those squared numbers don’t match up with the original quantities you were measuring, so they can feel a little abstract. If you want the error back in terms of pizzas (or whatever you’re predicting), you can take the square root of the MSE. That’s called the <em>root mean squared error (RMSE)</em>. In this case, the square root of 7.5 is about 2.74, meaning your average error is around 2.74 pizzas.</p>
</div></section>

<section data-pdf-bookmark="Coefficient of determination" data-type="sect3"><div class="sect3" id="i04_chapter4_coefficient_of_determination_1742068262072270">
<h3>Coefficient of determination</h3>

<p>What <a contenteditable="false" data-primary="R² (coefficient of determination)" data-type="indexterm" id="id541"/><a contenteditable="false" data-primary="coefficient of determination (R²)" data-type="indexterm" id="id542"/>if you want to understand how well your model explains the variation in your data? Enter <em>R</em>², also called the <em>coefficient of determination</em>. This metric tells you how much of the difference between actual and predicted values your model accounts for.</p>

<p>For example, if you’re trying to predict how many cupcakes a bakery sells each day, <em>R</em>² tells you how much your model can explain. If your <em>R</em>² value is 0.85, that means your model explains 85% of the variation in cupcake sales—the rest might be due to some unexpected cupcake-related event, like a new bakery opening next door. <em>R</em>² ranges from 0 to 1, and the closer it is to 1, the better your model fits the data.</p>

<p>All of these metrics are certainly helpful. But in real-world scenarios, ML isn’t a one-shot deal. Data scientists typically train models over and over, tweaking different aspects to improve performance. Here’s what they adjust:</p>

<dl>
	<dt>Feature selection and preparation</dt>
	<dd>
	<p>You <a contenteditable="false" data-primary="features, in training datasets" data-secondary="selection and preparation of" data-type="indexterm" id="id543"/>can choose which factors or features to include in the model and how to tweak them for better results. For instance, maybe you realize that cupcake sales don’t depend just on weather but also on nearby events.</p>
	</dd>
	<dt>Algorithm selection</dt>
	<dd>
	<p>There’s<a contenteditable="false" data-primary="algorithms" data-secondary="selection of" data-type="indexterm" id="id544"/> more than one way to predict the number of cupcakes sold. While one algorithm might focus on simple linear relationships, another might use more complex patterns.</p>
	</dd>
	<dt>Algorithm parameters</dt>
	<dd>
	<p>These <a contenteditable="false" data-primary="algorithms" data-secondary="parameters for" data-type="indexterm" id="id545"/>are<a contenteditable="false" data-primary="fine-tuning models" data-type="indexterm" id="id546"/> the settings you adjust to fine-tune your algorithm. Think of them like the dials on an oven. If your cupcakes are coming out undercooked, you tweak the temperature <a contenteditable="false" data-primary="hyperparameters" data-type="indexterm" id="id547"/>(or in ML, the <em>hyperparameters</em>) to get better results.</p>
	</dd>
</dl>

<p>After several rounds of this iterative process, you’ll settle on the version of the model that performs best for your specific problem.</p>

<p>Let’s sum up what we have learned about <a contenteditable="false" data-primary="regression analysis" data-secondary="key concepts in" data-type="indexterm" id="id548"/>regression <a contenteditable="false" data-primary="regression analysis" data-secondary="evaluation metrics for" data-startref="icd412" data-type="indexterm" id="id549"/>analysis <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-startref="icd408" data-tertiary="regression analysis" data-type="indexterm" id="id550"/><a contenteditable="false" data-primary="regression analysis" data-startref="icd409" data-type="indexterm" id="id551"/><a contenteditable="false" data-primary="supervised learning" data-secondary="regression analysis" data-startref="icd410" data-type="indexterm" id="id552"/>in <a data-type="xref" href="#i04_chapter4_table_5_1742068262048633">Table 4-5</a>.</p>

<table class="border" id="i04_chapter4_table_5_1742068262048633">
	<caption><span class="label">Table 4-5. </span>Key concepts in regression analysis</caption>
	<thead>
		<tr>
			<th>Factors</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Purpose</p>
			</td>
			<td>
			<p>To predict a numeric outcome label based on one or more predictor features by identifying patterns in historical data</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Process</p>
			</td>
			<td>
			<ol>
				<li>Split data into training and testing sets.</li>
				<li>Train the model to identify relationships.</li>
				<li>Test the model to make predictions.</li>
				<li>Evaluate model accuracy and refine if needed.</li>
			</ol>
			</td>
		</tr>
		<tr>
			<td>
			<p>Training the model</p>
			</td>
			<td>
			<p>Uses an algorithm, such as linear regression, to analyze patterns in the training data, establishing a predictive relationship between the independent and dependent variables</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Evaluation metrics</p>
			</td>
			<td>
			<ul>
				<li>MAE: Measures average prediction error without considering direction (over/underestimate)</li>
				<li>MSE: Emphasizes larger errors by squaring them</li>
				<li>RMSE: Provide the error in original measurement units</li>
				<li>R² (coefficient of determination): Indicates the model’s explanatory power, with values closer to 1 suggesting a better fit</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>
</div></section>

<section data-pdf-bookmark="Classification" data-type="sect1"><div class="sect1" id="i04_chapter4_classification_1742068262072376">
<h1>Classification</h1>

<p><em>Classification</em> in<a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-tertiary="classification" data-type="indexterm" id="icd413"/><a contenteditable="false" data-primary="classification" data-type="indexterm" id="icd414"/><a contenteditable="false" data-primary="supervised learning" data-secondary="classification" data-type="indexterm" id="icd415"/> ML is a supervised learning task where the goal is to predict the category or class to which a given data point belongs. It involves training a model on labeled data where the label is categorical, such as “yes” or “no.” The model learns the relationship between input features and output classes, enabling it to assign new, unseen data points to one of the predefined categories.</p>

<p>There are various types of classification techniques. In the next few sections, we’ll take a look at binary and multiclass classification.</p>

<section data-pdf-bookmark="Binary Classification" data-type="sect2"><div class="sect2" id="i04_chapter4_binary_classification_1742068262072483">
<h2>Binary Classification</h2>

<p><em>Binary classification</em> is <a contenteditable="false" data-primary="classification" data-secondary="binary" data-type="indexterm" id="icd416"/><a contenteditable="false" data-primary="binary classification" data-type="indexterm" id="icd417"/>one of the most common types of classification tasks. At its core, it’s about predicting one of two possible outcomes. When you feed your model data, the goal is to get it to categorize new information into one of two buckets, often labeled as 0 and 1. For example, if you’re building a model to predict whether an email is spam or not, binary classification is your go-to technique. Each email gets analyzed, and the model spits out a prediction: “spam” or “not spam.”</p>

<p>What makes binary classification different from something like regression is that you aren’t predicting a continuous value, like a temperature or a sales figure. Instead, you’re focused on making a choice between two discrete options. The model looks at the features of the data you provide, such as email content, and it uses that to assign a probability. Based on this probability, it then makes a final classification.</p>

<p>Let’s walk through a simple example to show how binary classification works. Imagine<a contenteditable="false" data-primary="binary classification" data-secondary="loan default prediction example" data-type="indexterm" id="icd417a"/> we want to predict whether a person will default on a loan using one feature: their credit score (<em>x</em>). Our goal is to classify them into one of two categories: either they default (<em>y</em> = 1) or they don’t (<em>y</em> = 0). The model will learn from the data in <a data-type="xref" href="#i04_chapter4_table_6_1742068262048658">Table 4-6</a> to make predictions.</p>

<table class="border" id="i04_chapter4_table_6_1742068262048658">
	<caption><span class="label">Table 4-6. </span>Defaults based on credit scores</caption>
	<thead>
		<tr>
			<th>Credit score (<em>x</em>)</th>
			<th>Default? (<em>y</em>)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>580</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>720</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>610</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>750</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>590</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>800</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
	</tbody>
</table>

<p>Based on the patterns in the training data, the model will eventually predict whether someone is likely to default or not. As you can see, it’s about using past information to make a binary choice. In our example, people with a credit score of 610 or lower are predicted to default while anyone with a score of 720 or higher is predicted not to default.</p>

<p>To train our model, we’ll use an algorithm that analyzes the training data and fits it to a function that calculates the probability of a person defaulting on a loan. For example, if the model predicts a probability of 0.8 for default, that means there’s an 80% chance the person will default and a 20% chance they won’t.</p>

<p>There are several algorithms that handle binary classification, <a contenteditable="false" data-primary=" logistic regression" data-type="indexterm" id="id553"/>but <em>logistic regression</em> is a common choice. Logistic regression gives us an S-shaped curve, called a <em>sigmoid function</em>, that <a contenteditable="false" data-primary="sigmoid function" data-type="indexterm" id="ibd4001"/>assigns values between 0 and 1 based on the input data. This is shown in <a data-type="xref" href="#i04_chapter4_figure_6_1742068262037256">Figure 4-6</a>. Even though its name includes <em>regression</em>, the logistic regression algorithm is used for classification because it models the probability of different outcomes. This is a common topic for the exam.</p>

<figure><div class="figure" id="i04_chapter4_figure_6_1742068262037256"><img src="assets/aaif_0406.png"/>
<h6><span class="label">Figure 4-6. </span>A sigmoid function showing the probability of loan default based on credit scores, with a threshold of 0.5 determining default (y = 1) or no default (y = 0)</h6>
</div></figure>

<p>In this case, the curve shows the probability that someone will default (<em>y </em>= 1) based on their credit score (<em>x</em>). Mathematically, the model’s function can be represented like this:</p>

<div data-type="equation">
<p>f(<em>x</em>) = P(<em>y </em>= 1 | <em>x</em>)</p>
</div>

<p>Note that for the exam, you will not have to memorize equations. They are used in this book as a way to better help you understand the concepts.</p>

<p>For some people in the training data, we already know they defaulted (<em>y </em>= 1), so the probability for them is 1.0. For others who didn’t default, the probability is 0.0. The sigmoid curve visually shows how the likelihood of default changes as credit scores increase.</p>

<p>In <a data-type="xref" href="#i04_chapter4_figure_6_1742068262037256">Figure 4-6</a>, the dashed line at 0.5 serves as the threshold for our model’s predictions. When the calculated probability is at or above 0.5, the model predicts that the person will default (<em>y </em>= 1). If the probability falls below 0.5, the prediction is that they won’t default (<em>y </em>= 0). For instance, if someone has a credit score of 580, and the model assigns a 0.9 probability of default, that’s well above the threshold, meaning the model would predict this person is likely to default.</p>

<p>Just like with regression models, when you train a binary classification model, it’s essential to hold back a set of data to validate how well the model performs. Let’s say we kept the credit score data in <a data-type="xref" href="#i04_chapter4_table_7_1742068262048681">Table 4-7</a> aside to validate our model predicting loan defaults.</p>

<table class="border" id="i04_chapter4_table_7_1742068262048681">
	<caption><span class="label">Table 4-7. </span>Data used to validate the predictions for loan defaults</caption>
	<thead>
		<tr>
			<th>Credit score (<em>x</em>)</th>
			<th>Default? (<em>y</em>)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>62</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>108</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>113</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>70</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>88</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>91</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
	</tbody>
</table>

<p>We can apply the logistic function we trained earlier to these values, which you can see in <a data-type="xref" href="#i04_chapter4_figure_7_1742068262037277">Figure 4-7</a>.</p>

<figure><div class="figure" id="i04_chapter4_figure_7_1742068262037277"><img src="assets/aaif_0407.png"/>
<h6><span class="label">Figure 4-7. </span>A sigmoid function used to evaluate a binary classification model</h6>
</div></figure>

<p>Based <a contenteditable="false" data-primary="sigmoid function" data-startref="ibd4001" data-type="indexterm" id="id554"/>on whether the calculated probability is above or below the threshold (usually 0.5), the model will predict either a default (1) or no default (0) for each credit score. We can then compare the predicted defaults (ŷ) to the actual defaults (<em>y</em>), as shown in <a data-type="xref" href="#i04_chapter4_table_8_1742068262048704">Table 4-8</a>.</p>

<table class="border less_space pagebreak-before" id="i04_chapter4_table_8_1742068262048704">
	<caption><span class="label">Table 4-8. </span>Comparing the predicted defaults to the actual defaults</caption>
	<thead>
		<tr>
			<th>Credit score (<em>x</em>)</th>
			<th>Actual default (<em>y</em>)</th>
			<th>Predicted default (<em>ŷ</em>)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>62</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>108</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>113</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>70</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>88</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>91</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
	</tbody>
</table>

<p>This comparison helps us see where the model is getting it right and where it might need <a contenteditable="false" data-primary="binary classification" data-secondary="loan default prediction example" data-startref="icd417a" data-type="indexterm" id="id555"/>improvement.</p>
</div></section>

<section data-pdf-bookmark="Evaluation Metrics for Binary Classification" data-type="sect2"><div class="sect2" id="i04_chapter4_evaluation_metrics_for_binary_classification_1742068262072572">
<h2>Evaluation Metrics for Binary Classification</h2>

<p>The<a contenteditable="false" data-primary="binary classification" data-secondary="evaluation metrics for" data-type="indexterm" id="icd418"/> first step in calculating evaluation metrics for a binary classification model is usually to create a <em>confusion matrix</em> of<a contenteditable="false" data-primary="confusion matrices" data-type="indexterm" id="icd1005"/> the number of correct and incorrect predictions for each possible class label, as you can see in <a data-type="xref" href="#i04_chapter4_table_9_1742068262048724">Table 4-9</a>.</p>

<table class="border" id="i04_chapter4_table_9_1742068262048724">
	<caption><span class="label">Table 4-9. </span>Confusion matrix</caption>
	<thead>
		<tr>
			<th> </th>
			<th>Positive</th>
			<th>Negative</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><strong>Positive</strong></td>
			<td>30</td>
			<td>45</td>
		</tr>
		<tr>
			<td><strong>Negative</strong></td>
			<td>20</td>
			<td>19</td>
		</tr>
	</tbody>
</table>

<p>The rows represent the actual values, labeled as “positive” or “negative,” while the columns show the predicted values. For example, the model predicted the positive class correctly 30 times, which is displayed as the true positive count. However, it also made 45 incorrect predictions where it classified negatives as positives. Similarly, there are 20 instances of false negatives and 19 true negatives. This distribution helps evaluate the accuracy and types of errors in the model’s predictions. One way to express the different possibilities for the confusion matrix is shown in <a data-type="xref" href="#i04_chapter4_table_10_1742068262048744">Table 4-10</a>.</p>

<table class="border" id="i04_chapter4_table_10_1742068262048744">
	<caption><span class="label">Table 4-10. </span>Confusion matrix with descriptions</caption>
	<thead>
		<tr>
			<th> </th>
			<th>Positive</th>
			<th>Negative</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><strong>Positive</strong></td>
			<td>True positive</td>
			<td>False positive</td>
		</tr>
		<tr>
			<td><strong>Negative</strong></td>
			<td>False negative</td>
			<td>True negative</td>
		</tr>
	</tbody>
</table>

<p>The rows and columns follow the same arrangement, but each cell now includes a label: true positive, false positive, false negative, and true negative. These labels offer a clearer understanding of the relationship between predicted and actual outcomes:</p>

<dl>
	<dt>True positive (TP)</dt>
	<dd>
	<p>The<a contenteditable="false" data-primary="true positive (TP) label, in confusion matrices" data-type="indexterm" id="id556"/><a contenteditable="false" data-primary="TP (true positive) label, in confusion matrices" data-type="indexterm" id="id557"/> model<a contenteditable="false" data-primary="confusion matrices" data-secondary="labels" data-type="indexterm" id="id558"/> predicted the positive class, and the actual class was also positive. This is a correct prediction for the positive class.</p>
	</dd>
	<dt>False positive (FP)</dt>
	<dd>
	<p>The <a contenteditable="false" data-primary="false positive (FP) label, in confusion matrices" data-type="indexterm" id="id559"/><a contenteditable="false" data-primary="FP (false positive) label, in confusion matrices" data-type="indexterm" id="id560"/>model predicted the positive class, but the actual class was negative. This is an incorrect prediction, often referred to as a <em>type I error</em>.</p>
	</dd>
	<dt>False negative (FN)</dt>
	<dd>
	<p>The <a contenteditable="false" data-primary="false negative (FN) label, in confusion matrices" data-type="indexterm" id="id561"/><a contenteditable="false" data-primary="FN (false negative) label, in confusion matrices" data-type="indexterm" id="id562"/>model predicted the negative class, but the actual class was positive. This is another incorrect prediction, known as a <em>type II error</em>.</p>
	</dd>
	<dt>True negative (TN)</dt>
	<dd>
	<p>The<a contenteditable="false" data-primary="true negative (TN) label, in confusion matrices" data-type="indexterm" id="id563"/><a contenteditable="false" data-primary="TN (true negative) label, in confusion matrices" data-type="indexterm" id="id564"/> model predicted the negative class, and the actual class was also negative. This is a correct prediction for the negative class.</p>
	</dd>
</dl>

<p>Calling this a <em>confusion matrix</em> is certainly apt: it can be tough to understand this. Yet the confusion matrix is likely to be on the exam. This is why it’s a good idea to memorize the four outcomes.</p>

<p>Let’s now take a look at other common evaluation metrics.</p>

<section data-pdf-bookmark="Accuracy" data-type="sect3"><div class="sect3" id="i04_chapter4_accuracy_1742068262072639">
<h3>Accuracy</h3>

<p>One <a contenteditable="false" data-primary="accuracy" data-type="indexterm" id="id565"/>basic metric derived from the <a contenteditable="false" data-primary="confusion matrices" data-secondary="accuracy" data-type="indexterm" id="id566"/>confusion matrix is <em>accuracy</em>, which is simply the proportion of correct predictions out of the total. It’s calculated as:</p>

<div data-type="equation">
<p>Accuracy = (TN + TP) ÷ (TN + FN + FP + TP)</p>
</div>

<p>For our credit score example, the calculation is:</p>

<div data-type="equation">
<p>(2 + 3) ÷ (2 + 1 + 0 + 3) = 5 ÷ 6 = 0.83</p>
</div>

<p>This means our model correctly predicted loan defaults 83% of the time. While accuracy seems like a good measure, it can be misleading if the data is imbalanced. For instance, if only a small percentage of people default, a model could predict no default for everyone and still achieve high accuracy, but it wouldn’t truly capture the patterns in the <a contenteditable="false" data-primary="confusion matrices" data-startref="icd1005" data-type="indexterm" id="id567"/>​data.</p>
</div></section>

<section data-pdf-bookmark="Recall" data-type="sect3"><div class="sect3" id="i04_chapter4_recall_1742068262072694">
<h3>Recall</h3>

<p><em>Recall</em> measures<a contenteditable="false" data-primary="recall" data-type="indexterm" id="id568"/> how well the model identifies positive cases (people who defaulted on a loan). It’s calculated as:</p>

<div data-type="equation">
<p>Recall = TP ÷ (TP + FN)</p>
</div>

<p>For our example, the calculation is:</p>

<div data-type="equation">
<p>3 ÷ (3 + 1) = 3 ÷ 4 = 0.75</p>
</div>

<p>So our model correctly identified 75% of those who defaulted.</p>
</div></section>

<section data-pdf-bookmark="Precision" data-type="sect3"><div class="sect3" id="i04_chapter4_precision_1742068262072747">
<h3>Precision</h3>

<p><em>Precision</em> tells<a contenteditable="false" data-primary="precision" data-type="indexterm" id="id569"/> us how accurate the positive predictions are—that is, how many of the predicted defaults were actual defaults. It’s calculated as:</p>

<div data-type="equation">
<p>Precision = TP ÷ (TP + FP)</p>
</div>

<p>In our case, the calculation is:</p>

<div data-type="equation">
<p>3 ÷ (3 + 0) = 3 ÷ 3 = 1.0</p>
</div>

<p>This means that every time the model predicted a default, it was correct 100% of the time.</p>
</div></section>

<section data-pdf-bookmark="F1 score" data-type="sect3"><div class="sect3" id="i04_chapter4_f1_score_1742068262072803">
<h3>F1 score</h3>

<p>The <em>F1 score</em> provides<a contenteditable="false" data-primary="F1 score" data-type="indexterm" id="id570"/> a harmonic mean between precision and recall, which is a type of average that emphasizes the smaller values in a dataset. The harmonic mean is calculated as the reciprocal of the average of the reciprocals of the values, ensuring that both precision and recall are given equal weight. Unlike accuracy, which can be misleading when dealing with imbalanced datasets, the F1 score takes both false positives and false negatives into account, offering a more nuanced view of a model’s <span class="keep-together">effectiveness</span>.</p>

<p>The formula is:</p>

<div data-type="equation">
<p>F1 score = 2 × Precision × Recall ÷ Precision + Recall</p>
</div>

<p>For our example, the calculation is:</p>

<div data-type="equation">
<p>2 × 1.0 × 0.75 ÷ 1.0 + 0.75 = 1.5 ÷ 1.75 = 0.86</p>
</div>

<p>The resulting F1 score of 0.86 indicates that the model achieves a good balance between precision and recall, performing effectively in correctly identifying positive cases while minimizing false positives and false negatives.</p>
</div></section>

<section data-pdf-bookmark="Area under the curve" data-type="sect3"><div class="sect3" id="i04_chapter4_area_under_the_curve_1742068262072862">
<h3>Area under the curve</h3>

<p>Another<a contenteditable="false" data-primary="area under the curve (AUC)" data-type="indexterm" id="id571"/><a contenteditable="false" data-primary="AUC (area under the curve)" data-type="indexterm" id="id572"/> way to think about recall is to call it <a contenteditable="false" data-primary="true positive rate (TPR)" data-type="indexterm" id="id573"/><a contenteditable="false" data-primary="TPR (true positive rate)" data-type="indexterm" id="id574"/>the <em>true positive rate (TPR)</em>, which shows how well the model identifies positive cases (defaults, in our example). On the flip side, we also have the <em>false positive rate (FPR)</em>, <a contenteditable="false" data-primary="false positive rate (FPR)" data-type="indexterm" id="id575"/><a contenteditable="false" data-primary="FPR (false positive rate)" data-type="indexterm" id="id576"/>which measures how often the model incorrectly predicts a default when there wasn’t one. The FPR is calculated as FP ÷ (FP + TN). In our case, since there were no false positives, the FPR is 0 ÷ 2 = 0.</p>

<p>Now, if we adjust the threshold for predicting a default (for instance, moving it higher or lower than 0.5), that would change the balance of positive and negative predictions, which means both the TPR and FPR would shift. A common way to visualize this is by plotting <a contenteditable="false" data-primary="receiver operating characteristic (ROC) curve" data-type="indexterm" id="id577"/><a contenteditable="false" data-primary="ROC (receiver operating characteristic) curve" data-type="indexterm" id="id578"/>a <em>receiver operating characteristic (ROC) curve</em>, which compares the TPR and FPR across all possible threshold values. <a data-type="xref" href="#i04_chapter4_figure_8_1742068262037296">Figure 4-8</a> illustrates this.</p>

<figure><div class="figure" id="i04_chapter4_figure_8_1742068262037296"><img src="assets/aaif_0408.png"/>
<h6><span class="label">Figure 4-8. </span>ROC plots the true positive and false positive rates</h6>
</div></figure>

<p>The perfect ROC curve would shoot straight up along the TPR axis and then run across the top, giving it an area under the curve (AUC) of 1.0, as shown in <a data-type="xref" href="#i04_chapter4_figure_8_1742068262037296">Figure 4-8</a>. This means that the model makes perfect predictions. A completely random model would follow a diagonal line, with an AUC of 0.5—basically just guessing. For our credit score example, let’s assume we generated the ROC curve and the AUC is 0.875. This tells us that the model is much better than guessing and performs well in predicting loan defaults.</p>

<p><a data-type="xref" href="#i04_chapter4_table_11_1742068262048765">Table 4-11</a> sums up what we <a contenteditable="false" data-primary="binary classification" data-secondary="key concepts in" data-type="indexterm" id="id579"/>have <a contenteditable="false" data-primary="binary classification" data-secondary="evaluation metrics for" data-startref="icd418" data-type="indexterm" id="id580"/>learned <a contenteditable="false" data-primary="classification" data-secondary="binary" data-startref="icd416" data-type="indexterm" id="id581"/><a contenteditable="false" data-primary="binary classification" data-startref="icd417" data-type="indexterm" id="id582"/>about binary classification.</p>

<table class="border" id="i04_chapter4_table_11_1742068262048765">
	<caption><span class="label">Table 4-11. </span>Key concepts in binary classification</caption>
	<thead>
		<tr>
			<th>Factors</th>
			<th>Definition</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Purpose</p>
			</td>
			<td>
			<p>To categorize data into one of two distinct classes, often labeled as 0 and 1, such as predicting “spam” versus “not spam” or “default” versus “no default”</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Process</p>
			</td>
			<td>
			<ol>
				<li>Split data into training and testing sets.</li>
				<li>Train the model to classify based on patterns in the data.</li>
				<li>Test and validate model accuracy on new data.</li>
			</ol>
			</td>
		</tr>
		<tr>
			<td>
			<p>Training the model</p>
			</td>
			<td>
			<p>Commonly uses logistic regression to fit a function that estimates the probability of an outcome between 0 and 1, applying a threshold (e.g., 0.5) to classify results</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Evaluation metrics</p>
			</td>
			<td>
			<ul>
				<li>Accuracy: Overall correct predictions divided by total predictions</li>
				<li>Confusion matrix: A table that summarizes the performance of a classification model by displaying the counts of TPs, FPs, TNs, and FNs.</li>
				<li>Recall: TPR assessing how well the model identifies positive cases</li>
				<li>Precision: Accuracy of positive predictions</li>
				<li>F1 score: Harmonic mean of precision and recall for balanced evaluation</li>
				<li>AUC: Measures the model’s ability to distinguish between classes across all thresholds, with 1.0 indicating a perfect classifier</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Multiclass Classification" data-type="sect2"><div class="sect2" id="i04_chapter4_multiclass_classification_1742068262072926">
<h2>Multiclass Classification</h2>

<p>When <a contenteditable="false" data-primary="classification" data-secondary="multiclass" data-type="indexterm" id="icd419"/><a contenteditable="false" data-primary="multiclass classification" data-type="indexterm" id="icd420"/>you’re working with <em>multiclass classification</em>, you’re figuring out which category, out of several, best fits a specific observation. This is based on calculating the likelihood of various outcomes. It allows a model to predict which option is most likely for a given case.</p>

<p>Let’s <a contenteditable="false" data-primary="multiclass classification" data-secondary="flower example" data-type="indexterm" id="icd420a"/>use a group of flowers as an example. For each flower, we’ve measured the petal length (<em>x</em>), and we’re trying to predict the flower type (<em>y</em>), which could be one of the following:</p>

<ul>
	<li>
	<p>0 = rose</p>
	</li>
	<li>
	<p>1 = daisy</p>
	</li>
	<li>
	<p>2 = tulip</p>
	</li>
</ul>

<p>Of course, in a real-world scenario, you’d usually have more than just one feature (<em>x</em>) to work with. But for simplicity, we’re sticking to a single feature here. The data is in <a data-type="xref" href="#i04_chapter4_table_12_1742068262048789">Table 4-12</a>.</p>

<table class="border" id="i04_chapter4_table_12_1742068262048789">
	<caption><span class="label">Table 4-12. </span>The relationship between petal length and flower type</caption>
	<thead>
		<tr>
			<th>Petal length (<em>x</em>)</th>
			<th>Flower type (<em>y</em>)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>167</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>172</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>225</p>
			</td>
			<td>
			<p>2</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>197</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>189</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>232</p>
			</td>
			<td>
			<p>2</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>158</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
	</tbody>
</table>

<p>To train our model, we need to select an algorithm, and we have two main options to choose from: one-vs-rest (OVR) and multinomial.</p>

<section data-pdf-bookmark="One-vs-rest algorithms" data-type="sect3"><div class="sect3" id="i04_chapter4_one_vs_rest_algorithms_1742068262073007">
<h3>One-vs-rest algorithms</h3>

<p>With <a contenteditable="false" data-primary="one-vs-rest (OVR) algorithms" data-type="indexterm" id="id583"/><a contenteditable="false" data-primary="OVR (one-vs-rest) algorithms" data-type="indexterm" id="id584"/>OVR, <a contenteditable="false" data-primary="multiclass classification" data-secondary="OVR algorithms" data-type="indexterm" id="id585"/>the idea is to build a separate binary classification model for each class. Each model decides whether a given observation belongs to its specific class. Using our flower example, this method would create three models:</p>

<dl>
	<dt>Model 1</dt>
	<dd>
	<p>Determines if a flower is a rose (class 0)</p>
	</dd>
	<dt>Model 2</dt>
	<dd>
	<p>Determines if a flower is a daisy (class 1)</p>
	</dd>
	<dt>Model 3</dt>
	<dd>
	<p>Determines if a flower is a tulip (class 2)</p>
	</dd>
</dl>

<p>Each model calculates a probability between 0 and 1. The class with the highest probability is selected as the prediction.</p>
</div></section>

<section data-pdf-bookmark="Multinomial algorithms" data-type="sect3"><div class="sect3" id="i04_chapter4_multinomial_algorithms_1742068262073065">
<h3>Multinomial algorithms</h3>

<p>A multinomial algorithm<a contenteditable="false" data-primary="multinomial algorithms" data-type="indexterm" id="id586"/> generates<a contenteditable="false" data-primary="multiclass classification" data-secondary="multinomial algorithms" data-type="indexterm" id="id587"/> a single function that produces a probability distribution for all the classes at once. Instead of having multiple models, it gives you a vector with probabilities for each class, and the total of these probabilities always equals 1.</p>

<p>For example, you might get an output like this:</p>

<div data-type="equation">
<p>[0.2, 0.3, 0.5]</p>
</div>

<p>This means there’s a 20% chance the flower is a rose, a 30% chance it’s a daisy, and a 50% chance it’s a tulip. Since 0.5 is the highest, the model predicts tulip.</p>

<p>No matter which algorithm you go with, the model uses these probabilities to predict the most likely class for any observation.</p>
</div></section>
</div></section>

<section data-pdf-bookmark="Evaluation of a Multiclass Classification Model" data-type="sect2"><div class="sect2" id="i04_chapter4_evaluation_of_a_multiclass_classification_model_1742068262073136">
<h2>Evaluation of a Multiclass Classification Model</h2>

<p>You <a contenteditable="false" data-primary="multiclass classification" data-secondary="evaluation metrics for" data-type="indexterm" id="icd421"/>can measure the performance of a multiclass classifier by calculating binary metrics for each individual class or by using aggregate metrics that consider all classes together. <a data-type="xref" href="#i04_chapter4_table_13_1742068262048811">Table 4-13</a> shows the evaluation for our flower example.</p>

<table class="border" id="i04_chapter4_table_13_1742068262048811">
	<caption><span class="label">Table 4-13. </span>Measuring the performance of a multiclass classifier</caption>
	<thead>
		<tr>
			<th>Petal length (<em>x</em>)</th>
			<th>Actual flower (<em>y</em>)</th>
			<th>Predicted flower (ŷ)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>165</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>171</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>205</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>195</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>183</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>1</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>221</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>2</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>214</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>2</p>
			</td>
		</tr>
	</tbody>
</table>

<p>The confusion matrix<a contenteditable="false" data-primary="confusion matrices" data-secondary="multiclass classifiers" data-type="indexterm" id="id588"/> for a multiclass classifier works similarly to the confusion matrix for a binary one, but instead of two classes, it captures predictions across multiple classes. This shows the number of predictions for each combination of actual and predicted class labels. <a data-type="xref" href="#i04_chapter4_figure_10_1742068262037358">Figure 4-9</a> displays the confusion matrix.</p>

<figure><div class="figure" id="i04_chapter4_figure_10_1742068262037358"><img src="assets/aaif_0409.png"/>
<h6><span class="label">Figure 4-9. </span>Confusion matrix showing the actual and predicted flower classes</h6>
</div></figure>

<p>From this confusion matrix, we can figure out the key metrics for each flower class, listed in <a data-type="xref" href="#i04_chapter4_table_14_1742068262048835">Table 4-14</a>.</p>

<table class="border less_space pagebreak-before" id="i04_chapter4_table_14_1742068262048835">
	<caption><span class="label">Table 4-14. </span>Confusion matrix for the key metrics for each flower class</caption>
	<thead>
		<tr>
			<th>Class</th>
			<th>TP</th>
			<th>TN</th>
			<th>FP</th>
			<th>FN</th>
			<th>Accuracy</th>
			<th>Recall</th>
			<th>Precision</th>
			<th>F1 score</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>5</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>1.0</p>
			</td>
			<td>
			<p>1.0</p>
			</td>
			<td>
			<p>1.0</p>
			</td>
			<td>
			<p>1.0</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>4</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>0.86</p>
			</td>
			<td>
			<p>1.0</p>
			</td>
			<td>
			<p>0.67</p>
			</td>
			<td>
			<p>0.8</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>4</p>
			</td>
			<td>
			<p>0</p>
			</td>
			<td>
			<p>1</p>
			</td>
			<td>
			<p>0.86</p>
			</td>
			<td>
			<p>0.67</p>
			</td>
			<td>
			<p>1.0</p>
			</td>
			<td>
			<p>0.8</p>
			</td>
		</tr>
	</tbody>
</table>

<p>To get the overall accuracy, recall, and precision for the model, we sum the values for TPs, TNs, FPs, and FNs:</p>

<div data-type="equation">
<p>Overall accuracy = (13 + 6) ÷ (13 + 6 + 1 + 1) = 0.90</p>

<p>Overall recall = 6 ÷ (6 + 1) = 0.86</p>

<p>Overall precision = 6 ÷ (6 + 1) = 0.86</p>
</div>

<p>Finally, the overall F1 score is calculated using the overall recall and precision values:</p>

<div data-type="equation">
<p>Overall F1 score = (2 × 0.86 × 0.86) ÷ (0.86 + 0.86) = 0.86</p>
</div>

<p>These metrics help us understand how well our flower classifier is performing across all  <a contenteditable="false" data-primary="multiclass classification" data-secondary="flower example" data-startref="icd420a" data-type="indexterm" id="id589"/>classes.</p>

<p>Let’s sum up what we have learned <a contenteditable="false" data-primary="multiclass classification" data-secondary="evaluation metrics for" data-startref="icd421" data-type="indexterm" id="id590"/>about <a contenteditable="false" data-primary="multiclass classification" data-secondary="key concepts in" data-type="indexterm" id="id591"/>multiclass <a contenteditable="false" data-primary="classification" data-secondary="multiclass" data-startref="icd419" data-type="indexterm" id="id592"/><a contenteditable="false" data-primary="multiclass classification" data-startref="icd420" data-type="indexterm" id="id593"/>classification <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-startref="icd413" data-tertiary="classification" data-type="indexterm" id="id594"/><a contenteditable="false" data-primary="classification" data-startref="icd414" data-type="indexterm" id="id595"/><a contenteditable="false" data-primary="supervised learning" data-secondary="classification" data-startref="icd415" data-type="indexterm" id="id596"/>in <a data-type="xref" href="#i04_chapter4_table_15_1742068262048858">Table 4-15</a>.</p>

<table class="border" id="i04_chapter4_table_15_1742068262048858">
	<caption><span class="label">Table 4-15. </span>Key concepts in multiclass classification</caption>
	<thead>
		<tr>
			<th>Factors</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Purpose</p>
			</td>
			<td>
			<p>To categorize data into one of multiple possible classes by estimating the likelihood of each class for a given observation, such as predicting flower types based on features like petal length</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Process</p>
			</td>
			<td>
			<ol>
				<li>Select and train a model using either OVR or multinomial algorithms.</li>
				<li>Predict the class based on the highest probability output.</li>
				<li>Evaluate accuracy and refine the model as needed.</li>
			</ol>
			</td>
		</tr>
		<tr>
			<td>
			<p>Modeling techniques</p>
			</td>
			<td>
			<ul>
				<li>OVR: Builds a binary classifier for each class to predict if a given observation belongs to that class</li>
				<li>Multinomial: Uses a single model that outputs probabilities for all classes simultaneously, with the highest probability indicating the predicted class</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>
			<p>Evaluation metrics</p>
			</td>
			<td>
			<ul>
				<li>Confusion matrix: Visualizes correct and incorrect predictions across all classes</li>
				<li>Accuracy, recall, precision, and F1 score: Calculated per class and as an aggregate to gauge overall performance</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>
</div></section>
</div></section>

<section data-pdf-bookmark="Clustering" data-type="sect1"><div class="sect1" id="i04_chapter4_clustering_1742068262073208">
<h1>Clustering</h1>

<p><em>Clustering</em> is<a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-tertiary="clustering" data-type="indexterm" id="icd422"/><a contenteditable="false" data-primary="clustering" data-type="indexterm" id="icd423"/> an ML technique used to organize data into groups, or clusters, based on the similarity of their features. It falls under the category of unsupervised learning because it does not rely on prelabeled data to identify these groups. Instead, the <span class="keep-together">algorithm</span> analyzes the data’s features and assigns each data point to a cluster based on shared characteristics. In essence, the clusters act as labels generated by the model after analyzing the inherent structure of the data.</p>

<p>Clustering is particularly useful in scenarios where you need to uncover hidden patterns or groupings within data. For example, it can help in market segmentation, where customers are grouped based on purchasing behavior, or in biology, where genes with similar functions are grouped together. It’s also valuable for anomaly detection, such as identifying unusual transactions in fraud detection, or in image segmentation to group pixels into distinct regions.</p>

<p>Let’s take a look at a more detailed example. Suppose you’re a marine biologist studying a <a contenteditable="false" data-primary="clustering" data-secondary="fish example" data-type="indexterm" id="icd423a"/>group of fish. Instead of focusing on identifying the species, you’re only interested in grouping the fish based on two characteristics: the length of the fish and the number of fins. The data is in <a data-type="xref" href="#i04_chapter4_table_16_1742068262048881">Table 4-16</a>.</p>

<table class="border" id="i04_chapter4_table_16_1742068262048881">
	<caption><span class="label">Table 4-16. </span>Grouping of fish based on similarities in length and fin count</caption>
	<thead>
		<tr>
			<th>Length (<em>x</em>1)</th>
			<th>Fins (<em>x</em>2)</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>10</p>
			</td>
			<td>
			<p>3</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>12</p>
			</td>
			<td>
			<p>4</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>15</p>
			</td>
			<td>
			<p>2</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>15</p>
			</td>
			<td>
			<p>2</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>16</p>
			</td>
			<td>
			<p>5</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>18</p>
			</td>
			<td>
			<p>6</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>20</p>
			</td>
			<td>
			<p>2</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>22</p>
			</td>
			<td>
			<p>5</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>24</p>
			</td>
			<td>
			<p>6</p>
			</td>
		</tr>
	</tbody>
</table>

<p>In this case, the goal is to cluster fish with similar lengths and fin counts together, not to classify them into any specific species.</p>

<p>There are several ways to approach clustering. One of the most popular methods is called<a contenteditable="false" data-primary="k-means clustering" data-type="indexterm" id="id597"/> <em>k-means clustering</em>.</p>

<p>Let’s see how to do this. First, you turn your data into vectors, which means assigning each data point a coordinate in a space. If you have two features, like the length of a fish (<em>x</em>1) and the number of fins (<em>x</em>2), these become two coordinates—[x1, x2]—that can be plotted on a two-dimensional graph.</p>

<p>Next, you decide how many clusters you want. Let’s say you choose three clusters (<span class="keep-together"><em>k</em> = 3</span>). You randomly place three points on the graph to represent the centers of your clusters, <a contenteditable="false" data-primary="centroids" data-type="indexterm" id="id598"/>called <em>centroids</em>. Then, each fish is assigned to the centroid it is closest to. The centroid itself is recalculated to be the average of the data points (fish) assigned to it.</p>

<p>This process repeats: as the centroids move, some fish may become closer to a different centroid, so the assignments change. This back-and-forth of reassigning fish to clusters and adjusting the centroids continues until the groups become <a contenteditable="false" data-primary="clustering" data-secondary="fish example" data-startref="icd423a" data-type="indexterm" id="id599"/>stable. You can see this in <a data-type="xref" href="#i04_chapter4_figure_11_1742068262037379">Figure 4-10</a>.</p>

<figure><div class="figure" id="i04_chapter4_figure_11_1742068262037379"><img src="assets/aaif_0410.png"/>
<h6><span class="label">Figure 4-10. </span>The clusters of fish grouped by length and number of fins</h6>
</div></figure>

<p>Since there’s no known label to which to compare the cluster assignments, the effectiveness of a clustering model is judged by how well the clusters are distinct from one another. <a contenteditable="false" data-primary="clustering" data-secondary="evaluation metrics for" data-type="indexterm" id="id600"/>You can use several metrics to measure how well the clusters are separated:</p>

<dl>
	<dt>Average distance to cluster center</dt>
	<dd>
	<p>Measures<a contenteditable="false" data-primary="average distance to cluster center metric" data-type="indexterm" id="id601"/> how close, on average, each point in the cluster is to the centroid</p>
	</dd>
	<dt>Average distance to other centers</dt>
	<dd>
	<p>Looks<a contenteditable="false" data-primary="average distance to other centers metric" data-type="indexterm" id="id602"/> at how close, on average, each point in the cluster is to the centroids of other clusters</p>
	</dd>
	<dt>Maximum distance to cluster center</dt>
	<dd>
	<p>Finds<a contenteditable="false" data-primary="maximum distance to cluster center metric" data-type="indexterm" id="id603"/> the farthest point from the centroid within the cluster</p>
	</dd>
	<dt>Silhouette score</dt>
	<dd>
	<p>A <a contenteditable="false" data-primary="silhouette score" data-type="indexterm" id="id604"/>number between -1 and 1 that shows how well separated the clusters are (with 1 indicating the best <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-startref="icd422" data-tertiary="clustering" data-type="indexterm" id="id605"/><a contenteditable="false" data-primary="clustering" data-startref="icd423" data-type="indexterm" id="id606"/>separation)</p>
	</dd>
</dl>
</div></section>

<section data-pdf-bookmark="Deep Learning" data-type="sect1"><div class="sect1" id="i04_chapter4_deep_learning_1742068262073276">
<h1>Deep Learning</h1>

<p>DL <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-tertiary="deep learning" data-type="indexterm" id="icd424"/><a contenteditable="false" data-primary="DL (deep learning)" data-type="indexterm" id="icd425"/>is<a contenteditable="false" data-primary="deep learning" data-see="DL" data-type="indexterm" id="id607"/> a more sophisticated type of ML that tries to mimic how our brains learn. At its core, DL relies on what’s called an <em>artificial neural network</em>, <a contenteditable="false" data-primary="artificial neural networks" data-type="indexterm" id="id608"/>which behaves similarly to how biological neurons work by using mathematical functions. These artificial neural networks consist of multiple layers of neurons, which creates a structure that processes data through increasingly complex layers. This is why the method is called “deep” learning, and the resulting models are known as <em>deep neural networks (DNNs)</em>. <a contenteditable="false" data-primary="deep neural networks (DNNs)" data-type="indexterm" id="id609"/><a contenteditable="false" data-primary="DNNs (deep neural networks)" data-type="indexterm" id="id610"/>You can apply DL to various tasks, such as predicting outcomes (regression), classifying data, and even more complex problems like understanding language or recognizing images.</p>

<p>Like other forms of ML, DL involves training a model to predict an output based on one or more inputs. The model’s job is to figure out a function that connects these inputs to the correct outputs. This function is built up across the neural network layers, with each layer handling a part of the process. During training, the model repeatedly adjusts its internal parameters (called <em>weights</em>) to minimize how far its predictions are from the correct values. Over time, the model fine-tunes these weights to improve accuracy and make better predictions.</p>

<p>Let’s break down how a neural network works by looking at an example that classifies three <a contenteditable="false" data-primary="DL (deep learning)" data-secondary="fruit example" data-type="indexterm" id="icd425a"/>different types of fruit: apples, oranges, and bananas. In this case, the input data (<em>x</em>) is a vector of measurements for these features. Let’s say we measure:</p>

<ul>
	<li>
	<p>The fruit’s weight</p>
	</li>
	<li>
	<p>The color’s hue value</p>
	</li>
	<li>
	<p>The curvature of the fruit</p>
	</li>
</ul>

<p>So <em>x</em> is a vector with three values:</p>

<div data-type="equation">
<p><em>x</em> = [x1, x2, x3]</p>
</div>

<p>Now, the label (<em>y</em>) is the fruit’s type, which could be one of the three: apple, orange, or banana. Since this is a classification problem, the neural network will predict probabilities for each class. So <em>y</em> will be a vector representing the probabilities for each fruit: [P(apple|x), P(orange|x), P(banana|x)].</p>

<p>Here’s how the network makes a prediction:</p>

<ol>
	<li>
	<p>The feature data for a fruit, like [180g, 0.8, 0.3], is fed into the input layer of the neural network.</p>
	</li>
	<li>
	<p>Each neuron in the input layer takes the feature values, multiplies them by their assigned weights, and passes the result through an activation function.</p>
	</li>
	<li>
	<p>The neurons in each layer are connected to neurons in the next layer, forming a fully connected network where the results are fed forward through the network’s layers.</p>
	</li>
	<li>
	<p>The output layer generates a vector of probabilities—for example, [0.2, 0.7, 0.1]—which represents the likelihood of the fruit being an apple, orange, or banana. Since 0.7 is the highest value, the network predicts the fruit is an orange.</p>
	</li>
</ol>

<p><a data-type="xref" href="#i04_chapter4_figure_12_1742068262037397">Figure 4-11</a> shows a graphical representation of a deep learning network.</p>

<figure><div class="figure" id="i04_chapter4_figure_12_1742068262037397"><img src="assets/aaif_0411.png"/>
<h6><span class="label">Figure 4-11. </span>A deep learning network</h6>
</div></figure>

<p>The magic of neural networks lies in adjusting the weights during training to make better predictions. Initially, these weights are random, but through a process called <em>backpropagation</em>, the <a contenteditable="false" data-primary="backpropagation" data-type="indexterm" id="id611"/>network refines them to improve accuracy. Here’s a high-level view of the training process:</p>

<ol>
	<li>
	<p>Define your training data—basically, a set of known fruit types with their corresponding measurements.</p>
	</li>
	<li>
	<p>The data is passed through the neural network, and the output is compared to the actual labels using a loss function. A loss function is a mathematical formula that quantifies the difference between the predicted output and the true labels. For example, if the network predicts [0.3, 0.1, 0.6] for a banana, but the true label is [0, 0, 1], the loss function calculates the discrepancy between these values. This difference, or loss, serves as a signal for the model to adjust its parameters and improve its predictions. The ultimate goal is to minimize this loss to enhance the network’s accuracy.</p>
	</li>
	<li>
	<p>The network calculates how much each weight contributed to the loss and uses this information to adjust the weights.</p>
	</li>
	<li>
	<p>This process is repeated over many cycles (called <em>epochs</em>) until<a contenteditable="false" data-primary="epochs" data-type="indexterm" id="id612"/> the loss is minimized and the model is accurate enough.</p>
	</li>
</ol>

<p>This training typically happens in batches of data, using powerful hardware like GPUs to handle the heavy computation involved. Over time, the network becomes good at identifying whether the fruit is an apple, orange, or <a contenteditable="false" data-primary="DL (deep learning)" data-secondary="fruit example" data-startref="icd425a" data-type="indexterm" id="id613"/>banana.</p>

<p>For the AI-900 exam, you may see questions that ask about the differences between ML <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="deep learning versus" data-type="indexterm" id="id614"/>and DL. <a data-type="xref" href="#i04_chapter4_table_17_1742068262048902">Table 4-17</a> lays out these <a contenteditable="false" data-primary="DL (deep learning)" data-secondary="key differences between ML and" data-type="indexterm" id="id615"/>key differences to help you get a solid grasp <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-startref="icd424" data-tertiary="deep learning" data-type="indexterm" id="id616"/><a contenteditable="false" data-primary="DL (deep learning)" data-startref="icd425" data-type="indexterm" id="id617"/>of <a contenteditable="false" data-primary="ML (machine learning)" data-secondary="types of" data-startref="icd407" data-type="indexterm" id="id618"/>each.</p>

<table class="border" id="i04_chapter4_table_17_1742068262048902">
	<caption><span class="label">Table 4-17. </span>Key differences between ML and DL</caption>
	<thead>
		<tr>
			<th>Factor</th>
			<th>Machine learning</th>
			<th>Deep learning</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
			<p>Complexity</p>
			</td>
			<td>
			<p>Less</p>
			</td>
			<td>
			<p>More</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Data requirements</p>
			</td>
			<td>
			<p>Performs well with structured, smaller datasets</p>
			</td>
			<td>
			<p>Requires large volumes of data to achieve accuracy</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Training time</p>
			</td>
			<td>
			<p>Faster</p>
			</td>
			<td>
			<p>Longer</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Hardware requirements</p>
			</td>
			<td>
			<p>Can run on standard CPUs</p>
			</td>
			<td>
			<p>Often requires GPUs</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>Interpretability</p>
			</td>
			<td>
			<p>Easier</p>
			</td>
			<td>
			<p>More difficult</p>
			</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="i04_chapter4_conclusion_1742068262073336">
<h1>Conclusion</h1>

<p>In this chapter, we’ve explored the core concepts of ML, diving into essential techniques like regression, classification, and clustering. Each of these approaches offers unique ways to uncover patterns in data and make accurate predictions. They also empower you to tackle real-world challenges head-on.</p>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Quiz" data-type="sect1"><div class="sect1" id="chapter4quiz">
	<h1>Quiz</h1>
	<p>To check your answers, please refer to the <a data-type="xref" href="app02.html#answers_chapter_4_sample_questions_1745932457451742">“Chapter 4 Answer Key”</a>.</p>
	<ol>
		<li>
		  <p>What is the primary purpose of regression analysis in machine learning (ML)? </p>
		  <ol type="a">
			<li>
			  <p>To categorize data into distinct classes </p>
			</li>
			<li>
			  <p>To cluster similar data points</p>
			</li>
			<li>
			  <p>To predict a numerical outcome based on variables</p>
			</li>
			<li>
			  <p>To analyze images and videos</p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which of the following is an example of supervised learning? </p>
		  <ol type="a">
			<li>
			  <p>K-means clustering </p>
			</li>
			<li>
			  <p>Predicting house prices based on features </p>
			</li>
			<li>
			  <p>Segmenting customers based on purchase history </p>
			</li>
			<li>
			  <p>Identifying anomalies in financial transactions </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>In binary classification, which algorithm is commonly used to predict probabilities between two classes? </p>
		  <ol type="a">
			<li>
			  <p>Linear regression </p>
			</li>
			<li>
			  <p>Logistic regression </p>
			</li>
			<li>
			  <p>Decision trees </p>
			</li>
			<li>
			  <p>K-means </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What does the F1 score represent in model evaluation? </p>
		  <ol type="a">
			<li>
			  <p>The model’s ability to distinguish between classes </p>
			</li>
			<li>
			  <p>The average of errors in predictions</p>
			</li>
			<li>
			  <p>A balance between precision and recall</p>
			</li>
			<li>
			  <p>The total accuracy of the model</p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which step in the ML workflow involves using the model to generate <span class="keep-together">predictions</span>? </p>
		  <ol type="a">
			<li>
			  <p>Training </p>
			</li>
			<li>
			  <p>Inferencing </p>
			</li>
			<li>
			  <p>Validation </p>
			</li>
			<li>
			  <p>Data preparation </p>
			</li>
		  </ol>
		</li>
		<li class="less_space pagebreak-before">
		  <p>What type of learning is K-means clustering associated with? </p>
		  <ol type="a">
			<li>
			  <p>Supervised </p>
			</li>
			<li>
			  <p>Semisupervised </p>
			</li>
			<li>
			  <p>Reinforcement </p>
			</li>
			<li>
			  <p>Unsupervised </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What metric measures how well a regression model explains the variation in data? </p>
		  <ol type="a">
			<li>
			  <p>Mean squared error (MSE) </p>
			</li>
			<li>
			  <p>Coefficient of determination (<em>R</em>²) </p>
			</li>
			<li>
			  <p>Root mean squared error (RMSE) </p>
			</li>
			<li>
			  <p>Mean absolute error (MAE) </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which approach is used to address missing data by estimating based on patterns in available data? </p>
		  <ol type="a">
			<li>
			  <p>Mean imputation </p>
			</li>
			<li>
			  <p>Predictive imputation </p>
			</li>
			<li>
			  <p>Removal of incomplete data </p>
			</li>
			<li>
			  <p>Data normalization </p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>What is the primary goal of classification in ML? </p>
		  <ol type="a">
			<li>
			  <p>To identify hidden patterns in unlabeled data </p>
			</li>
			<li>
			  <p>To predict numerical outcomes</p>
			</li>
			<li>
			  <p>To assign data points to predefined categories</p>
			</li>
			<li>
			  <p>To generate new content</p>
			</li>
		  </ol>
		</li>
		<li>
		  <p>Which ML technique is suitable for grouping similar data points without labels? </p>
		  <ol type="a">
			<li>
			  <p>Regression </p>
			</li>
			<li>
			  <p>Classification </p>
			</li>
			<li>
			  <p>Clustering </p>
			</li>
			<li>
			  <p>Deep learning (DL)</p>
			</li>
		  </ol>
		</li>
	  </ol>
	
	</div></section>
</div></section></body></html>