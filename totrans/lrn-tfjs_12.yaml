- en: Chapter 11\. Transfer Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。迁移学习
- en: “Learn from the mistakes of others. You can’t live long enough to make them
    all yourself.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “向他人的错误学习。你活不到足够长的时间来犯所有的错误。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Eleanor Roosevelt
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —埃莉诺·罗斯福
- en: It can be challenging to have an extensive collection of data, battle-tested
    model structure, and processing power. Wouldn’t it be nice to cut a corner? That
    nifty trick in [Chapter 7](ch07.html#the_chapter_7) where you could use Teachable
    Machine to transfer the qualities of a trained model to a novel one was pretty
    useful. In fact, this is a common trick in the machine learning world. While Teachable
    Machine hid the specifics and offered you only a single model, you can understand
    the mechanics of this trick and use it on all kinds of cool tasks. In this chapter,
    we will reveal the magic behind this process. While we’ll be focused on the example
    of MobileNet for simplicity, this can be applied to all kinds of models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有大量数据、经过实战检验的模型结构和处理能力可能是具有挑战性的。能不能简单点？在第7章中，您可以使用 Teachable Machine 将训练好的模型的特质转移到新模型中，这是非常有用的。事实上，这是机器学习世界中的一个常见技巧。虽然
    Teachable Machine 隐藏了具体细节，只提供了一个模型，但您可以理解这个技巧的原理，并将其应用于各种酷炫的任务。在本章中，我们将揭示这个过程背后的魔法。虽然我们将简单地以
    MobileNet 为例，但这可以应用于各种模型。
- en: Transfer learning is the act of taking a trained model and repurposing it for
    a second related task.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是指将训练好的模型重新用于第二个相关任务。
- en: 'There are a few repeatable benefits to using transfer learning for your machine
    learning solution. Most projects utilize some amount of transfer learning for
    these reasons:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迁移学习为您的机器学习解决方案带来一些可重复的好处。大多数项目出于以下原因利用一定程度的迁移学习：
- en: Reutilizing a battle-tested model structure
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复使用经过实战检验的模型结构
- en: Getting a solution faster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快地获得解决方案
- en: Getting a solution via less data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过较少的数据获得解决方案
- en: In this chapter, you’ll learn several strategies for transfer learning. You
    will focus on MobileNet as a fundamental example that can be reused to identify
    a myriad of new classes in various ways.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习几种迁移学习策略。您将专注于 MobileNet 作为一个基本示例，可以以各种方式重复使用来识别各种新类别。
- en: 'We will:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将：
- en: Review how transfer learning works
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾迁移学习的工作原理
- en: See how to reuse feature vectors
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看如何重复使用特征向量
- en: Cut into Layers models and reconstruct new models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型切割成层并重构新模型
- en: Learn about KNN and deferred classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 KNN 和延迟分类
- en: When you finish this chapter, you’ll be able to take models that have been trained
    for a long time with lots of data and apply them to your own needs with smaller
    datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够将长时间训练并具有大量数据的模型应用于您自己的需求，即使只有较小的数据集。
- en: How Does Transfer Learning Work?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习是如何工作的？
- en: How does a model that has been trained on different data suddenly work well
    for your *new* data? It sounds miraculous, but it happens in humans every day.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经过不同数据训练的模型如何突然对您的*新*数据起作用？听起来像奇迹，但这在人类中每天都发生。
- en: You’ve spent years identifying animals, and you’ve probably seen hundreds of
    camels, guinea pigs, and beavers from cartoons, zoos, and commercials. Now I’m
    going to show you an animal you’ve probably not seen often, or even at all. The
    animal in [Figure 11-1](#capybara) is called a capybara (*Hydrochoerus hydrochaeris*).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您花了多年时间识别动物，可能看过数百只骆驼、天竺鼠和海狸的卡通、动物园和广告。现在我将向您展示一种您可能很少见到甚至从未见过的动物。[图11-1](#capybara)中的动物被称为水豚（*Hydrochoerus
    hydrochaeris*）。
- en: '![profile of a capybara](assets/ltjs_1101.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![水豚的侧面](assets/ltjs_1101.png)'
- en: Figure 11-1\. The capybara
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1。水豚
- en: For some of you, this is the first time (or one of the few times) you’ve seen
    a photo of a capybara. Now, take a look at the lineup in [Figure 11-2](#transfer_quiz).
    Can you find the capybara?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你们中的一些人，这可能是第一次（或者是少数几次）看到水豚的照片。现在，看看[图11-2](#transfer_quiz)中的阵容。你能找到水豚吗？
- en: '![three mammals quiz](assets/ltjs_1102.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![三种哺乳动物测验](assets/ltjs_1102.png)'
- en: Figure 11-2\. Which one is the capybara?
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2。哪一个是水豚？
- en: The training set of a single photo was enough for you to make a choice because
    you’ve been distinguishing between animals your entire life. With a novel color,
    angle, and photo size, your brain probably detected with absolute certainty that
    animal C was another capybara. The features learned by your years of experience
    have helped you make an educated decision. In that same way, powerful models that
    have significant experience can be taught to learn new things from small amounts
    of new data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一张单独的照片的训练集足以让您做出选择，因为您一生中一直在区分动物。即使是新的颜色、角度和照片尺寸，您的大脑可能也能绝对确定地检测到动物 C 是另一只水豚。您多年的经验学习到的特征帮助您做出了明智的决定。同样地，具有丰富经验的强大模型可以被教导从少量新数据中学习新事物。
- en: Transfer Learning Neural Networks
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习神经网络
- en: Let’s bring things back to MobileNet for a moment. The MobileNet model was trained
    to identify features that distinguish a thousand items from each other. That means
    there are convolutions to detect fur, metal, round things, ears, and all kinds
    of crucial differential features. All these features are chewed up and simplified
    before they are flattened into a neural network, where the combination of various
    features creates a classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时回到 MobileNet。MobileNet 模型经过训练，可以识别区分一千种物品之间的特征。这意味着有卷积来检测毛发、金属、圆形物体、耳朵以及各种关键的差异特征。所有这些特征在被压缩和简化之前都被吸收到一个神经网络中，各种特征的组合形成了分类。
- en: The MobileNet model can identify different breeds of dogs, and even distinguish
    a Maltese terrier from a Tibetan terrier. If you were to make a [“dog or cat”](https://oreil.ly/i9Xxm)
    classifier, it makes sense that a majority of those advanced features would be
    reusable in your simpler model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet 模型可以识别不同品种的狗，甚至可以区分马耳他犬和西藏犬。如果您要制作一个[“狗还是猫”](https://oreil.ly/i9Xxm)分类器，那么在您更简单的模型中，大多数这些高级特征是可以重复使用的。
- en: The previously learned convolutional filters would be extremely useful in identifying
    key features for brand-new classifications, like our capybara example in [Figure 11-2](#transfer_quiz).
    The trick is to take the feature identification portion of the model and apply
    your own neural network to the convolutional output, as illustrated in [Figure 11-3](#transfer_visual).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![changing the NN flowchart](assets/ltjs_1103.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. CNN transfer learning
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So how do you separate and recombine these sections of previously trained models?
    You’ve got lots of options. Again, we’ll learn a bit more about Graph and Layers
    models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Easy MobileNet Transfer Learning
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, [TensorFlow Hub](https://tfhub.dev) already has a MobileNet model
    that is disconnected from any neural network. It offers half a model for you to
    use for transfer learning. Half a model means it hasn’t been tied down into a
    final softmax layer for what it’s meant to classify. This allows us to let MobileNet
    derive the features of an image and then provide us with tensors that we can then
    pass to our own trained network for classification.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: TFHub calls these models *image feature vector* models. [You refine your search
    to show only these models](https://oreil.ly/BkokR) or identify them by looking
    at the problem domain tags, as illustrated in [Figure 11-4](#image_feature_vector).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![screenshot of proper tags](assets/ltjs_1104.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. Problem domain tags for image feature vectors
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You might notice small variations of MobileNet and wonder what the differences
    are. Once you learn a few sneaky terms, each of these model descriptions becomes
    quite readable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we’ll use [Example 11-1](#example_mobile_net).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-1\. One of the image feature vector models
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: imagenet
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: This model was trained on the ImageNet dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: mobilenet_v2
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: The model’s architecture is MobileNet v2.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '130'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The model’s depth multiplier was 1.30\. This results in more features. If you
    want to speed things up, you can choose “05,” which would have less than half
    the feature output with a boost in speed. This is a fine-tuning option when you’re
    ready to modify speed versus depth.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '224'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The model’s expected input size is 224 x 224 images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: feature_vector
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: We already know from the tag, but this model outputs tensors meant to be features
    of the image for a second model to interpret.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a trained model that can identify features in an image, we
    will run our training data through the MobileNet image feature vector model and
    then train a model on the output from that. In other words, the training images
    will turn into a feature vector, and we’ll train a model to interpret that feature
    vector.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this strategy is that it’s straightforward to implement. The
    major drawback is that you’ll have to load two models when you’re ready to use
    the newly trained model (one to generate features and one to interpret them).
    Creatively, there might be some cases where it’s quite useful to “featurize” an
    image and then run that through multiple neural networks. Regardless, let’s see
    it in action.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub Check, Mate!
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to use transfer learning with MobileNet to identify chess pieces
    like the one shown in [Figure 11-5](#chess_promo).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![image of a chess knight on a table](assets/ltjs_1105.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 11-5\. Simple chess pieces classifier
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll only have a few images of each chess piece. That’s not normally enough,
    but with the magic of transfer learning, you’ll get an efficient model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Loading chess images
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this exercise, I’ve compiled a collection of 150 images and loaded them
    into a CSV file for quick use. This isn’t something I’d recommend doing in most
    cases because it’s inefficient for processing and disk space, but it serves as
    a simple vector for some quick in-browser training. The code to load these images
    is now trivial.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can access the chess images and the code that converted them into a CSV
    file in the [*chapter11/extra/node-make-csvs*](https://oreil.ly/INWAN) folder.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以访问象棋图像和将它们转换为 CSV 文件的代码在[*chapter11/extra/node-make-csvs*](https://oreil.ly/INWAN)文件夹中。
- en: The files *chess_labels.csv* and *chess_images.csv* can be found in the [*chess_data.zip*](https://oreil.ly/bcFop)
    file in code associated with this lesson. Unzip this file and use Danfo.js to
    load the contents.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文件 *chess_labels.csv* 和 *chess_images.csv* 可以在与本课程相关的[*chess_data.zip*](https://oreil.ly/bcFop)文件中找到。解压这个文件并使用
    Danfo.js 加载内容。
- en: Many browsers may have issues with concurrently reading all 150 images, so I’ve
    limited the demo to process only 130 images. Working against concurrent data limitations
    is a common issue with machine learning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 许多浏览器可能会在同时读取所有150个图像时出现问题，所以我限制了演示只处理130个图像。与并发数据限制作斗争是机器学习中常见的问题。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Once the image has been featurized, it takes up a lot less space. Feel free
    to experiment with creating features in batches, but that’s outside the scope
    of this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像被提取特征，它所占用的空间就会少得多。随意尝试批量创建特征，但这超出了本章的范围。
- en: 'The images are already 224 x 224, so you can load them with the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图像已经是224 x 224，所以你可以用以下代码加载它们：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO1-1)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_transfer_learning_CO1-1)'
- en: The second parameter to `read_csv` limits the row count to the specified number.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv`的第二个参数限制了行数到指定的数字。'
- en: '[![2](assets/2.png)](#co_transfer_learning_CO1-2)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_transfer_learning_CO1-2)'
- en: The DataFrames are then converted to tensors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将 DataFrames 转换为张量。
- en: '[![3](assets/3.png)](#co_transfer_learning_CO1-3)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_transfer_learning_CO1-3)'
- en: The images were flattened to become serialized but are now reshaped into a rank-four
    batch of RGB images.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被展平以变成序列化，但现在被重新塑造成一个四维的 RGB 图像批次。
- en: 'After a bit of time, this code prints out the X and Y shapes of 130 ready-to-go
    images and encodings:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间，这段代码会打印出130个准备好的图像和编码的 X 和 Y 形状：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If your computer is unable to handle the 130 images, you can lower the `numImages`
    variable and still play along. However, the load time for the CSV file is always
    constant because the entire file must be processed.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机无法处理130个图像，你可以降低`numImages`变量，仍然可以继续。然而，CSV文件的加载时间始终是恒定的，因为整个文件必须被处理。
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Images like chess pieces are perfect for image augmentation because skewing
    chess pieces would never cause one piece to be confused with another. If you ever
    need more images, you can mirror the entire set to effectively double your data.
    [Entire libraries exist to mirror, tilt, and skew images](https://oreil.ly/tCMTN)
    so you can create more data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 象棋棋子这样的图像非常适合进行图像增强，因为扭曲棋子永远不会导致一个棋子被误认为是另一个。如果你需要更多的图像，你可以镜像整个集合，有效地将你的数据翻倍。[存在整个库来镜像、倾斜和扭曲图像](https://oreil.ly/tCMTN)，这样你就可以创建更多数据。
- en: Loading the feature model
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载特征模型
- en: You can load the feature model just like you’d load any model from TensorFlow
    Hub. You can pass the code through the model for prediction, and it will result
    in `numImages` predictions. The code looks like [Example 11-2](#example_load_tfhub_feature).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像加载 TensorFlow Hub 中的任何模型一样加载特征模型。你可以通过模型进行预测，结果将是`numImages`个预测。代码看起来像[示例
    11-2](#example_load_tfhub_feature)。
- en: Example 11-2\. Loading and using the feature vector model
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-2\. 加载和使用特征向量模型
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output of the console log is
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台日志的输出是
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each of the 130 images has become a set of 1,664 floating-point values that
    are sensitive to features of the image. If you change the model to use a different
    depth, the number of features will change. The number 1,664 is unique to the 1.30
    depth version of MobileNet.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个130个图像已经变成了一组1,664个浮点值，这些值对图像的特征敏感。如果你改变模型以使用不同的深度，特征的数量也会改变。1,664这个数字是 MobileNet
    1.30深度版本独有的。
- en: As previously mentioned, the 1,664 `Float32` feature set is significantly smaller
    than the `224*224*3 = 150,528` `Float32` input of each image. This will speed
    up training and be kinder to your computer memory.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，1,664个`Float32`特征集比每个图像的`224*224*3 = 150,528`个`Float32`输入要小得多。这将加快训练速度，并对计算机内存更友好。
- en: Creating your neural network
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建你自己的神经网络
- en: Now that you have a collection of features, you can create a new and utterly
    untrained model that fits those 1,664 features to your labels.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一组特征，你可以创建一个新的完全未经训练的模型，将这1,664个特征与你的标签相匹配。
- en: Example 11-3\. A small 64-layer model with a final layer of 6
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-3\. 一个包含64层的小模型，最后一层是6
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO2-1)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_transfer_learning_CO2-1)'
- en: This Layers model is using a slightly different syntax than you’re used to.
    Rather than calling `.add`, all the layers are being presented in an array of
    the initial configuration. This syntax is nice for a small model like this.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Layers 模型使用了一个与你习惯的略有不同的语法。而不是调用`.add`，所有的层都被呈现在初始配置的数组中。这种语法对于像这样的小模型很好。
- en: '[![2](assets/2.png)](#co_transfer_learning_CO2-2)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_transfer_learning_CO2-2)'
- en: The `inputShape` of the model is set to `1,664` dynamically, in the case that
    you’d like to change the model’s depth multiplier by updating the model URL.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的`inputShape`被动态设置为`1,664`，以防你想通过更新模型 URL 来改变模型的深度乘数。
- en: Training results
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练结果
- en: Nothing is new in the training code. The model trains based on the feature output.
    Because the feature output is so small compared to the original image tensor,
    the training happens extremely quickly.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练代码中没有什么新的。模型基于特征输出进行训练。由于特征输出与原始图像张量相比非常小，训练速度非常快。
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Within a few epochs, the model has outstanding accuracy. Take a look at [Figure 11-6](#hub_transfer_results).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 几个周期后，模型的准确率就会非常出色。查看[图 11-6](#hub_transfer_results)。
- en: '![transfer learning results](assets/ltjs_1106.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![迁移学习结果](assets/ltjs_1106.png)'
- en: Figure 11-6\. From 50% to 96% validation accuracy in 20 epochs
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. 从50%到96%的验证准确率在20个周期内
- en: Transfer learning using an existing model on TensorFlow Hub relieves you of
    architectural headaches and rewards you with high accuracy. But it’s not the only
    way you can implement transfer learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Layers Models for Transfer Learning
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some obvious and not-so-obvious limitations to the previous method.
    First, the feature model cannot be trained. All your training was on a new model
    that consumed the features of the Graph model, but the convolutional layers and
    size were fixed. You have small variations of the convolutional network model
    available but no way to update or fine-tune it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The previous model from TensorFlow Hub was a Graph model. The Graph model was
    optimized for speed and, as you know, cannot be modified or trained. On the other
    side, Layers models are primed for modification, so you can rewire them for transfer
    learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Also, in the previous example, you were essentially dealing with two models
    every time you would need to classify an image. You would have to load two JSON
    models and run your image through the feature model and then the new model to
    categorize your images. It’s not the end of the world, but a single model is possible
    via combining Layers models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Let’s solve the same chess problem again, but with a Layers version of MobileNet
    so we can inspect the difference.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Shaving Layers on MobileNet
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, you will use a version of MobileNet v1.0 that is set up for
    being a Layers model. This is the model Teachable Machine uses, and while it’s
    sufficient for small exploratory projects, you’ll notice it’s not as accurate
    as the MobileNet v2 with 1.30 depth. You’re already well versed in converting
    models with the wizard, as you learned in [Chapter 7](ch07.html#the_chapter_7),
    so you can create a larger, newer Layers model when needed. Accuracy is an important
    metric, but it’s far from the only metric you should evaluate when shopping for
    a transfer model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'MobileNet has a vast collection of layers, and some of these are layers you’ve
    never seen before. Let’s take a look. Load the MobileNet model associated with
    this chapter and review the summary of layers with `model.summary()`. This prints
    a huge list of layers. Don’t feel overwhelmed. When you read from the bottom to
    the top, the last two convolutional layers with activations are called `conv_preds`
    and `conv_pw_13_relu`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The last convolution, `conv_preds`, serves as a `flatten` layer of the features
    to the 1,000 possible classes. This is somewhat specific to the model’s trained
    classes, so because of that, we’ll jump up to the second convolution (`conv_pw_13_relu`)
    and cut there.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet is a complex model, and even though you don’t have to understand all
    the layers to use it for transfer learning, there’s a bit of art in deciding what
    to remove. In simpler models, like the one for the upcoming Chapter Challenge,
    it’s common to keep the entire convolutional workflow and cut at the flatten layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: You can cut to a layer by knowing its unique name. The code shown in [Example 11-4](#printing_shaved_layers)
    is [available on GitHub](https://oreil.ly/KfhNb).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-4\.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The code from [Example 11-4](#printing_shaved_layers) prints out two large models,
    but the key difference is that the second model suddenly stops at `conv_pw_13_relu`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The last layer is now the one we identified. When you review the summary of
    the shaved-down model, it’s like a feature extractor. There is a key difference
    that should be noted. The final layer is a convolution, so the first layer of
    your constructed transfer model should flatten the convolutional input so it can
    be densely connected to a neural network.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Layers Feature Model
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now you can use the shaved model as a features model. This gets you the same
    two-model system you had from TFHub. Your second model will need to read the output
    of `conv_pw_13_relu`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are setting the shape as defined by the intermediate features. This could
    also be directly tied to the shaved model’s output shape (`shavedModel.outputs[0].shape.slice(1)`).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: From here, you’re right back to where you were in the TFHub model. The base
    model creates features, and the second model interprets those features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Training with these two layers achieves around 80%+ accuracy. Keep in mind we’re
    using a completely different model architecture (this is MobileNet v1) and a lower
    depth multiplier. Getting at least 80% from this rough model is good.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: A Unified Model
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as with the feature vector model, your training only has access to a few
    layers and does not update the convolutional layers. Now that you’ve trained two
    models, you can unify their layers again into a single model. You might be wondering
    why you’re combining the model after training instead of before. It’s a common
    practice to train your new layers with your feature layers locked or “frozen”
    to their original weights.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Once the new layers have gotten trained up, you can generally “unfreeze” more
    layers and train the new and the old together. This phase is often called *fine-tuning*
    the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do you unify these two models now? The answer is surprisingly simple.
    Create a third sequential model and add the two models with `model.add`. The code
    looks like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The new `combo` model can be downloaded or trained further.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: If you had joined the models before training the new layers, you’d likely see
    your model overfit the data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: No Training Needed
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s worth noting that there’s a witty way to use two models for transfer learning
    with zero training. The trick is to use a second model that identifies distances
    in similarity.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The second model is called K-Nearest Neighbors (KNN)^([1](ch11.html#idm45049236839224))
    model, and it groups a data element with K of the most similar data elements in
    a feature space. The idiom “birds of a feather flock together” is the premise
    for KNN.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-7](#knn_graph), X would be identified as a bunny because the three
    nearest examples in features are also bunnies.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![feature distance](assets/ltjs_1107.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Figure 11-7\. Identify with neighbors in feature space
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: KNN is sometimes called *instance-based learning* or *lazy learning* because
    you’re moving all the necessary processing to the moment of classification of
    the data around it. This differed model is straightforward to update. You can
    always add more images and classes dynamically to define edge cases or new categories
    without retraining. The cost comes from the fact that the feature graph grows
    with each example you add, unlike the fixed space of a single trained model. The
    more data points you add to a KNN solution, the larger the feature set that accompanies
    the models will become.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, since there is no training, similarity is the *only* metric. This
    makes this system nonideal for some problems. For instance, if you were trying
    to train a model to see if people were wearing face masks or not, then you’re
    looking for a model to focus on a single feature rather than the collection of
    several features. Two people who are dressed the same might share more similarities
    and therefore be placed in the same category with KNN. For KNN to work on masks,
    your feature vector model would have to be face-specific, where trained models
    can learn differentiating patterns.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Easy KNN: Bunnies Versus Sports Cars'
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KNN, like MobileNet, has a JS wrapper provided by Google. We can implement KNN
    transfer learning quickly by hiding all the complexity details use MobileNet and
    KNN NPM packages to make a quick transfer learning demo.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Not only are we going to avoid running any training, but we’ll also use existing
    libraries to avoid any deep dive into TensorFlow.js. We’ll be doing this for a
    flashy demo, but if you decide to build something more robust with these models,
    you should probably evaluate avoiding abstract packages that you don’t control.
    You already understand all the inner workings of transfer learning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this quick demo, you’ll import the three NPM modules:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For simplicity, the example code from this chapter has all the images on the
    page, so you can directly reference them. Now you can load MobileNet with `mobileNet
    = await mobilenet.load();` and the KNN classifier with `knnClassifier.create();`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'The KNN classifier needs examples of each class. To simplify this process I’ve
    created the following helper function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO3-1)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The `infer` method returns values rather than the rich JavaScript object of
    detections.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_transfer_learning_CO3-2)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: The image `id` on the page will tell MobileNet what image to resize and process.
    The tensor logic is hidden by JavaScript, but many chapters in this book have
    explained what is actually happening.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_transfer_learning_CO3-3)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The MobileNet model returns the features (sometimes called *embeddings*) of
    the image. If this is not set, then the tensor of 1,000 raw values is returned
    (sometimes called *logits*).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can add examples of each class with this helper method. You just name
    the image element’s unique DOM ID and what class it should be associated with.
    Adding three examples of each is as simple as this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Lastly, it’s the same system to predict. Get the features of an image, and ask
    the classifier to identify which class it believes the input is based on KNN.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_transfer_learning_CO4-1)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The `classIndex` is the number as passed in `addExample`. If a third class is
    added, that new index would be a possible output.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_transfer_learning_CO4-2)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The web page text is changed from “???” to the result.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The result is that the AI can identify the correct class for a new image by
    comparing against six examples, as shown in [Figure 11-8](#bunnyvscars).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![screenshot of AI page](assets/ltjs_1108.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 11-8\. With only three images of each class, the KNN model predicts correctly
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can dynamically add more and more classes. KNN is an exciting and expandable
    way to utilize the experience of advanced models through transfer learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Review
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because this chapter has explained the mystery of transfer learning with MobileNet,
    you now have the ability to apply this power-up to any preexisting model you can
    somewhat comprehend. Perhaps you want to adjust the pet’s faces model to find
    cartoon or human faces. You don’t have to start from scratch!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning adds a new utility to your toolbelt of AI. Now when you find
    a new model in the wild, you can ask yourself how you could use it directly *and*
    how you can use it in transfer learning for something similar.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter Challenge: Warp-Speed Learning'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Hogwarts sorting model from the previous chapter has thousands of black-and-white
    drawing images of experience in the convolutional layers. Unfortunately, those
    thousands of images were limited to animals and skulls. They all have nothing
    to do with *Star Trek*. Don’t fret; with only 50 or so new images, you can re-train
    the model from the previous chapter to identify the three *Star Trek* symbols
    shown in [Figure 11-9](#transfer_trek_logos).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Perfect validation accuracy in a few epochs](assets/ltjs_1109.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 11-9\. Star Trek symbols
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Set phasers to fun and use the methods you learned in this chapter to take the
    Layers model you trained in [Chapter 10](ch10.html#the_chapter_10) (or download
    the trained one from the associated [book source code](https://oreil.ly/v3tvg)),
    and train a new model that can identify these images from a mere few examples.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The new training image data can be found in CSV form in [the associated book
    source code](https://oreil.ly/3dqcq). The training image data has been put in
    a CSV so you can easily import it with Danfo.js. The files are *images.csv* and
    *labels.csv*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 新的训练图像数据可以在[相关书籍源代码](https://oreil.ly/3dqcq)中以CSV形式找到。训练图像数据已经放在CSV中，因此您可以使用Danfo.js轻松导入它。文件是*images.csv*和*labels.csv*。
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[附录B](app02.html#appendix_b)中找到这个挑战的答案。
- en: Review Questions
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复习问题
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下您在本章编写的代码中学到的教训。花点时间回答以下问题：
- en: What does KNN stand for?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KNN代表什么？
- en: Whenever you have a small training set, there’s a danger of what?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当您有一个小的训练集时，存在什么危险？
- en: When you’re looking for the convolutional half of a CNN model on TensorFlow
    Hub, what tag are you looking for?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在TensorFlow Hub上寻找CNN模型的卷积部分时，您要寻找哪个标签？
- en: Which depth multiplier will have a more extensive feature output, 0.50 or 1.00?
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个深度乘数会产生更广泛的特征输出，0.50还是1.00？
- en: What method can you call on the MobileNet NPM module to gather the feature embeddings
    of an image?
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以调用MobileNet NPM模块的哪种方法来收集图像的特征嵌入？
- en: Should you combine your transfer model parts and then train, or train and then
    combine your models?
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该先组合您的转移模型部分然后训练，还是先训练然后组合您的模型？
- en: When you cut a model at the convolutional layer, what do you have to do before
    importing that information to a neural network’s dense layers?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在卷积层上切割模型时，在将该信息导入神经网络的密集层之前，您需要做什么？
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在[附录A](app01.html#book_appendix)中找到。
- en: ^([1](ch11.html#idm45049236839224-marker)) KNN was developed by Evelyn Fix and
    Joseph Hodges in 1951.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.html#idm45049236839224-marker)) KNN是由Evelyn Fix和Joseph Hodges于1951年开发的。
