- en: Chapter 11\. Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Learn from the mistakes of others. You can’t live long enough to make them
    all yourself.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Eleanor Roosevelt
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It can be challenging to have an extensive collection of data, battle-tested
    model structure, and processing power. Wouldn’t it be nice to cut a corner? That
    nifty trick in [Chapter 7](ch07.html#the_chapter_7) where you could use Teachable
    Machine to transfer the qualities of a trained model to a novel one was pretty
    useful. In fact, this is a common trick in the machine learning world. While Teachable
    Machine hid the specifics and offered you only a single model, you can understand
    the mechanics of this trick and use it on all kinds of cool tasks. In this chapter,
    we will reveal the magic behind this process. While we’ll be focused on the example
    of MobileNet for simplicity, this can be applied to all kinds of models.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is the act of taking a trained model and repurposing it for
    a second related task.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few repeatable benefits to using transfer learning for your machine
    learning solution. Most projects utilize some amount of transfer learning for
    these reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Reutilizing a battle-tested model structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting a solution faster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting a solution via less data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn several strategies for transfer learning. You
    will focus on MobileNet as a fundamental example that can be reused to identify
    a myriad of new classes in various ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Review how transfer learning works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See how to reuse feature vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cut into Layers models and reconstruct new models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn about KNN and deferred classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you finish this chapter, you’ll be able to take models that have been trained
    for a long time with lots of data and apply them to your own needs with smaller
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Transfer Learning Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does a model that has been trained on different data suddenly work well
    for your *new* data? It sounds miraculous, but it happens in humans every day.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve spent years identifying animals, and you’ve probably seen hundreds of
    camels, guinea pigs, and beavers from cartoons, zoos, and commercials. Now I’m
    going to show you an animal you’ve probably not seen often, or even at all. The
    animal in [Figure 11-1](#capybara) is called a capybara (*Hydrochoerus hydrochaeris*).
  prefs: []
  type: TYPE_NORMAL
- en: '![profile of a capybara](assets/ltjs_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. The capybara
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For some of you, this is the first time (or one of the few times) you’ve seen
    a photo of a capybara. Now, take a look at the lineup in [Figure 11-2](#transfer_quiz).
    Can you find the capybara?
  prefs: []
  type: TYPE_NORMAL
- en: '![three mammals quiz](assets/ltjs_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Which one is the capybara?
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The training set of a single photo was enough for you to make a choice because
    you’ve been distinguishing between animals your entire life. With a novel color,
    angle, and photo size, your brain probably detected with absolute certainty that
    animal C was another capybara. The features learned by your years of experience
    have helped you make an educated decision. In that same way, powerful models that
    have significant experience can be taught to learn new things from small amounts
    of new data.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s bring things back to MobileNet for a moment. The MobileNet model was trained
    to identify features that distinguish a thousand items from each other. That means
    there are convolutions to detect fur, metal, round things, ears, and all kinds
    of crucial differential features. All these features are chewed up and simplified
    before they are flattened into a neural network, where the combination of various
    features creates a classification.
  prefs: []
  type: TYPE_NORMAL
- en: The MobileNet model can identify different breeds of dogs, and even distinguish
    a Maltese terrier from a Tibetan terrier. If you were to make a [“dog or cat”](https://oreil.ly/i9Xxm)
    classifier, it makes sense that a majority of those advanced features would be
    reusable in your simpler model.
  prefs: []
  type: TYPE_NORMAL
- en: The previously learned convolutional filters would be extremely useful in identifying
    key features for brand-new classifications, like our capybara example in [Figure 11-2](#transfer_quiz).
    The trick is to take the feature identification portion of the model and apply
    your own neural network to the convolutional output, as illustrated in [Figure 11-3](#transfer_visual).
  prefs: []
  type: TYPE_NORMAL
- en: '![changing the NN flowchart](assets/ltjs_1103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-3\. CNN transfer learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So how do you separate and recombine these sections of previously trained models?
    You’ve got lots of options. Again, we’ll learn a bit more about Graph and Layers
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Easy MobileNet Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, [TensorFlow Hub](https://tfhub.dev) already has a MobileNet model
    that is disconnected from any neural network. It offers half a model for you to
    use for transfer learning. Half a model means it hasn’t been tied down into a
    final softmax layer for what it’s meant to classify. This allows us to let MobileNet
    derive the features of an image and then provide us with tensors that we can then
    pass to our own trained network for classification.
  prefs: []
  type: TYPE_NORMAL
- en: TFHub calls these models *image feature vector* models. [You refine your search
    to show only these models](https://oreil.ly/BkokR) or identify them by looking
    at the problem domain tags, as illustrated in [Figure 11-4](#image_feature_vector).
  prefs: []
  type: TYPE_NORMAL
- en: '![screenshot of proper tags](assets/ltjs_1104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-4\. Problem domain tags for image feature vectors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You might notice small variations of MobileNet and wonder what the differences
    are. Once you learn a few sneaky terms, each of these model descriptions becomes
    quite readable.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we’ll use [Example 11-1](#example_mobile_net).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-1\. One of the image feature vector models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: imagenet
  prefs: []
  type: TYPE_NORMAL
- en: This model was trained on the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: mobilenet_v2
  prefs: []
  type: TYPE_NORMAL
- en: The model’s architecture is MobileNet v2.
  prefs: []
  type: TYPE_NORMAL
- en: '130'
  prefs: []
  type: TYPE_NORMAL
- en: The model’s depth multiplier was 1.30\. This results in more features. If you
    want to speed things up, you can choose “05,” which would have less than half
    the feature output with a boost in speed. This is a fine-tuning option when you’re
    ready to modify speed versus depth.
  prefs: []
  type: TYPE_NORMAL
- en: '224'
  prefs: []
  type: TYPE_NORMAL
- en: The model’s expected input size is 224 x 224 images.
  prefs: []
  type: TYPE_NORMAL
- en: feature_vector
  prefs: []
  type: TYPE_NORMAL
- en: We already know from the tag, but this model outputs tensors meant to be features
    of the image for a second model to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a trained model that can identify features in an image, we
    will run our training data through the MobileNet image feature vector model and
    then train a model on the output from that. In other words, the training images
    will turn into a feature vector, and we’ll train a model to interpret that feature
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this strategy is that it’s straightforward to implement. The
    major drawback is that you’ll have to load two models when you’re ready to use
    the newly trained model (one to generate features and one to interpret them).
    Creatively, there might be some cases where it’s quite useful to “featurize” an
    image and then run that through multiple neural networks. Regardless, let’s see
    it in action.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub Check, Mate!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to use transfer learning with MobileNet to identify chess pieces
    like the one shown in [Figure 11-5](#chess_promo).
  prefs: []
  type: TYPE_NORMAL
- en: '![image of a chess knight on a table](assets/ltjs_1105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-5\. Simple chess pieces classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll only have a few images of each chess piece. That’s not normally enough,
    but with the magic of transfer learning, you’ll get an efficient model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading chess images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this exercise, I’ve compiled a collection of 150 images and loaded them
    into a CSV file for quick use. This isn’t something I’d recommend doing in most
    cases because it’s inefficient for processing and disk space, but it serves as
    a simple vector for some quick in-browser training. The code to load these images
    is now trivial.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can access the chess images and the code that converted them into a CSV
    file in the [*chapter11/extra/node-make-csvs*](https://oreil.ly/INWAN) folder.
  prefs: []
  type: TYPE_NORMAL
- en: The files *chess_labels.csv* and *chess_images.csv* can be found in the [*chess_data.zip*](https://oreil.ly/bcFop)
    file in code associated with this lesson. Unzip this file and use Danfo.js to
    load the contents.
  prefs: []
  type: TYPE_NORMAL
- en: Many browsers may have issues with concurrently reading all 150 images, so I’ve
    limited the demo to process only 130 images. Working against concurrent data limitations
    is a common issue with machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the image has been featurized, it takes up a lot less space. Feel free
    to experiment with creating features in batches, but that’s outside the scope
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The images are already 224 x 224, so you can load them with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_transfer_learning_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter to `read_csv` limits the row count to the specified number.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_transfer_learning_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrames are then converted to tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_transfer_learning_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The images were flattened to become serialized but are now reshaped into a rank-four
    batch of RGB images.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a bit of time, this code prints out the X and Y shapes of 130 ready-to-go
    images and encodings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If your computer is unable to handle the 130 images, you can lower the `numImages`
    variable and still play along. However, the load time for the CSV file is always
    constant because the entire file must be processed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Images like chess pieces are perfect for image augmentation because skewing
    chess pieces would never cause one piece to be confused with another. If you ever
    need more images, you can mirror the entire set to effectively double your data.
    [Entire libraries exist to mirror, tilt, and skew images](https://oreil.ly/tCMTN)
    so you can create more data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the feature model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can load the feature model just like you’d load any model from TensorFlow
    Hub. You can pass the code through the model for prediction, and it will result
    in `numImages` predictions. The code looks like [Example 11-2](#example_load_tfhub_feature).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-2\. Loading and using the feature vector model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output of the console log is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each of the 130 images has become a set of 1,664 floating-point values that
    are sensitive to features of the image. If you change the model to use a different
    depth, the number of features will change. The number 1,664 is unique to the 1.30
    depth version of MobileNet.
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, the 1,664 `Float32` feature set is significantly smaller
    than the `224*224*3 = 150,528` `Float32` input of each image. This will speed
    up training and be kinder to your computer memory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have a collection of features, you can create a new and utterly
    untrained model that fits those 1,664 features to your labels.
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-3\. A small 64-layer model with a final layer of 6
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_transfer_learning_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This Layers model is using a slightly different syntax than you’re used to.
    Rather than calling `.add`, all the layers are being presented in an array of
    the initial configuration. This syntax is nice for a small model like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_transfer_learning_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `inputShape` of the model is set to `1,664` dynamically, in the case that
    you’d like to change the model’s depth multiplier by updating the model URL.
  prefs: []
  type: TYPE_NORMAL
- en: Training results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nothing is new in the training code. The model trains based on the feature output.
    Because the feature output is so small compared to the original image tensor,
    the training happens extremely quickly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Within a few epochs, the model has outstanding accuracy. Take a look at [Figure 11-6](#hub_transfer_results).
  prefs: []
  type: TYPE_NORMAL
- en: '![transfer learning results](assets/ltjs_1106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-6\. From 50% to 96% validation accuracy in 20 epochs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Transfer learning using an existing model on TensorFlow Hub relieves you of
    architectural headaches and rewards you with high accuracy. But it’s not the only
    way you can implement transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Layers Models for Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some obvious and not-so-obvious limitations to the previous method.
    First, the feature model cannot be trained. All your training was on a new model
    that consumed the features of the Graph model, but the convolutional layers and
    size were fixed. You have small variations of the convolutional network model
    available but no way to update or fine-tune it.
  prefs: []
  type: TYPE_NORMAL
- en: The previous model from TensorFlow Hub was a Graph model. The Graph model was
    optimized for speed and, as you know, cannot be modified or trained. On the other
    side, Layers models are primed for modification, so you can rewire them for transfer
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in the previous example, you were essentially dealing with two models
    every time you would need to classify an image. You would have to load two JSON
    models and run your image through the feature model and then the new model to
    categorize your images. It’s not the end of the world, but a single model is possible
    via combining Layers models.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s solve the same chess problem again, but with a Layers version of MobileNet
    so we can inspect the difference.
  prefs: []
  type: TYPE_NORMAL
- en: Shaving Layers on MobileNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, you will use a version of MobileNet v1.0 that is set up for
    being a Layers model. This is the model Teachable Machine uses, and while it’s
    sufficient for small exploratory projects, you’ll notice it’s not as accurate
    as the MobileNet v2 with 1.30 depth. You’re already well versed in converting
    models with the wizard, as you learned in [Chapter 7](ch07.html#the_chapter_7),
    so you can create a larger, newer Layers model when needed. Accuracy is an important
    metric, but it’s far from the only metric you should evaluate when shopping for
    a transfer model.
  prefs: []
  type: TYPE_NORMAL
- en: 'MobileNet has a vast collection of layers, and some of these are layers you’ve
    never seen before. Let’s take a look. Load the MobileNet model associated with
    this chapter and review the summary of layers with `model.summary()`. This prints
    a huge list of layers. Don’t feel overwhelmed. When you read from the bottom to
    the top, the last two convolutional layers with activations are called `conv_preds`
    and `conv_pw_13_relu`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The last convolution, `conv_preds`, serves as a `flatten` layer of the features
    to the 1,000 possible classes. This is somewhat specific to the model’s trained
    classes, so because of that, we’ll jump up to the second convolution (`conv_pw_13_relu`)
    and cut there.
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet is a complex model, and even though you don’t have to understand all
    the layers to use it for transfer learning, there’s a bit of art in deciding what
    to remove. In simpler models, like the one for the upcoming Chapter Challenge,
    it’s common to keep the entire convolutional workflow and cut at the flatten layer.
  prefs: []
  type: TYPE_NORMAL
- en: You can cut to a layer by knowing its unique name. The code shown in [Example 11-4](#printing_shaved_layers)
    is [available on GitHub](https://oreil.ly/KfhNb).
  prefs: []
  type: TYPE_NORMAL
- en: Example 11-4\.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The code from [Example 11-4](#printing_shaved_layers) prints out two large models,
    but the key difference is that the second model suddenly stops at `conv_pw_13_relu`.
  prefs: []
  type: TYPE_NORMAL
- en: The last layer is now the one we identified. When you review the summary of
    the shaved-down model, it’s like a feature extractor. There is a key difference
    that should be noted. The final layer is a convolution, so the first layer of
    your constructed transfer model should flatten the convolutional input so it can
    be densely connected to a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Layers Feature Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now you can use the shaved model as a features model. This gets you the same
    two-model system you had from TFHub. Your second model will need to read the output
    of `conv_pw_13_relu`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We are setting the shape as defined by the intermediate features. This could
    also be directly tied to the shaved model’s output shape (`shavedModel.outputs[0].shape.slice(1)`).
  prefs: []
  type: TYPE_NORMAL
- en: From here, you’re right back to where you were in the TFHub model. The base
    model creates features, and the second model interprets those features.
  prefs: []
  type: TYPE_NORMAL
- en: Training with these two layers achieves around 80%+ accuracy. Keep in mind we’re
    using a completely different model architecture (this is MobileNet v1) and a lower
    depth multiplier. Getting at least 80% from this rough model is good.
  prefs: []
  type: TYPE_NORMAL
- en: A Unified Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as with the feature vector model, your training only has access to a few
    layers and does not update the convolutional layers. Now that you’ve trained two
    models, you can unify their layers again into a single model. You might be wondering
    why you’re combining the model after training instead of before. It’s a common
    practice to train your new layers with your feature layers locked or “frozen”
    to their original weights.
  prefs: []
  type: TYPE_NORMAL
- en: Once the new layers have gotten trained up, you can generally “unfreeze” more
    layers and train the new and the old together. This phase is often called *fine-tuning*
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do you unify these two models now? The answer is surprisingly simple.
    Create a third sequential model and add the two models with `model.add`. The code
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The new `combo` model can be downloaded or trained further.
  prefs: []
  type: TYPE_NORMAL
- en: If you had joined the models before training the new layers, you’d likely see
    your model overfit the data.
  prefs: []
  type: TYPE_NORMAL
- en: No Training Needed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s worth noting that there’s a witty way to use two models for transfer learning
    with zero training. The trick is to use a second model that identifies distances
    in similarity.
  prefs: []
  type: TYPE_NORMAL
- en: The second model is called K-Nearest Neighbors (KNN)^([1](ch11.html#idm45049236839224))
    model, and it groups a data element with K of the most similar data elements in
    a feature space. The idiom “birds of a feather flock together” is the premise
    for KNN.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 11-7](#knn_graph), X would be identified as a bunny because the three
    nearest examples in features are also bunnies.
  prefs: []
  type: TYPE_NORMAL
- en: '![feature distance](assets/ltjs_1107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-7\. Identify with neighbors in feature space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: KNN is sometimes called *instance-based learning* or *lazy learning* because
    you’re moving all the necessary processing to the moment of classification of
    the data around it. This differed model is straightforward to update. You can
    always add more images and classes dynamically to define edge cases or new categories
    without retraining. The cost comes from the fact that the feature graph grows
    with each example you add, unlike the fixed space of a single trained model. The
    more data points you add to a KNN solution, the larger the feature set that accompanies
    the models will become.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, since there is no training, similarity is the *only* metric. This
    makes this system nonideal for some problems. For instance, if you were trying
    to train a model to see if people were wearing face masks or not, then you’re
    looking for a model to focus on a single feature rather than the collection of
    several features. Two people who are dressed the same might share more similarities
    and therefore be placed in the same category with KNN. For KNN to work on masks,
    your feature vector model would have to be face-specific, where trained models
    can learn differentiating patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Easy KNN: Bunnies Versus Sports Cars'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KNN, like MobileNet, has a JS wrapper provided by Google. We can implement KNN
    transfer learning quickly by hiding all the complexity details use MobileNet and
    KNN NPM packages to make a quick transfer learning demo.
  prefs: []
  type: TYPE_NORMAL
- en: Not only are we going to avoid running any training, but we’ll also use existing
    libraries to avoid any deep dive into TensorFlow.js. We’ll be doing this for a
    flashy demo, but if you decide to build something more robust with these models,
    you should probably evaluate avoiding abstract packages that you don’t control.
    You already understand all the inner workings of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this quick demo, you’ll import the three NPM modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For simplicity, the example code from this chapter has all the images on the
    page, so you can directly reference them. Now you can load MobileNet with `mobileNet
    = await mobilenet.load();` and the KNN classifier with `knnClassifier.create();`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The KNN classifier needs examples of each class. To simplify this process I’ve
    created the following helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_transfer_learning_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `infer` method returns values rather than the rich JavaScript object of
    detections.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_transfer_learning_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The image `id` on the page will tell MobileNet what image to resize and process.
    The tensor logic is hidden by JavaScript, but many chapters in this book have
    explained what is actually happening.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_transfer_learning_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The MobileNet model returns the features (sometimes called *embeddings*) of
    the image. If this is not set, then the tensor of 1,000 raw values is returned
    (sometimes called *logits*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can add examples of each class with this helper method. You just name
    the image element’s unique DOM ID and what class it should be associated with.
    Adding three examples of each is as simple as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, it’s the same system to predict. Get the features of an image, and ask
    the classifier to identify which class it believes the input is based on KNN.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_transfer_learning_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `classIndex` is the number as passed in `addExample`. If a third class is
    added, that new index would be a possible output.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_transfer_learning_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The web page text is changed from “???” to the result.
  prefs: []
  type: TYPE_NORMAL
- en: The result is that the AI can identify the correct class for a new image by
    comparing against six examples, as shown in [Figure 11-8](#bunnyvscars).
  prefs: []
  type: TYPE_NORMAL
- en: '![screenshot of AI page](assets/ltjs_1108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-8\. With only three images of each class, the KNN model predicts correctly
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can dynamically add more and more classes. KNN is an exciting and expandable
    way to utilize the experience of advanced models through transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because this chapter has explained the mystery of transfer learning with MobileNet,
    you now have the ability to apply this power-up to any preexisting model you can
    somewhat comprehend. Perhaps you want to adjust the pet’s faces model to find
    cartoon or human faces. You don’t have to start from scratch!
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning adds a new utility to your toolbelt of AI. Now when you find
    a new model in the wild, you can ask yourself how you could use it directly *and*
    how you can use it in transfer learning for something similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter Challenge: Warp-Speed Learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Hogwarts sorting model from the previous chapter has thousands of black-and-white
    drawing images of experience in the convolutional layers. Unfortunately, those
    thousands of images were limited to animals and skulls. They all have nothing
    to do with *Star Trek*. Don’t fret; with only 50 or so new images, you can re-train
    the model from the previous chapter to identify the three *Star Trek* symbols
    shown in [Figure 11-9](#transfer_trek_logos).
  prefs: []
  type: TYPE_NORMAL
- en: '![Perfect validation accuracy in a few epochs](assets/ltjs_1109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-9\. Star Trek symbols
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Set phasers to fun and use the methods you learned in this chapter to take the
    Layers model you trained in [Chapter 10](ch10.html#the_chapter_10) (or download
    the trained one from the associated [book source code](https://oreil.ly/v3tvg)),
    and train a new model that can identify these images from a mere few examples.
  prefs: []
  type: TYPE_NORMAL
- en: The new training image data can be found in CSV form in [the associated book
    source code](https://oreil.ly/3dqcq). The training image data has been put in
    a CSV so you can easily import it with Danfo.js. The files are *images.csv* and
    *labels.csv*.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the answer to this challenge in [Appendix B](app02.html#appendix_b).
  prefs: []
  type: TYPE_NORMAL
- en: Review Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the lessons you’ve learned from the code you’ve written in this
    chapter. Take a moment to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What does KNN stand for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whenever you have a small training set, there’s a danger of what?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you’re looking for the convolutional half of a CNN model on TensorFlow
    Hub, what tag are you looking for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which depth multiplier will have a more extensive feature output, 0.50 or 1.00?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What method can you call on the MobileNet NPM module to gather the feature embeddings
    of an image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Should you combine your transfer model parts and then train, or train and then
    combine your models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you cut a model at the convolutional layer, what do you have to do before
    importing that information to a neural network’s dense layers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available in [Appendix A](app01.html#book_appendix).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11.html#idm45049236839224-marker)) KNN was developed by Evelyn Fix and
    Joseph Hodges in 1951.
  prefs: []
  type: TYPE_NORMAL
