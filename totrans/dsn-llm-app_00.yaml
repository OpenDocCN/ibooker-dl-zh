- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: In the past few years, progress in the field of artificial intelligence has
    been occurring at breakneck speeds, spearheaded by advances in LLMs. It was not
    too long ago that LLMs were a nascent technology that struggled to generate a
    coherent paragraph; today they are able to solve complex mathematical problems,
    write convincing essays, and conduct long engaging conversations with humans.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，人工智能领域的进步速度之快，主要是由LLMs的进步推动的。LLMs曾经是一种新兴技术，难以生成连贯的段落；而如今，它们能够解决复杂的数学问题，撰写令人信服的论文，并与人类进行长时间的深入对话。
- en: As AI advances from strength to strength, it is rapidly being woven into the
    fabric of society, touching so many facets of our lives. Learning how to use AI
    models like LLMs effectively might be one of the most useful skills to learn this
    decade. LLMs are revolutionizing the world of software, and have made possible
    the development of applications previously considered impossible.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能从强到强，它正迅速融入社会的织锦中，触及我们生活的许多方面。学习如何有效地使用LLMs等AI模型可能是这个十年中最有用的技能之一。LLMs正在改变软件世界，使得以前被认为不可能的应用开发成为可能。
- en: With all the promise that LLMs bring, the reality is that they are still not
    a mature technology and have many limitations like deficiencies in reasoning,
    lack of adherence to factuality, “hallucinations”, difficulties in steering them
    toward our goals, bias and fairness issues, and so on. Despite the existence of
    these limitations, we can still harness LLMs for good use and build a variety
    of helpful applications provided we effectively address their shortcomings.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs带来了如此多的希望，但现实是它们仍然不是一个成熟的技术，存在许多限制，如推理不足、缺乏事实性、“幻觉”、难以引导它们实现我们的目标、偏见和公平性问题等。尽管存在这些限制，只要我们有效地解决它们的不足，我们仍然可以利用LLMs进行有益的应用，并构建各种有用的应用。
- en: Plenty of software frameworks have emerged that enable rapid prototype development
    of LLM applications. However, advancing from prototypes to production-grade applications
    is a road much less traveled, and is still a very challenging task. This is where
    this book comes in—a holistic overview of the LLM landscape that provides you
    with the intuition and tools to build complex LLM applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 已经出现了许多软件框架，它们能够快速原型开发LLM应用。然而，从原型发展到生产级应用的道路却鲜有人走，并且仍然是一项极具挑战性的任务。这正是本书的作用所在——提供一个对LLM领域的全面概述，为你提供构建复杂LLM应用的直觉和工具。
- en: With this book, my goal is to provide you with an intuitive understanding of
    how LLMs work, the tools you have at your disposal to harness them, and the various
    application paradigms they can be built with. Unique to this book are the exercises;
    more than 80 exercises are sprinkled throughout to help you solidify your intuitions
    and sharpen your understanding of what is happening underneath the hood. While
    preparing the content of the book, I read over 800 research papers, with many
    of them referenced and linked at appropriate locations in the book, providing
    you with a jumping off point for further exploration. All in all, I am confident
    that you will come out of the book an LLM expert if you read the book in its entirety,
    complete all the exercises, and explore the recommended references.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这本书，我的目标是帮助你直观地理解LLMs的工作原理，你可用于利用它们的工具，以及它们可以构建的各种应用范式。本书的独特之处在于练习；超过80个练习散布全书，帮助你巩固直觉并加深对底层发生事情的理解。在准备本书内容的过程中，我阅读了800多篇研究论文，其中许多在书中适当位置引用并链接，为你提供了进一步探索的起点。总的来说，如果你完整地阅读本书，完成所有练习，并探索推荐的参考文献，我坚信你将成为LLM领域的专家。
- en: Who This Book Is For
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书面向对象
- en: This book is intended for a broad audience, including software engineers transitioning
    to AI application development, machine learning practitioners and scientists,
    and product managers. Much of the content in this book is borne from my own experiments
    with LLMs, so even if you are an experienced scientist, I expect you will find
    value in it. Similarly, even if you have very limited exposure to the world of
    AI, I expect you will still find the book useful for understanding the fundamentals
    of this technology.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在面向广泛的读者群体，包括正在转向人工智能应用开发的软件工程师、机器学习实践者和科学家，以及产品经理。本书的大部分内容源于我对大型语言模型（LLMs）的实验，因此即使你是经验丰富的科学家，我也期待你能在其中找到价值。同样，即使你对人工智能领域接触非常有限，我也期待你仍然能从这本书中理解到这项技术的根本。
- en: The only prerequisites for this book are knowledge of Python coding and an understanding
    of basic machine learning and deep learning principles. Where required, I provide
    links to external resources that you can use to sharpen or develop your prerequisites.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: How This Book Is Structured
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The book is divided into 3 parts with a total of 13 chapters. The first part
    deals with understanding the ingredients of a language model. I strongly feel
    that even though you may never train a language model from scratch yourself, knowing
    what goes into making it is crucial. The second part discusses various ways to
    harness language models, be it by directly prompting the model, or by fine-tuning
    it in various ways. It also addresses limitations such as hallucinations and reasoning
    constraints, along with methods to mitigate these issues. Finally, the third part
    of the book deals with application paradigms like retrieval augmented generation
    (RAG) and agents, positioning LLMs within the broader context of an entire software
    system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: For an extended table of contents, see my [Substack blog post](https://oreil.ly/-2zkH).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: What This Book Is Not About
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To keep the book at a reasonable length, certain topics were deemed out of scope.
    I have taken care to not cover topics that I am not confident will stand the test
    of time. This field is very fast moving, so writing a book that maintains its
    relevance over time is extremely challenging.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: This book focuses only on English-language LLMs and leaves out discussion on
    multilingual models for the most part. I also disagree with the notion of mushing
    all the non-English languages of the world under the “multilingual” banner. Every
    language has its own nuances and deserves its own book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: This book also doesn’t cover multimodal models. New models are increasingly
    multimodal, i.e., a single model supports multiple modalities like text, image,
    video, speech, etc. However, text remains the most important modality and is the
    binding substrate in these models. Thus, reading this book will still help you
    prepare for the multimodal future.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: This book does not focus on theory or go too deep into math. There are plenty
    of other books that cover that, and I have generously linked to them where needed.
    This book contains minimal math equations and instead focuses on building intuitions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: This book contains only a rudimentary introduction to reasoning models, the
    latest LLM paradigm. At the time of the book’s writing, reasoning models are still
    in their infancy, and the jury is still out on which techniques will prove to
    be most effective.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: How to Read the Book
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The best way to consume this book is to read it sequentially, while working
    on the exercises and exploring the reference links. That said, there are a few
    alternative paths, depending on your interests:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: If your interest lies in understanding the LLM landscape and not necessarily
    in building applications with them, you can focus on Chapters [1](ch01.html#chapter_llm-introduction),
    [2](ch02.html#ch02), [3](ch03.html#chapter-LLM-tokenization), [4](ch04.html#chapter_transformer-architecture),
    [5](ch05.html#chapter_utilizing_llms), [10](ch10.html#ch10), and [11](ch11.html#chapter_llm_interfaces).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对了解LLM领域感兴趣，而不一定在于构建应用程序，那么你可以专注于第[1](ch01.html#chapter_llm-introduction)、[2](ch02.html#ch02)、[3](ch03.html#chapter-LLM-tokenization)、[4](ch04.html#chapter_transformer-architecture)、[5](ch05.html#chapter_utilizing_llms)、[10](ch10.html#ch10)和[11](ch11.html#chapter_llm_interfaces)章。
- en: If you are a product manager seeking to understand the scope of possibilities
    for LLM applications, Chapters [1](ch01.html#chapter_llm-introduction), [2](ch02.html#ch02),
    [3](ch03.html#chapter-LLM-tokenization), [5](ch05.html#chapter_utilizing_llms),
    [8](ch08.html#ch8), [10](ch10.html#ch10), [11](ch11.html#chapter_llm_interfaces),
    [12](ch12.html#ch12), and [13](ch13.html#ch13) are a good bet.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是一名寻求了解LLM应用可能性范围的产品经理，那么第[1](ch01.html#chapter_llm-introduction)、[2](ch02.html#ch02)、[3](ch03.html#chapter-LLM-tokenization)、[5](ch05.html#chapter_utilizing_llms)、[8](ch08.html#ch8)、[10](ch10.html#ch10)、[11](ch11.html#chapter_llm_interfaces)、[12](ch12.html#ch12)和[13](ch13.html#ch13)章是一个不错的选择。
- en: If you are an ML scientist, then Chapters [7](ch07.html#ch07), [8](ch08.html#ch8),
    [9](ch09.html#ch09), [10](ch10.html#ch10), [11](ch11.html#chapter_llm_interfaces),
    and [12](ch12.html#ch12) will be sure to give you food-for-thought and new research
    challenges.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是一名机器学习科学家，那么第[7](ch07.html#ch07)、[8](ch08.html#ch8)、[9](ch09.html#ch09)、[10](ch10.html#ch10)、[11](ch11.html#chapter_llm_interfaces)、[12](ch12.html#ch12)章一定会给你带来思考和新研究挑战。
- en: If you want to train your own LLM from scratch, Chapters [2](ch02.html#ch02),
    [3](ch03.html#chapter-LLM-tokenization), [4](ch04.html#chapter_transformer-architecture),
    [5](ch05.html#chapter_utilizing_llms), and [7](ch07.html#ch07) will provide you
    with the foundational principles.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想要从头开始训练自己的LLM，第[2](ch02.html#ch02)、[3](ch03.html#chapter-LLM-tokenization)、[4](ch04.html#chapter_transformer-architecture)、[5](ch05.html#chapter_utilizing_llms)、[7](ch07.html#ch07)章将为你提供基础原理。
- en: Conventions Used in This Book
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: '*Italic*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 指示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`等宽字体`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序列表，以及段落中引用程序元素，如变量或函数名、数据库、数据类型、环境变量、语句和关键字。
- en: '**`Constant width bold`**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**`等宽粗体`**'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 显示用户应直接输入的命令或其他文本。
- en: '*`Constant width italic`*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*`等宽斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应替换为用户提供的值或由上下文确定的值的文本。
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: This element signifies a tip or suggestion.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般性说明。
- en: Warning
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This element indicates a warning or caution.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示警告或注意事项。
- en: Using Code Examples
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://oreil.ly/llm-playbooks*](https://oreil.ly/llm-playbooks).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料（代码示例、练习等）可在[*https://oreil.ly/llm-playbooks*](https://oreil.ly/llm-playbooks)下载。
- en: If you have a technical question or a problem using the code examples, please
    send email to [*support@oreilly.com*](mailto:support@oreilly.com).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对技术问题或使用代码示例时遇到的问题有疑问，请发送电子邮件至[*support@oreilly.com*](mailto:support@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在帮助你完成工作。一般来说，如果本书提供了示例代码，你可以在你的程序和文档中使用它。除非你正在复制代码的很大一部分，否则你不需要联系我们获取许可。例如，编写一个使用本书中几个代码片段的程序不需要许可。通过引用本书并引用示例代码来回答问题不需要许可。将本书的大量示例代码纳入你产品的文档中则需要许可。
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*Designing Large
    Language Model Applications* by Suhas Pai (O’Reilly). Copyright 2025 Suhas Pai,
    978-1-098-15050-1.”'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢，但通常不需要署名。署名通常包括标题、作者、出版社和ISBN。例如：“*《设计大型语言模型应用》* 由苏哈斯·帕伊（O’Reilly）著。版权所有
    2025 苏哈斯·帕伊，978-1-098-15050-1。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为你对代码示例的使用超出了合理使用或上述许可的范围，请随时通过 [*permissions@oreilly.com*](mailto:permissions@oreilly.com)
    联系我们。
- en: O’Reilly Online Learning
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly 在线学习
- en: Note
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 40 多年来，[*O’Reilly 媒体*](https://oreilly.com) 为公司提供技术培训和商业知识，帮助他们取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专业知识。O’Reilly 的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境和来自
    O’Reilly 和 200 多家其他出版商的大量文本和视频。更多信息，请访问 [*https://oreilly.com*](https://oreilly.com)。
- en: How to Contact Us
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题寄给出版社：
- en: O’Reilly Media, Inc.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly 媒体公司
- en: 1005 Gravenstein Highway North
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加州塞巴斯蒂波尔 95472
- en: 800-889-8969 (in the United States or Canada)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-889-8969 (美国或加拿大)
- en: 707-827-7019 (international or local)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-827-7019 (国际或本地)
- en: 707-829-0104 (fax)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104 (传真)
- en: '[*support@oreilly.com*](mailto:support@oreilly.com)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*support@oreilly.com*](mailto:support@oreilly.com)'
- en: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/designing-llm-applications-1e*](https://oreil.ly/designing-llm-applications-1e).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书有一个网页，其中列出了勘误表、示例和任何其他信息。您可以通过 [*https://oreil.ly/designing-llm-applications-1e*](https://oreil.ly/designing-llm-applications-1e)
    访问此页面。
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解我们书籍和课程的新闻和信息，请访问 [*https://oreilly.com*](https://oreilly.com)。
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LinkedIn 上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)。
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 YouTube 上观看我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)。
- en: Acknowledgments
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: They say it takes a village to raise a child; I now realize it takes a metropolis
    to write a book.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 他们说，养育一个孩子需要整个村庄；我现在意识到，写一本书需要一座大都市。
- en: Firstly, I would like to thank the O’Reilly team for the meticulous professionalism
    and finesse with which they worked with me throughout the development and launch
    of the book. No wonder they are the world’s top technical book publishers. I would
    particularly like to thank Nicole Butterfield for signing me up as an author and
    Michele Cronin, the world’s best editor, whose frequent reviews ensured that the
    book developed a coherent structure. I will miss our regular check-ins! Thanks
    to Ashley Stussy, Kristen Brown, and the rest of the production team for their
    diligent work in getting the book to production.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我要感谢O’Reilly团队在整个书籍开发和发布过程中与我合作时的细致专业和精湛技艺。难怪他们是世界上最好的技术书籍出版商。我特别想感谢Nicole
    Butterfield，她邀请我成为作者，以及Michele Cronin，世界上最优秀的编辑，她频繁的审阅确保了书籍形成了连贯的结构。我会怀念我们定期的检查！感谢Ashley
    Stussy、Kristen Brown以及整个制作团队为将书籍投入生产所付出的辛勤工作。
- en: I am deeply thankful to my friend Amber Teng, who helped me with drawing the
    book illustrations and setting up the book’s Github repository. I am also immensely
    indebted to my technical reviewers Serena McDonnell, Yenson Lau, Susan Shu Chang,
    Gordon Gibson, and Nour Fahmy for the dozens of hours each of them spent in writing
    extremely detailed and thoughtful technical reviews. The book is so much better
    for it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我对朋友Amber Teng深表感激，她帮助我绘制书籍插图并设置书籍的Github仓库。我还对技术审阅员Serena McDonnell、Yenson
    Lau、Susan Shu Chang、Gordon Gibson和Nour Fahmy深表感激，他们各自花费了数十小时撰写了极其详细和周到的技术审阅。这本书因此变得更加出色。
- en: I am thankful to the Toronto AI ecosystem, especially the Aggregate Intellect,
    TMLS (Toronto Machine Learning Summit), and SharpestMinds communities for providing
    me with the space to engage with the community and ensure that I always had a
    finger on the pulse of the industry. Special thanks go to my friends Madhav Singhal,
    Jay Alammar, and Megan Risdal (who helped me coin the phrase “token etymology”)
    for our regular intellectually stimulating conversations on LLMs and for being
    the first readers of the book. I also want to give a shout out to my open-source
    collaborator Huu Nguyen, who I worked with on various open-source LLM projects,
    for the dozens of late night discussions on the most audacious ideas in LLM research.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我对多伦多人工智能生态系统表示感谢，特别是Aggregate Intellect、TMLS（多伦多机器学习峰会）和SharpestMinds社区，它们为我提供了与社区互动的空间，并确保我始终能够紧跟行业脉搏。特别感谢我的朋友Madhav
    Singhal、Jay Alammar和Megan Risdal（他们帮助我创造了“token etymology”这个短语），我们经常就LLMs进行富有启发性的对话，并且他们是这本书的第一批读者。我还想向我的开源合作者Huu
    Nguyen表示感谢，我们在多个开源LLM项目上合作，进行了数十次关于LLM研究中最大胆想法的深夜讨论。
- en: Writing a book while also being the cofounder of an AI startup was possible
    only due to the unwavering support of my partner in business and crime, Kris Bennatti
    (who also convinced me to remove the word “orifice” from the book). I will forever
    be in gratitude to the entire Hudson Labs team for their steadfast and consistent
    backing throughout, with a special shout out to Xiao Quan, whose steady hands
    ensured that I found the time to focus on the book. Additionally, I would like
    to thank my friends Kaaveh Shoamanesh, Abdullah Al-hayali, Zach Nguyen, Samarth
    Bhasin, Sadegh Raeisi, and Ian Yu for their moral support throughout and regularly
    checking that I was getting the right amount of sleep.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在同时作为一家AI初创公司的联合创始人的情况下写书，仅因我的商业和犯罪伙伴Kris Bennatti（他同时也说服我删除了书中的“orifice”一词）坚定不移的支持才成为可能。我将永远感激整个Hudson
    Labs团队在整个过程中的坚定和持续支持，特别感谢Xiao Quan，他稳定的手确保了我找到时间专注于书籍。此外，我还想感谢我的朋友Kaaveh Shoamanesh、Abdullah
    Al-hayali、Zach Nguyen、Samarth Bhasin、Sadegh Raeisi和Ian Yu，他们在整个过程中给予了我道德支持，并定期检查我是否得到了足够的睡眠。
- en: Finally, I would like to dedicate this book to my mom, Kusuma Pai, whom I simply
    refer to as “The Legend” for her lifelong sacrifices to ensure that I grew up
    and was in a position to write the book. Any success of this book should be predominantly
    credited to my mother for molding me into the person I am today.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想将这本书献给我的母亲，Kusuma Pai，我简单地称她为“传奇”，因为她一生的牺牲，确保我能够长大成人并有机会写这本书。这本书的任何成功都应该主要归功于我的母亲，因为她塑造了我今天成为的人。
