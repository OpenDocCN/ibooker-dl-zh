- en: Chapter 10\. Interfacing LLMs with External Tools
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章\. 与外部工具接口LLM
- en: In the first two parts of the book, we have seen how impactful standalone LLMs
    can be in solving a wide variety of tasks. To effectively harness their full range
    of capabilities in an organization, they have to be integrated into the existing
    data and software ecosystem. Unlike traditional software systems, LLMs can generate
    autonomous actions to interact with other ecosystem components, bringing a degree
    of flexibility never seen before in the software world. This flexibility unlocks
    a whole host of use cases that were previously considered impossible.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前两部分中，我们已经看到独立的LLM在解决各种任务中的影响有多么大。为了有效地利用它们在组织中的全部能力，它们必须集成到现有的数据和软件生态系统中。与传统软件系统不同，LLM可以生成自主行动来与其他生态系统组件交互，带来了软件世界中前所未有的灵活性。这种灵活性解锁了一大批之前被认为不可能用例。
- en: 'Another reason we need LLMs to interact with software and external data: as
    we know all too well, current LLMs have significant limitations, some of which
    we discussed in [Chapter 1](ch01.html#chapter_llm-introduction). To recap some
    key points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要LLM与软件和外部数据交互的另一个原因是：正如我们所深知的那样，当前的LLM存在重大限制，其中一些我们在[第1章](ch01.html#chapter_llm-introduction)中讨论过。为了回顾一些关键点：
- en: Since it is expensive to retrain LLMs or keep them continuously updated, they
    have a knowledge cutoff date and thus possess no knowledge of more recent events.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于重新训练LLM或持续更新它们成本高昂，它们有一个知识截止日期，因此对最近的事件一无所知。
- en: Even though they are getting better over time, LLMs don’t always get math right.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管它们随着时间的推移在变得更好，但LLM并不总是能正确处理数学问题。
- en: They can’t provide factuality guarantees or accurately cite the sources of their
    outputs.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们无法保证输出的真实性或准确引用其输出的来源。
- en: Feeding them your own data effectively is a challenge; fine-tuning is nontrivial,
    and in-context learning is limited by the length of the effective context window.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向它们提供自己的数据是一个挑战；微调非同小可，并且情境学习受限于有效上下文窗口的长度。
- en: As we have been noticing throughout the book, the consolidation effect is leading
    us to a future (unless we hit a technological wall) where many of the aforementioned
    limitations might be addressed within the model itself. But we don’t necessarily
    need to wait for that moment to arrive, as many of these limitations can be addressed
    today by offloading the tasks and subtasks to external tools.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在整本书中注意到的那样，巩固效应正在引导我们走向一个未来（除非我们遇到技术障碍），在这个未来中，许多上述限制可能可以在模型内部得到解决。但我们不一定需要等待这一时刻的到来，因为许多这些限制可以通过将任务和子任务卸载到外部工具来解决。
- en: 'In this chapter, we will define the three canonical LLM interaction paradigms
    and provide guidance on how to choose between them for your application. Broadly
    speaking, there are two types of external entities that LLMs need to interact
    with: data stores and software/models, collectively called tools. We will demonstrate
    how to interface LLMs with various tools like APIs and code interpreters. We will
    show how to make the best use of libraries like LangChain and LlamaIndex, which
    have vastly simplified LLM integrations. We will explore the various scaffolding
    software that needs to be constructed to facilitate seamless interactions with
    the environment. We will also push the limits of what today’s LLMs are capable
    of, by demonstrating how they can be deployed as an agent that can make autonomous
    decisions.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将定义三种经典的LLM交互范式，并提供如何为您的应用程序选择它们的指导。从广义上讲，LLM需要与之交互的外部实体有两种类型：数据存储和软件/模型，统称为工具。我们将演示如何将LLM与各种工具（如API和代码解释器）接口。我们将展示如何充分利用LangChain和LlamaIndex等库，这些库极大地简化了LLM的集成。我们将探索需要构建的各种脚手架软件，以促进与环境的无缝交互。我们还将通过演示它们可以作为可以做出自主决定的代理来部署，来推动今天LLM的能力极限。
- en: LLM Interaction Paradigms
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM交互范式
- en: 'Suppose you have a task you want the LLM to solve. There are several possible
    options:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个任务想要让LLM来解决。有几种可能的选择：
- en: The LLM uses its own memory and capabilities encoded in its parameters to solve
    the task.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM使用其参数中编码的自身记忆和能力来解决任务。
- en: You feed the LLM all the context it needs to solve the task within the prompt,
    and the LLM uses the provided context and its capabilities to solve it.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你向LLM提供解决任务所需的所有上下文，LLM使用提供的上下文和其能力来解决它。
- en: The LLM doesn’t have the requisite information or skills to solve this task,
    so you update the model parameters (fine-tuning etc., as detailed in Chapters
    [6](ch06.html#llm-fine-tuning)–[8](ch08.html#ch8)) so that it is able to activate
    the skills and knowledge needed to solve it.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 没有解决此任务所需的信息或技能，因此你需要更新模型参数（微调等，如第 6 章[6](ch06.html#llm-fine-tuning)–第 8
    章[8](ch08.html#ch8) 中详细说明）以便它能激活解决任务所需的技能和知识。
- en: You don’t know a priori what context is needed to solve the task, so you use
    mechanisms to automatically fetch the relevant context and insert it into the
    prompt (passive approach).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不知道在解决任务之前需要什么上下文，所以你使用机制自动获取相关上下文并将其插入到提示中（被动方法）。
- en: You provide explicit instructions to the LLM on how to interact with external
    tools and data stores to solve your task, which the LLM follows (explicit approach).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你向 LLM 提供明确的指令，说明如何与外部工具和数据存储交互以解决你的任务，LLM 将遵循这些指令（显式方法）。
- en: The LLM breaks the task into multiple subtasks if needed, interacts with its
    environment to gather the information/knowledge needed to solve the task, and
    delegates subtasks to external models and tools when it doesn’t have the requisite
    capabilities to solve that subtask (autonomous approach).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，LLM 将任务分解成多个子任务，与它的环境交互以收集解决任务所需的信息/知识，并在它没有解决该子任务所需的能力时将子任务委托给外部模型和工具（自主方法）。
- en: As you can see, the last three involve the LLM interacting with its environment
    (passive, explicit, and autonomous). Let’s explore the three interaction paradigms
    in detail.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，最后三个例子涉及 LLM 与其环境的交互（被动、显式和自主）。让我们详细探讨这三种交互范式。
- en: Passive Approach
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 被动方法
- en: '[Figure 10-1](#passive-interaction) shows the typical workflow of an application
    that involves an LLM passively interacting with a data store.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-1](#passive-interaction) 展示了涉及 LLM 被动与数据存储交互的应用程序的典型工作流程。'
- en: '![Passive Interaction](assets/dllm_1001.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![被动交互](assets/dllm_1001.png)'
- en: Figure 10-1\. An LLM passively interacting with a data store
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 一个 LLM 被动地与数据存储进行交互
- en: A large number of use cases involve leveraging LLMs to use your own data. Examples
    include building a question-answering assistant over your company’s internal knowledge
    base that is spread over a bunch of Notion documents, or an airline chatbot that
    responds to customer queries about flight status or booking policies.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用例涉及利用 LLM 来使用自己的数据。例如，在你的公司内部知识库上构建一个问答助手，该知识库分布在多个 Notion 文档中，或者一个航空公司聊天机器人，它能够响应客户关于航班状态或预订政策的查询。
- en: 'To allow the LLM to access external information, we need two types of components:
    “data stores” that contain the required information and retrieval engines that
    can retrieve relevant data from data stores given a query. The retrieval engine
    can be powered by an LLM itself, or it can be as simple as a keyword-matching
    algorithm. The data store(s) can be a repository of data like a database, knowledge
    graph, vector database, or even just a collection of text files. Data in the data
    store is represented and indexed to make retrieval more efficient. Data representation,
    indexing, and retrieval are topics important enough to merit their own chapter:
    we will defer detailed discussions on them to [Chapter 11](ch11.html#chapter_llm_interfaces).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许 LLM 访问外部信息，我们需要两种类型的组件：“数据存储”包含所需信息，以及检索引擎可以从数据存储中根据查询检索相关数据。检索引擎可以由 LLM
    本身提供动力，或者它可能只是一个简单的关键词匹配算法。数据存储可以是数据库、知识图谱、向量数据库，甚至只是一些文本文件的集合。数据存储中的数据以表示和索引的形式存在，以提高检索效率。数据表示、索引和检索是足够重要的主题，值得单独成章：我们将推迟对这些内容的详细讨论到[第
    11 章](ch11.html#chapter_llm_interfaces)。
- en: When a user issues a query, the retrieval engine uses the query to find the
    documents or text segments that are most relevant to answering this query. After
    ensuring that these fit into the context window of the LLM, they are fed to the
    LLM along with the query. The LLM is expected to answer the query given the relevant
    context provided in the prompt. This approach is popularly known as RAG, although
    as we will see in [Chapter 12](ch12.html#ch12), RAG refers to an even broader
    concept. RAG is an important paradigm that deserves its own chapter, so we will
    defer detailed coverage of the paradigm to [Chapter 12](ch12.html#ch12).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提出查询时，检索引擎使用查询来找到与回答此查询最相关的文档或文本片段。在确保这些内容适合 LLM 的上下文窗口后，它们连同查询一起被输入到 LLM
    中。LLM 预期将根据提示中提供的相关上下文回答查询。这种方法通常被称为 RAG，尽管正如我们将在第 12 章（ch12.html#ch12）中看到的，RAG
    指的是一个更广泛的概念。RAG 是一个重要的范式，值得拥有自己的章节，因此我们将推迟对这个范式的详细讨论到第 12 章（ch12.html#ch12）。
- en: Note that the distinguishing feature of this paradigm is the passive nature
    of the LLM in the interaction. The LLM simply responds to the prompt and furnishes
    an answer. It does not know the source of the content inside the prompt. This
    paradigm is often used for building QA assistants or chatbots, where external
    information is required to understand the context of the conversation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个范式的显著特征是 LLM 在交互中的被动性质。LLM 只是响应提示并提供答案。它不知道提示内容中内容的来源。这个范式通常用于构建需要外部信息来理解对话上下文的问答助手或聊天机器人。
- en: Note
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: From this point forward, we will refer to user requests to the LLM as *queries*
    and textual units that are retrieved from external data stores as *documents*.
    Documents can be full documents, passages, paragraphs, or sentences.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将把用户对 LLM 的请求称为 *查询*，并将从外部数据存储中检索到的文本单元称为 *文档*。文档可以是完整的文档、段落、段落或句子。
- en: The Explicit Approach
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 明确方法
- en: '[Figure 10-2](#explicit-approach) demonstrates the explicit approach to interface
    LLMs with external tools.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-2](#explicit-approach) 展示了将 LLM 与外部工具接口的明确方法。'
- en: '![Explicit Approach](assets/dllm_1002.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![明确方法](assets/dllm_1002.png)'
- en: Figure 10-2\. The explicit interaction approach in action
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 明确交互方法在实际中的应用
- en: Unlike in the passive approach, the LLM is no longer a passive participant.
    We provide the LLM with explicit instructions on how and when to invoke external
    data stores and tools. The LLM interacts with its environment based on a pre-programmed
    set of conditions. This approach is recommended when the interaction sequence
    is fixed, limited in scope, and preferably involves a very small number of steps.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与被动方法不同，LLM 已不再是被动参与者。我们向 LLM 提供明确的指令，说明如何以及何时调用外部数据存储和工具。LLM 根据预先编程的条件集与环境进行交互。当交互序列固定、范围有限且最好涉及非常少的步骤时，推荐使用这种方法。
- en: 'For an AI data analyst assistant, an example interaction sequence could be:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 AI 数据分析师助手，一个示例交互序列可能是：
- en: User expresses query in natural language asking to visualize some data trends
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户用自然语言表达查询，要求可视化某些数据趋势
- en: The LLM generates SQL to retrieve the data needed to resolve the user query
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 生成 SQL 来检索解决用户查询所需的数据
- en: After receiving the data, the LLM uses it to generate code that can be run by
    a code interpreter to generate statistics or visualizations
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在收到数据后，LLM 使用它来生成可以由代码解释器运行的代码，以生成统计数据或可视化
- en: '[Figure 10-3](#ai-data-analyst) shows a fixed interaction sequence implemented
    for an AI data analyst.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-3](#ai-data-analyst) 展示了一个为 AI 数据分析师实现的固定交互序列。'
- en: '![ai-data-analyst](assets/dllm_1003.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![ai-data-analyst](assets/dllm_1003.png)'
- en: Figure 10-3\. An example workflow for an AI data analyst
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. AI 数据分析师的一个示例工作流程
- en: In this paradigm, the interaction sequence is predetermined and rule-based.
    The LLM exercises no agency in determining which step to take next. I recommend
    this approach for building robust applications that have stricter reliability
    requirements.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个范式下，交互序列是预先确定且基于规则的。LLM 在决定下一步采取哪个步骤时没有任何自主权。我推荐这种方法来构建具有更严格可靠性要求的稳健型应用。
- en: The Autonomous Approach
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自主方法
- en: '[Figure 10-4](#agentic-approach) shows how we can turn an LLM into an autonomous
    agent that can solve complex tasks by itself.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-4](#agentic-approach) 展示了如何将 LLM 转换为能够独立解决复杂任务的自主代理。'
- en: '![Agentic Approach](assets/dllm_1004.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![自主方法](assets/dllm_1004.png)'
- en: Figure 10-4\. A typical autonomous LLM-driven agent workflow
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 典型的由 LLM 驱动的自主代理工作流程
- en: 'The autonomous approach, or the Holy Grail approach as I like to call it, turns
    an LLM into an autonomous agent that can solve tasks on its own by interacting
    with its environment. Here is a typical workflow of an autonomous agent:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 自主方法，或者像我喜欢叫的圣杯方法，将 LLM 转变为一个可以自主通过与环境交互来完成任务的自主代理。以下是一个典型自主代理的工作流程：
- en: The user formulates their requirements in natural language, optionally providing
    the format in which they want the LLM to provide the answer.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户以自然语言形式提出他们的要求，可选地提供他们希望 LLM 提供答案的格式。
- en: The LLM decomposes the user query into manageable subtasks.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 将用户查询分解为可管理的子任务。
- en: The LLM synchronously or asynchronously solves each subtask of the problem.
    Where possible, the LLM uses its own memory and knowledge to solve a specific
    subtask. For subtasks where the LLM cannot answer on its own, it chooses a tool
    to invoke from a list of available tools. Where possible, the LLM uses the outputs
    from solutions of already executed subtasks as inputs to other subtasks.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 同步或异步地解决问题的每个子任务。在可能的情况下，LLM 使用自己的记忆和知识来解决特定的子任务。对于 LLM 无法自行回答的子任务，它从可用工具列表中选择一个工具来调用。在可能的情况下，LLM
    使用已执行子任务的解决方案的输出作为其他子任务的输入。
- en: The LLM synthesizes the final answer using the solutions of the subtasks, generating
    the output in the requested output format.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 使用子任务的解决方案综合最终答案，以请求的输出格式生成输出。
- en: This paradigm is general enough to capture just about any use case. It is also
    a risky paradigm, as we are assigning the LLM too much responsibility and agency.
    At this juncture, I would not recommend using this paradigm for any mission-critical
    applications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个范式足够通用，可以捕捉几乎所有用例。它也是一个有风险的范式，因为我们正在赋予 LLM 过多的责任和代理权。在这个阶段，我不建议将此范式用于任何关键任务应用。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why am I calling for caution in deploying agents? Humans often underestimate
    the accuracy requirements for applications. For a lot of use cases, getting it
    right 99% of the time is still not good enough, especially when the failures are
    unpredictable and the 1% of failures can be potentially catastrophic. The 99%
    problem is also the one that has long plagued self-driving cars and prevented
    their broader adoption. This doesn’t mean we can’t deploy autonomous LLM agents;
    we just need clever product design that can shield the user from their failures.
    We also need robust human-in-the-loop paradigms.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我呼吁在部署代理时要谨慎？人类往往低估了应用程序的准确性要求。对于许多用例，99% 的时间正确仍然不够好，尤其是当失败不可预测且 1% 的失败可能具有潜在的灾难性时。99%
    的问题也是长期以来困扰自动驾驶汽车并阻碍其更广泛采用的问题。这并不意味着我们不能部署自主 LLM 代理；我们只需要巧妙的产品设计来保护用户免受其失败的影响。我们还需要强大的人机交互范式。
- en: We have used the word “agent” several times now without defining it. Let’s correct
    that and consider what agents mean and how we can build them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次使用了“代理”这个词，但没有对其进行定义。让我们纠正这一点，并考虑代理的含义以及我们如何构建它们。
- en: Defining Agents
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义代理
- en: 'As the hype starts building over LLM-based agents, the colloquial definition
    of agents has already started to expand from its traditional definition. This
    is because truly agentic systems are hard to build, so there is a tendency to
    shift the goalposts and claim best-effort systems to be already agentic even though
    they technically may not fit the requirements. In this book, we will stick to
    a more conservative definition of agents, defining them as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于 LLM 的代理的炒作开始升温，代理的通俗定义已经从其传统定义开始扩展。这是因为真正具有代理性质的系统很难构建，因此存在一种趋势，即改变目标，声称尽最大努力构建的系统已经是代理，尽管从技术上讲可能不符合要求。在这本书中，我们将坚持更保守的代理定义，将其定义为：
- en: LLM-driven software systems that are able to interact with their environment
    and take autonomous actions to complete a task.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 能够与环境互动并采取自主行动以完成任务的自定义软件系统。
- en: 'Key characteristics of agents are:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的关键特征包括：
- en: Their autonomous nature
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的自主性质
- en: The sequence of steps required to perform a task need not be specified to the
    agent. Agents can decide to perform any sequence of actions, unprompted by humans.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 执行任务所需的步骤不需要指定给代理。代理可以决定执行任何未经人类提示的动作序列。
- en: Their ability to interact with their environment
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 他们与环境互动的能力
- en: Agents can be connected to external data sources and software tools, which allows
    agents to retrieve data, invoke tools, execute code, and provide instructions
    when appropriate to solve a task.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以连接到外部数据源和软件工具，这使得代理能够检索数据、调用工具、执行代码，并在适当的时候提供指令以解决问题。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many definitions of “agent” do not require them to be autonomous. According
    to their definitions, applications following the explicit paradigm can also be
    called agents (albeit as non-autonomous or semi-autonomous agents).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 许多“代理”的定义并不要求它们必须是自主的。根据它们的定义，遵循显式范式的应用程序也可以被称为代理（尽管是非自主或半自主代理）。
- en: The agentic paradigm as we defined it is extremely powerful and general. Let’s
    take a moment to appreciate it. If an agent receives a task that it doesn’t know
    how to solve (and it *knows* that it doesn’t know), then instead of just giving
    up, it can potentially learn to solve the task by itself by searching the web
    or knowledge bases for pointers, or even by collecting data and fine-tuning a
    model that can help solve the task.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的代理范式非常强大且通用。让我们花点时间来欣赏它。如果一个代理接收到一个它不知道如何解决的问题（并且它*知道*它不知道），那么它不会只是放弃，它可以通过在网络上或知识库中搜索提示，甚至通过收集数据并微调一个可以帮助解决任务的模型来学习自行解决问题。
- en: Given these enviable abilities, are machines going to take over the world? In
    practice, current autonomous agents are limited in what they can actually achieve.
    They tend to get stuck in loops, they take incorrect actions, and they are unable
    to reliably self-correct. It is more practical to build partially autonomous agents,
    where the LLM is provided with guidance throughout its workflow, either through
    agent orchestration software or with a human in the loop. For the rest of this
    chapter, our focus will be on building practical agents that can reliably solve
    a narrower class of tasks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些令人羡慕的能力，机器是否会接管世界？在实践中，当前的自主代理在实际上能实现的事情上有限。它们往往会陷入循环，采取错误的行为，并且无法可靠地自我纠正。构建部分自主代理更为实际，其中LLM在其工作流程中得到指导，无论是通过代理编排软件还是通过在循环中的人类。在本章的剩余部分，我们的重点将放在构建能够可靠解决更窄任务类别的实用代理。
- en: Agentic Workflow
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理工作流程
- en: 'Using our definition of agents, let’s explore how agents work in practice.
    As an example, let’s consider an agent that is asked to answer this question:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们对代理的定义，让我们探讨代理在实际中的工作方式。作为一个例子，让我们考虑一个被要求回答以下问题的代理：
- en: Who was the CFO of Apple when its stock price was at its lowest point in the
    last 10 years?
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在过去10年中，苹果公司股价最低时，谁是公司的首席财务官？
- en: Let’s say the agent has all the information it needs to solve this task. It
    has access to the web, to SQL databases containing stock price information, and
    to knowledge bases containing CFO tenure information. It is connected to a code
    interpreter so that it can generate and run code, and it has access to financial
    APIs. The system prompt contains details about all the tools and data stores the
    LLM has access to.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设代理拥有解决此任务所需的所有信息。它可以访问网络、包含股价信息的SQL数据库以及包含首席财务官任期信息的知识库。它连接到一个代码解释器，以便它可以生成和运行代码，并且它可以访问金融API。系统提示包含有关LLM可以访问的所有工具和数据存储的详细信息。
- en: 'To answer the given query, the LLM has to perform this sequence of steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答给定的查询，LLM必须执行以下步骤：
- en: To calculate the date range, it needs the current date. If this is not included
    in the system prompt, it either searches the web to find the current date or generates
    code for returning the system time, which is then executed by a code interpreter.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了计算日期范围，它需要当前日期。如果这个日期没有包含在系统提示中，它要么在网络上搜索以找到当前日期，要么生成代码以返回系统时间，然后由代码解释器执行。
- en: Using the current date, it finds the other end of the date range by executing
    a simple arithmetic operation by itself, or by generating code for it. Steps 1
    and 2 could be combined into a single program.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前日期，它通过执行简单的算术运算自行找到日期范围的另一端，或者为它生成代码。步骤1和2可以合并成一个程序。
- en: It finds a database table in the available datastore list that contains stock
    price information. It retrieves the schema of the table, inserts it into the prompt,
    and generates a SQL query for finding the date when the stock price was at its
    minimum in the last 10 years.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在可用的数据存储列表中找到一个包含股价信息的数据库表。它检索表的架构，将其插入到提示中，并生成一个SQL查询，以找到过去10年中股价最低的日期。
- en: With the date in hand, it needs to find the CFO of Apple on that date. It can
    call a search engine API to check if there is an explicit mention of the CFO on
    that particular date.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拿到日期后，它需要找到那天苹果公司的首席财务官。它可以调用搜索引擎API来检查那天是否有关于首席财务官的明确提及。
- en: If the search engine query fails to provide a result, it finds a financial API
    in its tools list and retrieves and inserts the API documentation into its context.
    It then generates and invokes code for an API call to retrieve the list of Apple
    CFOs and their tenures.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果搜索引擎查询未能提供结果，它会在其工具列表中找到一个金融API，并检索并插入API文档到其上下文中。然后它生成并调用API调用的代码来检索苹果公司首席财务官及其任期的列表。
- en: It uses its arithmetic reasoning skills to find the CFO tenure that matches
    the date of the lowest stock price.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使用其算术推理技能来找到与最低股价日期相匹配的首席财务官任期。
- en: It generates the final answer. If there is a requested output format, it tries
    to adhere to that.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它生成最终答案。如果有请求的输出格式，它会尽量遵守那个格式。
- en: Depending on the implementation, the sequence of steps could vary slightly.
    For example, you can fine-tune a model so that it can generate code for API calls
    or SQL queries directly without having to retrieve the schema from a data store
    or API.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 根据实现方式，步骤序列可能会有所不同。例如，你可以微调一个模型，使其能够直接生成API调用或SQL查询的代码，而无需从数据存储或API检索模式。
- en: To perform the given sequence of tasks, the model should first understand that
    the given task needs to be decomposed into a series of subtasks. This is called
    task decomposition. Task decomposition and planning can be performed by the LLM
    or offloaded to an external tool.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行给定的任务序列，模型首先应该理解给定的任务需要分解成一系列子任务。这被称为任务分解。任务分解和规划可以由LLM执行，也可以外包给外部工具。
- en: Components of an Agentic System
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理系统的组件
- en: 'While the specific architecture of any given agentic system depends heavily
    on the use cases it is intended to support, each of its components can be classified
    into one of the following types:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然任何给定代理系统的具体架构很大程度上取决于它打算支持的使用案例，但它的每个组件都可以归类为以下类型之一：
- en: Models
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型
- en: Tools
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具
- en: Data stores
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储
- en: Agent loop prompt
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理循环提示
- en: Guardrails and verifiers
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界和验证器
- en: Orchestration software
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排软件
- en: '[Figure 10-5](#agentic-system) shows a canonical agentic system and how its
    components interact.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-5](#agentic-system)显示了一个典型的代理系统及其组件的交互方式。'
- en: '![agentic-system](assets/dllm_1005.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![agentic-system](assets/dllm_1005.png)'
- en: Figure 10-5\. A production-grade agentic system
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. 一套生产级别的代理系统
- en: Let’s explore each of these types.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索这些类型中的每一个。
- en: Models
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: Language models are the backbone of agentic systems, responsible for their autonomous
    nature and problem-solving capabilities. A single agentic system could be composed
    of multiple language models, with each model playing a distinct role.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是代理系统的骨架，负责它们的自主性和问题解决能力。一个代理系统可以由多个语言模型组成，每个模型都扮演着不同的角色。
- en: For example, you can build an agent consisting of two models; one model solves
    user tasks and another model takes its output and converts it into a structured
    form according to user requirements.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以构建一个由两个模型组成的代理；一个模型解决用户任务，另一个模型将其输出转换为用户要求的结构化形式。
- en: Tip
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Agentic workflows can consume a lot of language model tokens, which can be cost
    prohibitive. To keep costs under control, consider using multiple language models
    of different sizes, with the smaller (and cheaper) models performing easier tasks.
    For more details on how to accomplish division of labor among these models, see
    [Chapter 13](ch13.html#ch13).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 代理工作流程可能会消耗大量的语言模型令牌，这可能会变得成本高昂。为了控制成本，考虑使用不同大小的多个语言模型，其中较小的（且更便宜的）模型执行更简单的任务。有关如何在这些模型之间完成劳动分工的更多详细信息，请参阅[第13章](ch13.html#ch13)。
- en: More generally, you can build agents with specialized models catering to each
    part of the agentic workflow. For example, a code-LLM can be used to generate
    code, and task-specific fine-tuned models that specialize in individual workflow
    steps can be used. This setup can be interpreted as a *multi-agent architecture*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，你可以构建专门针对代理工作流程各个部分的代理。例如，一个代码-LLM可以用来生成代码，而针对特定任务的微调模型可以用来执行单个工作流程步骤。这种设置可以解释为*多代理架构*。
- en: '[Figure 10-6](#multi-agent-setup) shows an agentic system made up of multiple
    LLMs.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-6](#multi-agent-setup)显示了由多个LLM组成的代理系统。'
- en: '![multi-agent-setup](assets/dllm_1006.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![multi-agent-setup](assets/dllm_1006.png)'
- en: Figure 10-6\. An agentic system with multiple LLMs
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 具有多个LLM的代理系统
- en: Finally, any kind of model, including non-LLMs, can be plugged into an agentic
    system to solve specific tasks. For example, the planning stage can be performed
    using [symbolic planners](https://oreil.ly/sXPWG).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，任何类型的模型，包括非LLM模型，都可以插入到代理系统中以解决特定任务。例如，规划阶段可以使用[符号规划器](https://oreil.ly/sXPWG)。
- en: Tools
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具
- en: As described earlier, software or models that can be invoked by an LLM are called
    tools. Libraries like [LangChain](https://oreil.ly/35Lgu) and [LlamaIndex](https://oreil.ly/WF-d1)
    provide connectors to various software interfaces, including code interpreters,
    search engines, databases, ML models, and a variety of APIs. Let’s explore how
    to work with some of these in practice.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，可以被LLM调用的软件或模型称为工具。像[LangChain](https://oreil.ly/35Lgu)和[LlamaIndex](https://oreil.ly/WF-d1)这样的库为各种软件接口提供了连接器，包括代码解释器、搜索引擎、数据库、ML模型和各种API。让我们探索如何在实践中使用其中的一些。
- en: Web search
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络搜索
- en: 'LangChain provides connectors for major search engines like Google, Bing, and
    DuckDuckGo. Let’s try out DuckDuckGo:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain为主要的搜索引擎如Google、Bing和DuckDuckGo提供了连接器。让我们试试DuckDuckGo：
- en: '[PRE0]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The response can be fed back to the language model where it is further processed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 响应可以反馈给语言模型，进一步处理。
- en: API connectors
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: API连接器
- en: 'To illustrate calling APIs, we will showcase LangChain’s Wikipedia API wrapper:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明调用API，我们将展示LangChain的维基百科API包装器：
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `load()` function runs a search on Wikipedia and returns the page text and
    metadata information of the top-k results. (top-k = 3 by default). You can also
    use the `run()` function to return only page summaries of the top-k matches.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`load()`函数在维基百科上执行搜索，并返回前k个结果页面的文本和元数据信息。（默认为top-k = 3）。您也可以使用`run()`函数仅返回前k个匹配项的页面摘要。'
- en: Code interpreter
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码解释器
- en: 'Next, let’s explore how you can invoke a code interpreter and run arbitrary
    code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索如何调用代码解释器并运行任意代码：
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Warning
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be wary of running code generated by LLMs in response to user prompts. Users
    can induce the model to generate malicious code!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在用户提示下由LLM生成的代码运行。用户可能会诱导模型生成恶意代码！
- en: Database connectors
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据库连接器
- en: 'Finally, let’s check out how to connect to a database and run queries:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何连接到数据库并运行查询：
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `run()` function executes the provided SQL query and returns the response
    as a string. Replace `*DATABASE_URI*` with your own database and queries, and
    verify the responses.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()`函数执行提供的SQL查询，并将响应作为字符串返回。将`*DATABASE_URI*`替换为您自己的数据库和查询，并验证响应。'
- en: Tip
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: For more customizability, you can fork the LangChain connectors and repurpose
    them for your own use.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可定制性，您可以分叉LangChain连接器，并将它们重新用于自己的用途。
- en: Next, let’s see how we can interface LLMs with these tools in an agentic workflow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何将这些工具与LLM在代理工作流程中进行接口连接。
- en: First, we need to make the LLM aware that it has access to these tools. One
    of the ways to achieve this is to provide the names and short descriptions of
    the tools, called the *tool list*, to the LLM through the system prompt.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要让LLM意识到它可以访问这些工具。实现这一目标的一种方法是通过系统提示向LLM提供工具的名称和简短描述，称为*工具列表*。
- en: Next, the LLM needs to be able to select the right tool at the appropriate juncture
    in the workflow. For example, if the next step in solving a task is to find the
    weather in Chicago this evening, the web search tool has to be invoked rather
    than the Wikipedia one. Later in this chapter, we will discuss techniques to help
    the LLM select the right tool.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，LLM需要能够在工作流程的适当阶段选择正确的工具。例如，如果解决任务的下一步是查找今晚芝加哥的天气，则需要调用网络搜索工具而不是维基百科工具。在本章的后面部分，我们将讨论帮助LLM选择正确工具的技术。
- en: Under the hood, tool invocation is typically achieved by the LLM generating
    special tokens indicating that it is entering tool invocation mode, along with
    tokens representing the tool functions and arguments to be invoked. The actual
    tool invocation is performed by an agent orchestration framework.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，工具调用通常是通过LLM生成特殊令牌来实现的，这些令牌表示LLM正在进入工具调用模式，以及表示要调用的工具功能和参数的令牌。实际的工具调用是由代理编排框架执行的。
- en: 'In LangChain, we can make a tool available to an LLM and have it invoked:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangChain中，我们可以将工具提供给LLM，并调用它：
- en: '[PRE4]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Some models come with native tool-calling abilities. For models that don’t,
    you can fine-tune the base model to impart them with tool-calling abilities. Among
    open models, Llama 3.1 Instruct (8B/70B/405B) is an example of a model having
    native tool-calling support. Here’s how tool calling works with Llama 3.1.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型具有原生的工具调用能力。对于没有这种能力的模型，您可以微调基础模型以赋予它们工具调用能力。在开源模型中，Llama 3.1 Instruct (8B/70B/405B)
    是一个具有原生工具调用支持的模型示例。以下是 Llama 3.1 中工具调用是如何工作的。
- en: 'Llama 3.1 comes with native support for three tools: Brave web search, Wolfram|Alpha
    mathematical engine, and a code interpreter. These can be *activated* by defining
    them in the system prompt:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3.1 内置了对三个工具的原生支持：Brave 网络搜索、Wolfram|Alpha 数学引擎和代码解释器。这些工具可以通过在系统提示中定义它们来
    *激活*：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s ask the LLM a question by appending a user prompt to the system prompt:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将用户提示附加到系统提示中，我们可以向 LLM 提出一个问题：
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Llama 3.1 responds with a tool invocation that looks like this:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3.1 响应一个看起来像这样的工具调用：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]` The `<|python_tag|>` token is a special token generated by Llama 3.1
    to indicate that it is entering tool-calling mode. The `<|eom_id|>` special token
    indicates that the model has not ended its turn yet and will wait to be fed with
    the results of the tool invocation.    You can also provide your own tools in
    the prompt: using JSON is recommended.    ###### Tip    If you have a lot of tools,
    then the detailed descriptions of the tools can be represented in a data store
    and retrieved only if they are selected. The prompt then needs to contain only
    the name of the tool and a short description.    Here is an example of a tool
    definition in JSON describing a local function that can be called:    [PRE9] `}`         `},`         `"required"``:`
    `[``"claim_sentence"``,` `"model"``]`     `}`     `}` `}` [PRE10]   `` `The tool
    call is generated by the model in JSON with the prescribed format.    ###### Note    The
    actual tool invocation is performed by an agent orchestration software. Llama
    3.1 comes with [llama-stack-apps](https://oreil.ly/SSmkI), a library that facilitates
    agentic workflows.    Sometimes the tool call can be more complex than just returning
    the name of a function and its arguments. An example of this is querying a database.
    For the LLM to generate the right SQL query, you should provide the schema of
    the database tables in the system prompt. If the database has too many tables,
    then their schema can be retrieved on demand by the LLM.    ###### Tip    You
    can use a separate specialized model for code and SQL query generation. A general-purpose
    model can generate a textual description of the desired outcome, and this can
    be used as input to a code LLM or an LLM fine-tuned on text-to-SQL.    For large-scale
    or high-stakes applications, you can fine-tune your models to make them better
    at tool use. A good fine-tuning recipe to follow is Qin et al.’s [ToolLLaMA](https://oreil.ly/Ewlxt).`
    `` [PRE11]``  [PRE12] [PRE13]`py  [PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE8]`<|python_tag|>` 标记是 Llama 3.1 生成的一个特殊标记，用来指示它正在进入工具调用模式。`<|eom_id|>`
    特殊标记表示模型尚未结束其回合，并等待接收工具调用的结果。您也可以在提示中提供自己的工具：推荐使用 JSON 格式。#### 提示 如果您有很多工具，那么工具的详细描述可以存储在数据存储中，并且只有在选择时才检索。提示中只需要包含工具的名称和简短描述。以下是一个工具定义的示例，描述了一个可以调用的本地函数：[PRE9]
    `}`         `},`         `"required"``:` `[``"claim_sentence"``,` `"model"``]`     `}`     `}`
    `}` [PRE10]   `` `工具调用是由模型按照规定的格式以 JSON 格式生成的。#### 注意 实际的工具调用是由代理编排软件执行的。Llama
    3.1 随附 [llama-stack-apps](https://oreil.ly/SSmkI)，这是一个简化代理工作流程的库。有时工具调用可能比仅仅返回函数名称及其参数更复杂。例如，查询数据库。为了使
    LLM 生成正确的 SQL 查询，您应该在系统提示中提供数据库表的模式。如果数据库有太多的表，那么它们的模式可以由 LLM 按需检索。#### 提示 您可以使用专门的模型来生成代码和
    SQL 查询。通用模型可以生成所需结果的文本描述，这可以用作代码 LLM 或文本到 SQL 的 LLM 微调的输入。对于大规模或高风险应用，您可以微调模型以使其在工具使用方面表现得更好。一个很好的微调方案是遵循
    Qin 等人的 [ToolLLaMA](https://oreil.ly/Ewlxt)。` `` [PRE11]``  [PRE12] [PRE13]`py  [PRE14]'
