- en: Chapter 10\. Interfacing LLMs with External Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first two parts of the book, we have seen how impactful standalone LLMs
    can be in solving a wide variety of tasks. To effectively harness their full range
    of capabilities in an organization, they have to be integrated into the existing
    data and software ecosystem. Unlike traditional software systems, LLMs can generate
    autonomous actions to interact with other ecosystem components, bringing a degree
    of flexibility never seen before in the software world. This flexibility unlocks
    a whole host of use cases that were previously considered impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another reason we need LLMs to interact with software and external data: as
    we know all too well, current LLMs have significant limitations, some of which
    we discussed in [Chapter 1](ch01.html#chapter_llm-introduction). To recap some
    key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Since it is expensive to retrain LLMs or keep them continuously updated, they
    have a knowledge cutoff date and thus possess no knowledge of more recent events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though they are getting better over time, LLMs don’t always get math right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can’t provide factuality guarantees or accurately cite the sources of their
    outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding them your own data effectively is a challenge; fine-tuning is nontrivial,
    and in-context learning is limited by the length of the effective context window.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have been noticing throughout the book, the consolidation effect is leading
    us to a future (unless we hit a technological wall) where many of the aforementioned
    limitations might be addressed within the model itself. But we don’t necessarily
    need to wait for that moment to arrive, as many of these limitations can be addressed
    today by offloading the tasks and subtasks to external tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will define the three canonical LLM interaction paradigms
    and provide guidance on how to choose between them for your application. Broadly
    speaking, there are two types of external entities that LLMs need to interact
    with: data stores and software/models, collectively called tools. We will demonstrate
    how to interface LLMs with various tools like APIs and code interpreters. We will
    show how to make the best use of libraries like LangChain and LlamaIndex, which
    have vastly simplified LLM integrations. We will explore the various scaffolding
    software that needs to be constructed to facilitate seamless interactions with
    the environment. We will also push the limits of what today’s LLMs are capable
    of, by demonstrating how they can be deployed as an agent that can make autonomous
    decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM Interaction Paradigms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you have a task you want the LLM to solve. There are several possible
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM uses its own memory and capabilities encoded in its parameters to solve
    the task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You feed the LLM all the context it needs to solve the task within the prompt,
    and the LLM uses the provided context and its capabilities to solve it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM doesn’t have the requisite information or skills to solve this task,
    so you update the model parameters (fine-tuning etc., as detailed in Chapters
    [6](ch06.html#llm-fine-tuning)–[8](ch08.html#ch8)) so that it is able to activate
    the skills and knowledge needed to solve it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t know a priori what context is needed to solve the task, so you use
    mechanisms to automatically fetch the relevant context and insert it into the
    prompt (passive approach).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You provide explicit instructions to the LLM on how to interact with external
    tools and data stores to solve your task, which the LLM follows (explicit approach).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM breaks the task into multiple subtasks if needed, interacts with its
    environment to gather the information/knowledge needed to solve the task, and
    delegates subtasks to external models and tools when it doesn’t have the requisite
    capabilities to solve that subtask (autonomous approach).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the last three involve the LLM interacting with its environment
    (passive, explicit, and autonomous). Let’s explore the three interaction paradigms
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Passive Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 10-1](#passive-interaction) shows the typical workflow of an application
    that involves an LLM passively interacting with a data store.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Passive Interaction](assets/dllm_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. An LLM passively interacting with a data store
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A large number of use cases involve leveraging LLMs to use your own data. Examples
    include building a question-answering assistant over your company’s internal knowledge
    base that is spread over a bunch of Notion documents, or an airline chatbot that
    responds to customer queries about flight status or booking policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow the LLM to access external information, we need two types of components:
    “data stores” that contain the required information and retrieval engines that
    can retrieve relevant data from data stores given a query. The retrieval engine
    can be powered by an LLM itself, or it can be as simple as a keyword-matching
    algorithm. The data store(s) can be a repository of data like a database, knowledge
    graph, vector database, or even just a collection of text files. Data in the data
    store is represented and indexed to make retrieval more efficient. Data representation,
    indexing, and retrieval are topics important enough to merit their own chapter:
    we will defer detailed discussions on them to [Chapter 11](ch11.html#chapter_llm_interfaces).'
  prefs: []
  type: TYPE_NORMAL
- en: When a user issues a query, the retrieval engine uses the query to find the
    documents or text segments that are most relevant to answering this query. After
    ensuring that these fit into the context window of the LLM, they are fed to the
    LLM along with the query. The LLM is expected to answer the query given the relevant
    context provided in the prompt. This approach is popularly known as RAG, although
    as we will see in [Chapter 12](ch12.html#ch12), RAG refers to an even broader
    concept. RAG is an important paradigm that deserves its own chapter, so we will
    defer detailed coverage of the paradigm to [Chapter 12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Note that the distinguishing feature of this paradigm is the passive nature
    of the LLM in the interaction. The LLM simply responds to the prompt and furnishes
    an answer. It does not know the source of the content inside the prompt. This
    paradigm is often used for building QA assistants or chatbots, where external
    information is required to understand the context of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this point forward, we will refer to user requests to the LLM as *queries*
    and textual units that are retrieved from external data stores as *documents*.
    Documents can be full documents, passages, paragraphs, or sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The Explicit Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 10-2](#explicit-approach) demonstrates the explicit approach to interface
    LLMs with external tools.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Explicit Approach](assets/dllm_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. The explicit interaction approach in action
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike in the passive approach, the LLM is no longer a passive participant.
    We provide the LLM with explicit instructions on how and when to invoke external
    data stores and tools. The LLM interacts with its environment based on a pre-programmed
    set of conditions. This approach is recommended when the interaction sequence
    is fixed, limited in scope, and preferably involves a very small number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an AI data analyst assistant, an example interaction sequence could be:'
  prefs: []
  type: TYPE_NORMAL
- en: User expresses query in natural language asking to visualize some data trends
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM generates SQL to retrieve the data needed to resolve the user query
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After receiving the data, the LLM uses it to generate code that can be run by
    a code interpreter to generate statistics or visualizations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 10-3](#ai-data-analyst) shows a fixed interaction sequence implemented
    for an AI data analyst.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ai-data-analyst](assets/dllm_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. An example workflow for an AI data analyst
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this paradigm, the interaction sequence is predetermined and rule-based.
    The LLM exercises no agency in determining which step to take next. I recommend
    this approach for building robust applications that have stricter reliability
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The Autonomous Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 10-4](#agentic-approach) shows how we can turn an LLM into an autonomous
    agent that can solve complex tasks by itself.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Agentic Approach](assets/dllm_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. A typical autonomous LLM-driven agent workflow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The autonomous approach, or the Holy Grail approach as I like to call it, turns
    an LLM into an autonomous agent that can solve tasks on its own by interacting
    with its environment. Here is a typical workflow of an autonomous agent:'
  prefs: []
  type: TYPE_NORMAL
- en: The user formulates their requirements in natural language, optionally providing
    the format in which they want the LLM to provide the answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM decomposes the user query into manageable subtasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM synchronously or asynchronously solves each subtask of the problem.
    Where possible, the LLM uses its own memory and knowledge to solve a specific
    subtask. For subtasks where the LLM cannot answer on its own, it chooses a tool
    to invoke from a list of available tools. Where possible, the LLM uses the outputs
    from solutions of already executed subtasks as inputs to other subtasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM synthesizes the final answer using the solutions of the subtasks, generating
    the output in the requested output format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This paradigm is general enough to capture just about any use case. It is also
    a risky paradigm, as we are assigning the LLM too much responsibility and agency.
    At this juncture, I would not recommend using this paradigm for any mission-critical
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why am I calling for caution in deploying agents? Humans often underestimate
    the accuracy requirements for applications. For a lot of use cases, getting it
    right 99% of the time is still not good enough, especially when the failures are
    unpredictable and the 1% of failures can be potentially catastrophic. The 99%
    problem is also the one that has long plagued self-driving cars and prevented
    their broader adoption. This doesn’t mean we can’t deploy autonomous LLM agents;
    we just need clever product design that can shield the user from their failures.
    We also need robust human-in-the-loop paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: We have used the word “agent” several times now without defining it. Let’s correct
    that and consider what agents mean and how we can build them.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the hype starts building over LLM-based agents, the colloquial definition
    of agents has already started to expand from its traditional definition. This
    is because truly agentic systems are hard to build, so there is a tendency to
    shift the goalposts and claim best-effort systems to be already agentic even though
    they technically may not fit the requirements. In this book, we will stick to
    a more conservative definition of agents, defining them as:'
  prefs: []
  type: TYPE_NORMAL
- en: LLM-driven software systems that are able to interact with their environment
    and take autonomous actions to complete a task.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Key characteristics of agents are:'
  prefs: []
  type: TYPE_NORMAL
- en: Their autonomous nature
  prefs: []
  type: TYPE_NORMAL
- en: The sequence of steps required to perform a task need not be specified to the
    agent. Agents can decide to perform any sequence of actions, unprompted by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Their ability to interact with their environment
  prefs: []
  type: TYPE_NORMAL
- en: Agents can be connected to external data sources and software tools, which allows
    agents to retrieve data, invoke tools, execute code, and provide instructions
    when appropriate to solve a task.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many definitions of “agent” do not require them to be autonomous. According
    to their definitions, applications following the explicit paradigm can also be
    called agents (albeit as non-autonomous or semi-autonomous agents).
  prefs: []
  type: TYPE_NORMAL
- en: The agentic paradigm as we defined it is extremely powerful and general. Let’s
    take a moment to appreciate it. If an agent receives a task that it doesn’t know
    how to solve (and it *knows* that it doesn’t know), then instead of just giving
    up, it can potentially learn to solve the task by itself by searching the web
    or knowledge bases for pointers, or even by collecting data and fine-tuning a
    model that can help solve the task.
  prefs: []
  type: TYPE_NORMAL
- en: Given these enviable abilities, are machines going to take over the world? In
    practice, current autonomous agents are limited in what they can actually achieve.
    They tend to get stuck in loops, they take incorrect actions, and they are unable
    to reliably self-correct. It is more practical to build partially autonomous agents,
    where the LLM is provided with guidance throughout its workflow, either through
    agent orchestration software or with a human in the loop. For the rest of this
    chapter, our focus will be on building practical agents that can reliably solve
    a narrower class of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Agentic Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using our definition of agents, let’s explore how agents work in practice.
    As an example, let’s consider an agent that is asked to answer this question:'
  prefs: []
  type: TYPE_NORMAL
- en: Who was the CFO of Apple when its stock price was at its lowest point in the
    last 10 years?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s say the agent has all the information it needs to solve this task. It
    has access to the web, to SQL databases containing stock price information, and
    to knowledge bases containing CFO tenure information. It is connected to a code
    interpreter so that it can generate and run code, and it has access to financial
    APIs. The system prompt contains details about all the tools and data stores the
    LLM has access to.
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer the given query, the LLM has to perform this sequence of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the date range, it needs the current date. If this is not included
    in the system prompt, it either searches the web to find the current date or generates
    code for returning the system time, which is then executed by a code interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the current date, it finds the other end of the date range by executing
    a simple arithmetic operation by itself, or by generating code for it. Steps 1
    and 2 could be combined into a single program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It finds a database table in the available datastore list that contains stock
    price information. It retrieves the schema of the table, inserts it into the prompt,
    and generates a SQL query for finding the date when the stock price was at its
    minimum in the last 10 years.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the date in hand, it needs to find the CFO of Apple on that date. It can
    call a search engine API to check if there is an explicit mention of the CFO on
    that particular date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the search engine query fails to provide a result, it finds a financial API
    in its tools list and retrieves and inserts the API documentation into its context.
    It then generates and invokes code for an API call to retrieve the list of Apple
    CFOs and their tenures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It uses its arithmetic reasoning skills to find the CFO tenure that matches
    the date of the lowest stock price.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It generates the final answer. If there is a requested output format, it tries
    to adhere to that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on the implementation, the sequence of steps could vary slightly.
    For example, you can fine-tune a model so that it can generate code for API calls
    or SQL queries directly without having to retrieve the schema from a data store
    or API.
  prefs: []
  type: TYPE_NORMAL
- en: To perform the given sequence of tasks, the model should first understand that
    the given task needs to be decomposed into a series of subtasks. This is called
    task decomposition. Task decomposition and planning can be performed by the LLM
    or offloaded to an external tool.
  prefs: []
  type: TYPE_NORMAL
- en: Components of an Agentic System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the specific architecture of any given agentic system depends heavily
    on the use cases it is intended to support, each of its components can be classified
    into one of the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent loop prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guardrails and verifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestration software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 10-5](#agentic-system) shows a canonical agentic system and how its
    components interact.'
  prefs: []
  type: TYPE_NORMAL
- en: '![agentic-system](assets/dllm_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. A production-grade agentic system
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s explore each of these types.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models are the backbone of agentic systems, responsible for their autonomous
    nature and problem-solving capabilities. A single agentic system could be composed
    of multiple language models, with each model playing a distinct role.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can build an agent consisting of two models; one model solves
    user tasks and another model takes its output and converts it into a structured
    form according to user requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Agentic workflows can consume a lot of language model tokens, which can be cost
    prohibitive. To keep costs under control, consider using multiple language models
    of different sizes, with the smaller (and cheaper) models performing easier tasks.
    For more details on how to accomplish division of labor among these models, see
    [Chapter 13](ch13.html#ch13).
  prefs: []
  type: TYPE_NORMAL
- en: More generally, you can build agents with specialized models catering to each
    part of the agentic workflow. For example, a code-LLM can be used to generate
    code, and task-specific fine-tuned models that specialize in individual workflow
    steps can be used. This setup can be interpreted as a *multi-agent architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-6](#multi-agent-setup) shows an agentic system made up of multiple
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![multi-agent-setup](assets/dllm_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. An agentic system with multiple LLMs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, any kind of model, including non-LLMs, can be plugged into an agentic
    system to solve specific tasks. For example, the planning stage can be performed
    using [symbolic planners](https://oreil.ly/sXPWG).
  prefs: []
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As described earlier, software or models that can be invoked by an LLM are called
    tools. Libraries like [LangChain](https://oreil.ly/35Lgu) and [LlamaIndex](https://oreil.ly/WF-d1)
    provide connectors to various software interfaces, including code interpreters,
    search engines, databases, ML models, and a variety of APIs. Let’s explore how
    to work with some of these in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Web search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LangChain provides connectors for major search engines like Google, Bing, and
    DuckDuckGo. Let’s try out DuckDuckGo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The response can be fed back to the language model where it is further processed.
  prefs: []
  type: TYPE_NORMAL
- en: API connectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To illustrate calling APIs, we will showcase LangChain’s Wikipedia API wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `load()` function runs a search on Wikipedia and returns the page text and
    metadata information of the top-k results. (top-k = 3 by default). You can also
    use the `run()` function to return only page summaries of the top-k matches.
  prefs: []
  type: TYPE_NORMAL
- en: Code interpreter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, let’s explore how you can invoke a code interpreter and run arbitrary
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be wary of running code generated by LLMs in response to user prompts. Users
    can induce the model to generate malicious code!
  prefs: []
  type: TYPE_NORMAL
- en: Database connectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s check out how to connect to a database and run queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `run()` function executes the provided SQL query and returns the response
    as a string. Replace `*DATABASE_URI*` with your own database and queries, and
    verify the responses.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For more customizability, you can fork the LangChain connectors and repurpose
    them for your own use.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see how we can interface LLMs with these tools in an agentic workflow.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to make the LLM aware that it has access to these tools. One
    of the ways to achieve this is to provide the names and short descriptions of
    the tools, called the *tool list*, to the LLM through the system prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the LLM needs to be able to select the right tool at the appropriate juncture
    in the workflow. For example, if the next step in solving a task is to find the
    weather in Chicago this evening, the web search tool has to be invoked rather
    than the Wikipedia one. Later in this chapter, we will discuss techniques to help
    the LLM select the right tool.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, tool invocation is typically achieved by the LLM generating
    special tokens indicating that it is entering tool invocation mode, along with
    tokens representing the tool functions and arguments to be invoked. The actual
    tool invocation is performed by an agent orchestration framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'In LangChain, we can make a tool available to an LLM and have it invoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Some models come with native tool-calling abilities. For models that don’t,
    you can fine-tune the base model to impart them with tool-calling abilities. Among
    open models, Llama 3.1 Instruct (8B/70B/405B) is an example of a model having
    native tool-calling support. Here’s how tool calling works with Llama 3.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama 3.1 comes with native support for three tools: Brave web search, Wolfram|Alpha
    mathematical engine, and a code interpreter. These can be *activated* by defining
    them in the system prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s ask the LLM a question by appending a user prompt to the system prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Llama 3.1 responds with a tool invocation that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]` The `<|python_tag|>` token is a special token generated by Llama 3.1
    to indicate that it is entering tool-calling mode. The `<|eom_id|>` special token
    indicates that the model has not ended its turn yet and will wait to be fed with
    the results of the tool invocation.    You can also provide your own tools in
    the prompt: using JSON is recommended.    ###### Tip    If you have a lot of tools,
    then the detailed descriptions of the tools can be represented in a data store
    and retrieved only if they are selected. The prompt then needs to contain only
    the name of the tool and a short description.    Here is an example of a tool
    definition in JSON describing a local function that can be called:    [PRE9] `}`         `},`         `"required"``:`
    `[``"claim_sentence"``,` `"model"``]`     `}`     `}` `}` [PRE10]   `` `The tool
    call is generated by the model in JSON with the prescribed format.    ###### Note    The
    actual tool invocation is performed by an agent orchestration software. Llama
    3.1 comes with [llama-stack-apps](https://oreil.ly/SSmkI), a library that facilitates
    agentic workflows.    Sometimes the tool call can be more complex than just returning
    the name of a function and its arguments. An example of this is querying a database.
    For the LLM to generate the right SQL query, you should provide the schema of
    the database tables in the system prompt. If the database has too many tables,
    then their schema can be retrieved on demand by the LLM.    ###### Tip    You
    can use a separate specialized model for code and SQL query generation. A general-purpose
    model can generate a textual description of the desired outcome, and this can
    be used as input to a code LLM or an LLM fine-tuned on text-to-SQL.    For large-scale
    or high-stakes applications, you can fine-tune your models to make them better
    at tool use. A good fine-tuning recipe to follow is Qin et al.’s [ToolLLaMA](https://oreil.ly/Ewlxt).`
    `` [PRE11]``  [PRE12] [PRE13]`py  [PRE14]'
  prefs: []
  type: TYPE_NORMAL
