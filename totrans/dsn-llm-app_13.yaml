- en: Chapter 10\. Interfacing LLMs with External Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first two parts of the book, we have seen how impactful standalone LLMs
    can be in solving a wide variety of tasks. To effectively harness their full range
    of capabilities in an organization, they have to be integrated into the existing
    data and software ecosystem. Unlike traditional software systems, LLMs can generate
    autonomous actions to interact with other ecosystem components, bringing a degree
    of flexibility never seen before in the software world. This flexibility unlocks
    a whole host of use cases that were previously considered impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another reason we need LLMs to interact with software and external data: as
    we know all too well, current LLMs have significant limitations, some of which
    we discussed in [Chapter 1](ch01.html#chapter_llm-introduction). To recap some
    key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Since it is expensive to retrain LLMs or keep them continuously updated, they
    have a knowledge cutoff date and thus possess no knowledge of more recent events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though they are getting better over time, LLMs don’t always get math right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can’t provide factuality guarantees or accurately cite the sources of their
    outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding them your own data effectively is a challenge; fine-tuning is nontrivial,
    and in-context learning is limited by the length of the effective context window.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have been noticing throughout the book, the consolidation effect is leading
    us to a future (unless we hit a technological wall) where many of the aforementioned
    limitations might be addressed within the model itself. But we don’t necessarily
    need to wait for that moment to arrive, as many of these limitations can be addressed
    today by offloading the tasks and subtasks to external tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will define the three canonical LLM interaction paradigms
    and provide guidance on how to choose between them for your application. Broadly
    speaking, there are two types of external entities that LLMs need to interact
    with: data stores and software/models, collectively called tools. We will demonstrate
    how to interface LLMs with various tools like APIs and code interpreters. We will
    show how to make the best use of libraries like LangChain and LlamaIndex, which
    have vastly simplified LLM integrations. We will explore the various scaffolding
    software that needs to be constructed to facilitate seamless interactions with
    the environment. We will also push the limits of what today’s LLMs are capable
    of, by demonstrating how they can be deployed as an agent that can make autonomous
    decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: LLM Interaction Paradigms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you have a task you want the LLM to solve. There are several possible
    options:'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM uses its own memory and capabilities encoded in its parameters to solve
    the task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You feed the LLM all the context it needs to solve the task within the prompt,
    and the LLM uses the provided context and its capabilities to solve it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM doesn’t have the requisite information or skills to solve this task,
    so you update the model parameters (fine-tuning etc., as detailed in Chapters
    [6](ch06.html#llm-fine-tuning)–[8](ch08.html#ch8)) so that it is able to activate
    the skills and knowledge needed to solve it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t know a priori what context is needed to solve the task, so you use
    mechanisms to automatically fetch the relevant context and insert it into the
    prompt (passive approach).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You provide explicit instructions to the LLM on how to interact with external
    tools and data stores to solve your task, which the LLM follows (explicit approach).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM breaks the task into multiple subtasks if needed, interacts with its
    environment to gather the information/knowledge needed to solve the task, and
    delegates subtasks to external models and tools when it doesn’t have the requisite
    capabilities to solve that subtask (autonomous approach).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the last three involve the LLM interacting with its environment
    (passive, explicit, and autonomous). Let’s explore the three interaction paradigms
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Passive Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 10-1](#passive-interaction) shows the typical workflow of an application
    that involves an LLM passively interacting with a data store.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Passive Interaction](assets/dllm_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. An LLM passively interacting with a data store
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A large number of use cases involve leveraging LLMs to use your own data. Examples
    include building a question-answering assistant over your company’s internal knowledge
    base that is spread over a bunch of Notion documents, or an airline chatbot that
    responds to customer queries about flight status or booking policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow the LLM to access external information, we need two types of components:
    “data stores” that contain the required information and retrieval engines that
    can retrieve relevant data from data stores given a query. The retrieval engine
    can be powered by an LLM itself, or it can be as simple as a keyword-matching
    algorithm. The data store(s) can be a repository of data like a database, knowledge
    graph, vector database, or even just a collection of text files. Data in the data
    store is represented and indexed to make retrieval more efficient. Data representation,
    indexing, and retrieval are topics important enough to merit their own chapter:
    we will defer detailed discussions on them to [Chapter 11](ch11.html#chapter_llm_interfaces).'
  prefs: []
  type: TYPE_NORMAL
- en: When a user issues a query, the retrieval engine uses the query to find the
    documents or text segments that are most relevant to answering this query. After
    ensuring that these fit into the context window of the LLM, they are fed to the
    LLM along with the query. The LLM is expected to answer the query given the relevant
    context provided in the prompt. This approach is popularly known as RAG, although
    as we will see in [Chapter 12](ch12.html#ch12), RAG refers to an even broader
    concept. RAG is an important paradigm that deserves its own chapter, so we will
    defer detailed coverage of the paradigm to [Chapter 12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Note that the distinguishing feature of this paradigm is the passive nature
    of the LLM in the interaction. The LLM simply responds to the prompt and furnishes
    an answer. It does not know the source of the content inside the prompt. This
    paradigm is often used for building QA assistants or chatbots, where external
    information is required to understand the context of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this point forward, we will refer to user requests to the LLM as *queries*
    and textual units that are retrieved from external data stores as *documents*.
    Documents can be full documents, passages, paragraphs, or sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The Explicit Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 10-2](#explicit-approach) demonstrates the explicit approach to interface
    LLMs with external tools.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Explicit Approach](assets/dllm_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. The explicit interaction approach in action
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike in the passive approach, the LLM is no longer a passive participant.
    We provide the LLM with explicit instructions on how and when to invoke external
    data stores and tools. The LLM interacts with its environment based on a pre-programmed
    set of conditions. This approach is recommended when the interaction sequence
    is fixed, limited in scope, and preferably involves a very small number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an AI data analyst assistant, an example interaction sequence could be:'
  prefs: []
  type: TYPE_NORMAL
- en: User expresses query in natural language asking to visualize some data trends
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM generates SQL to retrieve the data needed to resolve the user query
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After receiving the data, the LLM uses it to generate code that can be run by
    a code interpreter to generate statistics or visualizations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 10-3](#ai-data-analyst) shows a fixed interaction sequence implemented
    for an AI data analyst.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ai-data-analyst](assets/dllm_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. An example workflow for an AI data analyst
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this paradigm, the interaction sequence is predetermined and rule-based.
    The LLM exercises no agency in determining which step to take next. I recommend
    this approach for building robust applications that have stricter reliability
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The Autonomous Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 10-4](#agentic-approach) shows how we can turn an LLM into an autonomous
    agent that can solve complex tasks by itself.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Agentic Approach](assets/dllm_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. A typical autonomous LLM-driven agent workflow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The autonomous approach, or the Holy Grail approach as I like to call it, turns
    an LLM into an autonomous agent that can solve tasks on its own by interacting
    with its environment. Here is a typical workflow of an autonomous agent:'
  prefs: []
  type: TYPE_NORMAL
- en: The user formulates their requirements in natural language, optionally providing
    the format in which they want the LLM to provide the answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM decomposes the user query into manageable subtasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM synchronously or asynchronously solves each subtask of the problem.
    Where possible, the LLM uses its own memory and knowledge to solve a specific
    subtask. For subtasks where the LLM cannot answer on its own, it chooses a tool
    to invoke from a list of available tools. Where possible, the LLM uses the outputs
    from solutions of already executed subtasks as inputs to other subtasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM synthesizes the final answer using the solutions of the subtasks, generating
    the output in the requested output format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This paradigm is general enough to capture just about any use case. It is also
    a risky paradigm, as we are assigning the LLM too much responsibility and agency.
    At this juncture, I would not recommend using this paradigm for any mission-critical
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why am I calling for caution in deploying agents? Humans often underestimate
    the accuracy requirements for applications. For a lot of use cases, getting it
    right 99% of the time is still not good enough, especially when the failures are
    unpredictable and the 1% of failures can be potentially catastrophic. The 99%
    problem is also the one that has long plagued self-driving cars and prevented
    their broader adoption. This doesn’t mean we can’t deploy autonomous LLM agents;
    we just need clever product design that can shield the user from their failures.
    We also need robust human-in-the-loop paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: We have used the word “agent” several times now without defining it. Let’s correct
    that and consider what agents mean and how we can build them.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the hype starts building over LLM-based agents, the colloquial definition
    of agents has already started to expand from its traditional definition. This
    is because truly agentic systems are hard to build, so there is a tendency to
    shift the goalposts and claim best-effort systems to be already agentic even though
    they technically may not fit the requirements. In this book, we will stick to
    a more conservative definition of agents, defining them as:'
  prefs: []
  type: TYPE_NORMAL
- en: LLM-driven software systems that are able to interact with their environment
    and take autonomous actions to complete a task.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Key characteristics of agents are:'
  prefs: []
  type: TYPE_NORMAL
- en: Their autonomous nature
  prefs: []
  type: TYPE_NORMAL
- en: The sequence of steps required to perform a task need not be specified to the
    agent. Agents can decide to perform any sequence of actions, unprompted by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Their ability to interact with their environment
  prefs: []
  type: TYPE_NORMAL
- en: Agents can be connected to external data sources and software tools, which allows
    agents to retrieve data, invoke tools, execute code, and provide instructions
    when appropriate to solve a task.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many definitions of “agent” do not require them to be autonomous. According
    to their definitions, applications following the explicit paradigm can also be
    called agents (albeit as non-autonomous or semi-autonomous agents).
  prefs: []
  type: TYPE_NORMAL
- en: The agentic paradigm as we defined it is extremely powerful and general. Let’s
    take a moment to appreciate it. If an agent receives a task that it doesn’t know
    how to solve (and it *knows* that it doesn’t know), then instead of just giving
    up, it can potentially learn to solve the task by itself by searching the web
    or knowledge bases for pointers, or even by collecting data and fine-tuning a
    model that can help solve the task.
  prefs: []
  type: TYPE_NORMAL
- en: Given these enviable abilities, are machines going to take over the world? In
    practice, current autonomous agents are limited in what they can actually achieve.
    They tend to get stuck in loops, they take incorrect actions, and they are unable
    to reliably self-correct. It is more practical to build partially autonomous agents,
    where the LLM is provided with guidance throughout its workflow, either through
    agent orchestration software or with a human in the loop. For the rest of this
    chapter, our focus will be on building practical agents that can reliably solve
    a narrower class of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Agentic Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using our definition of agents, let’s explore how agents work in practice.
    As an example, let’s consider an agent that is asked to answer this question:'
  prefs: []
  type: TYPE_NORMAL
- en: Who was the CFO of Apple when its stock price was at its lowest point in the
    last 10 years?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s say the agent has all the information it needs to solve this task. It
    has access to the web, to SQL databases containing stock price information, and
    to knowledge bases containing CFO tenure information. It is connected to a code
    interpreter so that it can generate and run code, and it has access to financial
    APIs. The system prompt contains details about all the tools and data stores the
    LLM has access to.
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer the given query, the LLM has to perform this sequence of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the date range, it needs the current date. If this is not included
    in the system prompt, it either searches the web to find the current date or generates
    code for returning the system time, which is then executed by a code interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the current date, it finds the other end of the date range by executing
    a simple arithmetic operation by itself, or by generating code for it. Steps 1
    and 2 could be combined into a single program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It finds a database table in the available datastore list that contains stock
    price information. It retrieves the schema of the table, inserts it into the prompt,
    and generates a SQL query for finding the date when the stock price was at its
    minimum in the last 10 years.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the date in hand, it needs to find the CFO of Apple on that date. It can
    call a search engine API to check if there is an explicit mention of the CFO on
    that particular date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the search engine query fails to provide a result, it finds a financial API
    in its tools list and retrieves and inserts the API documentation into its context.
    It then generates and invokes code for an API call to retrieve the list of Apple
    CFOs and their tenures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It uses its arithmetic reasoning skills to find the CFO tenure that matches
    the date of the lowest stock price.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It generates the final answer. If there is a requested output format, it tries
    to adhere to that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on the implementation, the sequence of steps could vary slightly.
    For example, you can fine-tune a model so that it can generate code for API calls
    or SQL queries directly without having to retrieve the schema from a data store
    or API.
  prefs: []
  type: TYPE_NORMAL
- en: To perform the given sequence of tasks, the model should first understand that
    the given task needs to be decomposed into a series of subtasks. This is called
    task decomposition. Task decomposition and planning can be performed by the LLM
    or offloaded to an external tool.
  prefs: []
  type: TYPE_NORMAL
- en: Components of an Agentic System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the specific architecture of any given agentic system depends heavily
    on the use cases it is intended to support, each of its components can be classified
    into one of the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent loop prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guardrails and verifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestration software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Figure 10-5](#agentic-system) shows a canonical agentic system and how its
    components interact.'
  prefs: []
  type: TYPE_NORMAL
- en: '![agentic-system](assets/dllm_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. A production-grade agentic system
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s explore each of these types.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models are the backbone of agentic systems, responsible for their autonomous
    nature and problem-solving capabilities. A single agentic system could be composed
    of multiple language models, with each model playing a distinct role.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can build an agent consisting of two models; one model solves
    user tasks and another model takes its output and converts it into a structured
    form according to user requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Agentic workflows can consume a lot of language model tokens, which can be cost
    prohibitive. To keep costs under control, consider using multiple language models
    of different sizes, with the smaller (and cheaper) models performing easier tasks.
    For more details on how to accomplish division of labor among these models, see
    [Chapter 13](ch13.html#ch13).
  prefs: []
  type: TYPE_NORMAL
- en: More generally, you can build agents with specialized models catering to each
    part of the agentic workflow. For example, a code-LLM can be used to generate
    code, and task-specific fine-tuned models that specialize in individual workflow
    steps can be used. This setup can be interpreted as a *multi-agent architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-6](#multi-agent-setup) shows an agentic system made up of multiple
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![multi-agent-setup](assets/dllm_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. An agentic system with multiple LLMs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, any kind of model, including non-LLMs, can be plugged into an agentic
    system to solve specific tasks. For example, the planning stage can be performed
    using [symbolic planners](https://oreil.ly/sXPWG).
  prefs: []
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As described earlier, software or models that can be invoked by an LLM are called
    tools. Libraries like [LangChain](https://oreil.ly/35Lgu) and [LlamaIndex](https://oreil.ly/WF-d1)
    provide connectors to various software interfaces, including code interpreters,
    search engines, databases, ML models, and a variety of APIs. Let’s explore how
    to work with some of these in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Web search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LangChain provides connectors for major search engines like Google, Bing, and
    DuckDuckGo. Let’s try out DuckDuckGo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The response can be fed back to the language model where it is further processed.
  prefs: []
  type: TYPE_NORMAL
- en: API connectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To illustrate calling APIs, we will showcase LangChain’s Wikipedia API wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `load()` function runs a search on Wikipedia and returns the page text and
    metadata information of the top-k results. (top-k = 3 by default). You can also
    use the `run()` function to return only page summaries of the top-k matches.
  prefs: []
  type: TYPE_NORMAL
- en: Code interpreter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, let’s explore how you can invoke a code interpreter and run arbitrary
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be wary of running code generated by LLMs in response to user prompts. Users
    can induce the model to generate malicious code!
  prefs: []
  type: TYPE_NORMAL
- en: Database connectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s check out how to connect to a database and run queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `run()` function executes the provided SQL query and returns the response
    as a string. Replace `*DATABASE_URI*` with your own database and queries, and
    verify the responses.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For more customizability, you can fork the LangChain connectors and repurpose
    them for your own use.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see how we can interface LLMs with these tools in an agentic workflow.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to make the LLM aware that it has access to these tools. One
    of the ways to achieve this is to provide the names and short descriptions of
    the tools, called the *tool list*, to the LLM through the system prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the LLM needs to be able to select the right tool at the appropriate juncture
    in the workflow. For example, if the next step in solving a task is to find the
    weather in Chicago this evening, the web search tool has to be invoked rather
    than the Wikipedia one. Later in this chapter, we will discuss techniques to help
    the LLM select the right tool.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, tool invocation is typically achieved by the LLM generating
    special tokens indicating that it is entering tool invocation mode, along with
    tokens representing the tool functions and arguments to be invoked. The actual
    tool invocation is performed by an agent orchestration framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'In LangChain, we can make a tool available to an LLM and have it invoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Some models come with native tool-calling abilities. For models that don’t,
    you can fine-tune the base model to impart them with tool-calling abilities. Among
    open models, Llama 3.1 Instruct (8B/70B/405B) is an example of a model having
    native tool-calling support. Here’s how tool calling works with Llama 3.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama 3.1 comes with native support for three tools: Brave web search, Wolfram|Alpha
    mathematical engine, and a code interpreter. These can be *activated* by defining
    them in the system prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s ask the LLM a question by appending a user prompt to the system prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Llama 3.1 responds with a tool invocation that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `<|python_tag|>` token is a special token generated by Llama 3.1 to indicate
    that it is entering tool-calling mode. The `<|eom_id|>` special token indicates
    that the model has not ended its turn yet and will wait to be fed with the results
    of the tool invocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also provide your own tools in the prompt: using JSON is recommended.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you have a lot of tools, then the detailed descriptions of the tools can
    be represented in a data store and retrieved only if they are selected. The prompt
    then needs to contain only the name of the tool and a short description.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a tool definition in JSON describing a local function
    that can be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The tool call is generated by the model in JSON with the prescribed format.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The actual tool invocation is performed by an agent orchestration software.
    Llama 3.1 comes with [llama-stack-apps](https://oreil.ly/SSmkI), a library that
    facilitates agentic workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the tool call can be more complex than just returning the name of
    a function and its arguments. An example of this is querying a database. For the
    LLM to generate the right SQL query, you should provide the schema of the database
    tables in the system prompt. If the database has too many tables, then their schema
    can be retrieved on demand by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can use a separate specialized model for code and SQL query generation.
    A general-purpose model can generate a textual description of the desired outcome,
    and this can be used as input to a code LLM or an LLM fine-tuned on text-to-SQL.
  prefs: []
  type: TYPE_NORMAL
- en: For large-scale or high-stakes applications, you can fine-tune your models to
    make them better at tool use. A good fine-tuning recipe to follow is Qin et al.’s
    [ToolLLaMA](https://oreil.ly/Ewlxt).
  prefs: []
  type: TYPE_NORMAL
- en: Data Stores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A typical agent may need to interact with several types of data sources to accomplish
    its tasks. Commonly used data sources include prompt repositories, session memory,
    and tools data.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt repository
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A prompt repository is a collection of detailed prompts instructing the language
    model how to perform a specific task. If you can anticipate the types of tasks
    that an agent will be asked to perform while in production, you can construct
    prompts providing detailed instructions on how to solve them. The prompts can
    even include directions on how to advance a specific workflow. Let’s look at an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many language models struggle with basic arithmetic operations, even simple
    questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Until recently, even state-of-the-art language models claimed that 9.11 is greater
    than 9.9\. (They were recently updated with a fix after this limitation went viral
    on [social media](https://oreil.ly/ztWGW).)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are aware of such limitations that are relevant to your use case, then
    you can mitigate a proportion of them using detailed prompts. For the number comparison
    issue, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* If you are asked to compare two numbers using the greater than/lesser
    than operation, then perform the following:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Take the two numbers and ensure they have the same number of decimal places.
    After that, subtract one from the other. If the result is a positive number, then
    the first number is greater. If the result is a negative number, then the second
    number is greater. If the result is zero, the two numbers are equal.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, if the agent needs to perform a task that includes number comparison, it
    first retrieves this prompt from the prompt repository. This enables it to overcome
    its inherent limitation, as it will follow the detailed step-by-step instructions
    in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why don’t we just add all these prompts to the context window, thus eschewing
    retrieval? For one, the prompts may be too numerous and may not fit within the
    context window. Secondly, tokens are expensive, and it is inefficient to include
    prompts that may not be relevant to the current task. Finally, language models
    can adhere to only a limited set of concurrent instructions, so it is more efficient
    to retrieve them on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts can also include input-output examples, known as few-shot learning,
    as introduced in [Chapter 1](ch01.html#chapter_llm-introduction). Agents can retrieve
    them on demand to help accomplish their tasks. We will discuss effective ways
    of retrieving relevant examples in [Chapter 12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore how agents use session memory to advance their goals.
  prefs: []
  type: TYPE_NORMAL
- en: Session memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We would like to store logs of the steps undertaken by the agent during current
    and past sessions. These logs can also be augmented with feedback from verification
    processes, both automated and human. The agent can then retrieve session logs
    for better context understanding and as a guide to determine next actions in the
    current workflow. Let’s explore this with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* Is it going to be warmer tomorrow than today in Paris? Give me details
    in Celsius.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The agent goes through a sequence of steps to solve this query, which is all
    logged in the session history. The session history for this query might look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*User prompt*: Is it going to be warmer tomorrow than today in Paris? Give
    me details in Celsius.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*LLM observation*: The user has a preference for Celsius units for temperature
    data.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*LLM Output*: Let’s perform this step-by-step.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First, let’s retrieve today’s temperature.
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, let’s retrieve tomorrow’s temperature.
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: We calculate the difference and generate the answer.
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*External verifier*: Task decomposition is correct'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Tool selection:* <WeatherAPI> selected'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Tool info retrieval*: Retrieve information relevant to the query *Find temperature
    data from <WeatherAPI>*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Tool invocation*: get_temperature_curweek(*Paris*)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Tool output* : Weather data - Paris…​'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*LLM output*: Today’s temperature is 27 degrees Celsius. Tomorrow’s forecast
    is 23 degrees Celsius. Therefore, tomorrow is not going to be warmer in Paris
    than today.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*External Verifier*: Arithmetic operation is correct.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Agent*: LLM output is dispatched to the user'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*User feedback*: User marked this as correct'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we can see, session history can contain very rich information that can provide
    valuable personalized context to the LLM about the current user as well as guide
    the model toward the correct agentic workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In more advanced implementations, multiple levels of logging can be defined,
    so that during retrieval, one can retrieve all the logs of a session or only the
    important steps, based on the logging level specified.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Along with session history, the agent could also be provided with access to
    gold-truth training examples representing correct workflows, which can be used
    by the agent to guide its trajectory during test time.
  prefs: []
  type: TYPE_NORMAL
- en: Session memory can also include records of interaction between the human and
    the agentic system. These can be used to personalize models. We will discuss this
    further in [Chapter 12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore how the agent can interact with tools data.
  prefs: []
  type: TYPE_NORMAL
- en: Tools data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tools data comprise detailed information necessary to invoke a tool, such as
    database schemas, API documentation, sample API calls, and more. When the agent
    decides to invoke a tool, the model retrieves the pertinent tool information from
    the tools data store.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a SQL tool for retrieving data from a database. To generate
    the right SQL query, the model could retrieve the database schema from the tools
    data store. The tools data contains information about the tables and columns,
    the descriptions of each column and their data types, and optionally information
    about indices and primary/secondary keys.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also fine-tune the LLM on a dataset representing valid SQL queries to
    your database, which can potentially remove the need to consult the schema before
    generating a query.
  prefs: []
  type: TYPE_NORMAL
- en: To sum it up, agents can use data stores in several ways. They can access prompts
    and few-shot examples from a prompt repository, they can access agentic workflow
    history and intermediate outputs by models in previous sessions for better personalized
    context understanding and workflow guidance, and they can access tool documentation
    to invoke tools correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Agents can also access external knowledge from the web, databases, knowledge
    graphs, etc. Retrieving the right information from these sources is an entire
    sub-system unto itself. We will discuss the mechanics of retrieval in Chapters
    [11](ch11.html#chapter_llm_interfaces) and [12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss the agent loop prompt, which is responsible for driving
    the LLM’s behavior during an agentic session.
  prefs: []
  type: TYPE_NORMAL
- en: Agent Loop Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that LLMs do not have session memory. But a typical agentic workflow
    relies on several LLM calls! We need a mechanism to provide information about
    session state and the expected role of the LLM at any given time in the session.
    This agent loop is driven by a system prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a simple agent loop system prompt is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* You are an AI model currently answering questions. You have access
    to the following tools: {tool_description}. For each question, you can invoke
    one or more tools where necessary to access information or execute actions. You
    can invoke a tool in this format: <TOOLNAME> <Tool Arguments>. The results of
    these tool calls are not provided to the user. When you are ready with the final
    answer, output the answer using the <Answer> tag.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I find that a prompt like this is sufficient for most use cases. However, if
    you feel like the model is not reasoning correctly, you can try ReAct prompting.
  prefs: []
  type: TYPE_NORMAL
- en: ReAct
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the time of this writing, ReAct (Reasoning + Acting) prompting is the most
    popular prompt for the agent loop. A typical ReAct prompt looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* You are an AI assistant capable of reasoning and acting. For each
    question, follow this process:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Thought: Reflect on the current state and plan your next steps.'
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Action: Execute the steps to gather information or call tools.'
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Observation: Record the results of your actions.'
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Final Answer: If you have an answer, provide a final response. Else continue
    the Thought → Action → Observation → loop until you have an answer.'
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: Despite its popularity, ReAct prompting has been shown to be [brittle](https://oreil.ly/RRZO9).
  prefs: []
  type: TYPE_NORMAL
- en: Reflection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The agent loop may include self-verification or correction steps. This was pioneered
    by [Shinn et al.](https://oreil.ly/xFVt0) with the Reflexion paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the system prompt for [Reflection-Llama-3.1](https://oreil.ly/foB-P)
    that uses reflection techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* You are a world-class AI system, capable of complex reasoning and
    reflection. Reason through the query inside <thinking> tags, and then provide
    your final response inside <output> tags. If you detect that you made a mistake
    in your reasoning at any point, correct yourself inside <reflection> tags.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The <reflection> tags are meant for the model to self-introspect and self-correct.
    We can also specify conditions when <reflection> tags should be activated, for
    example, when the agent performs the same action consecutively more than three
    times (which might mean it is stuck in a loop).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The effectiveness of reflection-based methods are overstated. They might do
    more harm than good if they are invoked too often, causing the model to second-guess
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss guardrails and verifiers, components that ensure that an
    agentic system can thrive in production.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails and Verifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In production environments, mistakes can be catastrophic. Depending on the use
    case, the agent might need to adhere to strict standards in factuality, safety,
    accuracy, and many other criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Safety is ensured by using guardrails, components that ensure models do not
    overstep their bounds during the course of their workflows. Some examples of guardrails
    include toxic language detectors, personally identifiable information (PII) detectors,
    input filters that restrict the type of queries users are permitted to make, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Verifiers ensure that quality standards of the agentic system are so that the
    agent is able to recover and self-correct from mistakes. As agentic systems are
    still in their infancy, the importance of good and well-placed verifiers is paramount.
    Verifiers can be as simple as token-matching tools but can also be fine-tuned
    models, symbolic verifiers, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn more about guardrails and verifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Safety Guardrails
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall from [Chapter 2](ch02.html#ch02) that LLMs are trained largely on human-generated
    web text. Unfortunately a significant proportion of human-generated text contains
    toxic, abusive, violent, or pornographic content. We do not want our LLM applications
    to generate content that violates the safety of the user, nor do we want users
    to misuse the model to generate unsafe content. While we can certainly use techniques
    like alignment training to make the model less likely to emit harmful content,
    we cannot guarantee 100% success and therefore need to institute inference-time
    guardrails to ensure safe usage. Libraries like [Guardrails](https://oreil.ly/F7yax)
    and NVIDIA’s [NeMo-Guardrails](https://oreil.ly/p7Dqz), and models like [Llama
    Guard](https://oreil.ly/8S08P) facilitate setting up these guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Guardrails library provides a large (and growing) number of data validators
    to ensure safety and validity of LLM inputs and outputs. Here are some important
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Detect PII
  prefs: []
  type: TYPE_NORMAL
- en: This validator can be used to detect personally identifiable information in
    both the input and output text. [Microsoft Presidio](https://oreil.ly/eG8T1) is
    employed under the hood to perform the PII identification.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt injection
  prefs: []
  type: TYPE_NORMAL
- en: This validator can detect certain types of adversarial prompting and thus can
    be used to prevent users from misusing the LLM. The [Rebuff](https://oreil.ly/nIyE5)
    library is used under the hood to detect prompt injection.
  prefs: []
  type: TYPE_NORMAL
- en: Not safe for work (NSFW) text
  prefs: []
  type: TYPE_NORMAL
- en: This validator detects NSFW text in the LLM output. This includes text with
    profanity, violence, and sexual content. The *Profanity free* validator also exists
    for detecting only profanity in text.
  prefs: []
  type: TYPE_NORMAL
- en: Politeness check
  prefs: []
  type: TYPE_NORMAL
- en: This validator checks if the LLM output text is sufficiently polite. A related
    validator is *Toxic language*.
  prefs: []
  type: TYPE_NORMAL
- en: Web sanitization
  prefs: []
  type: TYPE_NORMAL
- en: This validator checks the LLM output for any security vulnerabilities, including
    if it contains code that can be executed in a browser. The [Bleach](https://oreil.ly/r3Xrl)
    library is used under the hood to find potential vulnerabilities and sanitize
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens if the validation checks fail and there is indeed harmful content
    in the input or output? Guardrails provides a few options:'
  prefs: []
  type: TYPE_NORMAL
- en: Re-ask
  prefs: []
  type: TYPE_NORMAL
- en: In this method, the LLM is asked to regenerate the output, with the prompt containing
    instructions to specifically abide by the criteria on which the output previously
    failed validation.
  prefs: []
  type: TYPE_NORMAL
- en: Fix
  prefs: []
  type: TYPE_NORMAL
- en: In this method, the library fixes the output by itself without asking the LLM
    for a regeneration. Fixes can involve deletion or replacement of certain parts
    of the input or output.
  prefs: []
  type: TYPE_NORMAL
- en: Filter
  prefs: []
  type: TYPE_NORMAL
- en: If structured data generation is used, this option enables filtering out only
    the attribute for which the validation failed. The rest of the output will be
    fed back to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Refrain
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, the output is simply not returned to the user, and the user
    receives a refusal.
  prefs: []
  type: TYPE_NORMAL
- en: Noop
  prefs: []
  type: TYPE_NORMAL
- en: No action is taken, but the validation failure is logged for further inspection.
  prefs: []
  type: TYPE_NORMAL
- en: Exception
  prefs: []
  type: TYPE_NORMAL
- en: This raises a software exception when the validation fails. Exception handlers
    can be written to activate custom behavior.
  prefs: []
  type: TYPE_NORMAL
- en: fix_reask
  prefs: []
  type: TYPE_NORMAL
- en: In this method, the library tries to fix the output by itself and then runs
    validation on the new output. If the validation still fails, then the LLM is asked
    to regenerate the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the PII guardrail as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s look at how verification modules work.
  prefs: []
  type: TYPE_NORMAL
- en: Verification modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen throughout the book, current LLMs suffer from problems like
    reasoning limitations and hallucinations that severely limit their robustness.
    However, production-ready applications need to demonstrate a certain level of
    reliability to be accepted by users. One way to extend the reliability of LLM-based
    systems is to use a human-in-the-loop who can manually verify the output and provide
    feedback. However, in the real world a human-in-the-loop is not always desired
    or feasible. The most popular alternative is to use external verification modules
    as part of the LLM system. These modules can range from rule-based programs to
    smaller fine-tuned LLMs to symbolic solvers. There are also efforts to use LLMs
    as verifiers, called “LLM-as-a-judge.”
  prefs: []
  type: TYPE_NORMAL
- en: Related components include fallback modules. These modules are activated when
    the verification process fails and retrying/fixing doesn’t work. Fallback modules
    can be as simple as messages like, “I am sorry I cannot entertain your request”
    to more complex workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss an example. Consider an abstractive summarization application
    that operates on financial documents. To ensure quality and reliability of the
    generated summaries, we need to embed verification and self-fixing into the system
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: How do we verify the quality of an abstractive summary? While single-number
    metrics are available to automatically quantify the quality of a summary, a more
    holistic approach would be to define a list of criteria that a good summary should
    satisfy and verify whether each criterion is fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Several single-number quantitative metrics exist for evaluating summaries. These
    include metrics like [BLEU, ROUGE](https://oreil.ly/LPlFJ), and [BERTScore](https://oreil.ly/gsOGl).
    BLEU and ROUGE rely on token overlap heuristics and have been shown to be [woefully
    inadequate](https://oreil.ly/rSzbR). Techniques like BERTScore that apply semantic
    similarity have been shown to be more promising, but in the end, the reality is
    that summaries have subjective notions of quality and need a more holistic approach
    for verification.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the summarization of financial documents application, here is a list of
    important criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Factuality
  prefs: []
  type: TYPE_NORMAL
- en: The summary is factually correct and does not make incorrect assumptions or
    conclusions from the source text.
  prefs: []
  type: TYPE_NORMAL
- en: Specificity
  prefs: []
  type: TYPE_NORMAL
- en: The summary doesn’t *oversummarize*; it avoids being generic and provides specific
    details, whether numbers or named entities.
  prefs: []
  type: TYPE_NORMAL
- en: Relevance
  prefs: []
  type: TYPE_NORMAL
- en: Also called precision, this is calculated as the percentage of sentences in
    the summary that are deemed relevant and thus merit inclusion in the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  prefs: []
  type: TYPE_NORMAL
- en: Also called recall, this is calculated as the percentage of relevant items in
    the source document that are included in the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Repetitiveness
  prefs: []
  type: TYPE_NORMAL
- en: The summary should not be repetitive, even if there is repetition in the source
    document.
  prefs: []
  type: TYPE_NORMAL
- en: Coherence
  prefs: []
  type: TYPE_NORMAL
- en: When read in full, the summary should provide a clear picture of the content
    in the source document, while minimizing ambiguity. This is one of the list’s
    more subjective criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Structure
  prefs: []
  type: TYPE_NORMAL
- en: While defining the summarization task, we might specify a structure for the
    summaries. For example, the summary could be expected to contain some predefined
    sections and subsections. The generated summary should follow the specified structure.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting
  prefs: []
  type: TYPE_NORMAL
- en: The generated summary should follow proper formatting. For example, if the summary
    is to be generated as a bulleted list, then all the items in the summary should
    be represented by bullets.
  prefs: []
  type: TYPE_NORMAL
- en: Ordering
  prefs: []
  type: TYPE_NORMAL
- en: The ordering of the items in the summary should not impede the understanding
    of the summary content. We also might want to specify an order for the summaries,
    for example, chronological.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs: []
  type: TYPE_NORMAL
- en: In case of errors or omissions in the source document, there should be appropriate
    error handling.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we automatically verify whether a given summary meets all these criteria?
    We can use a combination of rule-based methods and fine-tuned models. Ultimately,
    the rigor of the methods used for verification depends on the degree of reliability
    needed for your application. However, we notice that once we reduce the scope
    of the verification process to verify fitness of individual criteria rather than
    the application as a whole, it becomes easier to verify accurately using inexpensive
    techniques. Let’s look at how we can build verifiers for each criteria of the
    abstractive summarization task:'
  prefs: []
  type: TYPE_NORMAL
- en: Factuality
  prefs: []
  type: TYPE_NORMAL
- en: Verifying whether an LLM-generated statement is factual is extremely difficult
    if we do not have access to ground truth. But for summarization applications,
    we do have access to the ground truth. Therefore, we can verify factuality by
    taking each sentence in the summary and checking whether, given the source text,
    one can logically conclude the statement in the summary. This can be framed as
    a natural language inference (NLI) problem, which is a standard NLP task.
  prefs: []
  type: TYPE_NORMAL
- en: In the NLI task, we have a hypothesis and a premise, and the goal is to check
    if the hypothesis is logically entailed by the premise. In our example, the hypothesis
    is a sentence in the summary and the premise is the source text.
  prefs: []
  type: TYPE_NORMAL
- en: Training an NLI model specific to your domain might be a cumbersome task. If
    you do not have access to an NLI model, you can use token overlap and similar
    statistics to approximate factuality verification.
  prefs: []
  type: TYPE_NORMAL
- en: For numbers and named entities, factuality verification can be performed by
    using string matches. You can verify if all the numbers and named entities in
    the summary are indeed present in the source text.
  prefs: []
  type: TYPE_NORMAL
- en: Specificity
  prefs: []
  type: TYPE_NORMAL
- en: One way for a summary to be specific is to include numbers and named entities
    where relevant. For each sentence in the summary, we can check whether the content
    in the source document related to the topic of the sentence contains any numbers
    and named entities, and if these are reflected in the summary. Numbers and named
    entities can be tagged and detected using regular expressions or libraries like
    [spaCy](https://oreil.ly/zatAW).
  prefs: []
  type: TYPE_NORMAL
- en: Relevance/precision
  prefs: []
  type: TYPE_NORMAL
- en: We can train a classification model that detects whether a sentence in the summary
    is relevant. Note that there are limits to this approach. If this classification
    model was good enough, we could have directly used it to select relevant sentences
    from the source text to build the summary! In practice, this classification model
    can be used to remove irrelevant content that is more obvious.
  prefs: []
  type: TYPE_NORMAL
- en: Recall/completeness
  prefs: []
  type: TYPE_NORMAL
- en: What content merits inclusion in the summary is a difficult question, especially
    if there is a hard limit on the summary length. You can train a ranking model
    that ranks sentences in the source document by importance, and then verify if
    the top-ranked sentences are represented in the summary. You can also specify
    beforehand the type of content that you need represented in the summary and build
    a classification model for determining which parts of the source document contain
    pertinent information. Using similarity metrics like embedding similarity, you
    can then find if the content has been adequately represented in the summary.
  prefs: []
  type: TYPE_NORMAL
- en: Repetitiveness
  prefs: []
  type: TYPE_NORMAL
- en: This can be discovered by using string difference algorithms like the [Jaccard
    distance](https://oreil.ly/Ny_Ku) or by calculating the embedding similarity between
    pairs of summary sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Coherence
  prefs: []
  type: TYPE_NORMAL
- en: This is perhaps one of the most difficult criteria to verify. One way to solve
    this, albeit a more expensive solution, is to build a prerequisite detection model.
    For each sentence in the summary, we detect if all the sentences that come before
    it are sufficient prerequisites for understanding the correct sentence. For more
    information on prerequisite detection techniques, see [Thareja et al.](https://oreil.ly/6JnRs)
  prefs: []
  type: TYPE_NORMAL
- en: Structure
  prefs: []
  type: TYPE_NORMAL
- en: If we specify a predetermined structure (sections and subsections) for the summary,
    we can easily identify if the structure is adhered to by checking if the desired
    section and subsection titles are present in the summary. We can also verify using
    embedding similarity techniques if the content within the sections and subsections
    is faithful to the title of the section/subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting
  prefs: []
  type: TYPE_NORMAL
- en: This involves checking whether the content is in the appropriate formatting,
    for example, whether it is a bulleted list or a valid JSON object.
  prefs: []
  type: TYPE_NORMAL
- en: Ordering
  prefs: []
  type: TYPE_NORMAL
- en: The desired order can be chronological, alphabetical, a domain, or task-specific
    ordering. If it is supposed to be chronological, you can verify by extracting
    dates in the summary and checking if the summary contains dates in a chronological
    order. If the ordering requirements are more complex, then verifying adherence
    to order may become an extremely difficult task.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Do not expect your verification process to be strictly better than your summary
    model. If that was the case, you could have used the verification process to generate
    the summary!
  prefs: []
  type: TYPE_NORMAL
- en: We can also deploy symbolic verifiers like [SAT](https://oreil.ly/lOsg_) (Boolean
    satisfiability) solvers and logic planners. This type of verification is beyond
    the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Once verification modules are part of our system architecture, we will also
    need to decide what action to perform when the verification fails. One option
    is to just resample from the language model again. Regeneration can be performed
    for the full output or only for the output that failed verification. We can also
    develop antifragile architectures that have fallbacks in case of failure, which
    we will discuss in [Chapter 13](ch13.html#ch13).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adding more verifiers can drastically increase system latency. Thus, their inclusion
    has to be balanced with accuracy and system latency needs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s discuss agent orchestration software that connects all these
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Agent Orchestration Software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For agentic workflows to proceed smoothly, we need software that connects all
    the components. Orchestration software manages state; invokes tools; initiates
    retrieval; pipes buffers; and logs intermediate and final outputs. Many agentic
    frameworks, both open source and proprietary, perform this function, including
    [LangChain](https://oreil.ly/7vmlY), [LlamaIndex](https://oreil.ly/uxejK), [CrewAI](https://oreil.ly/Ntxii),
    [AutoGen](https://oreil.ly/tx3qy), [MetaGPT](https://oreil.ly/HI-Jn), [XAgent](https://oreil.ly/sA_DR),
    [llama-stack-apps](https://oreil.ly/SBGC_), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Agents are a relatively new paradigm, so all these agentic frameworks are expected
    to change a lot in the coming months and years. These frameworks are implemented
    in an opinionated fashion and hence are less flexible. For prototyping, I suggest
    picking LangChain or LlamaIndex for ease of use. For production use, you might
    want to build a framework internally from scratch or by extending the open source
    ones. This book’s [GitHub repo](https://oreil.ly/llm-playbooks) contains a rudimentary
    agentic framework as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned all the different agentic system components, it is
    time to get building! The book’s [GitHub repository](https://oreil.ly/llm-playbooks)
    contains sample implementations of various types of agents. Try modifying them
    for your use case to understand the tradeoffs being made.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The keep it simple, stupid (KISS) principle applies to agents perhaps more than
    any other recent paradigm. Don’t complicate your agentic architecture unless there
    is a compelling reason to do so. We will discuss this more in [Chapter 13](ch13.html#ch13).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the different ways in which LLMs can interface
    with external tools. We introduced the agentic paradigm and provided a formal
    definition of agents. We identified the components of an agentic system in detail,
    exploring models, tools, data stores, guardrails and verifiers, and agentic orchestration
    software. We learned how to define and implement our own tools.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore data representation and retrieval, crucial
    elements of interfacing LLMs with external data.
  prefs: []
  type: TYPE_NORMAL
