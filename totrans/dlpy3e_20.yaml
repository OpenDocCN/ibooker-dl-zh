- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter20_conclusion](https://deeplearningwithpython.io/chapters/chapter20_conclusion)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ll start with a bird’s-eye view of what you should take away from this book.
    This should refresh your memory regarding some of the concepts you’ve learned.
    Then we’ll give you a short list of resources and strategies for learning further
    about machine learning and staying up to date with new advances.
  prefs: []
  type: TYPE_NORMAL
- en: Becoming an effective AI practitioner is a journey, and finishing this book
    is merely your first step on it. I want to make sure you realize this and are
    properly equipped to take the next steps of this journey on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts in review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section briefly synthesizes key takeaways from this book. If you ever need
    a quick refresher to help you recall what you’ve learned, you can read these few
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: Various approaches to artificial intelligence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, deep learning isn’t synonymous with artificial intelligence (AI), or
    even with machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence* (AI) is an ancient, broad field that can generally
    be understood as “all attempts to automate human cognitive processes.” This can
    range from the very basic, such as an Excel spreadsheet, to the very advanced,
    like a humanoid robot that can walk and talk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine learning* is a specific subfield of AI that aims at automatically
    developing programs (called *models*) purely from exposure to training data. This
    process of turning data into a program is called *learning*. Although machine
    learning has been around for a long time, it only started to take off in the 1990s,
    before becoming the dominant form of AI in the 2000s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep learning* is one of many branches of machine learning, where the models
    are long chains of geometric transformations, applied one after the other. These
    operations are structured into modules called *layers*: deep learning models are
    typically stacks of layers — or, more generally, graphs of layers. These layers
    are parameterized by *weights*, which are the parameters learned during training.
    The *knowledge* of a model is stored in its weights, and the process of learning
    consists of finding “good values” for these weights — values that minimize a *loss
    function*. Because the chain of geometric transformations considered is differentiable,
    updating the weights to minimize the loss function is done efficiently via *gradient
    descent*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generative AI* is a specific subset of deep learning, where models are capable
    of generating text, images, videos, or sound. These models tend to be very large
    — billions of parameters. They’re trained in a self-supervised manner; that is,
    they’re trained to reconstruct artificially missing or corrupted parts of an input
    — for instance, denoising images, predicting the next word in a sentence, and
    so on. This learning process enables the models to learn sophisticated “maps”
    (embedding manifolds) of their input space, which can be used for sampling new
    inputs. These models have launched AI into its “consumer” era with the rise of
    products like ChatGPT or Midjourney.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though deep learning is just one among many approaches to machine learning,
    it isn’t on an equal footing with the others. Deep learning is a breakout success.
    Here’s why.
  prefs: []
  type: TYPE_NORMAL
- en: What makes deep learning special within the field of machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the span of only a few years, deep learning has achieved tremendous breakthroughs
    across a wide range of tasks that have been historically perceived as extremely
    difficult for computers, especially in the area of machine perception: extracting
    useful information from images, videos, sound, and more. Given sufficient training
    data (in particular, training data appropriately labeled by humans), deep learning
    makes it possible to extract from perceptual data almost anything a human could.
    Hence, it’s sometimes said that deep learning has “solved perception” — although
    that’s true only for a fairly narrow definition of perception.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to its unprecedented technical successes, deep learning has singlehandedly
    brought about the third and by far the largest *AI summer*: a period of intense
    interest, investment, and hype in the field of AI. As this book is being written,
    we’re in the middle of it. Whether this period will end in the near future and
    what happens after it ends are topics of debate. One thing is certain: in stark
    contrast with previous AI summers, deep learning has provided enormous business
    value to both large and small technology companies and has become a huge consumer
    success, enabling human-level speech recognition, chatbot assistants, photorealistic
    image generation, human-level machine translation, and more. The hype may (and
    likely will) recede, but the sustained economic and technological impact of deep
    learning will remain. In that sense, deep learning could be analogous to the internet:
    it may be overly hyped up for a few years, but in the longer term, it will still
    be a major revolution that will transform our economy and our lives.'
  prefs: []
  type: TYPE_NORMAL
- en: One reason I’m particularly optimistic about deep learning is that even if we
    were to make no further technological progress in the next decade, deploying existing
    algorithms to every applicable problem would be a game changer for most industries.
    Deep learning is nothing short of a revolution, and progress is currently happening
    at an incredibly fast rate due to an exponential investment in resources and headcount.
    From where we stand, the future looks bright, although short-term expectations
    are somewhat overoptimistic; deploying deep learning to the full extent of its
    potential will likely take multiple decades.
  prefs: []
  type: TYPE_NORMAL
- en: How to think about deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most surprising thing about deep learning is how simple it is. Fifteen years
    ago, no one expected that we would achieve such amazing results on machine-perception
    and natural language processing problems by using simple parametric models trained
    with gradient descent. Now it turns out that all you need is sufficiently large
    parametric models trained with gradient descent on sufficiently many examples.
    As Feynman once said about the universe, “It’s not complicated, it’s just a lot
    of it.”^([[1]](#footnote-1))
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, everything is a vector; that is, everything is a *point* in
    a *geometric space*. Model inputs (text, images, and so on) and targets are first
    *vectorized* — turned into an initial input vector space and target vector space.
    Each layer in a deep learning model operates one simple geometric transformation
    on the data that goes through it. Together, the chain of layers in the model forms
    one complex geometric transformation, broken down into a series of simple ones.
    This complex transformation attempts to map the input space to the target space,
    one point at a time. This transformation is parameterized by the weights of the
    layers, which are iteratively updated based on how well the model is currently
    performing. A key characteristic of this geometric transformation is that it must
    be *differentiable*, which is required for us to be able to learn its parameters
    via gradient descent. Intuitively, this means the geometric morphing from inputs
    to outputs must be smooth and continuous — a significant constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire process of applying this complex geometric transformation to the
    input data can be visualized in 3D by imagining a person trying to uncrumple a
    paper ball: the crumpled paper ball is the manifold of the input data that the
    model starts with. Each movement operated by the person on the paper ball is similar
    to a simple geometric transformation operated by one layer. The full uncrumpling
    gesture sequence is the complex transformation of the entire model. Deep learning
    models are mathematical machines for uncrumpling complicated manifolds of high-dimensional
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: That’s the magic of deep learning — turning meaning into vectors, into geometric
    spaces, and then incrementally learning complex geometric transformations that
    map one space to another. All you need are spaces of sufficiently high dimensionality
    to capture the full scope of the relationships found in the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole thing hinges on two core ideas: that *meaning is derived from the
    pairwise relationship between things* (between words in a language, between pixels
    in an image, and so on) and that *these relationships can be captured by a distance
    function*. But note that whether the brain implements meaning via geometric spaces
    is an entirely separate question. Vector spaces are efficient to work with from
    a computational standpoint, but different data structures for intelligence can
    easily be envisioned — in particular, graphs. Neural networks initially emerged
    from the idea of using graphs as a way to encode meaning, which is why they’re
    named *neural networks*; the surrounding field of research used to be called *connectionism*.
    Nowadays, the name *neural network* exists purely for historical reasons — it’s
    an extremely misleading name because they’re neither neural nor networks. In particular,
    neural networks have hardly anything to do with the brain. A more appropriate
    name would have been *layered representations learning* or *hierarchical representations
    learning*, or maybe even deep *differentiable models* or *chained geometric transforms*,
    to emphasize the fact that continuous geometric space manipulation is at their
    core.'
  prefs: []
  type: TYPE_NORMAL
- en: Key enabling technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The technological revolution that’s currently unfolding didn’t start with any
    single breakthrough invention. Rather, like any other revolution, it’s the product
    of a vast accumulation of enabling factors — slowly at first, and then suddenly.
    In the case of deep learning, we can point out the following key factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Incremental algorithmic innovations, first spread over two decades (starting
    with backpropagation) and then happening increasingly faster as more research
    effort was poured into deep learning after 2012\. One major such breakthrough
    was the Transformer architecture in 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of large amounts of image, video, and text data, which is a
    requirement to realize that sufficiently large models trained on sufficiently
    large data are all we need. This is, in turn, a by-product of the rise of the
    consumer internet and Moore’s law applied to storage media. Today, state-of-the-art
    language models are trained on a large fraction of the entire internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of fast, highly parallel computation hardware at a low price,
    especially the GPUs produced by NVIDIA — first gaming GPUs and then chips designed
    from the ground up for deep learning. Early on, NVIDIA CEO Jensen Huang took note
    of the deep learning boom and decided to bet the company’s future on it, which
    paid off in a big way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A complex stack of software layers that makes this computational power available
    to humans: the CUDA language, frameworks like TensorFlow, JAX, and PyTorch that
    do automatic differentiation, and Keras, which makes deep learning accessible
    to most people.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the future, deep learning will not be used only by specialists such as researchers,
    graduate students, and engineers with an academic profile; it will be a tool in
    the toolbox of every developer, much like web technology today. Everyone needs
    to build intelligent apps: just as every business today needs a website, every
    product will need to intelligently make sense of user-generated data. Bringing
    about this future will require us to build tools that make deep learning radically
    easy to use and accessible to anyone with basic coding abilities. Keras has been
    the first major step in that direction.'
  prefs: []
  type: TYPE_NORMAL
- en: The universal machine learning workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having access to an extremely powerful tool for creating models that map any
    input space to any target space is great, but the difficult part of the machine
    learning workflow is often everything that comes before designing and training
    such models (and, for production models, what comes after, as well). Understanding
    the problem domain to be able to determine what to attempt to predict, given what
    data, and how to measure success is a prerequisite for any successful application
    of machine learning, and it isn’t something that advanced tools like Keras and
    TensorFlow can help you with. As a reminder, here’s a quick summary of the typical
    machine learning workflow as described in chapter 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Define the problem.* What data is available, and what are you trying to predict?
    Will you need to collect more data or hire people to manually label a dataset?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Identify a way to reliably measure success on your goal.* For simple tasks,
    this may be prediction accuracy, but in many cases, it will require sophisticated,
    domain-specific metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prepare the validation process that you’ll use to evaluate your models.* In
    particular, you should define a training set, a validation set, and a test set.
    The validation-set and test-set labels shouldn’t leak into the training data:
    for instance, with temporal prediction, the validation and test data should be
    posterior to the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vectorize the data by turning it into vectors and preprocessing it in a way
    that makes it more easily approachable by a neural network (normalization and
    so on).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Develop a first model that beats a trivial common-sense baseline, thus demonstrating
    that machine learning can work on your problem.* This may not always be the case!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gradually refine your model architecture by tuning hyperparameters and adding
    regularization.* Make changes based on performance on the validation data only,
    not the test data or the training data. Remember that you should get your model
    to overfit (thus identifying a model capacity level that’s greater than you need)
    and only then begin to add regularization or downsize your model. Beware of validation-set
    overfitting when tuning hyperparameters — the fact that your hyperparameters may
    end up being overspecialized to the validation set. Avoiding this is the purpose
    of having a separate test set!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deploy your final model in production — as a web API, as part of a JavaScript
    or C++ application, on an embedded device, etc.* Keep monitoring its performance
    on real-world data and use your findings to refine the next iteration of the model!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key network architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The families of network architectures that you should be familiar with after
    reading this book are *densely connected networks*, *convolutional networks*,
    *recurrent networks*, *Diffusion Models*, and *Transformers*. Each type of model
    is meant for specific data modalities: a network architecture encodes *assumptions*
    about the structure of the data — a *hypothesis space* within which the search
    for a good model will proceed. Whether a given architecture will work on a given
    problem depends entirely on the match between the structure of the data and the
    assumptions of the network architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: These different network types can easily be combined to achieve larger multimodal
    models, much as you combine LEGO bricks. In a way, deep learning layers are LEGO
    bricks for information processing. Table 20.1 shows a quick overview of the mapping
    between input and output modalities and the appropriate network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '| Input | Output | Model |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vector data | Class probability, Regression value | Densely connected network
    |'
  prefs: []
  type: TYPE_TB
- en: '| Timeseries data | Class probability, Regression value | RNN, Transformer
    |'
  prefs: []
  type: TYPE_TB
- en: '| Images | Class probability, Regression value | ConvNet |'
  prefs: []
  type: TYPE_TB
- en: '| Text | Class probability, Regression value | Transformer |'
  prefs: []
  type: TYPE_TB
- en: '| Text, Images | Text | Transformer |'
  prefs: []
  type: TYPE_TB
- en: '| Text, Images | Images | VAE, Diffusion Model |'
  prefs: []
  type: TYPE_TB
- en: '[Table 20.1](#table-20-1): Model architectures for different data types'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s quickly review the specificities of each network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Densely connected networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A densely connected network is a stack of `Dense` layers, meant to process
    vector data (where each sample is a vector of numerical or categorical attributes).
    Such networks assume no specific structure in the input features: they’re called
    *densely connected* because the units of a `Dense` layer are connected to every
    other unit. The layer attempts to map relationships between any two input features;
    this is unlike a 2D convolution layer, for instance, which only looks at *local*
    relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: Densely connected networks are most commonly used for categorical data (for
    example, where the input features are lists of attributes), such as the Boston
    Housing Price dataset used in chapter 4\. They’re also used as the final classification
    or regression stage of most networks. For instance, the ConvNets covered in chapter
    8 typically end with one or two `Dense` layers, and so do the recurrent networks
    in chapter 13.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, to perform *binary classification*, end your stack of layers with
    a `Dense` layer with a single unit and a `sigmoid` activation, and use `binary_crossentropy`
    as the loss. Your targets should be either 0 or 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform *single-label categorical classification* (where each sample has
    exactly one class, no more), end your stack of layers with a `Dense` layer with
    a number of units equal to the number of classes and a `softmax` activation. If
    your targets are one-hot encoded, use `categorical_crossentropy` as the loss;
    if they’re integers, use `sparse_categorical_ crossentropy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform *multilabel categorical classification* (where each sample can have
    several classes), end your stack of layers with a `Dense` layer with a number
    of units equal to the number of classes and a `sigmoid` activation, and use `binary_crossentropy`
    as the loss. Your targets should be k-hot encoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform *regression* toward a vector of continuous values, end your stack
    of layers with a `Dense` layer with a number of units equal to the number of values
    you’re trying to predict (often a single one, such as the price of a house) and
    no activation. Various losses can be used for regression — most commonly `mean_squared_error`
    (MSE):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ConvNets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Convolution layers look at spatially local patterns by applying the same geometric
    transformation to different spatial locations (*patches*) in an input tensor.
    This results in representations that are *translation invariant*, making convolution
    layers highly data efficient and modular. This idea is applicable to spaces of
    any dimensionality: 1D (continuous sequences), 2D (images), 3D (volumes), and
    so on. You can use the `Conv1D` layer to process sequences, the `Conv2D` layer
    to process images, and the `Conv3D` layer to process volumes. As a leaner, more
    efficient alternative to convolution layers, you can also use *depthwise separable
    convolution* layers, such as `SeparableConv2D`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*ConvNets*, or *convolutional networks*, consist of stacks of convolution and
    max-pooling layers. The pooling layers let you spatially downsample the data,
    which is required to keep feature maps to a reasonable size as the number of features
    grows and to allow subsequent convolution layers to “see” a greater spatial extent
    of the inputs. ConvNets are often ended with either a `Flatten` operation or a
    global pooling layer, turning spatial feature maps into vectors, followed by `Dense`
    layers to achieve classification or regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a typical image-classification network (categorical classification,
    in this case) using `SeparableConv2D` layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When building a very deep ConvNet, it’s common to add *batch normalization*
    layers as well as *residual connections* — two architecture patterns that help
    gradient information flow smoothly through the network.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Transformer looks at a set of vectors (such as word vectors) and uses *neural
    attention* to transform each vector into a representation that is aware of the
    *context* provided by the other vectors in the set. When the set in question is
    an ordered sequence, you can also use *positional encoding* to create Transformers
    that can take into account both global context and word order, capable of processing
    long text paragraphs much more effectively than RNNs or 1D ConvNets.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers can be used for any set-processing or sequence-processing task,
    including text classification, but they excel especially at *sequence-to-sequence
    learning*, such as translating paragraphs in a source language into a target language.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequence-to-sequence Transformer is made of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: A `TransformerEncoder` that turns an input vector sequence into a context-aware,
    order-aware output vector sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `TransformerDecoder` that takes the output of the `TransformerEncoder`, as
    well as a target sequence, and predicts what should come next in the target sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re only processing a single sequence (or set) of vectors, you’d only
    use the `TransformerEncoder`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a sequence-to-sequence Transformer for mapping a source sequence
    to a target sequence (this setup could be used for machine translation or question-answering,
    for instance):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is a lone `TransformerEncoder` for binary classification of integer
    sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Recurrent neural networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Recurrent neural networks* (RNNs) work by processing sequences of inputs one
    timestep at a time and maintaining a state throughout (a state is typically a
    vector or set of vectors). They should be used preferentially over 1D ConvNets
    in the case of sequences where patterns of interest aren’t invariant by temporal
    translation (for instance, timeseries data where the recent past is more important
    than the distant past).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three RNN layers are available in Keras: `SimpleRNN`, `GRU`, and `LSTM`. For
    most practical purposes, you should use either `GRU` or `LSTM`. `LSTM` is the
    more powerful of the two but is also more expensive; you can think of `GRU` as
    a simpler, cheaper alternative to it.'
  prefs: []
  type: TYPE_NORMAL
- en: To stack multiple RNN layers on top of each other, each layer prior to the last
    layer in the stack should return the full sequence of its outputs (each input
    timestep will correspond to an output timestep); if you aren’t stacking any further
    RNN layers, then it’s common to return only the last output, which contains information
    about the entire sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a single RNN layer for binary classification of vector sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is a stacked RNN layer for binary classification of vector sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Limitations of deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Building deep learning models is like playing with LEGO bricks: layers can
    be plugged together to map essentially anything to anything, given that you have
    appropriate training data available and that the mapping is achievable via a continuous
    geometric transformation of reasonable complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the catch, though — this mapping is often not learnable in a way that
    will generalize. Deep learning models operate like vast, interpolative databases
    of patterns. Their pattern-matching strength is also their core weakness:'
  prefs: []
  type: TYPE_NORMAL
- en: '*They fundamentally struggle to adapt to novelty.* Because their parameters
    are fixed after training, they can only retrieve or replicate patterns similar
    to their training data. Faced with inputs significantly outside this familiar
    distribution — no matter how simple the underlying task — their performance degrades
    drastically, as they lack mechanisms for fluid generalization beyond their memorized
    experience. This explains why even large models fail on novel tasks or simple
    variations of familiar problems — like ARC-AGI tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*They’re very sensitive to phrasing and other distractors.* Deep learning models
    exhibit high sensitivity to superficial variations in input presentation, such
    as minor phrasing changes (consider prompt sensitivity in LLMs) or imperceptible
    perturbations (consider adversarial examples in vision), indicating a lack of
    robust, human-like understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*They often can’t learn generalizable algorithms.* The continuous, geometric
    nature of deep learning models makes them fundamentally ill-suited for learning
    exact, discrete, step-by-step algorithms, such as those that are the bread and
    butter of classical computer science. Models approximate such processes through
    interpolation rather than implementing robust, generalizable procedures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should always resist the temptation to anthropomorphize deep learning models.
    Their performance is built on pointwise statistical patterns rather than human-like
    experiential grounding, making it brittle when encountering deviations from training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The narrative that simply scaling up model size and training data would lead
    to general intelligence has proven insufficient. While scaling enhances performance
    on benchmarks that amount to memorization tests, it fails to address the fundamental
    limitations of deep learning, which stem from the core paradigm of fitting static,
    interpolative curves to data. Five years of exponential scaling of base LLMs haven’t
    overcome these constraints because the underlying approach remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: By 2024, this realization spurred a transition toward test-time adaptation (TTA),
    where models perform search or fine-tuning during the inference phase to adapt
    to novel problems. While TTA methods have yielded major breakthroughs, such as
    OpenAI’s o3 surpassing human baseline on ARC-AGI-1 in late 2024, this performance
    has come at an extreme computational cost. Efficient, human-like adaptation is
    still a completely open problem, and the slightly harder ARC-AGI-2 benchmark remains
    completely unsolved as of today. We still need further conceptual advances beyond
    mere scaling or brute-force search.
  prefs: []
  type: TYPE_NORMAL
- en: What might lie ahead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solving human-like fluid intelligence (and ARC-AGI-2) requires moving beyond
    the limitations inherent in current approaches. While deep learning excels at
    *value-centric abstraction*, which enables pattern recognition and intuition,
    it fundamentally lacks capabilities for *program-centric abstraction*, which underpins
    discrete reasoning, planning, and causal understanding. Human intelligence seamlessly
    integrates both — future AI must do the same.
  prefs: []
  type: TYPE_NORMAL
- en: Future key developments may include
  prefs: []
  type: TYPE_NORMAL
- en: '*Hybrid models* — Future models will likely integrate learned algorithmic modules
    (providing reasoning and symbolic manipulation) with deep learning modules (providing
    perception and intuition). These systems might learn to use programming primitives
    like control flow, variables, recursion, and complex data structures dynamically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep-learning guided program search* — Program synthesis — automatically discovering
    executable code that meets specifications — offers a route to program-centric
    abstraction. However, its reliance on inefficient discrete search is a major bottleneck.
    A crucial advance will be using deep learning to guide this search, utilizing
    learned intuition about program structure to navigate vast, combinatorial spaces
    of programs efficiently, much like human developers use experience and intuition
    to narrow down their choices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Modular recombination and lifelong learning* — We’ll move away from monolithic,
    end-to-end models trained from scratch. Instead, future AI systems will use massive
    libraries of reusable, modular components that can be repurposed across many problems,
    acquired from experience. These libraries will feature both “geometric” (deep
    learning based) and “algorithmic” modules. When faced with a new problem, such
    AI systems will fetch relevant modules and dynamically recombine them into a new
    model adapted to the situation at hand. Whenever the system ends up developing
    a reusable component as a by-product of this problem-solving loop, the new component
    would get added to the library, becoming available for every future task the system
    might encounter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, developing AI that mirrors human-like fluid intelligence will require
    blending continuous pattern recognition together with discrete, symbolic programs
    and fully embracing the paradigm of on-the-fly adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Staying up to date in a fast-moving field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As final parting words, I want to give you some pointers about how to keep learning
    and updating your knowledge and skills after you’ve turned the last page of this
    book. The field of modern deep learning, as we know it today, is only a few years
    old, despite a long, slow prehistory stretching back decades. With an exponential
    increase in financial resources and research headcount since 2013, the field as
    a whole is now moving at a frenetic pace. What you’ve learned in this book won’t
    stay relevant forever, and it isn’t all you’ll need for the rest of your career.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are plenty of free online resources that you can use to stay
    up to date and expand your horizons. Here are a few.
  prefs: []
  type: TYPE_NORMAL
- en: Practice on real-world problems using Kaggle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An effective way to acquire real-world experience is to try your hand at machine
    learning competitions on Kaggle ([https://kaggle.com](https://kaggle.com)). The
    only real way to learn is through practice and actual coding — that’s the philosophy
    of this book, and Kaggle competitions are the natural continuation of this. On
    Kaggle, you’ll find an array of constantly renewed data science competitions,
    many of which involve deep learning, prepared by companies interested in obtaining
    novel solutions to some of their most challenging machine learning problems. Fairly
    large monetary prizes are offered to top entrants.
  prefs: []
  type: TYPE_NORMAL
- en: By participating in a few competitions, maybe as part of a team, you’ll become
    more familiar with the practical side of some of the advanced best practices described
    in this book, especially hyperparameter tuning, avoiding validation-set overfitting,
    and model ensembling.
  prefs: []
  type: TYPE_NORMAL
- en: Read about the latest developments on arXiv
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep learning research, in contrast with some other scientific fields, takes
    place completely in the open. Papers are made publicly and freely accessible as
    soon as they’re finalized, and a lot of related software is open source. arXiv
    ([https://arxiv.org](https://arxiv.org)) — pronounced “archive” (the X stands
    for the Greek *chi*) — is an open access preprint server for physics, mathematics,
    and computer science research papers. It has become the de facto way to stay up
    to date on the cutting edge of machine learning and deep learning. The large majority
    of deep learning researchers upload any paper they write to arXiv shortly after
    completion. This allows them to plant a flag and claim a specific finding without
    waiting for a conference acceptance (which takes months), which is necessary given
    the fast pace of research and the intense competition in the field. It also allows
    the field to move extremely fast: all new findings are immediately available for
    all to see and to build on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important downside is that the sheer quantity of new papers posted every
    day on arXiv makes it impossible to even skim them all, and the fact that they
    aren’t peer-reviewed makes it difficult to identify those that are both important
    and high quality. It’s challenging, and becoming increasingly more so, to find
    the signal in the noise. But some tools can help: in particular, you can use Google
    Scholar ([https://scholar.google.com](https://scholar.google.com)) to keep track
    of publications by your favorite authors.'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the Keras ecosystem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With over 2.5 million users as of early 2025 and still growing, Keras has a
    large ecosystem of tutorials, guides, and related open source projects:'
  prefs: []
  type: TYPE_NORMAL
- en: Your main reference for working with Keras is the online documentation at [https://keras.io](https://keras.io).
    In particular, you’ll find extensive developer guides at [https://keras.io/guides](https://keras.io/guides),
    and you’ll find dozens of high-quality Keras code examples at [https://keras.io/examples](https://keras.io/examples).
    Make sure to check them out!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras source code can be found at [https://github.com/keras-team/keras](https://github.com/keras-team/keras),
    and Keras Hub can be found at [https://github.com/keras-team/keras-hub](https://github.com/keras-team/keras-hub).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can follow François (@fchollet) and Matt (@mattdangerw) on X (formerly Twitter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the end of *Deep Learning with Python*! I hope you’ve learned a thing
    or two about machine learning, deep learning, Keras, and maybe even cognition
    in general. Learning is a lifelong journey, especially in the field of AI, where
    we have far more unknowns on our hands than certitudes. So please go on learning,
    questioning, and researching. Never stop. Because even given the progress made
    so far, most of the fundamental questions in AI remain unanswered. Many haven’t
    even been properly asked yet.
  prefs: []
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Richard Feynman, interview, *The World from Another Point of View*, Yorkshire
    Television, 1972. [[↩]](#footnote-link-1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
