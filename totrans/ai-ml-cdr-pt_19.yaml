- en: Chapter 18\. Introduction to RAG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 18 章\. RAG 简介
- en: Remember that first time you chatted with an LLM like ChatGPT—and how it was
    extremely insightful about things you didn’t expect it to know? I had worked with
    LLMs before the release of ChatGPT and on projects that highlighted LLM abilities,
    and I *still* was surprised by what they could do. Remember the famous on-stage
    demonstration by Google, where the CEO had a conversation with the planet Pluto?
    It was one of those fundamental mind shifts in the possibilities of AI that we’re
    *still* exploring as it continues to evolve.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 记得你第一次与像 ChatGPT 这样的 LLM 聊天的情景——以及它对你意想不到的事情有多么深刻的见解吗？在 ChatGPT 发布之前，我曾与 LLMs
    合作，并在一些突出 LLM 能力的项目中工作，我仍然对他们能做什么感到惊讶。记得谷歌在舞台上进行的著名演示吗？CEO 与冥王星进行了对话？这是我们在 AI
    可能性方面经历的根本性思维转变之一，随着其不断进化，我们仍在探索。
- en: 'But, despite all that brilliance, there were still limitations, and the more
    I and others worked with LLMs, the more we encountered them. The transformer-based
    architecture that we discussed in [Chapter 15](ch15.html#ch15_transformers_and_transformers_1748549808974580)
    was brilliant at snarfing up text data, creating QKV mappings from it, and learning
    how to artificially understand the semantics of the text as a result. But despite
    the volume of text used to build those mappings, there was—and always is—one blind
    spot: private data. In particular, if there is data that you want to work with
    that the model was not trained on, you’re at a major risk of hallucination!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，尽管如此出色，仍然存在局限性，我和其他人越是用 LLMs 工作，就越会遇到它们。我们在[第 15 章](ch15.html#ch15_transformers_and_transformers_1748549808974580)中讨论的基于
    Transformer 的架构在抓取文本数据、从中创建 QKV 映射以及学习如何人工理解文本的语义方面非常出色。但尽管用于构建这些映射的文本量很大，但仍然存在——并且始终存在的一个盲点：私人数据。特别是，如果你想要处理模型未训练过的数据，你面临的主要风险是出现幻觉！
- en: Gaining skills to help mitigate this blind spot could potentially be the *most*
    valuable thing you can do as a software developer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 获得帮助减轻这种盲点的技能可能对你作为软件开发者来说是最有价值的事情。
- en: For this chapter, I want you to think about AI models and in particular large
    generative models like LLMs *differently*. Stop seeing them as intelligent and
    knowledgeable and start seeing them as *utilities* to help you parse your data
    better. Think of everything they have learned not as a knowledge base in and of
    itself but as a way that they have generalized understanding of language by being
    extensively well read.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我希望你以不同的方式思考 AI 模型，特别是像 LLMs 这样的大型生成模型。停止将它们视为聪明和有知识的，开始将它们视为*工具*，帮助你更好地解析数据。将他们所学的所有内容不是看作一个知识库本身，而是看作他们通过广泛阅读而获得的语言泛化理解的方式。
- en: I call this *artificial understanding,* as a complementary technology to AI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我把这种理解称为*人工理解*，作为人工智能的补充技术。
- en: Then, once you treat your favorite LLM as an engine for artificial understanding,
    you can start having it understand your private text—stuff that wasn’t in its
    training set—and through that understanding, process your text in new and interesting
    ways.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦你将你最喜欢的 LLM 视为一个用于人工理解的引擎，你就可以开始让它理解你的私人文本——那些不在其训练集中的内容——通过这种理解，以新的和有趣的方式处理你的文本。
- en: Let’s explore this with a scenario. Imagine you’re discussing your favorite
    sci-fi novel with an AI model. You want to ask about characters, plot, theme,
    and stuff like that, but the model struggles with the specifics, offering only
    general responses—or worse, hallucinating them. For example, take a look at [Figure 18-1](#ch18_figure_1_1748550073457538),
    which shows the results I got when I was chatting with ChatGPT about a character
    from a novel called *Space Cadets.*
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个场景来探讨这个问题。想象一下，你正在与一个 AI 模型讨论你最喜欢的科幻小说。你想要询问关于角色、情节、主题等内容，但模型在具体细节上遇到了困难，只能提供一些泛泛的回答——或者更糟糕的是，甚至出现了幻觉。例如，看看[图
    18-1](#ch18_figure_1_1748550073457538)，它展示了我在与 ChatGPT 讨论一本名为 *Space Cadets* 的小说中的角色时得到的结果。
- en: '![](assets/aiml_1801.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1801.png)'
- en: Figure 18-1\. Chatting with GPT about a character
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-1\. 与 GPT 讨论一个角色
- en: This is all very interesting—except that it’s wrong. First of all, the character
    is from *North* Korea, not *South* Korea.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都非常有趣——但它是错误的。首先，这个角色来自*北*朝鲜，而不是*南*朝鲜。
- en: GPT is being confidently incorrect. Why? Because this novel isn’t in the training
    set! I wrote it in 2014, and it was published by a small press that folded just
    a few months afterward. As such, it’s relatively obscure and the perfect fodder
    for us to use to explore RAG. By the end of this chapter, you’ll have used your
    PyTorch skills to create an application that is much smarter at understanding
    this novel and, indeed, the character in question. And yes, you’ll have the full
    novel to work with!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 正在自信地犯错。为什么？因为这个小说不在训练集中！我是在 2014 年写的，由一家小型出版社出版，几个月后就倒闭了。因此，它相对较为冷门，是我们用来探索
    RAG 的完美素材。到本章结束时，你将使用你的 PyTorch 技能创建一个能够更智能地理解这个小说以及所讨论角色的应用程序。而且，你将拥有整部小说来工作！
- en: 'A small aside: when I first used an LLM for tasks like this, my mind was blown.
    Its ability to *artificially understand* the contents and context of my own writing
    was like having a partner beside me to critique my work and to help me dig deep
    into the characters and themes. The book ends on a cliffhanger, and I never came
    back to write any sequels. Having conversations with an LLM about the character
    arcs, etc., gave me a whole new fount of wisdom about where it could go.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 稍作补充：当我第一次使用 LLM 来完成这类任务时，我感到非常震惊。它能够 *人工理解* 我自己写作的内容和上下文，就像身边有一个伙伴来批评我的作品并帮助我深入挖掘角色和主题一样。这本书以悬念结束，我从未回来写任何续集。与
    LLM 讨论角色弧线等问题，让我对它的发展方向有了全新的智慧来源。
- en: And of course, you aren’t limited to works of fiction. Almost every business
    has a trove of internal intelligence that’s locked up in documents that would
    take a human too much time to read, index, cross-correlate, and understand to
    be able to answer queries—so the ability of an LLM to artificially understand
    them to help you mine the text for knowledge is second-to-none.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你不仅限于文学作品。几乎每个企业都有大量内部智力资源被锁在文档中，这些文档需要人类花费大量时间阅读、索引、交叉关联和理解，才能回答查询——因此，LLM
    能够人工理解这些文档以帮助您挖掘知识的能力是无与伦比的。
- en: That’s why I’m excited about RAG. And I hope you will be, too, after you finish
    this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，我对 RAG 感到兴奋。并且我希望你在完成本章后也会如此。
- en: What Is RAG?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 RAG？
- en: The acronym *RAG* stands for *retrieval augmented generation*, which works to
    bridge the knowledge gap between what an LLM has been trained on and private data
    you own that it doesn’t have mappings for. At query time, as well as with a prompt
    like “Tell me about the character…,” we’ll also feed it information snippets from
    the local datastore. So, for example, if we’re querying about a character from
    a novel, local data might include things like her hometown, her favorite food,
    her values, and how she speaks. When we pass *that* data along with the query,
    a lot of it *is* in the training set for the LLM, and as such, the LLM can have
    a much more informed opinion about her. Not least, the mistake the LLM made in
    [Figure 18-1](#ch18_figure_1_1748550073457538) can be mitigated—when the LLM is
    given her hometown, it can at least get the country right!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写 *RAG* 代表 *retrieval augmented generation*，其目的是弥合 LLM 训练时所依据的知识与您拥有的、LLM 没有映射的私有数据之间的知识差距。在查询时间以及像“告诉我关于这个角色……”这样的提示下，我们还会从本地数据存储中提供信息片段。例如，如果我们正在查询小说中的一个角色，本地数据可能包括她的家乡、她最喜欢的食物、她的价值观以及她的说话方式。当我们把
    *这些* 数据与查询一起传递时，其中很多都是 LLM 训练集中的，因此 LLM 可以对她的了解更加深入。更重要的是，LLM 在 [图 18-1](#ch18_figure_1_1748550073457538)
    中犯的错误可以得到缓解——当 LLM 被提供她的家乡时，它至少可以正确地确定她的国家！
- en: '[Figure 18-2](#ch18_figure_2_1748550073457600) shows the flow of a typical
    query to an LLM. It’s quite basic: you pass in a prompt and the transformers do
    their magic by going through the knowledge that the LLM learned to produce QKV
    values to generate a response.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-2](#ch18_figure_2_1748550073457600) 展示了向 LLM 发出典型查询的流程。它相当基础：你传递一个提示，然后变压器通过处理
    LLM 学到的知识来生成 QKV 值，从而产生一个响应。'
- en: '![](assets/aiml_1802.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1802.png)'
- en: Figure 18-2\. Typical flow of a query to an LLM
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-2\. 向 LLM 发出查询的典型流程
- en: As we’ve demonstrated , if the LLM doesn’t have much knowledge of the specifics,
    it will fill in the gaps—and it does a pretty good job. For example, even though
    it got her nationality wrong in the example shown in [Figure 18-1](#ch18_figure_1_1748550073457538),
    it was at least able to infer that her name is Korean!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所展示的，如果 LLM 对具体知识了解不多，它会填补这些空白——并且它做得相当不错。例如，尽管它在 [图 18-1](#ch18_figure_1_1748550073457538)
    中展示的例子中错误地推断出她的国籍，但它至少能够推断出她的名字是韩国的！
- en: With RAG, we change this flow to augment the query with extra information that
    we bundle in (see [Figure 18-3](#ch18_figure_3_1748550073457629)). We do this
    by having a local database of the content of the book, and then we search that
    for things that are *similar* to the query. You’ll see the details of how that
    works shortly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RAG，我们将查询流程改为通过我们捆绑的额外信息来增强查询（见[图 18-3](#ch18_figure_3_1748550073457629)）。我们通过拥有一个包含书籍内容的本地数据库来实现这一点，然后我们搜索与查询相似的内容。你很快就会看到它是如何工作的细节。
- en: '![](assets/aiml_1803.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_1803.png](assets/aiml_1803.png)'
- en: Figure 18-3\. Typical flow of a RAG query with an LLM
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-3\. RAG 查询的典型流程与 LLM
- en: The goal here is to enhance the initial prompt with a lot of additional context.
    So, scenes in the book might have her mention her hometown, her family history,
    favorite foods, why she likes people or things, etc. When that is passed to the
    LLM along with the query, the LLM has a lot more to work with—including things
    that it *has* learned about, so its interpretation of the character becomes a
    lot more intelligent. It therefore *artificially understands* the content better.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是增强初始提示，添加大量的额外上下文。因此，书中的场景可能会提到她的家乡、她的家族历史、最喜欢的食物、她为什么喜欢人或事物等。当这些内容与查询一起传递给
    LLM 时，LLM 有更多的工作内容——包括它已经学习过的内容，因此其对角色的解读变得更加智能。因此，它能够*人为地更好地理解*内容。
- en: The key to all of this, of course, is in being able to retrieve the best information
    to bundle with the prompt to make the most of the LLM. You can achieve this by
    storing content from the source material (in this case, the book) in a way that
    lets you do searches for things that are semantically relevant. To that end, you’ll
    use a vector store. We’ll explore that next.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，所有这一切的关键在于能够检索最佳信息来捆绑提示，以充分利用 LLM。你可以通过以允许你搜索语义相关内容的方式存储源材料（在这种情况下，是书籍）的内容来实现这一点。为此，你将使用向量存储。我们将在下一节中探讨这一点。
- en: Getting Started with RAG
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG 入门
- en: To get started, let’s first explore how to create a vector database. To do this,
    you’ll use a database engine that supports vectors and similarity search.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们首先探索如何创建向量数据库。为此，你需要使用支持向量和相似性搜索的数据库引擎。
- en: These work with the idea of storing text as vectors that represent it by using
    embeddings. We saw these in action in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).
    For simplicity, you’ll start by using a pre-built, pre-learned set of embeddings
    from OpenAI with an API provided by LangChain. These will be combined with a vector
    store database called Chroma that is free and open source.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工作原理是将文本存储为向量，通过使用嵌入来表示它。我们在[第 6 章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)中看到了这些技术的实际应用。为了简单起见，你将首先使用
    LangChain 提供的 API 从 OpenAI 获取预构建、预学习的嵌入集。这些将与一个名为 Chroma 的向量存储数据库相结合，该数据库免费且开源。
- en: 'Let’s include the following imports:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们包含以下导入：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `PyPDFLoader`, as its name suggests, is used for managing PDF files in Python.
    I’m providing the book as a PDF, so we’ll need this.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`PyPDFLoader` 用于在 Python 中管理 PDF 文件。我提供这本书作为 PDF，因此我们需要这个。
- en: The `RecursiveCharacterTextSplitter` is a really useful class for slicing the
    book up into text chunks. It provides flexibility on the size of the chunk and
    the overlap between chunks. We’ll explore that in detail a little later.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecursiveCharacterTextSplitter` 是一个非常有用的类，可以将书籍分割成文本块。它提供了对块大小和块之间重叠的灵活性。我们将在稍后详细探讨这一点。'
- en: The `OpenAIEmbeddings` class gives us access to the embeddings learned by Open
    AI while training GPT, and it’s a nice shortcut to make things quicker for us.
    We don’t need to learn our own embeddings for this application—as long as our
    text is encoded in a set of embeddings and our prompt uses the same ones, we can
    use them for similarity search. There are lots of options for this, and Hugging
    Face is a great repository where you can look for the latest and greatest.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`OpenAIEmbeddings` 类为我们提供了访问 Open AI 在训练 GPT 时学习的嵌入的能力，这是一个让我们更快完成工作的便捷途径。对于这个应用，我们不需要学习自己的嵌入——只要我们的文本被编码在一系列嵌入中，并且我们的提示使用相同的嵌入，我们就可以用于相似性搜索。有很多这样的选项，Hugging
    Face 是一个很好的仓库，你可以在这里找到最新和最好的。'
- en: Finally, the `Chroma` database provides us with the ability to store and search
    text based on similarity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`Chroma` 数据库为我们提供了基于相似性存储和搜索文本的能力。
- en: Understanding Similarity
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解相似性
- en: We’ve mentioned similarity a few times now, and it’s important for you to understand
    where it can be useful for you. Recall that in [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    we discussed how embeddings can be used to turn words into vectors. A simple representation
    of this is shown in [Figure 18-4](#ch18_figure_4_1748550073457653).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了几次相似度，重要的是你要理解它对你有什么用。回想一下，在[第6章](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888)，我们讨论了如何使用嵌入将单词转换为向量。这种简单表示如图[18-4](#ch18_figure_4_1748550073457653)所示。
- en: '![](assets/aiml_1804.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1804.png)'
- en: Figure 18-4\. Words as vectors
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-4. 单词作为向量
- en: Here, we plot the words *Awesome*, *Great*, and *Terrible* based on their learned
    vectors. It’s an oversimplification in two dimensions, but hopefully it’s enough
    to demonstrate the concept. In this case, we can visualize that *Awesome* and
    *Great* are similar because they’re close to each other, but we can quantify that
    by looking at the angle of the vectors between them. Taking a function of that
    angle, like its *cosine,* can give us a great indication of how close the vectors
    are to each other. Similarly, if we look at the word *Terrible*, the angle between
    *Awesome* and *Terrible* is very large, indicating that the two words aren’t similar.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们根据它们学习到的向量绘制了单词**Awesome**、**Great**和**Terrible**。在二维空间中这是一个过于简化的表示，但希望这足以演示这个概念。在这种情况下，我们可以可视化**Awesome**和**Great**是相似的，因为它们彼此很近，但我们可以通过观察它们之间向量的角度来量化这一点。通过取那个角度的函数，比如它的**余弦**，可以给我们一个很好的指示，说明向量彼此有多接近。同样，如果我们看单词**Terrible**，**Awesome**和**Terrible**之间的角度非常大，这表明这两个单词不相似。
- en: This process is called *cosine similarity*, and we’ll be using it as we create
    our RAG. We’ll split the book into chunks, calculate the embedding for those chunks,
    and store them in the database. Then, by using a store (ChromaDB, in this case)
    that provides a search based on cosine similarity, we’ll have the key to our RAG.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为**余弦相似度**，在我们创建我们的RAG时，我们会使用它。我们将把书籍分成块，计算这些块的嵌入，并将它们存储在数据库中。然后，通过使用提供基于余弦相似度搜索的存储（在这种情况下是ChromaDB），我们将拥有我们RAG的钥匙。
- en: There are many different ways to calculate similarity, with cosine similarity
    being one of them. It’s worth looking into these other ways to fine-tune your
    RAG solution, but for the rest of this chapter, I’ll use cosine similarity because
    of its simplicity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的方法可以计算相似度，余弦相似度只是其中之一。值得研究这些其他方法以微调你的RAG解决方案，但在这章的其余部分，我将使用余弦相似度，因为它很简单。
- en: Creating the Database
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据库
- en: To create the vector store, we’ll go through the process of loading the PDF
    file, splitting it into chunks, calculating the chunks’ embeddings, and then storing
    them. Let’s look at this step-by-step.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建向量存储，我们将通过加载PDF文件、将其分割成块、计算块的嵌入，然后存储它们的过程。让我们一步一步地看看。
- en: 'First, we’ll load the PDF file by using `PyPDFLoader`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用`PyPDFLoader`加载PDF文件：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we’ll set up a text splitter that reads what we’ll use to chunk the text.
    An important part of your application will be establishing the appropriate sizes
    of chunks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将设置一个文本分割器，它读取我们将用于分割文本的内容。你的应用程序的一个重要部分将是确定块的大小：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this case, the code will split the text into chunks of one thousand characters.
    But it uses a recursive strategy to calculate the split, in which it tries to
    do it on the natural boundaries in the text, rather than making hard cuts at exactly
    one thousand characters. It tries to split on newlines first, then on sentences,
    then on punctuation, and then on spaces. As a last resort, it will split in the
    middle of a word.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，代码将把文本分成一千个字符的块。但它使用递归策略来计算分割，其中它试图在文本的自然边界上执行分割，而不是在正好一千个字符的地方进行硬切割。它首先尝试在新行上分割，然后是句子，然后是标点符号，然后是空格。作为最后的手段，它将在单词的中间进行分割。
- en: The overlap means that the next chunk won’t start at the immediate next character
    but around two hundred characters back. If we have these overlaps, some text will
    be included twice in the data—and that’s OK. It means that we won’t lose content
    by splitting in the middle of a sentence, etc. You should explore the size of
    the chunk and overlap based on what suits your scenario. Larger chunks like this
    will be faster to search because there will be fewer chunks than if they were
    smaller, but it also lowers the likelihood of the chunks being very similar to
    your prompt if the prompt is shorter than the chunk size.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重叠意味着下一个块不会从下一个字符立即开始，而是在大约两百个字符之前开始。如果我们有这些重叠，一些文本会在数据中出现两次——这是可以的。这意味着我们不会在句子中间分割时丢失内容，等等。你应该根据你的场景探索块的大小和重叠。像这样的较大块将更快地搜索，因为块的数量会比它们更小，但这也降低了块与提示非常相似的可能性，如果提示的长度小于块的大小。
- en: The splitter provides the ability for you to specify your own length function
    if you want to measure length differently. In this case, I’m just using Python’s
    default `len` function. Typically, for a RAG like this, you may not need to override
    the `len` function, but the idea is that different models and encoders may count
    tokens in different ways. For example, GPT 3.5 recognizes a phrase like `lol`
    as a single token, but an emoji can be four tokens.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分割器提供了让你指定自己的长度函数的能力，如果你想要以不同的方式测量长度。在这种情况下，我只是在使用 Python 的默认 `len` 函数。通常，对于像这样的
    RAG，你可能不需要覆盖 `len` 函数，但这个想法是不同的模型和编码器可能会以不同的方式计算标记。例如，GPT 3.5 将 `lol` 这样的短语识别为一个单独的标记，但一个表情符号可以是四个标记。
- en: The `add_start_index` parameter adds metadata to each chunk, indicating where
    it was located in the original text. This is useful for debugging, in which you
    can trace back where each chunk came from or provide things like citations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`add_start_index` 参数为每个块添加元数据，指示它在原始文本中的位置。这对于调试很有用，你可以回溯每个块来自哪里，或者提供类似引用的东西。'
- en: 'Once you’ve specified the text, you can use it to split the PDF into multiple
    texts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你指定了文本，你可以用它来将 PDF 分割成多个文本：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that you have the texts, you can turn them into embeddings by using the
    `OpenAIEmbeddings` class, and you can also specify that you want a vector store
    using Chroma by passing it the documents:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了文本，你可以通过使用 `OpenAIEmbeddings` 类将它们转换为嵌入，你也可以指定使用 Chroma 来存储向量，通过传递文档：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As shown, you then simply pass the texts and embeddings you specified and a
    directory to store the embeddings. Then save the vector store to disk with this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，你只需简单地传递你指定的文本和嵌入以及存储嵌入的目录。然后使用以下命令将向量存储保存到磁盘：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `OpenAIEmbeddings` requires an `OPENAI_API_KEY` environment variable. You
    can get one at the [Open AIPlatform website](https://oreil.ly/41hwI) and then
    follow the instructions for your operating system by setting one. Make sure you
    name it exactly as shown.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`OpenAIEmbeddings` 需要一个 `OPENAI_API_KEY` 环境变量。你可以在 [Open AIPlatform 网站](https://oreil.ly/41hwI)
    上获取一个，然后根据你的操作系统设置一个，确保你将其命名为显示的确切名称。'
- en: The underlying database is an SQLite3 one (see [Figure 18-5](#ch18_figure_5_1748550073457676)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基础数据库是一个 SQLite3 数据库（见[图 18-5](#ch18_figure_5_1748550073457676)）。
- en: '![](assets/aiml_1805.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_1805.png](assets/aiml_1805.png)'
- en: Figure 18-5\. The directory containing the ChromaDB content
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-5\. 包含 ChromaDB 内容的目录
- en: This gives you the ability to browse and inspect the database by using any tools
    that work with SQLite. So, for example, you can use the free [DB Browser for SQLite](https://sqlitebrowser.org)
    to access the data (see [Figure 18-6](#ch18_figure_6_1748550073457698)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这让你能够使用任何与 SQLite 兼容的工具来浏览和检查数据库。例如，你可以使用免费的 [DB Browser for SQLite](https://sqlitebrowser.org)
    来访问数据（见[图 18-6](#ch18_figure_6_1748550073457698)）。
- en: '![](assets/aiml_1806.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_1806.png](assets/aiml_1806.png)'
- en: Figure 18-6\. Browsing data in the SQLite browser
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-6\. 在 SQLite 浏览器中浏览数据
- en: Now that we have the vector store, let’s explore what happens when we want to
    search it for similar text.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了向量存储，让我们探索当我们想要在其中搜索相似文本时会发生什么。
- en: Performing a Similarity Search
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行相似性搜索
- en: Once you have the vector store set up, it’s easy to search it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了向量存储，搜索它就很容易了。
- en: 'Here’s a function you can use to perform a similarity search with the vector
    store:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个你可以用来使用向量存储执行相似性搜索的函数：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, it’s pretty straightforward! You can override or extend some
    of the functionality if you like with optional parameters, including the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这很简单！如果你喜欢，你可以使用可选参数覆盖或扩展一些功能，包括以下内容：
- en: Search_type
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索类型
- en: This defaults to `similarity` but can also be `mmr` for *maximum marginal relevance*
    (MMR), which is worth experimenting with as you build out production systems.
    MMR is particularly useful when you want to avoid redundant results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下为`similarity`，但也可以设置为`mmr`，代表*最大边际相关性*（MMR），在构建生产系统时值得尝试。MMR在您希望避免冗余结果时特别有用。
- en: Distance_metric
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Distance_metric
- en: This defaults to `cosine`, as we saw earlier, but it can also be `l2`, which
    is the *distance*—effectively, the straight-line distance between the two vectors
    in the embedding space. Alternatively, it can be `ip` for *inner product*, which
    provides a very fast calculation but at the cost of lower accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下为`cosine`，正如我们之前所看到的，但它也可以是`l2`，这代表*距离*——实际上，是嵌入空间中两个向量之间的直线距离。或者，它可以是`ip`，代表*内积*，这提供了一种非常快速的计算方法，但代价是精度较低。
- en: Lambda_mult
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda_mult
- en: This is an optional value between 0 and 1 that you use to control the strictness
    of the distance measurement. A value of 1.0 will give highly relevant scores,
    and a value of 0.0 will give much more diverse scores.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个介于0和1之间的可选值，您可以使用它来控制距离测量的严格程度。值为1.0将给出高度相关的评分，而值为0.0将给出更多样化的评分。
- en: As you build systems, I recommend that you try multiple approaches to see which
    works best for your scenario.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建系统时，我建议您尝试多种方法，以查看哪种方法最适合您的场景。
- en: Putting It All Together
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'Now, you can use code like the following to take your PDF, slice and store
    it as vectors in the store, and run a query against it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用以下代码来处理您的PDF文件，将其切割并存储为存储中的向量，然后对其运行查询：
- en: '[PRE7] `results` `=` `search_vectorstore``(``vectorstore``,` `query``,` `5``)`
    [PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE7] `results` `=` `search_vectorstore``(``vectorstore``,` `query``,` `5``)`
    [PRE8]'
- en: '[PRE9]`When running this, I got detailed results about her character. Here
    are some snippets:    [PRE10]    So, when we’re making a query about the character
    to an LLM, we have all this extra content. We’ll explore that next.[PRE11]``  [PRE12][PRE13]``py[PRE14]`py`
    [PRE15] [PRE16]`` [PRE17]` Your results will vary, based on the temperature, the
    slicing size for the chunks, and various other factors.    One thing to note is
    that you can also use a *really* small model like Gemma2b and still get really
    good results. However, the context window of a model this small could have issues
    when you’re retrieving and augmenting your query with lots of information. As
    you saw earlier in this chapter, we were using one-thousand-character chunks,
    and we’re retrieving the 10 closest ones to the prompt. This is already in order
    of 10 k characters, and depending on the tokenization strategy, that could be
    more than 10 k tokens. Given that the context window for that model is only 2
    k tokens, you could hit a problem. Watch out for that!    ## Extending to Hosted
    Models    In the example we just walked through, we used smaller models like Llama
    and Gemma to perform RAG on a local Ollama server. If you want to use larger,
    hosted models like GPT, the process is exactly the same. One change I would make,
    though, is with the system prompt. Given that these models have huge amounts of
    parameters that have learned a lot, it’s good to unshackle them a bit and not
    expect them to be limited solely to the context provided!    For example, for
    GPT, you can import classes that support OpenAI’s GPT models like this:    [PRE18]    You
    can then instantiate this class like this:    [PRE19]    The model value is a
    string containing the name of the model you want to use. For example, you could
    use `gpt-3.5-turbo` or `gpt-4`. Check the [OpenAI API documentation for model
    versions](https://oreil.ly/SVBXr) available at the time you’re reading this.    Then,
    you can create the prompt very simply. First, create a prompt template to hold
    the system and user prompts:    [PRE20]`` `"Please provide as much detail as possible
    in a comprehensive` [PRE21] `(``"system"``,` `"Context:``\n``{context}``"``),`     `(``"user"``,`
    `"``{question}``"``)` `])` [PRE22]` [PRE23]   [PRE24] [PRE25]`py [PRE26]py`` [PRE27]py[PRE28][PRE29]
    `` `# Summary    In this chapter, you dipped your toes into the RAG waters, where
    you learned a powerful technique that enhances the capabilities of LLMs by combining
    their general understanding skills with local, private data. You saw how RAG works
    by creating a vector database with the contents of a book, and then you searched
    that database for information that was relevant to your given prompts.    We also
    explored querying a character from the book to learn more about her—and despite
    models like Llama and GPT not being trained on content about her, they were able
    to artificially understand the text and provide great information and analysis.    You
    also explored tools like ChromaDB (for vector storage) and pretrained embeddings
    (such as OpenAIs for vector encoding of text allowing similarity searches). You
    also explored various models that could be enhanced by using RAG, both small and
    local ones (like Llama and Gemma with Ollama) and large hosted models (like GPT
    via the OpenAI API). This took you through the process end to end: slicing text,
    encoding it, storing it, searching it based on similarity, and bundling it with
    a prompt to a model to perform RAG.    In the next chapter, we’ll shift gears
    a bit to another exciting aspect of AI: generative image models. We’ll explore
    a number of different models that provide images from text prompts, and we’ll
    dig down a little into how they work.` `` [PRE30][PRE31][PRE32]py[PRE33]`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE9]`当运行这个程序时，我得到了关于她性格的详细信息。以下是一些片段：    [PRE10]    因此，当我们向一个LLM查询角色信息时，我们会有所有这些额外内容。我们将在下一节中探讨这一点。[PRE11]``  [PRE12][PRE13]``py[PRE14]`py`
    [PRE15] [PRE16]`` [PRE17]` 你的结果将因温度、块的大小以及各种其他因素而异。    一件需要注意的事情是，你也可以使用一个非常小的模型，比如Gemma2b，仍然可以得到非常好的结果。然而，如此小的模型的上下文窗口在检索和增强你的查询时可能会出现问题。正如你在这章前面看到的，我们使用了一千字符的块，并检索与提示最接近的10个。这已经是10
    k字符的顺序，根据分词策略，这可能会超过10 k个标记。鉴于该模型的上下文窗口仅为2 k个标记，你可能会遇到问题。请注意这一点！    ## 扩展到托管模型    在我们刚刚讨论的例子中，我们使用了像Llama和Gemma这样的较小模型，在本地Ollama服务器上执行RAG。如果你想使用像GPT这样的大规模托管模型，过程完全相同。不过，我会做一个小改动，那就是系统提示。鉴于这些模型拥有大量参数，并且已经学习了很多，最好让它们稍微自由一些，不要期望它们仅限于提供的上下文！    例如，对于GPT，你可以导入支持OpenAI的GPT模型的类，如下所示：    [PRE18]    然后，你可以这样实例化这个类：    [PRE19]    模型值是一个包含你想要使用的模型名称的字符串。例如，你可以使用`gpt-3.5-turbo`或`gpt-4`。检查你阅读此内容时的[OpenAI
    API文档中的模型版本](https://oreil.ly/SVBXr)。    然后，你可以非常简单地创建提示。首先，创建一个提示模板来保存系统和用户提示：    [PRE20]``
    `"请尽可能详细地提供一个全面的` [PRE21] `(``"system"``,` `"Context:``\n``{context}``"``),`     `(``"user"``,`
    `"``{question}``"``)` `])` [PRE22]` [PRE23]   [PRE24] [PRE25]`py [PRE26]py`` [PRE27]py[PRE28][PRE29]
    `` `# 摘要    在本章中，你涉足了RAG的领域，学习了一种强大的技术，通过结合LLM的通用理解技能和本地、私有数据，增强了LLM的能力。你通过创建包含一本书内容的向量数据库，然后搜索该数据库以查找与你的给定提示相关的信息来了解RAG是如何工作的。    我们还探讨了查询书中的角色以了解更多关于她的信息——尽管像Llama和GPT这样的模型没有在关于她的内容上进行训练，但它们能够人为地理解文本并提供出色的信息和分析。    你还探索了像ChromaDB（用于向量存储）和预训练嵌入（如OpenAIs用于文本的向量编码，允许相似度搜索）这样的工具。你还探索了可以使用RAG增强的各种模型，包括小型本地模型（如Llama和Gemma与Ollama）和大型托管模型（如通过OpenAI
    API的GPT）。这带你完成了整个过程：切片文本、编码它、存储它、基于相似度搜索它，并将它捆绑到提示中，以执行RAG。    在下一章中，我们将稍微转换一下方向，探讨AI的另一个令人兴奋的方面：生成式图像模型。我们将探索提供文本提示的图像的多个不同模型，并深入探讨它们是如何工作的。`
    `` [PRE30][PRE31][PRE32]py[PRE33]`'
