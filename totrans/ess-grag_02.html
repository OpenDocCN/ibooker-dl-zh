<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">3</span> </span> <span class="chapter-title-text">Advanced vector retrieval strategies</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers </h3>
<ul>
<li class="readable-text" id="p2">Query rewriting techniques</li>
<li class="readable-text" id="p3">Advanced text-embedding strategies</li>
<li class="readable-text" id="p4">Implementing parent document retrieval </li>
</ul>
</div>
<div class="readable-text" id="p5">
<p>In chapter 2 of this book, you learned about the basics of text embeddings and vector similarity search. By converting text into numerical vectors, you have seen how machines can understand the semantic meaning of content. Combining text-embedding and vector similarity search techniques allows for optimized and accurate retrieval of relevant unstructured text from vast amounts of documents, enabling more accurate and up-to-date answers in RAG applications. Suppose you have implemented and deployed a RAG application as described in chapter 2. After some testing, you and the users of the RAG application noticed that the accuracy of the generated answers is lacking due to incomplete or irrelevant information in the retrieved documents. Consequently, you have been assigned the task of enhancing the retrieval system to improve the accuracy of the generated answers. </p>
</div>
<div class="readable-text intended-text" id="p6">
<p>As with any technology, the basic implementations of text embeddings and vector similarity search can produce insufficient retrieval accuracy and recall. The embeddings generated from a user’s query might not always align closely with those of documents containing the crucial information needed due to differences in terminology or context. This discrepancy can lead to situations where documents highly relevant to the query’s intent are overlooked, as the embedding representation of the query does not capture the essence of the information sought. </p>
</div>
<div class="readable-text intended-text" id="p7">
<p>One strategy to improve the retrieval accuracy and recall is to rewrite the query used to find relevant documents. The query-rewriting approach aims to bridge the gap between the user’s query and the information-rich documents by reformulating the query in a way that better aligns with the language and context of the target documents. This query refinement improves the chances of finding documents containing relevant information, thereby enhancing the accuracy of responses to the original query. Examples of query-rewriting strategies are hypothetical document retriever (Gao et al., 2022) or step-back prompting (Zheng et al., 2023). The step-back prompting strategy is visualized in figure 3.1. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p8">
<img alt="figure" height="324" src="../Images/3-1.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.1</span> Query rewriting by using the step-back technique to increase the vector retrieval accuracy</h5>
</div>
<div class="readable-text" id="p9">
<p>Figure 3.1 outlines a process where a user’s query is transformed to improve document retrieval outcomes, a technique known as <em>step-back prompting</em>. In the scenario presented, the user poses a detailed question regarding Estella Leopold’s educational history during a specific timeframe. This initial question is then processed by a language model such as GPT-4 with query-rewriting capabilities, which rephrases it into a more general inquiry about Estella Leopold’s educational background. The purpose of this step is to cast a wider net during the search process, as the rewritten query is more likely to align with a range of documents that may contain the required information.</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>Another way to improve retrieval accuracy is by changing the document embedding strategy. In the previous chapter, you embedded a section of text, retrieved that same text, and used it as input to an LLM to generate an answer. However, vector retrieval systems are flexible, as you’re not limited to embedding the exact text you plan to retrieve. Instead, you can embed content that better represents the document’s meaning, such as more contextually relevant sections, synthetic questions, or paraphrased versions. These alternatives can better capture key ideas and themes, resulting in more accurate and relevant retrieval. Two examples of advanced embedding strategies are shown in figure 3.2. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p11">
<img alt="figure" height="749" src="../Images/3-2.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.2</span> Hypothetical question and parent document retriever strategies</h5>
</div>
<div class="readable-text" id="p12">
<p>The left side of figure 3.2 demonstrates the hypothetical question strategy. With the hypothetical question–embedding strategy, you must determine the questions the information in the document can answer. For example, you could use an LLM to generate hypothetical questions, or you could use the conversation history of your chatbot to come up with the questions a document can answer. The idea is that instead of embedding the original document itself, you embed the questions the document can answer. For instance, the question “What did Leopold study at the University of California?” is encoded by the vector <code>[1,2,3,0,5]</code> in figure 3.2. When a user poses a question, the system computes the query’s embedding and searches for the nearest neighbors among the precomputed question embeddings. The goal is to locate questions that closely match and are semantically similar to the user question. The system then retrieves the documents that contain the information that can answer these similar questions. In essence, the hypothetical question–embedding strategy involves embedding potential questions a document can answer and using these embeddings to match and retrieve relevant documents in response to user queries. </p>
</div>
<div class="readable-text intended-text" id="p13">
<p>The right side of figure 3.2 illustrates the parent document–embedding strategy. In this approach, the original document—referred to as the parent—is split into smaller units called <em>child chunks</em>, typically based on a fixed token count. Instead of embedding the entire parent document as a single unit, you compute a separate embedding for each child chunk. For example, the chunk “Leopold attained her master’s in botany” might be embedded as the vector <code>[1,</code> <code>0,</code> <code>3,</code> <code>0,</code> <code>1]</code>. When a user submits a query, the system compares it against these child embeddings to find the most relevant matches. However, rather than returning only the matched chunk, the system retrieves the entire original parent document associated with it. This allows the language model to operate with the full context of the information, increasing the chances of generating accurate and complete answers. </p>
</div>
<div class="readable-text intended-text" id="p14">
<p>This strategy addresses a common limitation of embedding long documents: when you embed the full parent document, the resulting vector can blur distinct ideas through averaging, making it harder to match specific queries effectively. By contrast, splitting the document into smaller chunks allows for more precise matching while still enabling the system to return the full context when needed.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p15">
<h5 class="callout-container-h5 readable-text-h5">Other strategies to improve retrieval accuracy</h5>
</div>
<div class="readable-text" id="p16">
<p>Beyond changing the document-embedding strategy, several other techniques can enhance retrieval accuracy:</p>
</div>
<ul>
<li class="readable-text" id="p17"> <em>Finetuning the text-embedding model</em>—By adjusting the embedding model on domain-specific data, you can improve its ability to capture the context of user queries, leading to a closer semantic match with relevant documents. Note that finetuning typically requires more compute and infrastructure. In addition, once the model is updated, all existing document embeddings must be recomputed to reflect the changes—this can be resource intensive for large document repositories. </li>
<li class="readable-text" id="p18"> <em>Reranking strategies</em>—After an initial set of documents is retrieved, reranking algorithms can reorder them based on relevance to the user’s intent. This second pass often uses more complex models or scoring heuristics to refine the results. Reranking helps surface the most relevant content even if the initial match was suboptimal. </li>
<li class="readable-text" id="p19"> <em>Metadata-based contextual filtering</em>—Many documents contain structured metadata such as authorship, publication date, topic tags, or source type. Applying filters based on this metadata—either manually or as part of the retrieval pipeline—can significantly narrow the candidate documents before semantic </li>
</ul>
<ul>
<li class="readable-text" id="p20"> matching, increasing precision. For example, a query about recent policy updates can be restricted to documents published within the last year. </li>
<li class="readable-text" id="p21"> <em>Hybrid retrieval (keyword + dense vector search)</em>—Combining sparse retrieval (e.g., keyword-based search) with dense vector retrieval (semantic search) offers the best of both worlds. Keyword search excels at precise matches and rare terms, while dense retrieval captures the broader meaning. Hybrid systems can merge and rerank results from both methods to maximize both recall and precision. </li>
</ul>
<div class="readable-text" id="p22">
<p>While all these strategies can improve retrieval quality, detailed implementation guidance is beyond the scope of this book, except for hybrid retrieval, which was introduced in chapter 2.</p>
</div>
</div>
<div class="readable-text" id="p23">
<p>In the remainder of this chapter, we’ll move from concepts to code and walk through the implementation step by step. To follow along, you’ll need access to a running, blank Neo4j instance. This can be a local installation or a cloud-hosted instance; just make sure it’s empty. You can follow the implementation directly in the accompanying Jupyter notebook available here: <a href="https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch03.ipynb">https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch03.ipynb</a>. </p>
</div>
<div class="readable-text intended-text" id="p24">
<p>Imagine you’ve implemented the basic RAG system from chapter 2, but the retrieval accuracy wasn’t quite good enough. The responses lacked relevance or missed important context, and you suspect the system isn’t retrieving the most useful documents to support high-quality answers. To address this, you’ve decided to enhance the existing RAG pipeline by adding a step-back prompting step to improve the quality of the query itself. Additionally, you’ll switch from the basic retriever to a parent document retriever strategy. This approach enables more granular and accurate information retrieval by matching on smaller chunks while still providing the full parent document as context.</p>
</div>
<div class="readable-text intended-text" id="p25">
<p>These improvements aim to boost both the relevance of retrieved content and the overall accuracy of the generated answers.</p>
</div>
<div class="readable-text" id="p26">
<h2 class="readable-text-h2"><span class="num-string">3.1</span> Step-back prompting</h2>
</div>
<div class="readable-text" id="p27">
<p>As mentioned, step-back prompting is a query-rewriting technique that aims to improve the accuracy of vector retrieval. An example from the original paper (Zheng et al., 2023) demonstrates this process: the specific query “Which team did Thierry Audel play for from 2007 to 2008?” is broadened to “Which teams did Thierry Audel play for in his career?” to improve vector search precision and consequently the accuracy of the generated answers. By transforming a detailed question into a broader, high-level query, step-back prompting reduces the complexity of the vector search process. The idea is that broader queries typically encompass a more comprehensive range of information, making it easier for the model to identify relevant facts without getting bogged down by the specifics.</p>
</div>
<div class="readable-text intended-text" id="p28">
<p>The authors used an LLM for the query rewriting task, as shown in figure 3.3.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p29">
<img alt="figure" height="429" src="../Images/3-3.png" width="874"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.3</span> Rewriting queries using the step-back approach with an LLM</h5>
</div>
<div class="readable-text" id="p30">
<p>LLMs are an excellent fit for query-rewriting tasks as they excel at natural language comprehension and generation. You don’t have to train or finetune a new model for each task. Instead, you can provide task instructions in the input prompt.</p>
</div>
<div class="readable-text intended-text" id="p31">
<p>The authors of the step-back prompting paper used the system prompt in the following listing to instruct the LLM on how to rewrite the input query.</p>
</div>
<div class="browsable-container listing-container" id="p32">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.1</span> System prompt of an LLM for generating step-back questions</h5>
<div class="code-area-container">
<pre class="code-area">stepback_system_message = f"""    
You are an expert at world knowledge. Your task is to step back<span class="aframe-location"/> #1
and paraphrase a question to a more generic step-back question, which
is easier to answer. Here are a few examples

"input": "Could the members of The Police perform lawful arrests?"<span class="aframe-location"/> #2
"output": "what can the members of The Police do?"

"input": "Jan Sindel’s was born in what country?"
"output": "what is Jan Sindel’s personal history?"
"""</pre>
<div class="code-annotations-overlay-container">
     #1 Query rewriting instructions
     <br/>#2 Few-shot examples
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p33">
<p>The system prompt in listing 3.1 begins by giving the LLM a simple instruction to rewrite a user’s question into a more generic, step-back version. On its own, this kind of instruction is known as <em>zero-shot prompting</em>, which relies solely on the LLM’s general capabilities and understanding of the task, without providing any examples. However, to guide the model more effectively and ensure consistent results, the authors chose to expand the prompt with several examples of the desired paraphrasing behavior. This technique is called <em>few-shot prompting</em>, where a small number of examples (typically two to five) are included in the prompt to illustrate the task. Few-shot prompting helps the LLM better understand the expected transformation by anchoring it in concrete instances, improving the quality and reliability of the output.</p>
</div>
<div class="readable-text intended-text" id="p34">
<p>To achieve the query rewriting, all you need to do is send the system prompt found in listing 3.1 along with the user’s question to an LLM. The specific function for this task is outlined in the next listing.</p>
</div>
<div class="browsable-container listing-container" id="p35">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.2</span> Function to generate a step-back question</h5>
<div class="code-area-container">
<pre class="code-area">def generate_stepback(question: str):
    user_message = f"""{question}"""
    step_back_question = chat(
        messages=[
            {"role": "system", "content": stepback_system_message},
            {"role": "user", "content": user_message},
        ]
    )
    return step_back_question</pre>
</div>
</div>
<div class="readable-text" id="p36">
<p>You can now test the step-back prompt generation by executing the code shown next.</p>
</div>
<div class="browsable-container listing-container" id="p37">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.3</span> Executing the step-back prompt function</h5>
<div class="code-area-container">
<pre class="code-area">question = "Which team did Thierry Audel play for from 2007 to 2008?"
step_back_question = generate_stepback(question)
print(f"Stepback results: {step_back_question}")
# Stepback results: What is the career history of Thierry Audel?</pre>
</div>
</div>
<div class="readable-text" id="p38">
<p>The results in listing 3.3 demonstrate a successful execution of the step-back prompt generation function. By transforming the specific query about Thierry Audel’s team from 2007 to 2008 into a broader question regarding his entire career history, the function effectively broadens the context and should increase the retrieval accuracy and recall.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p39">
<h5 class="callout-container-h5 readable-text-h5">Exercise 3.1</h5>
</div>
<div class="readable-text" id="p40">
<p>To explore the step-back prompt generation’s effectiveness, try applying it to various questions and observe how it broadens the context. You can also change the system prompt to observe how it affects the output.</p>
</div>
</div>
<div class="readable-text" id="p41">
<h2 class="readable-text-h2"><span class="num-string">3.2</span> Parent document retriever</h2>
</div>
<div class="readable-text" id="p42">
<p>The parent document retriever strategy involves dividing a large document into smaller sections, calculating embeddings for each section rather than the whole document, and using these embeddings to match user queries more accurately, ultimately retrieving the entire document for context-rich responses. However, as you cannot feed the whole PDF directly to the LLM, you first need to split the PDF into parent documents and then further divide those parent documents into child documents for embedding and retrieval. The graph representation of the parent and child documents is shown in figure 3.4. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p43">
<img alt="figure" height="419" src="../Images/3-4.png" width="1009"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.4</span> Parent document graph representation</h5>
</div>
<div class="readable-text" id="p44">
<p>Figure 3.4 illustrates a graph-based approach to storing and organizing documents for the parent document retrieval strategy. At the top, a PDF node represents the entire document, labeled with a title and an identifier. This node is connected to several parent document nodes. You will use a 2,000-character limit to split the PDF into parent documents in this example. These parent document nodes are, in turn, linked to child document nodes, with each child node containing a 500-character chunk of the corresponding parent node text. The child nodes have an embedding vector representing the child chunk of the text for retrieval purposes.</p>
</div>
<div class="readable-text intended-text" id="p45">
<p>We will be using the same text as in chapter 2, which is a paper titled “Einstein’s Patents and Inventions” by Asis Kumar Chaudhuri (<a href="https://arxiv.org/abs/1709.00666">https://arxiv.org/abs/1709.00666</a>). Additionally, when segmenting a document into smaller parts for processing, it is best to start by splitting the text based on structural elements like paragraphs or sections. This approach maintains the coherence and context of the content, as paragraphs or sections typically encapsulate complete ideas or topics. Therefore, we will start by splitting the PDF text into sections.</p>
</div>
<div class="browsable-container listing-container" id="p46">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.4</span> Splitting the text into sections with a regular expression</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">import re
def split_text_by_titles(text):
    # A regular expression pattern for titles that
    # match lines starting with one or more digits, an optional uppercase letter,
    # followed by a dot, a space, and then up to 60 characters
    title_pattern = re.compile(r"(\n\d+[A-Z]?\. {1,3}.{0,60}\n)", re.DOTALL)
    titles = title_pattern.findall(text)
    # Split the text at these titles
    sections = re.split(title_pattern, text)
    sections_with_titles = []
    # Append the first section
    sections_with_titles.append(sections[0])
    # Iterate over the rest of the sections
    for i in range(1, len(titles) + 1):
        section_text = sections[i * 2 - 1].strip() + "\n" +
<span class="">↪</span> sections[i * 2].strip()
        sections_with_titles.append(section_text)

    return sections_with_titles

sections = split_text_by_titles(text)
print(f"Number of sections: {len(sections)}")
# Number of sections: 9</pre>
</div>
</div>
<div class="readable-text" id="p47">
<p>The <code>split_text_by_titles</code> function in listing 3.4 uses a regular expression to split the text by sections. The regular expression is based on the fact that sections in the text are organized as a numbered list, where each new section starts with a number and an optional character, followed by a dot and the section title. The output of the <code>split_ text_by_titles</code> function is nine sections. If you check the PDF, you will notice only four main sections. However, there are also four subsections (3A–3D) describing some of the patents, and if you count the introduction abstract as its own section, you get a total of nine sections. </p>
</div>
<div class="readable-text intended-text" id="p48">
<p>Before continuing with the parent document retriever, you will count the number of tokens per section to better understand their length. You will use the <code>tiktoken</code>, a package developed by OpenAI, to count the number of tokens in a given text. </p>
</div>
<div class="browsable-container listing-container" id="p49">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.5</span> Counting the number of tokens in sections</h5>
<div class="code-area-container">
<pre class="code-area">def num_tokens_from_string(string: str, model: str = "gpt-4") -&gt; int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.encoding_for_model(model)
    num_tokens = len(encoding.encode(string))
    return num_tokens

for s in sections:
    print(num_tokens_from_string(s))
# 154, 254, 4186, 570, 2703, 1441, 194, 600</pre>
</div>
</div>
<div class="readable-text" id="p50">
<p>Most sections have a relatively small size of up to 600 tokens, which fits most LLM context prompts. However, the third section has over 4,000 tokens, which could lead to token limit errors during LLM generation. Therefore, you must split the sections into parent documents, where each document has at most 2,000 characters. You will use the <code>chunk_text</code> from the previous chapter to achieve this.</p>
</div>
<div class="browsable-container listing-container" id="p51">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.6</span> Splitting sections into parent documents of max size of 2,000 characters</h5>
<div class="code-area-container">
<pre class="code-area">parent_chunks = []
for s in sections:
    parent_chunks.extend(chunk_text(s, 2000, 40))</pre>
</div>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p52">
<h5 class="callout-container-h5 readable-text-h5">Exercise 3.2</h5>
</div>
<div class="readable-text" id="p53">
<p>Use the <code>num_tokens_from_string</code> function to determine the token count of each parent document. The token count can help you decide about additional steps in the preprocessing. For instance, longer sections that exceed a reasonable token count should be split further. On the other hand, if some segments are exceptionally brief, consisting of 20 tokens or fewer, you should consider eliminating them entirely as they might not add any information value. </p>
</div>
</div>
<div class="readable-text" id="p54">
<p>Instead of splitting the child chunks and importing them in a subsequent step, you will perform the splitting and the import in a single step. Performing the two operations in a single step allows you to skip slightly more complex data structures storing intermediate results. Before importing the graph, you need to define the import Cypher statement. The Cypher statement to import the parent document structure is relatively straightforward. </p>
</div>
<div class="browsable-container listing-container" id="p55">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.7</span> Cypher query used to import the parent document strategy graph</h5>
<div class="code-area-container">
<pre class="code-area">cypher_import_query = """   <span class="aframe-location"/> #1
MERGE (pdf:PDF {id:$pdf_id})   <span class="aframe-location"/> #2
MERGE (p:Parent {id:$pdf_id + '-' + $id})
SET p.text = $parent
MERGE (pdf)-[:HAS_PARENT]-&gt;(p)   <span class="aframe-location"/> #3
WITH p, $children AS children, $embeddings as embeddings
UNWIND range(0, size(children) - 1) AS child_index
MERGE (c:Child {id: $pdf_id + '-' + $id + '-' + toString(child_index)})
SET c.text = children[child_index], c.embedding = embeddings[child_index]
MERGE (p)-[:HAS_CHILD]-&gt;(c);
"""</pre>
<div class="code-annotations-overlay-container"> #1 Merges PDF node based on the id property
     <br/>#2 Merges Parent node and set its text property
     <br/>#3 Merges multiple Child nodes for each Parent node
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p56">
<p>The Cypher statement in listing 3.7 starts by merging a <code>PDF</code> node. Next, it merges the <code>Parent</code> node using a unique ID. The <code>Parent</code> node is then linked to the <code>PDF</code> node through a <code>HAS_PARENT</code> relationship and has the <code>text</code> property set. Lastly, it iterates over a list of child documents. It creates a <code>Child</code> node for each element in the list, sets the text and embedding properties, and links it to its <code>Parent</code> node with a <code>HAS_CHILD</code> relationship. </p>
</div>
<div class="readable-text intended-text" id="p57">
<p>Now that everything is prepared, you can import the parent document structure into the graph database.</p>
</div>
<div class="browsable-container listing-container" id="p58">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.8</span> Importing the parent document data into the graph database</h5>
<div class="code-area-container">
<pre class="code-area">for i, chunk in enumerate(parent_chunks):

    child_chunks = chunk_text(chunk, 500, 20)<span class="aframe-location"/> #1

    embeddings = embed(child_chunks)<span class="aframe-location"/> #2
    # Add to neo4j

    neo4j_driver.execute_query(<span class="aframe-location"/> #3
        cypher_import_query,
        id=str(i),
        pdf_id='1709.00666'
        parent=chunk,
        children=child_chunks,
        embeddings=embeddings,
    )</pre>
<div class="code-annotations-overlay-container"> #1 Splits the parent documents into child chunks
     <br/>#2 Calculates text embeddings for child chunks
     <br/>#3 Imports into Neo4j
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p59">
<p>The code in listing 3.8 starts by iterating over the parent document chunks. Each parent document chunk is divided into multiple child chunks using the <code>chunk_text</code> function. The code then calculates text embeddings for these child chunks with the <code>embed</code> function. Following the embedding generation, the <code>execute_query</code> method imports the data into a Neo4j graph database. </p>
</div>
<div class="readable-text intended-text" id="p60">
<p>You can examine the generated graph structure by running the Cypher statement shown in the following listing in Neo4j Browser.</p>
</div>
<div class="browsable-container listing-container" id="p61">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.9</span> Create a vector index on child nodes</h5>
<div class="code-area-container">
<pre class="code-area">MATCH p=(pdf:PDF)-[:HAS_PARENT]-&gt;()-[:HAS_CHILD]-&gt;()
RETURN p LIMIT 25</pre>
</div>
</div>
<div class="readable-text" id="p62">
<p>The Cypher statement in listing 3.9 produces the graph shown in figure 3.5. This graph visualization depicts a central PDF node connected to several parent nodes, illustrating the hierarchical relationship between the document and its sections. Each parent node is further linked to multiple child nodes, indicating the breakdown of sections into smaller chunks within the document structure.</p>
</div>
<div class="readable-text intended-text" id="p63">
<p>To ensure efficient comparison of document embeddings, you will add a vector index.</p>
</div>
<div class="browsable-container listing-container" id="p64">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.10</span> Creating a vector index on child nodes</h5>
<div class="code-area-container">
<pre class="code-area">driver.execute_query("""CREATE VECTOR INDEX parent IF NOT EXISTS
FOR (c:Child)
ON c.embedding""")</pre>
</div>
</div>
<div class="readable-text" id="p65">
<p>The code to generate the vector index in listing 3.10 is identical to the one used in chapter 2. Here, you created a vector index on the <code>embedding</code> property of the <code>Child</code>. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p66">
<img alt="figure" height="1135" src="../Images/3-5.png" width="1100"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 3.5</span> Graph visualization of part of the imported data in Neo4j Browser </h5>
</div>
<div class="readable-text" id="p67">
<h3 class="readable-text-h3"><span class="num-string">3.2.1</span> Retrieving parent document strategy data</h3>
</div>
<div class="readable-text" id="p68">
<p>After importing the data and defining the vector index, you can focus on implementing the retrieval part. To retrieve relevant documents from the graph, you must define the retrieval Cypher statement described in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p69">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.11</span> Parent document retrieval Cypher statement</h5>
<div class="code-area-container">
<pre class="code-area">retrieval_query = """  
CALL db.index.vector.queryNodes($index_name, $k * 4, $question_embedding)<span class="aframe-location"/> #1
YIELD node, score     <span class="aframe-location"/> #2
MATCH (node)&lt;-[:HAS_CHILD]-(parent)  <span class="aframe-location"/> #3
WITH parent, max(score) AS score
RETURN parent.text AS text, score
ORDER BY score DESC   <span class="aframe-location"/> #4
LIMIT toInteger($k)
"""</pre>
<div class="code-annotations-overlay-container"> #1 Vector index search
     <br/>#2 Traverses to parent documents
     <br/>#3 Deduplicates parent documents
     <br/>#4 Ensures final limit
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p70">
<p>The Cypher statement in listing 3.11 starts by executing a vector-based search within a graph database to identify child nodes closely aligned with a specified question embedding. You can see that we retrieve <code>k</code> <code>*</code> <code>4</code> documents in the initial vector search. The reason for using the <code>k</code> <code>*</code> <code>4</code> value in the initial vector search is that you anticipate a scenario where multiple similar child nodes from the vector search may actually belong to the same parent document. Therefore, it becomes crucial to deduplicate the parent documents. Without deduplication, the result set could include multiple entries for the same parent document, each corresponding to a different child node of that parent. However, to guarantee a final count of <code>k</code> unique parent documents, you start with a larger pool of <code>k</code> <code>*</code> <code>4</code> child nodes, effectively creating a safety buffer. In the end of the Cypher statement, you enforce the final <code>k</code> limit.</p>
</div>
<div class="readable-text intended-text" id="p71">
<p>The function that utilizes the Cypher statement in listing 3.11 to retrieve parent documents from the database is shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p72">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.12</span> Parent document retrieval function</h5>
<div class="code-area-container">
<pre class="code-area">def parent_retrieval(question: str, k: int = 4) -&gt; List[str]:
    question_embedding = embed([question])[0]

    similar_records, _, _ = neo4j_driver.execute_query(
        retrieval_query,
        question_embedding=question_embedding,
        k=k,
        index_name=index_name,
    )

    return [record["text"] for record in similar_records]</pre>
</div>
</div>
<div class="readable-text" id="p73">
<p>The <code>parent_retrieval</code> function in listing 3.12 first generates a text embedding for a given question and then uses the previously mentioned Cypher statement to retrieve a list of the most relevant documents from the database. </p>
</div>
<div class="readable-text" id="p74">
<h2 class="readable-text-h2"><span class="num-string">3.3</span> Complete RAG pipeline</h2>
</div>
<div class="readable-text" id="p75">
<p>The last piece of the pipeline is the answer-generating function. </p>
</div>
<div class="browsable-container listing-container" id="p76">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 3.13</span> Generating answers with an LLM</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">system_message = "You're en Einstein expert, but can only use the<span class="">↪</span>
<span class="">↪</span> provided documents to respond to the questions."
def generate_answer(question: str, documents: List[str]) -&gt; str:
    user_message = f"""
    Use the following documents to answer the question that will follow:
    {documents}

    ---

    The question to answer using information only from the above<span class="">↪</span>
<span class="">↪</span> documents: {question}
    """
    result = chat(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message},
        ]
    )
    print("Response:", result)</pre>
</div>
</div>
<div class="readable-text" id="p77">
<p>The code in listing 3.13 is identical to that in chapter 2. You pass the question along with the relevant documents to an LLM and prompt it to generate an answer.</p>
</div>
<div class="readable-text intended-text" id="p78">
<p>After implementing the step-back prompting and parent document retrieval, you are ready to bring it all together in a single function.</p>
</div>
<div class="browsable-container listing-container" id="p79">
<h5 class="listing-container-h5 browsable-container-h5"><strong><span class="num-string">Listing 3.14</span> Complete parent document retriever with step-back prompting RAG pipeline</strong></h5>
<div class="code-area-container">
<pre class="code-area">def rag_pipeline(question: str) -&gt; str:
    stepback_prompt = generate_stepback(question)
    print(f"Stepback prompt: {stepback_prompt}")
    documents = parent_retrieval(stepback_prompt)
    answer = generate_answer(question, documents)
    return answer</pre>
</div>
</div>
<div class="readable-text" id="p80">
<p>The <code>rag_pipeline</code> function in listing 3.14 takes a question as input and creates a step-back prompt. It then retrieves related documents based on the step-back prompt and passes them along with the original question to an LLM to generate the final answer. </p>
</div>
<div class="readable-text intended-text" id="p81">
<p>You can now test the <code>rag_pipeline</code> implementation. </p>
</div>
<div class="browsable-container listing-container" id="p82">
<h5 class="listing-container-h5 browsable-container-h5"><strong><span class="num-string">Listing 3.15</span> Complete parent document retriever with step-back prompting RAG pipeline</strong></h5>
<div class="code-area-container">
<pre class="code-area">rag_pipeline("When was Einstein granted the patent for his blouse design?")
# Stepback prompt: What are some notable achievements in Einstein's life?
# Response: Einstein was granted the patent for his blouse design on October 27, 1936.</pre>
</div>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p83">
<h5 class="callout-container-h5 readable-text-h5">Exercise 3.3</h5>
</div>
<div class="readable-text" id="p84">
<p>Evaluate how well the <code>rag_pipeline</code> implementation performs by asking other questions about Einstein’s life mentioned in the PDF. Additionally, you can remove the step-back prompting step to compare if it improves the results. </p>
</div>
</div>
<div class="readable-text" id="p85">
<p>Congratulations! You have successfully implemented an advanced vector search retrieval strategy by combining query rewriting and parent document retrieval. </p>
</div>
<div class="readable-text" id="p86">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p87"> Query rewriting can enhance the accuracy of document retrieval by aligning user queries more closely with the language and context of target documents. </li>
<li class="readable-text" id="p88"> Techniques like hypothetical document retriever and step-back prompting effectively bridge the gap between the user’s intent and the document’s content, reducing the chances of missing relevant information. </li>
<li class="readable-text" id="p89"> The effectiveness of a retrieval system can be improved by embedding not just the exact text but also contextually relevant summaries or paraphrases, capturing the essence of documents. </li>
<li class="readable-text" id="p90"> Implementing strategies like hypothetical question embedding and parent document retrieval can lead to more precise matching between queries and documents, enhancing the relevance and accuracy of retrieved information. </li>
<li class="readable-text" id="p91"> Splitting documents into smaller, more manageable chunks for embedding purposes allows for a more granular approach to information retrieval, ensuring that specific queries find the most relevant document sections.  </li>
</ul>
</div></body></html>