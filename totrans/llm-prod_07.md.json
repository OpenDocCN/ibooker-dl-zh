["```py\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Simple Chat App</title>\n\n        <style>         #1\n            body {\n                font-family: Arial, sans-serif;\n                margin: 0;\n                padding: 0;\n                box-sizing: border-box;\n            }\n\n            #message-input {\n                width: 95%;\n                padding: 8px;\n            }\n\n            #chat-container {\n                width: 95%;\n                margin: 20px auto;\n                border: 1px solid #ccc;\n                padding: 10px;\n                overflow-y: scroll;\n                max-height: 300px;\n            }\n        </style>\n    </head>\n\n    <body>                                 #2\n        <form onsubmit=\"return false;\"\">\n            <input type=\"text\" id=\"message-input\" placeholder=\"Type your message...\">\n            <button onclick=\"sendMessage()\" type=\"submit\">Send</button>\n        </form>\n        <div id=\"chat-container\"></div>\n    </body>\n\n    <script>                           #3\n        function sendMessage() {                      #4\n            var messageInput = document.getElementById('message-input');\n            var message = messageInput.value.trim();\n\n            if (message !== '') {\n                appendMessage('You: ' + message);\n                messageInput.value = '';\n                sendToServer(message);\n            }\n        }\n\n        function appendMessage(message) {            #5\n            var chatContainer = document.getElementById('chat-container');\n            var messageElement = document.createElement('div');\n            messageElement.textContent = message;\n            chatContainer.appendChild(messageElement);\n            chatContainer.scrollTop = chatContainer.scrollHeight;\n            return messageElement\n        }\n\n        async function sendToServer(message) {       #6\n            var payload = {\n                prompt: message\n            }\n\n            const response = await fetch('http://localhost:8000/generate', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify(payload),\n            });\n\n            var responseText = 'LLM: ';\n            messageElement = appendMessage(responseText);\n\n            for await (const chunk of streamAsyncIterator(response.body)) {\n                var strChunk = String.fromCharCode.apply(null, chunk);\n                responseText += strChunk;\n                messageElement.textContent = responseText;\n            }\n        }\n\n        async function* streamAsyncIterator(stream) {      #7\n            const reader = stream.getReader();\n            try {\n                while (true) {\n                    const {done, value} = await reader.read();\n                    if (done) return;\n                    yield value;\n                }\n            }\n            finally {\n                reader.releaseLock();\n            }\n        }\n    </script>\n</html>\n```", "```py\nimport streamlit as st\nimport requests\nimport json\n\nurl = \"http://localhost:8000/generate\"    #1\n\nst.title(\"Chatbot with History\")\n\nif \"chat_history\" not in st.session_state:      #2\n    st.session_state.chat_history = []\n\nfor chat in st.session_state.chat_history:      #3\n    with st.chat_message(chat[\"role\"]):\n        st.markdown(chat[\"content\"])\n\nif user_input := st.chat_input(\"Your question here\"):     #4\n    with st.chat_message(\"user\"):                   #5\n        st.markdown(user_input)\n\n    st.session_state.chat_history.append(          #6\n        {\"role\": \"user\", \"content\": user_input}\n    )\n\n    with st.chat_message(\"assistant\"):     #7\n        placeholder = st.empty()\n        full_response = \"\"\n\n        prompt = \"You are an assistant who helps the user. \"     #8\n        \"Answer their questions as accurately as possible. Be concise. \"\n        history = [\n            f'{ch[\"role\"]}: {ch[\"content\"]}'\n            for ch in st.session_state.chat_history\n        ]\n        prompt += \" \".join(history)\n        prompt += \" assistant: \"\n        data = json.dumps({\"prompt\": prompt})\n\n        with requests.post(url, data=data, stream=True) as r:     #9\n            for line in r.iter_lines(decode_unicode=True):\n                full_response += line.decode(\"utf-8\")\n                placeholder.markdown(full_response + \"▌\")     #10\n        placeholder.markdown(full_response)\n\n    st.session_state.chat_history.append(      #11\n        {\"role\": \"assistant\", \"content\": full_response}\n    )\n```", "```py\nimport gradio as gr\nimport requests\nimport json\n\nurl = \"http://localhost:8000/generate\"      #1\n\ndef generate(message, history):\n    history_transformer_format = history + [[message, \"\"]]\n    messages = \"\".join(\n        [\n            \"\".join([\"\\n<human>:\" + h, \"\\n<bot>:\" + b])\n            for h, b in history_transformer_format\n        ]\n    )\n    data = json.dumps({\"prompt\": messages})\n\n    full_response = \"\"\n    with requests.post(url, data=data, stream=True) as r:   #2\n        for line in r.iter_lines(decode_unicode=True):\n            full_response += line.decode(\"utf-8\")\n            yield full_response + \"▌\"              #3\n        yield full_response\n\ngr.ChatInterface(generate, theme=\"soft\").queue().launch()\n```", "```py\nimport tiktoken\n\nencoding = tiktoken.get_encoding(\"cl100k_base\")\nprint(encoding.encode(\"You're users chat message goes here.\"))     \n# [2675, 2351, 3932, 6369, 1984, 5900, 1618, 13]\ndef count_tokens(string: str) -> int:\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoding.encode(string))\n\nnum_tokens = count_tokens(\"You're users chat message goes here.\")\nprint(num_tokens)       \n# 8\n```", "```py\nimport os\nimport pinecone\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains import RetrievalQAWithSourcesChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Pinecone\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")           #1\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")      #2\n\nindex_name = \"pincecone-llm-example\"      #3\nindex = pinecone.Index(index_name)\nembedder = OpenAIEmbeddings(\n    model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY\n)\ntext_field = \"text\"\nvectorstore = Pinecone(index, embedder.embed_query, text_field)\n\nquery = \"Who was Johannes Gutenberg?\"      #4\nvectorstore.similarity_search(\n    query, k=3                  #5\n)\n\nllm = ChatOpenAI(               #6\n    openai_api_key=OPENAI_API_KEY,\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0.0,\n)\n\nqa = RetrievalQA.from_chain_type(         #7\n    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n)\nqa.run(query)\nqa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(     #8\n    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n)\nqa_with_sources(query)\n```", "```py\n$ git clone https://github.com/ggerganov/llama.cpp.git\n$ cd llama.cpp\n$ pip install -r requirments/requirements-convert.txt\n$ python convert.py -h\n```", "```py\n$ pip install -U huggingface_hub\n$ huggingface-cli download TheBloke/WizardCoder-Python-7B-V1.0-GGUF --\n↪ local-dir ./models --local-dir-use-symlinks False --include='*Q2_K*gguf'\n```", "```py\nimport time\nfrom llama_cpp import Llama\n\nllm = Llama(model_path=\"./models/wizardcoder-python-7b-v1.0.Q2_K.gguf\")\n\nstart_time = time.time()\noutput = llm(\n    \"Q: Write python code to reverse a linked list. A: \",\n    max_tokens=200,\n    stop=[\"Q:\"],\n    echo=True,\n)\nend_time = time.time()\n\nprint(output[\"choices\"])\n```", "```py\n# [\n#     {'text': \"Q: Write python code to reverse a linked list. A: \n#         class Node(object):\n#             def __init__(self, data=None):\n#                 self.data = data\n#                 self.next = None\n\n#         def reverse_list(head):\n#             prev = None\n#             current = head\n#             while current is not None:\n#                 next = current.next\n#                 current.next = prev\n#                 prev = current\n#                 current = next\n#             return prev\n#             # example usage;\n#             # initial list\n#         head = Node('a')     \n#         head.next=Node('b')\n#         head.next.next=Node('c')\n#         head.next.next.next=Node('d')\n#         print(head)\n#          reverse_list(head) # call the function\n#         print(head)\n# Expected output: d->c->b->a\",\n#     'index': 0,         \n#     'logprobs': None,\n#     'finish_reason': 'stop'\n#     }\n# ]\n\nprint(f\"Elapsed time: {end_time - start_time:.3f} seconds\")\n# Elapsed time: 239.457 seconds\n```", "```py\nfrom langchain.tools import DuckDuckGoSearchRun, YouTubeSearchTool\n\nsearch = DuckDuckGoSearchRun()     #1\nhot_topic = search.run(\n    \"Tiktoker finds proof of Fruit of the Loom cornucopia in the logo\"\n)\n\nyoutube_tool = YouTubeSearchTool()\nfun_channel = youtube_tool.run(\"jaubrey\", 3)\n\nprint(hot_topic, fun_channel)\n```", "```py\n# Rating: False About this rating If asked to describe underwear\n# manufacturer Fruit of the Loom's logo from memory, some will invariably\n# say it includes — or at least included at some point in... A viral claim\n# recently surfaced stating that Fruit of the Loom, the American underwear\n# and casualwear brand, had a cornucopia in their logo at some point in the\n# past. It refers to a goat's... The Fruit of the Loom Mandela Effect is\n# really messing with people's memories of the clothing company's iconic\n# logo.. A viral TikTok has thousands of people not only thinking about what\n# they remember the logo to look like, but also has many searching for proof\n# that we're not all losing our minds.. A TikTok Creator Is Trying To Get To\n# The Bottom Of The Fruit Of The Loom Mandela Effect What Is 'The Mandela\n# Effect?' To understand why people care so much about the Fruit of the Loom\n# logo, one must first understand what the Mandela Effect is in the first\n# place. It's a slang term for a cultural phenomenon in which a large group\n# of people shares false memories of past events. About Fruit of the Loom\n# Cornucopia and Fruit of the Loom Mandela Effect refer to the Mandela\n# Effect involving a large number of people remembering the clothing company\n# Fruit of the Loom having a cornucopia on its logo despite the logo never\n# having the item on it.\n# ['https://www.youtube.com/watch?v=x81gguSPGcQ&pp=ygUHamF1YnJleQ%3D%3D',\n#'https://www.youtube.com/watch?v=bEvxuG6mevQ&pp=ygUHamF1YnJleQ%3D%3D']\n```", "```py\n$ huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF --local-\n↪ dir ./models --local-dir-use-symlinks False --include='*Q4_0*gguf'\n```", "```py\nfrom langchain.llms import LlamaCpp\nfrom langchain.agents import AgentType\nfrom langchain_experimental.agents import create_csv_agent\nfrom langchain_experimental.agents.agent_toolkits import create_python_agent\nfrom langchain_experimental.tools import PythonREPLTool\n\nllm = LlamaCpp(\n    model_path=\"./models/mistral-7b-instruct-v0.1.Q4_0.gguf\",\n    n_gpu_layers=0,     #1\n    n_batch=512,\n    n_ctx=32768,               #2\n    verbose=False,\n)\n\nagent = create_python_agent(     #3\n    llm=llm,\n    tool=PythonREPLTool(),\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n)\n\nagent.run(\n    \"\"\"Using Python_REPL please write a neural network in Pytorch.\n          Use Python_REPL as the Action and your code as the Action Input.\n          Use synthetic data from a normal distribution.\n          Train for 1000 epochs and print every 100 epochs.\n          Return a prediction for x = 5.\"\"\"\n)\n```", "```py\n# > Entering new AgentExecutor chain…\n# This is a task about creating and training a neural network. I should\n# start by importing Pytorch and defining the synthetic data. Then I will\n# define the neural network and train it for 1000 epochs. Finally, I will\n# print the prediction for x = 5.\n# Action: Python_REPL\n# Action Input:\n# ```", "```pyPython REPL can execute arbitrary code. Use with caution.\n# Observation: ImportError(\"cannot import name 'make_classification' from\n#'torchvision.datasets'\n# Thought: It seems like the code is trying to use a function that does not\n# exist, I will check the import statement.\n# Action: Python_REPL\n# Action Input:\n# ```", "```py\n# Observation: ImportError(\"cannot import name 'make_classification' from\n#'torchvision.datasets'\n# Thought: It still seems like the code is trying to use a function that\n# does not exist, I will make sure that the function name matches the\n# correct import statement.\n# Action: Python_REPL\n# Action Input:\n# ```", "```py\n# Observation: ImportError(\"cannot import name 'MakeClassification' from\n#'torchvision.datasets'\n# Thought:\n# > Finished chain.\n```", "```py\nagent = create_csv_agent(    #1\n    llm,\n    \"./data/Slack_Dataset.csv\",\n    verbose=True,\n    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n)\nagent.run(\n    \"\"\"Using python_repl_ast please tell me whether the user polite in their\n    messages. Use python_repl_ast as the Action and the command as the\n    Action input.\"\"\"\n)\n```", "```py\n# > Entering new AgentExecutor chain…\n# Action: python_repl_ast\n# Action Input: df['text'].str.contains('thank you')\n# Observation:\n# 0    False\n# 1    False\n# 2    False\n# 3    False\n# 4    False\n#  …\n# 286    False\n# 287    False\n# 288    False\n# 289    False\n# 290    False\n# Name: text, Length: 291, dtype: bool\n# Thought: It seems the user was not polite in their messages.\n# Final Answer: The user was not polite in their messages.\n# > Finished chain.\n```", "```py\nfrom langchain.llms import LlamaCpp\nfrom langchain.chains.conversation.memory import (\n    ConversationBufferWindowMemory,\n)\nfrom langchain.agents import load_tools, initialize_agent, Tool\nfrom langchain_experimental.tools import PythonREPLTool\nfrom langchain.tools import DuckDuckGoSearchRun, YouTubeSearchTool\nllm = LlamaCpp(\n    model_path=\"./models/mistral-7b-instruct-v0.1.Q4_0.gguf\",\n    n_gpu_layers=0,        #1\n    n_batch=512,\n    n_ctx=32768,          #2\n    verbose=False,\n)\n\nsearch = DuckDuckGoSearchRun()     #3\nduckduckgo_tool = Tool(\n    name=\"DuckDuckGo Search\",\n    func=search.run,\n    description=\"Useful for when an internet search is needed\",\n)\nyoutube_tool = YouTubeSearchTool()\ncoding_tool = PythonREPLTool()\n\ntools = load_tools([\"llm-math\"], llm=llm)\ntools += [duckduckgo_tool, youtube_tool, coding_tool]\n\nmemory = ConversationBufferWindowMemory(     #4\n    memory_key=\"chat_history\",\n    k=5,\n    return_messages=True,\n    output_key=\"output\",\n)\n\nagent = initialize_agent(     #5\n    tools=tools,\n    llm=llm,\n    agent=\"chat-conversational-react-description\",\n    verbose=True,\n    memory=memory,\n    handle_parsing_errors=True,\n)\n\nB_INST, E_INST = \"[INST]\", \"[/INST]\"        #6\nB_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n\nsys_msg = (     #7\n    \"<s>\"\n    + B_SYS\n    + \"\"\"Assistant is a expert JSON builder designed to assist with a wide \\\nrange of tasks.\n\nAssistant is able to respond to the User and use tools using JSON strings \\\nthat contain \"action\" and \"action_input\" parameters.\n\nAll of Assistant's communication is performed using this JSON format.\n\nAssistant can also use tools by responding to the user with tool use \\\ninstructions in the same \"action\" and \"action_input\" JSON format. Tools \\\navailable to Assistant are:\n\n- \"Calculator\": Useful for when you need to answer questions about math.\n  - To use the calculator tool, Assistant should write like so:\n    ```", "```py\n- \"DuckDuckGo Search\": Useful for when an internet search is needed.\n  - To use the duckduckgo search tool, Assistant should write like so:\n    ```", "```pyjson\n{{\"action\": \"Final Answer\",\n \"action_input\": \"I'm good thanks, how are you?\"}}\n```", "```pyjson\n{{\"action\": \"Calculator\",\n \"action_input\": \"sqrt(4)\"}}\n```", "```pyjson\n{{\"action\": \"Final Answer\",\n \"action_input\": \"It looks like the answer is 2!\"}}\n```", "```pyjson\n{{\"action\": \"DuckDuckGo Search\",\n \"action_input\": \"When was the Jonas Brothers' first concert\"}}\n```", "```pyjson\n{{\"action\": \"Final Answer\",\n \"action_input\": \"They had their first concert in 2005!\"}}\n```", "```pyjson\n{{\"action\": \"Calculator\",\n \"action_input\": \"4**2\"}}\n```", "```pyjson\n{{\"action\": \"Final Answer\",\n \"action_input\": \"It looks like the answer is 16!\"}}\n```", "```py\n\n#1 1 if NEON, any number if CUBLAS, else 0\n#2 Context window for the model\n#3 Δefines our own agent tools\n#4 Δefines our agent’s memory\n#5 Sets up and initializes our custom agent\n#6 Special tokens used by llama 2 chat\n#7 Creates the system prompt\n#8 Adds system prompt to agent\n#9 Adds instruction to agent\n#10 Runs with user input\n\nRemember that for this, we asked the model to respond in JSON:\n\n```"]