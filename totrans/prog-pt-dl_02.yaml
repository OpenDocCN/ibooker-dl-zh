- en: Chapter 2\. Image Classification with PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。使用PyTorch进行图像分类
- en: After you’ve set up PyTorch, deep learning textbooks normally throw a bunch
    of jargon at you before doing anything interesting. I try to keep that to a minimum
    and work through an example, albeit one that can easily be expanded as you get
    more comfortable working with PyTorch. We use this example throughout the book
    to demonstrate how to debug a model ([Chapter 7](ch07.html#debugging-pytorch-models))
    or deploy it to production ([Chapter 8](ch08.html#pytorch-in-production)).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置PyTorch之后，深度学习教材通常会在做任何有趣的事情之前向你抛出一堆行话。我尽量将其减少到最低限度，并通过一个例子来解释，尽管这个例子可以在你更熟悉使用PyTorch的过程中轻松扩展。我们在整本书中使用这个例子来演示如何调试模型([第7章](ch07.html#debugging-pytorch-models))或将其部署到生产环境([第8章](ch08.html#pytorch-in-production))。
- en: 'What we’re going to construct from now until the end of [Chapter 4](ch04.html#transfer-learning-and-other-tricks)
    is an *image classifier*. Neural networks are commonly used as image classifiers;
    the network is given a picture and asked what is, to us, a simple question: “What
    is this?”'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始直到[第4章](ch04.html#transfer-learning-and-other-tricks)结束，我们将构建一个*图像分类器*。神经网络通常用作图像分类器；网络被给予一张图片，并被问到对我们来说是一个简单的问题：“这是什么？”
- en: Let’s get started with building our PyTorch application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的PyTorch应用程序。
- en: Our Classification Problem
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的分类问题
- en: Here we build a simple classifier that can tell the difference between fish
    and cats. We’ll be iterating over the design and how we build our model to make
    it more and more accurate.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们构建一个简单的分类器，可以区分鱼和猫之间的区别。我们将不断迭代设计和构建模型的过程，使其变得更加准确。
- en: Figures [2-1](#a-fish) and [2-2](#helvetica-the-cat) show a fish and a cat in
    all their glory. I’m not sure whether the fish has a name, but the cat is called
    Helvetica.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2-1](#a-fish)和[2-2](#helvetica-the-cat)展示了一条鱼和一只猫的全貌。我不确定这条鱼是否有名字，但这只猫叫Helvetica。
- en: Let’s begin with a discussion of the traditional challenges involved in classification.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论传统分类中涉及的挑战开始。
- en: '![An image of a fish](assets/ppdl_0201.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![一条鱼的图片](assets/ppdl_0201.png)'
- en: Figure 2-1\. A fish!
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 一条鱼！
- en: '![An image of a black cat in a box](assets/ppdl_0202.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![一个黑猫在盒子里的图片](assets/ppdl_0202.png)'
- en: Figure 2-2\. Helvetica in a box
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 盒子里的Helvetica
- en: Traditional Challenges
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统挑战
- en: How would you go about writing a program that could tell a fish from a cat?
    Maybe you’d write a set of rules describing that a cat has a tail, or that a fish
    has scales, and apply those rules to an image to determine what you’re looking
    at. But that would take time, effort, and skill. Plus, what happens if you encounter
    something like a Manx cat; while it is clearly a cat, it doesn’t have a tail.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何编写一个程序来区分鱼和猫？也许你会编写一组规则，描述猫有尾巴，或者鱼有鳞片，并将这些规则应用于图像以确定你看到的是什么。但这需要时间、精力和技能。另外，如果你遇到像曼克斯猫这样的东西会发生什么；虽然它显然是一只猫，但它没有尾巴。
- en: You can see how these rules are just going get more and more complicated to
    describe all possible scenarios. Also, I’ll admit that I’m absolutely terrible
    at graphics programming, so the idea of having to manually code all these rules
    fills me with dread.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这些规则只会变得越来越复杂，以描述所有可能的情况。此外，我承认我在图形编程方面非常糟糕，所以不得不手动编写所有这些规则的想法让我感到恐惧。
- en: What we’re after is a function that, given the input of an image, returns *cat*
    or *fish*. That function is hard for us to construct by exhaustively listing all
    the criteria. But deep learning essentially makes the computer do all the hard
    work of constructing all those rules that we just talked about—provided we create
    a structure, give the network lots of data, and give it a way to work out whether
    it is getting the right answer. So that’s what we’re going to do. Along the way,
    you’ll learn some key concepts of how to use PyTorch.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们追求的是一个函数，给定一张图片的输入，返回*猫*或*鱼*。对于我们来说，通过详细列出所有标准来构建这个函数是困难的。但深度学习基本上让计算机做所有那些我们刚刚谈到的规则的艰苦工作——只要我们创建一个结构，给网络大量数据，并让它找出是否得到了正确答案的方法。这就是我们要做的。在这个过程中，你将学习如何使用PyTorch的一些关键概念。
- en: But First, Data
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但首先，数据
- en: First, we need data. How much data? Well, that depends. The idea that for any
    deep learning technique to work, you need vast quantities of data to train the
    neural network is not necessarily true, as you’ll see in [Chapter 4](ch04.html#transfer-learning-and-other-tricks).
    However, right now we’re going to be training from scratch, which often does require
    access to a large quantity of data. We need a lot of pictures of fish and cats.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要数据。需要多少数据？这取决于情况。对于任何深度学习技术都需要大量数据来训练神经网络的想法并不一定正确，正如你将在[第4章](ch04.html#transfer-learning-and-other-tricks)中看到的那样。然而，现在我们将从头开始训练，这通常需要大量数据。我们需要很多鱼和猫的图片。
- en: 'Now, we could spend some time downloading many images from something like Google
    image search, but in this instance we have a shortcut: a standard collection of
    images used to train neural networks, called *ImageNet*. It contains more than
    14 million images and 20,000 image categories. It’s the standard that all image
    classifiers judge themselves against. So I take images from there, though feel
    free to download other ones yourself if you prefer.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以花一些时间从Google图像搜索等地方下载许多图片，但在这种情况下，我们有一个捷径：一个用于训练神经网络的标准图像集合，称为*ImageNet*。它包含超过1400万张图片和20000个图像类别。这是所有图像分类器用来评判自己的标准。所以我从那里获取图片，但如果你愿意，可以自行下载其他图片。
- en: Along with the data, PyTorch needs a way to determine what is a cat and what
    is a fish. That’s easy enough for us, but it’s somewhat harder for the computer
    (which is why we are building the program in the first place!). We use a *label*
    attached to the data, and training in this manner is called *supervised learning*.
    (When you don’t have access to any labels, you have to use, perhaps unsurprisingly,
    *unsupervised learning* methods for training.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据，PyTorch还需要一种确定什么是猫和什么是鱼的方法。这对我们来说很容易，但对计算机来说有点困难（这也是我们首次构建程序的原因！）。我们使用附加到数据的*标签*，以这种方式进行训练称为*监督学习*。（当您无法访问任何标签时，您必须使用*无监督学习*方法进行训练，这可能并不令人惊讶。）
- en: Now, if we’re using ImageNet data, its labels aren’t going to be all that useful,
    because they contain *too* much information for us. A label of *tabby cat* or
    *trout* is, to the computer, separate from *cat* or *fish*. We’ll need to relabel
    these. Because ImageNet is such a vast collection of images, I have pulled together
    a list of [image URLs and labels](https://oreil.ly/NbtEU) for both fish and cats.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们使用ImageNet数据，它的标签对我们来说并不是那么有用，因为它们包含了对我们来说*太多*的信息。*tabby cat*或*trout*这样的标签，在计算机看来，与*cat*或*fish*是分开的。我们需要重新标记这些。因为ImageNet是如此庞大的图像集合，我已经整理了一份[图像URL和标签的列表](https://oreil.ly/NbtEU)供鱼类和猫类使用。
- en: You can run the *download.py* script in that directory, and it will download
    the images from the URLs and place them in the appropriate locations for training.
    The *relabeling* is simple; the script stores cat pictures in the directory *train/cat*
    and fish pictures in *train/fish*. If you’d prefer to not use the script for downloading,
    just create these directories and put the appropriate pictures in the right locations.
    We now have our data, but we need to get it into a format that PyTorch can understand.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在该目录中运行*download.py*脚本，它将从URL下载图像并将其放置在适当的位置进行训练。*重新标记*很简单；脚本将猫的图片存储在*train/cat*目录中，将鱼的图片存储在*train/fish*目录中。如果您不想使用下载脚本，只需创建这些目录并将适当的图片放在正确的位置。现在我们有了数据，但我们需要将其转换为PyTorch可以理解的格式。
- en: PyTorch and Data Loaders
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch和数据加载器
- en: Loading and converting data into formats that are ready for training can often
    end up being one of the areas in data science that sucks up far too much of our
    time. PyTorch has developed standard conventions of interacting with data that
    make it fairly consistent to work with, whether you’re working with images, text,
    or audio.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 加载和转换数据为训练准备的格式通常会成为数据科学中吸收我们太多时间的领域之一。PyTorch已经发展了与数据交互的标准约定，使得与之一起工作变得相当一致，无论您是在处理图像、文本还是音频。
- en: The two main conventions of interacting with data are *datasets* and *data loaders*.
    A *dataset* is a Python class that allows us to get at the data we’re supplying
    to the neural network. A *data loader* is what feeds data from the dataset into
    the network. (This can encompass information such as, *How many worker processes
    are feeding data into the network?* or *How many images are we passing in at once?*)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据交互的两个主要约定是*数据集*和*数据加载器*。*数据集*是一个Python类，允许我们访问我们提供给神经网络的数据。*数据加载器*是将数据从数据集传送到网络的工具。（这可能包括信息，例如，*有多少个工作进程正在将数据传送到网络中？*或*我们一次传入多少张图片？*）
- en: 'Let’s look at the dataset first. Every dataset, no matter whether it includes
    images, audio, text, 3D landscapes, stock market information, or whatever, can
    interact with PyTorch if it satisfies this abstract Python class:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看数据集。无论数据集包含图像、音频、文本、3D景观、股市信息还是其他任何内容，只要满足这个抽象的Python类，就可以与PyTorch进行交互：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is fairly straightforward: we have to implement a method that returns
    the size of our dataset (`len`), and implement a method that can retrieve an item
    from our dataset in a (`*label*`, `*tensor*`) pair. This is called by the data
    loader as it is pushing data into the neural network for training. So we have
    to write a body for `getitem` that can take an image and transform it into a tensor
    and return that and the label back so PyTorch can operate on it. This is fine,
    but you can imagine that this scenario comes up a lot, so maybe PyTorch can make
    things easier for us?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相当直接的：我们必须实现一个返回数据集大小的方法（`len`），并实现一个可以检索数据集中项目的方法，返回一个（`*label*`，`*tensor*`）对。这是由数据加载器调用的，因为它正在将数据推送到神经网络进行训练。因此，我们必须编写一个`getitem`的主体，它可以获取图像并将其转换为张量，然后返回该张量和标签，以便PyTorch可以对其进行操作。这很好，但你可以想象到这种情况经常发生，所以也许PyTorch可以让事情变得更容易？
- en: Building a Training Dataset
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建训练数据集
- en: 'The `torchvision` package includes a class called `ImageFolder` that does pretty
    much everything for us, providing our images are in a structure where each directory
    is a label (e.g., all cats are in a directory called *cat*). For our cats and
    fish example, here’s what you need:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision`包含一个名为`ImageFolder`的类，几乎为我们做了一切，只要我们的图像结构中每个目录都是一个标签（例如，所有猫都在一个名为*cat*的目录中）。对于我们的猫和鱼的示例，这是您需要的：'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A little bit more is going on here because `torchvision` also allows you to
    specify a list of transforms that will be applied to an image before it gets fed
    into the neural network. The default transform is to take image data and turn
    it into a tensor (the `transforms.ToTensor()` method seen in the preceding code),
    but we’re also doing a couple of other things that might not seem obvious.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了更多的事情，因为`torchvision`还允许您指定一系列将应用于图像的转换，然后将其馈送到神经网络之前。默认转换是将图像数据转换为张量（在前面的代码中看到的`transforms.ToTensor()`方法），但我们还在做一些其他可能不太明显的事情。
- en: Firstly, GPUs are built to be fast at performing calculations that are a standard
    size. But we probably have an assortment of images at many resolutions. To increase
    our processing performance, we scale every incoming image to the same resolution
    of 64 × 64 via the `Resize(64)` transform. We then convert the images to a tensor,
    and finally, we normalize the tensor around a specific set of mean and standard
    deviation points.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，GPU被设计为快速执行标准大小的计算。但我们可能有许多分辨率的图像。为了提高我们的处理性能，我们通过`Resize(64)`转换将每个传入的图像缩放到相同的分辨率64×64。然后我们将图像转换为张量，最后我们将张量归一化到一组特定的均值和标准差点周围。
- en: Normalizing is important because a lot of multiplication will be happening as
    the input passes through the layers of the neural network; keeping the incoming
    values between 0 and 1 prevents the values from getting too large during the training
    phase (known as the *exploding gradient* problem). And that magic incarnation
    is just the mean and standard deviation of the ImageNet dataset as a whole. You
    could calculate it specifically for this fish and cat subset, but these values
    are decent enough. (If you were working on a completely different dataset, you’d
    have to calculate that mean and deviation, although many people just use these
    ImageNet constants and report acceptable results.)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化很重要，因为当输入通过神经网络的层时会发生大量的乘法运算；保持输入值在0和1之间可以防止值在训练阶段变得过大（称为*梯度爆炸*问题）。这种神奇的化身只是ImageNet数据集作为整体的均值和标准差。你可以专门为这个猫和鱼子集计算它，但这些值已经足够好了。（如果你在完全不同的数据集上工作，你将不得不计算那个均值和偏差，尽管许多人只是使用这些ImageNet常数并报告可接受的结果。）
- en: The composable transforms also allow us to easily do things like image rotation
    and skewing for data augmentation, which we’ll come back to in [Chapter 4](ch04.html#transfer-learning-and-other-tricks).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可组合的转换还允许我们轻松地进行图像旋转和扭曲以进行数据增强，我们将在[第4章](ch04.html#transfer-learning-and-other-tricks)中回到这个话题。
- en: Note
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We’re resizing the images to 64 × 64 in this example. I’ve made that arbitrary
    choice in order to make the computation in our upcoming first network fast. Most
    existing architectures that you’ll see in [Chapter 3](ch03.html#convolutional-neural-networks)
    use 224 × 224 or 299 × 299 for their image inputs. In general, the larger the
    input size, the more data for the network to learn from. The flip side is that
    you can often fit a smaller batch of images within the GPU’s memory.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将图像调整为64×64。我做出了这个任意选择，以便使我们即将到来的第一个网络的计算变得快速。大多数现有的架构在[第3章](ch03.html#convolutional-neural-networks)中使用224×224或299×299作为图像输入。一般来说，输入尺寸越大，网络学习的数据就越多。另一方面，你通常可以将更小的图像批次适应到GPU的内存中。
- en: We’re not quite done with datasets yet. But why do we need more than just a
    training dataset?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据集还没有完成。但是为什么我们需要不止一个训练数据集呢？
- en: Building Validation and Test Datasets
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建验证和测试数据集
- en: 'Our training data is set up, but we need to repeat the same steps for our *validation*
    data. What’s the difference here? One danger of deep learning (and all machine
    learning, in fact) is the concept of *overfitting*: your model gets really good
    at recognizing what it has been trained on, but cannot generalize to examples
    it hasn’t seen. So it sees a picture of a cat, and unless all other pictures of
    cats resemble that picture very closely, the model doesn’t think it’s a cat, despite
    it obviously being so. To prevent our network from doing this, we download a *validation
    set* in *download.py*, which is a series of cat and fish pictures that do not
    occur in the training set. At the end of each training cycle (also known as an
    *epoch*), we compare against this set to make sure our network isn’t getting things
    wrong. But don’t worry—the code for this is incredibly easy because it’s just
    the earlier code with a few variable names changed:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据已经设置好了，但我们需要为我们的*验证*数据重复相同的步骤。这里有什么区别？深度学习（实际上所有机器学习）的一个危险是*过拟合*的概念：你的模型在训练过的内容上表现得非常好，但无法推广到它没有见过的例子。所以它看到一张猫的图片，除非所有其他猫的图片都与那张图片非常相似，否则模型不认为它是一只猫，尽管它显然是一只猫。为了防止我们的网络这样做，我们在*download.py*中下载了一个*验证集*，其中包含一系列不在训练集中出现的猫和鱼的图片。在每个训练周期（也称为*epoch*）结束时，我们会与这个集合进行比较，以确保我们的网络没有出错。但不用担心，这段代码非常简单，因为它只是稍微更改了一些变量名的早期代码：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We just reused the `transforms` chain instead of having to define it once again.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是重新使用了`transforms`链，而不必再次定义它。
- en: 'In addition to a validation set, we should also create a *test set*. This is
    used to test the model after all training has been completed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了验证集，我们还应该创建一个*测试集*。这用于在所有训练完成后测试模型：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Distinguishing the types of sets can be a little confusing, so I’ve compiled
    a table to indicate which set is used for which part of model training; see [Table 2-1](#dataset-type-table).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 区分数据集类型可能有点困惑，所以我编制了一张表来指示哪个数据集用于模型训练的哪个部分；请参见[表2-1](#dataset-type-table)。
- en: Table 2-1\. Dataset types
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1. 数据集类型
- en: '| Training set | Used in the training pass to update the model |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 训练集 | 用于训练过程中更新模型的数据集 |'
- en: '| Validation set | Used to evaluate how the model is generalizing to the problem
    domain, rather than fitting to the training data; not used to update the model
    directly |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 验证集 | 用于评估模型在问题领域中的泛化能力，而不是适应训练数据；不直接用于更新模型 |'
- en: '| Test set | A final dataset that provides a final evaluation of the model’s
    performance after training is complete |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 测试集 | 在训练完成后提供最终评估模型性能的最终数据集 |'
- en: 'We can then build our data loaders with a few more lines of Python:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以用几行Python代码构建我们的数据加载器：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The new thing to note from this code is `batch_size`. This tells us how many
    images will go through the network before we train and update it. We could, in
    theory, set the `batch_size` to the number of images in the test and training
    sets so the network sees every image before it updates. In practice, we tend not
    to do this because smaller batches (more commonly known as *mini-batches* in the
    literature) require less memory than having to store all the information about
    *every* image in the dataset, and the smaller batch size ends up making training
    faster as we’re updating our network much more quickly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从这段代码中需要注意的新内容是`batch_size`。这告诉我们在训练和更新之前有多少图像会通过网络。理论上，我们可以将`batch_size`设置为测试集和训练集中图像的数量，以便网络在更新之前看到每个图像。实际上，我们通常不这样做，因为较小的批次（在文献中更常被称为*小批量*）需要比存储数据集中*每个*图像的所有信息更少的内存，并且较小的批次大小会使训练更快，因为我们更快地更新我们的网络。
- en: 'By default, PyTorch’s data loaders are set to a `batch_size` of 1\. You will
    almost certainly want to change that. Although I’ve chosen 64 here, you might
    want to experiment to see how big of a minibatch you can use without exhausting
    your GPU’s memory. You may also want to experiment with some of the additional
    parameters: you can specify how datasets are sampled, whether the entire set is
    shuffled on each run, and how many worker processes are used to pull data out
    of the dataset. This can all be found in the [PyTorch documentation](https://oreil.ly/XORs1).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch的数据加载器设置为`batch_size`为1。你几乎肯定会想要更改这个值。虽然我在这里选择了64，但你可能想要尝试一下，看看你可以使用多大的小批量而不会耗尽GPU的内存。你可能还想尝试一些额外的参数：你可以指定数据集如何被采样，是否在每次运行时对整个集合进行洗牌，以及使用多少个工作进程来从数据集中提取数据。所有这些都可以在[PyTorch文档](https://oreil.ly/XORs1)中找到。
- en: That covers getting data into PyTorch, so let’s now introduce a simple neural
    network to actually start classifying our images.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了将数据导入PyTorch，所以现在让我们介绍一个简单的神经网络来开始对我们的图像进行分类。
- en: Finally, a Neural Network!
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后，一个神经网络！
- en: 'We’re going to start with the simplest deep learning network: an input layer,
    which will work on the input tensors (our images); our output layer, which will
    be the size of the number of our output classes (2); and a hidden layer between
    them. In our first example, we’ll use fully connected layers. [Figure 2-3](#simple-neural-network)
    illustrates what that looks like with an input layer of three nodes, a hidden
    layer of three nodes, and our two-node output.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最简单的深度学习网络开始：一个输入层，用于处理输入张量（我们的图像）；一个输出层，其大小将是输出类别数量（2）的大小；以及它们之间的一个隐藏层。在我们的第一个示例中，我们将使用全连接层。[图2-3](#simple-neural-network)展示了一个具有三个节点的输入层，三个节点的隐藏层和两个节点输出的样子。
- en: '![A simple neural network](assets/ppdl_0203.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![一个简单的神经网络](assets/ppdl_0203.png)'
- en: Figure 2-3\. A simple neural network
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 一个简单的神经网络
- en: As you can see, in this fully connected example, every node in a layer affects
    every node in the next layer, and each connection has a *weight* that determines
    the strength of the signal from that node going into the next layer. (It is these
    weights that will be updated when we train the network, normally from a random
    initialization.) As an input passes through the network, we (or PyTorch) can simply
    do a matrix multiplication of the weights and biases of that layer onto the input.
    Before feeding it into the next function, that result goes into an *activation
    function*, which is simply a way of inserting nonlinearity into our system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，在这个全连接的例子中，每一层中的每个节点都会影响到下一层中的每个节点，并且每个连接都有一个*权重*，它决定了从该节点传入下一层的信号的强度。当我们训练网络时，这些权重通常会从随机初始化中更新。当一个输入通过网络时，我们（或PyTorch）可以简单地将该层的权重和偏置进行矩阵乘法，然后将结果传递到下一个函数中，该结果会经过一个*激活函数*，这只是一种在我们的系统中插入非线性的方法。
- en: Activation Functions
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions sound complicated, but the most common activation function
    you’ll come across in the literature these days is `ReLU`, or *rectified linear
    unit*. Which again sounds complicated! But all it turns out to be is a function
    that implements *max(0,x)*, so the result is 0 if the input is negative, or just
    the input (*x*) if *x* is positive. Simple!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数听起来很复杂，但你在文献中最常见的激活函数是`ReLU`，或者*修正线性单元*。这再次听起来很复杂！但事实证明，它只是实现*max(0,x)*的函数，所以如果输入是负数，则结果为0，如果*x*是正数，则结果就是输入（*x*）。简单！
- en: Another activation function you’ll likely come across is *softmax*, which is
    a little more complicated mathematically. Basically it produces a set of values
    between 0 and 1 that adds up to 1 (probabilities!) and weights the values so it
    exaggerates differences—that is, it produces one result in a vector higher than
    everything else. You’ll often see it being used at the end of a classification
    network to ensure that that network makes a definite prediction about what class
    it thinks the input belongs to.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到的另一个激活函数是*softmax*，在数学上稍微复杂一些。基本上它会产生一组介于0和1之间的值，加起来等于1（概率！），并且加权这些值以夸大差异——也就是说，它会在向量中产生一个比其他所有值都高的结果。你经常会看到它被用在分类网络的末尾，以确保网络对输入属于哪个类别做出明确的预测。
- en: With all these building blocks in place, we can start to build our first neural
    network.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有了所有这些构建块，我们可以开始构建我们的第一个神经网络。
- en: Creating a Network
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个网络
- en: 'Creating a network in PyTorch is a very Pythonic affair. We inherit from a
    class called `torch.nn.Network` and fill out the `__init__` and `forward` methods:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中创建一个网络是一个非常Pythonic的事情。我们从一个名为`torch.nn.Network`的类继承，并填写`__init__`和`forward`方法：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Again, this is not too complicated. We do any setup required in `init()`, in
    this case calling our superclass constructor and the three fully connected layers
    (called `Linear` in PyTorch, as opposed to `Dense` in Keras). The `forward()`
    method describes how data flows through the network in both training and making
    predictions (*inference*). First, we have to convert the 3D tensor (*x* and *y*
    plus three-channel color information—red, green, blue) in an image, remember!—into
    a 1D tensor so that it can be fed into the first `Linear` layer, and we do that
    using the `view()`. From there, you can see that we apply the layers and the activation
    functions in order, finally returning the `softmax` output to give us our prediction
    for that image.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这并不太复杂。我们在`init()`中进行任何所需的设置，这种情况下调用我们的超类构造函数和三个全连接层（在PyTorch中称为`Linear`，而不是Keras中的`Dense`）。`forward()`方法描述了数据如何在网络中流动，无论是在训练还是进行预测（*推理*）。首先，我们必须将图像中的3D张量（*x*和*y*加上三通道的颜色信息—红色、绿色、蓝色）转换为1D张量，以便将其馈送到第一个`Linear`层中，我们使用`view()`来实现这一点。从那里，您可以看到我们按顺序应用层和激活函数，最后返回`softmax`输出以给出我们对该图像的预测。
- en: The numbers in the hidden layers are somewhat arbitrary, with the exception
    of the output of the final layer, which is 2, matching up with our two classes
    of cat or fish. In general, you want the data in your layers to be *compressed*
    as it goes down the stack. If a layer is going to, say, 50 inputs to 100 outputs,
    then the network might *learn* by simply passing the 50 connections to 50 of the
    100 outputs and consider its job done. By reducing the size of the output with
    respect to the input, we force that part of the network to learn a representation
    of the original input with fewer resources, which hopefully means that it extracts
    some features of the images that are important to the problem we’re trying to
    solve; for example, learning to spot a fin or a tail.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的数字有些是任意的，除了最终层的输出是2，与我们的两类猫或鱼相匹配。一般来说，您希望在层中的数据在向下堆栈时*压缩*。如果一个层要将50个输入传递到100个输出，那么网络可能会通过简单地将50个连接传递给100个输出中的50个来*学习*，并认为其工作完成。通过减小输出相对于输入的大小，我们迫使网络的这部分学习使用更少的资源来学习原始输入的表示，这希望意味着它提取了一些对我们要解决的问题重要的图像特征；例如，学习识别鳍或尾巴。
- en: We have a prediction, and we can compare that with the actual label of the original
    image to see whether the prediction was correct. But we need some way of allowing
    PyTorch to quantify not just whether a prediction is right or wrong, but just
    how wrong or right it is. This is handled by a loss function.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个预测，我们可以将其与原始图像的实际标签进行比较，以查看预测是否正确。但是我们需要一种让PyTorch能够量化预测是正确还是错误，以及有多错误或正确的方法。这由损失函数处理。
- en: Loss Functions
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: '*Loss functions* are one of the key pieces of an effective deep learning solution.
    PyTorch uses loss functions to determine how it will update the network to reach
    the desired results.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数*是有效深度学习解决方案的关键组成部分之一。PyTorch使用损失函数来确定如何更新网络以达到期望的结果。'
- en: Loss functions can be as complicated or as simple as you desire. PyTorch comes
    complete with a comprehensive collection of them that will cover most of the applications
    you’re likely to encounter, plus of course you can write your own if you have
    a very custom domain. In our case, we’re going to use a built-in loss function
    called `CrossEntropyLoss`, which is recommended for multiclass categorization
    tasks like we’re doing here. Another loss function you’re likely to come across
    is `MSELoss`, which is a standard mean squared loss that you might use when making
    a numerical prediction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数可以是您想要的复杂或简单。PyTorch配备了一个全面的损失函数集合，涵盖了您可能会遇到的大多数应用程序，当然，如果您有一个非常自定义的领域，您也可以编写自己的损失函数。在我们的情况下，我们将使用一个名为`CrossEntropyLoss`的内置损失函数，这是推荐用于多类别分类任务的，就像我们在这里所做的那样。您可能会遇到的另一个损失函数是`MSELoss`，这是一个标准的均方损失，您可能在进行数值预测时使用。
- en: 'One thing to be aware of with `CrossEntropyLoss` is that it also incorporates
    `softmax()` as part of its operation, so our `forward()` method becomes the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意的一件事是，`CrossEntropyLoss`还将`softmax()`作为其操作的一部分，因此我们的`forward()`方法变为以下内容：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now let’s look at how a neural network’s layers are updated during the training
    loop.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在训练循环期间神经网络的层如何更新。
- en: Optimizing
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: Training a network involves passing data through the network, using the loss
    function to determine the difference between the prediction and the actual label,
    and then using that information to update the weights of the network in an attempt
    to make the loss function return as small a loss as possible. To perform the updates
    on the neural network, we use an *optimizer*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络涉及通过网络传递数据，使用损失函数确定预测和实际标签之间的差异，然后使用该信息来更新网络的权重，以尽可能使损失函数返回尽可能小的损失。为了对神经网络进行更新，我们使用一个*优化器*。
- en: If we just had one weight, we could plot a graph of the loss value against the
    value of the weight, and it might look something like [Figure 2-4](#loss-plot-2d).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有一个权重，我们可以绘制损失值与权重值的图表，它可能看起来像[图2-4](#loss-plot-2d)。
- en: '![A 2D plot of loss](assets/ppdl_0204.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![损失的二维图](assets/ppdl_0204.png)'
- en: Figure 2-4\. A 2D plot of loss
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4。损失的二维图
- en: If we start at a random position, marked in [Figure 2-4](#loss-plot-2d) by the
    X, with our weight value on the x-axis and the loss function on the y-axis, we
    need to get to the lowest point on the curve to find our optimal solution. We
    can move by altering the value of the weight, which will give us a new value for
    the loss function. To know how good a move we’re making, we can check against
    the gradient of the curve. One common way to visualize the optimizer is like rolling
    a marble, trying to find the lowest point (or *minima*) in a series of valleys.
    This is perhaps clearer if we extend our view to two parameters, creating a 3D
    graph as shown in [Figure 2-5](#loss-plot-3d).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从一个随机位置开始，用X标记，将我们的权重值放在x轴上，损失函数放在y轴上，我们需要到曲线的最低点找到我们的最佳解决方案。我们可以通过改变权重的值来移动，这将给我们一个新的损失函数值。要知道我们正在做出的移动有多好，我们可以根据曲线的梯度进行检查。可视化优化器的一种常见方法是像滚动大理石一样，试图找到一系列山谷中的最低点（或*最小值*）。如果我们将视图扩展到两个参数，创建一个如图2-5所示的3D图，这可能更清晰。
- en: '![A 3D plot of loss](assets/ppdl_0205.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![损失的3D图](assets/ppdl_0205.png)'
- en: Figure 2-5\. A 3D plot of loss
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。损失的3D图
- en: And in this case, at every point, we can check the gradients of all the potential
    moves and choose the one that moves us most down the hill.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以在每个点检查所有潜在移动的梯度，并选择使我们在山下移动最多的那个。
- en: You need to be aware of a couple of issues, though. The first is the danger
    of getting trapped in *local minima*, areas that look like they’re the shallowest
    parts of the loss curve if we check our gradients, but actually shallower areas
    exist elsewhere. If we go back to our 1D curve in [Figure 2-4](#loss-plot-2d),
    we can see that if we end up in the minima on the left by taking short hops down,
    we’d never have any reason to leave that position. And if we took giant hops,
    we might find ourselves getting onto the path that leads to the actual lowest
    point, but because we keep making jumps that are so big, we keep bouncing all
    over the place.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您需要注意一些问题。首先是陷入*局部最小值*的危险，这些区域看起来像是损失曲线最浅的部分，如果我们检查梯度，但实际上在其他地方存在更浅的区域。如果我们回到[图2-4](#loss-plot-2d)中的1D曲线，我们可以看到如果通过短跳下陷入左侧的最小值，我们永远不会有离开该位置的理由。如果我们采取巨大的跳跃，我们可能会发现自己进入通往实际最低点的路径，但由于我们一直跳得太大，我们一直在到处弹跳。
- en: 'The size of our hops is known as the *learning rate*, and is often the *key*
    parameter that needs to be tweaked in order to get your network learning properly
    and efficiently. You’ll see a way of determining a good learning rate in [Chapter 4](ch04.html#transfer-learning-and-other-tricks),
    but for now, you’ll be experimenting with different values: try something like
    0.001 to begin with. As just mentioned, large learning rates will cause your network
    to bounce all over the place in training, and it will not *converge* on a good
    set of weights.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的跳跃大小被称为*学习率*，通常是需要调整的*关键*参数，以便使您的网络学习正确和高效。您将在[第4章](ch04.html#transfer-learning-and-other-tricks)中看到确定良好学习率的方法，但现在，您将尝试不同的值：尝试从0.001开始。正如刚才提到的，较大的学习率会导致网络在训练过程中到处反弹，并且不会*收敛*到一组良好的权重上。
- en: As for the local minima problem, we make a slight alteration to our taking all
    the possible gradients and indicate sample random gradients during a batch. Known
    as *stochastic gradient descent* (SGD), this is the traditional approach to optimizing
    neural networks and other machine learning techniques. But other optimizers are
    available, and indeed for deep learning, preferable. PyTorch ships with SGD and
    others such as AdaGrad and RMSProp, as well as Adam, the optimizer we will be
    using for the majority of the book.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 至于局部最小值问题，我们对获取所有可能梯度进行了轻微修改，并在批处理期间指示样本随机梯度。称为*随机梯度下降*（SGD），这是优化神经网络和其他机器学习技术的传统方法。但是还有其他优化器可用，事实上对于深度学习来说更可取。PyTorch提供了SGD和其他优化器，如AdaGrad和RMSProp，以及Adam，我们将在本书的大部分内容中使用的优化器。
- en: One of the key improvements that Adam makes (as does RMSProp and AdaGrad) is
    that it uses a learning rate per parameter, and adapts that learning rate depending
    on the rate of change of those parameters. It keeps an exponentially decaying
    list of gradients and the square of those gradients and uses those to scale the
    global learning rate that Adam is working with. Adam has been empirically shown
    to outperform most other optimizers in deep learning networks, but you can swap
    out Adam for SGD or RMSProp or another optimizer to see if using a different technique
    yields faster and better training for your particular application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Adam的一个关键改进（RMSProp和AdaGrad也是如此）是它为每个参数使用一个学习率，并根据这些参数的变化速率调整该学习率。它保持梯度和这些梯度的平方的指数衰减列表，并使用这些来缩放Adam正在使用的全局学习率。经验表明，Adam在深度学习网络中优于大多数其他优化器，但您可以将Adam替换为SGD或RMSProp或另一个优化器，以查看是否使用不同的技术能够为您的特定应用程序提供更快更好的训练。
- en: 'Creating an Adam-based optimizer is simple. We call `optim.Adam()` and pass
    in the weights of the network that it will be updating (obtained via `simplenet.parameters()`)
    and our example learning rate of 0.001:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 创建基于Adam的优化器很简单。我们调用`optim.Adam()`并传入网络的权重（通过`simplenet.parameters()`获得）和我们示例的学习率0.001：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The optimizer is the last piece of the puzzle, so we can finally start training
    our network.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是拼图的最后一块，所以我们终于可以开始训练我们的网络了。
- en: Training
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'Here’s our complete training loop, which combines everything you’ve seen so
    far to train the network. We’re going to write this as a function so parts such
    as the loss function and optimizer can be passed in as parameters. It looks quite
    generic at this point:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们完整的训练循环，将迄今为止看到的所有内容结合起来训练网络。我们将其编写为一个函数，以便可以将诸如损失函数和优化器之类的部分作为参数传递。目前看起来相当通用：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s fairly straightforward, but you should note a few things. We take a batch
    from our training set on every iteration of the loop, which is handled by our
    data loader. We then run those through our model and compute the loss from the
    expected output. To compute the gradients, we call the `backward()` method on
    the model. The `optimizer.step()` method uses those gradients afterward to perform
    the adjustment of the weights that we talked about in the previous section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相当简单的，但你应该注意几点。我们在循环的每次迭代中从训练集中取一个批次，这由我们的数据加载器处理。然后我们通过模型运行这些数据，并计算出期望输出的损失。为了计算梯度，我们在模型上调用`backward()`方法。`optimizer.step()`方法随后使用这些梯度来执行我们在前一节中讨论过的权重调整。
- en: What is that `zero_grad()` call doing, though? It turns out that the calculated
    gradients accumulate by default, meaning that if we didn’t zero the gradients
    at the end of the batch’s iteration, the next batch would have to deal with this
    batch’s gradients as well as its own, and the batch after that would have to cope
    with the previous two, and so on. This isn’t helpful, as we want to look at only
    the gradients of the current batch for our optimization in each iteration. We
    use `zero_grad()` to make sure they are reset to zero after we’re done with our
    loop.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`zero_grad()`调用是在做什么呢？事实证明，默认情况下计算的梯度会累积，这意味着如果我们在批次迭代结束时不将梯度清零，下一个批次将不得不处理这个批次的梯度以及自己的梯度，接下来的批次将不得不处理前两个批次的梯度，依此类推。这并不有用，因为我们希望在每次迭代中只查看当前批次的梯度进行优化。我们使用`zero_grad()`确保在我们完成循环后将它们重置为零。
- en: That’s the abstracted version of the training loop, but we have to address a
    few more things before we can write our complete function.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练循环的抽象版本，但在写完我们的完整函数之前，我们还需要解决一些问题。
- en: Making It Work on the GPU
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使其在GPU上运行
- en: 'If you’ve run any of the code so far, you might have noticed that it’s not
    all that fast. What about that shiny GPU that’s sitting attached to our instance
    in the cloud (or the very expensive machine we’ve put together on our desktop)?
    PyTorch, by default, does CPU-based calculations. To take advantage of the GPU,
    we need to move our input tensors and the model itself to the GPU by explicitly
    using the `to()` method. Here’s an example that copies the `SimpleNet` to the
    GPU:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，如果你运行了任何代码，你可能已经注意到它并不那么快。那么那块闪亮的GPU呢，它就坐在我们云端实例上（或者我们在桌面上组装的非常昂贵的机器上）？PyTorch默认使用CPU进行计算。为了利用GPU，我们需要通过显式地使用`to()`方法将输入张量和模型本身移动到GPU上。这里有一个将`SimpleNet`复制到GPU的示例：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we copy the model to the GPU if PyTorch reports that one is available,
    or otherwise keep the model on the CPU. By using this construction, we can determine
    whether a GPU is available at the start of our code and use `tensor|model.to(device)`
    throughout the rest of the program, being confident that it will go to the correct
    place.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，如果PyTorch报告有GPU可用，我们将模型复制到GPU上，否则保持模型在CPU上。通过使用这种构造，我们可以确定GPU是否在我们的代码开始时可用，并在程序的其余部分中使用`tensor|model.to(device)`，确信它会到达正确的位置。
- en: Note
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In earlier versions of PyTorch, you would use the `cuda()` method to copy data
    to the GPU instead. If you come across that method when looking at other people’s
    code, just be aware that it’s doing the same thing as `to()`!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期版本的PyTorch中，你会使用`cuda()`方法将数据复制到GPU上。如果在查看其他人的代码时遇到这个方法，只需注意它与`to()`做的是相同的事情！
- en: And that wraps up all the steps required for training. We’re almost done!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是训练所需的所有步骤。我们快要完成了！
- en: Putting It All Together
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'You’ve seen a lot of different pieces of code throughout this chapter, so let’s
    consolidate it. We put it all together to create a generic training method that
    takes in a model, as well as training and validation data, along with learning
    rate and batch size options, and performs training on that model. We use this
    code throughout the rest of the book:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经看到了许多不同的代码片段，让我们整合它们。我们将它们放在一起，创建一个通用的训练方法，接受一个模型，以及训练和验证数据，还有学习率和批次大小选项，并对该模型进行训练。我们将在本书的其余部分中使用这段代码：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'That’s our training function, and we can kick off training by calling it with
    the required parameters:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的训练函数，我们可以通过传入所需的参数来启动训练：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The network will train for 20 epochs (you can adjust this by passing in a value
    for `epoch` to `train()`), and you should get a printout of the model’s accuracy
    on the validation set at the end of each epoch.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将训练20个epochs（你可以通过向`train()`传入一个值来调整这个值），并且你应该在每个epoch结束时得到模型在验证集上的准确性打印输出。
- en: You have trained your first neural network—congratulations! You can now use
    it to make predictions, so let’s look at how to do that.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经训练了你的第一个神经网络——恭喜！现在你可以用它进行预测，让我们看看如何做到这一点。
- en: Making Predictions
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: 'Way back at the start of the chapter, I said we would make a neural network
    that could classify whether an image is a cat or a fish. We’ve now trained one
    to do just that, but how do we use it to generate a prediction for a single image?
    Here’s a quick bit of Python code that will load an image from the filesystem
    and print out whether our network says *cat* or *fish*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我说过我们将制作一个神经网络，可以对图像进行分类，判断是猫还是鱼。我们现在已经训练了一个可以做到这一点的网络，但是我们如何使用它来为单个图像生成预测呢？这里有一段快速的Python代码，它将从文件系统加载一张图像，并打印出我们的网络是说“猫”还是“鱼”：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Most of this code is straightforward; we reuse the transform pipeline we made
    earlier to convert the image into the correct form for our neural network. However,
    because our network uses batches, it actually expects a 4D tensor, with the first
    dimension denoting the different images within a batch. We don’t have a batch,
    but we can create a batch of length 1 by using `unsqueeze(0)`, which adds a new
    dimension at the front of our tensor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码都很简单；我们重用了之前制作的转换流水线，将图像转换为神经网络所需的正确形式。然而，因为我们的网络使用批次，实际上它期望一个4D张量，第一个维度表示批次中的不同图像。我们没有批次，但我们可以通过使用`unsqueeze(0)`创建一个长度为1的批次，这会在我们的张量前面添加一个新的维度。
- en: Getting predictions is as simple as passing our *batch* into the model. We then
    have to find out the class with the higher probability. In this case, we could
    simply convert the tensor to an array and compare the two elements, but there
    are often many more than that. Helpfully, PyTorch provides the `argmax()` function,
    which returns the index of the highest value of the tensor. We then use that to
    index into our labels array and print out our prediction. As an exercise, use
    the preceding code as a basis to work out predictions on the test set that we
    created at the start of the chapter. You don’t need to use `unsqueeze()` because
    you get batches from the `test_data_loader`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 获取预测就像将我们的*batch*传递到模型中一样简单。然后我们必须找出具有更高概率的类别。在这种情况下，我们可以简单地将张量转换为数组并比较两个元素，但通常情况下不止这两个元素。幸运的是，PyTorch提供了`argmax()`函数，它返回张量中最高值的索引。然后我们使用该索引来索引我们的标签数组并打印出我们的预测。作为练习，使用前面的代码作为基础，在本章开头创建的测试集上进行预测。您不需要使用`unsqueeze()`，因为您从`test_data_loader`中获取批次。
- en: That’s about all you need to know about making predictions for now; we return
    to this in [Chapter 8](ch08.html#pytorch-in-production) when we harden things
    for production usage.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您现在需要了解的有关进行预测的全部内容；在[第8章](ch08.html#pytorch-in-production)中，我们将为生产使用加固事项时再次回顾这一点。
- en: In addition to making predictions, we probably would like to be able to reload
    the model at any point in the future with our trained parameters, so let’s take
    a look at how that’s done with PyTorch.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了进行预测，我们可能希望能够在将来的任何时间点重新加载模型，使用我们训练好的参数，因此让我们看看如何在PyTorch中完成这个任务。
- en: Model Saving
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型保存
- en: If you’re happy with the performance of a model or need to stop for any reason,
    you can save the current state of a model in Python’s *pickle* format by using
    the `torch.save()` method. Conversely, you can load a previously saved iteration
    of a model by using the `torch.load()` method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对模型的性能感到满意或因任何原因需要停止，您可以使用`torch.save()`方法将模型的当前状态保存为Python的*pickle*格式。相反，您可以使用`torch.load()`方法加载先前保存的模型迭代。
- en: 'Saving our current parameters and model structure would therefore work like
    this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，保存我们当前的参数和模型结构将像这样工作：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And we can reload as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式重新加载：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This stores both the parameters and the structure of the model to a file. This
    might be a problem if you change the structure of the model at a later point.
    For this reason, it’s more common to save a model’s `state_dict` instead. This
    is a standard Python `dict` that contains the maps of each layer’s parameters
    in the model. Saving the `state_dict` looks like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这将模型的参数和结构都存储到文件中。如果以后更改模型的结构，这可能会成为一个问题。因此，更常见的做法是保存模型的`state_dict`。这是一个标准的Python`dict`，其中包含模型中每个层的参数映射。保存`state_dict`看起来像这样：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To restore, create an instance of the model first and then use `load_state_dict`.
    For `SimpleNet`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要恢复，首先创建模型的一个实例，然后使用`load_state_dict`。对于`SimpleNet`：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The benefit here is that if you extend the model in some fashion, you can supply
    a `strict=False` parameter to `load_state_dict` that assigns parameters to layers
    in the model that do exist in the `state_dict`, but does not fail if the loaded
    `state_dict` has layers missing or added from the model’s current structure. Because
    it’s just a normal Python `dict`, you can change the key names to fit your model,
    which can be handy if you are pulling in parameters from a completely different
    model altogether.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的好处是，如果以某种方式扩展了模型，可以向`load_state_dict`提供一个`strict=False`参数，该参数将参数分配给模型中存在的层，但如果加载的`state_dict`中的层缺失或添加到模型的当前结构中，则不会失败。因为它只是一个普通的Python`dict`，您可以更改键名称以适应您的模型，如果您从完全不同的模型中提取参数，这可能会很方便。
- en: Models can be saved to a disk during a training run and reloaded at another
    point so that training can continue where you left off. That is quite useful when
    using something like Google Colab, which lets you have continuous access to a
    GPU for only around 12 hours. By keeping track of time, you can save the model
    before the cutoff and continue training in a new 12-hour session.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练运行期间可以将模型保存到磁盘，并在另一个时间点重新加载，以便可以在离开的地方继续训练。当使用像Google Colab这样的工具时，这非常有用，它让您在大约12小时内持续访问GPU。通过跟踪时间，您可以在截止日期之前保存模型，并在新的12小时会话中继续训练。
- en: Conclusion
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You’ve taken a whirlwind tour through the basics of neural networks and learned
    how, using PyTorch, you can train them with a dataset, make predictions on other
    images, and save/restore models to and from disk.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经快速浏览了神经网络的基础知识，并学会了如何使用PyTorch对其进行训练，对其他图像进行预测，并将模型保存/恢复到磁盘。
- en: Before you read the next chapter, experiment with the `SimpleNet` architecture
    we created here. Adjust the number of parameters in the `Linear` layers, and maybe
    add another layer or two. Have a look at the various activation functions available
    in PyTorch and swap out `ReLU` for something else. See what happens to training
    if you adjust the learning rate or switch out the optimizer from Adam to another
    option (perhaps try vanilla SGD). Maybe alter the batch size and the initial size
    of the image as it gets turned into a 1D tensor at the start of the forward pass.
    A lot of deep learning work is still in the phase of artisanal construction; learning
    rates are tinkered with by hand until a network is trained appropriately, so it’s
    good to get a handle on how all the moving parts interact.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读下一章之前，尝试一下我们在这里创建的`SimpleNet`架构。调整`Linear`层中的参数数量，也许添加一两个额外的层。查看PyTorch中提供的各种激活函数，并将`ReLU`替换为其他函数。看看如果调整学习率或将优化器从Adam切换到其他选项（也许尝试普通的SGD），训练会发生什么变化。也许改变批量大小和图像在前向传递开始时被转换为1D张量的初始大小。许多深度学习工作仍处于手工调整阶段；学习率是手动调整的，直到网络被适当训练，因此了解所有移动部件如何相互作用是很重要的。
- en: You might be a little disappointed with the accuracy of the `SimpleNet` architecture,
    but don’t worry! [Chapter 3](ch03.html#convolutional-neural-networks) provides
    some definite improvements as we introduce the convolutional neural network in
    place of the very simple network we’ve been using so far.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对`SimpleNet`架构的准确性有些失望，但不用担心！[第三章](ch03.html#convolutional-neural-networks)将引入卷积神经网络，带来明显的改进，取代我们目前使用的非常简单的网络。
- en: Further Reading
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[PyTorch documentation](https://oreil.ly/x6pO7)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch文档](https://oreil.ly/x6pO7)'
- en: '[“Adam: A Method for Stochastic Optimization”](https://arxiv.org/abs/1412.6980)
    by Diederik P. Kingma and Jimmy Ba (2014)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《Adam：一种随机优化方法》（2014）作者Diederik P. Kingma和Jimmy Ba
- en: '[“An Overview of Gradient Descent Optimization Algorithms”](https://arxiv.org/abs/1609.04747)
    by Sebstian Ruder (2016)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《梯度下降优化算法概述》（2016）作者Sebstian Ruder
