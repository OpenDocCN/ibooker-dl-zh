# 第十章. 生成模型

生成模型试图理解产生我们所看到的数据的*潜在*或基础过程。例如，在分解 MNIST 数据集中的数字图像时，我们可以将生成每个图像的潜在过程的一些属性解释为数字本身（从零到九的离散变量）、将绘制图像的方向或角度、生成图像的大小、线条的粗细以及一些噪声成分（所有这些都是连续变量）。到目前为止，我们关注的是*鉴别*模型，无论是在回归还是分类设置中。在分类设置中，鉴别模型将 MNIST 数据集中的图像作为输入，并尝试确定输入最有可能属于的数字类别，从零到九。生成模型则尝试完全建模数据分布，并在此过程中可能隐含地尝试学习先前提到的一些特征，以生成看起来就像最初来自 MNIST 数据集的图像。请注意，生成建模比鉴别建模更难，因为例如，鉴别模型可能只需要很好地学习几个特征，以令人满意地区分 MNIST 数据集中不同数字之间的差异。生成模型有许多种类，在本章中，我们提供了一个瞥见过去十年才开始蓬勃发展的广阔研究领域。

# 生成对抗网络

*生成对抗网络*，简称 GANs，是一种旨在从噪声中生成逼真实体样本（如图像）的生成模型。它们由 Goodfellow 等人于 2014 年提出。在本节的其余部分，我们将假设我们正在处理图像数据集，如 MNIST 或 CIFAR-10。原始的 GAN 架构被分解为两个神经网络：*鉴别器*和*生成器*。

生成器从一些噪声分布（如多元高斯分布）中获取样本，并输出一幅图像。鉴别器的任务是预测这幅图像是由生成器生成的，还是从原始数据集中抽样的。随着生成器越来越擅长生成看起来真实的图像，鉴别器越来越难以确定给定图像是由生成器生成的还是从数据集中抽样的。我们可以将这两个网络看作参与一场游戏，彼此竞争以发展。每个网络都会不断演化，直到生成器最终能够生成看起来就像直接从原始数据集中绘制出来的图像，而鉴别器无法区分这两组图像，即预测任何图像来自数据集的概率为 1/2。

更严格地说，我们定义数据分布为 p_data(x)。虽然我们永远无法真正了解真实的数据分布，但在实践中，我们通常认为我们手头的数据集足够好地近似了它（p_data(x)只是数据集中所有图像的均匀分布，并且与不在数据集中的所有图像相关联的可能性为零）。

我们另外定义由生成器参数化的分布为<p<sub>g</sub>(x)>。随机变量*x*代表一个实体，比如一幅图像，一个由每个像素组成的集合，每个像素可以被视为自己的随机变量。我们也称之为*G*的生成器通过将从噪声分布中采样的样本映射到数据空间来定义<p<sub>g</sub>(x)，我们将其称为*p(z)*，数据空间包括所有可能的图像（不仅仅是数据集中的图像）。重要的是要记住*G*本身是一个确定性函数，但通过作用于噪声分布隐含地定义了一个分布。请注意，这个分布是隐含的，因为我们只能通过*G(z)*生成样本，而不能直接使用它并查询图像的可能性。图 10-1 展示了典型的 GAN 架构。

![](img/fdl2_1001.png)

###### 图 10-1。鉴别器确定任何输入图像是从数据集还是生成器中采样的。生成器的目标是欺骗鉴别器，使其相信其图像是从数据集中采样的。

对于给定数据集的最佳生成器还会参数化<p<sub>data</sub>(x)，因为这将完全混淆甚至最好的鉴别器。换句话说，如果生成器参数化与数据集完全相同的分布，并且从生成器或数据集中采样的可能性相等，那么没有鉴别器能够告诉查询的来源，因为两者始终是同等可能的。我们将在下一段正式表达这种直觉。

回想一下第二章，假设有一个生成器参数化与数据集相同的分布，我们有<math alttext="p left-parenthesis x vertical-bar y equals generator right-parenthesis equals p left-parenthesis x vertical-bar y equals dataset right-parenthesis comma for-all x"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math>，其中*y*是一个伯努利随机变量，有两个选项：生成器或数据集。请注意，我们可以互换使用<math alttext="p Subscript g Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>和<math alttext="p left-parenthesis x vertical-bar y equals generator right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow></math>，以及<math alttext="p Subscript data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>和<math alttext="p left-parenthesis x vertical-bar y equals dataset right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow></math>，因为它们表示相同的含义。每个后者选项让我们记住我们正在处理条件概率。再次假设从生成器采样和从数据集采样是同等可能的，或<math alttext="p left-parenthesis y equals generator right-parenthesis equals p left-parenthesis y equals dataset right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow></math>，我们可以使用贝叶斯定理得到等式：<math alttext="p left-parenthesis y equals generator vertical-bar x right-parenthesis equals p left-parenthesis y equals dataset vertical-bar x right-parenthesis comma for-all x"><mrow><mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math>。由于只有两个选项，因为*y*是一个伯努利随机变量，我们得到了之前提到的完全混淆的鉴别器，预测任何图像被从数据集中采样的概率为<math alttext="one-half"><mfrac><mn>1</mn> <mn>2</mn></mfrac></math>。

了解我们的最终目标后，我们现在可以开始设计一个目标函数，用于同时训练我们的生成器和鉴别器。在原始的 GAN 论文中，提出的目标是：

<math alttext="upper V left-parenthesis upper G comma upper D right-parenthesis equals double-struck upper E Subscript x tilde p Sub Subscript data Subscript left-parenthesis x right-parenthesis Baseline left-bracket log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck upper E Subscript z tilde p left-parenthesis z right-parenthesis Baseline left-bracket log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis right-bracket"><mrow><mi>V</mi> <mrow><mo>(</mo> <mi>G</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mrow></math>

*G(z)*代表从噪声分布到数据空间的映射，*D(x)*代表分配给输入图像的分数。*D(x)*被解释为输入图像来自数据集的概率。当然，鉴别器*D*希望最大化这个目标——这对应于为从数据集中绘制的图像分配高概率，而不是由生成器*G*生成的图像。另一方面，*G*希望最小化这个目标，因为这对应于生成逼真的图像，甚至是与数据集中的图像完全相同的图像，这会让*D*感到困惑，并导致它为这些生成器生成的图像返回高分数。这种最大化一个网络的目标和最小化另一个网络的目标的想法被称为*minimax*，优化过程如下：

<math alttext="min Subscript upper G Baseline max Subscript upper D Baseline double-struck upper E Subscript x tilde p Sub Subscript data Subscript left-parenthesis x right-parenthesis Baseline left-bracket log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck upper E Subscript z tilde p left-parenthesis z right-parenthesis Baseline left-bracket log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis right-bracket"><mrow><msub><mtext>min</mtext> <mi>G</mi></msub> <msub><mtext>max</mtext> <mi>D</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mrow></math>

论文继续展示，对于固定生成器*G*，在这个目标下训练的最优鉴别器将输出以下得分：

<math alttext="StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></math>

对于给定的图像*x*。首先，我们考虑为什么这应该描述一个固定生成器下最优鉴别器的行为。在我们深入“为什么”之前，重要的是要记住*D*可以被另外表示为<math alttext="p Subscript theta Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，或者鉴别器认为图像是从数据集中抽取的信念。这里<math alttext="theta"><mi>θ</mi></math>代表*D*的参数或权重。当我们执行诸如梯度下降的更新操作时，<math alttext="theta"><mi>θ</mi></math>代表正在更新的权重集。重要的是要记住，这个分布与之前提到的<math alttext="p left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></math>是不同的——后者是给定图像从数据集中抽取的真实概率。

最优鉴别器永远无法准确知道图像的确切来源，除非生成器无法产生图像，即<math alttext="p Subscript g Baseline left-parenthesis x right-parenthesis equals 0"><mrow><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>。我们可以量化鉴别器预测的不确定性，作为图像在数据分布下的可能性函数，或者<math alttext="p Subscript data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，以及在*G*定义的分布下的图像可能性函数，或者<math alttext="p Subscript g Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>。如果图像在生成器定义的分布下的可能性小于数据分布下的可能性，那么最优鉴别器应该相应地受到影响，并且应该将图像评分更接近于一而不是零。

###### 注意

请注意，一个快速的估算显示这个性质对于得分<math alttext="StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction"><mfrac><msub><mi>p</mi> <mtext>data</mtext></msub> <mo>(</mo><mi>x</mi><mo>)</mo> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></math>是真实的。但为什么这个性质的确切比例是真实的呢？让我们更具体地看一下得分<math alttext="StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction"><mfrac><msub><mi>p</mi> <mtext>data</mtext></msub> <mo>(</mo><mi>x</mi><mo>)</mo> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></math>并确定为什么这是两个概率的最优函数。

从我们关于完全混淆的鉴别器的讨论中获得一些启示，我们可以用条件概率的术语来表达所提出的最优鉴别器得分：

<math alttext="StartFraction p left-parenthesis x vertical-bar y equals dataset right-parenthesis Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis plus p left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction"><mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>+</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mfrac></math>

此外，假设从数据集中抽样与从生成器中抽样具有相同的概率（<math alttext="p left-parenthesis y equals dataset right-parenthesis equals p left-parenthesis y equals generator right-parenthesis equals 0.5"><mrow><mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>），我们可以得到一个更具解释性的最优得分表示：

<math alttext="upper D Superscript asterisk Baseline left-parenthesis x right-parenthesis equals StartFraction p left-parenthesis x vertical-bar y equals dataset right-parenthesis Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis plus p left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction"><mrow><msup><mi>D</mi> <mo>*</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>+</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mfrac></mrow></math>

<math alttext="equals StartFraction p left-parenthesis x vertical-bar y equals dataset right-parenthesis asterisk p left-parenthesis y equals dataset right-parenthesis Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis asterisk p left-parenthesis y equals dataset right-parenthesis plus p left-parenthesis x vertical-bar y equals generator right-parenthesis asterisk p left-parenthesis y equals generator right-parenthesis EndFraction"><mrow><mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>*</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>*</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>+</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo><mo>*</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mfrac></mrow></math>

<math alttext="equals StartFraction p left-parenthesis x comma y equals dataset right-parenthesis Over p left-parenthesis x right-parenthesis EndFraction"><mrow><mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></math>

<math alttext="equals p left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></math>

第三个等式中的分母是通过边缘化*y*得到的结果。最终结果只是在给定输入图像的情况下从数据集中抽样的条件概率。最优鉴别器，<math alttext="p Subscript theta Sub Superscript asterisk Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><msub><mi>p</mi> <msup><mi>θ</mi> <mo>*</mo></msup></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，应该努力匹配输入图像来自数据集的真实概率，<math alttext="p left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></math>。

现在，我们考虑为什么之前定义的极小极大目标由<math alttext="p left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></math>或在固定生成器的假设下给定图像*x*时从数据集中抽样的真实条件概率最大化。让我们仔细观察目标，并尝试以更具信息性的方式重新表述它，这可能为我们提供一些见解：

<math alttext="upper V left-parenthesis upper G comma upper D right-parenthesis equals double-struck upper E Subscript x tilde p Sub Subscript data Subscript left-parenthesis x right-parenthesis Baseline left-bracket log upper D left-parenthesis x right-parenthesis right-bracket plus double-struck upper E Subscript z tilde p left-parenthesis z right-parenthesis Baseline left-bracket log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis right-bracket"><mrow><mi>V</mi> <mrow><mo>(</mo> <mi>G</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript x tilde p left-parenthesis x vertical-bar y equals dataset right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis right-bracket plus double-struck upper E Subscript p Sub Subscript phi Subscript left-parenthesis x vertical-bar y equals generator right-parenthesis Baseline left-bracket log left-parenthesis 1 minus p Subscript theta Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript x tilde p left-parenthesis x vertical-bar y equals dataset right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis right-bracket plus double-struck upper E Subscript p Sub Subscript phi Subscript left-parenthesis x vertical-bar y equals generator right-parenthesis Baseline left-bracket log left-parenthesis p Subscript theta Baseline left-parenthesis y equals generator vertical-bar x right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

像往常一样，我们已经用条件概率的术语制定了目标。从第一个等式到第二个等式，我们注意到，对于噪声分布*p(z)*的期望值，然后对每个样本应用*G*等函数，等效于对由*G*的映射定义的数据空间上的分布取期望值。这在精神上类似于我们在第二章中讨论过的一个概念，即随机变量可以是其他随机变量的函数。还要注意从第二行开始添加字母<math alttext="phi"><mi>φ</mi></math> ——这个字母代表*G*的参数或权重。

仔细观察最终表达式，我们开始看到目标与熵和交叉熵的概念之间有很多相似之处，这些概念在第二章中介绍过。原来，我们可以稍微调整目标，而不影响最佳的θ，从而获得两个交叉熵项的负和的总和：

<math alttext="theta Superscript asterisk Baseline equals argmin Subscript theta Baseline upper V left-parenthesis upper G comma upper D right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <mi>V</mi> <mrow><mo>(</mo> <mi>G</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></mrow></math>

<math alttext="equals argmin Subscript theta Baseline double-struck upper E Subscript x tilde p left-parenthesis x vertical-bar y equals dataset right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis right-bracket plus double-struck upper E Subscript p Sub Subscript phi Subscript left-parenthesis x vertical-bar y equals generator right-parenthesis Baseline left-bracket log left-parenthesis p Subscript theta Baseline left-parenthesis y equals generator vertical-bar x right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals argmin Subscript theta Baseline minus upper H left-parenthesis p left-parenthesis x comma y equals dataset right-parenthesis comma p Subscript theta Baseline left-parenthesis x comma y equals dataset right-parenthesis right-parenthesis minus upper H left-parenthesis p left-parenthesis x comma y equals generator right-parenthesis comma p Subscript theta Baseline left-parenthesis x comma y equals generator right-parenthesis right-parenthesis"><mrow><mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <mo>-</mo> <mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>,</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow> <mo>,</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>

正如在第二章中讨论的那样，两个分布之间的交叉熵在两个分布完全相同时被最小化——在这里，我们通过简单地最大化负交叉熵来实现等效。因此，<math alttext="theta"><mi>θ</mi></math> 在<math alttext="theta Superscript asterisk"><msup><mi>θ</mi> <mo>*</mo></msup></math>时实现了最优权重集<math alttext="theta Superscript asterisk"><msup><mi>θ</mi> <mo>*</mo></msup></math>，当<math alttext="p Subscript theta Baseline left-parenthesis x comma y equals dataset right-parenthesis equals p left-parenthesis x comma y equals dataset right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow></mrow></math>和<math alttext="p Subscript theta Baseline left-parenthesis x comma y equals generator right-parenthesis equals p left-parenthesis x comma y equals generator right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow></mrow></math>。

作为我们的最后一步，我们希望展示在<math alttext="theta Superscript asterisk"><msup><mi>θ</mi> <mo>*</mo></msup></math>时，<math alttext="p Subscript theta Sub Superscript asterisk Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis equals p left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><msub><mi>p</mi> <msup><mi>θ</mi> <mo>*</mo></msup></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，正如承诺的那样。我们已经知道<math alttext="p Subscript theta Sub Superscript asterisk Baseline left-parenthesis x comma y equals dataset right-parenthesis equals p left-parenthesis x comma y equals dataset right-parenthesis"><mrow><msub><mi>p</mi> <msup><mi>θ</mi> <mo>*</mo></msup></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow></mrow></math>来自我们之前的工作。在两边都除以*p(x)*后，我们得到了期望的结果。

到目前为止，我们已经假设了一个固定的*G*，并展示了关于最优*D*的各种属性。不幸的是，在实践中我们不能假设一个固定的*G*，因为我们必须训练生成器和鉴别器。但是现在我们已经展示了一些关于最优*D*的属性，我们可以开始讨论*G*必须满足的属性，以实现全局最优——一个甚至可以完全混淆最优鉴别器的生成器。如果我们假设一个最优鉴别器，并将其得分<math alttext="StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></math>代入目标*V(G,D)*，我们得到一个仅依赖于*G*的参数或权重的目标：

<math alttext="upper C left-parenthesis upper G right-parenthesis equals double-struck upper E Subscript x tilde p Sub Subscript data Subscript left-parenthesis x right-parenthesis Baseline left-bracket log StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction right-bracket plus double-struck upper E Subscript x tilde p Sub Subscript g Subscript left-parenthesis x right-parenthesis Baseline left-bracket log left-parenthesis 1 minus StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction right-parenthesis right-bracket"><mrow><mi>C</mi> <mrow><mo>(</mo> <mi>G</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript x tilde p Sub Subscript data Subscript left-parenthesis x right-parenthesis Baseline left-bracket log StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction right-bracket plus double-struck upper E Subscript x tilde p Sub Subscript g Subscript left-parenthesis x right-parenthesis Baseline left-bracket log StartFraction p Subscript g Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript x tilde p left-parenthesis x vertical-bar y equals dataset right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis x vertical-bar y equals dataset right-parenthesis Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis plus p Subscript phi Baseline left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction right-bracket plus double-struck upper E Subscript x tilde p Sub Subscript phi Subscript left-parenthesis x vertical-bar y equals generator right-parenthesis Baseline left-bracket log StartFraction p Subscript phi Baseline left-parenthesis x vertical-bar y equals generator right-parenthesis Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis plus p Subscript phi Baseline left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow> <mrow><mi>p</mi><mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><msub><mi>p</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow> <mrow><mi>p</mi><mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow></mrow></math>

现在我们可以通过优化生成器权重<math alttext="phi"><mi>φ</mi></math> 来最小化这个目标。我们建议您参考原始 GAN 论文进行严格推导。然而，正如现在可能已经预料到的那样，最优分布*G*代表，或者 <math alttext="p Subscript g Sub Superscript asterisk Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <msup><mi>g</mi> <mo>*</mo></msup></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，等于 <math alttext="p Subscript data Baseline left-parenthesis x right-parenthesis comma for-all x"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math>。这符合我们最初对完全迷惑鉴别器的直觉，并显示原始 GAN 论文中提出的目标函数确实在理论上收敛到这个全局最优解。

现在我们有了一个最优的生成器和鉴别器，我们如何进行图像生成呢？我们只需要从我们的噪声分布*p(z)*中抽样，并将每个样本通过生成器运行。生成器，作为最优解，应该生成看起来就像是从数据集本身中绘制的图像。也许让你惊讶的是，在这个阶段鉴别器不再需要了，但它已经发挥了作用。鉴别器在与生成器竞争中发挥了关键作用，每个都在演变，直到后者能够生成完全迷惑鉴别器的图像。

请注意，与生成建模的标准解释不同，*z*并不代表一组生成数据的潜在变量。*z*只是扮演一个随机变量的角色，分布为我们的标准分布之一，比如均匀分布或标准多元高斯分布，这些分布易于抽样。当完全训练和优化时，*G*是一个复杂的、可微分的函数，将从*p(z)*中抽样的样本转换为从 <math alttext="p Subscript data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 中抽样的样本，这个分布近似于*p(x)*。在下一节中，我们将看到*G(z)*和重参数化技巧之间的相似之处，这也允许我们通过从易于抽样的分布中转换样本（通过一个可微分函数）来抽样。

# 变分自动编码器

与 GAN 的引入同时，Kingma 和 Welling 在他们 2014 年的开创性论文“自动编码变分贝叶斯”中引入了*变分自动编码器*，简称 VAE。VAE 的理念更多地根植于概率建模，而不是前面提到的 GAN。VAE 假设存在一组未观察到的潜在变量，我们将其表示为*z*，生成我们看到的数据，我们将其表示为*x*。更正式地说，我们说存在一个联合概率分布*p(x,z)*，覆盖潜在变量*z*和观察到的数据*x*，这个分布可以分解为 <math alttext="p left-parenthesis x vertical-bar z right-parenthesis p left-parenthesis z right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo> <mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></math>（见图 10-2）。回想第二章，这种分解是非常直观的。考虑到*z*和*x*的预定义角色，*z*取某个值并且*x*从这个*z*的设置中生成的宇宙比另一种方式更有意义。

![](img/fdl2_1002.png)

###### 图 10-2。z 代表生成每个*x*实例的潜在变量。从 z 指向 x 的箭头表示这种关系。

*x*可以代表任何类型的连续或离散数据，包括图像。由于我们对数据集的了解，我们还知道*x*的定义域。另一方面，*z*则更加难以捉摸。我们不知道*z*是什么样子，所以我们对它做了一些初始假设。例如，我们可能假设它最初采用高斯分布的形式，即*p(z)*是高斯的。再次回想第二章，我们说*p(z)*，或者我们对*z*的先验，是高斯的。

每当我们考虑这样的数据生成过程时，一些自然的概率问题就会浮现在脑海中。例如，分布是什么，或者已知*x*的情况下*z*的后验是什么？当我们观察数据时，关于潜在参数的信念通常会发生变化。以第二章中的硬币抛掷实验为例。我们最初假设抛掷正面的机会是 50-50，其中 50-50 可以被认为是我们的潜在参数*α*——决定正反面序列数据生成过程的参数。这有点简化了——实际上，我们最初对抛掷正面的概率*α*有一个分布，这是我们的先验分布。当然，先验的定义域是范围[0,1]，在这个范围内，设计先验*p(α)*使得*p(α=0.5)*大于所有其他设置的*α*是合理的。当我们观察到一系列抛掷时，我们通过贝叶斯定理更新了我们的先验。类似地，我们最初假设*p(z)*是一个具有一定均值和方差的高斯分布；但当我们观察数据时，我们重新计算了我们对后验*p(z|x)*的信念（见图 10-3）。

另一个自然而然的问题是：数据*x*在给定潜在变量*z*的某个设置时的分布*p(x|z)*是什么？在硬币抛掷设置中，*p(x|z)*很容易想象。由于我们完全了解实验，我们知道任何序列的概率只是每次抛掷的概率的乘积，这直接由*z*定义。然而，在更复杂的设置中，比如图像，我们可以假设数据*x*与潜在变量*z*之间的关系比这复杂得多。例如，观察图像时，很明显，给定像素的值受其相邻像素的值以及有时甚至比人们想象的更远的像素的影响很大。我们对硬币抛掷的简单独立假设不足以满足我们的需求。这就是为什么我们不能简单地使用贝叶斯定理来学习*z*上的后验的一个原因——它需要比我们目前立即可获得的更多关于系统的知识。

![](img/fdl2_1003.png)

###### 图 10-3。这里我们有一个硬币抛掷实验，其中先验设计为 0.5 具有最高的可能性。一旦我们看到一系列的正面和反面，由于正面比反面多，后验就会向右移动。

在变分自动编码器中，我们将这些分布编码为神经网络，可以看作是复杂的非线性函数，可以准确地建模潜在变量 z 和观察数据 x 之间的关系。我们将输出给定潜在变量设置下数据分布的神经网络，也称为解码器，表示为<pθ(x|z)，其中θ代表神经网络的权重。换句话说，θ的设置，除了神经网络的预定架构外，完全定义了模型对真实分布 p(x|z)的信念。我们优化θ以实现与真实分布最接近的设置。

我们还将后验 z|x 或 p(z|x)编码为神经网络。我们将这个神经网络称为编码器，表示为 qφ(z|x)。与解码器类似，我们优化φ以实现与真实后验最接近的设置。

Kingma 和 Welling 做出了一些关键观察，使变分自动编码器成为生成建模的实用手段（图 10-4）。第一点是*证据下界*（ELBO 简称），它是数据真实对数似然的一个下界，可以重新表述为一种允许在编码器和解码器参数上进行可处理优化的方式。第二点是一种重新参数化技巧，使得能够计算关于编码器参数φ的梯度的低方差估计。尽管现在这听起来像是很多行话，我们将更详细地讨论这些关键观察，并具体说明编码器-解码器架构。

![](img/fdl2_1004.png)

###### 图 10-4。Kingma 和 Welling 提出的整体 VAE 架构。请注意，z 和解码器后的图像都是来自编码器分布和解码器分布的样本。

假设我们观察到一些数据 *x*，其中每个个体示例可以表示为 <math alttext="x 上标左括号 i 右括号"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>。请注意，我们仍然假设存在一组潜在变量 *z* 生成我们所见的数据。我们将我们对观察到的数据的分析分为对每个个体示例 <math alttext="x 上标左括号 i 右括号"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> 的分析。我们知道存在一个真实的潜在变量后验 <math alttext="p 左括号 z 竖线 x 上标左括号 i 右括号 Baseline 右括号"><mrow><mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>，但我们不知道真实的后验是什么。我们假设它可以通过一些潜在变量的分布来近似 <math alttext="q 下标 phi Baseline 左括号 z 竖线 x 上标左括号 i 右括号 Baseline 右括号"><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>，其中 *q* 是一个分布族，优化更容易，但足够复杂以准确建模真实的后验。一个例子是多层神经网络，正如我们已经看到的，可以通过梯度下降有效优化，并且可以表示复杂的非线性函数。请注意，我们所见的每个示例 <math alttext="x 上标左括号 i 右括号"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> 都有一定的发生概率，我们可以写为 <math alttext="p 左括号 x 上标左括号 i 右括号 Baseline 右括号"><mrow><mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>。我们改为使用 <math alttext="log p 左括号 x 上标左括号 i 右括号 Baseline 右括号"><mrow><mo form="prefix">log</mo> <mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>，因为这使我们能够对我们之前遇到的项进行方便的分解，并且不会影响优化过程的真实性：

<math alttext="log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis equals log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis minus log p left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>=</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>

<math alttext="equals log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis minus log p left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis plus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis minus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo>=</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>+</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis minus log p left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis plus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis minus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>+</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis Over q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis EndFraction right-bracket plus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log StartFraction q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis Over p left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis EndFraction right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo><mi>z</mi><mo>)</mo></mrow> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow> <mrow><mi>p</mi><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>

= <math alttext="ELBO 加 KL 左括号 q 下标 phi Baseline 左括号 z 竖线 x 上标左括号 i 右括号 Baseline 右括号 StartAbsoluteValue EndAbsoluteValue p 左括号 z 竖线 x 上标左括号 i 右括号 Baseline 右括号 右括号"><mrow><mtext>ELBO</mtext> <mo>+</mo> <mtext>KL</mtext> <mo>(</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>|</mo> <mo>|</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>)</mo></mrow></math>

第一步是将单个示例<math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>的边际似然表达为示例本身和潜在因子*z*的函数。正如我们之前学到的，边际似然可以分解为联合分布<math alttext="p left-parenthesis z comma x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>,</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>和条件分布<math alttext="p left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>的商。对数函数使我们能够将这个商分解为两个项的对数之间的差异。在第二步中，我们使用一个小技巧，使我们可以方便地将近似后验插入等式中——添加和减去相同的项不应影响等式。在第三步中，我们插入了对近似后验的期望。为什么允许这样做？嗯，我们事先知道<math alttext="log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo form="prefix">log</mo> <mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>是一个常数。它只是在真实分布下发生示例的概率的对数，是固定的。因此，在等式的左侧进行期望不会改变任何东西，因为常数的期望就是常数本身。在右侧，我们现在已经更接近以前看到的用术语表达边际似然的对数。在倒数第二步中，我们将对数组合成商，并利用期望的线性性得到两个项的和：(1) 近似后验与真实后验之间的 KL 散度，以及(2) ELBO，或证据下界。

到目前为止，您可能已经注意到 KL 散度的形式与我们在第二章中遇到的略有不同。回想一下之前介绍的标准 KL 散度，其中真实分布是*p(x)*，其近似值是*q(x)*。我们定义的 KL 散度是两个分布的交叉熵与真实分布的熵之间的差异，表达如下：

<math alttext="double-struck upper E Subscript p left-parenthesis x right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis x right-parenthesis Over q left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>

我们可以看到，在这个推导中 KL 散度是完全相反的。期望是针对近似后验而不是真实后验，分子和分母被颠倒了。基本上我们看到的是<math alttext="double-struck upper E Subscript q left-parenthesis x right-parenthesis Baseline left-bracket log StartFraction q left-parenthesis x right-parenthesis Over p left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math> 而不是<math alttext="double-struck upper E Subscript p left-parenthesis x right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis x right-parenthesis Over q left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math> 。我们称之为*反向 KL 散度*，因为模型和真相的角色已经交换，这是我们在 VAE 中试图最小化的量。尽管这没有像标准 KL 那样清晰的物理解释，但请注意，反向 KL 散度*只是一种 KL 散度*，并保留了我们在第二章中讨论的所有属性。因此，优化反向 KL 散度仍然在<math alttext="q left-parenthesis x right-parenthesis equals p left-parenthesis x right-parenthesis comma for-all x"><mrow><mi>q</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math>时实现零的唯一全局最小值，因此它是一个有效的优化目标，因为当近似后验与真实后验完全相同时，它达到其唯一最小值。

然而，现实是真实后验<math alttext="p left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math> 对我们仍然是未知的。因此，我们无法直接最小化任何 KL 散度与真实后验。这就是 ELBO 发挥关键作用的地方。正如我们之前讨论的，<math alttext="log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo form="prefix">log</mo> <mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>是一个常数。因此，最小化反向 KL 散度等同于最大化 ELBO。现在证据下界这个名字应该更有意义了——随着我们最大化这个项，它提供了一个越来越好的真实示例对数概率的下界。如果我们能够开发一种有效地最大化 ELBO 的方法，我们就应该在开发生成模型的道路上走得更顺利。让我们重新表述 ELBO，使其更容易处理：

<math alttext="double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis Over q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis EndFraction right-bracket equals double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis minus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo><mi>z</mi><mo>)</mo></mrow> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis plus log p left-parenthesis z right-parenthesis minus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>+</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket plus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis z right-parenthesis minus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket minus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log StartFraction q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis Over p left-parenthesis z right-parenthesis EndFraction right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow> <mrow><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>

<math alttext="equals minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mrow><mo>=</mo> <mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

在这一点上，我们可以开始看到一个架构和一个最大化 ELBO 的优化过程的开端。例如，第一项只是近似后验和先验之间的反向 KL 散度，我们已经假设为高斯分布。我们可以使用一个神经网络，或编码器，来表示近似后验。反向 KL 散度作为一个正则化项作用在近似后验上，因为最大化反向 KL 的负值等同于最小化反向 KL。正则化防止近似后验偏离先验分布太远。这是可取的，因为我们只见过一个例子，因此我们不希望我们对潜在变量的信念从我们的先验中偏离太多。第二项是给定潜在变量*z*的设置的例子的期望真实对数似然，其中*z*是从近似后验中抽样的。希望相对于 <math alttext="phi"><mi>φ</mi></math> 最大化这个数量是直观合理的。这影响近似后验分配更高的可能性给*z*的设置，反过来，解释输入例子 <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>。在正则化和最大似然估计之间的平衡，防止过拟合，以及最大似然估计本身会达到一个最优点，其中 <math alttext="q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math> 只是一个最好描述 <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> 的*z*设置的点质量，是一个经典的优化过程，你可能在许多数据科学和机器学习问题中遇到过。

然而，正如前面所指出的，我们不幸地无法访问真实的条件分布 <math alttext="p left-parenthesis x vertical-bar z right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></math>。相反，我们尝试使用第二个神经网络——解码器来学习它。我们将解码器的参数表示为 <math alttext="theta"><mi>θ</mi></math>，并让解码器表示分布 <math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>。总之，我们执行以下优化过程：

<math alttext="phi Superscript asterisk Baseline comma theta Superscript asterisk Baseline equals argmax Subscript phi comma theta Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><msup><mi>φ</mi> <mo>*</mo></msup> <mo>,</mo> <msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext> <mrow><mi>φ</mi><mo>,</mo><mi>θ</mi></mrow></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

我们已经讨论过为什么这是对编码器参数φ的有效优化过程，假设 pθ|x,z=p|x,z。当然，在训练开始时，这个假设并不成立。然而，随着训练的进行和θ变得越来越优化，我们最终会达到期望的理论优化。但问题仍然存在：为什么这是对θ的有效优化过程？如果我们假设编码器代表真实的后验分布，我们希望最大化从我们的编码器样本*z*中恢复原始示例 xi 的可能性。当然，就像对φ的优化一样，我们对近似后验的假设在训练开始时并不成立，但随着训练的进行和两个网络共同改进，我们希望最终达到我们的目标。

这引导我们如何实际进行优化。对于θ，事实证明我们可以直接使用标准的小批量梯度下降技术：

<math alttext="normal nabla Subscript theta Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>θ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript theta Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus normal nabla Subscript theta Baseline double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript theta Baseline double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket normal nabla Subscript theta Baseline log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="almost-equals StartFraction 1 Over n EndFraction sigma-summation Underscript j equals 1 Overscript n Endscripts normal nabla Subscript theta Baseline log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z equals z Subscript j Baseline right-parenthesis"><mrow><mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msub><mi>∇</mi> <mi>θ</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>

第一个等式源于一个事实，即一组项的梯度等于每个项的梯度之和。由于第一项不是θ的函数，它对θ的梯度为 0，导致我们得到第二个等式。从那里我们有了标准的小批量梯度估计推导。

对于φ的优化并不那么简单。如果我们尝试对φ做与θ相同的操作，我们会遇到一个意想不到的问题：

<math alttext="normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus normal nabla Subscript phi Baseline double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus normal nabla Subscript phi Baseline integral q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis d z"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo>∫</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>z</mi></mrow></math>

<math alttext="equals normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus integral normal nabla Subscript phi Baseline q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis d z"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo> <mo>∫</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>z</mi></mrow></math>

在最后一步中，我们无法将第二项表示为期望。这是因为梯度是针对我们抽样的分布的参数。我们不能像对θ那样简单地交换期望和梯度的顺序。为了解决这个问题，我们做出以下观察：

<math alttext="normal nabla Subscript phi Baseline q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis equals normal nabla Subscript phi Baseline q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis asterisk StartFraction q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis Over q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis EndFraction"><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>*</mo> <mfrac><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></mfrac></mrow></math>

<math alttext="equals q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis asterisk StartFraction normal nabla Subscript phi Baseline q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis Over q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis EndFraction"><mrow><mo>=</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>*</mo> <mfrac><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></mfrac></mrow></math>

<math alttext="equals q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis normal nabla Subscript phi Baseline log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo>=</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>

通过一点微积分和代数，我们得到了梯度的等价形式。如果我们将这种重述替换到我们卡住的步骤中：

<math alttext="equals normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus integral q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis normal nabla Subscript phi Baseline log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis d z"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo> <mo>∫</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>z</mi></mrow></math>

<math alttext="equals normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket normal nabla Subscript phi Baseline log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="almost-equals normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus StartFraction 1 Over n EndFraction sigma-summation Underscript j equals 1 Overscript n Endscripts normal nabla Subscript phi Baseline log q Subscript phi Baseline left-parenthesis z equals z Subscript j Baseline vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z equals z Subscript j Baseline right-parenthesis"><mrow><mo>≈</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>j</mi></msub> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>

现在我们可以使用标准的小批量梯度估计技术来优化我们关于φ的目标。我们所做的观察是机器学习社区中一个众所周知的技术，称为*log 技巧*。在强化学习章节中介绍策略梯度方法时，我们将再次看到这种技术的应用。

现在我们已经完全剖析了 Kingma 和 Welling 提出的第一个观察，我们现在转向第二个观察：计算相对于 <math alttext="phi"><mi>φ</mi></math> 的梯度的低方差估计。正如我们之前提到的，对数技巧使我们能够估计这个梯度。然而，已经证明这种估计具有很高的方差。这意味着如果我们进行试验，在每次试验中，我们从近似后验中抽取一些样本 <math alttext="z Subscript j"><msub><mi>z</mi> <mi>j</mi></msub></math> 并估计相对于 <math alttext="phi"><mi>φ</mi></math> 的梯度，我们预计在不同试验中会看到梯度的估计大不相同。当然，这是不可取的，因为我们希望对于相同的输入示例，试验之间保持一致，以对我们的训练过程有信心。我们可以尝试通过为每个示例从近似后验中抽取许多样本来改善这一点，但这对于相对较小的收益而言在计算上是不可行的。

Kingma 和 Welling 提出了一种绕过使用网络权重参数化分布的梯度的问题的对数技巧的替代方法。这种方法被称为*重新参数化技巧*，它允许我们计算梯度的低方差估计，与对数技巧相反。为什么会这样超出了本文的范围，但我们建议您参考存在于这个和类似主题上的大量学术文献。

重新参数化技巧涉及假设近似后验服从某种形式，比如多元高斯分布，然后将这个分布表示为另一个分布的函数，该分布不依赖于编码器的权重。假设 <math alttext="q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math> 采用形式 <math alttext="upper N left-parenthesis z semicolon ModifyingAbove mu With caret Subscript phi Baseline comma ModifyingAbove sigma With caret Subscript phi Superscript 2 Baseline upper I right-parenthesis"><mrow><mi>N</mi> <mo>(</mo> <mi>z</mi> <mo>;</mo> <msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>,</mo> <msubsup><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi> <mn>2</mn></msubsup> <mi>I</mi> <mo>)</mo></mrow></math> 。这代表一个多元高斯分布，其中每个分量 <math alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math> 独立于所有其他分量，并且 <math alttext="z Subscript i Baseline tilde upper N left-parenthesis mu Subscript phi comma i Baseline comma sigma Subscript phi comma i Superscript 2 Baseline right-parenthesis"><mrow><msub><mi>z</mi> <mi>i</mi></msub> <mo>∼</mo> <mi>N</mi> <mrow><mo>(</mo> <msub><mi>μ</mi> <mrow><mi>φ</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>,</mo> <msubsup><mi>σ</mi> <mrow><mi>φ</mi><mo>,</mo><mi>i</mi></mrow> <mn>2</mn></msubsup> <mo>)</mo></mrow></mrow></math>，对于所有 i。我们在下标中使用 <math alttext="phi"><mi>φ</mi></math> 明确显示近似后验依赖于编码器参数的均值和方差向量，这些向量由编码器定义。在当前形式中，我们遇到了无法交换期望和梯度顺序的问题。使用重新参数化技巧，我们可以将抽样过程重写为：

<math alttext="z tilde upper N left-parenthesis ModifyingAbove mu With caret Subscript phi Baseline comma ModifyingAbove sigma With caret Subscript phi Superscript 2 Baseline upper I right-parenthesis left right double arrow z equals ModifyingAbove mu With caret Subscript phi Baseline plus ModifyingAbove sigma With caret Subscript phi Baseline asterisk epsilon comma epsilon tilde upper N left-parenthesis 0 comma upper I right-parenthesis"><mrow><mi>z</mi> <mo>∼</mo> <mi>N</mi> <mrow><mo>(</mo> <msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>,</mo> <msubsup><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi> <mn>2</mn></msubsup> <mi>I</mi> <mo>)</mo></mrow> <mo>⇔</mo> <mi>z</mi> <mo>=</mo> <msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>+</mo> <msub><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>*</mo> <mi>ϵ</mi> <mo>,</mo> <mi>ϵ</mi> <mo>∼</mo> <mi>N</mi> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>I</mi> <mo>)</mo></mrow></mrow></math>

我们强烈鼓励您通过高斯分布的定义来解释为什么采样过程可以以这种方式重写。首先考虑单变量情况会更容易，其中*X*是一个标准高斯随机变量，然后展示*Y = c*X*是一个均值为零，方差为<math alttext="c squared"><msup><mi>c</mi> <mn>2</mn></msup></math>的高斯随机变量。然后，考虑*X*是任意高斯随机变量的一般单变量情况，并展示*Y = X + c*是一个均值为*E[X]* + *c*，方差为*Var(X)*的高斯随机变量。将这些步骤结合起来，您将得到先前描述的重构采样过程。

总之，我们将近似后验表示为与<math alttext="phi"><mi>φ</mi></math>无关的分布函数，以及依赖于<math alttext="phi"><mi>φ</mi></math>的均值向量和标准差向量。我们将随机变量<math alttext="epsilon"><mi>ϵ</mi></math>称为*辅助随机变量*。将这种重构插入我们先前的棘手梯度表达式中：

<math alttext="normal nabla Subscript phi Baseline double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript phi Baseline double-struck upper E Subscript epsilon tilde upper N left-parenthesis 0 comma upper I right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar g Subscript phi Baseline left-parenthesis epsilon right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mo>(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="equals double-struck upper E Subscript epsilon tilde upper N left-parenthesis 0 comma upper I right-parenthesis Baseline left-bracket normal nabla Subscript phi Baseline log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar g Subscript phi Baseline left-parenthesis epsilon right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mo>(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>

<math alttext="almost-equals StartFraction 1 Over n EndFraction sigma-summation Underscript j equals 1 Overscript n Endscripts normal nabla Subscript phi Baseline log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar g Subscript phi Baseline left-parenthesis epsilon Subscript j Baseline right-parenthesis right-parenthesis"><mrow><mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <msub><mi>ϵ</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>

其中<math alttext="g Subscript phi Baseline left-parenthesis epsilon right-parenthesis equals ModifyingAbove mu With caret Subscript phi Baseline plus ModifyingAbove sigma With caret Subscript phi Baseline asterisk epsilon"><mrow><msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>+</mo> <msub><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>*</mo> <mi>ϵ</mi></mrow></math>。我们将*z*重写为<math alttext="g Subscript phi Baseline left-parenthesis epsilon right-parenthesis"><mrow><msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow></mrow></math>，明确显示了对编码器参数的依赖现在仅通过应用于采样分布的确定性函数，而不是采样分布本身。这使我们能够无缝切换期望和梯度的顺序，从而使其适用于标准的小批量梯度估计技术。

这种变化如何体现在编码器架构中？早期，当使用对数技巧时，我们可以通过编码器直接参数化近似后验。现在，我们改为编码器，对于每个示例<math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>，输出均值向量<math alttext="ModifyingAbove mu With caret Subscript phi"><msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math>，标准差向量<math alttext="ModifyingAbove sigma With caret Subscript phi"><msub><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math>，并从与编码器-解码器 VAE 架构完全分离的标准高斯分布中采样<math alttext="epsilon"><mi>ϵ</mi></math>。请注意，重新参数化技术有其自身的限制——我们必须假设近似后验的形式，本例中为高斯分布，这使我们能够定义一个可微函数，如<math alttext="g Subscript phi"><msub><mi>g</mi> <mi>φ</mi></msub></math>。然而，并没有保证真实后验是高斯分布——它很可能是一个复杂的分布，无法表示为我们标准分布的函数。这是我们必须做出的权衡，以获得可处理的优化的低方差梯度估计（图 10-5）。

![](img/fdl2_1005.png)

###### 图 10-5。在重新参数化后编码器的外观。它返回一个均值和标准差向量，我们可以将其与 ϵ 结合起来生成 z 的设置。圆形与矩形之间的区别在于 ϵ 的抽样是唯一发生的，完全独立于编码器架构。均值和标准差向量是确定性地从输入图像中产生的。此外，一旦我们知道 ϵ 的值，z 就是确定性的。

请注意，VAE 的训练过程非常简单——其复杂性在于架构和优化背后的动机和数学。我们需要做的只是：

1.  从数据集中抽样一个示例 <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>。

1.  通过编码器网络运行 <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> 生成一个均值向量 <math alttext="ModifyingAbove mu With caret Subscript phi"><msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math> 和一个标准差向量 <math alttext="ModifyingAbove sigma With caret Subscript phi"><msub><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math>。

1.  抽样 <math alttext="epsilon"><mi>ϵ</mi></math> 并计算 <math alttext="g Subscript phi Baseline left-parenthesis epsilon right-parenthesis"><mrow><msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow></mrow></math> 的结果。

1.  通过解码器网络运行结果，现在表示分布 <math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z equals g Subscript phi Baseline left-parenthesis epsilon right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>。

1.  用我们的初始示例 <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> 查询这个分布，并取结果似然的对数。这将是我们的 *解码器损失*。如果在步骤 3 中对 <math alttext="epsilon"><mi>ϵ</mi></math> 进行了多次抽样，请对每个样本运行上述过程，并取平均值以获得解码器损失。

1.  将解码器损失与 *编码器损失* <math alttext="minus upper K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis"><mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>|</mo> <mo>|</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></math> 相加，得到最终损失。在下一步中使用最终损失的负值，因为我们希望最大化它而不是最小化它。

1.  执行经典的 SGD/小批量梯度下降来更新 <math alttext="phi"><mi>φ</mi></math> 和 <math alttext="theta"><mi>θ</mi></math>。

现在我们已经讨论了如何训练 VAE，那么一旦训练完成，我们如何将其用作生成模型呢？请注意，我们最初将生成过程定义为<math alttext="p left-parenthesis x vertical-bar z right-parenthesis p left-parenthesis z right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo> <mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></math>，其中我们从先验分布中对潜在变量*z*进行采样，并通过条件似然将*z*映射到数据空间中的实例*x*。我们已经学习了这个生成过程的形式<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>，或者解码器，并假设先验分布<math alttext="p left-parenthesis z right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></math>最初是一个多元标准高斯分布。要从 VAE 生成样本，我们从先验分布*p(z)*中采样<math alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math>，将此样本通过解码器，使其现在表示分布<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z equals z Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>，最后从<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z equals z Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>中采样<math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>。请注意，在这一步我们不再需要近似后验 - 但是，在解码器的训练中起着关键作用，并且在理解我们的潜在变量分布在看到数据集中的示例后如何变化方面仍然很有用。

# 实现 VAE

在这一部分，我们将在 PyTorch 中从头开始构建一个 VAE。我们还将提供一些在著名的 MNIST 数字数据集上进行训练和测试的示例代码。

在开始之前，这里是您需要的软件包列表，以便自己重现本节内容：

```py
import torch
from torch.distributions.multivariate_normal \
  import MultivariateNormal
import torch.nn as nn
from torchvision import datasets, transforms
from torchvision.utils import save_image
import torch.optim as optim

```

让我们从编码器开始。正如我们在上一节中讨论的，编码器是一个神经网络，它输出一组均值向量和一组标准差向量。每个索引代表一个单变量高斯分布，整个向量代表一个多元高斯分布，其中每个分量与其他分量独立。虽然我们处理的是图像数据，但为了简单起见，我们在开始时将每个图像展平为一个向量。这使我们能够在输入上应用标准的全连接层。由于 MNIST 数据集中的每个图像大小为 28×28，因此每个结果表示是一个 784 维向量。我们还需要决定用于表示潜在空间的成分或潜在变量的数量。我们可以将成分的数量视为超参数 - 如果我们注意到即使经过大量训练，输入示例的解码器对数似然仍然一直很低，这可能表明近似后验不够表达。在这种情况下增加成分的数量并重新训练是明智的。

以下是一个编码器的示例代码：

```py
# Encoder layers (Gaussian MLP)
D_in, H, D_out = 784, 200, 20
input_layer = nn.Linear(D_in, H)
hidden_layer_mean = nn.Linear(H, D_out)
hidden_layer_var = nn.Linear(H, D_out)

```

为了简单起见，我们暂时忽略层之间的非线性。我们的编码器由两个级别的层组成。第一级别对输入进行操作，将向量嵌入到较低维度的表示中。第二级别对 200 维表示进行操作，由两个独立的层组成：一个用于确定每个单变量高斯分量的均值，一个用于确定每个单变量高斯分量的标准差。在这里，我们使用 20 个分量。正如我们之前所述，我们假设<math alttext="q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>采用多元高斯的形式，其中每个分量独立于其他分量。请注意，尝试学习完整的协方差矩阵在计算上是不可行的（除其他问题外），因为其大小随分量数量的增加呈二次增长。

以下是解码器的示例代码：

```py
# Decoder layers (Bernoulli MLP for MNIST data)
recon_layer = nn.Linear(D_out, H)
recon_output = nn.Linear(H, D_in)

```

为了简单起见，我们再次忽略非线性。解码器操作采样的*z*，我们知道它是一个 20 维向量。解码器的其余架构与编码器对称，并输出输入数据的分布。尽管代码中还没有，但最终会应用一个 sigmoid 层到`recon_output`层的输出上，回想一下，它将每个输入维度压缩到范围(0,1)。由于我们使用离散的 MNIST 数据集，其中每个像素表示为零或一，最终 sigmoid 层的输出用于表示每个像素的伯努利分布。回想一下第二章中的伯努利分布，表示为*Ber(p)*，其中*p*是返回一的概率，1-*p*是返回零的概率。

更正式地，我们有解码器的似然分布<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>可以重写为每个像素的乘积：

<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis equals product Underscript j equals 1 Overscript 784 Endscripts p Subscript theta Baseline left-parenthesis x Subscript j Baseline vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mn>784</mn></msubsup> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>

其中<math alttext="p left-parenthesis x Subscript j Baseline vertical-bar z right-parenthesis equals upper B e r left-parenthesis decoder left-parenthesis z right-parenthesis Subscript j Baseline right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>B</mi> <mi>e</mi> <mi>r</mi> <mrow><mo>(</mo> <mtext>decoder</mtext> <msub><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>

请注意，decoder(*z*)表示应用 Sigmoid 层后的 784 维向量。对于输入示例<math alttext="x Subscript j Superscript left-parenthesis i right-parenthesis"><msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></math> 中的给定像素<math alttext="x Subscript j Superscript left-parenthesis i right-parenthesis"><msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></math> ，我们希望其对应的概率*p*在<math alttext="x Subscript j Superscript left-parenthesis i right-parenthesis Baseline equals 1"><mrow><msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>=</mo> <mn>1</mn></mrow></math>时接近于 1，而在<math alttext="x Subscript j Superscript left-parenthesis i right-parenthesis Baseline equals 0"><mrow><msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>=</mo> <mn>0</mn></mrow></math>时接近于 0。正如您可能从上一节中记得的那样，我们使用<math alttext="log p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math> ，这可以简化为<math alttext="sigma-summation Underscript j equals 1 Overscript 784 Endscripts log p Subscript theta Baseline left-parenthesis x Subscript j Baseline vertical-bar z right-parenthesis"><mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mn>784</mn></msubsup> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math> 。

现在，我们可以将编码器和解码器放在一起，构建一个单一的 VAE 架构：

```py
class VAE(nn.Module):
  def __init__(self, D_in, H, D_out):
    super(VAE, self).__init__()
    self.D_in, self.H, self.D_out = D_in, H, D_out

    # Encoder layers (Gaussian MLP)
    self.input_layer = nn.Linear(D_in, H)
    self.hidden_layer_mean = nn.Linear(H, D_out)
    self.hidden_layer_var = nn.Linear(H, D_out)

    # Decoder layers (Bernoulli MLP for MNIST data)
    self.recon_layer = nn.Linear(D_out, H)
    self.recon_output = nn.Linear(H, D_in)
    self.tanh = nn.Tanh()
    self.sigmoid = nn.Sigmoid()

  def encode(self, inp):
    h_vec = self.input_layer(inp)
    h_vec = self.sigmoid(h_vec)
    means = self.hidden_layer_mean(h_vec)
    log_vars = self.hidden_layer_var(h_vec)
    return means, log_vars

  def decode(self, means, log_vars):
    # Reparametrization trick
    std_devs = torch.pow(2,log_vars)**0.5
    aux = MultivariateNormal(torch.zeros(self.D_out), \
    torch.eye(self.D_out)).sample()
    sample = means + aux * std_devs

    # Reconstruction
    h_vec = self.recon_layer(sample)
    h_vec = self.tanh(h_vec)
    output = self.sigmoid(self.recon_output(h_vec))
    return output

  def forward(self, inp):
    means, log_vars = self.encode(inp)
    output = self.decode(means, log_vars)
    return output, means, log_vars

  def reconstruct(self, sample):
    h_vec = self.recon_layer(sample)
    h_vec = self.tanh(h_vec)
    output = self.sigmoid(self.recon_output(h_vec))
    return output

```

在前向函数中，对 encode 的调用后面是对 decode 的调用。请注意，decode 仅使用来自近似后验的单个样本，因为我们发现对于 MNIST 数据集，单个样本就足够了，但这可以很容易地修改为适用于多个样本。为了计算反向 KL 散度，前向函数返回了 encode 调用的结果以及解码器的似然分布的结果。

这里是计算损失的示例代码：

```py
def compute_loss(inp, recon_inp, means, log_vars):
  # Calculate reverse KL divergence
  # (formula provided in Kingma and Welling)
  kl_loss = -0.5 * torch.sum(1 + log_vars
                            - means ** 2 - torch.pow(2,log_vars))

  # Calculate BCE loss
  loss = nn.BCELoss(reduction="sum")
  recon_loss = loss(recon_inp, inp)
  return kl_loss + recon_loss

```

我们建议您查看 PyTorch 文档中关于`nn.BCELoss`的内容，并验证它是否确实计算了输入示例<math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> 的负对数似然： <math alttext="minus sigma-summation Underscript j equals 1 Overscript 784 Endscripts log p Subscript theta Baseline left-parenthesis x Subscript j Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis"><mrow><mo>-</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mn>784</mn></msubsup> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math> 。我们还建议您验证`kl_loss`项是否是 Kingma 和 Welling 中推导的两个高斯分布之间的反向 KL 散度。将负对数似然和反向 KL 散度的总和作为最终损失项返回，使我们达到了上一节中第 6 步的结尾。最后，一些训练代码：

```py
D_in, H, D_out = 784, 500, 20
vae = VAE(D_in, H, D_out)
vae.to("cpu")

def train():
  vae.train()
  optimizer = optim.Adam(vae.parameters(), lr=1e-3)

  train_loader = torch.utils.data.DataLoader(
      datasets.MNIST('../data',
                     train=True,
                     download=True,
                     transform=transforms.ToTensor()),
                     batch_size=100,
                     shuffle=True)

  epochs = 10
  for epoch in range(epochs):
    for batch_idx, (data, _) in enumerate(train_loader):
      optimizer.zero_grad()
      data = data.view((100,784))
      output, means, log_vars = vae(data)
      loss = compute_loss(data, output, means, log_vars)
      loss.backward()
      optimizer.step()
      if (batch_idx * len(data)) % 10000 == 0:
        print(
            'Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}' \
            .format(
            epoch, batch_idx * len(data), len(train_loader.dataset),
            100\. * batch_idx / len(train_loader), loss.item()))
  torch.save(vae.state_dict(), "vae.%d" % epoch)

```

在这里，我们训练 VAE 10 个 epochs，保存每个 epoch 结束时的 VAE 状态。请注意，我们在这里固定了一些超参数，比如优化器的学习率和潜变量的数量。我们建议编写一些验证代码，除了这里呈现的训练代码，以选择最佳的超参数设置。

最后，我们如何测试我们完全训练的 VAE 的生成能力？我们知道生成过程可以写成*p(z)p(x|z)*，其中我们首先从先验中抽取一个样本<math alttext="z Subscript j"><msub><mi>z</mi> <mi>j</mi></msub></math>，将样本通过解码器运行，使解码器的似然分布现在表示为<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z equals z Subscript j Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>，然后从该分布中抽取<math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>。以下是将此逻辑付诸实践的代码：

```py
def test():
  dist = MultivariateNormal(torch.zeros(D_out), torch.eye(D_out))
  vae = VAE(D_in, H, D_out)
  vae.load_state_dict(torch.load("vae.%d" % 9))
  vae.eval()
  outputs = []

  for i in range(100):
    sample = dist.sample()
    outputs.append(vae.reconstruct(sample).view((1,1,28,28)))
  outputs = torch.stack(outputs).view(100,1,28,28)
  save_image(outputs, "prior_reconstruct_100.png", nrow=10)

```

`for`循环从近似后验中生成 100 个样本，对于每个样本，从相应的解码器似然分布中抽取输入数据的样本。代码的最后几行允许我们将样本保存在一个 10×10 的网格中，如图 10-6 所示。

![](img/fdl2_1006.png)

###### 图 10-6. 在 MNIST 数据集上训练的 VAE 生成的 100 个样本，训练了 10 个 epochs。

尽管图像有点模糊，我们仍然可以看出大多数样本中的数字。通过使用更复杂的架构，如 RNN、超参数调整和更长的训练时间，我们肯定会看到更好的结果。在下一节中，我们将介绍一种略有不同的生成模型方法，最近越来越受欢迎。

# 基于分数的生成模型

在本节中，我们通过一个略有不同的视角来接近生成建模，这与我们迄今为止遇到的情况有所不同。在经过最佳训练的 GAN 中，我们首先从一些噪声分布*p(z)*中抽样，然后将这个样本<math alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math>通过一个生成器*G*，将<math alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math>确定性地转换为一个样本<math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>，这个样本来自真实数据分布（我们使用我们的数据集近似真实数据分布*p(x)*，<math alttext="p Subscript data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>）。虽然*G*本身是一个确定性函数，*G(z)*是一个随机变量，其分布与真实数据分布相同。总之，我们通过生成器对*p(z)*中的样本的操作隐式地定义了我们域上的分布，以及通过一个简单的分布*p(z)*（如多元高斯分布）从真实数据分布中抽样的方法。

VAEs 在其概率建模方面更为明确。我们定义*z*为生成我们看到的数据*x*的一组潜在变量。我们明确地学习数据的条件分布<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>通过解码器，我们可以从中抽样。在经过最佳训练的 VAE 中，<math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis equals p left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>，是数据的真实条件概率。要使用经过最佳训练的 VAE 生成数据，我们首先从*p(z)*中抽样一组潜在变量的设置，然后将这个样本<math alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math>通过解码器运行，解码器现在参数化了分布<math alttext="p left-parenthesis x vertical-bar z equals z Subscript i Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>。这是一个明确的概率分布，我们现在可以从中抽样。

请注意，尽管 GAN 和 VAE 本身是非常不同的，但它们的架构和操作都涉及到一个额外的分布*p(z)*（无论是 GAN 中的噪声分布还是 VAE 中潜在变量的先验）。有没有一种方法可以从真实数据分布中抽样而不需要额外的分布？基于分数的生成模型试图做到这一点。

从概率分布中抽样的一种方法是一种称为*Langevin 动力学*的迭代过程。这个过程实际上是一类算法的一个实例，被称为*马尔可夫链蒙特卡罗（MCMC）*算法。激励 MCMC 算法并证明它们以无偏的方式从概率分布中抽样超出了本节的范围，但我们建议您参考关于这个主题的大量学术文献。

Langevin 动力学遵循以下定义的过程：

<math alttext="x Superscript left-parenthesis i plus 1 right-parenthesis Baseline equals x Superscript left-parenthesis i right-parenthesis Baseline plus eta normal nabla Subscript x Baseline log p left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis plus StartRoot 2 eta EndRoot epsilon comma epsilon tilde upper N left-parenthesis 0 comma upper I right-parenthesis"><mrow><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup> <mo>=</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo> <mi>η</mi> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>+</mo> <msqrt><mrow><mn>2</mn> <mi>η</mi></mrow></msqrt> <mi>ϵ</mi> <mo>,</mo> <mi>ϵ</mi> <mo>∼</mo> <mi>N</mi> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>I</mi> <mo>)</mo></mrow></mrow></math>

这里表示从*p(x)*中抽取的样本，这个动态方程向我们展示了如何生成下一个样本<math alttext="x Superscript left-parenthesis i plus 1 right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>，给定我们当前的样本。

请注意，如果我们在动态方程的末尾去除高斯噪声成分，我们将只是沿着*p(x)*的梯度向最大值移动，即，使用一些步长<math alttext="eta"><mi>η</mi></math>执行梯度上升。这个动态方程背后的直觉是，噪声成分的添加阻止我们简单地到达最大值*x*，而是允许我们探索高概率区域，从而探索低概率区域较少（图 10-7）。再次强调，为什么这种方式以无偏的方式产生*p(x)*的样本超出了本文的范围，但我们强烈鼓励您从学术文献中了解更多。

![](img/fdl2_1007.png)

###### 图 10-7。这里我们使用*f*来表示一个均值（也是最大值）在原点的高斯分布。每个等高线代表具有相等概率的位置。从图中可以看出，梯度直接指向最大值，但添加一点噪声使我们能够探索和从高密度区域中抽样，而不会收敛到最大值。

尽管我们使用对数概率的梯度而不是概率的梯度，但最大化 <math alttext="log p left-parenthesis x right-parenthesis"><mrow><mo form="prefix">log</mo> <mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> 的值与最大化 *p(x)* 的值相同，因为对数的凹性。更一般地，对数的凹性也保留了所有可能的 *x* 值之间的顺序关系，即，如果 <math alttext="p left-parenthesis x 1 right-parenthesis greater-than-or-equal-to p left-parenthesis x 2 right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>≥</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>，那么 <math alttext="log p left-parenthesis x 1 right-parenthesis greater-than-or-equal-to log p left-parenthesis x 2 right-parenthesis"><mrow><mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>≥</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>，反之亦然。因此，正如我们在“实现 VAE”中看到的那样，这些优化过程往往不会受到对数的影响。

然而，与 Langevin 动力学的主要问题，正如我们之前在其他生成模型中遇到的那样，我们不知道 *p(x)*，更不用说其对数的梯度了！但也许有一种方法可以对 <math alttext="normal nabla Subscript x Baseline log p left-parenthesis x right-parenthesis"><mrow><msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，我们称之为 <math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> 的*分数函数*，进行建模。这将使我们能够将分数直接插入 Langevin 动力学方程，并从 *p(x)* 中抽样，就好像我们一直知道 *p(x)* 一样。这就是基于分数的生成建模的想法。

暂时忘记从未知分布 *p(x)* 中抽样的问题，而是考虑学习 *p(x)* 的问题。从现在到本节结束，我们将只考虑学习和从连续概率分布中抽样的问题。与显式学习近似概率分布的 VAE 类似，我们可以尝试用学习版本 <math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 来近似 *p(x)*，其中 <math alttext="theta"><mi>θ</mi></math> 表示学习模型的参数。我们设想的是一个学习函数，比如一个神经网络，它以示例 *x* 作为输入，并输出一个似然度 <math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>。然而，没有办法确保 <math alttext="integral p Subscript theta Baseline left-parenthesis x right-parenthesis d x equals 1"><mrow><mo>∫</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>x</mi> <mo>=</mo> <mn>1</mn></mrow></math>，这是任何概率分布的必要条件。

相反，我们只能学习我们称之为非归一化概率分布的函数 <math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>。这是一个接受一个示例 *x* 并输出非归一化可能性的函数。理论上，我们可以通过 <math alttext="StartFraction q Subscript theta Baseline left-parenthesis x right-parenthesis Over upper Z left-parenthesis theta right-parenthesis EndFraction"><mfrac><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mi>Z</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mfrac></math> 来表示归一化概率分布 <math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，其中 <math alttext="upper Z left-parenthesis theta right-parenthesis equals integral q Subscript theta Baseline left-parenthesis x right-parenthesis d x"><mrow><mi>Z</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>∫</mo> <msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>x</mi></mrow></math>。不幸的是，这个积分通常是难以处理的，没有闭合形式的解。当然，也有例外。例如，对于一元高斯分布，<math alttext="upper Z left-parenthesis theta right-parenthesis equals sigma asterisk StartRoot 2 pi EndRoot"><mrow><mi>Z</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>σ</mi> <mo>*</mo> <msqrt><mrow><mn>2</mn> <mi>π</mi></mrow></msqrt></mrow></math>，其中 <math alttext="theta equals left-parenthesis mu comma sigma right-parenthesis"><mrow><mi>θ</mi> <mo>=</mo> <mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow></math> 是高斯分布的均值和标准差。但是，如果我们想通过神经网络来建模更具表现力的分布，几乎总是不可能可靠地计算 <math alttext="upper Z left-parenthesis theta right-parenthesis"><mrow><mi>Z</mi> <mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></math>，我们也将其称为*分区函数*。

我们如何学习这样一个非归一化概率分布？研究人员在机器学习和推理的历史上提出了许多学习 <math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 的方法，但一种特定的方法开始弥合学习非归一化概率分布和从其归一化版本中抽样之间的差距，比如通过类似 Langevin 动力学的过程。*分数匹配*，或者学习 <math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 的想法通过最小化 <math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 的得分函数与真实分布 <math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> 的得分函数之间的差异，最初由 Hyvarinen 在 2005 年提出。

在这里，我们展示了将差异最小化等同于最小化<math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>的得分函数和<math alttext="p left-parenthesis x right-parenthesis colon"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>:</mo></mrow></math>的得分函数之间的差异。

<math alttext="normal nabla Subscript x Baseline log q Subscript theta Baseline left-parenthesis x right-parenthesis equals normal nabla Subscript x Baseline log left-parenthesis p Subscript theta Baseline left-parenthesis x right-parenthesis asterisk upper Z left-parenthesis theta right-parenthesis right-parenthesis"><mrow><msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>Z</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript x Baseline log p Subscript theta Baseline left-parenthesis x right-parenthesis plus normal nabla Subscript x Baseline log upper Z left-parenthesis theta right-parenthesis"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <mi>Z</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript x Baseline log p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>

事实证明，<math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>的得分函数与<math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>的得分函数相同，<math alttext="for-all x"><mrow><mo>∀</mo> <mi>x</mi></mrow></math>。这是因为对数首先将<math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>和分区函数的乘积分离为对数的和，最后关于*x*的梯度消除了分区函数的对数，因为这个项仅取决于权重<math alttext="theta"><mi>θ</mi></math>，而不是*x*本身的函数。因此，最小化所提出的差异的最优<math alttext="theta"><mi>θ</mi></math>等同于最小化<math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>和*p(x)*之间得分的差异。以下是我们称之为*显式得分匹配*的优化过程。

<math alttext="upper J left-parenthesis theta right-parenthesis equals double-struck upper E Subscript p left-parenthesis x right-parenthesis Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal nabla Subscript x Baseline log q Subscript theta Baseline left-parenthesis x right-parenthesis minus normal nabla Subscript x Baseline log p left-parenthesis x right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><mi>J</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>∇</mi> <mi>x</mi></msub> <msubsup><mrow><mo form="prefix">log</mo><mi>p</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>

*<math alttext="theta Superscript asterisk Baseline equals argmin Subscript theta Baseline upper J left-parenthesis theta right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>*

领先的 <math alttext="one-half"><mfrac><mn>1</mn> <mn>2</mn></mfrac></math> 的原因是为了简化结果的梯度（与将从范数的平方中下拉的 2 相互抵消）。请注意，在我们的分析中完全消除了对分区函数的依赖，现在我们有了一种方法来（1）学习一个未归一化的分布 <math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，以及（2）通过我们的神经网络计算 <math alttext="p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math> 的得分。对于第一项，在我们找到一个导致 <math alttext="upper J left-parenthesis theta right-parenthesis equals 0"><mrow><mi>J</mi> <mo>(</mo> <mi>θ</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math> 的设置 <math alttext="theta"><mi>θ</mi></math> 的情况下，<math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 和 *p(x)* 对于所有 *x* 都是相同的，因为它们对于所有 *x* 的梯度都是相同的。当然，在一般情况下，两个在任何地方具有相同梯度的函数仍然可以通过相互偏离一个非零常数而成为不同的函数。然而，在我们的情况下，这两个函数不能通过非零常数偏离，因为它们都是必须总和为一的概率分布。因此，我们有一个有效的优化过程来学习一个未归一化的分布，当归一化时，应该很好地逼近真实分布。

执行项目 2，理论上我们只需要首先通过我们的神经网络运行示例<math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>，得到<math alttext="q Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>，取<math alttext="q Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>的对数，并将这个结果反向传播到输入端。我们已经证明了结果分数等同于<math alttext="p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>的分数。接下来，我们将把<math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>（和<math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>）的分数函数称为<math alttext="normal upper Psi Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>Ψ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，以及<math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>的分数函数称为<math alttext="normal upper Psi left-parenthesis x right-parenthesis"><mrow><mi>Ψ</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>。使用我们的新符号，我们将显式分数匹配目标重写为：

<math alttext="theta Superscript asterisk Baseline equals argmin Subscript theta Baseline double-struck upper E Subscript p left-parenthesis x right-parenthesis Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal upper Psi Subscript theta Baseline left-parenthesis x right-parenthesis minus normal upper Psi left-parenthesis x right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>Ψ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <msubsup><mrow><mi>Ψ</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>

尽管我们已经解决了分区函数的问题，但我们仍然不知道<math alttext="normal upper Psi left-parenthesis x right-parenthesis"><mrow><mi>Ψ</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>是什么。Hyvarinen 在 2005 年提出了显式分数匹配的概念，并证明了关于显式分数匹配的一个惊人性质（在某些弱正则条件下满足）：

<math alttext="double-struck upper E Subscript p left-parenthesis x right-parenthesis Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal upper Psi Subscript theta Baseline left-parenthesis x right-parenthesis minus normal upper Psi left-parenthesis x right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket equals double-struck upper E Subscript p left-parenthesis x right-parenthesis Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal upper Psi Subscript theta Baseline left-parenthesis x right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline plus sigma-summation Underscript i equals 1 Overscript d Endscripts normal nabla Subscript x Sub Subscript i Subscript Baseline normal upper Psi Subscript theta comma i Baseline left-parenthesis x right-parenthesis plus c right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>Ψ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <msubsup><mrow><mi>Ψ</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo> <mo>=</mo></mrow> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>Ψ</mi> <mi>θ</mi></msub> <msubsup><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>d</mi></msubsup> <msub><mi>∇</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub> <msub><mi>Ψ</mi> <mrow><mi>θ</mi><mo>,</mo><mi>i</mi></mrow></msub> <mrow><mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mo>]</mo></mrow></mrow></math>

其中<math alttext="normal upper Psi Subscript theta comma i Baseline left-parenthesis x right-parenthesis equals normal nabla Subscript x Sub Subscript i Baseline log p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>Ψ</mi> <mrow><mi>θ</mi><mo>,</mo><mi>i</mi></mrow></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>∇</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>——分数函数只是一个长度为*d*的向量（假设*x*是*d*维），其中每个索引<math alttext="i"><mi>i</mi></math>对应于对*log*概率关于<math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>的偏导数。*c*是一个与<math alttext="theta"><mi>θ</mi></math>无关的常数，因此在优化过程中可以简单忽略。这是社区已知的一种方法，称为*隐式分数匹配*。

请注意，等效表达式不依赖于真实概率分布，因此我们可以直接优化*θ*，将其用作任何其他目标一样。一旦我们学习到最佳的*θ*，进行生成建模所需做的就是：

1.  按照之前提出的方法计算*p<sub>θ</sub>(x<sup>(i)</sup>)*的得分：通过我们学习的网络运行示例，取结果的对数，并反向传播直到输入。

1.  从*N(0,I)*中抽取样本*epsilon*。

1.  将步骤 1 和 2 的结果插入 Langevin 动力学方程中，以获得下一个样本*x<sup>(i+1)</sup>*。

1.  重复步骤 1 到 3，使用*x<sup>(i+1)</sup>*。

这个过程允许我们从*p<sub>θ</sub>(x)*中抽取样本，正如之前所示，一旦网络训练完成，它应该很好地逼近*p(x)*。

我们能否比隐式得分匹配做得更好？首先，隐式得分匹配要求我们计算二阶梯度，正如在隐式得分匹配目标中的*∑<sub>i=1</sub><sup>d</sup>∇<sub>x<sub>i</sub></sub>Ψ<sub>θ,i</sub>(x)*一项中所示。这取决于*x*的大小，这可能会非常耗费计算资源。在 PyTorch 等框架中，这将首先通过标准方法（如反向传播）计算第一阶梯度，然后手动循环遍历每个*x<sub>i</sub>*来计算其二阶梯度。在接下来的部分中，我们将介绍*去噪自动编码器*和*去噪得分匹配*，这些修改了目标并使我们能够避开这些复杂性问题。

# 去噪自动编码器和得分匹配

在解释去噪自动编码器和得分匹配之间的联系之前，我们首先要激发去噪自动编码器的架构。在第九章中，我们通过表示学习的视角了解了自动编码器。我们使用自动编码器将高维数据（如图像）压缩成保留信息或重构原始数据所需的有用特征的低维表示。此外，通过我们在 MNIST 上的实验，我们能够相当好地重构数据，并且通常看到给定数字实例的低维表示聚类。这意味着如果我们在这些低维表示上训练标准分类器，标签为它们的原始数字类别，我们预计会看到很高的准确性。

然而，根据我们尝试压缩的数据，有时候我们的压缩并不能捕捉到有用的特征。换句话说，当我们在现实世界中使用我们训练过的自动编码器处理可能略有损坏、旋转、移位或在各种光照条件下拍摄的图像时，我们使用它们的低维表示来对这些图像进行分类的能力会大幅下降。

理想情况下，我们希望我们学到的表示对这种噪声是不变的。2008 年，Vincent 提出了去噪自动编码器作为对标准自动编码器存在问题的一种方法。去噪自动编码器首先用噪声损坏原始输入数据，然后将损坏的输入通过标准自动编码器运行，并最终尝试重构原始输入（图 10-8）。原始论文使用了一个随机将输入的一部分置零的损坏方案，但承认可以使用各种损坏方案。直觉上，从这样一个过程中学到的表示应该对真实世界图像所面临的挑战更加稳健。事实上，Vincent 在 2008 年对 MNIST 数据集进行的实验表明，在旋转和背景噪声等各种数据增强下，去噪自动编码器在分类准确度方面明显优于标准自动编码器。

![](img/fdl2_1008.png)

###### 图 10-8。去噪自动编码器的架构与标准自动编码器相同，只是不再最小化 y 和输入 x'之间的重构误差，而是最小化 y 和原始 x 之间的重构误差。

根据 Vincent 2011 年的研究，首次注意到去噪自动编码器和评分匹配之间的联系，我们将定义损坏方案为向原始数据添加高斯噪声。形式上，我们有<math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>代表数据的真实分布，<math alttext="p Subscript d a t a Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>代表使用我们的训练集的数据分布，<math alttext="p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis"><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>代表给定原始数据的损坏数据的条件分布。特别地：

<math alttext="p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis equals upper N left-parenthesis x prime semicolon x comma sigma squared upper I right-parenthesis"><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>N</mi> <mrow><mo>(</mo> <mi>x</mi> <mi>â</mi> <mi></mi> <mi></mi> <mo>;</mo> <mi>x</mi> <mo>,</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi> <mo>)</mo></mrow></mrow></math>

其中分布的均值是原始数据，下标<math alttext="sigma"><mi>σ</mi></math>代表应用于原始数据的高斯噪声的标准差。注意*x'*和*x*定义在相同的域上（例如所有可能的图像）。我们现在可以计算损坏数据的分布：

<math alttext="p Subscript sigma Baseline left-parenthesis x prime right-parenthesis equals sigma-summation Underscript x Endscripts p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis p left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mi>x</mi></msub> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>

<math alttext="almost-equals sigma-summation Underscript x Endscripts p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis p Subscript d a t a Baseline left-parenthesis x right-parenthesis"><mrow><mo>≈</mo> <msub><mo>∑</mo> <mi>x</mi></msub> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>

<math alttext="equals StartFraction 1 Over n EndFraction sigma-summation Underscript i equals 1 Overscript n Endscripts p Subscript sigma Baseline left-parenthesis x prime vertical-bar x equals x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>=</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>

这是使用我们数据集中的每个数据点作为参考计算条件概率的经验平均值。这自然地遵循了让真实分布由数据集定义的分布来近似（与“生成对抗网络”中的定义方式相同）。

在 2011 年，Vincent 探讨了使用<p><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>作为参考的可能性，而不是我们在显式分数匹配中使用的<p><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>。这样做的原因是<p><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>可以被视为对真实分布<p><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>的连续近似。由<p><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>定义的近似是无偏的，但不幸的是，由于它是数据集中所有图像的均匀分布，在数据集中不存在的地方是不连续的，其概率在其他地方都为零。当然，随着<p><mi>σ</mi></math>变大，<p><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>被视为对<p><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>的近似越来越不忠实，因此我们希望使用小的<p><mi>σ</mi></math>。

Vincent 在 2011 年首次提出了使用<p><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>作为参考的显式分数匹配：

*<math alttext="upper J left-parenthesis theta right-parenthesis equals double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x prime right-parenthesis Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal nabla Subscript x prime Baseline log p Subscript theta Baseline left-parenthesis x prime right-parenthesis minus normal nabla Subscript x prime Baseline log p Subscript sigma Baseline left-parenthesis x prime right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><mi>J</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>*

*<math alttext="theta Superscript asterisk Baseline equals argmin Subscript theta Baseline upper J left-parenthesis theta right-parenthesis"><mrow><msup><mi>θ</mi> <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <mi>J</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>*

请注意，为什么这是一个有效的优化过程对于<math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>的推理与前一节中的相同——唯一的区别在于我们试图匹配的参考分布。Vincent 2011 实际上走了一步额外，并展示了这个优化过程等效于：

<math alttext="upper J Subscript DSM Baseline left-parenthesis theta right-parenthesis equals double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal nabla Subscript x prime Baseline log p Subscript theta Baseline left-parenthesis x prime right-parenthesis minus normal nabla Subscript x prime Baseline log p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><msub><mi>J</mi> <mtext>DSM</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>

<math alttext="theta Subscript DSM Superscript asterisk Baseline equals argmin Subscript theta Baseline upper J Subscript DSM Baseline left-parenthesis theta right-parenthesis"><mrow><msubsup><mi>θ</mi> <mtext>DSM</mtext> <mo>*</mo></msubsup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <msub><mi>J</mi> <mtext>DSM</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>

虽然我们不会在这里展示证明，并将您转至 Vincent 2011 以获取完整细节，但它确实利用了我们在 “实现 VAE” 中描述的对数技巧。我们将优化这个目标称为*去噪得分匹配*，简称为*DSM*，正如我们将很快展示的那样，它作为与去噪自编码器的连接。

我们知道<math alttext="p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis equals upper N left-parenthesis x prime semicolon x comma sigma squared upper I right-parenthesis"><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>N</mi> <mrow><mo>(</mo> <mi>x</mi> <mi>â</mi> <mi></mi> <mi></mi> <mo>;</mo> <mi>x</mi> <mo>,</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi> <mo>)</mo></mrow></mrow></math>，现在计算其对数的梯度：

<math alttext="normal nabla Subscript x prime Baseline log p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis equals normal nabla Subscript x prime Baseline log left-parenthesis StartFraction 1 Over StartRoot left-parenthesis 2 pi right-parenthesis Superscript d Baseline StartAbsoluteValue sigma squared upper I EndAbsoluteValue EndRoot EndFraction e Superscript StartFraction minus left-parenthesis x prime minus x right-parenthesis Super Superscript upper T Superscript left-parenthesis x prime minus x right-parenthesis Over 2 sigma squared EndFraction Baseline right-parenthesis"><mrow><msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn> <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow> <mi>d</mi></msup> <mrow><mo>|</mo><msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi><mo>|</mo></mrow></mrow></msqrt></mfrac> <msup><mi>e</mi> <mfrac><mrow><mo>-</mo><msup><mrow><mo>(</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mi>T</mi></msup> <mrow><mo>(</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac></msup> <mo>)</mo></mrow></mrow></math>

<math alttext="equals normal nabla Subscript x prime Baseline log StartFraction 1 Over StartRoot left-parenthesis 2 pi right-parenthesis Superscript d Baseline StartAbsoluteValue sigma squared upper I EndAbsoluteValue EndRoot EndFraction plus normal nabla Subscript x prime Baseline log e Superscript StartFraction minus left-parenthesis x prime minus x right-parenthesis Super Superscript upper T Superscript left-parenthesis x prime minus x right-parenthesis Over 2 sigma squared EndFraction"><mrow><mo>=</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <mfrac><mn>1</mn> <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow> <mi>d</mi></msup> <mrow><mo>|</mo><msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi><mo>|</mo></mrow></mrow></msqrt></mfrac> <mo>+</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msup><mi>e</mi> <mfrac><mrow><mo>-</mo><msup><mrow><mo>(</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mi>T</mi></msup> <mrow><mo>(</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac></msup></mrow></math>

<math alttext="equals minus StartFraction 1 Over 2 sigma squared EndFraction normal nabla Subscript x prime Baseline left-parenthesis x prime minus x right-parenthesis Superscript upper T Baseline left-parenthesis x prime minus x right-parenthesis"><mrow><mo>=</mo> <mo>-</mo> <mfrac><mn>1</mn> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <msup><mrow><mo>(</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mi>T</mi></msup> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>

<math alttext="equals minus StartFraction 1 Over 2 sigma squared EndFraction left-parenthesis normal nabla Subscript x prime Baseline x Superscript prime upper T Baseline x prime minus 2 normal nabla Subscript x prime Baseline x Superscript prime upper T Baseline x plus normal nabla Subscript x prime Baseline x Superscript upper T Baseline x right-parenthesis"><mrow><mo>=</mo> <mo>-</mo> <mfrac><mn>1</mn> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac> <mrow><mo>(</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup> <msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo> <mn>2</mn> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup> <mi>x</mi> <mo>+</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <msup><mi>x</mi> <mi>T</mi></msup> <mi>x</mi> <mo>)</mo></mrow></mrow></math>

<math alttext="equals StartFraction 1 Over sigma squared EndFraction left-parenthesis x minus x prime right-parenthesis"><mrow><mo>=</mo> <mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo> <mi>x</mi> <mo>-</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>

让我们来分解这个数学问题。第一个等式只是高斯分布的定义，均值为*x*，方差为<math alttext="sigma squared upper I"><mrow><msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi></mrow></math>。第二个等式是对数将乘积分解为对数和的结果，以及和的梯度为梯度之和。在第三个等式中，我们看到第一项已被移除，因为它不是*x’*的函数，因此其梯度为零。此外，*e*的对数提升到任意幂仅仅是幂本身，因为这里使用的对数具有基数*e*。最后，我们展开*x’ – x*与自身的点积，并将梯度应用于结果和的每个单独项。请注意，我们可以简单地将<math alttext="minus x Superscript prime upper T Baseline x minus x Superscript upper T Baseline x prime"><mrow><mo>-</mo> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup> <mi>x</mi> <mo>-</mo> <msup><mi>x</mi> <mi>T</mi></msup> <msup><mi>x</mi> <mo>'</mo></msup></mrow></math>重写为<math alttext="minus 2 x Superscript prime upper T Baseline x"><mrow><mo>-</mo> <mn>2</mn> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup> <mi>x</mi></mrow></math>，因为这两项是彼此的转置，并且结果是相同的标量。我们建议您阅读 KB Petersen 和 Michael Syskind Pedersen 撰写的一本名为*The Matrix Cookbook*的精彩文本，它可以作为评估这些梯度（以及更多内容）并得出最终等式的指南。对于<math alttext="x Superscript prime upper T Baseline x prime"><mrow><msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup> <msup><mi>x</mi> <mo>'</mo></msup></mrow></math>的梯度的直觉是，它类似于单变量微积分中变量平方的导数。

最后一步，我们将展示，优化去噪得分匹配的目标等效于优化去噪自编码器的目标。回顾一下，去噪自编码器的架构与标准自编码器相同——唯一的区别在于输入数据和训练目标。去噪自编码器的训练目标如下：

<math alttext="upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis equals double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket StartAbsoluteValue EndAbsoluteValue decode left-parenthesis encode left-parenthesis x Superscript prime Baseline right-parenthesis right-parenthesis minus x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><msub><mi>J</mi> <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo>|</mo> <mo>|</mo> <mtext>decode</mtext></mrow> <mrow><mo>(</mo> <mtext>encode</mtext> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <msubsup><mrow><mi>x</mi><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>

<math alttext="theta Subscript DAE Superscript asterisk Baseline equals argmin Subscript theta Baseline upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis"><mrow><msubsup><mi>θ</mi> <mtext>DAE</mtext> <mo>*</mo></msubsup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <msub><mi>J</mi> <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>

请注意，decode() 和 encode() 的参数或权重都由 <math alttext="theta"><mi>θ</mi></math> 包含。总结一下，我们必须证明之前定义的 <math alttext="theta Subscript DAE Superscript asterisk"><msubsup><mi>θ</mi> <mtext>DAE</mtext> <mo>*</mo></msubsup></math> 和 <math alttext="theta Subscript DSM Superscript asterisk"><msubsup><mi>θ</mi> <mtext>DSM</mtext> <mo>*</mo></msubsup></math> 对于某种未归一化似然形式是等价的。再次，按照 Vincent 2011 的方法，我们将去噪自动编码器定义为一个编码器，由一个全连接层和一个 Sigmoid 层组成，以及一个解码器，仅由一个全连接层组成。此外，我们添加了两个全连接层是权重绑定的约束，使它们互为转置。现在可以将训练目标指定为，其中 <math alttext="theta equals left-parenthesis upper W comma b comma c right-parenthesis"><mrow><mi>θ</mi> <mo>=</mo> <mo>(</mo> <mi>W</mi> <mo>,</mo> <mi>b</mi> <mo>,</mo> <mi>c</mi> <mo>)</mo></mrow></math>：

<math alttext="upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis equals double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket StartAbsoluteValue EndAbsoluteValue upper W Superscript upper T Baseline left-parenthesis upper W x prime plus b right-parenthesis plus c minus x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><msub><mi>J</mi> <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo>|</mo> <mo>|</mo></mrow> <msup><mi>W</mi> <mi>T</mi></msup> <mrow><mo>(</mo> <mi>W</mi> <msup><mi>x</mi> <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mo>-</mo> <msubsup><mrow><mi>x</mi><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>

<math alttext="equals 2 sigma Superscript 4 Baseline asterisk double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket StartFraction 1 Over 2 sigma Superscript 4 Baseline EndFraction StartAbsoluteValue EndAbsoluteValue upper W Superscript upper T Baseline left-parenthesis upper W x prime plus b right-parenthesis plus c minus x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><mo>=</mo> <mn>2</mn> <msup><mi>σ</mi> <mn>4</mn></msup> <mo>*</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>4</mn></msup></mrow></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msup><mi>W</mi> <mi>T</mi></msup> <mrow><mo>(</mo> <mi>W</mi> <msup><mi>x</mi> <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mo>-</mo> <msubsup><mrow><mi>x</mi><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>

<math alttext="equals 2 sigma Superscript 4 Baseline asterisk double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue StartFraction 1 Over sigma squared EndFraction left-parenthesis upper W Superscript upper T Baseline left-parenthesis upper W x prime plus b right-parenthesis plus c minus x prime right-parenthesis minus StartFraction 1 Over sigma squared EndFraction left-parenthesis x minus x prime right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><mo>=</mo> <mn>2</mn> <msup><mi>σ</mi> <mn>4</mn></msup> <mo>*</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>T</mi></msup> <mrow><mo>(</mo> <mi>W</mi> <msup><mi>x</mi> <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mo>-</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>-</mo> <mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo> <mi>x</mi> <mo>-</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>

您可能会注意到，我们的代数操作导致了 <math alttext="normal nabla Subscript x prime Baseline log p Subscript sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis"><mrow><msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 的出现。现在我们只需要找到一个未归一化似然的形式，其相对于 *x’* 的梯度为 <math alttext="StartFraction 1 Over sigma squared EndFraction left-parenthesis upper W Superscript upper T Baseline left-parenthesis upper W x prime plus b right-parenthesis plus c minus x prime right-parenthesis"><mrow><mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>T</mi></sup> <mrow><mo>(</mo> <mi>W</mi> <msup><mi>x</mi> <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mo>-</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>。

事实证明，如果我们定义未归一化的似然<math alttext="q Subscript theta Baseline left-parenthesis x prime right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math> 为<math alttext="minus StartFraction 1 Over sigma squared EndFraction left-parenthesis c Superscript upper T Baseline x minus one-half StartAbsoluteValue EndAbsoluteValue x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline plus sigma-summation Underscript j equals 1 Overscript d Endscripts softplus left-parenthesis upper W Subscript j Superscript upper T Baseline x plus b Subscript j Baseline right-parenthesis right-parenthesis"><mrow><mo>-</mo> <mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo></mrow> <msup><mi>c</mi> <mi>T</mi></msup> <mi>x</mi> <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <msubsup><mrow><mo>|</mo><mo>|</mo><mi>x</mi><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>d</mi></msubsup> <mtext>softplus</mtext> <mrow><mo>(</mo> <msubsup><mi>W</mi> <mi>j</mi> <mi>T</mi></msubsup> <mi>x</mi> <mo>+</mo> <msub><mi>b</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mrow><mo>)</mo></mrow></mrow></math> 并将此表达式插入去噪得分匹配目标，我们得到的目标只是<math alttext="StartFraction 1 Over 2 sigma Superscript 4 Baseline EndFraction upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis"><mrow><mfrac><mn>1</mn> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>4</mn></msup></mrow></mfrac> <msub><mi>J</mi> <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math> 。我们建议您参考 Vincent 2011 年的原因。

优化这个新目标关于<math alttext="theta"><mi>θ</mi></math> 的情况与优化去噪自编码器没有区别。这是因为<math alttext="sigma"><mi>σ</mi></math> 是一个正常数，不依赖于<math alttext="theta"><mi>θ</mi></math>，因此只是缩放梯度的幅度而不影响其方向。总之，我们发现训练去噪自编码器与优化去噪得分匹配目标是相同的，其中未归一化的似然采用前一段指定的形式。更简单地说，训练好的去噪自编码器的权重将与通过去噪得分匹配指定的未归一化似然的权重相同。

要使用去噪自编码器执行生成建模，我们需要做的是：

1.  通过最小化<math alttext="upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis"><mrow><msub><mi>J</mi> <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math> 来完全训练去噪自编码器。

1.  对于给定的<math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>，通过评估<math alttext="StartFraction 1 Over sigma squared EndFraction left-parenthesis decode left-parenthesis encode left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis right-parenthesis minus x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo> <mtext>decode</mtext> <mrow><mo>(</mo> <mtext>encode</mtext> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>来计算其分数。

1.  从*N(0,I)*中抽取样本<math alttext="epsilon"><mi>ϵ</mi></math>。

1.  将 2 和 3 的结果代入 Langevin 动力学方程中，得到下一个样本<math alttext="x Superscript left-parenthesis i plus 1 right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>。

1.  重复步骤 2 到 4，使用<math alttext="x Superscript left-parenthesis i plus 1 right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>。

尽管我们通过使用这种方法解决了需要计算二阶梯度的问题，但仍然存在只能从*p(x)*的嘈杂近似中进行采样的问题。最近的工作借鉴了隐式分数匹配和去噪分数匹配的概念，以实现更强大和更逼真的生成能力。我们强烈建议您进一步探索文献，因为这些部分已经涵盖了大部分先决条件材料。

# 总结

总之，我们已经学到了很多关于生成模型的知识。我们涵盖了 GANs、VAEs 以及一些形式的分数匹配背后的动机和数学，甚至从头开始实现了一个 VAE。我们还了解了这些方法之间的相似之处和不同之处。例如，GAN 隐式地建模了一个复杂的分布，我们可以通过其生成器进行采样，而 VAE 明确地学习分布，但在其可以建模的分布复杂性方面略微受限。隐式分数匹配与 GAN 类似，允许我们通过 Langevin 动力学从复杂的分布中进行采样（而无需使用额外的噪声分布*p(z)），但是必须计算二阶梯度导致我们开发了去噪分数匹配以及其与现有去噪 AE 的联系。此外，VAE 通过定义一组潜在变量并明确学习近似后验（给定输入示例）和似然函数（给定潜在变量设置）而采取了三种方法中最强大的概率建模方法。相比之下，对于 GANs，附加变量*z*的目的仅仅是作为采样的中间变量。尽管所有这些模型从不同的角度和动机来解决生成建模问题，但它们都取得了强大的结果，并为当前和未来的研究奠定了坚实的基础。

^(1) Goodfellow 等人。“生成对抗网络。”*arXiv 预印本 arXiv*:1406.2661。2014 年。

^(2) Kingma 等人。“自动编码变分贝叶斯。”*arXiv 预印本 arXiv*:1312.6114。2014 年。
