["```py\npath = untar_data(URLs.IMAGENETTE_160)\n```", "```py\nt = get_image_files(path)\nt[0]\n```", "```py\nPath('/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JP\n > EG')\n```", "```py\nfrom glob import glob\nfiles = L(glob(f'{path}/**/*.JPEG', recursive=True)).map(Path)\nfiles[0]\n```", "```py\nPath('/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JP\n > EG')\n```", "```py\nim = Image.open(files[0])\nim\n```", "```py\nim_t = tensor(im)\nim_t.shape\n```", "```py\ntorch.Size([160, 213, 3])\n```", "```py\nlbls = files.map(Self.parent.name()).unique(); lbls\n```", "```py\n(#10) ['n03417042','n03445777','n03888257','n03394916','n02979186','n03000684','\n > n03425413','n01440764','n03028079','n02102040']\n```", "```py\nv2i = lbls.val2idx(); v2i\n```", "```py\n{'n03417042': 0,\n 'n03445777': 1,\n 'n03888257': 2,\n 'n03394916': 3,\n 'n02979186': 4,\n 'n03000684': 5,\n 'n03425413': 6,\n 'n01440764': 7,\n 'n03028079': 8,\n 'n02102040': 9}\n```", "```py\nclass Dataset:\n    def __init__(self, fns): self.fns=fns\n    def __len__(self): return len(self.fns)\n    def __getitem__(self, i):\n        im = Image.open(self.fns[i]).resize((64,64)).convert('RGB')\n        y = v2i[self.fns[i].parent.name]\n        return tensor(im).float()/255, tensor(y)\n```", "```py\ntrain_filt = L(o.parent.parent.name=='train' for o in files)\ntrain,valid = files[train_filt],files[~train_filt]\nlen(train),len(valid)\n```", "```py\n(9469, 3925)\n```", "```py\ntrain_ds,valid_ds = Dataset(train),Dataset(valid)\nx,y = train_ds[0]\nx.shape,y\n```", "```py\n(torch.Size([64, 64, 3]), tensor(0))\n```", "```py\nshow_image(x, title=lbls[y]);\n```", "```py\ndef collate(idxs, ds):\n    xb,yb = zip(*[ds[i] for i in idxs])\n    return torch.stack(xb),torch.stack(yb)\n```", "```py\nx,y = collate([1,2], train_ds)\nx.shape,y\n```", "```py\n(torch.Size([2, 64, 64, 3]), tensor([0, 0]))\n```", "```py\nclass DataLoader:\n    def __init__(self, ds, bs=128, shuffle=False, n_workers=1):\n        self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers\n\n    def __len__(self): return (len(self.ds)-1)//self.bs+1\n\n    def __iter__(self):\n        idxs = L.range(self.ds)\n        if self.shuffle: idxs = idxs.shuffle()\n        chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)]\n        with ProcessPoolExecutor(self.n_workers) as ex:\n            yield from ex.map(collate, chunks, ds=self.ds)\n```", "```py\nn_workers = min(16, defaults.cpus)\ntrain_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers)\nvalid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers)\nxb,yb = first(train_dl)\nxb.shape,yb.shape,len(train_dl)\n```", "```py\n(torch.Size([128, 64, 64, 3]), torch.Size([128]), 74)\n```", "```py\nstats = [xb.mean((0,1,2)),xb.std((0,1,2))]\nstats\n```", "```py\n[tensor([0.4544, 0.4453, 0.4141]), tensor([0.2812, 0.2766, 0.2981])]\n```", "```py\nclass Normalize:\n    def __init__(self, stats): self.stats=stats\n    def __call__(self, x):\n        if x.device != self.stats[0].device:\n            self.stats = to_device(self.stats, x.device)\n        return (x-self.stats[0])/self.stats[1]\n```", "```py\nnorm = Normalize(stats)\ndef tfm_x(x): return norm(x).permute((0,3,1,2))\n```", "```py\nt = tfm_x(x)\nt.mean((0,2,3)),t.std((0,2,3))\n```", "```py\n(tensor([0.3732, 0.4907, 0.5633]), tensor([1.0212, 1.0311, 1.0131]))\n```", "```py\nclass Parameter(Tensor):\n    def __new__(self, x): return Tensor._make_subclass(Parameter, x, True)\n    def __init__(self, *args, **kwargs): self.requires_grad_()\n```", "```py\nParameter(tensor(3.))\n```", "```py\ntensor(3., requires_grad=True)\n```", "```py\nclass Module:\n    def __init__(self):\n        self.hook,self.params,self.children,self._training = None,[],[],False\n\n    def register_parameters(self, *ps): self.params += ps\n    def register_modules   (self, *ms): self.children += ms\n\n    @property\n    def training(self): return self._training\n    @training.setter\n    def training(self,v):\n        self._training = v\n        for m in self.children: m.training=v\n\n    def parameters(self):\n        return self.params + sum([m.parameters() for m in self.children], [])\n\n    def __setattr__(self,k,v):\n        super().__setattr__(k,v)\n        if isinstance(v,Parameter): self.register_parameters(v)\n        if isinstance(v,Module):    self.register_modules(v)\n\n    def __call__(self, *args, **kwargs):\n        res = self.forward(*args, **kwargs)\n        if self.hook is not None: self.hook(res, args)\n        return res\n\n    def cuda(self):\n        for p in self.parameters(): p.data = p.data.cuda()\n```", "```py\nself.params + sum([m.parameters() for m in self.children], [])\n```", "```py\nif isinstance(v,Parameter): self.register_parameters(v)\n```", "```py\nclass ConvLayer(Module):\n    def __init__(self, ni, nf, stride=1, bias=True, act=True):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni,3,3))\n        self.b = Parameter(torch.zeros(nf)) if bias else None\n        self.act,self.stride = act,stride\n        init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_\n        init(self.w)\n\n    def forward(self, x):\n        x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1)\n        if self.act: x = F.relu(x)\n        return x\n```", "```py\nl = ConvLayer(3, 4)\nlen(l.parameters())\n```", "```py\n2\n```", "```py\nxbt = tfm_x(xb)\nr = l(xbt)\nr.shape\n```", "```py\ntorch.Size([128, 4, 64, 64])\n```", "```py\nclass Linear(Module):\n    def __init__(self, ni, nf):\n        super().__init__()\n        self.w = Parameter(torch.zeros(nf,ni))\n        self.b = Parameter(torch.zeros(nf))\n        nn.init.xavier_normal_(self.w)\n\n    def forward(self, x): return x@self.w.t() + self.b\n```", "```py\nl = Linear(4,2)\nr = l(torch.ones(3,4))\nr.shape\n```", "```py\ntorch.Size([3, 2])\n```", "```py\nclass T(Module):\n    def __init__(self):\n        super().__init__()\n        self.c,self.l = ConvLayer(3,4),Linear(4,2)\n```", "```py\nt = T()\nlen(t.parameters())\n```", "```py\n4\n```", "```py\nt.cuda()\nt.l.w.device\n```", "```py\ndevice(type='cuda', index=5)\n```", "```py\nclass Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = layers\n        self.register_modules(*layers)\n\n    def forward(self, x):\n        for l in self.layers: x = l(x)\n        return x\n```", "```py\nclass AdaptivePool(Module):\n    def forward(self, x): return x.mean((2,3))\n```", "```py\ndef simple_cnn():\n    return Sequential(\n        ConvLayer(3 ,16 ,stride=2), #32\n        ConvLayer(16,32 ,stride=2), #16\n        ConvLayer(32,64 ,stride=2), # 8\n        ConvLayer(64,128,stride=2), # 4\n        AdaptivePool(),\n        Linear(128, 10)\n    )\n```", "```py\nm = simple_cnn()\nlen(m.parameters())\n```", "```py\n10\n```", "```py\ndef print_stats(outp, inp): print (outp.mean().item(),outp.std().item())\nfor i in range(4): m.layers[i].hook = print_stats\n\nr = m(xbt)\nr.shape\n```", "```py\n0.5239089727401733 0.8776043057441711\n0.43470510840415955 0.8347987532615662\n0.4357188045978546 0.7621666193008423\n0.46562111377716064 0.7416611313819885\ntorch.Size([128, 10])\n```", "```py\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n```", "```py\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\nsm = log_softmax(r); sm[0][0]\n```", "```py\ntensor(-1.2790, grad_fn=<SelectBackward>)\n```", "```py\nloss = nll(sm, yb)\nloss\n```", "```py\ntensor(2.5666, grad_fn=<NegBackward>)\n```", "```py\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nsm = log_softmax(r); sm[0][0]\n```", "```py\ntensor(-1.2790, grad_fn=<SelectBackward>)\n```", "```py\nx = torch.rand(5)\na = x.max()\nx.exp().sum().log() == a + (x-a).exp().sum().log()\n```", "```py\ntensor(True)\n```", "```py\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\nlogsumexp(r)[0]\n```", "```py\ntensor(3.9784, grad_fn=<SelectBackward>)\n```", "```py\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n```", "```py\nsm = log_softmax(r); sm[0][0]\n```", "```py\ntensor(-1.2790, grad_fn=<SelectBackward>)\n```", "```py\ndef cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean()\n```", "```py\nclass SGD:\n    def __init__(self, params, lr, wd=0.): store_attr(self, 'params,lr,wd')\n    def step(self):\n        for p in self.params:\n            p.data -= (p.grad.data + p.data*self.wd) * self.lr\n            p.grad.data.zero_()\n```", "```py\nclass DataLoaders:\n    def __init__(self, *dls): self.train,self.valid = dls\n\ndls = DataLoaders(train_dl,valid_dl)\n```", "```py\nclass Learner:\n    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD):\n        store_attr(self, 'model,dls,loss_func,lr,cbs,opt_func')\n        for cb in cbs: cb.learner = self\n```", "```py\n    def one_batch(self):\n        self('before_batch')\n        xb,yb = self.batch\n        self.preds = self.model(xb)\n        self.loss = self.loss_func(self.preds, yb)\n        if self.model.training:\n            self.loss.backward()\n            self.opt.step()\n        self('after_batch')\n\n    def one_epoch(self, train):\n        self.model.training = train\n        self('before_epoch')\n        dl = self.dls.train if train else self.dls.valid\n        for self.num,self.batch in enumerate(progress_bar(dl, leave=False)):\n            self.one_batch()\n        self('after_epoch')\n\n    def fit(self, n_epochs):\n        self('before_fit')\n        self.opt = self.opt_func(self.model.parameters(), self.lr)\n        self.n_epochs = n_epochs\n        try:\n            for self.epoch in range(n_epochs):\n                self.one_epoch(True)\n                self.one_epoch(False)\n        except CancelFitException: pass\n        self('after_fit')\n\n    def __call__(self,name):\n        for cb in self.cbs: getattr(cb,name,noop)()\n```", "```py\nfor self.epoch in range(n_epochs)\n```", "```py\nfor cb in cbs: cb.learner = self\n```", "```py\nclass Callback(GetAttr): _default='learner'\n```", "```py\nclass SetupLearnerCB(Callback):\n    def before_batch(self):\n        xb,yb = to_device(self.batch)\n        self.learner.batch = tfm_x(xb),yb\n\n    def before_fit(self): self.model.cuda()\n```", "```py\nclass TrackResults(Callback):\n    def before_epoch(self): self.accs,self.losses,self.ns = [],[],[]\n\n    def after_epoch(self):\n        n = sum(self.ns)\n        print(self.epoch, self.model.training,\n              sum(self.losses).item()/n, sum(self.accs).item()/n)\n\n    def after_batch(self):\n        xb,yb = self.batch\n        acc = (self.preds.argmax(dim=1)==yb).float().sum()\n        self.accs.append(acc)\n        n = len(xb)\n        self.losses.append(self.loss*n)\n        self.ns.append(n)\n```", "```py\ncbs = [SetupLearnerCB(),TrackResults()]\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs)\nlearn.fit(1)\n```", "```py\n0 True 2.1275552130636814 0.2314922378287042\n\n0 False 1.9942575636942674 0.2991082802547771\n```", "```py\nclass LRFinder(Callback):\n    def before_fit(self):\n        self.losses,self.lrs = [],[]\n        self.learner.lr = 1e-6\n\n    def before_batch(self):\n        if not self.model.training: return\n        self.opt.lr *= 1.2\n\n    def after_batch(self):\n        if not self.model.training: return\n        if self.opt.lr>10 or torch.isnan(self.loss): raise CancelFitException\n        self.losses.append(self.loss.item())\n        self.lrs.append(self.opt.lr)\n```", "```py\nlrfind = LRFinder()\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind])\nlearn.fit(2)\n```", "```py\n0 True 2.6336045582954903 0.11014890695955222\n\n0 False 2.230653363853503 0.18318471337579617\n```", "```py\nplt.plot(lrfind.lrs[:-2],lrfind.losses[:-2])\nplt.xscale('log')\n```", "```py\nclass OneCycle(Callback):\n    def __init__(self, base_lr): self.base_lr = base_lr\n    def before_fit(self): self.lrs = []\n\n    def before_batch(self):\n        if not self.model.training: return\n        n = len(self.dls.train)\n        bn = self.epoch*n + self.num\n        mn = self.n_epochs*n\n        pct = bn/mn\n        pct_start,div_start = 0.25,10\n        if pct<pct_start:\n            pct /= pct_start\n            lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr\n        else:\n            pct = (pct-pct_start)/(1-pct_start)\n            lr = (1-pct)*self.base_lr\n        self.opt.lr = lr\n        self.lrs.append(lr)\n```", "```py\nonecyc = OneCycle(0.1)\nlearn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc])\n```", "```py\nlearn.fit(8)\n```", "```py\nplt.plot(onecyc.lrs);\n```"]