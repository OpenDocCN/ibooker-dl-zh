["```py\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, \n                 hidden_dim=24, lstm_layers=1):\n        super(TextClassificationModel, self).__init__()\n\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=lstm_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n\n        # Activation functions\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x shape: (batch_size, sequence_length)\n\n        # Get embeddings\n        embedded = self.embedding(x)  \n        # Shape: (batch_size, sequence_length, embedding_dim)\n\n        # Pass through LSTM\n        lstm_out, _ = self.lstm(embedded)  \n        # Shape: (batch_size, sequence_length, hidden_dim)\n\n        # Transpose for global pooling \n        # (expecting: batch, channels, sequence_length)\n        lstm_out = lstm_out.transpose(1, 2)  \n        # Shape: (batch_size, hidden_dim, sequence_length)\n\n        # Apply global pooling\n        pooled = self.global_pool(lstm_out)  \n        # Shape: (batch_size, hidden_dim, 1)\n        pooled = pooled.squeeze(–1)  # Shape: (batch_size, hidden_dim)\n\n        # Pass through fully connected layers\n        x = self.relu(self.fc1(pooled))\n        x = self.sigmoid(self.fc2(x))\n\n        return x\n```", "```py\n# Define loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, \n                       betas=(0.9, 0.999), amsgrad=False)\n```", "```py\n==========================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================\nTextClassificationModel                  [32, 1]                   --\n├─Embedding: 1-1                         [32, 85, 7]               14,000\n├─LSTM: 1-2                              [32, 85, 48]              6,336\n├─AdaptiveAvgPool1d: 1-3                 [32, 48, 1]               --\n├─Linear: 1-4                            [32, 24]                  1,176\n├─ReLU: 1-5                              [32, 24]                  --\n├─Linear: 1-6                            [32, 1]                   25\n├─Sigmoid: 1-7                           [32, 1]                   --\n==========================================================================\nTotal params: 21,537\nTrainable params: 21,537\nNon-trainable params: 0\nTotal mult-adds (M): 17.72\n==========================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.20\nParams size (MB): 0.09\nEstimated Total Size (MB): 1.31\n==========================================================================\n```", "```py\n# First LSTM layer\nself.lstm1 = nn.LSTM(\n    input_size=embedding_dim,\n    hidden_size=hidden_dim,\n    num_layers=lstm_layers,\n    batch_first=True,\n    bidirectional=True\n)\n\n# Second LSTM layer\n# Note: Input size is hidden_dim*2 because first LSTM is bidirectional.\nself.lstm2 = nn.LSTM(\n    input_size=hidden_dim * 2,\n    hidden_size=hidden_dim,\n    num_layers=lstm_layers,\n    batch_first=True,\n    bidirectional=True\n)\n\n```", "```py\n==========================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================\nTextClassificationModel                  [32, 1]                   --\n├─Embedding: 1-1                         [32, 85, 7]               14,000\n├─LSTM: 1-2                              [32, 85, 48]              6,336\n├─LSTM: 1-3                              [32, 85, 48]              14,208\n├─AdaptiveAvgPool1d: 1-4                 [32, 48, 1]               --\n├─Linear: 1-5                            [32, 24]                  1,176\n├─ReLU: 1-6                              [32, 24]                  --\n├─Linear: 1-7                            [32, 1]                   25\n├─Sigmoid: 1-8                           [32, 1]                   --\n==========================================================================\nTotal params: 35,745\nTrainable params: 35,745\nNon-trainable params: 0\nTotal mult-adds (M): 56.37\n==========================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 2.25\nParams size (MB): 0.14\nEstimated Total Size (MB): 2.41\n==========================================================================\n\n```", "```py\n# Define loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), `lr``=``0.00005`, \n                       betas=(0.9, 0.999), amsgrad=False)\n\n```", "```py\nself.embedding_dropout = nn.Dropout(dropout_rate)\nself.lstm_dropout = nn.Dropout(dropout_rate)\nself.final_dropout = nn.Dropout(dropout_rate)\n```", "```py\ndef forward(self, x):\n    # Get embeddings\n    embedded = self.embedding(x)  \n\n    # Apply first dropout after embedding layer\n    embedded = self.embedding_dropout(embedded)\n\n    lstm1_out, _ = self.lstm1(embedded)\n\n    # Apply dropout between LSTM layers\n    lstm1_out = self.lstm_dropout(lstm1_out)\n\n    lstm2_out, _ = self.lstm2(lstm1_out)\n\n    # Apply final dropout\n    lstm2_out = self.final_dropout(lstm2_out)\n\n    lstm_out = lstm2_out.transpose(1, 2)\n\n    pooled = self.global_pool(lstm_out)  \n    pooled = pooled.squeeze(–1) \n\n    x = self.relu(self.fc1(pooled))\n    x = self.sigmoid(self.fc2(x))\n\n    return x\n```", "```py\nimport urllib.request\nimport zipfile\n\n# Download GloVe embeddings\nurl = \"https://nlp.stanford.edu/data/glove.6B.zip\"\nurllib.request.urlretrieve(url, \"glove.6B.zip\")\n\n# Unzip\nwith zipfile.ZipFile(\"glove.6B.zip\", 'r') as zip_ref:\n    zip_ref.extractall()\n\n# You can use glove.6B.50d.txt (50 dimensions)\n# or glove.6B.100d.txt (100 dimensions)\n```", "```py\nimport numpy as np\nglove_embeddings = dict()\nf = open('glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    glove_embeddings[word] = coefs\nf.close()\n```", "```py\nglove_embeddings['frog']\n```", "```py\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=16, \n                 dropout_rate=0.25, pretrained_embeddings=None, \n                 freeze_embeddings=True, lstm_layers=2):\n        super(TextClassificationModel, self).__init__()\n\n        # Initialize embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # Load pretrained embeddings if provided\n        if pretrained_embeddings is not None:\n            self.embedding.weight.data.copy_(pretrained_embeddings)\n            if freeze_embeddings:\n                self.embedding.weight.requires_grad = False\n\n        # LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=lstm_layers,\n            batch_first=True\n        )\n\n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n\n        # Activation functions\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n```", "```py\n==========================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================\nTextClassificationModel                  [32, 1]                   --\n├─Embedding: 1-1                         [32, 60, 50]              (400,000)\n├─Dropout: 1-2                           [32, 60, 50]              --\n├─LSTM: 1-3                              [32, 60, 16]              6,528\n├─Dropout: 1-4                           [32, 60, 16]              --\n├─AdaptiveAvgPool1d: 1-5                 [32, 16, 1]               --\n├─Linear: 1-6                            [32, 16]                  272\n├─ReLU: 1-7                              [32, 16]                  --\n├─Dropout: 1-8                           [32, 16]                  --\n├─Linear: 1-9                            [32, 1]                   17\n├─Sigmoid: 1-10                          [32, 1]                   --\n==========================================================================\nTotal params: 406,817\nTrainable params: 6,817\nNon-trainable params: 400,000\nTotal mult-adds (M): 25.34\n==========================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.02\nParams size (MB): 1.63\nEstimated Total Size (MB): 2.66\n==========================================================================\n\n```", "```py\nword_index = build_vocab_glove(training_sentences, max_vocab_size=100,000)\n```", "```py\nembeddings_dict = {}\nembedding_dim = 50\nglove_file = f'glove.6B.{embedding_dim}d.txt'\n\n# Read GloVe embeddings\nprint(f\"Loading GloVe embeddings from {glove_file}...\")\nwith open(glove_file, 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embeddings_dict[word] = vector\n```", "```py\nfound_words = 0\nfor word, idx in word_index.items():\n    if word in embeddings_dict:\n        found_words += 1\nprint(found_words)\n```", "```py\ntest_sentences = [\"It Was, For, Uh, Medical Reasons, Says Doctor To \n                   `Boris` `Johnson``,` `Explaining` `Why` `They` `Had` `To` `Give` `Him` `Haircut``\",` ```", "```py `\"It's a beautiful sunny day\"``,`                   `\"I lived in Ireland, so in high school they made me` ```", "```py` `\"Census Foot Soldiers Swarm Neighborhoods, Kick Down` ```", "```py ```", "```py`` ```", "```py\n```", "```py```", "````` ```py` The results for these headlines are as follows—remember that values close to 50% (0.5) are considered neutral, those close to 0 are considered nonsarcastic, and those close to 1 are considered sarcastic:    ``` tensor([[0.9316],         [0.1603],         [0.6959],         [0.9594]], device='cuda:0')   Text: It Was, For, Uh, Medical Reasons, Says Doctor To Boris Johnson,        Explaining Why They Had To Give Him Haircut Probability: 0.9316 Classification: Sarcastic --------------------------------------------------------------------------   Text: It's a beautiful sunny day `Probability``:` `0.1603` `Classification``:` `Not` `Sarcastic` `--------------------------------------------------------------------------`   `Text``:` `I` `lived` `in` `Ireland``,` `so` `in` `high` `school` `they` `made` `me` `learn` `to` `speak`        `and` `write` `in` `Gaelic` `Probability``:` `0.6959` `Classification``:` `Sarcastic` `--------------------------------------------------------------------------`   `Text``:` `Census` `Foot` `Soldiers` `Swarm` `Neighborhoods``,` `Kick` `Down` `Doors` `To` `Tally`        `Household` `Sizes` `Probability``:` `0.9594` `Classification``:` `Sarcastic` `--------------------------------------------------------------------------` ```py   `The first and fourth sentences, which are taken from *The Onion*, showed 93%+ likelihood of sarcasm. The statement about the weather was strongly nonsarcastic (16%), and the sentence about going to high school in Ireland was deemed to be potentially sarcastic but not with high confidence (69%).` ```` ```py`` `````", "``` `` `# Summary    This chapter introduced you to recurrent neural networks, which use sequence-oriented logic in their design and can help you understand the sentiment in sentences based not only on the words they contain but also on the order in which they appear. You saw how a basic RNN works, as well as how an LSTM can build on this to enable context to be preserved over the long term. These models are the precursors to the popular and famous “transformers” models used to underpin generative AI.    You also used LSTMs to improve the sentiment analysis model you’ve been working on, and you then looked into overfitting issues with RNNs and techniques to improve them, including by using transfer learning from pretrained embeddings.    In [Chapter 8](ch08.html#ch08_using_ml_to_create_text_1748549671852453), you’ll use what you’ve learned so far to explore how to predict words, and from there, you’ll be able to create a model that creates text and writes poetry for you!` `` ```"]