- en: Chapter 7\. Question Answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 问答
- en: Whether you’re a researcher, analyst, or data scientist, chances are that at
    some point you’ve needed to wade through oceans of documents to find the information
    you’re looking for. To make matters worse, you’re constantly reminded by Google
    and Bing that there exist better ways to search! For instance, if we search for
    “When did Marie Curie win her first Nobel Prize?” on Google, we immediately get
    the correct answer of “1903,” as illustrated in [Figure 7-1](#marie-curie).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是研究人员、分析师还是数据科学家，都有可能在某个时候需要浏览大量文档以找到您正在寻找的信息。更糟糕的是，您不断地被谷歌和必应提醒，存在更好的搜索方式！例如，如果我们在谷歌上搜索“玛丽·居里何时获得她的第一个诺贝尔奖？”我们立即得到了“1903”这个正确答案，如[图7-1](#marie-curie)所示。
- en: '![Marie Curie](Images/nlpt_0701.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Marie Curie](Images/nlpt_0701.png)'
- en: Figure 7-1\. A Google search query and corresponding answer snippet
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1 谷歌搜索查询和相应的答案片段
- en: In this example, Google first retrieved around 319,000 documents that were relevant
    to the query, and then performed an additional processing step to extract the
    answer snippet with the corresponding passage and web page. It’s not hard to see
    why these answer snippets are useful. For example, if we search for a trickier
    question like “Which guitar tuning is the best?” Google doesn’t provide an answer,
    and instead we have to click on one of the web pages returned by the search engine
    to find it ourselves.^([1](ch07.xhtml#idm46238714372976))
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，谷歌首先检索了大约319,000个与查询相关的文档，然后进行了额外的处理步骤，提取了带有相应段落和网页的答案片段。很容易看出这些答案片段是有用的。例如，如果我们搜索一个更棘手的问题，比如“哪种吉他调音是最好的？”谷歌没有提供答案，而是我们必须点击搜索引擎返回的网页之一来找到答案。^([1](ch07.xhtml#idm46238714372976))
- en: 'The general approach behind this technology is called *question answering*
    (QA). There are many flavors of QA, but the most common is *extractive QA*, which
    involves questions whose answer can be identified as a span of text in a document,
    where the document might be a web page, legal contract, or news article. The two-stage
    process of first retrieving relevant documents and then extracting answers from
    them is also the basis for many modern QA systems, including semantic search engines,
    intelligent assistants, and automated information extractors. In this chapter,
    we’ll apply this process to tackle a common problem facing ecommerce websites:
    helping consumers answer specific queries to evaluate a product. We’ll see that
    customer reviews can be used as a rich and challenging source of information for
    QA, and along the way we’ll learn how transformers act as powerful *reading comprehension*
    models that can extract meaning from text. Let’s begin by fleshing out the use
    case.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术背后的一般方法被称为*问答*（QA）。有许多种类的QA，但最常见的是*抽取式QA*，它涉及到问题的答案可以在文档中识别为文本段落的一部分，文档可能是网页、法律合同或新闻文章。首先检索相关文档，然后从中提取答案的两阶段过程也是许多现代QA系统的基础，包括语义搜索引擎、智能助手和自动信息提取器。在本章中，我们将应用这一过程来解决电子商务网站面临的一个常见问题：帮助消费者回答特定的查询以评估产品。我们将看到客户评论可以作为QA的丰富而具有挑战性的信息来源，并且在这个过程中，我们将学习transformers如何作为强大的*阅读理解*模型，可以从文本中提取含义。让我们从详细说明用例开始。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: This chapter focuses on extractive QA, but other forms of QA may be more suitable
    for your use case. For example, *community QA* involves gathering question-answer
    pairs that are generated by users on forums like [Stack Overflow](https://stackoverflow.com),
    and then using semantic similarity search to find the closest matching answer
    to a new question. There is also *long-form QA*, which aims to generate complex
    paragraph-length answers to open-ended questions like “Why is the sky blue?” Remarkably,
    it is also possible to do QA over tables, and transformer models like [TAPAS](https://oreil.ly/vVPWO)
    can even perform aggregations to produce the final answer!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍抽取式QA，但其他形式的QA可能更适合您的用例。例如，*社区QA*涉及收集用户在论坛上生成的问题-答案对，然后使用语义相似性搜索找到与新问题最接近的答案。还有*长篇QA*，旨在对开放性问题生成复杂的段落长度答案，比如“天空为什么是蓝色？”值得注意的是，还可以对表格进行QA，transformer模型如[TAPAS](https://oreil.ly/vVPWO)甚至可以执行聚合以生成最终答案！
- en: Building a Review-Based QA System
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基于评论的问答系统
- en: If you’ve ever purchased a product online, you probably relied on customer reviews
    to help inform your decision. These reviews can often help answer specific questions
    like “Does this guitar come with a strap?” or “Can I use this camera at night?”
    that may be hard to answer from the product description alone. However, popular
    products can have hundreds to thousands of reviews, so it can be a major drag
    to find one that is relevant. One alternative is to post your question on the
    community QA platforms provided by websites like Amazon, but it usually takes
    days to get an answer (if you get one at all). Wouldn’t it be nice if we could
    get an immediate answer, like in the Google example from [Figure 7-1](#marie-curie)?
    Let’s see if we can do this using transformers!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您曾经在网上购买过产品，您可能会依赖客户评论来帮助您做出决定。这些评论通常可以帮助回答特定问题，比如“这把吉他带吗？”或者“我可以在晚上使用这个相机吗？”这些问题可能很难仅通过产品描述就能回答。然而，热门产品可能会有数百甚至数千条评论，因此找到相关的评论可能会很麻烦。一个替代方法是在像亚马逊这样的网站提供的社区问答平台上发布您的问题，但通常需要几天时间才能得到答案（如果有的话）。如果我们能像[图7-1](#marie-curie)中的谷歌示例那样立即得到答案，那不是挺好的吗？让我们看看是否可以使用transformers来实现这一点！
- en: The Dataset
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'To build our QA system we’ll use the SubjQA dataset,^([2](ch07.xhtml#idm46238714344800))
    which consists of more than 10,000 customer reviews in English about products
    and services in six domains: TripAdvisor, Restaurants, Movies, Books, Electronics,
    and Grocery. As illustrated in [Figure 7-2](#phone), each review is associated
    with a question that can be answered using one or more sentences from the review.^([3](ch07.xhtml#idm46238714342272))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的QA系统，我们将使用SubjQA数据集^([2](ch07.xhtml#idm46238714344800))，该数据集包括英语中关于六个领域的产品和服务的10,000多条客户评论：TripAdvisor、餐馆、电影、书籍、电子产品和杂货。如[图7-2](#phone)所示，每条评论都与一个问题相关联，可以使用评论中的一句或多句来回答。^([3](ch07.xhtml#idm46238714342272))
- en: '![Phone with Query](Images/nlpt_0702.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![带有查询的手机](Images/nlpt_0702.png)'
- en: Figure 7-2\. A question about a product and the corresponding review (the answer
    span is underlined)
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2. 有关产品的问题和相应的评论（答案范围已划线）
- en: The interesting aspect of this dataset is that most of the questions and answers
    are *subjective*; that is, they depend on the personal experience of the users.
    The example in [Figure 7-2](#phone) shows why this feature makes the task potentially
    more difficult than finding answers to factual questions like “What is the currency
    of the United Kingdom?” First, the query is about “poor quality,” which is subjective
    and depends on the user’s definition of quality. Second, important parts of the
    query do not appear in the review at all, which means it cannot be answered with
    shortcuts like keyword search or paraphrasing the input question. These features
    make SubjQA a realistic dataset to benchmark our review-based QA models on, since
    user-generated content like that shown in [Figure 7-2](#phone) resembles what
    we might encounter in the wild.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集的有趣之处在于，大多数问题和答案都是*主观*的；也就是说，它们取决于用户的个人经验。[图7-2](#phone)中的示例显示了为什么这一特点使得这个任务可能比寻找对像“英国的货币是什么？”这样的事实问题的答案更困难。首先，查询是关于“质量差”，这是主观的，取决于用户对质量的定义。其次，查询的重要部分根本不出现在评论中，这意味着不能用关键词搜索或释义输入问题来回答。这些特点使得SubjQA成为一个真实的数据集，可以用来对我们基于评论的QA模型进行基准测试，因为像[图7-2](#phone)中显示的用户生成内容类似于我们可能在野外遇到的内容。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: QA systems are usually categorized by the *domain* of data that they have access
    to when responding to a query. *Closed-domain* QA deals with questions about a
    narrow topic (e.g., a single product category), while *open-domain* QA deals with
    questions about almost anything (e.g., Amazon’s whole product catalog). In general,
    closed-domain QA involves searching through fewer documents than the open-domain
    case.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: QA系统通常根据其在响应查询时可以访问的数据的*领域*进行分类。*封闭领域*QA处理关于狭窄主题的问题（例如单个产品类别），而*开放领域*QA处理几乎任何问题（例如亚马逊的整个产品目录）。一般来说，封闭领域QA涉及搜索的文档比开放领域情况少。
- en: 'To get started, let’s download the dataset from the [Hugging Face Hub](https://oreil.ly/iO0s5).
    As we did in [Chapter 4](ch04.xhtml#chapter_ner), we can use the `get_dataset_config_names()`
    function to find out which subsets are available:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，让我们从[Hugging Face Hub](https://oreil.ly/iO0s5)下载数据集。就像我们在[第4章](ch04.xhtml#chapter_ner)中所做的那样，我们可以使用`get_dataset_config_names()`函数来找出哪些子集是可用的：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For our use case, we’ll focus on building a QA system for the Electronics domain.
    To download the `electronics` subset, we just need to pass this value to the `name`
    argument of the `load_dataset()` function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们将专注于构建电子产品领域的QA系统。要下载`electronics`子集，我们只需要将该值传递给`load_dataset()`函数的`name`参数：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Like other question answering datasets on the Hub, SubjQA stores the answers
    to each question as a nested dictionary. For example, if we inspect one of the
    rows in the `answers` column:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与Hub上的其他问答数据集一样，SubjQA将每个问题的答案存储为嵌套字典。例如，如果我们检查`answers`列中的一行：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'we can see that the answers are stored in a `text` field, while the starting
    character indices are provided in `answer_start`. To explore the dataset more
    easily, we’ll flatten these nested columns with the `flatten()` method and convert
    each split to a Pandas `DataFrame` as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到答案存储在`text`字段中，而起始字符索引则在`answer_start`中提供。为了更轻松地探索数据集，我们将使用`flatten()`方法展平这些嵌套列，并将每个拆分转换为Pandas的`DataFrame`，如下所示：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that the dataset is relatively small, with only 1,908 examples in total.
    This simulates a real-world scenario, since getting domain experts to label extractive
    QA datasets is labor-intensive and expensive. For example, the CUAD dataset for
    extractive QA on legal contracts is estimated to have a value of $2 million to
    account for the legal expertise needed to annotate its 13,000 examples!^([4](ch07.xhtml#idm46238714139488))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集相对较小，总共只有1,908个示例。这模拟了真实世界的情况，因为让领域专家标记抽取式QA数据集是费时费力的。例如，用于法律合同抽取式QA的CUAD数据集估计价值为200万美元，以应对标注其13,000个示例所需的法律专业知识！^([4](ch07.xhtml#idm46238714139488))
- en: There are quite a few columns in the SubjQA dataset, but the most interesting
    ones for building our QA system are shown in [Table 7-1](#subjqa-columns).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SubjQA数据集中有相当多的列，但对于构建我们的QA系统来说，最有趣的是[表7-1](#subjqa-columns)中显示的那些。
- en: Table 7-1\. Column names and their descriptions from the SubjQA dataset
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-1. SubjQA数据集的列名及其描述
- en: '| Column name | Description |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 描述 |'
- en: '| --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `title` | The Amazon Standard Identification Number (ASIN) associated with
    each product |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `title` | 与每个产品相关联的亚马逊标准识别号（ASIN） |'
- en: '| `question` | The question |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `question` | 问题 |'
- en: '| `answers.answer_text` | The span of text in the review labeled by the annotator
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `answers.answer_text` | 标注者标记的评论文本范围 |'
- en: '| `answers.answer_start` | The start character index of the answer span |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `answers.answer_start` | 答案范围的起始字符索引 |'
- en: '| `context` | The customer review |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `context` | 客户评论 |'
- en: 'Let’s focus on these columns and take a look at a few of the training examples.
    We can use the `sample()` method to select a random sample:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于这些列，并查看一些训练示例。我们可以使用`sample()`方法选择一个随机样本：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '| title | question | answers.text | answers.answer_start | context |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| B005DKZTMG | Does the keyboard lightweight? | [this keyboard is compact]
    | [215] | I really like this keyboard. I give it 4 stars because it doesn’t have
    a CAPS LOCK key so I never know if my caps are on. But for the price, it really
    suffices as a wireless keyboard. I have very large hands and this keyboard is
    compact, but I have no complaints. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| B00AAIPT76 | How is the battery? | [] | [] | I bought this after the first
    spare gopro battery I bought wouldn’t hold a charge. I have very realistic expectations
    of this sort of product, I am skeptical of amazing stories of charge time and
    battery life but I do expect the batteries to hold a charge for a couple of weeks
    at least and for the charger to work like a charger. In this I was not disappointed.
    I am a river rafter and found that the gopro burns through power in a hurry so
    this purchase solved that issue. the batteries held a charge, on shorter trips
    the extra two batteries were enough and on longer trips I could use my friends
    JOOS Orange to recharge them.I just bought a newtrent xtreme powerpak and expect
    to be able to charge these with that so I will not run out of power again. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: 'From these examples we can make a few observations. First, the questions are
    not grammatically correct, which is quite common in the FAQ sections of ecommerce
    websites. Second, an empty `answers.text` entry denotes “unanswerable” questions
    whose answer cannot be found in the review. Finally, we can use the start index
    and length of the answer span to slice out the span of text in the review that
    corresponds to the answer:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, let’s get a feel for what types of questions are in the training set
    by counting the questions that begin with a few common starting words:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](Images/nlpt_07in01.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'We can see that questions beginning with “How”, “What”, and “Is” are the most
    common ones, so let’s have a look at some examples:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we’ve explored our dataset a bit, let’s dive into understanding how
    transformers can extract answers from text.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Answers from Text
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we’ll need for our QA system is to find a way to identify a
    potential answer as a span of text in a customer review. For example, if a we
    have a question like “Is it waterproof?” and the review passage is “This watch
    is waterproof at 30m depth”, then the model should output “waterproof at 30m”.
    To do this we’ll need to understand how to:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Frame the supervised learning problem.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenize and encode text for QA tasks.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deal with long passages that exceed a model’s maximum context size.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by taking a look at how to frame the problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Span classification
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common way to extract answers from text is by framing the problem as
    a *span classification* task, where the start and end tokens of an answer span
    act as the labels that a model needs to predict. This process is illustrated in
    [Figure 7-4](#qa-head).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![QA Head](Images/nlpt_0704.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. The span classification head for QA tasks
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since our training set is relatively small, with only 1,295 examples, a good
    strategy is to start with a language model that has already been fine-tuned on
    a large-scale QA dataset like SQuAD. In general, these models have strong reading
    comprehension capabilities and serve as a good baseline upon which to build a
    more accurate system. This is a somewhat different approach to that taken in previous
    chapters, where we typically started with a pretrained model and fine-tuned the
    task-specific head ourselves. For example, in [Chapter 2](ch02.xhtml#chapter_classification),
    we had to fine-tune the classification head because the number of classes was
    tied to the dataset at hand. For extractive QA, we can actually start with a fine-tuned
    model since the structure of the labels remains the same across datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: You can find a list of extractive QA models by navigating to the [Hugging Face
    Hub](https://oreil.ly/dzCsC) and searching for “squad” on the Models tab ([Figure 7-5](#squad-models)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![SQuAD models](Images/nlpt_0705.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. A selection of extractive QA models on the Hugging Face Hub
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, at the time of writing, there are more than 350 QA models to
    choose from—so which one should you pick? In general, the answer depends on various
    factors like whether your corpus is mono- or multilingual and the constraints
    of running the model in a production environment. [Table 7-2](#squad-models-table)
    lists a few models that provide a good foundation to build on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. Baseline transformer models that are fine-tuned on SQuAD 2.0
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformer | Description | Number of parameters | *F*[1]-score on SQuAD
    2.0 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| MiniLM | A distilled version of BERT-base that preserves 99% of the performance
    while being twice as fast | 66M | 79.5 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| RoBERTa-base | RoBERTa models have better performance than their BERT counterparts
    and can be fine-tuned on most QA datasets using a single GPU | 125M | 83.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| ALBERT-XXL | State-of-the-art performance on SQuAD 2.0, but computationally
    intensive and difficult to deploy | 235M | 88.1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| XLM-RoBERTa-large | Multilingual model for 100 languages with strong zero-shot
    performance | 570M | 83.8 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: For the purposes of this chapter, we’ll use a fine-tuned MiniLM model since
    it is fast to train and will allow us to quickly iterate on the techniques that
    we’ll be exploring.^([8](ch07.xhtml#idm46238713719168)) As usual, the first thing
    we need is a tokenizer to encode our texts, so let’s take a look at how this works
    for QA tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing text for QA
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To encode our texts, we’ll load the MiniLM model checkpoint from the [Hugging
    Face Hub](https://oreil.ly/df5Cu) as usual:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To see the model in action, let’s first try to extract an answer from a short
    passage of text. In extractive QA tasks, the inputs are provided as (question,
    context) pairs, so we pass them both to the tokenizer as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here we’ve returned PyTorch `Tensor` objects, since we’ll need them to run
    the forward pass through the model. If we view the tokenized inputs as a table:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '| input_ids | 101 | 2129 | 2172 | 2189 | 2064 | 2023 | ... | 5834 | 2006 |
    5371 | 2946 | 1012 | 102 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| token_type_ids | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| attention_mask | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: we can see the familiar `input_ids` and `attention_mask` tensors, while the
    `token_type_ids` tensor indicates which part of the inputs corresponds to the
    question and context (a 0 indicates a question token, a 1 indicates a context
    token).^([9](ch07.xhtml#idm46238713601696))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how the tokenizer formats the inputs for QA tasks, let’s decode
    the `input_ids` tensor:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We see that for each QA example, the inputs take the format:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'where the location of the first `[SEP]` token is determined by the `token_type_ids`.
    Now that our text is tokenized, we just need to instantiate the model with a QA
    head and run the inputs through the forward pass:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here we can see that we get a `QuestionAnsweringModelOutput` object as the
    output of the QA head. As illustrated in [Figure 7-4](#qa-head), the QA head corresponds
    to a linear layer that takes the hidden states from the encoder and computes the
    logits for the start and end spans.^([10](ch07.xhtml#idm46238713530720)) This
    means that we treat QA as a form of token classification, similar to what we encountered
    for named entity recognition in [Chapter 4](ch04.xhtml#chapter_ner). To convert
    the outputs into an answer span, we first need to get the logits for the start
    and end tokens:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we compare the shapes of these logits to the input IDs:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: we see that there are two logits (a start and end) associated with each input
    token. As illustrated in [Figure 7-6](#qa-scores), larger, positive logits correspond
    to more likely candidates for the start and end tokens. In this example we can
    see that the model assigns the highest start token logits to the numbers “1” and
    “6000”, which makes sense since our question is asking about a quantity. Similarly,
    we see that the end tokens with the highest logits are “minute” and “hours”.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到每个输入标记都有两个logits（一个起始和一个结束）。如[图7-6](#qa-scores)所示，较大的正logits对应于更有可能成为起始和结束标记的候选标记。在这个例子中，我们可以看到模型将最高的起始标记logits分配给数字“1”和“6000”，这是有道理的，因为我们的问题是关于数量的。同样，我们看到最高logits的结束标记是“minute”和“hours”。
- en: '![](Images/nlpt_0706.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_0706.png)'
- en: Figure 7-6\. Predicted logits for the start and end tokens; the token with the
    highest score is colored in orange
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6。预测的起始和结束标记的logits；得分最高的标记以橙色标出
- en: 'To get the final answer, we can compute the argmax over the start and end token
    logits and then slice the span from the inputs. The following code performs these
    steps and decodes the result so we can print the resulting text:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到最终答案，我们可以计算起始和结束标记logits的argmax，然后从输入中切片出这个范围。以下代码执行这些步骤并解码结果，以便我们可以打印出结果文本：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Great, it worked! In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, all
    of these preprocessing and postprocessing steps are conveniently wrapped in a
    dedicated pipeline. We can instantiate the pipeline by passing our tokenizer and
    fine-tuned model as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，成功了！在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，所有这些预处理和后处理步骤都方便地包装在一个专用的管道中。我们可以通过传递我们的分词器和微调模型来实例化管道，如下所示：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In addition to the answer, the pipeline also returns the model’s probability
    estimate in the `score` field (obtained by taking a softmax over the logits).
    This is handy when we want to compare multiple answers within a single context.
    We’ve also shown that we can have the model predict multiple answers by specifying
    the `topk` parameter. Sometimes, it is possible to have questions for which no
    answer is possible, like the empty `answers.answer_start` examples in SubjQA.
    In these cases the model will assign a high start and end score to the `[CLS]`
    token, and the pipeline maps this output to an empty string:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了答案，管道还在`score`字段中返回模型的概率估计（通过对logits进行softmax获得）。当我们想要在单个上下文中比较多个答案时，这是很方便的。我们还表明，通过指定`topk`参数，我们可以让模型预测多个答案。有时，可能会有问题没有答案的情况，比如SubjQA中空的`answers.answer_start`示例。在这些情况下，模型将为`[CLS]`标记分配高的起始和结束分数，管道将这个输出映射为空字符串：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In our simple example, we obtained the start and end indices by taking the argmax
    of the corresponding logits. However, this heuristic can produce out-of-scope
    answers by selecting tokens that belong to the question instead of the context.
    In practice, the pipeline computes the best combination of start and end indices
    subject to various constraints such as being in-scope, requiring the start indices
    to precede the end indices, and so on.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的简单示例中，我们通过获取相应logits的argmax来获得起始和结束索引。然而，这种启发式方法可能会产生超出范围的答案，因为它选择属于问题而不是上下文的标记。在实践中，管道计算最佳的起始和结束索引组合，受到各种约束的限制，比如在范围内，要求起始索引在结束索引之前等等。
- en: Dealing with long passages
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理长段落
- en: One subtlety faced by reading comprehension models is that the context often
    contains more tokens than the maximum sequence length of the model (which is usually
    a few hundred tokens at most). As illustrated in [Figure 7-7](#subjqa-dist), a
    decent portion of the SubjQA training set contains question-context pairs that
    won’t fit within MiniLM’s context size of 512 tokens.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读理解模型面临的一个微妙之处是，上下文通常包含的标记比模型的最大序列长度（通常最多几百个标记）多。如[图7-7](#subjqa-dist)所示，SubjQA训练集中相当一部分包含的问题-上下文对无法适应MiniLM的512个标记的上下文大小。
- en: '![](Images/nlpt_0707.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_0707.png)'
- en: Figure 7-7\. Distribution of tokens for each question-context pair in the SubjQA
    training set
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7。SubjQA训练集中每个问题-上下文对的标记分布
- en: For other tasks, like text classification, we simply truncated long texts under
    the assumption that enough information was contained in the embedding of the `[CLS]`
    token to generate accurate predictions. For QA, however, this strategy is problematic
    because the answer to a question could lie near the end of the context and thus
    would be removed by truncation. As illustrated in [Figure 7-8](#sliding-window),
    the standard way to deal with this is to apply a *sliding window* across the inputs,
    where each window contains a passage of tokens that fit in the model’s context.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他任务，比如文本分类，我们简单地截断长文本，假设`[CLS]`标记的嵌入中包含足够的信息来生成准确的预测。然而，对于QA来说，这种策略是有问题的，因为问题的答案可能位于上下文的末尾，因此会被截断。如[图7-8](#sliding-window)所示，处理这个问题的标准方法是在输入上应用*滑动窗口*，其中每个窗口包含适合模型上下文的标记段。
- en: '![Sliding window](Images/nlpt_0708.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![滑动窗口](Images/nlpt_0708.png)'
- en: Figure 7-8\. How the sliding window creates multiple question-context pairs
    for long documents—the first bar corresponds to the question, while the second
    bar is the context captured in each window
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8。滑动窗口如何为长文档创建多个问题-上下文对——第一个条形图对应问题，而第二个条形图是每个窗口中捕获的上下文
- en: 'In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we can set `return_overflowing_tokens=True`
    in the tokenizer to enable the sliding window. The size of the sliding window
    is controlled by the `max_seq_length` argument, and the size of the stride is
    controlled by `doc_stride`. Let’s grab the first example from our training set
    and define a small window to illustrate how this works:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，我们可以在分词器中设置`return_overflowing_tokens=True`来启用滑动窗口。滑动窗口的大小由`max_seq_length`参数控制，步幅的大小由`doc_stride`控制。让我们从训练集中抓取第一个例子，并定义一个小窗口来说明这是如何工作的：
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In this case we now get a list of `input_ids`, one for each window. Let’s check
    the number of tokens we have in each window:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we can see where two windows overlap by decoding the inputs:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now that we have some intuition about how QA models can extract answers from
    text, let’s look at the other components we need to build an end-to-end QA pipeline.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Using Haystack to Build a QA Pipeline
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our simple answer extraction example, we provided both the question and the
    context to the model. However, in reality our system’s users will only provide
    a question about a product, so we need some way of selecting relevant passages
    from among all the reviews in our corpus. One way to do this would be to concatenate
    all the reviews of a given product together and feed them to the model as a single,
    long context. Although simple, the drawback of this approach is that the context
    can become extremely long and thereby introduce an unacceptable latency for our
    users’ queries. For example, let’s suppose that on average, each product has 30
    reviews and each review takes 100 milliseconds to process. If we need to process
    all the reviews to get an answer, this would result in an average latency of 3
    seconds per user query—much too long for ecommerce websites!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle this, modern QA systems are typically based on the *retriever-reader*
    architecture, which has two main components:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '*Retriever*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Responsible for retrieving relevant documents for a given query. Retrievers
    are usually categorized as *sparse* or *dense*. Sparse retrievers use word frequencies
    to represent each document and query as a sparse vector.^([11](ch07.xhtml#idm46238713054704))
    The relevance of a query and a document is then determined by computing an inner
    product of the vectors. On the other hand, dense retrievers use encoders like
    transformers to represent the query and document as contextualized embeddings
    (which are dense vectors). These embeddings encode semantic meaning, and allow
    dense retrievers to improve search accuracy by understanding the content of the
    query.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '*Reader*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Responsible for extracting an answer from the documents provided by the retriever.
    The reader is usually a reading comprehension model, although at the end of the
    chapter we’ll see examples of models that can generate free-form answers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 7-9](#retriever-reader), there can also be other components
    that apply post-processing to the documents fetched by the retriever or to the
    answers extracted by the reader. For example, the retrieved documents may need
    reranking to eliminate noisy or irrelevant ones that can confuse the reader. Similarly,
    postprocessing of the reader’s answers is often needed when the correct answer
    comes from various passages in a long document.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![QA Architecture](Images/nlpt_0709.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. The retriever-reader architecture for modern QA systems
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To build our QA system, we’ll use the [*Haystack* library](https://haystack.deepset.ai)
    developed by [deepset](https://deepset.ai), a German company focused on NLP. Haystack
    is based on the retriever-reader architecture, abstracts much of the complexity
    involved in building these systems, and integrates tightly with ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the retriever and reader, there are two more components involved
    when building a QA pipeline with Haystack:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '*Document store*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: A document-oriented database that stores documents and metadata which are provided
    to the retriever at query time
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '*Pipeline*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Combines all the components of a QA system to enable custom query flows, merging
    documents from multiple retrievers, and more
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll look at how we can use these components to quickly build
    a prototype QA pipeline. Later, we’ll examine how we can improve its performance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This chapter was written using version 0.9.0 of the Haystack library. In [version
    0.10.0](https://oreil.ly/qbqgv), the pipeline and evaluation APIs were redesigned
    to make it easier to inspect whether the retriever or reader are impacting performance.
    To see what this chapter’s code looks like with the new API, check out the [GitHub
    repository](https://github.com/nlp-with-transformers/notebooks).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a document store
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Haystack, there are various document stores to choose from and each one can
    be paired with a dedicated set of retrievers. This is illustrated in [Table 7-3](#doc-stores),
    where the compatibility of sparse (TF-IDF, BM25) and dense (Embedding, DPR) retrievers
    is shown for each of the available document stores. We’ll explain what all these
    acronyms mean later in this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-3\. Compatibility of Haystack retrievers and document stores
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '|  | In memory | Elasticsearch | FAISS | Milvus |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| TF-IDF | Yes | Yes | No | No |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| BM25 | No | Yes | No | No |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| Embedding | Yes | Yes | Yes | Yes |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| DPR | Yes | Yes | Yes | Yes |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: Since we’ll be exploring both sparse and dense retrievers in this chapter, we’ll
    use the `ElasticsearchDocumentStore`, which is compatible with both retriever
    types. Elasticsearch is a search engine that is capable of handling a diverse
    range of data types, including textual, numerical, geospatial, structured, and
    unstructured. Its ability to store huge volumes of data and quickly filter it
    with full-text search features makes it especially well suited for developing
    QA systems. It also has the advantage of being the industry standard for infrastructure
    analytics, so there’s a good chance your company already has a cluster that you
    can work with.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'To initialize the document store, we first need to download and install Elasticsearch.
    By following Elasticsearch’s [guide](https://oreil.ly/bgmKq),^([12](ch07.xhtml#idm46238713004128))
    we can grab the latest release for Linux with `wget` and unpack it with the `tar`
    shell command:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next we need to start the Elasticsearch server. Since we’re running all the
    code in this book within Jupyter notebooks, we’ll need to use Python’s `Popen()`
    function to spawn a new process. While we’re at it, let’s also run the subprocess
    in the background using the `chown` shell command:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the `Popen()` function, the `args` specify the program we wish to execute,
    while `stdout=PIPE` creates a new pipe for the standard output and `stderr=STDOUT`
    collects the errors in the same pipe. The `preexec_fn` argument specifies the
    ID of the subprocess we wish to use. By default, Elasticsearch runs locally on
    port 9200, so we can test the connection by sending an HTTP request to `localhost`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now that our Elasticsearch server is up and running, the next thing to do is
    instantiate the document store:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'By default, `ElasticsearchDocumentStore` creates two indices on Elasticsearch:
    one called `document` for (you guessed it) storing documents, and another called
    `label` for storing the annotated answer spans. For now, we’ll just populate the
    `document` index with the SubjQA reviews, and Haystack’s document stores expect
    a list of dictionaries with `text` and `meta` keys as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The fields in `meta` can be used for applying filters during retrieval. For
    our purposes we’ll include the `item_id` and `q_review_id` columns of SubjQA so
    we can filter by product and question ID, along with the corresponding training
    split. We can then loop through the examples in each `DataFrame` and add them
    to the index with the `write_documents()` method as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Great, we’ve loaded all our reviews into an index! To search the index we’ll
    need a retriever, so let’s look at how we can initialize one for Elasticsearch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a retriever
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Elasticsearch document store can be paired with any of the Haystack retrievers,
    so let’s start by using a sparse retriever based on BM25 (short for “Best Match
    25”). BM25 is an improved version of the classic Term Frequency-Inverse Document
    Frequency (TF-IDF) algorithm and represents the question and context as sparse
    vectors that can be searched efficiently on Elasticsearch. The BM25 score measures
    how much matched text is about a search query and improves on TF-IDF by saturating
    TF values quickly and normalizing the document length so that short documents
    are favored over long ones.^([13](ch07.xhtml#idm46238712667392))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch文档存储可以与Haystack检索器中的任何一种配对，因此让我们首先使用基于BM25的稀疏检索器（简称“Best Match
    25”）。BM25是经典的词项频率-逆文档频率（TF-IDF）算法的改进版本，它将问题和上下文表示为可以在Elasticsearch上高效搜索的稀疏向量。BM25分数衡量了匹配文本与搜索查询的相关程度，并通过迅速饱和TF值和规范化文档长度来改进TF-IDF，以便短文档优于长文档。^([13](ch07.xhtml#idm46238712667392))
- en: 'In Haystack, the BM25 retriever is used by default in `ElasticsearchRetriever`,
    so let’s initialize this class by specifying the document store we wish to search
    over:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在Haystack中，默认情况下使用BM25检索器在`ElasticsearchRetriever`中，因此让我们通过指定我们希望搜索的文档存储来初始化这个类：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, let’s look at a simple query for a single electronics product in the
    training set. For review-based QA systems like ours, it’s important to restrict
    the queries to a single item because otherwise the retriever would source reviews
    about products that are not related to a user’s query. For example, asking “Is
    the camera quality any good?” without a product filter could return reviews about
    phones, when the user might be asking about a specific laptop camera instead.
    By themselves, the ASIN values in our dataset are a bit cryptic, but we can decipher
    them with online tools like [*amazon ASIN*](https://amazon-asin.com) or by simply
    appending the value of `item_id` to the *www.amazon.com/dp/* URL. The following
    item ID corresponds to one of Amazon’s Fire tablets, so let’s use the retriever’s
    `retrieve()` method to ask if it’s any good for reading with:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下训练集中单个电子产品的简单查询。对于像我们这样基于评论的问答系统，将查询限制在单个项目是很重要的，否则检索器会返回与用户查询无关的产品评论。例如，询问“相机质量如何？”如果没有产品过滤器，可能会返回关于手机的评论，而用户可能是在询问特定笔记本电脑相机的情况。我们的数据集中的ASIN值本身有点神秘，但我们可以使用在线工具如[*amazon
    ASIN*](https://amazon-asin.com)或者简单地将`item_id`的值附加到*www.amazon.com/dp/* URL来解密它们。以下项目ID对应于亚马逊的Fire平板电脑之一，所以让我们使用检索器的`retrieve()`方法来询问它是否适合阅读：
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here we’ve specified how many documents to return with the `top_k` argument
    and applied a filter on both the `item_id` and `split` keys that were included
    in the `meta` field of our documents. Each element of `retrieved_docs` is a Haystack
    `Document` object that is used to represent documents and includes the retriever’s
    query score along with other metadata. Let’s have a look at one of the retrieved
    documents:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经指定了`top_k`参数返回多少个文档，并在我们的文档的`meta`字段中包含的`item_id`和`split`键上应用了过滤器。`retrieved_docs`的每个元素都是一个Haystack
    `Document`对象，用于表示文档并包括检索器的查询分数以及其他元数据。让我们看一下其中一个检索到的文档：
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In addition to the document’s text, we can see the `score` that Elasticsearch
    computed for its relevance to the query (larger scores imply a better match).
    Under the hood, Elasticsearch relies on [Lucene](https://lucene.apache.org) for
    indexing and search, so by default it uses Lucene’s *practical scoring function*.
    You can find the nitty-gritty details behind the scoring function in the [Elasticsearch
    documentation](https://oreil.ly/b1Seu), but in brief terms it first filters the
    candidate documents by applying a Boolean test (does the document match the query?),
    and then applies a similarity metric that’s based on representing both the document
    and the query as vectors.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除了文档的文本，我们还可以看到Elasticsearch为其与查询的相关性计算的`score`（较大的分数意味着更好的匹配）。在幕后，Elasticsearch依赖于[Lucene](https://lucene.apache.org)进行索引和搜索，因此默认情况下它使用Lucene的*practical
    scoring function*。您可以在[Elasticsearch文档](https://oreil.ly/b1Seu)中找到得分函数背后的细节，但简而言之，它首先通过应用布尔测试（文档是否与查询匹配）来过滤候选文档，然后应用基于将文档和查询表示为向量的相似度度量。
- en: Now that we have a way to retrieve relevant documents, the next thing we need
    is a way to extract answers from them. This is where the reader comes in, so let’s
    take a look at how we can load our MiniLM model in Haystack.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了检索相关文档的方法，接下来我们需要的是从中提取答案的方法。这就是读取器的作用，让我们看看如何在Haystack中加载我们的MiniLM模型。
- en: Initializing a reader
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化读取器
- en: 'In Haystack, there are two types of readers one can use to extract answers
    from a given context:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在Haystack中，有两种类型的读取器可以用来从给定的上下文中提取答案：
- en: '`FARMReader`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`FARMReader`'
- en: Based on deepset’s [*FARM* framework](https://farm.deepset.ai) for fine-tuning
    and deploying transformers. Compatible with models trained using ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers and can load models directly from the Hugging Face Hub.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于deepset的[*FARM*框架](https://farm.deepset.ai)进行transformers的微调和部署。与使用![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers训练的模型兼容，并且可以直接从Hugging Face Hub加载模型。
- en: '`TransformersReader`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransformersReader`'
- en: Based on the QA pipeline from ![nlpt_pin01](Images/nlpt_pin01.png) Transformers.
    Suitable for running inference only.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 基于![nlpt_pin01](Images/nlpt_pin01.png) Transformers的QA流水线。适用于仅运行推理。
- en: 'Although both readers handle a model’s weights in the same way, there are some
    differences in the way the predictions are converted to produce answers:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管两种读取器都以相同的方式处理模型的权重，但在转换预测以生成答案方面存在一些差异：
- en: In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, the QA pipeline normalizes
    the start and end logits with a softmax in each passage. This means that it is
    only meaningful to compare answer scores between answers extracted from the same
    passage, where the probabilities sum to 1\. For example, an answer score of 0.9
    from one passage is not necessarily better than a score of 0.8 in another. In
    FARM, the logits are not normalized, so inter-passage answers can be compared
    more easily.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `TransformersReader` sometimes predicts the same answer twice, but with
    different scores. This can happen in long contexts if the answer lies across two
    overlapping windows. In FARM, these duplicates are removed.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we will be fine-tuning the reader later in the chapter, we’ll use the
    `FARMReader`. As with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, to load
    the model we just need to specify the MiniLM checkpoint on the Hugging Face Hub
    along with some QA-specific arguments:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also possible to fine-tune a reading comprehension model directly in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers and then load it in `TransformersReader` to run inference. For details
    on how to do the fine-tuning step, see the question answering tutorial in the
    [library’s documentation](https://oreil.ly/VkhIQ).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'In `FARMReader`, the behavior of the sliding window is controlled by the same
    `max_seq_length` and `doc_stride` arguments that we saw for the tokenizer. Here
    we’ve used the values from the MiniLM paper. To confirm, let’s now test the reader
    on our simple example from earlier:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Great, the reader appears to be working as expected—so next, let’s tie together
    all our components using one of Haystack’s pipelines.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Haystack provides a `Pipeline` abstraction that allows us to combine retrievers,
    readers, and other components together as a graph that can be easily customized
    for each use case. There are also predefined pipelines analogous to those in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, but specialized for QA systems. In our case, we’re interested in
    extracting answers, so we’ll use the `ExtractiveQAPipeline`, which takes a single
    retriever-reader pair as its arguments:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Each `Pipeline` has a `run()` method that specifies how the query flow should
    be executed. For the `ExtractiveQAPipeline` we just need to pass the `query`,
    the number of documents to retrieve with `top_k_retriever`, and the number of
    answers to extract from these documents with `top_k_reader`. In our case, we also
    need to specify a filter over the item ID, which can be done using the `filters`
    argument as we did with the retriever earlier. Let’s run a simple example using
    our question about the Amazon Fire tablet again, but this time returning the extracted
    answers:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Great, we now have an end-to-end QA system for Amazon product reviews! This
    is a good start, but notice that the second and third answers are closer to what
    the question is actually asking. To do better, we’ll need some metrics to quantify
    the performance of the retriever and reader. We’ll take a look at that next.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Improving Our QA Pipeline
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although much of the recent research on QA has focused on improving reading
    comprehension models, in practice it doesn’t matter how good your reader is if
    the retriever can’t find the relevant documents in the first place! In particular,
    the retriever sets an upper bound on the performance of the whole QA system, so
    it’s important to make sure it’s doing a good job. With this in mind, let’s start
    by introducing some common metrics to evaluate the retriever so that we can compare
    the performance of sparse and dense representations.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Retriever
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common metric for evaluating retrievers is *recall*, which measures the fraction
    of all relevant documents that are retrieved. In this context, “relevant” simply
    means whether the answer is present in a passage of text or not, so given a set
    of questions, we can compute recall by counting the number of times an answer
    appears in the top *k* documents returned by the retriever.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'In Haystack, there are two ways to evaluate retrievers:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Use the retriever’s in-built `eval()` method. This can be used for both open-
    and closed-domain QA, but not for datasets like SubjQA where each document is
    paired with a single product and we need to filter by product ID for every query.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a custom `Pipeline` that combines a retriever with the `EvalRetriever`
    class. This enables the implementation of custom metrics and query flows.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A complementary metric to recall is *mean average precision* (mAP), which rewards
    retrievers that can place the correct answers higher up in the document ranking.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we need to evaluate the recall per product and then aggregate across
    all products, we’ll opt for the second approach. Each node in the `Pipeline` graph
    represents a class that takes some inputs and produces some outputs via a `run()`
    method:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Here `kwargs` corresponds to the outputs from the previous node in the graph,
    which is manipulated within the `run()` method to return a tuple of the outputs
    for the next node, along with a name for the outgoing edge. The only other requirement
    is to include an `outgoing_edges` attribute that indicates the number of outputs
    from the node (in most cases `outgoing_edges=1`, unless you have branches in the
    pipeline that route the inputs according to some criterion).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we need a node to evaluate the retriever, so we’ll use the `EvalRetriever`
    class whose `run()` method keeps track of which documents have answers that match
    the ground truth. With this class we can then build up a `Pipeline` graph by adding
    the evaluation node after a node that represents the retriever itself:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Notice that each node is given a `name` and a list of `inputs`. In most cases,
    each node has a single outgoing edge, so we just need to include the name of the
    previous node in `inputs`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our evaluation pipeline, we need to pass some queries and
    their corresponding answers. To do this, we’ll add the answers to a dedicated
    `label` index on our document store. Haystack provides a `Label` object that represents
    the answer spans and their metadata in a standardized fashion. To populate the
    `label` index, we’ll first create a list of `Label` objects by looping over each
    question in the test set and extracting the matching answers and additional metadata:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'If we peek at one of these labels:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'we can see the question-answer pair, along with an `origin` field that contains
    the unique question ID so we can filter the document store per question. We’ve
    also added the product ID to the `meta` field so we can filter the labels by product.
    Now that we have our labels, we can write them to the `label` index on Elasticsearch
    as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we need to build up a mapping between our question IDs and corresponding
    answers that we can pass to the pipeline. To get all the labels, we can use the
    `get_all_labels_aggregated()` method from the document store that will aggregate
    all question-answer pairs associated with a unique ID. This method returns a list
    of `MultiLabel` objects, but in our case we only get one element since we’re filtering
    by question ID. We can build up a list of aggregated labels as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'By peeking at one of these labels we can see that all the answers associated
    with a given question are aggregated together in a `multiple_answers` field:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We now have all the ingredients for evaluating the retriever, so let’s define
    a function that feeds each question-answer pair associated with each product to
    the evaluation pipeline and tracks the correct retrievals in our `pipe` object:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Great, it works! Notice that we picked a specific value for `top_k_retriever`
    to specify the number of documents to retrieve. In general, increasing this parameter
    will improve the recall, but at the expense of providing more documents to the
    reader and slowing down the end-to-end pipeline. To guide our decision on which
    value to pick, we’ll create a function that loops over several *k* values and
    compute the recall across the whole test set for each *k*:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'If we plot the results, we can see how the recall improves as we increase *k*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](Images/nlpt_07in02.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: From the plot, we can see that there’s an inflection point around <math alttext="k
    equals 5"><mrow><mi>k</mi> <mo>=</mo> <mn>5</mn></mrow></math> and we get almost
    perfect recall from <math alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>
    onwards. Let’s now take a look at retrieving documents with dense vector techniques.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Dense Passage Retrieval
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen that we get almost perfect recall when our sparse retriever returns
    <math alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>
    documents, but can we do better at smaller values of *k*? The advantage of doing
    so is that we can pass fewer documents to the reader and thereby reduce the overall
    latency of our QA pipeline. A well-known limitation of sparse retrievers like
    BM25 is that they can fail to capture the relevant documents if the user query
    contains terms that don’t match exactly those of the review. One promising alternative
    is to use dense embeddings to represent the question and document, and the current
    state of the art is an architecture known as *Dense Passage Retrieval* (DPR).^([14](ch07.xhtml#idm46238711297696))
    The main idea behind DPR is to use two BERT models as encoders for the question
    and the passage. As illustrated in [Figure 7-10](#dpr), these encoders map the
    input text into a *d*-dimensional vector representation of the `[CLS]` token.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![DPR Architecture](Images/nlpt_0710.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. DPR’s bi-encoder architecture for computing the relevance of a
    document and query
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Haystack, we can initialize a retriever for DPR in a similar way to what
    we did for BM25\. In addition to specifying the document store, we also need to
    pick the BERT encoders for the question and passage. These encoders are trained
    by giving them questions with relevant (positive) passages and irrelevant (negative)
    passages, where the goal is to learn that relevant question-passage pairs have
    a higher similarity. For our use case, we’ll use encoders that have been fine-tuned
    on the NQ corpus in this way:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Here we’ve also set `embed_title=False` since concatenating the document’s
    title (i.e., `item_id`) doesn’t provide any additional information because we
    filter per product. Once we’ve initialized the dense retriever, the next step
    is to iterate over all the indexed documents in our Elasticsearch index and apply
    the encoders to update the embedding representation. This can be done as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We’re now set to go! We can evaluate the dense retriever in the same way we
    did for BM25 and compare the top-*k* recall:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '![](Images/nlpt_07in03.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Here we can see that DPR does not provide a boost in recall over BM25 and saturates
    around <math alttext="k equals 3"><mrow><mi>k</mi> <mo>=</mo> <mn>3</mn></mrow></math>
    .
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Performing similarity search of the embeddings can be sped up by using Facebook’s
    [FAISS library](https://oreil.ly/1E8Z0) as the document store. Similarly, the
    performance of the DPR retriever can be improved by fine-tuning on the target
    domain. If you’d like to learn how to fine-tune DPR, check out the Haystack [tutorial](https://oreil.ly/eXyro).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored the evaluation of the retriever, let’s turn to evaluating
    the reader.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Reader
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In extractive QA, there are two main metrics that are used for evaluating readers:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '*Exact Match (EM)*'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: A binary metric that gives EM = 1 if the characters in the predicted and ground
    truth answers match exactly, and EM = 0 otherwise. If no answer is expected, the
    model gets EM = 0 if it predicts any text at all.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '*F*[1]-score'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Measures the harmonic mean of the precision and recall.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how these metrics work by importing some helper functions from FARM
    and applying them to a simple example:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Under the hood, these functions first normalize the prediction and label by
    removing punctuation, fixing whitespace, and converting to lowercase. The normalized
    strings are then tokenized as a bag-of-words, before finally computing the metric
    at the token level. From this simple example we can see that EM is a much stricter
    metric than the *F*[1]-score: adding a single token to the prediction gives an
    EM of zero. On the other hand, the *F*[1]-score can fail to catch truly incorrect
    answers. For example, if our predicted answer span is “about 6000 dollars”, then
    we get:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Relying on just the *F*[1]-score is thus misleading, and tracking both metrics
    is a good strategy to balance the trade-off between underestimating (EM) and overestimating
    (*F*[1]-score) model performance.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Now in general, there are multiple valid answers per question, so these metrics
    are calculated for each question-answer pair in the evaluation set, and the best
    score is selected over all possible answers. The overall EM and *F*[1] scores
    for the model are then obtained by averaging over the individual scores of each
    question-answer pair.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the reader we’ll create a new pipeline with two nodes: a reader
    node and a node to evaluate the reader. We’ll use the `EvalReader` class that
    takes the predictions from the reader and computes the corresponding EM and *F*[1]
    scores. To compare with the SQuAD evaluation, we’ll take the best answers for
    each query with the `top_1_em` and `top_1_f1` metrics that are stored in `EvalAnswers`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Notice that we specified `skip_incorrect_retrieval=False`. This is to ensure
    that the retriever always passes the context to the reader (as in the SQuAD evaluation).
    Now that we’ve run every question through the reader, let’s print the scores:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![](Images/nlpt_07in04.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: OK, it seems that the fine-tuned model performs significantly worse on SubjQA
    than on SQuAD 2.0, where MiniLM achieves EM and *F*[1] scores of 76.1 and 79.5,
    respectively. One reason for the performance drop is that customer reviews are
    quite different from the Wikipedia articles the SQuAD 2.0 dataset is generated
    from, and the language they use is often informal. Another factor is likely the
    inherent subjectivity of our dataset, where both questions and answers differ
    from the factual information contained in Wikipedia. Let’s look at how to fine-tune
    a model on a dataset to get better results with domain adaptation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Domain Adaptation
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although models that are fine-tuned on SQuAD will often generalize well to other
    domains, we’ve seen that for SubjQA the EM and *F*[1] scores of our model were
    much worse than for SQuAD. This failure to generalize has also been observed in
    other extractive QA datasets and is understood as evidence that transformer models
    are particularly adept at overfitting to SQuAD.^([15](ch07.xhtml#idm46238710735456))
    The most straightforward way to improve the reader is by fine-tuning our MiniLM
    model further on the SubjQA training set. The `FARMReader` has a `train()` method
    that is designed for this purpose and expects the data to be in SQuAD JSON format,
    where all the question-answer pairs are grouped together for each item as illustrated
    in [Figure 7-11](#squad-schema).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![SQuAD Schema](Images/nlpt_0711.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: Figure 7-11\. Visualization of the SQuAD JSON format
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is quite a complex data format, so we’ll need a few functions and some
    Pandas magic to help us do the conversion. The first thing we need to do is implement
    a function that can create the `paragraphs` array associated with each product
    ID. Each element in this array contains a single context (i.e., review) and a
    `qas` array of question-answer pairs. Here’s a function that builds up the `paragraphs`
    array:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, when we apply to the rows of a `DataFrame` associated with a single product
    ID, we get the SQuAD format:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The final step is to then apply this function to each product ID in the `DataFrame`
    of each split. The following `convert_to_squad()` function does this trick and
    stores the result in an *electronics-{split}.json* file:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now that we have the splits in the right format, let’s fine-tune our reader
    by specifying the locations of the train and dev splits, along with where to save
    the fine-tuned model:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'With the reader fine-tuned, let’s now compare its performance on the test set
    against our baseline model:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](Images/nlpt_07in05.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Wow, domain adaptation has increased our EM score by a factor of six and more
    than doubled the *F*[1]-score! At this point, you might be wondering why we didn’t
    just fine-tune a pretrained language model directly on the SubjQA training set.
    One reason is that we only have 1,295 training examples in SubjQA while SQuAD
    has over 100,000, so we might run into challenges with overfitting. Nevertheless,
    let’s take a look at what naive fine-tuning produces. For a fair comparison, we’ll
    use the same language model that was used for fine-tuning our baseline on SQuAD.
    As before, we’ll load up the model with the `FARMReader`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Next, we fine-tune for one epoch:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'and include the evaluation on the test set:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '![](Images/nlpt_07in06.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: We can see that fine-tuning the language model directly on SubjQA results in
    considerably worse performance than fine-tuning on SQuAD and SubjQA.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When dealing with small datasets, it is best practice to use cross-validation
    when evaluating transformers as they can be prone to overfitting. You can find
    an example of how to perform cross-validation with SQuAD-formatted datasets in
    the [FARM repository](https://oreil.ly/K3nK8).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Whole QA Pipeline
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve seen how to evaluate the reader and retriever components individually,
    let’s tie them together to measure the overall performance of our pipeline. To
    do so, we’ll need to augment our retriever pipeline with nodes for the reader
    and its evaluation. We’ve seen that we get almost perfect recall at <math alttext="k
    equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math> , so we can fix
    this value and assess the impact this has on the reader’s performance (since it
    will now receive multiple contexts per query compared to the SQuAD-style evaluation):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: We can then compare the top 1 EM and *F*[1] scores for the model to predict
    an answer in the documents returned by the retriever in [Figure 7-12](#reader-vs-pipeline).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/nlpt_0712.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: Figure 7-12\. Comparison of EM and *F*[1] scores for the reader against the
    whole QA pipeline
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this plot we can see the effect that the retriever has on the overall performance.
    In particular, there is an overall degradation compared to matching the question-context
    pairs, as is done in the SQuAD-style evaluation. This can be circumvented by increasing
    the number of possible answers that the reader is allowed to predict.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Until now we have only extracted answer spans from the context, but in general
    it could be that bits and pieces of the answer are scattered throughout the document
    and we would like our model to synthesize these fragments into a single coherent
    answer. Let’s have a look at how we can use generative QA to succeed at this task.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Going Beyond Extractive QA
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One interesting alternative to extracting answers as spans of text in a document
    is to generate them with a pretrained language model. This approach is often referred
    to as *abstractive* or *generative QA* and has the potential to produce better-phrased
    answers that synthesize evidence across multiple passages. Although less mature
    than extractive QA, this is a fast-moving field of research, so chances are that
    these approaches will be widely adopted in industry by the time you are reading
    this! In this section we’ll briefly touch on the current state of the art: *retrieval-augmented
    generation* (RAG).^([16](ch07.xhtml#idm46238709770880))'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: RAG extends the classic retriever-reader architecture that we’ve seen in this
    chapter by swapping the reader for a *generator* and using DPR as the retriever.
    The generator is a pretrained sequence-to-sequence transformer like T5 or BART
    that receives latent vectors of documents from DPR and then iteratively generates
    an answer based on the query and these documents. Since DPR and the generator
    are differentiable, the whole process can be fine-tuned end-to-end as illustrated
    in [Figure 7-13](#rag).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![RAG Architecture](Images/nlpt_0713.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: Figure 7-13\. The RAG architecture for fine-tuning a retriever and generator
    end-to-end (courtesy of Ethan Perez)
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To show RAG in action we’ll use the `DPRetriever` from earlier, so we just
    need to instantiate a generator. There are two types of RAG models to choose from:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '*RAG-Sequence*'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Uses the same retrieved document to generate the complete answer. In particular,
    the top *k* documents from the retriever are fed to the generator, which produces
    an output sequence for each document, and the result is marginalized to obtain
    the best answer.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '*RAG-Token*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Can use a different document to generate each token in the answer. This allows
    the generator to synthesize evidence from multiple documents.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Since RAG-Token models tend to perform better than RAG-Sequence ones, we’ll
    use the token model that was fine-tuned on NQ as our generator. Instantiating
    a generator in Haystack is similar to instantiating the reader, but instead of
    specifying the `max_seq_length` and `doc_stride` parameters for a sliding window
    over the contexts, we specify hyperparameters that control the text generation:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Here `num_beams` specifies the number of beams to use in beam search (text generation
    is covered at length in [Chapter 5](ch05.xhtml#chapter_generation)). As we did
    with the DPR retriever, we don’t embed the document titles since our corpus is
    always filtered per product ID.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing to do is tie together the retriever and generator using Haystack’s
    `GenerativeQAPipeline`:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Note
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In RAG, both the query encoder and the generator are trained end-to-end, while
    the context encoder is frozen. In Haystack, the `GenerativeQAPipeline` uses the
    query encoder from `RAGenerator` and the context encoder from `DensePassageRetriever`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now give RAG a spin by feeding in some queries about the Amazon Fire
    tablet from before. To simplify the querying, we’ll write a simple function that
    takes the query and prints out the top answers:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'OK, now we’re ready to give it a test:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This result isn’t too bad for an answer, but it does suggest that the subjective
    nature of the question is confusing the generator. Let’s try with something a
    bit more factual:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: This is more sensible! To get better results we could fine-tune RAG end-to-end
    on SubjQA; we’ll leave this as an exercise, but if you’re interested in exploring
    it there are scripts in the ![nlpt_pin01](Images/nlpt_pin01.png) [Transformers
    repository](https://oreil.ly/oZz4S) to help you get started.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, that was a whirlwind tour of QA, and you probably have many more questions
    that you’d like answered (pun intended!). In this chapter, we discussed two approaches
    to QA (extractive and generative) and examined two different retrieval algorithms
    (BM25 and DPR). Along the way, we saw that domain adaptation can be a simple technique
    to boost the performance of our QA system by a significant margin, and we looked
    at a few of the most common metrics that are used for evaluating such systems.
    Although we focused on closed-domain QA (i.e., a single domain of electronic products),
    the techniques in this chapter can easily be generalized to the open-domain case;
    we recommend reading Cloudera’s excellent Fast Forward [QA series](https://oreil.ly/Fd6lc)
    to see what’s involved.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Deploying QA systems in the wild can be a tricky business to get right, and
    our experience is that a significant part of the value comes from first providing
    end users with useful search capabilities, followed by an extractive component.
    In this respect, the reader can be used in novel ways beyond answering on-demand
    user queries. For example, researchers at [Grid Dynamics](https://oreil.ly/CGLh1)
    were able to use their reader to automatically extract a set of pros and cons
    for each product in a client’s catalog. They also showed that a reader can be
    used to extract named entities in a zero-shot fashion by creating queries like
    “What kind of camera?” Given its infancy and subtle failure modes, we recommend
    exploring generative QA only once the other two approaches have been exhausted.
    This “hierarchy of needs” for tackling QA problems is illustrated in [Figure 7-14](#qa-pyramid).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![QA Pyramid](Images/nlpt_0714.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: Figure 7-14\. The QA hierarchy of needs
  id: totrans-347
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking ahead, one exciting research area is *multimodal QA*, which involves
    QA over multiple modalities like text, tables, and images. As described in the
    MultiModalQA benchmark,^([17](ch07.xhtml#idm46238709566048)) such systems could
    enable users to answer complex questions that integrate information across different
    modalities, like “When was the famous painting with two touching fingers completed?”
    Another area with practical business applications is QA over a *knowledge graph*,
    where the nodes of the graph correspond to real-world entities and their relations
    are defined by the edges. By encoding factoids as (*subject*, *predicate*, *object*)
    triples, one can use the graph to answer questions about a missing element. For
    an example that combines transformers with knowledge graphs, see the [Haystack
    tutorials](https://oreil.ly/n7lZb). One more promising direction is *automatic
    question generation* as a way to do some form of unsupervised/weakly supervised
    training using unlabeled data or data augmentation. Two recent examples include
    the papers on the Probably Answered Questions (PAQ) benchmark and synthetic data
    augmentation for cross-lingual settings.^([18](ch07.xhtml#idm46238709514224))
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ve seen that in order to successfully use QA models for real-world
    use cases we need to apply a few tricks, such as implementing a fast retrieval
    pipeline to make predictions in near real time. Still, applying a QA model to
    a handful of preselected documents can take a couple of seconds on production
    hardware. Although this may not sound like much, imagine how different your experience
    would be if you had to wait a few seconds to get the results of a Google search—a
    few seconds of wait time can decide the fate of your transformer-powered application.
    In the next chapter we’ll have a look at a few methods to accelerate model predictions
    further.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.xhtml#idm46238714372976-marker)) Although, in this particular case,
    everyone agrees that Drop C is the best guitar tuning.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch07.xhtml#idm46238714344800-marker)) J. Bjerva et al., [“SubjQA: A Dataset
    for Subjectivity and Review Comprehension”](https://arxiv.org/abs/2004.14283),
    (2020).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.xhtml#idm46238714342272-marker)) As we’ll soon see, there are also
    *unanswerable* questions that are designed to produce more robust models.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch07.xhtml#idm46238714139488-marker)) D. Hendrycks et al., [“CUAD: An
    Expert-Annotated NLP Dataset for Legal Contract Review”](https://arxiv.org/abs/2103.06268),
    (2021).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch07.xhtml#idm46238713782624-marker)) P. Rajpurkar et al., [“SQuAD: 100,000+
    Questions for Machine Comprehension of Text”](https://arxiv.org/abs/1606.05250),
    (2016).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch07.xhtml#idm46238713780368-marker)) P. Rajpurkar, R. Jia, and P. Liang,
    [“Know What You Don’t Know: Unanswerable Questions for SQuAD”](https://arxiv.org/abs/1806.03822),
    (2018).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch07.xhtml#idm46238713770064-marker)) T. Kwiatkowski et al., “Natural
    Questions: A Benchmark for Question Answering Research,” *Transactions of the
    Association for Computational Linguistics* 7 (March 2019): 452–466, [*http://dx.doi.org/10.1162/tacl_a_00276*](http://dx.doi.org/10.1162/tacl_a_00276).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch07.xhtml#idm46238713719168-marker)) W. Wang et al., [“MINILM: Deep
    Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers”](https://arxiv.org/abs/2002.10957),
    (2020).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch07.xhtml#idm46238713601696-marker)) Note that the `token_type_ids` are
    not present in all transformer models. In the case of BERT-like models such as
    MiniLM, the `token_type_ids` are also used during pretraining to incorporate the
    next sentence prediction task.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch07.xhtml#idm46238713530720-marker)) See [Chapter 2](ch02.xhtml#chapter_classification)
    for details on how these hidden states can be extracted.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch07.xhtml#idm46238713054704-marker)) A vector is sparse if most of its
    elements are zero.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch07.xhtml#idm46238713004128-marker)) The guide also provides installation
    instructions for macOS and Windows.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch07.xhtml#idm46238712667392-marker)) For an in-depth explanation of
    document scoring with TF-IDF and BM25 see Chapter 23 of *Speech and Language Processing*,
    3rd edition, by D. Jurafsky and J.H. Martin (Prentice Hall).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch07.xhtml#idm46238711297696-marker)) V. Karpukhin et al., [“Dense Passage
    Retrieval for Open-Domain Question Answering”](https://arxiv.org/abs/2004.04906),
    (2020).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch07.xhtml#idm46238710735456-marker)) D. Yogatama et al., [“Learning
    and Evaluating General Linguistic Intelligence”](https://arXiv.org/abs/1901.11373),
    (2019).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch07.xhtml#idm46238709770880-marker)) P. Lewis et al., [“Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”](https://arxiv.org/abs/2005.11401),
    (2020).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch07.xhtml#idm46238709566048-marker)) A. Talmor et al., [“MultiModalQA:
    Complex Question Answering over Text, Tables and Images”](https://arxiv.org/abs/2104.06039),
    (2021).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '^([18](ch07.xhtml#idm46238709514224-marker)) P. Lewis et al., [“PAQ: 65 Million
    Probably-Asked Questions and What You Can Do with Them”](https://arxiv.org/abs/2102.07033),
    (2021); A. Riabi et al., [“Synthetic Data Augmentation for Zero-Shot Cross-Lingual
    Question Answering”](https://arxiv.org/abs/2010.12643), (2020).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
