- en: Chapter 7\. Question Answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you’re a researcher, analyst, or data scientist, chances are that at
    some point you’ve needed to wade through oceans of documents to find the information
    you’re looking for. To make matters worse, you’re constantly reminded by Google
    and Bing that there exist better ways to search! For instance, if we search for
    “When did Marie Curie win her first Nobel Prize?” on Google, we immediately get
    the correct answer of “1903,” as illustrated in [Figure 7-1](#marie-curie).
  prefs: []
  type: TYPE_NORMAL
- en: '![Marie Curie](Images/nlpt_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. A Google search query and corresponding answer snippet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this example, Google first retrieved around 319,000 documents that were relevant
    to the query, and then performed an additional processing step to extract the
    answer snippet with the corresponding passage and web page. It’s not hard to see
    why these answer snippets are useful. For example, if we search for a trickier
    question like “Which guitar tuning is the best?” Google doesn’t provide an answer,
    and instead we have to click on one of the web pages returned by the search engine
    to find it ourselves.^([1](ch07.xhtml#idm46238714372976))
  prefs: []
  type: TYPE_NORMAL
- en: 'The general approach behind this technology is called *question answering*
    (QA). There are many flavors of QA, but the most common is *extractive QA*, which
    involves questions whose answer can be identified as a span of text in a document,
    where the document might be a web page, legal contract, or news article. The two-stage
    process of first retrieving relevant documents and then extracting answers from
    them is also the basis for many modern QA systems, including semantic search engines,
    intelligent assistants, and automated information extractors. In this chapter,
    we’ll apply this process to tackle a common problem facing ecommerce websites:
    helping consumers answer specific queries to evaluate a product. We’ll see that
    customer reviews can be used as a rich and challenging source of information for
    QA, and along the way we’ll learn how transformers act as powerful *reading comprehension*
    models that can extract meaning from text. Let’s begin by fleshing out the use
    case.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This chapter focuses on extractive QA, but other forms of QA may be more suitable
    for your use case. For example, *community QA* involves gathering question-answer
    pairs that are generated by users on forums like [Stack Overflow](https://stackoverflow.com),
    and then using semantic similarity search to find the closest matching answer
    to a new question. There is also *long-form QA*, which aims to generate complex
    paragraph-length answers to open-ended questions like “Why is the sky blue?” Remarkably,
    it is also possible to do QA over tables, and transformer models like [TAPAS](https://oreil.ly/vVPWO)
    can even perform aggregations to produce the final answer!
  prefs: []
  type: TYPE_NORMAL
- en: Building a Review-Based QA System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve ever purchased a product online, you probably relied on customer reviews
    to help inform your decision. These reviews can often help answer specific questions
    like “Does this guitar come with a strap?” or “Can I use this camera at night?”
    that may be hard to answer from the product description alone. However, popular
    products can have hundreds to thousands of reviews, so it can be a major drag
    to find one that is relevant. One alternative is to post your question on the
    community QA platforms provided by websites like Amazon, but it usually takes
    days to get an answer (if you get one at all). Wouldn’t it be nice if we could
    get an immediate answer, like in the Google example from [Figure 7-1](#marie-curie)?
    Let’s see if we can do this using transformers!
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build our QA system we’ll use the SubjQA dataset,^([2](ch07.xhtml#idm46238714344800))
    which consists of more than 10,000 customer reviews in English about products
    and services in six domains: TripAdvisor, Restaurants, Movies, Books, Electronics,
    and Grocery. As illustrated in [Figure 7-2](#phone), each review is associated
    with a question that can be answered using one or more sentences from the review.^([3](ch07.xhtml#idm46238714342272))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Phone with Query](Images/nlpt_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. A question about a product and the corresponding review (the answer
    span is underlined)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The interesting aspect of this dataset is that most of the questions and answers
    are *subjective*; that is, they depend on the personal experience of the users.
    The example in [Figure 7-2](#phone) shows why this feature makes the task potentially
    more difficult than finding answers to factual questions like “What is the currency
    of the United Kingdom?” First, the query is about “poor quality,” which is subjective
    and depends on the user’s definition of quality. Second, important parts of the
    query do not appear in the review at all, which means it cannot be answered with
    shortcuts like keyword search or paraphrasing the input question. These features
    make SubjQA a realistic dataset to benchmark our review-based QA models on, since
    user-generated content like that shown in [Figure 7-2](#phone) resembles what
    we might encounter in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: QA systems are usually categorized by the *domain* of data that they have access
    to when responding to a query. *Closed-domain* QA deals with questions about a
    narrow topic (e.g., a single product category), while *open-domain* QA deals with
    questions about almost anything (e.g., Amazon’s whole product catalog). In general,
    closed-domain QA involves searching through fewer documents than the open-domain
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, let’s download the dataset from the [Hugging Face Hub](https://oreil.ly/iO0s5).
    As we did in [Chapter 4](ch04.xhtml#chapter_ner), we can use the `get_dataset_config_names()`
    function to find out which subsets are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For our use case, we’ll focus on building a QA system for the Electronics domain.
    To download the `electronics` subset, we just need to pass this value to the `name`
    argument of the `load_dataset()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Like other question answering datasets on the Hub, SubjQA stores the answers
    to each question as a nested dictionary. For example, if we inspect one of the
    rows in the `answers` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'we can see that the answers are stored in a `text` field, while the starting
    character indices are provided in `answer_start`. To explore the dataset more
    easily, we’ll flatten these nested columns with the `flatten()` method and convert
    each split to a Pandas `DataFrame` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the dataset is relatively small, with only 1,908 examples in total.
    This simulates a real-world scenario, since getting domain experts to label extractive
    QA datasets is labor-intensive and expensive. For example, the CUAD dataset for
    extractive QA on legal contracts is estimated to have a value of $2 million to
    account for the legal expertise needed to annotate its 13,000 examples!^([4](ch07.xhtml#idm46238714139488))
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few columns in the SubjQA dataset, but the most interesting
    ones for building our QA system are shown in [Table 7-1](#subjqa-columns).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Column names and their descriptions from the SubjQA dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `title` | The Amazon Standard Identification Number (ASIN) associated with
    each product |'
  prefs: []
  type: TYPE_TB
- en: '| `question` | The question |'
  prefs: []
  type: TYPE_TB
- en: '| `answers.answer_text` | The span of text in the review labeled by the annotator
    |'
  prefs: []
  type: TYPE_TB
- en: '| `answers.answer_start` | The start character index of the answer span |'
  prefs: []
  type: TYPE_TB
- en: '| `context` | The customer review |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s focus on these columns and take a look at a few of the training examples.
    We can use the `sample()` method to select a random sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '| title | question | answers.text | answers.answer_start | context |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| B005DKZTMG | Does the keyboard lightweight? | [this keyboard is compact]
    | [215] | I really like this keyboard. I give it 4 stars because it doesn’t have
    a CAPS LOCK key so I never know if my caps are on. But for the price, it really
    suffices as a wireless keyboard. I have very large hands and this keyboard is
    compact, but I have no complaints. |'
  prefs: []
  type: TYPE_TB
- en: '| B00AAIPT76 | How is the battery? | [] | [] | I bought this after the first
    spare gopro battery I bought wouldn’t hold a charge. I have very realistic expectations
    of this sort of product, I am skeptical of amazing stories of charge time and
    battery life but I do expect the batteries to hold a charge for a couple of weeks
    at least and for the charger to work like a charger. In this I was not disappointed.
    I am a river rafter and found that the gopro burns through power in a hurry so
    this purchase solved that issue. the batteries held a charge, on shorter trips
    the extra two batteries were enough and on longer trips I could use my friends
    JOOS Orange to recharge them.I just bought a newtrent xtreme powerpak and expect
    to be able to charge these with that so I will not run out of power again. |'
  prefs: []
  type: TYPE_TB
- en: 'From these examples we can make a few observations. First, the questions are
    not grammatically correct, which is quite common in the FAQ sections of ecommerce
    websites. Second, an empty `answers.text` entry denotes “unanswerable” questions
    whose answer cannot be found in the review. Finally, we can use the start index
    and length of the answer span to slice out the span of text in the review that
    corresponds to the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s get a feel for what types of questions are in the training set
    by counting the questions that begin with a few common starting words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_07in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that questions beginning with “How”, “What”, and “Is” are the most
    common ones, so let’s have a look at some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve explored our dataset a bit, let’s dive into understanding how
    transformers can extract answers from text.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Answers from Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we’ll need for our QA system is to find a way to identify a
    potential answer as a span of text in a customer review. For example, if a we
    have a question like “Is it waterproof?” and the review passage is “This watch
    is waterproof at 30m depth”, then the model should output “waterproof at 30m”.
    To do this we’ll need to understand how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Frame the supervised learning problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenize and encode text for QA tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deal with long passages that exceed a model’s maximum context size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by taking a look at how to frame the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Span classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common way to extract answers from text is by framing the problem as
    a *span classification* task, where the start and end tokens of an answer span
    act as the labels that a model needs to predict. This process is illustrated in
    [Figure 7-4](#qa-head).
  prefs: []
  type: TYPE_NORMAL
- en: '![QA Head](Images/nlpt_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. The span classification head for QA tasks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since our training set is relatively small, with only 1,295 examples, a good
    strategy is to start with a language model that has already been fine-tuned on
    a large-scale QA dataset like SQuAD. In general, these models have strong reading
    comprehension capabilities and serve as a good baseline upon which to build a
    more accurate system. This is a somewhat different approach to that taken in previous
    chapters, where we typically started with a pretrained model and fine-tuned the
    task-specific head ourselves. For example, in [Chapter 2](ch02.xhtml#chapter_classification),
    we had to fine-tune the classification head because the number of classes was
    tied to the dataset at hand. For extractive QA, we can actually start with a fine-tuned
    model since the structure of the labels remains the same across datasets.
  prefs: []
  type: TYPE_NORMAL
- en: You can find a list of extractive QA models by navigating to the [Hugging Face
    Hub](https://oreil.ly/dzCsC) and searching for “squad” on the Models tab ([Figure 7-5](#squad-models)).
  prefs: []
  type: TYPE_NORMAL
- en: '![SQuAD models](Images/nlpt_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. A selection of extractive QA models on the Hugging Face Hub
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, at the time of writing, there are more than 350 QA models to
    choose from—so which one should you pick? In general, the answer depends on various
    factors like whether your corpus is mono- or multilingual and the constraints
    of running the model in a production environment. [Table 7-2](#squad-models-table)
    lists a few models that provide a good foundation to build on.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. Baseline transformer models that are fine-tuned on SQuAD 2.0
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformer | Description | Number of parameters | *F*[1]-score on SQuAD
    2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MiniLM | A distilled version of BERT-base that preserves 99% of the performance
    while being twice as fast | 66M | 79.5 |'
  prefs: []
  type: TYPE_TB
- en: '| RoBERTa-base | RoBERTa models have better performance than their BERT counterparts
    and can be fine-tuned on most QA datasets using a single GPU | 125M | 83.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ALBERT-XXL | State-of-the-art performance on SQuAD 2.0, but computationally
    intensive and difficult to deploy | 235M | 88.1 |'
  prefs: []
  type: TYPE_TB
- en: '| XLM-RoBERTa-large | Multilingual model for 100 languages with strong zero-shot
    performance | 570M | 83.8 |'
  prefs: []
  type: TYPE_TB
- en: For the purposes of this chapter, we’ll use a fine-tuned MiniLM model since
    it is fast to train and will allow us to quickly iterate on the techniques that
    we’ll be exploring.^([8](ch07.xhtml#idm46238713719168)) As usual, the first thing
    we need is a tokenizer to encode our texts, so let’s take a look at how this works
    for QA tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing text for QA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To encode our texts, we’ll load the MiniLM model checkpoint from the [Hugging
    Face Hub](https://oreil.ly/df5Cu) as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the model in action, let’s first try to extract an answer from a short
    passage of text. In extractive QA tasks, the inputs are provided as (question,
    context) pairs, so we pass them both to the tokenizer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we’ve returned PyTorch `Tensor` objects, since we’ll need them to run
    the forward pass through the model. If we view the tokenized inputs as a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| input_ids | 101 | 2129 | 2172 | 2189 | 2064 | 2023 | ... | 5834 | 2006 |
    5371 | 2946 | 1012 | 102 |'
  prefs: []
  type: TYPE_TB
- en: '| token_type_ids | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| attention_mask | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: we can see the familiar `input_ids` and `attention_mask` tensors, while the
    `token_type_ids` tensor indicates which part of the inputs corresponds to the
    question and context (a 0 indicates a question token, a 1 indicates a context
    token).^([9](ch07.xhtml#idm46238713601696))
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how the tokenizer formats the inputs for QA tasks, let’s decode
    the `input_ids` tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that for each QA example, the inputs take the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'where the location of the first `[SEP]` token is determined by the `token_type_ids`.
    Now that our text is tokenized, we just need to instantiate the model with a QA
    head and run the inputs through the forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see that we get a `QuestionAnsweringModelOutput` object as the
    output of the QA head. As illustrated in [Figure 7-4](#qa-head), the QA head corresponds
    to a linear layer that takes the hidden states from the encoder and computes the
    logits for the start and end spans.^([10](ch07.xhtml#idm46238713530720)) This
    means that we treat QA as a form of token classification, similar to what we encountered
    for named entity recognition in [Chapter 4](ch04.xhtml#chapter_ner). To convert
    the outputs into an answer span, we first need to get the logits for the start
    and end tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we compare the shapes of these logits to the input IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: we see that there are two logits (a start and end) associated with each input
    token. As illustrated in [Figure 7-6](#qa-scores), larger, positive logits correspond
    to more likely candidates for the start and end tokens. In this example we can
    see that the model assigns the highest start token logits to the numbers “1” and
    “6000”, which makes sense since our question is asking about a quantity. Similarly,
    we see that the end tokens with the highest logits are “minute” and “hours”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/nlpt_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Predicted logits for the start and end tokens; the token with the
    highest score is colored in orange
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To get the final answer, we can compute the argmax over the start and end token
    logits and then slice the span from the inputs. The following code performs these
    steps and decodes the result so we can print the resulting text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, it worked! In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, all
    of these preprocessing and postprocessing steps are conveniently wrapped in a
    dedicated pipeline. We can instantiate the pipeline by passing our tokenizer and
    fine-tuned model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the answer, the pipeline also returns the model’s probability
    estimate in the `score` field (obtained by taking a softmax over the logits).
    This is handy when we want to compare multiple answers within a single context.
    We’ve also shown that we can have the model predict multiple answers by specifying
    the `topk` parameter. Sometimes, it is possible to have questions for which no
    answer is possible, like the empty `answers.answer_start` examples in SubjQA.
    In these cases the model will assign a high start and end score to the `[CLS]`
    token, and the pipeline maps this output to an empty string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In our simple example, we obtained the start and end indices by taking the argmax
    of the corresponding logits. However, this heuristic can produce out-of-scope
    answers by selecting tokens that belong to the question instead of the context.
    In practice, the pipeline computes the best combination of start and end indices
    subject to various constraints such as being in-scope, requiring the start indices
    to precede the end indices, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with long passages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One subtlety faced by reading comprehension models is that the context often
    contains more tokens than the maximum sequence length of the model (which is usually
    a few hundred tokens at most). As illustrated in [Figure 7-7](#subjqa-dist), a
    decent portion of the SubjQA training set contains question-context pairs that
    won’t fit within MiniLM’s context size of 512 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/nlpt_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Distribution of tokens for each question-context pair in the SubjQA
    training set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For other tasks, like text classification, we simply truncated long texts under
    the assumption that enough information was contained in the embedding of the `[CLS]`
    token to generate accurate predictions. For QA, however, this strategy is problematic
    because the answer to a question could lie near the end of the context and thus
    would be removed by truncation. As illustrated in [Figure 7-8](#sliding-window),
    the standard way to deal with this is to apply a *sliding window* across the inputs,
    where each window contains a passage of tokens that fit in the model’s context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sliding window](Images/nlpt_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. How the sliding window creates multiple question-context pairs
    for long documents—the first bar corresponds to the question, while the second
    bar is the context captured in each window
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we can set `return_overflowing_tokens=True`
    in the tokenizer to enable the sliding window. The size of the sliding window
    is controlled by the `max_seq_length` argument, and the size of the stride is
    controlled by `doc_stride`. Let’s grab the first example from our training set
    and define a small window to illustrate how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case we now get a list of `input_ids`, one for each window. Let’s check
    the number of tokens we have in each window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can see where two windows overlap by decoding the inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have some intuition about how QA models can extract answers from
    text, let’s look at the other components we need to build an end-to-end QA pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Using Haystack to Build a QA Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our simple answer extraction example, we provided both the question and the
    context to the model. However, in reality our system’s users will only provide
    a question about a product, so we need some way of selecting relevant passages
    from among all the reviews in our corpus. One way to do this would be to concatenate
    all the reviews of a given product together and feed them to the model as a single,
    long context. Although simple, the drawback of this approach is that the context
    can become extremely long and thereby introduce an unacceptable latency for our
    users’ queries. For example, let’s suppose that on average, each product has 30
    reviews and each review takes 100 milliseconds to process. If we need to process
    all the reviews to get an answer, this would result in an average latency of 3
    seconds per user query—much too long for ecommerce websites!
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle this, modern QA systems are typically based on the *retriever-reader*
    architecture, which has two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Retriever*'
  prefs: []
  type: TYPE_NORMAL
- en: Responsible for retrieving relevant documents for a given query. Retrievers
    are usually categorized as *sparse* or *dense*. Sparse retrievers use word frequencies
    to represent each document and query as a sparse vector.^([11](ch07.xhtml#idm46238713054704))
    The relevance of a query and a document is then determined by computing an inner
    product of the vectors. On the other hand, dense retrievers use encoders like
    transformers to represent the query and document as contextualized embeddings
    (which are dense vectors). These embeddings encode semantic meaning, and allow
    dense retrievers to improve search accuracy by understanding the content of the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: '*Reader*'
  prefs: []
  type: TYPE_NORMAL
- en: Responsible for extracting an answer from the documents provided by the retriever.
    The reader is usually a reading comprehension model, although at the end of the
    chapter we’ll see examples of models that can generate free-form answers.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 7-9](#retriever-reader), there can also be other components
    that apply post-processing to the documents fetched by the retriever or to the
    answers extracted by the reader. For example, the retrieved documents may need
    reranking to eliminate noisy or irrelevant ones that can confuse the reader. Similarly,
    postprocessing of the reader’s answers is often needed when the correct answer
    comes from various passages in a long document.
  prefs: []
  type: TYPE_NORMAL
- en: '![QA Architecture](Images/nlpt_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. The retriever-reader architecture for modern QA systems
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To build our QA system, we’ll use the [*Haystack* library](https://haystack.deepset.ai)
    developed by [deepset](https://deepset.ai), a German company focused on NLP. Haystack
    is based on the retriever-reader architecture, abstracts much of the complexity
    involved in building these systems, and integrates tightly with ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the retriever and reader, there are two more components involved
    when building a QA pipeline with Haystack:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Document store*'
  prefs: []
  type: TYPE_NORMAL
- en: A document-oriented database that stores documents and metadata which are provided
    to the retriever at query time
  prefs: []
  type: TYPE_NORMAL
- en: '*Pipeline*'
  prefs: []
  type: TYPE_NORMAL
- en: Combines all the components of a QA system to enable custom query flows, merging
    documents from multiple retrievers, and more
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll look at how we can use these components to quickly build
    a prototype QA pipeline. Later, we’ll examine how we can improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This chapter was written using version 0.9.0 of the Haystack library. In [version
    0.10.0](https://oreil.ly/qbqgv), the pipeline and evaluation APIs were redesigned
    to make it easier to inspect whether the retriever or reader are impacting performance.
    To see what this chapter’s code looks like with the new API, check out the [GitHub
    repository](https://github.com/nlp-with-transformers/notebooks).
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a document store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Haystack, there are various document stores to choose from and each one can
    be paired with a dedicated set of retrievers. This is illustrated in [Table 7-3](#doc-stores),
    where the compatibility of sparse (TF-IDF, BM25) and dense (Embedding, DPR) retrievers
    is shown for each of the available document stores. We’ll explain what all these
    acronyms mean later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-3\. Compatibility of Haystack retrievers and document stores
  prefs: []
  type: TYPE_NORMAL
- en: '|  | In memory | Elasticsearch | FAISS | Milvus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TF-IDF | Yes | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| BM25 | No | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding | Yes | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| DPR | Yes | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: Since we’ll be exploring both sparse and dense retrievers in this chapter, we’ll
    use the `ElasticsearchDocumentStore`, which is compatible with both retriever
    types. Elasticsearch is a search engine that is capable of handling a diverse
    range of data types, including textual, numerical, geospatial, structured, and
    unstructured. Its ability to store huge volumes of data and quickly filter it
    with full-text search features makes it especially well suited for developing
    QA systems. It also has the advantage of being the industry standard for infrastructure
    analytics, so there’s a good chance your company already has a cluster that you
    can work with.
  prefs: []
  type: TYPE_NORMAL
- en: 'To initialize the document store, we first need to download and install Elasticsearch.
    By following Elasticsearch’s [guide](https://oreil.ly/bgmKq),^([12](ch07.xhtml#idm46238713004128))
    we can grab the latest release for Linux with `wget` and unpack it with the `tar`
    shell command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we need to start the Elasticsearch server. Since we’re running all the
    code in this book within Jupyter notebooks, we’ll need to use Python’s `Popen()`
    function to spawn a new process. While we’re at it, let’s also run the subprocess
    in the background using the `chown` shell command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `Popen()` function, the `args` specify the program we wish to execute,
    while `stdout=PIPE` creates a new pipe for the standard output and `stderr=STDOUT`
    collects the errors in the same pipe. The `preexec_fn` argument specifies the
    ID of the subprocess we wish to use. By default, Elasticsearch runs locally on
    port 9200, so we can test the connection by sending an HTTP request to `localhost`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our Elasticsearch server is up and running, the next thing to do is
    instantiate the document store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, `ElasticsearchDocumentStore` creates two indices on Elasticsearch:
    one called `document` for (you guessed it) storing documents, and another called
    `label` for storing the annotated answer spans. For now, we’ll just populate the
    `document` index with the SubjQA reviews, and Haystack’s document stores expect
    a list of dictionaries with `text` and `meta` keys as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The fields in `meta` can be used for applying filters during retrieval. For
    our purposes we’ll include the `item_id` and `q_review_id` columns of SubjQA so
    we can filter by product and question ID, along with the corresponding training
    split. We can then loop through the examples in each `DataFrame` and add them
    to the index with the `write_documents()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Great, we’ve loaded all our reviews into an index! To search the index we’ll
    need a retriever, so let’s look at how we can initialize one for Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a retriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Elasticsearch document store can be paired with any of the Haystack retrievers,
    so let’s start by using a sparse retriever based on BM25 (short for “Best Match
    25”). BM25 is an improved version of the classic Term Frequency-Inverse Document
    Frequency (TF-IDF) algorithm and represents the question and context as sparse
    vectors that can be searched efficiently on Elasticsearch. The BM25 score measures
    how much matched text is about a search query and improves on TF-IDF by saturating
    TF values quickly and normalizing the document length so that short documents
    are favored over long ones.^([13](ch07.xhtml#idm46238712667392))
  prefs: []
  type: TYPE_NORMAL
- en: 'In Haystack, the BM25 retriever is used by default in `ElasticsearchRetriever`,
    so let’s initialize this class by specifying the document store we wish to search
    over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s look at a simple query for a single electronics product in the
    training set. For review-based QA systems like ours, it’s important to restrict
    the queries to a single item because otherwise the retriever would source reviews
    about products that are not related to a user’s query. For example, asking “Is
    the camera quality any good?” without a product filter could return reviews about
    phones, when the user might be asking about a specific laptop camera instead.
    By themselves, the ASIN values in our dataset are a bit cryptic, but we can decipher
    them with online tools like [*amazon ASIN*](https://amazon-asin.com) or by simply
    appending the value of `item_id` to the *www.amazon.com/dp/* URL. The following
    item ID corresponds to one of Amazon’s Fire tablets, so let’s use the retriever’s
    `retrieve()` method to ask if it’s any good for reading with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we’ve specified how many documents to return with the `top_k` argument
    and applied a filter on both the `item_id` and `split` keys that were included
    in the `meta` field of our documents. Each element of `retrieved_docs` is a Haystack
    `Document` object that is used to represent documents and includes the retriever’s
    query score along with other metadata. Let’s have a look at one of the retrieved
    documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the document’s text, we can see the `score` that Elasticsearch
    computed for its relevance to the query (larger scores imply a better match).
    Under the hood, Elasticsearch relies on [Lucene](https://lucene.apache.org) for
    indexing and search, so by default it uses Lucene’s *practical scoring function*.
    You can find the nitty-gritty details behind the scoring function in the [Elasticsearch
    documentation](https://oreil.ly/b1Seu), but in brief terms it first filters the
    candidate documents by applying a Boolean test (does the document match the query?),
    and then applies a similarity metric that’s based on representing both the document
    and the query as vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a way to retrieve relevant documents, the next thing we need
    is a way to extract answers from them. This is where the reader comes in, so let’s
    take a look at how we can load our MiniLM model in Haystack.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a reader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Haystack, there are two types of readers one can use to extract answers
    from a given context:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FARMReader`'
  prefs: []
  type: TYPE_NORMAL
- en: Based on deepset’s [*FARM* framework](https://farm.deepset.ai) for fine-tuning
    and deploying transformers. Compatible with models trained using ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers and can load models directly from the Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '`TransformersReader`'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the QA pipeline from ![nlpt_pin01](Images/nlpt_pin01.png) Transformers.
    Suitable for running inference only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although both readers handle a model’s weights in the same way, there are some
    differences in the way the predictions are converted to produce answers:'
  prefs: []
  type: TYPE_NORMAL
- en: In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, the QA pipeline normalizes
    the start and end logits with a softmax in each passage. This means that it is
    only meaningful to compare answer scores between answers extracted from the same
    passage, where the probabilities sum to 1\. For example, an answer score of 0.9
    from one passage is not necessarily better than a score of 0.8 in another. In
    FARM, the logits are not normalized, so inter-passage answers can be compared
    more easily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `TransformersReader` sometimes predicts the same answer twice, but with
    different scores. This can happen in long contexts if the answer lies across two
    overlapping windows. In FARM, these duplicates are removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we will be fine-tuning the reader later in the chapter, we’ll use the
    `FARMReader`. As with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, to load
    the model we just need to specify the MiniLM checkpoint on the Hugging Face Hub
    along with some QA-specific arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also possible to fine-tune a reading comprehension model directly in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers and then load it in `TransformersReader` to run inference. For details
    on how to do the fine-tuning step, see the question answering tutorial in the
    [library’s documentation](https://oreil.ly/VkhIQ).
  prefs: []
  type: TYPE_NORMAL
- en: 'In `FARMReader`, the behavior of the sliding window is controlled by the same
    `max_seq_length` and `doc_stride` arguments that we saw for the tokenizer. Here
    we’ve used the values from the MiniLM paper. To confirm, let’s now test the reader
    on our simple example from earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Great, the reader appears to be working as expected—so next, let’s tie together
    all our components using one of Haystack’s pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Haystack provides a `Pipeline` abstraction that allows us to combine retrievers,
    readers, and other components together as a graph that can be easily customized
    for each use case. There are also predefined pipelines analogous to those in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, but specialized for QA systems. In our case, we’re interested in
    extracting answers, so we’ll use the `ExtractiveQAPipeline`, which takes a single
    retriever-reader pair as its arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Each `Pipeline` has a `run()` method that specifies how the query flow should
    be executed. For the `ExtractiveQAPipeline` we just need to pass the `query`,
    the number of documents to retrieve with `top_k_retriever`, and the number of
    answers to extract from these documents with `top_k_reader`. In our case, we also
    need to specify a filter over the item ID, which can be done using the `filters`
    argument as we did with the retriever earlier. Let’s run a simple example using
    our question about the Amazon Fire tablet again, but this time returning the extracted
    answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Great, we now have an end-to-end QA system for Amazon product reviews! This
    is a good start, but notice that the second and third answers are closer to what
    the question is actually asking. To do better, we’ll need some metrics to quantify
    the performance of the retriever and reader. We’ll take a look at that next.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Our QA Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although much of the recent research on QA has focused on improving reading
    comprehension models, in practice it doesn’t matter how good your reader is if
    the retriever can’t find the relevant documents in the first place! In particular,
    the retriever sets an upper bound on the performance of the whole QA system, so
    it’s important to make sure it’s doing a good job. With this in mind, let’s start
    by introducing some common metrics to evaluate the retriever so that we can compare
    the performance of sparse and dense representations.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Retriever
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common metric for evaluating retrievers is *recall*, which measures the fraction
    of all relevant documents that are retrieved. In this context, “relevant” simply
    means whether the answer is present in a passage of text or not, so given a set
    of questions, we can compute recall by counting the number of times an answer
    appears in the top *k* documents returned by the retriever.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Haystack, there are two ways to evaluate retrievers:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the retriever’s in-built `eval()` method. This can be used for both open-
    and closed-domain QA, but not for datasets like SubjQA where each document is
    paired with a single product and we need to filter by product ID for every query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a custom `Pipeline` that combines a retriever with the `EvalRetriever`
    class. This enables the implementation of custom metrics and query flows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A complementary metric to recall is *mean average precision* (mAP), which rewards
    retrievers that can place the correct answers higher up in the document ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we need to evaluate the recall per product and then aggregate across
    all products, we’ll opt for the second approach. Each node in the `Pipeline` graph
    represents a class that takes some inputs and produces some outputs via a `run()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here `kwargs` corresponds to the outputs from the previous node in the graph,
    which is manipulated within the `run()` method to return a tuple of the outputs
    for the next node, along with a name for the outgoing edge. The only other requirement
    is to include an `outgoing_edges` attribute that indicates the number of outputs
    from the node (in most cases `outgoing_edges=1`, unless you have branches in the
    pipeline that route the inputs according to some criterion).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we need a node to evaluate the retriever, so we’ll use the `EvalRetriever`
    class whose `run()` method keeps track of which documents have answers that match
    the ground truth. With this class we can then build up a `Pipeline` graph by adding
    the evaluation node after a node that represents the retriever itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Notice that each node is given a `name` and a list of `inputs`. In most cases,
    each node has a single outgoing edge, so we just need to include the name of the
    previous node in `inputs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our evaluation pipeline, we need to pass some queries and
    their corresponding answers. To do this, we’ll add the answers to a dedicated
    `label` index on our document store. Haystack provides a `Label` object that represents
    the answer spans and their metadata in a standardized fashion. To populate the
    `label` index, we’ll first create a list of `Label` objects by looping over each
    question in the test set and extracting the matching answers and additional metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If we peek at one of these labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'we can see the question-answer pair, along with an `origin` field that contains
    the unique question ID so we can filter the document store per question. We’ve
    also added the product ID to the `meta` field so we can filter the labels by product.
    Now that we have our labels, we can write them to the `label` index on Elasticsearch
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to build up a mapping between our question IDs and corresponding
    answers that we can pass to the pipeline. To get all the labels, we can use the
    `get_all_labels_aggregated()` method from the document store that will aggregate
    all question-answer pairs associated with a unique ID. This method returns a list
    of `MultiLabel` objects, but in our case we only get one element since we’re filtering
    by question ID. We can build up a list of aggregated labels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'By peeking at one of these labels we can see that all the answers associated
    with a given question are aggregated together in a `multiple_answers` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have all the ingredients for evaluating the retriever, so let’s define
    a function that feeds each question-answer pair associated with each product to
    the evaluation pipeline and tracks the correct retrievals in our `pipe` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, it works! Notice that we picked a specific value for `top_k_retriever`
    to specify the number of documents to retrieve. In general, increasing this parameter
    will improve the recall, but at the expense of providing more documents to the
    reader and slowing down the end-to-end pipeline. To guide our decision on which
    value to pick, we’ll create a function that loops over several *k* values and
    compute the recall across the whole test set for each *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot the results, we can see how the recall improves as we increase *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_07in02.png)'
  prefs: []
  type: TYPE_IMG
- en: From the plot, we can see that there’s an inflection point around <math alttext="k
    equals 5"><mrow><mi>k</mi> <mo>=</mo> <mn>5</mn></mrow></math> and we get almost
    perfect recall from <math alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>
    onwards. Let’s now take a look at retrieving documents with dense vector techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Dense Passage Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve seen that we get almost perfect recall when our sparse retriever returns
    <math alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>
    documents, but can we do better at smaller values of *k*? The advantage of doing
    so is that we can pass fewer documents to the reader and thereby reduce the overall
    latency of our QA pipeline. A well-known limitation of sparse retrievers like
    BM25 is that they can fail to capture the relevant documents if the user query
    contains terms that don’t match exactly those of the review. One promising alternative
    is to use dense embeddings to represent the question and document, and the current
    state of the art is an architecture known as *Dense Passage Retrieval* (DPR).^([14](ch07.xhtml#idm46238711297696))
    The main idea behind DPR is to use two BERT models as encoders for the question
    and the passage. As illustrated in [Figure 7-10](#dpr), these encoders map the
    input text into a *d*-dimensional vector representation of the `[CLS]` token.
  prefs: []
  type: TYPE_NORMAL
- en: '![DPR Architecture](Images/nlpt_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. DPR’s bi-encoder architecture for computing the relevance of a
    document and query
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Haystack, we can initialize a retriever for DPR in a similar way to what
    we did for BM25\. In addition to specifying the document store, we also need to
    pick the BERT encoders for the question and passage. These encoders are trained
    by giving them questions with relevant (positive) passages and irrelevant (negative)
    passages, where the goal is to learn that relevant question-passage pairs have
    a higher similarity. For our use case, we’ll use encoders that have been fine-tuned
    on the NQ corpus in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we’ve also set `embed_title=False` since concatenating the document’s
    title (i.e., `item_id`) doesn’t provide any additional information because we
    filter per product. Once we’ve initialized the dense retriever, the next step
    is to iterate over all the indexed documents in our Elasticsearch index and apply
    the encoders to update the embedding representation. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re now set to go! We can evaluate the dense retriever in the same way we
    did for BM25 and compare the top-*k* recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_07in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we can see that DPR does not provide a boost in recall over BM25 and saturates
    around <math alttext="k equals 3"><mrow><mi>k</mi> <mo>=</mo> <mn>3</mn></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Performing similarity search of the embeddings can be sped up by using Facebook’s
    [FAISS library](https://oreil.ly/1E8Z0) as the document store. Similarly, the
    performance of the DPR retriever can be improved by fine-tuning on the target
    domain. If you’d like to learn how to fine-tune DPR, check out the Haystack [tutorial](https://oreil.ly/eXyro).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored the evaluation of the retriever, let’s turn to evaluating
    the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Reader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In extractive QA, there are two main metrics that are used for evaluating readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Exact Match (EM)*'
  prefs: []
  type: TYPE_NORMAL
- en: A binary metric that gives EM = 1 if the characters in the predicted and ground
    truth answers match exactly, and EM = 0 otherwise. If no answer is expected, the
    model gets EM = 0 if it predicts any text at all.
  prefs: []
  type: TYPE_NORMAL
- en: '*F*[1]-score'
  prefs: []
  type: TYPE_NORMAL
- en: Measures the harmonic mean of the precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how these metrics work by importing some helper functions from FARM
    and applying them to a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the hood, these functions first normalize the prediction and label by
    removing punctuation, fixing whitespace, and converting to lowercase. The normalized
    strings are then tokenized as a bag-of-words, before finally computing the metric
    at the token level. From this simple example we can see that EM is a much stricter
    metric than the *F*[1]-score: adding a single token to the prediction gives an
    EM of zero. On the other hand, the *F*[1]-score can fail to catch truly incorrect
    answers. For example, if our predicted answer span is “about 6000 dollars”, then
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Relying on just the *F*[1]-score is thus misleading, and tracking both metrics
    is a good strategy to balance the trade-off between underestimating (EM) and overestimating
    (*F*[1]-score) model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now in general, there are multiple valid answers per question, so these metrics
    are calculated for each question-answer pair in the evaluation set, and the best
    score is selected over all possible answers. The overall EM and *F*[1] scores
    for the model are then obtained by averaging over the individual scores of each
    question-answer pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the reader we’ll create a new pipeline with two nodes: a reader
    node and a node to evaluate the reader. We’ll use the `EvalReader` class that
    takes the predictions from the reader and computes the corresponding EM and *F*[1]
    scores. To compare with the SQuAD evaluation, we’ll take the best answers for
    each query with the `top_1_em` and `top_1_f1` metrics that are stored in `EvalAnswers`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we specified `skip_incorrect_retrieval=False`. This is to ensure
    that the retriever always passes the context to the reader (as in the SQuAD evaluation).
    Now that we’ve run every question through the reader, let’s print the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_07in04.png)'
  prefs: []
  type: TYPE_IMG
- en: OK, it seems that the fine-tuned model performs significantly worse on SubjQA
    than on SQuAD 2.0, where MiniLM achieves EM and *F*[1] scores of 76.1 and 79.5,
    respectively. One reason for the performance drop is that customer reviews are
    quite different from the Wikipedia articles the SQuAD 2.0 dataset is generated
    from, and the language they use is often informal. Another factor is likely the
    inherent subjectivity of our dataset, where both questions and answers differ
    from the factual information contained in Wikipedia. Let’s look at how to fine-tune
    a model on a dataset to get better results with domain adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Domain Adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although models that are fine-tuned on SQuAD will often generalize well to other
    domains, we’ve seen that for SubjQA the EM and *F*[1] scores of our model were
    much worse than for SQuAD. This failure to generalize has also been observed in
    other extractive QA datasets and is understood as evidence that transformer models
    are particularly adept at overfitting to SQuAD.^([15](ch07.xhtml#idm46238710735456))
    The most straightforward way to improve the reader is by fine-tuning our MiniLM
    model further on the SubjQA training set. The `FARMReader` has a `train()` method
    that is designed for this purpose and expects the data to be in SQuAD JSON format,
    where all the question-answer pairs are grouped together for each item as illustrated
    in [Figure 7-11](#squad-schema).
  prefs: []
  type: TYPE_NORMAL
- en: '![SQuAD Schema](Images/nlpt_0711.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-11\. Visualization of the SQuAD JSON format
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is quite a complex data format, so we’ll need a few functions and some
    Pandas magic to help us do the conversion. The first thing we need to do is implement
    a function that can create the `paragraphs` array associated with each product
    ID. Each element in this array contains a single context (i.e., review) and a
    `qas` array of question-answer pairs. Here’s a function that builds up the `paragraphs`
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we apply to the rows of a `DataFrame` associated with a single product
    ID, we get the SQuAD format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to then apply this function to each product ID in the `DataFrame`
    of each split. The following `convert_to_squad()` function does this trick and
    stores the result in an *electronics-{split}.json* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the splits in the right format, let’s fine-tune our reader
    by specifying the locations of the train and dev splits, along with where to save
    the fine-tuned model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'With the reader fine-tuned, let’s now compare its performance on the test set
    against our baseline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_07in05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Wow, domain adaptation has increased our EM score by a factor of six and more
    than doubled the *F*[1]-score! At this point, you might be wondering why we didn’t
    just fine-tune a pretrained language model directly on the SubjQA training set.
    One reason is that we only have 1,295 training examples in SubjQA while SQuAD
    has over 100,000, so we might run into challenges with overfitting. Nevertheless,
    let’s take a look at what naive fine-tuning produces. For a fair comparison, we’ll
    use the same language model that was used for fine-tuning our baseline on SQuAD.
    As before, we’ll load up the model with the `FARMReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we fine-tune for one epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'and include the evaluation on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_07in06.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that fine-tuning the language model directly on SubjQA results in
    considerably worse performance than fine-tuning on SQuAD and SubjQA.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When dealing with small datasets, it is best practice to use cross-validation
    when evaluating transformers as they can be prone to overfitting. You can find
    an example of how to perform cross-validation with SQuAD-formatted datasets in
    the [FARM repository](https://oreil.ly/K3nK8).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Whole QA Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve seen how to evaluate the reader and retriever components individually,
    let’s tie them together to measure the overall performance of our pipeline. To
    do so, we’ll need to augment our retriever pipeline with nodes for the reader
    and its evaluation. We’ve seen that we get almost perfect recall at <math alttext="k
    equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math> , so we can fix
    this value and assess the impact this has on the reader’s performance (since it
    will now receive multiple contexts per query compared to the SQuAD-style evaluation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: We can then compare the top 1 EM and *F*[1] scores for the model to predict
    an answer in the documents returned by the retriever in [Figure 7-12](#reader-vs-pipeline).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/nlpt_0712.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-12\. Comparison of EM and *F*[1] scores for the reader against the
    whole QA pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this plot we can see the effect that the retriever has on the overall performance.
    In particular, there is an overall degradation compared to matching the question-context
    pairs, as is done in the SQuAD-style evaluation. This can be circumvented by increasing
    the number of possible answers that the reader is allowed to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Until now we have only extracted answer spans from the context, but in general
    it could be that bits and pieces of the answer are scattered throughout the document
    and we would like our model to synthesize these fragments into a single coherent
    answer. Let’s have a look at how we can use generative QA to succeed at this task.
  prefs: []
  type: TYPE_NORMAL
- en: Going Beyond Extractive QA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One interesting alternative to extracting answers as spans of text in a document
    is to generate them with a pretrained language model. This approach is often referred
    to as *abstractive* or *generative QA* and has the potential to produce better-phrased
    answers that synthesize evidence across multiple passages. Although less mature
    than extractive QA, this is a fast-moving field of research, so chances are that
    these approaches will be widely adopted in industry by the time you are reading
    this! In this section we’ll briefly touch on the current state of the art: *retrieval-augmented
    generation* (RAG).^([16](ch07.xhtml#idm46238709770880))'
  prefs: []
  type: TYPE_NORMAL
- en: RAG extends the classic retriever-reader architecture that we’ve seen in this
    chapter by swapping the reader for a *generator* and using DPR as the retriever.
    The generator is a pretrained sequence-to-sequence transformer like T5 or BART
    that receives latent vectors of documents from DPR and then iteratively generates
    an answer based on the query and these documents. Since DPR and the generator
    are differentiable, the whole process can be fine-tuned end-to-end as illustrated
    in [Figure 7-13](#rag).
  prefs: []
  type: TYPE_NORMAL
- en: '![RAG Architecture](Images/nlpt_0713.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-13\. The RAG architecture for fine-tuning a retriever and generator
    end-to-end (courtesy of Ethan Perez)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To show RAG in action we’ll use the `DPRetriever` from earlier, so we just
    need to instantiate a generator. There are two types of RAG models to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '*RAG-Sequence*'
  prefs: []
  type: TYPE_NORMAL
- en: Uses the same retrieved document to generate the complete answer. In particular,
    the top *k* documents from the retriever are fed to the generator, which produces
    an output sequence for each document, and the result is marginalized to obtain
    the best answer.
  prefs: []
  type: TYPE_NORMAL
- en: '*RAG-Token*'
  prefs: []
  type: TYPE_NORMAL
- en: Can use a different document to generate each token in the answer. This allows
    the generator to synthesize evidence from multiple documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since RAG-Token models tend to perform better than RAG-Sequence ones, we’ll
    use the token model that was fine-tuned on NQ as our generator. Instantiating
    a generator in Haystack is similar to instantiating the reader, but instead of
    specifying the `max_seq_length` and `doc_stride` parameters for a sliding window
    over the contexts, we specify hyperparameters that control the text generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Here `num_beams` specifies the number of beams to use in beam search (text generation
    is covered at length in [Chapter 5](ch05.xhtml#chapter_generation)). As we did
    with the DPR retriever, we don’t embed the document titles since our corpus is
    always filtered per product ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing to do is tie together the retriever and generator using Haystack’s
    `GenerativeQAPipeline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In RAG, both the query encoder and the generator are trained end-to-end, while
    the context encoder is frozen. In Haystack, the `GenerativeQAPipeline` uses the
    query encoder from `RAGenerator` and the context encoder from `DensePassageRetriever`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now give RAG a spin by feeding in some queries about the Amazon Fire
    tablet from before. To simplify the querying, we’ll write a simple function that
    takes the query and prints out the top answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, now we’re ready to give it a test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'This result isn’t too bad for an answer, but it does suggest that the subjective
    nature of the question is confusing the generator. Let’s try with something a
    bit more factual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: This is more sensible! To get better results we could fine-tune RAG end-to-end
    on SubjQA; we’ll leave this as an exercise, but if you’re interested in exploring
    it there are scripts in the ![nlpt_pin01](Images/nlpt_pin01.png) [Transformers
    repository](https://oreil.ly/oZz4S) to help you get started.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, that was a whirlwind tour of QA, and you probably have many more questions
    that you’d like answered (pun intended!). In this chapter, we discussed two approaches
    to QA (extractive and generative) and examined two different retrieval algorithms
    (BM25 and DPR). Along the way, we saw that domain adaptation can be a simple technique
    to boost the performance of our QA system by a significant margin, and we looked
    at a few of the most common metrics that are used for evaluating such systems.
    Although we focused on closed-domain QA (i.e., a single domain of electronic products),
    the techniques in this chapter can easily be generalized to the open-domain case;
    we recommend reading Cloudera’s excellent Fast Forward [QA series](https://oreil.ly/Fd6lc)
    to see what’s involved.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying QA systems in the wild can be a tricky business to get right, and
    our experience is that a significant part of the value comes from first providing
    end users with useful search capabilities, followed by an extractive component.
    In this respect, the reader can be used in novel ways beyond answering on-demand
    user queries. For example, researchers at [Grid Dynamics](https://oreil.ly/CGLh1)
    were able to use their reader to automatically extract a set of pros and cons
    for each product in a client’s catalog. They also showed that a reader can be
    used to extract named entities in a zero-shot fashion by creating queries like
    “What kind of camera?” Given its infancy and subtle failure modes, we recommend
    exploring generative QA only once the other two approaches have been exhausted.
    This “hierarchy of needs” for tackling QA problems is illustrated in [Figure 7-14](#qa-pyramid).
  prefs: []
  type: TYPE_NORMAL
- en: '![QA Pyramid](Images/nlpt_0714.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-14\. The QA hierarchy of needs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking ahead, one exciting research area is *multimodal QA*, which involves
    QA over multiple modalities like text, tables, and images. As described in the
    MultiModalQA benchmark,^([17](ch07.xhtml#idm46238709566048)) such systems could
    enable users to answer complex questions that integrate information across different
    modalities, like “When was the famous painting with two touching fingers completed?”
    Another area with practical business applications is QA over a *knowledge graph*,
    where the nodes of the graph correspond to real-world entities and their relations
    are defined by the edges. By encoding factoids as (*subject*, *predicate*, *object*)
    triples, one can use the graph to answer questions about a missing element. For
    an example that combines transformers with knowledge graphs, see the [Haystack
    tutorials](https://oreil.ly/n7lZb). One more promising direction is *automatic
    question generation* as a way to do some form of unsupervised/weakly supervised
    training using unlabeled data or data augmentation. Two recent examples include
    the papers on the Probably Answered Questions (PAQ) benchmark and synthetic data
    augmentation for cross-lingual settings.^([18](ch07.xhtml#idm46238709514224))
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ve seen that in order to successfully use QA models for real-world
    use cases we need to apply a few tricks, such as implementing a fast retrieval
    pipeline to make predictions in near real time. Still, applying a QA model to
    a handful of preselected documents can take a couple of seconds on production
    hardware. Although this may not sound like much, imagine how different your experience
    would be if you had to wait a few seconds to get the results of a Google search—a
    few seconds of wait time can decide the fate of your transformer-powered application.
    In the next chapter we’ll have a look at a few methods to accelerate model predictions
    further.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.xhtml#idm46238714372976-marker)) Although, in this particular case,
    everyone agrees that Drop C is the best guitar tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch07.xhtml#idm46238714344800-marker)) J. Bjerva et al., [“SubjQA: A Dataset
    for Subjectivity and Review Comprehension”](https://arxiv.org/abs/2004.14283),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.xhtml#idm46238714342272-marker)) As we’ll soon see, there are also
    *unanswerable* questions that are designed to produce more robust models.
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch07.xhtml#idm46238714139488-marker)) D. Hendrycks et al., [“CUAD: An
    Expert-Annotated NLP Dataset for Legal Contract Review”](https://arxiv.org/abs/2103.06268),
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch07.xhtml#idm46238713782624-marker)) P. Rajpurkar et al., [“SQuAD: 100,000+
    Questions for Machine Comprehension of Text”](https://arxiv.org/abs/1606.05250),
    (2016).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch07.xhtml#idm46238713780368-marker)) P. Rajpurkar, R. Jia, and P. Liang,
    [“Know What You Don’t Know: Unanswerable Questions for SQuAD”](https://arxiv.org/abs/1806.03822),
    (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch07.xhtml#idm46238713770064-marker)) T. Kwiatkowski et al., “Natural
    Questions: A Benchmark for Question Answering Research,” *Transactions of the
    Association for Computational Linguistics* 7 (March 2019): 452–466, [*http://dx.doi.org/10.1162/tacl_a_00276*](http://dx.doi.org/10.1162/tacl_a_00276).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch07.xhtml#idm46238713719168-marker)) W. Wang et al., [“MINILM: Deep
    Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers”](https://arxiv.org/abs/2002.10957),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch07.xhtml#idm46238713601696-marker)) Note that the `token_type_ids` are
    not present in all transformer models. In the case of BERT-like models such as
    MiniLM, the `token_type_ids` are also used during pretraining to incorporate the
    next sentence prediction task.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch07.xhtml#idm46238713530720-marker)) See [Chapter 2](ch02.xhtml#chapter_classification)
    for details on how these hidden states can be extracted.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch07.xhtml#idm46238713054704-marker)) A vector is sparse if most of its
    elements are zero.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch07.xhtml#idm46238713004128-marker)) The guide also provides installation
    instructions for macOS and Windows.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch07.xhtml#idm46238712667392-marker)) For an in-depth explanation of
    document scoring with TF-IDF and BM25 see Chapter 23 of *Speech and Language Processing*,
    3rd edition, by D. Jurafsky and J.H. Martin (Prentice Hall).
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch07.xhtml#idm46238711297696-marker)) V. Karpukhin et al., [“Dense Passage
    Retrieval for Open-Domain Question Answering”](https://arxiv.org/abs/2004.04906),
    (2020).
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch07.xhtml#idm46238710735456-marker)) D. Yogatama et al., [“Learning
    and Evaluating General Linguistic Intelligence”](https://arXiv.org/abs/1901.11373),
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch07.xhtml#idm46238709770880-marker)) P. Lewis et al., [“Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”](https://arxiv.org/abs/2005.11401),
    (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch07.xhtml#idm46238709566048-marker)) A. Talmor et al., [“MultiModalQA:
    Complex Question Answering over Text, Tables and Images”](https://arxiv.org/abs/2104.06039),
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([18](ch07.xhtml#idm46238709514224-marker)) P. Lewis et al., [“PAQ: 65 Million
    Probably-Asked Questions and What You Can Do with Them”](https://arxiv.org/abs/2102.07033),
    (2021); A. Riabi et al., [“Synthetic Data Augmentation for Zero-Shot Cross-Lingual
    Question Answering”](https://arxiv.org/abs/2010.12643), (2020).'
  prefs: []
  type: TYPE_NORMAL
