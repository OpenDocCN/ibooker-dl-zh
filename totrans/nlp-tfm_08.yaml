- en: Chapter 7\. Question Answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 问答
- en: Whether you’re a researcher, analyst, or data scientist, chances are that at
    some point you’ve needed to wade through oceans of documents to find the information
    you’re looking for. To make matters worse, you’re constantly reminded by Google
    and Bing that there exist better ways to search! For instance, if we search for
    “When did Marie Curie win her first Nobel Prize?” on Google, we immediately get
    the correct answer of “1903,” as illustrated in [Figure 7-1](#marie-curie).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是研究人员、分析师还是数据科学家，都有可能在某个时候需要浏览大量文档以找到您正在寻找的信息。更糟糕的是，您不断地被谷歌和必应提醒，存在更好的搜索方式！例如，如果我们在谷歌上搜索“玛丽·居里何时获得她的第一个诺贝尔奖？”我们立即得到了“1903”这个正确答案，如[图7-1](#marie-curie)所示。
- en: '![Marie Curie](Images/nlpt_0701.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Marie Curie](Images/nlpt_0701.png)'
- en: Figure 7-1\. A Google search query and corresponding answer snippet
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1 谷歌搜索查询和相应的答案片段
- en: In this example, Google first retrieved around 319,000 documents that were relevant
    to the query, and then performed an additional processing step to extract the
    answer snippet with the corresponding passage and web page. It’s not hard to see
    why these answer snippets are useful. For example, if we search for a trickier
    question like “Which guitar tuning is the best?” Google doesn’t provide an answer,
    and instead we have to click on one of the web pages returned by the search engine
    to find it ourselves.^([1](ch07.xhtml#idm46238714372976))
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，谷歌首先检索了大约319,000个与查询相关的文档，然后进行了额外的处理步骤，提取了带有相应段落和网页的答案片段。很容易看出这些答案片段是有用的。例如，如果我们搜索一个更棘手的问题，比如“哪种吉他调音是最好的？”谷歌没有提供答案，而是我们必须点击搜索引擎返回的网页之一来找到答案。^([1](ch07.xhtml#idm46238714372976))
- en: 'The general approach behind this technology is called *question answering*
    (QA). There are many flavors of QA, but the most common is *extractive QA*, which
    involves questions whose answer can be identified as a span of text in a document,
    where the document might be a web page, legal contract, or news article. The two-stage
    process of first retrieving relevant documents and then extracting answers from
    them is also the basis for many modern QA systems, including semantic search engines,
    intelligent assistants, and automated information extractors. In this chapter,
    we’ll apply this process to tackle a common problem facing ecommerce websites:
    helping consumers answer specific queries to evaluate a product. We’ll see that
    customer reviews can be used as a rich and challenging source of information for
    QA, and along the way we’ll learn how transformers act as powerful *reading comprehension*
    models that can extract meaning from text. Let’s begin by fleshing out the use
    case.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术背后的一般方法被称为*问答*（QA）。有许多种类的QA，但最常见的是*抽取式QA*，它涉及到问题的答案可以在文档中识别为文本段落的一部分，文档可能是网页、法律合同或新闻文章。首先检索相关文档，然后从中提取答案的两阶段过程也是许多现代QA系统的基础，包括语义搜索引擎、智能助手和自动信息提取器。在本章中，我们将应用这一过程来解决电子商务网站面临的一个常见问题：帮助消费者回答特定的查询以评估产品。我们将看到客户评论可以作为QA的丰富而具有挑战性的信息来源，并且在这个过程中，我们将学习transformers如何作为强大的*阅读理解*模型，可以从文本中提取含义。让我们从详细说明用例开始。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: This chapter focuses on extractive QA, but other forms of QA may be more suitable
    for your use case. For example, *community QA* involves gathering question-answer
    pairs that are generated by users on forums like [Stack Overflow](https://stackoverflow.com),
    and then using semantic similarity search to find the closest matching answer
    to a new question. There is also *long-form QA*, which aims to generate complex
    paragraph-length answers to open-ended questions like “Why is the sky blue?” Remarkably,
    it is also possible to do QA over tables, and transformer models like [TAPAS](https://oreil.ly/vVPWO)
    can even perform aggregations to produce the final answer!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍抽取式QA，但其他形式的QA可能更适合您的用例。例如，*社区QA*涉及收集用户在论坛上生成的问题-答案对，然后使用语义相似性搜索找到与新问题最接近的答案。还有*长篇QA*，旨在对开放性问题生成复杂的段落长度答案，比如“天空为什么是蓝色？”值得注意的是，还可以对表格进行QA，transformer模型如[TAPAS](https://oreil.ly/vVPWO)甚至可以执行聚合以生成最终答案！
- en: Building a Review-Based QA System
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基于评论的问答系统
- en: If you’ve ever purchased a product online, you probably relied on customer reviews
    to help inform your decision. These reviews can often help answer specific questions
    like “Does this guitar come with a strap?” or “Can I use this camera at night?”
    that may be hard to answer from the product description alone. However, popular
    products can have hundreds to thousands of reviews, so it can be a major drag
    to find one that is relevant. One alternative is to post your question on the
    community QA platforms provided by websites like Amazon, but it usually takes
    days to get an answer (if you get one at all). Wouldn’t it be nice if we could
    get an immediate answer, like in the Google example from [Figure 7-1](#marie-curie)?
    Let’s see if we can do this using transformers!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您曾经在网上购买过产品，您可能会依赖客户评论来帮助您做出决定。这些评论通常可以帮助回答特定问题，比如“这把吉他带吗？”或者“我可以在晚上使用这个相机吗？”这些问题可能很难仅通过产品描述就能回答。然而，热门产品可能会有数百甚至数千条评论，因此找到相关的评论可能会很麻烦。一个替代方法是在像亚马逊这样的网站提供的社区问答平台上发布您的问题，但通常需要几天时间才能得到答案（如果有的话）。如果我们能像[图7-1](#marie-curie)中的谷歌示例那样立即得到答案，那不是挺好的吗？让我们看看是否可以使用transformers来实现这一点！
- en: The Dataset
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'To build our QA system we’ll use the SubjQA dataset,^([2](ch07.xhtml#idm46238714344800))
    which consists of more than 10,000 customer reviews in English about products
    and services in six domains: TripAdvisor, Restaurants, Movies, Books, Electronics,
    and Grocery. As illustrated in [Figure 7-2](#phone), each review is associated
    with a question that can be answered using one or more sentences from the review.^([3](ch07.xhtml#idm46238714342272))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的QA系统，我们将使用SubjQA数据集^([2](ch07.xhtml#idm46238714344800))，该数据集包括英语中关于六个领域的产品和服务的10,000多条客户评论：TripAdvisor、餐馆、电影、书籍、电子产品和杂货。如[图7-2](#phone)所示，每条评论都与一个问题相关联，可以使用评论中的一句或多句来回答。^([3](ch07.xhtml#idm46238714342272))
- en: '![Phone with Query](Images/nlpt_0702.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![带有查询的手机](Images/nlpt_0702.png)'
- en: Figure 7-2\. A question about a product and the corresponding review (the answer
    span is underlined)
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2. 有关产品的问题和相应的评论（答案范围已划线）
- en: The interesting aspect of this dataset is that most of the questions and answers
    are *subjective*; that is, they depend on the personal experience of the users.
    The example in [Figure 7-2](#phone) shows why this feature makes the task potentially
    more difficult than finding answers to factual questions like “What is the currency
    of the United Kingdom?” First, the query is about “poor quality,” which is subjective
    and depends on the user’s definition of quality. Second, important parts of the
    query do not appear in the review at all, which means it cannot be answered with
    shortcuts like keyword search or paraphrasing the input question. These features
    make SubjQA a realistic dataset to benchmark our review-based QA models on, since
    user-generated content like that shown in [Figure 7-2](#phone) resembles what
    we might encounter in the wild.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集的有趣之处在于，大多数问题和答案都是*主观*的；也就是说，它们取决于用户的个人经验。[图7-2](#phone)中的示例显示了为什么这一特点使得这个任务可能比寻找对像“英国的货币是什么？”这样的事实问题的答案更困难。首先，查询是关于“质量差”，这是主观的，取决于用户对质量的定义。其次，查询的重要部分根本不出现在评论中，这意味着不能用关键词搜索或释义输入问题来回答。这些特点使得SubjQA成为一个真实的数据集，可以用来对我们基于评论的QA模型进行基准测试，因为像[图7-2](#phone)中显示的用户生成内容类似于我们可能在野外遇到的内容。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: QA systems are usually categorized by the *domain* of data that they have access
    to when responding to a query. *Closed-domain* QA deals with questions about a
    narrow topic (e.g., a single product category), while *open-domain* QA deals with
    questions about almost anything (e.g., Amazon’s whole product catalog). In general,
    closed-domain QA involves searching through fewer documents than the open-domain
    case.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: QA系统通常根据其在响应查询时可以访问的数据的*领域*进行分类。*封闭领域*QA处理关于狭窄主题的问题（例如单个产品类别），而*开放领域*QA处理几乎任何问题（例如亚马逊的整个产品目录）。一般来说，封闭领域QA涉及搜索的文档比开放领域情况少。
- en: 'To get started, let’s download the dataset from the [Hugging Face Hub](https://oreil.ly/iO0s5).
    As we did in [Chapter 4](ch04.xhtml#chapter_ner), we can use the `get_dataset_config_names()`
    function to find out which subsets are available:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，让我们从[Hugging Face Hub](https://oreil.ly/iO0s5)下载数据集。就像我们在[第4章](ch04.xhtml#chapter_ner)中所做的那样，我们可以使用`get_dataset_config_names()`函数来找出哪些子集是可用的：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For our use case, we’ll focus on building a QA system for the Electronics domain.
    To download the `electronics` subset, we just need to pass this value to the `name`
    argument of the `load_dataset()` function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们将专注于构建电子产品领域的QA系统。要下载`electronics`子集，我们只需要将该值传递给`load_dataset()`函数的`name`参数：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Like other question answering datasets on the Hub, SubjQA stores the answers
    to each question as a nested dictionary. For example, if we inspect one of the
    rows in the `answers` column:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与Hub上的其他问答数据集一样，SubjQA将每个问题的答案存储为嵌套字典。例如，如果我们检查`answers`列中的一行：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'we can see that the answers are stored in a `text` field, while the starting
    character indices are provided in `answer_start`. To explore the dataset more
    easily, we’ll flatten these nested columns with the `flatten()` method and convert
    each split to a Pandas `DataFrame` as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到答案存储在`text`字段中，而起始字符索引则在`answer_start`中提供。为了更轻松地探索数据集，我们将使用`flatten()`方法展平这些嵌套列，并将每个拆分转换为Pandas的`DataFrame`，如下所示：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that the dataset is relatively small, with only 1,908 examples in total.
    This simulates a real-world scenario, since getting domain experts to label extractive
    QA datasets is labor-intensive and expensive. For example, the CUAD dataset for
    extractive QA on legal contracts is estimated to have a value of $2 million to
    account for the legal expertise needed to annotate its 13,000 examples!^([4](ch07.xhtml#idm46238714139488))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集相对较小，总共只有1,908个示例。这模拟了真实世界的情况，因为让领域专家标记抽取式QA数据集是费时费力的。例如，用于法律合同抽取式QA的CUAD数据集估计价值为200万美元，以应对标注其13,000个示例所需的法律专业知识！^([4](ch07.xhtml#idm46238714139488))
- en: There are quite a few columns in the SubjQA dataset, but the most interesting
    ones for building our QA system are shown in [Table 7-1](#subjqa-columns).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SubjQA数据集中有相当多的列，但对于构建我们的QA系统来说，最有趣的是[表7-1](#subjqa-columns)中显示的那些。
- en: Table 7-1\. Column names and their descriptions from the SubjQA dataset
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-1. SubjQA数据集的列名及其描述
- en: '| Column name | Description |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 描述 |'
- en: '| --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `title` | The Amazon Standard Identification Number (ASIN) associated with
    each product |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| `title` | 与每个产品相关联的亚马逊标准识别号（ASIN） |'
- en: '| `question` | The question |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `question` | 问题 |'
- en: '| `answers.answer_text` | The span of text in the review labeled by the annotator
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `answers.answer_text` | 标注者标记的评论文本范围 |'
- en: '| `answers.answer_start` | The start character index of the answer span |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `answers.answer_start` | 答案范围的起始字符索引 |'
- en: '| `context` | The customer review |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `context` | 客户评论 |'
- en: 'Let’s focus on these columns and take a look at a few of the training examples.
    We can use the `sample()` method to select a random sample:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于这些列，并查看一些训练示例。我们可以使用`sample()`方法选择一个随机样本：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '| title | question | answers.text | answers.answer_start | context |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 问题 | 答案文本 | 答案起始 | 上下文 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| B005DKZTMG | Does the keyboard lightweight? | [this keyboard is compact]
    | [215] | I really like this keyboard. I give it 4 stars because it doesn’t have
    a CAPS LOCK key so I never know if my caps are on. But for the price, it really
    suffices as a wireless keyboard. I have very large hands and this keyboard is
    compact, but I have no complaints. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| B005DKZTMG | 键盘轻便吗？ | [这个键盘很紧凑] | [215] | 我真的很喜欢这个键盘。我给它4颗星，因为它没有大写锁定键，所以我永远不知道我的大写是否打开。但是就价格而言，它确实足够作为一个无线键盘。我手很大，这个键盘很紧凑，但我没有抱怨。
    |'
- en: '| B00AAIPT76 | How is the battery? | [] | [] | I bought this after the first
    spare gopro battery I bought wouldn’t hold a charge. I have very realistic expectations
    of this sort of product, I am skeptical of amazing stories of charge time and
    battery life but I do expect the batteries to hold a charge for a couple of weeks
    at least and for the charger to work like a charger. In this I was not disappointed.
    I am a river rafter and found that the gopro burns through power in a hurry so
    this purchase solved that issue. the batteries held a charge, on shorter trips
    the extra two batteries were enough and on longer trips I could use my friends
    JOOS Orange to recharge them.I just bought a newtrent xtreme powerpak and expect
    to be able to charge these with that so I will not run out of power again. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| B00AAIPT76 | 电池如何？ | [] | [] | 我在购买了第一个备用gopro电池后发现它无法保持充电。我对这种产品有非常现实的期望，对于充电时间和电池寿命的惊人故事我持怀疑态度，但我确实希望电池至少能保持几周的充电，并且充电器能像充电器一样工作。在这方面我并不失望。我是一名漂流者，发现gopro电池很快就用完了，所以这次购买解决了这个问题。电池保持了充电，在短途旅行上，额外的两块电池足够使用，在长途旅行上，我可以使用我的朋友的JOOS
    Orange来给它们充电。我刚刚购买了一个newtrent xtreme powerpak，期望能用它给这些电池充电，所以我不会再用完电了。 |'
- en: 'From these examples we can make a few observations. First, the questions are
    not grammatically correct, which is quite common in the FAQ sections of ecommerce
    websites. Second, an empty `answers.text` entry denotes “unanswerable” questions
    whose answer cannot be found in the review. Finally, we can use the start index
    and length of the answer span to slice out the span of text in the review that
    corresponds to the answer:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些例子中，我们可以做出一些观察。首先，问题在语法上不正确，这在电子商务网站的常见FAQ部分中是很常见的。其次，空的`answers.text`条目表示“无法回答”的问题，其答案无法在评论中找到。最后，我们可以使用答案跨度的起始索引和长度来切出对应于答案的评论文本跨度：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, let’s get a feel for what types of questions are in the training set
    by counting the questions that begin with a few common starting words:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过计算以一些常见起始词开头的问题来了解训练集中有哪些类型的问题：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](Images/nlpt_07in01.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_07in01.png)'
- en: 'We can see that questions beginning with “How”, “What”, and “Is” are the most
    common ones, so let’s have a look at some examples:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到以“How”、“What”和“Is”开头的问题是最常见的，所以让我们看一些例子：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we’ve explored our dataset a bit, let’s dive into understanding how
    transformers can extract answers from text.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对数据集有了一些了解，让我们深入了解transformers如何从文本中提取答案。
- en: Extracting Answers from Text
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文本中提取答案
- en: 'The first thing we’ll need for our QA system is to find a way to identify a
    potential answer as a span of text in a customer review. For example, if a we
    have a question like “Is it waterproof?” and the review passage is “This watch
    is waterproof at 30m depth”, then the model should output “waterproof at 30m”.
    To do this we’ll need to understand how to:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的问答系统首先需要找到一种方法来识别客户评论中的潜在答案作为文本跨度。例如，如果我们有一个问题“它防水吗？”和评论段落是“这个手表在30米深处防水”，那么模型应该输出“在30米处防水”。为了做到这一点，我们需要了解如何：
- en: Frame the supervised learning problem.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建监督学习问题。
- en: Tokenize and encode text for QA tasks.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为问答任务对文本进行标记和编码。
- en: Deal with long passages that exceed a model’s maximum context size.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理超出模型最大上下文大小的长段落。
- en: Let’s start by taking a look at how to frame the problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从如何构建问题开始。
- en: Span classification
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨度分类
- en: The most common way to extract answers from text is by framing the problem as
    a *span classification* task, where the start and end tokens of an answer span
    act as the labels that a model needs to predict. This process is illustrated in
    [Figure 7-4](#qa-head).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取答案的最常见方法是将问题构建为*跨度分类*任务，其中答案跨度的起始和结束标记作为模型需要预测的标签。这个过程在[图7-4](#qa-head)中有所说明。
- en: '![QA Head](Images/nlpt_0704.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![QA Head](Images/nlpt_0704.png)'
- en: Figure 7-4\. The span classification head for QA tasks
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4。QA任务的跨度分类头部
- en: Since our training set is relatively small, with only 1,295 examples, a good
    strategy is to start with a language model that has already been fine-tuned on
    a large-scale QA dataset like SQuAD. In general, these models have strong reading
    comprehension capabilities and serve as a good baseline upon which to build a
    more accurate system. This is a somewhat different approach to that taken in previous
    chapters, where we typically started with a pretrained model and fine-tuned the
    task-specific head ourselves. For example, in [Chapter 2](ch02.xhtml#chapter_classification),
    we had to fine-tune the classification head because the number of classes was
    tied to the dataset at hand. For extractive QA, we can actually start with a fine-tuned
    model since the structure of the labels remains the same across datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的训练集相对较小，只有1295个例子，一个好的策略是从已经在大规模QA数据集（如SQuAD）上进行了微调的语言模型开始。一般来说，这些模型具有很强的阅读理解能力，并且可以作为构建更准确系统的良好基线。这与之前章节中的方法有些不同，在之前的章节中，我们通常是从预训练模型开始，然后自己微调特定任务的头部。例如，在[第2章](ch02.xhtml#chapter_classification)中，我们不得不微调分类头部，因为类别数量与手头的数据集相关联。对于抽取式问答，我们实际上可以从一个经过微调的模型开始，因为标签的结构在不同数据集之间保持不变。
- en: You can find a list of extractive QA models by navigating to the [Hugging Face
    Hub](https://oreil.ly/dzCsC) and searching for “squad” on the Models tab ([Figure 7-5](#squad-models)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过导航到[Hugging Face Hub](https://oreil.ly/dzCsC)并在Models选项卡上搜索“squad”来找到一系列抽取式QA模型（[图7-5](#squad-models)）。
- en: '![SQuAD models](Images/nlpt_0705.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![SQuAD模型](Images/nlpt_0705.png)'
- en: Figure 7-5\. A selection of extractive QA models on the Hugging Face Hub
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5。Hugging Face Hub上一些抽取式QA模型的选择
- en: As you can see, at the time of writing, there are more than 350 QA models to
    choose from—so which one should you pick? In general, the answer depends on various
    factors like whether your corpus is mono- or multilingual and the constraints
    of running the model in a production environment. [Table 7-2](#squad-models-table)
    lists a few models that provide a good foundation to build on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，在撰写本文时，有超过350个QA模型可供选择，那么您应该选择哪一个呢？一般来说，答案取决于各种因素，比如您的语料库是单语还是多语，以及在生产环境中运行模型的约束。[表7-2](#squad-models-table)列出了一些提供了良好基础的模型。
- en: Table 7-2\. Baseline transformer models that are fine-tuned on SQuAD 2.0
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-2。在SQuAD 2.0上进行微调的基线变压器模型
- en: '| Transformer | Description | Number of parameters | *F*[1]-score on SQuAD
    2.0 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 描述 | 参数数量 | SQuAD 2.0上的*F*分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MiniLM | A distilled version of BERT-base that preserves 99% of the performance
    while being twice as fast | 66M | 79.5 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| MiniLM | 保留了99%的性能，同时运行速度是BERT-base的两倍 | 66M | 79.5 |'
- en: '| RoBERTa-base | RoBERTa models have better performance than their BERT counterparts
    and can be fine-tuned on most QA datasets using a single GPU | 125M | 83.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa-base | RoBERTa模型的性能比BERT模型更好，并且可以在大多数QA数据集上使用单个GPU进行微调 | 125M | 83.0
    |'
- en: '| ALBERT-XXL | State-of-the-art performance on SQuAD 2.0, but computationally
    intensive and difficult to deploy | 235M | 88.1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ALBERT-XXL | 在SQuAD 2.0上表现出色，但计算密集且难以部署 | 235M | 88.1 |'
- en: '| XLM-RoBERTa-large | Multilingual model for 100 languages with strong zero-shot
    performance | 570M | 83.8 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| XLM-RoBERTa-large | 用于100种语言的多语言模型，具有强大的零热门性能 | 570M | 83.8 |'
- en: For the purposes of this chapter, we’ll use a fine-tuned MiniLM model since
    it is fast to train and will allow us to quickly iterate on the techniques that
    we’ll be exploring.^([8](ch07.xhtml#idm46238713719168)) As usual, the first thing
    we need is a tokenizer to encode our texts, so let’s take a look at how this works
    for QA tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的目的是，我们将使用经过微调的MiniLM模型，因为它训练速度快，可以让我们快速迭代我们将要探索的技术。^([8](ch07.xhtml#idm46238713719168))
    像往常一样，我们需要的第一件事是一个标记器来对我们的文本进行编码，所以让我们看看这对于QA任务是如何工作的。
- en: Tokenizing text for QA
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QA的文本标记化
- en: 'To encode our texts, we’ll load the MiniLM model checkpoint from the [Hugging
    Face Hub](https://oreil.ly/df5Cu) as usual:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对我们的文本进行编码，我们将像往常一样从[Hugging Face Hub](https://oreil.ly/df5Cu)加载MiniLM模型检查点：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To see the model in action, let’s first try to extract an answer from a short
    passage of text. In extractive QA tasks, the inputs are provided as (question,
    context) pairs, so we pass them both to the tokenizer as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到模型的实际效果，让我们首先尝试从文本的简短段落中提取答案。在抽取式QA任务中，输入以（问题，上下文）对的形式提供，因此我们将它们都传递给标记器，如下所示：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here we’ve returned PyTorch `Tensor` objects, since we’ll need them to run
    the forward pass through the model. If we view the tokenized inputs as a table:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们返回了PyTorch`Tensor`对象，因为我们需要它们来运行模型的前向传递。如果我们将标记化的输入视为表格：
- en: '| input_ids | 101 | 2129 | 2172 | 2189 | 2064 | 2023 | ... | 5834 | 2006 |
    5371 | 2946 | 1012 | 102 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| input_ids | 101 | 2129 | 2172 | 2189 | 2064 | 2023 | ... | 5834 | 2006 |
    5371 | 2946 | 1012 | 102 |'
- en: '| token_type_ids | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| token_type_ids | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: '| attention_mask | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| attention_mask | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: we can see the familiar `input_ids` and `attention_mask` tensors, while the
    `token_type_ids` tensor indicates which part of the inputs corresponds to the
    question and context (a 0 indicates a question token, a 1 indicates a context
    token).^([9](ch07.xhtml#idm46238713601696))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到熟悉的`input_ids`和`attention_mask`张量，而`token_type_ids`张量指示了输入的哪个部分对应于问题和上下文（0表示问题标记，1表示上下文标记）。^([9](ch07.xhtml#idm46238713601696))
- en: 'To understand how the tokenizer formats the inputs for QA tasks, let’s decode
    the `input_ids` tensor:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解标记器如何为QA任务格式化输入，让我们解码`input_ids`张量：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We see that for each QA example, the inputs take the format:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，对于每个QA示例，输入采用以下格式：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'where the location of the first `[SEP]` token is determined by the `token_type_ids`.
    Now that our text is tokenized, we just need to instantiate the model with a QA
    head and run the inputs through the forward pass:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一个`[SEP]`标记的位置由`token_type_ids`确定。现在我们的文本已经被标记化，我们只需要用QA头实例化模型并通过前向传递运行输入：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here we can see that we get a `QuestionAnsweringModelOutput` object as the
    output of the QA head. As illustrated in [Figure 7-4](#qa-head), the QA head corresponds
    to a linear layer that takes the hidden states from the encoder and computes the
    logits for the start and end spans.^([10](ch07.xhtml#idm46238713530720)) This
    means that we treat QA as a form of token classification, similar to what we encountered
    for named entity recognition in [Chapter 4](ch04.xhtml#chapter_ner). To convert
    the outputs into an answer span, we first need to get the logits for the start
    and end tokens:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们得到了一个`QuestionAnsweringModelOutput`对象作为QA头的输出。正如[图7-4](#qa-head)中所示，QA头对应于一个线性层，它获取来自编码器的隐藏状态并计算开始和结束跨度的logits。^([10](ch07.xhtml#idm46238713530720))
    这意味着我们将QA视为一种令牌分类，类似于我们在[第4章](ch04.xhtml#chapter_ner)中遇到的命名实体识别。要将输出转换为答案跨度，我们首先需要获取开始和结束令牌的logits：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we compare the shapes of these logits to the input IDs:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较这些logits的形状和输入ID：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: we see that there are two logits (a start and end) associated with each input
    token. As illustrated in [Figure 7-6](#qa-scores), larger, positive logits correspond
    to more likely candidates for the start and end tokens. In this example we can
    see that the model assigns the highest start token logits to the numbers “1” and
    “6000”, which makes sense since our question is asking about a quantity. Similarly,
    we see that the end tokens with the highest logits are “minute” and “hours”.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到每个输入标记都有两个logits（一个起始和一个结束）。如[图7-6](#qa-scores)所示，较大的正logits对应于更有可能成为起始和结束标记的候选标记。在这个例子中，我们可以看到模型将最高的起始标记logits分配给数字“1”和“6000”，这是有道理的，因为我们的问题是关于数量的。同样，我们看到最高logits的结束标记是“minute”和“hours”。
- en: '![](Images/nlpt_0706.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_0706.png)'
- en: Figure 7-6\. Predicted logits for the start and end tokens; the token with the
    highest score is colored in orange
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6。预测的起始和结束标记的logits；得分最高的标记以橙色标出
- en: 'To get the final answer, we can compute the argmax over the start and end token
    logits and then slice the span from the inputs. The following code performs these
    steps and decodes the result so we can print the resulting text:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到最终答案，我们可以计算起始和结束标记logits的argmax，然后从输入中切片出这个范围。以下代码执行这些步骤并解码结果，以便我们可以打印出结果文本：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Great, it worked! In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, all
    of these preprocessing and postprocessing steps are conveniently wrapped in a
    dedicated pipeline. We can instantiate the pipeline by passing our tokenizer and
    fine-tuned model as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，成功了！在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，所有这些预处理和后处理步骤都方便地包装在一个专用的管道中。我们可以通过传递我们的分词器和微调模型来实例化管道，如下所示：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In addition to the answer, the pipeline also returns the model’s probability
    estimate in the `score` field (obtained by taking a softmax over the logits).
    This is handy when we want to compare multiple answers within a single context.
    We’ve also shown that we can have the model predict multiple answers by specifying
    the `topk` parameter. Sometimes, it is possible to have questions for which no
    answer is possible, like the empty `answers.answer_start` examples in SubjQA.
    In these cases the model will assign a high start and end score to the `[CLS]`
    token, and the pipeline maps this output to an empty string:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了答案，管道还在`score`字段中返回模型的概率估计（通过对logits进行softmax获得）。当我们想要在单个上下文中比较多个答案时，这是很方便的。我们还表明，通过指定`topk`参数，我们可以让模型预测多个答案。有时，可能会有问题没有答案的情况，比如SubjQA中空的`answers.answer_start`示例。在这些情况下，模型将为`[CLS]`标记分配高的起始和结束分数，管道将这个输出映射为空字符串：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In our simple example, we obtained the start and end indices by taking the argmax
    of the corresponding logits. However, this heuristic can produce out-of-scope
    answers by selecting tokens that belong to the question instead of the context.
    In practice, the pipeline computes the best combination of start and end indices
    subject to various constraints such as being in-scope, requiring the start indices
    to precede the end indices, and so on.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的简单示例中，我们通过获取相应logits的argmax来获得起始和结束索引。然而，这种启发式方法可能会产生超出范围的答案，因为它选择属于问题而不是上下文的标记。在实践中，管道计算最佳的起始和结束索引组合，受到各种约束的限制，比如在范围内，要求起始索引在结束索引之前等等。
- en: Dealing with long passages
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理长段落
- en: One subtlety faced by reading comprehension models is that the context often
    contains more tokens than the maximum sequence length of the model (which is usually
    a few hundred tokens at most). As illustrated in [Figure 7-7](#subjqa-dist), a
    decent portion of the SubjQA training set contains question-context pairs that
    won’t fit within MiniLM’s context size of 512 tokens.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读理解模型面临的一个微妙之处是，上下文通常包含的标记比模型的最大序列长度（通常最多几百个标记）多。如[图7-7](#subjqa-dist)所示，SubjQA训练集中相当一部分包含的问题-上下文对无法适应MiniLM的512个标记的上下文大小。
- en: '![](Images/nlpt_0707.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_0707.png)'
- en: Figure 7-7\. Distribution of tokens for each question-context pair in the SubjQA
    training set
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7。SubjQA训练集中每个问题-上下文对的标记分布
- en: For other tasks, like text classification, we simply truncated long texts under
    the assumption that enough information was contained in the embedding of the `[CLS]`
    token to generate accurate predictions. For QA, however, this strategy is problematic
    because the answer to a question could lie near the end of the context and thus
    would be removed by truncation. As illustrated in [Figure 7-8](#sliding-window),
    the standard way to deal with this is to apply a *sliding window* across the inputs,
    where each window contains a passage of tokens that fit in the model’s context.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他任务，比如文本分类，我们简单地截断长文本，假设`[CLS]`标记的嵌入中包含足够的信息来生成准确的预测。然而，对于QA来说，这种策略是有问题的，因为问题的答案可能位于上下文的末尾，因此会被截断。如[图7-8](#sliding-window)所示，处理这个问题的标准方法是在输入上应用*滑动窗口*，其中每个窗口包含适合模型上下文的标记段。
- en: '![Sliding window](Images/nlpt_0708.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![滑动窗口](Images/nlpt_0708.png)'
- en: Figure 7-8\. How the sliding window creates multiple question-context pairs
    for long documents—the first bar corresponds to the question, while the second
    bar is the context captured in each window
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8。滑动窗口如何为长文档创建多个问题-上下文对——第一个条形图对应问题，而第二个条形图是每个窗口中捕获的上下文
- en: 'In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we can set `return_overflowing_tokens=True`
    in the tokenizer to enable the sliding window. The size of the sliding window
    is controlled by the `max_seq_length` argument, and the size of the stride is
    controlled by `doc_stride`. Let’s grab the first example from our training set
    and define a small window to illustrate how this works:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，我们可以在分词器中设置`return_overflowing_tokens=True`来启用滑动窗口。滑动窗口的大小由`max_seq_length`参数控制，步幅的大小由`doc_stride`控制。让我们从训练集中抓取第一个例子，并定义一个小窗口来说明这是如何工作的：
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In this case we now get a list of `input_ids`, one for each window. Let’s check
    the number of tokens we have in each window:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们现在得到了一个`input_ids`列表，每个窗口一个。让我们检查每个窗口中的标记数：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we can see where two windows overlap by decoding the inputs:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过解码输入来看到两个窗口重叠的位置：
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now that we have some intuition about how QA models can extract answers from
    text, let’s look at the other components we need to build an end-to-end QA pipeline.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对QA模型如何从文本中提取答案有了一些直觉，让我们看看构建端到端QA管道所需的其他组件。
- en: Using Haystack to Build a QA Pipeline
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Haystack构建QA管道
- en: In our simple answer extraction example, we provided both the question and the
    context to the model. However, in reality our system’s users will only provide
    a question about a product, so we need some way of selecting relevant passages
    from among all the reviews in our corpus. One way to do this would be to concatenate
    all the reviews of a given product together and feed them to the model as a single,
    long context. Although simple, the drawback of this approach is that the context
    can become extremely long and thereby introduce an unacceptable latency for our
    users’ queries. For example, let’s suppose that on average, each product has 30
    reviews and each review takes 100 milliseconds to process. If we need to process
    all the reviews to get an answer, this would result in an average latency of 3
    seconds per user query—much too long for ecommerce websites!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简单的答案提取示例中，我们向模型提供了问题和上下文。然而，在现实中，我们系统的用户只会提供有关产品的问题，因此我们需要一种方法从我们语料库中的所有评论中选择相关的段落。做到这一点的一种方法是将给定产品的所有评论连接在一起，并将它们作为单个长上下文输入模型。虽然简单，但这种方法的缺点是上下文可能变得非常长，从而为我们用户的查询引入不可接受的延迟。例如，假设平均每个产品有30条评论，每条评论需要100毫秒处理。如果我们需要处理所有评论来得到答案，这将导致每个用户查询的平均延迟为3秒，对于电子商务网站来说太长了！
- en: 'To handle this, modern QA systems are typically based on the *retriever-reader*
    architecture, which has two main components:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这一点，现代QA系统通常基于*检索器-阅读器*架构，它有两个主要组件：
- en: '*Retriever*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索器*'
- en: Responsible for retrieving relevant documents for a given query. Retrievers
    are usually categorized as *sparse* or *dense*. Sparse retrievers use word frequencies
    to represent each document and query as a sparse vector.^([11](ch07.xhtml#idm46238713054704))
    The relevance of a query and a document is then determined by computing an inner
    product of the vectors. On the other hand, dense retrievers use encoders like
    transformers to represent the query and document as contextualized embeddings
    (which are dense vectors). These embeddings encode semantic meaning, and allow
    dense retrievers to improve search accuracy by understanding the content of the
    query.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 负责为给定查询检索相关文档。检索器通常被分类为*稀疏*或*密集*。稀疏检索器使用词频来表示每个文档和查询，形成稀疏向量。^([11](ch07.xhtml#idm46238713054704))然后通过计算向量的内积来确定查询和文档的相关性。另一方面，密集检索器使用编码器（如transformers）将查询和文档表示为上下文化的嵌入（密集向量）。这些嵌入编码语义含义，并允许密集检索器通过理解查询的内容来提高搜索准确性。
- en: '*Reader*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*阅读器*'
- en: Responsible for extracting an answer from the documents provided by the retriever.
    The reader is usually a reading comprehension model, although at the end of the
    chapter we’ll see examples of models that can generate free-form answers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 负责从检索器提供的文档中提取答案。阅读器通常是一个阅读理解模型，尽管在本章末尾我们将看到可以生成自由形式答案的模型示例。
- en: As illustrated in [Figure 7-9](#retriever-reader), there can also be other components
    that apply post-processing to the documents fetched by the retriever or to the
    answers extracted by the reader. For example, the retrieved documents may need
    reranking to eliminate noisy or irrelevant ones that can confuse the reader. Similarly,
    postprocessing of the reader’s answers is often needed when the correct answer
    comes from various passages in a long document.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图7-9](#retriever-reader)所示，还可以有其他组件对检索器获取的文档或阅读器提取的答案进行后处理。例如，检索到的文档可能需要重新排名，以消除可能混淆阅读器的嘈杂或无关的文档。类似地，当正确答案来自长文档中的各个段落时，通常需要对阅读器的答案进行后处理。
- en: '![QA Architecture](Images/nlpt_0709.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![QA架构](Images/nlpt_0709.png)'
- en: Figure 7-9\. The retriever-reader architecture for modern QA systems
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9。现代QA系统的检索器-阅读器架构
- en: To build our QA system, we’ll use the [*Haystack* library](https://haystack.deepset.ai)
    developed by [deepset](https://deepset.ai), a German company focused on NLP. Haystack
    is based on the retriever-reader architecture, abstracts much of the complexity
    involved in building these systems, and integrates tightly with ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的QA系统，我们将使用由[deepset](https://deepset.ai)开发的[*Haystack*库](https://haystack.deepset.ai)，deepset是一家专注于NLP的德国公司。Haystack基于检索器-阅读器架构，抽象了构建这些系统所涉及的大部分复杂性，并与![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers紧密集成。
- en: 'In addition to the retriever and reader, there are two more components involved
    when building a QA pipeline with Haystack:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检索器和阅读器之外，在构建Haystack的QA管道时还涉及另外两个组件：
- en: '*Document store*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*文档存储*'
- en: A document-oriented database that stores documents and metadata which are provided
    to the retriever at query time
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 存储文档和元数据的面向文档的数据库，这些文档和元数据在查询时提供给检索器
- en: '*Pipeline*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*管道*'
- en: Combines all the components of a QA system to enable custom query flows, merging
    documents from multiple retrievers, and more
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 将QA系统的所有组件结合在一起，以实现自定义查询流程，合并来自多个检索器的文档等
- en: In this section we’ll look at how we can use these components to quickly build
    a prototype QA pipeline. Later, we’ll examine how we can improve its performance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看如何使用这些组件快速构建原型QA管道。稍后，我们将探讨如何提高其性能。
- en: Warning
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This chapter was written using version 0.9.0 of the Haystack library. In [version
    0.10.0](https://oreil.ly/qbqgv), the pipeline and evaluation APIs were redesigned
    to make it easier to inspect whether the retriever or reader are impacting performance.
    To see what this chapter’s code looks like with the new API, check out the [GitHub
    repository](https://github.com/nlp-with-transformers/notebooks).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是使用Haystack库的0.9.0版本编写的。在[版本0.10.0](https://oreil.ly/qbqgv)中，重新设计了管道和评估API，以便更容易检查检索器或阅读器是否影响性能。要查看使用新API的本章代码是什么样子，请查看[GitHub存储库](https://github.com/nlp-with-transformers/notebooks)。
- en: Initializing a document store
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化文档存储
- en: In Haystack, there are various document stores to choose from and each one can
    be paired with a dedicated set of retrievers. This is illustrated in [Table 7-3](#doc-stores),
    where the compatibility of sparse (TF-IDF, BM25) and dense (Embedding, DPR) retrievers
    is shown for each of the available document stores. We’ll explain what all these
    acronyms mean later in this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在Haystack中，有各种可供选择的文档存储，并且每个文档存储都可以与一组专用的检索器配对。这在[表7-3](#doc-stores)中有所说明，其中显示了每个可用文档存储的稀疏（TF-IDF、BM25）和密集（嵌入、DPR）检索器的兼容性。我们将在本章后面解释所有这些首字母缩略词的含义。
- en: Table 7-3\. Compatibility of Haystack retrievers and document stores
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-3。Haystack检索器和文档存储的兼容性
- en: '|  | In memory | Elasticsearch | FAISS | Milvus |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | 内存 | Elasticsearch | FAISS | Milvus |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| TF-IDF | Yes | Yes | No | No |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| TF-IDF | 是 | 是 | 否 | 否 |'
- en: '| BM25 | No | Yes | No | No |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | 否 | 是 | 否 | 否 |'
- en: '| Embedding | Yes | Yes | Yes | Yes |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入 | 是 | 是 | 是 | 是 |'
- en: '| DPR | Yes | Yes | Yes | Yes |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| DPR | 是 | 是 | 是 | 是 |'
- en: Since we’ll be exploring both sparse and dense retrievers in this chapter, we’ll
    use the `ElasticsearchDocumentStore`, which is compatible with both retriever
    types. Elasticsearch is a search engine that is capable of handling a diverse
    range of data types, including textual, numerical, geospatial, structured, and
    unstructured. Its ability to store huge volumes of data and quickly filter it
    with full-text search features makes it especially well suited for developing
    QA systems. It also has the advantage of being the industry standard for infrastructure
    analytics, so there’s a good chance your company already has a cluster that you
    can work with.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章将探讨稀疏和密集检索器，我们将使用与两种检索器类型兼容的`ElasticsearchDocumentStore`。Elasticsearch是一种能够处理各种数据类型的搜索引擎，包括文本、数字、地理空间、结构化和非结构化数据。它存储大量数据并能够快速进行全文搜索过滤，因此特别适用于开发问答系统。它还具有成为基础设施分析行业标准的优势，因此您的公司很有可能已经有一个集群可以使用。
- en: 'To initialize the document store, we first need to download and install Elasticsearch.
    By following Elasticsearch’s [guide](https://oreil.ly/bgmKq),^([12](ch07.xhtml#idm46238713004128))
    we can grab the latest release for Linux with `wget` and unpack it with the `tar`
    shell command:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化文档存储，我们首先需要下载并安装Elasticsearch。通过按照Elasticsearch的[指南](https://oreil.ly/bgmKq)^[12](ch07.xhtml#idm46238713004128)
    ，我们可以使用`wget`获取Linux的最新版本，并使用`tar` shell命令解压缩它：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next we need to start the Elasticsearch server. Since we’re running all the
    code in this book within Jupyter notebooks, we’ll need to use Python’s `Popen()`
    function to spawn a new process. While we’re at it, let’s also run the subprocess
    in the background using the `chown` shell command:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要启动Elasticsearch服务器。由于我们在Jupyter笔记本中运行本书中的所有代码，我们需要使用Python的`Popen()`函数来生成一个新的进程。在此过程中，让我们还使用`chown`
    shell命令在后台运行子进程：
- en: '[PRE35]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the `Popen()` function, the `args` specify the program we wish to execute,
    while `stdout=PIPE` creates a new pipe for the standard output and `stderr=STDOUT`
    collects the errors in the same pipe. The `preexec_fn` argument specifies the
    ID of the subprocess we wish to use. By default, Elasticsearch runs locally on
    port 9200, so we can test the connection by sending an HTTP request to `localhost`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Popen()`函数中，`args`指定我们希望执行的程序，而`stdout=PIPE`创建一个新的管道用于标准输出，`stderr=STDOUT`收集相同管道中的错误。`preexec_fn`参数指定我们希望使用的子进程的ID。默认情况下，Elasticsearch在本地端口9200上运行，因此我们可以通过向`localhost`发送HTTP请求来测试连接：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now that our Elasticsearch server is up and running, the next thing to do is
    instantiate the document store:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的Elasticsearch服务器已经启动运行，接下来要做的事情是实例化文档存储：
- en: '[PRE38]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'By default, `ElasticsearchDocumentStore` creates two indices on Elasticsearch:
    one called `document` for (you guessed it) storing documents, and another called
    `label` for storing the annotated answer spans. For now, we’ll just populate the
    `document` index with the SubjQA reviews, and Haystack’s document stores expect
    a list of dictionaries with `text` and `meta` keys as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`ElasticsearchDocumentStore`在Elasticsearch上创建两个索引：一个称为`document`用于（你猜对了）存储文档，另一个称为`label`用于存储注释的答案跨度。现在，我们将使用SubjQA评论填充`document`索引，Haystack的文档存储期望一个带有`text`和`meta`键的字典列表，如下所示：
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The fields in `meta` can be used for applying filters during retrieval. For
    our purposes we’ll include the `item_id` and `q_review_id` columns of SubjQA so
    we can filter by product and question ID, along with the corresponding training
    split. We can then loop through the examples in each `DataFrame` and add them
    to the index with the `write_documents()` method as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`meta`中的字段可用于在检索期间应用过滤器。为了我们的目的，我们将包括SubjQA的`item_id`和`q_review_id`列，以便我们可以按产品和问题ID进行过滤，以及相应的训练拆分。然后，我们可以循环遍历每个`DataFrame`中的示例，并使用`write_documents()`方法将它们添加到索引中，如下所示：'
- en: '[PRE40]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Great, we’ve loaded all our reviews into an index! To search the index we’ll
    need a retriever, so let’s look at how we can initialize one for Elasticsearch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，我们已经将所有的评论加载到了一个索引中！要搜索索引，我们需要一个检索器，因此让我们看看如何为Elasticsearch初始化一个检索器。
- en: Initializing a retriever
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化检索器
- en: The Elasticsearch document store can be paired with any of the Haystack retrievers,
    so let’s start by using a sparse retriever based on BM25 (short for “Best Match
    25”). BM25 is an improved version of the classic Term Frequency-Inverse Document
    Frequency (TF-IDF) algorithm and represents the question and context as sparse
    vectors that can be searched efficiently on Elasticsearch. The BM25 score measures
    how much matched text is about a search query and improves on TF-IDF by saturating
    TF values quickly and normalizing the document length so that short documents
    are favored over long ones.^([13](ch07.xhtml#idm46238712667392))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch文档存储可以与Haystack检索器中的任何一种配对，因此让我们首先使用基于BM25的稀疏检索器（简称“Best Match
    25”）。BM25是经典的词项频率-逆文档频率（TF-IDF）算法的改进版本，它将问题和上下文表示为可以在Elasticsearch上高效搜索的稀疏向量。BM25分数衡量了匹配文本与搜索查询的相关程度，并通过迅速饱和TF值和规范化文档长度来改进TF-IDF，以便短文档优于长文档。^([13](ch07.xhtml#idm46238712667392))
- en: 'In Haystack, the BM25 retriever is used by default in `ElasticsearchRetriever`,
    so let’s initialize this class by specifying the document store we wish to search
    over:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在Haystack中，默认情况下使用BM25检索器在`ElasticsearchRetriever`中，因此让我们通过指定我们希望搜索的文档存储来初始化这个类：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, let’s look at a simple query for a single electronics product in the
    training set. For review-based QA systems like ours, it’s important to restrict
    the queries to a single item because otherwise the retriever would source reviews
    about products that are not related to a user’s query. For example, asking “Is
    the camera quality any good?” without a product filter could return reviews about
    phones, when the user might be asking about a specific laptop camera instead.
    By themselves, the ASIN values in our dataset are a bit cryptic, but we can decipher
    them with online tools like [*amazon ASIN*](https://amazon-asin.com) or by simply
    appending the value of `item_id` to the *www.amazon.com/dp/* URL. The following
    item ID corresponds to one of Amazon’s Fire tablets, so let’s use the retriever’s
    `retrieve()` method to ask if it’s any good for reading with:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下训练集中单个电子产品的简单查询。对于像我们这样基于评论的问答系统，将查询限制在单个项目是很重要的，否则检索器会返回与用户查询无关的产品评论。例如，询问“相机质量如何？”如果没有产品过滤器，可能会返回关于手机的评论，而用户可能是在询问特定笔记本电脑相机的情况。我们的数据集中的ASIN值本身有点神秘，但我们可以使用在线工具如[*amazon
    ASIN*](https://amazon-asin.com)或者简单地将`item_id`的值附加到*www.amazon.com/dp/* URL来解密它们。以下项目ID对应于亚马逊的Fire平板电脑之一，所以让我们使用检索器的`retrieve()`方法来询问它是否适合阅读：
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here we’ve specified how many documents to return with the `top_k` argument
    and applied a filter on both the `item_id` and `split` keys that were included
    in the `meta` field of our documents. Each element of `retrieved_docs` is a Haystack
    `Document` object that is used to represent documents and includes the retriever’s
    query score along with other metadata. Let’s have a look at one of the retrieved
    documents:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经指定了`top_k`参数返回多少个文档，并在我们的文档的`meta`字段中包含的`item_id`和`split`键上应用了过滤器。`retrieved_docs`的每个元素都是一个Haystack
    `Document`对象，用于表示文档并包括检索器的查询分数以及其他元数据。让我们看一下其中一个检索到的文档：
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In addition to the document’s text, we can see the `score` that Elasticsearch
    computed for its relevance to the query (larger scores imply a better match).
    Under the hood, Elasticsearch relies on [Lucene](https://lucene.apache.org) for
    indexing and search, so by default it uses Lucene’s *practical scoring function*.
    You can find the nitty-gritty details behind the scoring function in the [Elasticsearch
    documentation](https://oreil.ly/b1Seu), but in brief terms it first filters the
    candidate documents by applying a Boolean test (does the document match the query?),
    and then applies a similarity metric that’s based on representing both the document
    and the query as vectors.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除了文档的文本，我们还可以看到Elasticsearch为其与查询的相关性计算的`score`（较大的分数意味着更好的匹配）。在幕后，Elasticsearch依赖于[Lucene](https://lucene.apache.org)进行索引和搜索，因此默认情况下它使用Lucene的*practical
    scoring function*。您可以在[Elasticsearch文档](https://oreil.ly/b1Seu)中找到得分函数背后的细节，但简而言之，它首先通过应用布尔测试（文档是否与查询匹配）来过滤候选文档，然后应用基于将文档和查询表示为向量的相似度度量。
- en: Now that we have a way to retrieve relevant documents, the next thing we need
    is a way to extract answers from them. This is where the reader comes in, so let’s
    take a look at how we can load our MiniLM model in Haystack.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了检索相关文档的方法，接下来我们需要的是从中提取答案的方法。这就是读取器的作用，让我们看看如何在Haystack中加载我们的MiniLM模型。
- en: Initializing a reader
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化读取器
- en: 'In Haystack, there are two types of readers one can use to extract answers
    from a given context:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在Haystack中，有两种类型的读取器可以用来从给定的上下文中提取答案：
- en: '`FARMReader`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`FARMReader`'
- en: Based on deepset’s [*FARM* framework](https://farm.deepset.ai) for fine-tuning
    and deploying transformers. Compatible with models trained using ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers and can load models directly from the Hugging Face Hub.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于deepset的[*FARM*框架](https://farm.deepset.ai)进行transformers的微调和部署。与使用![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers训练的模型兼容，并且可以直接从Hugging Face Hub加载模型。
- en: '`TransformersReader`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`TransformersReader`'
- en: Based on the QA pipeline from ![nlpt_pin01](Images/nlpt_pin01.png) Transformers.
    Suitable for running inference only.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 基于![nlpt_pin01](Images/nlpt_pin01.png) Transformers的QA流水线。适用于仅运行推理。
- en: 'Although both readers handle a model’s weights in the same way, there are some
    differences in the way the predictions are converted to produce answers:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管两种读取器都以相同的方式处理模型的权重，但在转换预测以生成答案方面存在一些差异：
- en: In ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, the QA pipeline normalizes
    the start and end logits with a softmax in each passage. This means that it is
    only meaningful to compare answer scores between answers extracted from the same
    passage, where the probabilities sum to 1\. For example, an answer score of 0.9
    from one passage is not necessarily better than a score of 0.8 in another. In
    FARM, the logits are not normalized, so inter-passage answers can be compared
    more easily.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中，QA管道使用softmax在每个段落中对开始和结束的logits进行归一化。这意味着只有在从同一段落中提取的答案之间才有意义比较得分，其中概率总和为1。例如，来自一个段落的答案得分为0.9并不一定比另一个段落中的得分为0.8好。在FARM中，logits没有被归一化，因此可以更容易地比较跨段落的答案。
- en: The `TransformersReader` sometimes predicts the same answer twice, but with
    different scores. This can happen in long contexts if the answer lies across two
    overlapping windows. In FARM, these duplicates are removed.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TransformersReader`有时会预测相同的答案两次，但得分不同。如果答案跨越两个重叠的窗口，这可能会发生。在FARM中，这些重复项会被删除。'
- en: 'Since we will be fine-tuning the reader later in the chapter, we’ll use the
    `FARMReader`. As with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, to load
    the model we just need to specify the MiniLM checkpoint on the Hugging Face Hub
    along with some QA-specific arguments:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在本章后面对读取器进行微调，我们将使用`FARMReader`。与![nlpt_pin01](Images/nlpt_pin01.png) Transformers一样，要加载模型，我们只需要在Hugging
    Face Hub上指定MiniLM检查点以及一些特定于QA的参数：
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is also possible to fine-tune a reading comprehension model directly in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers and then load it in `TransformersReader` to run inference. For details
    on how to do the fine-tuning step, see the question answering tutorial in the
    [library’s documentation](https://oreil.ly/VkhIQ).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以直接在![nlpt_pin01](Images/nlpt_pin01.png) Transformers中微调阅读理解模型，然后加载到`TransformersReader`中进行推理。有关如何进行微调的详细信息，请参阅[库的文档](https://oreil.ly/VkhIQ)中的问答教程。
- en: 'In `FARMReader`, the behavior of the sliding window is controlled by the same
    `max_seq_length` and `doc_stride` arguments that we saw for the tokenizer. Here
    we’ve used the values from the MiniLM paper. To confirm, let’s now test the reader
    on our simple example from earlier:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在`FARMReader`中，滑动窗口的行为由我们在标记器中看到的`max_seq_length`和`doc_stride`参数控制。这里我们使用了MiniLM论文中的值。现在让我们在之前的简单示例上测试一下读取器：
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Great, the reader appears to be working as expected—so next, let’s tie together
    all our components using one of Haystack’s pipelines.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，读取器似乎正在按预期工作——接下来，让我们使用Haystack的一个管道将所有组件联系在一起。
- en: Putting it all together
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容放在一起
- en: 'Haystack provides a `Pipeline` abstraction that allows us to combine retrievers,
    readers, and other components together as a graph that can be easily customized
    for each use case. There are also predefined pipelines analogous to those in ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers, but specialized for QA systems. In our case, we’re interested in
    extracting answers, so we’ll use the `ExtractiveQAPipeline`, which takes a single
    retriever-reader pair as its arguments:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack提供了一个`Pipeline`抽象，允许我们将检索器、读取器和其他组件组合成一个图，可以根据每个用例轻松定制。还有预定义的管道类似于![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers中的管道，但专门为QA系统设计。在我们的案例中，我们对提取答案感兴趣，所以我们将使用`ExtractiveQAPipeline`，它以单个检索器-读取器对作为其参数：
- en: '[PRE49]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Each `Pipeline` has a `run()` method that specifies how the query flow should
    be executed. For the `ExtractiveQAPipeline` we just need to pass the `query`,
    the number of documents to retrieve with `top_k_retriever`, and the number of
    answers to extract from these documents with `top_k_reader`. In our case, we also
    need to specify a filter over the item ID, which can be done using the `filters`
    argument as we did with the retriever earlier. Let’s run a simple example using
    our question about the Amazon Fire tablet again, but this time returning the extracted
    answers:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`Pipeline`都有一个`run()`方法，指定查询流程应如何执行。对于`ExtractiveQAPipeline`，我们只需要传递`query`，用`top_k_retriever`指定要检索的文档数量，用`top_k_reader`指定要从这些文档中提取的答案数量。在我们的案例中，我们还需要使用`filters`参数指定对项目ID的过滤器，就像我们之前对检索器所做的那样。让我们再次运行一个关于亚马逊Fire平板电脑的简单示例，但这次返回提取的答案：
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Great, we now have an end-to-end QA system for Amazon product reviews! This
    is a good start, but notice that the second and third answers are closer to what
    the question is actually asking. To do better, we’ll need some metrics to quantify
    the performance of the retriever and reader. We’ll take a look at that next.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在我们有了一个端到端的Amazon产品评论QA系统！这是一个很好的开始，但请注意第二和第三个答案更接近实际问题。为了做得更好，我们需要一些指标来量化检索器和读取器的性能。我们接下来将看一下这一点。
- en: Improving Our QA Pipeline
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进我们的QA管道
- en: Although much of the recent research on QA has focused on improving reading
    comprehension models, in practice it doesn’t matter how good your reader is if
    the retriever can’t find the relevant documents in the first place! In particular,
    the retriever sets an upper bound on the performance of the whole QA system, so
    it’s important to make sure it’s doing a good job. With this in mind, let’s start
    by introducing some common metrics to evaluate the retriever so that we can compare
    the performance of sparse and dense representations.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近关于QA的研究大部分集中在改进阅读理解模型上，但实际上，如果检索器一开始就找不到相关文档，你的读取器有多好并不重要！特别是，检索器为整个QA系统的性能设定了一个上限，因此确保它做得好很重要。考虑到这一点，让我们首先介绍一些常见的指标来评估检索器，以便我们可以比较稀疏和密集表示的性能。
- en: Evaluating the Retriever
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估检索器
- en: A common metric for evaluating retrievers is *recall*, which measures the fraction
    of all relevant documents that are retrieved. In this context, “relevant” simply
    means whether the answer is present in a passage of text or not, so given a set
    of questions, we can compute recall by counting the number of times an answer
    appears in the top *k* documents returned by the retriever.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 评估检索器的常见指标是*召回率*，它衡量了检索到的所有相关文档的比例。在这种情况下，“相关”只是指答案是否出现在文本段落中，因此给定一组问题，我们可以通过计算答案出现在检索器返回的前*k*个文档中的次数来计算召回率。
- en: 'In Haystack, there are two ways to evaluate retrievers:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在Haystack中，有两种评估检索器的方法：
- en: Use the retriever’s in-built `eval()` method. This can be used for both open-
    and closed-domain QA, but not for datasets like SubjQA where each document is
    paired with a single product and we need to filter by product ID for every query.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用检索器内置的`eval()`方法。这可以用于开放域和封闭域的问答，但不能用于像SubjQA这样的数据集，其中每个文档都与一个产品配对，我们需要为每个查询按产品ID进行过滤。
- en: Build a custom `Pipeline` that combines a retriever with the `EvalRetriever`
    class. This enables the implementation of custom metrics and query flows.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个自定义的`Pipeline`，将检索器与`EvalRetriever`类结合在一起。这样可以实现自定义指标和查询流程。
- en: Note
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A complementary metric to recall is *mean average precision* (mAP), which rewards
    retrievers that can place the correct answers higher up in the document ranking.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与召回率相辅相成的指标是*平均精度*（mAP），它奖励能够将正确答案排在文档排名中较高位置的检索器。
- en: 'Since we need to evaluate the recall per product and then aggregate across
    all products, we’ll opt for the second approach. Each node in the `Pipeline` graph
    represents a class that takes some inputs and produces some outputs via a `run()`
    method:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要对每个产品进行召回率评估，然后在所有产品中进行聚合，我们将选择第二种方法。流水线图中的每个节点都代表一个通过`run()`方法获取一些输入并产生一些输出的类：
- en: '[PRE52]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Here `kwargs` corresponds to the outputs from the previous node in the graph,
    which is manipulated within the `run()` method to return a tuple of the outputs
    for the next node, along with a name for the outgoing edge. The only other requirement
    is to include an `outgoing_edges` attribute that indicates the number of outputs
    from the node (in most cases `outgoing_edges=1`, unless you have branches in the
    pipeline that route the inputs according to some criterion).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`kwargs`对应于图中前一个节点的输出，在`run()`方法中进行操作，以返回下一个节点的输出的元组，以及出边的名称。唯一的其他要求是包括一个`outgoing_edges`属性，指示节点的输出数量（在大多数情况下`outgoing_edges=1`，除非流水线中有根据某些标准路由输入的分支）。
- en: 'In our case, we need a node to evaluate the retriever, so we’ll use the `EvalRetriever`
    class whose `run()` method keeps track of which documents have answers that match
    the ground truth. With this class we can then build up a `Pipeline` graph by adding
    the evaluation node after a node that represents the retriever itself:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们需要一个节点来评估检索器，因此我们将使用`EvalRetriever`类，其`run()`方法跟踪哪些文档具有与地面真相匹配的答案。有了这个类，我们可以通过在代表检索器本身的节点之后添加评估节点来构建`Pipeline`图：
- en: '[PRE53]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Notice that each node is given a `name` and a list of `inputs`. In most cases,
    each node has a single outgoing edge, so we just need to include the name of the
    previous node in `inputs`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每个节点都有一个`name`和一个`inputs`列表。在大多数情况下，每个节点都有一个单独的出边，因此我们只需要在`inputs`中包含前一个节点的名称。
- en: 'Now that we have our evaluation pipeline, we need to pass some queries and
    their corresponding answers. To do this, we’ll add the answers to a dedicated
    `label` index on our document store. Haystack provides a `Label` object that represents
    the answer spans and their metadata in a standardized fashion. To populate the
    `label` index, we’ll first create a list of `Label` objects by looping over each
    question in the test set and extracting the matching answers and additional metadata:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了评估流水线，我们需要传递一些查询及其相应的答案。为此，我们将答案添加到我们文档存储中的专用`label`索引。Haystack提供了一个`Label`对象，以标准化的方式表示答案跨度及其元数据。为了填充`label`索引，我们将首先通过循环遍历测试集中的每个问题，并提取匹配的答案和附加的元数据来创建`Label`对象的列表：
- en: '[PRE54]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'If we peek at one of these labels:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看其中一个标签：
- en: '[PRE55]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'we can see the question-answer pair, along with an `origin` field that contains
    the unique question ID so we can filter the document store per question. We’ve
    also added the product ID to the `meta` field so we can filter the labels by product.
    Now that we have our labels, we can write them to the `label` index on Elasticsearch
    as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到问题-答案对，以及一个包含唯一问题ID的`origin`字段，以便我们可以根据问题过滤文档存储。我们还将产品ID添加到`meta`字段中，以便我们可以按产品筛选标签。现在我们有了标签，我们可以按以下方式将它们写入Elasticsearch上的`label`索引：
- en: '[PRE57]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we need to build up a mapping between our question IDs and corresponding
    answers that we can pass to the pipeline. To get all the labels, we can use the
    `get_all_labels_aggregated()` method from the document store that will aggregate
    all question-answer pairs associated with a unique ID. This method returns a list
    of `MultiLabel` objects, but in our case we only get one element since we’re filtering
    by question ID. We can build up a list of aggregated labels as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要建立一个映射，将我们的问题ID和相应的答案传递给流水线。为了获取所有标签，我们可以使用文档存储中的`get_all_labels_aggregated()`方法，该方法将聚合与唯一ID相关联的所有问题-答案对。该方法返回一个`MultiLabel`对象的列表，但在我们的情况下，由于我们按问题ID进行过滤，所以只会得到一个元素。我们可以按以下方式建立一个聚合标签列表：
- en: '[PRE59]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'By peeking at one of these labels we can see that all the answers associated
    with a given question are aggregated together in a `multiple_answers` field:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看其中一个标签，我们可以看到与给定问题相关联的所有答案都聚合在一个`multiple_answers`字段中：
- en: '[PRE61]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We now have all the ingredients for evaluating the retriever, so let’s define
    a function that feeds each question-answer pair associated with each product to
    the evaluation pipeline and tracks the correct retrievals in our `pipe` object:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了评估检索器的所有要素，让我们定义一个函数，将与每个产品相关联的每个问题-答案对传递到评估流水线中，并在我们的`pipe`对象中跟踪正确的检索：
- en: '[PRE63]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Great, it works! Notice that we picked a specific value for `top_k_retriever`
    to specify the number of documents to retrieve. In general, increasing this parameter
    will improve the recall, but at the expense of providing more documents to the
    reader and slowing down the end-to-end pipeline. To guide our decision on which
    value to pick, we’ll create a function that loops over several *k* values and
    compute the recall across the whole test set for each *k*:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，它起作用了！请注意，我们选择了一个特定的值作为`top_k_retriever`，以指定要检索的文档数。一般来说，增加这个参数将提高召回率，但会增加向读者提供更多文档并减慢端到端流程的速度。为了指导我们选择哪个值，我们将创建一个循环遍历几个*k*值并计算每个*k*在整个测试集上的召回率的函数：
- en: '[PRE66]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'If we plot the results, we can see how the recall improves as we increase *k*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制结果，我们可以看到随着*k*的增加，召回率如何提高：
- en: '[PRE67]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](Images/nlpt_07in02.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_07in02.png)'
- en: From the plot, we can see that there’s an inflection point around <math alttext="k
    equals 5"><mrow><mi>k</mi> <mo>=</mo> <mn>5</mn></mrow></math> and we get almost
    perfect recall from <math alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>
    onwards. Let’s now take a look at retrieving documents with dense vector techniques.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，在<math alttext="k equals 5"><mrow><mi>k</mi> <mo>=</mo> <mn>5</mn></mrow></math>附近有一个拐点，从<math
    alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>开始我们几乎可以完美地召回。现在让我们来看看使用密集向量技术检索文档。
- en: Dense Passage Retrieval
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密集通道检索
- en: We’ve seen that we get almost perfect recall when our sparse retriever returns
    <math alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>
    documents, but can we do better at smaller values of *k*? The advantage of doing
    so is that we can pass fewer documents to the reader and thereby reduce the overall
    latency of our QA pipeline. A well-known limitation of sparse retrievers like
    BM25 is that they can fail to capture the relevant documents if the user query
    contains terms that don’t match exactly those of the review. One promising alternative
    is to use dense embeddings to represent the question and document, and the current
    state of the art is an architecture known as *Dense Passage Retrieval* (DPR).^([14](ch07.xhtml#idm46238711297696))
    The main idea behind DPR is to use two BERT models as encoders for the question
    and the passage. As illustrated in [Figure 7-10](#dpr), these encoders map the
    input text into a *d*-dimensional vector representation of the `[CLS]` token.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，当我们的稀疏检索器返回<math alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>个文档时，我们几乎可以完美地召回，但在较小的*k*值上我们能做得更好吗？这样做的优势在于我们可以向读者传递更少的文档，从而减少我们问答流程的总体延迟。像BM25这样的稀疏检索器的一个众所周知的局限性是，如果用户查询包含与评论完全不匹配的术语，它们可能无法捕获相关文档。一个有前途的替代方案是使用密集嵌入来表示问题和文档，当前的技术水平是一种被称为*密集通道检索*（DPR）的架构。^([14](ch07.xhtml#idm46238711297696))
    DPR背后的主要思想是使用两个BERT模型作为问题和段落的编码器。如[图7-10](#dpr)所示，这些编码器将输入文本映射到`[CLS]`标记的*d*维向量表示。
- en: '![DPR Architecture](Images/nlpt_0710.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![DPR架构](Images/nlpt_0710.png)'
- en: Figure 7-10\. DPR’s bi-encoder architecture for computing the relevance of a
    document and query
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。DPR的双编码器架构用于计算文档和查询的相关性
- en: 'In Haystack, we can initialize a retriever for DPR in a similar way to what
    we did for BM25\. In addition to specifying the document store, we also need to
    pick the BERT encoders for the question and passage. These encoders are trained
    by giving them questions with relevant (positive) passages and irrelevant (negative)
    passages, where the goal is to learn that relevant question-passage pairs have
    a higher similarity. For our use case, we’ll use encoders that have been fine-tuned
    on the NQ corpus in this way:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在Haystack中，我们可以像为BM25那样初始化DPR的检索器。除了指定文档存储外，我们还需要选择用于问题和段落的BERT编码器。通过给它们提供具有相关（正面）段落和不相关（负面）段落的问题进行训练，这些编码器被训练，目标是学习相关的问题-段落对具有更高的相似性。对于我们的用例，我们将使用已经以这种方式在NQ语料库上进行了微调的编码器：
- en: '[PRE68]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Here we’ve also set `embed_title=False` since concatenating the document’s
    title (i.e., `item_id`) doesn’t provide any additional information because we
    filter per product. Once we’ve initialized the dense retriever, the next step
    is to iterate over all the indexed documents in our Elasticsearch index and apply
    the encoders to update the embedding representation. This can be done as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们还设置了`embed_title=False`，因为连接文档的标题（即`item_id`）不会提供任何额外信息，因为我们按产品进行过滤。一旦我们初始化了密集检索器，下一步是迭代我们Elasticsearch索引中的所有索引文档，并应用编码器来更新嵌入表示。这可以通过以下方式完成：
- en: '[PRE69]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We’re now set to go! We can evaluate the dense retriever in the same way we
    did for BM25 and compare the top-*k* recall:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好了！我们可以像为BM25那样评估密集检索器，并比较前*k*个召回率：
- en: '[PRE70]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '![](Images/nlpt_07in03.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_07in03.png)'
- en: Here we can see that DPR does not provide a boost in recall over BM25 and saturates
    around <math alttext="k equals 3"><mrow><mi>k</mi> <mo>=</mo> <mn>3</mn></mrow></math>
    .
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到，DPR在召回率上并没有比BM25提供提升，并且在<math alttext="k equals 3"><mrow><mi>k</mi>
    <mo>=</mo> <mn>3</mn></mrow></math>左右饱和。
- en: Tip
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Performing similarity search of the embeddings can be sped up by using Facebook’s
    [FAISS library](https://oreil.ly/1E8Z0) as the document store. Similarly, the
    performance of the DPR retriever can be improved by fine-tuning on the target
    domain. If you’d like to learn how to fine-tune DPR, check out the Haystack [tutorial](https://oreil.ly/eXyro).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Facebook的[FAISS库](https://oreil.ly/1E8Z0)作为文档存储，可以加速嵌入的相似性搜索。同样，通过在目标领域进行微调，可以提高DPR检索器的性能。如果您想了解如何微调DPR，请查看Haystack的[教程](https://oreil.ly/eXyro)。
- en: Now that we’ve explored the evaluation of the retriever, let’s turn to evaluating
    the reader.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了检索器的评估，让我们转而评估读者。
- en: Evaluating the Reader
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估读者
- en: 'In extractive QA, there are two main metrics that are used for evaluating readers:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在抽取式问答中，有两个主要的指标用于评估读者：
- en: '*Exact Match (EM)*'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确匹配（EM）*'
- en: A binary metric that gives EM = 1 if the characters in the predicted and ground
    truth answers match exactly, and EM = 0 otherwise. If no answer is expected, the
    model gets EM = 0 if it predicts any text at all.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二进制度量，如果预测答案中的字符与真实答案完全匹配，则EM = 1，否则EM = 0。如果不期望有答案，那么如果模型预测任何文本，EM = 0。
- en: '*F*[1]-score'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*F*[1]-score'
- en: Measures the harmonic mean of the precision and recall.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 度量精确度和召回率的调和平均值。
- en: 'Let’s see how these metrics work by importing some helper functions from FARM
    and applying them to a simple example:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过从FARM导入一些辅助函数并将它们应用于一个简单的例子来看看这些度量是如何工作的：
- en: '[PRE71]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Under the hood, these functions first normalize the prediction and label by
    removing punctuation, fixing whitespace, and converting to lowercase. The normalized
    strings are then tokenized as a bag-of-words, before finally computing the metric
    at the token level. From this simple example we can see that EM is a much stricter
    metric than the *F*[1]-score: adding a single token to the prediction gives an
    EM of zero. On the other hand, the *F*[1]-score can fail to catch truly incorrect
    answers. For example, if our predicted answer span is “about 6000 dollars”, then
    we get:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，这些函数首先通过去除标点，修复空白和转换为小写来对预测和标签进行规范化。然后对规范化的字符串进行词袋式标记化，最后在标记级别计算度量。从这个简单的例子中，我们可以看到EM比*F*[1]-score严格得多：向预测中添加一个标记会使EM为零。另一方面，*F*[1]-score可能无法捕捉真正不正确的答案。例如，如果我们的预测答案范围是“大约6000美元”，那么我们得到：
- en: '[PRE73]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Relying on just the *F*[1]-score is thus misleading, and tracking both metrics
    is a good strategy to balance the trade-off between underestimating (EM) and overestimating
    (*F*[1]-score) model performance.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，仅依赖*F*[1]-score是误导性的，跟踪这两个度量是在权衡低估（EM）和高估（*F*[1]-score）模型性能之间的一个好策略。
- en: Now in general, there are multiple valid answers per question, so these metrics
    are calculated for each question-answer pair in the evaluation set, and the best
    score is selected over all possible answers. The overall EM and *F*[1] scores
    for the model are then obtained by averaging over the individual scores of each
    question-answer pair.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，每个问题可能有多个有效答案，因此这些度量是针对评估集中的每个问题-答案对计算的，并且从所有可能的答案中选择最佳分数。然后通过对每个问题-答案对的单独分数进行平均来获得模型的整体EM和*F*[1]分数。
- en: 'To evaluate the reader we’ll create a new pipeline with two nodes: a reader
    node and a node to evaluate the reader. We’ll use the `EvalReader` class that
    takes the predictions from the reader and computes the corresponding EM and *F*[1]
    scores. To compare with the SQuAD evaluation, we’ll take the best answers for
    each query with the `top_1_em` and `top_1_f1` metrics that are stored in `EvalAnswers`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估阅读器，我们将创建一个包含两个节点的新管道：一个阅读器节点和一个用于评估阅读器的节点。我们将使用`EvalReader`类，该类获取阅读器的预测并计算相应的EM和*F*[1]分数。为了与SQuAD评估进行比较，我们将使用`EvalAnswers`中存储的`top_1_em`和`top_1_f1`度量来获取每个查询的最佳答案：
- en: '[PRE75]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Notice that we specified `skip_incorrect_retrieval=False`. This is to ensure
    that the retriever always passes the context to the reader (as in the SQuAD evaluation).
    Now that we’ve run every question through the reader, let’s print the scores:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们指定了`skip_incorrect_retrieval=False`。这是为了确保检索器始终将上下文传递给阅读器（就像在SQuAD评估中一样）。现在我们已经通过阅读器运行了每个问题，让我们打印分数：
- en: '[PRE76]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '![](Images/nlpt_07in04.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_07in04.png)'
- en: OK, it seems that the fine-tuned model performs significantly worse on SubjQA
    than on SQuAD 2.0, where MiniLM achieves EM and *F*[1] scores of 76.1 and 79.5,
    respectively. One reason for the performance drop is that customer reviews are
    quite different from the Wikipedia articles the SQuAD 2.0 dataset is generated
    from, and the language they use is often informal. Another factor is likely the
    inherent subjectivity of our dataset, where both questions and answers differ
    from the factual information contained in Wikipedia. Let’s look at how to fine-tune
    a model on a dataset to get better results with domain adaptation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看起来经过精细调整的模型在SubjQA上的表现明显比在SQuAD 2.0上差，MiniLM在SQuAD 2.0上的EM和*F*[1]分别为76.1和79.5。性能下降的一个原因是客户评论与SQuAD
    2.0数据集生成的维基百科文章非常不同，它们使用的语言通常是非正式的。另一个因素可能是我们数据集固有的主观性，其中问题和答案都与维基百科中包含的事实信息不同。让我们看看如何在数据集上进行精细调整以获得更好的领域自适应结果。
- en: Domain Adaptation
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域自适应
- en: Although models that are fine-tuned on SQuAD will often generalize well to other
    domains, we’ve seen that for SubjQA the EM and *F*[1] scores of our model were
    much worse than for SQuAD. This failure to generalize has also been observed in
    other extractive QA datasets and is understood as evidence that transformer models
    are particularly adept at overfitting to SQuAD.^([15](ch07.xhtml#idm46238710735456))
    The most straightforward way to improve the reader is by fine-tuning our MiniLM
    model further on the SubjQA training set. The `FARMReader` has a `train()` method
    that is designed for this purpose and expects the data to be in SQuAD JSON format,
    where all the question-answer pairs are grouped together for each item as illustrated
    in [Figure 7-11](#squad-schema).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在SQuAD上进行了精细调整的模型通常会很好地推广到其他领域，但我们发现对于SubjQA，我们的模型的EM和*F*[1]分数要比SQuAD差得多。这种泛化失败也在其他抽取式QA数据集中观察到，并且被认为是证明了变压器模型特别擅长过度拟合SQuAD的证据。([15](ch07.xhtml#idm46238710735456))改进阅读器的最直接方法是在SubjQA训练集上进一步对我们的MiniLM模型进行精细调整。`FARMReader`有一个`train()`方法，专门用于此目的，并且期望数据以SQuAD
    JSON格式提供，其中所有问题-答案对都被分组在一起，如[图7-11](#squad-schema)所示。
- en: '![SQuAD Schema](Images/nlpt_0711.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![SQuAD Schema](Images/nlpt_0711.png)'
- en: Figure 7-11\. Visualization of the SQuAD JSON format
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11\. SQuAD JSON格式的可视化
- en: 'This is quite a complex data format, so we’ll need a few functions and some
    Pandas magic to help us do the conversion. The first thing we need to do is implement
    a function that can create the `paragraphs` array associated with each product
    ID. Each element in this array contains a single context (i.e., review) and a
    `qas` array of question-answer pairs. Here’s a function that builds up the `paragraphs`
    array:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常复杂的数据格式，因此我们需要一些函数和一些Pandas魔术来帮助我们进行转换。我们需要做的第一件事是实现一个可以创建与每个产品ID相关联的`paragraphs`数组的函数。该数组中的每个元素包含单个上下文（即评论）和问题-答案对的`qas`数组。以下是一个构建`paragraphs`数组的函数：
- en: '[PRE77]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, when we apply to the rows of a `DataFrame` associated with a single product
    ID, we get the SQuAD format:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们将与单个产品ID相关联的`DataFrame`的行应用于SQuAD格式时：
- en: '[PRE78]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The final step is to then apply this function to each product ID in the `DataFrame`
    of each split. The following `convert_to_squad()` function does this trick and
    stores the result in an *electronics-{split}.json* file:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将此函数应用于每个拆分的`DataFrame`中的每个产品ID。以下`convert_to_squad()`函数可以做到这一点，并将结果存储在*electronics-{split}.json*文件中：
- en: '[PRE80]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now that we have the splits in the right format, let’s fine-tune our reader
    by specifying the locations of the train and dev splits, along with where to save
    the fine-tuned model:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将拆分格式正确，让我们通过指定训练和开发拆分的位置以及保存微调模型的位置来微调我们的阅读器：
- en: '[PRE81]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'With the reader fine-tuned, let’s now compare its performance on the test set
    against our baseline model:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读器微调后，让我们现在将其在测试集上的性能与我们的基线模型进行比较：
- en: '[PRE82]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](Images/nlpt_07in05.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_07in05.png)'
- en: 'Wow, domain adaptation has increased our EM score by a factor of six and more
    than doubled the *F*[1]-score! At this point, you might be wondering why we didn’t
    just fine-tune a pretrained language model directly on the SubjQA training set.
    One reason is that we only have 1,295 training examples in SubjQA while SQuAD
    has over 100,000, so we might run into challenges with overfitting. Nevertheless,
    let’s take a look at what naive fine-tuning produces. For a fair comparison, we’ll
    use the same language model that was used for fine-tuning our baseline on SQuAD.
    As before, we’ll load up the model with the `FARMReader`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，领域适应将我们的EM分数提高了六倍以上，并且*F*[1]-score增加了一倍多！此时，您可能会想知道为什么我们不直接在SubjQA训练集上对预训练的语言模型进行微调。一个原因是SubjQA中只有1,295个训练示例，而SQuAD有超过100,000个，因此我们可能会遇到过拟合的挑战。尽管如此，让我们看看天真的微调会产生什么结果。为了公平比较，我们将使用与在SQuAD上微调基线时使用的相同语言模型。与以前一样，我们将使用`FARMReader`加载模型：
- en: '[PRE83]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Next, we fine-tune for one epoch:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行一轮微调：
- en: '[PRE84]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'and include the evaluation on the test set:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 并在测试集上进行评估：
- en: '[PRE85]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '![](Images/nlpt_07in06.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_07in06.png)'
- en: We can see that fine-tuning the language model directly on SubjQA results in
    considerably worse performance than fine-tuning on SQuAD and SubjQA.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，直接在SubjQA上对语言模型进行微调的结果比在SQuAD和SubjQA上进行微调的性能要差得多。
- en: Warning
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When dealing with small datasets, it is best practice to use cross-validation
    when evaluating transformers as they can be prone to overfitting. You can find
    an example of how to perform cross-validation with SQuAD-formatted datasets in
    the [FARM repository](https://oreil.ly/K3nK8).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 处理小数据集时，最佳做法是在评估变压器时使用交叉验证，因为它们可能容易过拟合。您可以在[FARM存储库](https://oreil.ly/K3nK8)中找到如何使用SQuAD格式的数据集执行交叉验证的示例。
- en: Evaluating the Whole QA Pipeline
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估整个QA管道
- en: 'Now that we’ve seen how to evaluate the reader and retriever components individually,
    let’s tie them together to measure the overall performance of our pipeline. To
    do so, we’ll need to augment our retriever pipeline with nodes for the reader
    and its evaluation. We’ve seen that we get almost perfect recall at <math alttext="k
    equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math> , so we can fix
    this value and assess the impact this has on the reader’s performance (since it
    will now receive multiple contexts per query compared to the SQuAD-style evaluation):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何分别评估阅读器和检索器组件，让我们将它们联系在一起，以衡量我们管道的整体性能。为此，我们需要为我们的检索器管道增加阅读器及其评估的节点。我们已经看到，在<math
    alttext="k equals 10"><mrow><mi>k</mi> <mo>=</mo> <mn>10</mn></mrow></math>时，我们几乎可以完美地召回，因此我们可以固定这个值，并评估这对阅读器性能的影响（因为现在它将收到每个查询的多个上下文，而不是SQuAD风格的评估）：
- en: '[PRE86]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: We can then compare the top 1 EM and *F*[1] scores for the model to predict
    an answer in the documents returned by the retriever in [Figure 7-12](#reader-vs-pipeline).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将模型预测文档中检索器返回的答案的前1个EM和*F*[1]分数与[图7-12](#reader-vs-pipeline)中的进行比较。
- en: '![](Images/nlpt_0712.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_0712.png)'
- en: Figure 7-12\. Comparison of EM and *F*[1] scores for the reader against the
    whole QA pipeline
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12。EM和*F*[1]分数的阅读器与整个QA管道的比较
- en: From this plot we can see the effect that the retriever has on the overall performance.
    In particular, there is an overall degradation compared to matching the question-context
    pairs, as is done in the SQuAD-style evaluation. This can be circumvented by increasing
    the number of possible answers that the reader is allowed to predict.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中，我们可以看到检索器对整体性能的影响。特别是，与匹配问题-上下文对相比，整体性能有所下降，这是在SQuAD风格的评估中所做的。这可以通过增加阅读器被允许预测的可能答案数量来避免。
- en: Until now we have only extracted answer spans from the context, but in general
    it could be that bits and pieces of the answer are scattered throughout the document
    and we would like our model to synthesize these fragments into a single coherent
    answer. Let’s have a look at how we can use generative QA to succeed at this task.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只从上下文中提取了答案范围，但一般情况下，答案的各个部分可能分散在整个文档中，我们希望我们的模型能够将这些片段综合成一个连贯的答案。让我们看看如何使用生成式QA来成功完成这项任务。
- en: Going Beyond Extractive QA
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越抽取式QA
- en: 'One interesting alternative to extracting answers as spans of text in a document
    is to generate them with a pretrained language model. This approach is often referred
    to as *abstractive* or *generative QA* and has the potential to produce better-phrased
    answers that synthesize evidence across multiple passages. Although less mature
    than extractive QA, this is a fast-moving field of research, so chances are that
    these approaches will be widely adopted in industry by the time you are reading
    this! In this section we’ll briefly touch on the current state of the art: *retrieval-augmented
    generation* (RAG).^([16](ch07.xhtml#idm46238709770880))'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档中提取答案作为文本段的一个有趣的替代方法是使用预训练语言模型生成它们。这种方法通常被称为*抽象*或*生成型QA*，它有潜力产生更好措辞的答案，综合了多个段落中的证据。虽然这种方法比抽取式QA不够成熟，但这是一个快速发展的研究领域，所以很可能在您阅读本文时，这些方法已经被广泛应用于工业中！在本节中，我们将简要介绍当前的技术水平：*检索增强生成*（RAG）。
- en: RAG extends the classic retriever-reader architecture that we’ve seen in this
    chapter by swapping the reader for a *generator* and using DPR as the retriever.
    The generator is a pretrained sequence-to-sequence transformer like T5 or BART
    that receives latent vectors of documents from DPR and then iteratively generates
    an answer based on the query and these documents. Since DPR and the generator
    are differentiable, the whole process can be fine-tuned end-to-end as illustrated
    in [Figure 7-13](#rag).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: RAG通过将读者替换为*生成器*并使用DPR作为检索器来扩展了我们在本章中看到的经典检索器-阅读器架构。生成器是一个预训练的序列到序列变换器，如T5或BART，它接收来自DPR的文档的潜在向量，然后根据查询和这些文档迭代生成答案。由于DPR和生成器是可微分的，整个过程可以端到端地进行微调，如[图7-13](#rag)所示。
- en: '![RAG Architecture](Images/nlpt_0713.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![RAG Architecture](Images/nlpt_0713.png)'
- en: Figure 7-13\. The RAG architecture for fine-tuning a retriever and generator
    end-to-end (courtesy of Ethan Perez)
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-13。RAG架构用于对检索器和生成器进行端到端微调（由Ethan Perez提供）
- en: 'To show RAG in action we’ll use the `DPRetriever` from earlier, so we just
    need to instantiate a generator. There are two types of RAG models to choose from:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示RAG的工作原理，我们将使用之前的`DPRetriever`，因此我们只需要实例化一个生成器。有两种类型的RAG模型可供选择：
- en: '*RAG-Sequence*'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '*RAG-Sequence*'
- en: Uses the same retrieved document to generate the complete answer. In particular,
    the top *k* documents from the retriever are fed to the generator, which produces
    an output sequence for each document, and the result is marginalized to obtain
    the best answer.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的检索文档来生成完整的答案。特别是，从检索器中获取的前*k*个文档被馈送到生成器，生成器为每个文档产生一个输出序列，然后对结果进行边际化以获得最佳答案。
- en: '*RAG-Token*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*RAG-Token*'
- en: Can use a different document to generate each token in the answer. This allows
    the generator to synthesize evidence from multiple documents.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用不同的文档来生成答案中的每个标记。这允许生成器从多个文档中综合证据。
- en: 'Since RAG-Token models tend to perform better than RAG-Sequence ones, we’ll
    use the token model that was fine-tuned on NQ as our generator. Instantiating
    a generator in Haystack is similar to instantiating the reader, but instead of
    specifying the `max_seq_length` and `doc_stride` parameters for a sliding window
    over the contexts, we specify hyperparameters that control the text generation:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RAG-Token模型往往比RAG-Sequence模型表现更好，我们将使用在NQ上进行微调的标记模型作为我们的生成器。在Haystack中实例化生成器类似于实例化阅读器，但是我们不是为上下文中的滑动窗口指定`max_seq_length`和`doc_stride`参数，而是指定控制文本生成的超参数：
- en: '[PRE87]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Here `num_beams` specifies the number of beams to use in beam search (text generation
    is covered at length in [Chapter 5](ch05.xhtml#chapter_generation)). As we did
    with the DPR retriever, we don’t embed the document titles since our corpus is
    always filtered per product ID.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`num_beams`指定了在波束搜索中使用的波束数量（文本生成在[第5章](ch05.xhtml#chapter_generation)中有详细介绍）。与DPR检索器一样，我们不嵌入文档标题，因为我们的语料库始终根据产品ID进行过滤。
- en: 'The next thing to do is tie together the retriever and generator using Haystack’s
    `GenerativeQAPipeline`:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要做的事情是使用Haystack的`GenerativeQAPipeline`将检索器和生成器联系起来：
- en: '[PRE88]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Note
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In RAG, both the query encoder and the generator are trained end-to-end, while
    the context encoder is frozen. In Haystack, the `GenerativeQAPipeline` uses the
    query encoder from `RAGenerator` and the context encoder from `DensePassageRetriever`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG中，查询编码器和生成器都是端到端训练的，而上下文编码器是冻结的。在Haystack中，`GenerativeQAPipeline`使用`RAGenerator`的查询编码器和`DensePassageRetriever`的上下文编码器。
- en: 'Let’s now give RAG a spin by feeding in some queries about the Amazon Fire
    tablet from before. To simplify the querying, we’ll write a simple function that
    takes the query and prints out the top answers:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过输入一些关于之前的亚马逊Fire平板电脑的查询来尝试一下RAG。为了简化查询，我们将编写一个简单的函数，该函数接受查询并打印出前几个答案：
- en: '[PRE89]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'OK, now we’re ready to give it a test:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们准备好测试一下：
- en: '[PRE90]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This result isn’t too bad for an answer, but it does suggest that the subjective
    nature of the question is confusing the generator. Let’s try with something a
    bit more factual:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这个答案并不算太糟糕，但它确实表明问题的主观性使生成器感到困惑。让我们尝试一些更加客观的问题：
- en: '[PRE92]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: This is more sensible! To get better results we could fine-tune RAG end-to-end
    on SubjQA; we’ll leave this as an exercise, but if you’re interested in exploring
    it there are scripts in the ![nlpt_pin01](Images/nlpt_pin01.png) [Transformers
    repository](https://oreil.ly/oZz4S) to help you get started.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这更有意义！为了获得更好的结果，我们可以在SubjQA上对RAG进行端到端的微调；我们将把这留作练习，但如果您有兴趣探索，可以在![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers repository](https://oreil.ly/oZz4S)中找到一些脚本来帮助您入门。
- en: Conclusion
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Well, that was a whirlwind tour of QA, and you probably have many more questions
    that you’d like answered (pun intended!). In this chapter, we discussed two approaches
    to QA (extractive and generative) and examined two different retrieval algorithms
    (BM25 and DPR). Along the way, we saw that domain adaptation can be a simple technique
    to boost the performance of our QA system by a significant margin, and we looked
    at a few of the most common metrics that are used for evaluating such systems.
    Although we focused on closed-domain QA (i.e., a single domain of electronic products),
    the techniques in this chapter can easily be generalized to the open-domain case;
    we recommend reading Cloudera’s excellent Fast Forward [QA series](https://oreil.ly/Fd6lc)
    to see what’s involved.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这是QA的一个风风火火的介绍，你可能还有很多问题想要得到答案（双关语！）。在本章中，我们讨论了QA的两种方法（提取式和生成式），并检查了两种不同的检索算法（BM25和DPR）。在这个过程中，我们看到领域适应可以是一个简单的技术，可以显著提高我们的QA系统的性能，并且我们看了一些用于评估这种系统的最常见的指标。尽管我们专注于封闭领域的QA（即电子产品的单一领域），但本章中的技术可以很容易地推广到开放领域的情况；我们建议阅读Cloudera出色的Fast
    Forward [QA系列](https://oreil.ly/Fd6lc)来了解其中的内容。
- en: Deploying QA systems in the wild can be a tricky business to get right, and
    our experience is that a significant part of the value comes from first providing
    end users with useful search capabilities, followed by an extractive component.
    In this respect, the reader can be used in novel ways beyond answering on-demand
    user queries. For example, researchers at [Grid Dynamics](https://oreil.ly/CGLh1)
    were able to use their reader to automatically extract a set of pros and cons
    for each product in a client’s catalog. They also showed that a reader can be
    used to extract named entities in a zero-shot fashion by creating queries like
    “What kind of camera?” Given its infancy and subtle failure modes, we recommend
    exploring generative QA only once the other two approaches have been exhausted.
    This “hierarchy of needs” for tackling QA problems is illustrated in [Figure 7-14](#qa-pyramid).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在野外部署QA系统可能是一个棘手的问题，我们的经验是，其中相当大一部分价值来自首先为最终用户提供有用的搜索功能，然后是一个提取组件。在这方面，读者可以以新颖的方式使用，超出了按需回答用户查询的范围。例如，[Grid
    Dynamics](https://oreil.ly/CGLh1)的研究人员能够使用他们的读者自动提取客户目录中每种产品的优缺点。他们还表明，可以通过创建查询，如“什么样的相机？”以零-shot方式使用读者来提取命名实体。鉴于其幼稚和微妙的失败模式，我们建议只有在其他两种方法耗尽后才探索生成式QA。解决QA问题的这种“需求层次结构”在[图7-14](#qa-pyramid)中有所说明。
- en: '![QA Pyramid](Images/nlpt_0714.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![QA金字塔](Images/nlpt_0714.png)'
- en: Figure 7-14\. The QA hierarchy of needs
  id: totrans-347
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-14. QA需求层次结构
- en: Looking ahead, one exciting research area is *multimodal QA*, which involves
    QA over multiple modalities like text, tables, and images. As described in the
    MultiModalQA benchmark,^([17](ch07.xhtml#idm46238709566048)) such systems could
    enable users to answer complex questions that integrate information across different
    modalities, like “When was the famous painting with two touching fingers completed?”
    Another area with practical business applications is QA over a *knowledge graph*,
    where the nodes of the graph correspond to real-world entities and their relations
    are defined by the edges. By encoding factoids as (*subject*, *predicate*, *object*)
    triples, one can use the graph to answer questions about a missing element. For
    an example that combines transformers with knowledge graphs, see the [Haystack
    tutorials](https://oreil.ly/n7lZb). One more promising direction is *automatic
    question generation* as a way to do some form of unsupervised/weakly supervised
    training using unlabeled data or data augmentation. Two recent examples include
    the papers on the Probably Answered Questions (PAQ) benchmark and synthetic data
    augmentation for cross-lingual settings.^([18](ch07.xhtml#idm46238709514224))
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，一个令人兴奋的研究领域是*多模态QA*，它涉及对文本、表格和图像等多种模态进行QA。正如MultiModalQA基准所描述的那样，^([17](ch07.xhtml#idm46238709566048))这样的系统可以使用户回答跨越不同模态的信息的复杂问题，比如“著名的两根手指的画是什么时候完成的？”另一个具有实际业务应用的领域是在*知识图谱*上进行QA，其中图的节点对应于现实世界的实体，它们的关系由边定义。通过将事实编码为（*主语*，*谓语*，*宾语*）三元组，可以使用图来回答关于缺失元素的问题。有一个将变压器与知识图谱相结合的例子，请参见[Haystack教程](https://oreil.ly/n7lZb)。另一个有前途的方向是*自动生成问题*，作为一种使用未标记数据或数据增强进行无监督/弱监督训练的方式。最近的两个例子包括Probably
    Answered Questions（PAQ）基准和跨语言设置的合成数据增强的论文。^([18](ch07.xhtml#idm46238709514224))
- en: In this chapter we’ve seen that in order to successfully use QA models for real-world
    use cases we need to apply a few tricks, such as implementing a fast retrieval
    pipeline to make predictions in near real time. Still, applying a QA model to
    a handful of preselected documents can take a couple of seconds on production
    hardware. Although this may not sound like much, imagine how different your experience
    would be if you had to wait a few seconds to get the results of a Google search—a
    few seconds of wait time can decide the fate of your transformer-powered application.
    In the next chapter we’ll have a look at a few methods to accelerate model predictions
    further.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到，为了成功地将QA模型应用于实际用例，我们需要应用一些技巧，比如实现一个快速检索管道，以便在几乎实时进行预测。尽管这听起来可能不像什么，但想象一下，如果您必须等待几秒钟才能获得谷歌搜索结果，您的体验会有多不同——几秒钟的等待时间可能决定了您的变压器应用程序的命运。在下一章中，我们将看一些加速模型预测的方法。
- en: ^([1](ch07.xhtml#idm46238714372976-marker)) Although, in this particular case,
    everyone agrees that Drop C is the best guitar tuning.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#idm46238714372976-marker))尽管在这种特殊情况下，每个人都同意Drop C是最好的吉他调音。
- en: '^([2](ch07.xhtml#idm46238714344800-marker)) J. Bjerva et al., [“SubjQA: A Dataset
    for Subjectivity and Review Comprehension”](https://arxiv.org/abs/2004.14283),
    (2020).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch07.xhtml#idm46238714344800-marker))J. Bjerva等人，[“SubjQA: A Dataset
    for Subjectivity and Review Comprehension”](https://arxiv.org/abs/2004.14283)，（2020）。'
- en: ^([3](ch07.xhtml#idm46238714342272-marker)) As we’ll soon see, there are also
    *unanswerable* questions that are designed to produce more robust models.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.xhtml#idm46238714342272-marker)) 正如我们将很快看到的，还有一些*无法回答*的问题，旨在产生更健壮的模型。
- en: '^([4](ch07.xhtml#idm46238714139488-marker)) D. Hendrycks et al., [“CUAD: An
    Expert-Annotated NLP Dataset for Legal Contract Review”](https://arxiv.org/abs/2103.06268),
    (2021).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch07.xhtml#idm46238714139488-marker)) D. Hendrycks等人，[“CUAD: 专家注释的法律合同审查NLP数据集”](https://arxiv.org/abs/2103.06268)，（2021）。'
- en: '^([5](ch07.xhtml#idm46238713782624-marker)) P. Rajpurkar et al., [“SQuAD: 100,000+
    Questions for Machine Comprehension of Text”](https://arxiv.org/abs/1606.05250),
    (2016).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.xhtml#idm46238713782624-marker)) P. Rajpurkar等人，[“SQuAD：超过10万个文本理解问题”](https://arxiv.org/abs/1606.05250)，（2016）。
- en: '^([6](ch07.xhtml#idm46238713780368-marker)) P. Rajpurkar, R. Jia, and P. Liang,
    [“Know What You Don’t Know: Unanswerable Questions for SQuAD”](https://arxiv.org/abs/1806.03822),
    (2018).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.xhtml#idm46238713780368-marker)) P. Rajpurkar, R. Jia, and P. Liang，[“知道你不知道的：SQuAD的无法回答的问题”](https://arxiv.org/abs/1806.03822)，（2018）。
- en: '^([7](ch07.xhtml#idm46238713770064-marker)) T. Kwiatkowski et al., “Natural
    Questions: A Benchmark for Question Answering Research,” *Transactions of the
    Association for Computational Linguistics* 7 (March 2019): 452–466, [*http://dx.doi.org/10.1162/tacl_a_00276*](http://dx.doi.org/10.1162/tacl_a_00276).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.xhtml#idm46238713770064-marker)) T. Kwiatkowski等人，“自然问题：问答研究的基准”，*计算语言学协会交易*
    7（2019年3月）：452-466，[*http://dx.doi.org/10.1162/tacl_a_00276*](http://dx.doi.org/10.1162/tacl_a_00276)。
- en: '^([8](ch07.xhtml#idm46238713719168-marker)) W. Wang et al., [“MINILM: Deep
    Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers”](https://arxiv.org/abs/2002.10957),
    (2020).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch07.xhtml#idm46238713719168-marker)) W. Wang等人，[“MINILM: 用于任务不可知的预训练变压器的深度自注意压缩”](https://arxiv.org/abs/2002.10957)，（2020）。'
- en: ^([9](ch07.xhtml#idm46238713601696-marker)) Note that the `token_type_ids` are
    not present in all transformer models. In the case of BERT-like models such as
    MiniLM, the `token_type_ids` are also used during pretraining to incorporate the
    next sentence prediction task.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch07.xhtml#idm46238713601696-marker)) 请注意，`token_type_ids`并不是所有变压器模型都具有的。对于MiniLM等类似BERT的模型，`token_type_ids`在预训练期间也用于整合下一个句子预测任务。
- en: ^([10](ch07.xhtml#idm46238713530720-marker)) See [Chapter 2](ch02.xhtml#chapter_classification)
    for details on how these hidden states can be extracted.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch07.xhtml#idm46238713530720-marker)) 有关如何提取这些隐藏状态的详细信息，请参阅[第2章](ch02.xhtml#chapter_classification)。
- en: ^([11](ch07.xhtml#idm46238713054704-marker)) A vector is sparse if most of its
    elements are zero.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch07.xhtml#idm46238713054704-marker)) 如果一个向量的大部分元素都是零，那么它就是稀疏的。
- en: ^([12](ch07.xhtml#idm46238713004128-marker)) The guide also provides installation
    instructions for macOS and Windows.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch07.xhtml#idm46238713004128-marker)) 该指南还提供了macOS和Windows的安装说明。
- en: ^([13](ch07.xhtml#idm46238712667392-marker)) For an in-depth explanation of
    document scoring with TF-IDF and BM25 see Chapter 23 of *Speech and Language Processing*,
    3rd edition, by D. Jurafsky and J.H. Martin (Prentice Hall).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch07.xhtml#idm46238712667392-marker)) 有关使用TF-IDF和BM25进行文档评分的深入解释，请参阅D.
    Jurafsky和J.H. Martin（Prentice Hall）的《语音和语言处理》第3版第23章。
- en: ^([14](ch07.xhtml#idm46238711297696-marker)) V. Karpukhin et al., [“Dense Passage
    Retrieval for Open-Domain Question Answering”](https://arxiv.org/abs/2004.04906),
    (2020).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch07.xhtml#idm46238711297696-marker)) V. Karpukhin等人，[“用于开放领域问答的密集通道检索”](https://arxiv.org/abs/2004.04906)，（2020）。
- en: ^([15](ch07.xhtml#idm46238710735456-marker)) D. Yogatama et al., [“Learning
    and Evaluating General Linguistic Intelligence”](https://arXiv.org/abs/1901.11373),
    (2019).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch07.xhtml#idm46238710735456-marker)) D. Yogatama等人，[“学习和评估通用语言智能”](https://arXiv.org/abs/1901.11373)，（2019）。
- en: ^([16](ch07.xhtml#idm46238709770880-marker)) P. Lewis et al., [“Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks”](https://arxiv.org/abs/2005.11401),
    (2020).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch07.xhtml#idm46238709770880-marker)) P. Lewis等人，[“用于知识密集型NLP任务的检索增强生成”](https://arxiv.org/abs/2005.11401)，（2020）。
- en: '^([17](ch07.xhtml#idm46238709566048-marker)) A. Talmor et al., [“MultiModalQA:
    Complex Question Answering over Text, Tables and Images”](https://arxiv.org/abs/2104.06039),
    (2021).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch07.xhtml#idm46238709566048-marker)) A. Talmor等人，[“MultiModalQA：文本、表格和图像的复杂问答”](https://arxiv.org/abs/2104.06039)，（2021）。
- en: '^([18](ch07.xhtml#idm46238709514224-marker)) P. Lewis et al., [“PAQ: 65 Million
    Probably-Asked Questions and What You Can Do with Them”](https://arxiv.org/abs/2102.07033),
    (2021); A. Riabi et al., [“Synthetic Data Augmentation for Zero-Shot Cross-Lingual
    Question Answering”](https://arxiv.org/abs/2010.12643), (2020).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '^([18](ch07.xhtml#idm46238709514224-marker)) P. Lewis等人，[“PAQ: 6500万个可能被问到的问题及其用途”](https://arxiv.org/abs/2102.07033)，（2021）；A.
    Riabi等人，[“用于零样本跨语言问答的合成数据增强”](https://arxiv.org/abs/2010.12643)，（2020）。'
