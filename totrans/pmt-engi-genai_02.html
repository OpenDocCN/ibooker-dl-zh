<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Introduction to Large Language Models for Text Generation"><div class="chapter" id="intro_text_02">
<h1><span class="label">Chapter 2. </span>Introduction to Large Language Models <span class="keep-together">for Text Generation</span></h1>


<p>In artificial intelligence, a recent focus has been the evolution of large language models. Unlike their less-flexible predecessors, LLMs are capable of handling and learning from a much larger volume of data, resulting in the emergent capability of producing text that closely resembles human language output. These models have generalized across diverse applications, from writing content to automating software development and enabling real-time interactive chatbot experiences.</p>






<section data-type="sect1" data-pdf-bookmark="What Are Text Generation Models?"><div class="sect1" id="id8">
<h1>What Are Text Generation Models?</h1>

<p>Text generation models utilize advanced <a data-type="indexterm" data-primary="text generation" data-secondary="models" id="txgrmd"/>algorithms to understand the meaning in text and produce outputs that are often indistinguishable from human work. If you‚Äôve ever interacted with <a href="https://chat.openai.com">ChatGPT</a> or marveled at its ability to craft coherent and contextually relevant sentences, you‚Äôve witnessed the power of an LLM in action.</p>

<p>In natural language processing (NLP) and LLMs, the <a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="tokens" id="id460"/><a data-type="indexterm" data-primary="natural language processing (NLP)" data-see="See NLP (natural language processing)" id="id461"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="tokens" id="id462"/><a data-type="indexterm" data-primary="tokens" data-secondary="LLMs" id="id463"/><a data-type="indexterm" data-primary="tokens" data-secondary="NLP and" id="id464"/><a data-type="indexterm" data-primary="text generation" data-secondary="NLP and" id="txgpln"/>fundamental linguistic unit is a <em>token</em>. <a href="https://oreil.ly/3fOsM">Tokens</a> can represent sentences, words, or even subwords such as a set of characters. A useful way to understand the size of text data is by looking at the number of tokens it comprises; for instance, a text of 100 tokens roughly equates to about 75 words. This comparison can be essential for managing the processing limits of LLMs as different models may have varying token capacities.</p>

<p class="less_space pagebreak-before"><em>Tokenization</em>, the process of breaking <a data-type="indexterm" data-primary="tokenization" data-secondary="NLP" id="id465"/><a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="tokenization" id="id466"/>down text into tokens, is a crucial step in preparing data for NLP tasks. Several <a data-type="indexterm" data-primary="BPE (Byte-Pair Encoding)" id="id467"/><a data-type="indexterm" data-primary="WordPiece" id="id468"/><a data-type="indexterm" data-primary="SentencePiece" id="id469"/>methods can be used for tokenization, including <a href="https://oreil.ly/iSOp7">Byte-Pair Encoding (BPE)</a>, WordPiece, and SentencePiece. Each of these methods has its unique advantages and is suited to particular use cases. BPE is commonly used due to its efficiency in handling a wide range of vocabulary while keeping the number of tokens manageable.</p>

<p>BPE begins by viewing a text as a series of individual characters. Over time, it combines characters that frequently appear together into single units, or tokens. To understand this better, consider the word <em>apple</em>. Initially, BPE might see it as <em>a</em>, <em>p</em>, <em>p</em>, <em>l</em>, and <em>e</em>. But after noticing that <em>p</em> often comes after <em>a</em> and before <em>l</em> in the dataset, it might combine them and treat <em>appl</em> as a single token in future instances.</p>

<p>This approach helps LLMs recognize and generate words or phrases, even if they weren‚Äôt common in the training data, making the models more adaptable and <span class="keep-together">versatile.</span></p>

<p>Understanding the workings of LLMs requires a grasp of the underlying mathematical principles that power these systems. Although the computations can be complex, we can simplify the core elements to provide an intuitive understanding of how these models operate. Particularly within a business context, the accuracy and reliability of LLMs are paramount.</p>

<p>A significant part of achieving this reliability lies in the pretraining and fine-tuning phases of LLM development. Initially, models are trained on vast datasets during the pretraining phase, acquiring a broad understanding of language. Subsequently, in the fine-tuning phase, models are adapted for specific tasks, honing their capabilities to provide accurate and reliable outputs for specialized applications.</p>








<section data-type="sect2" data-pdf-bookmark="Vector Representations: The Numerical Essence of Language"><div class="sect2" id="id9">
<h2>Vector Representations: The Numerical Essence of Language</h2>

<p>In the realm of NLP, words aren‚Äôt just <a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="vectors" id="id470"/><a data-type="indexterm" data-primary="vectors" data-secondary="NLP and" id="id471"/><a data-type="indexterm" data-primary="text generation" data-secondary="vector representation" id="id472"/>alphabetic symbols. They can be tokenized and then represented in a numerical form, known as <em>vectors</em>. These vectors are multi-dimensional arrays of numbers that capture the semantic and syntactic relations:</p>
<div data-type="equation">
<math alttext="w right-arrow bold v equals left-bracket v 1 comma v 2 comma ellipsis comma v Subscript n Baseline right-bracket" display="block">
  <mrow>
    <mi>w</mi>
    <mo>‚Üí</mo>
    <mi>ùêØ</mi>
    <mo>=</mo>
    <mo>[</mo>
    <msub><mi>v</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>v</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>v</mi> <mi>n</mi> </msub>
    <mo>]</mo>
  </mrow>
</math>
</div>

<p>Creating word vectors, also known as <em>word embeddings</em>, relies <a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="vectors" data-tertiary="word vectors" id="id473"/><a data-type="indexterm" data-primary="vectors" data-secondary="NLP and" data-tertiary="word vectors" id="id474"/><a data-type="indexterm" data-primary="word vectors" id="id475"/><a data-type="indexterm" data-primary="word embeddings" id="id476"/><a data-type="indexterm" data-primary="NLP (natural language processing)" data-secondary="vectors" data-tertiary="word embeddings" id="id477"/><a data-type="indexterm" data-primary="vectors" data-secondary="NLP and" data-tertiary="word embeddings" id="id478"/><a data-type="indexterm" data-primary="text generation" data-secondary="word embeddings" id="id479"/>on intricate patterns within language. During an intensive training phase, models are designed to identify and learn these patterns, ensuring that words with similar meanings are mapped close to one another in a high-dimensional space (<a data-type="xref" href="#figure-2-1">Figure¬†2-1</a>).</p>

<figure><div id="figure-2-1" class="figure">
<img src="assets/pega_0201.png" alt="Word Embeddings" width="600" height="291"/>
<h6><span class="label">Figure 2-1. </span>Semantic proximity of word vectors within a word embedding space</h6>
</div></figure>

<p>The beauty of this approach is its ability to capture nuanced relationships between words and calculate their distance. When we examine word embeddings, it becomes evident that words with similar or related meanings like <em>virtue</em> and <em>moral</em> or <em>walked</em> and <em>walking</em> are situated near each other. This spatial closeness in the embedding space becomes a powerful tool in various NLP tasks, enabling models to understand context, semantics, and the intricate web of relationships that form language.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Transformer Architecture: Orchestrating Contextual Relationships"><div class="sect2" id="id10">
<h2>Transformer Architecture: Orchestrating Contextual Relationships</h2>

<p>Before we go deep into the mechanics of <a data-type="indexterm" data-primary="text generation" data-secondary="transformers" id="id480"/><a data-type="indexterm" data-primary="transformer architecture" id="id481"/>transformer architectures, let‚Äôs build a foundational understanding. In simple terms, when we have a sentence, say, <em>The cat sat on the mat</em>, each word in this sentence gets converted into its numerical vector representation. So, <em>cat</em> might become a series of numbers, as does <em>sat</em>, <em>on</em>, and <em>mat</em>.</p>

<p>As you‚Äôll explore in detail later in this chapter, the transformer architecture takes these word vectors and understands their relationships‚Äîboth in structure (syntax) and meaning (semantics). There are many types of transformers; <a data-type="xref" href="#figure-2-2">Figure¬†2-2</a> showcases both BERT and GPT‚Äôs architecture. Additionally, a transformer doesn‚Äôt just see words in isolation; it looks at <em>cat</em> and knows it‚Äôs related to <em>sat</em> and <em>mat</em> in a specific way in this sentence.</p>

<figure><div id="figure-2-2" class="figure">
<img src="assets/pega_0202.png" alt="BERT and GPT architecture" width="600" height="771"/>
<h6><span class="label">Figure 2-2. </span>BERT uses an encoder for input data, while GPT has a decoder for output</h6>
</div></figure>

<p>When the transformer processes these vectors, it uses mathematical operations to understand the relationships between the words, thereby producing new vectors with rich, contextual information:</p>
<div data-type="equation">
<math alttext="bold v prime Subscript i Baseline equals Transformer left-parenthesis bold v 1 comma bold v 2 comma ellipsis comma bold v Subscript m Baseline right-parenthesis" display="block">
  <mrow>
    <msubsup><mi>ùêØ</mi> <mi>i</mi> <mo>'</mo> </msubsup>
    <mo>=</mo>
    <mtext>Transformer</mtext>
    <mrow>
      <mo>(</mo>
      <msub><mi>ùêØ</mi> <mn>1</mn> </msub>
      <mo>,</mo>
      <msub><mi>ùêØ</mi> <mn>2</mn> </msub>
      <mo>,</mo>
      <mo>...</mo>
      <mo>,</mo>
      <msub><mi>ùêØ</mi> <mi>m</mi> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p class="less_space pagebreak-before">One of the remarkable features of transformers is their ability to comprehend the nuanced contextual meanings of words. The <a href="https://oreil.ly/xuovP">self-attention</a> mechanism in transformers lets each word in a sentence look at all other words to understand its context better. Think of it like each word casting votes on the importance of other words for its meaning. By considering the entire sentence, transformers can more accurately determine the role and meaning of <a data-type="indexterm" data-primary="contextual meanings" id="id482"/>each word, making their <em>interpretations more contextually rich.</em></p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Probabilistic Text Generation: The Decision Mechanism"><div class="sect2" id="id11">
<h2>Probabilistic Text Generation: The Decision Mechanism</h2>

<p>After the transformer understands the <a data-type="indexterm" data-primary="text generation" data-secondary="probability and" id="id483"/><a data-type="indexterm" data-primary="probability, text generation and" id="id484"/>context of the given text, it moves on to generating new text, guided by the concept of likelihood or probability. In mathematical terms, the model calculates how likely each possible next word is to follow the current sequence of words and picks the one that is most likely:</p>
<div data-type="equation">
<math alttext="w Subscript next Baseline equals argmax upper P left-parenthesis w vertical-bar w 1 comma w 2 comma ellipsis comma w Subscript m Baseline right-parenthesis" display="block">
  <mrow>
    <msub><mi>w</mi> <mtext>next</mtext> </msub>
    <mo>=</mo>
    <mtext>argmax</mtext>
    <mspace width="0.166667em"/>
    <mi>P</mi>
    <mrow>
      <mo>(</mo>
      <mi>w</mi>
      <mo>|</mo>
      <msub><mi>w</mi> <mn>1</mn> </msub>
      <mo>,</mo>
      <msub><mi>w</mi> <mn>2</mn> </msub>
      <mo>,</mo>
      <mo>...</mo>
      <mo>,</mo>
      <msub><mi>w</mi> <mi>m</mi> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>By repeating this process, as shown in <a data-type="xref" href="#figure-2-3">Figure¬†2-3</a>, the model generates a coherent and contextually relevant string of text as its output.</p>

<figure><div id="figure-2-3" class="figure">
<img src="assets/pega_0203.png" alt="An illustrative overview of how text is generated using transformer models like GPT-4." width="600" height="461"/>
<h6><span class="label">Figure 2-3. </span>How text is generated using transformer models such as GPT-4</h6>
</div></figure>

<p>The mechanisms driving LLMs are rooted in <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="vector mathematics and" id="id485"/><a data-type="indexterm" data-primary="vector mathematics" id="id486"/><a data-type="indexterm" data-primary="linear transformations" id="id487"/><a data-type="indexterm" data-primary="probabilistic models" id="id488"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="linear transformations" id="id489"/><a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="probabilistic models" id="id490"/>vector mathematics, linear transformations, and probabilistic models. While the under-the-hood operations are computationally intensive, the core concepts are built on these mathematical principles, offering a foundational understanding that bridges the gap between technical complexity and business <a data-type="indexterm" data-primary="text generation" data-secondary="models" data-startref="txgrmd" id="id491"/><a data-type="indexterm" data-primary="text generation" data-secondary="NLP and" data-startref="txgpln" id="id492"/>applicability.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Historical Underpinnings: The Rise of Transformer Architectures"><div class="sect1" id="id12">
<h1>Historical Underpinnings: The Rise of <span class="keep-together">Transformer Architectures</span></h1>

<p>Language models like ChatGPT (the <em>GPT</em> stands for <em>generative pretrained transformer</em>) didn‚Äôt magically <a data-type="indexterm" data-primary="transformer architecture" data-secondary="history" id="trfsrmary"/><a data-type="indexterm" data-primary="GPTs (generative pre-trained transformers)" id="id493"/><a data-type="indexterm" data-primary="transformer architecture" data-secondary="GPTs" id="id494"/>emerge. They‚Äôre the culmination of years of progress in the field of NLP, with particular acceleration since the late 2010s. At the heart of this advancement is the introduction of transformer architectures, which were detailed in the groundbreaking paper <a href="https://oreil.ly/6NNbg">‚ÄúAttention Is All You Need‚Äù</a> by the Google Brain team.</p>

<p>The real breakthrough of transformer architectures was the concept of <em>attention</em>. Traditional models <a data-type="indexterm" data-primary="transformer architecture" data-secondary="attention" id="id495"/>processed text sequentially, which limited their understanding of language structure especially over long distances of text. Attention transformed this by allowing models to directly relate distant words to one another irrespective of their positions in the text. This was a groundbreaking proposition. It meant that words and their context didn‚Äôt have to move through the entire model to affect each other. This not only significantly improved the models‚Äô text comprehension but also made them much more efficient.</p>

<p>This attention mechanism played a vital role in expanding the models‚Äô capacity to detect long-range dependencies in text. This was crucial for generating outputs that were not just contextually accurate and fluent, but also coherent over longer stretches.</p>

<p>According to AI pioneer and educator <a href="https://oreil.ly/JQd53">Andrew Ng</a>, much of the early NLP research, including the fundamental work on transformers, received significant funding from United States military intelligence agencies. Their keen interest in tools like machine translation and speech recognition, primarily for intelligence purposes, inadvertently paved the way for developments that transcended just translation.</p>

<p>Training LLMs requires extensive <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="training" id="id496"/>computational resources. These models are fed with vast amounts of data, ranging from terabytes to petabytes, including internet content, academic papers, books, and more niche datasets tailored for specific purposes. It‚Äôs important to note, however, that the <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="inherent bias" id="id497"/><a data-type="indexterm" data-primary="inherent bias" id="id498"/>data used to train LLMs can carry <em>inherent biases from their sources</em>. Thus, users should exercise caution and ideally employ human oversight when leveraging these models, ensuring responsible and ethical AI applications.</p>

<p>OpenAI‚Äôs GPT-4, for example, boasts an estimated <a href="https://oreil.ly/pZvMo">1.7 trillion parameters</a>, which is equivalent to an Excel spreadsheet that stretches across thirty thousand soccer fields. <em>Parameters</em> in the context of neural networks <a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-4" data-tertiary="parameters" id="id499"/><a data-type="indexterm" data-primary="GPT-4" data-secondary="parameters" id="id500"/>are the weights and biases adjusted throughout the training process, allowing the model to represent and generate complex patterns based on the data it‚Äôs trained on. The training cost for GPT-4 was estimated to be in the order of <a href="https://oreil.ly/_NAq5">$63 million</a>, and the training data would fill about <a href="https://oreil.ly/D7jL5">650 kilometers of bookshelves full of books</a>.</p>

<p>To meet these requirements, major technological companies such as Microsoft, Meta, and Google have invested heavily, making LLM development a high-stakes endeavor.</p>

<p>The rise of LLMs has provided an increased demand for the hardware industry, particularly companies specializing in <a data-type="indexterm" data-primary="LLMs (large language models)" data-secondary="GPUs (graphics processing units) and" id="id501"/><a data-type="indexterm" data-primary="GPUs (graphics processing units)" data-secondary="LLMs and" id="id502"/>graphics processing units (GPUs). NVIDIA, for instance, has become almost synonymous with high-performance GPUs that are essential for training LLMs.</p>

<p>The demand for powerful, efficient GPUs has skyrocketed as companies strive to build ever-larger and more complex models. It‚Äôs not just the raw computational power that‚Äôs sought after. GPUs also need to be fine-tuned for tasks endemic to machine learning, like <a data-type="indexterm" data-primary="tensors" id="id503"/><a data-type="indexterm" data-primary="GPUs (graphics processing units)" data-secondary="tensors" id="id504"/>tensor operations. <em>Tensors</em>, in a machine learning context, are multidimensional arrays of data, and operations on them are foundational to neural network computations. This emphasis on specialized capabilities has given rise to tailored hardware such as NVIDIA‚Äôs H100 Tensor Core GPUs, explicitly crafted to expedite machine learning workloads.</p>

<p>Furthermore, the overwhelming demand often outstrips the supply of these top-tier GPUs, sending prices on an upward trajectory. This supply-demand interplay has transformed the GPU market into a fiercely competitive and profitable arena. Here, an eclectic clientele, ranging from tech behemoths to academic researchers, scramble to procure the most advanced hardware.</p>

<p>This surge in demand has sparked a wave of innovation beyond just GPUs. Companies are now focusing on creating <a data-type="indexterm" data-primary="TPUs (Tensor Processing Units)" id="id505"/>dedicated AI hardware, such as Google‚Äôs Tensor Processing Units (TPUs), to cater to the growing computational needs of AI models.</p>

<p>This evolving landscape underscores not just the symbiotic ties between software and hardware in the AI sphere but also spotlights the ripple effect of the LLM <em>gold rush</em>. It‚Äôs steering innovations and funneling investments into various sectors, especially those <a data-type="indexterm" data-primary="transformer architecture" data-secondary="history" data-startref="trfsrmary" id="id506"/>offering the fundamental components for crafting these models.</p>
</div></section>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="OpenAI‚Äôs Generative Pretrained Transformers"><div class="sect1" id="id13">
<h1>OpenAI‚Äôs Generative Pretrained Transformers</h1>

<p>Founded with a mission to ensure that artificial general intelligence benefits all of humanity, <a href="https://openai.com">OpenAI</a> has recently been at the forefront of the AI revolution. One of their most groundbreaking contributions has been the GPT series of models, which have substantially redefined the boundaries of what LLMs can achieve.</p>

<p>The original GPT model by OpenAI was more than a mere research output; it was a compelling demonstration of the potential of transformer-based architectures. This model showcased the initial steps toward making machines understand and generate human-like language, laying the foundation for future advancements.</p>

<p>The unveiling of GPT-2 was met with <a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-2" id="id507"/><a data-type="indexterm" data-primary="GPT-2" id="id508"/>both anticipation and caution. Recognizing the model‚Äôs powerful capabilities, OpenAI initially hesitated in releasing it due to concerns about its potential misuse. Such was the might of GPT-2 that ethical concerns took center stage, which might look quaint compared to the power of today‚Äôs models. However, when OpenAI decided to release the project as <a href="https://oreil.ly/evOQE">open-source</a>, it didn‚Äôt just mean making the code public. It allowed businesses and researchers to use these pretrained models as building blocks, incorporating AI into their applications without starting from scratch. This move democratized access to high-level natural language processing capabilities, spurring innovation across various domains.</p>

<p>After GPT-2, OpenAI decided to focus on releasing paid, closed-source models. GPT-3‚Äôs arrival marked a monumental stride in the progression of LLMs. It garnered significant media attention, not just for its technical prowess but also for the societal implications of its capabilities. This model could produce text so convincing that it often became indistinguishable from human-written content. From crafting intricate pieces of literature to churning out operational code snippets, GPT-3 exemplified the seemingly boundless potential of AI.</p>








<section data-type="sect2" data-pdf-bookmark="GPT-3.5-turbo and ChatGPT"><div class="sect2" id="id14">
<h2>GPT-3.5-turbo and ChatGPT</h2>

<p>Bolstered by Microsoft‚Äôs significant <a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-3.5-turbo" id="id509"/><a data-type="indexterm" data-primary="GPT-3.5-turbo" id="id510"/><a data-type="indexterm" data-primary="OpenAI" data-secondary="ChatGPT" id="id511"/><a data-type="indexterm" data-primary="ChatGPT" id="id512"/>investment in their company, OpenAI introduced GPT-3.5-turbo, an optimized version of its already exceptional predecessor. Following a <a href="https://oreil.ly/1C8qm">$1 billion injection</a> from Microsoft in 2019, which later increased to a hefty $13 billion for a 49% stake in OpenAI‚Äôs for-profit arm, OpenAI used these resources to develop GPT-3.5-turbo, which offered improved efficiency and affordability, effectively making LLMs more accessible for a broader range of use cases.</p>

<p class="less_space pagebreak-before">OpenAI wanted to gather more world feedback for fine-tuning, and so <a href="https://chat.openai.com">ChatGPT</a> was born. Unlike its general-purpose siblings, <a href="https://oreil.ly/6ib-Q">ChatGPT was fine-tuned</a> to excel in conversational contexts, enabling a dialogue between humans and machines that felt natural and meaningful.</p>

<p><a data-type="xref" href="#figure-2-4">Figure¬†2-4</a> shows the training process for ChatGPT, which involves three main steps:</p>
<dl>
<dt>Collection of demonstration data</dt>
<dd>
<p>In this <a data-type="indexterm" data-primary="ChatGPT" data-secondary="demonstration data" id="id513"/>step, human labelers provide examples of the desired model behavior on a distribution of prompts. The labelers are trained on the project and follow specific instructions to annotate the prompts accurately.</p>
</dd>
<dt>Training a supervised policy</dt>
<dd>
<p>The demonstration <a data-type="indexterm" data-primary="ChatGPT" data-secondary="supervised learning" id="id514"/><a data-type="indexterm" data-primary="supervised learning, ChatGPT" id="id515"/>data collected in the previous step is used to fine-tune a pretrained GPT-3 model using supervised learning. In supervised learning, models are trained on a labeled dataset where the correct answers are provided. This step helps the model to learn to follow the given instructions and produce outputs that align with the desired behavior.</p>
</dd>
<dt>Collection of comparison data and reinforcement learning</dt>
<dd>
<p>In this step, a dataset of model outputs is collected, <a data-type="indexterm" data-primary="ChatGPT" data-secondary="comparison data" id="id516"/><a data-type="indexterm" data-primary="ChatGPT" data-secondary="reinforcement learning" id="id517"/>and human labelers rank the outputs based on their preference. A reward model is then trained to predict which outputs the labelers would prefer. Finally, reinforcement learning techniques, specifically the Proximal Policy Optimization (PPO) algorithm, are used to optimize the supervised policy to maximize the reward from the reward model.</p>
</dd>
</dl>

<p>This training process allows the ChatGPT model to <a data-type="indexterm" data-primary="ChatGPT" data-secondary="human intent" id="id518"/>align its behavior with human intent. The use of reinforcement learning with human feedback helped create a model that is more helpful, honest, and safe compared to the pretrained GPT-3 model.</p>

<figure><div id="figure-2-4" class="figure">
<img src="assets/pega_0204.png" alt="ChatGPT Fine Tuning Approach" width="544" height="800"/>
<h6><span class="label">Figure 2-4. </span>The fine-tuning process for ChatGPT</h6>
</div></figure>

<p>According to a <a href="https://oreil.ly/2Ivq2">UBS study</a>, by January 2023 ChatGPT set a new benchmark, amassing 100 million active users and becoming the fastest-growing consumer application in internet history. ChatGPT is now a go-to for customer service, virtual assistance, and numerous other applications that require the finesse of human-like conversation.</p>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="GPT-4"><div class="sect1" id="id15">
<h1>GPT-4</h1>

<p>In 2024, OpenAI released GPT-4, which <a data-type="indexterm" data-primary="OpenAI" data-secondary="GPT-4" id="id519"/><a data-type="indexterm" data-primary="GPT-4" id="id520"/>excels in understanding complex queries and generating contextually relevant and coherent text. For example, GPT-4 scored in the 90th percentile of the bar exam with a score of 298 out of 400. Currently, GPT-3.5-turbo is free to use in ChatGPT, but GPT-4 requires a <a href="https://oreil.ly/UOEBM">monthly payment</a>.</p>

<p>GPT-4 uses a <a href="https://oreil.ly/v45LZ">mixture-of-experts approach</a>; it goes beyond relying on a single model‚Äôs inference to produce even more accurate and insightful results.</p>

<p>On May 13, 2024, OpenAI introduced <a href="https://oreil.ly/4ttmq">GPT-4o</a>, an advanced model capable of processing and reasoning across text, audio, and vision inputs in real time. This model offers enhanced performance, particularly in vision and audio understanding; it is also faster and more cost-effective than its predecessors due to its ability to process all three modalities in one neural network.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Google‚Äôs Gemini"><div class="sect1" id="id16">
<h1>Google‚Äôs Gemini</h1>

<p>After Google lost search market <a data-type="indexterm" data-primary="Google Bard" id="id521"/><a data-type="indexterm" data-primary="Bard (Google)" id="id522"/>share due to ChatGPT usage, it initially released Bard on March 21, 2023. Bard was a bit <a href="https://oreil.ly/Sj24h">rough around the edges</a> and definitely didn‚Äôt initially have the same high-quality LLM responses that ChatGPT offered (<a data-type="xref" href="#figure-2-5">Figure¬†2-5</a>).</p>

<p>Google has kept adding extra features over time including code generation, visual AI, real-time search, and voice into Bard, bringing it closer to ChatGPT in terms of quality.</p>

<p>On March 14, 2023, Google <a data-type="indexterm" data-primary="Google PaLM API" id="id523"/><a data-type="indexterm" data-primary="PaLM API" id="id524"/>released <a href="https://oreil.ly/EbI8-">PaLM API</a>, allowing developers to access it on Google Cloud Platform. In April 2023, Amazon Web Services (AWS) released <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Amazon Bedrock" id="id525"/><a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="Amazon‚Äôs Titan FMs" id="id526"/><a data-type="indexterm" data-primary="Google Gemini" id="id527"/><a data-type="indexterm" data-primary="Gemini (Google)" id="id528"/>similar services such as <a href="https://oreil.ly/4fNQX">Amazon Bedrock</a> and <a href="https://oreil.ly/FJ-7D">Amazon‚Äôs Titan FMs</a>. Google <a href="https://oreil.ly/EO42O">rebranded Bard to Gemini</a> for their v1.5 release in February 2024 and started to get results similar to GPT-4.</p>

<figure><div id="figure-2-5" class="figure">
<img src="assets/pega_0205.png" alt="Google's bard which is a similar application to ChatGPT." width="600" height="321"/>
<h6><span class="label">Figure 2-5. </span>Bard hallucinating results about the James Webb Space Telescope</h6>
</div></figure>

<p>Also, Google released two smaller <a href="https://oreil.ly/LWIwv">open source models</a> based on the same architecture as Gemini. OpenAI is finally no longer the only obvious option for software engineers to integrate state-of-the-art LLMs into their <span class="keep-together">applications.</span></p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Meta‚Äôs Llama and Open Source"><div class="sect1" id="id17">
<h1>Meta‚Äôs Llama and Open Source</h1>

<p>Meta‚Äôs approach to language models differs <a data-type="indexterm" data-primary="Meta" data-secondary="Llama" id="id529"/><a data-type="indexterm" data-primary="Llama (Meta)" id="id530"/>significantly from other competitors in the industry. By sequentially releasing open source models <a href="https://oreil.ly/LroPn">Llama</a>, <a href="https://oreil.ly/NeZLw">Llama 2</a> and <a href="https://oreil.ly/Vwlo-">Llama 3</a>, Meta aims to foster a more inclusive and collaborative AI development ecosystem.</p>

<p>The open source nature of Llama 2 and Llama 3 has significant implications for the <a data-type="indexterm" data-primary="Meta" data-secondary="Llama 2" id="id531"/><a data-type="indexterm" data-primary="Llama 2 (Meta)" id="id532"/><a data-type="indexterm" data-primary="Meta" data-secondary="open-source strategy" id="id533"/><a data-type="indexterm" data-primary="Meta" data-secondary="Llama 3" id="id534"/><a data-type="indexterm" data-primary="Llama 3 (Meta)" id="id535"/>broader tech industry, especially for large enterprises. The transparency and collaborative ethos encourage rapid innovation, as problems and vulnerabilities can be quickly identified and addressed by the global developer community. As these models become more robust and secure, large corporations can adopt them with increased confidence.</p>

<p>Meta‚Äôs open source strategy not only democratizes access to state-of-the-art AI technologies but also has the potential to make a meaningful impact across the industry. By setting the stage for a collaborative, transparent, and decentralized development process, Llama 2 and Llama 3 are pioneering models that could very well define the future of generative AI. The models are available in 7, 8 and 70 billion parameter versions on AWS, Google Cloud, Hugging Face, and other platforms.</p>

<p>The open source nature of these models presents a double-edged sword. On one hand, it levels the playing field. This means that even smaller developers have the opportunity to contribute to innovation, improving and applying open source models to practical business applications. This kind of decentralized innovation could lead to breakthroughs that might not occur within the walled gardens of a single organization, enhancing the models‚Äô capabilities and applications.</p>

<p>However, the same openness that makes this possible also poses potential risks, as it could allow malicious actors to exploit this technology for detrimental ends. This indeed is a concern that organizations like OpenAI share, suggesting that some degree of control and restriction can actually serve to mitigate the dangerous applications of these powerful tools.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Leveraging Quantization and LoRA"><div class="sect1" id="id18">
<h1>Leveraging Quantization and LoRA</h1>

<p>One of the game-changing aspects of these open source <a data-type="indexterm" data-primary="quantization" id="id536"/><a data-type="indexterm" data-primary="LoRA (low-rank approximations)" id="id537"/>models is the potential for <a href="https://oreil.ly/bkWXk">quantization</a> and the use of <a href="https://oreil.ly/zORsB">LoRA</a> (low-rank approximations). These techniques allow developers to fit the models into smaller hardware footprints. Quantization helps to reduce the numerical precision of the model‚Äôs parameters, thereby shrinking the overall size of the model without a significant loss in performance. Meanwhile, LoRA assists in optimizing the network‚Äôs architecture, making it more efficient to run on consumer-grade hardware.</p>

<p>Such optimizations make fine-tuning these LLMs increasingly feasible on consumer hardware. This is a critical development because it allows for greater experimentation and adaptability. No longer confined to high-powered data centers, individual developers, small businesses, and start-ups can now work on these models in more resource-constrained environments.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Mistral"><div class="sect1" id="id19">
<h1>Mistral</h1>

<p>Mistral 7B, a brainchild of <a data-type="indexterm" data-primary="Mistral 7B" id="id538"/>French start-up <a href="https://mistral.ai">Mistral AI</a>, emerges as a powerhouse in the generative AI domain, with its 7.3 billion parameters making a significant impact. This model is not just about size; it‚Äôs about efficiency and capability, promising a bright future for open source large language models and their applicability across a myriad of use cases. The key to its efficiency is the implementation of sliding window attention, a technique released under a permissive Apache open source license. Many AI engineers have fine-tuned on top of this model as a base, including the impressive <a href="https://oreil.ly/Lg6_r">Zephr 7b beta</a> model. There is also  <a href="https://oreil.ly/itsJG">Mixtral 8x7b</a>, a mixture of experts model (similar to the architecture of GPT-4), which achieves results similar to GPT-3.5-turbo.</p>

<p>For a more detailed and up-to-date comparison of open source models and their performance metrics, visit the Chatbot <a href="https://oreil.ly/ttiji">Arena Leaderboard</a> hosted by Hugging Face.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Anthropic: Claude"><div class="sect1" id="id20">
<h1>Anthropic: Claude</h1>

<p>Released on July 11, 2023, <a href="https://claude.ai/login">Claude 2</a> is setting itself apart from other prominent LLMs such as ChatGPT and LLaMA, with its pioneering <a href="https://oreil.ly/Tim9W">Constitutional AI</a> approach <a data-type="indexterm" data-primary="Claude 2" id="id539"/>to AI safety and alignment‚Äîtraining the model using a list of rules or values. A notable enhancement in Claude 2 was its expanded context window of 100,000 tokens, as well as the ability to upload files. In the <a data-type="indexterm" data-primary="context window" id="id540"/>realm of generative AI, a <em>context window</em> refers to the amount of text or data the model can actively consider or keep in mind when generating a response. With a larger context window, the model can understand and generate based on a broader context.</p>

<p>This advancement garnered significant enthusiasm from AI engineers, as it opened up avenues for new and more intricate use cases. For instance, Claude 2‚Äôs augmented ability to process more information at once makes it adept at summarizing extensive documents or sustaining in-depth conversations. The advantage was short-lived, as OpenAI released their 128K version of GPT-4 only <a href="https://oreil.ly/BWxrn">six months later</a>. However, the fierce competition between rivals is pushing the field forward.</p>

<p>The next generation <a data-type="indexterm" data-primary="Opus" id="id541"/>of Claude included <a href="https://oreil.ly/NH0jh">Opus</a>, the first model to rival GPT-4 in terms of intelligence, as well as Haiku, a smaller model that is lightning-fast with the competitive price of $0.25 per million tokens (half the cost of GPT-3.5-turbo at the time).</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="GPT-4V(ision)"><div class="sect1" id="id21">
<h1>GPT-4V(ision)</h1>

<p>In a significant leap <a data-type="indexterm" data-primary="GPT-4 Vision" id="id542"/>forward, on September 23, 2023, OpenAI expanded the capabilities of GPT-4 with the introduction of Vision, enabling users to instruct GPT-4 to analyze images alongside text. This innovation was also reflected in the update to ChatGPT‚Äôs interface, which now supports the inclusion of both images and text as user inputs. This development signifies a major trend toward <em>multimodal models</em>, which <a data-type="indexterm" data-primary="multimodal models" id="id543"/>can seamlessly process and understand multiple types of data, such as images and text, within a single context.</p>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Model Comparison"><div class="sect1" id="id347">
<h1>Model Comparison</h1>

<p>The market for LLMs is dominated by OpenAI at the time of writing, with its state-of-the-art GPT-4 model widely considered to have a significant lead. The closest competitor is Anthropic, and there is widespread excitement at the potential of smaller open source models such as Llama and Mistral, particularly with respects to fine-tuning. Although commentators expect OpenAI to continue to deliver world-beating models in the future, as open source models get <em>good enough</em> at more tasks, AI workloads may shift toward local fine-tuned models. With advances in model performance and quantization (methods for trading off accuracy versus size and compute cost), it may be possible to one day run LLMs on your mobile phone or other devices.</p>

<p>For now, the best way to get a sense for what the models are good at is to run the same prompt across multiple models and compare the responses. One thing that regularly stands out in our work is that GPT-4 is much better at following instructions, as is demonstrated in the following example where it was the only model to respond in the right format, with names that matched the examples (starting with the letter <em>i</em>), as desired.</p>

<p id="comparing_text_models">Input:</p>

<pre data-type="programlisting">Brainstorm a list of product names for a shoe that fits any foot size,
in the style of Steve Jobs.

Return the results as a comma-separated list, in this format:
Product description: A shoe that fits any foot size
Product names: [list of three product names]

## Examples
Product description: A refrigerator that dispenses beer
Product names: iBarFridge, iFridgeBeer, iDrinkBeerFridge

Product description: A watch that can tell accurate time in space
Product names: iNaut, iSpace, iTime

Product description: A home milkshake maker
Product names: iShake, iSmoothie, iShake Mini</pre>

<p>Output (GPT-4):</p>

<pre data-type="programlisting">Product description: A shoe that fits any foot size
Product names: iFit, iShoe, iFlexible</pre>

<p>Output (Claude 3):</p>

<pre data-type="programlisting">Product description: A shoe that fits any foot size
Product names: iFit, iComfort, iSole</pre>

<p>Output (Llama 3 70b):</p>

<pre data-type="programlisting">Here is the list of product names for a shoe that fits any foot size,
in the style of Steve Jobs:

Product description: A shoe that fits any foot size Product names:
iFit, OneSize, iWalkFree</pre>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id348">
<h1>Summary</h1>

<p>The journey of LLMs from their inception to their current advanced states is a tale of relentless innovation, collaboration, and intense competition. As these models continue to evolve, they are likely to become even more integral parts of our daily lives, changing the way we interact with technology and even with each other.</p>

<p>By understanding the historical context and capabilities of these models, you can better appreciate the tools at our disposal for various applications, from prompt engineering to the development of intelligent virtual agents. It‚Äôs important to note, however, that while these models offer expansive possibilities, data privacy remains a crucial concern. If these models use your data for retraining or fine-tuning, exercise caution and refrain from inputting sensitive information.</p>

<p>In the next chapter, you will learn all the basic prompt engineering techniques for working with text LLMs. You‚Äôll learn the essential skills needed to get the most out of powerful language models like GPT-4. Exciting insights and practical methods await you as you unlock the true potential of generative AI.</p>
</div></section>
</div></section></div></div></body></html>