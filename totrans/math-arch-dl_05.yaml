- en: 6 Bayesian tools for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesâ€™ theorem, conditional probability, entropy, cross-entropy, and conditional
    entropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation
    of model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evidence maximization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KLD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian mixture models (GMM) and MLE estimation of GMM parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Bayesian approach to statistics tries to model the world by modeling the
    uncertainties and prevailing beliefs and knowledge about the system. This is in
    contrast to the frequentist paradigm, where probability is strictly measured by
    observing a phenomenon repeatedly and measuring the fraction of time an event
    occurs. Machine learning, in particular *unsupervised* machine learning, is a
    lot closer to the Bayesian paradigm of statisticsâ€”the subject of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In chapter [1](../Text/01.xhtml#chap-overview), we primarily discussed *supervised*
    machine learning, where the training data is labeled: each input value is accompanied
    by a manually created desired output value. Labeling training inputs is a manual,
    labor-intensive process and often the worst pain point in building a machine learningâ€“based
    system. This has led to considerable recent interest in *unsupervised* machine
    learning, where we build a model from *unlabeled* training data. How is this done?'
  prefs: []
  type: TYPE_NORMAL
- en: The general approach is best visualized geometrically. Each input data instance
    is a point in a high-dimensional space. These points form an overall pattern in
    the space of all possible inputs. If the inputs all have a common property, the
    points are not distributed randomly over the input space. Rather, they occupy
    a region in the input space with a definite shape. If the inputs have multiple
    classes, each class occupies a separate cluster in the space. Sometimes we apply
    a transformation to the input firstâ€”the transform is chosen or learned so that
    the transformed points exhibit a pattern more clearly than raw input points. We
    then identify a probability distribution whose sample point cloud matches the
    shape of the (potentially transformed) training data point cloud. We can generate
    faux input by sampling from this distribution. We can also classify an arbitrary
    input by observing which cluster it falls into.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng.bz](http://mng.bz/WdZa)
    [/WdZa](http://mng.bz/WdZa) in the form of fully functional and executable Jupyter
    notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Conditional probability and Bayesâ€™ theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, the discussion is accompanied by examples. In this context, we first
    offer a refresher on the concepts of joint and marginal probability from section
    [5.4](../Text/05.xhtml#sec-joint-prob) (you may want to revisit the topic of joint
    probability in sections [5.4](../Text/05.xhtml#sec-joint-prob), [5.4.1](../Text/05.xhtml#sec-marginal-prob),
    and [5.4.2](../Text/05.xhtml#sec-joint-prob-depend)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider two random variables: the height and weight of adult Statsville residents.
    Weight (denoted *W*) can take three quantized values: *E*[1], *E*[2], *E*[3].
    Height (*H*) can also take three quantized values: *F*[1], *F*[2], *F*[3]. Table
    [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob) shows their joint
    probability.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Joint and marginal probability revisited
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One glance at table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    tells us that the probabilities are concentrated along the main diagonal, which
    indicates dependent events. This can be validated by inspecting one joint probabilityâ€”say,
    *p*(*E*[1], *F*[1])â€”and the corresponding marginal probabilities *p*(*F*[1]) and
    *p*(*E*[1]). We can see that *p*(*E*[1], *F*[1]) = 0.2 â‰  *p*(*F*[1]) Ã— *p*(*E*[1])
    = 0.26 Ã— 0.26, establishing that the random variables weight *W* and height *H*
    are not independent. For contrast, look at table [5.6](../Text/05.xhtml#tab-jmarginal-prob).
    In that case, for any valid *i*, *j* pair, *p*(*E[i]*, *G[j]*) = *p*(*G[i]*) Ã—
    *p*(*E[j]*): the two events (weight and distance of a residentâ€™s home from the
    city center) are independent. Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 Example population sizes and joint probability distribution for variables
    *W* = {*E*[1], *E*[2], *E*[3]} and *H* = {*F*[1], *F*[2], *F*[3]} weights and
    heights of adult Statsville residents), showing marginal probabilities
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) | Marginals for Fs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Less than 160 cm** **(*F*[1])** | pop. = 20,000*p*(*E*[1], *F*[1]) = 0.2
    | pop. = 4,000*p*(*E*[2], *F*[1]) = 0.04 | pop. = 2,000*p*(*E*[3], *F*[1]) = 0.02
    | pop. = 26,000;*p*(*F*[1]) = 0.2 + 0.04 + 0.02 = 0.26 |'
  prefs: []
  type: TYPE_TB
- en: '| **Between 160 cm and 183 cm** (*F*[2]) | pop. = 4,000*p*(*E*[1], *F*[2])
    = 0.04 | pop. = 40,000*p*(*E*[2], *F*[2]) = 0.4 | pop. = 4,000*p*(*E*[3], *F*[2])
    = 0.04 | pop. = 48,000;*p*(*F*[2]) = 0.04 + 0.4 + 0.04 = 0.48 |'
  prefs: []
  type: TYPE_TB
- en: '| **More than 183 cm** (*F*[3]) | pop. = 2,000*p*(*E*[1], *F*[3]) = 0.02 |
    pop. = 4,000*p*(*E*[2], *F*[3]) = 0.04 | pop. = 20,000*p*(*E*[3], *F*[3]) = 0.2
    | pop. = 26,000;*p*(*F*[3]) = 0.02 + 0.04 + 0.2 = 0.26 |'
  prefs: []
  type: TYPE_TB
- en: '| **Marginals for** ***E*s** | *p*(*E*[1])= 0.2 + 0.04 + 0.02 = 0.26 | *p*(*E*[2])=
    0.04 + 0.4 + 0.04 = 0.48 | *p*(*E*[3])= 0.02 + 0.04 + 0.2 = 0.26 | Total pop.
    = 100,000;Total prob = 1 |'
  prefs: []
  type: TYPE_TB
- en: '*Joint probability*â€”This is the probability of a specific combination of values
    occurring *together*. Each cell in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    depicts one joint probability: for example, the probability that a residentâ€™s
    weight is between 60 and 90 kg *and* that their height is greater than 183 cm
    is *p*(*E*[2], *F*[3]) = 0.04.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sum rule*â€”The joint probabilities of all possible variable combinations sum
    to 1 (bottom right cell in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-00-a.png)'
  prefs: []
  type: TYPE_IMG
- en: The sum of probabilities is the probability of one or another of the corresponding
    events occurring. Here we are adding all possible event combinationsâ€”one or another
    of these combinations will certainly occur. Hence the sum is 1, which matches
    our intuition.
  prefs: []
  type: TYPE_NORMAL
- en: '*Marginal probability for a variable*â€”This is obtained by â€œsumming awayâ€ the
    other variables (right-most column and bottom-most row in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-00-b.png)'
  prefs: []
  type: TYPE_IMG
- en: We have added all possible combinations of other variables, so the sum represents
    the probability of this one variable.
  prefs: []
  type: TYPE_NORMAL
- en: '*Marginal probabilities*â€”These sum to 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-00-c.png)'
  prefs: []
  type: TYPE_IMG
- en: The sum of the marginal probabilities is the sum of all possible joint probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dependent vs. independent variables*â€”If and only if the variables are independent,
    the product of the marginal probabilities is the same as the joint probability:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*(*F[i] , E[j]*) â‰  *p*(*F[i]*) Ã— *p*(*E[j]*) âŸº for dependent variables in
    table [5.6](../Text/05.xhtml#tab-jmarginal-prob)'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*G[i] , E[j]*) = *p*(*G[i]*) Ã— *p*(*E[j]*) âŸº for independent variables
    in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)'
  prefs: []
  type: TYPE_NORMAL
- en: You should verify that this condition is *not satisfied* in table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    for the weight and height variables. It *is satisfied* in table [5.6](../Text/05.xhtml#tab-jmarginal-prob)
    for the weight and distance-of-home-from-city-center variables.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Conditional probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose we know that the height of a subject is between 160 and 183 cm (*H*
    = *F*[2]). What is the probability of the subjectâ€™s weight being more than 90
    kg (*W* = *E*[3])? In statistical parlance, this probability is denoted *p*(*W*
    = *E*[3]|*H* = *F*[2]). It is read â€œprobability of *W* = *E*[3] *given* *H* =
    *F*[2],â€ aka â€œprobability of *W* = *E*[3] *subject to the condition* *H* = *F*[2].â€
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of *conditional probability*. Note that if we are given
    that the height is between 160 and 183 cm (*H* = *F*[2]), our universe is restricted
    to the second row of table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob).
    In particular, our population size is not 100,000 (that is, the entire population
    of Statsville). Rather, it is 48,000: the size of the population satisfying the
    given condition *H* = *F*[2]. Using the frequentist definition,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-00-e.png)'
  prefs: []
  type: TYPE_IMG
- en: Table [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob)
    shows table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob) with conditional
    probabilities added.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2 Example population sizes and joint, marginal, and conditional probabilities
    for variables *W* = {*E*[1], *E*[2], *E*[3]} and *H* = {*F*[1], *F*[2], *F*[3]}
    weights and heights of adult Statsville residents). (This is table [6.1](../Text/06.xhtml#tab-joint-depend-with-marginal-prob)
    with conditional probabilities added.)
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Less than 60 kg (*E*[1]) | Between 60 and 90 kg (*E*[2]) | More than 90
    kg (*E*[3]) | Marginals for *F*s |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Less than 160 cm** (***F*[1]**) | pop. = 20,000*p*(*E*[1], *F*[1]) = 0.2*p*(*E*[1]&#124;
    *F*[1]) = *p*(*E*[1], *F*[1]) / *p*(*F*[1]) = 0.77*p*(*F*[1]&#124; *E*[1]) = *p*(*E*[1],
    *F*[1]) / *p*(*E*[1]) = 0.77 | pop. = 4,000*p*(*E*[2], *F*[1]) = 0.04*p*(*E*[2]&#124;
    *F*[1]) = *p*(*E*[2], *F*[1]) / *p*(*F*[1]) = 0.154*p*(*F*[1]&#124; *E*[2]) =
    *p*(*E*[2], *F*[1]) / *p*(*E*[2]) = 0.083 | pop. = 2,000*p*(*E*[3], *F*[1]) =
    0.02*p*(*E*[3]&#124; *F*[1]) = *p*(*E*[3], *F*[1]) / *p*(*F*[1]) = 0.077*p*(*F*[1]&#124;
    *E*[3]) = *p*(*E*[3], *F*[1]) / *p*(*E*[3]) = 0.077 | pop. = 26,000;*p*(*F*[1])
    = 0.2+ 0.04 + 0.02 = 0.26 |'
  prefs: []
  type: TYPE_TB
- en: '| **Between 160 cm and 183 cm** (*F*[2]) | pop. = 4,000*p*(*E*[1], *F*[2])
    = 0.04*p*(*E*[1]&#124; *F*[2]) = *p*(*E*[1], *F*[2]) / *p*(*F*[2]) = 0.083*p*(*F*[2])&#124;
    *E*[1]) = *p*(*E*[1], *F*[2]) / *p*(*E*[1]) = 0.154 | pop. = 40,000*p*(*E*[2],
    *F*[2]) = 0.4*p*(*E*[2]&#124; *F*[2]) = *p*(*E*[2], *F*[2]) / *p*(*F*[2]) = 0.83*p*(*F*[2])&#124;
    *E*[2]) = *p*(*E*[2], *F*[2]) / *p*(*E*[2]) = 0.83 | pop. = 4,000*p*(*E*[3], *F*[2])
    = 0.04*p*(*E*[3]&#124; *F*[2]) = *p*(*E*[3], *F*[2]) / *p*(*F*[2]) = 0.083*p*(*F*[2])&#124;
    *E*[3]) = *p*(*E*[3], *F*[2]) / *p*(*E*[3]) = 0.154 | pop. = 48,000;*p*(*F*[2])
    = 0.04 + 0.4 + 0.04 = 0.48 |'
  prefs: []
  type: TYPE_TB
- en: '| **More than 183 cm** (*F*[3]) | pop. = 2,000*p*(*E*[1], *F*[3]) = 0.02*p*(*E*[1]&#124;*F*[3])
    = *p*(*E*[1], *F*[3]) / *p*(*F*[3]) = 0.077*p*(*F*[3]&#124;*E*[1]) = *p*(*E*[1],
    *F*[33]) / *p*(*E*[1]) = 0.077 | pop. = 4,000*p*(*E*[2], *F*[3]) = 0.04*p*(*E*[2]&#124;*F*[3])
    = *p*(*E*[2], *F*[3]) / *p*(*F*[3]) = 0.154*p*(*F*[3]&#124;*E*[2]) = *p*(*E*[2],
    *F*[33]) / *p*(*E*[2]) = 0.083 | pop. = 20,000*p*(*E*[3], *F*[3]) = 0.2*p*(*E*[3]&#124;*F*[3])
    = *p*(*E*[3], *F*[3]) / *p*(*F*[3]) = 0.77*p*(*F*[3]&#124;*E*[3]) = *p*(*E*[3],
    *F*[33]) / *p*(*E*[3]) = 0.77 | pop. = 26,000;*p*(*F*[3]) = 0.02 + 0.04 + 0.2
    = 0.26 |'
  prefs: []
  type: TYPE_TB
- en: '| **Marginals for** ***E*s** | *p*(*E*[1]) = 0.2 + 0.04 + 0.02 = 0.26 | *p*(*E*[2])
    = 0.04 + 0.4 + 0.04 = 0.48 | *p*(*E*[3]) = 0.02 + 0.04 + 0.2 = 0.26 | Total pop.=
    100,000;Total prob = 1 |'
  prefs: []
  type: TYPE_TB
- en: 6.1.3 Bayesâ€™ theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As demonstrated in table [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob),
    in general,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-00-f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the essence of Bayesâ€™ theorem. We can generalize and say the following:
    given two random variables *X* and *Y*, the conditional probability of *X* taking
    the value *x* given the condition that *Y* has value ![](../../OEBPS/Images/AR_y.png)
    is given by the ratio of the joint probability of the two and the marginal probability
    of the condition'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.1
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we drop the names of the random variable and just use the values.
    Using such notation, Bayesâ€™ theorem can be stated as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-01-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the denominator is the marginal probability, which can be obtained
    by summing over the joint probabilities. For instance, for continuous variables,
    Bayesâ€™ theorem can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-01-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bayesâ€™ theorem can be generalized further to more than two variables and multiple
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.2
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.3
  prefs: []
  type: TYPE_NORMAL
- en: It is common practice to drop the name of the random variable uppercase), retain
    only the value (lowercase), and state these equations informally as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-03-a.png)'
  prefs: []
  type: TYPE_IMG
- en: What happens if the random variables are independent? Well, letâ€™s check out
    equation [6.1](../Text/06.xhtml#eq-bayes-theorem-2var). If *X* and *Y* are independent,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-03-b.png)'
  prefs: []
  type: TYPE_IMG
- en: and hence
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-03-c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This makes intuitive sense: if *X* and *Y* are independent, knowing *Y* does
    not make any difference to *p*(*X* = *x*), so the probability of *X* given *Y*
    is the same as the probability of *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose a daily meteorological bulletin informs the folks in the United States
    whether it rained in the Sahara desert yesterday. Is there much overall information
    in that bulletin? Not reallyâ€”it almost always reports the obvious. The probability
    of â€œno rainâ€ is overwhelmingly high it is almost certain that there will be no
    rain), and the uncertainty associated with the outcome is very low. Even without
    the bulletin, if we guess the outcome â€œno rain,â€ we will be right almost every
    time. Similarly, a daily news bulletin telling us whether it rained yesterday
    in Cherapunji, Indiaâ€”a place where it pretty much rains all the timeâ€”has little
    informational content because we can guess the results with high certainty even
    without the bulletin. Stated another way, the uncertainty associated with the
    probability distributions of â€œrain vs. no rain in the Saharaâ€ and or â€œrain vs.
    no rain in Cherapunjiâ€ is low. This is a direct consequence of the fact that the
    probability of one of the events is close to 1 and the probabilities of the other
    events are near 0: the probability density function (PDF) has a very tall peak
    at one location and very low heights elsewhere.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a daily bulletin reporting whether it rained in San Francisco
    is of considerable interest because the probability of â€œrainâ€ and â€œno rainâ€ are
    comparable. Without the bulletin, we cannot guess the result with much certainty.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of *entropy* attempts to quantify the uncertainty associated with
    a chancy event. If the probability for any one event is overwhelmingly high (meaning
    the probabilities of other events are very low since the sum is 1), the uncertainty
    is lowâ€”we pretty much know that the high-probability event will occur. On the
    other hand, if there are multiple events with comparable high probabilities, uncertainty
    is highâ€”we cannot predict which event will occur. Entropy captures this notion
    of uncertainty in a system. Letâ€™s look at another example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have tiny images, four pixels wide by four pixels high, and each
    pixel is one of four possible colors: G(reen), R(ed), B(lue), or Y(ellow). Two
    such images are shown in figure [6.1](../Text/06.xhtml#fig-entropy-img). We want
    to encode such images. The simplest thing to do is to use a two-bit representation
    for each color:'
  prefs: []
  type: TYPE_NORMAL
- en: G(*reen*) = 00
  prefs: []
  type: TYPE_NORMAL
- en: R(*ed*) = 01
  prefs: []
  type: TYPE_NORMAL
- en: B(*lue*) = 10
  prefs: []
  type: TYPE_NORMAL
- en: Y(*ellow*) = 11
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F01_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1 Two 4 Ã— 4 images with different pixel color distributions. In the
    left image, the four colors R, G, B, and Y are equally probable. In the right
    image, one color (green) is much likelier than the others. The left image has
    higher entropy (uncertainty): we cannot predict any color with much certainty.
    In the right image, we can predict green with relative certainty.'
  prefs: []
  type: TYPE_NORMAL
- en: The entire 16-pixel image on the left can be represented by the string 00 00
    00 00 01 01 01 01 10 10 10 10 11 11 11 11\. Here, we have iterated over the pixels
    in *raster scan order*, left to right and top to bottom. The total number of bits
    needed to store the 16-pixel image is 16 Ã— 2 = 32 bits. The right image can be
    represented as 00 00 00 00 00 00 00 00 00 00 00 00 01 01 10 11\. The total number
    of bits needed is 16 Ã— 2 = 32 bits. Both images need the same amount of storage.
    But is this optimal?
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the right-hand image. The color G appears much more frequently than
    the others. We can use this fact to reduce the total number of bits required to
    store the image. It is not mandatory to use the same number of bits to represent
    each color. How about using shorter representations for the more frequently occurring
    (higher-probability) colors and longer representations for the infrequent (lower-probability)
    colors? This is the core principle behind the technique of *variable bit-rate
    coding*. For instance, we can use the following representation:'
  prefs: []
  type: TYPE_NORMAL
- en: G(*reen*) = 0
  prefs: []
  type: TYPE_NORMAL
- en: R(*ed*) = 10
  prefs: []
  type: TYPE_NORMAL
- en: B(*lue*) = 110
  prefs: []
  type: TYPE_NORMAL
- en: Y(*ellow*) = 111
  prefs: []
  type: TYPE_NORMAL
- en: The right-hand image can thus be represented as 0 0 0 0 0 0 0 0 0 0 0 0 10 10
    110 111.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE This is an example of what is known as *prefix coding*: no two colors
    share the same prefix. It enables us to identify the color as soon as we see its
    code. For instance, if we see a 0 bit at the beginning, we immediately know the
    color is green since no other color code starts with 0. If we see 10, we immediately
    know the color is red since no other color code starts with 10, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: With this new color code, we need 12 Ã— 1 = 12 bits to store the 12 green pixels,
    2 Ã— 2 = 4 bits to store the 2 red pixels, 1 Ã— 3 = 3 bits to store the single blue
    pixel, and 1 Ã— 3 = 3 bits to store the single yellow pixelâ€”a total of 22 pixels.
    Equivalently, we need 22/16 = 1.375 bits per pixel. This is less than the 32 pixels
    at 2 bits per pixel we needed with the simple fixed bit-rate coding.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE You have just learned about Huffman encoding, an important technique in
    image compression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Does the new representation result in smaller storage for the left-hand image?
    There, we need 4 Ã— 1 = 4 bits to store the four green pixels, 4 Ã— 2 = 8 pixels
    to store the four red pixels, 4 Ã— 3 = 12 bits to store the four blue pixels, and
    4 Ã— 3 = 12 bits to store the single yellow pixel: a total of 36 pixels at 36/16
    = 2.25 bits per pixel. Here, variable bit-rate coding does worse than fixed bit-rate
    coding.'
  prefs: []
  type: TYPE_NORMAL
- en: So, the probability distribution of the various pixel colors in the image affects
    how much compression can be achieved. If the distribution of pixel colors is such
    that a few colors are much more probable than others, we can assign shorter codes
    to them to reduce storage for the whole image. Viewed another way, if low uncertainty
    is associated with the systemâ€”certain colors are more or less certain to occurâ€”we
    can achieve high compression. We assign shorter codes to nearly certain colors,
    resulting in compression. On the other hand, if high uncertainty is associated
    with the systemâ€”all colors are more or less equally probable, and no color occurs
    with high certaintyâ€”variable bit-rate coding will not be very effective. How do
    we quantify this notion? In other words, can we examine the pixel color distribution
    in an image and estimate whether variable bit-rate coding will be effective? The
    answer again is entropy. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: Entropy measures the overall uncertainty associated with a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is a measure that is *high* if everything is more or less equally probable
    and *low* if a few items have a much higher probability than the others. It measures
    the uncertainty in the system. If everything is equally probable, we cannot predict
    any one item with any extra certainty. Such a system has high entropy. On the
    other hand, if some items are much more probable than others, we can predict them
    with relative certainty. Such a system has low entropy.
  prefs: []
  type: TYPE_NORMAL
- en: In the discrete univariate case, for a random variable *X* that can take any
    one of the discrete values *x*[1], *x*[2], *x*[3], â‹¯, *x[n]* with probabilities
    *p*(*x*[1]), *p*(*x*[2]), *p*(*x*[3]), â‹¯, *p*(*x[n]*), entropy is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.4
  prefs: []
  type: TYPE_NORMAL
- en: The logarithm is taken with respect to the natural base *e*.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s apply equation [6.4](../Text/06.xhtml#eq-entropy-discr-univar4) to the
    images in figure [6.1](../Text/06.xhtml#fig-entropy-img) to see if the results
    agree with our intuition. The computations are shown in table [6.3](../Text/06.xhtml#tab-entropies-img).
    The notion of entropy applies to continuous and multidimensional random variables
    equally well.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.3 Entropy computation for the pair of images in figure [6.1](../Text/06.xhtml#fig-entropy-img).
    The right-hand image has lower entropy and can be compressed more.
  prefs: []
  type: TYPE_NORMAL
- en: '| Left image | Right image |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[1] = *G*, *p*(*x*[1]) = 4/16 = 0.25 | *x*[1] = *G*, *p*(*x*[1]) = 12/16
    = 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[2] = *R* , *p*(*x*[2]) = 4/16 = 0.25 | *x*[2] = *R* , *p*(*x*[2]) = 2/16
    = 0.125 |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[3] = *B*, *p*(*x*[3]) = 4/16 = 0.25 | *x*[3] = *B*, *p*(*x*[3]) = 1/16
    = 0.0625 |'
  prefs: []
  type: TYPE_TB
- en: '| *x*[4] = *Y*, *p*(*x*[4]) = 4/16 = 0.25 | *x*[4] = *Y*, *p*(*x*[4]) = 1/16
    = 0.0625 |'
  prefs: []
  type: TYPE_TB
- en: '| â„ = âˆ’(0.25 *log*(0.25)+0.25 *log*(0.25) + + 0.25 *log*(0.25) + 0.25 *log*(0.25))
    = 1.386294 | â„ = âˆ’(0.75 *log*(0.75)+0.125 *log*(0.125) + + 0.0625 *log*(0.0625)
    + 0.0625 *log*(0.0625)) = 0.822265 |'
  prefs: []
  type: TYPE_TB
- en: For a univariate continuous random variable *X* that takes values *x* âˆˆ {âˆ’âˆž,âˆž}
    with probabilities *p*(*x*),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.5
  prefs: []
  type: TYPE_NORMAL
- en: For a continuous multidimensional random variable *X* that takes values ![](../../OEBPS/Images/AR_x.png)
    in the domain *D*, (![](../../OEBPS/Images/AR_x.png) âˆˆ *D*) with probabilities
    *p*(![](../../OEBPS/Images/AR_x.png)),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.6
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Geometrical intuition for entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Geometrically speaking, entropy is a function of how lopsided the PDF is (see
    figure [6.2](../Text/06.xhtml#fig-entropy-density-cloud)). If all inputs are more
    or less equally probable, the density function is more or less flat and uniform
    in height everywhere (see figure [6.2a](../Text/06.xhtml#fig-entropy-high-density)).
    The corresponding sample point cloud has a diffused mass: there are no regions
    with a high concentration of points. Such a system has high uncertainty or high
    entropy (see figure [6.2b](../Text/06.xhtml#fig-entropy-high-cloud)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F02a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Flatter, wider PDFs correspond to higher entropy. Entropy = 12.04.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F02b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Diffused sample point clouds correspond to higher entropy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F02c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Taller, narrower peaks in probability density functions correspond to lower
    entropy. Entropy = 7.44.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F02d_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Concentrated sample point clouds correspond to lower entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 Entropies of peaked and flat distributions
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if a few of all the possible inputs have disproportionately
    high probabilities, the PDF has tall peaks in some regions and low heights elsewhere
    (see figure [6.2c](../Text/06.xhtml#fig-entropy-low-density)). The corresponding
    sample point cloud has regions of high concentration matching the peaks in the
    density function and low concentration elsewhere (see figure [6.2d](../Text/06.xhtml#fig-entropy-low-cloud)).
    Such a system has low uncertainty and low entropy.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Since the sum of all the probabilities is 1, if a few are high, the others
    have to be low. We cannot have all high or all low probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Entropy of Gaussians
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The wider a Gaussian is, the less peaked it is, and the closer it is to being
    a uniform distribution. A univariate Gaussianâ€™s variance, *Ïƒ*, determines its
    fatness (see figure [5.10b](../Text/05.xhtml#fig-multi-univar-gauss)). Consequently,
    we expect a Gaussianâ€™s entropy to be an increasing function of *Ïƒ*. Indeed, that
    is the case. In this section, we derive the entropy of a Gaussian in the univariate
    case and simply state the result for the multivariate case.
  prefs: []
  type: TYPE_NORMAL
- en: For a random variable *x* whose PDF is given by equation [5.22](../Text/05.xhtml#eq-univar-normal)
    (repeated here for convenience),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-06-a.png)'
  prefs: []
  type: TYPE_IMG
- en: From that, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-06-b.png)'
  prefs: []
  type: TYPE_IMG
- en: Using equation [6.6](../Text/06.xhtml#eq-entropy-cont-univar), the entropy is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-06-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Remembering the probability sum rule from equation [5.6](../Text/05.xhtml#eq-continuous-prob-sum),
    âˆ«[*x* = âˆ’âˆž]^âˆž *p*(*x*) *dx* = 1, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-06-d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, by definition (see section [5.7.2](../Text/05.xhtml#sec-var-covar-std)),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-06-e.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.7
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy for multivariate Gaussians is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-08.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.8
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 shows the Python PyTorch code to compute the entropy of a Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code to compute the entropy of a Gaussian distribution,
    executable via Jupyter Notebook, can be found at [http://mng.bz/zx7B](http://mng.bz/zx7B).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Computing the entropy of a Gaussian distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Equation [6.7](../Text/06.xhtml#eq-entropy-gauss-univar)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Instantiates a Gaussian distribution
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Computes the entropy using the direct formulaComputes the entropy using the
    direct formula
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Computes the entropy using the PyTorch interface
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Asserts that the entropies computed two different ways match
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Cross-entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider a *supervised* classification problem where we have to analyze an
    image and identify which of the following objects is present: *cat*, *dog*, *airplane*,
    or *automobile*. We assume that one of these will always be present in our universe
    of images. Given an input image, our machine emits four probabilities: *p*(*cat*),
    *p*(*dog*), *p*(*airplane*), and *p*(*automobile*). During training, for each
    training data instance, we have a ground truth (GT): a known class to which that
    training data instance belongs. We have to estimate how different the network
    output is from the GTâ€”this is the loss for that data instance. We adjust the machine
    parameters to minimize the loss and continue doing so until the loss stops decreasing.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we quantitatively estimate the lossâ€”the difference between the known
    GT and the probabilities of various classes emitted by the network? One principled
    approach is to use the cross-entropy loss. Here is how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a random variable *X* that can take four possible values: *X* = 1
    signifying *cat*, *X* = 2 signifying *dog*, *X* = 3 signifying *airplane*, and
    *X* = 4 signifying *automobile*. The random variable has the PDF *p*(*X* = 1)
    â‰¡ *p*(*cat*), *p*(*X* = 2) â‰¡ *p*(*dog*), *p*(*X* = 3) â‰¡ *p*(*airplane*), *p*(*X*
    = 4) â‰¡ *p*(*automobile*). The PDF for a GT, which selects one from the set of
    four possible classes, is a one-hot vector (one of the elements is 1, and the
    others are 0). Such random variables and corresponding PDFs can be associated
    with every GT and machine output. Here are some examples, which are also shown
    graphically in figure [6.3](../Text/06.xhtml#fig-cross-entropy). A PDF for GT
    *cat* (one-hot vector) is shown figure [6.3a](../Text/06.xhtml#fig-cross-entropy-gt):'
  prefs: []
  type: TYPE_NORMAL
- en: How do we quantitatively estimate the lossâ€”the difference between the known
    GT and the probabilities of various classes emitted by the network? One principled
    approach is to use the cross-entropy loss. Here is how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-08-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A PDF for a good prediction is shown figure [6.3b](../Text/06.xhtml#fig-cross-entropy-low-pred):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-08-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A PDF for a bad prediction is shown figure [6.3c](../Text/06.xhtml#fig-cross-entropy-high-pred):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-08-c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../../OEBPS/Images/CH06_F03a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Ground truth probability
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F03b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Good prediction: probabilities similar to ground truth. Cross-entropy loss
    = 0.22.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F03c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) Bad prediction: probabilities dissimilar to ground truth. Cross-entropy
    loss = 1.38.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 Cross-entropy loss
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *X[gt]* denote such a random variable for a specific GT and *p[gt]* denote
    the corresponding PDF. Similarly, let *X[pred]* and *p[pred]* denote the random
    variable and PDF for the machine prediction. Consider the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-09.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.9
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the expression for *cross-entropy*. It is a quantitative measure for
    how dissimilar the two PDFs *p[gt]* and *p[pred]* are: that is, how much error
    will be caused by approximating the PDF *p[gt]* with *p[pred]*. Equivalently,
    cross-entropy measures how well the machine is doing that output the prediction
    *p[pred]* when the correct PDF is *p[gt]*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain insight into how â„*[c]*(*X[gt]*, *X[pred]*) measures dissimilarity
    between PDFs, examine the expression carefully. Remember that Î£[i]â´[= 1] *p[gt]*
    (*i*) = Î£[i]â´[= 1] *p[pred]* (*i*) = 1 (using the probability sum rule from equation
    [5.3](../Text/05.xhtml#eq-discrete-prob-sum)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**: The *i* values where *p[gt]*(*i*) is high (close to 1).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1a**: If *p[pred]*(*i*) is also close to 1, then log (*p[pred]*(*i*))
    will be close to zero (since log 1 = 0). Hence the term *p[gt]*(*i*)log (*p[pred]*(*i*))
    will be close to zero since the product of anything with a near-zero number is
    near zero. These terms will contribute little to â„*[c]*(*X[gt]*, *X[pred]*).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1b**: On the other hand, at the *i* values where *p[gt]*(*i*) is high,
    if *p[pred]*(*i*) is low (close to zero), then âˆ’log (*p[pred]*(*i*)) will be very
    high (since log 0 â†’ âˆ’ âˆž).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 2**: The *i* values where *p[gt]*(*i*) is low (close to 0). These will
    have low values and will contribute little to â„*[c]*(*X[gt]*, *X[pred]*) since
    the product of anything with a near zero number is near zero.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, overall, large contributions can happen only in case 1b, where *p[gt]*(*i*)
    is high and *p[pred]*(*i*) is lowâ€”that is, *p[gt]* and *p[pred]* are very dissimilar.
    What if *p[gt]*(*i*) is low and *p[pred]*(*i*) is high? They are also dissimilar,
    so those terms will not contribute much! True, but if such terms exist, there
    must be other terms where *p[gt]*(*i*) is high and *p[pred]*(*i*) is low. This
    is because the sums of all *p[gt]*(*i*) and *p[pred]*(*i*) must be both 1. Either
    way, if there is dissimilarity, the cross-entropy is high.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider the case where *X[gt]* = *X[gt_cat]* and *X[pred]* =
    *X[good_pred]* or *X[pred]* = *X[bad_pred]*. We know *p[gt_cat]* is a one-hot
    selector vector, meaning it has 1 as one element and 0s elsewhere. Only a single
    term survives, corresponding to *i* = 0, and
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-09-a.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that cross-entropy is higher where similarity is lower (the prediction
    is bad).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we are ready to formally define the cross-entropy of two arbitrary
    random variables. Let *X*[1], *X*[2] be a pair of random variables that take values
    *x* from the same input domain *D* that is, *x* âˆˆ *D*), with probabilities *p*[1](*x*),
    *p*[2](*x*), respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.10
  prefs: []
  type: TYPE_NORMAL
- en: Note that cross-entropy in equation [6.10](../Text/06.xhtml#eq-cross-entropy-loss)
    reduces to entropy (equations [6.5](../Text/06.xhtml#eq-entropy-discr-univar),
    [6.6](../Text/06.xhtml#eq-entropy-cont-univar)) if *Y* = *X*. Listing 6.2 shows
    the Python PyTorch code to compute the entropy of a Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code to compute cross-entropy, executable via Jupyter
    Notebook, can be found at [http://mng.bz/0mjN](http://mng.bz/0mjN).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Computing cross-entropy
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Direct computation
  prefs: []
  type: TYPE_NORMAL
- en: of cross-entropy from equation [6.9](../Text/06.xhtml#eq-cross-entropy-discrete)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Probability density function for the ground truth (one-hot vector)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Probability density function for a good prediction
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Probability density function for a bad prediction
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Cross-entropy between *X[gt]* and *X[good_pred]* a low value)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¥ Cross-entropy between *X[gt]* and *X[bad_pred]* a high value)
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 KL divergence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In section [6.3](../Text/06.xhtml#sec-cross-entropy), we saw that cross-entropy,
    â„*[c]*(*X*[1], *X*[2]), measures the dissimilarity between the distributions of
    two random variables *X*[1] and *X*[2] with probabilities *p*[1](*x*) and *p*[2](*x*).
    But cross-entropy has a curious property for a dissimilarity measure. If *X*[1]
    = *X*[2], the cross-entropy â„*[c]*(*X*[1], *X*[2]) reduces to the entropy â„(*X*[1]).
    This is somewhat counterintuitive: we expect the dissimilarity between two copies
    of the same thing to be zero.'
  prefs: []
  type: TYPE_NORMAL
- en: We should look at cross-entropy as a dissimilarity with an offset. Letâ€™s denote
    the pure dissimilarity measure as *D*(*X*[1], *X*[2]). Then
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-10-a.png)'
  prefs: []
  type: TYPE_IMG
- en: This means the pure dissimilarity measure
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-10-b.png)'
  prefs: []
  type: TYPE_IMG
- en: This pure dissimilarity measure, *D*(*X*[1], *X*[2]), is called *Kullbackâ€“Leibler
    divergence* (KL divergence or KLD). As expected, it is 0 when the two random variables
    are identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, KLD is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.11
  prefs: []
  type: TYPE_NORMAL
- en: For continuous univariate randoms,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.12
  prefs: []
  type: TYPE_NORMAL
- en: For continuous multivariate randoms,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.13
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s examine some properties of KLD:'
  prefs: []
  type: TYPE_NORMAL
- en: The KLD between identical random variables is zero. If *X*[1] = *X*[2], *p*[1](*x*)
    = *p*[2](*x*)âˆ€*x* âˆˆ *D*. Then the log term vanishes at every *x*, and KLD is zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The KLD between non-identical probability distributions is always positive.
    We can see this by examining equation [6.11](../Text/06.xhtml#eq-kld-discr-univar).
    At all values of *x* where *p*[1](*x*) > *p*[2](*x*), the log term is positive
    (since the logarithm of a number greater than 1 is positive). On the other hand,
    at all values of *x* where *p*[1](*x*) < *p*[2](*x*), the log term is negative
    (since the logarithm of a number less than 1 is negative). But the positive terms
    get higher weights because *p*[1](*x*) are higher at these points. In this context,
    it is worth noting that given any pair of PDFs, *one cannot be uniformly higher
    than the other at all points*. This is because both of them must sum to 1. If
    one PDF is higher somewhere, it must be lower somewhere else to compensate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a GT PDF *p[gt]* for a classification problem and a machine prediction
    *p[pred]*, minimizing the cross-entropy â„(*gt*, *pred*) is logically equivalent
    to minimizing the KLD *D*(*gt*, *pred*). This is because the entropy â„(*gt*) is
    a constant, independent of the machine parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The KLD is *not* symmetric: *D*(*X*[1], *X*[2]) â‰  *D*(*X*[2], *X*[1]).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.4.1 KLD between Gaussians
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the Gaussian probability distribution is so important, in this subsection
    we look at the KLD between two Gaussian random variables *X*[1] and *X*[2] having
    PDFs *p*[1](*x*) = ð’©(*x*; *Î¼*[1], *Ïƒ*[1]) and *p*[2](*x*) = ð’©(*x*; *Î¼*[2], *Ïƒ*[2]).
    We derive the expression for the univariate case and simply state the expression
    for the multivariate case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-13-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Opening the parentheses, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-13-b.png)'
  prefs: []
  type: TYPE_IMG
- en: Expanding the square term, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-13-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Since
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-13-d.png)'
  prefs: []
  type: TYPE_IMG
- en: the final equation for the KLD between two univariate Gaussian random variables
    *X*[1], *X*[2] with PDFs ð’©(*x*; *Î¼*[1], *Ïƒ*[1]) and ð’©(*x*; *Î¼*[2], *Ïƒ*[2]) becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.14
  prefs: []
  type: TYPE_NORMAL
- en: The KLD between two *d*-dimensional Gaussian random variables *X*[1], *X*[2]
    with PDFs ð’©(![](../../OEBPS/Images/AR_x.png); *Î¼*[1], **Î£**[1]) and ð’©(![](../../OEBPS/Images/AR_x.png);
    *Î¼*[2], **Î£**[2]) is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.15
  prefs: []
  type: TYPE_NORMAL
- en: where the operator *tr* denotes the *trace* of a matrix (sum of diagonal elements)
    and the operator *det* denotes the determinant.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 shows the Python PyTorch code to compute the KLD.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code to compute the KLD, executable via Jupyter Notebook,
    can be found at [http://mng.bz/KMyj](http://mng.bz/KMyj).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Computing the KLD
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Instantiates three Gaussian distributions
  prefs: []
  type: TYPE_NORMAL
- en: with the same means but different
  prefs: []
  type: TYPE_NORMAL
- en: standard deviations
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Computes the KLD between various pairs of distributions
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ The KLD between a distribution and itself is 0.
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ The KLD is not symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ See figure [6.4](../Text/06.xhtml#fig-kld-gaussian).
  prefs: []
  type: TYPE_NORMAL
- en: In figure [6.4](../Text/06.xhtml#fig-kld-gaussian), we compare three Gaussian
    distributions *p*, *q*, and *r* with the same *Î¼*s but different *Ïƒ*s. *KLD*(*p*,
    *q*) < *KLD*(*p*, *r*) because *Ïƒ[p]* is closer to *Ïƒ[q]* than *Ïƒ[r]*.
  prefs: []
  type: TYPE_NORMAL
- en: In figure [6.4](../Text/06.xhtml#fig-kld-uniform), we compare a uniform distribution
    *p* with two Gaussian distributions *q* and *r* that have different *Î¼*s but the
    same *Ïƒ*s. *KLD*(*p*, *q*) < *KLD*(*p*, *r*) because *Î¼[p]* is closer to *Î¼[q]*
    than *Î¼[r]*.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Conditional entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section [6.2](../Text/06.xhtml#sec-entropy), we learned that entropy measures
    the uncertainty in a system. Earlier, in section [6.1.2](../Text/06.xhtml#sec-cond-prob-bayes),
    we studied conditional probability, which measures the probability of occurrence
    of one set of random variables under the condition that another set has known
    fixed values. In this section, we combine the two concepts into a new concept
    called *conditional entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following question from table [6.2](../Text/06.xhtml#tab-joint-depend-with-conditional-marginal-prob).
    What is the entropy of the weight variable *W* under the condition that the value
    of the height variable *H* is *F*[1]? As observed in section [6.1.1](../Text/06.xhtml#sec-joint-marginal-prob-recap),
    the condition effectively restricts our universe to a single row (in this case,
    the top row) of the table. We can compute the entropy of the elements of that
    row mathematically, using equation [6.5](../Text/06.xhtml#eq-entropy-discr-univar),
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-15-a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../../OEBPS/Images/CH06_F04a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) *p* â‰¡ ð’©(*Î¼* = 0, *Ïƒ* = 5), *q* â‰¡ ð’©(*Î¼* = 0, *Ïƒ* = 10), *r* â‰¡ ð’©(*Î¼* = 0,
    *Ïƒ* = 20)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F04b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) *p* â‰¡ *U*(*a* = âˆ’20, *b* = 20), *q* â‰¡ ð’©(*Î¼* = 0, *Ïƒ* = 20), *r* â‰¡ ð’©(*Î¼*
    = âˆ’50, *Ïƒ* = 20)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 KLD between example distributions
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-15-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'â„(*W*|*H* = *F[i]*) is the entropy of *W* given *H* = *F[i]* for *i* = 1 or
    2 or 3. What is the overall conditional entropy of *W* given *H*: that is, â„(*W*|*H*)?
    To compute this, we take the expected value (that is, the probability-weighted
    average; see equation [5.8](../Text/05.xhtml#eq-discrete-univar-expected-val))
    of the conditional entropy â„(*W*|*H* = *F[i]*) over all possible values of *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-15-c.png)'
  prefs: []
  type: TYPE_IMG
- en: This idea can be generalized. Formally, given two random variables *X* and *Y*
    that can take values *x* âˆˆ *D[x]*, ![](../../OEBPS/Images/AR_y.png) âˆˆ *D[y]*,
    respectively,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.17
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Chain rule of conditional entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This rule states:'
  prefs: []
  type: TYPE_NORMAL
- en: â„(*X*|*Y*) = â„(*X*, *Y*) âˆ’ â„(*Y*)
  prefs: []
  type: TYPE_NORMAL
- en: Equation 6.18
  prefs: []
  type: TYPE_NORMAL
- en: This can be derived from equation [6.17](../Text/06.xhtml#eq-cond-entropy-cont).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-18-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying Bayesâ€™ theorem (equation [6.1](../Text/06.xhtml#eq-bayes-theorem-2var)),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.19
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Model parameter estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a set of sampled input data points *X* = {![](../../OEBPS/Images/AR_x.png)^((1)),
    ![](../../OEBPS/Images/AR_x.png)^((2)),â‹¯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}
    from a distribution. We refer to the set collectively as *training data*. Note
    that we are *not* assuming it is *labeled* training dataâ€”we do not know the outputs
    corresponding to the inputs ![](../../OEBPS/Images/AR_x.png)^((*i*)). Also, suppose
    that based on our knowledge of the problem, we have decided which model family
    to use. Of course, simply knowing the family is not enough; we need to know (or
    estimate) the model parameters before we can use the model. For instance, our
    model family might be Gaussian, ð’©(*x*; ![](../../OEBPS/Images/AR_micro.png), **Î£**).
    Until we know the actual value of the parameters ![](../../OEBPS/Images/AR_micro.png)
    and Î£, we do not fully know the model and cannot use it.
  prefs: []
  type: TYPE_NORMAL
- en: How do we estimate the model parameters from the unlabeled training data? This
    is what we cover in this section. At the moment, we are discussing it without
    referring to any specific model architecture, so letâ€™s denote model parameters
    with a generic symbol *Î¸*. For instance, when dealing with Gaussian models, *Î¸*
    = {![](../../OEBPS/Images/AR_micro.png), **Î£**}.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.1 Likelihood, evidence, and posterior and prior probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before tackling the problem of parameter estimation, it is important to have
    a clear understanding of the terms *likelihood*, *evidence*, *posterior probability*,
    and *prior probability* in the current context. Equation [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence)
    illustrates them. Using Bayesâ€™ theorem,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.20
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s first examine the likelihood term. Using the fact that data instances
    are independent of each other,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-20-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|*Î¸*) is essentially the probability
    density of the distribution family we have chosen. For instance, if the model
    in question in Gaussian, then given *Î¸* = {![](../../OEBPS/Images/AR_micro.png),
    **Î£**}, this will be
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-20-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which is basically an expression for the Gaussian PDF: a restatement of equation
    [5.23](../Text/05.xhtml#eq-multivar-normal) (but in equation [5.23](../Text/05.xhtml#eq-multivar-normal),
    we dropped the â€œgiven *Î¸*,â€ part in the notation and expressed *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)
    simply as *p*(![](../../OEBPS/Images/AR_x.png))). Thus, we can always express
    the likelihood from the PDF of the chosen model family using the independence
    of individual training data instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s examine the prior probability, *p*(*Î¸*). It typically comes from some
    physical constraintâ€”without referring to the input. A very popular approach is
    to say that, all other things being equal, we prefer parameters with smaller magnitudes.
    By this token, the larger the total magnitude ||*Î¸*||Â², the lower the prior probability.
    For instance, we may use
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*Î¸*) âˆ *e*^(âˆ’||*Î¸*||Â²)'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 6.21
  prefs: []
  type: TYPE_NORMAL
- en: An indirect justification for favoring parameter vectors with the smallest length
    (magnitude) can be found in the principle of Occamâ€™s razor. It states, *Entia
    non sunt multiplicanda praeter necessitatem*, which roughly translates to â€œOne
    should not multiply unnecessarily.â€ This is often interpreted in machine learning
    and other disciplines as â€œfavor the briefest representation.â€
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown previously, we can always express the likelihood and prior terms.
    Using them, we can formulate different paradigms, each with a different quantity,
    to optimize in order to estimate the unknown probability distribution parameters
    from training data. These techniques can be broadly classified into the following
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood parameter estimation MLE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum a posteriori (MAP) parameter estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide an overview of them next. You will notice that, in all the methods,
    we typically preselect a distribution family as a model and then estimate the
    parameter values by maximizing one probability or another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later in the chapter, we look at MLE in the special case of the Gaussian family
    of distributions. Further down the line, we look at MLE with respect to Gaussian
    mixture models. Another technique outlined later is evidence maximization: we
    will visit it in the context of variational autoencoders.'
  prefs: []
  type: TYPE_NORMAL
- en: The log-likelihood trick
  prefs: []
  type: TYPE_NORMAL
- en: If we choose a distribution family whose PDF is exponential (the most obvious
    example is Gaussian), instead of maximizing the likelihood, we usually maximize
    its logarithm, aka the *log-likelihood*. We can do this because whatever maximizes
    a quantity also maximizes its logarithm and vice versa. But the logarithm simplifies
    expressions in the case of exponential probability functions. This becomes obvious
    if we note that
  prefs: []
  type: TYPE_NORMAL
- en: '*log*(*e*^x) = *x*'
  prefs: []
  type: TYPE_NORMAL
- en: '*log*(Î  *e*^(*x*^((*i*)))) = Î£ *x*^((*i*))'
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.2 Maximum likelihood parameter estimation (MLE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In MLE of parameters, we ask, â€œWhat parameter values will maximize the joint
    likelihood of the training data instances?â€ In this context, remember that likelihood
    is the of a data instance occurring given specific parameter values (equation
    [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence)). Expressed mathematically,
  prefs: []
  type: TYPE_NORMAL
- en: 'MLE estimates what value of *Î¸* maximizes *p*(*X*|*Î¸*). The geometric mental
    picture is as follows: we want to estimate the unknown parameters for our model
    probability distribution such that if we draw many samples from that distribution,
    the sample point cloud will largely overlap the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Often we employ the log-likelihood trick and maximize the log-likelihood instead
    of the actual likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: For some models, such as Gaussians, this maximization problem can be solved
    analytically, and a closed-form solution can be obtained (as shown in section
    [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation)). For others, such
    as Gaussian mixture models (GMMs), the maximization problem ields no closed-form
    solution, and we go for an iterative solution (as shown in section [6.9.4](../Text/06.xhtml#sec-gmm_fit)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.3 Maximum a posteriori (MAP) parameter estimation and regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of asking what parameter value maximizes the probability of occurrence
    of the training data instances, we can ask, â€œWhat are the most probable parameter
    values, given the training data?â€ Expressed mathematically, in MAP, we directly
    estimate the *Î¸* that maximizes *p*(*Î¸*|*X*). Using equation [6.20](../Text/06.xhtml#eq-posterior-prior-likelihood-evidence),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-22.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.22
  prefs: []
  type: TYPE_NORMAL
- en: Since the denominator is independent of *Î¸*, maximizing the numerator with respect
    to *Î¸* maximizes the fraction. Thus
  prefs: []
  type: TYPE_NORMAL
- en: In MAP parameter estimation, we look for parameters *Î¸* that maximize *p*(*X*|*Î¸*)*p*(*Î¸*).
  prefs: []
  type: TYPE_NORMAL
- en: The first factor, *p*(*X*|*Î¸*), is what we optimized in MLE and comes from the
    model definition (such as equation [5.23](../Text/05.xhtml#eq-multivar-normal)
    for multivariate Gaussian models).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second factor, *p*(*Î¸*), is the prior term, which usually incentivizes the
    optimization system to choose a solution with predefined properties like smaller
    parameter magnitudes equation [6.21](../Text/06.xhtml#eq-prior-mag)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewed this way, MAP estimation is equivalent to *MLE parameter estimation with
    regularization*. Regularization is a technique often used in optimization. In
    regularized optimization, we add a term to the expression being maximized or minimized.
    This term effectively incentivizes the system to choose the solution with the
    smallest magnitudes of the unknown from the set of possible solutions. It is easy
    to see that MAP estimation essentially imposes the prior probability term on top
    of MLE. This extra term acts as a regularizer, incentivizing the system to choose
    the lowest magnitude parameters while still trying to maximize the likelihood
    of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation [6.22](../Text/06.xhtml#eq-MAP) can be interpreted another way. When
    we have no training data, all we can do is estimate the parameters from our prior
    beliefs about the system: the prior term *p*(*Î¸*). When the training data set
    *X* arrives, it influences the system through the likelihood term *p*(*X*|*Î¸*).
    As more and more training data arrives, the prior term (whose magnitude does not
    change with training data) dominates less and less, and the posterior probability
    *p*(*Î¸*|*X*) is dominated more by the likelihood.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Latent variables and evidence maximization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose we have the height and weight data for a population (say, for the adult
    residents of our favorite town, Statsville). A single data instance looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-22-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Although the data is not explicitly labeled or classified, we know the data
    points can be clustered into two distinct classes, *male* and *female*. It is
    reasonable to expect that the distribution of each class is much simpler than
    the overall distribution. For instance, here, the distributions for males and
    females may be Gaussians individually (presumably, the means for females will
    occur at smaller height and weight values). The combined distribution does not
    fit any of the distributions we have discussed so far (later, we see it is a Gaussian
    mixture).
  prefs: []
  type: TYPE_NORMAL
- en: 'We look at such situations in more detail in connection to Gaussian mixture
    modeling and variational autoencoders. Here we only note that in these cases,
    it is often beneficial to introduce a variable for the class, say *Z*. In this
    example, *Z* is discrete: it can take one of two values, *male* or *female*. Then
    we can model the overall distribution as a combination of simple distributions,
    each corresponding to a specific value of *Z*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Such variables *Z* that are *not* part of the observed data *X* but are introduced
    to facilitate modeling are called *latent* or *hidden* variables/parameters. Latent
    variables are connected to observed variables through the usual Bayesian expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-22-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How do we estimate the distribution of *Z*? One way is to ask, â€œWhat distribution
    of the hidden variables would maximize the probability of exactly these training
    data points being returned if we drew random samples from the distribution?â€ The
    philosophy behind this is as follows: we assume that the training data points
    are fairly typical and have a high probability of occurrence in the unknown data
    distribution. Hence, we try to find a distribution under which the training data
    points will have the highest probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometrically speaking, each data point (vector) can be viewed as a point in
    some *d*-dimensional space, where *d* is the number of elements in the vector
    ![](../../OEBPS/Images/AR_x.png)*[i]*. The training data points typically occupy
    a region within that space. We are looking for a distribution whose mass is largely
    aligned with the training data region. In other words, the probability associated
    with the training data points is as high as possibleâ€”the sample distribution cloud
    largely overlaps the training data cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Expressed mathematically, we want to identify *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_z.png))
    and *p*(![](../../OEBPS/Images/AR_z.png)) that maximize the quantity
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.23
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we get *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|![](../../OEBPS/Images/AR_z.png))
    from the PDF of our chosen model family and *p*(![](../../OEBPS/Images/AR_z.png))
    through some physical constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8 Maximum likelihood parameter estimation for Gaussians
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We look at this with a one-dimensional example, but the results derived apply
    to higher dimensions. Suppose we are trying to predict whether an adult Statsville
    resident is female, given that the residentâ€™s height lies in a specified range
    [*a*, *b*]. For this purpose, we have collected a set of height samples of adult
    *female* Statsville residents. These height samples constitute our training data.
    Letâ€™s denote them as *x*^((1)), *x*^((2)), â‹¯, *x*^((*n*)). Based on physical considerations,
    we expect the distribution of heights of adult Statsville females to be a Gaussian
    distribution with unknown mean and variance. Our goal is to determine them from
    the training data via MLE, which effectively estimates a distribution whose sample
    cloud maximally matches the distribution of the training data points.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s denote the (as yet unknown) mean and variance of the distribution as *Î¼*
    and *Ïƒ*. Then, from equation [5.22](../Text/05.xhtml#eq-univar-normal), we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Employing the log-likelihood trick,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-b.png)'
  prefs: []
  type: TYPE_IMG
- en: To maximize with respect to *Î¼*, we solve
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-c.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-d.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we get a closed-form expression for the unknown *Î¼* in terms of the
    training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-f.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, to maximize with respect to *Ïƒ*, we solve
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-g.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-h.png)'
  prefs: []
  type: TYPE_IMG
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-i.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we get a closed-form expression for the unknown *Ïƒ* in terms of the
    training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-23-j.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus we see that for a Gaussian, the maximum-likelihood solutions coincide
    with the sample mean and variance of the training data. Once we have the mean
    and standard deviation, we can calculate the probability that a female residentâ€™s
    height belongs to a specified range [*a*, *b*] by using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-24.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.24
  prefs: []
  type: TYPE_NORMAL
- en: 'In the multidimensional case:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a training dataset, {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),â‹¯,
    ![](../../OEBPS/Images/AR_x.png)^((*n*))}, the best fit Gaussian has the mean
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-25.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.25
  prefs: []
  type: TYPE_NORMAL
- en: and the covariance matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-26.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.26
  prefs: []
  type: TYPE_NORMAL
- en: We began this section by stating the problem of estimating the probability of
    an adult Statsville resident being female, given that their height lies in a specified
    range [*a*, *b*], when we are provided a training dataset of *n* height values
    of adult Statsville female residents. Letâ€™s now revisit that problem. Using (scalar
    versions of) equations [6.25](../Text/06.xhtml#eq-gauss-MLE-mean) and [6.26](../Text/06.xhtml#eq-gauss-MLE-covar),
    we can estimate *Î¼* and *Ïƒ* and thereby define a Gaussian probability distribution
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*x*) = ð’©(*x*; *Î¼*, *Ïƒ*)'
  prefs: []
  type: TYPE_NORMAL
- en: Using this, given any height *x*, we can compute the probability *p*(*x*) that
    the resident is female. Letâ€™s see this using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8.1 Python PyTorch code for maximum likelihood estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we assume that the height values of adult female residents of Statsville
    follow a Gaussian distribution. If we know the parameters of this Gaussian (*Î¼*
    and *Ïƒ*), we know the Gaussian distribution fully. That allows us to estimate
    many interesting things: for instance, the expected height of an adult female
    resident of Statsville, or the probability that the height of an adult female
    Statsville resident lies in a certain range such as between 160 and 170 cm. The
    problem is, in a typical real-life situation, we do not know the parameters *Î¼*
    cm and *Ïƒ*. All we have is a large dataset *X* of height values of adult Statsville
    female residentsâ€”training data. We have to use this data to estimate the unknown
    parameters *Î¼* cm and *Ïƒ*. Once we have these, we have an estimated distribution
    (aka model) from which we can predict the probabilities of events of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in section [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation),
    MLE is a technique to estimate the parameters from given training data when the
    family to which the distribution belongs is known but the exact values of the
    parameters are not known. Listing 6.4 shows the PyTorch implementation of MLE
    for the Gaussian family.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for model parameter estimation using MLE and MAP,
    executable via Jupyter Notebook, can be found at [http://mng.bz/9Mv7](http://mng.bz/9Mv7).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Maximum likelihood estimate for a Gaussian
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Estimates Gaussian MLE parameters ![](../../OEBPS/Images/AR_micro.png) and
    Î£. They equal the sample mean and sample covariance of the training data. See
    equations [6.25](../Text/06.xhtml#eq-gauss-MLE-mean) and [6.26](../Text/06.xhtml#eq-gauss-MLE-covar).
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Defines a Gaussian with the estimated parameters
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Once the Gaussian is estimated, we can use it to predict probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 6.8.2 Python PyTorch code for maximum likelihood estimation using gradient descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In listing 6.4, we computed the MLE using the closed-form solution. Now, letâ€™s
    try to compute the MLE using a different method: gradient descent. In real-life
    scenarios, we do not use gradient descent to compute the MLE because the closed-form
    solution is available. However, we discuss this method here to highlight some
    of the challenges of using gradient descent and how MAP estimation addresses these
    challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to maximize the likelihood function using gradient descent. This
    can alternatively be viewed as minimizing the negative log-likelihood function.
    We choose to use the logarithm of the likelihood function since that leads to
    simpler computation without any loss of generalization. (If you want a quick refresher
    on gradient descent, see section [3.5](../Text/03.xhtml#sec-gradient-descent).)
    Following is the equation for negative log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-27.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.27
  prefs: []
  type: TYPE_NORMAL
- en: Listings 6.5 and 6.6 show the PyTorch code for the minimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Gaussian negative log-likelihood for training data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Equation [6.27](../Text/06.xhtml#eq-neg-log-likelihood)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ *n*/2 *log* 2 *Ï€Ïƒ*Â²
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ (Î£[*i* = 1]*^n* (*x*[i] â€“ *Î¼*)Â²)/(2*Ïƒ*Â²)
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Note how all the training data *X* is crunched in a single operation. Such
    vector operations are parallel and very efficient in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Minimizing MLE loss via gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Negative log-likelihood (listing [6.5](../Text/06.xhtml#code-NLL))
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Iterates to train
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Computes the loss
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Computes the gradients of the loss with regard to Î¼ and Ïƒ. PyTorch stores
    the gradients in Î¼.grad and Ïƒ.grad.
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Scales the gradients by learning the rate and update parameters
  prefs: []
  type: TYPE_NORMAL
- en: â‘¥ Resets the gradients to zero post-update
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F05a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) MLE explodes: *Î¼[init]* = 1, *Ïƒ[init]* = 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F05b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) MLE converges: *Î¼[init]* = 100, *Ïƒ[init]* = 10.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F05c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: '(c) MAP converges: *Î¼[init]* = 1, *Ïƒ[init]* = 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 Gaussian parameter estimation using maximum likelihood estimate and
    maximum a posteriori estimation. In figure [6.5a](../Text/06.xhtml#fig-gaussian-mle-explode),
    the MLE explodes because *Î¼* and *Ïƒ* are initialized far from *Î¼[expected]* and
    *Ïƒ[expected]*. However, the MLE converges in figure [6.5b](../Text/06.xhtml#fig-gaussian-mle-fit)
    because *Î¼* and *Ïƒ* are initialized closed to *Î¼[expected]* and *Ïƒ[expected]*.
    Figure [6.5c](../Text/06.xhtml#fig-gaussian-map-fit) shows how, for MAP, *Î¼* and
    *Ïƒ* are able to converge to *Î¼*.*[expected]* and *Ïƒ[expected]* even though they
    are initialized far away.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6.5](../Text/06.xhtml#fig-mle-map) shows how *Î¼* and *Ïƒ* change with
    each iteration of gradient descent. We expect *Î¼* and *Ïƒ* to end up close to *Î¼[expected]*
    and *Ïƒ[expected]*, respectively. However, when *Î¼* and *Ïƒ* start off far from
    *Î¼[expected]* and *Ïƒ[expected]* as in figure [6.5a](../Text/06.xhtml#fig-gaussian-mle-explode)),
    they do not converge to the expected values and instead become very large numbers.
    On the other hand, when they are instantiated with values closer to *Î¼[expected]*
    and *Ïƒ[expected]* as in figure [6.5b](../Text/06.xhtml#fig-gaussian-mle-fit)),
    they converge to the expected values. MLE is very sensitive to the initial values
    and has no mechanism to prevent the parameters from exploding. This is why MAP
    estimation is preferred. The prior *p*(*Î¸*) acts as a regularizer and prevents
    the parameters from becoming too large. Figure [6.5c](../Text/06.xhtml#fig-gaussian-map-fit)
    shows how *Î¼* and *Ïƒ* converge to the expected values using MAP even though they
    started far away.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MAP loss function is as follows. Note that it is the same equation as the
    negative log-likelihood, but with two additional termsâ€”*Î¼*Â² and *Ïƒ*Â²â€”that act
    as regularizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-28.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.28
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Gaussian negative log-likelihood with regularization
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Equation [6.28](../Text/06.xhtml#eq-neg-log-likelihood-reg)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ *n*/2 *log* 2 *Ï€Ïƒ*Â²
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ (Î£[*i* = 1]*^n* (*x*[i] â€“ *Î¼*)Â²)/(2*Ïƒ*Â²)
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Negative log-likelihood
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Regularization
  prefs: []
  type: TYPE_NORMAL
- en: â‘¥ Note how all the training data *X* is crunched in a single operation. Such
    vector operations are parallel and very efficient in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 6.9 Gaussian mixture models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many real-life problems, the simple unimodal (single-peak) probability distributions
    we learned about in chapter [5](../Text/05.xhtml#chap-prob) fail to model the
    true underlying distribution of the data. For instance, consider a situation where
    we are given the heights of many adult Statsville residents. Say there are two
    classes of adults in Statsville: male and female. The height data we have is *unlabeled*,
    meaning we do not know whether a given instance of height data is associated with
    a male or a female. Thus the data is one-dimensional, and there are two classes.
    Figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls) depicts the situation. None of
    the simple probability distributions we discussed in chapter [5](../Text/05.xhtml#chap-prob)
    can be fitted to figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls). But the two partial
    bells in figure [6.6a](../Text/06.xhtml#fig-gmm_1d_2cls_PDF) suggest that we should
    be able to mix a pair of Gaussians (each of which looks like a bell) to mimic
    this distribution. This is also consistent with our knowledge that the distribution
    represents not one but two classes, each of which can be reasonably represented
    individually by Gaussians. The point cloud also indicates two separate clusters
    of points. While a single Gaussian will not work, a mixture of two separate 1D
    Gaussians can (and, as we shall shortly see, will) work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F06a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PDF
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F06b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Sample point distribution
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 Probability density functions (PDFs) and sample point distributions
    for 1D height data of adult male and female residents of Statsville
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s now discuss a slightly more complex problem in which the data is two-dimensional
    and has three classes. Here we are given the weights and heights of three classes
    of Statsville residents: adult females, adult males, and children. Again, the
    data is *unlabeled*, meaning we do not know whether a given instance of (height,
    weight) data is associated with a man, woman, or child. This is depicted in figure
    [6.7](../Text/06.xhtml#fig-gmm_2d_3cls). Once again, none of the simple probability
    distributions we studied in chapter [5](../Text/05.xhtml#chap-prob) can be fitted
    to this situation. But the PDF shows three bell-shaped peaks, the point cloud
    shows three clusters, and the physical nature of the problem indicates three separate
    classes, each of which can be reasonably represented by Gaussian. While a single
    Gaussian will not work, a mixture of three separate 2D Gaussians can (and, as
    we shall shortly see, will) work.'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Gaussian mixture model (GMM) is a weighted combination of a specific number
    of Gaussian components*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F07a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PDF
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F07b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Sample point distributions
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 Probability density functions (PDFs) and sample point distributions
    for 2D (height, weight) data of children, adult males, and adult females of Statsville
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in our first problem with one dimension and two classes, we choose
    a mixture of two 1D Gaussians. For the second problem, we take a mixture of three
    2D Gaussians. Each individual Gaussian component corresponds to a specific class.
  prefs: []
  type: TYPE_NORMAL
- en: 6.9.1 Probability density function of the GMM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Formally,
  prefs: []
  type: TYPE_NORMAL
- en: The PDF for a GMM is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.29
  prefs: []
  type: TYPE_NORMAL
- en: where *Ï€[k]* is the weight of the *k*th Gaussian component, satisfying
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29-a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*K* is the number of classes or Gaussian components, and ð’©(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*,Â **Î£***[k]*) defined in equation [5.23](../Text/05.xhtml#eq-multivar-normal))
    is the PDF for the *k*th Gaussian component. Such a GMM models a *K*-peaked PDF
    or, equivalently, a *K*-clustered sample point cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the PDF and sample point clouds shown in figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls)
    correspond to the following Gaussian mixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The 2D three-class problem, PDF, and sample point clouds shown in figure [6.7](../Text/06.xhtml#fig-gmm_2d_3cls)
    correspond to the following Gaussian mixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29-c.png)'
  prefs: []
  type: TYPE_IMG
- en: The PDF and sample point distribution of the GMM depend on the values of *Ï€[k]*s,
    *Î¼[k]*s, **Î£***[k]*s, and *K*. In particular, *K* influences the number of peaks
    in the PDF (although if two peaks are very close, sometimes they merge). It also
    influences the number of clusters in the sample point cloud (again, if two clusters
    are too close, they may not be visually distinct). The *Ï€[k]*s regulate the relative
    heights of the hills. The *Î¼[k]*s and **Î£***[k]*s influence the individual hills
    in the PDF as well as the individual clusters in the sample point cloud. Specifically,
    *Î¼[k]* regulates the locations of the *k*th peak in the PDF and the centroid of
    the *k*th cluster in the sample point cloud. The **Î£***[k]*s regulate the shape
    of the *k*th individual hill and the *k*th cluster in the sample point cloud.
    Figures [6.8](../Text/06.xhtml#fig-gmm-1d), [6.9](../Text/06.xhtml#fig-gmm-2d-pis),
    [6.10](../Text/06.xhtml#fig-gmm-2d-mussigmas), and [6.11](../Text/06.xhtml#fig-gmm-2d-pt-distr)
    show some example GMMs with various values of these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6.8](../Text/06.xhtml#fig-gmm-1d) shows a pair of Gaussian distributions
    and various GMMs with those as components, with different values for the parameters.
    Figure [6.9](../Text/06.xhtml#fig-gmm-2d-pis) depicts 2D GMMs with various *Ï€[k]*s.
    Figure [6.10](../Text/06.xhtml#fig-gmm-2d-mussigmas) shows GMMs with non-circular
    bases (non-symmetric Î£s) and various *Î¼*s).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F08a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Gaussian components *Î¼*[1] = 152, *Î¼*[2] = 175, *Ïƒ*[1] = *Ïƒ*[2] = 9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F08b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) GMM with *Ï€*[1] = 0.5, *Ï€*[2] = 0.5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F08c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) GMM with *Ï€*[1] = 0.7, *Ï€*[2] = 0.3
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F08d_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) GMM with *Ï€*[1] = 0.3, *Ï€*[2] = 0.7
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 Various GMMs (solid curves) with the same Gaussian components (dotted
    and dashed curves, respectively) but different *Ï€*[1] and *Ï€*[2] values
  prefs: []
  type: TYPE_NORMAL
- en: Another way to visualize GMMs is via sample point distributions. Figure [6.11](../Text/06.xhtml#fig-gmm-2d-pt-distr)
    shows the sample points from a pair of 2D Gaussians and the points sampled from
    a GMM having those Gaussians as components and various mixture-selections probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F09a_Chaudhury.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (a) *Ï€*[1] = 0.5, *Ï€*[2] = 0.5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F09b_Chaudhury.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (b) *Ï€*[1] = 0.4, *Ï€*[2] = 0.6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F09c_Chaudhury.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (c) *Ï€*[1] = 0.7, *Ï€*[2] = 0.3
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F09d_Chaudhury.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (d) *Ï€*[1] = 0.3, *Ï€*[2] = 0.7
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 Two-dimensional GMMs with circular bases,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29-d.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Note how the relative heights of the hills depend on *Ï€*s.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F10a_Chaudhury.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (a) ![](../../OEBPS/Images/eq_06-29-e.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F10b_Chaudhury.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (b) ![](../../OEBPS/Images/eq_06-29-f.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 Two-dimensional GMMs with elliptical bases, *Ï€*[1] = 0.3, *Ï€*[2]
    = 0.7. Note how the shape of the hill base depends on Î£ and how the hill positions
    depend on the ![](../../OEBPS/Images/AR_micro.png)s.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F11a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F11b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 (a) ![](../../OEBPS/Images/eq_06-29-g.png). (b) 1, 000 random samples
    from a GMM with the same three component Gaussians as in (a) and *Ï€*[1] = *Ï€*[2]
    = 0.4, *Ï€*[3] = 0.2. Note how the GMM sample distribution shape mimics the combined
    sample distribution shape of the component Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be proved that equation [6.29](../Text/06.xhtml#eq-gmm) is a proper
    probability: that is, it sums to 1 over the space of all possible inputs (all
    possible values of ![](../../OEBPS/Images/AR_x.png) in the *d*-dimensional space).
    Here is the proof outline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29-h.png)'
  prefs: []
  type: TYPE_IMG
- en: 6.9.2 Latent variables for class selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Letâ€™s discuss GMMs in more detail. In particular, we look at the physical meaning
    of the various terms in equation [6.29](../Text/06.xhtml#eq-gmm).
  prefs: []
  type: TYPE_NORMAL
- en: Before diving in, letâ€™s introduce an auxiliary random variable *Z*, which effectively
    is a *class selector*. In the context of equation [6.29](../Text/06.xhtml#eq-gmm),
    *Z* can take discrete values in the range [1â‹¯*K*]. It thus follows a categorical
    distribution (see section [5.9.6](../Text/05.xhtml#sec-categorical-distr)). Physically,
    *Z* = *k* means the *k*th classâ€”that is, the *k*th component of the Gaussian mixtureâ€”has
    been selected.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE As usual, we are denoting the random variable with uppercase and the specific
    value it takes in a given instance with lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in the two-class problem shown in figure [6.6](../Text/06.xhtml#fig-gmm_1d_2cls),
    *Z* can take one of two values: 1 (implying adult female) or 2 (implying adult
    male). For the three-class problem shown in figure [6.7](../Text/06.xhtml#fig-gmm_2d_3cls),
    *Z* can take one of three values: 1 (adult female), 2 (adult male), or 3 (child).
    *Z* is called a *latent (hidden) random variable* because its values are not directly
    observed. Contrast this with the input random variable ![](../../OEBPS/Images/AR_x.png)
    whose values are explicitly observed. You may recognize *Z* as a latent variable
    in the GMM (latent variables were introduced in section [6.7](../Text/06.xhtml#sec-evidence_maximization)).'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the joint probability *p*(*X* = ![](../../OEBPS/Images/AR_x.png), *Z*
    = *k*), which we sometimes informally denote as *p*(![](../../OEBPS/Images/AR_x.png),
    *k*). This is the probability of the input variable ![](../../OEBPS/Images/AR_x.png)
    occurring together with the class *k*. Using Bayesâ€™ theorem,
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(![](../../OEBPS/Images/AR_x.png), *k*) = *p*(![](../../OEBPS/Images/AR_x.png)|*k*)*p*(*k*)'
  prefs: []
  type: TYPE_NORMAL
- en: The conditional probability term *p*(![](../../OEBPS/Images/AR_x.png)|*k*) is
    the probability of ![](../../OEBPS/Images/AR_x.png) when the *k*th class has been
    selected. This means it is the PDF for the *k*th Gaussian component, which is
    a Gaussian distribution by assumption. As such, using equation [5.23](../Text/05.xhtml#eq-multivar-normal),
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(![](../../OEBPS/Images/AR_x.png)|*k*) = ð’©(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Î£***[k]*) *k* âˆˆ [1, *K*]'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, *p*(*Z* = *k*), which we sometimes informally refer to as
    *p*(*k*), is the *prior probability (that is, without reference to the input)
    of the input belonging to one of the classes*. Letâ€™s denote it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*k*) = *Ï€^k*, âˆ€*k* âˆˆ {1, *K*}'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is often modeled as the *fraction of training data points belonging to
    class k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29-i.png)'
  prefs: []
  type: TYPE_IMG
- en: where *N[k]* is the number of training data instances belonging to class *k*,
    and *N* is the total number of training data instances.
  prefs: []
  type: TYPE_NORMAL
- en: From this, we get
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(![](../../OEBPS/Images/AR_x.png), *k*) = *p*(*k*)*p*(![](../../OEBPS/Images/AR_x.png)|*k*)
    = *Ï€^k* ð’©(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Î£***[k]*) *k* âˆˆ [1, *K*]'
  prefs: []
  type: TYPE_NORMAL
- en: From equation [5.5](../Text/05.xhtml#eq-marginal-prob), we get the marginal
    probability *p*(*x*)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-29-j.png)'
  prefs: []
  type: TYPE_IMG
- en: which is the same as equation [6.29](../Text/06.xhtml#eq-gmm).
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following physical interpretations:'
  prefs: []
  type: TYPE_NORMAL
- en: A GMM can be viewed as a weighted sum of *K* Gaussian components. Equation [6.29](../Text/06.xhtml#eq-gmm)
    depicts the PDF of the overall GMM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *weights* *Ï€[k]* *are component selection probabilities*. Specifically,
    *Ï€[k]* can be interpreted as the prior probability *p*(*Z* = *k*), aka *p*(*k*),
    of selecting the *k*th subclassâ€”modeled as the fraction of the population belonging
    to the *k*th subclass. The *Ï€[k]* are probabilities in a categorical distribution
    with *K* classes. The *Ï€[k]*s sum up to 1. Sampling from the GMM can be viewed
    as a two-step process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly select a component. The probability of the *k*th component being selected
    is *Ï€[k]*. The sum of all *Ï€[k]*s is 1, which signifies that one or another component
    must be selected.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Random sample from the selected Gaussian component. The probability of generating
    vector ![](../../OEBPS/Images/AR_x.png) is ð’©(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Î£***[k]*).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of the *K* Gaussian components models an individual class. Geometrically
    speaking, the components correspond to the clusters in the sample point cloud
    or the peaks in the PDF of the GMM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *k*th Gaussian component, ð’©(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Î£***[k]*), can be interpreted as the conditional probability, *p*(![](../../OEBPS/Images/AR_x.png)|*k*).
    This is the likelihoodâ€”the probability of data value ![](../../OEBPS/Images/AR_x.png)
    occurring, *given* that the *k*th subclass has been selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The product *Ï€[k]* ð’©(![](../../OEBPS/Images/AR_x.png); *![](../../OEBPS/Images/AR_micro.png)[k]*,
    **Î£***[k]*) then represents the joint probability *p*(![](../../OEBPS/Images/AR_x.png),
    *k*) = *p*(![](../../OEBPS/Images/AR_x.png)|*k*)Â *p*(*k*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of all the joint subclass probabilities is the marginal probability
    *p*(![](../../OEBPS/Images/AR_x.png)) of the data value ![](../../OEBPS/Images/AR_x.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 6.8 Gaussian mixture model distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Pytorch supports distributions that are mixtures of the same family (here,
    Gaussian)
  prefs: []
  type: TYPE_NORMAL
- en: 'â‘¡ Prior probabilities over the three classes (male, female, child): categorical
    distribution'
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Mean height, weight for the three classes (male, female, child)
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Covariance matrices for the three classes male, female, child)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Creates the component Gaussians
  prefs: []
  type: TYPE_NORMAL
- en: â‘¥ Creates the GMM
  prefs: []
  type: TYPE_NORMAL
- en: 6.9.3 Classification via GMM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A typical practical problem involving GMMs goes as follows. A set of unlabeled
    input data *X* training data) is provided. It is important to note that this is
    unsupervised machine learningâ€”the training data does not come with known output
    classes. The physical nature of the problem indicates the subclasses in the data
    (denoted by indices [1â‹¯*K*]). The goal is to classify any arbitrary input ![](../../OEBPS/Images/AR_x.png):
    that is, map it to one of the *K* classes. To do this, we have to fit a GMM (that
    is, derive the values of *Ï€[k]*, *![](../../OEBPS/Images/AR_micro.png)[k]*, **Î£***[k]*
    for all *k* âˆˆ [1 â‹¯Â *K*]). Given an arbitrary ![](../../OEBPS/Images/AR_x.png),
    we compute *p*(*k*|![](../../OEBPS/Images/AR_x.png)) for all the classes (all
    values of *k*). The value of *k* yielding the max value for *p*(*k*|![](../../OEBPS/Images/AR_x.png))
    is the class corresponding to ![](../../OEBPS/Images/AR_x.png). How do we compute
    *p*(*k*|![](../../OEBPS/Images/AR_x.png))?'
  prefs: []
  type: TYPE_NORMAL
- en: Using Bayesâ€™ theorem,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-30.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.30
  prefs: []
  type: TYPE_NORMAL
- en: 'If we know all the GMM parameters, evaluating equation [6.30](../Text/06.xhtml#eq-gmm-posterior-for-classification)
    is straightforward. We classify the input ![](../../OEBPS/Images/AR_x.png) by
    assigning it to the cluster *k* that yields the highest value of *p*(*Z* = *k*|*X*
    = *x*). Geometrically, this assigns the input to the cluster with the â€œclosestâ€
    meanâ€”with distance normalized by the variance of the respective distribution.
    Basically, we are measuring the distance from the mean, but in clusters of high
    variance, we are more tolerant of distance from the mean. This makes intuitive
    sense: if the cluster is widely spread has high variance), a point relatively
    far from the cluster mean can be said to belong to the cluster. On the other hand,
    a point the same distance from the mean of a tightly packed cluster may be deemed
    to be outside the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.9.4 Maximum likelihood estimation of GMM parameters (GMM fit)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A GMM is fully described in terms of its parameter set *Î¸* = {*Ï€^k*, *![](../../OEBPS/Images/AR_micro.png)[k]*,Â **Î£***[k]*
    âˆ€*kÂ *âˆˆÂ [1 â‹¯Â *K*]}. But how do we estimate these parameter values? In typical real-life
    situations, they are not given to us. We only have a set of observed unlabeled
    training data points *X* = {![](../../OEBPS/Images/AR_x.png)^((*i*))}, such as
    (weight, height) values for Statsville residents.
  prefs: []
  type: TYPE_NORMAL
- en: Geometrically speaking, each data instance in the training dataset corresponds
    to a single point in the multidimensional feature space. The training dataset
    is a point cloud that naturally clusters into Gaussian subclouds (otherwise, we
    should not be trying GMMs). Our GMM mimicking this dataset should have as many
    components as there are natural clusters in the data. The parameter values *Ï€[k]*,
    *![](../../OEBPS/Images/AR_micro.png)[k]*, **Î£***[k]* for *kÂ * âˆˆÂ  [1 â‹¯Â *K*] should
    be estimated such that the GMMâ€™s sample point cloud overlaps the training data
    point cloud as much as possible. That is the basic problem we try to solve in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE We do not estimate *K*, the number of classes; rather, we use a fixed value
    of *K*, usually estimated from the physical conditions of the problem. For example,
    in the problem with men, women, and children, it is pretty obvious that *K* =
    3.
  prefs: []
  type: TYPE_NORMAL
- en: In section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation), we did
    MLE for a simple Gaussian. We computed an expression for the joint log-likelihood
    of all the training data given a Gaussian probability distribution. Then we took
    the gradient of that expression with respect to the parameters and equated it
    to zero. We were able to solve that equation to derive a *closed-form* solution
    for the parameters, ![](../../OEBPS/Images/AR_micro.png) and Î£ (equations [6.25](../Text/06.xhtml#eq-gauss-MLE-mean)
    and [6.26](../Text/06.xhtml#eq-gauss-MLE-covar) ). This means we simplified the
    equation into a form where the unknown (to be solved) appeared alone on the left-hand
    side and there were only known entities on the right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, with GMMs, equating the gradient of the log-likelihood to zero
    leads to an equation that has no closed-form solution. So, we cannot reduce the
    equation to a form where the unknowns *Ï€[k]*s, *Î¼[k]*s, and **Î£***[k]* appear
    alone on the left-hand sides and only known entities (![](../../OEBPS/Images/AR_x.png)*[i]*s)
    appear on the right-hand side. Consequently, we have to go for an iterative approximation.
    We rewrite the equation we get by equating the gradient of the log-likelihood
    to zero such that the unknowns *Î¼*s and *Ïƒ*s appear alone on the right-hand side.
    It looks something like
  prefs: []
  type: TYPE_NORMAL
- en: '*Ï€^k* = *f*[1](*X, Î˜*)'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](../../OEBPS/Images/AR_micro.png)[k]* = *f*[2](*X, Î˜*)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Î£***[k]* = *f*[3](*X, Î˜*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'where *f*[1], *f*[2], *f*[3] are some functions whose exact nature is unimportant
    at the moment. Note that the right-hand side also contains the unknowns: *Î¸* contains
    *Ï€[k]*s, *Î¼[k]*s, and **Î£***[k]*. We cannot directly solve such equations, but
    we can use *iterative relaxation*, which works roughly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with random values of *Ï€[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Î£***[k]*s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the right-hand side by plugging current values of *Ï€[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Î£***[k]*s into functions *f*[1], *f*[2], and *f*[3].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the values estimated in step 2 to set new values of *Ï€[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Î£***[k]*s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1â€“3 until the parameter values stop changing appreciably.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The actual functions *f*[1], *f*[2], *f*[3] are worked out in equations [6.36](../Text/06.xhtml#eq-gmmfit-pi),
    [6.37](../Text/06.xhtml#eq-gmmfit-mu), and [6.38](../Text/06.xhtml#eq-gmmfit-sigma)).
    As iteration progresses, the values of *Ï€[k]*s, *![](../../OEBPS/Images/AR_micro.png)[k]*s,
    and **Î£***[k]*s start to converge to their true values. This is not a lucky coincidence.
    If we follow algorithm [6.3](../Text/06.xhtml#alg-gmm_fit), it can be proved that
    every iteration improves the approximation, even if by a minuscule amount. Eventually,
    we reach a point when the approximation is no longer improving appreciably. This
    is called the *fixed point*, and we should stop iterating and declare the current
    values final.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [6.12](../Text/06.xhtml#fig-GMM-fit-iters) shows the progression of
    an iterative GMM fit algorithm. Figure [6.12a](../Text/06.xhtml#fig-GMM-fit-gaussians)
    shows the sampled training data distribution. Figure [6.12b](../Text/06.xhtml#fig-GMM-fit-step0)
    shows the fitted GMM at the beginning: the parameters are essentially random,
    and the GMM looks nothing like the target training data distribution. It improves
    slowly until at iteration 15, it matches the target distribution snugly figure
    [6.12d](../Text/06.xhtml#fig-GMM-fit-step15)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F12a_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Training data point cloud (target for fitting)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F12b_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Fitted GMMâ€™s sample point cloud at step 0
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F12c_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Fitted GMMâ€™s sample point cloud at step 5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH06_F12d_Chaudhury.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Fitted GMMâ€™s sample point cloud at step 15\. It almost matches the target.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 Progression of maximum likelihood estimation for GMM parameters
  prefs: []
  type: TYPE_NORMAL
- en: Now letâ€™s discuss the details. We *already know* the dataset *X* that has been
    observed. What parameter set *Î¸* will maximize the conditional probability, *p*(*X*|*Î¸*),
    of exactly these data points, given the parameter set? In other words, what model
    parameters will maximize the overall likelihood of the training data? Those will
    be our best guesses for the unknown model parameters. This is MLE, which we encountered
    in section [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation).
  prefs: []
  type: TYPE_NORMAL
- en: Let {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((*n*)),â‹¯![](../../OEBPS/Images/AR_x.png)^((*n*))}
    be the set of observed data points, aka training data. From equation [6.29](../Text/06.xhtml#eq-gmm),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-30-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Henceforth, for simplicity, we drop the â€œgiven *Î¸*â€ part and refer to *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))|*Î¸*)
    simply as *p*(![](../../OEBPS/Images/AR_x.png)^((*i*))). As usual, instead of
    maximizing the likelihood directly, we maximize its logarithm, the *log-likelihood*.
    This will yield the same parameters as maximizing the likelihood directly.
  prefs: []
  type: TYPE_NORMAL
- en: Since the *x*^((*i*))s are independent, their joint probability, as per equation
    [5.4](../Text/05.xhtml#eq-joint-prob-indep), is
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(![](../../OEBPS/Images/AR_x.png)^((1)))*p*(![](../../OEBPS/Images/AR_x.png)^((2)))â‹¯*p*(![](../../OEBPS/Images/AR_x.png)^((*n*)))'
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding log joint probability is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.31
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we begin to see a difficulty peculiar to GMMs. We have a logarithm
    of a sum, which is not a very friendly expression to handle; the logarithm of
    products is much nicer to deal with. But letâ€™s soldier on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To identify the parameters ![](../../OEBPS/Images/AR_micro.png)Â¹, Î£[1], ![](../../OEBPS/Images/AR_micro.png)Â²,
    Î£[2], â‹¯ that will maximize the log joint probability, we take the gradient of
    the log joint probability with respect to these parameters, equate them to zero,
    and solve for the parameter value (as discussed in section [3.3.1](../Text/03.xhtml#sec-gradient)).
    Here we demonstrate the process with respect to ![](../../OEBPS/Images/AR_micro.png)[1]:'
  prefs: []
  type: TYPE_NORMAL
- en: âˆ‡[![](../../OEBPS/Images/AR_micro.png)[1]]*log*(*p*(![](../../OEBPS/Images/AR_x.png)^((1)))*p*(![](../../OEBPS/Images/AR_x.png)^((2)))â‹¯*p*(![](../../OEBPS/Images/AR_x.png)^((*n*))))
    = 0
  prefs: []
  type: TYPE_NORMAL
- en: Since the log of products is the sum of logs, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying equation [6.29](../Text/06.xhtml#eq-gmm), we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the gradient is a linear operator, we can move it inside the summation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Since *d*/*dx* log (*f*(*x*)) = 1/*f*(*x*) *df*/*dx*, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if *x*[1] and *x*[2] are independent variables, *dx*[2]/*dx*[1] = 0. Consequently,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-e.png)'
  prefs: []
  type: TYPE_IMG
- en: Only a single term corresponding to *k* = 1 survives the differentiation (gradient)
    in the numerator. So,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now *d*/*dx* *e*^(â€“(*x â€“ Î¼*)Â²) = â€“2(*x â€“ Î¼*) *e*^(â€“(*x â€“ Î¼*)Â²), and in multiple
    dimensions,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-g.png)'
  prefs: []
  type: TYPE_IMG
- en: Plugging equation [5.23](../Text/05.xhtml#eq-multivar-normal) into our maximization
    problem, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-31-h.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, with a little effort, you can prove the following about the gradient
    of a quadratic form:'
  prefs: []
  type: TYPE_NORMAL
- en: âˆ‡[![](../../OEBPS/Images/AR_x.png)](![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png))
    = *A*![](../../OEBPS/Images/AR_x.png)
  prefs: []
  type: TYPE_NORMAL
- en: Equation 6.32
  prefs: []
  type: TYPE_NORMAL
- en: Applying equation [6.32](../Text/06.xhtml#eq-grad-quad_form) to our problem,
    we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-32-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiplying both sides by the constant **Î£**[1], we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-32-b.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-33.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.33
  prefs: []
  type: TYPE_NORMAL
- en: we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-33-a.png)'
  prefs: []
  type: TYPE_IMG
- en: This expression has *Î¼*[1] inside *Î³*[*i*1] as well. It is impossible to extract
    *Î¼*[1] alone on the left side of the equation. In other words, we cannot create
    a *closed-form* solution for *Î¼*[1]. Hence, we have to solve it iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: We can rewrite the previous equation as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-33-b.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-34.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.34
  prefs: []
  type: TYPE_NORMAL
- en: 'Proceeding similarly, we can derive the corresponding expressions for *Ï€*[1]
    and **Î£**[1]. Letâ€™s collect all the equations for updating the GMM parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-35.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-36.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-37.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-38.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 6.38
  prefs: []
  type: TYPE_NORMAL
- en: Equations [6.36](../Text/06.xhtml#eq-gmmfit-pi), [6.37](../Text/06.xhtml#eq-gmmfit-mu),
    and [6.38](../Text/06.xhtml#eq-gmmfit-sigma) provide the definitions for functions
    *f*[1], *f*[2], and *f*[3] that we saw at the beginning of this section in the
    context of iterative relaxation. We can deal similarly with *k* = 2â‹¯*K*.
  prefs: []
  type: TYPE_NORMAL
- en: Physical significance of *Î³[ik]*
  prefs: []
  type: TYPE_NORMAL
- en: We encountered the entity *Î³[ik]* while computing the gradient of the log-likelihood.
    It appeared as a multiplicative weight in the final iterative expression for computing
    *Î¼[k]* and *Î£[k]* in equations [6.37](../Text/06.xhtml#eq-gmmfit-mu) and [6.38](../Text/06.xhtml#eq-gmmfit-sigma).
    It is not an arbitrary entity. By comparing equations [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
    and [6.30](../Text/06.xhtml#eq-gmm-posterior-for-classification), we can see that
  prefs: []
  type: TYPE_NORMAL
- en: '*Î³[ik]* = *p*(*k*|![](../../OEBPS/Images/AR_x.png)^((*i*)))'
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the quantity *Î³[ik]* is really the posterior probability: the
    conditional probability of the class *k* given the *i*th data point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us a new way to look at equations [6.35](../Text/06.xhtml#eq-gmmfit-Nk),
    [6.36](../Text/06.xhtml#eq-gmmfit-pi), [6.37](../Text/06.xhtml#eq-gmmfit-mu),
    and [6.38](../Text/06.xhtml#eq-gmmfit-sigma):'
  prefs: []
  type: TYPE_NORMAL
- en: Equation [6.35](../Text/06.xhtml#eq-gmmfit-Nk) essentially assigns to *N*[1]
    the probability mass concentrated in class 1 as per the current parameter values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equation [6.36](../Text/06.xhtml#eq-gmmfit-pi) assigns to *Ï€*[1] the fractional
    mass in class 1 as per the current parameter values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equation [6.37](../Text/06.xhtml#eq-gmmfit-mu) assigns to *Î¼*[1] the centroid
    of all the training data points. Each data pointâ€™s contribution is weighted by
    the posterior probability, as per the current parameter values, of that data point
    belonging to class 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equation [6.38](../Text/06.xhtml#eq-gmmfit-sigma) assigns to **Î£**[1] the covariance
    of the training data points. Each data pointâ€™s contribution is weighted by the
    posterior probability, as per the current parameter values, of that data point
    belonging to class 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm [6.3](../Text/06.xhtml#alg-gmm_fit) ties together equations [6.33](../Text/06.xhtml#eq-gmmfit-gamma),
    [6.36](../Text/06.xhtml#eq-gmmfit-pi), [6.37](../Text/06.xhtml#eq-gmmfit-mu),
    and [6.38](../Text/06.xhtml#eq-gmmfit-sigma) into a complete approach for iterative
    MLE of GMM parameters. It is an example of a general class of algorithms called
    *expectation maximization*.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 6.3 GMM fit (MLE of GMM parameters from unlabeled training data)
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: *X* = ![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),
    â€¦ , ![](../../OEBPS/Images/AR_x.png)^((*n*))'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize parameters *Î˜* = {*Ï€^k* , *Î¼[k]* , **Î£***[k]* *k* âˆˆ [1, *K*]} with
    random values
  prefs: []
  type: TYPE_NORMAL
- en: âŠ³ repeat E-step and M-step until likelihood stops increasing
  prefs: []
  type: TYPE_NORMAL
- en: '**while** (likelihood is increasing ) **do**'
  prefs: []
  type: TYPE_NORMAL
- en: âŠ³E-step
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-38-a.png)'
  prefs: []
  type: TYPE_IMG
- en: âŠ³M-step
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_06-38-b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**end** **while**'
  prefs: []
  type: TYPE_NORMAL
- en: return {*x*[1] , *Î¼*[1], **Î£[1]**, *x*[2], *Î¼*[2], **Î£[2]** , â€¦ , *x[K]*, *Î¼[K]*,
    **Î£*[K]***}
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for Gaussian mixture modeling, executable via Jupyter
    Notebook, can be found at [http://mng.bz/j4er](http://mng.bz/j4er).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 GMM fit
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: â‘  Repeats until the likelihood increase is negligible
  prefs: []
  type: TYPE_NORMAL
- en: â‘¡ Computes the posterior probabilities *Î³[i,k]* = *p*(*Z* = *k*|*X* = *x[i]*)
    using current *![](../../OEBPS/Images/AR_micro.png)[k]*s and Î£*[k]*s, equation
    [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¢ Tensor of shape [K] holding *Ï€[k]*s for all [k]
  prefs: []
  type: TYPE_NORMAL
- en: â‘£ Gaussian objects ð’©(![](../../OEBPS/Images/AR_x.png), *![](../../OEBPS/Images/AR_micro.png)[k]*,
    Î£*[k]*) for all *k*
  prefs: []
  type: TYPE_NORMAL
- en: â‘¤ Vector computation of log of *Î³[i, k]* numerators for all i, k, equation [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¥ In practice, the probability involving an exponential goes to 0\. So we use
    the log probability.
  prefs: []
  type: TYPE_NORMAL
- en: â‘¦ Vector computation of the log of *Î³*[*i,* *k*] denominators for all i, k,
    equation [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  prefs: []
  type: TYPE_NORMAL
- en: â‘§ Vector computation of the [*n* *Ã—* *K*] tensor *Î³*[*i,* *k*], equation [6.33](../Text/06.xhtml#eq-gmmfit-gamma)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¨ Updates *![](../../OEBPS/Images/AR_micro.png)[k]* and Î£[k] for all *k* using
    *Î³*[*i,* *k*] = *p*(*Z* = *k*|*X*) from the E-step via equations [6.36](../Text/06.xhtml#eq-gmmfit-pi),
    [6.37](../Text/06.xhtml#eq-gmmfit-mu), and [6.38](../Text/06.xhtml#eq-gmmfit-sigma)
  prefs: []
  type: TYPE_NORMAL
- en: â‘© Number of data points
  prefs: []
  type: TYPE_NORMAL
- en: â‘ª Vector update of *Ï€[k]* for all k, equation [6.36](../Text/06.xhtml#eq-gmmfit-pi)
  prefs: []
  type: TYPE_NORMAL
- en: â‘« Vector update of [*K* Ã— *d*] tensor, *![](../../OEBPS/Images/AR_micro.png)[k]*
    for all k, equation [6.37](../Text/06.xhtml#eq-gmmfit-mu)
  prefs: []
  type: TYPE_NORMAL
- en: â‘¬ Vector computation of (*![](../../OEBPS/Images/AR_x.png)[i]* â€“ *![](../../OEBPS/Images/AR_micro.png)[k]*)
    (*![](../../OEBPS/Images/AR_x.png)[i]* â€“ *![](../../OEBPS/Images/AR_micro.png)[k]*)^T
    for all *l, k*
  prefs: []
  type: TYPE_NORMAL
- en: â‘­ Vector update of *K* Ã— *d* Ã— *d*] tensor Î£[k] for all k, equation [6.38](../Text/06.xhtml#eq-gmmfit-sigma)
  prefs: []
  type: TYPE_NORMAL
- en: â‘® log likelihood, equation [6.31](../Text/06.xhtml#eq-gmm-loglikelihood)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we looked at the Bayesian tools for decision-making in uncertain
    systems. We discussed conditional probability and Bayesâ€™ theorem, which connects
    conditional probabilities to joint and marginal probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability is the probability of an event occurring subject to
    the condition that another event has already occurred. In machine learning, we
    are often interested in the conditional probability *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)
    of an input ![](../../OEBPS/Images/AR_x.png) given that the parameters of the
    model predicting the input are *Î¸*. This conditional probability is known as the
    likelihood of the input. We are also interested in the conditional probability
    *p*(*Î¸*|![](../../OEBPS/Images/AR_x.png)), known as the posterior probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joint probability is the probability of a set of events occurring together.
    If the events are independent, the joint probability is the product of their individual
    probabilities. Whether events are independent or not, Bayesâ€™ theorem connects
    joint and conditional probabilities. Of particular interest in machine learning
    is the Bayesâ€™ theorem expression connecting the likelihood and joint and posterior
    probabilities of inputs and parameters: *p*(![](../../OEBPS/Images/AR_x.png),
    *Î¸*) = *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)*p*(*Î¸*) and *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)*p*(*Î¸*)
    / *p*(![](../../OEBPS/Images/AR_x.png)). *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)
    is the probability distribution function of the chosen distribution family. *p*(*Î¸*)
    is the prior probability that codifies our belief, sans data, about the system.
    A popular choice is *p*(*Î¸*) âˆ *e*^(âˆ’||*Î¸*||Â²), implying smaller probabilities
    for higher-magnitude parameters and vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy models the uncertainty in a system. Systems where all events have more
    or less similar probabilities tend to be high-entropy. Systems where a particular
    subset of possible events have significantly high probabilities and others have
    significantly low probabilities tend to be low-entropy. Equivalently, the probability
    density functions of low-entropy systems tend to have tall peaks, and their sample
    point clouds have a high concentration of points in some regions. High-entropy
    systems tend to have flat probability density functions and diffused sample point
    clouds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-entropy allows us to quantify how good our modeling is against a known
    ground truth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kullbackâ€“Leibler divergence gives us a measure of the dissimilarity between
    two probability distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation
    are two paradigms for estimating model parameters. MLE maximizes *p*(*X*|*Î¸*),
    and MAP maximizes *p*(*X*|*Î¸*)*p*(*Î¸*). MLE essentially tries to estimate probability
    distribution parameters that maximize the overlap between the sample point cloud
    of the probability distribution and the training data point cloud. MAP is MLE
    with a regularization condition. The regularization condition is injected via
    the prior probability term *p*(*Î¸*), which favors solutions with a certain property
    (such as small parameter magnitudes) that we believe to be true from empirical
    knowledge without data. MLE for Gaussian distributions has a closed-form solution.
    The mean and variance (covariance in the multidimensional case) of the optimal
    probability distribution that best fits the training data are the sample mean
    and sample variance or covariance on the training dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent variables in a machine learning system are auxiliary variables that are
    not directly observed but can be derived from the input. They facilitate the expression
    of the goal of optimization or the loss to be minimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian mixture models (GMM) are unsupervised probability models that fit multiclass
    data distributions having multiple clusters in the training dataset, each corresponding
    to a different class. Here, MLE does not yield a closed-form solution but instead
    yields an iterative solution to estimate the mixture weights, means, and variances
    of the individual Gaussians in the mixture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
