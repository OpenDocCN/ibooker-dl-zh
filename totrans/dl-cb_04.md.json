["```py\n04.1 Collect movie data from Wikipedia\n04.2 Build a recommender system based on outgoing Wikipedia links\n```", "```py\nindex = requests.get('https://dumps.wikimedia.org/backup-index.html').text\nsoup_index = BeautifulSoup(index, 'html.parser')\ndumps = [a['href'] for a in soup_index.find_all('a')\n             if a.has_attr('href') and a.text[:-1].isdigit()]\n```", "```py\nfor dump_url in sorted(dumps, reverse=True):\n    print(dump_url)\n    dump_html = index = requests.get(\n        'https://dumps.wikimedia.org/enwiki/' + dump_url).text\n    soup_dump = BeautifulSoup(dump_html, 'html.parser')\n    pages_xml = [a['href'] for a in soup_dump.find_all('a')\n                 if a.has_attr('href')\n                 and a['href'].endswith('-pages-articles.xml.bz2')]\n    if pages_xml:\n        break\n    time.sleep(0.8)\n```", "```py\nwikipedia_dump = pages_xml[0].rsplit('/')[-1]\nurl = url = 'https://dumps.wikimedia.org/' + pages_xml[0]\npath = get_file(wikipedia_dump, url)\npath\n```", "```py\nclass WikiXmlHandler(xml.sax.handler.ContentHandler):\n    def __init__(self):\n        xml.sax.handler.ContentHandler.__init__(self)\n        self._buffer = None\n        self._values = {}\n        self._movies = []\n        self._curent_tag = None\n\n    def characters(self, content):\n        if self._curent_tag:\n            self._buffer.append(content)\n\n    def startElement(self, name, attrs):\n        if name in ('title', 'text'):\n            self._curent_tag = name\n            self._buffer = []\n\n    def endElement(self, name):\n        if name == self._curent_tag:\n            self._values[name] = ' '.join(self._buffer)\n\n        if name == 'page':\n            movie = process_article(**self._values)\n            if movie:\n                self._movies.append(movie)\n```", "```py\ndef process_article(title, text):\n    rotten = [(re.findall('\\d\\d?\\d?%', p),\n        re.findall('\\d\\.\\d\\/\\d+|$', p), p.lower().find('rotten tomatoes'))\n        for p in text.split('\\n\\n')]\n    rating = next(((perc[0], rating[0]) for perc, rating, idx in rotten\n        if len(perc) == 1 and idx > -1), (None, None))\n    wikicode = mwparserfromhell.parse(text)\n    film = next((template for template in wikicode.filter_templates()\n                 if template.name.strip().lower() == 'infobox film'),\n                 None)\n    if film:\n        properties = {param.name.strip_code().strip():\n                      param.value.strip_code().strip()\n                      for param in film.params\n                      if param.value.strip_code().strip()\n                     }\n        links = [x.title.strip_code().strip()\n                 for x in wikicode.filter_wikilinks()]\n        return (title, properties, links) + rating\n```", "```py\nparser = xml.sax.make_parser()\nhandler = WikiXmlHandler()\nparser.setContentHandler(handler)\nfor line in subprocess.Popen(['bzcat'],\n                             stdin=open(path),\n                             stdout=subprocess.PIPE).stdout:\n  try:\n    parser.feed(line)\n  except StopIteration:\n    break\n```", "```py\nwith open('wp_movies.ndjson', 'wt') as fout:\n  for movie in handler._movies:\n    fout.write(json.dumps(movie) + '\\n')\n```", "```py\nlink_counts = Counter()\nfor movie in movies:\n    link_counts.update(movie[2])\nlink_counts.most_common(3)\n```", "```py\n[(u'Rotten Tomatoes', 9393),\n (u'Category:English-language films', 5882),\n (u'Category:American films', 5867)]\n```", "```py\ntop_links = [link for link, c in link_counts.items() if c >= 3]\nlink_to_idx = {link: idx for idx, link in enumerate(top_links)}\nmovie_to_idx = {movie[0]: idx for idx, movie in enumerate(movies)}\npairs = []\nfor movie in movies:\n    pairs.extend((link_to_idx[link], movie_to_idx[movie[0]])\n                  for link in movie[2] if link in link_to_idx)\npairs_set = set(pairs)\n```", "```py\ndef movie_embedding_model(embedding_size=30):\n    link = Input(name='link', shape=(1,))\n    movie = Input(name='movie', shape=(1,))\n    link_embedding = Embedding(name='link_embedding',\n        input_dim=len(top_links), output_dim=embedding_size)(link)\n    movie_embedding = Embedding(name='movie_embedding',\n        input_dim=len(movie_to_idx), output_dim=embedding_size)(movie)\n    dot = Dot(name='dot_product', normalize=True, axes=2)(\n        [link_embedding, movie_embedding])\n    merged = Reshape((1,))(dot)\n    model = Model(inputs=[link, movie], outputs=[merged])\n    model.compile(optimizer='nadam', loss='mse')\n    return model\n\nmodel = movie_embedding_model()\n```", "```py\ndef batchifier(pairs, positive_samples=50, negative_ratio=5):\n    batch_size = positive_samples * (1 + negative_ratio)\n    batch = np.zeros((batch_size, 3))\n    while True:\n        for idx, (link_id, movie_id) in enumerate(\n                random.sample(pairs, positive_samples)):\n            batch[idx, :] = (link_id, movie_id, 1)\n        idx = positive_samples\n        while idx < batch_size:\n            movie_id = random.randrange(len(movie_to_idx))\n            link_id = random.randrange(len(top_links))\n            if not (link_id, movie_id) in pairs_set:\n                batch[idx, :] = (link_id, movie_id, -1)\n                idx += 1\n        np.random.shuffle(batch)\n        yield {'link': batch[:, 0], 'movie': batch[:, 1]}, batch[:, 2]\n```", "```py\npositive_samples_per_batch=512\n\nmodel.fit_generator(\n    batchifier(pairs,\n               positive_samples=positive_samples_per_batch,\n               negative_ratio=10),\n    epochs=25,\n    steps_per_epoch=len(pairs) // positive_samples_per_batch,\n    verbose=2\n)\n```", "```py\nmovie = model.get_layer('movie_embedding')\nmovie_weights = movie.get_weights()[0]\nlens = np.linalg.norm(movie_weights, axis=1)\nnormalized = (movie_weights.T / lens).T\n```", "```py\ndef neighbors(movie):\n    dists = np.dot(normalized, normalized[movie_to_idx[movie]])\n    closest = np.argsort(dists)[-10:]\n    for c in reversed(closest):\n        print(c, movies[c][0], dists[c])\n\nneighbors('Rogue One')\n```", "```py\n29 Rogue One 0.9999999\n3349 Star Wars: The Force Awakens 0.9722805\n101 Prometheus (2012 film) 0.9653338\n140 Star Trek Into Darkness 0.9635347\n22 Jurassic World 0.962336\n25 Star Wars sequel trilogy 0.95218825\n659 Rise of the Planet of the Apes 0.9516557\n62 Fantastic Beasts and Where to Find Them (film) 0.94662267\n42 The Avengers (2012 film) 0.94634\n37 Avatar (2009 film) 0.9460137\n```", "```py\nbest = ['Star Wars: The Force Awakens', 'The Martian (film)',\n        'Tangerine (film)', 'Straight Outta Compton (film)',\n        'Brooklyn (film)', 'Carol (film)', 'Spotlight (film)']\nworst = ['American Ultra', 'The Cobbler (2014 film)',\n         'Entourage (film)', 'Fantastic Four (2015 film)',\n         'Get Hard', 'Hot Pursuit (2015 film)', 'Mortdecai (film)',\n         'Serena (2014 film)', 'Vacation (2015 film)']\ny = np.asarray([1 for _ in best] + [0 for _ in worst])\nX = np.asarray([normalized_movies[movie_to_idx[movie]]\n                for movie in best + worst])\n```", "```py\nclf = svm.SVC(kernel='linear')\nclf.fit(X, y)\n```", "```py\nestimated_movie_ratings = clf.decision_function(normalized_movies)\nbest = np.argsort(estimated_movie_ratings)\nprint('best:')\nfor c in reversed(best[-5:]):\n    print(c, movies[c][0], estimated_movie_ratings[c])\n\nprint('worst:')\nfor c in best[:5]:\n    print(c, movies[c][0], estimated_movie_ratings[c])\n```", "```py\nbest:\n(6870, u'Goodbye to Language', 1.24075226186855)\n(6048, u'The Apu Trilogy', 1.2011876298842317)\n(481, u'The Devil Wears Prada (film)', 1.1759994747169913)\n(307, u'Les Mis\\xe9rables (2012 film)', 1.1646775074857494)\n(2106, u'A Separation', 1.1483743944891462)\nworst:\n(7889, u'The Comebacks', -1.5175929012505527)\n(8837, u'The Santa Clause (film series)', -1.4651252650867073)\n(2518, u'The Hot Chick', -1.464982008376793)\n(6285, u'Employee of the Month (2006 film)', -1.4620595013243951)\n(7339, u'Club Dread', -1.4593221506016203)\n```", "```py\nrotten_y = np.asarray([float(movie[-2][:-1]) / 100\n                       for movie in movies if movie[-2]])\nrotten_X = np.asarray([normalized_movies[movie_to_idx[movie[0]]]\n                       for movie in movies if movie[-2]])\n```", "```py\nTRAINING_CUT_OFF = int(len(rotten_X) * 0.8)\nregr = LinearRegression()\nregr.fit(rotten_X[:TRAINING_CUT_OFF], rotten_y[:TRAINING_CUT_OFF])\n```", "```py\nerror = (regr.predict(rotten_X[TRAINING_CUT_OFF:]) -\n         rotten_y[TRAINING_CUT_OFF:])\n'mean square error %2.2f' % np.mean(error ** 2)\n```", "```py\nmean square error 0.06\n```", "```py\nerror = (np.mean(rotten_y[:TRAINING_CUT_OFF]) - rotten_y[TRAINING_CUT_OFF:])\n'mean square error %2.2f' % np.mean(error ** 2)\n```", "```py\n'mean square error 0.09'\n```"]