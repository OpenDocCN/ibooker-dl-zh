# 第一章 探索人工智能的领域

以下是 Dr. May Carson（图 1-1）关于人工智能（AI）在 21 世纪人类生活中角色变化的重要论文中的话：

> 人工智能经常被称为 21 世纪的电力。今天，人工智能程序将有能力推动各种行业（包括医疗），设计医疗设备并构建新型产品和服务，包括机器人和汽车。随着 AI 的发展，组织已经在努力确保这些人工智能程序能够胜任工作，并且重要的是，避免错误或危险事故。组织需要 AI，但他们也意识到并非所有可以用 AI 做的事情都是一个好主意。
> 
> 我们对使用这些技术和政策操作人工智能所需的资金进行了广泛研究。主要结论是，每年每人在 AI 项目上花费的资金与用于研究、构建和生产这些项目的资金大致相等。这似乎是显而易见的，但并非完全如此。
> 
> 首先，AI 系统需要支持和维护来帮助它们的功能。为了真正可靠，他们需要人们具备运行它们和帮助他们执行一些任务的技能。AI 组织提供工作人员来执行这些服务所需的复杂任务是至关重要的。了解从事这些工作的人员也很重要，尤其是当 AI 比人类更复杂时。例如，人们通常会从事需要高级知识的工作，但不一定擅长处理需要构建和维护的系统。

![Dr. May Carson](img/00247.jpeg)

###### 图 1-1. Dr. May Carson

# 致歉

现在我们必须坦白承认，本章直到现在为止的所有内容都是完全虚假的。真的是一切！所有的文本（除了第一个斜体句子，这是我们写的种子）都是使用 GPT-2 模型（由 Adam King 构建）在网站*[TalkToTransformer.com](http://TalkToTransformer.com)*上生成的。作者的名字是使用网站*[Onitools.moe](http://Onitools.moe)*上的“Nado Name Generator”生成的。至少作者的照片必须是真实的，对吧？不，这张照片是从网站*[ThisPersonDoesNotExist.com](http://ThisPersonDoesNotExist.com)*生成的，该网站利用生成对抗网络（GANs）的魔法每次重新加载页面时向我们展示不存在的人的新图片。

尽管我们对以不诚实的方式开始整本书感到矛盾，但我们认为展示 AI 的最新技术是重要的，当您，我们的读者，最不期望时。看到 AI 已经能够创造出比一些世界领导人更聪明和雄辩的句子，这实在是令人难以置信、惊人和可怕的。它能够凭空创造出句子，这让我们不禁要说，这是大联盟。

话虽如此，AI 目前还无法从我们这里获得的一件事是有趣的能力。我们希望这本书中前三段虚假的段落将是整本书中最枯燥的部分。毕竟，我们不想被称为“比机器更无聊的作者”。

# 真正的介绍

还记得你看过的一个魔术表演，其中一个把戏让你眼花缭乱，让你想，“他们到底是怎么做到的？！”你是否曾经对一则 AI 应用程序登上新闻感到好奇？在这本书中，我们希望为您提供知识和工具，不仅可以解构，还可以构建类似的应用程序。

通过易于理解的逐步解释，我们剖析了使用人工智能的真实应用程序，并展示了您如何在各种平台上创建这些应用程序——从云端到浏览器，再到智能手机，再到边缘人工智能设备，最终着眼于当前人工智能领域的最终挑战：自动驾驶汽车。

在大多数章节中，我们从一个激励性问题开始，然后一步一步地构建一个端到端的解决方案。在本书的前部分，我们培养了构建人工智能大脑所需的技能。但这只是一半的战斗。构建人工智能的真正价值在于创建可用的应用程序。我们不是在谈论玩具原型。我们希望您构建的软件可以在现实世界中被真实的人们使用，以改善他们的生活。因此，书名中有“实用”一词。为此，我们讨论了我们可以选择的各种选项，并根据性能、能耗、可扩展性、可靠性和隐私权的权衡选择适当的选项。

在这第一章中，我们退后一步，欣赏人工智能历史上的这一时刻。我们探讨人工智能的含义，特别是在深度学习的背景下以及导致深度学习成为 21 世纪初最具突破性技术进步领域之一的事件序列。我们还研究了构成完整深度学习解决方案的核心组件，为我们做好准备，以便在随后的章节中实际动手。

因此，我们的旅程从这里开始，从一个非常基本的问题开始。

# 什么是人工智能？

在本书中，我们经常使用“人工智能”、“机器学习”和“深度学习”这些术语，有时可以互换使用。但从严格的技术角度来看，它们有不同的含义。以下是每个术语的概要（也可参见图 1-2）。

人工智能

这使得机器具备模仿人类行为的能力。IBM 的深蓝是人工智能的一个著名例子。

机器学习

这是人工智能的一个分支，其中机器使用统计技术从以前的信息和经验中学习。目标是让机器根据过去的学习观察来未来采取行动。如果您看过 IBM 的沃森在*危险边缘*上与肯·詹宁斯和布拉德·拉特对决，您就看到了机器学习的实际应用。更具相关性的是，下次垃圾邮件没有进入您的收件箱时，您可以感谢机器学习。

深度学习

这是机器学习的一个子领域，其中使用深度、多层神经网络进行预测，特别擅长计算机视觉、语音识别、自然语言理解等领域。

![人工智能、机器学习和深度学习之间的关系](img/00160.jpeg)

###### 图 1-2。人工智能、机器学习和深度学习之间的关系

在本书中，我们主要关注深度学习。

## 激励性例子

让我们直奔主题。是什么促使我们写这本书？为什么您要花自己辛苦挣来的钱购买这本书？我们的动机很简单：让更多的人参与到人工智能的世界中。您正在阅读这本书意味着我们的工作已经完成了一半。

然而，为了真正激起您的兴趣，让我们看一些出色的例子，展示人工智能已经能够做到什么：

+   “DeepMind 的 AI 代理在《星际争霸 II》中征服人类专家”：*The Verge*，2019

+   “由 AI 生成的艺术品在佳士得拍卖会上以近 50 万美元售出”：*AdWeek*，2018

+   “AI 在检测肺癌方面击败放射科医生”：*美国管理护理杂志*，2019

+   “波士顿动力的 Atlas 机器人可以做跑酷”：*ExtremeTech*，2018

+   “Facebook，卡内基梅隆大学首次打败 6 名扑克专家的 AI”：*ai.facebook.com*，2019

+   “盲人现在可以通过微软的 Seeing AI 触摸探索照片”：*TechCrunch*，2019

+   “IBM 的沃森超级计算机在最后的《危险边缘》比赛中击败人类”：*VentureBeat*，2011

+   “谷歌的 ML-Jam 挑战音乐家即兴演奏并与人工智能合作”：*VentureBeat*，2019

+   “无需人类知识即可掌握围棋游戏”：*Nature*，2017

+   “中国人工智能在诊断脑瘤方面击败医生”：*Popular Mechanics*，2018

+   “使用人工智能发现了两颗新行星”：*Phys.org*，2019

+   “英伟达最新的人工智能软件将粗糙的涂鸦转化为逼真的风景”：*The Verge*，2019

这些人工智能的应用是我们的北极星。这些成就的水平相当于获得金牌的奥运表现。然而，在现实世界中解决一系列问题的应用相当于完成一场 5K 比赛。开发这些应用并不需要多年的训练，但当开发者跨过终点线时，会获得巨大的满足感。我们在这里指导你完成这场 5K 比赛。

在本书中，我们有意地优先考虑广度。人工智能领域的变化如此之快，以至于我们只能希望为您提供正确的思维方式和一系列工具。除了解决个别问题外，我们还将探讨不同的、看似无关的问题之间的基本重叠，这些重叠可以为我们所用。例如，声音识别使用卷积神经网络（CNNs），这也是现代计算机视觉的基础。我们涉及多个领域的实际方面，因此您将能够迅速从 0 到 80，解决现实世界中的问题。如果我们引起了足够的兴趣，让您决定从 80 到 95，我们将认为我们的目标已经实现。正如常用的短语所说，我们希望“民主化人工智能”。

重要的是要注意，人工智能领域的许多进展都是在过去几年中取得的——这一点很难言之过早。为了说明我们已经取得了多大的进步，举个例子：五年前，你需要一个博士学位才能进入这个行业。五年后，你甚至不需要博士学位就能写一本关于这个主题的整本书。（真的，查看我们的个人资料吧！）

尽管现代深度学习的应用似乎非常惊人，但它们并不是靠自己就能达到这一点的。它们站在许多行业巨头的肩膀上，这些人已经在推动行业的极限数十年了。事实上，我们不能完全欣赏这个时代的重要性，而不看整个历史。

# 人工智能简史

让我们回到过去一点：我们整个宇宙都处于一个炽热而密集的状态。然后大约在 140 亿年前开始膨胀，等等...好吧，我们不必回溯那么久（但现在你一整天都会被这首歌困扰，对吧？）。其实，70 年前才种下了人工智能的第一颗种子。艾伦·图灵在他 1950 年的论文《计算机器械与智能》中首次提出了“机器能思考吗？”这实际上涉及到了更大的哲学辩论，即意识和成为人类的含义。拥有创作协奏曲的能力并知道你已经创作了它，这是什么意思？图灵发现这种框架相当狭隘，而提出了一个测试：如果一个人无法区分机器和另一个人，那真的重要吗？一个能够模仿人类的人工智能本质上就是人类。

## 令人兴奋的开端

“人工智能”这个术语是由约翰·麦卡锡在 1956 年的达特茅斯暑期研究项目中创造的。当时实际的计算机甚至并不是真正存在的东西，所以令人惊讶的是，他们能够讨论诸如语言模拟、自我改进的学习机器、对感官数据的抽象等未来领域。当然，其中很多是理论性的。这是人工智能第一次成为一个研究领域而不是一个单一项目。

1957 年，弗兰克·罗森布拉特在他的论文“感知器：一个感知和识别的自动机”中奠定了深度神经网络的基础。他假设应该可以构建一个电子或电机系统，学会识别光学、电气或音调信息的模式相似性。这个系统将类似于人类大脑。他提出使用统计模型进行预测，而不是使用基于规则的模型（当时算法的标准）。

###### 注

在整本书中，我们反复提到*神经网络*这个词组。什么是神经网络？它是人类大脑的简化模型。就像大脑一样，它有*神经元*，当遇到熟悉的东西时会被激活。不同的神经元通过连接（对应我们大脑中的突触）相连，帮助信息从一个神经元流向另一个神经元。

在图 1-3 中，我们可以看到最简单的神经网络的一个示例：感知器。从数学上讲，感知器可以表示如下：

*输出 = f(x[1], x[2], x[3]) = x[1] w[1] + x[2] w[2] + x[3] w[3] + b*

![感知器的示例](img/00270.jpeg)

###### 图 1-3。感知器的示例

1965 年，伊瓦赫涅科和拉帕在他们的论文“数据处理的群方法——随机逼近的竞争方法”中发表了第一个工作的神经网络。在这个领域存在一些争议，但伊瓦赫涅科被一些人认为是深度学习之父。

在这个时候，人们对机器能够做到什么做出了大胆的预测。机器翻译、语音识别等将比人类表现更好。世界各国政府兴奋起来，开始敞开钱包资助这些项目。这场淘金热始于 20 世纪 50 年代末，并一直持续到 20 世纪 70 年代中期。

## 寒冷黑暗的日子

投入了数百万美元后，第一个系统被投入实践。结果发现，很多最初的预言是不现实的。语音识别只有在特定方式下才有效，即使如此，也只适用于有限的一组词汇。语言翻译结果错误严重，成本远远超过人类的成本。感知器（本质上是单层神经网络）很快达到了可靠预测的上限。这限制了它们在现实世界中大多数问题中的实用性。这是因为它们是线性函数，而现实世界中的问题通常需要非线性分类器进行准确预测。想象一下试图将一条直线拟合到一条曲线上！

那么当你过度承诺而未能兑现时会发生什么？你会失去资金。国防高级研究计划局，通常称为 DARPA（是的，那些人；建立了 ARPANET，后来成为互联网的人），资助了美国的许多最初项目。然而，近 20 年来缺乏结果越来越让这个机构感到沮丧。登上月球比获得可用的语音识别器更容易！

同样，在大西洋的另一边，1974 年发表了莱特希尔报告，其中说：“通用机器人是一个幻觉。”想象一下，作为 1974 年的英国人，看着计算机科学的权威在 BBC 上辩论人工智能是否是资源浪费。结果，人工智能研究在英国遭到破坏，随后在全世界范围内也受到影响，摧毁了许多职业生涯。这种对人工智能失去信心的阶段持续了大约两十年，被称为“人工智能寒冬”。如果当时能有尼德·史塔克在场警告他们就好了。

## 一线希望

即使在那些寒冷的日子里，这个领域也做了一些开创性的工作。当然，感知器——作为线性函数——具有有限的能力。如何解决这个问题？通过将它们链接在一个网络中，使得一个（或多个）感知器的输出是另一个（或多个）感知器的输入。换句话说，一个多层神经网络，如图 1-4 所示。层数越多，学习的非线性就越多，从而产生更好的预测。只有一个问题：如何训练它？乔治·希尔顿和他的朋友们登场了。他们在 1986 年的论文“通过反向传播错误学习表示”中发表了一种称为*反向传播*的技术。它是如何工作的？做出预测，看看预测与现实有多大偏差，然后将错误的大小传播回网络，以便学会修正它。重复这个过程，直到错误变得微不足道。一个简单而强大的概念。我们在本书中反复使用反向传播这个术语。

![一个多层神经网络的示例（图片来源）](img/00121.jpeg)

###### 图 1-4。一个多层神经网络的示例（[图片来源](https://oreil.ly/Jn-T6)）

1989 年，乔治·赛本科提供了*通用逼近定理*的第一个证明，该定理指出具有单个隐藏层的神经网络在理论上能够模拟任何问题。这是令人瞩目的，因为这意味着神经网络可以（至少在理论上）超越任何机器学习方法。它甚至可以模仿人类大脑。但所有这些都只是纸上谈兵。这个网络的规模很快就会在现实世界中限制。通过使用多个隐藏层并使用...等待...反向传播来训练网络，这个问题可以在一定程度上得到解决！

在更实际的一面，卡内基梅隆大学的一个团队于 1986 年建造了第一辆自主车辆 NavLab 1（图 1-5）。最初，它使用单层神经网络来控制方向盘的角度。这最终导致了 1995 年的 NavLab 5。在一次演示中，一辆汽车独自驾驶了从匹兹堡到圣迭戈的 2850 英里旅程中的所有路程，只剩下 50 英里。NavLab 在许多特斯拉工程师出生之前就获得了驾驶执照！

![1986 年的自主 NavLab 1 的全貌（图片来源）](img/00116.jpeg)

###### 图 1-5。1986 年的自主 NavLab 1 的全貌（[图片来源](https://oreil.ly/b2Bnn)）

上世纪 80 年代的另一个突出例子是美国邮政服务（USPS）。该服务需要根据邮政编码（邮政编码）自动对邮件进行分类。由于许多邮件一直是手写的，因此无法使用光学字符识别（OCR）。为了解决这个问题，杨立昆等人使用了来自国家标准技术研究所（NIST）的手写数据，以证明神经网络能够识别这些手写数字，他们在论文“应用于手写邮政编码识别的反向传播”中使用了这些手写数据。该机构的网络 LeNet 成为 USPS 几十年来用于自动扫描和分类邮件的工具。这是令人瞩目的，因为这是第一个真正在野外工作的卷积神经网络。最终，在 1990 年代，银行将使用该模型的演变版本 LeNet-5 来读取支票上的手写数字。这为现代计算机视觉奠定了基础。

###### 注意

那些已经了解 MNIST 数据集的人可能已经注意到我们刚刚提到的 NIST 的联系。这是因为 MNIST 数据集本质上包含了原始 NIST 数据集中的一部分图像，对它们进行了一些修改（“MNIST”中的“M”），以便为神经网络的训练和测试过程提供便利。修改包括将它们调整为 28 x 28 像素，将数字居中在该区域，抗锯齿等。

![MNIST 数据集中手写数字的样本](img/00196.jpeg)

###### 图 1-6。MNIST 数据集中手写数字的样本

一些人继续他们的研究，包括尤尔根·施密德胡伯，他提出了像长短期记忆（LSTM）这样的网络，具有文本和语音的有前途的应用。

在那时，尽管理论已经足够先进，但实际上无法证明结果。主要原因是当时的硬件计算成本太高，将它们扩展到更大的任务是一个挑战。即使通过某种奇迹硬件是可用的，要实现其全部潜力的数据肯定不容易获得。毕竟，互联网仍处于拨号阶段。支持向量机（SVM），一种用于分类问题的机器学习技术在 1995 年引入，速度更快，在较小量的数据上提供了相当不错的结果，因此已经成为常态。

因此，人工智能和深度学习的声誉很差。研究生被警告不要进行深度学习研究，因为这是“聪明的科学家的职业终结的领域”。在该领域工作的人和公司会使用替代词，如信息学、认知系统、智能代理、机器学习等，以与人工智能名称区分开来。这有点像美国战争部被重新品牌为国防部，以使人们更容易接受。

## 深度学习如何成为一种事物

幸运的是，2000 年代带来了高速互联网、智能手机摄像头、视频游戏以及像 Flickr 和知识共享这样的照片分享网站（带来了合法重用他人照片的能力）。大量的人们能够迅速用口袋里的设备拍照，然后立即上传。数据湖正在填满，逐渐有了足够的机会去尝试。这个快乐的交汇产生了 1400 万张图像的 ImageNet 数据集，并得益于（当时的普林斯顿大学）李飞飞等人的一些巨大工作。

在同一个十年里，个人电脑和游戏机游戏变得非常严肃。玩家们要求他们的视频游戏拥有越来越好的图形。这反过来促使了图形处理单元（GPU）制造商，如 NVIDIA 不断改进他们的硬件。这里要记住的关键一点是，GPU 在矩阵运算方面非常出色。为什么会这样？因为数学需要！在计算机图形学中，常见的任务，如移动对象、旋转对象、改变形状、调整光照等都使用矩阵运算。而 GPU 专门擅长执行这些任务。你知道还有什么需要大量矩阵计算吗？神经网络。这是一个巧合。

有了 ImageNet 准备好后，2010 年设立了年度 ImageNet 大规模视觉识别挑战赛（ILSVRC），公开挑战研究人员提出更好的分类这些数据的技术。提供了一个包含大约 120 万张图像的 1,000 个类别的子集，以推动研究的边界。像尺度不变特征变换（SIFT）+ SVM 这样的最新计算机视觉技术在 2010 年产生了 28%（2010 年）和 25%（2011 年）的前五错误率（即，如果按概率排名的前五个猜测之一匹配，则被认为是准确的）。

然后到了 2012 年，排行榜上的一个条目将错误率几乎减少到 16%。来自多伦多大学的 Alex Krizhevsky，Ilya Sutskever（最终创立了 OpenAI）和 Geoffrey Hinton 提交了该条目。AlexNet 恰如其名，受到 LeNet-5 的启发。即使只有八层，AlexNet 拥有庞大的 6000 万参数和 65 万个神经元，导致一个 240 MB 的模型。它使用两个 NVIDIA GPU 在一周内训练。这一事件让所有人都感到惊讶，证明了 CNN 的潜力，这导致了现代深度学习时代的发展。

图 1-7 量化了 CNN 在过去十年中取得的进展。自 2012 年深度学习出现以来，我们看到 ImageNet LSVRC 获奖作品的分类错误率每年减少 40%。随着 CNN 变得更深，错误率继续下降。

![ImageNet LSVRC 获奖作品的演变](img/00264.jpeg)

###### 图 1-7\. ImageNet LSVRC 获奖作品的演变

请记住，我们正在大大简化 AI 的历史，并且肯定忽略了一些细节。基本上，这是数据、GPU 和更好技术的融合导致了这个现代深度学习时代。进展不断扩展到新的领域。正如表 1-1 所强调的，科幻小说中的东西已经成为现实。

表 1-1\. 现代深度学习时代的精华片段

| 2012 | 来自 Google Brain 团队的神经网络开始观看 YouTube 视频后识别猫 |
| --- | --- |
| 2013 |

+   研究人员开始在各种任务上尝试深度学习

+   word2vec 为单词和短语带来上下文，使理解含义更接近一步

+   语音识别的错误率下降了 25%

|

| 2014 |
| --- |

+   GANs 被发明

+   Skype 实时翻译语音

+   聊天机器人 Eugene Goostman 通过图灵测试

+   使用神经网络进行序列到序列学习

+   图像字幕将图像翻译成句子

|

| 2015 |
| --- |

+   微软 ResNet 在图像准确性上击败人类，训练 1000 层网络

+   百度的 Deep Speech 2 进行端到端语音识别

+   Gmail 推出智能回复

+   YOLO（You Only Look Once）实时进行目标检测

+   视觉问答允许根据图像提出问题

|

| 2016 |
| --- |

+   AlphaGo 击败专业人类围棋选手

+   Google WaveNets 帮助生成逼真的音频

+   微软在对话语音识别中实现了与人类的平等

|

| 2017 |
| --- |

+   AlphaGo Zero 在 3 天内学会自己下围棋

+   胶囊网络修复 CNN 中的缺陷

+   引入张量处理单元（TPUs）

+   加利福尼亚州允许出售自动驾驶汽车

+   Pix2Pix 允许从草图生成图像

|

| 2018 |
| --- |

+   AI 设计比人类更好的 AI，使用神经结构搜索

+   Google Duplex 演示代表我们预订餐厅

+   Deepfakes 在视频中交换一个面孔

+   Google 的 BERT 在语言理解任务中成功超越人类

+   DawnBench 和 MLPerf 建立用于基准测试 AI 训练

|

| 2019 |
| --- |

+   OpenAI Five 击败 Dota2 世界冠军

+   StyleGan 生成逼真的图像

+   OpenAI GPT-2 生成逼真的文本段落

+   富士通在 75 秒内训练 ImageNet

+   微软向 OpenAI 投资 10 亿美元

+   艾伦研究所的 AI 通过 12 年级科学考试，得分 80%

|

希望现在您对 AI 和深度学习有了历史背景，并了解为什么这一时刻如此重要。重要的是要认识到在这一领域进展迅速的速度。但正如我们迄今所见，情况并非总是如此。

根据该领域的两位先驱，实现真实世界的计算机视觉的最初估计是在 20 世纪 60 年代的“一个夏天”。他们只差了半个世纪！成为未来学家并不容易。亚历山大·维斯纳-格罗斯的一项研究发现，算法提出和取得突破之间的平均时间约为 18 年。另一方面，数据集提供和帮助实现突破之间的平均时间仅为三年！看看过去十年的任何突破。实现该突破的数据集很可能在几年前才被提供。

数据显然是限制因素。这显示了一个好数据集对深度学习可以发挥的关键作用。然而，数据并不是唯一的因素。让我们看看构成完美深度学习解决方案基础的其他支柱。

# 完美深度学习解决方案的配方

在 Gordon Ramsay 开始烹饪之前，他确保所有的配料都准备就绪。解决问题时使用深度学习也是如此（图 1-8）。

![完美深度学习解决方案的成分](img/00098.jpeg)

###### 图 1-8。完美深度学习解决方案的成分

这就是你的深度学习*准备好了！

```py
*Dataset + Model + Framework + Hardware = Deep Learning Solution*
```

让我们更详细地看看这些内容。

## 数据集

就像吃豆子的吃豆人渴望数据一样，深度学习也渴望数据——大量的数据。它需要这么多数据来发现有意义的模式，从而帮助做出稳健的预测。传统机器学习在 20 世纪 80 年代和 90 年代是主流，因为即使只有几百到几千个示例，它也可以正常运行。相比之下，从头开始构建的深度神经网络（DNNs）通常需要更多的数据来完成典型的预测任务。这里的好处是预测更准确。

在本世纪，我们每天都会产生数量巨大的数据——图像、文本、视频、传感器数据等等，但要有效利用这些数据，我们需要标签。要构建一个情感分类器，以了解亚马逊评论是积极的还是消极的，我们需要成千上万的句子，并为每个句子分配一个情感。要训练一个用于 Snapchat 镜头的面部分割系统，我们需要在成千上万的图像上准确标记眼睛、嘴唇、鼻子等位置。要训练一辆自动驾驶汽车，我们需要用人类驾驶员对控制设备的反应标记视频片段，例如刹车、油门、方向盘等。这些标签对我们的人工智能起着教师的作用，比仅有未标记数据更有价值。

获取标签可能会很昂贵。难怪有成千上万的工人在众包标注任务周围形成了一个整个行业。每个标签的成本可能从几分钱到几美元不等，取决于工人分配标签所花费的时间。例如，在开发微软 COCO（上下文中的常见对象）数据集期间，大约需要三秒来标记图像中每个对象的名称，大约需要 30 秒来在每个对象周围放置一个边界框，以及 79 秒来为每个对象绘制轮廓。重复这个过程数十万次，你就能开始估算一些更大数据集的成本。一些标注公司，如 Appen 和 Scale AI，已经价值超过十亿美元。

我们的银行账户可能没有一百万美元。但幸运的是，在这次深度学习革命中发生了两件好事：

+   大型标记数据集已经被主要公司和大学慷慨地公开。

+   一种称为 *迁移学习* 的技术，允许我们将模型调整到具有数百个示例的数据集上，只要我们的模型最初是在类似于我们当前集合的更大数据集上进行训练的。我们在本书中反复使用这种技术，包括在第五章中，我们进行实验并证明即使有几十个示例，也可以通过这种技术获得良好的性能。迁移学习打破了训练良好模型需要大数据的神话。欢迎来到 *微小数据* 的世界！

表格 1-2 展示了当今一些流行的数据集，用于各种深度学习任务。

表格 1-2. 多样的公共数据集

| **数据类型** | **名称** | **详情** |
| --- | --- | --- |
| 图像 | 开放图像 V4（来自谷歌） |

+   19700 个类别中的九百万张图像

+   拥有 600 个类别的 174 万张图像（带有边界框）

|

|   | Microsoft COCO |
| --- | --- |

+   拥有 80 个对象类别的 33 万张图像

+   包含边界框、分割和每张图像五个标题

|

| 视频 | YouTube-8M |
| --- | --- |

+   610 万个视频，3862 个类别，26 亿个视听特征

+   3.0 标签/视频

+   1.53 TB 的随机抽样视频

|

| 视频、图像 | BDD100K（来自加州大学伯克利分校） |
| --- | --- |

+   超过 1100 小时的 10 万个驾驶视频

+   拥有 10 个类别的边界框的 10 万张图像

+   拥有车道标记的 10 万张图像

+   拥有可驾驶区域分割的 10 万张图像

+   拥有像素级实例分割的 1 万张图像

|

| Waymo 开放数据集 | 总计 3,000 个驾驶场景，16.7 小时的视频数据，60 万帧，约 2500 万个 3D 边界框和 2200 万个 2D 边界框 |
| --- | --- |
| 文本 | SQuAD | 来自维基百科的 15 万个问题和答案片段 |
|   | Yelp 评论 | 五百万条 Yelp 评论 |
| 卫星数据 | Landsat 数据 | 数百万卫星图像（100 海里宽度和高度），以及八个光谱波段（15 到 60 米的空间分辨率） |
| 音频 | Google AudioSet | 来自 YouTube 的 2,084,320 个 10 秒音频片段，包含 632 个类别 |
| LibriSpeech | 1000 小时的英语朗读语音 |

## 模型架构

在高层次上，模型只是一个函数。它接受一个或多个输入并给出一个输出。输入可以是文本、图像、音频、视频等形式。输出是一个预测。一个好的模型是那些预测可靠地匹配预期现实的模型。模型在数据集上的准确性是决定其是否适用于实际应用的主要因素。对于许多人来说，这就是他们真正需要了解的关于深度学习模型的全部。但当我们窥探模型的内部工作时，它变得真正有趣（图 1-9）。

![一个深度学习模型的黑盒视图](img/00077.jpeg)

###### 图 1-9. 一个深度学习模型的黑盒视图

模型内部是一个由节点和边组成的图。节点代表数学运算，而边代表数据如何从一个节点流向另一个节点。换句话说，如果一个节点的输出可以成为一个或多个节点的输入，那么这些节点之间的连接由边表示。这个图的结构决定了准确性的潜力、速度、它消耗的资源（内存、计算和能量）以及它能够处理的输入类型。

节点和边的布局被称为模型的*架构*。本质上，它是一个蓝图。现在，蓝图只是一部分。我们仍然需要实际的建筑。训练是利用这个蓝图来构建那座建筑的过程。我们通过反复进行以下步骤来训练模型：1）输入数据，2）从中获取输出，3）监视这些预测与预期现实（即与数据相关联的标签）之间的差距，然后，4）将错误的大小传播回模型，以便它逐渐学会自我纠正。这个训练过程是迭代进行的，直到我们对预测的准确性感到满意。

这种训练的结果是一组数字（也称为权重），分配给每个节点。这些权重是图中的节点在给定的输入上操作所必需的参数。在训练开始之前，我们通常将随机数分配为权重。训练过程的目标基本上是逐渐调整每组这些权重的值，直到它们与相应的节点一起产生令人满意的预测。

为了更好地理解权重，让我们来看下面的数据集，其中有两个输入和一个输出：

表 1-3.示例数据集

| **input[1]** | **input[2]** | **output** |
| --- | --- | --- |
| --- | --- | --- |
| 1 | 6 | 20 |
| 2 | 5 | 19 |
| 3 | 4 | 18 |
| 4 | 3 | 17 |
| 5 | 2 | 16 |
| 6 | 1 | 15 |

使用线性代数（或我们头脑中的猜测），我们可以推断控制这个数据集的方程是：

*output = f(input[1], input[2])* = 2 x *input[1]* + 3 x *input[2]*

在这种情况下，这个数学运算的权重是 2 和 3。一个深度神经网络有数百万个这样的权重参数。

根据使用的节点类型不同，不同主题的模型架构将更适合不同类型的输入数据。例如，CNNs 用于图像和音频，而循环神经网络（RNNs）和 LSTM 通常用于文本处理。

一般来说，从头开始训练这些模型可能需要相当长的时间，可能需要几周。幸运的是，许多研究人员已经完成了在通用数据集（如 ImageNet）上训练它们的艰苦工作，并使它们可供所有人使用。更好的是，我们可以拿这些可用的模型并将它们调整到我们的特定数据集。这个过程称为迁移学习，占了从业者绝大多数需求。

与从头开始训练相比，迁移学习提供了双重优势：显著减少的训练时间（几分钟到几小时，而不是几周），并且可以使用大大较小的数据集（数百到数千个数据样本，而不是数百万个）。表 1-4 显示了一些著名的模型架构示例。

表 1-4.多年来的示例模型架构

| **任务** | **示例模型架构** |
| --- | --- |
| --- | --- |
| 图像分类 | ResNet-152（2015 年），MobileNet（2017 年） |
| 文本分类 | BERT（2018 年），XLNet（2019 年） |
| 图像分割 | U-Net（2015 年），DeepLabV3（2018 年） |
| 图像翻译 | Pix2Pix（2017 年） |
| 目标检测 | YOLO9000（2016 年），Mask R-CNN（2017 年） |
| 语音生成 | WaveNet（2016 年） |

表 1-4 中的每个模型都在参考数据集（例如，分类的 ImageNet，检测的 MS COCO）上有一个已发布的准确度指标。此外，这些架构有它们自己的特征资源需求（以兆字节为单位的模型大小，以浮点运算为单位的计算需求，或 FLOPS）。

我们将在接下来的章节深入探讨迁移学习。现在，让我们看看我们可以使用的深度学习框架和服务。

###### 注

当 Kaiming He 等人在 2015 年提出了 152 层的 ResNet 架构时——考虑到之前最大的 GoogLeNet 模型由 22 层组成，这是当时的壮举——每个人心中只有一个问题：“为什么不是 153 层？”原来，原因是 Kaiming 的 GPU 内存用完了！

## 框架

有几个深度学习库可以帮助我们训练模型。此外，还有专门用于使用这些训练模型进行预测（或*推理*）的框架，优化应用程序所在的位置。

从历史上看，就像通常的软件一样，许多库已经出现并消失了——Torch（2002 年）、Theano（2007 年）、Caffe（2013 年）、Microsoft Cognitive Toolkit（2015 年）、Caffe2（2017 年）——并且这个领域一直在迅速发展。从每个库中学到的东西使其他库更容易上手，引起了兴趣，并提高了初学者和专家的生产力。表 1-5 看一些流行的框架。

表 1-5. 流行的深度学习框架

| **框架** | **最适用于** | **典型目标平台** |
| --- | --- | --- |
| TensorFlow（包括 Keras） | 训练 | 台式机、服务器 |
| PyTorch | 训练 | 台式机、服务器 |
| MXNet | 训练 | 台式机、服务器 |
| TensorFlow Serving | 推理 | 服务器 |
| TensorFlow Lite | 推理 | 移动和嵌入式设备 |
| TensorFlow.js | 推理 | 浏览器 |
| ml5.js | 推理 | 浏览器 |
| Core ML | 推理 | 苹果设备 |
| Xnor AI2GO | 推理 | 嵌入式设备 |

### TensorFlow

2011 年，Google Brain 开发了用于内部研究和工程的 DNN 库 DistBelief。它帮助训练了 Inception（2014 年 ImageNet 大规模视觉识别挑战的获奖作品），并帮助提高了 Google 产品中语音识别的质量。与 Google 的基础设施紧密联系，它不容易配置和与外部机器学习爱好者共享代码。意识到这些限制，Google 开始研发第二代分布式机器学习框架，承诺是通用、可扩展、高性能且可移植到许多硬件平台。而且最重要的是，它是开源的。Google 称之为 TensorFlow，并于 2015 年 11 月宣布发布。

TensorFlow 实现了许多前述承诺，从开发到部署形成了一个端到端的生态系统，并在这个过程中获得了大量的追随者。在 GitHub 上拥有超过 10 万颗星星，显示出没有停止的迹象。然而，随着采用的增加，该库的用户 rightly 批评它使用起来不够简单。正如笑话所说，TensorFlow 是由 Google 工程师制作的库，为 Google 工程师制作的库，如果你足够聪明使用 TensorFlow，你就足够聪明被雇佣在那里。

但 Google 并不孤单。说实话，即使到 2015 年，使用深度学习库仍然是一种令人不愉快的经历。甚至忘记使用这些库，安装其中一些框架就让人想拔头发。（Caffe 的用户们，这是不是让你们想起了什么？）

### Keras

作为深度学习从业者面临困难的答案，François Chollet 于 2015 年 3 月发布了开源框架 Keras，自那以后世界就变了。这个解决方案突然使深度学习对初学者变得可访问。Keras 提供了一个直观且易于使用的编码界面，然后使用其他深度学习库作为后端计算框架。从其第一个后端 Theano 开始，Keras 鼓励快速原型设计并减少代码行数。最终，这种抽象扩展到其他框架，包括 Cognitive Toolkit、MXNet、PlaidML，以及 TensorFlow。

### PyTorch

同时，PyTorch 在 2016 年初在 Facebook 开始，工程师们有幸观察到 TensorFlow 的局限性。PyTorch 从一开始就支持本地 Python 构造和 Python 调试，使其灵活且易于使用，很快成为 AI 研究人员的最爱。它是第二大端到端深度学习系统。Facebook 另外构建了 Caffe2，用于将 PyTorch 模型部署到生产环境，为超过十亿用户提供服务。PyTorch 推动了研究，而 Caffe2 主要用于生产。2018 年，Caffe2 被吸收到 PyTorch 中，形成一个完整的框架。

### 一个不断发展的领域

如果这个故事以 Keras 和 PyTorch 的便利结束，这本书的副标题就不会有“TensorFlow”这个词了。TensorFlow 团队意识到，如果真的想要扩大工具的影响力并使 AI 民主化，就需要让工具更容易使用。因此，当 Keras 正式作为 TensorFlow 的一部分包含在内时，这是一个好消息，提供了两全其美的选择。这使开发人员可以使用 Keras 定义模型和训练模型，使用核心 TensorFlow 进行高性能数据管道，包括分布式训练和部署生态系统。这是天作之合！最重要的是，TensorFlow 2.0（2019 年发布）包括对本地 Python 构造和急切执行的支持，正如我们在 PyTorch 中看到的那样。

有了这么多竞争框架可用，可移植性的问题不可避免地出现。想象一下，一篇新的研究论文以 PyTorch 公开发布的最先进模型。如果我们不在 PyTorch 中工作，我们将无法参与研究，必须重新实现和训练。开发人员喜欢能够自由共享模型，而不受限于特定的生态系统。许多开发人员自然地编写了库，将模型格式从一个库转换为另一个库。这是一个简单的解决方案，但由于转换工具的数量庞大，缺乏官方支持和足够的质量，导致了组合爆炸。为了解决这个问题，微软和 Facebook 等行业主要参与者发起了开放神经网络交换（ONNX）。ONNX 提供了一个通用模型格式的规范，可以被多个流行库官方读写。此外，它为不支持此格式的库提供了转换器。这使开发人员可以在一个框架中进行训练，然后在另一个框架中进行推断。

除了这些框架外，还有几个图形用户界面（GUI）系统，可以实现无代码训练。使用迁移学习，它们可以快速生成多种格式的训练模型，用于推断。即使是您的祖母也可以通过点按界面快速训练神经网络！

表 1-6.流行的基于 GUI 的模型训练工具

| **服务** | **平台** |
| --- | --- |
| 微软 CustomVision.AI | 基于 Web |
| Google AutoML | 基于 Web |
| Clarifai | 基于 Web |
| IBM 视觉识别 | 基于 Web |
| 苹果 Create ML | macOS |
| NVIDIA DIGITS | 桌面 |
| Runway ML | 桌面 |

那么为什么我们选择 TensorFlow 和 Keras 作为本书的主要框架？考虑到可用的材料数量，包括文档、Stack Overflow 答案、在线课程、庞大的贡献者社区、平台和设备支持、行业采用以及可用的工作岗位（在美国，与 PyTorch 相比，大约有三倍的 TensorFlow 相关角色），当涉及到框架时，TensorFlow 和 Keras 目前主导着这个领域。对我们来说选择这种组合是有道理的。也就是说，本书讨论的技术也适用于其他库。学习一个新的框架不应该花费太长时间。因此，如果您真的想要加入一个专门使用 PyTorch 的公司，不要犹豫去申请。

## 硬件

1848 年，当詹姆斯·W·马歇尔在加利福尼亚州发现金矿时，这一消息迅速传遍美国。成千上万的人涌向该州开始挖掘财富。这被称为*加利福尼亚淘金热*。早期行动者能够挖掘出一大笔财富，但后来者则没有那么幸运。但淘金热多年来并未停止。你能猜到在这段时间内谁赚了最多钱吗？铲子制造商！

云和硬件公司是 21 世纪的铲子制造商。不相信？看看过去十年微软和英伟达的股票表现。1849 年和现在唯一的区别是我们可供选择的铲子数量之多令人难以置信。

鉴于可用的硬件种类繁多，重要的是根据应用程序的资源、延迟、预算、隐私和法律要求来做出正确的选择。

根据您的应用程序与用户的交互方式，推理阶段通常有用户在另一端等待响应。这对可以使用的硬件类型以及硬件的位置施加了限制。例如，由于网络延迟问题，Snapchat 镜头无法在云端运行。此外，它需要在接近实时的情况下运行，以提供良好的用户体验（UX），因此对每秒处理的帧数设置了最低要求（通常>15 fps）。另一方面，上传到图像库（如 Google 照片）的照片不需要立即进行图像分类。几秒钟或几分钟的延迟是可以接受的。

走向另一个极端，培训需要更多的时间；从几分钟到几小时到几天不等。根据我们的培训场景，更好的硬件的真正价值在于能够加快实验速度和增加迭代次数。对于比基本神经网络更严肃的事情，更好的硬件可以产生巨大的差异。通常情况下，与 CPU 相比，GPU 的速度会提高 10 到 15 倍，并且性能每瓦更高，将实验完成的等待时间从一周减少到几个小时。这可能是观看大峡谷纪录片（两小时）与实际前往大峡谷旅行（四天）之间的区别。

以下是几个基本的硬件类别可供选择以及它们通常的特征（另请参见图 1-10）：

中央处理器（CPU）

便宜、灵活、慢。例如，英特尔酷睿 i9-9900K。

图形处理器（GPU）

高吞吐量，非常适合批处理以利用并行处理，价格昂贵。例如，英伟达 GeForce RTX 2080 Ti。

现场可编程门阵列（FPGA）

快速、低功耗、可重新编程以适应定制解决方案，价格昂贵。知名公司包括赛灵思、Lattice Semiconductor、Altera（英特尔）。由于能够在几秒钟内运行并可配置到任何 AI 模型，微软必应将大部分 AI 运行在 FPGA 上。

专用集成电路（ASIC）

定制芯片。设计成本极高，但在大规模生产时成本低廉。就像在制药行业一样，第一件物品的成本最高，因为设计和制造所需的研发工作。大规模生产相对廉价。具体示例包括以下内容：

张量处理单元（TPU）

专门用于神经网络操作的 ASIC，仅在 Google Cloud 上提供。

边缘 TPU

小于美国一分硬币大小，加速边缘推理。

神经处理单元（NPU）

通常由智能手机制造商使用，这是一种专用芯片，用于加速神经网络推理。

![相对于灵活性、性能和成本的不同类型硬件的比较](img/00092.jpeg)

###### 图 1-10。相对于灵活性、性能和成本的不同类型硬件的比较

让我们看看每种情况下会使用哪些方案：

+   开始训练→CPU

+   训练大型网络→GPU 和 TPU

+   智能手机推断→移动 CPU、GPU、数字信号处理器（DSP）、NPU

+   可穿戴设备（例如智能眼镜、智能手表）→Edge TPU、NPU

+   嵌入式 AI 项目（例如洪水调查无人机、自动轮椅）→加速器如谷歌 Coral、英特尔 Movidius 与树莓派，或像 NVIDIA Jetson Nano 这样的 GPU，一直到用于智能音箱唤醒词检测的 15 美元微控制器（MCU）

随着我们阅读本书，我们将仔细探讨其中许多内容。

# 负责任的 AI

到目前为止，我们已经探讨了 AI 的力量和潜力。它展现了极大的潜力，可以增强我们的能力，使我们更加高效，赋予我们超能力。

但伟大的力量伴随着伟大的责任。

尽管 AI 可以帮助人类，但当设计得不经过深思熟虑时（无论是有意还是无意），它也有同等的潜力伤害我们。AI 本身不应受责备；而是 AI 的设计者。

考虑一些过去几年在新闻中曝光的真实事件。

+   “____ 据称可以通过分析你的面部判断你是否是恐怖分子”（图 1-11）：《计算机世界》，2016 年

+   “AI 正在错误地将人们送进监狱”：《麻省理工科技评论》，2019 年

+   “____ 超级计算机推荐‘不安全和不正确’的癌症治疗方法，内部文件显示”：《STAT 新闻》，2018 年

+   “____ 开发了一个用于招聘人才的 AI 工具，但不得不关闭它，因为它歧视女性”：《商业内幕》，2018 年

+   “____AI 研究：主要物体识别系统偏向于拥有更多金钱的人”：《风险投资者》，2019 年

+   “____ 将黑人标记为‘大猩猩’”：《今日美国》，2015 年。“两年后，____ 通过从图像分类器中清除‘大猩猩’标签解决了‘种族主义算法’问题”：《Boing Boing》，2018 年

+   “____ 在 Twitter 用户教会它种族主义后，让其新的 AI 机器人 Tay 保持沉默”：《科技博客》，2016 年

+   “AI 误将公交车侧面广告误认为著名 CEO，指控其违章横穿马路”：《财新全球》，2018 年

+   “____ 因员工反对‘战争生意’而决定放弃五角大楼的 AI 合同”：《华盛顿邮报》，2018 年

+   “自动驾驶 ____ 致命车辆‘在撞倒并杀死行人前六秒发现了她’”：《太阳报》，2018 年

![声称根据面部结构对人进行分类的初创公司](img/00324.jpeg)

###### 图 1-11。声称根据面部结构对人进行分类的初创公司

你能在这里填上空白吗？我们给你一些选项——亚马逊、微软、谷歌、IBM 和优步。继续填写。我们会等待。

我们将它们留空是有原因的。这是为了认识到这不是属于特定个人或公司的问题。这是每个人的问题。尽管这些事情发生在过去，可能不反映当前状态，但我们可以从中学习，尽量避免犯同样的错误。这里的一线希望是每个人都从这些错误中学到了东西。

作为 AI 的开发者、设计师、架构师和领导者，我们有责任超越表面上的技术问题。以下是一些与我们解决的任何问题（无论是 AI 还是其他问题）相关的主题。它们不应该被忽视。

## 偏见

在我们日常工作中，我们常常带入自己的偏见，有意或无意。这是多种因素的结果，包括我们的环境、成长背景、文化规范，甚至我们固有的天性。毕竟，AI 和驱动它们的数据集并非在真空中创造出来的——它们是由带有自己偏见的人类创造的。计算机不会自己神奇地产生偏见，它们反映和放大现有的偏见。

以 YouTube 应用程序早期的一个例子为例，开发人员注意到大约 10%的上传视频是颠倒的。也许如果这个数字更低，比如 1%，可能会被视为用户错误。但 10%的数字太高了，不能被忽视。你知道谁恰好占人口的 10%吗？左撇子！这些用户将手机保持在与右撇子同向相反的方向。但 YouTube 的工程师在开发和测试移动应用程序时没有考虑到这种情况，因此 YouTube 将上传的视频以相同的方向上传到服务器，供左撇子和右撇子用户使用。

如果开发人员团队中有一个左撇子，这个问题本可以更早被发现。这个简单的例子展示了多样性的重要性。左右利手只是定义个体的一个小属性。许多其他因素，通常超出他们的控制，经常起作用。诸如性别、肤色、经济地位、残疾、出生国家、语音模式，甚至像头发长度这样微不足道的事情，都可能决定某人的改变生活结果，包括算法如何对待他们。

谷歌的[机器学习词汇表](https://oreil.ly/ySfNv)列出了可能影响机器学习流程的几种偏见形式。以下只是其中一些：

选择偏见

数据集不代表真实世界问题的分布，并偏向于某些类别的子集。例如，在许多虚拟助手和智能家居扬声器中，一些口音被过度代表，而其他口音在训练数据集中根本没有数据，导致世界大部分人口的用户体验不佳。

选择偏见也可能是因为概念的共同出现。例如，当使用谷歌翻译将句子“她是医生。他是护士”翻译成土耳其等性别中立语言，然后再翻译回来时，性别会发生变化，如图 1-12 所示。这可能是因为数据集包含了大量的男性代词和“医生”一词的共同出现，以及女性代词和“护士”一词的共同出现。

![谷歌翻译反映了数据中的潜在偏见（截至 2019 年 9 月）](img/00013.jpeg)

###### 图 1-12。谷歌翻译反映了数据中的潜在偏见（截至 2019 年 9 月）

隐性偏见

这种偏见是因为我们在看到某事时都会做出的隐性假设。考虑图 1-13 中的突出部分。任何看到它的人都可能非常肯定地认为那些条纹属于斑马。事实上，考虑到 ImageNet 训练的网络对纹理有多么偏向，大多数网络都会将整个图像分类为斑马。除了我们知道这张图片是用斑马样式的织物装饰的沙发。

![Glen Edelson 的斑马沙发（图片来源）](img/00134.jpeg)

###### 图 1-13。Glen Edelson 的斑马沙发（[图片来源](https://oreil.ly/Xg4MP)）

报告偏见

有时，房间里最响亮的声音是最极端的声音，并主导了对话。看一眼 Twitter 可能会让人觉得世界正在末日，而大多数人都在忙于过着平凡的生活。不幸的是，无聊是卖不动的。

内外群体偏见

来自东亚的注释员可能会看到自由女神像的照片，并给予“美国”或“美利坚合众国”等标签，而来自美国的人可能会看同一张照片，并分配更细致的标签，如“纽约”或“自由岛”。人类天性是看待自己的群体时会有细微差别，而看待其他群体时会更加同质化，这也反映在我们的数据集中。

## 问责和可解释性

想象一下，在 19 世纪末，卡尔·本茨先生告诉你，他发明了这种四轮设备，可以比任何其他东西更快地将你运送。除了他不知道它是如何工作的。他只知道它消耗了一种高度易燃的液体，在内部爆炸了几次以推动它前进。是什么让它移动？是什么让它停下来？是什么阻止它燃烧坐在里面的人？他没有答案。如果这是汽车的起源故事，你可能不想进入那个装置。

这正是目前人工智能正在发生的事情。以前，传统机器学习中，数据科学家必须手动从数据中选择特征（预测变量），然后机器学习模型会学习这些特征。尽管这种手动选择过程虽然繁琐和受限，但给了他们更多的控制和洞察力，以了解预测是如何产生的。然而，使用深度学习，这些特征是自动选择的。数据科学家可以通过提供大量数据来构建模型，这些模型在大多数情况下会可靠地进行预测。但数据科学家并不知道模型是如何工作的，它学习了哪些特征，在什么情况下模型有效，更重要的是，在什么情况下模型无法工作。当 Netflix 基于我们已经观看的内容向我们推荐电视节目时，这种方法可能是可以接受的（尽管我们相当肯定他们的代码中的某处有`recommendations.append("Stranger Things")`）。但是如今人工智能所做的远不止推荐电影。警察和司法系统开始依赖算法来决定某人是否对社会构成风险，以及他们是否应该在审判前被拘留。许多人的生命和自由岌岌可危。我们绝对不能将重要的决策外包给一个不负责任的黑匣子。幸运的是，有改变的势头，投资于*可解释人工智能*，其中模型不仅能够提供预测，还能解释导致其做出某种预测的因素，并揭示限制的领域。

此外，一些城市（如纽约）开始让他们的算法对公众负责，承认公众有权知道他们用于重要决策的算法以及它们的工作原理，允许专家进行审查和审计，提高政府机构的专业知识以更好地评估他们添加的每个系统，并提供争议决定的机制，这些决定是由算法做出的。

## 可重复性

科学领域的研究只有在可重复时才会得到社区的广泛认可；也就是说，任何研究者都应该能够复制测试条件并获得相同的结果。除非我们能够重现模型的过去结果，否则在将来使用它时无法追究责任。在没有可重复性的情况下，研究容易受到*p-hacking*的影响——调整实验参数直到获得期望的结果。研究人员有必要广泛记录他们的实验条件，包括数据集、基准和算法，并在进行实验之前声明他们将要测试的假设。对机构的信任达到历史最低点，而那些不基于现实但被媒体夸大的研究可能会进一步侵蚀这种信任。传统上，复制研究论文被认为是一种黑暗的艺术，因为许多实现细节被省略。令人振奋的消息是，研究人员现在逐渐开始使用公开可用的基准（而不是他们私下构建的数据集）并开源他们用于研究的代码。社区成员可以借助这些代码，证明它有效，并使其更好，从而迅速导致新的创新。

## 稳健性

有一个关于对 CNN 进行一像素攻击的研究领域。基本上，目标是找到并修改图像中的一个像素，使 CNN 预测完全不同的东西。例如，在一个苹果图片中改变一个像素可能导致 CNN 将其分类为狗。许多其他因素可以影响预测，如噪音、光照条件、摄像头角度等，这些因素不会影响人类做出类似判断的能力。这对于自动驾驶汽车尤为重要，因为街上的坏人可能会修改汽车看到的输入，以操纵它做坏事。事实上，腾讯的 Keen Security Lab 能够利用特斯拉 AutoPilot 的漏洞，通过在路上策略性地放置小贴纸，导致汽车变道并驶入对向车道。如果我们要信任 AI，那么必须具备能够抵御噪音、轻微偏差和故意操纵的强大 AI。

## 隐私

在追求构建更好的人工智能的过程中，企业需要收集大量数据。不幸的是，有时它们会越过界限，过度热衷于收集超出当前任务所需的信息。一个企业可能认为它只是为了善良目的使用收集的数据。但如果被一家对数据使用没有相同道德底线的公司收购呢？消费者的信息可能被用于超出最初目标的目的。此外，所有这些数据集中在一个地方使其成为黑客的目标，他们窃取个人信息并将其出售给犯罪企业。此外，政府已经在试图追踪每个个人时越界。

所有这些都与普遍认可的隐私人权相悖。消费者希望能够透明地了解有关他们的数据收集情况，谁可以访问这些数据，数据如何被使用，以及退出数据收集过程的机制，以及删除已经收集的数据。

作为开发者，我们希望意识到我们正在收集的所有数据，并问自己是否有必要收集某个数据。为了最小化我们收集的数据，我们可以实施隐私感知的机器学习技术，如联邦学习（在谷歌键盘中使用），这使我们能够在用户设备上训练网络，而无需将任何个人身份信息发送到服务器。

事实证明，在本节开头提到的许多标题中，是糟糕的公关后果引起了这些话题的主流意识，引入了问责制，并导致了整个行业思维的转变，以防止未来的重复发生。我们必须继续要求自己、学者、行业领袖和政客在每一次失误时负责，并迅速采取措施纠正错误。我们做出的每个决定和采取的每个行动都有可能为未来几十年树立先例。随着人工智能的普及，我们需要共同努力提出艰难的问题，并为这些问题找到答案，以便在最大程度上减少潜在的危害。

# 总结

本章探讨了令人兴奋的人工智能和深度学习世界的格局。我们追溯了人工智能的时间线，从其谦逊的起源，伟大的希望时期，到黑暗的人工智能冬季，再到如今的复苏。在这个过程中，我们回答了为什么这一次不同的问题。然后，我们看了构建深度学习解决方案所需的必要要素，包括数据集、模型架构、框架和硬件。这为我们在接下来的章节中进一步探索做好了准备。希望您喜欢本书的其余部分。现在是深入研究的时候了！

# 常见问题

1.  我刚刚开始。我需要花很多钱购买强大的硬件吗？

    幸运的是，您甚至可以通过网络浏览器开始。我们所有的脚本都可以在线获取，并且可以在 Google Colab 提供的免费 GPU 上运行（感谢慷慨的 Google Colab 团队，他们免费提供强大的 GPU（每次最多 12 小时）。这应该让您开始。随着您通过执行更多实验（尤其是在专业环境或大型数据集上）变得更加熟练，您可能希望通过在云上租用（Microsoft Azure、Amazon Web Services（AWS）、Google Cloud Platform（GCP）等）或购买硬件来获得 GPU。不过要注意那些电费账单！

    ![在 Colab 中运行的 GitHub 笔记本的屏幕截图](img/00278.jpeg)

    ###### 图 1-14。在 Colab 中运行的 GitHub 笔记本的屏幕截图

1.  Colab 很棒，但我已经有一台为玩<插入视频游戏名称>而购买的强大计算机。我应该如何设置我的环境？

    理想的设置涉及 Linux，但 Windows 和 macOS 也可以。对于大多数章节，您需要以下内容：

    +   Python 3 和 PIP

    +   `tensorflow`或`tensorflow-gpu` PIP 包（版本 2 或更高）

    +   Pillow

        我们喜欢保持事情整洁和自包含，因此我们建议使用 Python 虚拟环境。每当您安装软件包或运行脚本或笔记本时，都应该使用虚拟环境。

        如果您没有 GPU，您的设置就完成了。

        如果您有 NVIDIA GPU，您需要安装适当的驱动程序，然后安装 CUDA，然后安装 cuDNN，然后安装`tensorflow-gpu`软件包。如果您使用 Ubuntu，有一个比手动安装这些软件包更简单的解决方案，即使用[Lambda Stack](https://oreil.ly/93oon)一行代码安装整个环境。

        或者，您可以使用 Anaconda Distribution 安装所有软件包，这对 Windows、Mac 和 Linux 同样有效。

1.  我在哪里可以找到本书中使用的代码？

    您可以在[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)找到可直接运行的示例。

1.  阅读本书的最低先决条件是什么？

    博士学位包括微积分、统计分析、变分自动编码器、运筹学等领域...绝对*不*是必需的，才能阅读这本书（让您有点紧张，是吧？）。一些基本的编码技能、熟悉 Python、健康的好奇心和幽默感应该会在吸收材料的过程中大有裨益。尽管初级水平的移动开发理解（使用 Swift 和/或 Kotlin）会有所帮助，但我们设计的示例是自给自足且足够简单，可以由以前从未编写过移动应用的人部署。

1.  我们将使用哪些框架？

    Keras + TensorFlow 用于训练。逐章探讨不同的推理框架。

1.  我读完这本书后会成为专家吗？

    如果您跟着学习，您将掌握从训练到推理再到性能最大化等各种主题的知识。尽管这本书主要关注计算机视觉，您也可以将相同的知识应用到其他领域，如文本、音频等，并且能够迅速上手。

1.  本章前面的猫是谁？

    那是 Meher 的猫，Vader。他将在本书中多次客串。不用担心，他已经签署了模特释放表。

1.  我可以联系您吗？

    当然。如果有任何问题、更正或其他问题，请给我们发送电子邮件至 PracticalDLBook@gmail.com，或在 Twitter 上@PracticalDLBook 发推文。

如果您正在阅读盗版副本，请考虑我们对您感到失望。

^(2) [Robert Geirhos 等人](https://arxiv.org/pdf/1811.12231.pdf)
